<!DOCTYPE html><html><head><title>Help for package class</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {class}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#batchSOM'>
<p>Self-Organizing Maps: Batch Algorithm</p></a></li>
<li><a href='#condense'>
<p>Condense training set for k-NN classifier</p></a></li>
<li><a href='#knn'>
<p>k-Nearest Neighbour Classification</p></a></li>
<li><a href='#knn.cv'>
<p>k-Nearest Neighbour Cross-Validatory Classification</p></a></li>
<li><a href='#knn1'>
<p>1-Nearest Neighbour Classification</p></a></li>
<li><a href='#lvq1'>
<p>Learning Vector Quantization 1</p></a></li>
<li><a href='#lvq2'>
<p>Learning Vector Quantization 2.1</p></a></li>
<li><a href='#lvq3'>
<p>Learning Vector Quantization 3</p></a></li>
<li><a href='#lvqinit'>
<p>Initialize a LVQ Codebook</p></a></li>
<li><a href='#lvqtest'>
<p>Classify Test Set from LVQ Codebook</p></a></li>
<li><a href='#multiedit'>
<p>Multiedit for k-NN Classifier</p></a></li>
<li><a href='#olvq1'>
<p>Optimized Learning Vector Quantization 1</p></a></li>
<li><a href='#reduce.nn'>
<p>Reduce Training Set for a k-NN Classifier</p></a></li>
<li><a href='#SOM'>
<p>Self-Organizing Maps: Online Algorithm</p></a></li>
<li><a href='#somgrid'>
<p>Plot SOM Fits</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Priority:</td>
<td>recommended</td>
</tr>
<tr>
<td>Version:</td>
<td>7.3-22</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-05-02</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0), stats, utils</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS</td>
</tr>
<tr>
<td>Description:</td>
<td>Various functions for classification, including k-nearest
  neighbour, Learning Vector Quantization and Self-Organizing Maps.</td>
</tr>
<tr>
<td>Title:</td>
<td>Functions for Classification</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www.stats.ox.ac.uk/pub/MASS4/">http://www.stats.ox.ac.uk/pub/MASS4/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-02 16:15:43 UTC; ripley</td>
</tr>
<tr>
<td>Author:</td>
<td>Brian Ripley [aut, cre, cph],
  William Venables [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Brian Ripley &lt;ripley@stats.ox.ac.uk&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-03 11:01:28 UTC</td>
</tr>
</table>
<hr>
<h2 id='batchSOM'>
Self-Organizing Maps: Batch Algorithm
</h2><span id='topic+batchSOM'></span>

<h3>Description</h3>

<p>Kohonen's Self-Organizing Maps are a crude form of multidimensional scaling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>batchSOM(data, grid = somgrid(), radii, init)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="batchSOM_+3A_data">data</code></td>
<td>

<p>a matrix or data frame of observations, scaled so that Euclidean
distance is appropriate.
</p>
</td></tr>
<tr><td><code id="batchSOM_+3A_grid">grid</code></td>
<td>

<p>A grid for the representatives: see <code><a href="#topic+somgrid">somgrid</a></code>.
</p>
</td></tr>
<tr><td><code id="batchSOM_+3A_radii">radii</code></td>
<td>

<p>the radii of the neighbourhood to be used for each pass: one pass is
run for each element of <code>radii</code>.
</p>
</td></tr>
<tr><td><code id="batchSOM_+3A_init">init</code></td>
<td>

<p>the initial representatives.  If missing, chosen (without replacement)
randomly from <code>data</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The batch SOM algorithm of Kohonen(1995, section 3.14) is used.
</p>


<h3>Value</h3>

<p>An object of class <code>"SOM"</code> with components
</p>
<table>
<tr><td><code>grid</code></td>
<td>
<p>the grid, an object of class <code>"somgrid"</code>.</p>
</td></tr>
<tr><td><code>codes</code></td>
<td>
<p>a matrix of representatives.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kohonen, T. (1995) <em>Self-Organizing Maps.</em> Springer-Verlag.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+somgrid">somgrid</a></code>, <code><a href="#topic+SOM">SOM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)
data(crabs, package = "MASS")

lcrabs &lt;- log(crabs[, 4:8])
crabs.grp &lt;- factor(c("B", "b", "O", "o")[rep(1:4, rep(50,4))])
gr &lt;- somgrid(topo = "hexagonal")
crabs.som &lt;- batchSOM(lcrabs, gr, c(4, 4, 2, 2, 1, 1, 1, 0, 0))
plot(crabs.som)

bins &lt;- as.numeric(knn1(crabs.som$codes, lcrabs, 0:47))
plot(crabs.som$grid, type = "n")
symbols(crabs.som$grid$pts[, 1], crabs.som$grid$pts[, 2],
        circles = rep(0.4, 48), inches = FALSE, add = TRUE)
text(crabs.som$grid$pts[bins, ] + rnorm(400, 0, 0.1),
     as.character(crabs.grp))
</code></pre>

<hr>
<h2 id='condense'>
Condense training set for k-NN classifier
</h2><span id='topic+condense'></span>

<h3>Description</h3>

<p>Condense training set for k-NN classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>condense(train, class, store, trace = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="condense_+3A_train">train</code></td>
<td>

<p>matrix for training set
</p>
</td></tr>
<tr><td><code id="condense_+3A_class">class</code></td>
<td>

<p>vector of classifications for test set
</p>
</td></tr>
<tr><td><code id="condense_+3A_store">store</code></td>
<td>

<p>initial store set. Default one randomly chosen element of the set.
</p>
</td></tr>
<tr><td><code id="condense_+3A_trace">trace</code></td>
<td>

<p>logical. Trace iterations?
</p>
</td></tr></table>


<h3>Details</h3>

<p>The store set is used to 1-NN classify the rest, and misclassified
patterns are added to the store set. The whole set is checked until
no additions occur.
</p>


<h3>Value</h3>

<p>Index vector of cases to be retained (the final store set).
</p>


<h3>References</h3>

<p>P. A. Devijver and J. Kittler (1982)
<em>Pattern Recognition. A Statistical Approach.</em>
Prentice-Hall, pp. 119&ndash;121.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+reduce.nn">reduce.nn</a></code>, <code><a href="#topic+multiedit">multiedit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
keep &lt;- condense(train, cl)
knn(train[keep, , drop=FALSE], test, cl[keep])
keep2 &lt;- reduce.nn(train, keep, cl)
knn(train[keep2, , drop=FALSE], test, cl[keep2])
</code></pre>

<hr>
<h2 id='knn'>
k-Nearest Neighbour Classification
</h2><span id='topic+knn'></span>

<h3>Description</h3>

<p>k-nearest neighbour classification for test set from training set. For
each row of the test set, the <code>k</code> nearest (in Euclidean distance)
training set vectors are found, and the classification is decided by
majority vote, with ties broken at random. If there are ties for the
<code>k</code>th nearest vector, all candidates are included in the vote.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knn(train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knn_+3A_train">train</code></td>
<td>

<p>matrix or data frame of training set cases.
</p>
</td></tr>
<tr><td><code id="knn_+3A_test">test</code></td>
<td>

<p>matrix or data frame of test set cases. A vector will be interpreted
as a row vector for a single case.
</p>
</td></tr>
<tr><td><code id="knn_+3A_cl">cl</code></td>
<td>

<p>factor of true classifications of training set
</p>
</td></tr>
<tr><td><code id="knn_+3A_k">k</code></td>
<td>

<p>number of neighbours considered.
</p>
</td></tr>
<tr><td><code id="knn_+3A_l">l</code></td>
<td>

<p>minimum vote for definite decision, otherwise <code>doubt</code>. (More
precisely, less than <code>k-l</code> dissenting votes are allowed, even if <code>k</code>
is increased by ties.)
</p>
</td></tr>
<tr><td><code id="knn_+3A_prob">prob</code></td>
<td>

<p>If this is true, the proportion of the votes for the winning class
are returned as attribute <code>prob</code>.
</p>
</td></tr>
<tr><td><code id="knn_+3A_use.all">use.all</code></td>
<td>

<p>controls handling of ties. If true, all distances equal to the <code>k</code>th
largest are included. If false, a random selection of distances
equal to the <code>k</code>th is chosen to use exactly <code>k</code> neighbours.
</p>
</td></tr></table>


<h3>Value</h3>

<p>Factor of classifications of test set. <code>doubt</code> will be returned as <code>NA</code>.
</p>


<h3>References</h3>

<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+knn1">knn1</a></code>, <code><a href="#topic+knn.cv">knn.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
knn(train, test, cl, k = 3, prob=TRUE)
attributes(.Last.value)
</code></pre>

<hr>
<h2 id='knn.cv'>
k-Nearest Neighbour Cross-Validatory Classification
</h2><span id='topic+knn.cv'></span>

<h3>Description</h3>

<p>k-nearest neighbour cross-validatory classification from training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knn.cv(train, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knn.cv_+3A_train">train</code></td>
<td>

<p>matrix or data frame of training set cases.
</p>
</td></tr>
<tr><td><code id="knn.cv_+3A_cl">cl</code></td>
<td>

<p>factor of true classifications of training set
</p>
</td></tr>
<tr><td><code id="knn.cv_+3A_k">k</code></td>
<td>

<p>number of neighbours considered.
</p>
</td></tr>
<tr><td><code id="knn.cv_+3A_l">l</code></td>
<td>

<p>minimum vote for definite decision, otherwise <code>doubt</code>. (More
precisely, less than <code>k-l</code> dissenting votes are allowed, even
if <code>k</code> is increased by ties.)
</p>
</td></tr>
<tr><td><code id="knn.cv_+3A_prob">prob</code></td>
<td>

<p>If this is true, the proportion of the votes for the winning class
are returned as attribute <code>prob</code>.
</p>
</td></tr>
<tr><td><code id="knn.cv_+3A_use.all">use.all</code></td>
<td>

<p>controls handling of ties. If true, all distances equal to the <code>k</code>th
largest are included. If false, a random selection of distances
equal to the <code>k</code>th is chosen to use exactly <code>k</code> neighbours.
</p>
</td></tr></table>


<h3>Details</h3>

<p>This uses leave-one-out cross validation.
For each row of the training set <code>train</code>, the <code>k</code> nearest
(in Euclidean distance) other
training set vectors are found, and the classification is decided by
majority vote, with ties broken at random. If there are ties for the
<code>k</code>th nearest vector, all candidates are included in the vote.
</p>


<h3>Value</h3>

<p>Factor of classifications of training set. <code>doubt</code> will be returned as <code>NA</code>.
</p>


<h3>References</h3>

<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+knn">knn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- rbind(iris3[,,1], iris3[,,2], iris3[,,3])
cl &lt;- factor(c(rep("s",50), rep("c",50), rep("v",50)))
knn.cv(train, cl, k = 3, prob = TRUE)
attributes(.Last.value)
</code></pre>

<hr>
<h2 id='knn1'>
1-Nearest Neighbour Classification
</h2><span id='topic+knn1'></span>

<h3>Description</h3>

<p>Nearest neighbour classification for test set from training set. For
each row of the test set, the nearest (by Euclidean distance) training
set vector is found, and its classification used. If there is more than
one nearest, a majority vote is used with ties broken at random.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knn1(train, test, cl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knn1_+3A_train">train</code></td>
<td>

<p>matrix or data frame of training set cases.
</p>
</td></tr>
<tr><td><code id="knn1_+3A_test">test</code></td>
<td>

<p>matrix or data frame of test set cases. A vector will be interpreted
as a row vector for a single case.
</p>
</td></tr>
<tr><td><code id="knn1_+3A_cl">cl</code></td>
<td>

<p>factor of true classification of training set.
</p>
</td></tr></table>


<h3>Value</h3>

<p>Factor of classifications of test set.
</p>


<h3>References</h3>

<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+knn">knn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
knn1(train, test, cl)
</code></pre>

<hr>
<h2 id='lvq1'>
Learning Vector Quantization 1
</h2><span id='topic+lvq1'></span>

<h3>Description</h3>

<p>Moves examples in a codebook to better represent the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lvq1(x, cl, codebk, niter = 100 * nrow(codebk$x), alpha = 0.03)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lvq1_+3A_x">x</code></td>
<td>

<p>a matrix or data frame of examples
</p>
</td></tr>
<tr><td><code id="lvq1_+3A_cl">cl</code></td>
<td>

<p>a vector or factor of classifications for the examples
</p>
</td></tr>
<tr><td><code id="lvq1_+3A_codebk">codebk</code></td>
<td>

<p>a codebook
</p>
</td></tr>
<tr><td><code id="lvq1_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="lvq1_+3A_alpha">alpha</code></td>
<td>

<p>constant for training
</p>
</td></tr></table>


<h3>Details</h3>

<p>Selects <code>niter</code> examples at random  with replacement, and adjusts the nearest
example in the codebook for each.
</p>


<h3>Value</h3>

<p>A codebook, represented as a list with components <code>x</code> and <code>cl</code>
giving the examples and classes.
</p>


<h3>References</h3>

<p>Kohonen, T. (1990) The self-organizing map.
<em>Proc. IEEE </em>
<b>78</b>, 1464&ndash;1480.
</p>
<p>Kohonen, T. (1995)
<em>Self-Organizing Maps.</em>
Springer, Berlin.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lvqinit">lvqinit</a></code>, <code><a href="#topic+olvq1">olvq1</a></code>, <code><a href="#topic+lvq2">lvq2</a></code>, <code><a href="#topic+lvq3">lvq3</a></code>, <code><a href="#topic+lvqtest">lvqtest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
cd &lt;- lvqinit(train, cl, 10)
lvqtest(cd, train)
cd0 &lt;- olvq1(train, cl, cd)
lvqtest(cd0, train)
cd1 &lt;- lvq1(train, cl, cd0)
lvqtest(cd1, train)
</code></pre>

<hr>
<h2 id='lvq2'>
Learning Vector Quantization 2.1
</h2><span id='topic+lvq2'></span>

<h3>Description</h3>

<p>Moves examples in a codebook to better represent the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lvq2(x, cl, codebk, niter = 100 * nrow(codebk$x), alpha = 0.03,
     win = 0.3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lvq2_+3A_x">x</code></td>
<td>

<p>a matrix or data frame of examples
</p>
</td></tr>
<tr><td><code id="lvq2_+3A_cl">cl</code></td>
<td>

<p>a vector or factor of classifications for the examples
</p>
</td></tr>
<tr><td><code id="lvq2_+3A_codebk">codebk</code></td>
<td>

<p>a codebook
</p>
</td></tr>
<tr><td><code id="lvq2_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="lvq2_+3A_alpha">alpha</code></td>
<td>

<p>constant for training
</p>
</td></tr>
<tr><td><code id="lvq2_+3A_win">win</code></td>
<td>

<p>a tolerance for the closeness of the two nearest vectors.
</p>
</td></tr></table>


<h3>Details</h3>

<p>Selects <code>niter</code> examples at random  with replacement, and adjusts the nearest
two examples in the codebook if one is correct and the other incorrect.
</p>


<h3>Value</h3>

<p>A codebook, represented as a list with components <code>x</code> and <code>cl</code>
giving the examples and classes.
</p>


<h3>References</h3>

<p>Kohonen, T. (1990) The self-organizing map.
<em>Proc. IEEE</em>
<b>78</b>, 1464&ndash;1480.
</p>
<p>Kohonen, T. (1995)
<em>Self-Organizing Maps.</em>
Springer, Berlin.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lvqinit">lvqinit</a></code>, <code><a href="#topic+lvq1">lvq1</a></code>, <code><a href="#topic+olvq1">olvq1</a></code>,
<code><a href="#topic+lvq3">lvq3</a></code>, <code><a href="#topic+lvqtest">lvqtest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
cd &lt;- lvqinit(train, cl, 10)
lvqtest(cd, train)
cd0 &lt;- olvq1(train, cl, cd)
lvqtest(cd0, train)
cd2 &lt;- lvq2(train, cl, cd0)
lvqtest(cd2, train)
</code></pre>

<hr>
<h2 id='lvq3'>
Learning Vector Quantization 3
</h2><span id='topic+lvq3'></span>

<h3>Description</h3>

<p>Moves examples in a codebook to better represent the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lvq3(x, cl, codebk, niter = 100*nrow(codebk$x), alpha = 0.03,
     win = 0.3, epsilon = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lvq3_+3A_x">x</code></td>
<td>

<p>a matrix or data frame of examples
</p>
</td></tr>
<tr><td><code id="lvq3_+3A_cl">cl</code></td>
<td>

<p>a vector or factor of classifications for the examples
</p>
</td></tr>
<tr><td><code id="lvq3_+3A_codebk">codebk</code></td>
<td>

<p>a codebook
</p>
</td></tr>
<tr><td><code id="lvq3_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="lvq3_+3A_alpha">alpha</code></td>
<td>

<p>constant for training
</p>
</td></tr>
<tr><td><code id="lvq3_+3A_win">win</code></td>
<td>

<p>a tolerance for the closeness of the two nearest vectors.
</p>
</td></tr>
<tr><td><code id="lvq3_+3A_epsilon">epsilon</code></td>
<td>

<p>proportion of move for correct vectors
</p>
</td></tr></table>


<h3>Details</h3>

<p>Selects <code>niter</code> examples at random  with replacement, and adjusts the nearest
two examples in the codebook for each.
</p>


<h3>Value</h3>

<p>A codebook, represented as a list with components <code>x</code> and <code>cl</code>
giving the examples and classes.
</p>


<h3>References</h3>

<p>Kohonen, T. (1990) The self-organizing map.
<em>Proc. IEEE</em>
<b>78</b>, 1464&ndash;1480.
</p>
<p>Kohonen, T. (1995)
<em>Self-Organizing Maps.</em>
Springer, Berlin.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lvqinit">lvqinit</a></code>, <code><a href="#topic+lvq1">lvq1</a></code>, <code><a href="#topic+olvq1">olvq1</a></code>,
<code><a href="#topic+lvq2">lvq2</a></code>, <code><a href="#topic+lvqtest">lvqtest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
cd &lt;- lvqinit(train, cl, 10)
lvqtest(cd, train)
cd0 &lt;- olvq1(train, cl, cd)
lvqtest(cd0, train)
cd3 &lt;- lvq3(train, cl, cd0)
lvqtest(cd3, train)
</code></pre>

<hr>
<h2 id='lvqinit'>
Initialize a LVQ Codebook
</h2><span id='topic+lvqinit'></span>

<h3>Description</h3>

<p>Construct an initial codebook for LVQ methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lvqinit(x, cl, size, prior, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lvqinit_+3A_x">x</code></td>
<td>

<p>a matrix or data frame of training examples, <code>n</code> by <code>p</code>.
</p>
</td></tr>
<tr><td><code id="lvqinit_+3A_cl">cl</code></td>
<td>

<p>the classifications for the training examples. A vector or factor of
length <code>n</code>.
</p>
</td></tr>
<tr><td><code id="lvqinit_+3A_size">size</code></td>
<td>

<p>the size of the codebook. Defaults to <code>min(round(0.4*ng*(ng-1 + p/2),0), n)</code>
where <code>ng</code> is the number of classes.
</p>
</td></tr>
<tr><td><code id="lvqinit_+3A_prior">prior</code></td>
<td>

<p>Probabilities to represent classes in the codebook. Default proportions in the
training set.
</p>
</td></tr>
<tr><td><code id="lvqinit_+3A_k">k</code></td>
<td>

<p>k used for k-NN test of correct classification. Default is 5.
</p>
</td></tr></table>


<h3>Details</h3>

<p>Selects <code>size</code> examples from the training set without replacement with
proportions proportional to the prior or the original proportions.
</p>


<h3>Value</h3>

<p>A codebook, represented as a list with components <code>x</code> and <code>cl</code> giving
the examples and classes.
</p>


<h3>References</h3>

<p>Kohonen, T. (1990) The self-organizing map.
<em>Proc. IEEE </em>
<b>78</b>, 1464&ndash;1480.
</p>
<p>Kohonen, T. (1995)
<em>Self-Organizing Maps.</em>
Springer, Berlin.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lvq1">lvq1</a></code>, <code><a href="#topic+lvq2">lvq2</a></code>, <code><a href="#topic+lvq3">lvq3</a></code>, <code><a href="#topic+olvq1">olvq1</a></code>, <code><a href="#topic+lvqtest">lvqtest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
cd &lt;- lvqinit(train, cl, 10)
lvqtest(cd, train)
cd1 &lt;- olvq1(train, cl, cd)
lvqtest(cd1, train)
</code></pre>

<hr>
<h2 id='lvqtest'>
Classify Test Set from LVQ Codebook
</h2><span id='topic+lvqtest'></span>

<h3>Description</h3>

<p>Classify a test set by 1-NN from a specified LVQ codebook.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lvqtest(codebk, test)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lvqtest_+3A_codebk">codebk</code></td>
<td>

<p>codebook object returned by other LVQ software
</p>
</td></tr>
<tr><td><code id="lvqtest_+3A_test">test</code></td>
<td>

<p>matrix of test examples
</p>
</td></tr></table>


<h3>Details</h3>

<p>Uses 1-NN to classify each test example against the codebook.
</p>


<h3>Value</h3>

<p>Factor of classification for each row of <code>x</code>
</p>


<h3>References</h3>

<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lvqinit">lvqinit</a></code>, <code><a href="#topic+olvq1">olvq1</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The function is currently defined as
function(codebk, test) knn1(codebk$x, test, codebk$cl)
</code></pre>

<hr>
<h2 id='multiedit'>
Multiedit for k-NN Classifier
</h2><span id='topic+multiedit'></span>

<h3>Description</h3>

<p>Multiedit for k-NN classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiedit(x, class, k = 1, V = 3, I = 5, trace = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiedit_+3A_x">x</code></td>
<td>

<p>matrix of training set.
</p>
</td></tr>
<tr><td><code id="multiedit_+3A_class">class</code></td>
<td>

<p>vector of classification of training set.
</p>
</td></tr>
<tr><td><code id="multiedit_+3A_k">k</code></td>
<td>

<p>number of neighbours used in k-NN.
</p>
</td></tr>
<tr><td><code id="multiedit_+3A_v">V</code></td>
<td>

<p>divide training set into V parts.
</p>
</td></tr>
<tr><td><code id="multiedit_+3A_i">I</code></td>
<td>

<p>number of null passes before quitting.
</p>
</td></tr>
<tr><td><code id="multiedit_+3A_trace">trace</code></td>
<td>

<p>logical for statistics at each pass.
</p>
</td></tr></table>


<h3>Value</h3>

<p>Index vector of cases to be retained.
</p>


<h3>References</h3>

<p>P. A. Devijver and J. Kittler (1982)
<em>Pattern Recognition. A Statistical Approach.</em>
Prentice-Hall, p. 115.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+condense">condense</a></code>, <code><a href="#topic+reduce.nn">reduce.nn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tr &lt;- sample(1:50, 25)
train &lt;- rbind(iris3[tr,,1], iris3[tr,,2], iris3[tr,,3])
test &lt;- rbind(iris3[-tr,,1], iris3[-tr,,2], iris3[-tr,,3])
cl &lt;- factor(c(rep(1,25),rep(2,25), rep(3,25)), labels=c("s", "c", "v"))
table(cl, knn(train, test, cl, 3))
ind1 &lt;- multiedit(train, cl, 3)
length(ind1)
table(cl, knn(train[ind1, , drop=FALSE], test, cl[ind1], 1))
ntrain &lt;- train[ind1,]; ncl &lt;- cl[ind1]
ind2 &lt;- condense(ntrain, ncl)
length(ind2)
table(cl, knn(ntrain[ind2, , drop=FALSE], test, ncl[ind2], 1))
</code></pre>

<hr>
<h2 id='olvq1'>
Optimized Learning Vector Quantization 1
</h2><span id='topic+olvq1'></span>

<h3>Description</h3>

<p>Moves examples in a codebook to better represent the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>olvq1(x, cl, codebk, niter = 40 * nrow(codebk$x), alpha = 0.3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="olvq1_+3A_x">x</code></td>
<td>

<p>a matrix or data frame of examples
</p>
</td></tr>
<tr><td><code id="olvq1_+3A_cl">cl</code></td>
<td>

<p>a vector or factor of classifications for the examples
</p>
</td></tr>
<tr><td><code id="olvq1_+3A_codebk">codebk</code></td>
<td>

<p>a codebook
</p>
</td></tr>
<tr><td><code id="olvq1_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="olvq1_+3A_alpha">alpha</code></td>
<td>

<p>constant for training
</p>
</td></tr></table>


<h3>Details</h3>

<p>Selects <code>niter</code> examples at random with replacement, and adjusts the
nearest example in the codebook for each.
</p>


<h3>Value</h3>

<p>A codebook, represented as a list with components <code>x</code> and <code>cl</code> giving
the examples and classes.
</p>


<h3>References</h3>

<p>Kohonen, T. (1990) The self-organizing map.
<em>Proc. IEEE</em>
<b>78</b>, 1464&ndash;1480.
</p>
<p>Kohonen, T. (1995)
<em>Self-Organizing Maps.</em>
Springer, Berlin.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lvqinit">lvqinit</a></code>, <code><a href="#topic+lvqtest">lvqtest</a></code>, <code><a href="#topic+lvq1">lvq1</a></code>, <code><a href="#topic+lvq2">lvq2</a></code>, <code><a href="#topic+lvq3">lvq3</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
cd &lt;- lvqinit(train, cl, 10)
lvqtest(cd, train)
cd1 &lt;- olvq1(train, cl, cd)
lvqtest(cd1, train)
</code></pre>

<hr>
<h2 id='reduce.nn'>
Reduce Training Set for a k-NN Classifier
</h2><span id='topic+reduce.nn'></span>

<h3>Description</h3>

<p>Reduce training set for a k-NN classifier. Used after <code>condense</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reduce.nn(train, ind, class)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reduce.nn_+3A_train">train</code></td>
<td>

<p>matrix for training set
</p>
</td></tr>
<tr><td><code id="reduce.nn_+3A_ind">ind</code></td>
<td>

<p>Initial list of members of the training set (from <code>condense</code>).
</p>
</td></tr>
<tr><td><code id="reduce.nn_+3A_class">class</code></td>
<td>

<p>vector of classifications for test set
</p>
</td></tr></table>


<h3>Details</h3>

<p>All the members of the training set are tried in random order.
Any which when dropped do not cause any members of the training set to
be wrongly classified are dropped.
</p>


<h3>Value</h3>

<p>Index vector of cases to be retained.
</p>


<h3>References</h3>

<p>Gates, G.W. (1972) The reduced nearest neighbor rule.
<em>IEEE Trans. Information Theory</em>
<b>IT-18</b>, 431&ndash;432.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+condense">condense</a></code>, <code><a href="#topic+multiedit">multiedit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
keep &lt;- condense(train, cl)
knn(train[keep,], test, cl[keep])
keep2 &lt;- reduce.nn(train, keep, cl)
knn(train[keep2,], test, cl[keep2])
</code></pre>

<hr>
<h2 id='SOM'>
Self-Organizing Maps: Online Algorithm
</h2><span id='topic+SOM'></span>

<h3>Description</h3>

<p>Kohonen's Self-Organizing Maps are a crude form of multidimensional scaling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SOM(data, grid = somgrid(), rlen = 10000, alpha, radii, init)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SOM_+3A_data">data</code></td>
<td>

<p>a matrix or data frame of observations, scaled so that Euclidean
distance is appropriate.
</p>
</td></tr>
<tr><td><code id="SOM_+3A_grid">grid</code></td>
<td>

<p>A grid for the representatives: see <code><a href="#topic+somgrid">somgrid</a></code>.
</p>
</td></tr>
<tr><td><code id="SOM_+3A_rlen">rlen</code></td>
<td>

<p>the number of updates: used only in the defaults for <code>alpha</code> and <code>radii</code>.
</p>
</td></tr>
<tr><td><code id="SOM_+3A_alpha">alpha</code></td>
<td>

<p>the amount of change: one update is done for each element of <code>alpha</code>.
Default is to decline linearly from 0.05 to 0 over <code>rlen</code> updates.
</p>
</td></tr>
<tr><td><code id="SOM_+3A_radii">radii</code></td>
<td>

<p>the radii of the neighbourhood to be used for each update: must be the
same length as <code>alpha</code>.  Default is to decline linearly from 4 to 1
over <code>rlen</code> updates.
</p>
</td></tr>
<tr><td><code id="SOM_+3A_init">init</code></td>
<td>

<p>the initial representatives.  If missing, chosen (without replacement)
randomly from <code>data</code>.
</p>
</td></tr></table>


<h3>Details</h3>

<p><code>alpha</code> and <code>radii</code> can also be lists, in which case each component is
used in turn, allowing two- or more phase training.
</p>


<h3>Value</h3>

<p>An object of class <code>"SOM"</code> with components
</p>
<table>
<tr><td><code>grid</code></td>
<td>

<p>the grid, an object of class <code>"somgrid"</code>.
</p>
</td></tr>
<tr><td><code>codes</code></td>
<td>

<p>a matrix of representatives.
</p>
</td></tr></table>


<h3>References</h3>

<p>Kohonen, T. (1995) <em>Self-Organizing Maps.</em> Springer-Verlag
</p>
<p>Kohonen, T., Hynninen, J., Kangas, J. and Laaksonen, J. (1996)
<em>SOM PAK: The self-organizing map program package.</em>
Laboratory of Computer and Information Science, Helsinki University
of Technology, Technical Report A31.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+somgrid">somgrid</a></code>, <code><a href="#topic+batchSOM">batchSOM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)
data(crabs, package = "MASS")

lcrabs &lt;- log(crabs[, 4:8])
crabs.grp &lt;- factor(c("B", "b", "O", "o")[rep(1:4, rep(50,4))])
gr &lt;- somgrid(topo = "hexagonal")
crabs.som &lt;- SOM(lcrabs, gr)
plot(crabs.som)

## 2-phase training
crabs.som2 &lt;- SOM(lcrabs, gr,
    alpha = list(seq(0.05, 0, length.out = 1e4), seq(0.02, 0, length.out = 1e5)),
    radii = list(seq(8, 1, length.out = 1e4), seq(4, 1, length.out = 1e5)))
plot(crabs.som2)
</code></pre>

<hr>
<h2 id='somgrid'>
Plot SOM Fits
</h2><span id='topic+somgrid'></span><span id='topic+plot.somgrid'></span><span id='topic+plot.SOM'></span>

<h3>Description</h3>

<p>Plotting functions for SOM results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>somgrid(xdim = 8, ydim = 6, topo = c("rectangular", "hexagonal"))

## S3 method for class 'somgrid'
plot(x, type = "p", ...)

## S3 method for class 'SOM'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="somgrid_+3A_xdim">xdim</code>, <code id="somgrid_+3A_ydim">ydim</code></td>
<td>
<p>dimensions of the grid</p>
</td></tr>
<tr><td><code id="somgrid_+3A_topo">topo</code></td>
<td>
<p>the topology of the grid.</p>
</td></tr>
<tr><td><code id="somgrid_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>"somgrid"</code> or <code>"SOM"</code>.</p>
</td></tr>
<tr><td><code id="somgrid_+3A_type">type</code>, <code id="somgrid_+3A_...">...</code></td>
<td>
<p>graphical parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The class <code>"somgrid"</code> records the coordinates of the grid to be
used for (batch or on-line) SOM: this has a plot method.
</p>
<p>The plot method for class <code>"SOM"</code> plots a <code><a href="graphics.html#topic+stars">stars</a></code>
plot of the representative at each grid point.
</p>


<h3>Value</h3>

<p>For <code>somgrid</code>, an object of class <code>"somgrid"</code>, a list with
components
</p>
<table>
<tr><td><code>pts</code></td>
<td>
<p>a two-column matrix giving locations for the grid points.</p>
</td></tr>
<tr><td><code>xdim</code>, <code>ydim</code>, <code>topo</code></td>
<td>
<p>as in the arguments to <code>somgrid</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+batchSOM">batchSOM</a></code>, <code><a href="#topic+SOM">SOM</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
