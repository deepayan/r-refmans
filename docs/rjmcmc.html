<!DOCTYPE html><html><head><title>Help for package rjmcmc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rjmcmc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adiff'><p>Automatic Differentiation Using Madness</p></a></li>
<li><a href='#defaultpost'><p>Perform Post-Processing Using Default Bijections</p></a></li>
<li><a href='#getsampler'><p>Define Function To Sample From MCMC Output</p></a></li>
<li><a href='#rjmcmc'><p>The rjmcmc Package</p></a></li>
<li><a href='#rjmcmcpost'><p>Perform Reversible-Jump MCMC Post-Processing</p></a></li>
<li><a href='#rjmethods'><p>Methods for the rj Class</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Reversible-Jump MCMC Using Post-Processing</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-07-07</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs reversible-jump Markov chain Monte Carlo (Green, 1995)
    &lt;<a href="https://doi.org/10.2307%2F2337340">doi:10.2307/2337340</a>&gt;, specifically the restriction introduced by 
    Barker &amp; Link (2013) &lt;<a href="https://doi.org/10.1080%2F00031305.2013.791644">doi:10.1080/00031305.2013.791644</a>&gt;. By utilising 
    a 'universal parameter' space, RJMCMC is treated as a Gibbs sampling 
    problem. Previously-calculated posterior distributions are used to 
    quickly estimate posterior model probabilities. Jacobian matrices are 
    found using automatic differentiation. For a detailed description of
    the package, see Gelling, Schofield &amp; Barker (2019)
    &lt;<a href="https://doi.org/10.1111%2Fanzs.12263">doi:10.1111/anzs.12263</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>madness, R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>utils, coda, mvtnorm</td>
</tr>
<tr>
<td>Suggests:</td>
<td>FSAdata</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-07-07 00:02:12 UTC; rachaelyoung</td>
</tr>
<tr>
<td>Author:</td>
<td>Nick Gelling [aut, cre],
  Matthew R. Schofield [aut],
  Richard J. Barker [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nick Gelling &lt;nickcjgelling@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-07-09 14:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='adiff'>Automatic Differentiation Using Madness</h2><span id='topic+adiff'></span>

<h3>Description</h3>

<p>A wrapper function to the functionality of the <code>madness</code> package. 
Converts a given <code>x</code> to a madness object and then applies <code>func</code> to
it, returning the result and the Jacobian for the transformation <code>func</code>.
<code>adiff</code> is used by the <code><a href="#topic+rjmcmcpost">rjmcmcpost</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adiff(func, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adiff_+3A_func">func</code></td>
<td>
<p>The function to be differentiated (usually user-defined).</p>
</td></tr>
<tr><td><code id="adiff_+3A_x">x</code></td>
<td>
<p>The values at which to evaluate the function <code>func</code>.</p>
</td></tr>
<tr><td><code id="adiff_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>func</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector or matrix containing the result of the function 
evaluation <code>func(x, ...)</code>. The <code>"gradient"</code> attribute of this 
object contains the Jacobian matrix of the transformation <code>func</code>.
</p>


<h3>References</h3>

<p>Pav, S. E. (2016) Madness: Automatic Differentiation of
Multivariate Operations.
</p>


<h3>See Also</h3>

<p><code><a href="madness.html#topic+madness">madness</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x2x3 = function(x){
return(c(x^2, x^3))
}

y = adiff(x2x3, c(5,6))
attr(y, "gradient")

</code></pre>

<hr>
<h2 id='defaultpost'>Perform Post-Processing Using Default Bijections</h2><span id='topic+defaultpost'></span>

<h3>Description</h3>

<p>Performs Bayesian multimodel inference, estimating Bayes factors and 
posterior model probabilities for N candidate models. Unlike 
<code><a href="#topic+rjmcmcpost">rjmcmcpost</a></code>, this function uses a default bijection scheme based
on approximating each posterior by a multivariate normal distribution. The 
result is reminiscent of the algorithm of Carlin &amp; Chib (1995) with a 
multivariate normal pseudo-prior. Transformation Jacobians are computed using
automatic differentiation so do not need to be specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>defaultpost(posterior, likelihood, param.prior, model.prior,
  chainlength = 10000, TM.thin = chainlength/10, progress = TRUE,
  save.all = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="defaultpost_+3A_posterior">posterior</code></td>
<td>
<p>A list of N matrices containing the posterior distributions 
under each model. Generally this will be obtained from MCMC output. Note
that each parameter should be real-valued so some parameters may need to be
transformed, using logarithms for example.</p>
</td></tr>
<tr><td><code id="defaultpost_+3A_likelihood">likelihood</code></td>
<td>
<p>A list of N functions specifying the log-likelihood 
functions for the data under each model.</p>
</td></tr>
<tr><td><code id="defaultpost_+3A_param.prior">param.prior</code></td>
<td>
<p>A list of N functions specifying the prior distributions 
for each model-specific parameter vector.</p>
</td></tr>
<tr><td><code id="defaultpost_+3A_model.prior">model.prior</code></td>
<td>
<p>A numeric vector of the prior model probabilities. Note 
that this argument is not required to sum to one as it is automatically 
normalised.</p>
</td></tr>
<tr><td><code id="defaultpost_+3A_chainlength">chainlength</code></td>
<td>
<p>How many iterations to run the Markov chain for.</p>
</td></tr>
<tr><td><code id="defaultpost_+3A_tm.thin">TM.thin</code></td>
<td>
<p>How regularly to calculate transition matrices as the chain 
progresses.</p>
</td></tr>
<tr><td><code id="defaultpost_+3A_progress">progress</code></td>
<td>
<p>A logical determining whether a progress bar is drawn.</p>
</td></tr>
<tr><td><code id="defaultpost_+3A_save.all">save.all</code></td>
<td>
<p>A logical determining whether to save the value of the 
universal parameter at each iteration, as well as the corresponding 
likelihoods, priors and posteriors. If <code>TRUE</code>, the output object 
occupies significantly more memory.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>rj</code> (see <code><a href="#topic+rjmethods">rjmethods</a></code>). 
If <code>save.all=TRUE</code>, the output has named elements <code>result</code>, 
<code>densities</code>, <code>psidraws</code>, <code>progress</code> and <code>meta</code>. If 
<code>save.all=FALSE</code>, the <code>densities</code> and <code>psidraws</code> elements 
are omitted.
</p>
<p><code>result</code> contains useful point estimates, <code>progress</code> contains
snapshots of these estimates over time, and <code>meta</code> contains
information about the function call.
</p>


<h3>References</h3>

<p>Carlin, B. P. and Chib, S. (1995) Bayesian Model Choice via 
Markov Chain Monte Carlo Methods. <em>Journal of the Royal Statistical 
Society, Series B, 473-484</em>.
</p>
<p>Barker, R. J. and Link, W. A. (2013) Bayesian multimodel 
inference by RJMCMC: A Gibbs sampling approach. <em>The American 
Statistician, 67(3), 150-156</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adiff">adiff</a></code> <code><a href="#topic+rjmcmcpost">rjmcmcpost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Comparing two binomial models -- see Barker &amp; Link (2013) for further details.

y=c(8,16); sumy=sum(y)
n=c(20,30); sumn=sum(n)

L1=function(p){if((all(p&gt;=0))&amp;&amp;(all(p&lt;=1))) sum(dbinom(y,n,p,log=TRUE)) else -Inf}
L2=function(p){if((p[1]&gt;=0)&amp;&amp;(p[1]&lt;=1)) sum(dbinom(y,n,p[1],log=TRUE)) else -Inf}

p.prior1=function(p){sum(dbeta(p,1,1,log=TRUE))}
p.prior2=function(p){dbeta(p[1],1,1,log=TRUE)+dbeta(p[2],17,15,log=TRUE)}

draw1=matrix(rbeta(2000,y+1,n-y+1), 1000, 2, byrow=TRUE)  ## full conditional posterior
draw2=matrix(c(rbeta(1000,sumy+1,sumn-sumy+1),rbeta(1000,17,15)), 1000, 2)

out=defaultpost(posterior=list(draw1,draw2), likelihood=list(L1,L2), 
                param.prior=list(p.prior1,p.prior2), model.prior=c(1,1), chainlength=1000)

</code></pre>

<hr>
<h2 id='getsampler'>Define Function To Sample From MCMC Output</h2><span id='topic+getsampler'></span>

<h3>Description</h3>

<p>A utility function which accepts a matrix of MCMC output and creates a 
function which samples from the posterior distribution for the parameters of 
the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getsampler(modelfit, sampler.name = "post.draw", order = "default",
  envir = .GlobalEnv)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getsampler_+3A_modelfit">modelfit</code></td>
<td>
<p>A matrix of output from a previously-run MCMC algorithm, with 
one column per variable and one row per iteration.</p>
</td></tr>
<tr><td><code id="getsampler_+3A_sampler.name">sampler.name</code></td>
<td>
<p>A string giving the desired name for the function to be 
defined.</p>
</td></tr>
<tr><td><code id="getsampler_+3A_order">order</code></td>
<td>
<p>A numeric vector of indices specifying the desired parameters to
extract from <code>modelfit</code>, and in which order.</p>
</td></tr>
<tr><td><code id="getsampler_+3A_envir">envir</code></td>
<td>
<p>The environment in which to define the sampling function. 
Defaults to the global environment.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A function is defined in <code>envir</code> which randomly samples from the
posterior distribution for the parameters. Note that this function does not
take any arguments. A function generated in this way is suitable for 
passing to the <code>rjmcmcpost</code> function.
</p>


<h3>References</h3>

<p>Plummer, M. (2003) JAGS: A program for analysis of Bayesian 
graphical models using Gibbs sampling. <em>Proceedings of the 3rd 
international workshop on distributed statistical computing (Vol. 124, p. 
125)</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rjmcmcpost">rjmcmcpost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate synthetic 'MCMC output' for a model with 3 parameters. There is
# one column per parameter, and 1000 iterations.
matrix_output = matrix(c(runif(1000,0,1), rnorm(1000,5,2), rgamma(1000,2,2)), 1000, 3)

getsampler(modelfit=matrix_output, sampler.name="posterior1")
set.seed(100)
posterior1()

## Alternatively
posterior1b = getsampler(modelfit=matrix_output)  # this successfully defines a function named
# posterior1b but also defines an identical function corresponding to the value 
# of sampler.name, i.e. the default "post.draw" in this case.
set.seed(100)
posterior1b()
set.seed(100)
posterior1()

</code></pre>

<hr>
<h2 id='rjmcmc'>The rjmcmc Package</h2><span id='topic+rjmcmc'></span><span id='topic+rjmcmc-package'></span>

<h3>Description</h3>

<p>Performs reversible-jump MCMC (Green, 1995), specifically the reformulation 
introduced by Barker &amp; Link (2013). Using a 'universal parameter' space, 
RJMCMC is treated as Gibbs sampling making it simpler to implement. The 
required Jacobian matrices are calculated automatically, utilising the 
<code>madness</code> package.
</p>


<h3>Functions</h3>

<p><code><a href="#topic+rjmcmcpost">rjmcmcpost</a></code> <code><a href="#topic+defaultpost">defaultpost</a></code> <code><a href="#topic+adiff">adiff</a></code>
<code><a href="#topic+getsampler">getsampler</a></code>
</p>


<h3>Methods</h3>

<p><code><a href="#topic+rjmethods">rjmethods</a></code>
</p>


<h3>References</h3>

<p>Green, P. J. (1995) Reversible jump Markov chain Monte Carlo
computation and Bayesian model determination. <em>Biometrika, 711-732</em>.
</p>
<p>Barker, R. J. and Link, W. A. (2013) Bayesian multimodel
inference by RJMCMC: A Gibbs sampling approach. <em>The American
Statistician, 67(3), 150-156</em>.
</p>

<hr>
<h2 id='rjmcmcpost'>Perform Reversible-Jump MCMC Post-Processing</h2><span id='topic+rjmcmcpost'></span>

<h3>Description</h3>

<p>Performs Bayesian multimodel inference, estimating Bayes factors and 
posterior model probabilities for N candidate models. Using the 'universal 
parameter' restriction in Barker &amp; Link (2013), RJMCMC is treated as a Gibbs 
sampling problem, where the algorithm alternates between updating the model 
and the model specific parameters. Transformation Jacobians are computed 
using automatic differentiation so do not need to be specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rjmcmcpost(post.draw, g, ginv, likelihood, param.prior, model.prior,
  chainlength = 10000, TM.thin = chainlength/10, save.all = FALSE,
  progress = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rjmcmcpost_+3A_post.draw">post.draw</code></td>
<td>
<p>A list of N functions that randomly draw from the posterior 
distribution under each model. Generally these functions sample from the 
output of a model fitted using MCMC.</p>
</td></tr>
<tr><td><code id="rjmcmcpost_+3A_g">g</code></td>
<td>
<p>A list of N functions specifying the bijections from the universal 
parameter <code>psi</code> to each model-specific parameter set.</p>
</td></tr>
<tr><td><code id="rjmcmcpost_+3A_ginv">ginv</code></td>
<td>
<p>A list of N functions specifying the bijections from each 
model-specific parameter set to <code>psi</code>. These are the inverse 
transformations of <code>g</code>.</p>
</td></tr>
<tr><td><code id="rjmcmcpost_+3A_likelihood">likelihood</code></td>
<td>
<p>A list of N functions specifying the log-likelihood 
functions for the data under each model.</p>
</td></tr>
<tr><td><code id="rjmcmcpost_+3A_param.prior">param.prior</code></td>
<td>
<p>A list of N functions specifying the log-prior 
distributions for each model-specific parameter vector.</p>
</td></tr>
<tr><td><code id="rjmcmcpost_+3A_model.prior">model.prior</code></td>
<td>
<p>A numeric vector of the prior model probabilities. Note 
that this argument is not required to sum to one as it is automatically 
normalised.</p>
</td></tr>
<tr><td><code id="rjmcmcpost_+3A_chainlength">chainlength</code></td>
<td>
<p>How many iterations to run the Markov chain for.</p>
</td></tr>
<tr><td><code id="rjmcmcpost_+3A_tm.thin">TM.thin</code></td>
<td>
<p>How regularly to calculate transition matrices as the chain 
progresses.</p>
</td></tr>
<tr><td><code id="rjmcmcpost_+3A_save.all">save.all</code></td>
<td>
<p>A logical determining whether to save the value of the 
universal parameter at each iteration, as well as the corresponding 
likelihoods, priors and posteriors. If <code>TRUE</code>, the output object 
occupies significantly more memory.</p>
</td></tr>
<tr><td><code id="rjmcmcpost_+3A_progress">progress</code></td>
<td>
<p>A logical determining whether a progress bar is drawn.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>rj</code> (see <code><a href="#topic+rjmethods">rjmethods</a></code>). 
If <code>save.all=TRUE</code>, the output has named elements <code>result</code>, 
<code>densities</code>, <code>psidraws</code>, <code>progress</code> and <code>meta</code>. If 
<code>save.all=FALSE</code>, the <code>densities</code> and <code>psidraws</code> elements 
are omitted.
</p>
<p><code>result</code> contains useful point estimates, <code>progress</code> contains
snapshots of these estimates over time, and <code>meta</code> contains
information about the function call.
</p>


<h3>References</h3>

<p>Barker, R. J. and Link, W. A. (2013) Bayesian multimodel 
inference by RJMCMC: A Gibbs sampling approach. <em>The American 
Statistician, 67(3), 150-156</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adiff">adiff</a></code> <code><a href="#topic+getsampler">getsampler</a></code> 
<code><a href="#topic+defaultpost">defaultpost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Comparing two binomial models -- see Barker &amp; Link (2013) for further details.

y=c(8,16); sumy=sum(y)
n=c(20,30); sumn=sum(n)

L1=function(p){if((all(p&gt;=0))&amp;&amp;(all(p&lt;=1))) sum(dbinom(y,n,p,log=TRUE)) else -Inf}
L2=function(p){if((p[1]&gt;=0)&amp;&amp;(p[1]&lt;=1)) sum(dbinom(y,n,p[1],log=TRUE)) else -Inf}

g1=function(psi){p=psi}
g2=function(psi){w=n[1]/sum(n); p=c(w*psi[1]+(1-w)*psi[2],psi[2])}
ginv1=function(p){p}
ginv2=function(p){c(sum(n)/n[1]*p[1]-n[2]/n[1]*p[2],p[2])}

p.prior1=function(p){sum(dbeta(p,1,1,log=TRUE))}
p.prior2=function(p){dbeta(p[1],1,1,log=TRUE)+dbeta(p[2],17,15,log=TRUE)}

draw1=function(){rbeta(2,y+1,n-y+1)}
draw2=function(){c(rbeta(1,sumy+1,sumn-sumy+1),rbeta(1,17,15))}

out=rjmcmcpost(post.draw=list(draw1,draw2), g=list(g1,g2), ginv=list(ginv1,ginv2),
               likelihood=list(L1,L2), param.prior=list(p.prior1,p.prior2),
               model.prior=c(0.5,0.5), chainlength=1500)

</code></pre>

<hr>
<h2 id='rjmethods'>Methods for the rj Class</h2><span id='topic+rjmethods'></span><span id='topic+print.rj'></span><span id='topic+plot.rj'></span><span id='topic+summary.rj'></span>

<h3>Description</h3>

<p>An object of class <code>rj</code> is returned from the functions 
<code><a href="#topic+rjmcmcpost">rjmcmcpost</a></code> or <code><a href="#topic+defaultpost">defaultpost</a></code>. The following methods 
can be applied to an object of this class. See Details for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rj'
print(x, ...)

## S3 method for class 'rj'
plot(x, legend = TRUE, col = "maroon4", ylim = c(0, 1),
  lwd = 2, lty = c(1, 1, 1), ...)

## S3 method for class 'rj'
summary(object, quantiles = c(0.025, 0.25, 0.5, 0.75,
  0.975), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rjmethods_+3A_x">x</code>, <code id="rjmethods_+3A_object">object</code></td>
<td>
<p>An object of class <code>rj</code>.</p>
</td></tr>
<tr><td><code id="rjmethods_+3A_...">...</code></td>
<td>
<p>Further arguments to the generic method.</p>
</td></tr>
<tr><td><code id="rjmethods_+3A_legend">legend</code>, <code id="rjmethods_+3A_col">col</code>, <code id="rjmethods_+3A_ylim">ylim</code>, <code id="rjmethods_+3A_lwd">lwd</code>, <code id="rjmethods_+3A_lty">lty</code></td>
<td>
<p>Some useful graphical arguments to the generic
<code>plot</code> method.</p>
</td></tr>
<tr><td><code id="rjmethods_+3A_quantiles">quantiles</code></td>
<td>
<p>The desired density quantiles for <code>summary</code> to find.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>print</code> method prints the point estimates obtained from the 
algorithm, including the transition matrix, posterior model probabilities and
Bayes factors.
</p>
<p>The <code>plot</code> method plots how the estimates of the posterior probabilities
changed as the algorithm progressed, illustrating convergence.
</p>
<p>The <code>summary</code> method returns quantiles of the posterior densities for
each model (as well as likelihoods and prior densities). The point estimates
as in <code>print</code> are also returned. Note that this requires <code>save.all</code>
must be <code>TRUE</code> in the <code>rjmcmcpost</code> call.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
