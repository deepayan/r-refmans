<!DOCTYPE html><html lang="en"><head><title>Help for package snfa</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {snfa}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#allocative.efficiency'><p>Allocative efficiency estimation</p></a></li>
<li><a href='#fit.boundary'><p>Multivariate smooth boundary fitting with additional constraints</p></a></li>
<li><a href='#fit.mean'><p>Kernel smoothing with additional constraints</p></a></li>
<li><a href='#fit.sf'><p>Non-parametric stochastic frontier</p></a></li>
<li><a href='#H.inv.select'><p>Bandwidth matrix selection</p></a></li>
<li><a href='#panel.production'><p>Randomly generated panel of production data</p></a></li>
<li><a href='#reflect.data'><p>Data reflection for kernel smoothing</p></a></li>
<li><a href='#technical.efficiency.change'><p>Technical and efficiency change estimation</p></a></li>
<li><a href='#univariate'><p>Randomly generated univariate data</p></a></li>
<li><a href='#USMacro'><p>US Macroeconomic Data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Smooth Non-Parametric Frontier Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Fitting of non-parametric production frontiers for use in efficiency analysis.
    Methods are provided for both a smooth analogue of Data Envelopment Analysis (DEA) and a 
    non-parametric analogue of Stochastic Frontier Analysis (SFA). Frontiers are constructed for 
    multiple inputs and a single output using constrained kernel smoothing as in 
    Racine et al. (2009), which allow for the imposition of monotonicity and concavity constraints 
    on the estimated frontier.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Taylor McKenzie &lt;tkmckenzie@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>abind (&ge; 1.4.5), ggplot2 (&ge; 3.1.0), prodlim (&ge; 2018.4.18),
quadprog (&ge; 1.5.5), Rdpack (&ge; 0.10.1), rootSolve (&ge; 1.7)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-11-20 16:08:34 UTC; tmcken</td>
</tr>
<tr>
<td>Author:</td>
<td>Taylor McKenzie [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-12-01 00:00:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='allocative.efficiency'>Allocative efficiency estimation</h2><span id='topic+allocative.efficiency'></span>

<h3>Description</h3>

<p>Fits frontier to data and estimates technical and allocative efficiency
</p>


<h3>Usage</h3>

<pre><code class='language-R'>allocative.efficiency(X, y, X.price, y.price, X.constrained = NA,
  H.inv = NA, H.mult = 1, model = "br", method = "u",
  scale.constraints = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="allocative.efficiency_+3A_x">X</code></td>
<td>
<p>Matrix of inputs</p>
</td></tr>
<tr><td><code id="allocative.efficiency_+3A_y">y</code></td>
<td>
<p>Vector of outputs</p>
</td></tr>
<tr><td><code id="allocative.efficiency_+3A_x.price">X.price</code></td>
<td>
<p>Matrix of input prices</p>
</td></tr>
<tr><td><code id="allocative.efficiency_+3A_y.price">y.price</code></td>
<td>
<p>Vector of output prices</p>
</td></tr>
<tr><td><code id="allocative.efficiency_+3A_x.constrained">X.constrained</code></td>
<td>
<p>Matrix of inputs where constraints apply</p>
</td></tr>
<tr><td><code id="allocative.efficiency_+3A_h.inv">H.inv</code></td>
<td>
<p>Inverse of the smoothing matrix (must be positive definite); defaults to rule of thumb</p>
</td></tr>
<tr><td><code id="allocative.efficiency_+3A_h.mult">H.mult</code></td>
<td>
<p>Scaling factor for rule of thumb smoothing matrix</p>
</td></tr>
<tr><td><code id="allocative.efficiency_+3A_model">model</code></td>
<td>
<p>Type of frontier to use; &quot;br&quot; for boundary regression, &quot;sf&quot; for stochastic frontier</p>
</td></tr>
<tr><td><code id="allocative.efficiency_+3A_method">method</code></td>
<td>
<p>Constraints to apply; &quot;u&quot; for unconstrained, &quot;m&quot; for monotonically increasing, and &quot;mc&quot; for monotonically increasing and concave</p>
</td></tr>
<tr><td><code id="allocative.efficiency_+3A_scale.constraints">scale.constraints</code></td>
<td>
<p>Boolean, whether to scale constraints by their average value, can help with convergence</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function estimates allocative inefficiency using the methodology in McKenzie
(2018). The estimation process is a non-parametric analogue of Schmidt and Lovell
(1979). First, the frontier is fit using either a boundary regression or stochastic
frontier as in Racine et al. (2009), from which technical efficiency is estimated.
Then, gradients and price ratios are computed for each observation and compared to 
determine the extent of misallocation. Specifically, log-overallocation is computed as
</p>
<p style="text-align: center;"><code class="reqn">\log\left(\frac{w_i^j}{p_i}\right) - \log\left(\phi_i\frac{\partial f(x_i)}{\partial x^j}\right),</code>
</p>

<p>where <code class="reqn">\phi_i</code> is the efficiency of observation <code class="reqn">i</code>,
<code class="reqn">\partial f(x_i) / \partial x^j</code> is the marginal productivity of input <code class="reqn">j</code>
at observation <code class="reqn">i</code>, <code class="reqn">w_i^j</code> is the cost of input <code class="reqn">j</code> for observation
<code class="reqn">i</code>, and <code class="reqn">p_i</code> is the price of output for observation <code class="reqn">i</code>.
</p>


<h3>Value</h3>

<p>Returns a list with the following elements
</p>
<table role = "presentation">
<tr><td><code>y.fit</code></td>
<td>
<p>Estimated value of the frontier at X.fit</p>
</td></tr>
<tr><td><code>gradient.fit</code></td>
<td>
<p>Estimated gradient of the frontier at X.fit</p>
</td></tr>
<tr><td><code>technical.efficiency</code></td>
<td>
<p>Estimated technical efficiency</p>
</td></tr>
<tr><td><code>log.overallocation</code></td>
<td>
<p>Estimated log-overallocation of each input for each observation</p>
</td></tr>
<tr><td><code>X.eval</code></td>
<td>
<p>Matrix of inputs used for fitting</p>
</td></tr>
<tr><td><code>X.constrained</code></td>
<td>
<p>Matrix of inputs where constraints apply</p>
</td></tr>
<tr><td><code>H.inv</code></td>
<td>
<p>Inverse smoothing matrix used in fitting</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Method used to fit frontier</p>
</td></tr>
<tr><td><code>scaling.factor</code></td>
<td>
<p>Factor by which constraints are multiplied before quadratic programming</p>
</td></tr>
</table>


<h3>References</h3>

<p>Aigner D, Lovell CK, Schmidt P (1977).
&ldquo;Formulation and estimation of stochastic frontier production function models.&rdquo;
<em>Journal of econometrics</em>, <b>6</b>(1), 21&ndash;37.<br /><br />
McKenzie T (2018).
&ldquo;Semi-Parametric Estimation of Allocative Inefficiency Using Smooth Non-Parametric Frontier Analysis.&rdquo;
Working Paper.<br /><br />
Racine JS, Parmeter CF, Du P (2009).
&ldquo;Constrained nonparametric kernel regression: Estimation and inference.&rdquo;
Working paper.<br /><br />
Schmidt P, Lovell CK (1979).
&ldquo;Estimating technical and allocative inefficiency relative to stochastic production and cost frontiers.&rdquo;
<em>Journal of econometrics</em>, <b>9</b>(3), 343&ndash;366.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(USMacro)

USMacro &lt;- USMacro[complete.cases(USMacro),]

#Extract data
X &lt;- as.matrix(USMacro[,c("K", "L")])
y &lt;- USMacro$Y

X.price &lt;- as.matrix(USMacro[,c("K.price", "L.price")])
y.price &lt;- rep(1e9, nrow(USMacro)) #Price of $1 billion of output is $1 billion

#Run model
efficiency.model &lt;- allocative.efficiency(X, y,
                                         X.price, y.price,
                                         X.constrained = X,
                                         model = "br",
                                         method = "mc")

#Plot technical/allocative efficiency over time
library(ggplot2)

technical.df &lt;- data.frame(Year = USMacro$Year,
                          Efficiency = efficiency.model$technical.efficiency)

ggplot(technical.df, aes(Year, Efficiency)) +
  geom_line()

allocative.df &lt;- data.frame(Year = rep(USMacro$Year, times = 2),
                            log.overallocation = c(efficiency.model$log.overallocation[,1],
                                                   efficiency.model$log.overallocation[,2]),
                            Variable = rep(c("K", "L"), each = nrow(USMacro)))

ggplot(allocative.df, aes(Year, log.overallocation)) +
  geom_line(aes(color = Variable))

#Estimate average overallocation across sample period
lm.model &lt;- lm(log.overallocation ~ 0 + Variable, allocative.df)
summary(lm.model)
  
</code></pre>

<hr>
<h2 id='fit.boundary'>Multivariate smooth boundary fitting with additional constraints</h2><span id='topic+fit.boundary'></span>

<h3>Description</h3>

<p>Fits boundary of data with kernel smoothing, imposing monotonicity and/or concavity constraints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit.boundary(X.eval, y.eval, X.bounded, y.bounded, X.constrained = NA,
  X.fit = NA, y.fit.observed = NA, H.inv = NA, H.mult = 1,
  method = "u", scale.constraints = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.boundary_+3A_x.eval">X.eval</code></td>
<td>
<p>Matrix of inputs used for fitting</p>
</td></tr>
<tr><td><code id="fit.boundary_+3A_y.eval">y.eval</code></td>
<td>
<p>Vector of outputs used for fitting</p>
</td></tr>
<tr><td><code id="fit.boundary_+3A_x.bounded">X.bounded</code></td>
<td>
<p>Matrix of inputs where bounding constraints apply</p>
</td></tr>
<tr><td><code id="fit.boundary_+3A_y.bounded">y.bounded</code></td>
<td>
<p>Vector of outputs where bounding constraints apply</p>
</td></tr>
<tr><td><code id="fit.boundary_+3A_x.constrained">X.constrained</code></td>
<td>
<p>Matrix of inputs where monotonicity/concavity constraints apply</p>
</td></tr>
<tr><td><code id="fit.boundary_+3A_x.fit">X.fit</code></td>
<td>
<p>Matrix of inputs where curve is fit; defaults to X.constrained</p>
</td></tr>
<tr><td><code id="fit.boundary_+3A_y.fit.observed">y.fit.observed</code></td>
<td>
<p>Vector of outputs corresponding to observations in X.fit; used for efficiency calculation</p>
</td></tr>
<tr><td><code id="fit.boundary_+3A_h.inv">H.inv</code></td>
<td>
<p>Inverse of the smoothing matrix (must be positive definite); defaults to rule of thumb</p>
</td></tr>
<tr><td><code id="fit.boundary_+3A_h.mult">H.mult</code></td>
<td>
<p>Scaling factor for rule of thumb smoothing matrix</p>
</td></tr>
<tr><td><code id="fit.boundary_+3A_method">method</code></td>
<td>
<p>Constraints to apply; &quot;u&quot; for unconstrained, &quot;m&quot; for monotonically increasing, and &quot;mc&quot; for monotonically increasing and concave</p>
</td></tr>
<tr><td><code id="fit.boundary_+3A_scale.constraints">scale.constraints</code></td>
<td>
<p>Boolean, whether to scale constraints by their average value, can help with convergence</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method fits a smooth boundary of the data (with all data points below the boundary)
while imposing specified monotonicity and concavity constraints. The procedure is
derived from Racine et al. (2009), which develops kernel smoothing methods with
bounding, monotonicity and concavity constraints. Specifically, the smoothing procedure
involves finding optimal weights for a Nadaraya-Watson estimator of the form 
</p>
<p style="text-align: center;"><code class="reqn">\hat{y} = m(x) = \sum_{i=1}^N p_i A(x, x_i) y_i,</code>
</p>

<p>where <code class="reqn">x</code> are inputs, <code class="reqn">y</code> are outputs, <code class="reqn">p</code> are weights, subscripts
index observations, and 
</p>
<p style="text-align: center;"><code class="reqn">A(x, x_i) = \frac{K(x, x_i)}{\sum_{h=1}^N K(x, x_h)}</code>
</p>

<p>for a kernel <code class="reqn">K</code>. This method uses a multivariate normal kernel of the form
</p>
<p style="text-align: center;"><code class="reqn">K(x, x_h) = \exp\left(-\frac12 (x - x_h)'H^{-1}(x - x_h)\right),</code>
</p>

<p>where <code class="reqn">H</code> is a bandwidth matrix. Bandwidth selection is performed via Silverman's
(1986) rule-of-thumb, in the function <code>H.inv.select</code>.
</p>
<p>Optimal weights <code class="reqn">\hat{p}</code> are selected by solving the quadratic programming problem
</p>
<p style="text-align: center;"><code class="reqn">\min_p \mbox{\ \ }-\mathbf{1}'p + \frac12 p'p.</code>
</p>

<p>This method always imposes bounding constraints as specified points, given by
</p>
<p style="text-align: center;"><code class="reqn">m(x_i) - y_i = \sum_{h=1}^N p_h A(x_i, x_h) y_h - y_i \geq 0 \mbox{\ \ \ \ }\forall i.</code>
</p>

<p>Additionally, monotonicity constraints of the following form can be imposed at 
specified points:
</p>
<p style="text-align: center;"><code class="reqn">\frac{\partial m(x)}{\partial x^j} = \sum_{h=1}^N p_h \frac{\partial A(x, x_h)}{\partial x^j} y_h \geq 0 \mbox{\ \ \ \ }\forall x, j,</code>
</p>

<p>where superscripts index inputs. Finally concavity constraints of the following form can also be imposed using Afriat's
(1967) conditions:
</p>
<p style="text-align: center;"><code class="reqn">m(x) - m(z) \leq \nabla_x m(z) \cdot (x - z) \mbox{\ \ \ \ }\forall x, z.</code>
</p>

<p>The gradient of the frontier at a point <code class="reqn">x</code> is given by 
</p>
<p style="text-align: center;"><code class="reqn">\nabla_x m(x) = \sum_{i=1}^N \hat{p}_i \nabla_x A(x, x_i) y_i,</code>
</p>

<p>where <code class="reqn">\hat{p}_i</code> are estimated weights.
</p>


<h3>Value</h3>

<p>Returns a list with the following elements
</p>
<table role = "presentation">
<tr><td><code>y.fit</code></td>
<td>
<p>Estimated value of the frontier at X.fit</p>
</td></tr>
<tr><td><code>gradient.fit</code></td>
<td>
<p>Estimated gradient of the frontier at X.fit</p>
</td></tr>
<tr><td><code>efficiency</code></td>
<td>
<p>Estimated efficiencies of y.fit.observed</p>
</td></tr>
<tr><td><code>solution</code></td>
<td>
<p>Boolean; TRUE if frontier successfully estimated</p>
</td></tr>
<tr><td><code>X.eval</code></td>
<td>
<p>Matrix of inputs used for fitting</p>
</td></tr>
<tr><td><code>X.constrained</code></td>
<td>
<p>Matrix of inputs where monotonicity/concavity constraints apply</p>
</td></tr>
<tr><td><code>X.fit</code></td>
<td>
<p>Matrix of inputs where curve is fit</p>
</td></tr>
<tr><td><code>H.inv</code></td>
<td>
<p>Inverse smoothing matrix used in fitting</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Method used to fit frontier</p>
</td></tr>
<tr><td><code>scaling.factor</code></td>
<td>
<p>Factor by which constraints are multiplied before quadratic programming</p>
</td></tr>
</table>


<h3>References</h3>

<p>Racine JS, Parmeter CF, Du P (2009).
&ldquo;Constrained nonparametric kernel regression: Estimation and inference.&rdquo;
Working paper.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(univariate)

#Set up data for fitting

X &lt;- as.matrix(univariate$x)
y &lt;- univariate$y

N.fit &lt;- 100
X.fit &lt;- as.matrix(seq(min(X), max(X), length.out = N.fit))

#Reflect data for fitting
reflected.data &lt;- reflect.data(X, y)
X.eval &lt;- reflected.data$X
y.eval &lt;- reflected.data$y

#Fit frontiers
frontier.u &lt;- fit.boundary(X.eval, y.eval, 
                           X.bounded = X, y.bounded = y,
                           X.constrained = X.fit,
                           X.fit = X.fit,
                           method = "u")
                          
frontier.m &lt;- fit.boundary(X.eval, y.eval, 
                           X.bounded = X, y.bounded = y,
                           X.constrained = X.fit,
                           X.fit = X.fit,
                           method = "m")
                          
frontier.mc &lt;- fit.boundary(X.eval, y.eval, 
                            X.bounded = X, y.bounded = y,
                            X.constrained = X.fit,
                            X.fit = X.fit,
                            method = "mc")

#Plot frontier
library(ggplot2)

frontier.df &lt;- data.frame(X = rep(X.fit, times = 3),
                          y = c(frontier.u$y.fit, frontier.m$y.fit, frontier.mc$y.fit),
                          model = rep(c("u", "m", "mc"), each = N.fit))

ggplot(univariate, aes(X, y)) +
  geom_point() +
  geom_line(data = frontier.df, aes(color = model))

#Plot slopes
slope.df &lt;- data.frame(X = rep(X.fit, times = 3),
                       slope = c(frontier.u$gradient.fit,
                                 frontier.m$gradient.fit,
                                 frontier.mc$gradient.fit),
                       model = rep(c("u", "m", "mc"), each = N.fit))

ggplot(slope.df, aes(X, slope)) +
  geom_line(aes(color = model))
  
</code></pre>

<hr>
<h2 id='fit.mean'>Kernel smoothing with additional constraints</h2><span id='topic+fit.mean'></span>

<h3>Description</h3>

<p>Fits conditional mean of data with kernel smoothing, imposing monotonicity and/or concavity constraints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit.mean(X.eval, y.eval, X.constrained = NA, X.fit = NA, H.inv = NA,
  H.mult = 1, method = "u", scale.constraints = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.mean_+3A_x.eval">X.eval</code></td>
<td>
<p>Matrix of inputs used for fitting</p>
</td></tr>
<tr><td><code id="fit.mean_+3A_y.eval">y.eval</code></td>
<td>
<p>Vector of outputs used for fitting</p>
</td></tr>
<tr><td><code id="fit.mean_+3A_x.constrained">X.constrained</code></td>
<td>
<p>Matrix of inputs where constraints apply</p>
</td></tr>
<tr><td><code id="fit.mean_+3A_x.fit">X.fit</code></td>
<td>
<p>Matrix of inputs where curve is fit; defaults to X.constrained</p>
</td></tr>
<tr><td><code id="fit.mean_+3A_h.inv">H.inv</code></td>
<td>
<p>Inverse of the smoothing matrix (must be positive definite); defaults to rule of thumb</p>
</td></tr>
<tr><td><code id="fit.mean_+3A_h.mult">H.mult</code></td>
<td>
<p>Scaling factor for rule of thumb smoothing matrix</p>
</td></tr>
<tr><td><code id="fit.mean_+3A_method">method</code></td>
<td>
<p>Constraints to apply; &quot;u&quot; for unconstrained, &quot;m&quot; for monotonically increasing, and &quot;mc&quot; for monotonically increasing and concave</p>
</td></tr>
<tr><td><code id="fit.mean_+3A_scale.constraints">scale.constraints</code></td>
<td>
<p>Boolean, whether to scale constraints by their average value, can help with convergence</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method uses kernel smoothing to fit the mean of the data
while imposing specified monotonicity and concavity constraints. The procedure is
derived from Racine et al. (2009), which develops kernel smoothing methods with
bounding, monotonicity and concavity constraints. Specifically, the smoothing procedure
involves finding optimal weights for a Nadaraya-Watson estimator of the form 
</p>
<p style="text-align: center;"><code class="reqn">\hat{y} = m(x) = \sum_{i=1}^N p_i A(x, x_i) y_i,</code>
</p>

<p>where <code class="reqn">x</code> are inputs, <code class="reqn">y</code> are outputs, <code class="reqn">p</code> are weights, subscripts
index observations, and 
</p>
<p style="text-align: center;"><code class="reqn">A(x, x_i) = \frac{K(x, x_i)}{\sum_{h=1}^N K(x, x_h)}</code>
</p>

<p>for a kernel <code class="reqn">K</code>. This method uses a multivariate normal kernel of the form
</p>
<p style="text-align: center;"><code class="reqn">K(x, x_h) = \exp\left(-\frac12 (x - x_h)'H^{-1}(x - x_h)\right),</code>
</p>

<p>where <code class="reqn">H</code> is a bandwidth matrix. Bandwidth selection is performed via Silverman's
(1986) rule-of-thumb, in the function <code>H.inv.select</code>.
</p>
<p>Optimal weights <code class="reqn">\hat{p}</code> are selected by solving the quadratic programming problem
</p>
<p style="text-align: center;"><code class="reqn">\min_p \mbox{\ \ }-\mathbf{1}'p + \frac12 p'p.</code>
</p>

<p>Monotonicity constraints of the following form can be imposed at 
specified points:
</p>
<p style="text-align: center;"><code class="reqn">\frac{\partial m(x)}{\partial x^j} = \sum_{h=1}^N p_h \frac{\partial A(x, x_h)}{\partial x^j} y_h \geq 0 \mbox{\ \ \ \ }\forall x, j,</code>
</p>

<p>where superscripts index inputs. Finally concavity constraints of the following form can also be imposed using Afriat's
(1967) conditions:
</p>
<p style="text-align: center;"><code class="reqn">m(x) - m(z) \leq \nabla_x m(z) \cdot (x - z) \mbox{\ \ \ \ }\forall x, z.</code>
</p>

<p>The gradient of the estimated curve at a point <code class="reqn">x</code> is given by 
</p>
<p style="text-align: center;"><code class="reqn">\nabla_x m(x) = \sum_{i=1}^N \hat{p}_i \nabla_x A(x, x_i) y_i,</code>
</p>

<p>where <code class="reqn">\hat{p}_i</code> are estimated weights.
</p>


<h3>Value</h3>

<p>Returns a list with the following elements
</p>
<table role = "presentation">
<tr><td><code>y.fit</code></td>
<td>
<p>Estimated value of the frontier at X.fit</p>
</td></tr>
<tr><td><code>gradient.fit</code></td>
<td>
<p>Estimated gradient of the frontier at X.fit</p>
</td></tr>
<tr><td><code>solution</code></td>
<td>
<p>Boolean; TRUE if frontier successfully estimated</p>
</td></tr>
<tr><td><code>X.eval</code></td>
<td>
<p>Matrix of inputs used for fitting</p>
</td></tr>
<tr><td><code>X.constrained</code></td>
<td>
<p>Matrix of inputs where constraints apply</p>
</td></tr>
<tr><td><code>X.fit</code></td>
<td>
<p>Matrix of inputs where curve is fit</p>
</td></tr>
<tr><td><code>H.inv</code></td>
<td>
<p>Inverse smoothing matrix used in fitting</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Method used to fit frontier</p>
</td></tr>
<tr><td><code>scaling.factor</code></td>
<td>
<p>Factor by which constraints are multiplied before quadratic programming</p>
</td></tr>
</table>


<h3>References</h3>

<p>Racine JS, Parmeter CF, Du P (2009).
&ldquo;Constrained nonparametric kernel regression: Estimation and inference.&rdquo;
Working paper.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(USMacro)

USMacro &lt;- USMacro[complete.cases(USMacro),]

#Extract data
X &lt;- as.matrix(USMacro[,c("K", "L")])
y &lt;- USMacro$Y

#Reflect data for fitting
reflected.data &lt;- reflect.data(X, y)
X.eval &lt;- reflected.data$X
y.eval &lt;- reflected.data$y

#Fit frontier
fit.mc &lt;- fit.mean(X.eval, y.eval, 
                   X.constrained = X,
                   X.fit = X,
                   method = "mc")

#Plot input productivities over time
library(ggplot2)
plot.df &lt;- data.frame(Year = rep(USMacro$Year, times = 2),
                      Elasticity = c(fit.mc$gradient.fit[,1] * X[,1] / y,
                                     fit.mc$gradient.fit[,2] * X[,2] / y),
                      Variable = rep(c("Capital", "Labor"), each = nrow(USMacro)))

ggplot(plot.df, aes(Year, Elasticity)) +
  geom_line() +
  facet_grid(Variable ~ ., scales = "free_y")
  
</code></pre>

<hr>
<h2 id='fit.sf'>Non-parametric stochastic frontier</h2><span id='topic+fit.sf'></span>

<h3>Description</h3>

<p>Fits stochastic frontier of data with kernel smoothing, imposing monotonicity and/or concavity constraints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit.sf(X, y, X.constrained = NA, H.inv = NA, H.mult = 1,
  method = "u", scale.constraints = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.sf_+3A_x">X</code></td>
<td>
<p>Matrix of inputs</p>
</td></tr>
<tr><td><code id="fit.sf_+3A_y">y</code></td>
<td>
<p>Vector of outputs</p>
</td></tr>
<tr><td><code id="fit.sf_+3A_x.constrained">X.constrained</code></td>
<td>
<p>Matrix of inputs where constraints apply</p>
</td></tr>
<tr><td><code id="fit.sf_+3A_h.inv">H.inv</code></td>
<td>
<p>Inverse of the smoothing matrix (must be positive definite); defaults to rule of thumb</p>
</td></tr>
<tr><td><code id="fit.sf_+3A_h.mult">H.mult</code></td>
<td>
<p>Scaling factor for rule of thumb smoothing matrix</p>
</td></tr>
<tr><td><code id="fit.sf_+3A_method">method</code></td>
<td>
<p>Constraints to apply; &quot;u&quot; for unconstrained, &quot;m&quot; for monotonically increasing, and &quot;mc&quot; for monotonically increasing and concave</p>
</td></tr>
<tr><td><code id="fit.sf_+3A_scale.constraints">scale.constraints</code></td>
<td>
<p>Boolean, whether to scale constraints by their average value, can help with convergence</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method fits non-parametric stochastic frontier models. The data-generating process
is assumed to be of the form
</p>
<p style="text-align: center;"><code class="reqn">\ln y_i = \ln f(x_i) + v_i - u_i,</code>
</p>

<p>where <code class="reqn">y_i</code> is the <code class="reqn">i</code>th observation of output, <code class="reqn">f</code> is a continuous
function, <code class="reqn">x_i</code> is the <code class="reqn">i</code>th observation of input, <code class="reqn">v_i</code> is a
normally-distributed error term (<code class="reqn">v_i\sim N(0, \sigma_v^2)</code>), and <code class="reqn">u_i</code> is a
normally-distributed error term truncated below at zero (<code class="reqn">u_i\sim N^+(0, \sigma_u)</code>). 
Aigner et al. developed methods to decompose
<code class="reqn">\varepsilon_i = v_i - u_i</code> into its basic components.
</p>
<p>This procedure first fits the mean of the data using <code>fit.mean</code>,
producing estimates of output <code class="reqn">\hat{y}</code>. Log-proportional errors are calculated as
</p>
<p style="text-align: center;"><code class="reqn">\varepsilon_i = \ln(y_i / \hat{y}_i).</code>
</p>

<p>Following Aigner et al. (1977), parameters of one- and two-sided error distributions
are estimated via maximum likelihood. First,
</p>
<p style="text-align: center;"><code class="reqn">\hat{\sigma}^2 = \frac1N \sum_{i=1}^N \varepsilon_i^2.</code>
</p>

<p>Then, <code class="reqn">\hat{\lambda}</code> is estimated by solving
</p>
<p style="text-align: center;"><code class="reqn">\frac1{\hat{\sigma}^2} \sum_{i=1}^N \varepsilon_i\hat{y}_i + \frac{\hat{\lambda}}{\hat{\sigma}} \sum_{i=1}^N \frac{f_i^*}{1 - F_i^*}y_i = 0,</code>
</p>

<p>where <code class="reqn">f_i^*</code> and <code class="reqn">F_i^*</code> are standard normal density and distribution function,
respectively, evaluated at <code class="reqn">\varepsilon_i\hat{\lambda}\hat{\sigma}^{-1}</code>. Parameters of
the one- and two-sided distributions are found by solving the identities
</p>
<p style="text-align: center;"><code class="reqn">\sigma^2 = \sigma_u^2 + \sigma_v^2</code>
</p>

<p style="text-align: center;"><code class="reqn">\lambda = \frac{\sigma_u}{\sigma_v}.</code>
</p>

<p>Mean efficiency over the sample is given by 
</p>
<p style="text-align: center;"><code class="reqn">\exp\left(-\frac{\sqrt{2}}{\sqrt{\pi}}\right)\sigma_u,</code>
</p>

<p>and modal efficiency for each observation is given by
</p>
<p style="text-align: center;"><code class="reqn">-\varepsilon(\sigma_u^2/\sigma^2).</code>
</p>



<h3>Value</h3>

<p>Returns a list with the following elements
</p>
<table role = "presentation">
<tr><td><code>y.fit</code></td>
<td>
<p>Estimated value of the frontier at X.fit</p>
</td></tr>
<tr><td><code>gradient.fit</code></td>
<td>
<p>Estimated gradient of the frontier at X.fit</p>
</td></tr>
<tr><td><code>mean.efficiency</code></td>
<td>
<p>Average efficiency for X, y as a whole</p>
</td></tr>
<tr><td><code>mode.efficiency</code></td>
<td>
<p>Modal efficiencies for each observation in X, y</p>
</td></tr>
<tr><td><code>X.eval</code></td>
<td>
<p>Matrix of inputs used for fitting</p>
</td></tr>
<tr><td><code>X.constrained</code></td>
<td>
<p>Matrix of inputs where constraints apply</p>
</td></tr>
<tr><td><code>X.fit</code></td>
<td>
<p>Matrix of inputs where curve is fit</p>
</td></tr>
<tr><td><code>H.inv</code></td>
<td>
<p>Inverse smoothing matrix used in fitting</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Method used to fit frontier</p>
</td></tr>
<tr><td><code>scaling.factor</code></td>
<td>
<p>Factor by which constraints are multiplied before quadratic programming</p>
</td></tr>
</table>


<h3>References</h3>

<p>Aigner D, Lovell CK, Schmidt P (1977).
&ldquo;Formulation and estimation of stochastic frontier production function models.&rdquo;
<em>Journal of econometrics</em>, <b>6</b>(1), 21&ndash;37.<br /><br />
Racine JS, Parmeter CF, Du P (2009).
&ldquo;Constrained nonparametric kernel regression: Estimation and inference.&rdquo;
Working paper.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(USMacro)

USMacro &lt;- USMacro[complete.cases(USMacro),]

#Extract data
X &lt;- as.matrix(USMacro[,c("K", "L")])
y &lt;- USMacro$Y

#Fit frontier
fit.sf &lt;- fit.sf(X, y,
                 X.constrained = X,
                 method = "mc")

print(fit.sf$mean.efficiency)
# [1] 0.9772484

#Plot efficiency over time
library(ggplot2)

plot.df &lt;- data.frame(Year = USMacro$Year,
                      Efficiency = fit.sf$mode.efficiency)

ggplot(plot.df, aes(Year, Efficiency)) +
  geom_line()
  
</code></pre>

<hr>
<h2 id='H.inv.select'>Bandwidth matrix selection</h2><span id='topic+H.inv.select'></span>

<h3>Description</h3>

<p>Computes inverse of bandwidth matrix using rule-of-thumb from Silverman (1986).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>H.inv.select(X, H.mult = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="H.inv.select_+3A_x">X</code></td>
<td>
<p>Matrix of inputs</p>
</td></tr>
<tr><td><code id="H.inv.select_+3A_h.mult">H.mult</code></td>
<td>
<p>Scaling factor for rule-of-thumb smoothing matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method performs selection of (inverse) multivariate bandwidth matrices using
Silverman's (1986) rule-of-thumb. Specifically, Silverman recommends setting the bandwidth
matrix to
</p>
<p style="text-align: center;"><code class="reqn">H_{jj}^{1/2} = \left(\frac{4}{M + 2}\right)^{1 / (M + 4)}\times N^{-1 / (M + 4)}\times \mbox{sd}(x^j) \mbox{\ \ \ \ for }j=1,...,M</code>
</p>

<p style="text-align: center;"><code class="reqn">H_{ab} = 0\mbox{\ \ \ \ for }a\neq b</code>
</p>

<p>where <code class="reqn">M</code> is the number of inputs, <code class="reqn">N</code> is the number of observations, and
<code class="reqn">\mbox{sd}(x^j)</code> is the sample standard deviation of input <code class="reqn">j</code>.
</p>


<h3>Value</h3>

<p>Returns inverse bandwidth matrix
</p>


<h3>References</h3>

<p>Silverman BW (1986).
<em>Density estimation for statistics and data analysis</em>, volume 26.
CRC press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(USMacro)

USMacro &lt;- USMacro[complete.cases(USMacro),]

#Extract data
X &lt;- as.matrix(USMacro[,c("K", "L")])

#Generate bandwidth matrix
print(H.inv.select(X))
#              [,1]         [,2]
# [1,] 3.642704e-08 0.000000e+00
# [2,] 0.000000e+00 1.215789e-08

</code></pre>

<hr>
<h2 id='panel.production'>Randomly generated panel of production data</h2><span id='topic+panel.production'></span>

<h3>Description</h3>

<p>A dataset for illustrating technical and efficiency
changes using smooth non-parametric frontiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>panel.production
</code></pre>


<h3>Format</h3>

<p>A data frame with 200 observations of six variables.
</p>

<dl>
<dt>Firm</dt><dd><p>Firm identifier</p>
</dd>
<dt>Year</dt><dd><p>Year of observation</p>
</dd>
<dt>X.1</dt><dd><p>Input 1</p>
</dd>
<dt>X.2</dt><dd><p>Input 2</p>
</dd>
<dt>X.3</dt><dd><p>Input 3</p>
</dd>
<dt>y</dt><dd><p>Output</p>
</dd>
</dl>


<h3>Details</h3>

<p>Generated with the following code:
</p>
<pre>
set.seed(100)

num.firms &lt;- 20
num.inputs &lt;- 3
num.years &lt;- 10

beta &lt;- runif(num.inputs, 0, 1)
TFP.trend = 0.25
TFP &lt;- cumsum(rnorm(num.years)) + TFP.trend * (1:num.years)

sd.measurement &lt;- 0.05
sd.inefficiency &lt;- 0.01

f &lt;- function(X){
  return(TFP + X 
}
gen.firm.data &lt;- function(i){
  X = matrix(runif(num.years * num.inputs, 1, 10), ncol = num.inputs)
  y = f(X) +
    rnorm(num.years, sd = sd.measurement) -
    abs(rnorm(num.years, sd = sd.inefficiency))
  firm.df &lt;- data.frame(Firm = i,
                        Year = 1:num.years,
                        X = exp(X),
                        y = exp(y))
}

panel.production = Reduce(rbind, lapply(1:num.firms, gen.firm.data))
panel.production$Firm = as.factor(panel.production$Firm)
</pre>

<hr>
<h2 id='reflect.data'>Data reflection for kernel smoothing</h2><span id='topic+reflect.data'></span>

<h3>Description</h3>

<p>This function reflects data below minimum and above maximum for use in reducing endpoint bias in kernel smoothing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reflect.data(X, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reflect.data_+3A_x">X</code></td>
<td>
<p>Matrix of inputs</p>
</td></tr>
<tr><td><code id="reflect.data_+3A_y">y</code></td>
<td>
<p>Vector of outputs</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following elements
</p>
<table role = "presentation">
<tr><td><code>X.reflected</code></td>
<td>
<p>Reflected values of X</p>
</td></tr>
<tr><td><code>y.reflected</code></td>
<td>
<p>Reflected values of y</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(univariate)

#Extract data
X &lt;- as.matrix(univariate$x)
y &lt;- univariate$y

#Reflect data
reflected.data &lt;- reflect.data(X, y)

X.reflected &lt;- reflected.data$X
y.reflected &lt;- reflected.data$y

#Plot
library(ggplot2)

plot.df &lt;- data.frame(X = X.reflected,
                      y = y.reflected,
                      data = rep(c("reflected", "actual", "reflected"), each = nrow(X)))

ggplot(plot.df, aes(X, y)) +
  geom_point(aes(color = data))

</code></pre>

<hr>
<h2 id='technical.efficiency.change'>Technical and efficiency change estimation</h2><span id='topic+technical.efficiency.change'></span>

<h3>Description</h3>

<p>Estimates technical and efficiency change using SNFA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>technical.efficiency.change(df, input.var.names, output.var.name,
  firm.var.name, time.var.name, method = "u")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="technical.efficiency.change_+3A_df">df</code></td>
<td>
<p>Data frame with variables used in estimation</p>
</td></tr>
<tr><td><code id="technical.efficiency.change_+3A_input.var.names">input.var.names</code></td>
<td>
<p>Names of input variables; must appear in df</p>
</td></tr>
<tr><td><code id="technical.efficiency.change_+3A_output.var.name">output.var.name</code></td>
<td>
<p>Name of output variable; must appear in df</p>
</td></tr>
<tr><td><code id="technical.efficiency.change_+3A_firm.var.name">firm.var.name</code></td>
<td>
<p>Name of firm variable; must appear in df</p>
</td></tr>
<tr><td><code id="technical.efficiency.change_+3A_time.var.name">time.var.name</code></td>
<td>
<p>Name of time variable; must appear in df</p>
</td></tr>
<tr><td><code id="technical.efficiency.change_+3A_method">method</code></td>
<td>
<p>Constraints to apply; &quot;u&quot; for unconstrained, &quot;m&quot; for monotonically increasing, and &quot;mc&quot; for monotonically increasing and concave</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function decomposes change in productivity into efficiency and technical change, as in
Fare et al. (1994), using smooth non-parametric frontier analysis. Denoting <code class="reqn">D_s(x_t, y_t)</code>
as the efficiency of the production plan in year t relative to the production frontier in year s,
efficiency change for a given firm in year t is calculated as
</p>
<p style="text-align: center;"><code class="reqn">\frac{D_{t+1}(x_{t+1}, y_{t+1})}{D_t(x_t, y_t)},</code>
</p>

<p>and technical change is given by
</p>
<p style="text-align: center;"><code class="reqn">\left(
\frac{D_t(x_{t+1}, y_{t+1})}{D_{t+1}(x_{t+1}, y_{t+1})}\times
\frac{D_t(x_t, y_t)}{D_{t+1}(x_t, y_t)}
\right)^{1/2}.</code>
</p>



<h3>Value</h3>

<p>Returns a data.frame with the following columns
</p>
<table role = "presentation">
<tr><td><code>firm.var.name</code></td>
<td>
<p>Column of firm name data</p>
</td></tr>
<tr><td><code>time.var.name</code></td>
<td>
<p>Column of time period data</p>
</td></tr>
<tr><td><code>efficiency.change</code></td>
<td>
<p>Average annual efficiency change since the previous period in data</p>
</td></tr>
<tr><td><code>technical.change</code></td>
<td>
<p>Average annual technical change since the previous period in data</p>
</td></tr>
<tr><td><code>productivity.change</code></td>
<td>
<p>Average annual productivity change since the previous period in data</p>
</td></tr>
</table>


<h3>References</h3>

<p>Fare R, Grosskopf S, Norris M, Zhang Z (1994).
&ldquo;Productivity Growth, Technical Progress, and Efficiency Change in Industrialized Countries.&rdquo;
<em>The American Economic Review</em>, <b>84</b>(1), 66-83.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(panel.production)

results.df &lt;- technical.efficiency.change(df = panel.production,
                                          input.var.names = c("X.1", "X.2", "X.3"),
                                          output.var.name = "y",
                                          firm.var.name = "Firm",
                                          time.var.name = "Year")

#Plot changes over time by firm
library(ggplot2)

ggplot(results.df, aes(Year, technical.change)) +
  geom_line(aes(color = Firm))
ggplot(results.df, aes(Year, efficiency.change)) +
  geom_line(aes(color = Firm))
ggplot(results.df, aes(Year, productivity.change)) +
  geom_line(aes(color = Firm))
  
</code></pre>

<hr>
<h2 id='univariate'>Randomly generated univariate data</h2><span id='topic+univariate'></span>

<h3>Description</h3>

<p>A dataset for illustrating univariate non-parametric boundary
regressions and various constraints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>univariate
</code></pre>


<h3>Format</h3>

<p>A data frame with 50 observations of two variables.
</p>

<dl>
<dt>x</dt><dd><p>Input</p>
</dd>
<dt>y</dt><dd><p>Output</p>
</dd>
</dl>


<h3>Details</h3>

<p>Generated with the following code:
</p>
<pre>
set.seed(100)

N &lt;- 50
x &lt;- runif(N, 10, 100)
y &lt;- sapply(x, function(x) 500 * x^0.25 - dnorm(x, mean = 70, sd = 10) * 8000) - abs(rnorm(N, sd = 20))
y &lt;- y - min(y) + 10
df &lt;- data.frame(x, y)
</pre>

<hr>
<h2 id='USMacro'>US Macroeconomic Data</h2><span id='topic+USMacro'></span>

<h3>Description</h3>

<p>A dataset of real output, labor force, capital stock,
wages, and interest rates for the U.S. between 1929 and 2014, 
as available. All nominal values converted to 2010 U.S. dollars
using GDP price deflator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>USMacro
</code></pre>


<h3>Format</h3>

<p>A data frame with 89 observations of four variables.
</p>

<dl>
<dt>Year</dt><dd><p>Year</p>
</dd>
<dt>Y</dt><dd><p>Real GDP, in billions of dollars</p>
</dd>
<dt>K</dt><dd><p>Capital stock, in billions of dollars</p>
</dd>
<dt>K.price</dt><dd><p>Annual cost of $1 billion of capital, using 10-year treasury</p>
</dd>
<dt>L</dt><dd><p>Labor force, in thousands of people</p>
</dd>
<dt>L.price</dt><dd><p>Annual wage for one thousand people</p>
</dd>
</dl>


<h3>Source</h3>

<p><a href="https://fred.stlouisfed.org/">https://fred.stlouisfed.org/</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
