<!DOCTYPE html><html lang="en"><head><title>Help for package rapidsplithalf</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rapidsplithalf}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#rapidsplithalf'><p>rapidsplithalf package</p></a></li>
<li><a href='#bootstrapWeights'><p>Bootstrap Weights</p></a></li>
<li><a href='#colAggregators'><p>Fast matrix column aggregators</p></a></li>
<li><a href='#corByColumns'><p>Correlate two matrices by column</p></a></li>
<li><a href='#cormean'><p>Compute a minimally biased average of correlation values</p></a></li>
<li><a href='#correlation-tools'><p>Miscellaneous correlation tools</p></a></li>
<li><a href='#excludeOutliersByMask'><p>Exclude SD-based outliers</p></a></li>
<li><a href='#foodAAT'><p>Approach-Avoidance Task examining approach bias to different foods</p></a></li>
<li><a href='#generateSplits'><p>A balanced split-half generator</p></a></li>
<li><a href='#maskAggregators'><p>Multi-mask/weight based aggregators</p></a></li>
<li><a href='#OutlierMaskers'><p>Exclude SD-based outliers in each matrix column</p></a></li>
<li><a href='#raceIAT'><p>Implicit Association Task examining implicit bias towards White and Black people</p></a></li>
<li><a href='#rapidsplit'><p>rapidsplit</p></a></li>
<li><a href='#spearmanBrown'><p>Spearman-Brown correction</p>
Perform a Spearman-Brown correction on the provided correlation score.</a></li>
<li><a href='#stratifiedItersplits'><p>stratifiedItersplits</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Fast Permutation-Based Split-Half Reliability Algorithm</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4</td>
</tr>
<tr>
<td>Description:</td>
<td>Accurately estimates the reliability of cognitive tasks using a fast and flexible permutation-based split-half reliability algorithm that supports stratified splitting while maintaining equal split sizes. See Kahveci, Bathke, and Blechert (2022) &lt;<a href="https://doi.org/10.31234%2Fosf.io%2Fta59r">doi:10.31234/osf.io/ta59r</a>&gt; for details.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/Spiritspeak/rapidsplit/issues/">https://github.com/Spiritspeak/rapidsplit/issues/</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.5), doParallel, foreach</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-28 08:13:51 UTC; b1066151</td>
</tr>
<tr>
<td>Author:</td>
<td>Sercan Kahveci <a href="https://orcid.org/0000-0002-4139-5710"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Sercan Kahveci &lt;sercan.kahveci@plus.ac.at&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-28 08:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='rapidsplithalf'>rapidsplithalf package</h2><span id='topic+rapidsplithalf'></span><span id='topic+rapidsplithalf-package'></span>

<h3>Description</h3>

<p>To learn more about rapidsplithalf, view the introductory vignette:
<code>vignette("rapidsplithalf",package="rapidsplithalf")</code>
</p>


<h3>Author(s)</h3>

<p>Sercan Kahveci
</p>

<hr>
<h2 id='bootstrapWeights'>Bootstrap Weights</h2><span id='topic+bootstrapWeights'></span>

<h3>Description</h3>

<p>Create a matrix of bootstrap samples expressed as frequency weights
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrapWeights(size, times)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootstrapWeights_+3A_size">size</code></td>
<td>
<p>Number of values to bootstrap</p>
</td></tr>
<tr><td><code id="bootstrapWeights_+3A_times">times</code></td>
<td>
<p>Number of bootstraps</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with bootstrap samples expressed as frequency weights. 
Each column represents a single bootstrap iteration and each row represents a case.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Rapidly compute a bootstrapped median to obtain its standard error
myweights&lt;-bootstrapWeights(size=50, times=100)
meds&lt;-mediansByWeight(x=rnorm(50),weights=myweights)
# SE
sd(meds)

</code></pre>

<hr>
<h2 id='colAggregators'>Fast matrix column aggregators</h2><span id='topic+colAggregators'></span><span id='topic+colMedians'></span><span id='topic+colProds'></span><span id='topic+colSds'></span><span id='topic+colMediansMasked'></span><span id='topic+colMeansMasked'></span><span id='topic+colSdsMasked'></span>

<h3>Description</h3>

<p>Fast matrix column aggregators
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colMedians(x)

colProds(x)

colSds(x)

colMediansMasked(x, mask)

colMeansMasked(x, mask)

colSdsMasked(x, mask)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="colAggregators_+3A_x">x</code></td>
<td>
<p>A numeric matrix to compute column aggregates of.</p>
</td></tr>
<tr><td><code id="colAggregators_+3A_mask">mask</code></td>
<td>
<p>A logical matrix determining which data points to include in 
the column-wise aggregations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector representing values aggregated by column.
</p>


<h3>Author(s)</h3>

<p>Sercan Kahveci
</p>


<h3>See Also</h3>

<p><a href="base.html#topic+colMeans">colMeans</a>, <a href="#topic+mediansByMask">mediansByMask</a>, <a href="#topic+maskAggregators">maskAggregators</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- cbind(x1 = 3, x2 = c(4:1, 2:5))
colMedians(x)

colProds(x)

colSds(x)

mask&lt;-cbind(rep(c(TRUE,FALSE),4),
            rep(c(TRUE,FALSE),each=4))
colMediansMasked(x,mask)

colMeansMasked(x,mask)

colSdsMasked(x,mask)

</code></pre>

<hr>
<h2 id='corByColumns'>Correlate two matrices by column</h2><span id='topic+corByColumns'></span><span id='topic+corByColumns_mask'></span><span id='topic+corStatsByColumns'></span>

<h3>Description</h3>

<p>Correlate each column of 1 matrix with the same column in another matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corByColumns(x, y)

corByColumns_mask(x, y, mask)

corStatsByColumns(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="corByColumns_+3A_x">x</code>, <code id="corByColumns_+3A_y">y</code></td>
<td>
<p>Matrices whose values to correlate by column.</p>
</td></tr>
<tr><td><code id="corByColumns_+3A_mask">mask</code></td>
<td>
<p>Logical matrix marking which data points to include.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The primary use for these functions is to rapidly compute the correlations
between two sets of split-half scores stored in matrix columns.
</p>
<p><code>corStatsByColumns</code> produces the mean correlation of all column-pairs
using the formula <code>mean(covariances) / sqrt(mean(col1variance) * mean(col2variance))</code>
</p>
<p>This method is more accurate than <code><a href="#topic+cormean">cormean()</a></code> and was suggested by
prof. John Christie of Dalhousie University.
</p>


<h3>Value</h3>

<p><code>corByColumns()</code> and <code>corByColumns_mask()</code> return
a numeric vector of correlations of each pair of columns.
</p>
<p><code>corStatsByColumns()</code> returns a list with named items:
</p>

<ul>
<li><p> cormean: the aggregated correlation coefficient of all column pairs (see Details)
</p>
</li>
<li><p> allcors: the correlations of each column pair
</p>
</li>
<li><p> xvar: the column variances of matrix <code>x</code>
</p>
</li>
<li><p> yvar: the column variances of matrix <code>y</code>
</p>
</li>
<li><p> covar: the covariances of each column pair
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Sercan Kahveci
</p>


<h3>Examples</h3>

<pre><code class='language-R'>m1&lt;-matrix((1:9)+rnorm(9),ncol=3)
m2&lt;-matrix((9:1)+rnorm(9),ncol=3)
corByColumns(m1,m2)

mask&lt;-1-diag(3)
corByColumns_mask(m1,m2,mask)

corStatsByColumns(m1,m2)

</code></pre>

<hr>
<h2 id='cormean'>Compute a minimally biased average of correlation values</h2><span id='topic+cormean'></span>

<h3>Description</h3>

<p>This function computes a minimally biased average of correlation values.
This is needed because simple averaging of correlations is negatively biased,
and the often used z-transformation method of averaging correlations is positively biased.
The algorithm was developed by Olkin &amp; Pratt (1958).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cormean(
  r,
  n,
  weights = c("none", "n", "df"),
  type = c("OP5", "OP2", "OPK"),
  na.rm = FALSE,
  incl.trans = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cormean_+3A_r">r</code></td>
<td>
<p>A vector containing correlation values/</p>
</td></tr>
<tr><td><code id="cormean_+3A_n">n</code></td>
<td>
<p>A single value or vector containing sample sizes/</p>
</td></tr>
<tr><td><code id="cormean_+3A_weights">weights</code></td>
<td>
<p>Character. How should the correlations be weighted?
<code>none</code> leads to no weighting, <code>n</code> weights by sample size,
<code>df</code> weights by sample size minus one.</p>
</td></tr>
<tr><td><code id="cormean_+3A_type">type</code></td>
<td>
<p>Character. Determines which averaging algorithm to use, 
with &quot;OP5&quot; usually being the most accurate.</p>
</td></tr>
<tr><td><code id="cormean_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical. Should missing values be removed?</p>
</td></tr>
<tr><td><code id="cormean_+3A_incl.trans">incl.trans</code></td>
<td>
<p>Logical. Should the transformed correlations be included?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An average correlation.
</p>


<h3>References</h3>

<p>Olkin, I., &amp; Pratt, J. (1958). Unbiased estimation of certain correlation coefficients.
The Annals of Mathematical Statistics, 29. https://doi.org/10.1214/aoms/1177706717
</p>
<p>Shieh, G. (2010). Estimation of the simple correlation coefficient. Behavior Research Methods,
42(4), 906-917. https://doi.org/10.3758/BRM.42.4.906
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cormean(c(0,.3,.5),c(30,30,60))

</code></pre>

<hr>
<h2 id='correlation-tools'>Miscellaneous correlation tools</h2><span id='topic+correlation-tools'></span><span id='topic+r2z'></span><span id='topic+z2r'></span><span id='topic+r2t'></span><span id='topic+t2r'></span><span id='topic+r2p'></span><span id='topic+rconfint'></span><span id='topic+compcorr'></span><span id='topic+print.compcorr'></span>

<h3>Description</h3>

<p>Helper functions to compute important statistics from correlation coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2z(r)

z2r(z)

r2t(r, n)

t2r(t, n)

r2p(r, n)

rconfint(r, n, alpha = 0.05)

compcorr(r1, r2, n1, n2)

## S3 method for class 'compcorr'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="correlation-tools_+3A_r">r</code>, <code id="correlation-tools_+3A_r1">r1</code>, <code id="correlation-tools_+3A_r2">r2</code></td>
<td>
<p>Correlation values.</p>
</td></tr>
<tr><td><code id="correlation-tools_+3A_z">z</code></td>
<td>
<p>Z-scores.</p>
</td></tr>
<tr><td><code id="correlation-tools_+3A_n">n</code>, <code id="correlation-tools_+3A_n1">n1</code>, <code id="correlation-tools_+3A_n2">n2</code></td>
<td>
<p>Sample sizes.</p>
</td></tr>
<tr><td><code id="correlation-tools_+3A_t">t</code></td>
<td>
<p>t-scores.</p>
</td></tr>
<tr><td><code id="correlation-tools_+3A_alpha">alpha</code></td>
<td>
<p>The significance level to use.</p>
</td></tr>
<tr><td><code id="correlation-tools_+3A_x">x</code></td>
<td>
<p>A <code>compcorr</code> object to print.</p>
</td></tr>
<tr><td><code id="correlation-tools_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>r2z()</code>, <code>z2r</code>, <code>r2t</code>, <code>t2r</code>, and <code>r2p</code>, 
a numeric vector with the requested transformation applied. 
For <code>rconfint()</code>, a numeric vector with two values representing 
the lower and upper confidence intervals of the correlation coefficient.
For <code>compcorr()</code>, a <code>compcorr</code> object containing
a z and p value for the requested comparison, 
which can be printed with <code>print.compcorr()</code>.
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>r2z()</code>: Converts correlation coefficients to z-scores.
</p>
</li>
<li> <p><code>z2r()</code>: Converts z-scores to correlation coefficients.
</p>
</li>
<li> <p><code>r2t()</code>: Converts correlation coefficients to t-scores.
</p>
</li>
<li> <p><code>t2r()</code>: Converts t-scores to correlation coefficients.
</p>
</li>
<li> <p><code>r2p()</code>: Computes the two-sided p-value for a given correlation.
</p>
</li>
<li> <p><code>rconfint()</code>: Computes confidence intervals for one or multiple correlation coefficients.
</p>
</li>
<li> <p><code>compcorr()</code>: Computes the significance of the difference between two correlation coefficients.
</p>
</li>
<li> <p><code>print(compcorr)</code>: Computes the significance of the difference between two correlation coefficients.
</p>
</li></ul>


<h3>Author(s)</h3>

<p>Sercan Kahveci
</p>


<h3>See Also</h3>

<p><a href="#topic+cormean">cormean</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- r2z(.5)
r &lt;- z2r(z)
t&lt;-r2t(r,30)
r&lt;-t2r(t,30)
r2p(r,30)
print(rconfint(r,30))
print(compcorr(.5,.7,20,20))
</code></pre>

<hr>
<h2 id='excludeOutliersByMask'>Exclude SD-based outliers</h2><span id='topic+excludeOutliersByMask'></span>

<h3>Description</h3>

<p>Different masks (columns of a logical matrix) are applied to the same input vector, 
and outliers in each resulting subvector are marked with <code>FALSE</code> in the mask.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>excludeOutliersByMask(x, mask, sdlim = 3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="excludeOutliersByMask_+3A_x">x</code></td>
<td>
<p>Vector to exclude outliers from.</p>
</td></tr>
<tr><td><code id="excludeOutliersByMask_+3A_mask">mask</code></td>
<td>
<p>A logical matrix determining which data points to include and which not to.</p>
</td></tr>
<tr><td><code id="excludeOutliersByMask_+3A_sdlim">sdlim</code></td>
<td>
<p>Standard deviation limit to apply; 
values beyond are classified as outliers and masked.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated mask.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-rnorm(50)
x[1]&lt;-100
x[2]&lt;-50
mask&lt;-matrix(TRUE,ncol=3,nrow=50)
mask[1,2]&lt;-FALSE
mask[2,3]&lt;-FALSE
excludeOutliersByMask(x,mask)
</code></pre>

<hr>
<h2 id='foodAAT'>Approach-Avoidance Task examining approach bias to different foods</h2><span id='topic+foodAAT'></span>

<h3>Description</h3>

<p>This data originates from an approach-avoidance task examining approach bias towards food.
Participants responded to the stimulus category (food or object) by pulling or pushing a joystick.
Instructions were flipped from one block to the next.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(foodAAT)
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"data.frame"</code>.
</p>


<h3>Details</h3>


<ul>
<li><p> subjectid: Participant ID.
</p>
</li>
<li><p> stimid: Stimulus ID.
</p>
</li>
<li><p> is_pull: Whether the trial required an approach response (1) or an avoid response (0).
</p>
</li>
<li><p> is_target: Whether the trial featured a food stimulus (1) or an object stimulus (0).
</p>
</li>
<li><p> error: Whether the response was incorrect (1) or correct (0).
</p>
</li>
<li><p> RT: The response initiation time.
</p>
</li>
<li><p> FullRT: The time from stimulus onset to response completion.
</p>
</li>
<li><p> trialnum: The trial number.
</p>
</li>
<li><p> blocknum: The block number.
</p>
</li>
<li><p> palatability: The participant's palatability rating for the stimulus (foods only).
</p>
</li>
<li><p> valence: The participant's valence rating for the stimulus.
</p>
</li>
<li><p> FCQS_2_craving: The participant's FCQS state food craving score at time of testing.
</p>
</li>
<li><p> FCQS_2_hunger: The participant's FCQS state hunger score at time of testing.
</p>
</li></ul>



<h3>Source</h3>

<p><a href="https://doi.org/10.1016/j.appet.2018.01.032">doi:10.1016/j.appet.2018.01.032</a>
</p>


<h3>References</h3>

<p>Lender, A., Meule, A., Rinck, M., Brockmeyer, T., &amp; Blechert, J. (2018).
Measurement of food-related approach–avoidance biases:
Larger biases when food stimuli are task relevant. Appetite, 125, 42–47.
<a href="https://doi.org/10.1016/j.appet.2018.01.032">doi:10.1016/j.appet.2018.01.032</a>
</p>

<hr>
<h2 id='generateSplits'>A balanced split-half generator</h2><span id='topic+generateSplits'></span>

<h3>Description</h3>

<p>Generates split-half indices that can be stratified by multiple subgroup variables
while guaranteeing near-equal numbers of trials in both halves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateSplits(data, subsetvars, stratvars = NULL, splits, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generateSplits_+3A_data">data</code></td>
<td>
<p>A dataset to generate split-halves from.</p>
</td></tr>
<tr><td><code id="generateSplits_+3A_subsetvars">subsetvars</code></td>
<td>
<p>Variables identifying subgroups that must be individually split 
into equally sized halves, e.g. participant number and experimental condition.</p>
</td></tr>
<tr><td><code id="generateSplits_+3A_stratvars">stratvars</code></td>
<td>
<p>Variables identifying subgroups that are nested within the subsetvars, 
and must be split as fairly as possible, while preserving the equal size of 
the two halves of each subset identified by the subsetvars, e.g. stimulus ID.</p>
</td></tr>
<tr><td><code id="generateSplits_+3A_splits">splits</code></td>
<td>
<p>How many splits to generate.</p>
</td></tr>
<tr><td><code id="generateSplits_+3A_verbose">verbose</code></td>
<td>
<p>Display progress bar?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical <code>matrix</code> in which each row represents a row of the input dataset,
and each column represents a single split.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(foodAAT)
mysplits&lt;-generateSplits(data=foodAAT,
                         subsetvars=c("subjectid","is_pull","is_target"),
                         stratvars="stimid",
                         splits=1)
half1&lt;-foodAAT[ mysplits[,1],]
half2&lt;-foodAAT[!mysplits[,1],]

</code></pre>

<hr>
<h2 id='maskAggregators'>Multi-mask/weight based aggregators</h2><span id='topic+maskAggregators'></span><span id='topic+mediansByMask'></span><span id='topic+meansByMask'></span><span id='topic+sdsByMask'></span><span id='topic+mediansByWeight'></span><span id='topic+meansByWeight'></span><span id='topic+sdsByWeight'></span>

<h3>Description</h3>

<p>Methods to aggregate the same vector with different masks or frequency weights.
Useful for fast bootstrapping or split-half scoring.
A single aggregate value of <code>x</code> is computed for each column of the mask or weight matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mediansByMask(x, mask)

meansByMask(x, mask)

sdsByMask(x, mask)

mediansByWeight(x, weights)

meansByWeight(x, weights)

sdsByWeight(x, weights)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="maskAggregators_+3A_x">x</code></td>
<td>
<p>A vector to aggregate over with different masks or weights.</p>
</td></tr>
<tr><td><code id="maskAggregators_+3A_mask">mask</code></td>
<td>
<p>Logical matrix where each column represents a separate vector of masks 
to aggregate <code>x</code> with. Only values marked <code>TRUE</code> are included in the aggregation.</p>
</td></tr>
<tr><td><code id="maskAggregators_+3A_weights">weights</code></td>
<td>
<p>Integer matrix where each column represents frequency weights 
to weight the aggregation by.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with each value representing an aggregate of the same single input vector 
but with different masks or frequency weights applied.
</p>


<h3>Author(s)</h3>

<p>Sercan Kahveci
</p>


<h3>See Also</h3>

<p><a href="#topic+colMedians">colMedians</a>, <a href="#topic+colAggregators">colAggregators</a>, <a href="#topic+generateSplits">generateSplits</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Demonstration of mediansByMask()
x&lt;-1:6
mask&lt;-rbind(c(TRUE,FALSE,FALSE),
            c(TRUE,FALSE,FALSE),
            c(FALSE,TRUE,FALSE),
            c(FALSE,TRUE,FALSE),
            c(FALSE,FALSE,TRUE),
            c(FALSE,FALSE,TRUE))
mediansByMask(x,mask)

# Compute split-halves for a single 
# participant, stratified by stimulus
data(foodAAT)
currdata&lt;-foodAAT[foodAAT$subjectid==3,]
currdata$stratfactor&lt;-
  interaction(currdata$is_pull,
              currdata$is_target,
              currdata$stimid)
currdata&lt;-currdata[order(currdata$stratfactor),]
groupsizes&lt;-
  rle(as.character(currdata$stratfactor))$lengths
mysplits&lt;-
  stratifiedItersplits(splits=1000,
                       groupsizes=groupsizes)

# Median for half 1
mediansByMask(currdata$RT,mysplits==1)
 
#How to use meansByMask()
meansByMask(x,mask)
sd(meansByMask(currdata$RT,mysplits==1))

# How to use sdsByMask() to compute
# mask-based D-scores
meansByMask(currdata$RT,mysplits==1) / 
  sdsByMask(currdata$RT,mysplits==1)

# Compute the bootstrapped 
# standard error of a median
weights&lt;-
  bootstrapWeights(size=nrow(currdata),
                   times=1000)
bootmeds&lt;-mediansByWeight(currdata$RT,weights)
sd(bootmeds) # bootstrapped standard error

# Compute the bootstrapped 
# standard error of a mean
bootmeans&lt;-meansByWeight(currdata$RT,weights)
sd(bootmeans) # bootstrapped standard error
# exact standard error for comparison
sd(currdata$RT)/sqrt(length(currdata$RT)) 

# Use sdsByWeight to compute bootstrapped D-scores
bootsds&lt;-sdsByWeight(currdata$RT,weights)
# bootstrapped standard error of D-score
sd(bootmeans/bootsds)

</code></pre>

<hr>
<h2 id='OutlierMaskers'>Exclude SD-based outliers in each matrix column</h2><span id='topic+OutlierMaskers'></span><span id='topic+maskOutliers'></span><span id='topic+maskOutliersMasked'></span>

<h3>Description</h3>

<p>Generate or update a mask matrix based on outlyingness of values in each column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maskOutliers(x, sdlim = 3)

maskOutliersMasked(x, mask, sdlim = 3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="OutlierMaskers_+3A_x">x</code></td>
<td>
<p>Matrix in which to mark SD-based outliers by column.</p>
</td></tr>
<tr><td><code id="OutlierMaskers_+3A_sdlim">sdlim</code></td>
<td>
<p>Standard deviation limit to apply; 
values beyond are classified as outliers and masked.</p>
</td></tr>
<tr><td><code id="OutlierMaskers_+3A_mask">mask</code></td>
<td>
<p>A logical matrix determining which data points to include and which not to.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical matrix with outliers (and previously masked values) marked as <code>FALSE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate data with outliers
testmat&lt;-matrix(rnorm(100),ncol=2)
testmat[1,]&lt;-100
testmat[2,]&lt;-50

# Detect outliers
maskOutliers(testmat)

# Generate a mask
testmask&lt;-matrix(TRUE,ncol=2,nrow=50)
testmask[1,1]&lt;-FALSE

# Detect outliers with pre-existing mask
maskOutliersMasked(x=testmat, 
                   mask=testmask, sdlim = 3)

</code></pre>

<hr>
<h2 id='raceIAT'>Implicit Association Task examining implicit bias towards White and Black people</h2><span id='topic+raceIAT'></span>

<h3>Description</h3>

<p>This data originates from the publicly available
implicit association test (IAT) on racial prejudice hosted by Project Implicit.
200 participants were randomly sampled from the full trial-level data
available for participants from 2002 to 2022.
We included only those IAT blocks relevant to scoring (3,4,6,7) and
only individuals with full data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(raceIAT)
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"data.frame"</code>.
</p>


<h3>Details</h3>


<ul>
<li><p> session_id: The session id, proxy for participant number.
</p>
</li>
<li><p> task_name: Subtype of IAT used.
</p>
</li>
<li><p> block_number: IAT block number.
</p>
</li>
<li><p> block_pairing_definition: Stimulus pairing displayed in block.
</p>
</li>
<li><p> block_trial_number: Trial number within block.
</p>
</li>
<li><p> stimulus: Stimulus name.
</p>
</li>
<li><p> required_response: The response required from the participant.
</p>
</li>
<li><p> latency: Participant's response latency.
</p>
</li>
<li><p> error: Whether the response was wrong (<code>TRUE</code>).
</p>
</li>
<li><p> trial_number: Experimentwise trial number.
</p>
</li>
<li><p> stimcat: The stimulus category.
</p>
</li>
<li><p> respcat: Category of the required response.
</p>
</li>
<li><p> blocktype: Either practice block or full IAT block.
</p>
</li>
<li><p> congruent: Whether the block was congruent with anti-black bias (<code>TRUE</code>) or not.
</p>
</li>
<li><p> latency2: Response latencies with those for incorrect responses
replaced by the block mean plus a penalty.
</p>
</li></ul>



<h3>Source</h3>

<p><a href="https://osf.io/y9hiq/">OSF.io repository</a>
</p>


<h3>References</h3>

<p>Xu, K., Nosek, B., &amp; Greenwald, A. G. (2014).
Psychology data from the race implicit association test on the project implicit demo website.
Journal of open psychology data, 2(1), e3-e3.
<a href="https://doi.org/10.5334/jopd.ac">doi:10.5334/jopd.ac</a>
</p>

<hr>
<h2 id='rapidsplit'>rapidsplit</h2><span id='topic+rapidsplit'></span><span id='topic+print.rapidsplit'></span><span id='topic+plot.rapidsplit'></span><span id='topic+rapidsplit.chunks'></span>

<h3>Description</h3>

<p>A very fast algorithm for computing stratified permutation-based split-half reliability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rapidsplit(
  data,
  subjvar,
  diffvars = NULL,
  stratvars = NULL,
  subscorevar = NULL,
  aggvar,
  splits = 6000,
  aggfunc = c("means", "medians"),
  errorhandling = list(type = c("none", "fixedpenalty"), errorvar = NULL, fixedpenalty =
    600, blockvar = NULL),
  standardize = FALSE,
  include.scores = TRUE,
  verbose = TRUE,
  check = TRUE
)

## S3 method for class 'rapidsplit'
print(x, ...)

## S3 method for class 'rapidsplit'
plot(
  x,
  type = c("average", "minimum", "maximum", "random", "all"),
  show.labels = TRUE,
  ...
)

rapidsplit.chunks(
  data,
  subjvar,
  diffvars = NULL,
  stratvars = NULL,
  subscorevar = NULL,
  aggvar,
  splits = 6000,
  aggfunc = c("means", "medians"),
  errorhandling = list(type = c("none", "fixedpenalty"), errorvar = NULL, fixedpenalty =
    600, blockvar = NULL),
  standardize = FALSE,
  include.scores = TRUE,
  verbose = TRUE,
  check = TRUE,
  chunks = 4,
  cluster = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rapidsplit_+3A_data">data</code></td>
<td>
<p>Dataset, a <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_subjvar">subjvar</code></td>
<td>
<p>Subject ID variable name, a <code>character</code>.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_diffvars">diffvars</code></td>
<td>
<p>Names of variables that determine which conditions
need to be subtracted from each other, <code>character</code>.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_stratvars">stratvars</code></td>
<td>
<p>Additional variables that the splits should
be stratified by; a <code>character</code>.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_subscorevar">subscorevar</code></td>
<td>
<p>A <code>character</code> variable identifying subgroups within
a participant's data from which separate scores should be computed.
To compute a participant's final score, these subscores will be averaged together.
A typical use case is the D-score of the implicit association task.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_aggvar">aggvar</code></td>
<td>
<p>Name of variable whose values to aggregate, a <code>character</code>.
Examples include reaction times and error rates.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_splits">splits</code></td>
<td>
<p>Number of split-halves to average, an <code>integer</code>.
It is recommended to use around 5000.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_aggfunc">aggfunc</code></td>
<td>
<p>The function by which to aggregate the variable
defined in <code>aggvar</code>; can be <code>"means"</code>, <code>"medians"</code>,
or a custom function (not a function name).
This custom function must take a numeric vector and output a single value.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_errorhandling">errorhandling</code></td>
<td>
<p>A list with 4 named items, to be used to replace error trials
with the block mean of correct responses plus a fixed penalty, as in the IAT D-score.
The 4 items are <code>type</code> which can be set to <code>"none"</code> for no error replacement,
or <code>"fixedpenalty"</code> to replace error trials as described;
<code>errorvar</code> requires name of the <code>logical</code> variable indicating an incorrect response
(as <code>TRUE</code>); <code>fixedpenalty</code> indicates how much of a penalty should be added
to said block mean; and <code>blockvar</code> indicates the name of the block variable.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_standardize">standardize</code></td>
<td>
<p>Whether to divide by scores by the subject's SD; a <code>logical</code>.
Regardless of whether error penalization is utilized, this standardization
will be based on the unpenalized SD of correct and incorrect trials, as in the IAT D-score.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_include.scores">include.scores</code></td>
<td>
<p>Include all individual split-half scores?</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_verbose">verbose</code></td>
<td>
<p>Display progress bars? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_check">check</code></td>
<td>
<p>Check input for possible problems?</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_x">x</code></td>
<td>
<p><code>rapidsplit</code> object to print or plot.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_type">type</code></td>
<td>
<p>Character argument indicating what should be plotted. 
By default, this plots the random split whose correlation is closest to the average.
However, this can also plot the random split with 
the <code>"minimum"</code> or <code>"maximum"</code> split-half correlation, or any <code>"random"</code> split. 
<code>"all"</code> splits can also be plotted together in one figure.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_show.labels">show.labels</code></td>
<td>
<p>Should participant IDs be shown above their points in the scatterplot?
Defaults to <code>TRUE</code> and is ignored when <code>type</code> is <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_chunks">chunks</code></td>
<td>
<p>Number of chunks to divide the splits in, for more memory-efficient computation,
and to divide over multiple cores if requested.</p>
</td></tr>
<tr><td><code id="rapidsplit_+3A_cluster">cluster</code></td>
<td>
<p>Chunks will be run on separate cores if a cluster is provided, 
or an <code>integer</code> specifying the number of cores. Otherwise, if the value is <code>NULL</code>,
the chunks are run sequentially.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The order of operations (with optional steps between brackets) is:
</p>

<ul>
<li><p> Splitting
</p>
</li>
<li><p> (Replacing error trials within block within split)
</p>
</li>
<li><p> Computing aggregates per condition (per subscore) per person
</p>
</li>
<li><p> Subtracting conditions from each other
</p>
</li>
<li><p> (Dividing the resulting (sub)score by the SD of the data used to compute that (sub)score)
</p>
</li>
<li><p> (Averaging subscores together into a single score per person)
</p>
</li>
<li><p> Computing the covariances of scores from one half with scores from the other half
for every split
</p>
</li>
<li><p> Computing the variances of scores within each half for every split
</p>
</li>
<li><p> Computing the average split-half correlation with the average variances and covariance
across all splits, using <code><a href="#topic+corStatsByColumns">corStatsByColumns()</a></code>
</p>
</li>
<li><p> Applying the Spearman-Brown formula to the absolute correlation
using <code><a href="#topic+spearmanBrown">spearmanBrown()</a></code>, and restoring the original sign after
</p>
</li></ul>

<p><code><a href="#topic+cormean">cormean()</a></code> was used to aggregate correlations in previous versions
of this package &amp; in the associated manuscript, but the method based on
(co)variance averaging was found to be more accurate. This was suggested by
prof. John Christie of Dalhousie University.
</p>


<h3>Value</h3>

<p>A <code>list</code> containing
</p>

<ul>
<li> <p><code>r</code>: the averaged reliability.
</p>
</li>
<li> <p><code>ci</code>: the 95% confidence intervals.
</p>
</li>
<li> <p><code>allcors</code>: a vector with the reliability of each iteration.
</p>
</li>
<li> <p><code>nobs</code>: the number of participants.
</p>
</li>
<li> <p><code>scores</code>: the individual participants scores in each split-half,
contained in a list with two matrices (Only included if requested with <code>include.scores</code>).
</p>
</li></ul>



<h3>Note</h3>


<ul>
<li><p> This function can use a lot of memory in one go.
If you are computing the reliability of a large dataset or you have little RAM,
it may pay off to use the sequential version of this function instead:
<code><a href="#topic+rapidsplit.chunks">rapidsplit.chunks()</a></code>
</p>
</li>
<li><p> It is currently unclear it is better to pre-process your data before or after splitting it.
If you are computing the IAT D-score,
you can therefore use <code>errorhandling</code> and <code>standardize</code> to perform these two actions
after splitting, or you can process your data before splitting and forgo these two options.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Sercan Kahveci
</p>


<h3>References</h3>

<p>Kahveci, S., Bathke, A.C. &amp; Blechert, J. (2024)
Reaction-time task reliability is more accurately computed with
permutation-based split-half correlations than with Cronbach’s alpha.
Psychonomic Bulletin and Review. <a href="https://doi.org/10.3758/s13423-024-02597-y">doi:10.3758/s13423-024-02597-y</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(foodAAT)
# Reliability of the double difference score:
# [RT(push food)-RT(pull food)] - [RT(push object)-RT(pull object)]

frel&lt;-rapidsplit(data=foodAAT,
                 subjvar="subjectid",
                 diffvars=c("is_pull","is_target"),
                 stratvars="stimid",
                 aggvar="RT",
                 splits=100)
                 
print(frel)

plot(frel,type="all")

           
# Compute a single random split-half reliability of the error rate
rapidsplit(data=foodAAT,
           subjvar="subjectid",
           aggvar="error",
           splits=1,
           aggfunc="means")

# Compute the reliability of an IAT D-score
data(raceIAT)
rapidsplit(data=raceIAT,
           subjvar="session_id",
           diffvars="congruent",
           subscorevar="blocktype",
           aggvar="latency",
           errorhandling=list(type="fixedpenalty",errorvar="error",
                              fixedpenalty=600,blockvar="block_number"),
           splits=100,
           standardize=TRUE)


# Unstratified reliability of the median RT
rapidsplit.chunks(data=foodAAT,
                  subjvar="subjectid",
                  aggvar="RT",
                  splits=100,
                  aggfunc="medians",
                  chunks=8)

# Compute the reliability of Tukey's trimean of the RT
# on 2 CPU cores
trimean&lt;-function(x){ 
  sum(quantile(x,c(.25,.5,.75))*c(1,2,1))/4
}
rapidsplit.chunks(data=foodAAT,
                  subjvar="subjectid",
                  aggvar="RT",
                  splits=200,
                  aggfunc=trimean,
                  cluster=2)

</code></pre>

<hr>
<h2 id='spearmanBrown'>Spearman-Brown correction
Perform a Spearman-Brown correction on the provided correlation score.</h2><span id='topic+spearmanBrown'></span>

<h3>Description</h3>

<p>Spearman-Brown correction
Perform a Spearman-Brown correction on the provided correlation score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spearmanBrown(r, ntests = 2, fix.negative = c("mirror", "nullify", "none"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spearmanBrown_+3A_r">r</code></td>
<td>
<p>To-be-corrected correlation coefficient.</p>
</td></tr>
<tr><td><code id="spearmanBrown_+3A_ntests">ntests</code></td>
<td>
<p>An integer indicating how many times larger the full test is,
for which the corrected correlation coefficient is being computed.</p>
</td></tr>
<tr><td><code id="spearmanBrown_+3A_fix.negative">fix.negative</code></td>
<td>
<p>How will negative input values be dealt with?
</p>

<ul>
<li> <p><code>"mirror"</code> submits the absolute correlations to the formula
and restores the original sign afterwards.
</p>
</li>
<li> <p><code>"nullify"</code> sets negative correlations to zero.
</p>
</li>
<li> <p><code>"none"</code> leaves them as-is (not recommended).
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>ntests=2</code>, the formula will compute what the correlation coefficient would be
if the test were twice as long.
</p>


<h3>Value</h3>

<p>Spearman-Brown corrected correlation coefficients.
</p>


<h3>Author(s)</h3>

<p>Sercan Kahveci
</p>


<h3>Examples</h3>

<pre><code class='language-R'>spearmanBrown(.5)

</code></pre>

<hr>
<h2 id='stratifiedItersplits'>stratifiedItersplits</h2><span id='topic+stratifiedItersplits'></span>

<h3>Description</h3>

<p>Generate stratified splits for a single participant
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stratifiedItersplits(splits, groupsizes)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stratifiedItersplits_+3A_splits">splits</code></td>
<td>
<p>Number of iterations.</p>
</td></tr>
<tr><td><code id="stratifiedItersplits_+3A_groupsizes">groupsizes</code></td>
<td>
<p>An integer vector of how many RTs per group need to be stratified.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This equally splits what can be equally split within groups.
Then it randomly splits all the leftovers to ensure near-equal split sizes.
This function is moreso used internally, 
but you can use it if you know what you are doing.
</p>


<h3>Value</h3>

<p>A matrix with zeroes and ones. Each column is a random split.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# We will create splits stratified by stimulus for a single participant
data(foodAAT)
currdata&lt;-foodAAT[foodAAT$subjectid==3,]
currdata$stratfactor&lt;-interaction(currdata$is_pull,currdata$is_target,currdata$stimid)
currdata&lt;-currdata[order(currdata$stratfactor),]
groupsizes&lt;-rle(as.character(currdata$stratfactor))$lengths

mysplits&lt;-stratifiedItersplits(splits=1000,groupsizes=groupsizes)

# Now the data can be split with the values from any column.
half1&lt;-currdata[mysplits[,1]==1,]
half2&lt;-currdata[mysplits[,1]==0,]

# Or the split objects can be used as masks for the aggregation functions in this package
meansByMask(x=currdata$RT,mask=mysplits==1)
 
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
