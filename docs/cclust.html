<!DOCTYPE html><html><head><title>Help for package cclust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cclust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cclust'><p>Convex Clustering</p></a></li>
<li><a href='#clustIndex'><p>Cluster Indexes</p></a></li>
<li><a href='#predict.cclust'><p>Assign clusters to new data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Convex Clustering Methods and Clustering Indexes</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6-26</td>
</tr>
<tr>
<td>Description:</td>
<td>Convex Clustering methods, including K-means algorithm,
  On-line Update algorithm (Hard Competitive Learning) and Neural Gas
  algorithm (Soft Competitive Learning), and calculation of several
  indexes for finding the number of clusters in a data set.</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-02 10:08:25 UTC; hornik</td>
</tr>
<tr>
<td>Author:</td>
<td>Evgenia Dimitriadou [aut],
  Kurt Hornik [ctb, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kurt Hornik &lt;Kurt.Hornik@R-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-02 11:43:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='cclust'>Convex Clustering </h2><span id='topic+cclust'></span><span id='topic+kmeans'></span><span id='topic+hardcl'></span><span id='topic+neuralgas'></span><span id='topic+print.cclust'></span>

<h3>Description</h3>

<p>The data given by <code>x</code> is clustered by an algorithm.
</p>
<p>If <code>centers</code> is a matrix, its rows are taken as the initial
cluster centers. If <code>centers</code> is an integer, <code>centers</code> rows
of <code>x</code> are randomly chosen as initial values.
</p>
<p>The algorithm stops, if no cluster center has changed during the last
iteration or the maximum number of iterations (given by
<code>iter.max</code>) is reached.
</p>
<p>If <code>verbose</code> is <code>TRUE</code>, only for <code>"kmeans"</code> method,
displays for each iteration the number of the iteration and the
numbers of cluster indices which have changed since the last iteration
is given.
</p>
<p>If <code>dist</code> is <code>"euclidean"</code>, the distance between the cluster
center and the data points is the Euclidian distance (ordinary kmeans
algorithm). If <code>"manhattan"</code>, the distance between the cluster
center and the data points is the sum of the absolute values of the
distances of the coordinates.
</p>
<p>If <code>method</code> is <code>"kmeans"</code>, then we have the kmeans
clustering method, which works by repeatedly moving all cluster
centers to the mean of their Voronoi sets. If <code>"hardcl"</code> we have
the On-line Update (Hard Competitive learning) method, which works by
performing an update directly after each input signal, and if
<code>"neuralgas"</code> we have the Neural Gas (Soft Competitive learning)
method, that sorts for each input signal the units of the network
according to the distance of their reference vectors to input signal.
</p>
<p>If <code>rate.method</code> is <code>"polynomial"</code>, the polynomial learning
rate is used, that means <code class="reqn">1/t</code>, where <code class="reqn">t</code> stands for the
number of input data for which a particular cluster has been the
winner so far.  If <code>"exponentially decaying"</code>, the exponential
decaying learning rate is used according to
<code class="reqn">par1*{(par2/par1)}^{(iter/itermax)}</code> 
where <code class="reqn">par1</code> and <code class="reqn">par2</code> are the initial and final values of
the learning rate.
</p>
<p>The parameters <code>rate.par</code> of the learning rate, where
if <code>rate.method</code> is <code>"polynomial"</code> then by default
<code>rate.par=1.0</code>, otherwise <code>rate.par=(0.5,1e-5)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cclust (x, centers, iter.max=100, verbose=FALSE, dist="euclidean",
        method= "kmeans", rate.method="polynomial", rate.par=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cclust_+3A_x">x</code></td>
<td>
<p>Data matrix where columns correspond to variables and rows to
observations</p>
</td></tr>
<tr><td><code id="cclust_+3A_centers">centers</code></td>
<td>
<p>Number of clusters or initial values for cluster
centers</p>
</td></tr>
<tr><td><code id="cclust_+3A_iter.max">iter.max</code></td>
<td>
<p>Maximum number of iterations</p>
</td></tr>
<tr><td><code id="cclust_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, make some output during learning</p>
</td></tr>
<tr><td><code id="cclust_+3A_dist">dist</code></td>
<td>
<p>If <code>"euclidean"</code>, then mean square error, if
<code>"manhattan "</code>, the mean absolute error is used.</p>
</td></tr>
<tr><td><code id="cclust_+3A_method">method</code></td>
<td>
<p>If <code>"kmeans"</code>, then we have the kmeans clustering
method, if <code>"hardcl"</code> we have the On-line Update (Hard
Competitive learning) method, and if <code>"neuralgas"</code>, we have the
Neural Gas (Soft Competitive learning) method. Abbreviations of
the method names are accepted.</p>
</td></tr>
<tr><td><code id="cclust_+3A_rate.method">rate.method</code></td>
<td>
<p>If <code>"kmeans"</code>, then k-means learning rate,
otherwise exponential decaying learning rate.
It is used only for the Hardcl method.</p>
</td></tr>
<tr><td><code id="cclust_+3A_rate.par">rate.par</code></td>
<td>
<p>The parameters of the learning rate.</p>
</td></tr> 
</table>


<h3>Value</h3>

<p><code>cclust</code> returns an object of class <code>"cclust"</code>.
</p>
<table>
<tr><td><code>centers</code></td>
<td>
<p>The final cluster centers.</p>
</td></tr>
<tr><td><code>initcenters</code></td>
<td>
<p>The initial cluster centers.</p>
</td></tr>
<tr><td><code>ncenters</code></td>
<td>
<p>The number of the centers.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>Vector containing the indices of the clusters where
the data points are assigned to.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of data points in each cluster.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of iterations performed.</p>
</td></tr>
<tr><td><code>changes</code></td>
<td>
<p>The number of changes performed in each iteration
step with the Kmeans algorithm.</p>
</td></tr>
<tr><td><code>dist</code></td>
<td>
<p>The distance measure used.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The algorithm method being used.</p>
</td></tr>
<tr><td><code>rate.method</code></td>
<td>
<p>The learning rate being used by the Hardcl clustering
method.</p>
</td></tr>
<tr><td><code>rate.par</code></td>
<td>
<p>The parameters of the learning rate.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>Returns a call in which all of the arguments are
specified by their names.</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>Returns the sum of square distances within the clusters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Evgenia Dimitriadou</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.cclust">predict.cclust</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## a 2-dimensional example
x&lt;-rbind(matrix(rnorm(100,sd=0.3),ncol=2),
         matrix(rnorm(100,mean=1,sd=0.3),ncol=2))
cl&lt;-cclust(x,2,20,verbose=TRUE,method="kmeans")
plot(x, col=cl$cluster)   

## a 3-dimensional example 
x&lt;-rbind(matrix(rnorm(150,sd=0.3),ncol=3),
         matrix(rnorm(150,mean=1,sd=0.3),ncol=3),
         matrix(rnorm(150,mean=2,sd=0.3),ncol=3))
cl&lt;-cclust(x,6,20,verbose=TRUE,method="kmeans")
plot(x, col=cl$cluster)

## assign classes to some new data
y&lt;-rbind(matrix(rnorm(33,sd=0.3),ncol=3),
         matrix(rnorm(33,mean=1,sd=0.3),ncol=3),
         matrix(rnorm(3,mean=2,sd=0.3),ncol=3))
         ycl&lt;-predict(cl, y)
         plot(y, col=ycl$cluster)
</code></pre>

<hr>
<h2 id='clustIndex'>Cluster Indexes</h2><span id='topic+clustIndex'></span>

<h3>Description</h3>

<p><code>y</code> is the result of a clustering algorithm of class such
as <code>"cclust"</code>.
This function is calculating the values of several clustering
indexes. The values of the indexes can be independently used in order
to determine the number of clusters existing in a data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> clustIndex ( y, x, index = "all" ) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clustIndex_+3A_y">y</code></td>
<td>
<p>Object of class <code>"cclust"</code> returned by a clustering algorithm such as <code><a href="#topic+kmeans">kmeans</a></code></p>
</td></tr>
<tr><td><code id="clustIndex_+3A_x">x</code></td>
<td>
<p>Data matrix where columns correspond to variables and rows to observations</p>
</td></tr>
<tr><td><code id="clustIndex_+3A_index">index</code></td>
<td>
<p>The indexes that are calculated <code>"calinski"</code>,
<code>"cindex"</code>, <code>"db"</code>, <code>"hartigan"</code>, <code>"ratkowsky"</code>,
<code>"scott"</code>, <code>"marriot"</code>, <code>"ball"</code>, <code>"trcovw"</code>,
<code>"tracew"</code>, <code>"friedman"</code>, <code>"rubin"</code>, <code>"ssi"</code>,
<code>"likelihood"</code>, and <code>"all"</code> for all the
indexes. Abbreviations of these names are also accepted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The description of the indexes is categorized into 3 groups, based
on the statistics mainly used to compute them.
</p>
<p>The first group is based on the sum of squares within (<code class="reqn">SSW</code>)
and between (<code class="reqn">SSB</code>) the clusters. These statistics measure the
dispersion of the data points in a cluster and between the clusters
respectively. These indexes are:
</p>

<dl>
<dt><b>calinski</b>:</dt><dd>
<p><code class="reqn">(SSB/(k-1))/(SSW/(n-k))</code>, where <code class="reqn">n</code> is the number of
data points and <code class="reqn">k</code> is the number of clusters.
</p>
</dd>
<dt><b>hartigan</b>:</dt><dd><p>then <code class="reqn">\log(SSB/SSW)</code>.</p>
</dd>
<dt><b>ratkowsky</b>:</dt><dd>
<p><code class="reqn">mean(\sqrt{(varSSB/varSST)})</code>, where <code class="reqn">varSSB</code> stands
for the <code class="reqn">SSB</code> for every variable and <code class="reqn">varSST</code> for the
total sum of squares for every variable.
</p>
</dd>
<dt><b>ball</b>:</dt><dd>
<p><code class="reqn">SSW/k</code>, where <code class="reqn">k</code> is the number of clusters.
</p>
</dd>
</dl>

<p>The second group is based on the statistics of <code class="reqn">T</code>, i.e., the
scatter matrix of the data points, and <code class="reqn">W</code>, which is the sum of the
scatter matrices in every group. These indexes are:
</p>

<dl>
<dt><b>scott</b>:</dt><dd>
<p><code class="reqn">n\log(|T|/|W|)</code>, where <code class="reqn">n</code> is the number of data points
and <code class="reqn">|\cdot|</code> stands for the determinant of a matrix.</p>
</dd>
<dt><b>marriot</b>:</dt><dd>
<p><code class="reqn">k^2 |W|</code>, where <code class="reqn">k</code> is the number of clusters.</p>
</dd>
<dt><b>trcovw</b>:</dt><dd><p><code class="reqn">Trace Cov W</code>.</p>
</dd>
<dt><b>tracew</b>:</dt><dd><p><code class="reqn">Trace W</code>.</p>
</dd>
<dt><b>friedman</b>:</dt><dd>
<p><code class="reqn">Trace W^{(-1)} B</code>, where <code class="reqn">B</code> is the scatter matrix of
the cluster centers.</p>
</dd>
<dt><b>rubin</b>:</dt><dd><p><code class="reqn">|T|/|W|</code>.</p>
</dd>
</dl>

<p>The third group consists of four algorithms not belonging to the
previous ones and not having anything in common.
</p>

<dl>
<dt><b>cindex</b>:</dt><dd>
<p>if the data set is binary, then while the C-Index is a cluster
similarity measure, is expressed as:<br />
<code class="reqn">[d_{(w)}-\min(d_{(w)})]/[\max(d_{(w)})-\min(d_{(w)})]</code>,
where <code class="reqn">d_{(w)}</code> is the sum of all <code class="reqn">n_{(d)}</code> within
cluster distances, <code class="reqn">\min(d_{(w)})</code> is the sum of the
<code class="reqn">n_{(d)}</code> smallest pairwise distances in the data set, and
<code class="reqn">\max (d_{(w)})</code> is the sum of the <code class="reqn">n_{(d)}</code> biggest
pairwise distances.  In order to compute the C-Index all
pairwise distances in the data set have to be computed and
stored.  In the case of binary data, the storage of the
distances is creating no problems since there are only a few
possible distances.  However, the computation of all distances
can make this index prohibitive for large data sets.</p>
</dd>
<dt><b>db</b>:</dt><dd>
<p><code class="reqn">R=(1/n)*sum(R_{(i)})</code>
where <code class="reqn">R_{(i)}</code> stands for the maximum value of
<code class="reqn">R_{(ij)}</code> for <code class="reqn">i\neq j</code>, and <code class="reqn">R_{(ij)}</code> for
<code class="reqn">R_{(ij)}=(SSW_{(i)}+SSW_{(j)})/DC_{(ij)}</code>, where
<code class="reqn">DC_{(ij)}</code> is the 
distance between the centers of two clusters <code class="reqn">i, j</code>.</p>
</dd>
<dt><b>likelihood</b>:</dt><dd>
<p>under the assumption of
independence of the variables within a cluster, a cluster solution
can be regarded as a mixture model for the data, where the cluster
centers give the probabilities for each variable to be
<code class="reqn">1</code>. Therefore, the negative Log-likelihood can be computed and
used as a quantity measure for a cluster solution. Note that the
assumptions for applying special penalty terms, like in AIC or BIC,
are not fulfilled in this model, and also they show no effect for
these data sets.</p>
</dd>
<dt><b>ssi</b>:</dt><dd><p>this &ldquo;Simple Structure Index&rdquo;
combines three elements which influence the interpretability of a
solution, i.e., the maximum difference of each variable between the
clusters, the sizes of the most contrasting clusters and the
deviation of a variable in the cluster centers compared to its
overall mean. These three elements are multiplicatively combined and
normalized to give a value between <code class="reqn">0</code> and <code class="reqn">1</code>.</p>
</dd>
</dl>



<h3>Value</h3>

<p>Returns an vector with the indexes values.
</p>


<h3>Author(s)</h3>

<p>Evgenia Dimitriadou and Andreas Weingessel
</p>


<h3>References</h3>

<p>Andreas Weingessel, Evgenia Dimitriadou and Sara Dolnicar,
An Examination Of Indexes For Determining The Number
Of Clusters In Binary Data Sets,<br />
<a href="https://epub.wu.ac.at/1542/">https://epub.wu.ac.at/1542/</a><br />
and the references therein.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cclust">cclust</a></code>, <code><a href="#topic+kmeans">kmeans</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># a 2-dimensional example
x&lt;-rbind(matrix(rnorm(100,sd=0.3),ncol=2),
         matrix(rnorm(100,mean=1,sd=0.3),ncol=2))
cl&lt;-cclust(x,2,20,verbose=TRUE,method="kmeans")
resultindexes &lt;- clustIndex(cl,x, index="all")
resultindexes   
</code></pre>

<hr>
<h2 id='predict.cclust'>Assign clusters to new data</h2><span id='topic+predict.cclust'></span>

<h3>Description</h3>

<p>Assigns each data point (row in <code>newdata</code>) the cluster corresponding to
the closest center found in <code>object</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cclust'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cclust_+3A_object">object</code></td>
<td>
<p>Object of class <code>"cclust"</code> returned by a clustering algorithm such as <code><a href="#topic+cclust">cclust</a></code></p>
</td></tr>
<tr><td><code id="predict.cclust_+3A_newdata">newdata</code></td>
<td>
<p>Data matrix where columns correspond to variables and
rows to observations</p>
</td></tr>
<tr><td><code id="predict.cclust_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>predict.cclust</code> returns an object of class <code>"cclust"</code>.
Only <code>size</code> is changed as compared to the argument
<code>object</code>. 
</p>
<table>
<tr><td><code>cluster</code></td>
<td>
<p>Vector containing the indices of the clusters where
the data is mapped.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of data points in each cluster.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Evgenia Dimitriadou</p>


<h3>See Also</h3>

<p><code><a href="#topic+cclust">cclust</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># a 2-dimensional example
x&lt;-rbind(matrix(rnorm(100,sd=0.3),ncol=2),
         matrix(rnorm(100,mean=1,sd=0.3),ncol=2))
cl&lt;-cclust(x,2,20,verbose=TRUE,method="kmeans")
plot(x, col=cl$cluster)   

# a 3-dimensional example
x&lt;-rbind(matrix(rnorm(150,sd=0.3),ncol=3),
         matrix(rnorm(150,mean=1,sd=0.3),ncol=3),
         matrix(rnorm(150,mean=2,sd=0.3),ncol=3))
cl&lt;-cclust(x,6,20,verbose=TRUE,method="kmeans")
plot(x, col=cl$cluster)

# assign classes to some new data
y&lt;-rbind(matrix(rnorm(33,sd=0.3),ncol=3),
         matrix(rnorm(33,mean=1,sd=0.3),ncol=3),
         matrix(rnorm(3,mean=2,sd=0.3),ncol=3))
ycl&lt;-predict(cl, y)
plot(y, col=ycl$cluster)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
