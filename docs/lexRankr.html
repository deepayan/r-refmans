<!DOCTYPE html><html lang="en"><head><title>Help for package lexRankr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {lexRankr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bind_lexrank_'><p>Bind lexrank scores to a dataframe of text</p></a></li>
<li><a href='#lexRank'><p>Extractive text summarization with LexRank</p></a></li>
<li><a href='#lexRankFromSimil'><p>Compute LexRanks from pairwise sentence similarities</p></a></li>
<li><a href='#sentence_parser'><p>Utility to parse sentences from text</p></a></li>
<li><a href='#sentenceParse'><p>Parse text into sentences</p></a></li>
<li><a href='#sentenceSimil'><p>Compute distance between sentences</p></a></li>
<li><a href='#sentenceTokenParse'><p>Parse text into sentences and tokens</p></a></li>
<li><a href='#smart_stopwords'><p>SMART English Stopwords</p></a></li>
<li><a href='#tokenize'><p>Tokenize a character vector</p>
Parse the elements of a character vector into a list of cleaned tokens.</a></li>
<li><a href='#unnest_sentences_'><p>Split a column of text into sentences</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Extractive Summarization of Text with the LexRank Algorithm</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.2</td>
</tr>
<tr>
<td>Author:</td>
<td>Adam Spannbauer [aut, cre], Bryan White [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Adam Spannbauer &lt;spannbaueradam@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An R implementation of the LexRank algorithm described by G. Erkan and D. R. Radev (2004) &lt;<a href="https://doi.org/10.1613%2Fjair.1523">doi:10.1613/jair.1523</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/AdamSpannbauer/lexRankr/">https://github.com/AdamSpannbauer/lexRankr/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/AdamSpannbauer/lexRankr/issues/">https://github.com/AdamSpannbauer/lexRankr/issues/</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>SnowballC, igraph, Rcpp</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, testthat, R.rsp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-03-17 20:40:20 UTC; adamspannbauer</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-03-17 21:00:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='bind_lexrank_'>Bind lexrank scores to a dataframe of text</h2><span id='topic+bind_lexrank_'></span><span id='topic+bind_lexrank'></span>

<h3>Description</h3>

<p>Bind lexrank scores to a dataframe of sentences or to a dataframe of tokens with sentence ids
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bind_lexrank_(tbl, text, doc_id, sent_id = NULL, level = c("sentences",
  "tokens"), threshold = 0.2, usePageRank = TRUE, damping = 0.85,
  continuous = FALSE, ...)

bind_lexrank(tbl, text, doc_id, sent_id = NULL, level = c("sentences",
  "tokens"), threshold = 0.2, usePageRank = TRUE, damping = 0.85,
  continuous = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bind_lexrank__+3A_tbl">tbl</code></td>
<td>
<p>dataframe containing column of sentences to be lexranked</p>
</td></tr>
<tr><td><code id="bind_lexrank__+3A_text">text</code></td>
<td>
<p>name of column containing sentences or tokens to be lexranked</p>
</td></tr>
<tr><td><code id="bind_lexrank__+3A_doc_id">doc_id</code></td>
<td>
<p>name of column containing document ids corresponding to <code>text</code></p>
</td></tr>
<tr><td><code id="bind_lexrank__+3A_sent_id">sent_id</code></td>
<td>
<p>Only needed if <code>level</code> is &quot;tokens&quot;. name of column containing sentence ids corresponding to <code>text</code></p>
</td></tr>
<tr><td><code id="bind_lexrank__+3A_level">level</code></td>
<td>
<p>the parsed level of the text column to be lexranked.  i.e. is <code>text</code> a column of &quot;sentences&quot; or &quot;tokens&quot;?  The &quot;tokens&quot; level is provided to allow users to implement custom tokenization.  Note: even if the input <code>level</code> is &quot;tokens&quot; lexrank scores are assigned at the sentence level.</p>
</td></tr>
<tr><td><code id="bind_lexrank__+3A_threshold">threshold</code></td>
<td>
<p>The minimum simililarity value a sentence pair must have to be represented in the graph where lexRank is calculated.</p>
</td></tr>
<tr><td><code id="bind_lexrank__+3A_usepagerank">usePageRank</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to use the page rank algorithm for ranking sentences.  If <code>FALSE</code>, a sentences unweighted centrality will be used as the rank.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="bind_lexrank__+3A_damping">damping</code></td>
<td>
<p>The damping factor to be passed to page rank algorithm.  Ignored if <code>usePageRank</code> is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="bind_lexrank__+3A_continuous">continuous</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to use continuous LexRank.  Only applies if <code>usePageRank==TRUE</code>.  If <code>TRUE</code>, <code>threshold</code> will be ignored and lexRank will be computed using a weighted graph representation of the sentences. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="bind_lexrank__+3A_...">...</code></td>
<td>
<p>tokenizing options to be passed to lexRankr::tokenize.  Ignored if <code>level</code> is &quot;sentences&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe with an additional column of lexrank scores (column is given name lexrank)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
df &lt;- data.frame(doc_id = 1:3, 
                 text = c("Testing the system. Second sentence for you.", 
                          "System testing the tidy documents df.", 
                          "Documents will be parsed and lexranked."),
                 stringsAsFactors = FALSE)

## Not run: 
library(magrittr)

df %&gt;% 
  unnest_sentences(sents, text) %&gt;% 
  bind_lexrank(sents, doc_id, level = "sentences")

df %&gt;% 
  unnest_sentences(sents, text) %&gt;% 
  bind_lexrank_("sents", "doc_id", level = "sentences")

df &lt;- data.frame(doc_id  = c(1, 1, 1, 1, 1, 1, 1, 2, 2, 2,
                             2, 2, 2, 3, 3, 3, 3, 3, 3), 
                 sent_id = c(1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 
                             1, 1, 1, 1, 1, 1, 1, 1, 1), 
                 tokens = c("testing", "the", "system", "second", 
                            "sentence", "for", "you", "system", 
                            "testing", "the", "tidy", "documents", 
                            "df", "documents", "will", "be", "parsed", 
                            "and", "lexranked"),
                 stringsAsFactors = FALSE)

df %&gt;% 
  bind_lexrank(tokens, doc_id, sent_id, level = 'tokens')

## End(Not run)
</code></pre>

<hr>
<h2 id='lexRank'>Extractive text summarization with LexRank</h2><span id='topic+lexRank'></span>

<h3>Description</h3>

<p>Compute LexRanks from a vector of documents using the page rank algorithm or degree centrality the methods used to compute lexRank are discussed in &quot;LexRank: Graph-based Lexical Centrality as Salience in Text Summarization.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lexRank(text, docId = "create", threshold = 0.2, n = 3,
  returnTies = TRUE, usePageRank = TRUE, damping = 0.85,
  continuous = FALSE, sentencesAsDocs = FALSE, removePunc = TRUE,
  removeNum = TRUE, toLower = TRUE, stemWords = TRUE,
  rmStopWords = TRUE, Verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lexRank_+3A_text">text</code></td>
<td>
<p>A character vector of documents to be cleaned and processed by the LexRank algorithm</p>
</td></tr>
<tr><td><code id="lexRank_+3A_docid">docId</code></td>
<td>
<p>A vector of document IDs with length equal to the length of <code>text</code>.  If <code>docId == "create"</code> then doc IDs will be created as an index from 1 to <code>n</code>, where <code>n</code> is the length of <code>text</code>.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_threshold">threshold</code></td>
<td>
<p>The minimum simil value a sentence pair must have to be represented in the graph where lexRank is calculated.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_n">n</code></td>
<td>
<p>The number of sentences to return as the extractive summary.  The function will return the top <code>n</code> lexRanked sentences.  See <code>returnTies</code> for handling ties in lexRank.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_returnties">returnTies</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to return greater than <code>n</code> sentence IDs if there is a tie in lexRank.  If <code>TRUE</code>, the returned number of sentences will not be limited to <code>n</code>, but rather will return every sentence with a top 3 score.  If <code>FALSE</code>, the returned number of sentences will be <code>&lt;=n</code>. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_usepagerank">usePageRank</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to use the page rank algorithm for ranking sentences.  If <code>FALSE</code>, a sentences unweighted centrality will be used as the rank.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_damping">damping</code></td>
<td>
<p>The damping factor to be passed to page rank algorithm.  Ignored if <code>usePageRank</code> is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_continuous">continuous</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to use continuous LexRank.  Only applies if <code>usePageRank==TRUE</code>.  If <code>TRUE</code>, <code>threshold</code> will be ignored and lexRank will be computed using a weighted graph representation of the sentences. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_sentencesasdocs">sentencesAsDocs</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code>, indicating whether or not to treat sentences as documents when calculating tfidf scores for similarity. If <code>TRUE</code>, inverse document frequency will be calculated as inverse sentence frequency (useful for single document extractive summarization).</p>
</td></tr>
<tr><td><code id="lexRank_+3A_removepunc">removePunc</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to remove punctuation from text while tokenizing.  If <code>TRUE</code>, punctuation will be removed.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_removenum">removeNum</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to remove numbers from text while tokenizing.  If <code>TRUE</code>, numbers will be removed.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_tolower">toLower</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to coerce all of text to lowercase while tokenizing.  If <code>TRUE</code>, <code>text</code> will be coerced to lowercase.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_stemwords">stemWords</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to stem resulting tokens.  If <code>TRUE</code>, the outputted tokens will be tokenized using <code>SnowballC::wordStem()</code>.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_rmstopwords">rmStopWords</code></td>
<td>
<p><code>TRUE</code>, <code>FALSE</code>, or character vector of stopwords to remove from tokens. If <code>TRUE</code>, words in <code>lexRankr::smart_stopwords</code> will be removed prior to stemming. If <code>FALSE</code>, no stopword removal will occur. If a character vector is passed, this vector will be used as the list of stopwords to be removed.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lexRank_+3A_verbose">Verbose</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to <code>cat</code> progress messages to the console while running.  Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 2 column dataframe with columns <code>sentenceId</code> and <code>value</code>. <code>sentence</code> contains the ids of the top <code>n</code> sentences in descending order by <code>value</code>. <code>value</code> contains page rank score (if <code>usePageRank==TRUE</code>) or degree centrality (if <code>usePageRank==FALSE</code>).
</p>


<h3>References</h3>

<p><a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html">http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lexRank(c("This is a test.","Tests are fun.",
"Do you think the exam will be hard?","Is an exam the same as a test?",
"How many questions are going to be on the exam?"))
</code></pre>

<hr>
<h2 id='lexRankFromSimil'>Compute LexRanks from pairwise sentence similarities</h2><span id='topic+lexRankFromSimil'></span>

<h3>Description</h3>

<p>Compute LexRanks from sentence pair similarities using the page rank algorithm or degree centrality the methods used to compute lexRank are discussed in &quot;LexRank: Graph-based Lexical Centrality as Salience in Text Summarization.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lexRankFromSimil(s1, s2, simil, threshold = 0.2, n = 3,
  returnTies = TRUE, usePageRank = TRUE, damping = 0.85,
  continuous = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lexRankFromSimil_+3A_s1">s1</code></td>
<td>
<p>A character vector of sentence IDs corresponding to the <code>s2</code> and <code>simil</code> arguments</p>
</td></tr>
<tr><td><code id="lexRankFromSimil_+3A_s2">s2</code></td>
<td>
<p>A character vector of sentence IDs corresponding to the <code>s1</code> and <code>simil</code> arguments</p>
</td></tr>
<tr><td><code id="lexRankFromSimil_+3A_simil">simil</code></td>
<td>
<p>A numeric vector of similarity values that represents the similarity between the sentences represented by the IDs in <code>s1</code> and <code>s2</code>.</p>
</td></tr>
<tr><td><code id="lexRankFromSimil_+3A_threshold">threshold</code></td>
<td>
<p>The minimum simil value a sentence pair must have to be represented in the graph where lexRank is calculated.</p>
</td></tr>
<tr><td><code id="lexRankFromSimil_+3A_n">n</code></td>
<td>
<p>The number of sentences to return as the extractive summary.  The function will return the top <code>n</code> lexRanked sentences.  See <code>returnTies</code> for handling ties in lexRank.</p>
</td></tr>
<tr><td><code id="lexRankFromSimil_+3A_returnties">returnTies</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to return greater than <code>n</code> sentence IDs if there is a tie in lexRank.  If <code>TRUE</code>, the returned number of sentences will not be limited to <code>n</code>, but rather will return every sentence with a top 3 score.  If <code>FALSE</code>, the returned number of sentences will be <code>&lt;=n</code>. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lexRankFromSimil_+3A_usepagerank">usePageRank</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to use the page rank algorithm for ranking sentences.  If <code>FALSE</code>, a sentences unweighted centrality will be used as the rank.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lexRankFromSimil_+3A_damping">damping</code></td>
<td>
<p>The damping factor to be passed to page rank algorithm.  Ignored if <code>usePageRank</code> is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="lexRankFromSimil_+3A_continuous">continuous</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to use continuous LexRank.  Only applies if <code>usePageRank==TRUE</code>.  If <code>TRUE</code>, <code>threshold</code> will be ignored and lexRank will be computed using a weighted graph representation of the sentences. Defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 2 column dataframe with columns <code>sentenceId</code> and <code>value</code>. <code>sentenceId</code> contains the ids of the top <code>n</code> sentences in descending order by <code>value</code>. <code>value</code> contains page rank score (if <code>usePageRank==TRUE</code>) or degree centrality (if <code>usePageRank==FALSE</code>).
</p>


<h3>References</h3>

<p><a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html">http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lexRankFromSimil(s1=c("d1_1","d1_1","d1_2"), s2=c("d1_2","d2_1","d2_1"), simil=c(.01,.03,.5))
</code></pre>

<hr>
<h2 id='sentence_parser'>Utility to parse sentences from text</h2><span id='topic+sentence_parser'></span>

<h3>Description</h3>

<p>Utility to parse sentences from text; created to have a central shared sentence parsing function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentence_parser(text)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sentence_parser_+3A_text">text</code></td>
<td>
<p>Character vector to be parsed into sentences</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with length equal to 'length(text)'; list elements are character vectors of text parsed with sentence regex
</p>

<hr>
<h2 id='sentenceParse'>Parse text into sentences</h2><span id='topic+sentenceParse'></span>

<h3>Description</h3>

<p>Parse the elements of a character vector into a dataframe of sentences with additional identifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentenceParse(text, docId = "create")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sentenceParse_+3A_text">text</code></td>
<td>
<p>Character vector to be parsed into sentences</p>
</td></tr>
<tr><td><code id="sentenceParse_+3A_docid">docId</code></td>
<td>
<p>A vector of document IDs with length equal to the length of <code>text</code>.  If <code>docId == "create"</code> then doc IDs will be created as an index from 1 to <code>n</code>, where <code>n</code> is the length of <code>text</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with 3 columns and <code>n</code> rows, where <code>n</code> is the number of sentences found by the routine.  Column 1: <code>docId</code> document id for the sentence. Column 2: <code>sentenceId</code> sentence id for the sentence.  Column 3: <code>sentence</code> the sentences found in the routine.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sentenceParse("Bill is trying to earn a Ph.D.", "You have to have a 5.0 GPA.")
sentenceParse(c("Bill is trying to earn a Ph.D.", "You have to have a 5.0 GPA."),
               docId=c("d1","d2"))
</code></pre>

<hr>
<h2 id='sentenceSimil'>Compute distance between sentences</h2><span id='topic+sentenceSimil'></span>

<h3>Description</h3>

<p>Compute distance between sentences using modified idf cosine distance from &quot;LexRank: Graph-based Lexical Centrality as Salience in Text Summarization&quot;.  Output can be used as input to <code><a href="#topic+lexRankFromSimil">lexRankFromSimil</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentenceSimil(sentenceId, token, docId = NULL, sentencesAsDocs = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sentenceSimil_+3A_sentenceid">sentenceId</code></td>
<td>
<p>A character vector of sentence IDs corresponding to the <code>docId</code> and <code>token</code> arguments</p>
</td></tr>
<tr><td><code id="sentenceSimil_+3A_token">token</code></td>
<td>
<p>A character vector of tokens corresponding to the <code>docId</code> and <code>sentenceId</code> arguments</p>
</td></tr>
<tr><td><code id="sentenceSimil_+3A_docid">docId</code></td>
<td>
<p>A character vector of document IDs corresponding to the <code>sentenceId</code> and <code>token</code> arguments.  Can be <code>NULL</code> if <code>sentencesAsDocs</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sentenceSimil_+3A_sentencesasdocs">sentencesAsDocs</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code>, indicating whether or not to treat sentences as documents when calculating tfidf scores. If <code>TRUE</code>, inverse document frequency will be calculated as inverse sentence frequency (useful for single document extractive summarization)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 3 column dataframe of pairwise distances between sentences. Columns: <code>sent1</code> (sentence id), <code>sent2</code> (sentence id), &amp; <code>dist</code> (distance between <code>sent1</code> and <code>sent2</code>).
</p>


<h3>References</h3>

<p><a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html">http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sentenceSimil(docId=c("d1","d1","d2","d2"),
               sentenceId=c("d1_1","d1_1","d2_1","d2_1"),
               token=c("i", "ran", "jane", "ran"))
</code></pre>

<hr>
<h2 id='sentenceTokenParse'>Parse text into sentences and tokens</h2><span id='topic+sentenceTokenParse'></span>

<h3>Description</h3>

<p>Parse a character vector of documents into into both sentences and a clean vector of tokens.  The resulting output includes IDs for document and sentence for use in other <code>lexRank</code> functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentenceTokenParse(text, docId = "create", removePunc = TRUE,
  removeNum = TRUE, toLower = TRUE, stemWords = TRUE,
  rmStopWords = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sentenceTokenParse_+3A_text">text</code></td>
<td>
<p>A character vector of documents to be parsed into sentences and tokenized.</p>
</td></tr>
<tr><td><code id="sentenceTokenParse_+3A_docid">docId</code></td>
<td>
<p>A character vector of document Ids the same length as <code>text</code>.  If <code>docId=="create"</code> document Ids will be created.</p>
</td></tr>
<tr><td><code id="sentenceTokenParse_+3A_removepunc">removePunc</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to remove punctuation from <code>text</code> while tokenizing.  If <code>TRUE</code>, punctuation will be removed.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sentenceTokenParse_+3A_removenum">removeNum</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to remove numbers from <code>text</code> while tokenizing.  If <code>TRUE</code>, numbers will be removed.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sentenceTokenParse_+3A_tolower">toLower</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to coerce all of <code>text</code> to lowercase while tokenizing.  If <code>TRUE</code>, <code>text</code> will be coerced to lowercase.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sentenceTokenParse_+3A_stemwords">stemWords</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to stem resulting tokens.  If <code>TRUE</code>, the outputted tokens will be tokenized using <code>SnowballC::wordStem()</code>.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sentenceTokenParse_+3A_rmstopwords">rmStopWords</code></td>
<td>
<p><code>TRUE</code>, <code>FALSE</code>, or character vector of stopwords to remove from tokens. If <code>TRUE</code>, words in <code>lexRankr::smart_stopwords</code> will be removed prior to stemming. If <code>FALSE</code>, no stopword removal will occur. If a character vector is passed, this vector will be used as the list of stopwords to be removed.  Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of dataframes.  The first element of the list returned is the <code>sentences</code> dataframe; this dataframe has columns <code>docId</code>, <code>sentenceId</code>, &amp; <code>sentence</code> (the actual text of the sentence).  The second element of the list returned is the <code>tokens</code> dataframe; this dataframe has columns <code>docId</code>, <code>sentenceId</code>, &amp; <code>token</code> (the actual text of the token).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sentenceTokenParse(c("Bill is trying to earn a Ph.D.", "You have to have a 5.0 GPA."),
                   docId=c("d1","d2"))
</code></pre>

<hr>
<h2 id='smart_stopwords'>SMART English Stopwords</h2><span id='topic+smart_stopwords'></span>

<h3>Description</h3>

<p>English stopwords from the SMART information retrieval system (as documented in Appendix 11 of <a href="http://jmlr.csail.mit.edu/papers/volume5/lewis04a/">http://jmlr.csail.mit.edu/papers/volume5/lewis04a/</a>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smart_stopwords
</code></pre>


<h3>Format</h3>

<p>a character vector with 571 elements</p>


<h3>Source</h3>

<p><a href="http://jmlr.csail.mit.edu/papers/volume5/lewis04a/">http://jmlr.csail.mit.edu/papers/volume5/lewis04a/</a>
</p>

<hr>
<h2 id='tokenize'>Tokenize a character vector
Parse the elements of a character vector into a list of cleaned tokens.</h2><span id='topic+tokenize'></span>

<h3>Description</h3>

<p>Tokenize a character vector
Parse the elements of a character vector into a list of cleaned tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize(text, removePunc = TRUE, removeNum = TRUE, toLower = TRUE,
  stemWords = TRUE, rmStopWords = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tokenize_+3A_text">text</code></td>
<td>
<p>The character vector to be tokenized</p>
</td></tr>
<tr><td><code id="tokenize_+3A_removepunc">removePunc</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to remove punctuation from <code>text</code>.  If <code>TRUE</code>, punctuation will be removed.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_removenum">removeNum</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to remove numbers from <code>text</code>.  If <code>TRUE</code>, numbers will be removed.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_tolower">toLower</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to coerce all of <code>text</code> to lowercase.  If <code>TRUE</code>, <code>text</code> will be coerced to lowercase.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_stemwords">stemWords</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> indicating whether or not to stem resulting tokens.  If <code>TRUE</code>, the outputted tokens will be tokenized using <code>SnowballC::wordStem()</code>.  Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_rmstopwords">rmStopWords</code></td>
<td>
<p><code>TRUE</code>, <code>FALSE</code>, or character vector of stopwords to remove. If <code>TRUE</code>, words in <code>lexRankr::smart_stopwords</code> will be removed prior to stemming. If <code>FALSE</code>, no stopword removal will occur. If a character vector is passed, this vector will be used as the list of stopwords to be removed.  Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tokenize("Mr. Feeny said the test would be on Sat. At least I'm 99.9% sure that's what he said.")
tokenize("Bill is trying to earn a Ph.D. in his field.", rmStopWords=FALSE)
</code></pre>

<hr>
<h2 id='unnest_sentences_'>Split a column of text into sentences</h2><span id='topic+unnest_sentences_'></span><span id='topic+unnest_sentences'></span>

<h3>Description</h3>

<p>Split a column of text into sentences
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unnest_sentences_(tbl, output, input, doc_id = NULL,
  output_id = "sent_id", drop = TRUE)

unnest_sentences(tbl, output, input, doc_id = NULL,
  output_id = "sent_id", drop = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="unnest_sentences__+3A_tbl">tbl</code></td>
<td>
<p>dataframe containing column of text to be split into sentences</p>
</td></tr>
<tr><td><code id="unnest_sentences__+3A_output">output</code></td>
<td>
<p>name of column to be created to store parsed sentences</p>
</td></tr>
<tr><td><code id="unnest_sentences__+3A_input">input</code></td>
<td>
<p>name of input column of text to be parsed into sentences</p>
</td></tr>
<tr><td><code id="unnest_sentences__+3A_doc_id">doc_id</code></td>
<td>
<p>column of document ids; if not provided it will be assumed that each row is a different document</p>
</td></tr>
<tr><td><code id="unnest_sentences__+3A_output_id">output_id</code></td>
<td>
<p>name of column to be created to store sentence ids</p>
</td></tr>
<tr><td><code id="unnest_sentences__+3A_drop">drop</code></td>
<td>
<p>whether original input column should get dropped</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame of parsed sentences and sentence ids
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
df &lt;- data.frame(doc_id = 1:3, 
                 text = c("Testing the system. Second sentence for you.", 
                          "System testing the tidy documents df.", 
                          "Documents will be parsed and lexranked."),
                 stringsAsFactors=FALSE)

unnest_sentences(df, sents, text)
unnest_sentences_(df, "sents", "text")

## Not run: 
library(magrittr)

df %&gt;% 
  unnest_sentences(sents, text)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
