<!DOCTYPE html><html><head><title>Help for package tensorA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tensorA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#tensorA-package'><p>The tensorA package for tensor arithmetic</p></a></li>
<li><a href='#add.tensor'><p>Element-wise arithmetic operations +,-,*,/ with tensors</p></a></li>
<li><a href='#as.tensor'><p>Coercion to a tensor</p></a></li>
<li><a href='#bind.tensor'><p>A cbind/rbind for tensors</p></a></li>
<li><a href='#chol.tensor'><p>Cholesky decomposition of a tensor</p></a></li>
<li><a href='#delta.tensor'><p>Creates a Kronecker delta tensor</p></a></li>
<li><a href='#diag.tensor'><p>Creates a &quot;diagonal&quot; tensor</p></a></li>
<li><a href='#diagmul.tensor'><p>Multiplication of a tensor with a tensor given by its diagonal</p></a></li>
<li><a href='#drag.tensor'><p>Managing covariate and contravariate indices</p></a></li>
<li><a href='#einstein.tensor'><p>Tensor multiplication with Einstein's convention, by summing over</p>
all equally named indices.</a></li>
<li><a href='#ftable.tensor'><p>Pretty printing of tensors</p></a></li>
<li><a href='#inv.tensor'><p>Inversion of a tensor as linear mapping from tensors to tensors</p></a></li>
<li><a href='#is.tensor'><p>Checking for being a tensor</p></a></li>
<li><a href='#level.tensor'><p>The level (number of indices) of a tensor</p></a></li>
<li><a href='#margin.tensor'><p>Marginalization of tensors</p></a></li>
<li><a href='#mark.tensor'><p>Marks the names of a tensor with a mark</p></a></li>
<li><a href='#mean.tensor'><p>Mean and variance of tensors</p></a></li>
<li><a href='#mul.tensor'><p>Tensor multiplication for the tensor class</p></a></li>
<li><a href='#names.tensor'><p>Getting and setting index and dimensionnames of a tensor</p></a></li>
<li><a href='#norm.tensor'><p>Calculate the Euclidean norm or Euclidean operator norm of a tensor or its subtensors</p></a></li>
<li><a href='#one.tensor'><p>Creates a tensor with all entries 1</p></a></li>
<li><a href='#pos.tensor'><p>enumeration of index combinations</p></a></li>
<li><a href='#power.tensor'><p>Compute the power of a symmetric tensor</p></a></li>
<li><a href='#reorder.tensor'><p>Permutation of indices and storage sequence of a tensor</p></a></li>
<li><a href='#reptensor'><p>Repeats a tensor</p></a></li>
<li><a href='#riemann.tensor'><p>Tensor multiplication with Riemann's convention</p></a></li>
<li><a href='#sequencing'><p>Working with index sequences</p></a></li>
<li><a href='#slice.tensor'><p>Working with the indices of a tensor (accessing, slicing, renaming, ...)</p></a></li>
<li><a href='#solve.tensor'><p>Solving linear equations with tensors</p></a></li>
<li><a href='#svd.tensor'><p>Singular value decomposition of tensors</p></a></li>
<li><a href='#to.matrix.tensor'><p>The matrix corresponding to a tensor seen as a linear mapping of tensors.</p></a></li>
<li><a href='#to.tensor'><p>Creates a tensor object</p></a></li>
<li><a href='#toPos.tensor'><p>get the position of an index of tensor</p></a></li>
<li><a href='#trace.tensor'><p>Collapse a tensor</p></a></li>
<li><a href='#tripledelta.tensor'><p>A tensor with entry 1 if and only if three indices are equal</p></a></li>
<li><a href='#undrop.tensor'><p>Adds a spurious dimension to a tensor</p></a></li>
<li><a href='#untensor'><p>Removes indices/dimensions from a tensor</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.36.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-11-13</td>
</tr>
<tr>
<td>Title:</td>
<td>Advanced Tensor Arithmetic with Named Indices</td>
</tr>
<tr>
<td>Author:</td>
<td>K. Gerald van den Boogaart &lt;boogaart@uni-greifswald.de&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>K. Gerald van den Boogaart &lt;boogaart@math.tu-freiberg.de&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.2.0), stats</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides convenience functions for advanced
        linear algebra with tensors and computation with data sets of
        tensors on a higher level abstraction. It includes Einstein and
        Riemann summing conventions, dragging, co- and contravariate
        indices, parallel computations on sequences of tensors.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www.stat.boogaart.de/tensorA/">http://www.stat.boogaart.de/tensorA/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-13 17:01:17 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-13 17:28:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='tensorA-package'>The tensorA package for tensor arithmetic</h2><span id='topic+tensorA-package'></span><span id='topic+tensor'></span><span id='topic+Tensor'></span><span id='topic+tensorA'></span>

<h3>Description</h3>

<p>tensorA stands for &quot;tensor arithmetic&quot;. A tensor is a mathematical
generalization of vector and matrix with
many applications in physics, geometry and in the statistics of
vectors valued data. However the package is also useful in any case,
where computations on sequences of matrices, vectors or even tensors
is involved. 
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
    Package: </td><td style="text-align: left;"> tensorA</td>
</tr>
<tr>
 <td style="text-align: left;">
    Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
    Version: </td><td style="text-align: left;"> 0.1</td>
</tr>
<tr>
 <td style="text-align: left;">
    Date: </td><td style="text-align: left;"> 2006-06-08</td>
</tr>
<tr>
 <td style="text-align: left;">
    License: </td><td style="text-align: left;"> GPL Version 2 or newer</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>

<p>The tensorA package is made to allow programming for tensors in R on
the same level of abstraction as we know from matrices. It
provides many of the
mathematical operations common in tensor arithmetics including the
whole tensor calculus of covariate and contravariate indices, naming
of indices, sequence of indices, decompositions of tensors, Einstein
and Riemann summing conventions and vectorized computations on datasets
of tensors just like the well vectorization of numbers in R. It
provides tools to write tensor formulae very close to there paper form
and to handle tensors of arbitrary level with simple programs.
<br />
The whole documentation of the package is best read in
pdf or dvi format since it contains complicated mathematical formulae
with multi-indices.
<br />
</p>
<p>Simply speaking a tensor (see <code><a href="#topic+to.tensor">to.tensor</a></code>) is just a
multidimensional array
<code>A[,,]</code>. The number
of indices (i.e. <code>length(dim(A))</code> is called the level of the
tensor (see <code><a href="#topic+level.tensor">level.tensor</a></code>). A tensor is mathematically it
is denoted
by a core symbol (e.g. A) with multiple indices:e.g.
</p>
<p style="text-align: center;"><code class="reqn">A_{ijk}</code>
</p>

<p>The indices <code class="reqn">i,j,k</code> can be seen as names for the dimensions
and as integer numbers giving the respective index into the array.
However the tensor is an algebraical object with many algebraical
operations defined on it, which are also of relevancy for programming,
e.g. in the parallel treatment of multiple linear equation systems.
</p>
<p>To understand the package we need to understand tensors including
their mathematical origin, the corresponding calculus, notation and
basic
operations.
<br />
One mathematical interpretation of a tensor, which is most relevant
for physics, that of a multi-linear
form of <code class="reqn">level(A)</code> vectors, i.e. a function of <code class="reqn">level(A)</code> many
vectors
to the real or complex numbers, which is linear with respect to each
of its arguments. E.g. the two vectors &quot;plane face direction&quot; and &quot;force
direction&quot; are mapped to the actual force by the stress tensor.
<br />
Row vectors are a special case of that and likewise column vectors as
linear forms for row vectors. Matrices are bilinear forms of a row
vector and a column vector. Thus Vectors and Matrices are examples of
tensors of level 1 and 2. 
</p>
<p>Another interpretation of a tensor is the that of a linear mapping,
quite like a matrix, but from a tensor space (e.g. the space of
matrices or vectors seen as tensor) to another tensor space
(e.g. again a space of matrices). An
example for that is the Hook elasticity tensor mapping the strain
tensor (i.e. a
matrix describing the local deformation) to the stress tensor (i.e. a
matrix describing the local forces). The Hook tensor is a tensor of
level 4. Statistically relevant tensors of level 4 are
e.g. covariances of matrices mapping two linear forms (i.e. 2 level 2
tensors) on observed
matrices to there covariance.  The mapping is performed with the
tensor product, which is not unlike a matrix product, however more
general. Let denote <code class="reqn">A</code> a matrix and <code class="reqn">v</code> a vector, we
would write <code class="reqn">r=Ab</code> for the matrix product and <code>r &lt;- A%*%b</code> in R,
which is defined as:
</p>
<p style="text-align: center;"><code class="reqn">r_i = \sum_{j=1}^{j_{\max}} A_{ij}b_j </code>
</p>

<p>We know that we have to use the \(j\)-dimension in the summing, since
the matrix multiplication rule says &quot;row times column&quot;. 
Since a tensor can have more than two indices there is no row or
column specified and we need to specify the inner product
differently. To do this in
the Einstein-Notation writing the tensor always with indices
<code class="reqn">r_i=A_{ij}b_j</code> and according to the Einstein summing rule the
entries of \(r_i\) are given by an implicit sum over all indices which
show
up twice in this notation:
</p>
<p style="text-align: center;"><code class="reqn"> r_i=\sum_{j=1}^{j_{\max}} A_{ij}b_j </code>
</p>

<p>This notation allows for a multitude of other products:
<code class="reqn"> A_{ij}b_i=t(A)b </code>, <code class="reqn"> A_{ij}b_k=outer(A,b)
  </code> ,
<code class="reqn"> A_{ii}b_j=trace(A)b </code> with equal
simplicity and without any
additional functions. 
More complicated products involving more than tensors of level two
can not even be formulated in pure matrix algebra without
re-dimensioning of arrays e.g. <code class="reqn">b_ib_jb_k</code>,
<code class="reqn">A_{ijk}b_j</code>. The
Einstein summing rule is implemented in
<code><a href="#topic+einstein.tensor">einstein.tensor</a></code> and supported by the index sequencing
functions <code><a href="#topic++24.tensor">$.tensor</a></code> and <code><a href="#topic++7C.tensor">|.tensor</a></code>. A general
multiplication allowing to
identify and sum over any two indices is implemented in
<code><a href="#topic+trace.tensor">trace.tensor</a></code>, when the indices are in the same tensor
and in <code><a href="#topic+mul.tensor">mul.tensor</a></code>, when the indices to sum over are in
different tensors. 
<br />
Tensors with the same level and dimensions (identified by name
and dimension) can also be added like matrices according to the rule
that the values with the same combination of index values are added
(see <code><a href="#topic+add.tensor">add.tensor</a></code>). The implementation takes care of the
sequence of the indices and rearranges them accordingly to match
dimensions with the same name. E.g. the tensor addition
</p>
<p style="text-align: center;"><code class="reqn">E_ijk=A_{ijk}+B_{kji}</code>
</p>
 
<p>has the effect, which is expressed by the same formula read in
entries, which is also true for the more surprising
</p>
<p style="text-align: center;"><code class="reqn">E_ijk=A_{ij}+B_{kj}</code>
</p>
 
<p><br />
Like a matrix a tensor can also be seen as a mapping from one tensor
space to another:
</p>
<p style="text-align: center;"><code class="reqn">A_{i_1\ldots i_d j_1 \ldots j_e}x_{j_1 \ldots j_e}=b_{i_1\ldots
      i_d}</code>
</p>

<p>In this reading all the standard matrix computations and
decompositions get a tensorial interpretation and generalization. The
package provides some of these (see <code><a href="#topic+svd.tensor">svd.tensor</a></code>). 
<br />
Another interpretation of tensors is as a sequence of tensors of lower
level. E.g. a data matrix is seen as a sequence of vectors in
multivariate dataset. The tensorA library provides means to do
computation on these in parallel on these sequences of tensors like we
can do parallel computation on sequences of numbers. This is typically
done by the <code>by=</code> argument present in most functions and giving
the index enumerating the elements of the sequence.<br />
E.g. If we have
sequence <code class="reqn">V_{ijd}</code> of variance matrices <code class="reqn">V_{ij}</code> of some sequence
<code class="reqn">v_{id}</code> of vectors and we would like to transform the vectors
with some Matrix <code class="reqn">M_{i'i}</code> we would get the sequence of
transformed variances
by <code class="reqn">V_{ijd} M_{i'i}M_{j'j}</code>.
However if the <code class="reqn">M_{ki}</code> are different for each of the elements
in sequence we would have stored them in a tensor <code class="reqn">M_{kid}</code> and
would have to replace  <code class="reqn">M_{kid}</code> with <code class="reqn">M_{kidd'}=M_{kid}</code> if
<code class="reqn">d=d'</code> and zero otherwise. We can than get our result by
</p>
<p style="text-align: center;"><code class="reqn">V_{ijd}M_{i'id'd}M_{j'jd'd''}</code>
</p>
<p> and we would have a by dimension
of
<code>by="d"</code>. These operations are not strictly
mathematical tensor operation, but generalizations of the
vectorization approach of R. This is also closely related to
<code><a href="#topic+diagmul.tensor">diagmul.tensor</a></code> or <code><a href="#topic+diag.tensor">diag.tensor</a></code>.   
<br />
To complicate things the Einstein rule is only valid in case of
tensors represented with respect to a orthogonal basis. Otherwise
tensors get lower and upper indices like
</p>
<p style="text-align: center;"><code class="reqn">A_{i\cdot k}^{\cdot j \cdot}</code>
</p>

<p>for representation in the covariate and contravariate form of the
basis. In this case the Riemann summing rule applies which only sums
over pairs of the same index, where one is in the lower and one is in
the upper position. The contravariate form is represented with indices
prefixed by <code>^</code>. 
<br /> 
The state of being covariate or contravariate can be changed by the
dragging rule, which allows to switch between both state through the
multiplication with the geometry tensors <code class="reqn">g_i^{\;j}</code>. This
can be done through <code><a href="#topic+drag.tensor">drag.tensor</a></code>.
</p>


<h3>Author(s)</h3>

<p>K.Gerald van den Boogaart &lt;boogaart@uni-greifswald.de</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>, <code><a href="#topic+mul.tensor">mul.tensor</a></code> ,
<code><a href="#topic+einstein.tensor">einstein.tensor</a></code>, <code><a href="#topic+add.tensor">add.tensor</a></code>,
<code><a href="#topic++5B+5B.tensor">[[.tensor</a></code>, <code><a href="#topic++7C.tensor">|.tensor</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor( 1:20, c(a=2,b=2,c=5) )
A
ftable(A)
B &lt;- to.tensor( c(0,1,1,0) , c(a=2,"a'"=2))
A %e% B
drag.tensor( A , B, c("a","b"))
A %e% one.tensor(c(c=5))/5     # a mean of matrices
reorder.tensor(A,c("c","b","a"))
A -  reorder.tensor(A,c("c","b","a"))  # =0 since sequence is irrelevant
inv.tensor(A,"a",by="c")  

</code></pre>

<hr>
<h2 id='add.tensor'>Element-wise arithmetic operations +,-,*,/ with tensors</h2><span id='topic+add.tensor'></span><span id='topic+-.tensor'></span><span id='topic++2B.tensor'></span><span id='topic++2A.tensor'></span><span id='topic++2F.tensor'></span>

<h3>Description</h3>

<p>Adds/subs/multiplies/devides tensors element by element . 
The luxury difference to a simple <code>+</code> is that we
do not need to consider the correct permutation of indices  or rules
on implicit replication, since all of this is handled automatically.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add.tensor(X,Y,op="+",only=NULL)
## Methods for class tensor
# x + y
# x - y
# x * y
# x - y
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add.tensor_+3A_x">X</code></td>
<td>
<p>a tensor</p>
</td></tr>
<tr><td><code id="add.tensor_+3A_y">Y</code></td>
<td>
<p>a tensor</p>
</td></tr>
<tr><td><code id="add.tensor_+3A_op">op</code></td>
<td>
<p>a binary function used to perform the &quot;addition&quot;</p>
</td></tr>
<tr><td><code id="add.tensor_+3A_only">only</code></td>
<td>
<p>a list of dimnames that may be considered as equal. This
parameter is here to allow parallelization of tensors with only
partially known structure.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The tensors are properly reordered such that dimensions of the same
name are identified. If dimensions are missing in one of the tensors
it is correspondingly repeated. 
</p>


<h3>Value</h3>

<p>A tensor giving the element-wise operation X,Y. If some of the
indices are missing in one of the tensors they are added by repetition.  
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(U=2,V=2,W=5))
add.tensor(A,A)/2 -A
(A+A)/2
A/A
A * 1/A
norm.tensor(reorder.tensor(A,c(2,3,1)) - A)
</code></pre>

<hr>
<h2 id='as.tensor'>Coercion to a tensor</h2><span id='topic+as.tensor'></span><span id='topic+as.tensor.default'></span><span id='topic+as.tensor.tensor'></span>

<h3>Description</h3>

<p>Coerces a array to a tensor keeping dimension and names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.tensor(X,...)
## Default S3 method:
as.tensor(X,...,dims=NULL)
## S3 method for class 'tensor'
as.tensor(X,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.tensor_+3A_x">X</code></td>
<td>
<p>a multidimensional array</p>
</td></tr>
<tr><td><code id="as.tensor_+3A_...">...</code></td>
<td>
<p>further generic arguments</p>
</td></tr>
<tr><td><code id="as.tensor_+3A_dims">dims</code></td>
<td>
<p>the new dim attribute to be used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main idea is that a multiway array like a vector or a matrix is
nothing else than a tensor for R, but it still needs the tensor class
be used with the tensorA library. However this is more a convenience
function for
migraters than a proper way construct a tensor, which is done by
<code><a href="#topic+to.tensor">to.tensor</a></code>. 
</p>


<h3>Value</h3>

<p>a tensor containing the same data as X
</p>


<h3>Note</h3>

<p>You should typically use the <code><a href="#topic+to.tensor">to.tensor</a></code> to generate a
tensor, when you want to write vectorizable functions for tensors.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- diag(5)
as.tensor(A)
</code></pre>

<hr>
<h2 id='bind.tensor'>A cbind/rbind for tensors</h2><span id='topic+bind.tensor'></span>

<h3>Description</h3>

<p>Tensors can be put side by side in one dimension if they are of equal
size in all other dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bind.tensor(A,dA=NULL,B,dB=dA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bind.tensor_+3A_a">A</code></td>
<td>
<p>the first tensor</p>
</td></tr>
<tr><td><code id="bind.tensor_+3A_da">dA</code></td>
<td>
<p>the dimension of A to be used for binding the tensors</p>
</td></tr>
<tr><td><code id="bind.tensor_+3A_b">B</code></td>
<td>
<p>the second tensor</p>
</td></tr>
<tr><td><code id="bind.tensor_+3A_db">dB</code></td>
<td>
<p>the dimension of B to be used for binding the tensors</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function works like a cbind or rbind function for tensors.  
</p>


<h3>Value</h3>

<p>a tensor with the tensors combined to one
</p>


<h3>Note</h3>

<p>binding does not preserve the sequence of the dimensions.
</p>


<h3>Author(s)</h3>

<p>K.Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+base">base</a>{cbind}</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  A &lt;- to.tensor(1:6,c(a=2,b=3))
bind.tensor(A,"a",A)
bind.tensor(A,"b",A)
</code></pre>

<hr>
<h2 id='chol.tensor'>Cholesky decomposition of a tensor</h2><span id='topic+chol.tensor'></span>

<h3>Description</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. This
function computes its Cholesky decomposition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chol.tensor(X,i,j,...,name="lambda")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chol.tensor_+3A_x">X</code></td>
<td>
<p>The tensor to be decomposed</p>
</td></tr>
<tr><td><code id="chol.tensor_+3A_i">i</code></td>
<td>
<p>The image dimensions of the linear mapping</p>
</td></tr>
<tr><td><code id="chol.tensor_+3A_j">j</code></td>
<td>
<p>The coimage dimensions of the linear mapping</p>
</td></tr>
<tr><td><code id="chol.tensor_+3A_name">name</code></td>
<td>
<p>The name of the eigenspace dimension. This is the
dimension created by the decompositions, in which the eigenvectors
are <code class="reqn">e_i</code></p>
</td></tr>
<tr><td><code id="chol.tensor_+3A_...">...</code></td>
<td>
<p>for generic use only</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. Let
denote <code class="reqn">R_i</code> the space of real tensors with dimensions
<code class="reqn">i_1...i_d</code>.
</p>

<dl>
<dt>chol.tensor</dt><dd><p>Computes for a  tensor
<code class="reqn"> a_{i_1 \ldots i_dj_1 \ldots j_d} </code> representing a positive definit mapping
form <code class="reqn">R_j</code> to <code class="reqn">R_i</code> with equal dimension structure in <code class="reqn">i</code>
and <code class="reqn">j</code> its &quot;Cholesky&quot; decomposition
<code class="reqn">L_{i_1 \ldots i_d \lambda{}}</code> such that
</p>
<p style="text-align: center;"><code class="reqn">
    a_{i_1...i_dj_1...j_d}=\sum_{\lambda{}} L_{i_1...i_d \lambda{}}L_{j_1...j_d \lambda{}}
  </code>
</p>

</dd>
</dl>



<h3>Value</h3>

<p>a tensor
</p>


<h3>Note</h3>

<p>A <code>by</code> argument is not necessary, since both processing
dimensions have to be given.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>, <code><a href="#topic+svd.tensor">svd.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

A &lt;- to.tensor(rnorm(15),c(a=3,b=5))
AAt &lt;- einstein.tensor(A,mark(A,i="a"))
ch &lt;- chol.tensor(AAt,"a","a'",name="lambda")
#names(ch)[1]&lt;-"lambda"
einstein.tensor(ch,mark(ch,i="a")) # AAt

A &lt;- to.tensor(rnorm(30),c(a=3,b=5,c=2))
AAt &lt;- einstein.tensor(A,mark(A,i="a"),by="c")
ch &lt;- chol.tensor(AAt,"a","a'",name="lambda")
einstein.tensor(ch,mark(ch,i="a"),by="c") #AAt

	     

</code></pre>

<hr>
<h2 id='delta.tensor'>Creates a Kronecker delta tensor</h2><span id='topic+delta.tensor'></span>

<h3>Description</h3>

<p>The delta tensor is the tensor equivalent of the identity. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delta.tensor(d,mark="'",dn=NULL,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="delta.tensor_+3A_d">d</code></td>
<td>
<p>the row dimensions </p>
</td></tr>
<tr><td><code id="delta.tensor_+3A_mark">mark</code></td>
<td>
<p>a character to be concatenated to the names of the row
dimensions to  get the column dimension names</p>
</td></tr>
<tr><td><code id="delta.tensor_+3A_dn">dn</code></td>
<td>
<p>dimnames for the result</p>
</td></tr>
<tr><td><code id="delta.tensor_+3A_by">by</code></td>
<td>
<p>the dimensions which should not be duplicated</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">E_{i_1\ldots i_n j_1\ldots
      j_n}=\delta_{i_1j_1}\ldots\delta_{i_nj_n}</code>
</p>



<h3>Value</h3>

<p>a tensor with dimension <code>c(d,mark(d,mark))</code>
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>delta.tensor(c(a=2,b=3))
</code></pre>

<hr>
<h2 id='diag.tensor'>Creates a &quot;diagonal&quot; tensor</h2><span id='topic+diag.tensor'></span>

<h3>Description</h3>

<p>The diagonal tensor is the tensor equivalent of the diagonal matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diag.tensor(X,mark="'",dn=NULL,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diag.tensor_+3A_x">X</code></td>
<td>
<p> a tensor containing the diagonal entries. </p>
</td></tr>
<tr><td><code id="diag.tensor_+3A_mark">mark</code></td>
<td>
<p>a character to be concatenated to the names of the row
dimensions to  get the column dimension names</p>
</td></tr>
<tr><td><code id="diag.tensor_+3A_dn">dn</code></td>
<td>
<p>dimnames which are used twice</p>
</td></tr>
<tr><td><code id="diag.tensor_+3A_by">by</code></td>
<td>
<p>The diagonal tensor is created for each level of the indices
in <code>by</code>. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">E_{i_1\ldots i_n j_1\ldots
      j_n}=\delta_{i_1j_1}\ldots\delta_{i_nj_n}</code>
</p>



<h3>Value</h3>

<p>a tensor with dimension <code>c(dim(X),mark(dim(X),mark))</code>
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:4,c(a=2,b=2))
diag.tensor(A)
diag.tensor(A,by="b")

</code></pre>

<hr>
<h2 id='diagmul.tensor'>Multiplication of a tensor with a tensor given by its diagonal</h2><span id='topic+diagmul.tensor'></span>

<h3>Description</h3>

<p>This is a convenience function for scaling elements of a tensor with
different numbers based on their position in the tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diagmul.tensor(X,i=names(D),D,j=i,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diagmul.tensor_+3A_x">X</code></td>
<td>
<p>The tensor to be scaled</p>
</td></tr>
<tr><td><code id="diagmul.tensor_+3A_d">D</code></td>
<td>
<p>A tensor containing scaling constants</p>
</td></tr>
<tr><td><code id="diagmul.tensor_+3A_i">i</code></td>
<td>
<p>numeric of character vector giving the dimensions of X to be
used for the product.</p>
</td></tr>
<tr><td><code id="diagmul.tensor_+3A_j">j</code></td>
<td>
<p>numeric of character vector giving the dimensions of D to be
used for the product.</p>
</td></tr>
<tr><td><code id="diagmul.tensor_+3A_by">by</code></td>
<td>
<p>Every operation is parallel for all levels of by in X and/or D. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let </p>
<p style="text-align: center;"><code class="reqn">X_{i_1\ldots i_d k_1 \ldots k_d}</code>
</p>
<p> and
</p>
<p style="text-align: center;"><code class="reqn">D_{j_1\ldots j_d}</code>
</p>
<p> than the result is:
</p>
<p style="text-align: center;"><code class="reqn">E_{i_1\ldots i_d k_1 \ldots k_d}=X_{i_1\ldots i_d k_1 \ldots k_d}D_{j_1\ldots j_d}</code>
</p>



<h3>Value</h3>

<p>A tensor with the shape   and dimensions as X with entries
<code class="reqn">X_{ik}</code> scaled by <code class="reqn">D_im</code>, where  <code class="reqn">i</code> and
<code class="reqn">k</code> can represent multi-indices.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>(A &lt;- matrix(rep(1:3,each=3),nrow=3))
(b &lt;- to.tensor(c(1,1/2,1/3)))
diagmul.tensor(as.tensor(A),2,as.tensor(c(1,1/2,1/3)),1)
diagmul.tensor(as.tensor(A),1,as.tensor(c(1,1/2,1/3)),1)
A %*% diag(b)
diag(b) %*% A
</code></pre>

<hr>
<h2 id='drag.tensor'>Managing covariate and contravariate indices</h2><span id='topic+drag.tensor'></span><span id='topic+is.covariate'></span><span id='topic+is.covariate.tensor'></span><span id='topic+is.covariate.numeric'></span><span id='topic+is.covariate.character'></span><span id='topic+is.contravariate'></span><span id='topic+is.contravariate.tensor'></span><span id='topic+is.contravariate.numeric'></span><span id='topic+is.contravariate.character'></span><span id='topic+contraname'></span><span id='topic+as.covariate'></span><span id='topic+as.contravariate'></span><span id='topic+as.covariate.character'></span><span id='topic+as.contravariate.character'></span>

<h3>Description</h3>

<p>Each index of a tensor can be covariate or contravariate. The <code>is.*</code>
routines check the state of the individual indices based on the
tensor, its dimension or its index names. <code>drag.tensor</code> can
change the state for the tensor and contraname for the names of the tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>drag.tensor(x,g,d)
contraname(x)
is.covariate(x,...)
## S3 method for class 'tensor'
is.covariate(x,...)
## S3 method for class 'numeric'
is.covariate(x,...)
## S3 method for class 'character'
is.covariate(x,...)
as.covariate(x,...)
## S3 method for class 'character'
as.covariate(x,...)
is.contravariate(x,...)
## S3 method for class 'numeric'
is.contravariate(x,...)
## S3 method for class 'character'
is.contravariate(x,...)
as.contravariate(x,...)
## S3 method for class 'character'
as.contravariate(x,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="drag.tensor_+3A_x">x</code></td>
<td>
<p>the tensor, its dimension (for <code>*.numeric</code>) or its
index-names (for <code>*.character</code> and <code>contraname</code>)
</p>
</td></tr>
<tr><td><code id="drag.tensor_+3A_g">g</code></td>
<td>
<p>The geometry tensor <code class="reqn">g_{ij}</code> giving the
transformation between covariate and contravariate. It needs to have
either  covariate and or contravariate indices.
</p>
</td></tr>
<tr><td><code id="drag.tensor_+3A_d">d</code></td>
<td>
<p>a vector (or list) of indices that should be dragged,
i.e. multiplied with <code class="reqn">g_i^{\;j}</code> in the right way such
that it changes from covariate to contravariate or vice versa. The
name of the index is kept, only its state changes. The index is thus
dragged from one state to the other. Indices can given in covariate
or contravariate form.
</p>
</td></tr>
<tr><td><code id="drag.tensor_+3A_...">...</code></td>
<td>
<p>only for generic use</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The covariate and contravariate state of a dimension corresponds to
column and row  vectors. The transformation between these type is done
by a linear mapping give by the geometry tensor g, which is the
identity matrix if the enclosing the geometry is represented by the
orthonormal basis and ordinary scalar product.  
</p>


<h3>Value</h3>

<table>
<tr><td><code>drag.tensor</code></td>
<td>
<p>returns a tensor like x but with the dimension </p>
</td></tr>
<tr><td><code>is.covariate</code></td>
<td>
<p>returns a boolean vector giving true for every
covariate index</p>
</td></tr>
<tr><td><code>is.contravariate</code></td>
<td>
<p>returns a boolean vector giving true for every
contravariate index</p>
</td></tr>
<tr><td><code>as.*</code></td>
<td>
<p>changes the state of the indices</p>
</td></tr>
<tr><td><code>contraname</code></td>
<td>
<p>returns the names with opposite the opposite covariate
and contravariate state</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+riemann.tensor">riemann.tensor</a></code>, <code><a href="#topic+to.tensor">to.tensor</a></code>, <code><a href="#topic+Tensor">Tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>g &lt;- to.tensor(c(1,2,0,1),c(i=2,j=2))
A &lt;- to.tensor(rnorm(8),c(a=2,b=2,c=2))
A2 &lt;- drag.tensor(A,g,c("b","c"))
A2
names(A2)
as.covariate(names(A2))
as.contravariate(names(A2))
is.covariate(A2)
is.contravariate(A2)
riemann.tensor(A2,g)

</code></pre>

<hr>
<h2 id='einstein.tensor'>Tensor multiplication with Einstein's convention, by summing over
all equally named indices.</h2><span id='topic+einstein.tensor'></span><span id='topic++25e+25'></span><span id='topic++25e+25.tensor'></span>

<h3>Description</h3>

<p>Multiplies tensors by multiplying over all duplicate names according
to Einsteins summing convention by doing an implicit inner product
over all dimensions with the same name.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>einstein.tensor(...,only=NULL,by=NULL)
## Methods for class tensor
# x %e% y
## Default method
# x %e% y
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="einstein.tensor_+3A_...">...</code></td>
<td>
<p>some tensors, or a renaming code</p>
</td></tr>
<tr><td><code id="einstein.tensor_+3A_only">only</code></td>
<td>
<p>optional list, if given only names in this list are
automatically processed</p>
</td></tr>
<tr><td><code id="einstein.tensor_+3A_x">x</code></td>
<td>
<p>a tensor</p>
</td></tr>
<tr><td><code id="einstein.tensor_+3A_y">y</code></td>
<td>
<p>a tensor</p>
</td></tr>
<tr><td><code id="einstein.tensor_+3A_by">by</code></td>
<td>
<p>the parallel dimensions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>see <code><a href="#topic+mul.tensor">mul.tensor</a></code> on details on tensor
multiplication. In <code>einstein.tensor</code> complex operations can be
performed by command and renaming code: The arguments are processed
from left to right and multiplied. Unnamed attributes are regarded as
tensors or scalars and
multiplied with the current result by the Einstein summing convention,
which means an inner product over all dimensions with the same
name. Named attributes can either have the name diag, which performs a
diagmul according to the same-name convention or be of the form
<code>A="B"</code> or <code>"A"="B"</code>, for which we have two cases. If both
names are
present in the current result, an inner multiplication (trace) of on
these two dimensions is
performed. If only the first is a name up to this point, the specific
dimension is renamed to the second name. This renaming might be
visible in the result or inducing a multiplication according to the
Einstein convention later. 
</p>


<h3>Value</h3>

<p>the tensor product of all the tensors along all duplicate dimensions.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+mul.tensor">mul.tensor</a></code>, <code><a href="#topic+to.tensor">to.tensor</a></code>, <code><a href="#topic+riemann.tensor">riemann.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(U=2,V=2,W=5))
B &lt;- to.tensor(1:30,list(U=c("a","b","c"),V=c("B1","B2"),W=1:5))
einstein.tensor(A,U="U'",B)
einstein.tensor(A,U="U'",mark(B,"k"))
einstein.tensor(A,U="U'",mark(B,"k"),V="Vk",W="Wk")
einstein.tensor(A,U="U'",mark(B,"k"),V="Vk",W="Wk",1/10)
einstein.tensor(A,U="U'",mark(B,"k"),V="Vk",W="Wk",diag=to.tensor(c(1,1/10,1/100),c(Uk=3)))

ftable(einstein.tensor(A,U="U'",B))
ftable(einstein.tensor(A,U="U'",mark(B,"k")))
ftable(einstein.tensor(A,U="U'",mark(B,"k"),V="Vk",W="Wk"))
ftable(einstein.tensor(A,U="U'",mark(B,"k"),V="Vk",W="Wk",1/10))
ftable(einstein.tensor(A,U="U'",mark(B,"k"),V="Vk",W="Wk",diag=to.tensor(c(1,1/10,1/100),c(Uk=3))))

dim(A[[U=~M]])
A[[U=~M]] 
A[[U=~M,V=~"L"]] 

</code></pre>

<hr>
<h2 id='ftable.tensor'>Pretty printing of tensors</h2><span id='topic+ftable.tensor'></span>

<h3>Description</h3>

<p>Returns the tensor as (flat) ftable, providing a pretty output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tensor'
ftable(x,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ftable.tensor_+3A_x">x</code></td>
<td>
<p>the tensor</p>
</td></tr>
<tr><td><code id="ftable.tensor_+3A_...">...</code></td>
<td>
<p>additional arguments to ftable</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is called for a pretty output of a tensor, just try it.
</p>


<h3>Value</h3>

<p>an ftable containing the same data as the tensor
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ftable">ftable</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(U=2,V=2,W=5))
A
dim(A)
names(A)
dimnames(A)

ftable(to.tensor(A))
ftable(to.tensor(c(A),dim(A)))

</code></pre>

<hr>
<h2 id='inv.tensor'>Inversion of a tensor as linear mapping from tensors to tensors</h2><span id='topic+inv.tensor'></span>

<h3>Description</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. This
function computes its (generalized-Moore-Penrose) inverse.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inv.tensor(X,i,...,allowSingular=FALSE,eps=1E-10,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inv.tensor_+3A_x">X</code></td>
<td>
<p>The tensor to be decomposed</p>
</td></tr>
<tr><td><code id="inv.tensor_+3A_i">i</code></td>
<td>
<p>The image dimensions of the linear mapping</p>
</td></tr>
<tr><td><code id="inv.tensor_+3A_allowsingular">allowSingular</code></td>
<td>
<p>A boolean, indicating that a
Moore-Penrose-Inverse should be computed rather than an error
generated in case of a numerically singular mapping.</p>
</td></tr>
<tr><td><code id="inv.tensor_+3A_...">...</code></td>
<td>
<p>further arguments for generic use</p>
</td></tr>
<tr><td><code id="inv.tensor_+3A_eps">eps</code></td>
<td>
<p>The limit for condition-number, to select an generalized inverse.</p>
</td></tr>
<tr><td><code id="inv.tensor_+3A_by">by</code></td>
<td>
<p>the operation is done in parallel for these dimensions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. 
</p>

<dl>
<dt>inv.tensor</dt><dd><p> Computes the inverse of the mapping</p>
</dd>
</dl>



<h3>Value</h3>

<p>a tensor containing the inverse mapping. If allowSingular is given and
the condition number of the matrix is bellow eps a generalized inverse
is returned. 
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>, <code><a href="#topic+solve.tensor">solve.tensor</a></code>, <code><a href="#topic+svd.tensor">svd.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># SVD
# inv.tensor
R1  &lt;- matrix(rnorm(9),nrow=3)
R1i &lt;- solve(R1)
R2 &lt;- to.tensor(R1,c(a=3,b=3),what=1:2)
R2i &lt;- to.tensor(R1i,c(b=3,a=3),what=1:2)

inv.tensor(R2,"a","b") - R2i
inv.tensor(R2,"a","b",allowSingular=TRUE) - R2i

inv.tensor(rep(R2,4,1,"K"),"a","b",by="K") - rep(R2i,4,1,"K")
inv.tensor(rep(R2,4,1,"K"),"a","b",by="K",allowSingular=TRUE) - rep(R2i,4,3,"K")


</code></pre>

<hr>
<h2 id='is.tensor'>Checking for being a tensor</h2><span id='topic+is.tensor'></span>

<h3>Description</h3>

<p>Checks whether the object has a tensor attribute.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.tensor(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.tensor_+3A_x">X</code></td>
<td>
<p>the objected to be checked</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simple convenience function to check for the property of
being a tensor.
</p>


<h3>Value</h3>

<p>boolean 
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- matrix(1:9,nrow=3)
is.tensor(A) # no
A &lt;- to.tensor(A)
is.tensor(A) # yes
</code></pre>

<hr>
<h2 id='level.tensor'>The level (number of indices) of a tensor</h2><span id='topic+level.tensor'></span>

<h3>Description</h3>

<p>The level of a tensor is the number of dimensions or subscripts used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>level.tensor(X,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="level.tensor_+3A_x">X</code></td>
<td>
<p>the tensor to be used</p>
</td></tr>
<tr><td><code id="level.tensor_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The level of the tensor is the length of its dim attribute. Objects
without a dim attribute get level 1 if they are of length &gt; 1 and are
marked as scalars by 0 level otherwise.
</p>


<h3>Value</h3>

<p>the number of levels
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:24,c(a=1,b=2,c=3,d=4))
level.tensor(A)
level.tensor(matrix(1))
level.tensor(1:10)
level.tensor(1)
</code></pre>

<hr>
<h2 id='margin.tensor'>Marginalization of tensors</h2><span id='topic+margin.tensor'></span>

<h3>Description</h3>

<p>The function removes dimensions from a tensor by summing all entries
which only differ in these dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>margin.tensor(X,i=NULL,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="margin.tensor_+3A_x">X</code></td>
<td>
<p>the tensor</p>
</td></tr>
<tr><td><code id="margin.tensor_+3A_i">i</code></td>
<td>
<p>the dimensions to be removed</p>
</td></tr>
<tr><td><code id="margin.tensor_+3A_by">by</code></td>
<td>
<p>instead of i the dimensions to be kept</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a tensor multiplication with the <code class="reqn">1_i</code> tensor.
</p>


<h3>Value</h3>

<p>The tensor with all elements only differing only in the dimensions
specified added up and only the other dimensions left over.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- diag(1:5)
A
margin.tensor(A,1)

A &lt;- to.tensor(1:30,dim=c(i=3,j=5,k=2))
ftable(A)
margin.tensor(A,"j")

</code></pre>

<hr>
<h2 id='mark.tensor'>Marks the names of a tensor with a mark</h2><span id='topic+mark'></span><span id='topic+mark.character'></span><span id='topic+mark.numeric'></span><span id='topic+mark.tensor'></span>

<h3>Description</h3>

<p>This modifies the names of the dimensions in a simple and reversible
way by adding a mark.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mark(X,mark,...)
## S3 method for class 'tensor'
mark(X,mark="'",i=1:level.tensor(X),...,by=NULL)
## S3 method for class 'numeric'
mark(X,mark="'",i=1:length(X),...,by=NULL)
## S3 method for class 'character'
mark(X,mark="'",i=1:length(X),...,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mark.tensor_+3A_x">X</code></td>
<td>
<p>A tensor or dimension to be marked </p>
</td></tr>
<tr><td><code id="mark.tensor_+3A_mark">mark</code></td>
<td>
<p>a character giving the mark</p>
</td></tr>
<tr><td><code id="mark.tensor_+3A_i">i</code></td>
<td>
<p>the dimensions to be marked</p>
</td></tr>
<tr><td><code id="mark.tensor_+3A_...">...</code></td>
<td>
<p>generic arguments</p>
</td></tr>
<tr><td><code id="mark.tensor_+3A_by">by</code></td>
<td>
<p>Dimensions not to be marked. Wins in case of conflicts.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The concept is very important in tensor algebra since it allows to
keep dimensions connected without but still
distinguishable. Eventually later a function for the Riemann summing
rule will make use of marks to distinguish covariate and contravariate
dimensions.
</p>


<h3>Value</h3>

<p>A object similar to X but with marked dimensions.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+delta.tensor">delta.tensor</a></code>, <code><a href="#topic+diag.tensor">diag.tensor</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The outer product
A &lt;- to.tensor(1:4,c(a=2,b=2))
A 
</code></pre>

<hr>
<h2 id='mean.tensor'>Mean and variance of tensors</h2><span id='topic+mean.tensor'></span><span id='topic+var.tensor'></span>

<h3>Description</h3>

<p>Mean and variance of tensors again tensors. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'> ## S3 method for class 'tensor'
mean(x,along,...,na.rm=FALSE)
 ## S3 method for class 'tensor'
var(x,y=NULL,...,along,by=NULL,na.rm=FALSE,mark="'")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean.tensor_+3A_x">x</code></td>
<td>
<p>(set of) dataset(s) of tensors represented by a tensor</p>
</td></tr>
<tr><td><code id="mean.tensor_+3A_y">y</code></td>
<td>
<p>a second dataset of connected tensors represented by a tensor</p>
</td></tr>
<tr><td><code id="mean.tensor_+3A_along">along</code></td>
<td>
<p>the indices indexing the datasets</p>
</td></tr>
<tr><td><code id="mean.tensor_+3A_...">...</code></td>
<td>
<p>here for generic compatibility with the compositions package</p>
</td></tr>
<tr><td><code id="mean.tensor_+3A_by">by</code></td>
<td>
<p>the indices indexing the set of datasets</p>
</td></tr>
<tr><td><code id="mean.tensor_+3A_na.rm">na.rm</code></td>
<td>
<p>a boolean, if FALSE and missings are in the dataset a
error is given. If TRUE pairwise exclusion is used.</p>
</td></tr>
<tr><td><code id="mean.tensor_+3A_mark">mark</code></td>
<td>
<p>the to mark the second instance of indices in var(x,...)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let denote <code class="reqn">a</code> the along dimension,
<code class="reqn">i_1,\ldots,i_k</code> and <code class="reqn">j_1,\ldots,j_l</code>
the data dimension, and
b the by dimension, then the mean is given by:
</p>
<p style="text-align: center;"><code class="reqn">M^x_{bi_1,\ldots,i_k}=\frac1{n}\sum_{a}x_{abi_1,\ldots,i_k}</code>
</p>

<p>the covariance by
</p>
<p style="text-align: center;"><code class="reqn">C_{ab i_1,\ldots,i_kj_1,\ldots,j_l}=\frac1{n-1}\sum_{a}
    (x_{abi_1,\ldots,i_k}-M^x_{bi_1,\ldots,i_k})(y_{abj_1,\ldots,j_l}-M^y_{bj_1,\ldots,j_l})</code>
</p>

<p>and the variance by
</p>
<p style="text-align: center;"><code class="reqn">V_{ab i_1,\ldots,i_ki'_1,\ldots,i'_l}=\frac1{n-1}\sum_{a}
    (x_{abi_1,\ldots,i_k}-M^x_{bi_1,\ldots,i_k})(x_{abi'_1,\ldots,i'_k}-M^x_{bi'_1,\ldots,i'_l})</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>mean</code></td>
<td>
<p>gives a tensor like x without the along dimensions
representing the a mean over all tensors in the dataset. It is
not necessary to have a by dimension since everything not in along
is automatically treated parallel</p>
</td></tr>
<tr><td><code>var(x</code>, <code>...)</code></td>
<td>
<p>Gives the covariate tensor representing the covariance
of x and y. The data tensor indices of x any y should be different,
since otherwise duplicated names exist in the result.</p>
</td></tr>
<tr><td><code>var(x</code>, <code>...)</code></td>
<td>
<p>Gives the covariate representation of the variance
of x. All data indices (i.e. all indices neither in by nor in along
are duplicated. One with and one without the given mark.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>K.Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><a href="#topic+tensorA">tensorA</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> d1 &lt;- c(a=2,b=2)
 d2 &lt;- c("a'"=2,"b'"=2)
 # a mean tensor:
 m &lt;- to.tensor(1:4,d1)             
 # a positive definite variance tensor:
 V &lt;- delta.tensor(d1)+one.tensor(c(d1,d2))
 V
 # Simulate Normally distributed tensors with these moments:
 X &lt;- (power.tensor(V,c("a","b"),c("a'","b'"),p=1/2)  %e%
      to.tensor(rnorm(1000*2*2),c(i=1000,d2))) + m
 # The mean
 mean.tensor(X,along="i")
 # Full tensorial covariance:
 var.tensor(X,along="i")
 # Variance of the slices  X[[b=1]] and X[[b=2]] :
 var.tensor(X,along="i",by="b")
 # Covariance of the slices X[[b=1]] and X[[b=2]] :
 var.tensor(X[[b=1]],X[[a=~"a'",b=2]],along="i")

</code></pre>

<hr>
<h2 id='mul.tensor'>Tensor multiplication for the tensor class</h2><span id='topic+mul.tensor'></span>

<h3>Description</h3>

<p>Performs a tensor multiplication like tensor(), but with named indices,
keeping dimnames, and vectorized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mul.tensor(X,i=c(),Y,j=i,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mul.tensor_+3A_x">X</code></td>
<td>
<p>a tensor to be multiplied</p>
</td></tr>
<tr><td><code id="mul.tensor_+3A_i">i</code></td>
<td>
<p>numeric or character vector specifying the dimension to be
used in the multiplication for X</p>
</td></tr>
<tr><td><code id="mul.tensor_+3A_y">Y</code></td>
<td>
<p>a tensor to be multiplied</p>
</td></tr>
<tr><td><code id="mul.tensor_+3A_j">j</code></td>
<td>
<p>numeric or character vector specifying the dimension to be
used in the multiplication for Y</p>
</td></tr>
<tr><td><code id="mul.tensor_+3A_by">by</code></td>
<td>
<p>the by dimensions if present and not mentioned in i or j are
used as sequence dimensions. tensors in these dimensions are
processed in parallel. So in this dimension the product is neither
inner nor outer but parallel like <code>a*b</code>, rather than
<code>a%*%b</code> or <code>a%o%b</code>. Unmentioned dimensions get an
outer product. Mentioned dimensions an inner. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Say </p>
<p style="text-align: center;"><code class="reqn">X_{i_1\ldots i_n h_1 \ldots h_l}</code>
</p>

<p>and </p>
<p style="text-align: center;"><code class="reqn">Y_{j_1\ldots j_n k_1 \ldots k_m}</code>
</p>

<p>the the result is:
</p>
<p style="text-align: center;"><code class="reqn">E_{h_1\ldots h_l k_1 \ldots k_m}= \sum_{i_1,\ldots,i_n} X_{i_1\ldots i_n h_1 \ldots h_l}Y_{j_1\ldots j_n k_1 \ldots k_m}</code>
</p>

<p>This is an full outer product with i,j not given and a full inner product
product of i=dim(X)
</p>


<h3>Value</h3>

<p>The tensor product of X and Y with respect to the regarding
dimensions. 
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>,  <code><a href="#topic++25e+25">%e%</a></code>,
<code><a href="#topic++25r+25">%r%</a></code>, <code><a href="#topic+diagmul.tensor">diagmul.tensor</a></code>,
<code><a href="#topic+einstein.tensor">einstein.tensor</a></code>, <code><a href="#topic+riemann.tensor">riemann.tensor</a></code>,
<code><a href="#topic+solve.tensor">solve.tensor</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(A=2,B=2,C=5))
B &lt;- to.tensor(1:20,c(D=2,B=2,E=5))
mul.tensor(A,"A",A,"B")


</code></pre>

<hr>
<h2 id='names.tensor'>Getting and setting index and dimensionnames of a tensor</h2><span id='topic+names.tensor'></span><span id='topic+names+3C-.tensor'></span><span id='topic+dimnames+3C-.tensor'></span><span id='topic+dimnames.tensor'></span><span id='topic+dim+3C-.tensor'></span>

<h3>Description</h3>

<p>The names of a tensor are the names of its dimension 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tensor'
names(x)
## S3 replacement method for class 'tensor'
names(x) &lt;- value
## S3 method for class 'tensor'
dimnames(x) 
## S3 replacement method for class 'tensor'
dimnames(x) &lt;- value
## S3 replacement method for class 'tensor'
dim(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="names.tensor_+3A_x">x</code></td>
<td>
<p>a tensor object</p>
</td></tr>
<tr><td><code id="names.tensor_+3A_value">value</code></td>
<td>
<p>The new value. If this is a named list it replaces the
names of the dimensions. If its an unnamed list it gets the names of
the dimensions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The names of the dimensions of the tensor are very relevant in any
tensor arithmetic since they are the principle way to specify the
dimensions to be involved in an operation. The dimnames function is
here only for convenice to guarantee that the names of the dimnames are
always the same as the names of the dimensions and to ensure that
always at least a list with the right length and names. 
</p>


<h3>Value</h3>

<p>the names of the dimensions the tensor
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+mul.tensor">mul.tensor</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(U=2,V=2,W=5))
A
dim(A)
names(A)
names(A) &lt;- c("A","B","C")
A
dim(A)
names(A)

</code></pre>

<hr>
<h2 id='norm.tensor'>Calculate the Euclidean norm or Euclidean operator norm of a tensor or its subtensors</h2><span id='topic+norm'></span><span id='topic+norm.tensor'></span><span id='topic+opnorm'></span><span id='topic+opnorm.tensor'></span>

<h3>Description</h3>

<p>Calculates the Euclidean norm of a tensor or its subtensors. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>norm(X,...)
## S3 method for class 'tensor'
norm(X,i=NULL,...,by=NULL)
opnorm(X,...)
## S3 method for class 'tensor'
opnorm(X,i=NULL,...,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="norm.tensor_+3A_x">X</code></td>
<td>
<p>The tensor</p>
</td></tr>
<tr><td><code id="norm.tensor_+3A_i">i</code></td>
<td>
<p>For norm the dimensions to of the subtensors to be used. If missing
the norm of the whole tensor is computed. For opnorm the dimensions
of the image.</p>
</td></tr>
<tr><td><code id="norm.tensor_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
<tr><td><code id="norm.tensor_+3A_by">by</code></td>
<td>
<p>the list dimension, if i is not specified the norm is
calculated for each of these
in parallel.</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt>norm</dt><dd>
<p>The function computes the Euclidean norm, which is the square root
over the sum of all entries and not the operator norm. </p>
</dd>
<dt>opnorm</dt><dd>
<p>The function computes the Euclidean operator norm, which is largest
factor in changing the Euclidean norm, when mapped with the linear
mapping corresponding to the tensor. </p>
</dd>
</dl>



<h3>Value</h3>

<table>
<tr><td><code>norm</code></td>
<td>
<p>either a single number giving the norm of the tensor or a tensors with
the dimensions i removed
containing the individual norms in each entry.</p>
</td></tr>
<tr><td><code>opnorm</code></td>
<td>
<p>a tensor of dimension <code>dim(X)[by]</code> giving the
Euclidean operator norm of the tensor (i.e. its largest singular
value)
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>C &lt;- to.tensor(1:20,c(A=4,B=5))
norm(C,"A")
norm(C,2)
norm(C,c("A","B"))
opnorm(C,"A")
</code></pre>

<hr>
<h2 id='one.tensor'>Creates a tensor with all entries 1</h2><span id='topic+one.tensor'></span>

<h3>Description</h3>

<p>Creates a tensor with all entries one. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>one.tensor(d=NULL,dn=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="one.tensor_+3A_d">d</code></td>
<td>
<p>the dimensions of the new tensor</p>
</td></tr>
<tr><td><code id="one.tensor_+3A_dn">dn</code></td>
<td>
<p>the dimnames of the new tensor</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">E_{i_1\ldots i_n}=1</code>
</p>



<h3>Value</h3>

<p>A tensor with dim d and all elements one
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>one.tensor(c(a=3,b=3,c=3))
</code></pre>

<hr>
<h2 id='pos.tensor'>enumeration of index combinations</h2><span id='topic+pos.tensor'></span>

<h3>Description</h3>

<p>This gives all combinations of indices of a tensor with dimension d in
the order of the numbers in the memory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pos.tensor(d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pos.tensor_+3A_d">d</code></td>
<td>
<p>a dim attribute of a tensor</p>
</td></tr>
</table>


<h3>Details</h3>

<p>tensors are stored according to the R-convention that the leftmost
index varies fastest.
</p>


<h3>Value</h3>

<p>a matrix with the same number of rows as the tensor has entries an the
same number of columns as the tensor has dimensions. Each row
represents the index combination of a the corresponding element.
</p>


<h3>Author(s)</h3>

<p>K.Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+reorder.tensor">reorder.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   (A &lt;- to.tensor(1:20,dim=c(A=2,B=2,C=5)))
   pos.tensor(dim(A))
</code></pre>

<hr>
<h2 id='power.tensor'>Compute the power of a symmetric tensor</h2><span id='topic+power.tensor'></span>

<h3>Description</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. If
domain and image are the same and the tensor is definite, we can define
powers. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.tensor(X,i,j,p=0.5,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="power.tensor_+3A_x">X</code></td>
<td>
<p>The tensor to be decomposed</p>
</td></tr>
<tr><td><code id="power.tensor_+3A_i">i</code></td>
<td>
<p>The image dimensions of the linear mapping</p>
</td></tr>
<tr><td><code id="power.tensor_+3A_j">j</code></td>
<td>
<p>The domain dimensions of the linear mapping</p>
</td></tr>
<tr><td><code id="power.tensor_+3A_p">p</code></td>
<td>
<p>the power of the tensor to be computed</p>
</td></tr>
<tr><td><code id="power.tensor_+3A_by">by</code></td>
<td>
<p>the operation is done in parallel for these dimensions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. Let
denote <code class="reqn">R_i</code> the space of real tensors with dimensions
<code class="reqn">i_1...i_d</code>.
<br />
To compute a power <code>dim(X)[i]</code> and <code>dim(X)[j]</code> need to be
equal and the tensor symmetric between these dimension. Some exponents
are only valid with positive definite mappings. None of these
conditions is checked. 
</p>


<h3>Value</h3>

<p>a tensor
</p>


<h3>Note</h3>

<p>symmetry of the matrix is assumed but not checked.</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+svd.tensor">svd.tensor</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(rnorm(120),c(a=2,b=2,c=5,d=3,e=2))
AAt &lt;- A %e% mark(A,"'",c("a","b"))
AAt

power.tensor(AAt,c("a","b"),c("a'","b'"),-1)

inv.tensor(AAt,c("a","b"))

power.tensor(AAt,c("a","b"),c("a'","b'"),2)
mul.tensor(AAt,c("a","b"),AAt,c("a'","b'"))

power.tensor(power.tensor(AAt,c("a","b"),c("a'","b'"),1/pi),
                     c("a","b"),c("a'","b'"),pi)


AAt &lt;- einstein.tensor(A , mark(A,"'",c("a","b")),by="e")

power.tensor(AAt,c("a","b"),c("a'","b'"),-1,by="e")

inv.tensor(AAt,c("a","b"),by="e")

power.tensor(AAt,c("a","b"),c("a'","b'"),2,by="e")
mul.tensor(AAt,c("a","b"),AAt,c("a'","b'"),by="e")

power.tensor(power.tensor(AAt,c("a","b"),c("a'","b'"),1/pi,by="e"),
c("a","b"),c("a'","b'"),pi,by="e")


</code></pre>

<hr>
<h2 id='reorder.tensor'>Permutation of indices and storage sequence of a tensor</h2><span id='topic+reorder.tensor'></span>

<h3>Description</h3>

<p>This permutes tensor dimensions like aperm. However the interface is
more flexible since not all dimensions have to given and names can be
used instead of numbers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tensor'
reorder(x,i=NULL,...,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reorder.tensor_+3A_x">x</code></td>
<td>
<p>the tensor</p>
</td></tr>
<tr><td><code id="reorder.tensor_+3A_i">i</code></td>
<td>
<p>numeric or character giving dimensions intended to come
first</p>
</td></tr>
<tr><td><code id="reorder.tensor_+3A_...">...</code></td>
<td>
<p>further arguments to other instances of the generic function</p>
</td></tr>
<tr><td><code id="reorder.tensor_+3A_by">by</code></td>
<td>
<p>the complement of i, if i is not given</p>
</td></tr>
</table>


<h3>Details</h3>

<p>the remaining dimensions keep their relative sequence and follow at
the end of the dimension attribute.
</p>


<h3>Value</h3>

<p><code>reorder.tensor</code> returns a tensor equal to x but stored with a different
sequence of dimensions.



</p>


<h3>Author(s)</h3>

<p>K.Gerald v.d. Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(A=2,B=2,C=5))
A
reorder(A,"C")
reorder(A,"B")
</code></pre>

<hr>
<h2 id='reptensor'>Repeats a tensor</h2><span id='topic+rep.tensor'></span>

<h3>Description</h3>

<p>The tensor is repeated like a number is repeated by rep and an
additional dimension is added to select the different tensors. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tensor'
rep(x,times,pos=1,name="i",...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reptensor_+3A_x">x</code></td>
<td>
<p>the tensor to be repeated</p>
</td></tr>
<tr><td><code id="reptensor_+3A_times">times</code></td>
<td>
<p>the number of copies that should be created. If <code>times</code> is
a vector, x is seen as a sequence of tensors in dimension <code>pos</code> and
each of the tensors is repeated according to the corresponding
entry of times. </p>
</td></tr>
<tr><td><code id="reptensor_+3A_name">name</code></td>
<td>
<p>the name of the additional dimension. if NA no additional
dimension is used.</p>
</td></tr>
<tr><td><code id="reptensor_+3A_pos">pos</code></td>
<td>
<p>the position where the extra dimension should be added</p>
</td></tr>
<tr><td><code id="reptensor_+3A_...">...</code></td>
<td>
<p>not used, only here for generic consistency</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is modeled as much as possible to mimic rep, by
repeating tensors rather than numbers. The
<code>each</code> argument is not necessary, since sequence of the dimensions
can more precisely be controlled by pos. Another problem is the a
ambiguity between <code>rep(x,3)</code> and <code>rep(x,c(3))</code> as a special
case of <code>rep(x,c(3,2))</code>. If the second is wanted it can be forced by
<code>rep(x,c(3),NA)</code> through setting the name argument to NA. 
</p>


<h3>Value</h3>

<p>A tensor with one  additional dimensions of length times. 
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+rep">rep</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:4,c(A=2,B=2))
rep(A,3)
rep(A,3,3,"u")
rep(A,c(2,3))
A &lt;- to.tensor(1:4,c(A=1,B=4))
rep(A,5,pos="A",name=NA)
</code></pre>

<hr>
<h2 id='riemann.tensor'>Tensor multiplication with Riemann's convention</h2><span id='topic+riemann.tensor'></span><span id='topic++25r+25'></span><span id='topic++25r+25.tensor'></span>

<h3>Description</h3>

<p>Multiplies tensors by multiplying over all pairs with one covariate
and one contravariate variable with the same name according
to Riemann's summing convention.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>riemann.tensor(...,only=NULL,by=NULL)
## Methods for class tensor
# x %r% y
## Default method
# x %r% y
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="riemann.tensor_+3A_...">...</code></td>
<td>
<p>some tensors, or a renaming code</p>
</td></tr>
<tr><td><code id="riemann.tensor_+3A_only">only</code></td>
<td>
<p>an optional list of the dimension names to be recognized
for duplication to allow parallel processing on lists of tensors</p>
</td></tr>
<tr><td><code id="riemann.tensor_+3A_x">x</code></td>
<td>
<p>a tensor</p>
</td></tr>
<tr><td><code id="riemann.tensor_+3A_y">y</code></td>
<td>
<p>a tensor</p>
</td></tr>
<tr><td><code id="riemann.tensor_+3A_by">by</code></td>
<td>
<p>Riemannian summing is done in parallel in these dimensions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>see <code><a href="#topic+mul.tensor">mul.tensor</a></code> on details on tensor
multiplication. In <code>einstein.tensor</code> complex operations can be
performed by command and renaming code: The arguments are processed
from left to right and multiplied. Unnamed attributes are regarded as
tensors or scalars and
multiplied with the current result by the Riemann summing convention,
which means an inner product over all pairs of covariate and
contravariate indices with the same
name. Named attributes can either have the name <code>diag</code>, which performs a
<code>diagmul</code> according to the same-name convention or be of the form
<code>A="B"</code> or <code>"A"="B"</code>, for which we have two cases. Typically
both are given covariate. The first specifies the covariate to be used
in the multiplication and the second the contravariate.
If both
names are
present in the current result, an inner multiplication (trace) of on
these two dimensions is
performed. If only the covariate or the contravariate is present up to
this point, the specific
dimension is renamed to the second name, but keeps its type. This
renaming might be
visible in the result or inducing a multiplication according to the
Riemann convention later if the other shows up. 
</p>


<h3>Value</h3>

<p>the tensor product of all the tensors along all duplicate dimensions.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+mul.tensor">mul.tensor</a></code>, <code><a href="#topic+to.tensor">to.tensor</a></code>, <code><a href="#topic+riemann.tensor">riemann.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(U=2,"^V"=2,W=5))
B &lt;- to.tensor(1:20,c("^U"=2,V=2,Q=5))
riemann.tensor(A,B)
A %r% B 

</code></pre>

<hr>
<h2 id='sequencing'>Working with index sequences</h2><span id='topic++24.tensor'></span><span id='topic++5E.tensor'></span><span id='topic++7C.tensor'></span><span id='topic+renamefirst.tensor'></span>

<h3>Description</h3>

<p>In typical tensor notation the indices are not identified by names but
by positions. The operators allow to identify names and positions
transparently during calculation. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Methods for class tensor
# x $ y
# x ^ y
# x | y
renamefirst.tensor(x,y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sequencing_+3A_x">x</code></td>
<td>
<p>A tensor </p>
</td></tr>
<tr><td><code id="sequencing_+3A_y">y</code></td>
<td>

<p>Typically a character vector specifying a sequence of names for the
tensor. The names can be specified in various ways:<br />
The following specifications are equal and specify a sequence of the
names i,j and k:<br />
<code>x$ijk</code>, <code>x$i.j.k</code>, <code>i.j.k.</code>, <code>x"$ijk"</code>,
<code>x^"i.j.k"</code>, <code>x^c("i","j","k")</code>,<code>x^c("i.j","k")</code>,
<code>x^c("$i.j","k")</code>,<code>x^c("$ij","k")</code>,
<code>x^c("$","ijk")</code><br />
In general names are separated by dots.
All notations with <code>\$</code> either as operator or as the first
character of the first string allow to omit the dots assuming that
all names are single character. If any dot is present all dots must
be given. The difference of <code>\$</code> and <code>\^</code> is that the
first accepts a name and the second an character valued expression.
<br />
Multi letter indices like &quot;alpha&quot;,&quot;beta&quot;,&quot;gamma&quot;
can only be given in the dot-free version of the notation
making the following
specifications equal:
<code>x$alpha.beta.gamma</code>, <code>alpha.beta.gamma.</code>,
<code>x^"$alpha.beta.gamma"</code>,
<code>x^"alpha.beta.gamma"</code>, <code>x^c("alpha","beta","gamma")</code>,
<code>x^c("alpha.beta","gamma")</code>,
<code>x^c("$alpha.beta","k")</code>,
<code>x^c("$","alpha.beta.gammak")</code><br />
The specification for <code>|</code> is equal to that for <code>^</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are used to mimic the mathematical notation in tensor
analysis.
Formulae of the form (with Einstein convention):
</p>
<p style="text-align: center;"><code class="reqn">E_{ijk}= A_{ihl}C_{hj}C_{lk}</code>
</p>

<p>with defined tensors <code class="reqn">A_{ijk}</code> and <code class="reqn">C_{ij}</code> can
be given the
simple
form <br />
<code> E &lt;- A$ihl %e% C$hj %e% C$lk |"$ijk"</code><br />
or alternatively for multi letter names:<br />
<code> E &lt;- A$i.h.l %e% C$h.j %e% C$l.k |"i.j.k"</code><br />
or more flexible in computation with arguments I,J,K:<br />
<code> E &lt;- A^c(I,"h.l") %e% C^c("h",J) %e% C^c("l",K) | c(I,J,K)</code><br />
The <code>$</code> or <code>^</code> binds to the tensors with high precedence
and renames the first elements. The <code>|</code> binds with very low
precedence and reorders the tensor according to the
assumed index sequence of the result afterwards.
</p>


<h3>Value</h3>

<p>A tensor of the same shape as x but with reordered dimensions (for
<code>|</code>) or renamed dimensions (for the others) 
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+reorder.tensor">reorder.tensor</a></code>, <code><a href="#topic+names+3C-.tensor">names&lt;-.tensor</a></code>, <code><a href="#topic++5B+5B.tensor">[[.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(i=5,j=2,k=2))
C &lt;- to.tensor(1:4,c(i=2,j=2))
E &lt;- A$ihl %e% C$hj %e% C$lk |"$ijk"
E
# Same as:
E2 &lt;- reorder.tensor(A[[j=~h,k=~l]] %e% C[[i=~h]] %e% C[[i=~l,j=~k]],c("i","j","k"))
E-E2
E &lt;- A$i.h.l %e% C$h.j %e% C$l.k |"i.j.k"
E
E-E2
E &lt;- A^"i.h.l" %e% C^"h.j" %e% C^"l.k" |"i.j.k"
E
E-E2
</code></pre>

<hr>
<h2 id='slice.tensor'>Working with the indices of a tensor (accessing, slicing, renaming, ...)</h2><span id='topic+slice.tensor'></span><span id='topic++5B+5B.tensor'></span><span id='topic++5B.tensor'></span><span id='topic++5B+5B+3C-.tensor'></span><span id='topic++5B+3C-.tensor'></span>

<h3>Description</h3>

<p>Indexing of tensors allows beside the ordinary selection of ranges
of indices the renaming of indices. The functions are mainly here to
keep the the tensor property of the results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slice.tensor(X,i,what,drop=FALSE)
## Methods for class tensor
# X[...,drop=TRUE]
# X[...,drop=TRUE] &lt;- value
# X[[...,drop=TRUE]]
# X[[...,drop=TRUE]] &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slice.tensor_+3A_x">X</code></td>
<td>
<p>A tensor</p>
</td></tr>
<tr><td><code id="slice.tensor_+3A_i">i</code></td>
<td>
<p>an index given as number or character</p>
</td></tr>
<tr><td><code id="slice.tensor_+3A_what">what</code></td>
<td>
<p>levels of the index, a number or a character from
dimnames</p>
</td></tr>
<tr><td><code id="slice.tensor_+3A_drop">drop</code></td>
<td>
<p>a boolean, if true, indices with only a single level are
removed</p>
</td></tr>
<tr><td><code id="slice.tensor_+3A_...">...</code></td>
<td>
<p>arguments of the form <code>name=</code>indices, and for the
<code>[[ ]]</code> functions it also allowed to give names from the
corresponding dimnames <code>name=c("a","b")</code> to select indices by
names or <code>name=~newname</code> to rename dimensions, the first use
makes a usual array access in the given dimension, where <code>[[ ]]</code>
only supports a single index, while <code>[ ]</code> allows vectors. The
other type changes the names.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>The functions allow to rename dimensions and to take select a part of
the tensor.
</p>


<h3>Value</h3>

<p>a new tensor with dimensions renamed or individual levels selected
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+einstein.tensor">einstein.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(A=2,B=2,C=5))
A[C=1]
A[C=1:3]
A[[B=~b]]                  # renaming dimensions
A[[B=~b,A=~aaa]]      
A[[B=~b,A=~aaa,aaa=1]]      
A[[A=1,B=~gamma]][C=1:2]
A 
</code></pre>

<hr>
<h2 id='solve.tensor'>Solving linear equations with tensors</h2><span id='topic+solve.tensor'></span>

<h3>Description</h3>

<p>We can formulate linear equation systems with tensors. This functions
solves these systems or gives a least squares fit of minimal
norm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> ## S3 method for class 'tensor'
solve(a,b,i,j=i,...,allowSingular=FALSE,eps=1E-10,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve.tensor_+3A_a">a</code></td>
<td>
<p>The a of ax=b</p>
</td></tr>
<tr><td><code id="solve.tensor_+3A_b">b</code></td>
<td>
<p>The a of ax=b</p>
</td></tr>
<tr><td><code id="solve.tensor_+3A_i">i</code></td>
<td>
<p>The dimensions of the equation in a</p>
</td></tr>
<tr><td><code id="solve.tensor_+3A_j">j</code></td>
<td>
<p>The dimensions of the equation in b</p>
</td></tr>
<tr><td><code id="solve.tensor_+3A_allowsingular">allowSingular</code></td>
<td>
<p>A boolean, indicating the that a least squares
fit should be generated with singular equations systems.</p>
</td></tr>
<tr><td><code id="solve.tensor_+3A_...">...</code></td>
<td>
<p>further arguments for generic use</p>
</td></tr>
<tr><td><code id="solve.tensor_+3A_eps">eps</code></td>
<td>
<p>The limit for the smallest singular value in inversion</p>
</td></tr>
<tr><td><code id="solve.tensor_+3A_by">by</code></td>
<td>
<p>the operation is done in parallel for these dimensions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. Let
denote <code class="reqn">R_i</code> the space of real tensors with dimensions
<code class="reqn">i_1...i_d</code>.
</p>

<dl>
<dt>solve.tensor</dt><dd><p>Solves the equation for
<code class="reqn">a_{i_1...i_dk_1...k_p}</code>, <code class="reqn">b_{j_1...j_dl_1...l_q}</code> and
<code class="reqn">x_{k_1...k_pl_1...l_q}</code> the equation  
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{k_1,...,k_p}
    a_{i_1...i_dk_1...k_p}x_{k_1...k_pl_1...l_q}=
    b_{j_1...j_dl_1...l_q}</code>
</p>
<p>.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>a tensor such that ax=b as good as possible for each combination of by
values.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>, <code><a href="#topic+svd.tensor">svd.tensor</a></code>,
<code><a href="#topic+inv.tensor">inv.tensor</a></code>, <code><a href="#topic+chol.tensor">chol.tensor</a></code>,
<code><a href="#topic+power.tensor">power.tensor</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>R1  &lt;- matrix(rnorm(9),nrow=3)
R1i &lt;- solve(R1)
R2 &lt;- to.tensor(R1,c(a=3,b=3),what=1:2)
R2i &lt;- to.tensor(R1i,c(b=3,a=3),what=1:2)

inv.tensor(R2,"a","b") - R2i
inv.tensor(R2,"a","b",allowSingular=TRUE) - R2i

inv.tensor(rep(R2,4,1,"K"),"a","b",by="K") - rep(R2i,4,1,"K")
inv.tensor(rep(R2,4,1,"K"),"a","b",by="K",allowSingular=TRUE) - rep(R2i,4,3,"K")

R3 &lt;- to.tensor(rnorm(15),c(a=3,z=5))

mul.tensor(R2i,"b",mul.tensor(R2,"a",R3)) # R3

solve.tensor(R2i,R3[[z=1]],"a")
mul.tensor(R2,"a",R3[[z=1]])

solve.tensor(R2i,R3,"a")
mul.tensor(R2,"a",R3)

solve.tensor(R2i,R3[[z=1]],"a",allowSingular=TRUE)
mul.tensor(R2,"a",R3[[z=1]])

solve.tensor(R2i,R3,"a",allowSingular=TRUE)
mul.tensor(R2,"a",R3)

solve.tensor(rep(R2i,4,1,"K"),R3[[z=1]],"a",by="K")
rep(mul.tensor(R2,"a",R3[[z=1]]),4,1,"K")

solve.tensor(rep(R2i,4,1,"K"),rep(R3[[z=1]],4,1,"K"),"a",by="K")
rep(mul.tensor(R2,"a",R3[[z=1]]),4,1,"K")

</code></pre>

<hr>
<h2 id='svd.tensor'>Singular value decomposition of tensors</h2><span id='topic+svd.tensor'></span>

<h3>Description</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. This
function computes the singular value decomposition of this mapping
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svd.tensor(X,i,j=NULL,...,name="lambda",by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svd.tensor_+3A_x">X</code></td>
<td>
<p>The tensor to be decomposed</p>
</td></tr>
<tr><td><code id="svd.tensor_+3A_i">i</code></td>
<td>
<p>The image dimensions of the linear mapping</p>
</td></tr>
<tr><td><code id="svd.tensor_+3A_j">j</code></td>
<td>
<p>The coimage dimensions of the linear mapping</p>
</td></tr>
<tr><td><code id="svd.tensor_+3A_name">name</code></td>
<td>
<p>The name of the eigenspace dimension. This is the
dimension created by the decompositions, in which the eigenvectors
are <code class="reqn">e_i</code></p>
</td></tr>
<tr><td><code id="svd.tensor_+3A_...">...</code></td>
<td>
<p>further arguments for generic use</p>
</td></tr>
<tr><td><code id="svd.tensor_+3A_by">by</code></td>
<td>
<p>the operation is done in parallel for these dimensions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. Let
denote <code class="reqn">R_i</code> the space of real tensors with dimensions <code class="reqn">i_1...i_d</code>.
</p>

<dl>
<dt>svd.tensor</dt><dd><p>Computes a singular value decomposition
<code class="reqn">u_{i_1...i_d\lambda{}}</code>,<code class="reqn">d_\lambda{}</code>, <code class="reqn">v_{j_1...j_l}\lambda{}</code> such
that u and v correspond to orthogonal mappings from <code class="reqn">R_\lambda{}</code> to
<code class="reqn">R_i</code> or <code class="reqn">R_j</code> respectively.</p>
</dd>
</dl>



<h3>Value</h3>

<p>a tensor or in case of svd a list u,d,v, of tensors like in <code><a href="base.html#topic+svd">svd</a></code>.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>, <code><a href="#topic+to.matrix.tensor">to.matrix.tensor</a></code>,
<code><a href="#topic+inv.tensor">inv.tensor</a></code>, <code><a href="#topic+solve.tensor">solve.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># SVD
A &lt;- to.tensor(rnorm(120),c(a=2,b=2,c=5,d=3,e=2))

SVD &lt;- svd.tensor(A,c("a","d"),c("b","c"),by="e")
dim(SVD$v)
# Property of decomposition
einstein.tensor(SVD$v,diag=SVD$d,SVD$u,by="e") # A
# Property of orthogonality
SVD$v %e% SVD$v[[lambda=~"lambda'"]]         # 2*delta.tensor(c(lambda=6))
SVD$u %e% SVD$u[[lambda=~"lambda'"]]         # 2*delta.tensor(c(lambda=6)))
SVD$u %e% mark(SVD$u,"'",c("a","d"))  # 2*delta.tensor(c(a=2,d=3)))



</code></pre>

<hr>
<h2 id='to.matrix.tensor'>The matrix corresponding to a tensor seen as a linear mapping of tensors.</h2><span id='topic+to.matrix.tensor'></span>

<h3>Description</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. This
function gives the corresponding matrix of the mapping. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to.matrix.tensor(X,i,j,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="to.matrix.tensor_+3A_x">X</code></td>
<td>
<p>The tensor </p>
</td></tr>
<tr><td><code id="to.matrix.tensor_+3A_i">i</code></td>
<td>
<p>The image indices of the linear mapping</p>
</td></tr>
<tr><td><code id="to.matrix.tensor_+3A_j">j</code></td>
<td>
<p>The domain indices of the linear mapping</p>
</td></tr>
<tr><td><code id="to.matrix.tensor_+3A_by">by</code></td>
<td>
<p>the operation is done in parallel for these dimensions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A tensor can be seen as a linear mapping of a tensor to a tensor. This
function computes the corresponding matrix, mapping the entries of the
domain tensor to the entries of the image tensor.
</p>


<h3>Value</h3>

<p>if no <code>by</code> is given a matrix. Otherwise a tensor of level
<code>2+length(dim(X))[by]</code> 
giving matrices for each specification of the by dimensions.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>, <code><a href="#topic+solve.tensor">solve.tensor</a></code>,
<code><a href="#topic+inv.tensor">inv.tensor</a></code>, <code><a href="#topic+svd.tensor">svd.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- reorder.tensor(to.tensor(1:30,c(a=2,b=3,c=5)),c("c","a","b"))

to.matrix.tensor(A,"a",c("b","c"))              # matrix(1:30,nrow=2)

to.matrix.tensor(A,c("a","b"),c("c"))           # matrix(1:30,nrow=6)

to.matrix.tensor(A,c("a","b"),by=c("c")) # structure(1:30,dim=c(6,1,5)))
to.matrix.tensor(A,c("a"),by=c("c"))     # structure(1:30,dim=c(2,3,5)))
</code></pre>

<hr>
<h2 id='to.tensor'>Creates a tensor object</h2><span id='topic+to.tensor'></span><span id='topic+to.tensor.default'></span>

<h3>Description</h3>

<p>Constructs a &quot;tensor&quot;. 
A tensor is the generalization of vectors and matrices to multi-index
arrays. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to.tensor(X,...)
## Default S3 method:
to.tensor(X,dims=NULL,ndimnames=NULL,what=1,addIndex=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="to.tensor_+3A_x">X</code></td>
<td>
<p>the numeric data with the entries of the tensor. If the
object is already a tensor only the subtensors given by the dimension
<code>what</code> are converted </p>
</td></tr>
<tr><td><code id="to.tensor_+3A_dims">dims</code></td>
<td>
<p>These dimensions to be added for the new tensor.
If the object is to big or <code>addIndex</code> an extra dimension is
added. </p>
</td></tr>
<tr><td><code id="to.tensor_+3A_ndimnames">ndimnames</code></td>
<td>
<p>The new dimnames to be used</p>
</td></tr>
<tr><td><code id="to.tensor_+3A_what">what</code></td>
<td>
<p>a numeric or character vector specifying the dimensions
to be removed.</p>
</td></tr>
<tr><td><code id="to.tensor_+3A_addindex">addIndex</code></td>
<td>
<p>boolean or character, FALSE says  no additional
dimension, or string to give the name of the dimension</p>
</td></tr>
<tr><td><code id="to.tensor_+3A_...">...</code></td>
<td>
<p>further arguments to other instances of the generic function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This package provides a class <code>"tensor"</code>
allowing easy computation regarding tensorial computation in the
Einstein convention and allows an easier control of the computation
than aperm and tensor. The package is made to work with things like
matrices of matrices and linear mapping of matrices to matrices, etc.
</p>
<p>A tensor is a multidimensional array, with specific mathematical
meaning, generalizing vectors and matrices. Tensors can be added,
subtracted and
multiplied and used in linear equations. While two matrices A,B are
commonly only multiplied in two ways <code>A%*%B</code> or <code>B%*%A</code>
and have some more <code>t(A)%*%B</code>,
<code>B%*%t(A)</code>, <code>sum(A*B)</code>,
<code>sum(A*t(B))</code>,<code>kronecker(A,B)</code>
the tensor calculus brings all of them into a organized system. 
<br />
An important aspect for that is  the name of its dimensions. Thus we
are not bound to work with rows and columns, but can name the
dimensions to be multiplied. This leads to much more organized
computation of linear mappings of matrices or datasets of matrices or
other genuine tensor arithmetic gets involved.
<br />
The package provides a full linear algebra support of tensors
including tensor addition, tensor multiplication, norms, deltatensors,
binding,
inversion, normalization,
Einstein summing
convention, trace, , dimension renaming, smart display of tensors,
renaming and reshaping, solving
equation system and giving decompositions  and parallelized data
processing ,
</p>


<h3>Value</h3>

<p>a tensor of the specified shape
</p>


<h3>Note</h3>

<p>This constructor is not called tensor() according to the general
convention of constructors to avoid conflicts with the tensor
multiplication routine
in the <code>tensor</code> package
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><a href="#topic+tensorA">tensorA</a>,  <code><a href="#topic+level.tensor">level.tensor</a></code>,
<code><a href="#topic+diag.tensor">diag.tensor</a></code>, <code><a href="#topic+norm.tensor">norm.tensor</a></code>
<code><a href="#topic+drag.tensor">drag.tensor</a></code>, <code><a href="#topic+one.tensor">one.tensor</a></code>, 
<code><a href="#topic+mul.tensor">mul.tensor</a></code>, <code><a href="#topic++25e+25">%e%</a></code>, <code><a href="#topic++25r+25">%r%</a></code>, ,
<code><a href="#topic+drag.tensor">drag.tensor</a></code>, , <code><a href="#topic+trace.tensor">trace.tensor</a></code>, 
<code><a href="#topic+solve.tensor">solve.tensor</a></code>, <code><a href="#topic+svd.tensor">svd.tensor</a></code>,
<code><a href="#topic+mean.tensor">mean.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(U=2,V=2,W=5))
B &lt;- to.tensor(1:20,c(U=2,VV=2,WW=5))
A %e% B

</code></pre>

<hr>
<h2 id='toPos.tensor'>get the position of an index of tensor </h2><span id='topic+toPos.tensor'></span>

<h3>Description</h3>

<p>Calculates the position of a tensor index, which specified in any
possible way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toPos.tensor(M,l=NULL,mnames=names(dim(M)),by=NULL,...,both=FALSE,missing.ok=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="toPos.tensor_+3A_m">M</code></td>
<td>
<p>a tensor</p>
</td></tr>
<tr><td><code id="toPos.tensor_+3A_l">l</code></td>
<td>
<p>a vector specifying the indices as positions or names</p>
</td></tr>
<tr><td><code id="toPos.tensor_+3A_mnames">mnames</code></td>
<td>
<p>The names of the indices of the tensor. This can be
specified instead of M. </p>
</td></tr>
<tr><td><code id="toPos.tensor_+3A_both">both</code></td>
<td>
<p>Matches the index in its covariate and contravariate
form. </p>
</td></tr>
<tr><td><code id="toPos.tensor_+3A_by">by</code></td>
<td>
<p>the list dimension, all operations are done in parallel for
all levels of these dimensions. Thus in the case of toPos all other
dimensions are returned if they are not specified.</p>
</td></tr>
<tr><td><code id="toPos.tensor_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="toPos.tensor_+3A_missing.ok">missing.ok</code></td>
<td>
<p>If TRUE does give an error on missing dimension. Rather
returns NA in that place.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is only here to provide a consistent interface which
provides the same functionality for positions and characters.
</p>


<h3>Value</h3>

<p>a numeric vector giving the positions of the dimensions selected.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:30,c(a=2,b=3,c=5))
toPos.tensor(A,c("b","c"))
toPos.tensor(A,c(2,1))     # only returns the values
toPos.tensor(A,c("^a"),both=TRUE)
</code></pre>

<hr>
<h2 id='trace.tensor'>Collapse a tensor</h2><span id='topic+trace.tensor'></span>

<h3>Description</h3>

<p>Collapses the tensor over dimensions i and j. This is like a trace for
matrices or like an inner product of the dimensions i and j.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trace.tensor(X,i,j)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trace.tensor_+3A_x">X</code></td>
<td>
<p>the tensor</p>
</td></tr>
<tr><td><code id="trace.tensor_+3A_i">i</code></td>
<td>
<p>a numeric or character vector of dimensions of <code>X</code>,
used for the inner product.
</p>
</td></tr>
<tr><td><code id="trace.tensor_+3A_j">j</code></td>
<td>
<p>a numeric or character vector of dimensions of <code>X</code> with
the same length but other elements than i.  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let be
</p>
<p style="text-align: center;"><code class="reqn">X_{i_1\ldots i_n j_1\ldots j_n k_1 \ldots k_d}</code>
</p>
<p> the tensor. Then the result is given by
</p>
<p style="text-align: center;"><code class="reqn">E_{k_1 \ldots k_d}=sum_{i_1\ldots i_n} X_{i_1\ldots i_n
      i_1\ldots i_n k_1 \ldots k_d} </code>
</p>

<p>With the Einstein summing convention we would write:
</p>
<p style="text-align: center;"><code class="reqn">
    E_{k_1 \ldots k_d}=X_{i_1\ldots i_n j_1\ldots j_n k_1 \ldots
      k_d}\delta_{i_1j_1}\ldots \delta_{i_nj_n}{
      E_{k_1...k_d}=X_{i_1...i_n j_1...j_n k_1 ...
      k_d}\delta_{i_1j_1} ... \delta_{i_nj_n
      }
  }</code>
</p>



<h3>Value</h3>

<p>A tensor like X with the i and j dimensions removed.
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+mul.tensor">mul.tensor</a></code>, <code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:20,c(i=2,j=2,k=5))
A
trace.tensor(A,"i","j")
</code></pre>

<hr>
<h2 id='tripledelta.tensor'>A tensor with entry 1 if and only if three indices are equal</h2><span id='topic+tripledelta.tensor'></span>

<h3>Description</h3>

<p>The tensor mapping a tensor of dimension d to its corresponding
diagonal tensor of dimension c(d',d*)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tripledelta.tensor(d,mark1="'",mark2="*",dn=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tripledelta.tensor_+3A_d">d</code></td>
<td>
<p>the first of three dimension vectors</p>
</td></tr>
<tr><td><code id="tripledelta.tensor_+3A_mark1">mark1</code></td>
<td>
<p>the mark for the second dimension vectors</p>
</td></tr>
<tr><td><code id="tripledelta.tensor_+3A_mark2">mark2</code></td>
<td>
<p>the mark for the third dimension vectors</p>
</td></tr>
<tr><td><code id="tripledelta.tensor_+3A_dn">dn</code></td>
<td>
<p>list of character vectors, optional dimnames</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The tripledelta is the tensor mapping a tensor to a corresponding
diagonal tensor. 
</p>


<h3>Value</h3>

<p>The tensor given by:
</p>
<p style="text-align: center;"><code class="reqn">E_{i_1\ldots i_n j_1\ldots
      j_n k_1\ldots k_n}=\delta_{i_1j_1}\delta_{i_1k_1}
    \ldots \delta_{i_nj_n}\delta_{i_nk_1}</code>
</p>



<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+delta.tensor">delta.tensor</a></code>, <code><a href="#topic+diag.tensor">diag.tensor</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tripledelta.tensor(3)
</code></pre>

<hr>
<h2 id='undrop.tensor'>Adds a spurious dimension to a tensor</h2><span id='topic+undrop.tensor'></span>

<h3>Description</h3>

<p>A dimension of length 1 is added a given position to a tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>undrop.tensor(A,name,pos=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="undrop.tensor_+3A_a">A</code></td>
<td>
<p>the tensor</p>
</td></tr>
<tr><td><code id="undrop.tensor_+3A_name">name</code></td>
<td>
<p>the name of the dimension to be added</p>
</td></tr>
<tr><td><code id="undrop.tensor_+3A_pos">pos</code></td>
<td>
<p>the position, where to insert the new dimension</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is a pure convenience function.
</p>


<h3>Value</h3>

<p>A tensor with one extra dimension of length 1 with name <code>name</code> at
position <code>pos</code>. 
</p>


<h3>Author(s)</h3>

<p>K. Gerald van den Boogaart</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:4,c(a=2,b=2))
undrop.tensor(A,"i")
</code></pre>

<hr>
<h2 id='untensor'>Removes indices/dimensions from a tensor</h2><span id='topic+untensor'></span>

<h3>Description</h3>

<p>untensor is more or less the inverse of to.tensor. It flattens
tensorial dimensions. However the result is still a tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>untensor(X,i=NULL,name=NULL,pos=1,by=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="untensor_+3A_x">X</code></td>
<td>
<p>the tensor</p>
</td></tr>
<tr><td><code id="untensor_+3A_i">i</code></td>
<td>
<p>the names of the dimensions to be removed and combined to a
single new 
one as a character vector or a named list of character vectors if
the remove should be done in multiple chunks.
<code>pos</code> and <code>name</code> is in this case ignored.</p>
</td></tr>
<tr><td><code id="untensor_+3A_name">name</code></td>
<td>
<p>the name of the new dimension to replace the others</p>
</td></tr>
<tr><td><code id="untensor_+3A_pos">pos</code></td>
<td>
<p>where to insert the the new dimension</p>
</td></tr>
<tr><td><code id="untensor_+3A_by">by</code></td>
<td>
<p>if i not given the dimensions to be kept</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dimensions to be removed are gathered and 
</p>


<h3>Value</h3>

<p>a tensor with the dimensions i removed. 
</p>


<h3>Author(s)</h3>

<p>K.Gerald van den Boogaart</p>


<h3>See Also</h3>

<p><code><a href="#topic+to.tensor">to.tensor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- to.tensor(1:64,c(a=2,b=2,c=2,d=2,e=2,f=2))
untensor(A,list(c(1,5),c(2,4)),name=c("i","j"))
untensor(A,by=c("c","f"))
untensor(A,c("a","d"))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
