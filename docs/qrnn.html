<!DOCTYPE html><html><head><title>Help for package qrnn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {qrnn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#qrnn-package'><p>Quantile Regression Neural Network</p></a></li>
<li><a href='#adam'>
<p>Adaptive stochastic gradient descent optimization algorithm (Adam)</p></a></li>
<li><a href='#censored.mean'>
<p>A hybrid mean/median function for left censored variables</p></a></li>
<li><a href='#composite.stack'>
<p>Reformat data matrices for composite quantile regression</p></a></li>
<li><a href='#dummy.code'>
<p>Convert a factor to a matrix of dummy codes</p></a></li>
<li><a href='#gam.style'>
<p>Modified generalized additive model plots for interpreting QRNN models</p></a></li>
<li><a href='#huber'>
<p>Huber norm and Huber approximations to the ramp and tilted absolute value functions</p></a></li>
<li><a href='#mcqrnn'>
<p>Monotone composite quantile regression neural network (MCQRNN) for simultaneous estimation of multiple non-crossing quantiles</p></a></li>
<li><a href='#qrnn.cost'>
<p>Smooth approximation to the tilted absolute value cost function</p></a></li>
<li><a href='#qrnn.fit'>
<p>Main function used to fit a QRNN model or ensemble of QRNN models</p></a></li>
<li><a href='#qrnn.initialize'>
<p>Initialize a QRNN weight vector</p></a></li>
<li><a href='#qrnn.predict'>
<p>Evaluate quantiles from trained QRNN model</p></a></li>
<li><a href='#qrnn.rbf'>
<p>Radial basis function kernel</p></a></li>
<li><a href='#qrnn2'>
<p>Fit and make predictions from QRNN models with two hidden layers</p></a></li>
<li><a href='#quantile.dtn'>
<p>Interpolated quantile distribution with exponential tails and Nadaraya-Watson quantile distribution</p></a></li>
<li><a href='#tilted.abs'>
<p>Tilted absolute value function</p></a></li>
<li><a href='#transfer'>
<p>Transfer functions and their derivatives</p></a></li>
<li><a href='#YVRprecip'>
<p>Daily precipitation data at Vancouver Int'l Airport (YVR)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Quantile Regression Neural Network</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Fit quantile regression neural network models with optional
    left censoring, partial monotonicity constraints, generalized additive
    model constraints, and the ability to fit multiple non-crossing quantile
    functions following Cannon (2011) &lt;<a href="https://doi.org/10.1016%2Fj.cageo.2010.07.005">doi:10.1016/j.cageo.2010.07.005</a>&gt;
    and Cannon (2018) &lt;<a href="https://doi.org/10.1007%2Fs00477-018-1573-6">doi:10.1007/s00477-018-1573-6</a>&gt;.</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-29</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Author:</td>
<td>Alex J. Cannon <a href="https://orcid.org/0000-0002-8025-3790"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Alex J. Cannon &lt;alex.cannon@ec.gc.ca&gt;</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-29 19:22:04 UTC; rac001</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-29 22:30:12 UTC</td>
</tr>
</table>
<hr>
<h2 id='qrnn-package'>Quantile Regression Neural Network</h2><span id='topic+qrnn-package'></span><span id='topic+qrnn'></span>

<h3>Description</h3>

<p>This package implements the quantile regression neural network (QRNN)
(Taylor, 2000; Cannon, 2011; Cannon, 2018), which is a flexible nonlinear form
of quantile regression. While low level modelling functions are available, it is
recommended that the <code><a href="#topic+mcqrnn.fit">mcqrnn.fit</a></code> and <code><a href="#topic+mcqrnn.predict">mcqrnn.predict</a></code>
wrappers be used for most applications. More information is provided below.
</p>
<p>The goal of quantile regression is to estimate conditional quantiles of a
response variable that depend on covariates in some form of regression equation.
The QRNN adopts the multi-layer perceptron neural network architecture. The
implementation follows from previous work on the estimation of censored
regression quantiles, thus allowing predictions for mixed discrete-continuous
variables like precipitation (Friederichs and Hense, 2007). A differentiable
approximation to the quantile regression cost function is adopted so that a
simplified form of the finite smoothing algorithm (Chen, 2007) can be used to
estimate model parameters. This approximation can also be used to force the
model to solve a standard least squares regression problem or an expectile
regression problem (Cannon, 2018). Weight penalty regularization can be added
to help avoid overfitting, and ensemble models with bootstrap aggregation are
also provided.
</p>
<p>An optional monotone constraint can be invoked, which guarantees monotonic
non-decreasing behaviour of model outputs with respect to specified covariates
(Zhang, 1999). The input-hidden layer weight matrix can also be constrained
so that model relationships are strictly additive (see <code><a href="#topic+gam.style">gam.style</a></code>;
Cannon, 2018). Borrowing strength by using a composite model for multiple
regression quantiles (Zou et al., 2008; Xu et al., 2017) is also possible
(see <code><a href="#topic+composite.stack">composite.stack</a></code>). Weights can be applied to individual cases
(Jiang et al., 2012).
</p>
<p>Applying the monotone constraint in combination with the composite model allows
one to simultaneously estimate multiple non-crossing quantiles (Cannon, 2018);
the resulting monotone composite QRNN (MCQRNN) is provided by the
<code><a href="#topic+mcqrnn.fit">mcqrnn.fit</a></code> and <code><a href="#topic+mcqrnn.predict">mcqrnn.predict</a></code> wrapper functions.
Examples for <code><a href="#topic+qrnn.fit">qrnn.fit</a></code> and <code><a href="#topic+qrnn2.fit">qrnn2.fit</a></code> show how the
same functionality can be achieved using the low level
<code><a href="#topic+composite.stack">composite.stack</a></code> and fitting functions.
</p>
<p>QRNN models with a single layer of hidden nodes can be fitted using the
<code><a href="#topic+qrnn.fit">qrnn.fit</a></code> function. Predictions from a fitted model are made using
the <code><a href="#topic+qrnn.predict">qrnn.predict</a></code> function. The function <code><a href="#topic+gam.style">gam.style</a></code>
can be used to visualize and investigate fitted covariate/response relationships
from <code><a href="#topic+qrnn.fit">qrnn.fit</a></code> (Plate et al., 2000). Note: a single hidden layer
is usually sufficient for most modelling tasks. With added monotonicity
constraints, a second hidden layer may sometimes be beneficial
(Lang, 2005; Minin et al., 2010). QRNN models with two hidden layers are
available using the <code><a href="#topic+qrnn2.fit">qrnn2.fit</a></code> and
<code><a href="#topic+qrnn2.predict">qrnn2.predict</a></code> functions. For non-crossing quantiles, the
<code><a href="#topic+mcqrnn.fit">mcqrnn.fit</a></code> and <code><a href="#topic+mcqrnn.predict">mcqrnn.predict</a></code> wrappers also allow
models with one or two hidden layers to be fitted and predictions to be made
from the fitted models.
</p>
<p>In general, <code><a href="#topic+mcqrnn.fit">mcqrnn.fit</a></code> offers a convenient, single function for 
fitting multiple quantiles simultaneously. Note, however, that default 
settings in <code><a href="#topic+mcqrnn.fit">mcqrnn.fit</a></code> and other model fitting functions are 
not optimized for general speed, memory efficiency, or accuracy and should be 
adjusted for a particular regression problem as needed. In particular, the 
approximation to the quantile regression cost function <code>eps.seq</code>, the 
number of trials <code>n.trials</code>, and number of iterations <code>iter.max</code> 
can all influence fitting speed (and accuracy), as can changing the 
optimization algorithm via <code>method</code>. Non-crossing quantiles are 
implemented by stacking multiple copies of the <code>x</code> and <code>y</code> data, 
one copy per value of <code>tau</code>. Depending on the dataset size, this can 
lead to large matrices being passed to the optimization routine. In the 
<code><a href="#topic+adam">adam</a></code> adaptive stochastic gradient descent method, the
<code>minibatch</code> size can be adjusted to help offset this cost. Model complexity
is determined via the number of hidden nodes, <code>n.hidden</code> and
<code>n.hidden2</code>, as well as the optional weight penalty <code>penalty</code>; values
of these hyperparameters are crucial to obtaining a well performing model.
</p>
<p>When using <code><a href="#topic+mcqrnn.fit">mcqrnn.fit</a></code>, it is also possible to estimate the full
quantile regression process by specifying a single integer value for <code>tau</code>.
In this case, <code>tau</code> is the number of random samples used in the stochastic
estimation. For more information, see Tagasovska and Lopez-Paz (2019). It may be
necessary to restart the optimization multiple times from the previous weights
and biases, in which case <code>init.range</code> can be set to the <code>weights</code>
values from the previously completed optimization run. For large datasets, it is
recommended that the <code><a href="#topic+adam">adam</a></code> method with an appropriate integer
<code>tau</code> and <code>minibatch</code> size be used for optimization.
</p>
<p>If models for multiple quantiles have been fitted, for example by
<code><a href="#topic+mcqrnn.fit">mcqrnn.fit</a></code> or multiple calls to either <code><a href="#topic+qrnn.fit">qrnn.fit</a></code>
or <code><a href="#topic+qrnn2.fit">qrnn2.fit</a></code>, the (experimental) <code><a href="#topic+dquantile">dquantile</a></code>
function and its companion functions are available to create proper
probability density, distribution, and quantile functions
(Quiñonero-Candela et al., 2006; Cannon, 2011). Alternative distribution,
quantile, and random variate functions based on the Nadaraya-Watson estimator
(Passow and Donner, 2020) are also available in <code>[p,q,r]quantile.nw</code>.
These can be useful for assessing probabilistic calibration and evaluating
model performance.
</p>
<p>Note: the user cannot easily change the output layer transfer function
to be different than <code>hramp</code>, which provides either the identity function or a
ramp function to accommodate optional left censoring. Some applications, for
example fitting smoothed binary quantile regression models for a binary target
variable (Kordas, 2006), require an alternative like the logistic sigmoid.
While not straightforward, it is possible to change the output layer transfer
function by switching off <code>scale.y</code> in the call to the fitting
function and reassigning <code>hramp</code> and <code>hramp.prime</code> as follows:
</p>
<pre>
library(qrnn)

# Use the logistic sigmoid as the output layer transfer function
To.logistic &lt;- function(x, lower, eps) 0.5 + 0.5*tanh(x/2)
environment(To.logistic) &lt;- asNamespace("qrnn")
assignInNamespace("hramp", To.logistic, ns="qrnn")

# Change the derivative of the output layer transfer function
To.logistic.prime &lt;- function(x, lower, eps) 0.25/(cosh(x/2)^2)
environment(To.logistic.prime) &lt;- asNamespace("qrnn")
assignInNamespace("hramp.prime", To.logistic.prime, ns="qrnn")

</pre>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> qrnn</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>References</h3>

<p>Cannon, A.J., 2011. Quantile regression neural networks: implementation
in R and application to precipitation downscaling. Computers &amp; Geosciences,
37: 1277-1284. doi:10.1016/j.cageo.2010.07.005
</p>
<p>Cannon, A.J., 2018. Non-crossing nonlinear regression quantiles by
monotone composite quantile regression neural network, with application
to rainfall extremes. Stochastic Environmental Research and Risk Assessment,
32(11): 3207-3225. doi:10.1007/s00477-018-1573-6
</p>
<p>Chen, C., 2007. A finite smoothing algorithm for quantile regression.
Journal of Computational and Graphical Statistics, 16: 136-164.
</p>
<p>Friederichs, P. and A. Hense, 2007. Statistical downscaling of extreme
precipitation events using censored quantile regression. Monthly Weather
Review, 135: 2365-2378. 
</p>
<p>Jiang, X., J. Jiang, and X. Song, 2012. Oracle model selection for nonlinear
models based on weighted composite quantile regression. Statistica Sinica,
22(4): 1479-1506.
</p>
<p>Kordas, G., 2006. Smoothed binary regression quantiles. Journal of Applied
Econometrics, 21(3): 387-407.
</p>
<p>Lang, B., 2005. Monotonic multi-layer perceptron networks as universal
approximators. International Conference on Artificial Neural Networks,
Artificial Neural Networks: Formal Models and Their Applications-ICANN 2005,
pp. 31-37.
</p>
<p>Minin, A., M. Velikova, B. Lang, and H. Daniels, 2010. Comparison of universal
approximators incorporating partial monotonicity by structure.
Neural Networks, 23(4): 471-475.
</p>
<p>Passow, C., R.V. Donner, 2020. Regression-based distribution mapping for
bias correction of climate model outputs using linear quantile regression.
Stochastic Environmental Research and Risk Assessment, 34: 87-102.
</p>
<p>Plate, T., J. Bert, J. Grace, and P. Band, 2000. Visualizing the function
computed by a feedforward neural network. Neural Computation,
12(6): 1337-1354.
</p>
<p>Quiñonero-Candela, J., C. Rasmussen, F. Sinz, O. Bousquet,
B. Scholkopf, 2006. Evaluating Predictive Uncertainty Challenge.
Lecture Notes in Artificial Intelligence, 3944: 1-27.
</p>
<p>Tagasovska, N., D. Lopez-Paz, 2019. Single-model uncertainties for deep
learning. Advances in Neural Information Processing Systems, 32,
NeurIPS 2019. doi:10.48550/arXiv.1811.00908
</p>
<p>Taylor, J.W., 2000. A quantile regression neural network approach to
estimating the conditional density of multiperiod returns. Journal of
Forecasting, 19(4): 299-311.
</p>
<p>Xu, Q., K. Deng, C. Jiang, F. Sun, and X. Huang, 2017. Composite quantile
regression neural network with applications. Expert Systems with Applications,
76, 129-139.
</p>
<p>Zhang, H. and Zhang, Z., 1999. Feedforward networks with monotone
constraints. In: International Joint Conference on Neural Networks,
vol. 3, p. 1820-1823. doi:10.1109/IJCNN.1999.832655
</p>
<p>Zou, H. and M. Yuan, 2008. Composite quantile regression and the oracle model
selection theory. The Annals of Statistics, 1108-1126.
</p>

<hr>
<h2 id='adam'>
Adaptive stochastic gradient descent optimization algorithm (Adam)
</h2><span id='topic+adam'></span>

<h3>Description</h3>

<p>From Kingma and Ba (2015): &quot;We introduce Adam, an algorithm for first-order 
gradient-based optimization of stochastic objective functions, based on 
adaptive estimates of lower-order moments. The method is straightforward to 
implement, is computationally efficient, has little memory requirements, is 
invariant to diagonal rescaling of the gradients, and is well suited for 
problems that are large in terms of data and/or parameters. The method is 
also appropriate for non-stationary objectives and problems with very noisy 
and/or sparse gradients. The hyper-parameters have intuitive interpretations 
and typically require little tuning. Some connections to related algorithms, 
on which Adam was inspired, are discussed. We also analyze the theoretical 
convergence properties of the algorithm and provide a regret bound on the 
convergence rate that is comparable to the best known results under the 
online convex optimization framework. Empirical results demonstrate that Adam 
works well in practice and compares favorably to other stochastic 
optimization methods. Finally, we discuss AdaMax, a variant of Adam based on 
the infinity norm.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adam(f, p, x, y, w, tau, ..., iterlim=5000, iterbreak=iterlim,
     alpha=0.01, minibatch=nrow(x), beta1=0.9, beta2=0.999,
     epsilon=1e-8, print.level=10)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adam_+3A_f">f</code></td>
<td>

<p>the function to be minimized, including gradient information
contained in the <code>gradient</code> attribute.
</p>
</td></tr>
<tr><td><code id="adam_+3A_p">p</code></td>
<td>

<p>the starting parameters for the minimization.
</p>
</td></tr>
<tr><td><code id="adam_+3A_x">x</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of
samples and number of columns equal to the number of variables.
</p>
</td></tr>
<tr><td><code id="adam_+3A_y">y</code></td>
<td>

<p>response column matrix with number of rows equal to the number
of samples.
</p>
</td></tr>
<tr><td><code id="adam_+3A_w">w</code></td>
<td>

<p>vector of weights with length equal to the number of samples.
</p>
</td></tr>
<tr><td><code id="adam_+3A_tau">tau</code></td>
<td>

<p>vector of desired tau-quantile(s) with length equal to the
number of samples.
</p>
</td></tr>
<tr><td><code id="adam_+3A_...">...</code></td>
<td>

<p>additional parameters passed to the <code>f</code> cost function.
</p>
</td></tr>
<tr><td><code id="adam_+3A_iterlim">iterlim</code></td>
<td>

<p>the maximum number of iterations before the optimization is stopped.
</p>
</td></tr>
<tr><td><code id="adam_+3A_iterbreak">iterbreak</code></td>
<td>

<p>the maximum number of iterations without progress before the
optimization is stopped.
</p>
</td></tr>
<tr><td><code id="adam_+3A_alpha">alpha</code></td>
<td>

<p>size of the learning rate.
</p>
</td></tr>
<tr><td><code id="adam_+3A_minibatch">minibatch</code></td>
<td>

<p>number of samples in each minibatch.
</p>
</td></tr>
<tr><td><code id="adam_+3A_beta1">beta1</code></td>
<td>

<p>controls the exponential decay rate used to scale the biased first
moment estimate.
</p>
</td></tr>
<tr><td><code id="adam_+3A_beta2">beta2</code></td>
<td>

<p>controls the exponential decay rate used to scale the biased second
raw moment estimate.
</p>
</td></tr>
<tr><td><code id="adam_+3A_epsilon">epsilon</code></td>
<td>

<p>smoothing term to avoid division by zero.
</p>
</td></tr>
<tr><td><code id="adam_+3A_print.level">print.level</code></td>
<td>

<p>the level of printing which is done during optimization. A value of
<code>0</code> suppresses any progress reporting, whereas positive values
report the value of <code>f</code> every <code>print.level</code> iterations.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table>
<tr><td><code>estimate</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>minimum</code></td>
<td>
<p>The value of <code>f</code> corresponding to <code>estimate</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kingma, D.P. and J. Ba, 2015. Adam: A method for stochastic optimization.
The International Conference on Learning Representations (ICLR) 2015.
http://arxiv.org/abs/1412.6980
</p>

<hr>
<h2 id='censored.mean'>
A hybrid mean/median function for left censored variables
</h2><span id='topic+censored.mean'></span>

<h3>Description</h3>

<p>Returns the median if the majority of values are censored and the mean otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>censored.mean(x, lower, trim=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="censored.mean_+3A_x">x</code></td>
<td>

<p>numeric vector.
</p>
</td></tr>
<tr><td><code id="censored.mean_+3A_lower">lower</code></td>
<td>

<p>left censoring point.
</p>
</td></tr>
<tr><td><code id="censored.mean_+3A_trim">trim</code></td>
<td>

<p>fraction of observations to be trimmed from each end of <code>x</code> before the mean is computed.
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+qrnn.fit">qrnn.fit</a></code>, <code><a href="#topic+qrnn.predict">qrnn.predict</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(0, 0, 1, 2, 3)
print(censored.mean(x, lower=0))
x.cens &lt;- c(0, 0, 0, 1, 2)
print(censored.mean(x.cens, lower=0))
</code></pre>

<hr>
<h2 id='composite.stack'>
Reformat data matrices for composite quantile regression
</h2><span id='topic+composite.stack'></span>

<h3>Description</h3>

<p>Returns stacked <code>x</code> and <code>y</code> matrices and <code>tau</code> vector,
which can be passed to <code>qrnn.fit</code> to fit composite quantile
regression and composite QRNN models (Zou et al., 2008;
Xu et al., 2017). In combination with the partial monotonicity
constraints, stacking can be used to fit multiple non-crossing
quantile functions (see <code><a href="#topic+mcqrnn">mcqrnn</a></code>). More details
are provided in Cannon (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>composite.stack(x, y, tau)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="composite.stack_+3A_x">x</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of samples and number of columns equal to 
the number of variables.
</p>
</td></tr>
<tr><td><code id="composite.stack_+3A_y">y</code></td>
<td>

<p>response column matrix with number of rows equal to the number of samples.
</p>
</td></tr>
<tr><td><code id="composite.stack_+3A_tau">tau</code></td>
<td>

<p>vector of tau-quantiles.
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Cannon, A.J., 2018. Non-crossing nonlinear regression quantiles by
monotone composite quantile regression neural network, with application
to rainfall extremes. Stochastic Environmental Research and Risk Assessment,
32(11): 3207-3225. doi:10.1007/s00477-018-1573-6
</p>
<p>Xu, Q., K. Deng, C. Jiang, F. Sun, and X. Huang, 2017. Composite quantile 
regression neural network with applications. Expert Systems with Applications,
76, 129-139.
</p>
<p>Zou, H. and M. Yuan, 2008. Composite quantile regression and the oracle model
selection theory. The Annals of Statistics, 1108-1126.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+qrnn.fit">qrnn.fit</a></code>, <code><a href="#topic+mcqrnn">mcqrnn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[,"Petal.Length",drop=FALSE])
y &lt;- as.matrix(iris[,"Petal.Width",drop=FALSE])

cases &lt;- order(x)
x &lt;- x[cases,,drop=FALSE]
y &lt;- y[cases,,drop=FALSE]

tau &lt;- seq(0.05, 0.95, by=0.05)
x.y.tau &lt;- composite.stack(x, y, tau)
binary.tau &lt;- dummy.code(as.factor(x.y.tau$tau))

set.seed(1)

# Composite QR
fit.cqr &lt;- qrnn.fit(cbind(binary.tau, x.y.tau$x), x.y.tau$y,
                    tau=x.y.tau$tau, n.hidden=1, n.trials=1,
                    Th=linear, Th.prime=linear.prime)
pred.cqr &lt;- matrix(qrnn.predict(cbind(binary.tau, x.y.tau$x), fit.cqr),
                   ncol=length(tau))
coef.cqr &lt;- lm.fit(cbind(1, x), pred.cqr)$coef
colnames(coef.cqr) &lt;- tau
print(coef.cqr)

# Composite QRNN
fit.cqrnn &lt;- qrnn.fit(x.y.tau$x, x.y.tau$y, tau=x.y.tau$tau,
                      n.hidden=1, n.trials=1, Th=sigmoid,
                      Th.prime=sigmoid.prime)
pred.cqrnn &lt;- qrnn.predict(x.y.tau$x, fit.cqrnn)
pred.cqrnn &lt;- matrix(pred.cqrnn, ncol=length(tau), byrow=FALSE)

matplot(x, pred.cqrnn, col="red", type="l")
points(x, y, pch=20)

</code></pre>

<hr>
<h2 id='dummy.code'>
Convert a factor to a matrix of dummy codes
</h2><span id='topic+dummy.code'></span>

<h3>Description</h3>

<p>Converts a factor (categorical) variable to a matrix of dummy codes
using a 1 of C-1 binary coding scheme.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dummy.code(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dummy.code_+3A_x">x</code></td>
<td>

<p>a factor variable.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with the number of rows equal to the number of cases in <code>x</code>
and the number of columns equal to one minus the number of factors in
<code>x</code>. The last factor serves as the reference group.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  print(dummy.code(iris$Species))
</code></pre>

<hr>
<h2 id='gam.style'>
Modified generalized additive model plots for interpreting QRNN models
</h2><span id='topic+gam.style'></span>

<h3>Description</h3>

<p>Generalized additive model (GAM)-style effects plots provide a graphical
means of interpreting relationships between covariates and conditional
quantiles predicted by a QRNN. From Plate et al. (2000): The effect of the
<code>i</code>th input variable at a particular input point <code>Delta.i.x</code>
is the change in <code>f</code> resulting from changing <code>X1</code> to <code>x1</code>
from <code>b1</code> (the baseline value [...]) while keeping the other
inputs constant. The effects are plotted as short line segments, centered
at (<code>x.i</code>, <code>Delta.i.x</code>), where the slope of the segment
is given by the partial derivative. Variables that strongly influence
the function value have a large total vertical range of effects.
Functions without interactions appear as possibly broken straight lines
(linear functions) or curves (nonlinear functions). Interactions show up as
vertical spread at a particular horizontal location, that is, a vertical
scattering of segments. Interactions are present when the effect of
a variable depends on the values of other variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam.style(x, parms, column, baseline=mean(x[,column]),
         epsilon=1e-5, seg.len=0.02, seg.cols="black",
         plot=TRUE, return.results=FALSE, trim=0,
         ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gam.style_+3A_x">x</code></td>
<td>

<p>matrix with number of rows equal to the number of samples and number of columns equal to the number of covariate variables.
</p>
</td></tr>
<tr><td><code id="gam.style_+3A_parms">parms</code></td>
<td>

<p>list returned by <code><a href="#topic+qrnn.fit">qrnn.fit</a></code>.
</p>
</td></tr>
<tr><td><code id="gam.style_+3A_column">column</code></td>
<td>

<p>column of <code>x</code> for which effects plots should be returned.
</p>
</td></tr>
<tr><td><code id="gam.style_+3A_baseline">baseline</code></td>
<td>

<p>value of <code>x[,column]</code> to be used as the baseline for calculation of covariate effects; defaults to <code>mean(x[,column])</code>.
</p>
</td></tr>
<tr><td><code id="gam.style_+3A_epsilon">epsilon</code></td>
<td>

<p>step-size used in the finite difference calculation of the partial derivatives.
</p>
</td></tr>
<tr><td><code id="gam.style_+3A_seg.len">seg.len</code></td>
<td>

<p>length of effects line segments expressed as a fraction of the range of <code>x[,column]</code>.
</p>
</td></tr>
<tr><td><code id="gam.style_+3A_seg.cols">seg.cols</code></td>
<td>

<p>colors of effects line segments.
</p>
</td></tr>
<tr><td><code id="gam.style_+3A_plot">plot</code></td>
<td>

<p>if <code>TRUE</code> (the default) then an effects plots for the given model is produced.
</p>
</td></tr>
<tr><td><code id="gam.style_+3A_return.results">return.results</code></td>
<td>

<p>if <code>TRUE</code> then values of effects and partial derivatives are returned.
</p>
</td></tr>
<tr><td><code id="gam.style_+3A_trim">trim</code></td>
<td>

<p>if <code>plot=TRUE</code> and <code>parms</code> is for a model with <code>n.ensemble &gt; 1</code>, value of <code>trim</code> passed to <code><a href="#topic+censored.mean">censored.mean</a></code>.
</p>
</td></tr>
<tr><td><code id="gam.style_+3A_...">...</code></td>
<td>

<p>further arguments to be passed to <code>plot</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements:
</p>
<table>
<tr><td><code>effects</code></td>
<td>
<p>a matrix of covariate effects.</p>
</td></tr>
<tr><td><code>partials</code></td>
<td>
<p>a matrix of covariate partial derivatives.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Cannon, A.J. and I.G. McKendry, 2002. A graphical sensitivity analysis
for interpreting statistical climate models: Application to Indian
monsoon rainfall prediction by artificial neural networks and
multiple linear regression models. International Journal of
Climatology, 22:1687-1708.
</p>
<p>Plate, T., J. Bert, J. Grace, and P. Band, 2000. Visualizing the function
computed by a feedforward neural network. Neural Computation,
12(6): 1337-1354.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+qrnn.fit">qrnn.fit</a></code>, <code><a href="#topic+qrnn.predict">qrnn.predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## YVR precipitation data with seasonal cycle and NCEP/NCAR Reanalysis
## covariates
data(YVRprecip)

y &lt;- YVRprecip$precip
x &lt;- cbind(sin(2*pi*seq_along(y)/365.25),
           cos(2*pi*seq_along(y)/365.25),
           YVRprecip$ncep)

## Fit QRNN, additive QRNN (QADD), and quantile regression (QREG)
## models for the conditional 75th percentile
set.seed(1)
train &lt;- c(TRUE, rep(FALSE, 49))
w.qrnn &lt;- qrnn.fit(x=x[train,], y=y[train,,drop=FALSE],
                   n.hidden=2, tau=0.75, iter.max=500,
                   n.trials=1, lower=0, penalty=0.01)
w.qadd &lt;- qrnn.fit(x=x[train,], y=y[train,,drop=FALSE],
                   n.hidden=ncol(x), tau=0.75, iter.max=250,
                   n.trials=1, lower=0, additive=TRUE)
w.qreg &lt;- qrnn.fit(x=x[train,], y=y[train,,drop=FALSE],
                   tau=0.75, iter.max=100, n.trials=1,
                   lower=0, Th=linear, Th.prime=linear.prime)

## GAM-style plots for slp, sh700, and z500
for (column in 3:5) {
    gam.style(x[train,], parms=w.qrnn, column=column,
              main="QRNN")
    gam.style(x[train,], parms=w.qadd, column=column,
              main="QADD")
    gam.style(x[train,], parms=w.qreg, column=column,
              main="QREG")
}
</code></pre>

<hr>
<h2 id='huber'>
Huber norm and Huber approximations to the ramp and tilted absolute value functions
</h2><span id='topic+huber'></span><span id='topic+huber.prime'></span><span id='topic+hramp'></span><span id='topic+hramp.prime'></span><span id='topic+tilted.approx'></span><span id='topic+tilted.approx.prime'></span>

<h3>Description</h3>

<p>Huber norm function providing a hybrid L1/L2 norm. Huber approximations to the ramp <code>hramp</code> and tilted absolute value <code>tilted.approx</code> functions. <code>huber.prime</code>, <code>hramp.prime</code>, and <code>tilted.approx.prime</code> provide the corresponding derivatives.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>huber(x, eps)
huber.prime(x, eps)
hramp(x, lower, eps)
hramp.prime(x, lower, eps)
tilted.approx(x, tau, eps)
tilted.approx.prime(x, tau, eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="huber_+3A_x">x</code></td>
<td>

<p>numeric vector.
</p>
</td></tr>
<tr><td><code id="huber_+3A_eps">eps</code></td>
<td>

<p>epsilon value used in <code><a href="#topic+huber">huber</a></code> and related functions.
</p>
</td></tr>
<tr><td><code id="huber_+3A_tau">tau</code></td>
<td>

<p>desired tau-quantile.
</p>
</td></tr>
<tr><td><code id="huber_+3A_lower">lower</code></td>
<td>

<p>left censoring point.
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+tilted.abs">tilted.abs</a></code>, <code><a href="#topic+qrnn.cost">qrnn.cost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(-10, 10, length=100)
plot(x, huber(x, eps=1), type="l", col="black", ylim=c(-2, 10), ylab="")
lines(x, hramp(x, lower=0, eps=1), col="red")
lines(x, tilted.approx(x, tau=0.1, eps=1), col="blue")
lines(x, huber.prime(x, eps=1), col="black", lty=2)
lines(x, hramp.prime(x, lower=0, eps=1), lty=2, col="red")
lines(x, tilted.approx.prime(x, tau=0.1, eps=1), lty=2, col="blue")
</code></pre>

<hr>
<h2 id='mcqrnn'>
Monotone composite quantile regression neural network (MCQRNN) for simultaneous estimation of multiple non-crossing quantiles
</h2><span id='topic+mcqrnn'></span><span id='topic+mcqrnn.fit'></span><span id='topic+mcqrnn.predict'></span>

<h3>Description</h3>

<p>High level wrapper functions for fitting and making predictions from a
monotone composite quantile regression neural network (MCQRNN) model for
multiple non-crossing regression quantiles (Cannon, 2018).
</p>
<p>Uses <code>composite.stack</code> and monotonicity constraints in
<code>qrnn.fit</code> or <code>qrnn2.fit</code> to fit MCQRNN models with
one or two hidden layers. Note: <code>Th</code> must be a non-decreasing
function to guarantee non-crossing.
</p>
<p>Following Tagasovska and Lopez-Paz (2019), it is also possible to estimate the
full quantile regression process by specifying a single integer value for 
<code>tau</code>. In this case, tau is the number of random samples used in the
stochastic estimation. It may be necessary to restart the optimization multiple
times from the previous weights and biases, in which case <code>init.range</code> can
be set to the weights values from the previously completed optimization run.
For large datasets, it is recommended that the <code>adam</code> method with an
appropriate <code>minibatch</code> size be used for optimization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcqrnn.fit(x, y, n.hidden=2, n.hidden2=NULL, w=NULL,
           tau=c(0.1, 0.5, 0.9), iter.max=5000, n.trials=5,
           lower=-Inf, init.range=c(-0.5, 0.5, -0.5, 0.5, -0.5, 0.5),
           monotone=NULL, eps.seq=2^seq(-8, -32, by=-4), Th=sigmoid,
           Th.prime=sigmoid.prime, penalty=0, n.errors.max=10,
           trace=TRUE, method=c("nlm", "adam"), scale.y=TRUE, ...)
mcqrnn.predict(x, parms, tau=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcqrnn_+3A_x">x</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of samples and number of columns equal to the number of variables.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_y">y</code></td>
<td>

<p>response column matrix with number of rows equal to the number of samples.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_n.hidden">n.hidden</code></td>
<td>

<p>number of hidden nodes in the first hidden layer.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_n.hidden2">n.hidden2</code></td>
<td>

<p>number of hidden nodes in the second hidden layer; <code>NULL</code> fits a model with a single hidden layer.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_w">w</code></td>
<td>

<p>if <code>tau</code> specifies a finite number of tau-quantiles, a vector of weights with length equal to the number of samples
times the length of <code>tau</code>; see <code>composite.stack</code>. Otherwise, a vector of weights with length equal to the
number of samples. <code>NULL</code> gives equal weight to each sample. 
</p>
</td></tr> 
<tr><td><code id="mcqrnn_+3A_tau">tau</code></td>
<td>

<p>desired tau-quantiles; <code>NULL</code> in <code>mcqrnn.predict</code> uses values from the original call to <code>mcqrnn.fit</code>.
If <code>tau</code> is an integer, specifies the number of random samples used for stochastic estimation of the full quantile
regression process.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_iter.max">iter.max</code></td>
<td>

<p>maximum number of iterations of the optimization algorithm.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_n.trials">n.trials</code></td>
<td>

<p>number of repeated trials used to avoid local minima.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_lower">lower</code></td>
<td>

<p>left censoring point.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_init.range">init.range</code></td>
<td>

<p>initial weight range for input-hidden, hidden-hidden, and hidden-output weight matrices. If supplied with a list
of weight matrices from a prior run of <code>mcqrnn.fit</code>, will restart model fitting with these values. 
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_monotone">monotone</code></td>
<td>

<p>column indices of covariates for which the monotonicity constraint should hold.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_eps.seq">eps.seq</code></td>
<td>

<p>sequence of <code>eps</code> values for the finite smoothing algorithm.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_th">Th</code></td>
<td>

<p>hidden layer transfer function; use <code><a href="#topic+sigmoid">sigmoid</a></code>, <code><a href="#topic+elu">elu</a></code>, <code><a href="#topic+relu">relu</a></code>,
<code><a href="#topic+lrelu">lrelu</a></code>, <code><a href="#topic+softplus">softplus</a></code>, or other non-decreasing function.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_th.prime">Th.prime</code></td>
<td>

<p>derivative of the hidden layer transfer function <code>Th</code>.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_penalty">penalty</code></td>
<td>

<p>weight penalty for weight decay regularization.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_n.errors.max">n.errors.max</code></td>
<td>

<p>maximum number of <code>nlm</code> optimization failures allowed before quitting.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_trace">trace</code></td>
<td>

<p>logical variable indicating whether or not diagnostic messages are printed during optimization.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_method">method</code></td>
<td>

<p>character string indicating which optimization algorithm to use when <code>n.hidden2 != NULL</code>.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_scale.y">scale.y</code></td>
<td>

<p>logical variable indicating whether <code>y</code> should be scaled to zero mean and unit standard deviation.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_...">...</code></td>
<td>

<p>additional parameters passed to the <code><a href="stats.html#topic+nlm">nlm</a></code> or <code><a href="#topic+adam">adam</a></code> optimization routines.
</p>
</td></tr>
<tr><td><code id="mcqrnn_+3A_parms">parms</code></td>
<td>

<p>list containing MCQRNN weight matrices and other parameters.
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Cannon, A.J., 2011. Quantile regression neural networks: implementation
in R and application to precipitation downscaling. Computers &amp; Geosciences,
37: 1277-1284. doi:10.1016/j.cageo.2010.07.005
</p>
<p>Cannon, A.J., 2018. Non-crossing nonlinear regression quantiles by
monotone composite quantile regression neural network, with application
to rainfall extremes. Stochastic Environmental Research and Risk Assessment,
32(11): 3207-3225. doi:10.1007/s00477-018-1573-6
</p>
<p>Tagasovska, N., D. Lopez-Paz, 2019. Single-model uncertainties for deep
learning. Advances in Neural Information Processing Systems, 32,
NeurIPS 2019. doi:10.48550/arXiv.1811.00908
</p>


<h3>See Also</h3>

<p><code><a href="#topic+composite.stack">composite.stack</a></code>, <code><a href="#topic+qrnn.fit">qrnn.fit</a></code>,
<code><a href="#topic+qrnn2.fit">qrnn2.fit</a></code>, <code><a href="#topic+qrnn.predict">qrnn.predict</a></code>,
<code><a href="#topic+qrnn2.predict">qrnn2.predict</a></code>, <code><a href="#topic+adam">adam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[,"Petal.Length",drop=FALSE])
y &lt;- as.matrix(iris[,"Petal.Width",drop=FALSE])

cases &lt;- order(x)
x &lt;- x[cases,,drop=FALSE]
y &lt;- y[cases,,drop=FALSE]

set.seed(1)

## MCQRNN model w/ 2 hidden layers for simultaneous estimation of
## multiple non-crossing quantile functions
fit.mcqrnn &lt;- mcqrnn.fit(x, y, tau=seq(0.1, 0.9, by=0.1),
                         n.hidden=2, n.hidden2=2, n.trials=1,
                         iter.max=500)
pred.mcqrnn &lt;- mcqrnn.predict(x, fit.mcqrnn)

## Estimate the full quantile regression process by specifying
## the number of samples/random values of tau used in training

fit.full &lt;- mcqrnn.fit(x, y, tau=1000L, n.hidden=3, n.hidden2=3,
                       n.trials=1, iter.max=300, eps.seq=1e-6,
                       method="adam", minibatch=64, print.level=100)
# Show how to initialize from previous weights
fit.full &lt;- mcqrnn.fit(x, y, tau=1000L, n.hidden=3, n.hidden2=3,
                       n.trials=1, iter.max=300, eps.seq=1e-6,
                       method="adam", minibatch=64, print.level=100,
                       init.range=fit.full$weights)
pred.full &lt;- mcqrnn.predict(x, fit.full, tau=seq(0.1, 0.9, by=0.1))

par(mfrow=c(1, 2))
matplot(x, pred.mcqrnn, col="blue", type="l")
points(x, y)
matplot(x, pred.full, col="blue", type="l")
points(x, y)

</code></pre>

<hr>
<h2 id='qrnn.cost'>
Smooth approximation to the tilted absolute value cost function
</h2><span id='topic+qrnn.cost'></span>

<h3>Description</h3>

<p>Smooth approximation to the tilted absolute value cost function
used to fit a QRNN model. Optional left censoring, monotone constraints,
and additive constraints are supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qrnn.cost(weights, x, y, n.hidden, w, tau, lower, monotone,
          additive, eps, Th, Th.prime, penalty, unpenalized)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qrnn.cost_+3A_weights">weights</code></td>
<td>

<p>weight vector of length returned by <code><a href="#topic+qrnn.initialize">qrnn.initialize</a></code>.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_x">x</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of samples and number of columns equal to the number of variables.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_y">y</code></td>
<td>

<p>response column matrix with number of rows equal to the number of samples.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_n.hidden">n.hidden</code></td>
<td>

<p>number of hidden nodes in the QRNN model.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_w">w</code></td>
<td>

<p>vector of weights with length equal to the number of samples;
<code>NULL</code> gives equal weight to each sample.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_tau">tau</code></td>
<td>

<p>desired tau-quantile.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_lower">lower</code></td>
<td>

<p>left censoring point.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_monotone">monotone</code></td>
<td>

<p>column indices of covariates for which the monotonicity constraint should hold.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_additive">additive</code></td>
<td>

<p>force additive relationships.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_eps">eps</code></td>
<td>

<p>epsilon value used in the approximation functions.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_th">Th</code></td>
<td>

<p>hidden layer transfer function; use <code><a href="#topic+sigmoid">sigmoid</a></code>, <code><a href="#topic+elu">elu</a></code>, <code><a href="#topic+relu">relu</a></code>, <code><a href="#topic+lrelu">lrelu</a></code>, 
<code><a href="#topic+softplus">softplus</a></code>, or other non-decreasing function for a nonlinear model and <code><a href="#topic+linear">linear</a></code> for a linear model.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_th.prime">Th.prime</code></td>
<td>

<p>derivative of the hidden layer transfer function <code>Th</code>.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_penalty">penalty</code></td>
<td>

<p>weight penalty for weight decay regularization.
</p>
</td></tr>
<tr><td><code id="qrnn.cost_+3A_unpenalized">unpenalized</code></td>
<td>

<p>column indices of covariates for which the weight penalty should not be applied to input-hidden layer weights.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric value indicating tilted absolute value cost function, along with attribute containing vector with gradient information.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+qrnn.fit">qrnn.fit</a></code>
</p>

<hr>
<h2 id='qrnn.fit'>
Main function used to fit a QRNN model or ensemble of QRNN models
</h2><span id='topic+qrnn.fit'></span>

<h3>Description</h3>

<p>Function used to fit a QRNN model or ensemble of QRNN models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qrnn.fit(x, y, n.hidden, w=NULL, tau=0.5, n.ensemble=1,
         iter.max=5000, n.trials=5, bag=FALSE, lower=-Inf,
         init.range=c(-0.5, 0.5, -0.5, 0.5), monotone=NULL,
         additive=FALSE, eps.seq=2^seq(-8, -32, by=-4),
         Th=sigmoid, Th.prime=sigmoid.prime, penalty=0,
         unpenalized=NULL, n.errors.max=10, trace=TRUE,
         scale.y=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qrnn.fit_+3A_x">x</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of samples and number of columns equal to the number of variables.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_y">y</code></td>
<td>

<p>response column matrix with number of rows equal to the number of samples.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_n.hidden">n.hidden</code></td>
<td>

<p>number of hidden nodes in the QRNN model.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_w">w</code></td>
<td>

<p>vector of weights with length equal to the number of samples;
<code>NULL</code> gives equal weight to each sample.
</p>
</td></tr> 
<tr><td><code id="qrnn.fit_+3A_tau">tau</code></td>
<td>

<p>desired tau-quantile(s).
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_n.ensemble">n.ensemble</code></td>
<td>

<p>number of ensemble members to fit.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_iter.max">iter.max</code></td>
<td>

<p>maximum number of iterations of the optimization algorithm.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_n.trials">n.trials</code></td>
<td>

<p>number of repeated trials used to avoid local minima.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_bag">bag</code></td>
<td>

<p>logical variable indicating whether or not bootstrap aggregation (bagging) should be used.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_lower">lower</code></td>
<td>

<p>left censoring point.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_init.range">init.range</code></td>
<td>

<p>initial weight range for input-hidden and hidden-output weight matrices.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_monotone">monotone</code></td>
<td>

<p>column indices of covariates for which the monotonicity constraint should hold.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_additive">additive</code></td>
<td>

<p>force additive relationships.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_eps.seq">eps.seq</code></td>
<td>

<p>sequence of <code>eps</code> values for the finite smoothing algorithm.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_th">Th</code></td>
<td>

<p>hidden layer transfer function; use <code><a href="#topic+sigmoid">sigmoid</a></code>, <code><a href="#topic+elu">elu</a></code>, <code><a href="#topic+relu">relu</a></code>, <code><a href="#topic+lrelu">lrelu</a></code>, 
<code><a href="#topic+softplus">softplus</a></code>, or other non-decreasing function for a nonlinear model and <code><a href="#topic+linear">linear</a></code> for a linear model.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_th.prime">Th.prime</code></td>
<td>

<p>derivative of the hidden layer transfer function <code>Th</code>.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_penalty">penalty</code></td>
<td>

<p>weight penalty for weight decay regularization.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_unpenalized">unpenalized</code></td>
<td>

<p>column indices of covariates for which the weight penalty should not be applied to input-hidden layer weights.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_n.errors.max">n.errors.max</code></td>
<td>

<p>maximum number of <code>nlm</code> optimization failures allowed before quitting.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_trace">trace</code></td>
<td>

<p>logical variable indicating whether or not diagnostic messages are printed during optimization.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_scale.y">scale.y</code></td>
<td>

<p>logical variable indicating whether <code>y</code> should be scaled to zero mean and unit standard deviation.
</p>
</td></tr>
<tr><td><code id="qrnn.fit_+3A_...">...</code></td>
<td>

<p>additional parameters passed to the <code><a href="stats.html#topic+nlm">nlm</a></code> optimization routine.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fit a censored quantile regression neural network model for the
<code>tau</code>-quantile by minimizing a cost function based on smooth
Huber-norm approximations to the tilted absolute value and ramp functions.
Left censoring can be turned on by setting <code>lower</code> to a value
greater than <code>-Inf</code>. A simplified form of the finite smoothing
algorithm, in which the <code><a href="stats.html#topic+nlm">nlm</a></code> optimization algorithm
is run with values of the <code>eps</code> approximation tolerance progressively
reduced in magnitude over the sequence <code>eps.seq</code>, is used to set the
QRNN weights and biases. Local minima of the cost function can be
avoided by setting <code>n.trials</code>, which controls the number of
repeated runs from different starting weights and biases, to a value
greater than one.
</p>
<p>(Note: if <code>eps.seq</code> is set to a single, sufficiently large value and <code>tau</code>
is set to <code>0.5</code>, then the result will be a standard least squares
regression model. The same value of <code>eps.seq</code> and other values
of <code>tau</code> leads to expectile regression.)
</p>
<p>If invoked, the <code>monotone</code> argument enforces non-decreasing behaviour
between specified columns of <code>x</code> and model outputs. This holds if
<code>Th</code> and <code>To</code> are monotone non-decreasing functions. In this case,
the <code>exp</code> function is applied to the relevant weights following
initialization and during optimization; manual adjustment of
<code>init.weights</code> or <code>qrnn.initialize</code> may be needed due to
differences in scaling of the constrained and unconstrained weights.
Non-increasing behaviour can be forced by transforming the relevant
covariates, e.g., by reversing sign.
</p>
<p>The <code>additive</code> argument sets relevant input-hidden layer weights
to zero, resulting in a purely additive model. Interactions between covariates
are thus suppressed, leading to a compromise in flexibility between
linear quantile regression and the quantile regression neural network.
</p>
<p>Borrowing strength by using a composite model for multiple regression quantiles
is also possible (see <code><a href="#topic+composite.stack">composite.stack</a></code>). Applying the monotone
constraint in combination with the composite model allows
one to simultaneously estimate multiple non-crossing quantiles;
the resulting monotone composite QRNN (MCQRNN) is demonstrated in
<code><a href="#topic+mcqrnn">mcqrnn</a></code>.
</p>
<p>In the linear case, model complexity does not depend on the number
of hidden nodes; the value of <code>n.hidden</code> is ignored and is instead
set to one internally. In the nonlinear case, <code>n.hidden</code>
controls the overall complexity of the model. As an added means of
avoiding overfitting, weight penalty regularization for the magnitude
of the input-hidden layer weights (excluding biases) can be applied
by setting <code>penalty</code> to a nonzero value. (For the linear model,
this penalizes both input-hidden and hidden-output layer weights,
leading to a quantile ridge regression model. In this case, kernel
quantile ridge regression can be performed with the aid of the
<code><a href="#topic+qrnn.rbf">qrnn.rbf</a></code> function.) Finally, if the <code>bag</code> argument
is set to <code>TRUE</code>, models are trained on bootstrapped <code>x</code> and
<code>y</code> sample pairs; bootstrap aggregation (bagging) can be turned
on by setting <code>n.ensemble</code> to a value greater than one. Averaging
over an ensemble of bagged models will also tend to alleviate
overfitting.
</p>
<p>The <code><a href="#topic+gam.style">gam.style</a></code> function can be used to plot modified
generalized additive model effects plots, which are useful for visualizing
the modelled covariate-response relationships.
</p>
<p>Note: values of <code>x</code> and <code>y</code> need not be standardized or
rescaled by the user. All variables are automatically scaled to zero
mean and unit standard deviation prior to fitting and parameters are
automatically rescaled by <code><a href="#topic+qrnn.predict">qrnn.predict</a></code> and other prediction
functions. Values of <code>eps.seq</code> are relative to the residuals in
standard deviation units. Note: scaling of <code>y</code> can be turned off using
the <code>scale.y</code> argument.
</p>


<h3>Value</h3>

<p>a list containing elements
</p>
<table>
<tr><td><code>weights</code></td>
<td>
<p>a list containing fitted weight matrices</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p>left censoring point</p>
</td></tr>
<tr><td><code>eps.seq</code></td>
<td>
<p>sequence of <code>eps</code> values for the finite smoothing algorithm</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>desired tau-quantile(s)</p>
</td></tr>
<tr><td><code>Th</code></td>
<td>
<p>hidden layer transfer function</p>
</td></tr>
<tr><td><code>x.center</code></td>
<td>
<p>vector of column means for <code>x</code></p>
</td></tr>
<tr><td><code>x.scale</code></td>
<td>
<p>vector of column standard deviations for <code>x</code></p>
</td></tr>
<tr><td><code>y.center</code></td>
<td>
<p>vector of column means for <code>y</code></p>
</td></tr>
<tr><td><code>y.scale</code></td>
<td>
<p>vector of column standard deviations for <code>y</code></p>
</td></tr>
<tr><td><code>monotone</code></td>
<td>
<p>column indices indicating covariate monotonicity constraints.</p>
</td></tr>
<tr><td><code>additive</code></td>
<td>
<p>force additive relationships.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Cannon, A.J., 2011. Quantile regression neural networks: implementation
in R and application to precipitation downscaling. Computers &amp; Geosciences,
37: 1277-1284. doi:10.1016/j.cageo.2010.07.005
</p>
<p>Cannon, A.J., 2018. Non-crossing nonlinear regression quantiles by
monotone composite quantile regression neural network, with application
to rainfall extremes. Stochastic Environmental Research and Risk Assessment,
32(11): 3207-3225. doi:10.1007/s00477-018-1573-6
</p>


<h3>See Also</h3>

<p><code><a href="#topic+qrnn.predict">qrnn.predict</a></code>, <code><a href="#topic+qrnn.cost">qrnn.cost</a></code>, <code><a href="#topic+composite.stack">composite.stack</a></code>, <code><a href="#topic+mcqrnn">mcqrnn</a></code>, <code><a href="#topic+gam.style">gam.style</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[,"Petal.Length",drop=FALSE])
y &lt;- as.matrix(iris[,"Petal.Width",drop=FALSE])

cases &lt;- order(x)
x &lt;- x[cases,,drop=FALSE]
y &lt;- y[cases,,drop=FALSE]

tau &lt;- c(0.05, 0.5, 0.95)
 
set.seed(1)

## QRNN models for conditional 5th, 50th, and 95th percentiles
w &lt;- p &lt;- vector("list", length(tau))
for(i in seq_along(tau)){
    w[[i]] &lt;- qrnn.fit(x=x, y=y, n.hidden=3, tau=tau[i],
                       iter.max=200, n.trials=1)
    p[[i]] &lt;- qrnn.predict(x, w[[i]])
}

## Monotone composite QRNN (MCQRNN) for simultaneous estimation of
## multiple non-crossing quantile functions
x.y.tau &lt;- composite.stack(x, y, tau)
fit.mcqrnn &lt;- qrnn.fit(cbind(x.y.tau$tau, x.y.tau$x), x.y.tau$y,
                       tau=x.y.tau$tau, n.hidden=3, n.trials=1,
                       iter.max=500, monotone=1)
pred.mcqrnn &lt;- matrix(qrnn.predict(cbind(x.y.tau$tau, x.y.tau$x),
                      fit.mcqrnn), ncol=length(tau))

par(mfrow=c(1, 2))
matplot(x, matrix(unlist(p), nrow=nrow(x), ncol=length(p)), col="red",
        type="l")
points(x, y)
matplot(x, pred.mcqrnn, col="blue", type="l")
points(x, y)

</code></pre>

<hr>
<h2 id='qrnn.initialize'>
Initialize a QRNN weight vector
</h2><span id='topic+qrnn.initialize'></span>

<h3>Description</h3>

<p>Random initialization of the weight vector used during fitting of a QRNN model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qrnn.initialize(x, y, n.hidden, init.range=c(-0.5, 0.5, -0.5, 0.5))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qrnn.initialize_+3A_x">x</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of samples and number of columns equal to the number of variables.
</p>
</td></tr>
<tr><td><code id="qrnn.initialize_+3A_y">y</code></td>
<td>

<p>response column matrix with number of rows equal to the number of samples.
</p>
</td></tr>
<tr><td><code id="qrnn.initialize_+3A_n.hidden">n.hidden</code></td>
<td>

<p>number of hidden nodes in the QRNN model.
</p>
</td></tr>
<tr><td><code id="qrnn.initialize_+3A_init.range">init.range</code></td>
<td>

<p>initial weight range for input-hidden and hidden-output weight matrices.
</p>
</td></tr>
</table>

<hr>
<h2 id='qrnn.predict'>
Evaluate quantiles from trained QRNN model
</h2><span id='topic+qrnn.predict'></span>

<h3>Description</h3>

<p>Evaluate a fitted QRNN model or ensemble of models, resulting in a list
containing the predicted quantiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qrnn.predict(x, parms)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qrnn.predict_+3A_x">x</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of samples and number of columns equal to the number of variables.
</p>
</td></tr>
<tr><td><code id="qrnn.predict_+3A_parms">parms</code></td>
<td>

<p>list containing QRNN input-hidden and hidden-output layer weight matrices and other parameters from <code><a href="#topic+qrnn.fit">qrnn.fit</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with number of elements equal to that of <code>parms</code>, each containing a column matrix of predicted quantiles.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+qrnn.fit">qrnn.fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[,"Petal.Length",drop=FALSE])
y &lt;- as.matrix(iris[,"Petal.Width",drop=FALSE])

cases &lt;- order(x)
x &lt;- x[cases,,drop=FALSE]
y &lt;- y[cases,,drop=FALSE]
y[y &lt; 0.5] &lt;- 0.5

set.seed(1)
parms &lt;- qrnn.fit(x=x, y=y, n.hidden=3, tau=0.5, lower=0.5,
                  iter.max=500, n.trials=1)
p &lt;- qrnn.predict(x=x, parms=parms)

matplot(x, cbind(y, p), type=c("p", "l"), pch=1, lwd=1)
</code></pre>

<hr>
<h2 id='qrnn.rbf'>
Radial basis function kernel
</h2><span id='topic+qrnn.rbf'></span>

<h3>Description</h3>

<p>Evaluate a kernel matrix based on the radial basis function kernel. Can
be used in conjunction with <code><a href="#topic+qrnn.fit">qrnn.fit</a></code> with <code>Th</code> set to
<code><a href="#topic+linear">linear</a></code> and <code>penalty</code> set to a nonzero value for
kernel quantile ridge regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qrnn.rbf(x, x.basis, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qrnn.rbf_+3A_x">x</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of samples and number of columns equal to the number of variables.
</p>
</td></tr>
<tr><td><code id="qrnn.rbf_+3A_x.basis">x.basis</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of basis functions and number of columns equal to the number of variables.
</p>
</td></tr>
<tr><td><code id="qrnn.rbf_+3A_sigma">sigma</code></td>
<td>

<p>kernel width
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>kernel matrix with number of rows equal to the number of samples and number of columns equal to the number of basis functions.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+qrnn.fit">qrnn.fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[,"Petal.Length",drop=FALSE])
y &lt;- as.matrix(iris[,"Petal.Width",drop=FALSE])

cases &lt;- order(x)
x &lt;- x[cases,,drop=FALSE]
y &lt;- y[cases,,drop=FALSE]

set.seed(1)
kern &lt;- qrnn.rbf(x, x.basis=x, sigma=1)

parms &lt;- qrnn.fit(x=kern, y=y, tau=0.5, penalty=0.1,
                  Th=linear, Th.prime=linear.prime,
                  iter.max=500, n.trials=1)
p &lt;- qrnn.predict(x=kern, parms=parms)

matplot(x, cbind(y, p), type=c("p", "l"), pch=1, lwd=1)
</code></pre>

<hr>
<h2 id='qrnn2'>
Fit and make predictions from QRNN models with two hidden layers
</h2><span id='topic+qrnn2.fit'></span><span id='topic+qrnn2.predict'></span>

<h3>Description</h3>

<p>Functions used to fit and make predictions from QRNN models with two hidden layers.
Note: <code>Th</code> must be a non-decreasing function if <code>monotone != NULL</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qrnn2.fit(x, y, n.hidden=2, n.hidden2=2, w=NULL, tau=0.5,
          n.ensemble=1, iter.max=5000, n.trials=5, bag=FALSE,
          lower=-Inf, init.range=c(-0.5, 0.5, -0.5, 0.5, -0.5, 0.5),
          monotone=NULL, eps.seq=2^seq(-8, -32, by=-4), Th=sigmoid,
          Th.prime=sigmoid.prime, penalty=0, unpenalized=NULL,
          n.errors.max=10, trace=TRUE, method=c("nlm", "adam"),
          scale.y=TRUE, ...)
qrnn2.predict(x, parms)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qrnn2_+3A_x">x</code></td>
<td>

<p>covariate matrix with number of rows equal to the number of samples and number of columns equal to the number of variables.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_y">y</code></td>
<td>

<p>response column matrix with number of rows equal to the number of samples.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_n.hidden">n.hidden</code></td>
<td>

<p>number of hidden nodes in the first hidden layer.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_n.hidden2">n.hidden2</code></td>
<td>

<p>number of hidden nodes in the second hidden layer.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_w">w</code></td>
<td>

<p>vector of weights with length equal to the number of samples;
<code>NULL</code> gives equal weight to each sample.
</p>
</td></tr> 
<tr><td><code id="qrnn2_+3A_tau">tau</code></td>
<td>

<p>desired tau-quantile(s).
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_n.ensemble">n.ensemble</code></td>
<td>

<p>number of ensemble members to fit.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_iter.max">iter.max</code></td>
<td>

<p>maximum number of iterations of the optimization algorithm.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_n.trials">n.trials</code></td>
<td>

<p>number of repeated trials used to avoid local minima.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_bag">bag</code></td>
<td>

<p>logical variable indicating whether or not bootstrap aggregation (bagging) should be used.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_lower">lower</code></td>
<td>

<p>left censoring point.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_init.range">init.range</code></td>
<td>

<p>initial weight range for input-hidden, hidden-hidden, and hidden-output weight matrices.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_monotone">monotone</code></td>
<td>

<p>column indices of covariates for which the monotonicity constraint should hold.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_eps.seq">eps.seq</code></td>
<td>

<p>sequence of <code>eps</code> values for the finite smoothing algorithm.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_th">Th</code></td>
<td>

<p>hidden layer transfer function; use <code><a href="#topic+sigmoid">sigmoid</a></code>, <code><a href="#topic+elu">elu</a></code>, <code><a href="#topic+relu">relu</a></code>, <code><a href="#topic+lrelu">lrelu</a></code>, 
<code><a href="#topic+softplus">softplus</a></code>, or other non-decreasing function for a nonlinear model and <code><a href="#topic+linear">linear</a></code> for a linear model.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_th.prime">Th.prime</code></td>
<td>

<p>derivative of the hidden layer transfer function <code>Th</code>.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_penalty">penalty</code></td>
<td>

<p>weight penalty for weight decay regularization.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_unpenalized">unpenalized</code></td>
<td>

<p>column indices of covariates for which the weight penalty should not be applied to input-hidden layer weights.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_n.errors.max">n.errors.max</code></td>
<td>

<p>maximum number of <code>nlm</code> optimization failures allowed before quitting.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_trace">trace</code></td>
<td>

<p>logical variable indicating whether or not diagnostic messages are printed during optimization.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_method">method</code></td>
<td>

<p>character string indicating which optimization algorithm to use.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_scale.y">scale.y</code></td>
<td>

<p>logical variable indicating whether <code>y</code> should be scaled to zero mean and unit standard deviation.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_...">...</code></td>
<td>

<p>additional parameters passed to the <code><a href="stats.html#topic+nlm">nlm</a></code> or <code><a href="#topic+adam">adam</a></code> optimization routines.
</p>
</td></tr>
<tr><td><code id="qrnn2_+3A_parms">parms</code></td>
<td>

<p>list containing QRNN weight matrices and other parameters from <code><a href="#topic+qrnn2.fit">qrnn2.fit</a></code>.
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Cannon, A.J., 2011. Quantile regression neural networks: implementation
in R and application to precipitation downscaling. Computers &amp; Geosciences,
37: 1277-1284. doi:10.1016/j.cageo.2010.07.005
</p>
<p>Cannon, A.J., 2018. Non-crossing nonlinear regression quantiles by
monotone composite quantile regression neural network, with application
to rainfall extremes. Stochastic Environmental Research and Risk Assessment,
32(11): 3207-3225. doi:10.1007/s00477-018-1573-6
</p>


<h3>See Also</h3>

<p><code><a href="#topic+qrnn.fit">qrnn.fit</a></code>, <code><a href="#topic+qrnn.predict">qrnn.predict</a></code>,
<code><a href="#topic+qrnn.cost">qrnn.cost</a></code>, <code><a href="#topic+composite.stack">composite.stack</a></code>,
<code><a href="#topic+mcqrnn">mcqrnn</a></code>, <code><a href="#topic+adam">adam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[,"Petal.Length",drop=FALSE])
y &lt;- as.matrix(iris[,"Petal.Width",drop=FALSE])

cases &lt;- order(x)
x &lt;- x[cases,,drop=FALSE]
y &lt;- y[cases,,drop=FALSE]

tau &lt;- c(0.05, 0.5, 0.95)
 
set.seed(1)

## QRNN models w/ 2 hidden layers (tau=0.05, 0.50, 0.95)
w &lt;- p &lt;- vector("list", length(tau))
for(i in seq_along(tau)){
    w[[i]] &lt;- qrnn2.fit(x=x, y=y, n.hidden=3, n.hidden2=3,
                       tau=tau[i], iter.max=200, n.trials=1)
    p[[i]] &lt;- qrnn2.predict(x, w[[i]])
}

## MCQRNN model w/ 2 hidden layers for simultaneous estimation of
## multiple non-crossing quantile functions
x.y.tau &lt;- composite.stack(x, y, tau)
fit.mcqrnn &lt;- qrnn2.fit(cbind(x.y.tau$tau, x.y.tau$x), x.y.tau$y,
                        tau=x.y.tau$tau, n.hidden=3, n.hidden2=3,
                        n.trials=1, iter.max=500, monotone=1)
pred.mcqrnn &lt;- matrix(qrnn2.predict(cbind(x.y.tau$tau, x.y.tau$x),
                      fit.mcqrnn), ncol=length(tau))

par(mfrow=c(1, 2))
matplot(x, matrix(unlist(p), nrow=nrow(x), ncol=length(p)), col="red",
        type="l")
points(x, y)
matplot(x, pred.mcqrnn, col="blue", type="l")
points(x, y)

</code></pre>

<hr>
<h2 id='quantile.dtn'>
Interpolated quantile distribution with exponential tails and Nadaraya-Watson quantile distribution
</h2><span id='topic+dquantile'></span><span id='topic+pquantile'></span><span id='topic+qquantile'></span><span id='topic+rquantile'></span><span id='topic+pquantile.nw'></span><span id='topic+qquantile.nw'></span><span id='topic+rquantile.nw'></span>

<h3>Description</h3>

<p><code>dquantile</code> gives a probability density function (pdf) by combining
step-interpolation of probability densities for specified
<code>tau</code>-quantiles (<code>quant</code>) with exponential lower/upper tails
(Quiñonero-Candela, 2006; Cannon, 2011). Point mass (e.g., as might occur
when using censored QRNN models) can be defined by setting <code>lower</code> to
the left censoring point. <code>pquantile</code> gives the cumulative distribution
function (cdf); the <code><a href="stats.html#topic+integrate">integrate</a></code> function is used for values
outside the range of <code>quant</code>. The inverse cdf is given by
<code>qquantile</code>; the <code><a href="stats.html#topic+uniroot">uniroot</a></code> function is used for values
outside the range of <code>tau</code>. <code>rquantile</code> is used for
generating random variates. 
</p>
<p>Alternative formulations (without left censoring) based on the
Nadaraya-Watson estimator <code>[p,q,r]quantile.nw</code> are also provided
(Passow and Donner, 2020).
</p>
<p>Note: these functions have not been extensively tested or optimized and 
should be considered experimental.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dquantile(x, tau, quant, lower=-Inf)
pquantile(q, tau, quant, lower=-Inf, ...)
pquantile.nw(q, tau, quant, h=0.001, ...)
qquantile(p, tau, quant, lower=-Inf,
          tol=.Machine$double.eps^0.25, maxiter=1000,
          range.mult=1.1, max.error=100, ...)
qquantile.nw(p, tau, quant, h=0.001)
rquantile(n, tau, quant, lower=-Inf,
          tol=.Machine$double.eps^0.25, maxiter=1000,
          range.mult=1.1, max.error=100, ...)
rquantile.nw(n, tau, quant, h=0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quantile.dtn_+3A_x">x</code>, <code id="quantile.dtn_+3A_q">q</code></td>
<td>

<p>vector of quantiles.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_p">p</code></td>
<td>

<p>vector of cumulative probabilities.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_n">n</code></td>
<td>

<p>number of random samples.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_tau">tau</code></td>
<td>

<p>ordered vector of cumulative probabilities associated with <code>quant</code> argument.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_quant">quant</code></td>
<td>

<p>ordered vector of quantiles associated with <code>tau</code> argument.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_lower">lower</code></td>
<td>

<p>left censoring point.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_tol">tol</code></td>
<td>

<p>tolerance passed to <code><a href="stats.html#topic+uniroot">uniroot</a></code>.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_h">h</code></td>
<td>

<p>bandwidth for Nadaraya-Watson kernel.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_maxiter">maxiter</code></td>
<td>

<p>maximum number of iterations passed to <code><a href="stats.html#topic+uniroot">uniroot</a></code>.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_range.mult">range.mult</code></td>
<td>

<p>values of <code>lower</code> and <code>upper</code> in <code><a href="stats.html#topic+uniroot">uniroot</a></code> are initialized to <br /> <code>quant[1]-range.mult*diff(range(quant))</code> and <br /> <code>quant[length(quant)]+range.mult*diff(range(quant))</code> respectively; <code>range.mult</code> is squared, <code>lower</code> and <code>upper</code> are recalculated, and <code><a href="stats.html#topic+uniroot">uniroot</a></code> is rerun if the current values lead to an exception.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_max.error">max.error</code></td>
<td>

<p>maximum number of <code>uniroot</code> errors allowed before termination.
</p>
</td></tr>
<tr><td><code id="quantile.dtn_+3A_...">...</code></td>
<td>

<p>additional arguments passed to <code><a href="stats.html#topic+integrate">integrate</a></code> or <code><a href="stats.html#topic+uniroot">uniroot</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>dquantile</code> gives the pdf, <code>pquantile</code> gives the cdf,
<code>qquantile</code> gives the inverse cdf (or quantile function), and
<code>rquantile</code> generates random deviates.
</p>


<h3>References</h3>

<p>Cannon, A.J., 2011. Quantile regression neural networks: implementation
in R and application to precipitation downscaling. Computers &amp; Geosciences,
37: 1277-1284. doi:10.1016/j.cageo.2010.07.005
</p>
<p>Passow, C., R.V. Donner, 2020. Regression-based distribution mapping for
bias correction of climate model outputs using linear quantile regression.
Stochastic Environmental Research and Risk Assessment, 34:87-102.
doi:10.1007/s00477-019-01750-7
</p>
<p>Quiñonero-Candela, J., C. Rasmussen, F. Sinz, O. Bousquet,
B. Scholkopf, 2006. Evaluating Predictive Uncertainty Challenge.
Lecture Notes in Artificial Intelligence, 3944: 1-27.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+integrate">integrate</a></code>, <code><a href="stats.html#topic+uniroot">uniroot</a></code>, <code><a href="#topic+qrnn.predict">qrnn.predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Normal distribution
tau &lt;- c(0.01, seq(0.05, 0.95, by=0.05), 0.99)
quant &lt;- qnorm(tau)

x &lt;- seq(-3, 3, length=500)
plot(x, dnorm(x), type="l", col="red", lty=2, lwd=2,
     main="pdf")
lines(x, dquantile(x, tau, quant), col="blue")

q &lt;- seq(-3, 3, length=20)
plot(q, pnorm(q), type="b", col="red", lty=2, lwd=2,
     main="cdf")
lines(q, pquantile(q, tau, quant),
      col="blue")

abline(v=1.96, lty=2)
abline(h=pnorm(1.96), lty=2)
abline(h=pquantile(1.96, tau, quant), lty=3)
abline(h=pquantile.nw(1.96, tau, quant, h=0.01), lty=3)

p &lt;- c(0.001, 0.01, 0.025, seq(0.05, 0.95, by=0.05),
       0.975, 0.99, 0.999)
plot(p, qnorm(p), type="b", col="red", lty=2, lwd=2,
     main="inverse cdf")
lines(p, qquantile(p, tau, quant), col="blue")

## Distribution with point mass at zero
tau.0 &lt;- c(0.3, 0.5, 0.7, 0.8, 0.9)
quant.0 &lt;- c(0, 5, 7, 15, 20)

r.0 &lt;- rquantile(500, tau=tau.0, quant=quant.0, lower=0)
x.0 &lt;- seq(0, 40, by=0.5)
d.0 &lt;- dquantile(x.0, tau=tau.0, quant=quant.0, lower=0)
p.0 &lt;- pquantile(x.0, tau=tau.0, quant=quant.0, lower=0)
q.0 &lt;- qquantile(p.0, tau=tau.0, quant=quant.0, lower=0)

par(mfrow=c(2, 2))
plot(r.0, pch=20, main="random")
plot(x.0, d.0, type="b", col="red", main="pdf")
plot(x.0, p.0, type="b", col="blue", ylim=c(0, 1),
     main="cdf")
plot(p.0, q.0, type="b", col="green", xlim=c(0, 1),
     main="inverse cdf")
</code></pre>

<hr>
<h2 id='tilted.abs'>
Tilted absolute value function
</h2><span id='topic+tilted.abs'></span>

<h3>Description</h3>

<p>Tilted absolute value function. Also known as the check function, hinge function, or the pinball loss function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tilted.abs(x, tau)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tilted.abs_+3A_x">x</code></td>
<td>

<p>numeric vector.
</p>
</td></tr>
<tr><td><code id="tilted.abs_+3A_tau">tau</code></td>
<td>

<p>desired tau-quantile.
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+tilted.approx">tilted.approx</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(-2, 2, length=200)
plot(x, tilted.abs(x, tau=0.75), type="l")
</code></pre>

<hr>
<h2 id='transfer'>
Transfer functions and their derivatives
</h2><span id='topic+sigmoid'></span><span id='topic+sigmoid.prime'></span><span id='topic+elu'></span><span id='topic+elu.prime'></span><span id='topic+softplus'></span><span id='topic+softplus.prime'></span><span id='topic+logistic'></span><span id='topic+logistic.prime'></span><span id='topic+lrelu'></span><span id='topic+lrelu.prime'></span><span id='topic+relu'></span><span id='topic+relu.prime'></span><span id='topic+linear'></span><span id='topic+linear.prime'></span><span id='topic+softmax'></span>

<h3>Description</h3>

<p>The <code>sigmoid</code>, exponential linear <code>elu</code>, <code>softplus</code>,
<code>lrelu</code>, and <code>relu</code> functions can be used as the hidden layer
transfer function for a nonlinear QRNN model. <code>sigmoid</code> is
used by default. The <code>linear</code> function is used as the
hidden layer transfer function for linear QRNN models.
<code>sigmoid.prime</code>, <code>elu.prime</code>, <code>softplus.prime</code>,
<code>lrelu.prime</code>, <code>relu.prime</code>, and <code>linear.prime</code>
provide the corresponding derivatives.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sigmoid(x)
sigmoid.prime(x)
elu(x, alpha=1)
elu.prime(x, alpha=1)
softplus(x, alpha=2)
softplus.prime(x, alpha=2)
logistic(x)
logistic.prime(x)
lrelu(x)
lrelu.prime(x)
relu(x)
relu.prime(x)
linear(x)
linear.prime(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transfer_+3A_x">x</code></td>
<td>

<p>numeric vector.
</p>
</td></tr>
<tr><td><code id="transfer_+3A_alpha">alpha</code></td>
<td>

<p>transition parameter for <code>elu</code> and <code>softplus</code> functions.
</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(-10, 10, length=100)
plot(x, sigmoid(x), type="l", col="black", ylab="")
lines(x, sigmoid.prime(x), lty=2, col="black")
lines(x, elu(x), col="red")
lines(x, elu.prime(x), lty=2, col="red")
lines(x, softplus(x), col="blue")
lines(x, softplus.prime(x), lty=2, col="blue")
lines(x, logistic(x), col="brown")
lines(x, logistic.prime(x), lty=2, col="brown")
lines(x, lrelu(x), col="orange")
lines(x, lrelu.prime(x), lty=2, col="orange")
lines(x, relu(x), col="pink")
lines(x, relu.prime(x), lty=2, col="pink")
lines(x, linear(x), col="green")
lines(x, linear.prime(x), lty=2, col="green")
</code></pre>

<hr>
<h2 id='YVRprecip'>
Daily precipitation data at Vancouver Int'l Airport (YVR)
</h2><span id='topic+YVRprecip'></span>

<h3>Description</h3>

<p>Daily precipitation totals (mm) at Vancouver Int'l Airport (YVR) for
the period 1971-2000.
</p>
<p>Covariates for a simple downscaling task include daily sea-level
pressures (Pa), 700-hPa specific humidities (kg/kg), and 500-hPa
geopotential heights (m) from the NCEP/NCAR Reanalysis
(Kalnay et al., 1996) grid point centered on 50 deg. N and 237.5 deg. E.
</p>
<p>NCEP/NCAR Reanalysis data provided by the NOAA/OAR/ESRL PSD, Boulder,
Colorado, USA, from their Web site at <a href="https://psl.noaa.gov/">https://psl.noaa.gov/</a>.
</p>


<h3>References</h3>

<p>Kalnay, E. et al., 1996. The NCEP/NCAR 40-year reanalysis project,
Bulletin of the American Meteorological Society, 77: 437-470. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## YVR precipitation data with seasonal cycle and NCEP/NCAR Reanalysis
## covariates

data(YVRprecip)
y &lt;- YVRprecip$precip
x &lt;- cbind(sin(2*pi*seq_along(y)/365.25),
           cos(2*pi*seq_along(y)/365.25),
           YVRprecip$ncep)

## Fit QRNN and quantile regression models for the conditional 75th
## percentile using the final 3 years of record for training and the
## remaining years for testing.
train &lt;- as.numeric(format(YVRprecip$date, "%Y")) &gt;= 1998
test &lt;- !train

set.seed(1)
w.qrnn &lt;- qrnn.fit(x=x[train,], y=y[train,,drop=FALSE],
                   n.hidden=1, tau=0.75, iter.max=200,
                   n.trials=1, lower=0)
p.qrnn &lt;- qrnn.predict(x=x[test,], parms=w.qrnn)
w.qreg &lt;- qrnn.fit(x=x[train,], y=y[train,,drop=FALSE],
                   tau=0.75, n.trials=1, lower=0,
                   Th=linear, Th.prime=linear.prime)
p.qreg &lt;- qrnn.predict(x=x[test,], parms=w.qreg)

## Tilted absolute value cost function on test dataset
qvs.qrnn &lt;- mean(tilted.abs(y[test]-p.qrnn, 0.75))
qvs.qreg &lt;- mean(tilted.abs(y[test]-p.qreg, 0.75))
cat("Cost QRNN", qvs.qrnn, "\n")
cat("Cost QREG", qvs.qreg, "\n")

## Plot first year of test dataset
plot(y[test][1:365], type="h", xlab="Day", ylab="Precip. (mm)")
points(p.qrnn[1:365], col="red", pch=19)
points(p.qreg[1:365], col="blue", pch=19)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
