<!DOCTYPE html><html><head><title>Help for package regress</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {regress}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#regress'><p>Fit a Gaussian Linear Model with Linear Covariance Structure</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.3-21</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-06-17</td>
</tr>
<tr>
<td>Title:</td>
<td>Gaussian Linear Models with Linear Covariance Structure</td>
</tr>
<tr>
<td>Author:</td>
<td>David Clifford [aut],
  Peter McCullagh [aut],
  HJ Auinger [ctb],
  Karl W Broman <a href="https://orcid.org/0000-0002-4914-6671"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Karl W Broman &lt;broman@wisc.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions to fit Gaussian linear model by maximising the
        residual log likelihood where the covariance structure can be
        written as a linear combination of known matrices.  Can be used
        for multivariate models and random effects models.  Easy
        straight forward manner to specify random effects models,
        including random interactions. Code now optimised to use
        Sherman Morrison Woodbury identities for matrix inversion in
        random effects models. We've added the ability to fit models
        using any kernel as well as a function to return the mean and
        covariance of random effects conditional on the data (best
        linear unbiased predictors, BLUPs).
        Clifford and McCullagh (2006)
        <a href="https://www.r-project.org/doc/Rnews/Rnews_2006-2.pdf">https://www.r-project.org/doc/Rnews/Rnews_2006-2.pdf</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/kbroman/regress">https://github.com/kbroman/regress</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/kbroman/regress/issues">https://github.com/kbroman/regress/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>nlme, MASS</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-06-17 13:39:03 UTC; kbroman</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-06-18 17:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='regress'>Fit a Gaussian Linear Model with Linear Covariance Structure</h2><span id='topic+regress'></span><span id='topic+print.regress'></span><span id='topic+summary.regress'></span><span id='topic+BLUP'></span>

<h3>Description</h3>

<p>Fits Gaussian linear models in which the covariance
structure can be expressed as a linear combination of known matrices.
For example, random effects, block effects models and spatial models
that include a nugget effect.  Fits model by maximising the
log-likelihood of the model. The choice of kernel affects which
likelihood and by default it is the REML log likelihood or restricted
log likelihood but the ordinary log-likelihood is also possible. The
regress algorithm uses a Newton-Raphson algorithm to locate the
maximum of the log-likelihood surface. Some computational efficiencies
are achieved when all variance components are associated with
factors. In such a random effects model the matrix inversion is
computed using the Sherman-Morrison-Woodbury identities.</p>


<h3>Usage</h3>

<pre><code class='language-R'>regress(formula, Vformula, identity=TRUE, kernel=NULL,
                    start=NULL, taper=NULL, pos, verbose=0, gamVals=NULL,
                    maxcyc=50, tol=1e-4, data)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regress_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of the model to be fitted. The
details of model specification are the same as for <code>lm</code></p>
</td></tr>
<tr><td><code id="regress_+3A_vformula">Vformula</code></td>
<td>
<p>Specifies the matrices to include in the covariance
structure.  Each term is either a symmetric matrix, or a factor.
Independent Gaussian random effects are included by passing the
corresponding block factor.</p>
</td></tr>
<tr><td><code id="regress_+3A_identity">identity</code></td>
<td>
<p>Logical variable, includes the identity as the
final matrix of the covariance structure.  Default is TRUE</p>
</td></tr>
<tr><td><code id="regress_+3A_kernel">kernel</code></td>
<td>
<p>Compute the log likelihood based on a reduced
observation TY where T has this kernel. Default value of NULL assumes
that the kernal matches the fixed effects model matrix X corresponding
to REML. Setting kernel=0 gives the ordinary likelihood and kernel=1
gives the one dimensional subspace of constant vectors. See examples
for more details.</p>
</td></tr>
<tr><td><code id="regress_+3A_start">start</code></td>
<td>
<p>Specify the variance components at which the
Newton-Raphson algorithm starts.  Default value is
<code>rep(var(y),k)</code>.</p>
</td></tr>
<tr><td><code id="regress_+3A_taper">taper</code></td>
<td>
<p>The proportion of each step to take.  A vector of values
from 0 to 1 of length maxcyc. Default value takes smaller steps initially. </p>
</td></tr>
<tr><td><code id="regress_+3A_pos">pos</code></td>
<td>
<p>logical vector of length k, where k is the number of
matrices in the covariance structure.  Indicates which variance
components are positive (TRUE) and which are real (FALSE).
Important for multivariate problems.</p>
</td></tr>
<tr><td><code id="regress_+3A_verbose">verbose</code></td>
<td>
<p>Controls level of time output, takes values 0, 1 or
2, Default is 0, level 1 gives parameter estimates and value of log
likelihood at each stage.</p>
</td></tr>
<tr><td><code id="regress_+3A_gamvals">gamVals</code></td>
<td>
<p>When k=2, the marginal log likelihood based on the
residual configuration statistic (see Tunnicliffe Wilson(1989)), is
evaluated first at <code>(1-gam) V1 + gam V2</code> for each value of
<code>gam</code> in <code>gamVals</code>, a set of values from the unit
interval.  Subsequently the Newton-Raphson algorithm is started at
variance components corresponding the the value of <code>gam</code> that
has the highest marginal log likelihood.  This is overridden if
<code>start</code> is specified.</p>
</td></tr>
<tr><td><code id="regress_+3A_maxcyc">maxcyc</code></td>
<td>
<p>Maximum number of cycles allowed.  Default value is
50. A warning is output to the screen if this is reached before
convergence.</p>
</td></tr>
<tr><td><code id="regress_+3A_tol">tol</code></td>
<td>
<p>Convergence criteria.  If the change in residual log
likelihood for one cycle is less than <code>10 x tol</code> the algorithm
finishes.  If each component of the change proposed by the
Newton-Raphson is lower in magnitude than <code>tol</code> the algorithm
finishes. Default value is <code>1e-4</code>.</p>
</td></tr>
<tr><td><code id="regress_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from 'environment(formula)',
typically the environment from which 'regress' is called.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As the code is running it can output the variance components, and the
residual log likelihood at each iteration when <code>verbose</code> is
non-zero.
</p>
<p>To avoid confusion over terminology.  I define variance components to
be the multipliers of the matrices and variance parameters to the
parameter space over which the Newton-Raphson algorithm is run.  I can
force a component to be positive be defining the corresponding variance
parameter on the log scale.
</p>
<p>All output to the screen is for variance components (i.e. the
multiples of the matrices).  Values for <code>start</code> are on the
variance component scale.  Use <code>pos</code> to force certain variance
components to be positive.
</p>
<p>NOTE: The final stage of the algorithm converts the estimates of the
variance components and the Fisher Information to the usual linear
scale, i.e. as if <code>pos</code> were a vector of zeroes.
</p>
<p>NOTE: No <code>predict</code> functionality is provided with regress due to
some ambiguity.  Are we predicting conditional on the observed data.
Are we predicting observations from the fitted model itself?  It is
all normal anyway so it is straightforward, see our paper on regress.
</p>
<p>When you fit a Gaussian regression model using fit &lt;- regress(y~X, ~V,
kernel=K) the function computes the log likelihood based on the
reduced observation $TY ~ N(TX, T V T')$, where $T$ is a linear
transformation with kernel $K$.  Only $n-k$ degrees of freedom are
available.  Ordinary likelihood corresponds to $K=0$, and REML to
$K=X$, but these are not the only options.
</p>
<p>When you fit two nested Gaussian models ($X0 subset of X1$ and $V0
subset of V1$) using the commands:
</p>
<p>fit0 &lt;- regress(y~X0, ~V0, kernel=K)
</p>
<p>fit1 &lt;- regress(y~X1, ~V1, kernel=K)
</p>
<p>then the likelihood ratio statistic fit1$llik - fit0$llik is the
ordinary likelihood ratio based on the Gaussian observation $TY$ where
the kernel of T is K.  So if you set kernel=0, you get the ordinary
likelihood ratio based on the complete observation $y$; And if you set
kernel=1, you get the likelihood ratio based on simple contrasts $y_i
- y_j$ only. In the latter case, you have only $n-1$ degrees of
freedom to work with.  And if you set kernel=X0, you get the
likelihood ratio based on contrasts $Ty$ with kernel $X0$, which for
fit0 is the REML likelihood.
</p>
<p>We recommend fitting the models with the &quot;largest&quot; kernel possible.
For example, with models fit0 and fit1 above, we could choose K=0, or
K=X0 to get the desired result. Our experience though is that the
model with K=X0 may be easier to fit with regress compared with a
model where K=0. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>trace</code></td>
<td>
<p>Matrix with one row for each iteration of algorithm.
Each row contains the residual log likelihood, marginal log
likelihood, variance parameters and increments.</p>
</td></tr>
<tr><td><code>llik</code></td>
<td>
<p>Value of the marginal log likelihood at the point
of convergence.</p>
</td></tr>
<tr><td><code>cycle</code></td>
<td>
<p>Number of cycles to convergence.</p>
</td></tr>
<tr><td><code>rdf</code></td>
<td>
<p>Residual degrees of freedom.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>Estimate of the linear effects.</p>
</td></tr>
<tr><td><code>beta.cov</code></td>
<td>
<p>Estimate of the covariance structure for terms in
beta.</p>
</td></tr>
<tr><td><code>beta.se</code></td>
<td>
<p>Standard errors for terms in beta.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Variance component estimates, interpretation does not depend on
value of <code>pos</code></p>
</td></tr>
<tr><td><code>sigma.cov</code></td>
<td>
<p>Covariance matrix for the variance component
estimates based on the Fisher Information at the point of
convergence.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>Inverse of covariance matrix at point of convergence.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>$I - X^T (X^T W X)^-1 X^T W$ at point of convergence.</p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p>$X beta$, the fitted values.</p>
</td></tr>
<tr><td><code>predicted</code></td>
<td>
<p>If <code>identity=TRUE</code>, decompose y into the part
associated with the identity and that assosicated with the rest of the
variance structure, this second part is the predicted values.  If
$Sigma = V1 + V2$ at point of convergence then y = V1 W y + V2 W y is
the decomposition. This is the conditional expectation for new observations conditional on the
observed data.</p>
</td></tr>
<tr><td><code>predictedVariance</code></td>
<td>
<p>Variance of new observations conditional on
the observed data</p>
</td></tr>
<tr><td><code>predictedVariance2</code></td>
<td>
<p>Additional variance associated with the
uncertainty of beta. Can be be added to predictedVariance</p>
</td></tr>
<tr><td><code>pos</code></td>
<td>
<p>Indicator for the scale for each variance parameter.</p>
</td></tr>
<tr><td><code>Vnames</code></td>
<td>
<p>Names associated with each variance component, used in
<code>print.regress</code>.</p>
</td></tr>
<tr><td><code>formula</code></td>
<td>
<p>Copy of formula</p>
</td></tr>
<tr><td><code>Vformula</code></td>
<td>
<p>Updated version of Vformula to include identity if
necessary</p>
</td></tr>
<tr><td><code>Kcolnames</code></td>
<td>
<p>Names associated with the kernel</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>Response, covariates and matrices/factors to be used for
model fitting</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>Design matrices associated with the random effects, used for
computation of BLUPs</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>David Clifford, Peter McCullagh</p>


<h3>References</h3>

<p>G. Tunnicliffe Wilson (1989), &quot;On the use of marginal likelihood in time
series model estimation.&quot;  <em>JRSS B</em>, Vol 51, No 1, 15-27.
</p>
<p>D. Clifford and P. McCullagh (2006), &quot;The regress function&quot; <em>R News</em>
6(2):6-10
</p>
<p>Weisstein, Eric W. &quot;Woodbury Formula.&quot; From MathWorld&ndash;A Wolfram Web
Resource. http://mathworld.wolfram.com/WoodburyFormula.html
</p>
<p>Weisstein, Eric W. &quot;Sherman-Morrison Formula.&quot; From MathWorld&ndash;A
Wolfram Web Resource. http://mathworld.wolfram.com/Sherman-MorrisonFormula.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  ######################
  ## Comparison with lme
  ######################

  ## Example of Random Effects model from Venables and Ripley, page 205
  library(nlme)
  library(regress)

  citation("regress")

  names(Oats) &lt;- c("B","V","N","Y")
  Oats$N &lt;- as.factor(Oats$N)

  ## Using regress
  oats.reg &lt;- regress(Y~N+V,~B+I(B:V),identity=TRUE,verbose=1,data=Oats)
  summary(oats.reg)

  ## Using lme
  oats.lme &lt;- lme(Y~N+V,random=~1|B/V,data=Oats,method="REML")
  summary(oats.lme)

  ## print and summary
  oats.reg
  print(oats.reg)
  summary(oats.reg)

  ranef(oats.lme)
  BLUP(oats.reg)

  rm(oats.reg, oats.lme, Oats)

  #######################
  ## Computation of BLUPs
  #######################

  ex2 &lt;- list()
  ex2 &lt;- within(ex2,{

    ## Set up example
    set.seed(1001)
    n &lt;- 101
    x1 &lt;- runif(n)
    x2 &lt;- seq(0,1,l=n)
    z1 &lt;- gl(4,10,n)
    z2 &lt;- gl(6,1,n)

    X &lt;- model.matrix(~1 + x1 + x2)
    Z1 &lt;- model.matrix(~z1-1)
    Z2 &lt;- model.matrix(~z2-1)

    ## Create the individual random and fixed effects
    beta &lt;- c(1,2,3)
    eta1 &lt;- rnorm(ncol(Z1),0,10)
    eta2 &lt;- rnorm(ncol(Z2),0,10)
    eps &lt;- rnorm(n,0,3)

    ## Combine them into a response
    y &lt;- X %*% beta + Z1 %*% eta1 + Z2 %*% eta2 + eps
  })

  ## Data frame containing all we need for model fitting
  regressDF &lt;- with(ex2,data.frame(y,x1,x2,z1,z2))
  rm(ex2)

  ## Fit the model using regress
  regress.output &lt;- regress(y~1 + x1 + x2,~z1 + z2,data=regressDF)

  summary(regress.output)

  blup1 &lt;- BLUP(regress.output,RE="z1")
  blup1$Mean
  blup1$Variance
  blup1$Covariance
  cov2cor(blup1$Covariance) ## Large correlation terms

  blup2 &lt;- BLUP(regress.output) ## Joint BLUP of z1 and z2 by default
  blup2$Mean
  blup2$Variance
  cov2cor(blup2$Covariance)  ## Strong negative correlation between BLUPs
                             ## for z1 and z2

  rm(blup1,blup2)

  ############################
  ## Examples of use of kernel
  ############################

  ## REML LRT for x2 which will be 0 as x2 lies in the kernel
  with(regressDF,{
       K &lt;- model.matrix(~1+x1+x2)
       model1 &lt;- regress(y~1+x1,~z1,kernel=K)
       model2 &lt;- regress(y~1+x1+x2,~z1,kernel=K)
       2*(model2$llik - model1$llik)
  })

  ## LRT for x2 using ordinary likelihood
  with(regressDF,{
       K &lt;- 0
       model1 &lt;- regress(y~1+x1,~z1,kernel=K)
       model2 &lt;- regress(y~1+x1+x2,~z1,kernel=K)
       2*(model2$llik - model1$llik)
  })

  ## LRT for x2 based on a reduced observation TY with kernel K. This
  ## LRT is approximately equal to the last one, and numerically this
  ## turns out to be the case also.
  with(regressDF,{
       K &lt;- model.matrix(~1+x1)
       model1 &lt;- regress(y~1+x1,~z1,kernel=K)
       model2 &lt;- regress(y~1+x1+x2,~z1,kernel=K)
       2*(model2$llik - model1$llik)
  })

  ## Two ways to drop out the 17th and 19th observations.
  with(regressDF,{
       n &lt;- length(y)
       K &lt;- matrix(0,n,2)
       K[17,1] &lt;- K[19,2] &lt;- 1
       model1 &lt;- regress(y~1+x1,~z1,kernel=K,tol=1e-8)
       drop &lt;- c(17,19)
       model2 &lt;- regress(y[-drop]~1+x1[-drop],~z1[-drop],kernel=0,tol=1e-8)
       print(model1)
       print(model2)
  })

  rm(regressDF, regress.output)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
