<!DOCTYPE html><html lang="en-US"><head><title>Help for package dsos</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dsos}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dsos-package'><p>dsos: Dataset Shift with Outlier Scores</p></a></li>
<li><a href='#as_bf'><p>Convert P-value to Bayes Factor</p></a></li>
<li><a href='#as_pvalue'><p>Convert Bayes Factor to P-value</p></a></li>
<li><a href='#at_from_os'><p>Asymptotic Test from Outlier Scores</p></a></li>
<li><a href='#at_oob'><p>Asymptotic Test With Out-Of-Bag Scores</p></a></li>
<li><a href='#bf_compare'><p>Bayesian and Frequentist Test from Outlier Scores</p></a></li>
<li><a href='#bf_from_os'><p>Bayesian Test from Outlier Scores</p></a></li>
<li><a href='#plot.outlier.bayes'><p>Plot Bayesian test for no adverse shift.</p></a></li>
<li><a href='#plot.outlier.test'><p>Plot frequentist test for no adverse shift.</p></a></li>
<li><a href='#print.outlier.bayes'><p>Print Bayesian test for no adverse shift.</p></a></li>
<li><a href='#print.outlier.test'><p>Print frequentist test for no adverse shift.</p></a></li>
<li><a href='#pt_from_os'><p>Permutation Test from Outlier Scores</p></a></li>
<li><a href='#pt_oob'><p>Permutation Test With Out-Of-Bag Scores</p></a></li>
<li><a href='#pt_refit'><p>Permutation Test By Refitting</p></a></li>
<li><a href='#wauc_from_os'><p>Weighted AUC from Outlier Scores</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Dataset Shift with Outlier Scores</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Test for no adverse shift in two-sample comparison when we
    have a training set, the reference distribution, and a test set. The
    approach is flexible and relies on a robust and powerful test
    statistic, the weighted AUC. Technical details are in Kamulete, V. M.
    (2021) &lt;<a href="https://doi.org/10.48550/arXiv.1908.04000">doi:10.48550/arXiv.1908.04000</a>&gt;. Modern notions of outlyingness such as
    trust scores and prediction uncertainty can be used as the underlying
    scores for example.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/vathymut/dsos">https://github.com/vathymut/dsos</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/vathymut/dsos/issues">https://github.com/vathymut/dsos/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table (&ge; 1.14.6), future.apply (&ge; 1.10.0), ggplot2 (&ge;
3.4.0), scales (&ge; 1.2.1), simctest (&ge; 2.6), stats (&ge; 4.2.1)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>fdrtool (&ge; 1.2.17), knitr (&ge; 1.42), rmarkdown (&ge; 2.20),
testthat (&ge; 3.1.6)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-02-19 04:23:47 UTC; vathy</td>
</tr>
<tr>
<td>Author:</td>
<td>Vathy M. Kamulete <a href="https://orcid.org/0000-0002-4451-3743"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Royal Bank of Canada (RBC) [cph] (Research supported and funded by RBC)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Vathy M. Kamulete &lt;vathymut@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-02-19 07:30:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='dsos-package'>dsos: Dataset Shift with Outlier Scores</h2><span id='topic+dsos'></span><span id='topic+dsos-package'></span>

<h3>Description</h3>

<p>Test for no adverse shift in two-sample comparison when we have a training set, the reference distribution, and a test set. The approach is flexible and relies on a robust and powerful test statistic, the weighted AUC. Technical details are in Kamulete, V. M. (2021) <a href="https://arxiv.org/abs/1908.04000">arXiv:1908.04000</a>. Modern notions of outlyingness such as trust scores and prediction uncertainty can be used as the underlying scores for example.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Vathy M. Kamulete <a href="mailto:vathymut@gmail.com">vathymut@gmail.com</a> (<a href="https://orcid.org/0000-0002-4451-3743">ORCID</a>)
</p>
<p>Other contributors:
</p>

<ul>
<li><p>  Royal Bank of Canada (RBC) (Research supported and funded by RBC) [copyright holder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/vathymut/dsos">https://github.com/vathymut/dsos</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/vathymut/dsos/issues">https://github.com/vathymut/dsos/issues</a>
</p>
</li></ul>


<hr>
<h2 id='as_bf'>Convert P-value to Bayes Factor</h2><span id='topic+as_bf'></span>

<h3>Description</h3>

<p>Convert P-value to Bayes Factor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_bf(pvalue)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_bf_+3A_pvalue">pvalue</code></td>
<td>
<p>P-value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Bayes Factor (scalar value).
</p>


<h3>References</h3>

<p>Marsman, M., &amp; Wagenmakers, E. J. (2017).
<em>Three insights from a Bayesian interpretation of the one-sided P value</em>.
Educational and Psychological Measurement, 77(3), 529-539.
</p>


<h3>See Also</h3>

<p>[as_pvalue()] to convert Bayes factor to p-value.
</p>
<p>Other bayesian-test: 
<code><a href="#topic+as_pvalue">as_pvalue</a>()</code>,
<code><a href="#topic+bf_compare">bf_compare</a>()</code>,
<code><a href="#topic+bf_from_os">bf_from_os</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dsos)
bf_from_pvalue &lt;- as_bf(pvalue = 0.5)
bf_from_pvalue


</code></pre>

<hr>
<h2 id='as_pvalue'>Convert Bayes Factor to P-value</h2><span id='topic+as_pvalue'></span>

<h3>Description</h3>

<p>Convert Bayes Factor to P-value
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_pvalue(bf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_pvalue_+3A_bf">bf</code></td>
<td>
<p>Bayes factor.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>p-value (scalar value).
</p>


<h3>References</h3>

<p>Marsman, M., &amp; Wagenmakers, E. J. (2017).
<em>Three insights from a Bayesian interpretation of the one-sided P value</em>.
Educational and Psychological Measurement, 77(3), 529-539.
</p>


<h3>See Also</h3>

<p>[as_bf()] to convert p-value to Bayes factor.
</p>
<p>Other bayesian-test: 
<code><a href="#topic+as_bf">as_bf</a>()</code>,
<code><a href="#topic+bf_compare">bf_compare</a>()</code>,
<code><a href="#topic+bf_from_os">bf_from_os</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dsos)
pvalue_from_bf &lt;- as_pvalue(bf = 1)
pvalue_from_bf


</code></pre>

<hr>
<h2 id='at_from_os'>Asymptotic Test from Outlier Scores</h2><span id='topic+at_from_os'></span>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training set, <code>x_train</code> as the
as the reference. The method checks whether the test set, <code>x_test</code>, is
worse off relative to this reference set. The function <code>scorer</code> assigns
an outlier score to each instance/observation in both training and test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>at_from_os(os_train, os_test)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="at_from_os_+3A_os_train">os_train</code></td>
<td>
<p>Outlier scores in training (reference) set.</p>
</td></tr>
<tr><td><code id="at_from_os_+3A_os_test">os_test</code></td>
<td>
<p>Outlier scores in test set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Li and Fine (2010) derives the asymptotic null distribution for the weighted
AUC (WAUC), the test statistic. This approach does not use permutations
and can, as a result, be much faster because it sidesteps the need to refit
the scoring function <code>scorer</code>. This works well for large samples. The
prefix <em>at</em> stands for asymptotic test to tell it apart from the
prefix <em>pt</em>, the permutation test.
</p>


<h3>Value</h3>

<p>A named list of class <code>outlier.test</code> containing:
</p>

<ul>
<li> <p><code>statistic</code>: observed WAUC statistic
</p>
</li>
<li> <p><code>seq_mct</code>: sequential Monte Carlo test, when applicable
</p>
</li>
<li> <p><code>p_value</code>: p-value
</p>
</li>
<li> <p><code>outlier_scores</code>: outlier scores from training and test set
</p>
</li></ul>



<h3>Notes</h3>

<p>The outlier scores should all mimic out-of-sample behaviour. Mind that the
training scores are not in-sample and thus, biased (overfitted) while the
test scores are out-of-sample. The mismatch &ndash; in-sample versus out-of-sample
scores &ndash; voids the test validity. A simple fix for this is to get the
training scores from an indepedent (fresh) validation set; this follows
the train/validation/test sample splitting convention and the validation set
is effectively the reference set or distribution in this case.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2022).
<em>Test for non-negligible adverse shifts</em>.
In The 38th Conference on Uncertainty in Artificial Intelligence. PMLR.
</p>
<p>Gandy, A. (2009).
<em>Sequential implementation of Monte Carlo tests with uniformly bounded resampling risk</em>.
Journal of the American Statistical Association, 104(488), 1504-1511.
</p>


<h3>See Also</h3>

<p>[at_oob()] for variant requiring a scoring function.
[pt_from_os()] for permutation test with the outlier scores.
</p>
<p>Other asymptotic-test: 
<code><a href="#topic+at_oob">at_oob</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dsos)
set.seed(12345)
os_train &lt;- rnorm(n = 100)
os_test &lt;- rnorm(n = 100)
test_result &lt;- at_from_os(os_train, os_test)
test_result


</code></pre>

<hr>
<h2 id='at_oob'>Asymptotic Test With Out-Of-Bag Scores</h2><span id='topic+at_oob'></span>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training set, <code>x_train</code> as the
as the reference. The method checks whether the test set, <code>x_test</code>, is
worse off relative to this reference set. The function <code>scorer</code> assigns
an outlier score to each instance/observation in both training and test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>at_oob(x_train, x_test, scorer)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="at_oob_+3A_x_train">x_train</code></td>
<td>
<p>Training (reference/validation) sample.</p>
</td></tr>
<tr><td><code id="at_oob_+3A_x_test">x_test</code></td>
<td>
<p>Test sample.</p>
</td></tr>
<tr><td><code id="at_oob_+3A_scorer">scorer</code></td>
<td>
<p>Function which returns a named list with outlier scores from
the training and test sample. The first argument to <code>scorer</code> must be
<code>x_train</code>; the second, <code>x_test</code>. The returned named list contains
two elements: <em>train</em> and <em>test</em>, each of which is a vector of
(outlier) scores. See notes for more information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Li and Fine (2010) derives the asymptotic null distribution for the weighted
AUC (WAUC), the test statistic. This approach does not use permutations
and can, as a result, be much faster because it sidesteps the need to refit
the scoring function <code>scorer</code>. This works well for large samples. The
prefix <em>at</em> stands for asymptotic test to tell it apart from the
prefix <em>pt</em>, the permutation test.
</p>


<h3>Value</h3>

<p>A named list of class <code>outlier.test</code> containing:
</p>

<ul>
<li> <p><code>statistic</code>: observed WAUC statistic
</p>
</li>
<li> <p><code>seq_mct</code>: sequential Monte Carlo test, when applicable
</p>
</li>
<li> <p><code>p_value</code>: p-value
</p>
</li>
<li> <p><code>outlier_scores</code>: outlier scores from training and test set
</p>
</li></ul>



<h3>Notes</h3>

<p>The scoring function, <code>scorer</code>, predicts out-of-bag scores to mimic
out-of-sample behaviour. The suffix <em>oob</em> stands for out-of-bag to
highlight this point. This out-of-bag variant avoids refitting the
underlying algorithm from <code>scorer</code> at every permutation. It can, as a
result, be computationally appealing.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2022).
<em>Test for non-negligible adverse shifts</em>.
In The 38th Conference on Uncertainty in Artificial Intelligence. PMLR.
</p>
<p>Gandy, A. (2009).
<em>Sequential implementation of Monte Carlo tests with uniformly bounded resampling risk</em>.
Journal of the American Statistical Association, 104(488), 1504-1511.
</p>


<h3>See Also</h3>

<p>[pt_oob()] for (faster) p-value approximation via out-of-bag predictions.
[pt_refit()] for (slower) p-value approximation via refitting.
</p>
<p>Other asymptotic-test: 
<code><a href="#topic+at_from_os">at_from_os</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dsos)
set.seed(12345)
data(iris)
setosa &lt;- iris[1:50, 1:4] # Training sample: Species == 'setosa'
versicolor &lt;- iris[51:100, 1:4] # Test sample: Species == 'versicolor'

# Using fake scoring function
scorer &lt;- function(tr, te) list(train=runif(nrow(tr)), test=runif(nrow(te)))
oob_test &lt;- at_oob(setosa, versicolor, scorer = scorer)
oob_test



</code></pre>

<hr>
<h2 id='bf_compare'>Bayesian and Frequentist Test from Outlier Scores</h2><span id='topic+bf_compare'></span>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training (outlier) scores,
<code>os_train</code>, as the reference. The method checks whether the test
scores, <code>os_test</code>, are worse off relative to the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bf_compare(os_train, os_test, threshold = 1/12, n_pt = 4000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bf_compare_+3A_os_train">os_train</code></td>
<td>
<p>Outlier scores in training (reference) set.</p>
</td></tr>
<tr><td><code id="bf_compare_+3A_os_test">os_test</code></td>
<td>
<p>Outlier scores in test set.</p>
</td></tr>
<tr><td><code id="bf_compare_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for adverse shift. Defaults to 1 / 12,
the asymptotic value of the test statistic when the two samples are drawn
from the same distribution.</p>
</td></tr>
<tr><td><code id="bf_compare_+3A_n_pt">n_pt</code></td>
<td>
<p>The number of permutations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This compares the Bayesian to the frequentist approach for convenience.
The Bayesian test mimics 'bf_from_os()' and the frequentist one,
'pt_from_os()'. The Bayesian test computes Bayes factors based on the
asymptotic (defaults to 1/12) and the exchangeable threshold. The latter
calculates the threshold as the median weighted AUC (WAUC) after <code>n_pt</code>
permutations assuming outlier scores are exchangeable. This is recommended
for small samples. The frequentist test converts the one-sided (one-tailed)
p-value to the Bayes factor - see <code>as_bf</code> function.
</p>


<h3>Value</h3>

<p>A list of factors (BF) for 3 different test specifications:
</p>

<ul>
<li> <p><code>frequentist</code>: Frequentist BF.
</p>
</li>
<li> <p><code>bayes_noperm</code>: Bayestion BF test with asymptotic threshold.
</p>
</li>
<li> <p><code>bayes_perm</code>: Bayestion BF with exchangeable threshold.
</p>
</li></ul>



<h3>Notes</h3>

<p>The outlier scores should all mimic out-of-sample behaviour. Mind that the
training scores are not in-sample and thus, biased (overfitted) while the
test scores are out-of-sample. The mismatch &ndash; in-sample versus out-of-sample
scores &ndash; voids the test validity. A simple fix for this is to get the
training scores from an indepedent (fresh) validation set; this follows
the train/validation/test sample splitting convention and the validation set
is effectively the reference set or distribution in this case.
</p>


<h3>See Also</h3>

<p>[bf_from_os()] for bayes factor, the Bayesian test.
[pt_from_os()] for p-value, the frequentist test.
</p>
<p>Other bayesian-test: 
<code><a href="#topic+as_bf">as_bf</a>()</code>,
<code><a href="#topic+as_pvalue">as_pvalue</a>()</code>,
<code><a href="#topic+bf_from_os">bf_from_os</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dsos)
set.seed(12345)
os_train &lt;- rnorm(n = 100)
os_test &lt;- rnorm(n = 100)
bayes_test &lt;- bf_compare(os_train, os_test)
bayes_test
# To run in parallel on local cluster, uncomment the next two lines.
# library(future)
# future::plan(future::multisession)
parallel_test &lt;- bf_compare(os_train, os_test)
parallel_test


</code></pre>

<hr>
<h2 id='bf_from_os'>Bayesian Test from Outlier Scores</h2><span id='topic+bf_from_os'></span>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training (outlier) scores,
<code>os_train</code>, as the reference. The method checks whether the test
scores, <code>os_test</code>, are worse off relative to the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bf_from_os(os_train, os_test, n_pt = 4000, threshold = 1/12)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bf_from_os_+3A_os_train">os_train</code></td>
<td>
<p>Outlier scores in training (reference) set.</p>
</td></tr>
<tr><td><code id="bf_from_os_+3A_os_test">os_test</code></td>
<td>
<p>Outlier scores in test set.</p>
</td></tr>
<tr><td><code id="bf_from_os_+3A_n_pt">n_pt</code></td>
<td>
<p>The number of permutations.</p>
</td></tr>
<tr><td><code id="bf_from_os_+3A_threshold">threshold</code></td>
<td>
<p>Threshold for adverse shift. Defaults to 1 / 12,
the asymptotic value of the test statistic when the two samples are drawn
from the same distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The posterior distribution of the test statistic is based on <code>n_pt</code>
(boostrap) permutations. The method uses the Bayesian bootstrap as a
resampling procedure as in Gu et al (2008). Johnson (2005) shows to
leverage (turn) a test statistic into a Bayes factor. The test statistic
is the weighted AUC (WAUC).
</p>


<h3>Value</h3>

<p>A named list of class <code>outlier.bayes</code> containing:
</p>

<ul>
<li> <p><code>posterior</code>: Posterior distribution of WAUC test statistic
</p>
</li>
<li> <p><code>threshold</code>: WAUC threshold for adverse shift
</p>
</li>
<li> <p><code>adverse_probability</code>: probability of adverse shift
</p>
</li>
<li> <p><code>bayes_factor</code>: Bayes factor
</p>
</li>
<li> <p><code>outlier_scores</code>: outlier scores from training and test set
</p>
</li></ul>



<h3>Notes</h3>

<p>The outlier scores should all mimic out-of-sample behaviour. Mind that the
training scores are not in-sample and thus, biased (overfitted) while the
test scores are out-of-sample. The mismatch &ndash; in-sample versus out-of-sample
scores &ndash; voids the test validity. A simple fix for this is to get the
training scores from an indepedent (fresh) validation set; this follows
the train/validation/test sample splitting convention and the validation set
is effectively the reference set or distribution in this case.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2023).
<em>Are you OK? A Bayesian test for adverse shift</em>.
Manuscript in preparation.
</p>
<p>Johnson, V. E. (2005).
<em>Bayes factors based on test statistics</em>.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(5), 689-701.
</p>
<p>Gu, J., Ghosal, S., &amp; Roy, A. (2008).
<em>Bayesian bootstrap estimation of ROC curve</em>.
Statistics in medicine, 27(26), 5407-5420.
</p>


<h3>See Also</h3>

<p>Other bayesian-test: 
<code><a href="#topic+as_bf">as_bf</a>()</code>,
<code><a href="#topic+as_pvalue">as_pvalue</a>()</code>,
<code><a href="#topic+bf_compare">bf_compare</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dsos)
set.seed(12345)
os_train &lt;- rnorm(n = 100)
os_test &lt;- rnorm(n = 100)
bayes_test &lt;- bf_from_os(os_train, os_test)
bayes_test
# To run in parallel on local cluster, uncomment the next two lines.
# library(future)
# future::plan(future::multisession)
parallel_test &lt;- bf_from_os(os_train, os_test)
parallel_test


</code></pre>

<hr>
<h2 id='plot.outlier.bayes'>Plot Bayesian test for no adverse shift.</h2><span id='topic+plot.outlier.bayes'></span>

<h3>Description</h3>

<p>Plot Bayesian test for no adverse shift.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'outlier.bayes'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.outlier.bayes_+3A_x">x</code></td>
<td>
<p>A <code>outlier.bayes</code> result from test of no adverse shift.</p>
</td></tr>
<tr><td><code id="plot.outlier.bayes_+3A_...">...</code></td>
<td>
<p>Placeholder to be compatible with S3 method <code>plot</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <span class="pkg">ggplot2</span> plot with outlier scores and p-value.
</p>


<h3>See Also</h3>

<p>Other s3-method: 
<code><a href="#topic+plot.outlier.test">plot.outlier.test</a>()</code>,
<code><a href="#topic+print.outlier.bayes">print.outlier.bayes</a>()</code>,
<code><a href="#topic+print.outlier.test">print.outlier.test</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(12345)
os_train &lt;- rnorm(n = 3e2)
os_test &lt;- rnorm(n = 3e2)
test_to_plot &lt;- bf_from_os(os_train, os_test)
plot(test_to_plot)


</code></pre>

<hr>
<h2 id='plot.outlier.test'>Plot frequentist test for no adverse shift.</h2><span id='topic+plot.outlier.test'></span>

<h3>Description</h3>

<p>Plot frequentist test for no adverse shift.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'outlier.test'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.outlier.test_+3A_x">x</code></td>
<td>
<p>A <code>outlier.test</code> result from test of no adverse shift.</p>
</td></tr>
<tr><td><code id="plot.outlier.test_+3A_...">...</code></td>
<td>
<p>Placeholder to be compatible with S3 method <code>plot</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <span class="pkg">ggplot2</span> plot with outlier scores and p-value.
</p>


<h3>See Also</h3>

<p>Other s3-method: 
<code><a href="#topic+plot.outlier.bayes">plot.outlier.bayes</a>()</code>,
<code><a href="#topic+print.outlier.bayes">print.outlier.bayes</a>()</code>,
<code><a href="#topic+print.outlier.test">print.outlier.test</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(12345)
os_train &lt;- rnorm(n = 3e2)
os_test &lt;- rnorm(n = 3e2)
test_to_plot &lt;- at_from_os(os_train, os_test)
# Also: pt_from_os(os_train, os_test) for permutation test
plot(test_to_plot)


</code></pre>

<hr>
<h2 id='print.outlier.bayes'>Print Bayesian test for no adverse shift.</h2><span id='topic+print.outlier.bayes'></span>

<h3>Description</h3>

<p>Print Bayesian test for no adverse shift.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'outlier.bayes'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.outlier.bayes_+3A_x">x</code></td>
<td>
<p>A <code>outlier.test</code> object from a D-SOS test.</p>
</td></tr>
<tr><td><code id="print.outlier.bayes_+3A_...">...</code></td>
<td>
<p>Placeholder to be compatible with S3 method <code>plot</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Print to screen: display Bayes factor and other information.
</p>


<h3>See Also</h3>

<p>Other s3-method: 
<code><a href="#topic+plot.outlier.bayes">plot.outlier.bayes</a>()</code>,
<code><a href="#topic+plot.outlier.test">plot.outlier.test</a>()</code>,
<code><a href="#topic+print.outlier.test">print.outlier.test</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(12345)
os_train &lt;- rnorm(n = 3e2)
os_test &lt;- rnorm(n = 3e2)
test_to_print &lt;- bf_from_os(os_train, os_test)
test_to_print


</code></pre>

<hr>
<h2 id='print.outlier.test'>Print frequentist test for no adverse shift.</h2><span id='topic+print.outlier.test'></span>

<h3>Description</h3>

<p>Print frequentist test for no adverse shift.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'outlier.test'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.outlier.test_+3A_x">x</code></td>
<td>
<p>A <code>outlier.test</code> object from a D-SOS test.</p>
</td></tr>
<tr><td><code id="print.outlier.test_+3A_...">...</code></td>
<td>
<p>Placeholder to be compatible with S3 method <code>plot</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Print to screen: display p-value and other information.
</p>


<h3>See Also</h3>

<p>Other s3-method: 
<code><a href="#topic+plot.outlier.bayes">plot.outlier.bayes</a>()</code>,
<code><a href="#topic+plot.outlier.test">plot.outlier.test</a>()</code>,
<code><a href="#topic+print.outlier.bayes">print.outlier.bayes</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(12345)
os_train &lt;- rnorm(n = 3e2)
os_test &lt;- rnorm(n = 3e2)
test_to_print &lt;- at_from_os(os_train, os_test)
# Also: pt_from_os(os_train, os_test) for permutation test
test_to_print


</code></pre>

<hr>
<h2 id='pt_from_os'>Permutation Test from Outlier Scores</h2><span id='topic+pt_from_os'></span>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training (outlier) scores,
<code>os_train</code>, as the reference. The method checks whether the test
scores, <code>os_test</code>, are worse off relative to the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pt_from_os(os_train, os_test, n_pt = 2000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pt_from_os_+3A_os_train">os_train</code></td>
<td>
<p>Outlier scores in training (reference) set.</p>
</td></tr>
<tr><td><code id="pt_from_os_+3A_os_test">os_test</code></td>
<td>
<p>Outlier scores in test set.</p>
</td></tr>
<tr><td><code id="pt_from_os_+3A_n_pt">n_pt</code></td>
<td>
<p>The number of permutations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The null distribution of the test statistic is based on <code>n_pt</code>
permutations. For speed, this is implemented as a sequential Monte Carlo test
with the <span class="pkg">simctest</span> package. See Gandy (2009) for details. The prefix
<em>pt</em> refers to permutation test. This approach does not use the
asymptotic null distribution for the test statistic. This is the recommended
approach for small samples. The test statistic is the weighted AUC (WAUC).
</p>


<h3>Value</h3>

<p>A named list of class <code>outlier.test</code> containing:
</p>

<ul>
<li> <p><code>statistic</code>: observed WAUC statistic
</p>
</li>
<li> <p><code>seq_mct</code>: sequential Monte Carlo test, when applicable
</p>
</li>
<li> <p><code>p_value</code>: p-value
</p>
</li>
<li> <p><code>outlier_scores</code>: outlier scores from training and test set
</p>
</li></ul>



<h3>Notes</h3>

<p>The outlier scores should all mimic out-of-sample behaviour. Mind that the
training scores are not in-sample and thus, biased (overfitted) while the
test scores are out-of-sample. The mismatch &ndash; in-sample versus out-of-sample
scores &ndash; voids the test validity. A simple fix for this is to get the
training scores from an indepedent (fresh) validation set; this follows
the train/validation/test sample splitting convention and the validation set
is effectively the reference set or distribution in this case.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2022).
<em>Test for non-negligible adverse shifts</em>.
In The 38th Conference on Uncertainty in Artificial Intelligence. PMLR.
</p>
<p>Gandy, A. (2009).
<em>Sequential implementation of Monte Carlo tests with uniformly bounded resampling risk</em>.
Journal of the American Statistical Association, 104(488), 1504-1511.
</p>


<h3>See Also</h3>

<p>[pt_oob()] for variant requiring a scoring function.
[at_from_os()] for asymptotic test with the outlier scores.
</p>
<p>Other permutation-test: 
<code><a href="#topic+pt_oob">pt_oob</a>()</code>,
<code><a href="#topic+pt_refit">pt_refit</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dsos)
set.seed(12345)
os_train &lt;- rnorm(n = 100)
os_test &lt;- rnorm(n = 100)
null_test &lt;- pt_from_os(os_train, os_test)
null_test


</code></pre>

<hr>
<h2 id='pt_oob'>Permutation Test With Out-Of-Bag Scores</h2><span id='topic+pt_oob'></span>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training set, <code>x_train</code> as the
as the reference. The method checks whether the test set, <code>x_test</code>, is
worse off relative to this reference set. The function <code>scorer</code> assigns
an outlier score to each instance/observation in both training and test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pt_oob(x_train, x_test, scorer, n_pt = 2000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pt_oob_+3A_x_train">x_train</code></td>
<td>
<p>Training (reference/validation) sample.</p>
</td></tr>
<tr><td><code id="pt_oob_+3A_x_test">x_test</code></td>
<td>
<p>Test sample.</p>
</td></tr>
<tr><td><code id="pt_oob_+3A_scorer">scorer</code></td>
<td>
<p>Function which returns a named list with outlier scores from
the training and test sample. The first argument to <code>scorer</code> must be
<code>x_train</code>; the second, <code>x_test</code>. The returned named list contains
two elements: <em>train</em> and <em>test</em>, each of which is a vector of
corresponding (outlier) scores. See notes below for more information.</p>
</td></tr>
<tr><td><code id="pt_oob_+3A_n_pt">n_pt</code></td>
<td>
<p>The number of permutations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The null distribution of the test statistic is based on <code>n_pt</code>
permutations. For speed, this is implemented as a sequential Monte Carlo test
with the <span class="pkg">simctest</span> package. See Gandy (2009) for details. The prefix
<em>pt</em> refers to permutation test. This approach does not use the
asymptotic null distribution for the test statistic. This is the recommended
approach for small samples. The test statistic is the weighted AUC (WAUC).
</p>


<h3>Value</h3>

<p>A named list of class <code>outlier.test</code> containing:
</p>

<ul>
<li> <p><code>statistic</code>: observed WAUC statistic
</p>
</li>
<li> <p><code>seq_mct</code>: sequential Monte Carlo test, when applicable
</p>
</li>
<li> <p><code>p_value</code>: p-value
</p>
</li>
<li> <p><code>outlier_scores</code>: outlier scores from training and test set
</p>
</li></ul>



<h3>Notes</h3>

<p>The scoring function, <code>scorer</code>, predicts out-of-bag scores to mimic
out-of-sample behaviour. The suffix <em>oob</em> stands for out-of-bag to
highlight this point. This out-of-bag variant avoids refitting the
underlying algorithm from <code>scorer</code> at every permutation. It can, as a
result, be computationally appealing.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2022).
<em>Test for non-negligible adverse shifts</em>.
In The 38th Conference on Uncertainty in Artificial Intelligence. PMLR.
</p>
<p>Gandy, A. (2009).
<em>Sequential implementation of Monte Carlo tests with uniformly bounded resampling risk</em>.
Journal of the American Statistical Association, 104(488), 1504-1511.
</p>


<h3>See Also</h3>

<p>[pt_refit()] for (slower) p-value approximation via refitting.
[at_oob()] for p-value approximation from asymptotic null distribution.
</p>
<p>Other permutation-test: 
<code><a href="#topic+pt_from_os">pt_from_os</a>()</code>,
<code><a href="#topic+pt_refit">pt_refit</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dsos)
set.seed(12345)
data(iris)
idx &lt;- sample(nrow(iris), 2 / 3 * nrow(iris))
iris_train &lt;- iris[idx, ]
iris_test &lt;- iris[-idx, ]
# Use a synthetic (fake) scoring function for illustration
scorer &lt;- function(tr, te) list(train=runif(nrow(tr)), test=runif(nrow(te)))
pt_test &lt;- pt_oob(iris_train, iris_test, scorer = scorer)
pt_test


</code></pre>

<hr>
<h2 id='pt_refit'>Permutation Test By Refitting</h2><span id='topic+pt_refit'></span>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training set, <code>x_train</code> as the
as the reference. The method checks whether the test set, <code>x_test</code>, is
worse off relative to this reference set. The function <code>scorer</code> assigns
an outlier score to each instance/observation in both training and test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pt_refit(x_train, x_test, scorer, n_pt = 2000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pt_refit_+3A_x_train">x_train</code></td>
<td>
<p>Training (reference/validation) sample.</p>
</td></tr>
<tr><td><code id="pt_refit_+3A_x_test">x_test</code></td>
<td>
<p>Test sample.</p>
</td></tr>
<tr><td><code id="pt_refit_+3A_scorer">scorer</code></td>
<td>
<p>Function which returns a named list with outlier scores from
the training and test sample. The first argument to <code>scorer</code> must be
<code>x_train</code>; the second, <code>x_test</code>. The returned named list contains
two elements: <em>train</em> and <em>test</em>, each of which is a vector of
corresponding (outlier) scores. See notes below for more information.</p>
</td></tr>
<tr><td><code id="pt_refit_+3A_n_pt">n_pt</code></td>
<td>
<p>The number of permutations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The null distribution of the test statistic is based on <code>n_pt</code>
permutations. For speed, this is implemented as a sequential Monte Carlo test
with the <span class="pkg">simctest</span> package. See Gandy (2009) for details. The prefix
<em>pt</em> refers to permutation test. This approach does not use the
asymptotic null distribution for the test statistic. This is the recommended
approach for small samples. The test statistic is the weighted AUC (WAUC).
</p>


<h3>Value</h3>

<p>A named list of class <code>outlier.test</code> containing:
</p>

<ul>
<li> <p><code>statistic</code>: observed WAUC statistic
</p>
</li>
<li> <p><code>seq_mct</code>: sequential Monte Carlo test, when applicable
</p>
</li>
<li> <p><code>p_value</code>: p-value
</p>
</li>
<li> <p><code>outlier_scores</code>: outlier scores from training and test set
</p>
</li></ul>



<h3>Notes</h3>

<p>The scoring function, <code>scorer</code>, predicts out-of-sample scores by
refitting the underlying algorithm from <code>scorer</code> at every permutation
The suffix <em>refit</em> emphasizes this point. This is in contrast to the
out-of-bag variant, <code>pt_oob</code>, which only fits once. This method can be
be computationally expensive.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2022).
<em>Test for non-negligible adverse shifts</em>.
In The 38th Conference on Uncertainty in Artificial Intelligence. PMLR.
</p>
<p>Gandy, A. (2009).
<em>Sequential implementation of Monte Carlo tests with uniformly bounded resampling risk</em>.
Journal of the American Statistical Association, 104(488), 1504-1511.
</p>


<h3>See Also</h3>

<p>[pt_oob()] for (faster) p-value approximation via out-of-bag predictions.
[at_oob()] for p-value approximation from asymptotic null distribution.
</p>
<p>Other permutation-test: 
<code><a href="#topic+pt_from_os">pt_from_os</a>()</code>,
<code><a href="#topic+pt_oob">pt_oob</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dsos)
set.seed(12345)
data(iris)
setosa &lt;- iris[1:50, 1:4] # Training sample: Species == 'setosa'
versicolor &lt;- iris[51:100, 1:4] # Test sample: Species == 'versicolor'
scorer &lt;- function(tr, te) list(train=runif(nrow(tr)), test=runif(nrow(te)))
pt_test &lt;- pt_refit(setosa, versicolor, scorer = scorer)
pt_test


</code></pre>

<hr>
<h2 id='wauc_from_os'>Weighted AUC from Outlier Scores</h2><span id='topic+wauc_from_os'></span>

<h3>Description</h3>

<p>Computes the weighted AUC with the weighting scheme described in
Kamulete, V. M. (2021). This assumes that the training set is the reference
distribution and specifies a particular functional form to derive weights
from threshold scores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wauc_from_os(os_train, os_test, weight = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wauc_from_os_+3A_os_train">os_train</code></td>
<td>
<p>Outlier scores in training (reference) set.</p>
</td></tr>
<tr><td><code id="wauc_from_os_+3A_os_test">os_test</code></td>
<td>
<p>Outlier scores in test set.</p>
</td></tr>
<tr><td><code id="wauc_from_os_+3A_weight">weight</code></td>
<td>
<p>Numeric vector of weights of length
<code>length(os_train) + length(os_test)</code>. The first <code>length(os_train)</code>
weights belongs to the training set, the rest is for the test set. If
<code>NULL</code>, the default, all weights are set to 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The weighted AUC (scalar value) given the weighting scheme.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2022).
<em>Test for non-negligible adverse shifts</em>.
In The 38th Conference on Uncertainty in Artificial Intelligence. PMLR.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dsos)
set.seed(12345)
os_train &lt;- rnorm(n = 100)
os_test &lt;- rnorm(n = 100)
test_stat &lt;- wauc_from_os(os_train, os_test)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
