<!DOCTYPE html><html><head><title>Help for package WCluster</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {WCluster}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cluster.predict'>
<p>Predict the closest clusters for a new dataset.</p></a></li>
<li><a href='#distw'>
<p>Distance between clusters based on Ward's method for observations with weights</p></a></li>
<li><a href='#DN.Whclust'>
<p>Hierarchical Clustering for data nuggets</p></a></li>
<li><a href='#DN.Wkmeans'>
<p>K-means Clustering for data nuggets</p></a></li>
<li><a href='#DN.Wpca'>
<p>Weighted PCA for data nuggets</p></a></li>
<li><a href='#DNcluster.predict'>
<p>Predict the closest clusters for a new dataset based on clusters of data nuggets.</p></a></li>
<li><a href='#WCluster-package'><p>Clustering and PCA with Observational Weights and/or Variable Weights, and Data Nuggets Clustering</p></a></li>
<li><a href='#Whclust'>
<p>Hierarchical Clustering with observational weights</p></a></li>
<li><a href='#Wkmeans'>
<p>K-means Clustering with observational weights</p></a></li>
<li><a href='#wmean'>
<p>Cluster Centers for observations with weights</p></a></li>
<li><a href='#Wpca'>
<p>Weighted PCA</p></a></li>
<li><a href='#wss'>
<p>Sums of squares of residuals for observations with weights</p></a></li>
<li><a href='#wwcss'>
<p>Weighted Within Cluster Sum of Squares</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Clustering and PCA with Weights, and Data Nuggets Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-15</td>
</tr>
<tr>
<td>Author:</td>
<td>Yajie Duan [aut, cre],
  Javier Cabrera [aut],
  Ge Cheng [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yajie Duan &lt;yajieritaduan@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>K-means clustering, hierarchical clustering, and PCA with observational 
    weights and/or variable weights. It also includes the corresponding functions 
    for data nuggets which serve as representative samples of large datasets.
    Cherasia et al., (2022) &lt;<a href="https://doi.org/10.1007%2F978-3-031-22687-8_20">doi:10.1007/978-3-031-22687-8_20</a>&gt;. 
    Amaratunga et al., (2009) &lt;<a href="https://doi.org/10.1002%2F9780470317129">doi:10.1002/9780470317129</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), stats, datanugget(&ge; 1.2.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cluster</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-17 17:22:00 UTC; yd254</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-17 17:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cluster.predict'>
Predict the closest clusters for a new dataset.
</h2><span id='topic+cluster.predict'></span>

<h3>Description</h3>

<p>Given observations with weights and cluster assignments, this function returns the cluster assignments for a new dataset by choosing the closest clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster.predict(x,w = rep(1,nrow(x)),cl,newx)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster.predict_+3A_x">x</code></td>
<td>

<p>A data matrix (data frame, data table, matrix, etc.) containing only entries of class numeric.
</p>
</td></tr>
<tr><td><code id="cluster.predict_+3A_w">w</code></td>
<td>

<p>Vector of length nrow(x) of weights for each observation in the dataset. Must be of class numeric or integer. If NULL, the default value is a vector of 1 with length nrow(x), i.e., weights equal 1 for all observations.
</p>
</td></tr>
<tr><td><code id="cluster.predict_+3A_cl">cl</code></td>
<td>

<p>Vector of length nrow(x) of cluster assignments for each observation in the dataset, indicating the cluster to which each observation is allocated. Must be of class integer.
</p>
</td></tr>
<tr><td><code id="cluster.predict_+3A_newx">newx</code></td>
<td>
<p>A new dataset (a data.frame), with the same variables as the learning dataset. Must be of class data.frame.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To obtain the cluster assignments for a new dataset, the weighted cluster centers are calculated firstly based on observations with weights and known cluster assginments. Then, the cluster with the minimal Euclidean distance between new observation and weighted cluster center is chosen as the closest cluster. In this way, the cluster assignments for all the new observations are returned.
</p>


<h3>Value</h3>

<p>Vector of length nrow(newx) containing the cluster assignments for each observation in the new dataset.
</p>


<h3>Author(s)</h3>

<p>Yajie Duan, Javier Cabrera, Ge Cheng
</p>


<h3>References</h3>

<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Wkmeans">Wkmeans</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    require(cluster)

    # The Ruspini data set from the package "cluster""
    data = as.matrix(ruspini)

    #take the first 70 observations for clustering,
    #and the last 5 observations for prediction
    x = data[1:70,]
    test.x = data[71:75,]

    # assign random weights to observations
    w = sample(1:20,nrow(x),replace = TRUE)

    #k-means clustering with observational weights
    cl = Wkmeans(dataset = x, k = 4, obs.weights = w, num.init = 3)

    #predict the cluster assignments for the test data
    cluster.predict(x,w, cl = cl$`Cluster Assignments`,newx = as.data.frame(test.x))


</code></pre>

<hr>
<h2 id='distw'>
Distance between clusters based on Ward's method for observations with weights
</h2><span id='topic+distw'></span>

<h3>Description</h3>

<p>This function calculates distances between pairs of clusters based on Ward's method for observations with weights. Specifically, for each pair of clusters, it computes the increase of weighted sum of squares after merging them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distw(x,cl,w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distw_+3A_x">x</code></td>
<td>

<p>A data matrix (data frame, data table, matrix, etc.) containing only entries of class numeric.
</p>
</td></tr>
<tr><td><code id="distw_+3A_cl">cl</code></td>
<td>

<p>Vector of length nrow(x) of cluster assignments for each observation in the dataset, indicating the cluster to which each observation is allocated. Must be of class integer.
</p>
</td></tr>
<tr><td><code id="distw_+3A_w">w</code></td>
<td>

<p>Vector of length nrow(x) of weights for each observation in the dataset. Must be of class numeric or integer. If NULL, the default value is a vector of 1 with length nrow(x), i.e., weights equal 1 for all observations.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on the Ward method, the distance between two clusters A and B, is the increase of sum of squares after merging them, which is the merging cost of combining two clusters. Specifically, <code>dist(A,B) = SS(A+B) - SS(A) - SS(B)</code>, where SS(A+B) is sum of squares of residuals with respect to mean considering A and B as one cluster, SS(A) and SS(B) are for the cluster A and B seperately.
</p>
<p>Here this function computes the merging costs for each pair of clusters, especially for a data set with observational weights. The sums of squares are calculated with observational weights. The distances of pairs of clusters could be used for agglomerative hierarchical clustering. The pair of clusters with minimal distance could be merged at the next step.
</p>


<h3>Value</h3>

<p>A k by k matrix where k is the number of clusters. The lower triangular part of the matrix contains distances for pairs of clusters based on Ward's method. There are NAs on all the other positions.
</p>


<h3>Author(s)</h3>

<p>Javier Cabrera, Yajie Duan, Ge Cheng
</p>


<h3>References</h3>

<p>Ward Jr, J. H. (1963). Hierarchical grouping to optimize an objective function. <em>Journal of the American statistical association</em>, 58(301), 236-244.
</p>
<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Whclust">Whclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    library(cluster)
    # The Ruspini data set from the package "cluster""
    x = as.matrix(ruspini)

    # assign random weights to observations
    w = sample(1:10,nrow(x),replace = TRUE)

    # assign random clusters to observations
    cl = sample(1:3,nrow(x),replace = TRUE)

    #output distances between clusters based on Ward's method under the random cluster assignments
    distw(x, cl, w)

</code></pre>

<hr>
<h2 id='DN.Whclust'>
Hierarchical Clustering for data nuggets
</h2><span id='topic+DN.Whclust'></span>

<h3>Description</h3>

<p>This function produces the hierarchical tree of data nuggets for an object of class datanugget, by agglomerative hierarchical clustering based on Ward's method considering data nugget centers and weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DN.Whclust(datanugget)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DN.Whclust_+3A_datanugget">datanugget</code></td>
<td>

<p>An object of class datanugget, i.e., the output of functions <code>create.DN</code> or <code>refine.DN</code> in the package <code>datanugget</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data nuggets are a representative sample meant to summarize Big Data by reducing a large dataset to a much smaller dataset by eliminating redundant points while also preserving the peripheries of the dataset. Each data nugget is defined by a center (location), weight (importance), and scale (internal variability). Data nuggets for a large dataset could be created and refined by functions <code>create.DN</code> or <code>refine.DN</code> in the package <code>datanugget</code>.
</p>
<p>Based on data nugget centers and weights, this function produces the hierarchical tree of data nuggets for an object of class datanugget, by agglomerative hierarchical clustering for data nugget centers with nugget weights as observational weights. Ward's method is used by computing the merging costs for each pair of clusters, i.e., the increase of sum of squares after merging two clusters. During the process of agglomerative hierarchical clustering, the sums of squares are calculated with data nugget weights, and the pair of clusters with minimal distance is merged at each step.
</p>


<h3>Value</h3>

<p>An object of class <em>hclust</em> which describes the tree produced by the clustering process for data nuggets. It's the same class of object as outputs from function <code>hclust</code> in the package <code>stats</code>. See details in <code><a href="stats.html#topic+hclust">hclust</a></code>. There are <code><a href="base.html#topic+print">print</a></code>, <code><a href="graphics.html#topic+plot">plot</a></code>, and <code><a href="stats.html#topic+cutree">cutree</a></code> methods for <code>hclust</code> objects.
</p>


<h3>Author(s)</h3>

<p>Yajie Duan, Javier Cabrera, Ge Cheng
</p>


<h3>References</h3>

<p>Ward Jr, J. H. (1963). Hierarchical grouping to optimize an objective function. <em>Journal of the American statistical association</em>, 58(301), 236-244.
</p>
<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="datanugget.html#topic+datanugget-package">datanugget-package</a></code>, <code><a href="datanugget.html#topic+create.DN">create.DN</a></code>, <code><a href="datanugget.html#topic+refine.DN">refine.DN</a></code>, <code><a href="#topic+Whclust">Whclust</a></code>, <code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="#topic+distw">distw</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
      require(datanugget)

      #2-d small example with visualization
      X = rbind.data.frame(matrix(rnorm(10^3, sd = 0.3), ncol = 2),
                matrix(rnorm(10^3, mean = 1, sd = 0.3), ncol = 2))


      #create data nuggets
      my.DN = create.DN(x = X,
                        R = 100,
                        delete.percent = .1,
                        DN.num1 = 100,
                        DN.num2 = 50,
                        no.cores = 0,
                        make.pbs = FALSE)


      #refine data nuggets
      my.DN2 = refine.DN(x = X,
                         DN = my.DN,
                         EV.tol = .9,
                         min.nugget.size = 2,
                         max.splits = 5,
                         no.cores = 0,
                         make.pbs = FALSE)

      #plot raw dataset
      plot(X)


      #transform weights to get colors for plot
      w_trans = my.DN2$`Data Nuggets`[, "Weight"]/sum(my.DN2$`Data Nuggets`[, "Weight"])
      w_trans = w_trans/quantile(w_trans,0.8)
      col = sapply(w_trans, function(t){rgb(0,min(t,1),0)})

      #plot refined data nugget centers with weights
      #lighter green means more weights
      plot(my.DN2$`Data Nuggets`[, c("Center1",
                                     "Center2")],col=col,lty = 2,pch=16, cex=0.5)


      #Hierarchical Clustering for data nuggets
      DN.h = DN.Whclust(my.DN2)

      #print the hclust object
      print(DN.h)

      #plot the hierarchical tree
      plot(DN.h)

      #cut the hierarchical tree to get 2 clusters
      k2 = cutree(DN.h,2)
      table(k2)

      #plot the clustering result for data nuggets
      plot(my.DN2$`Data Nuggets`[, c("Center1",
                                     "Center2")], col = k2, lty = 2,pch=16, cex=0.5)

</code></pre>

<hr>
<h2 id='DN.Wkmeans'>
K-means Clustering for data nuggets
</h2><span id='topic+DN.Wkmeans'></span>

<h3>Description</h3>

<p>This function clusters data nuggets for an object of class datanugget, using K-means considering data nugget centers and weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DN.Wkmeans(datanugget,
        k,
        cl.centers = NULL,
        num.init = 1,
        max.iterations = 10,
        seed = 291102)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DN.Wkmeans_+3A_datanugget">datanugget</code></td>
<td>

<p>An object of class datanugget, i.e., the output of functions <code>create.DN</code> or <code>refine.DN</code> in the package <code>datanugget</code>.
</p>
</td></tr>
<tr><td><code id="DN.Wkmeans_+3A_k">k</code></td>
<td>

<p>Number of desired clusters. Must be of class numeric or integer.
</p>
</td></tr>
<tr><td><code id="DN.Wkmeans_+3A_cl.centers">cl.centers</code></td>
<td>

<p>Chosen cluster centers. If NULL (default), random partition initialization would be used. If not NULL, must be a matrix containing only entries of class numeric with dimensions k by the dimension of data nugget centers.
</p>
</td></tr>
<tr><td><code id="DN.Wkmeans_+3A_num.init">num.init</code></td>
<td>

<p>Number of initial clusters to attempt. Ignored if cl.centers is not NULL. Must be of class numeric or integer.
</p>
</td></tr>
<tr><td><code id="DN.Wkmeans_+3A_max.iterations">max.iterations</code></td>
<td>

<p>Maximum number of iterations attempted for convergence before quitting. Must be of class numeric or integer.
</p>
</td></tr>
<tr><td><code id="DN.Wkmeans_+3A_seed">seed</code></td>
<td>

<p>Random seed for replication. Must be of class numeric or integer.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data nuggets are a representative sample meant to summarize Big Data by reducing a large dataset to a much smaller dataset by eliminating redundant points while also preserving the peripheries of the dataset. Each data nugget is defined by a center (location), weight (importance), and scale (internal variability). Data nuggets for a large dataset could be created and refined by functions <code>create.DN</code> or <code>refine.DN</code> in the package <code>datanugget</code>.
</p>
<p>K-means clustering with observation weigths can be used as an unsupervised learning technique to cluster observations contained in datasets that also have a measure of importance (e.g. weight) associated with them. In the case of data nuggets, this is the weight parameter associated with the data nuggets, so the centers of data nuggets are clustered using their weight parameters. The objective of the algorithm which performs this method of clustering is to minimize the weighted within cluster sum of squares (WWCSS) considering data nugget weights.
</p>
<p>In this function, if no chosen initial cluster centers for data nuggets, random partition initialization with nugget weights is used. Each data nugget is first randomly assigned to a random cluster ID, and then the weighted cluster centers are calculated considering nugget weights. The initial cluster assignments are obtained by choosing the clusters with minimal weighted sum of squares of residuals with respect to the weighted centers.
</p>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>Cluster Assignments for data nuggets</code></td>
<td>
<p>Vector of length nrow(datanugget$'Data Nuggets'), i.e., the number of data nuggets. It contains the cluster assignments for each data nugget.</p>
</td></tr>
<tr><td><code>Cluster Centers</code></td>
<td>
<p>k by dimension of data nuggets matrix containing the weighted cluster centers for each cluster.</p>
</td></tr>
<tr><td><code>Weighted WCSS</code></td>
<td>
<p>List containing the individual WWCSS for each cluster and the combined sum of all individual WWCSS's.</p>
</td></tr>
<tr><td><code>Cluster Assignments for original dataset</code></td>
<td>
<p>Vector of length(datanugget$'Data Nugget Assignments'), i.e., number of observations in the original large dataset. It contains the cluster assignments for each observation in the original large dataset.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yajie Duan, Javier Cabrera, Ge Cheng
</p>


<h3>References</h3>

<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="datanugget.html#topic+datanugget-package">datanugget-package</a></code>, <code><a href="datanugget.html#topic+create.DN">create.DN</a></code>, <code><a href="datanugget.html#topic+refine.DN">refine.DN</a></code>, <code><a href="#topic+Wkmeans">Wkmeans</a></code>, <code><a href="#topic+wwcss">wwcss</a></code>, <code><a href="#topic+wss">wss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>      require(datanugget)

      #2-d small example with visualization
      X = rbind.data.frame(matrix(rnorm(10^4, sd = 0.3), ncol = 2),
                matrix(rnorm(10^4, mean = 1, sd = 0.3), ncol = 2))


      #create data nuggets
      my.DN = create.DN(x = X,
                        R = 500,
                        delete.percent = .1,
                        DN.num1 = 500,
                        DN.num2 = 250,
                        no.cores = 0,
                        make.pbs = FALSE)


      #refine data nuggets
      my.DN2 = refine.DN(x = X,
                         DN = my.DN,
                         EV.tol = .9,
                         min.nugget.size = 2,
                         max.splits = 5,
                         no.cores = 0,
                         make.pbs = FALSE)

      #plot raw large dataset
      plot(X)


      #transform weights to get colors for plot
      w_trans = my.DN2$`Data Nuggets`[, "Weight"]/sum(my.DN2$`Data Nuggets`[, "Weight"])
      w_trans = w_trans/quantile(w_trans,0.8)
      col = sapply(w_trans, function(t){rgb(0,min(t,1),0)})

      #plot refined data nugget centers with weights
      #lighter green means more weights
      plot(my.DN2$`Data Nuggets`[, c("Center1",
                                     "Center2")],col=col,lty = 2,pch=16, cex=0.5)



      #K-means Clustering for data nuggets
      DN.clus = DN.Wkmeans(datanugget = my.DN2,
                  k = 2,
                  num.init = 1,
                  max.iterations = 5)


      DN.clus$`Cluster Centers`
      DN.clus$`Weighted WCSS`


      #plot the clustering result for data nuggets
      plot(my.DN2$`Data Nuggets`[, c("Center1",
                                     "Center2")],
          col = DN.clus$`Cluster Assignments for data nuggets`, lty = 2,pch=16, cex=0.5)
      points(DN.clus$`Cluster Centers`, col = 1:2, pch = 8, cex = 5)

      #plot the clustering result for raw large dataset
      plot(X, col = DN.clus$`Cluster Assignments for original dataset`)


</code></pre>

<hr>
<h2 id='DN.Wpca'>
Weighted PCA for data nuggets
</h2><span id='topic+DN.Wpca'></span>

<h3>Description</h3>

<p>This function conducts weighted PCA on data nuggets, considering data nugget centers and weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DN.Wpca(datanugget,wcol = NULL, corr = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DN.Wpca_+3A_datanugget">datanugget</code></td>
<td>
<p>An object of class datanugget, i.e., the output of functions <code>create.DN</code> or <code>refine.DN</code> in the package <code>datanugget</code>.
</p>
</td></tr>
<tr><td><code id="DN.Wpca_+3A_wcol">wcol</code></td>
<td>
<p>Column Weights: Vector of weights for each variable of data nuggets. Must be of class numeric or integer or table. If NULL, column weights are not considered, i.e., weights equal 1 for all columns.
</p>
</td></tr>
<tr><td><code id="DN.Wpca_+3A_corr">corr</code></td>
<td>
<p>A logical value indicating whether to use correlation matrix. This is recommended when the column weights are not equal. The default value is FALSE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data nuggets are a representative sample meant to summarize Big Data by reducing a large dataset to a much smaller dataset by eliminating redundant points while also preserving the peripheries of the dataset. Each data nugget is defined by a center (location), weight (importance), and scale (internal variability). Data nuggets for a large dataset could be created and refined by functions <code>create.DN</code> or <code>refine.DN</code> in the package <code>datanugget</code>. Based on data nugget centers and weights, this function conducts weighted PCA by eigen method for data nugget centers with nugget weights as observational weights. Variable weights could also be included and considered in this function. Correlation matrix is recommended to use when the column weights are not equal.
</p>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>sdev</code></td>
<td>
<p>the standard deviations of the weighted principal components (i.e., the square roots of the eigenvalues of the weighted covariance/correlation matrix).
</p>
</td></tr>
<tr><td><code>rotation</code></td>
<td>
<p>The matrix of the loading vectors for each of the weighted prinicipal components.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The weighted prinicipal components.</p>
</td></tr>
<tr><td><code>center</code>, <code>scale</code></td>
<td>
<p>the weighted centering and scaling used.</p>
</td></tr>
<tr><td><code>wrow</code>, <code>wcol</code></td>
<td>
<p>row weights and column weights used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yajie Duan, Javier Cabrera, Ge Cheng
</p>


<h3>References</h3>

<p>Amaratunga, D., &amp; Cabrera, J. (2009). Exploration and analysis of DNA microarray and protein array data. <em>John Wiley &amp; Sons</em> (Vol. 605).
</p>
<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="datanugget.html#topic+datanugget-package">datanugget-package</a></code>, <code><a href="datanugget.html#topic+create.DN">create.DN</a></code>, <code><a href="datanugget.html#topic+refine.DN">refine.DN</a></code>, <code><a href="#topic+Wpca">Wpca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
      require(datanugget)

      ## small example
      X = cbind.data.frame(rnorm(10^3),
                           rnorm(10^3),
                           rnorm(10^3))

      suppressMessages({

        my.DN = create.DN(x = X,
                          R = 500,
                          delete.percent = .1,
                          DN.num1 = 500,
                          DN.num2 = 250,
                          no.cores = 0,
                          make.pbs = FALSE)

        my.DN.PCA.info = DN.Wpca(my.DN)

      })

      my.DN.PCA.info$sdev
      my.DN.PCA.info$rotation
      my.DN.PCA.info$x

</code></pre>

<hr>
<h2 id='DNcluster.predict'>
Predict the closest clusters for a new dataset based on clusters of data nuggets.
</h2><span id='topic+DNcluster.predict'></span>

<h3>Description</h3>

<p>Given an object of class datanugget and cluster assignments for data nuggets, this function returns the cluster assignments for new observations or new data nuggets by choosing the closest clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DNcluster.predict(datanugget, cl, newx)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DNcluster.predict_+3A_datanugget">datanugget</code></td>
<td>

<p>An object of class datanugget, i.e., the output of functions <code>create.DN</code> or <code>refine.DN</code> in the package <code>datanugget</code>.
</p>
</td></tr>
<tr><td><code id="DNcluster.predict_+3A_cl">cl</code></td>
<td>

<p>Vector of length nrow(datanugget$'Data Nuggets'), i.e., the number of data nuggets, containing cluster assignments for each data nugget. Must be of class integer.
</p>
</td></tr>
<tr><td><code id="DNcluster.predict_+3A_newx">newx</code></td>
<td>
<p>A new dataset (a data.frame) with same variables as the original large dataset, or a new object of class datanugget with same variables of data nuggets as the learning datanugget object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To obtain the cluster assignments for new observations or new data nuggets, the weighted cluster centers are calculated firstly based on data nugget centers and weights with their known cluster assginments. Then, the cluster with the minimal Euclidean distance between new observation or new data nugget center and weighted cluster center is chosen as the closest cluster. In this way, the cluster assignments for all the new observations or new data nuggets are returned. The weights of new data nuggets are not considered here when predicting the cluster assignments.
</p>


<h3>Value</h3>

<p>Vector of length nrow(newx) or number of new data nuggets, containing the cluster assignments for each new observation or new data nugget.
</p>


<h3>Author(s)</h3>

<p>Yajie Duan, Javier Cabrera, Ge Cheng
</p>


<h3>References</h3>

<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+DN.Whclust">DN.Whclust</a></code>, <code><a href="#topic+DN.Wkmeans">DN.Wkmeans</a></code>, <code><a href="#topic+cluster.predict">cluster.predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
      require(datanugget)

      #2-d small example
      X = rbind.data.frame(matrix(rnorm(5*10^3, sd = 0.3), ncol = 2),
                matrix(rnorm(5*10^3, mean = 1, sd = 0.3), ncol = 2))


      #create data nuggets
      my.DN = create.DN(x = X,
                        R = 300,
                        delete.percent = .1,
                        DN.num1 = 300,
                        DN.num2 = 150,
                        no.cores = 0,
                        make.pbs = FALSE)


      #refine data nuggets
      my.DN2 = refine.DN(x = X,
                         DN = my.DN,
                         EV.tol = .9,
                         min.nugget.size = 2,
                         max.splits = 5,
                         no.cores = 0,
                         make.pbs = FALSE)


      #K-means Clustering for data nuggets
      DN.clus = DN.Wkmeans(datanugget = my.DN2,
                  k = 2,
                  num.init = 1,
                  max.iterations = 5)


      #new observations to predict cluster assignments
      newdata = matrix(rnorm(10^2, mean = 0.5, sd = 0.3), ncol = 2)

      #predict the cluster assignments for the new observations
      DNcluster.predict(my.DN2,
                        cl = DN.clus$`Cluster Assignments for data nuggets`,
                        newx = as.data.frame(newdata))



      #predict cluster assignments for new data nuggets from a new large dataset
      newdata = rbind.data.frame(matrix(rnorm(5*10^3, sd = 0.5), ncol = 2),
                matrix(rnorm(5*10^3, mean = 1, sd = 0.5), ncol = 2))

      #create data nuggets
      my.DN_new = create.DN(x = newdata,
                        R = 300,
                        delete.percent = .1,
                        DN.num1 = 300,
                        DN.num2 = 150,
                        no.cores = 0,
                        make.pbs = FALSE)


      #refine data nuggets
      my.DN2_new = refine.DN(x = newdata,
                         DN = my.DN_new,
                         EV.tol = .9,
                         min.nugget.size = 2,
                         max.splits = 5,
                         no.cores = 0,
                         make.pbs = FALSE)

      #predict the cluster assignments for the new data nuggets
      DNcluster.predict(my.DN2,
                        cl = DN.clus$`Cluster Assignments for data nuggets`,
                        newx = my.DN2_new)

</code></pre>

<hr>
<h2 id='WCluster-package'>Clustering and PCA with Observational Weights and/or Variable Weights, and Data Nuggets Clustering
</h2><span id='topic+WCluster-package'></span>

<h3>Description</h3>

<p>This package contains functions for K-means clustering, hierarchical clustering, and PCA with observational weights and/or variable weights. It also includes the corresponding functions for data nuggets which serve as representative samples of large datasets.
</p>


<h3>Author(s)</h3>

<p>Yajie Duan, Javier Cabrera, Ge Cheng
</p>


<h3>References</h3>

<p>Amaratunga, D., &amp; Cabrera, J. (2009). Exploration and analysis of DNA microarray and protein array data. <em>John Wiley &amp; Sons</em> (Vol. 605).
</p>
<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="datanugget.html#topic+datanugget-package">datanugget-package</a></code>, <code><a href="#topic+Wkmeans">Wkmeans</a></code>, <code><a href="#topic+Whclust">Whclust</a></code>, <code><a href="#topic+Wpca">Wpca</a></code>,  <code><a href="#topic+DN.Wkmeans">DN.Wkmeans</a></code>
</p>

<hr>
<h2 id='Whclust'>
Hierarchical Clustering with observational weights
</h2><span id='topic+Whclust'></span>

<h3>Description</h3>

<p>This function produces the hierarchical tree for observations with weights, by agglomerative hierarchical clustering based on Ward's method considering observational weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Whclust(x,w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Whclust_+3A_x">x</code></td>
<td>

<p>A data matrix (of class matrix, data.frame, or data.table) containing only entries of class numeric.
</p>
</td></tr>
<tr><td><code id="Whclust_+3A_w">w</code></td>
<td>

<p>Vector of length nrow(x) of weights for each observation in the dataset. Must be of class numeric or integer. If NULL, the default value is a vector of 1 with length nrow(x), i.e., weights equal 1 for all observations.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Agglomerative hierarchical clustering based on Ward's method considering observational weights are used to generate the hierarchical tree. Based on the Ward method, the distance between two clusters is the increase of sum of squares after merging them, which is the merging cost of combining two clusters. This function computes the merging costs for each pair of clusters for a data set with observational weights. During the process of agglomerative hierarchical clustering, the sums of squares are calculated with observational weights, and the pair of clusters with minimal distance is merged at each step.
</p>


<h3>Value</h3>

<p>An object of class <em>hclust</em> which describes the tree produced by the clustering process. It's the same class of object as outputs from function <code>hclust</code> in the package <code>stats</code>. See details in <code><a href="stats.html#topic+hclust">hclust</a></code>. There are <code><a href="base.html#topic+print">print</a></code>, <code><a href="graphics.html#topic+plot">plot</a></code>, and <code><a href="stats.html#topic+cutree">cutree</a></code> methods for <code>hclust</code> objects.
</p>


<h3>Author(s)</h3>

<p>Javier Cabrera, Yajie Duan, Ge Cheng
</p>


<h3>References</h3>

<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="#topic+distw">distw</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    require(cluster)
      t1 = Sys.time()

    # The Ruspini data set from the package "cluster""
    x = as.matrix(ruspini)

    # assign random weights to observations
    w = sample(1:20,nrow(x),replace = TRUE)

    # hierarchical clustering with observational weights
    h = Whclust(x,w)

    #print the hclust object
    print(h)

    #plot the hierarchical tree
    plot(h)

    #cut the hierarchical tree to get 4 clusters
    k4 = cutree(h,4)
    table(k4)

    #plot the clustering result
    plot(x,cex = log(w),pch = 16,col = k4)
      t2 = Sys.time()

</code></pre>

<hr>
<h2 id='Wkmeans'>
K-means Clustering with observational weights
</h2><span id='topic+Wkmeans'></span>

<h3>Description</h3>

<p>This function clusters data with observational weights using K-means.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Wkmeans(dataset,
        k,
        cl.centers = NULL,
        obs.weights = rep(1, nrow(dataset)),
        num.init = 1,
        max.iterations = 10,
        seed = 291102)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Wkmeans_+3A_dataset">dataset</code></td>
<td>

<p>A data matrix (data frame, data table, matrix, etc) containing only entries of class numeric.
</p>
</td></tr>
<tr><td><code id="Wkmeans_+3A_k">k</code></td>
<td>

<p>Number of desired clusters. Must be of class numeric or integer.
</p>
</td></tr>
<tr><td><code id="Wkmeans_+3A_cl.centers">cl.centers</code></td>
<td>

<p>Chosen cluster centers. If NULL (default), random partition initialization with observational weights would be used. If not NULL, must be a k by ncol(dataset) matrix containing only entries of class numeric.
</p>
</td></tr>
<tr><td><code id="Wkmeans_+3A_obs.weights">obs.weights</code></td>
<td>

<p>Vector of length nrow(dataset) of weights for each observation in the dataset. Must be of class numeric or integer or table. If NULL, the default value is a vector of 1 with length nrow(dataset), i.e., weights equal 1 for all observations.
</p>
</td></tr>
<tr><td><code id="Wkmeans_+3A_num.init">num.init</code></td>
<td>

<p>Number of initial clusters to attempt. Ignored if cl.centers is not NULL. Must be of class numeric or integer.
</p>
</td></tr>
<tr><td><code id="Wkmeans_+3A_max.iterations">max.iterations</code></td>
<td>

<p>Maximum number of iterations attempted for convergence before quitting. Must be of class numeric or integer.
</p>
</td></tr>
<tr><td><code id="Wkmeans_+3A_seed">seed</code></td>
<td>

<p>Random seed for replication. Must be of class numeric or integer.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>K-means clustering with observational weights can be used as an unsupervised learning technique to cluster observations contained in datasets that also have a measure of importance (e.g. weight) associated with them. The objective of the algorithm which performs this method of clustering is to minimize the total weighted within cluster sum of squares (WWCSS) considering observational weights.
</p>
<p>In this function, if no chosen initial cluster centers, random partition initialization with observational weights is used. Each point in the data is first randomly assigned to a random cluster ID, and then the weighted cluster centers are calculated considering observational weights. The initial cluster assignments are obtained by choosing the clusters with minimal weighted sum of squares of residuals with respect to the weighted centers.
</p>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>Cluster Assignments</code></td>
<td>
<p>Vector of length nrow(dataset) containing the cluster assignment for each observation.</p>
</td></tr>
<tr><td><code>Cluster Centers</code></td>
<td>
<p>k by ncol(dataset) matrix containing the weighted cluster centers for each cluster.</p>
</td></tr>
<tr><td><code>Weighted WCSS</code></td>
<td>
<p>List containing the individual WWCSS for each cluster and the combined sum of all individual WWCSS's.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Javier Cabrera, Yajie Duan, Ge Cheng
</p>


<h3>References</h3>

<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wss">wss</a></code>, <code><a href="#topic+wwcss">wwcss</a></code>, <code><a href="#topic+wmean">wmean</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    require(graphics)

    x &lt;- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
    colnames(x) &lt;- c("x", "y")

    # assign random weights to observations
    w = sample(1:20,nrow(x),replace = TRUE)

    #k-means with observational weights
    cl = Wkmeans(dataset = x, k = 2, obs.weights = w, num.init = 2)

    plot(x,cex = log(w),pch = 16,col = cl$`Cluster Assignments`)
    points(cl$`Cluster Centers`, col = 1:2, pch = 8, cex = 5)

    #individual WWCSS for each cluster and the combined sum of all individual WWCSS's
    cl$`Weighted WCSS`


    require(cluster)

    # The Ruspini data set from the package "cluster""
    x = as.matrix(ruspini)

    # assign random weights to observations
    w = sample(1:20,nrow(x),replace = TRUE)

    #k-means with observational weights
    cl = Wkmeans(dataset = x, k = 4, obs.weights = w, num.init = 3)

    plot(x,cex = log(w),pch = 16,col = cl$`Cluster Assignments`)
    points(cl$`Cluster Centers`, col = 1:4, pch = 8, cex = 5)

    #individual WWCSS for each cluster and the combined sum of all individual WWCSS's
    cl$`Weighted WCSS`

</code></pre>

<hr>
<h2 id='wmean'>
Cluster Centers for observations with weights
</h2><span id='topic+wmean'></span>

<h3>Description</h3>

<p>This function computes the weighted cluster centers for a set of cluster assignments provided to a dataset with observational weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wmean(x,cl,w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wmean_+3A_x">x</code></td>
<td>

<p>A data matrix (data frame, data table, matrix, etc.) containing only entries of class numeric.
</p>
</td></tr>
<tr><td><code id="wmean_+3A_cl">cl</code></td>
<td>

<p>Vector of length nrow(x) of cluster assignments for each observation in the dataset, indicating the cluster to which each observation is allocated. Must be of class integer.
</p>
</td></tr>
<tr><td><code id="wmean_+3A_w">w</code></td>
<td>

<p>Vector of length nrow(x) of weights for each observation in the dataset. Must be of class numeric or integer. If NULL, the default value is a vector of 1 with length nrow(x), i.e., weights equal 1 for all observations.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In this function, the function <code>weighted.mean</code> in the <code>stats</code> package is used to calculate the cluster centers for each cluster with observational weights.
</p>


<h3>Value</h3>

<p>A matrix of cluster centres. Each column is a weighted center for one cluster.
</p>


<h3>Author(s)</h3>

<p>Javier Cabrera, Yajie Duan, Ge Cheng
</p>


<h3>References</h3>

<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Wkmeans">Wkmeans</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    require(cluster)
    # The Ruspini data set from the package "cluster""
    x = as.matrix(ruspini)

    # assign random weights to observations
    w = sample(1:10,nrow(x),replace = TRUE)

    # assign random clusters to observations
    cl = sample(1:3,nrow(x),replace = TRUE)

    #output the weighted cluster centers for each cluster under the random cluster assignments
    wmean(x, cl, w)

</code></pre>

<hr>
<h2 id='Wpca'>
Weighted PCA
</h2><span id='topic+Wpca'></span>

<h3>Description</h3>

<p>This function performs PCA on the given data matrix, with row and column weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Wpca(x, wrow = rep(1, nrow(x)), wcol = rep(1, ncol(x)), corr = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Wpca_+3A_x">x</code></td>
<td>
<p>A data matrix (data frame, data table, matrix, etc) containing only entries of class numeric.</p>
</td></tr>
<tr><td><code id="Wpca_+3A_wrow">wrow</code></td>
<td>
<p>Row Weights: vector of length nrow(x) of weights for each observation in the dataset. Must be of class numeric or integer or table. If NULL, the default value is a vector of 1 with length nrow(x), i.e., weights equal 1 for all observations.
</p>
</td></tr>
<tr><td><code id="Wpca_+3A_wcol">wcol</code></td>
<td>
<p>Column Weights: Vector of length ncol(x) of weights for each variable in the dataset. Must be of class numeric or integer or table. If NULL, the default value is a vector of 1 with length ncol(x), i.e., weights equal 1 for all columns.
</p>
</td></tr>
<tr><td><code id="Wpca_+3A_corr">corr</code></td>
<td>
<p>A logical value indicating whether to use correlation matrix. This is recommended when the column weights are not equal. The default value is FALSE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>PCA with row and column weights is conducted by eigen method.
</p>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>sdev</code></td>
<td>
<p>the standard deviations of the weighted principal components (i.e., the square roots of the eigenvalues of the weighted covariance/correlation matrix).
</p>
</td></tr>
<tr><td><code>rotation</code></td>
<td>
<p>The matrix of the loading vectors for each of the weighted prinicipal components.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The weighted prinicipal components.</p>
</td></tr>
<tr><td><code>center</code>, <code>scale</code></td>
<td>
<p>the weighted centering and scaling used.</p>
</td></tr>
<tr><td><code>wrow</code>, <code>wcol</code></td>
<td>
<p>row weights and column weights used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Javier Cabrera, Yajie Duan, Ge Cheng
</p>


<h3>References</h3>

<p>Amaratunga, D., &amp; Cabrera, J. (2009). Exploration and analysis of DNA microarray and protein array data. <em>John Wiley &amp; Sons</em> (Vol. 605).
</p>
<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    require(cluster)

    # The Ruspini data set from the package "cluster""
    x = as.matrix(ruspini)

    # assign random weights to observations
    w = sample(1:20,nrow(x),replace = TRUE)

    #PCA with observational weights
    res = Wpca(x, wrow = w)

    #weighted prinicipal components
    pc = res$x
    pc

    #loading vectors
    loadings = res$rotation
    loadings
</code></pre>

<hr>
<h2 id='wss'>
Sums of squares of residuals for observations with weights
</h2><span id='topic+wss'></span>

<h3>Description</h3>

<p>This function calculates sums of squares of residuals with respect to mean for observations with weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wss(x,w = rep(1,nrow(x)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wss_+3A_x">x</code></td>
<td>

<p>A data matrix (data frame, data table, matrix, etc.) containing only entries of class numeric.
</p>
</td></tr>
<tr><td><code id="wss_+3A_w">w</code></td>
<td>

<p>Vector of length nrow(x) of weights for each observation in the dataset. Must be of class numeric or integer. If NULL, the default value is a vector of 1 with length nrow(x), i.e., weights equal 1 for all observations.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In this function, for a dataset with observational weights, the weighted mean for the dataset is calculated first. Based on it, the weighted sum of squares of residuals with respect to the weighted mean is calculated with observational weights. This could be used to calculate weighted within-cluster sum of squares for one cluster of data with observational weights.
</p>


<h3>Value</h3>

<p>a length-one numeric vector.
</p>


<h3>Author(s)</h3>

<p>Javier Cabrera, Yajie Duan, Ge Cheng
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    require(cluster)
    # The Ruspini data set from the package "cluster""
    x = as.matrix(ruspini)

    # assign random weights to observations
    w = sample(1:10,nrow(x),replace = TRUE)

    wss(x,w)

    </code></pre>

<hr>
<h2 id='wwcss'>
Weighted Within Cluster Sum of Squares
</h2><span id='topic+wwcss'></span>

<h3>Description</h3>

<p>This function computes the weighted within cluster sum of squares (WWCSS) for a set of cluster assignments provided to a dataset with observational weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wwcss(x, cl, w = rep(1,length(x)), groupSum = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wwcss_+3A_x">x</code></td>
<td>

<p>A data matrix (data frame, data table, matrix, etc.) containing only entries of class numeric.
</p>
</td></tr>
<tr><td><code id="wwcss_+3A_cl">cl</code></td>
<td>

<p>Vector of length nrow(x) of cluster assignments for each observation in the dataset, indicating the cluster to which each observation is allocated. Must be of class integer.
</p>
</td></tr>
<tr><td><code id="wwcss_+3A_w">w</code></td>
<td>

<p>Vector of length nrow(x) of weights for each observation in the dataset. Must be of class numeric or integer. If NULL, the default value is a vector of 1 with length nrow(x), i.e., weights equal 1 for all observations.
</p>
</td></tr>
<tr><td><code id="wwcss_+3A_groupsum">groupSum</code></td>
<td>

<p>A logical value indicating whether the weighted within-cluster sum of squres (WWCSS) of each cluster should be returned. If <code>TRUE</code> the total WWCSS and WWCSS for each cluster are returned. If <code>FALSE</code> (the default) only the total WWCSS is returned.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to evaluate clustering results for observations with weights, and also used for optimizing the cluster assignments in the Wkmeans function.
</p>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>WWCSS</code></td>
<td>
<p>If requested by <code>groupSum</code>, vector of individual WWCSS's for each cluster</p>
</td></tr>
<tr><td><code>TotalWWCSS</code></td>
<td>
<p>Combined sum of all individual WWCSS's.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Javier Cabrera, Yajie Duan, Ge Cheng
</p>


<h3>References</h3>

<p>Cherasia, K. E., Cabrera, J., Fernholz, L. T., &amp; Fernholz, R. (2022). Data Nuggets in Supervised Learning. <em>In Robust and Multivariate Statistical Methods: Festschrift in Honor of David E. Tyler</em> (pp. 429-449). Cham: Springer International Publishing.
</p>
<p>Beavers, T., Cheng, G., Duan, Y., Cabrera, J., Lubomirski, M., Amaratunga, D., Teigler, J. (2023). Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure (Submitted for Publication)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Wkmeans">Wkmeans</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    require(cluster)
    # The Ruspini data set from the package "cluster""
    x = as.matrix(ruspini)

    # assign random weights to observations
    w = sample(1:10,nrow(x),replace = TRUE)

    # assign random clusters to observations
    cl = sample(1:3,nrow(x),replace = TRUE)

    #output the total WWCSS and WWCSS for each cluster for the cluster assignments
    wwcss(x, cl, w, groupSum = TRUE)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
