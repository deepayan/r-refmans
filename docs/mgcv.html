<!DOCTYPE html><html><head><title>Help for package mgcv</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mgcv}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mgcv.package'><p>Mixed GAM Computation Vehicle with GCV/AIC/REML/NCV smoothness estimation and GAMMs by REML/PQL</p></a></li>
<li><a href='#anova.gam'><p>Approximate hypothesis tests related to  GAM fits</p></a></li>
<li><a href='#bam'><p>Generalized additive models for very large datasets</p></a></li>
<li><a href='#bam.update'><p>Update a strictly additive bam model for new data.</p></a></li>
<li><a href='#bandchol'><p>Choleski decomposition of a band diagonal matrix</p></a></li>
<li><a href='#betar'><p>GAM beta regression family</p></a></li>
<li><a href='#blas.thread.test'><p>BLAS thread safety</p></a></li>
<li><a href='#bug.reports.mgcv'><p>Reporting mgcv bugs.</p></a></li>
<li><a href='#choldrop'><p>Deletion and rank one Cholesky factor update</p></a></li>
<li><a href='#choose.k'><p>Basis dimension choice for smooths</p></a></li>
<li><a href='#cnorm'><p>GAM censored normal family for log-normal AFT and Tobit models</p></a></li>
<li><a href='#columb'><p>Reduced version of Columbus OH crime data</p></a></li>
<li><a href='#concurvity'><p>GAM concurvity measures</p></a></li>
<li><a href='#cox.ph'><p>Additive Cox Proportional Hazard Model</p></a></li>
<li><a href='#cox.pht'><p>Additive Cox proportional hazard models with time varying covariates</p></a></li>
<li><a href='#cSplineDes'><p>Evaluate cyclic B spline basis</p></a></li>
<li><a href='#dDeta'><p>Obtaining derivative w.r.t. linear predictor</p></a></li>
<li><a href='#dpnorm'><p>Stable evaluation of difference between normal c.d.f.s</p></a></li>
<li><a href='#exclude.too.far'><p>Exclude prediction grid points too far from data</p></a></li>
<li><a href='#extract.lme.cov'><p> Extract the data covariance matrix from an lme object</p></a></li>
<li><a href='#factor.smooth'><p>Factor smooth interactions in GAMs</p></a></li>
<li><a href='#family.mgcv'><p>Distribution families in mgcv</p></a></li>
<li><a href='#FFdes'><p>Level 5 fractional factorial designs</p></a></li>
<li><a href='#fix.family.link'><p>Modify families for use in GAM fitting and checking</p></a></li>
<li><a href='#fixDependence'><p>Detect linear dependencies of one matrix on another</p></a></li>
<li><a href='#formula.gam'><p>GAM formula</p></a></li>
<li><a href='#formXtViX'><p> Form component of GAMM covariance matrix</p></a></li>
<li><a href='#fs.test'><p>FELSPLINE test function</p></a></li>
<li><a href='#full.score'><p>GCV/UBRE score for use within nlm</p></a></li>
<li><a href='#gam'><p>Generalized additive models with integrated smoothness estimation</p></a></li>
<li><a href='#gam.check'><p>Some diagnostics for a fitted gam model</p></a></li>
<li><a href='#gam.control'><p>Setting GAM fitting defaults</p></a></li>
<li><a href='#gam.convergence'><p>GAM convergence and performance issues</p></a></li>
<li><a href='#gam.fit'><p>GAM P-IRLS estimation with GCV/UBRE smoothness estimation</p></a></li>
<li><a href='#gam.fit3'><p>P-IRLS GAM estimation with GCV, UBRE/AIC or RE/ML derivative calculation</p></a></li>
<li><a href='#gam.fit5.post.proc'><p>Post-processing output of gam.fit5</p></a></li>
<li><a href='#gam.mh'><p>Simple posterior simulation with gam fits</p></a></li>
<li><a href='#gam.models'><p>Specifying generalized additive models</p></a></li>
<li><a href='#gam.outer'><p>Minimize GCV or UBRE score of a GAM using &lsquo;outer&rsquo; iteration</p></a></li>
<li><a href='#gam.reparam'><p>Finding stable orthogonal re-parameterization of the square root penalty.</p></a></li>
<li><a href='#gam.scale'><p>Scale parameter estimation in GAMs</p></a></li>
<li><a href='#gam.selection'><p>Generalized Additive Model Selection</p></a></li>
<li><a href='#gam.side'><p>Identifiability side conditions for a GAM</p></a></li>
<li><a href='#gam.vcomp'><p>Report gam smoothness estimates as variance components</p></a></li>
<li><a href='#gam2objective'><p>Objective functions for GAM smoothing parameter estimation</p></a></li>
<li><a href='#gamlss.etamu'><p>Transform derivatives wrt mu to derivatives wrt linear predictor</p></a></li>
<li><a href='#gamlss.gH'><p>Calculating derivatives of log-likelihood wrt regression coefficients</p></a></li>
<li><a href='#gamm'><p>Generalized Additive Mixed Models</p></a></li>
<li><a href='#gammals'><p>Gamma location-scale model family</p></a></li>
<li><a href='#gamObject'><p>Fitted gam object</p></a></li>
<li><a href='#gamSim'><p>Simulate example data for GAMs</p></a></li>
<li><a href='#gaulss'><p>Gaussian location-scale model family</p></a></li>
<li><a href='#get.var'><p>Get named variable or evaluate expression from list or data.frame</p></a></li>
<li><a href='#gevlss'><p>Generalized Extreme Value location-scale model family</p></a></li>
<li><a href='#gfam'><p>Grouped families</p></a></li>
<li><a href='#ginla'><p>GAM Integrated Nested Laplace Approximation Newton Enhanced</p></a></li>
<li><a href='#gumbls'><p>Gumbel location-scale model family</p></a></li>
<li><a href='#identifiability'><p>Identifiability constraints</p></a></li>
<li><a href='#in.out'><p>Which of a set of points lie within a polygon defined region</p></a></li>
<li><a href='#influence.gam'><p>Extract the diagonal of the influence/hat matrix for a GAM</p></a></li>
<li><a href='#initial.sp'><p> Starting values for multiple smoothing parameter estimation</p></a></li>
<li><a href='#inSide'><p>Are points inside boundary?</p></a></li>
<li><a href='#interpret.gam'><p>Interpret a GAM formula</p></a></li>
<li><a href='#jagam'><p>Just Another Gibbs Additive Modeller: JAGS support for mgcv.</p></a></li>
<li><a href='#k.check'><p>Checking smooth basis dimension</p></a></li>
<li><a href='#ldetS'><p>Getting log generalized determinant of penalty matrices</p></a></li>
<li><a href='#ldTweedie'><p>Log Tweedie density evaluation</p></a></li>
<li><a href='#linear.functional.terms'><p>Linear functionals of a smooth in GAMs</p></a></li>
<li><a href='#logLik.gam'><p>AIC and Log likelihood for a fitted GAM</p></a></li>
<li><a href='#ls.size'><p>Size of list elements</p></a></li>
<li><a href='#magic'><p> Stable Multiple Smoothing Parameter Estimation by GCV or UBRE</p></a></li>
<li><a href='#magic.post.proc'><p>Auxilliary information from magic fit</p></a></li>
<li><a href='#mgcv.FAQ'><p>Frequently Asked Questions for package mgcv</p></a></li>
<li><a href='#mgcv.parallel'><p>Parallel computation in mgcv.</p></a></li>
<li><a href='#mini.roots'><p>Obtain square roots of penalty matrices</p></a></li>
<li><a href='#missing.data'><p>Missing data in GAMs</p></a></li>
<li><a href='#model.matrix.gam'><p>Extract model matrix from GAM fit</p></a></li>
<li><a href='#mono.con'><p>Monotonicity constraints for a cubic regression spline</p></a></li>
<li><a href='#mroot'><p>Smallest square root of matrix</p></a></li>
<li><a href='#multinom'><p>GAM multinomial logistic regression</p></a></li>
<li><a href='#mvn'><p>Multivariate normal additive models</p></a></li>
<li><a href='#NCV'><p>Neighbourhood Cross Validation</p></a></li>
<li><a href='#negbin'><p>GAM negative binomial families</p></a></li>
<li><a href='#new.name'><p>Obtain a name for a new variable that is not already in use</p></a></li>
<li><a href='#notExp'><p> Functions for better-than-log positive parameterization</p></a></li>
<li><a href='#notExp2'><p> Alternative to log parameterization for variance components</p></a></li>
<li><a href='#null.space.dimension'><p>The basis of the space of un-penalized functions for a TPRS</p></a></li>
<li><a href='#ocat'><p>GAM ordered categorical family</p></a></li>
<li><a href='#one.se.rule'><p>The one standard error rule for smoother models</p></a></li>
<li><a href='#pcls'><p> Penalized Constrained Least Squares Fitting</p></a></li>
<li><a href='#pdIdnot'><p>Overflow proof pdMat class for multiples of the identity matrix</p></a></li>
<li><a href='#pdTens'><p>Functions implementing a pdMat class for tensor product smooths</p></a></li>
<li><a href='#pen.edf'><p>Extract the effective degrees of freedom associated with each penalty in a gam fit</p></a></li>
<li><a href='#place.knots'><p> Automatically place a set of knots evenly through covariate values</p></a></li>
<li><a href='#plot.gam'><p>Default GAM plotting</p></a></li>
<li><a href='#polys.plot'><p>Plot geographic regions defined as polygons</p></a></li>
<li><a href='#predict.bam'><p>Prediction from fitted Big Additive Model model</p></a></li>
<li><a href='#predict.gam'><p>Prediction from fitted GAM model</p></a></li>
<li><a href='#Predict.matrix'><p>Prediction methods for smooth terms in a GAM</p></a></li>
<li><a href='#Predict.matrix.cr.smooth'><p>Predict matrix method functions</p></a></li>
<li><a href='#Predict.matrix.soap.film'><p>Prediction matrix for soap film smooth</p></a></li>
<li><a href='#print.gam'><p>Print a Generalized Additive Model object.</p></a></li>
<li><a href='#psum.chisq'><p>Evaluate the c.d.f. of a weighted sum of chi-squared deviates</p></a></li>
<li><a href='#qq.gam'><p>QQ plots for gam model residuals</p></a></li>
<li><a href='#random.effects'><p>Random effects in GAMs</p></a></li>
<li><a href='#residuals.gam'><p>Generalized Additive Model residuals</p></a></li>
<li><a href='#rig'><p>Generate inverse Gaussian random deviates</p></a></li>
<li><a href='#rmvn'><p>Generate from or evaluate multivariate normal or t densities.</p></a></li>
<li><a href='#Rrank'><p>Find rank of upper triangular matrix</p></a></li>
<li><a href='#rTweedie'><p>Generate Tweedie random deviates</p></a></li>
<li><a href='#s'><p>Defining smooths in GAM formulae</p></a></li>
<li><a href='#scat'><p>GAM scaled t family for heavy tailed data</p></a></li>
<li><a href='#sdiag'><p>Extract or modify diagonals of a matrix</p></a></li>
<li><a href='#shash'><p>Sinh-arcsinh location scale and shape model family</p></a></li>
<li><a href='#single.index'><p>Single index models with mgcv</p></a></li>
<li><a href='#Sl.inirep'><p>Re-parametrizing model matrix X</p></a></li>
<li><a href='#Sl.repara'><p>Applying re-parameterization from log-determinant of penalty matrix to</p>
model matrix.</a></li>
<li><a href='#Sl.setup'><p>Setting up a list representing a block diagonal penalty matrix</p></a></li>
<li><a href='#slanczos'><p>Compute truncated eigen decomposition of a symmetric matrix</p></a></li>
<li><a href='#smooth.construct'><p>Constructor functions for smooth terms in a GAM</p></a></li>
<li><a href='#smooth.construct.ad.smooth.spec'><p>Adaptive smooths in GAMs</p></a></li>
<li><a href='#smooth.construct.bs.smooth.spec'><p>Penalized B-splines in GAMs</p></a></li>
<li><a href='#smooth.construct.cr.smooth.spec'><p>Penalized Cubic regression splines in GAMs</p></a></li>
<li><a href='#smooth.construct.ds.smooth.spec'><p>Low rank Duchon 1977 splines</p></a></li>
<li><a href='#smooth.construct.fs.smooth.spec'><p>Factor smooth interactions in GAMs</p></a></li>
<li><a href='#smooth.construct.gp.smooth.spec'><p>Low rank Gaussian process smooths</p></a></li>
<li><a href='#smooth.construct.mrf.smooth.spec'><p>Markov Random Field Smooths</p></a></li>
<li><a href='#smooth.construct.ps.smooth.spec'><p>P-splines in GAMs</p></a></li>
<li><a href='#smooth.construct.re.smooth.spec'><p>Simple random effects in GAMs</p></a></li>
<li><a href='#smooth.construct.so.smooth.spec'><p>Soap film smoother constructer</p></a></li>
<li><a href='#smooth.construct.sos.smooth.spec'><p>Splines on the sphere</p></a></li>
<li><a href='#smooth.construct.sz.smooth.spec'><p>Constrained factor smooth interactions in GAMs</p></a></li>
<li><a href='#smooth.construct.t2.smooth.spec'><p>Tensor product smoothing constructor</p></a></li>
<li><a href='#smooth.construct.tensor.smooth.spec'><p>Tensor product smoothing constructor</p></a></li>
<li><a href='#smooth.construct.tp.smooth.spec'><p>Penalized thin plate regression splines in GAMs</p></a></li>
<li><a href='#smooth.info'><p>Generic function to provide extra information about smooth specification</p></a></li>
<li><a href='#smooth.terms'><p>Smooth terms in GAM</p></a></li>
<li><a href='#smooth2random'><p>Convert a smooth to a form suitable for estimating as random effect</p></a></li>
<li><a href='#smoothCon'><p>Prediction/Construction wrapper functions for GAM smooth terms</p></a></li>
<li><a href='#sp.vcov'><p>Extract smoothing parameter estimator covariance matrix from (RE)ML GAM fit</p></a></li>
<li><a href='#spasm.construct'><p>Experimental sparse smoothers</p></a></li>
<li><a href='#step.gam'><p>Alternatives to step.gam</p></a></li>
<li><a href='#summary.gam'><p>Summary for a GAM fit</p></a></li>
<li><a href='#t2'><p>Define alternative tensor product smooths in GAM formulae</p></a></li>
<li><a href='#te'><p>Define tensor product smooths or tensor product interactions in GAM formulae</p></a></li>
<li><a href='#tensor.prod.model.matrix'><p>Row Kronecker product/ tensor product smooth construction</p></a></li>
<li><a href='#totalPenaltySpace'><p>Obtaining (orthogonal) basis for null space and range of the penalty matrix</p></a></li>
<li><a href='#trichol'><p>Choleski decomposition of a tri-diagonal matrix</p></a></li>
<li><a href='#trind.generator'><p>Generates index arrays for upper triangular storage</p></a></li>
<li><a href='#Tweedie'><p>GAM Tweedie families</p></a></li>
<li><a href='#twlss'><p>Tweedie location scale family</p></a></li>
<li><a href='#uniquecombs'><p>find the unique rows in a matrix</p></a></li>
<li><a href='#vcov.gam'><p>Extract parameter (estimator) covariance matrix from GAM fit</p></a></li>
<li><a href='#vis.gam'><p>Visualization of GAM objects</p></a></li>
<li><a href='#XWXd'><p>Internal functions for discretized model matrix handling</p></a></li>
<li><a href='#ziP'><p>GAM zero-inflated (hurdle) Poisson regression family</p></a></li>
<li><a href='#ziplss'><p>Zero inflated (hurdle) Poisson location-scale model family</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.9-1</td>
</tr>
<tr>
<td>Author:</td>
<td>Simon Wood &lt;simon.wood@r-project.org&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Simon Wood &lt;simon.wood@r-project.org&gt;</td>
</tr>
<tr>
<td>Title:</td>
<td>Mixed GAM Computation Vehicle with Automatic Smoothness
Estimation</td>
</tr>
<tr>
<td>Description:</td>
<td>Generalized additive (mixed) models, some of their extensions and 
             other generalized ridge regression with multiple smoothing 
             parameter estimation by (Restricted) Marginal Likelihood, 
             Generalized Cross Validation and similar, or using iterated 
             nested Laplace approximation for fully Bayesian inference. See 
             Wood (2017) &lt;<a href="https://doi.org/10.1201%2F9781315370279">doi:10.1201/9781315370279</a>&gt; for an overview. 
             Includes a gam() function, a wide variety of smoothers, 'JAGS' 
             support and distributions beyond the exponential family. </td>
</tr>
<tr>
<td>Priority:</td>
<td>recommended</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0), nlme (&ge; 3.1-64)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, stats, graphics, Matrix, splines, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>parallel, survival, MASS</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-20 10:39:06 UTC; sw283</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-21 00:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='mgcv.package'>Mixed GAM Computation Vehicle with GCV/AIC/REML/NCV smoothness estimation and GAMMs by REML/PQL
</h2><span id='topic+mgcv.package'></span><span id='topic+mgcv-package'></span><span id='topic+mgcv'></span>

<h3>Description</h3>

<p><code>mgcv</code> provides functions for generalized additive modelling (<code><a href="#topic+gam">gam</a></code> and <code><a href="#topic+bam">bam</a></code>)  and
generalized additive mixed modelling (<code><a href="#topic+gamm">gamm</a></code>, and <code><a href="#topic+random.effects">random.effects</a></code>). The term GAM is taken to include 
any model dependent on unknown smooth functions of predictors and estimated by quadratically penalized (possibly quasi-) likelihood maximization. Available distributions are covered in <code><a href="#topic+family.mgcv">family.mgcv</a></code> and available smooths in <code><a href="#topic+smooth.terms">smooth.terms</a></code>.
</p>
<p>Particular features of the package are facilities for automatic smoothness selection (Wood, 2004, 2011), 
and the provision of a variety of smooths of more than one variable. User defined 
smooths can be added. A Bayesian approach to confidence/credible interval calculation is
provided. Linear functionals of smooths, penalization of parametric model terms and linkage 
of smoothing parameters are all supported. Lower level routines for generalized ridge 
regression and penalized linearly constrained least squares are also available. In addition to the main modelling functions, <code><a href="#topic+jagam">jagam</a></code> provided facilities to ease the set up of models for use with JAGS, while <code><a href="#topic+ginla">ginla</a></code> provides marginal inference via a version of Integrated Nested Laplace Approximation. 
</p>


<h3>Details</h3>

 <p><code>mgcv</code> provides generalized additive modelling functions <code><a href="#topic+gam">gam</a></code>,
<code><a href="#topic+predict.gam">predict.gam</a></code> and <code><a href="#topic+plot.gam">plot.gam</a></code>, which are very similar
in use to the S functions of the same name designed by Trevor Hastie (with some extensions). 
However the underlying representation and estimation of the models is based on a
penalized regression spline approach, with automatic smoothness selection. A
number of other functions  such as <code><a href="#topic+summary.gam">summary.gam</a></code> and <code><a href="#topic+anova.gam">anova.gam</a></code> 
are also provided, for extracting information from a fitted <code><a href="#topic+gamObject">gamObject</a></code>.
</p>
<p>Use of <code><a href="#topic+gam">gam</a></code> is much like use of <code><a href="stats.html#topic+glm">glm</a></code>, except that
within a <code>gam</code> model formula, isotropic smooths of any number of predictors can be specified using
<code><a href="#topic+s">s</a></code> terms, while scale invariant smooths of any number of
predictors can be specified using <code><a href="#topic+te">te</a></code>,  <code><a href="#topic+ti">ti</a></code> or <code><a href="#topic+t2">t2</a></code>  terms. 
<code><a href="#topic+smooth.terms">smooth.terms</a></code> provides an 
overview of the built in smooth classes, and <code><a href="#topic+random.effects">random.effects</a></code> should be refered to for an overview 
of random effects terms (see also <code><a href="#topic+mrf">mrf</a></code> for Markov random fields). Estimation is by
penalized likelihood or quasi-likelihood maximization, with smoothness
selection by GCV, GACV, gAIC/UBRE, <code><a href="#topic+NCV">NCV</a></code> or (RE)ML. See <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+gam.models">gam.models</a></code>, 
<code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code> and <code><a href="#topic+gam.selection">gam.selection</a></code> for some discussion of model specification and
selection. For detailed control of fitting see <code><a href="#topic+gam.convergence">gam.convergence</a></code>,
<code><a href="#topic+gam">gam</a></code> arguments <code>method</code> and <code>optimizer</code> and <code><a href="#topic+gam.control">gam.control</a></code>. For checking and
visualization see <code><a href="#topic+gam.check">gam.check</a></code>, <code><a href="#topic+choose.k">choose.k</a></code>, <code><a href="#topic+vis.gam">vis.gam</a></code> and <code><a href="#topic+plot.gam">plot.gam</a></code>.
While a number of types of smoother are built into the package, it is also
extendable with user defined smooths, see <code><a href="#topic+smooth.construct">smooth.construct</a></code>, for example.
</p>
<p>A Bayesian approach to smooth modelling is used to derive standard errors on
predictions, and hence credible intervals (see Marra and Wood, 2012). The Bayesian covariance matrix for
the model coefficients is returned in <code>Vp</code> of the
<code><a href="#topic+gamObject">gamObject</a></code>. See <code><a href="#topic+predict.gam">predict.gam</a></code> for examples of how
this can be used to obtain credible regions for any quantity derived from the
fitted model, either directly, or by direct simulation from the posterior
distribution of the model coefficients. Approximate p-values can also be obtained for testing 
individual smooth terms for equality to the zero function, using similar ideas (see Wood, 2013a,b). Frequentist
approximations can be used for hypothesis testing based model comparison. See <code><a href="#topic+anova.gam">anova.gam</a></code> and
<code><a href="#topic+summary.gam">summary.gam</a></code> for more on hypothesis testing.
</p>
<p>For large datasets (that is large n) see <code><a href="#topic+bam">bam</a></code> which is a version of <code><a href="#topic+gam">gam</a></code> with 
a much reduced memory footprint. <code>bam(...,discrete=TRUE)</code> offers the very efficient methods of Wood et al. (2017) and Li and Wood (2020).
</p>
<p>The package also provides a generalized additive mixed modelling function,
<code><a href="#topic+gamm">gamm</a></code>, based on a PQL approach and  
<code>lme</code> from the <code>nlme</code> library (for an <code>lme4</code> based version, see package <code>gamm4</code>). 
<code>gamm</code> is particularly useful
for modelling correlated data (i.e. where a simple independence model for the
residual variation is inappropriate). In addition, low level routine <code><a href="#topic+magic">magic</a></code>
can fit models to data with a known correlation structure.
</p>
<p>Some underlying GAM fitting methods are available as low level fitting
functions: see <code><a href="#topic+magic">magic</a></code>. But there is little functionality 
that can not be more conventiently accessed via <code><a href="#topic+gam">gam</a></code> . 
Penalized weighted least squares with linear equality and inequality constraints is provided by 
<code><a href="#topic+pcls">pcls</a></code>.
</p>
<p>For a complete list of functions type <code>library(help=mgcv)</code>. See also <code><a href="#topic+mgcv.FAQ">mgcv.FAQ</a></code>.
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon.wood@r-project.org&gt; 
</p>
<p>with contributions and/or help from Natalya Pya, Thomas Kneib, Kurt Hornik, Mike Lonergan, Henric Nilsson,
Fabian Scheipl and Brian Ripley. 
</p>
<p>Polish translation - Lukasz Daniel; German translation - Chris Leick, Detlef Steuer; 
French Translation - Philippe Grosjean
</p>
<p>Maintainer: Simon Wood &lt;simon.wood@r-project.org&gt;
</p>
<p>Part funded by EPSRC: EP/K005251/1
</p>


<h3>References</h3>

<p>These provide details for the underlying mgcv methods, and fuller 
references to the large literature on which the methods are based.
</p>
<p>Wood, S. N. (2020) Inference and computation with generalized
additive models and their extensions. Test 29(2): 307-339.
<a href="https://doi.org/10.1007/s11749-020-00711-5">doi:10.1007/s11749-020-00711-5</a>
</p>
<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models (with discussion).
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>
<p>Wood, S.N. (2011) Fast stable restricted maximum likelihood 
and marginal likelihood estimation of semiparametric generalized linear 
models. Journal of the Royal Statistical Society (B) 73(1):3-36
</p>
<p>Wood, S.N. (2004) Stable and efficient multiple smoothing parameter estimation for
generalized additive models. J. Amer. Statist. Ass. 99:673-686. 
</p>
<p>Marra, G and S.N. Wood (2012) Coverage Properties of Confidence Intervals for Generalized Additive
Model Components. Scandinavian Journal of Statistics, 39(1), 53-74.
</p>
<p>Wood, S.N. (2013a) A simple test for random effects in regression models. Biometrika 100:1005-1010 <a href="https://doi.org/10.1093/biomet/ast038">doi:10.1093/biomet/ast038</a>
</p>
<p>Wood, S.N. (2013b) On p-values for smooth components of an extended generalized additive model. Biometrika 100:221-228 <a href="https://doi.org/10.1093/biomet/ass048">doi:10.1093/biomet/ass048</a>
</p>
<p>Wood, S.N. (2017) <em>Generalized Additive Models: an introduction with R (2nd edition)</em>,
CRC <a href="https://doi.org/10.1201/9781315370279">doi:10.1201/9781315370279</a>
</p>
<p>Wood, S.N., Li, Z., Shaddick, G. &amp; Augustin N.H. (2017) Generalized additive models for gigadata: modelling the UK black smoke network daily data. Journal of the American Statistical Association. 112(519):1199-1210
<a href="https://doi.org/10.1080/01621459.2016.1195744">doi:10.1080/01621459.2016.1195744</a>
</p>
<p>Li, Z &amp; S.N. Wood (2020) Faster model matrix crossproducts for large generalized linear models with discretized covariates. Statistics and Computing. 30:19-25
<a href="https://doi.org/10.1007/s11222-019-09864-2">doi:10.1007/s11222-019-09864-2</a>
</p>
<p>Development of mgcv version 1.8 was part funded by EPSRC grants EP/K005251/1 and EP/I000917/1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see examples for gam, bam and gamm
</code></pre>

<hr>
<h2 id='anova.gam'>Approximate hypothesis tests related to  GAM fits</h2><span id='topic+anova.gam'></span><span id='topic+print.anova.gam'></span>

<h3>Description</h3>

<p> Performs hypothesis tests relating to one or more fitted
<code>gam</code> objects. For a single fitted <code>gam</code> object, Wald tests of
the significance of each parametric and smooth term are performed, so interpretation 
is analogous to <code><a href="stats.html#topic+drop1">drop1</a></code> rather than <code>anova.lm</code> (i.e. it's like type III ANOVA, 
rather than a sequential type I ANOVA). Otherwise the fitted models are compared using an analysis of deviance table or GLRT test: this latter approach should not be use to test the significance of terms which can be penalized 
to zero. Models to be compared should be fitted to the same data using the same smoothing parameter selection method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
anova(object, ..., dispersion = NULL, test = NULL,
                    freq = FALSE)
## S3 method for class 'anova.gam'
print(x, digits = max(3, getOption("digits") - 3),...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="anova.gam_+3A_object">object</code>, <code id="anova.gam_+3A_...">...</code></td>
<td>
<p> fitted model objects of class <code>gam</code> as produced by <code>gam()</code>.</p>
</td></tr>
<tr><td><code id="anova.gam_+3A_x">x</code></td>
<td>
<p>an <code>anova.gam</code> object produced by a single model call to <code>anova.gam()</code>.</p>
</td></tr> 
<tr><td><code id="anova.gam_+3A_dispersion">dispersion</code></td>
<td>
<p> a value for the dispersion parameter: not normally used.</p>
</td></tr>
<tr><td><code id="anova.gam_+3A_test">test</code></td>
<td>
<p>what sort of test to perform for a multi-model call. One of
<code>"Chisq"</code>, <code>"F"</code> or <code>"Cp"</code>. Reset to <code>"Chisq"</code> for extended and general families unless <code>NULL</code>. </p>
</td></tr>
<tr><td><code id="anova.gam_+3A_freq">freq</code></td>
<td>
<p>whether to use frequentist or Bayesian approximations for parametric term 
p-values. See <code><a href="#topic+summary.gam">summary.gam</a></code> for details.</p>
</td></tr>
<tr><td><code id="anova.gam_+3A_digits">digits</code></td>
<td>
<p>number of digits to use when printing output.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> If more than one fitted model is provided than <code>anova.glm</code> is
used, with the difference in model degrees of freedom being taken as the difference 
in effective degress of freedom (when possible this is a smoothing parameter uncertainty corrected version).
For extended and general families this is set so that a GLRT test is used.  The p-values resulting from the multi-model case are only approximate, 
and must be used with care. The approximation is most accurate when the comparison 
relates to unpenalized terms, or smoothers with a null space of dimension greater than zero.
(Basically we require that the difference terms could be well approximated by unpenalized 
terms with degrees of freedom approximately the effective degrees of freedom). In simulations the 
p-values are usually slightly too low. For terms with a zero-dimensional null space 
(i.e. those which can be penalized to zero) the approximation is often very poor, and significance 
can be greatly overstated: i.e. p-values are often substantially too low. This case applies to random effect terms. 
</p>
<p>Note also that in the multi-model call to <code>anova.gam</code>, it is quite possible for a model with more terms to end up with lower effective degrees of freedom, but better fit, than the notionally null model with fewer terms. In such cases it is very rare that it makes sense to perform any sort of test, since there is then no basis on which to accept the notional null model. 
</p>
<p>If only one model is provided then the significance of each model term
is assessed using Wald like tests, conditional on the smoothing parameter estimates: see <code><a href="#topic+summary.gam">summary.gam</a></code> 
and Wood (2013a,b) for details. The p-values  provided here are better justified than in the multi model case, and have close to the 
correct distribution under the null, unless smoothing parameters are poorly identified. ML or REML smoothing parameter selection leads to 
the best results in simulations as they tend to avoid occasional severe undersmoothing. In replication of the full simulation study of Scheipl et al. (2008) the tests give almost indistinguishable power to the method recommended there, but slightly too low p-values under the null in their section 3.1.8 test for a smooth interaction (the Scheipl et al. recommendation is not used directly, because it only applies in the Gaussian case, and requires model refits, but it is available in package <code>RLRsim</code>). 
</p>
<p>In the single model case <code>print.anova.gam</code> is used as the printing method. 
</p>
<p>By default the p-values for parametric model terms are also based on Wald tests using the Bayesian 
covariance matrix for the coefficients. This is appropriate when there are &quot;re&quot; terms present, and is 
otherwise rather similar to the results using the frequentist covariance matrix (<code>freq=TRUE</code>), since 
the parametric terms themselves are usually unpenalized. Default P-values for parameteric terms that are 
penalized using the <code>paraPen</code> argument will not be good.
</p>


<h3>Value</h3>

<p>In the multi-model case <code>anova.gam</code> produces output identical to
<code><a href="stats.html#topic+anova.glm">anova.glm</a></code>, which it in fact uses.
</p>
<p>In the single model case an object of class <code>anova.gam</code> is produced,
which is in fact an object returned from <code><a href="#topic+summary.gam">summary.gam</a></code>.
</p>
<p><code>print.anova.gam</code> simply produces tabulated output.
</p>


<h3>WARNING</h3>

<p> If models 'a' and 'b' differ only in terms with no un-penalized components (such as random effects) then 
p values from anova(a,b) are unreliable, and usually much too low.
</p>
<p>Default P-values will usually be wrong for parametric terms penalized using &lsquo;paraPen&rsquo;: use freq=TRUE
to obtain better p-values when the penalties are full rank and represent conventional random effects.
</p>
<p>For a single model, interpretation is similar to drop1, not anova.lm.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> with substantial
improvements by Henric Nilsson.</p>


<h3>References</h3>

<p>Scheipl, F., Greven, S. and Kuchenhoff, H. (2008) Size and power of tests for a zero random effect variance or polynomial 
regression in additive and linear mixed models. Comp. Statist. Data Anal. 52, 3283-3299
</p>
<p>Wood, S.N. (2013a) On p-values for smooth components of an extended generalized additive model. Biometrika 100:221-228 <a href="https://doi.org/10.1093/biomet/ass048">doi:10.1093/biomet/ass048</a>
</p>
<p>Wood, S.N. (2013b) A simple test for random effects in regression models. Biometrika 100:1005-1010 <a href="https://doi.org/10.1093/biomet/ast038">doi:10.1093/biomet/ast038</a>
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+predict.gam">predict.gam</a></code>,
<code><a href="#topic+gam.check">gam.check</a></code>, <code><a href="#topic+summary.gam">summary.gam</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(0)
dat &lt;- gamSim(5,n=200,scale=2)

b&lt;-gam(y ~ x0 + s(x1) + s(x2) + s(x3),data=dat)
anova(b)
b1&lt;-gam(y ~ x0 + s(x1) + s(x2),data=dat)
anova(b,b1,test="F")
</code></pre>

<hr>
<h2 id='bam'>Generalized additive models for very large datasets</h2><span id='topic+bam'></span>

<h3>Description</h3>

<p> Fits a generalized additive model (GAM) to a very large
data set, the term &lsquo;GAM&rsquo; being taken to include any quadratically penalized GLM (the extended families
listed in <code><a href="#topic+family.mgcv">family.mgcv</a></code> can also be used).  
The degree of smoothness of model terms is estimated as part of
fitting. In use the function is much like <code><a href="#topic+gam">gam</a></code>, except that the numerical methods
are designed for datasets containing upwards of several tens of thousands of data (see Wood, Goude and Shaw, 2015). The advantage 
of <code>bam</code> is much lower memory footprint than <code><a href="#topic+gam">gam</a></code>, but it can also be much faster, 
for large datasets. <code>bam</code> can also compute on a cluster set up by the <a href="parallel.html#topic+parallel-package">parallel</a> package.
</p>
<p>An alternative fitting approach (Wood et al. 2017, Li and Wood, 2019) is provided by the <code>discrete==TRUE</code> method. In this case a method based on discretization of covariate values and C code level parallelization (controlled by the <code>nthreads</code> argument instead of the <code>cluster</code> argument) is used. This extends both the data set and model size that are practical. Number of response data can not exceed <code>.Machine$integer.max</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bam(formula,family=gaussian(),data=list(),weights=NULL,subset=NULL,
    na.action=na.omit, offset=NULL,method="fREML",control=list(),
    select=FALSE,scale=0,gamma=1,knots=NULL,sp=NULL,min.sp=NULL,
    paraPen=NULL,chunk.size=10000,rho=0,AR.start=NULL,discrete=FALSE,
    cluster=NULL,nthreads=1,gc.level=0,use.chol=FALSE,samfrac=1,
    coef=NULL,drop.unused.levels=TRUE,G=NULL,fit=TRUE,drop.intercept=NULL,
    in.out=NULL,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="bam_+3A_formula">formula</code></td>
<td>
<p> A GAM formula (see <code><a href="#topic+formula.gam">formula.gam</a></code> and also <code><a href="#topic+gam.models">gam.models</a></code>). 
This is exactly like the formula for a GLM except that smooth terms, <code>s</code> and <code>te</code> can be added 
to the right hand side to specify that the linear predictor depends on smooth functions of predictors 
(or linear functionals of these).
</p>
</td></tr> 
<tr><td><code id="bam_+3A_family">family</code></td>
<td>

<p>This is a family object specifying the distribution and link to use in
fitting etc. See <code><a href="stats.html#topic+glm">glm</a></code> and <code><a href="stats.html#topic+family">family</a></code> for more
details. The extended families listed in <code><a href="#topic+family.mgcv">family.mgcv</a></code> can also be used.
</p>
</td></tr> 
<tr><td><code id="bam_+3A_data">data</code></td>
<td>
<p> A data frame or list containing the model response variable and 
covariates required by the formula. By default the variables are taken 
from <code>environment(formula)</code>: typically the environment from 
which <code>gam</code> is called.</p>
</td></tr> 
<tr><td><code id="bam_+3A_weights">weights</code></td>
<td>
<p>  prior weights on the contribution of the data to the log likelihood. Note that a weight of 2, for example, 
is equivalent to having made exactly the same observation twice. If you want to reweight the contributions 
of each datum without changing the overall magnitude of the log likelihood, then you should normalize the weights
(e.g. <code>weights &lt;- weights/mean(weights)</code>).</p>
</td></tr>
<tr><td><code id="bam_+3A_subset">subset</code></td>
<td>
<p> an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td></tr>
<tr><td><code id="bam_+3A_na.action">na.action</code></td>
<td>
<p> a function which indicates what should happen when the data
contain &lsquo;NA&rsquo;s.  The default is set by the &lsquo;na.action&rsquo; setting
of &lsquo;options&rsquo;, and is &lsquo;na.fail&rsquo; if that is unset.  The
&ldquo;factory-fresh&rdquo; default is &lsquo;na.omit&rsquo;.</p>
</td></tr>
<tr><td><code id="bam_+3A_offset">offset</code></td>
<td>
<p>Can be used to supply a model offset for use in fitting. Note
that this offset will always be completely ignored when predicting, unlike an offset 
included in <code>formula</code> (this used to conform to the behaviour of
<code>lm</code> and <code>glm</code>).</p>
</td></tr>
<tr><td><code id="bam_+3A_method">method</code></td>
<td>
<p>The smoothing parameter estimation method. <code>"GCV.Cp"</code> to use GCV for unknown scale parameter and
Mallows' Cp/UBRE/AIC for known scale. <code>"GACV.Cp"</code> is equivalent, but using GACV in place of GCV. <code>"REML"</code> 
for REML estimation, including of unknown scale, <code>"P-REML"</code> for REML estimation, but using a Pearson estimate 
of the scale. <code>"ML"</code> and <code>"P-ML"</code> are similar, but using maximum likelihood in place of REML. Default 
<code>"fREML"</code> uses fast REML computation.</p>
</td></tr>
<tr><td><code id="bam_+3A_control">control</code></td>
<td>
<p>A list of fit control parameters to replace defaults returned by 
<code><a href="#topic+gam.control">gam.control</a></code>. Any control parameters not supplied stay at their default values.</p>
</td></tr>
<tr><td><code id="bam_+3A_select">select</code></td>
<td>
<p>Should selection penalties be added to the smooth effects, so that they can in principle be 
penalized out of the model? See <code>gamma</code> to increase penalization.  Has the side effect that smooths no longer have a fixed effect component (improper prior from a Bayesian perspective) allowing REML comparison of models with the same fixed effect structure. 
</p>
</td></tr>
<tr><td><code id="bam_+3A_scale">scale</code></td>
<td>
<p> If this is positive then it is taken as the known scale parameter. Negative signals that the 
scale paraemter is unknown. 0 signals that the scale parameter is 1  for Poisson and binomial and unknown otherwise. 
Note that (RE)ML methods can only work with scale parameter 1 for the Poisson and binomial cases.    
</p>
</td></tr> 
<tr><td><code id="bam_+3A_gamma">gamma</code></td>
<td>
<p>Increase above 1 to force smoother fits. <code>gamma</code> is used to multiply the effective degrees of freedom in the GCV/UBRE/AIC score (so <code>log(n)/2</code> is BIC like). <code>n/gamma</code> can be viewed as an effective sample size, which allows it to play a similar role for RE/ML smoothing parameter estimation.</p>
</td></tr> 
<tr><td><code id="bam_+3A_knots">knots</code></td>
<td>
<p>this is an optional list containing user specified knot values to be used for basis construction. 
For most bases the user simply supplies the knots to be used, which must match up with the <code>k</code> value
supplied (note that the number of knots is not always just <code>k</code>). 
See <code><a href="#topic+tprs">tprs</a></code> for what happens in the <code>"tp"/"ts"</code> case. 
Different terms can use different numbers of knots, unless they share a covariate.
</p>
</td></tr>
<tr><td><code id="bam_+3A_sp">sp</code></td>
<td>
<p>A vector of smoothing parameters can be provided here.
Smoothing parameters must be supplied in the order that the smooth terms appear in the model 
formula. Negative elements indicate that the parameter should be estimated, and hence a mixture 
of fixed and estimated parameters is possible. If smooths share smoothing parameters then <code>length(sp)</code> 
must correspond to the number of underlying smoothing parameters. Note that <code>discrete=TRUE</code>may result in
re-ordering of variables in tensor product smooths for improved efficiency, and <code>sp</code> must be supplied in re-ordered order.</p>
</td></tr>
<tr><td><code id="bam_+3A_min.sp">min.sp</code></td>
<td>
<p>Lower bounds can be supplied for the smoothing parameters. Note
that if this option is used then the smoothing parameters <code>full.sp</code>, in the 
returned object, will need to be added to what is supplied here to get the 
smoothing parameters actually multiplying the penalties. <code>length(min.sp)</code> should 
always be the same as the total number of penalties (so it may be longer than <code>sp</code>,
if smooths share smoothing parameters).</p>
</td></tr>
<tr><td><code id="bam_+3A_parapen">paraPen</code></td>
<td>
<p>optional list specifying any penalties to be applied to parametric model terms. 
<code><a href="#topic+gam.models">gam.models</a></code> explains more.</p>
</td></tr>
<tr><td><code id="bam_+3A_chunk.size">chunk.size</code></td>
<td>
<p>The model matrix is created in chunks of this size, rather than ever being formed whole. 
Reset to <code>4*p</code> if <code>chunk.size &lt; 4*p</code> where <code>p</code> is the number of coefficients.</p>
</td></tr>
<tr><td><code id="bam_+3A_rho">rho</code></td>
<td>
<p>An AR1 error model can be used for the residuals (based on dataframe order), of Gaussian-identity 
link models. This is the AR1 correlation parameter. Standardized residuals (approximately 
uncorrelated under correct model) returned in 
<code>std.rsd</code> if non zero. Also usable with other models when <code>discrete=TRUE</code>, in which case the AR model
is applied to the working residuals and corresponds to a GEE approximation.</p>
</td></tr>
<tr><td><code id="bam_+3A_ar.start">AR.start</code></td>
<td>
<p>logical variable of same length as data, <code>TRUE</code> at first observation of an independent
section of AR1 correlation. Very first observation in data frame does not need this. If <code>NULL</code> then 
there are no breaks in AR1 correlaion.</p>
</td></tr>
<tr><td><code id="bam_+3A_discrete">discrete</code></td>
<td>
<p>with <code>method="fREML"</code> it is possible to discretize covariates for storage and efficiency reasons.
If <code>discrete</code> is <code>TRUE</code>, a number or a vector of numbers for each smoother term, then discretization happens. If numbers are supplied they give the number of discretization bins. Parametric terms use the maximum number specified.</p>
</td></tr>
<tr><td><code id="bam_+3A_cluster">cluster</code></td>
<td>
<p><code>bam</code> can compute the computationally dominant QR decomposition in parallel using <a href="parallel.html#topic+clusterApply">parLapply</a>
from the <code>parallel</code> package, if it is supplied with a cluster on which to do this (a cluster here can be some cores of a 
single machine). See details and example code. 
</p>
</td></tr>
<tr><td><code id="bam_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of threads to use for non-cluster computation (e.g. combining results from cluster nodes).
If <code>NA</code> set to <code>max(1,length(cluster))</code>. See details.</p>
</td></tr>
<tr><td><code id="bam_+3A_gc.level">gc.level</code></td>
<td>
<p>to keep the memory footprint down, it can help to call the garbage collector often, but this takes 
a substatial amount of time. Setting this to zero means that garbage collection only happens when R decides it should. Setting to 2 gives frequent garbage collection. 1 is in between. Not as much of a problem as it used to be, but can really matter for very large datasets.
</p>
</td></tr>
<tr><td><code id="bam_+3A_use.chol">use.chol</code></td>
<td>
<p>By default <code>bam</code> uses a very stable QR update approach to obtaining the QR decomposition
of the model matrix. For well conditioned models an alternative accumulates the crossproduct of the model matrix
and then finds its Choleski decomposition, at the end. This is somewhat more efficient, computationally.</p>
</td></tr>
<tr><td><code id="bam_+3A_samfrac">samfrac</code></td>
<td>
<p>For very large sample size Generalized additive models the number of iterations needed for the model fit can 
be reduced by first fitting a model to a random sample of the data, and using the results to supply starting values. This initial fit is run with sloppy convergence tolerances, so is typically very low cost. <code>samfrac</code> is the sampling fraction to use. 0.1 is often reasonable. </p>
</td></tr>
<tr><td><code id="bam_+3A_coef">coef</code></td>
<td>
<p>initial values for model coefficients</p>
</td></tr>
<tr><td><code id="bam_+3A_drop.unused.levels">drop.unused.levels</code></td>
<td>
<p>by default unused levels are dropped from factors before fitting. For some smooths 
involving factor variables you might want to turn this off. Only do so if you know what you are doing.</p>
</td></tr>
<tr><td><code id="bam_+3A_g">G</code></td>
<td>
<p>if not <code>NULL</code> then this should be the object returned by a previous call to <code>bam</code> with 
<code>fit=FALSE</code>. Causes all other arguments to be ignored except <code>sp</code>, <code>chunk.size</code>, <code>gamma</code>,<code>nthreads</code>, <code>cluster</code>, <code>rho</code>, <code>gc.level</code>, <code>samfrac</code>, <code>use.chol</code>, <code>method</code> and <code>scale</code> (if &gt;0).</p>
</td></tr>
<tr><td><code id="bam_+3A_fit">fit</code></td>
<td>
<p>if <code>FALSE</code> then the model is set up for fitting but not estimated, and an object is returned, suitable for passing as the <code>G</code> argument to <code>bam</code>.</p>
</td></tr>
<tr><td><code id="bam_+3A_drop.intercept">drop.intercept</code></td>
<td>
<p>Set to <code>TRUE</code> to force the model to really not have the a constant in the parametric model part,
even with factor variables present.</p>
</td></tr>
<tr><td><code id="bam_+3A_in.out">in.out</code></td>
<td>
<p>If supplied then this is a two item list of intial values. <code>sp</code> is initial smoothing parameter estiamtes and <code>scale</code> the initial scale parameter estimate (set to 1 if famiy does not have one).</p>
</td></tr>
<tr><td><code id="bam_+3A_...">...</code></td>
<td>
<p>further arguments for 
passing on e.g. to <code>gam.fit</code> (such as <code>mustart</code>). </p>
</td></tr>
</table>


<h3>Details</h3>

<p> When <code>discrete=FALSE</code>, <code>bam</code> operates by first setting up the basis characteristics for the smooths, using a representative subsample of the data. Then the model matrix is constructed in blocks using <code><a href="#topic+predict.gam">predict.gam</a></code>. For each block the factor R,
from the QR decomposition of the whole model matrix is updated, along with Q'y. and the sum of squares of y. At the end of 
block processing, fitting takes place, without the need to ever form the whole model matrix. 
</p>
<p>In the generalized case, the same trick is used with the weighted model matrix and weighted pseudodata, at each step of the PIRLS.
Smoothness selection is performed on the working model at each stage (performance oriented iteration), to maintain the 
small memory footprint. This is trivial to justify in the case of GCV or Cp/UBRE/AIC based model selection, and 
for REML/ML is justified via the asymptotic multivariate normality of Q'z where z is the IRLS pseudodata. 
</p>
<p>For full method details see Wood, Goude and Shaw (2015).
</p>
<p>Note that POI is not as stable as the default nested iteration used with <code><a href="#topic+gam">gam</a></code>, but that for very large, information rich,
datasets, this is unlikely to matter much. 
</p>
<p>Note also that it is possible to spend most of the computational time on basis evaluation, if an expensive basis is used. In practice this means that the default <code>"tp"</code> basis should be avoided: almost any other basis (e.g. <code>"cr"</code> or <code>"ps"</code>) 
can be used in the 1D case, and tensor product smooths (<code>te</code>) are typically much less costly in the multi-dimensional case. 
</p>
<p>If <code>cluster</code> is provided as a cluster set up using <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code> (or <code><a href="parallel.html#topic+makeCluster">makeForkCluster</a></code>) from the <code>parallel</code> package, then the rate limiting QR decomposition of the model matrix is performed in parallel using this cluster. Note that the speed ups are often not that great. On a multi-core machine it is usually best to set the cluster size to the number of physical cores, which is often less than what is reported by <code><a href="parallel.html#topic+detectCores">detectCores</a></code>. Using more than the number of physical cores can result in no speed up at all (or even a slow down). Note that a highly parallel BLAS may negate all advantage from using a cluster of cores. Computing in parallel of course requires more memory than computing in series. See examples.
</p>
<p>When <code>discrete=TRUE</code> the covariate data are first discretized. Discretization takes place on a smooth by smooth basis, or in the case of tensor product smooths (or any smooth that can be represented as such, such as random effects), separately for each marginal smooth. The required spline bases are then evaluated at the discrete values, and stored, along with index vectors indicating which original observation they relate to. Fitting is by a version of performance oriented iteration/PQL using REML smoothing parameter selection on each iterative working model (as for the default method). The iteration is based on the derivatives of the REML score, without computing the score itself, allowing the expensive computations to be reduced to one parallel block Cholesky decomposition per iteration (plus two basic operations of equal cost, but easily parallelized). Unlike standard POI/PQL, only one step of the smoothing parameter update for the working model is taken at each step (rather than iterating to the optimal set of smoothing parameters for each working model). At each step a weighted model matrix crossproduct of the model matrix is required - this is efficiently computed from the pre-computed basis functions evaluated at the discretized covariate values. Efficient computation with tensor product terms means that some terms within a tensor product may be re-ordered for maximum efficiency. See Wood et al (2017) and Li and Wood (2019) for full details.
</p>
<p>When <code>discrete=TRUE</code> parallel computation is controlled using the <code>nthreads</code> argument. For this method no cluster computation is used, and the <code>parallel</code> package is not required. Note that actual speed up from parallelization depends on the BLAS installed and your hardware. With the (R default) reference BLAS using several threads can make a substantial difference, but with a single threaded tuned BLAS, such as openblas, the effect is less marked (since cache use is typically optimized for one thread, and is then sub optimal for several). However the tuned BLAS is usually much faster than using the reference BLAS, however many threads you use. If you have a multi-threaded BLAS installed then you should leave <code>nthreads</code> at 1, since calling a multi-threaded BLAS from multiple threads usually slows things down: the only exception to this is that you might choose to form discrete matrix cross products (the main cost in the fitting routine) in a multi-threaded way, but use single threaded code for other computations: this can be achieved by e.g. <code>nthreads=c(2,1)</code>, which would use 2 threads for discrete inner products, and 1 for most code calling BLAS. Not that the basic reason that multi-threaded performance is often disappointing is that most computers are heavily memory bandwidth limited, not flop rate limited. It is hard to get data to one core fast enough, let alone trying to get data simultaneously to several cores.   
</p>
<p><code>discrete=TRUE</code> will often produce identical results to the methods without discretization, since covariates often only take a modest number of discrete values anyway, so no approximation at all is involved in the discretization process. Even when some approximation is involved, the differences are often very small as the algorithms discretize marginally whenever possible. For example each margin of a tensor product smooth is discretized separately, rather than discretizing onto a grid of covariate values (for an equivalent isotropic smooth we would have to discretize onto a grid). The marginal approach allows quite fine scale discretization and hence very low approximation error. Note that when using the smooth <code>id</code> mechanism to link smoothing parameters, the discrete method cannot force the linked bases to be identical, so some differences to the none discrete methods will be noticable.   
</p>
<p>The extended families given in <code><a href="#topic+family.mgcv">family.mgcv</a></code> can also be used. The extra parameters of these are estimated by maximizing the penalized likelihood, rather than the restricted marginal likelihood as in <code><a href="#topic+gam">gam</a></code>. So estimates may differ slightly from those returned by <code><a href="#topic+gam">gam</a></code>. Estimation is accomplished by a Newton iteration to find the extra parameters (e.g. the theta parameter of the negative binomial or the degrees of freedom and scale of the scaled t) maximizing the log likelihood given the model coefficients at each iteration of the fitting procedure. 
</p>


<h3>Value</h3>

 
<p>An object of class <code>"gam"</code> as described in <code><a href="#topic+gamObject">gamObject</a></code>.
</p>


<h3>WARNINGS </h3>

<p>The routine may be slower than optimal if the default <code>"tp"</code> basis is used. 
</p>
<p>This routine is less stable than &lsquo;gam&rsquo; for the same dataset.
</p>
<p>With <code>discrete=TRUE</code>, <code>te</code> terms are efficiently computed, but <code>t2</code> are not.
</p>
<p>Anything close to the maximum n of <code>.Machine$integer.max</code> will need a very large amount of RAM and probably <code>gc.level=1</code>. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N., Goude, Y. &amp; Shaw S. (2015) Generalized additive models for large datasets. Journal of the Royal Statistical Society, Series C 64(1): 139-155.
<a href="https://doi.org/10.1111/rssc.12068">doi:10.1111/rssc.12068</a>
</p>
<p>Wood, S.N., Li, Z., Shaddick, G. &amp; Augustin N.H. (2017) Generalized additive models for gigadata: modelling the UK black smoke network daily data. Journal of the American Statistical Association. 112(519):1199-1210
<a href="https://doi.org/10.1080/01621459.2016.1195744">doi:10.1080/01621459.2016.1195744</a>
</p>
<p>Li, Z &amp; S.N. Wood (2020) Faster model matrix crossproducts for large generalized linear models with discretized covariates. Statistics and Computing. 30:19-25
<a href="https://doi.org/10.1007/s11222-019-09864-2">doi:10.1007/s11222-019-09864-2</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mgcv.parallel">mgcv.parallel</a></code>, 
<code><a href="#topic+mgcv-package">mgcv-package</a></code>, <code><a href="#topic+gamObject">gamObject</a></code>, <code><a href="#topic+gam.models">gam.models</a></code>, <code><a href="#topic+smooth.terms">smooth.terms</a></code>,
<code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code>, <code><a href="#topic+s">s</a></code>,
<code><a href="#topic+te">te</a></code> <code><a href="#topic+predict.gam">predict.gam</a></code>,
<code><a href="#topic+plot.gam">plot.gam</a></code>, <code><a href="#topic+summary.gam">summary.gam</a></code>, <code><a href="#topic+gam.side">gam.side</a></code>,
<code><a href="#topic+gam.selection">gam.selection</a></code>, <code><a href="#topic+gam.control">gam.control</a></code>
<code><a href="#topic+gam.check">gam.check</a></code>, <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code> <code><a href="#topic+negbin">negbin</a></code>, <code><a href="#topic+magic">magic</a></code>,<code><a href="#topic+vis.gam">vis.gam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## See help("mgcv-parallel") for using bam in parallel

## Sample sizes are small for fast run times.

set.seed(3)
dat &lt;- gamSim(1,n=25000,dist="normal",scale=20)
bs &lt;- "cr";k &lt;- 12
b &lt;- bam(y ~ s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k)+
           s(x3,bs=bs),data=dat)
summary(b)
plot(b,pages=1,rug=FALSE)  ## plot smooths, but not rug
plot(b,pages=1,rug=FALSE,seWithMean=TRUE) ## `with intercept' CIs

 
ba &lt;- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
            s(x3,bs=bs,k=k),data=dat,method="GCV.Cp") ## use GCV
summary(ba)

## A Poisson example...

k &lt;- 15
dat &lt;- gamSim(1,n=21000,dist="poisson",scale=.1)

system.time(b1 &lt;- bam(y ~ s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k),
            data=dat,family=poisson()))
b1

## Similar using faster discrete method...

 
system.time(b2 &lt;- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
            s(x3,bs=bs,k=k),data=dat,family=poisson(),discrete=TRUE))
b2



</code></pre>

<hr>
<h2 id='bam.update'>Update a strictly additive bam model for new data.</h2><span id='topic+bam.update'></span>

<h3>Description</h3>

<p> Gaussian with identity link models fitted by <code><a href="#topic+bam">bam</a></code> can be efficiently updated as new data becomes available,
by simply updating the QR decomposition on which estimation is based, and re-optimizing the smoothing parameters, starting
from the previous estimates. This routine implements this.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bam.update(b,data,chunk.size=10000)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="bam.update_+3A_b">b</code></td>
<td>
<p> A <code>gam</code> object fitted by <code><a href="#topic+bam">bam</a></code> and representing a strictly additive model 
(i.e. <code>gaussian</code> errors, <code>identity</code> link).</p>
</td></tr>
<tr><td><code id="bam.update_+3A_data">data</code></td>
<td>
<p>Extra data to augment the original data used to obtain <code>b</code>. Must include a <code>weights</code> column if the 
original fit was weighted and a <code>AR.start</code> column if <code>AR.start</code> was non <code>NULL</code> in original fit.</p>
</td></tr>
<tr><td><code id="bam.update_+3A_chunk.size">chunk.size</code></td>
<td>
<p>size of subsets of data to process in one go when getting fitted values.</p>
</td></tr>
</table>


<h3>Details</h3>

 <p><code>bam.update</code> updates the QR decomposition of the (weighted) model matrix of the GAM represented by <code>b</code> to take 
account of the new data. The orthogonal factor multiplied by the response vector is also updated. Given these updates the model 
and smoothing parameters can be re-estimated, as if the whole dataset (original and the new data) had been fitted in one go. The 
function will use the same AR1 model for the residuals as that employed in the original model fit (see <code>rho</code> parameter 
of <code><a href="#topic+bam">bam</a></code>).
</p>
<p>Note that there may be small numerical differences in fit between fitting the data all at once, and fitting in 
stages by updating, if the smoothing bases used have any of their details set with reference 
to the data (e.g. default knot locations).
</p>


<h3>Value</h3>

 
<p>An object of class <code>"gam"</code> as described in <code><a href="#topic+gamObject">gamObject</a></code>.
</p>


<h3>WARNINGS </h3>

<p>AIC computation does not currently take account of AR model, if used.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mgcv-package">mgcv-package</a></code>, <code><a href="#topic+bam">bam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## following is not *very* large, for obvious reasons...
set.seed(8)
n &lt;- 5000
dat &lt;- gamSim(1,n=n,dist="normal",scale=5)
dat[c(50,13,3000,3005,3100),]&lt;- NA
dat1 &lt;- dat[(n-999):n,]
dat0 &lt;- dat[1:(n-1000),]
bs &lt;- "ps";k &lt;- 20
method &lt;- "GCV.Cp"
b &lt;- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
           s(x3,bs=bs,k=k),data=dat0,method=method)

b1 &lt;- bam.update(b,dat1)

b2 &lt;- bam.update(bam.update(b,dat1[1:500,]),dat1[501:1000,])
 
b3 &lt;- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
           s(x3,bs=bs,k=k),data=dat,method=method)
b1;b2;b3

## example with AR1 errors...

e &lt;- rnorm(n)
for (i in 2:n) e[i] &lt;- e[i-1]*.7 + e[i]
dat$y &lt;- dat$f + e*3
dat[c(50,13,3000,3005,3100),]&lt;- NA
dat1 &lt;- dat[(n-999):n,]
dat0 &lt;- dat[1:(n-1000),]

b &lt;- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
           s(x3,bs=bs,k=k),data=dat0,rho=0.7)

b1 &lt;- bam.update(b,dat1)


summary(b1);summary(b2);summary(b3)

</code></pre>

<hr>
<h2 id='bandchol'>Choleski decomposition of a band diagonal matrix</h2><span id='topic+bandchol'></span>

<h3>Description</h3>

 
<p>Computes Choleski decomposition of a (symmetric positive definite) band-diagonal matrix, <code>A</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bandchol(B)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bandchol_+3A_b">B</code></td>
<td>
<p>An n by k matrix containing the diagonals of the matrix <code>A</code> to be decomposed. First row is leading diagonal, next is first sub-diagonal, etc. sub-diagonals are zero padded at the end. Alternatively gives <code>A</code> directly, i.e. a square matrix with 2k-1 non zero diagonals (those from the lower triangle are not accessed).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calls <code>dpbtrf</code> from <code>LAPACK</code>. The point of this is that it has <code class="reqn">O(k^2n)</code> computational cost, rather than the <code class="reqn">O(n^3)</code> required by dense matrix methods.
</p>


<h3>Value</h3>

<p>Let <code>R</code> be the factor such that <code>t(R)%*%R = A</code>. <code>R</code> is upper triangular and if the rows of <code>B</code> contained the diagonals of <code>A</code> on entry, then what is returned is an n by k matrix containing the diagonals of <code>R</code>, packed as <code>B</code> was packed on entry. If <code>B</code> was square on entry, then <code>R</code> is returned directly. See examples.  
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Anderson, E., Bai, Z., Bischof, C., Blackford, S., Dongarra, J., Du Croz, J., Greenbaum, A., Hammarling, S., McKenney, A. and Sorensen, D., 1999. LAPACK Users' guide (Vol. 9). Siam.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
## simulate a banded diagonal matrix
n &lt;- 7;set.seed(8)
A &lt;- matrix(0,n,n)
sdiag(A) &lt;- runif(n);sdiag(A,1) &lt;- runif(n-1)
sdiag(A,2) &lt;- runif(n-2)
A &lt;- crossprod(A) 

## full matrix form...
bandchol(A)
chol(A) ## for comparison

## compact storage form...
B &lt;- matrix(0,3,n)
B[1,] &lt;- sdiag(A);B[2,1:(n-1)] &lt;- sdiag(A,1)
B[3,1:(n-2)] &lt;- sdiag(A,2)
bandchol(B)

</code></pre>

<hr>
<h2 id='betar'>GAM beta regression family</h2><span id='topic+betar'></span>

<h3>Description</h3>

<p>Family for use with <code><a href="#topic+gam">gam</a></code> or <code><a href="#topic+bam">bam</a></code>, implementing regression for beta distributed data on (0,1).
A linear predictor controls the mean, <code class="reqn">\mu</code> of the beta distribution, while the variance is then
<code class="reqn">\mu(1-\mu)/(1+\phi)</code>, with parameter <code class="reqn">\phi</code> being estimated during 
fitting, alongside the smoothing parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>betar(theta = NULL, link = "logit",eps=.Machine$double.eps*100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="betar_+3A_theta">theta</code></td>
<td>
<p>the extra parameter (<code class="reqn">\phi</code> above). </p>
</td></tr>
<tr><td><code id="betar_+3A_link">link</code></td>
<td>
<p>The link function: one of <code>"logit"</code>, <code>"probit"</code>, <code>"cloglog"</code> and <code>"cauchit"</code>.</p>
</td></tr>
<tr><td><code id="betar_+3A_eps">eps</code></td>
<td>
<p>the response variable will be truncated to the interval <code>[eps,1-eps]</code> if there are values outside this range.
This truncation is not entirely benign, but too small a value of <code>eps</code> will cause stability problems if there are 
zeroes or ones in the response.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These models are useful for proportions data which can not be modelled as binomial. Note the assumption that data are in 
(0,1), despite the fact that for some parameter values 0 and 1 are perfectly legitimate observations. The restriction is needed to 
keep the log likelihood bounded for all parameter values. Any data exactly at 0 or 1 are reset to be just above 0 or just below 1 using the <code>eps</code> argument (in fact any observation <code>&lt;eps</code> is reset to <code>eps</code> and any observation <code>&gt;1-eps</code> is reset to <code>1-eps</code>). Note the effect of this resetting. If <code class="reqn">\mu\phi&gt;1</code> then impossible 0s are replaced with highly improbable <code>eps</code> values. If the inequality is reversed then 0s with infinite probability density are replaced with <code>eps</code> values having high finite probability density. The equivalent condition for 1s is <code class="reqn">(1-\mu)\phi&gt;1</code>. Clearly all types of resetting are somewhat unsatisfactory, and care is needed if data contain 0s or 1s (often it makes sense to manually reset the 0s and 1s in a manner that somehow reflects the sampling setup).  
</p>


<h3>Value</h3>

<p>An object of class <code>extended.family</code>.
</p>


<h3>WARNINGS</h3>

<p>Do read the details section if your data contain 0s and or 1s. 
</p>


<h3>Author(s)</h3>

<p> Natalya Pya (nat.pya@gmail.com) and Simon Wood (s.wood@r-project.org)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## Simulate some beta data...
set.seed(3);n&lt;-400
dat &lt;- gamSim(1,n=n)
mu &lt;- binomial()$linkinv(dat$f/4-2)
phi &lt;- .5
a &lt;- mu*phi;b &lt;- phi - a;
dat$y &lt;- rbeta(n,a,b) 

bm &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=betar(link="logit"),data=dat)

bm
plot(bm,pages=1)
</code></pre>

<hr>
<h2 id='blas.thread.test'>BLAS thread safety</h2><span id='topic+blas.thread.test'></span>

<h3>Description</h3>

<p>Most BLAS implementations are thread safe, but some versions of OpenBLAS, for example, are not. This routine is a diagnostic helper function, which you will never need if you don't set <code>nthreads&gt;1</code>, and even then are unlikely to need.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blas.thread.test(n=1000,nt=4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blas.thread.test_+3A_n">n</code></td>
<td>
<p>Number of iterations to run of parallel BLAS calling code.</p>
</td></tr>
<tr><td><code id="blas.thread.test_+3A_nt">nt</code></td>
<td>
<p>Number of parallel threads to use</p>
</td></tr>
</table>


<h3>Details</h3>

<p>While single threaded OpenBLAS 0.2.20 was thread safe, versions 0.3.0-0.3.6 are not, and from version 0.3.7 thread safety of the single threaded OpenBLAS requires making it with the option <code>USE_LOCKING=1</code>. The reference BLAS is thread safe, as are MKL and ATLAS. This routine repeatedly calls the BLAS from multi-threaded code and is sufficient to detect the problem in single threaded OpenBLAS 0.3.x.
</p>
<p>A multi-threaded BLAS is often no faster than a single-threaded BLAS, while judicious use of threading in the code calling the BLAS can still deliver a modest speed improvement. For this reason it is often better to use a single threaded BLAS and the <code>nthreads</code> options to <code><a href="#topic+bam">bam</a></code> or <code><a href="#topic+gam">gam</a></code>. For <code>bam(...,discrete=TRUE)</code> using several threads can be a substantial benefit, especially with the reference BLAS.
</p>
<p>The MKL BLAS is mutlithreaded by default. Under linux setting environment variable <code>MKL_NUM_THREADS=1</code> before starting R gives single threaded operation. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>

<hr>
<h2 id='bug.reports.mgcv'>Reporting mgcv bugs.</h2><span id='topic+bug.reports.mgcv'></span>

<h3>Description</h3>

<p><code>mgcv</code> works largely because many people have reported bugs over the years. If you find something that looks like a bug, please report it, so that the package can be improved. <code>mgcv</code> does not have a large development budget, so it is a big help if bug reports follow the following guidelines.
</p>
<p>The ideal report consists of an email to <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> with a subject line including <code>mgcv</code> somewhere, containing
</p>

<ol>
<li><p> The results of running <code><a href="utils.html#topic+sessionInfo">sessionInfo</a></code> in the R session where the problem occurs. This provides platform details, R and package version numbers, etc.
</p>
</li>
<li><p> A brief description of the problem.
</p>
</li>
<li><p> Short cut and paste-able code that produces the problem, including the code for loading/generating the data (using standard R functions like <code>load</code>, <code>read.table</code> etc).
</p>
</li>
<li><p> Any required data files. If you send real data it will only be used for the purposes of de-bugging.
</p>
</li></ol>

<p>Of course if you have dug deeper and have an idea of what is causing the problem, that is also helpful to know, as is any suggested code fix. (Don't send a fixed package .tar.gz file, however - I can't use this).   
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>

<hr>
<h2 id='choldrop'>Deletion and rank one Cholesky factor update</h2><span id='topic+choldrop'></span><span id='topic+cholup'></span>

<h3>Description</h3>

<p>Given a Cholesky factor, <code>R</code>, of a matrix, <code>A</code>, <code>choldrop</code> finds the Cholesky factor of <code>A[-k,-k]</code>,
where <code>k</code> is an integer. <code>cholup</code> finds the factor of <code class="reqn">A + uu^T</code> (update) or <code class="reqn">A - uu^T</code> (downdate).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>choldrop(R,k)
cholup(R,u,up)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="choldrop_+3A_r">R</code></td>
<td>
<p>Cholesky factor of a matrix, <code>A</code>.</p>
</td></tr>
<tr><td><code id="choldrop_+3A_k">k</code></td>
<td>
<p>row and column of <code>A</code> to drop.</p>
</td></tr>
<tr><td><code id="choldrop_+3A_u">u</code></td>
<td>
<p>vector defining rank one update.</p>
</td></tr>
<tr><td><code id="choldrop_+3A_up">up</code></td>
<td>
<p>if <code>TRUE</code> compute update, otherwise downdate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>First consider <code>choldrop</code>. If <code>R</code> is upper triangular then <code>t(R[,-k])%*%R[,-k] == A[-k,-k]</code>, but <code>R[,-k]</code> has elements on the first sub-diagonal, from its kth column onwards. To get from this to a triangular Cholesky factor of <code>A[-k,-k]</code> we can apply a sequence of Givens rotations from the left to eliminate the sub-diagonal elements. The routine does this. If <code>R</code> is a lower triangular factor then Givens rotations from the right are needed to remove the extra elements. If <code>n</code> is the dimension of <code>R</code> then the update has <code class="reqn">O(n^2)</code> computational cost.
</p>
<p><code>cholup</code> (which assumes <code>R</code> is upper triangular) updates based on the observation that <code class="reqn"> R^TR + uu^T = [u,R^T][u,R^T]^T =  [u,R^T]Q^TQ[u,R^T]^T</code>, and therefore we can construct <code class="reqn">Q</code> so that <code class="reqn">Q[u,R^T]^T=[0,R_1^T]^T</code>, where <code class="reqn">R_1</code> is the modified factor. <code class="reqn">Q</code> is constructed from a sequence of Givens rotations in order to zero the elements of <code class="reqn">u</code>. Downdating is similar except that hyperbolic rotations have to be used in place of Givens rotations &mdash; see Golub and van Loan (2013, section 6.5.4) for details. Downdating only works if  <code class="reqn">A - uu^T</code> is positive definite. Again the computational cost is <code class="reqn">O(n^2)</code>.
</p>
<p>Note that the updates are vector oriented, and are hence not susceptible to speed up by use of an optimized BLAS. The updates are set up to be relatively Cache friendly, in that in the upper triangular case successive Givens rotations are stored for sequential application column-wise, rather than being applied row-wise as soon as they are computed. Even so, the upper triangular update is slightly slower than the lower triangular update. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Golub GH and CF Van Loan (2013) Matrix Computations (4th edition) Johns Hopkins
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(mgcv)
  set.seed(0)
  n &lt;- 6
  A &lt;- crossprod(matrix(runif(n*n),n,n))
  R0 &lt;- chol(A)
  k &lt;- 3
  Rd &lt;- choldrop(R0,k)
  range(Rd-chol(A[-k,-k]))
  Rd;chol(A[-k,-k])
  
  ## same but using lower triangular factor A = LL'
  L &lt;- t(R0)
  Ld &lt;- choldrop(L,k)
  range(Ld-t(chol(A[-k,-k])))
  Ld;t(chol(A[-k,-k]))

  ## Rank one update example
  u &lt;- runif(n)
  R &lt;- cholup(R0,u,TRUE)
  Ru &lt;- chol(A+u %*% t(u)) ## direct for comparison
  R;Ru
  range(R-Ru)

  ## Downdate - just going back from R to R0
  Rd &lt;-  cholup(R,u,FALSE)
  R0;Rd
  range(R0-Rd)
  
</code></pre>

<hr>
<h2 id='choose.k'>Basis dimension choice for smooths</h2><span id='topic+choose.k'></span>

<h3>Description</h3>

<p>Choosing the basis dimension, and checking the choice, when using
penalized regression smoothers.
</p>
<p>Penalized regression smoothers gain computational efficiency by virtue of
being defined using a basis of relatively modest size, <code>k</code>. When setting
up models in the <code>mgcv</code> package, using <code><a href="#topic+s">s</a></code> or <code><a href="#topic+te">te</a></code>
terms in a model formula, <code>k</code> must be chosen: the defaults are essentially arbitrary. 
</p>
<p>In practice <code>k-1</code> (or <code>k</code>) sets the upper limit on the degrees of freedom
associated with an <code><a href="#topic+s">s</a></code> smooth (1 degree of freedom is usually lost to the identifiability
constraint on the smooth). For <code><a href="#topic+te">te</a></code> smooths the upper limit of the
degrees of freedom is given by the product of the <code>k</code> values provided for
each marginal smooth less one, for the constraint. However the actual
effective degrees of freedom are controlled by the degree of penalization
selected during fitting, by GCV, AIC, REML or whatever is specified. The exception
to this is if a smooth is specified using the <code>fx=TRUE</code> option, in which
case it is unpenalized.  
</p>
<p>So, exact choice of <code>k</code> is not generally critical: it should be chosen to
be large enough that you are reasonably sure of having enough degrees of
freedom to represent the underlying &lsquo;truth&rsquo; reasonably well, but small enough
to maintain reasonable computational efficiency. Clearly &lsquo;large&rsquo; and &lsquo;small&rsquo;
are dependent on the particular problem being addressed. 
</p>
<p>As with all model assumptions, it is useful to be able to check the choice of
<code>k</code> informally. If the effective degrees of freedom for a model term are
estimated to be much less than <code>k-1</code> then this is unlikely to be very
worthwhile, but as the EDF approach <code>k-1</code>, checking can be important. A
useful general purpose approach goes as follows: (i) fit your model and
extract the deviance residuals; (ii) for each smooth term in your model, fit
an equivalent, single, smooth to the residuals, using a substantially
increased <code>k</code> to see if there is pattern in the residuals that could
potentially be explained by increasing <code>k</code>. Examples are provided below.
</p>
<p>The obvious, but more costly, alternative is simply to increase the suspect <code>k</code> 
and refit the original model. If there are no statistically important changes as a result of 
doing this, then <code>k</code> was large enough. (Change in the smoothness selection criterion, 
and/or the effective degrees of freedom, when <code>k</code> is increased, provide the obvious 
numerical measures for whether the fit has changed substantially.)
</p>
<p><code><a href="#topic+gam.check">gam.check</a></code> runs a simple simulation based check on the basis dimensions, which can 
help to flag up terms for which <code>k</code> is too low. Grossly too small <code>k</code>
will also be visible from partial residuals available with <code><a href="#topic+plot.gam">plot.gam</a></code>.
</p>
<p>One scenario that can cause confusion is this: a model is fitted with
<code>k=10</code> for a smooth term, and the EDF for the term is estimated as 7.6,
some way below the maximum of 9. The model is then refitted with <code>k=20</code>
and the EDF increases to 8.7 - what is happening - how come the EDF was not
8.7 the first time around? The explanation is that the function space with
<code>k=20</code> contains a larger subspace of functions with EDF 8.7 than did the
function space with <code>k=10</code>: one of the functions in this larger subspace
fits the data a little better than did any function in the smaller
subspace. These subtleties seldom have much impact on the statistical
conclusions to be drawn from a model fit, however. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). CRC/Taylor &amp; Francis.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Simulate some data ....
library(mgcv)
set.seed(1) 
dat &lt;- gamSim(1,n=400,scale=2)

## fit a GAM with quite low `k'
b&lt;-gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=6)+s(x3,k=6),data=dat)
plot(b,pages=1,residuals=TRUE) ## hint of a problem in s(x2)

## the following suggests a problem with s(x2)
gam.check(b)

## Another approach (see below for more obvious method)....
## check for residual pattern, removeable by increasing `k'
## typically `k', below, chould be substantially larger than 
## the original, `k' but certainly less than n/2.
## Note use of cheap "cs" shrinkage smoothers, and gamma=1.4
## to reduce chance of overfitting...
rsd &lt;- residuals(b)
gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine
gam(rsd~s(x1,k=40,bs="cs"),gamma=1.4,data=dat) ## fine
gam(rsd~s(x2,k=40,bs="cs"),gamma=1.4,data=dat) ## `k' too low
gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

## refit...
b &lt;- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=20)+s(x3,k=6),data=dat)
gam.check(b) ## better

## similar example with multi-dimensional smooth
b1 &lt;- gam(y~s(x0)+s(x1,x2,k=15)+s(x3),data=dat)
rsd &lt;- residuals(b1)
gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine
gam(rsd~s(x1,x2,k=100,bs="ts"),gamma=1.4,data=dat) ## `k' too low
gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

gam.check(b1) ## shows same problem

## and a `te' example
b2 &lt;- gam(y~s(x0)+te(x1,x2,k=4)+s(x3),data=dat)
rsd &lt;- residuals(b2)
gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine
gam(rsd~te(x1,x2,k=10,bs="cs"),gamma=1.4,data=dat) ## `k' too low
gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

gam.check(b2) ## shows same problem

## same approach works with other families in the original model
dat &lt;- gamSim(1,n=400,scale=.25,dist="poisson")
bp&lt;-gam(y~s(x0,k=5)+s(x1,k=5)+s(x2,k=5)+s(x3,k=5),
        family=poisson,data=dat,method="ML")

gam.check(bp)

rsd &lt;- residuals(bp)
gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine
gam(rsd~s(x1,k=40,bs="cs"),gamma=1.4,data=dat) ## fine
gam(rsd~s(x2,k=40,bs="cs"),gamma=1.4,data=dat) ## `k' too low
gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

rm(dat) 

## More obvious, but more expensive tactic... Just increase 
## suspicious k until fit is stable.

set.seed(0) 
dat &lt;- gamSim(1,n=400,scale=2)
## fit a GAM with quite low `k'
b &lt;- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=6)+s(x3,k=6),
         data=dat,method="REML")
b 
## edf for 3rd smooth is highest as proportion of k -- increase k
b &lt;- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=12)+s(x3,k=6),
         data=dat,method="REML")
b 
## edf substantially up, -ve REML substantially down
b &lt;- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=24)+s(x3,k=6),
         data=dat,method="REML")
b 
## slight edf increase and -ve REML change
b &lt;- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=40)+s(x3,k=6),
         data=dat,method="REML")
b 
## defintely stabilized (but really k around 20 would have been fine)

</code></pre>

<hr>
<h2 id='cnorm'>GAM censored normal family for log-normal AFT and Tobit models</h2><span id='topic+cnorm'></span><span id='topic+Tobit'></span>

<h3>Description</h3>

<p>Family for use with <code><a href="#topic+gam">gam</a></code> or <code><a href="#topic+bam">bam</a></code>, implementing regression for censored
normal data. If <code class="reqn">y</code> is the response with mean <code class="reqn">\mu</code> and standard deviation <code class="reqn">w^{-1/2}\exp(\theta)</code>,
then <code class="reqn">w^{1/2}(y-\mu)\exp(-\theta)</code> follows an <code class="reqn">N(0,1)</code> distribution. That is
</p>
<p style="text-align: center;"><code class="reqn">y \sim N(\mu,e^{2\theta}w^{-1}).</code>
</p>
 <p><code class="reqn">\theta</code> is a single scalar for all observations. Observations may be left, interval or right censored or uncensored.
</p>
<p>Useful for log-normal accelerated failure time (AFT) models, Tobit regression, and crudely rounded data, for example. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnorm(theta=NULL,link="identity")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnorm_+3A_theta">theta</code></td>
<td>
<p> log standard deviation parameter. If supplied and positive then taken as a fixed value of standard deviation (not its log). If supplied and negative taken as negative of initial value for standard deviation (not its log).</p>
</td></tr>
<tr><td><code id="cnorm_+3A_link">link</code></td>
<td>
<p>The link function: <code>"identity"</code>, <code>"log"</code> or <code>"sqrt"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the family is used with a vector response, then it is assumed that there is no censoring, and a regular Gaussian regression results. If there is censoring then the response should be supplied as a two column matrix. The first column is always numeric. Entries in the second column are as follows.
</p>

<ul>
<li><p> If an entry is identical to the corresponding first column entry, then it is an uncensored observation.
</p>
</li>
<li><p> If an entry is numeric and different to the first column entry then there is interval censoring. The first column entry is the lower interval limit and the second column entry is the upper interval limit. <code class="reqn">y</code> is only known to be between these limits.
</p>
</li>
<li><p> If the second column entry is <code>-Inf</code> then the observation is left censored at the value of the entry in the first column. It is only known that <code class="reqn">y</code> is less than or equal to the first column value.
</p>
</li>
<li><p> If the second column entry is <code>Inf</code> then the observation is right censored at the value of the entry in the first column. It is only known that <code class="reqn">y</code> is greater than or equal to the first column value.
</p>
</li></ul>

<p>Any mixture of censored and uncensored data is allowed, but be aware that data consisting only of right and/or left censored data contain very little information.
</p>


<h3>Value</h3>

<p>An object of class <code>extended.family</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)

#######################################################
## AFT model example for colon cancer survivial data...
#######################################################

library(survival) ## for data
col1 &lt;- colon[colon$etype==1,] ## concentrate on single event
col1$differ &lt;- as.factor(col1$differ)
col1$sex &lt;- as.factor(col1$sex)

## set up the AFT response... 
logt &lt;- cbind(log(col1$time),log(col1$time))
logt[col1$status==0,2] &lt;- Inf ## right censoring
col1$logt &lt;- -logt ## -ve conventional for AFT versus Cox PH comparison

## fit the model...
b &lt;- gam(logt~s(age,by=sex)+sex+s(nodes)+perfor+rx+obstruct+adhere,
         family=cnorm(),data=col1)
plot(b,pages=1)	 
## ... compare this to ?cox.ph

################################
## A Tobit regression example...
################################

set.seed(3);n&lt;-400
dat &lt;- gamSim(1,n=n)
ys &lt;- dat$y - 5 ## shift data down

## truncate at zero, and set up response indicating this has happened...
y &lt;- cbind(ys,ys)
y[ys&lt;0,2] &lt;- -Inf
y[ys&lt;0,1] &lt;- 0
dat$yt &lt;- y
b &lt;- gam(yt~s(x0)+s(x1)+s(x2)+s(x3),family=cnorm,data=dat)
plot(b,pages=1)

##############################
## A model for rounded data...
##############################

dat &lt;- gamSim(1,n=n)
y &lt;- round(dat$y)
y &lt;- cbind(y-.5,y+.5) ## set up to indicate interval censoring
dat$yi &lt;- y
b &lt;- gam(yi~s(x0)+s(x1)+s(x2)+s(x3),family=cnorm,data=dat)
plot(b,pages=1)

</code></pre>

<hr>
<h2 id='columb'>Reduced version of Columbus OH crime data</h2><span id='topic+columb'></span><span id='topic+columb.polys'></span>

<h3>Description</h3>

<p>By district crime data from Columbus OH, together with polygons describing district shape. Useful for 
illustrating use of simple Markov Random Field smoothers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(columb)
data(columb.polys)
</code></pre>


<h3>Format</h3>

 <p><code>columb</code> is a 49 row data frame with the following columns 
</p>

<dl>
<dt>area</dt><dd><p>land area of district</p>
</dd>
<dt>home.value</dt><dd><p>housing value in 1000USD.</p>
</dd>
<dt>income</dt><dd><p>household income in 1000USD.</p>
</dd>
<dt>crime</dt><dd><p>residential burglaries and auto thefts per 1000 households.</p>
</dd>
<dt>open.space</dt><dd><p>measure of open space in district.</p>
</dd>
<dt>district</dt><dd><p>code identifying district, and matching <code>names(columb.polys)</code>. </p>
</dd>
</dl>

<p><code>columb.polys</code> contains the polygons defining the areas in the format described below.
</p>


<h3>Details</h3>

<p>The data frame <code>columb</code> relates to the districts whose boundaries are coded in <code>columb.polys</code>.
<code>columb.polys[[i]]</code> is a 2 column matrix, containing the vertices of the polygons defining the boundary of the ith 
district. <code>columb.polys[[2]]</code> has an artificial hole inserted to illustrate how holes in districts can be spefified. Different polygons defining the boundary of a district are separated by NA rows in <code>columb.polys[[1]]</code>, 
and a polygon enclosed within another is treated as a hole in that region (a hole should never come first). 
<code>names(columb.polys)</code> matches <code>columb$district</code> (order unimportant).
</p>


<h3>Source</h3>

<p>The data are adapted from the <code>columbus</code> example in the <code>spdep</code> package, where the original source is given as: 
</p>
<p>Anselin, Luc. 1988. Spatial econometrics: methods and models. Dordrecht: Kluwer Academic, Table 12.1 p. 189.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see ?mrf help files
</code></pre>

<hr>
<h2 id='concurvity'>GAM concurvity measures</h2><span id='topic+concurvity'></span>

<h3>Description</h3>

<p>Produces summary measures of concurvity between <code><a href="#topic+gam">gam</a></code> components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concurvity(b,full=TRUE)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="concurvity_+3A_b">b</code></td>
<td>
<p>An object inheriting from class <code>"gam"</code>.</p>
</td></tr>
<tr><td><code id="concurvity_+3A_full">full</code></td>
<td>
<p>If <code>TRUE</code> then concurvity of each term with the whole of the rest of the model is considered. 
If <code>FALSE</code> then pairwise concurvity measures between each smooth term (as well as the parametric component) 
are considered.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Concurvity occurs when some smooth term in a model could be approximated by one or more of the other smooth terms 
in the model. This is often the case when a smooth of space is included in a model, along with smooths of other covariates 
that also vary more or less smoothly in space. Similarly it tends to be an issue in models including a smooth of time, 
along with smooths of other time varying covariates.
</p>
<p>Concurvity can be viewed as a generalization of co-linearity, and causes similar problems of interpretation. It can also make estimates somewhat unstable (so that they become sensitive to apparently innocuous modelling details, for example). 
</p>
<p>This routine computes three related indices of concurvity, all bounded between 0 and 1, with 0 indicating no problem, 
and 1 indicating total lack of identifiability. The three indices are all based on the idea that a smooth term, f,  
in the model can be decomposed into a part, g, that lies entirely in the space of one or more other terms 
in the model, and a remainder part that is completely within the term's own space. If g makes up a large part of f then there is a concurvity problem. The indices used are all based on the square of ||g||/||f||, that is the ratio of the squared 
Euclidean norms of the vectors of f and g evaluated at the observed covariate values. 
</p>
<p>The three measures are as follows 
</p>

<dl>
<dt>worst</dt><dd><p>This is the largest value that the square of ||g||/||f|| could take for any coefficient vector. This is a fairly pessimistic measure, as it looks at the worst case irrespective of data. This is the only measure that is symmetric.</p>
</dd>
<dt>observed</dt><dd><p>This just returns the value of the square of ||g||/||f|| according to the estimated coefficients. 
This could be a bit over-optimistic about the potential for a problem in some cases. </p>
</dd>
<dt>estimate</dt><dd><p>This is the squared F-norm of the basis for g divided by the F-norm of the basis for f. 
It is a measure of the 
extent to which the f basis can be explained by the g basis. It does not suffer from the pessimism or potential for 
over-optimism of the previous two measures, but is less easy to understand.</p>
</dd>
</dl>



<h3>Value</h3>

<p> If <code>full=TRUE</code> a matrix with one column for each term and one row for each of the 3 concurvity measures detailed below. If <code>full=FALSE</code> a list of 3 matrices, one for each of the three concurvity measures detailed below. Each row of 
the matrix relates to how the model terms depend on the model term supplying that rows name.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## simulate data with concurvity...
set.seed(8);n&lt;- 200
f2 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 *
            (10 * x)^3 * (1 - x)^10
t &lt;- sort(runif(n)) ## first covariate
## make covariate x a smooth function of t + noise...
x &lt;- f2(t) + rnorm(n)*3
## simulate response dependent on t and x...
y &lt;- sin(4*pi*t) + exp(x/20) + rnorm(n)*.3

## fit model...
b &lt;- gam(y ~ s(t,k=15) + s(x,k=15),method="REML")

## assess concurvity between each term and `rest of model'...
concurvity(b)

## ... and now look at pairwise concurvity between terms...
concurvity(b,full=FALSE)

</code></pre>

<hr>
<h2 id='cox.ph'>Additive Cox Proportional Hazard Model</h2><span id='topic+cox.ph'></span>

<h3>Description</h3>

<p>The <code>cox.ph</code> family implements the Cox Proportional Hazards model with Peto's 
correction for ties, optional stratification, and estimation by penalized partial likelihood maximization, for use with 
<code><a href="#topic+gam">gam</a></code>. In the model formula, event time is the response. Under stratification the response has two columns: time
and a numeric index for stratum.  The <code>weights</code> vector provides 
the censoring information (0 for censoring, 1 for event). <code>cox.ph</code> deals with the case in which each subject has
one event/censoring time and one row of covariate values. When each subject has several time dependent
covariates see <code><a href="#topic+cox.pht">cox.pht</a></code>. 
</p>
<p>See example below for conditional logistic regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cox.ph(link="identity")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cox.ph_+3A_link">link</code></td>
<td>
<p>currently (and possibly for ever) only <code>"identity"</code> supported.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Used with <code><a href="#topic+gam">gam</a></code> to fit Cox Proportional Hazards models to survival data. The model formula will 
have event/censoring times on the left hand side and the linear predictor specification on the right hand side. Censoring
information is provided by the <code>weights</code> argument to <code>gam</code>, with 1 indicating an event and 0 indicating 
censoring.
</p>
<p>Stratification is possible, allowing for different baseline hazards in different strata. In that case the response has two columns: the first is event/censoring time and the second is a numeric stratum index. See below for an example.  
</p>
<p>Prediction from the fitted model object (using the <code>predict</code> method) with <code>type="response"</code> will predict on the 
survivor function scale. This requires evaluation times to be provided as well as covariates (see example). Also see example code
below for extracting the cumulative baseline hazard/survival directly. The <code>fitted.values</code> stored in the model object are
survival function estimates for each subject at their event/censoring time.
</p>
<p><code>deviance</code>,<code>martingale</code>,<code>score</code>, or <code>schoenfeld</code> residuals can be extracted. See Klein amd Moeschberger (2003) for descriptions. The score residuals are returned as a matrix of the same dimension as the model matrix, with a <code>"terms"</code> attribute, which is a list indicating which model matrix columns belong to which model terms. The score residuals are scaled. For parameteric terms this is by the standard deviation of associated model coefficient. For smooth terms the sub matrix of score residuals for the term is postmultiplied by the transposed Cholesky factor of the covariance matrix for the term's coefficients. This is a transformation that makes the coefficients approximately independent, as required to make plots of the score residuals against event time interpretable for checking the proportional hazards assumption (see  Klein amd Moeschberger, 2003, p376). Penalization causes drift in the score residuals, which is also removed, to allow the residuals to be approximately interpreted as unpenalized score residuals. Schoenfeld and score residuals are computed by strata. See the examples for simple PH assuption checks by plotting score residuals, and Klein amd Moeschberger (2003, section 11.4) for details. Note that high correlation between terms can undermine these checks.  
</p>
<p>Estimation of model coefficients is by maximising the log-partial likelihood penalized by the smoothing penalties. See e.g. 
Hastie and Tibshirani, 1990, section 8.3. for the partial likelihood used (with Peto's approximation for ties), but note that 
optimization of the partial likelihood does not follow Hastie and Tibshirani. See Klein amd Moeschberger (2003) for 
estimation of residuals, the cumulative baseline hazard, survival function and associated standard errors (the survival standard error expression has a typo).  
</p>
<p>The percentage deviance explained reported for Cox PH models is based on the sum of squares of the deviance residuals, as the model deviance, and the sum of squares of the deviance residuals when the covariate effects are set to zero, as the null deviance. The same baseline hazard estimate is used for both.
</p>
<p>This family deals efficiently with the case in which each subject has one event/censoring time and one row of covariate values. For studies in which there are multiple time varying covariate measures for each subject then the equivalent Poisson model should be fitted to suitable pseudodata using <code>bam(...,discrete=TRUE)</code>. See <code><a href="#topic+cox.pht">cox.pht</a></code>.
</p>


<h3>Value</h3>

<p>An object inheriting from class <code>general.family</code>.
</p>


<h3>References</h3>

<p>Hastie and Tibshirani (1990) Generalized Additive Models, Chapman and Hall.
</p>
<p>Klein, J.P and Moeschberger, M.L. (2003) Survival Analysis: Techniques for
Censored and Truncated Data (2nd ed.) Springer.
</p>
<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cox.pht">cox.pht</a></code>, <code><a href="#topic+cnorm">cnorm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
library(survival) ## for data
col1 &lt;- colon[colon$etype==1,] ## concentrate on single event
col1$differ &lt;- as.factor(col1$differ)
col1$sex &lt;- as.factor(col1$sex)

b &lt;- gam(time~s(age,by=sex)+sex+s(nodes)+perfor+rx+obstruct+adhere,
         family=cox.ph(),data=col1,weights=status)

summary(b) 

plot(b,pages=1,all.terms=TRUE) ## plot effects

plot(b$linear.predictors,residuals(b))

## plot survival function for patient j...

np &lt;- 300;j &lt;- 6
newd &lt;- data.frame(time=seq(0,3000,length=np))
dname &lt;- names(col1)
for (n in dname) newd[[n]] &lt;- rep(col1[[n]][j],np)
newd$time &lt;- seq(0,3000,length=np)
fv &lt;- predict(b,newdata=newd,type="response",se=TRUE)
plot(newd$time,fv$fit,type="l",ylim=c(0,1),xlab="time",ylab="survival")
lines(newd$time,fv$fit+2*fv$se.fit,col=2)
lines(newd$time,fv$fit-2*fv$se.fit,col=2)

## crude plot of baseline survival...

plot(b$family$data$tr,exp(-b$family$data$h),type="l",ylim=c(0,1),
     xlab="time",ylab="survival")
lines(b$family$data$tr,exp(-b$family$data$h + 2*b$family$data$q^.5),col=2)
lines(b$family$data$tr,exp(-b$family$data$h - 2*b$family$data$q^.5),col=2)
lines(b$family$data$tr,exp(-b$family$data$km),lty=2) ## Kaplan Meier

## Checking the proportional hazards assumption via scaled score plots as
## in Klein and Moeschberger Section 11.4 p374-376... 

ph.resid &lt;- function(b,stratum=1) {
## convenience function to plot scaled score residuals against time,
## by term. Reference lines at 5% exceedance prob for Brownian bridge
## (see KS test statistic distribution).
  rs &lt;- residuals(b,"score");term &lt;- attr(rs,"term")
  if (is.matrix(b$y)) {
    ii &lt;- b$y[,2] == stratum;b$y &lt;- b$y[ii,1];rs &lt;- rs[ii,]
  }
  oy &lt;- order(b$y)
  for (i in 1:length(term)) {
    ii &lt;- term[[i]]; m &lt;- length(ii)
    plot(b$y[oy],rs[oy,ii[1]],ylim=c(-3,3),type="l",ylab="score residuals",
         xlab="time",main=names(term)[i])
    if (m&gt;1) for (k in 2:m) lines(b$y[oy],rs[oy,ii[k]],col=k);
    abline(-1.3581,0,lty=2);abline(1.3581,0,lty=2) 
  }  
}
par(mfrow=c(2,2))
ph.resid(b)

## stratification example, with 2 randomly allocated strata
## so that results should be similar to previous....
col1$strata &lt;- sample(1:2,nrow(col1),replace=TRUE) 
bs &lt;- gam(cbind(time,strata)~s(age,by=sex)+sex+s(nodes)+perfor+rx+obstruct
          +adhere,family=cox.ph(),data=col1,weights=status)
plot(bs,pages=1,all.terms=TRUE) ## plot effects

## baseline survival plots by strata...

for (i in 1:2) { ## loop over strata
## create index selecting elements of stored hazard info for stratum i...
ind &lt;- which(bs$family$data$tr.strat == i)
if (i==1) plot(bs$family$data$tr[ind],exp(-bs$family$data$h[ind]),type="l",
      ylim=c(0,1),xlab="time",ylab="survival",lwd=2,col=i) else
      lines(bs$family$data$tr[ind],exp(-bs$family$data$h[ind]),lwd=2,col=i)
lines(bs$family$data$tr[ind],exp(-bs$family$data$h[ind] +
      2*bs$family$data$q[ind]^.5),lty=2,col=i) ## upper ci
lines(bs$family$data$tr[ind],exp(-bs$family$data$h[ind] -
      2*bs$family$data$q[ind]^.5),lty=2,col=i) ## lower ci
lines(bs$family$data$tr[ind],exp(-bs$family$data$km[ind]),col=i) ## KM
}


## Simple simulated known truth example...
ph.weibull.sim &lt;- function(eta,gamma=1,h0=.01,t1=100) { 
  lambda &lt;- h0*exp(eta) 
  n &lt;- length(eta)
  U &lt;- runif(n)
  t &lt;- (-log(U)/lambda)^(1/gamma)
  d &lt;- as.numeric(t &lt;= t1)
  t[!d] &lt;- t1
  list(t=t,d=d)
}
n &lt;- 500;set.seed(2)
x0 &lt;- runif(n, 0, 1);x1 &lt;- runif(n, 0, 1)
x2 &lt;- runif(n, 0, 1);x3 &lt;- runif(n, 0, 1)
f0 &lt;- function(x) 2 * sin(pi * x)
f1 &lt;- function(x) exp(2 * x)
f2 &lt;- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
f3 &lt;- function(x) 0*x
f &lt;- f0(x0) + f1(x1)  + f2(x2)
g &lt;- (f-mean(f))/5
surv &lt;- ph.weibull.sim(g)
surv$x0 &lt;- x0;surv$x1 &lt;- x1;surv$x2 &lt;- x2;surv$x3 &lt;- x3

b &lt;- gam(t~s(x0)+s(x1)+s(x2,k=15)+s(x3),family=cox.ph,weights=d,data=surv)

plot(b,pages=1)

## Another one, including a violation of proportional hazards for
## effect of x2...

set.seed(2)
h &lt;- exp((f0(x0)+f1(x1)+f2(x2)-10)/5)
t &lt;- rexp(n,h);d &lt;- as.numeric(t&lt;20)

## first with no violation of PH in the simulation...
b &lt;- gam(t~s(x0)+s(x1)+s(x2)+s(x3),family=cox.ph,weights=d)
plot(b,pages=1)
ph.resid(b) ## fine

## Now violate PH for x2 in the simulation...
ii &lt;- t&gt;1.5
h1 &lt;- exp((f0(x0)+f1(x1)+3*f2(x2)-10)/5)
t[ii] &lt;- 1.5 + rexp(sum(ii),h1[ii]);d &lt;- as.numeric(t&lt;20)

b &lt;- gam(t~s(x0)+s(x1)+s(x2)+s(x3),family=cox.ph,weights=d)
plot(b,pages=1)
ph.resid(b) ## The checking plot picks up the problem in s(x2) 


## conditional logistic regression models are often estimated using the 
## cox proportional hazards partial likelihood with a strata for each
## case-control group. A dummy vector of times is created (all equal). 
## The following compares to 'clogit' for a simple case. Note that
## the gam log likelihood is not exact if there is more than one case
## per stratum, corresponding to clogit's approximate method.
library(survival);library(mgcv)
infert$dumt &lt;- rep(1,nrow(infert))
mg &lt;- gam(cbind(dumt,stratum) ~ spontaneous + induced, data=infert,
          family=cox.ph,weights=case)
ms &lt;- clogit(case ~ spontaneous + induced + strata(stratum), data=infert,
             method="approximate")
summary(mg)$p.table[1:2,]; ms
</code></pre>

<hr>
<h2 id='cox.pht'>Additive Cox proportional hazard models with time varying covariates</h2><span id='topic+cox.pht'></span>

<h3>Description</h3>

<p>The <code>cox.ph</code> family only allows one set of covariate values per subject. If each subject has several time varying covariate measurements then it is still possible to fit a proportional hazards regression model, via an equivalent Poisson model. The recipe is provided by Whitehead (1980) and is equally valid in the smooth additive case. Its drawback is that the equivalent Poisson dataset can be quite large.
</p>
<p>The trick is to generate an artificial Poisson observation for each subject in the risk set at each non-censored event time. The corresponding covariate values for each subject are whatever they are at the event time, while the Poisson response is zero for all subjects except those experiencing the event at that time (this corresponds to Peto's correction for ties). The linear predictor for the model must include an intercept for each event time (the cumulative sum of the exponential of these is the Breslow estimate of the baseline hazard). 
</p>
<p>Below is some example code employing this trick for the <code><a href="survival.html#topic+pbcseq">pbcseq</a></code> data from the <code>survival</code> package. It uses <code><a href="#topic+bam">bam</a></code> for fitting with the <code>discrete=TRUE</code> option for efficiency: there is some approximation involved in doing this, and the exact equivalent to what is done in <code><a href="#topic+cox.ph">cox.ph</a></code> is rather obtained by using <code><a href="#topic+gam">gam</a></code> with <code>method="REML"</code> (taking many times the computational time for the example below). An alternative fits the model as a conditional logistic model using stratified Cox PH with event times as strata (see example). This would be identical in the unpenalized case, but smoothing parameter estimates can differ.
</p>
<p>The function <code>tdpois</code> in the example code uses crude piecewise constant interpolation for the covariates, in which the covariate value at an event time is taken to be whatever it was the previous time that it was measured. Obviously more sophisticated interpolation schemes might be preferable. 
</p>


<h3>References</h3>

<p>Whitehead (1980) Fitting Cox's regression model to survival data using GLIM. Applied Statistics 29(3):268-275
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv);require(survival)

## First define functions for producing Poisson model data frame

app &lt;- function(x,t,to) {
## wrapper to approx for calling from apply...
  y &lt;- if (sum(!is.na(x))&lt;1) rep(NA,length(to)) else
       approx(t,x,to,method="constant",rule=2)$y
  if (is.factor(x)) factor(levels(x)[y],levels=levels(x)) else y
} ## app

tdpois &lt;- function(dat,event="z",et="futime",t="day",status="status1",
                                                             id="id") {
## dat is data frame. id is patient id; et is event time; t is
## observation time; status is 1 for death 0 otherwise;
## event is name for Poisson response.
  if (event %in% names(dat)) warning("event name in use")
  require(utils) ## for progress bar
  te &lt;- sort(unique(dat[[et]][dat[[status]]==1])) ## event times
  sid &lt;- unique(dat[[id]])
  inter &lt;- interactive()
  if (inter) prg &lt;- txtProgressBar(min = 0, max = length(sid), initial = 0,
         char = "=",width = NA, title="Progress", style = 3)
  ## create dataframe for poisson model data
  dat[[event]] &lt;- 0; start &lt;- 1
  dap &lt;- dat[rep(1:length(sid),length(te)),]
  for (i in 1:length(sid)) { ## work through patients
    di &lt;- dat[dat[[id]]==sid[i],] ## ith patient's data
    tr &lt;- te[te &lt;= di[[et]][1]] ## times required for this patient
    ## Now do the interpolation of covariates to event times...
    um &lt;- data.frame(lapply(X=di,FUN=app,t=di[[t]],to=tr))
    ## Mark the actual event...
    if (um[[et]][1]==max(tr)&amp;&amp;um[[status]][1]==1) um[[event]][nrow(um)] &lt;- 1 
    um[[et]] &lt;- tr ## reset time to relevant event times
    dap[start:(start-1+nrow(um)),] &lt;- um ## copy to dap
    start &lt;- start + nrow(um)
    if (inter) setTxtProgressBar(prg, i)
  }
  if (inter) close(prg)
  dap[1:(start-1),]
} ## tdpois

## The following typically takes a minute or less...


## Convert pbcseq to equivalent Poisson form...
pbcseq$status1 &lt;- as.numeric(pbcseq$status==2) ## death indicator
pb &lt;- tdpois(pbcseq) ## conversion
pb$tf &lt;- factor(pb$futime) ## add factor for event time

## Fit Poisson model...
b &lt;- bam(z ~ tf - 1 + sex + trt + s(sqrt(protime)) + s(platelet)+ s(age)+
s(bili)+s(albumin), family=poisson,data=pb,discrete=TRUE,nthreads=2)

pb$dumt &lt;- rep(1,nrow(pb)) ## dummy time
## Fit as conditional logistic... 
b1 &lt;- gam(cbind(dumt,tf) ~ sex + trt + s(sqrt(protime)) + s(platelet)
+ s(age) + s(bili) + s(albumin),family=cox.ph,data=pb,weights=z)

par(mfrow=c(2,3))
plot(b,scale=0)

## compute residuals...
chaz &lt;- tapply(fitted(b),pb$id,sum) ## cum haz by subject
d &lt;- tapply(pb$z,pb$id,sum) ## censoring indicator
mrsd &lt;- d - chaz ## Martingale
drsd &lt;- sign(mrsd)*sqrt(-2*(mrsd + d*log(chaz))) ## deviance

## plot survivor function and s.e. band for subject 25
te &lt;- sort(unique(pb$futime)) ## event times
di &lt;- pbcseq[pbcseq$id==25,] ## data for subject 25
pd &lt;- data.frame(lapply(X=di,FUN=app,t=di$day,to=te)) ## interpolate to te
pd$tf &lt;- factor(te)
X &lt;- predict(b,newdata=pd,type="lpmatrix")
eta &lt;- drop(X%*%coef(b)); H &lt;- cumsum(exp(eta))
J &lt;- apply(exp(eta)*X,2,cumsum)
se &lt;- diag(J%*%vcov(b)%*%t(J))^.5
plot(stepfun(te,c(1,exp(-H))),do.points=FALSE,ylim=c(0.7,1),
     ylab="S(t)",xlab="t (days)",main="",lwd=2)
lines(stepfun(te,c(1,exp(-H+se))),do.points=FALSE)
lines(stepfun(te,c(1,exp(-H-se))),do.points=FALSE)
rug(pbcseq$day[pbcseq$id==25]) ## measurement times

</code></pre>

<hr>
<h2 id='cSplineDes'>Evaluate cyclic B spline basis</h2><span id='topic+cSplineDes'></span>

<h3>Description</h3>

<p> Uses <code>splineDesign</code> to set up the model matrix for a cyclic B-spline basis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cSplineDes(x, knots, ord = 4, derivs=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cSplineDes_+3A_x">x</code></td>
<td>
<p> covariate values for smooth.</p>
</td></tr>
<tr><td><code id="cSplineDes_+3A_knots">knots</code></td>
<td>
<p>The knot locations: the range of these must include all the data.</p>
</td></tr>
<tr><td><code id="cSplineDes_+3A_ord">ord</code></td>
<td>
<p> order of the basis. 4 is a cubic spline basis. Must be &gt;1.</p>
</td></tr>
<tr><td><code id="cSplineDes_+3A_derivs">derivs</code></td>
<td>
<p> order of derivative of the spline to evaluate, between 0 and <code>ord</code>-1. Recycled to length of <code>x</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p> The routine is a wrapper that sets up a B-spline basis, where the basis functions wrap at the first and 
last knot locations.</p>


<h3>Value</h3>

<p> A matrix with <code>length(x)</code> rows and <code>length(knots)-1</code> columns.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+cyclic.p.spline">cyclic.p.spline</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> require(mgcv)
 ## create some x's and knots...
 n &lt;- 200
 x &lt;- 0:(n-1)/(n-1);k&lt;- 0:5/5
 X &lt;- cSplineDes(x,k) ## cyclic spline design matrix
 ## plot evaluated basis functions...
 plot(x,X[,1],type="l"); for (i in 2:5) lines(x,X[,i],col=i)
 ## check that the ends match up....
 ee &lt;- X[1,]-X[n,];ee 
 tol &lt;- .Machine$double.eps^.75
 if (all.equal(ee,ee*0,tolerance=tol)!=TRUE) 
   stop("cyclic spline ends don't match!")
 
 ## similar with uneven data spacing...
 x &lt;- sort(runif(n)) + 1 ## sorting just makes end checking easy
 k &lt;- seq(min(x),max(x),length=8) ## create knots
 X &lt;- cSplineDes(x,k) ## get cyclic spline model matrix  
 plot(x,X[,1],type="l"); for (i in 2:ncol(X)) lines(x,X[,i],col=i)
 ee &lt;- X[1,]-X[n,];ee ## do ends match??
 tol &lt;- .Machine$double.eps^.75
 if (all.equal(ee,ee*0,tolerance=tol)!=TRUE) 
   stop("cyclic spline ends don't match!")
</code></pre>

<hr>
<h2 id='dDeta'>Obtaining derivative w.r.t. linear predictor</h2><span id='topic+dDeta'></span>

<h3>Description</h3>

<p>INTERNAL function. Distribution families provide derivatives of the deviance and link w.r.t. <code>mu = inv_link(eta)</code>.
This routine converts these to the required derivatives of the deviance w.r.t. eta, the linear predictor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dDeta(y, mu, wt, theta, fam, deriv = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dDeta_+3A_y">y</code></td>
<td>
<p>vector of observations.</p>
</td></tr>
<tr><td><code id="dDeta_+3A_mu">mu</code></td>
<td>
<p>if <code>eta</code> is the linear predictor, <code>mu = inv_link(eta)</code>. In a traditional GAM <code>mu=E(y)</code>.</p>
</td></tr>
<tr><td><code id="dDeta_+3A_wt">wt</code></td>
<td>
<p>vector of weights.</p>
</td></tr>
<tr><td><code id="dDeta_+3A_theta">theta</code></td>
<td>
<p>vector of family parameters that are not regression coefficients (e.g. scale parameters).</p>
</td></tr>
<tr><td><code id="dDeta_+3A_fam">fam</code></td>
<td>
<p>the family object.</p>
</td></tr>
<tr><td><code id="dDeta_+3A_deriv">deriv</code></td>
<td>
<p>the order of derivative of the smoothing parameter score required.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of derivatives.
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>

<hr>
<h2 id='dpnorm'>Stable evaluation of difference between normal c.d.f.s</h2><span id='topic+dpnorm'></span>

<h3>Description</h3>

<p>Evaluates the difference between two <code class="reqn">N(0,1)</code> cumulative distribution functions avoiding cancellation error.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dpnorm(x0,x1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dpnorm_+3A_x0">x0</code></td>
<td>
<p>vector of lower values at which to evaluate standard normal distribution function.</p>
</td></tr>
<tr><td><code id="dpnorm_+3A_x1">x1</code></td>
<td>
<p>vector of upper values at which to evaluate standard normal distribution function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Equivalent to <code>pnorm(x1)-pnorm(x0)</code>, but stable when <code>x0</code> and <code>x1</code> values are very close, or in the upper tail of the standard normal.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
x &lt;- seq(-10,10,length=10000)
eps &lt;- 1e-10
y0 &lt;- pnorm(x+eps)-pnorm(x) ## cancellation prone
y1 &lt;- dpnorm(x,x+eps)       ## stable
## illustrate stable computation in black, and
## cancellation prone in red...
par(mfrow=c(1,2),mar=c(4,4,1,1))
plot(log(y1),log(y0),type="l")
lines(log(y1[x&gt;0]),log(y0[x&gt;0]),col=2)
plot(x,log(y1),type="l")
lines(x,log(y0),col=2)

</code></pre>

<hr>
<h2 id='exclude.too.far'>Exclude prediction grid points too far from data</h2><span id='topic+exclude.too.far'></span>

<h3>Description</h3>

<p> Takes two arrays defining the nodes of a grid over a 2D covariate space and two arrays 
defining the location of data in that space, and returns a logical vector with elements <code>TRUE</code> if 
the corresponding node is too far from data and <code>FALSE</code> otherwise. Basically a service routine for 
<code>vis.gam</code> and <code>plot.gam</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exclude.too.far(g1,g2,d1,d2,dist)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="exclude.too.far_+3A_g1">g1</code></td>
<td>
<p>co-ordinates of grid relative to first axis.</p>
</td></tr>
<tr><td><code id="exclude.too.far_+3A_g2">g2</code></td>
<td>
<p>co-ordinates of grid relative to second axis.</p>
</td></tr>
<tr><td><code id="exclude.too.far_+3A_d1">d1</code></td>
<td>
<p>co-ordinates of data relative to first axis.</p>
</td></tr>
<tr><td><code id="exclude.too.far_+3A_d2">d2</code></td>
<td>
<p>co-ordinates of data relative to second axis.</p>
</td></tr>
<tr><td><code id="exclude.too.far_+3A_dist">dist</code></td>
<td>
<p>how far away counts as too far. Grid and data are first scaled so that the grid lies exactly 
in the unit square, and <code>dist</code> is a distance within this unit square.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p> Linear scalings of the axes are first determined so that the grid defined by the nodes in 
<code>g1</code> and <code>g2</code> lies exactly in the unit square (i.e. on [0,1] by [0,1]). These scalings are 
applied to <code>g1</code>, <code>g2</code>, <code>d1</code> and <code>d2</code>. The minimum Euclidean 
distance from each node to a datum is then determined and if it is greater than <code>dist</code> the 
corresponding entry in the returned array is set to <code>TRUE</code> (otherwise to <code>FALSE</code>). The 
distance calculations are performed in compiled code for speed without storage overheads.
</p>


<h3>Value</h3>

<p>A logical array with <code>TRUE</code> indicating a node in the grid defined by <code>g1</code>, <code>g2</code> that 
is &lsquo;too far&rsquo; from any datum. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+vis.gam">vis.gam</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
x&lt;-rnorm(100);y&lt;-rnorm(100) # some "data"
n&lt;-40 # generate a grid....
mx&lt;-seq(min(x),max(x),length=n)
my&lt;-seq(min(y),max(y),length=n)
gx&lt;-rep(mx,n);gy&lt;-rep(my,rep(n,n))
tf&lt;-exclude.too.far(gx,gy,x,y,0.1)
plot(gx[!tf],gy[!tf],pch=".");points(x,y,col=2)
</code></pre>

<hr>
<h2 id='extract.lme.cov'> Extract the data covariance matrix from an lme object</h2><span id='topic+extract.lme.cov'></span><span id='topic+extract.lme.cov2'></span>

<h3>Description</h3>

<p> This is a service routine for <code><a href="#topic+gamm">gamm</a></code>. Extracts 
the estimated covariance matrix of the data from an <code>lme</code> object, allowing the 
user control about which levels of random effects to include in this 
calculation. <code>extract.lme.cov</code> forms the full matrix explicitly:
<code>extract.lme.cov2</code> tries to be more economical than this.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract.lme.cov(b,data=NULL,start.level=1)
extract.lme.cov2(b,data=NULL,start.level=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract.lme.cov_+3A_b">b</code></td>
<td>
<p> A fitted model object returned by a call to <code><a href="nlme.html#topic+lme">lme</a></code></p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="extract.lme.cov_+3A_data">data</code></td>
<td>
<p> The data frame/ model frame that was supplied to
<code><a href="nlme.html#topic+lme">lme</a></code>, but with any rows removed by the na action dropped. Uses
the data stored in the model object if not supplied.</p>
</td></tr>
<tr><td><code id="extract.lme.cov_+3A_start.level">start.level</code></td>
<td>
<p>The level of nesting at which to start including random 
effects in the calculation. This is used to allow smooth terms to be estimated
as random effects, but treated like fixed effects for variance calculations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> The random effects, correlation structure and variance structure used
for a linear mixed model combine to imply a covariance matrix for the 
response data being modelled. These routines extracts that covariance matrix.
The process is slightly complicated, because different components of the 
fitted model object are stored in different orders (see function code for 
details!).  
</p>
<p>The <code>extract.lme.cov</code> calculation is not optimally efficient, since it forms the full matrix,
which may in fact be sparse. <code>extract.lme.cov2</code> is more efficient. If the
covariance matrix is diagonal, then only the leading diagonal is returned; if
it can be written as a block diagonal matrix (under some permutation of the
original data) then a list of matrices defining the non-zero blocks is
returned along with an index indicating which row of the original data each
row/column of the block diagonal matrix relates to. The block sizes are defined by
the coarsest level of grouping in the random effect structure.
</p>
<p><code><a href="#topic+gamm">gamm</a></code> uses <code>extract.lme.cov2</code>.
</p>
<p><code>extract.lme.cov</code> does not currently deal with the situation in which the
grouping factors for a correlation structure are finer than those for the
random effects. <code>extract.lme.cov2</code> does deal with this situation.
</p>


<h3>Value</h3>

<p> For <code>extract.lme.cov</code> an estimated covariance matrix.
</p>
<p>For <code>extract.lme.cov2</code> a list containing the estimated covariance matrix
and an indexing array. The covariance matrix is stored as the elements on the
leading diagonal, a list of the matrices defining a block diagonal matrix, or
a full matrix if the previous two options are not possible.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>For <code>lme</code> see:
</p>
<p>Pinheiro J.C. and Bates, D.M. (2000) Mixed effects Models in S and S-PLUS. Springer
</p>
<p>For details of how GAMMs are set up here for estimation using <code>lme</code> see:
</p>
<p>Wood, S.N. (2006) Low rank scale invariant tensor product smooths for
Generalized Additive Mixed Models. Biometrics 62(4):1025-1036
</p>
<p>or 
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

  
<p><code><a href="#topic+gamm">gamm</a></code>, <code><a href="#topic+formXtViX">formXtViX</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see also ?formXtViX for use of extract.lme.cov2
require(mgcv)
library(nlme)
data(Rail)
b &lt;- lme(travel~1,Rail,~1|Rail)
extract.lme.cov(b)
extract.lme.cov2(b)
</code></pre>

<hr>
<h2 id='factor.smooth'>Factor smooth interactions in GAMs</h2><span id='topic+factor.smooth.interaction'></span><span id='topic+factor.smooth'></span>

<h3>Description</h3>

<p>The interaction of one or more factors with a smooth effect, produces a separate smooth for each factor level. These smooths can have different smoothing parameters, or all have the same smoothing parameter. There are several vays to set them up.
</p>

<dl>
<dt>Factor <code>by</code> variables.</dt><dd><p>If the <code>by</code> variables for a smooth (specified using <code><a href="#topic+s">s</a></code>, <code><a href="#topic+te">te</a></code>, <code><a href="#topic+ti">ti</a></code> or <code><a href="#topic+t2">t2</a></code>) is a factor, then a separate smooth is produced for each factor level. If the factor is ordered, then no smooth is produced for its first level: this is useful for setting up models which have a reference level smooth and then difference to reference smooths for each factor level except the first (which is the reference). Giving the smooth an <code>id</code> forces the same smoothing parameter to be used for all levels of the factor. For example <code>s(x,by=fac,id=1)</code> would produce a separate smooth of <code>x</code> for each level of <code>fac</code>, with each smooth having the same smoothing parameter. See <a href="#topic+gam.models">gam.models</a> for more.</p>
</dd>
<dt>Sum to zero smooth interactions</dt><dd><p><code>bs="sz"</code> These factor smooth interactions are specified using <code>s(...,bs="sz")</code>. There may be several factors supplied, and a smooth is produced for each combination of factor levels. The smooths are constructed to exclude the &lsquo;main effect&rsquo; smooth, or the effects of individual smooths produced for lower order combinations of factor levels. For example, with a single factor, the smooths for the different factor levels are so constrained that the sum over all factor levels of equivalent spline coefficients are all zero. This allows the meaningful and identifiable construction of models with a main effect smooth plus smooths for the difference between each factor level and the main effect. Such a construction is often more natural than the <code>by</code> variable with ordered factors construction. See <code><a href="#topic+smooth.construct.sz.smooth.spec">smooth.construct.sz.smooth.spec</a></code>.</p>
</dd>
<dt>Random wiggly curves</dt><dd><p><code>bs="fs"</code> This approach produces a smooth curve for each level of a single factor, treating the curves as entirely random. This means that in principle a model can be constructed with a main effect plus factor level smooth deviations from that effect. However the model is not forced to make the main effect do as much of the work as possible, in the way that the <code>"sz"</code> approach does. This approach can be very efficient with <code><a href="#topic+gamm">gamm</a></code> as it exploits the nested estimation available in <code>lme</code>. See <code><a href="#topic+smooth.construct.fs.smooth.spec">smooth.construct.fs.smooth.spec</a></code>.
</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> with input from Matteo Fasiolo.</p>


<h3>See Also</h3>

<p><code><a href="#topic+smooth.construct.fs.smooth.spec">smooth.construct.fs.smooth.spec</a></code>, <code><a href="#topic+smooth.construct.sz.smooth.spec">smooth.construct.sz.smooth.spec</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(0)
## simulate data...
f0 &lt;- function(x) 2 * sin(pi * x)
f1 &lt;- function(x,a=2,b=-1) exp(a * x)+b
f2 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
            (10 * x)^3 * (1 - x)^10
n &lt;- 500;nf &lt;- 25
fac &lt;- sample(1:nf,n,replace=TRUE)
x0 &lt;- runif(n);x1 &lt;- runif(n);x2 &lt;- runif(n)
a &lt;- rnorm(nf)*.2 + 2;b &lt;- rnorm(nf)*.5
f &lt;- f0(x0) + f1(x1,a[fac],b[fac]) + f2(x2)
fac &lt;- factor(fac)
y &lt;- f + rnorm(n)*2
## so response depends on global smooths of x0 and 
## x2, and a smooth of x1 for each level of fac.

## fit model...
bm &lt;- gamm(y~s(x0)+ s(x1,fac,bs="fs",k=5)+s(x2,k=20))
plot(bm$gam,pages=1)
summary(bm$gam)

bd &lt;- bam(y~s(x0)+ s(x1) + s(x1,fac,bs="sz",k=5)+s(x2,k=20),discrete=TRUE)
plot(bd,pages=1)
summary(bd)



## Could also use...
## b &lt;- gam(y~s(x0)+ s(x1,fac,bs="fs",k=5)+s(x2,k=20),method="ML")
## ... but its slower (increasingly so with increasing nf)
## b &lt;- gam(y~s(x0)+ t2(x1,fac,bs=c("tp","re"),k=5,full=TRUE)+
##        s(x2,k=20),method="ML"))
## ... is exactly equivalent. 
</code></pre>

<hr>
<h2 id='family.mgcv'>Distribution families in mgcv</h2><span id='topic+family.mgcv'></span>

<h3>Description</h3>

<p>As well as the standard families (of class <code>family</code>) documented in <code><a href="stats.html#topic+family">family</a></code> (see also <code><a href="stats.html#topic+glm">glm</a></code>) which can be used with functions <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+bam">bam</a></code> and <code><a href="#topic+gamm">gamm</a></code>, <code>mgcv</code> also supplies some extra families, most of which are currently only usable with <code><a href="#topic+gam">gam</a></code>, although some can also be used with <code><a href="#topic+bam">bam</a></code>. These are described here.
</p>


<h3>Details</h3>

<p>The following families (class <code>family</code>) are in the exponential family given the value of a single parameter. They are usable with all modelling functions.
</p>

<ul>
<li> <p><code><a href="#topic+Tweedie">Tweedie</a></code> An exponential family distribution for which the variance of the response is given by the mean response to the power <code>p</code>. 
<code>p</code> is in (1,2) and must be supplied. Alternatively, see <code><a href="#topic+tw">tw</a></code> to estimate <code>p</code> (<code>gam/bam</code> only).
</p>
</li>
<li> <p><code><a href="#topic+negbin">negbin</a></code> The negative binomial. Alternatively see <code><a href="#topic+nb">nb</a></code> to estimate the <code>theta</code> parameter of the negative binomial (<code>gam/bam</code> only). 
</p>
</li></ul>

<p>The following families (class <code>extended.family</code>) are for regression type models dependent on a single linear predictor, and with a log likelihood
which is a sum of independent terms, each corresponding to a single response observation. Usable with <code><a href="#topic+gam">gam</a></code>, with smoothing parameter estimation by <code>"NCV"</code>, <code>"REML"</code> or <code>"ML"</code> (the latter does not integrate the unpenalized and parameteric effects out of the marginal likelihood optimized for the smoothing parameters). Also usable with <code><a href="#topic+bam">bam</a></code>.
</p>

<ul>
<li> <p><code><a href="#topic+betar">betar</a></code> for proportions data on (0,1) when the binomial is not appropriate.
</p>
</li>
<li> <p><code><a href="#topic+cnorm">cnorm</a></code> censored normal distribution, for log normal accelerated failure time models, Tobit regression and rounded data, for example. 
</p>
</li>
<li> <p><code><a href="#topic+nb">nb</a></code> for negative binomial data when the <code>theta</code> parameter is to be estimated.
</p>
</li>
<li> <p><code><a href="#topic+ocat">ocat</a></code> for ordered categorical data.
</p>
</li>
<li> <p><code><a href="#topic+scat">scat</a></code> scaled t for heavy tailed data that would otherwise be modelled as Gaussian.
</p>
</li>
<li> <p><code><a href="#topic+tw">tw</a></code> for Tweedie distributed data, when the power parameter relating the variance to the mean is to be estimated.
</p>
</li>
<li> <p><code><a href="#topic+ziP">ziP</a></code> for zero inflated Poisson data, when the zero inflation rate depends simply on the Poisson mean.
</p>
</li></ul>
 
<p>The above families of class <code>family</code> and <code>extended.family</code> can be combined to model data where different response observations come from different distributions. For example, when modelling the combination of presence-absence and abundance data, <code>binomial</code> and <code>nb</code> families might be used.  
</p>

<ul>
<li> <p><code><a href="#topic+gfam">gfam</a></code> creates a 'grouped family' (or 'family group') from a list of families. The response is supplied as a two column matrix, the first containing the response observations, and the second an index of the family to which each observation relates.
</p>
</li></ul>

<p>The following families (class <code>general.family</code>) implement more general model classes. Usable only with <code><a href="#topic+gam">gam</a></code> and only with REML or NCV smoothing parameter estimation.
</p>

<ul>
<li> <p><code><a href="#topic+cox.ph">cox.ph</a></code> the Cox Proportional Hazards model for survival data (no NCV).
</p>
</li>
<li> <p><code><a href="#topic+gammals">gammals</a></code> a gamma location-scale model, where the mean and standared deviation are modelled with separate linear predictors.
</p>
</li>
<li> <p><code><a href="#topic+gaulss">gaulss</a></code> a Gaussian location-scale model where the mean and the standard deviation are both modelled using smooth linear predictors.
</p>
</li>
<li> <p><code><a href="#topic+gevlss">gevlss</a></code> a generalized extreme value (GEV) model where the location, scale and shape parameters are each modelled using a linear predictor.
</p>
</li>
<li> <p><code><a href="#topic+gumbls">gumbls</a></code> a Gumbel location-scale model (2 linear predictors).
</p>
</li>
<li> <p><code><a href="#topic+multinom">multinom</a></code>: multinomial logistic regression, for unordered categorical responses.  
</p>
</li>
<li> <p><code><a href="#topic+mvn">mvn</a></code>: multivariate normal additive models (no NCV).
</p>
</li>
<li> <p><code><a href="#topic+shash">shash</a></code> Sinh-arcsinh location scale and shape model family (4 linear predicors).
</p>
</li>
<li> <p><code><a href="#topic+twlss">twlss</a></code> Tweedie location scale and variance power model family (3 linear predicors).  Can only be fitted using EFS method.
</p>
</li>
<li> <p><code><a href="#topic+ziplss">ziplss</a></code> a &lsquo;two-stage&rsquo; zero inflated Poisson model, in which 'potential-presence' is modelled with one linear predictor, and Poisson mean abundance
given potential presence is modelled with a second linear predictor.
</p>
</li></ul>
 


<h3>Author(s)</h3>

<p> Simon N. Wood (s.wood@r-project.org) &amp; Natalya Pya
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>

<hr>
<h2 id='FFdes'>Level 5 fractional factorial designs</h2><span id='topic+FFdes'></span>

<h3>Description</h3>

<p>Computes level 5 fractional factorial designs for up to 120 factors
using the agorithm of Sanchez and Sanchez (2005), and optionally central composite designs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FFdes(size=5,ccd=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FFdes_+3A_size">size</code></td>
<td>
<p>number of factors up to 120.</p>
</td></tr>
<tr><td><code id="FFdes_+3A_ccd">ccd</code></td>
<td>
<p>if <code>TRUE</code>, adds points along each axis at the same distance from the origin as the points in the
fractional factorial design, to create the outer points of a central composite design. Add central points to complete.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Basically a translation of the code provided in the appendix of Sanchez and Sanchez (2005). 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Sanchez, S. M. &amp; Sanchez, P. J. (2005) Very large fractional factorial and central composite designs.
ACM Transactions on Modeling and Computer Simulation. 15: 362-377
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(mgcv)
  plot(rbind(0,FFdes(2,TRUE)),xlab="x",ylab="y",
       col=c(2,1,1,1,1,4,4,4,4),pch=19,main="CCD")
  FFdes(5)
  FFdes(5,TRUE)
</code></pre>

<hr>
<h2 id='fix.family.link'>Modify families for use in GAM fitting and checking</h2><span id='topic+fix.family.link'></span><span id='topic+fix.family.var'></span><span id='topic+fix.family.ls'></span><span id='topic+fix.family.qf'></span><span id='topic+fix.family.rd'></span>

<h3>Description</h3>

<p> Generalized Additive Model fitting by &lsquo;outer&rsquo; iteration,
requires extra derivatives of the variance and link functions to be 
added to family objects. The first 3 functions add what is needed. Model checking can
be aided by adding quantile and random deviate generating functions to the family. 
The final two functions do this.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fix.family.link(fam)
fix.family.var(fam)
fix.family.ls(fam)
fix.family.qf(fam)
fix.family.rd(fam)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fix.family.link_+3A_fam">fam</code></td>
<td>
<p>A <code>family</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>Consider the first 3 function first.
</p>
<p>Outer iteration GAM estimation requires derivatives of the GCV, UBRE/gAIC,
GACV, REML or ML score, which are obtained by finding the derivatives of the model
coefficients w.r.t. the log smoothing parameters, using the implicit function theorem. 
The expressions for the derivatives require the second and third derivatives of the link 
w.r.t. the mean (and the 4th derivatives if Fisher scoring is not used). Also required are the first
and second derivatives of the variance function w.r.t. the mean (plus the third 
derivative if Fisher scoring is not used). Finally REML or ML estimation of smoothing parameters
requires the log saturated likelihood and its first two derivatives w.r.t. the scale parameter.
These functions add functions evaluating these quantities to a family. 
</p>
<p>If the family already has functions <code>dvar</code>, <code>d2var</code>, <code>d3var</code>, <code>d2link</code>,
<code>d3link</code>, <code>d4link</code> and for RE/ML <code>ls</code>, then these functions simply 
return the family unmodified: this allows non-standard links
to be used with <code><a href="#topic+gam">gam</a></code> when using outer iteration (performance
iteration operates with unmodified families). Note that if you only need Fisher scoring then 
<code>d4link</code> and <code>d3var</code> can be dummy, as they are ignored. Similalry <code>ls</code> is only needed for 
RE/ML.
</p>
<p>The <code>dvar</code> function is a function of a mean vector, <code>mu</code>, and returns
a vector of corresponding first derivatives of the family variance
function. The <code>d2link</code> function is also a function of a vector of mean
values, <code>mu</code>: it returns a vector of second derivatives of the link,
evaluated at <code>mu</code>. Higher derivatives are defined similarly.
</p>
<p>If modifying your own family, note that you can often get away with supplying
only a <code>dvar</code> and <code>d2var</code>, function if your family only requires links that occur in
one of the standard families.
</p>
<p>The second two functions are useful for investigating the distribution of residuals and are used by 
<code><a href="#topic+qq.gam">qq.gam</a></code>. If possible the functions add quantile (<code>qf</code>) or random deviate (<code>rd</code>) generating functions 
to the family. If a family already has <code>qf</code> or <code>rd</code> 
functions then it is left unmodified. <code>qf</code> functions are only available for some families, and for quasi families 
neither type of function is available.
</p>


<h3>Value</h3>

<p>A family object with extra component functions <code>dvar</code>,
<code>d2var</code>, <code>d2link</code>, <code>d3link</code>, <code>d4link</code>, <code>ls</code>, and possibly <code>qf</code> and <code>rd</code>, 
depending on which functions are called. <code>fix.family.var</code> also adds a variable <code>scale</code> set to
negative to indicate that family has a free scale parameter.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>See Also</h3>

  
<p><code><a href="#topic+gam.fit3">gam.fit3</a></code>, <code><a href="#topic+qq.gam">qq.gam</a></code></p>

<hr>
<h2 id='fixDependence'>Detect linear dependencies of one matrix on another</h2><span id='topic+fixDependence'></span>

<h3>Description</h3>

<p>Identifies columns of a matrix <code>X2</code> which are linearly
dependent on columns of a matrix <code>X1</code>. Primarily of use in setting up 
identifiability constraints for nested GAMs. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fixDependence(X1,X2,tol=.Machine$double.eps^.5,rank.def=0,strict=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fixDependence_+3A_x1">X1</code></td>
<td>
<p> A matrix.</p>
</td></tr>
<tr><td><code id="fixDependence_+3A_x2">X2</code></td>
<td>
<p> A matrix, the columns of which may be partially linearly
dependent on the columns of <code>X1</code>.</p>
</td></tr>
<tr><td><code id="fixDependence_+3A_tol">tol</code></td>
<td>
<p>The tolerance to use when assessing linear dependence.</p>
</td></tr>
<tr><td><code id="fixDependence_+3A_rank.def">rank.def</code></td>
<td>
<p>If the degree of rank deficiency in <code>X2</code>, given <code>X1</code>, 
is known, then it can be supplied here, and <code>tol</code> is then ignored. 
Unused unless positive and not greater than the number of columns in <code>X2</code>.</p>
</td></tr>
<tr><td><code id="fixDependence_+3A_strict">strict</code></td>
<td>
<p>if <code>TRUE</code> then only columns individually dependent on <code>X1</code> are detected,
if <code>FALSE</code> then enough columns to make the reduced <code>X2</code> full rank and 
independent of <code>X1</code> are detected.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> The algorithm uses a simple approach based on QR decomposition: see
Wood (2017, section 5.6.3) for details.
</p>


<h3>Value</h3>

<p> A vector of the columns of <code>X2</code> which are linearly dependent on
columns of <code>X1</code> (or which need to be deleted to acheive independence and full rank 
if <code>strict==FALSE</code>). <code>NULL</code> if the two matrices are independent.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
n&lt;-20;c1&lt;-4;c2&lt;-7
X1&lt;-matrix(runif(n*c1),n,c1)
X2&lt;-matrix(runif(n*c2),n,c2)
X2[,3]&lt;-X1[,2]+X2[,4]*.1
X2[,5]&lt;-X1[,1]*.2+X1[,2]*.04
fixDependence(X1,X2)
fixDependence(X1,X2,strict=TRUE)
</code></pre>

<hr>
<h2 id='formula.gam'>GAM formula</h2><span id='topic+formula.gam'></span>

<h3>Description</h3>

<p> Description of <code><a href="#topic+gam">gam</a></code> formula (see Details), and how to extract it from a fitted <code>gam</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
formula(x,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="formula.gam_+3A_x">x</code></td>
<td>
<p> fitted model objects of class <code>gam</code> (see <code><a href="#topic+gamObject">gamObject</a></code>) as produced by <code>gam()</code>.</p>
</td></tr>
<tr><td><code id="formula.gam_+3A_...">...</code></td>
<td>
<p>un-used in this case</p>
</td></tr> 
</table>


<h3>Details</h3>

 <p><code><a href="#topic+gam">gam</a></code> will accept a formula or, with some families, a list of formulae. 
Other <code>mgcv</code> modelling functions will not accept a list. The list form provides a mechanism for 
specifying several linear predictors, and allows these to share terms: see below.
</p>
<p>The formulae supplied to <code><a href="#topic+gam">gam</a></code> are exactly like those supplied to 
<code><a href="stats.html#topic+glm">glm</a></code> except that smooth terms, <code><a href="#topic+s">s</a></code>, <code><a href="#topic+te">te</a></code>, <code><a href="#topic+ti">ti</a></code> and <code><a href="#topic+t2">t2</a></code> can
be added to the right hand side (and <code>.</code> is not supported in <code>gam</code> formulae).
</p>
<p>Smooth terms are specified by expressions of the form: <br />
<code>s(x1,x2,...,k=12,fx=FALSE,bs="tp",by=z,id=1)</code><br />
where <code>x1</code>, <code>x2</code>, etc. are the covariates which the smooth
is a function of, and <code>k</code> is the dimension of the basis used to
represent the smooth term. If <code>k</code> is not
specified then basis specific defaults are used. Note that these defaults are
essentially arbitrary, and it  is important to check that they are not so 
small that they cause oversmoothing (too large just slows down computation). 
Sometimes the modelling context suggests sensible values for <code>k</code>, but if not
informal checking is easy: see <code><a href="#topic+choose.k">choose.k</a></code> and <code><a href="#topic+gam.check">gam.check</a></code>.  
</p>
<p><code>fx</code> is used to indicate whether or not this term should be unpenalized, 
and therefore have a fixed number of degrees of freedom set by <code>k</code> 
(almost always <code>k-1</code>). <code>bs</code> indicates the basis to use for the smooth:
the built in options are described in <code><a href="#topic+smooth.terms">smooth.terms</a></code>, and user defined 
smooths can be added (see <code><a href="#topic+user.defined.smooth">user.defined.smooth</a></code>). If <code>bs</code> is not supplied 
then the default <code>"tp"</code> (<code><a href="#topic+tprs">tprs</a></code>) basis is used. 
<code>by</code> can be used to specify a variable by which
the smooth should be multiplied. For example <code>gam(y~s(x,by=z))</code>
would specify a model <code class="reqn"> E(y) = f(x)z</code> where
<code class="reqn">f(\cdot)</code> is a smooth function. The <code>by</code> 
option is particularly useful for models in
which different functions of the same variable are required for
each level of a factor and for &lsquo;varying coefficient models&rsquo;: see <code><a href="#topic+gam.models">gam.models</a></code>. 
<code>id</code> is used to give smooths identities: smooths with the same identity have
the same basis, penalty and smoothing parameter (but different coefficients, so they are 
different functions). 
</p>
<p>An alternative for specifying smooths of more than one covariate is e.g.: <br />
<code>te(x,z,bs=c("tp","tp"),m=c(2,3),k=c(5,10))</code><br /> which would specify a tensor product 
smooth of the two covariates <code>x</code> and <code>z</code> constructed from marginal t.p.r.s. bases 
of dimension 5 and 10 with marginal penalties of order 2 and 3. Any combination of basis types is 
possible, as is any number of covariates. <code><a href="#topic+te">te</a></code> provides further information. 
<code><a href="#topic+ti">ti</a></code> terms are a variant designed to be used as interaction terms when the main 
effects (and any lower order interactions) are present. <code><a href="#topic+t2">t2</a></code> produces tensor product
smooths that are the natural low rank analogue of smoothing spline anova models. 
</p>
<p><code>s</code>, <code>te</code>, <code>ti</code> and <code>t2</code> terms accept an <code>sp</code> argument of supplied smoothing parameters: positive 
values are taken  as fixed values to be used, negative to indicate that the parameter should be estimated. If
<code>sp</code> is supplied then it over-rides whatever is in the <code>sp</code> argument to <code>gam</code>, if it is not supplied 
then it defaults to all negative, but does not over-ride the <code>sp</code> argument to <code>gam</code>.
</p>
<p>Formulae can involve nested or &ldquo;overlapping&rdquo; terms such as <br />
<code>y~s(x)+s(z)+s(x,z)</code> or <code>y~s(x,z)+s(z,v)</code><br /> 
but nested models should really be set up using <code><a href="#topic+ti">ti</a></code> terms:
see <code><a href="#topic+gam.side">gam.side</a></code> for further details and examples. 
</p>
<p>Smooth terms in a <code>gam</code> formula will accept matrix arguments as covariates (and corresponding <code>by</code> variable), 
in which case a &lsquo;summation convention&rsquo; is invoked. Consider the example of <code>s(X,Z,by=L)</code> where <code>X</code>, <code>Z</code>
and <code>L</code> are n by m matrices. Let <code>F</code> be the n by m matrix that results from evaluating the smooth at the values in 
<code>X</code> and <code>Z</code>. Then the contribution to the linear predictor from the term will be
<code>rowSums(F*L)</code> (note the element-wise multiplication). This convention allows the linear predictor of the GAM
to depend on (a discrete approximation to) any linear functional of a smooth: see <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code> for more information and examples (including functional linear models/signal regression).
</p>
<p>Note that <code>gam</code> allows any term in the model formula to be penalized (possibly by multiple penalties), 
via the <code>paraPen</code> argument. See <code><a href="#topic+gam.models">gam.models</a></code> for details and example code.
</p>
<p>When several formulae are provided in a list, then they can be used to specify multiple linear predictors 
for families for which this makes sense (e.g. <code><a href="#topic+mvn">mvn</a></code>). The first formula in the list must include 
a response variable, but later formulae need not (depending on the requirements of the family). Let the linear predictors 
be indexed, 1 to d where d is the number of linear predictors, and the indexing is in the order in which the 
formulae appear in the list. It is possible to supply extra formulae specifying that several linear predictors 
should share some terms. To do this a formula is supplied in which the response is replaced by numbers specifying the 
indices of the linear predictors which will shre the terms specified on the r.h.s. For example <code>1+3~s(x)+z-1</code> specifies that linear predictors 1 and 3 will share the terms <code>s(x)</code> and <code>z</code> (but we don't want an extra intercept, as this would usually be unidentifiable). Note that it is possible that a linear predictor only includes shared terms: it must still have its own formula, but the r.h.s. would simply be <code>-1</code> (e.g. <code>y ~ -1</code> or <code>~ -1</code>). See <code><a href="#topic+multinom">multinom</a></code> for an example. 
</p>


<h3>Value</h3>

<p> Returns the model formula, <code>x$formula</code>. Provided so that <code>anova</code> methods
print an appropriate description of the model.
</p>


<h3>WARNING</h3>

<p>A <code>gam</code> formula should not refer to variables using e.g. <code>dat[["x"]]</code>. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>See Also</h3>

  <p><code><a href="#topic+gam">gam</a></code></p>

<hr>
<h2 id='formXtViX'> Form component of GAMM covariance matrix</h2><span id='topic+formXtViX'></span>

<h3>Description</h3>

<p> This is a service routine for <code><a href="#topic+gamm">gamm</a></code>. Given,
<code class="reqn">V</code>, an estimated covariance matrix obtained using <code><a href="#topic+extract.lme.cov2">extract.lme.cov2</a></code> this
routine forms a matrix square root of <code class="reqn"> X^TV^{-1}X</code> as efficiently as possible, given
the structure of <code class="reqn">V</code> (usually sparse).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>formXtViX(V,X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="formXtViX_+3A_v">V</code></td>
<td>
<p> A data covariance matrix list returned from <code><a href="#topic+extract.lme.cov2">extract.lme.cov2</a></code></p>
</td></tr>
<tr><td><code id="formXtViX_+3A_x">X</code></td>
<td>
<p> A model matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> The covariance matrix returned by <code><a href="#topic+extract.lme.cov2">extract.lme.cov2</a></code> may
be in a packed and re-ordered format, since it is usually sparse. Hence a
special service routine is required to form the required products involving
this matrix. 
</p>


<h3>Value</h3>

<p> A matrix, R such that <code>crossprod(R)</code> gives <code class="reqn"> X^TV^{-1}X</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>For <code>lme</code> see:
</p>
<p>Pinheiro J.C. and Bates, D.M. (2000) Mixed effects Models in S and S-PLUS. Springer
</p>
<p>For details of how GAMMs are set up for estimation using <code>lme</code> see:
</p>
<p>Wood, S.N. (2006) Low rank scale invariant tensor product smooths for
Generalized Additive Mixed Models. Biometrics 62(4):1025-1036
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

  
<p><code><a href="#topic+gamm">gamm</a></code>, <code><a href="#topic+extract.lme.cov2">extract.lme.cov2</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
library(nlme)
data(ergoStool)
b &lt;- lme(effort ~ Type, data=ergoStool, random=~1|Subject)
V1 &lt;- extract.lme.cov(b, ergoStool)
V2 &lt;- extract.lme.cov2(b, ergoStool)
X &lt;- model.matrix(b, data=ergoStool)
crossprod(formXtViX(V2, X))
t(X)
</code></pre>

<hr>
<h2 id='fs.test'>FELSPLINE test function</h2><span id='topic+fs.test'></span><span id='topic+fs.boundary'></span>

<h3>Description</h3>

<p>Implements a finite area test function based on one proposed by Tim Ramsay (2002).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
fs.test(x,y,r0=.1,r=.5,l=3,b=1,exclude=TRUE)
fs.boundary(r0=.1,r=.5,l=3,n.theta=20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.test_+3A_x">x</code>, <code id="fs.test_+3A_y">y</code></td>
<td>
<p>Points at which to evaluate the test function.</p>
</td></tr>
<tr><td><code id="fs.test_+3A_r0">r0</code></td>
<td>
<p>The test domain is a sort of bent sausage. This is the radius of
the inner bend</p>
</td></tr>
<tr><td><code id="fs.test_+3A_r">r</code></td>
<td>
<p>The radius of the curve at the centre of the sausage.</p>
</td></tr>
<tr><td><code id="fs.test_+3A_l">l</code></td>
<td>
<p>The length of an arm of the sausage.</p>
</td></tr>
<tr><td><code id="fs.test_+3A_b">b</code></td>
<td>
<p>The rate at which the function increases per unit increase in
distance along the centre line of the sausage.</p>
</td></tr>
<tr><td><code id="fs.test_+3A_exclude">exclude</code></td>
<td>
<p>Should exterior points be set to <code>NA</code>?</p>
</td></tr>
<tr><td><code id="fs.test_+3A_n.theta">n.theta</code></td>
<td>
<p>How many points to use in a piecewise linear representation of
a quarter of a circle, when generating the boundary curve.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function details are not given in the source article: but this is pretty
close. The function is modified from Ramsay (2002), in that it bulges, rather
than being flat: this makes a better test of the smoother.
</p>


<h3>Value</h3>

 <p><code>fs.test</code> returns function evaluations, or <code>NA</code>s for points
outside the boundary. <code>fs.boundary</code> returns a list of <code>x,y</code> points
to be jointed up in order to define/draw the boundary.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Tim Ramsay (2002) &quot;Spline smoothing over difficult regions&quot; J.R.Statist. Soc. B  64(2):307-319 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
## plot the function, and its boundary...
fsb &lt;- fs.boundary()
m&lt;-300;n&lt;-150 
xm &lt;- seq(-1,4,length=m);yn&lt;-seq(-1,1,length=n)
xx &lt;- rep(xm,n);yy&lt;-rep(yn,rep(m,n))
tru &lt;- matrix(fs.test(xx,yy),m,n) ## truth
image(xm,yn,tru,col=heat.colors(100),xlab="x",ylab="y")
lines(fsb$x,fsb$y,lwd=3)
contour(xm,yn,tru,levels=seq(-5,5,by=.25),add=TRUE)
</code></pre>

<hr>
<h2 id='full.score'>GCV/UBRE score for use within nlm</h2><span id='topic+full.score'></span>

<h3>Description</h3>

<p> Evaluates GCV/UBRE score for a GAM, given smoothing
parameters. The routine calls <code><a href="#topic+gam.fit">gam.fit</a></code> to fit the model, and is
usually called by <code><a href="stats.html#topic+nlm">nlm</a></code> to optimize the smoothing parameters. 
</p>
<p>This is basically a service routine for <code><a href="#topic+gam">gam</a></code>, and is not usually 
called directly by users. It is only used in this context for GAMs fitted by
outer iteration (see <code><a href="#topic+gam.outer">gam.outer</a></code>) when the the outer method is 
<code>"nlm.fd"</code> (see <code><a href="#topic+gam">gam</a></code> argument <code>optimizer</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>full.score(sp,G,family,control,gamma,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="full.score_+3A_sp">sp</code></td>
<td>
<p>The logs of the smoothing parameters</p>
</td></tr>
<tr><td><code id="full.score_+3A_g">G</code></td>
<td>
<p>a list returned by <code>mgcv:::gam.setup</code></p>
</td></tr>
<tr><td><code id="full.score_+3A_family">family</code></td>
<td>
<p>The family object for the GAM.</p>
</td></tr>
<tr><td><code id="full.score_+3A_control">control</code></td>
<td>
<p>a list returned be <code><a href="#topic+gam.control">gam.control</a></code></p>
</td></tr>
<tr><td><code id="full.score_+3A_gamma">gamma</code></td>
<td>
<p>the degrees of freedom inflation factor (usually 1).</p>
</td></tr>
<tr><td><code id="full.score_+3A_...">...</code></td>
<td>
<p>other arguments, typically for passing on to <code>gam.fit</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> The value of the GCV/UBRE score, with attribute <code>"full.gam.object"</code>
which is the full object returned by <code><a href="#topic+gam.fit">gam.fit</a></code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>

<hr>
<h2 id='gam'>Generalized additive models with integrated smoothness estimation</h2><span id='topic+gam'></span>

<h3>Description</h3>

<p> Fits a generalized additive model (GAM) to
data, the term &lsquo;GAM&rsquo; being taken to include any quadratically penalized GLM and a variety of 
other models estimated by a quadratically penalised likelihood type approach (see <code><a href="#topic+family.mgcv">family.mgcv</a></code>).  
The degree of smoothness of model terms is estimated as part of
fitting. <code>gam</code> can also fit any GLM subject to multiple quadratic penalties (including 
estimation of degree of penalization). Confidence/credible intervals are readily
available for any quantity predicted using a fitted model.
</p>
<p>Smooth terms are represented using penalized regression splines (or similar smoothers)
with smoothing parameters selected by GCV/UBRE/AIC/REML/NCV or by regression splines with
fixed degrees of freedom (mixtures of the two are permitted). Multi-dimensional smooths are 
available using penalized thin plate regression splines (isotropic) or tensor product splines 
(when an isotropic smooth is inappropriate), and users can add smooths. 
Linear functionals of smooths can also be included in models.
For an overview of the smooths available see <code><a href="#topic+smooth.terms">smooth.terms</a></code>. 
For more on specifying models see <code><a href="#topic+gam.models">gam.models</a></code>, <code><a href="#topic+random.effects">random.effects</a></code> and <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code>. For more on model selection see <code><a href="#topic+gam.selection">gam.selection</a></code>. Do read <code><a href="#topic+gam.check">gam.check</a></code> and <code><a href="#topic+choose.k">choose.k</a></code>.
</p>
<p>See package <code>gam</code>, for GAMs via the original Hastie and Tibshirani approach (see details for differences to this implementation).
</p>
<p>For very large datasets see <code><a href="#topic+bam">bam</a></code>, for mixed GAM see <code><a href="#topic+gamm">gamm</a></code> and <code><a href="#topic+random.effects">random.effects</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
gam(formula,family=gaussian(),data=list(),weights=NULL,subset=NULL,
    na.action,offset=NULL,method="GCV.Cp",
    optimizer=c("outer","newton"),control=list(),scale=0,
    select=FALSE,knots=NULL,sp=NULL,min.sp=NULL,H=NULL,gamma=1,
    fit=TRUE,paraPen=NULL,G=NULL,in.out,drop.unused.levels=TRUE,
    drop.intercept=NULL,nei=NULL,discrete=FALSE,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="gam_+3A_formula">formula</code></td>
<td>
<p> A GAM formula, or a list of formulae (see <code><a href="#topic+formula.gam">formula.gam</a></code> and also <code><a href="#topic+gam.models">gam.models</a></code>). 
These are exactly like the formula for a GLM except that smooth terms, <code><a href="#topic+s">s</a></code>, <code><a href="#topic+te">te</a></code>, <code><a href="#topic+ti">ti</a></code> 
and <code><a href="#topic+t2">t2</a></code>, can be added to the right hand side to specify that the linear predictor depends on smooth functions of predictors (or linear functionals of these).
</p>
</td></tr> 
<tr><td><code id="gam_+3A_family">family</code></td>
<td>

<p>This is a family object specifying the distribution and link to use in
fitting etc (see <code><a href="stats.html#topic+glm">glm</a></code> and <code><a href="stats.html#topic+family">family</a></code>). See 
<code><a href="#topic+family.mgcv">family.mgcv</a></code> for a full list of what is available, which goes well beyond exponential family.
Note that <code>quasi</code> families actually result in the use of extended quasi-likelihood 
if <code>method</code> is set to a RE/ML method (McCullagh and Nelder, 1989, 9.6).
</p>
</td></tr> 
<tr><td><code id="gam_+3A_data">data</code></td>
<td>
<p> A data frame or list containing the model response variable and 
covariates required by the formula. By default the variables are taken 
from <code>environment(formula)</code>: typically the environment from 
which <code>gam</code> is called.</p>
</td></tr> 
<tr><td><code id="gam_+3A_weights">weights</code></td>
<td>
<p> prior weights on the contribution of the data to the log likelihood. Note that a weight of 2, for example, 
is equivalent to having made exactly the same observation twice. If you want to re-weight the contributions 
of each datum without changing the overall magnitude of the log likelihood, then you should normalize the weights
(e.g. <code>weights &lt;- weights/mean(weights)</code>). </p>
</td></tr>
<tr><td><code id="gam_+3A_subset">subset</code></td>
<td>
<p> an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td></tr>
<tr><td><code id="gam_+3A_na.action">na.action</code></td>
<td>
<p> a function which indicates what should happen when the data
contain &lsquo;NA&rsquo;s.  The default is set by the &lsquo;na.action&rsquo; setting
of &lsquo;options&rsquo;, and is &lsquo;na.fail&rsquo; if that is unset.  The
&ldquo;factory-fresh&rdquo; default is &lsquo;na.omit&rsquo;.</p>
</td></tr>
<tr><td><code id="gam_+3A_offset">offset</code></td>
<td>
<p>Can be used to supply a model offset for use in fitting. Note
that this offset will always be completely ignored when predicting, unlike an offset 
included in <code>formula</code> (this used to conform to the behaviour of
<code>lm</code> and <code>glm</code>).</p>
</td></tr>
<tr><td><code id="gam_+3A_control">control</code></td>
<td>
<p>A list of fit control parameters to replace defaults returned by 
<code><a href="#topic+gam.control">gam.control</a></code>. Values not set assume default values. </p>
</td></tr>
<tr><td><code id="gam_+3A_method">method</code></td>
<td>
<p>The smoothing parameter estimation method. <code>"GCV.Cp"</code> to use GCV for unknown scale parameter and
Mallows' Cp/UBRE/AIC for known scale. <code>"GACV.Cp"</code> is equivalent, but using GACV in place of GCV. <code>"NCV"</code>
for neighbourhood cross-validation using the neighbourhood structure speficied by <code>nei</code> (<code>"QNCV"</code> for numerically more ribust version).  <code>"REML"</code> 
for REML estimation, including of unknown scale, <code>"P-REML"</code> for REML estimation, but using a Pearson estimate 
of the scale. <code>"ML"</code> and <code>"P-ML"</code> are similar, but using maximum likelihood in place of REML. Beyond the 
exponential family <code>"REML"</code> is the default, and the only other options are <code>"ML"</code>, <code>"NCV"</code> or <code>"QNCV"</code>.</p>
</td></tr>
<tr><td><code id="gam_+3A_optimizer">optimizer</code></td>
<td>
<p>An array specifying the numerical optimization method to use to optimize the smoothing 
parameter estimation criterion (given by <code>method</code>). <code>"outer"</code> 
for the direct nested optimization approach. <code>"outer"</code> can use several alternative optimizers, specified in the 
second element of <code>optimizer</code>: <code>"newton"</code> (default), <code>"bfgs"</code>, <code>"optim"</code> or <code>"nlm"</code>. <code>"efs"</code>
for the extended Fellner Schall method of Wood and Fasiolo (2017).</p>
</td></tr>
<tr><td><code id="gam_+3A_scale">scale</code></td>
<td>
<p> If this is positive then it is taken as the known scale parameter. Negative signals that the 
scale parameter is unknown. 0 signals that the scale parameter is 1  for Poisson and binomial and unknown otherwise. 
Note that (RE)ML methods can only work with scale parameter 1 for the Poisson and binomial cases.    
</p>
</td></tr> 
<tr><td><code id="gam_+3A_select">select</code></td>
<td>
<p> If this is <code>TRUE</code> then <code>gam</code> can add an extra penalty to each term so 
that it can be penalized to zero.  This means that the smoothing parameter estimation that is 
part of fitting can completely remove terms from the model. If the corresponding 
smoothing parameter is estimated as zero then the extra penalty has no effect. Use <code>gamma</code> to increase level of penalization.
</p>
</td></tr>
<tr><td><code id="gam_+3A_knots">knots</code></td>
<td>
<p>this is an optional list containing user specified knot values to be used for basis construction. 
For most bases the user simply supplies the knots to be used, which must match up with the <code>k</code> value
supplied (note that the number of knots is not always just <code>k</code>). 
See <code><a href="#topic+tprs">tprs</a></code> for what happens in the <code>"tp"/"ts"</code> case. 
Different terms can use different numbers of knots, unless they share a covariate.
</p>
</td></tr>
<tr><td><code id="gam_+3A_sp">sp</code></td>
<td>
<p>A vector of smoothing parameters can be provided here.
Smoothing parameters must be supplied in the order that the smooth terms appear in the model 
formula. Negative elements indicate that the parameter should be estimated, and hence a mixture 
of fixed and estimated parameters is possible. If smooths share smoothing parameters then <code>length(sp)</code> 
must correspond to the number of underlying smoothing parameters.</p>
</td></tr>
<tr><td><code id="gam_+3A_min.sp">min.sp</code></td>
<td>
<p>Lower bounds can be supplied for the smoothing parameters. Note
that if this option is used then the smoothing parameters <code>full.sp</code>, in the 
returned object, will need to be added to what is supplied here to get the 
smoothing parameters actually multiplying the penalties. <code>length(min.sp)</code> should 
always be the same as the total number of penalties (so it may be longer than <code>sp</code>,
if smooths share smoothing parameters).</p>
</td></tr>
<tr><td><code id="gam_+3A_h">H</code></td>
<td>
<p>A user supplied fixed quadratic penalty on the parameters of the 
GAM can be supplied, with this as its coefficient matrix. A common use of this term is 
to add a ridge penalty to the parameters of the GAM in circumstances in which the model
is close to un-identifiable on the scale of the linear predictor, but perfectly well
defined on the response scale.</p>
</td></tr>
<tr><td><code id="gam_+3A_gamma">gamma</code></td>
<td>
<p>Increase this beyond 1 to produce smoother models. <code>gamma</code> multiplies the effective degrees of freedom in the GCV or UBRE/AIC. <code>n/gamma</code> can be viewed as an effective sample size in the GCV score, and this also enables it to be used with REML/ML. Ignored with P-RE/ML or the <code>efs</code> optimizer. </p>
</td></tr> 
<tr><td><code id="gam_+3A_fit">fit</code></td>
<td>
<p>If this argument is <code>TRUE</code> then <code>gam</code> sets up the model and fits it, but if it is
<code>FALSE</code> then the model is set up and an object <code>G</code> containing what
would be required to fit is returned is returned. See argument <code>G</code>.</p>
</td></tr>
<tr><td><code id="gam_+3A_parapen">paraPen</code></td>
<td>
<p>optional list specifying any penalties to be applied to parametric model terms. 
<code><a href="#topic+gam.models">gam.models</a></code> explains more.</p>
</td></tr>
<tr><td><code id="gam_+3A_g">G</code></td>
<td>
<p>Usually <code>NULL</code>, but may contain the object returned by a previous call to <code>gam</code> with 
<code>fit=FALSE</code>, in which case all other arguments are ignored except for
<code>sp</code>, <code>gamma</code>, <code>in.out</code>, <code>scale</code>, <code>control</code>, <code>method</code> <code>optimizer</code> and <code>fit</code>.</p>
</td></tr>
<tr><td><code id="gam_+3A_in.out">in.out</code></td>
<td>
<p>optional list for initializing outer iteration. If supplied
then this must contain two elements: <code>sp</code> should be an array of
initialization values for all smoothing parameters (there must be a value for
all smoothing parameters, whether fixed or to be estimated, but those for
fixed s.p.s are not used); <code>scale</code> is the typical scale of the GCV/UBRE function,
for passing to the outer optimizer, or the the initial value of the scale parameter, if this is to
be estimated by RE/ML.</p>
</td></tr>
<tr><td><code id="gam_+3A_drop.unused.levels">drop.unused.levels</code></td>
<td>
<p>by default unused levels are dropped from factors before fitting. For some smooths 
involving factor variables you might want to turn this off. Only do so if you know what you are doing.</p>
</td></tr>
<tr><td><code id="gam_+3A_drop.intercept">drop.intercept</code></td>
<td>
<p>Set to <code>TRUE</code> to force the model to really not have a constant in the parametric model part,
even with factor variables present. Can be vector when <code>formula</code> is a list.</p>
</td></tr>
<tr><td><code id="gam_+3A_nei">nei</code></td>
<td>
<p>A list specifying the neighbourhood structure for <code><a href="#topic+NCV">NCV</a></code>. <code>k</code> is the vector of indices to be dropped for each neighbourhood and <code>m</code> gives the end of each neighbourhood. So <code>nei$k[(nei$m[j-1]+1):nei$m[j]]</code> gives the points dropped for the neighbourhood <code>j</code>. <code>i</code> is the vector of indices of points to predict, with corresponding endpoints <code>mi</code>. So <code>nei$i[(nei$mi[j-1]+1):nei$mi[j]]</code> indexes the points to predict for neighbourhood j. If <code>nei==NULL</code> (or <code>k</code> or <code>m</code> are missing) then leave-one-out cross validation is obtained. If <code>jackknife</code> is supplied then <code>TRUE</code> indicates to use raw jackknife covariances estimator and <code>FALSE</code> to use the conventional Bayes estimate. If not supplied then the estimator accounting for neighbourhood structure is used. <code>jackknife</code> ignored when <code>method</code> is not NCV.</p>
</td></tr>
<tr><td><code id="gam_+3A_discrete">discrete</code></td>
<td>
<p>experimental option for setting up models for use with discrete methods employed in <code><a href="#topic+bam">bam</a></code>. Do not modify.</p>
</td></tr>
<tr><td><code id="gam_+3A_...">...</code></td>
<td>
<p>further arguments for 
passing on e.g. to <code>gam.fit</code> (such as <code>mustart</code>). </p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>A generalized additive model (GAM) is a generalized linear model (GLM) in which the linear 
predictor is given by a user specified sum of smooth functions of the covariates plus a 
conventional parametric component of the linear predictor. A simple example is:
</p>
<p style="text-align: center;"><code class="reqn">\log\{E(y_i)\} = \alpha + f_1(x_{1i})+f_2(x_{2i})</code>
</p>

<p>where the (independent) response variables <code class="reqn">y_i \sim {\rm Poi }</code>, and
<code class="reqn">f_1</code> and <code class="reqn">f_2</code> are smooth functions of covariates <code class="reqn">x_1</code> and 
<code class="reqn">x_2</code>. The log is an example of a link function. Note that to be identifiable the model
requires constraints on the smooth functions. By default these are imposed automatically and require that the function sums to zero over the observed covariate values (the presence of a metric <code>by</code> variable is the only case which usually suppresses this). 
</p>
<p>If absolutely any smooth functions were allowed in model fitting then maximum likelihood 
estimation of such models would invariably result in complex over-fitting estimates of 
<code class="reqn">f_1</code>  and <code class="reqn">f_2</code>. For this reason the models are usually fit by 
penalized likelihood 
maximization, in which the model (negative log) likelihood is modified by the addition of 
a penalty for each smooth function, penalizing its &lsquo;wiggliness&rsquo;. To control the trade-off 
between penalizing wiggliness and penalizing badness of fit each penalty is multiplied by 
an associated smoothing parameter: how to estimate these parameters, and 
how to practically represent the smooth functions are the main statistical questions 
introduced by moving from GLMs to GAMs. 
</p>
<p>The <code>mgcv</code> implementation of <code>gam</code> represents the smooth functions using 
penalized regression splines, and by default uses basis functions for these splines that 
are designed to be optimal, given the number basis functions used. The smooth terms can be 
functions of any number of covariates and the user has some control over how smoothness of 
the functions is measured. 
</p>
<p><code>gam</code> in <code>mgcv</code> solves the smoothing parameter estimation problem by using the 
Generalized Cross Validation (GCV) criterion
</p>
<p style="text-align: center;"><code class="reqn">n D / (n - DoF)^2</code>
</p>

<p>or an Un-Biased Risk Estimator (UBRE )criterion
</p>
<p style="text-align: center;"><code class="reqn">D/n + 2 s DoF / n - s </code>
</p>
 
<p>where <code class="reqn">D</code> is the deviance, <code class="reqn">n</code> the number of data, <code class="reqn">s</code>
the scale parameter and 
<code class="reqn">DoF</code> the effective degrees of freedom of the model. Notice that UBRE is effectively
just AIC rescaled, but is only used when <code class="reqn">s</code> is known. 
</p>
<p>Alternatives are GACV, <code><a href="#topic+NCV">NCV</a></code> or a Laplace approximation to REML. There
is some evidence that the latter may actually be the most effective choice. 
The main computational challenge solved by the <code>mgcv</code> package is to optimize the smoothness selection criteria efficiently and reliably. 
</p>
<p>Broadly <code>gam</code> works by first constructing basis functions and one or more quadratic penalty 
coefficient matrices for each smooth term in the model formula, obtaining a model matrix for 
the strictly parametric part of the model formula, and combining these to obtain a 
complete model matrix (/design matrix) and a set of penalty matrices for the smooth terms. 
The linear identifiability constraints are also obtained at this point. The model is 
fit using <code><a href="#topic+gam.fit">gam.fit</a></code>, <code><a href="#topic+gam.fit3">gam.fit3</a></code> or variants, which are modifications 
of <code><a href="stats.html#topic+glm.fit">glm.fit</a></code>. The GAM 
penalized likelihood maximization problem is solved by Penalized Iteratively 
Re-weighted  Least Squares (P-IRLS) (see e.g. Wood 2000). 
Smoothing parameter selection is possible in one of three ways. (i)
&lsquo;Performance iteration&rsquo; uses the fact that at each P-IRLS step a working penalized linear model
is estimated, and the smoothing parameter estimation can be performed for each such working model.
Eventually, in most cases, both model parameter estimates and smoothing 
parameter estimates converge. This option is available in <code><a href="#topic+bam">bam</a></code> and <code><a href="#topic+gamm">gamm</a></code>.
(ii) Alternatively the P-IRLS scheme is iterated to convergence for each trial set of smoothing parameters,
and GCV, UBRE or REML scores are only evaluated on convergence - optimization is then &lsquo;outer&rsquo; to the P-IRLS
loop: in this case the P-IRLS iteration has to be differentiated, to
facilitate optimization, and <code><a href="#topic+gam.fit3">gam.fit3</a></code> or one of its variants is used in place of
<code>gam.fit</code>. (iii) The extended Fellner-Schall algorithm of Wood and Fasiolo (2017) alternates estimation of model coefficients with simple updates of smoothing parameters, eventually approximately maximizing the marginal likelihood of the model (REML). <code>gam</code> uses the second method, outer iteration, by default.
</p>
<p>Several alternative basis-penalty types  are built in for representing model
smooths, but alternatives can easily be added (see <code><a href="#topic+smooth.terms">smooth.terms</a></code> 
for an overview and <code><a href="#topic+smooth.construct">smooth.construct</a></code> for how to add smooth classes). The choice of the basis dimension 
(<code>k</code> in the <code>s</code>, <code>te</code>, <code>ti</code> and <code>t2</code> terms) is something that should be considered carefully 
(the exact value is not critical, but it is important not to make it restrictively small, nor very large and 
computationally costly). The basis should 
be chosen to be larger than is believed to be necessary to approximate the smooth function concerned. 
The effective degrees of freedom for the smooth will then be controlled by the smoothing penalty on 
the term, and (usually) selected automatically (with an upper limit set by
<code>k-1</code> or occasionally <code>k</code>). Of course 
the <code>k</code> should not be made too large, or computation will be slow (or in
extreme cases there will be more 
coefficients to estimate than there are data).
</p>
<p>Note that <code>gam</code> assumes a very inclusive definition of what counts as a GAM: 
basically any penalized GLM can be used: to this end <code>gam</code> allows the non smooth model 
components to be penalized via argument <code>paraPen</code> and allows the linear predictor to depend on 
general linear functionals of smooths, via the summation convention mechanism described in 
<code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code>. <code>link{family.mgcv}</code> details what is available beyond GLMs 
and the exponential family.
</p>
<p>Details of the default underlying fitting methods are given in Wood (2011, 2004) and Wood, Pya and Saefken (2016). Some alternative methods are discussed in Wood (2000, 2017). 
</p>
<p><code>gam()</code> is not a clone of Trevor Hastie's original (as supplied in S-PLUS or package <code>gam</code>). The major
differences are (i) that by default estimation of the
degree of smoothness of model terms is part of model fitting, (ii) a
Bayesian approach to variance estimation is employed that makes for easier
confidence interval calculation (with good coverage probabilities), (iii) that the model
can depend on any (bounded) linear functional of smooth terms, (iv) the parametric part of the model can be penalized, 
(v) simple random effects can be incorporated, and 
(vi) the facilities for incorporating smooths of more than one variable are
different: specifically there are no <code>lo</code> smooths, but instead (a) <code><a href="#topic+s">s</a></code>
terms can have more than one argument, implying an isotropic smooth and (b) <code><a href="#topic+te">te</a></code>, 
<code><a href="#topic+ti">ti</a></code> or <code><a href="#topic+t2">t2</a></code> smooths are
provided as an effective means for modelling smooth interactions of any
number of variables via scale invariant tensor product smooths. Splines on the sphere, Duchon splines 
and Gaussian Markov Random Fields are also available. (vii) Models beyond the exponential family are available. 
See package <code>gam</code>, for GAMs via the original Hastie and Tibshirani approach.
</p>


<h3>Value</h3>

 
<p>If <code>fit=FALSE</code> the function returns a list <code>G</code> of items needed to
fit a GAM, but doesn't actually fit it. 
</p>
<p>Otherwise the function returns an object of class <code>"gam"</code> as described in <code><a href="#topic+gamObject">gamObject</a></code>.
</p>


<h3>WARNINGS </h3>

<p>The default basis dimensions used for smooth terms are essentially arbitrary, and 
it should be checked that they are not too small. See <code><a href="#topic+choose.k">choose.k</a></code> and
<code><a href="#topic+gam.check">gam.check</a></code>. 
</p>
<p>Automatic smoothing parameter selection is not likely to work well when 
fitting models to very few response data.
</p>
<p>For data with many  zeroes clustered together in the covariate space it is quite easy to set up 
GAMs which suffer from identifiability problems, particularly when using Poisson or binomial
families. The problem is that with e.g. log or logit links, mean value zero corresponds to
an infinite range on the linear predictor scale.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>
<p>Front end design inspired by the S function of the same name based on the work
of Hastie and Tibshirani (1990). Underlying methods owe much to the work of
Wahba (e.g. 1990) and Gu (e.g. 2002).
</p>


<h3>References</h3>

<p>Key References on this implementation:
</p>
<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models (with discussion).
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>
<p>Wood, S.N. (2011) Fast stable restricted maximum likelihood 
and marginal likelihood estimation of semiparametric generalized linear 
models. Journal of the Royal Statistical Society (B) 73(1):3-36
<a href="https://doi.org/10.1111/j.1467-9868.2010.00749.x">doi:10.1111/j.1467-9868.2010.00749.x</a>
</p>
<p>Wood, S.N. (2004) Stable and efficient multiple smoothing parameter estimation for
generalized additive models. J. Amer. Statist. Ass. 99:673-686. [Default
method for additive case by GCV (but no longer for generalized)]
</p>
<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
<a href="https://doi.org/10.1111/1467-9868.00374">doi:10.1111/1467-9868.00374</a>
</p>
<p>Wood, S.N. (2006a) Low rank scale invariant tensor product smooths for
generalized additive mixed models. Biometrics 62(4):1025-1036
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press. <a href="https://doi.org/10.1201/9781315370279">doi:10.1201/9781315370279</a>
</p>
<p>Wood, S.N. and M. Fasiolo (2017) A generalized Fellner-Schall method for smoothing
parameter optimization with application to Tweedie location, scale and shape models.
Biometrics 73 (4), 1071-1081 <a href="https://doi.org/10.1111/biom.12666">doi:10.1111/biom.12666</a>
</p>
<p>Wood S.N., F. Scheipl and J.J. Faraway (2013) Straightforward intermediate rank tensor product smoothing
in mixed models. Statistics and Computing 23: 341-360. <a href="https://doi.org/10.1007/s11222-012-9314-z">doi:10.1007/s11222-012-9314-z</a>
</p>
<p>Marra, G and S.N. Wood (2012) Coverage Properties of Confidence Intervals for Generalized Additive
Model Components. Scandinavian Journal of Statistics, 39(1), 53-74. <a href="https://doi.org/10.1111/j.1467-9469.2011.00760.x">doi:10.1111/j.1467-9469.2011.00760.x</a>
</p>
<p>Key Reference on GAMs and related models:
</p>
<p>Wood, S. N. (2020) Inference and computation with generalized
additive models and their extensions. Test 29(2): 307-339.
<a href="https://doi.org/10.1007/s11749-020-00711-5">doi:10.1007/s11749-020-00711-5</a>
</p>
<p>Hastie (1993) in Chambers and Hastie (1993) Statistical Models in S. Chapman
and Hall.
</p>
<p>Hastie and Tibshirani (1990) Generalized Additive Models. Chapman and Hall.
</p>
<p>Wahba (1990) Spline Models of Observational Data. SIAM 
</p>
<p>Wood, S.N. (2000)  Modelling and Smoothing Parameter Estimation
with Multiple Quadratic Penalties. J.R.Statist.Soc.B 62(2):413-428 [The original
mgcv paper, but no longer the default methods.]
</p>
<p>Background References:
</p>
<p>Green and Silverman (1994) Nonparametric Regression and Generalized  Linear Models. Chapman and Hall.
</p>
<p>Gu and Wahba (1991) Minimizing GCV/GML scores with multiple smoothing parameters via
the Newton method. SIAM J. Sci. Statist. Comput. 12:383-398
</p>
<p>Gu (2002) Smoothing Spline ANOVA Models, Springer.
</p>
<p>McCullagh and Nelder (1989) Generalized Linear Models 2nd ed. Chapman &amp; Hall.
</p>
<p>O'Sullivan, Yandall and Raynor (1986) Automatic smoothing of regression
functions in generalized linear models.
J. Am. Statist.Ass. 81:96-103 
</p>
<p>Wood (2001) mgcv:GAMs and Generalized Ridge Regression for R. R News 1(2):20-25
</p>
<p>Wood and Augustin (2002) GAMs with integrated model selection using penalized regression splines and applications 
to environmental modelling. Ecological Modelling 157:157-177
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mgcv-package">mgcv-package</a></code>, <code><a href="#topic+gamObject">gamObject</a></code>, <code><a href="#topic+gam.models">gam.models</a></code>, <code><a href="#topic+smooth.terms">smooth.terms</a></code>,
<code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code>, <code><a href="#topic+s">s</a></code>,
<code><a href="#topic+te">te</a></code> <code><a href="#topic+predict.gam">predict.gam</a></code>,
<code><a href="#topic+plot.gam">plot.gam</a></code>, <code><a href="#topic+summary.gam">summary.gam</a></code>, <code><a href="#topic+gam.side">gam.side</a></code>,
<code><a href="#topic+gam.selection">gam.selection</a></code>, <code><a href="#topic+gam.control">gam.control</a></code>
<code><a href="#topic+gam.check">gam.check</a></code>, <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code> <code><a href="#topic+negbin">negbin</a></code>, <code><a href="#topic+magic">magic</a></code>,<code><a href="#topic+vis.gam">vis.gam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see also examples in ?gam.models (e.g. 'by' variables, 
## random effects and tricks for large binary datasets)

library(mgcv)
set.seed(2) ## simulate some data... 
dat &lt;- gamSim(1,n=400,dist="normal",scale=2)
b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
summary(b)
plot(b,pages=1,residuals=TRUE)  ## show partial residuals
plot(b,pages=1,seWithMean=TRUE) ## `with intercept' CIs
## run some basic model checks, including checking
## smoothing basis dimensions...
gam.check(b)

## same fit in two parts .....
G &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),fit=FALSE,data=dat)
b &lt;- gam(G=G)
print(b)

## 2 part fit enabling manipulation of smoothing parameters...
G &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),fit=FALSE,data=dat,sp=b$sp)
G$lsp0 &lt;- log(b$sp*10) ## provide log of required sp vec
gam(G=G) ## it's smoother

## change the smoothness selection method to REML
b0 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML")
## use alternative plotting scheme, and way intervals include
## smoothing parameter uncertainty...
plot(b0,pages=1,scheme=1,unconditional=TRUE) 

## Would a smooth interaction of x0 and x1 be better?
## Use tensor product smooth of x0 and x1, basis 
## dimension 49 (see ?te for details, also ?t2).
bt &lt;- gam(y~te(x0,x1,k=7)+s(x2)+s(x3),data=dat,
          method="REML")
plot(bt,pages=1) 
plot(bt,pages=1,scheme=2) ## alternative visualization
AIC(b0,bt) ## interaction worse than additive

## Alternative: test for interaction with a smooth ANOVA 
## decomposition (this time between x2 and x1)
bt &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3)+ti(x1,x2,k=6),
            data=dat,method="REML")
summary(bt)

## If it is believed that x0 and x1 are naturally on 
## the same scale, and should be treated isotropically 
## then could try...
bs &lt;- gam(y~s(x0,x1,k=40)+s(x2)+s(x3),data=dat,
          method="REML")
plot(bs,pages=1)
AIC(b0,bt,bs) ## additive still better. 

## Now do automatic terms selection as well
b1 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,
       method="REML",select=TRUE)
plot(b1,pages=1)


## set the smoothing parameter for the first term, estimate rest ...
bp &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),sp=c(0.01,-1,-1,-1),data=dat)
plot(bp,pages=1,scheme=1)
## alternatively...
bp &lt;- gam(y~s(x0,sp=.01)+s(x1)+s(x2)+s(x3),data=dat)


# set lower bounds on smoothing parameters ....
bp&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),
        min.sp=c(0.001,0.01,0,10),data=dat) 
print(b);print(bp)

# same with REML
bp&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),
        min.sp=c(0.1,0.1,0,10),data=dat,method="REML") 
print(b0);print(bp)


## now a GAM with 3df regression spline term &amp; 2 penalized terms

b0 &lt;- gam(y~s(x0,k=4,fx=TRUE,bs="tp")+s(x1,k=12)+s(x2,k=15),data=dat)
plot(b0,pages=1)


## now simulate poisson data...
set.seed(6)
dat &lt;- gamSim(1,n=2000,dist="poisson",scale=.1)

## use "cr" basis to save time, with 2000 data...
b2&lt;-gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+
        s(x3,bs="cr"),family=poisson,data=dat,method="REML")
plot(b2,pages=1)

## drop x3, but initialize sp's from previous fit, to 
## save more time...

b2a&lt;-gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr"),
         family=poisson,data=dat,method="REML",
         in.out=list(sp=b2$sp[1:3],scale=1))
par(mfrow=c(2,2))
plot(b2a)

par(mfrow=c(1,1))
## similar example using GACV...

dat &lt;- gamSim(1,n=400,dist="poisson",scale=.25)

b4&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,
        data=dat,method="GACV.Cp",scale=-1)
plot(b4,pages=1)

## repeat using REML as in Wood 2011...

b5&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,
        data=dat,method="REML")
plot(b5,pages=1)

 
## a binary example (see ?gam.models for large dataset version)...

dat &lt;- gamSim(1,n=400,dist="binary",scale=.33)

lr.fit &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=binomial,
              data=dat,method="REML")

## plot model components with truth overlaid in red
op &lt;- par(mfrow=c(2,2))
fn &lt;- c("f0","f1","f2","f3");xn &lt;- c("x0","x1","x2","x3")
for (k in 1:4) {
  plot(lr.fit,residuals=TRUE,select=k)
  ff &lt;- dat[[fn[k]]];xx &lt;- dat[[xn[k]]]
  ind &lt;- sort.int(xx,index.return=TRUE)$ix
  lines(xx[ind],(ff-mean(ff))[ind]*.33,col=2)
}
par(op)
anova(lr.fit)
lr.fit1 &lt;- gam(y~s(x0)+s(x1)+s(x2),family=binomial,
               data=dat,method="REML")
lr.fit2 &lt;- gam(y~s(x1)+s(x2),family=binomial,
               data=dat,method="REML")
AIC(lr.fit,lr.fit1,lr.fit2)

## For a Gamma example, see ?summary.gam...

## For inverse Gaussian, see ?rig

## now 2D smoothing...

eg &lt;- gamSim(2,n=500,scale=.1)
attach(eg)

op &lt;- par(mfrow=c(2,2),mar=c(4,4,1,1))

contour(truth$x,truth$z,truth$f) ## contour truth
b4 &lt;- gam(y~s(x,z),data=data) ## fit model
fit1 &lt;- matrix(predict.gam(b4,pr,se=FALSE),40,40)
contour(truth$x,truth$z,fit1)   ## contour fit
persp(truth$x,truth$z,truth$f)    ## persp truth
vis.gam(b4)                     ## persp fit
detach(eg)
par(op)

##################################################
## largish dataset example with user defined knots
##################################################

par(mfrow=c(2,2))
n &lt;- 5000
eg &lt;- gamSim(2,n=n,scale=.5)
attach(eg)

ind&lt;-sample(1:n,200,replace=FALSE)
b5&lt;-gam(y~s(x,z,k=40),data=data,
        knots=list(x=data$x[ind],z=data$z[ind]))
## various visualizations
vis.gam(b5,theta=30,phi=30)
plot(b5)
plot(b5,scheme=1,theta=50,phi=20)
plot(b5,scheme=2)

par(mfrow=c(1,1))
## and a pure "knot based" spline of the same data
b6&lt;-gam(y~s(x,z,k=64),data=data,knots=list(x= rep((1:8-0.5)/8,8),
        z=rep((1:8-0.5)/8,rep(8,8))))
vis.gam(b6,color="heat",theta=30,phi=30)

## varying the default large dataset behaviour via `xt'
b7 &lt;- gam(y~s(x,z,k=40,xt=list(max.knots=500,seed=2)),data=data)
vis.gam(b7,theta=30,phi=30)
detach(eg)

</code></pre>

<hr>
<h2 id='gam.check'>Some diagnostics for a fitted gam model</h2><span id='topic+gam.check'></span>

<h3>Description</h3>

<p> Takes a fitted <code>gam</code> object produced by <code>gam()</code> and produces some diagnostic information
about the fitting procedure and results. The default is to produce 4 residual
plots, some information about the convergence of the smoothness selection optimization, and to run 
diagnostic tests of whether the basis dimension choises are adequate. Care should be taken in interpreting the results when applied to <code>gam</code> objects returned by <code><a href="#topic+gamm">gamm</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam.check(b, old.style=FALSE,
          type=c("deviance","pearson","response"),
          k.sample=5000,k.rep=200,
          rep=0, level=.9, rl.col=2, rep.col="gray80", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gam.check_+3A_b">b</code></td>
<td>
<p>a fitted <code>gam</code> object as produced by <code><a href="#topic+gam">gam</a>()</code>.</p>
</td></tr>
<tr><td><code id="gam.check_+3A_old.style">old.style</code></td>
<td>
<p>If you want old fashioned plots, exactly as in Wood, 2006, set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="gam.check_+3A_type">type</code></td>
<td>
<p>type of residuals, see <code><a href="#topic+residuals.gam">residuals.gam</a></code>, used in
all plots.</p>
</td></tr>
<tr><td><code id="gam.check_+3A_k.sample">k.sample</code></td>
<td>
<p>Above this k testing uses a random sub-sample of data.</p>
</td></tr>
<tr><td><code id="gam.check_+3A_k.rep">k.rep</code></td>
<td>
<p>how many re-shuffles to do to get p-value for k testing.</p>
</td></tr>
<tr><td><code id="gam.check_+3A_rep">rep</code>, <code id="gam.check_+3A_level">level</code>, <code id="gam.check_+3A_rl.col">rl.col</code>, <code id="gam.check_+3A_rep.col">rep.col</code></td>
<td>

<p>arguments passed to <code><a href="#topic+qq.gam">qq.gam</a>()</code> when <code>old.style</code> is
false, see there.</p>
</td></tr>
<tr><td><code id="gam.check_+3A_...">...</code></td>
<td>
<p>extra graphics parameters to pass to plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Checking a fitted <code>gam</code> is like checking a fitted <code>glm</code>, with two main differences. Firstly, 
the basis dimensions used for smooth terms need to be checked, to ensure that they are not so small that they force 
oversmoothing: the defaults are arbitrary. <code><a href="#topic+choose.k">choose.k</a></code> provides more detail, but the diagnostic tests described below and reported by this function may also help. Secondly, fitting may not always be as robust to violation of the distributional assumptions as would be the case for a regular GLM, so slightly more care may be needed here. In particular, the thoery of quasi-likelihood implies that if the mean variance relationship is OK for a GLM, then other departures from the assumed distribution are not problematic: GAMs can sometimes be more sensitive. For example, un-modelled overdispersion will typically lead to overfit, as the smoothness selection criterion tries to reduce the scale parameter to the one specified. Similarly, it is not clear how sensitive REML and ML smoothness selection will be to deviations from the assumed response dsistribution. For these reasons this routine uses an enhanced residual QQ plot.
</p>
<p>This function plots 4 standard diagnostic plots,  some smoothing parameter estimation
convergence information and the results of tests which may indicate if the smoothing basis dimension
for a term is too low.  
</p>
<p>Usually the 4 plots are various residual plots. For the default optimization methods the convergence information is summarized in a readable way, but for other optimization methods, whatever is returned by way of
convergence diagnostics is simply printed. 
</p>
<p>The test of whether the basis dimension for a smooth is adequate (Wood, 2017, section 5.9) is based on computing an estimate of the residual variance 
based on differencing residuals that are near neighbours according to the (numeric) covariates of the smooth. This estimate divided by the residual variance is the <code>k-index</code> reported. The further below 1 this is, the more likely it is that there is missed pattern left in the residuals. The <code>p-value</code> is computed by simulation: the residuals are randomly re-shuffled <code>k.rep</code> times to obtain the null distribution of the differencing variance estimator, if there is no pattern in the residuals. For models fitted to more than <code>k.sample</code> data, the tests are based of <code>k.sample</code> randomly sampled data. Low p-values may indicate that the basis dimension, <code>k</code>, has been set too low, especially if the reported <code>edf</code> is close to k', the maximum possible EDF for the term. Note the disconcerting fact that if the test statistic itself is based on random resampling and the null is true, then the associated p-values will of course vary widely from one replicate to the next. Currently smooths of factor variables are not supported and will give an <code>NA</code> p-value.
</p>
<p>Doubling a suspect <code>k</code> and re-fitting is sensible: if the reported <code>edf</code> increases substantially then you may have been missing something in the first fit. Of course p-values can be low for  reasons other than a too low <code>k</code>. See <code><a href="#topic+choose.k">choose.k</a></code> for fuller discussion.
</p>
<p>The QQ plot produced is usually created by a call to <code><a href="#topic+qq.gam">qq.gam</a></code>, and plots deviance residuals 
against approximate theoretical quantilies of the deviance residual distribution, according to the fitted model. 
If this looks odd then investigate further using <code><a href="#topic+qq.gam">qq.gam</a></code>. Note that residuals for models fitted to binary data contain very little 
information useful for model checking (it is necessary to find some way of aggregating them first), so the QQ plot is unlikely 
to be useful in this case.
</p>
<p>Take care when interpreting results from applying this function to a model fitted using <code><a href="#topic+gamm">gamm</a></code>. In this case the returned <code>gam</code> object is based on the working model used for estimation, and will treat all the random effects as part of the error. This means that the residuals extracted from the <code>gam</code> object are not standardized for the family used or for the random effects or correlation structure. Usually it is necessary to produce your own residual checks based on consideration of the model structure you have used.  
</p>


<h3>Value</h3>

<p>A vector of reference quantiles for the residual distribution, if these can be computed.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>N.H. Augustin, E-A Sauleaub, S.N. Wood (2012) On quantile quantile plots for generalized linear models.
Computational Statistics &amp; Data Analysis. 56(8), 2404-3409.
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+choose.k">choose.k</a></code>,  <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+magic">magic</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(0)
dat &lt;- gamSim(1,n=200)
b&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
plot(b,pages=1)
gam.check(b,pch=19,cex=.3)
</code></pre>

<hr>
<h2 id='gam.control'>Setting GAM fitting defaults</h2><span id='topic+gam.control'></span>

<h3>Description</h3>

<p> This is an internal function of package <code>mgcv</code> which allows 
control of the numerical options for fitting a GAM. 
Typically users will want to modify the defaults if model fitting fails to
converge, or if the warnings are generated which suggest a 
loss of numerical stability during fitting.  To change the default
choise of fitting method, see <code><a href="#topic+gam">gam</a></code> arguments <code>method</code> and <code>optimizer</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam.control(nthreads=1,ncv.threads=1,irls.reg=0.0,epsilon = 1e-07,
            maxit = 200,mgcv.tol=1e-7,mgcv.half=15, trace = FALSE,
            rank.tol=.Machine$double.eps^0.5,nlm=list(),
	    optim=list(),newton=list(),
	    idLinksBases=TRUE,scalePenalty=TRUE,efs.lspmax=15,
	    efs.tol=.1,keepData=FALSE,scale.est="fletcher",
	    edge.correct=FALSE) 
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="gam.control_+3A_nthreads">nthreads</code></td>
<td>
<p>Some parts of some smoothing parameter selection methods (e.g. REML) can use some
parallelization in the C code if your R installation supports openMP, and <code>nthreads</code> is set to 
more than 1. Note that it is usually better to use the number of physical cores here, rather than the 
number of hyper-threading cores.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_ncv.threads">ncv.threads</code></td>
<td>
<p>The computations for neighbourhood cross-validation (NCV) typically scale better than the rest of the GAM computations and are worth parallelizing. <code>ncv.threads</code> allows you to set the number of theads to use separately. 
</p>
</td></tr>
<tr><td><code id="gam.control_+3A_irls.reg">irls.reg</code></td>
<td>
<p>For most models this should be 0. The iteratively re-weighted least squares method
by which GAMs are fitted  can fail to converge in some circumstances. For example, data with many zeroes can cause 
problems in a model with a log link, because a mean of zero corresponds to an infinite range of linear predictor 
values. Such convergence problems are caused by a fundamental lack of identifiability, but do not show up as 
lack of identifiability in the penalized linear model problems that have to be solved at each stage of iteration.
In such circumstances it is possible to apply a ridge regression penalty to the model to impose identifiability, and 
<code>irls.reg</code> is the size of the penalty.
</p>
</td></tr>
<tr><td><code id="gam.control_+3A_epsilon">epsilon</code></td>
<td>
<p>This is used for judging conversion of the GLM IRLS loop in
<code><a href="#topic+gam.fit">gam.fit</a></code> or <code><a href="#topic+gam.fit3">gam.fit3</a></code>.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of IRLS iterations to perform.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_mgcv.tol">mgcv.tol</code></td>
<td>
<p>The convergence tolerance parameter to use in GCV/UBRE optimization.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_mgcv.half">mgcv.half</code></td>
<td>
<p>If a step of  the GCV/UBRE optimization method leads 
to a worse GCV/UBRE score, then the step length is halved. This is
the number of halvings to try before giving up.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_trace">trace</code></td>
<td>
<p>Set this to <code>TRUE</code> to turn on diagnostic output.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_rank.tol">rank.tol</code></td>
<td>
<p>The tolerance used to estimate the rank of the fitting
problem.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_nlm">nlm</code></td>
<td>
<p>list of control parameters to pass to <code><a href="stats.html#topic+nlm">nlm</a></code> if this is
used for outer estimation of smoothing parameters (not default). See details.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_optim">optim</code></td>
<td>
<p>list of control parameters to pass to <code><a href="stats.html#topic+optim">optim</a></code> if this
is used for outer estimation of smoothing parameters (not default). See details.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_newton">newton</code></td>
<td>
<p>list of control parameters to pass to default Newton optimizer
used for outer estimation of log smoothing parameters. See details.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_idlinksbases">idLinksBases</code></td>
<td>
<p>If smooth terms have their smoothing parameters linked via 
the <code>id</code> mechanism (see <code><a href="#topic+s">s</a></code>), should they also have the same 
bases. Set this to <code>FALSE</code> only if you are sure you know what you are doing 
(you should almost surely set <code>scalePenalty</code> to <code>FALSE</code> as well in this 
case).</p>
</td></tr>
<tr><td><code id="gam.control_+3A_scalepenalty">scalePenalty</code></td>
<td>
<p><code><a href="#topic+gamm">gamm</a></code> is somewhat sensitive to the absolute scaling 
of the penalty matrices of a smooth relative to its model matrix. This option rescales 
the penalty matrices to accomodate this problem. Probably should be set to <code>FALSE</code> 
if you are linking smoothing parameters but have set <code>idLinkBases</code> to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_efs.lspmax">efs.lspmax</code></td>
<td>
<p>maximum log smoothing parameters to allow under extended Fellner Schall
smoothing parameter optimization.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_efs.tol">efs.tol</code></td>
<td>
<p>change in REML to count as negligible when testing for EFS convergence. If the
step is small and the last 3 steps led to a REML change smaller than this, then stop.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_keepdata">keepData</code></td>
<td>
<p>Should a copy of the original <code>data</code> argument be kept in the <code>gam</code> 
object? Strict compatibility with class <code>glm</code> would keep it, but it wastes space to
do so. </p>
</td></tr>
<tr><td><code id="gam.control_+3A_scale.est">scale.est</code></td>
<td>
<p>How to estimate the scale parameter for exponential family models estimated
by outer iteration. See <code><a href="#topic+gam.scale">gam.scale</a></code>.</p>
</td></tr>
<tr><td><code id="gam.control_+3A_edge.correct">edge.correct</code></td>
<td>
<p>With RE/ML smoothing parameter selection in <code>gam</code> using the default Newton RE/ML optimizer, it is possible to improve inference at the
&lsquo;completely smooth&rsquo; edge of the smoothing parameter space, by decreasing
smoothing parameters until there is a small increase in the negative RE/ML (e.g. 0.02). Set to <code>TRUE</code> or to a number representing the target increase to use. Only changes the corrected smoothing parameter matrix, <code>Vc</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>Outer iteration using <code>newton</code> is controlled by the list <code>newton</code>
with the following elements: <code>conv.tol</code> (default
1e-6) is the relative convergence tolerance; <code>maxNstep</code> is the maximum
length allowed for an element of the Newton search direction (default 5);
<code>maxSstep</code> is the maximum length allowed for an element of the steepest
descent direction (only used if Newton fails - default 2); <code>maxHalf</code> is
the maximum number of step halvings to permit before giving up (default 30).
</p>
<p>If outer iteration using <code><a href="stats.html#topic+nlm">nlm</a></code> is used for fitting, then the control list
<code>nlm</code> stores control arguments for calls to routine
<code><a href="stats.html#topic+nlm">nlm</a></code>. The list has the following named elements: (i) <code>ndigit</code> is
the number of significant digits in the GCV/UBRE score - by default this is
worked out from <code>epsilon</code>; (ii) <code>gradtol</code> is the tolerance used to
judge convergence of the gradient of the GCV/UBRE score to zero - by default
set to <code>10*epsilon</code>; (iii) <code>stepmax</code> is the maximum allowable log
smoothing parameter step - defaults to 2; (iv) <code>steptol</code> is the minimum
allowable step length - defaults to 1e-4; (v) <code>iterlim</code> is the maximum
number of optimization steps allowed - defaults to 200; (vi)
<code>check.analyticals</code> indicates whether the built in exact derivative
calculations should be checked numerically - defaults to <code>FALSE</code>. Any of
these which are not supplied and named in the list are set to their default
values.
</p>
<p>Outer iteration using <code><a href="stats.html#topic+optim">optim</a></code> is controlled using list
<code>optim</code>, which currently has one element: <code>factr</code> which takes
default value 1e7.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2011) Fast stable restricted maximum likelihood 
and marginal likelihood estimation of semiparametric generalized linear 
models. Journal of the Royal Statistical Society (B) 73(1):3-36
</p>
<p>Wood, S.N. (2004) Stable and efficient multiple smoothing parameter estimation for
generalized additive models. J. Amer. Statist. Ass.99:673-686.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+gam.fit">gam.fit</a></code>, <code><a href="stats.html#topic+glm.control">glm.control</a></code> </p>

<hr>
<h2 id='gam.convergence'>GAM convergence and performance issues</h2><span id='topic+gam.convergence'></span><span id='topic+gam.performance'></span>

<h3>Description</h3>

<p> When fitting GAMs there is a tradeoff between speed of 
fitting and probability of fit convergence. The fitting methods used 
by <code><a href="#topic+gam">gam</a></code> opt for certainty of convergence over speed of
fit. <code><a href="#topic+bam">bam</a></code> opts for speed.
</p>
<p><code><a href="#topic+gam">gam</a></code> uses a nested iteration method (see <code><a href="#topic+gam.outer">gam.outer</a></code>), in
which each trial set of smoothing parameters proposed by an outer Newton algorithm
require an inner Newton algorithm (penalized iteratively re-weighted least squares, PIRLS)
to find the corresponding best fit model coefficients. Implicit differentiation is used to
find the derivatives of the coefficients with respect to log smoothing parameters, so that the
derivatives of the smoothness selection criterion can be obtained, as required by the outer iteration.
This approach is less expensive than it at first appears, since excellent starting values for the inner
iteration are available as soon as the smoothing parameters start to converge. See Wood (2011) and Wood, Pya and Saefken (2016).  
</p>
<p><code><a href="#topic+bam">bam</a></code> uses an alternative approach similar to &lsquo;performance iteration&rsquo; or &lsquo;PQL&rsquo;. A single PIRLS iteration is run to find the model coefficients. At each step this requires the estimation of a working penalized linear model. Smoothing parameter selection is applied directly to this working model at each step (as if it were a Gaussian additive model). This approach is more straightforward to code and in principle less costly than the nested approach. However it is not guaranteed to converge, since the smoothness selection criterion is changing at each iteration. It is sometimes possible for the algorithm to cycle around a small set of smoothing parameter, coefficient combinations without ever converging. <code><a href="#topic+bam">bam</a></code> includes some checks to limit this behaviour, and the further checks in the algorithm used by <code>bam(...,discrete=TRUE)</code> actually guarantee convergence in some cases, but in general guarantees are not possible. See Wood, Goude and Shaw (2015) and  Wood et al. (2017). 
</p>
<p><code><a href="#topic+gam">gam</a></code> when used with &lsquo;general&rsquo; families (such as <code><a href="#topic+multinom">multinom</a></code> or <code>cox.ph</code>) can also use a potentially faster scheme based on the extended Fellner-Schall method (Wood and Fasiolo, 2017). This also operates with a single iteration and is not guaranteed to converge, theoretically. 
</p>
<p>There are three things that you can try to speed up GAM fitting. (i) if you have large 
numbers of smoothing parameters in the generalized case, then try the <code>"bfgs"</code> method 
option in <code><a href="#topic+gam">gam</a></code> argument <code>optimizer</code>: this can be faster than the default. (ii) Try using
<code><a href="#topic+bam">bam</a></code>
(iii) For large datasets it may be worth changing
the smoothing basis to use <code>bs="cr"</code> (see <code><a href="#topic+s">s</a></code> for details)
for 1-d smooths, and to use <code><a href="#topic+te">te</a></code> smooths in place of
<code><a href="#topic+s">s</a></code> smooths for smooths of more than one variable. This is because
the default thin plate regression spline basis <code>"tp"</code> is costly to set up
for large datasets.
</p>
<p>If you have convergence problems, it's worth noting that a GAM is just a (penalized)
GLM and the IRLS scheme used to estimate GLMs is not guaranteed to
converge. Hence non convergence of a GAM may relate to a lack of stability in
the basic IRLS scheme. Therefore it is worth trying to establish whether the IRLS iterations
are capable of converging. To do this fit the problematic GAM with all smooth
terms specified with <code>fx=TRUE</code> so that the smoothing parameters are all
fixed at zero. If this &lsquo;largest&rsquo; model can converge then, then the maintainer 
would quite like to know about your problem! If it doesn't converge, then its
likely that your model is just too flexible for the IRLS process itself. Having tried
increasing <code>maxit</code> in <code>gam.control</code>, there are several other
possibilities for stabilizing the iteration. It is possible to try (i) setting lower bounds on the
smoothing parameters using the <code>min.sp</code> argument of <code>gam</code>: this may
or may not change the model being fitted; (ii)
reducing the flexibility of the model by reducing the basis dimensions
<code>k</code> in the specification of <code>s</code> and <code>te</code> model terms: this
obviously changes the model being fitted somewhat.
</p>
<p>Usually, a major contributer to fitting difficulties is that the
model is a very poor description of the data.  
</p>
<p>Please report convergence problems, especially if you there is no obvious pathology in the data/model that
suggests convergence should fail. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Key References on this implementation:
</p>
<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models (with discussion).
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>
<p>Wood, S.N. (2011) Fast stable restricted maximum likelihood 
and marginal likelihood estimation of semiparametric generalized linear 
models. Journal of the Royal Statistical Society (B) 73(1):3-36
</p>
<p>Wood, S.N., Goude, Y. &amp; Shaw S. (2015) Generalized additive models for large datasets. Journal of the Royal Statistical Society, Series C 64(1): 139-155.
</p>
<p>Wood, S.N., Li, Z., Shaddick, G. &amp; Augustin N.H. (2017) Generalized additive models for gigadata: modelling the UK black smoke network daily data. Journal of the American Statistical Association.
</p>
<p>Wood, S.N. and M. Fasiolo (2017) A generalized Fellner-Schall method for smoothing parameter optimization with application to Tweedie location, scale and shape models, Biometrics.
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>

<hr>
<h2 id='gam.fit'>GAM P-IRLS estimation with GCV/UBRE smoothness estimation</h2><span id='topic+gam.fit'></span>

<h3>Description</h3>

<p> This is an internal function of package <code>mgcv</code>. It is a modification
of the function <code>glm.fit</code>, designed to be called from <code>gam</code> when perfomance iteration is selected (not the default). The major
modification is that rather than solving a weighted least squares problem at each IRLS step, 
a weighted, penalized least squares problem
is solved at each IRLS step with smoothing parameters associated with each penalty chosen by GCV or UBRE,
using routine <code><a href="#topic+magic">magic</a></code>. 
For further information on usage see code for <code>gam</code>. Some regularization of the 
IRLS weights is also permitted as a way of addressing identifiability related problems (see 
<code><a href="#topic+gam.control">gam.control</a></code>). Negative binomial parameter estimation is
supported.
</p>
<p>The basic idea of estimating smoothing parameters at each step of the P-IRLS
is due to Gu (1992), and is termed &lsquo;performance iteration&rsquo; or 'performance
oriented iteration'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> 

gam.fit(G, start = NULL, etastart = NULL, 
        mustart = NULL, family = gaussian(), 
        control = gam.control(),gamma=1,
        fixedSteps=(control$maxit+1),...) 

</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="gam.fit_+3A_g">G</code></td>
<td>
<p>An object of the type returned by <code><a href="#topic+gam">gam</a></code> when <code>fit=FALSE</code>.</p>
</td></tr>
<tr><td><code id="gam.fit_+3A_start">start</code></td>
<td>
<p>Initial values for the model coefficients.</p>
</td></tr>
<tr><td><code id="gam.fit_+3A_etastart">etastart</code></td>
<td>
<p>Initial values for the linear predictor.</p>
</td></tr>
<tr><td><code id="gam.fit_+3A_mustart">mustart</code></td>
<td>
<p>Initial values for the expected response.</p>
</td></tr>
<tr><td><code id="gam.fit_+3A_family">family</code></td>
<td>
<p>The family object, specifying the distribution and link to use.</p>
</td></tr>
<tr><td><code id="gam.fit_+3A_control">control</code></td>
<td>
<p>Control option list as returned by <code><a href="#topic+gam.control">gam.control</a></code>.</p>
</td></tr>
<tr><td><code id="gam.fit_+3A_gamma">gamma</code></td>
<td>
<p>Parameter which can be increased to up the cost of each effective degree of freedom in the
GCV or AIC/UBRE objective.</p>
</td></tr>
<tr><td><code id="gam.fit_+3A_fixedsteps">fixedSteps</code></td>
<td>
<p>How many steps to take: useful when only using this routine to get rough starting values for other methods.</p>
</td></tr>
<tr><td><code id="gam.fit_+3A_...">...</code></td>
<td>
<p>Other arguments: ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of fit information.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Gu (1992) Cross-validating non-Gaussian data. J. Comput. Graph. Statist. 1:169-179
</p>
<p>Gu and Wahba (1991) Minimizing GCV/GML scores with multiple smoothing parameters via
the Newton method. SIAM J. Sci. Statist. Comput. 12:383-398
</p>
<p>Wood, S.N. (2000)  Modelling and Smoothing Parameter Estimation
with Multiple  Quadratic Penalties. J.R.Statist.Soc.B 62(2):413-428
</p>
<p>Wood, S.N. (2004) Stable and efficient multiple smoothing parameter estimation for
generalized additive models. J. Amer. Statist. Ass. 99:637-686
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+gam.fit3">gam.fit3</a></code>,  <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+magic">magic</a></code></p>

<hr>
<h2 id='gam.fit3'>P-IRLS GAM estimation with GCV, UBRE/AIC or RE/ML derivative calculation</h2><span id='topic+gam.fit3'></span>

<h3>Description</h3>

<p>Estimation of GAM smoothing parameters is most stable if
optimization of the UBRE/AIC, GCV, GACV, REML or ML score is outer to the penalized iteratively
re-weighted least squares scheme used to estimate the model given smoothing 
parameters.
</p>
<p>This routine estimates a GAM (any quadratically penalized GLM) given log 
smoothing paramaters, and evaluates derivatives of the smoothness selection scores 
of the model with respect to the
log smoothing parameters. Calculation of exact derivatives is generally faster
than approximating them by finite differencing, as well as generally improving
the reliability of GCV/UBRE/AIC/REML score minimization.
</p>
<p>The approach is to run the P-IRLS
to convergence, and only then to iterate for first and second derivatives. 
</p>
<p>Not normally called directly, but rather service routines for <code><a href="#topic+gam">gam</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
gam.fit3(x, y, sp, Eb ,UrS=list(), 
         weights = rep(1, nobs), start = NULL, etastart = NULL, 
         mustart = NULL, offset = rep(0, nobs), U1 = diag(ncol(x)), 
         Mp = -1, family = gaussian(), control = gam.control(), 
         intercept = TRUE,deriv=2,gamma=1,scale=1,
         printWarn=TRUE,scoreType="REML",null.coef=rep(0,ncol(x)),
         pearson.extra=0,dev.extra=0,n.true=-1,Sl=NULL,nei=NULL,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="gam.fit3_+3A_x">x</code></td>
<td>
<p>The model matrix for the GAM (or any penalized GLM).</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_y">y</code></td>
<td>
<p>The response variable.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_sp">sp</code></td>
<td>
<p>The log smoothing parameters.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_eb">Eb</code></td>
<td>
<p>A balanced version of the total penalty matrix: usd for numerical rank determination.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_urs">UrS</code></td>
<td>
<p>List of square root penalties premultiplied by transpose of orthogonal
basis for the total penalty.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_weights">weights</code></td>
<td>
<p>prior weights for fitting.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_start">start</code></td>
<td>
<p>optional starting parameter guesses.</p>
</td></tr> 
<tr><td><code id="gam.fit3_+3A_etastart">etastart</code></td>
<td>
<p>optional starting values for the linear predictor.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_mustart">mustart</code></td>
<td>
<p>optional starting values for the mean.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_offset">offset</code></td>
<td>
<p>the model offset</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_u1">U1</code></td>
<td>
<p>An orthogonal basis for the range space of the penalty &mdash; required for ML smoothness estimation only.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_mp">Mp</code></td>
<td>
<p>The dimension of the total penalty null space &mdash; required for ML smoothness estimation only.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_family">family</code></td>
<td>
<p>the family - actually this routine would never be called with <code>gaussian()</code></p>
</td></tr> 
<tr><td><code id="gam.fit3_+3A_control">control</code></td>
<td>
<p>control list as returned from <code><a href="stats.html#topic+glm.control">glm.control</a></code></p>
</td></tr> 
<tr><td><code id="gam.fit3_+3A_intercept">intercept</code></td>
<td>
<p>does the model have and intercept, <code>TRUE</code> or
<code>FALSE</code></p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_deriv">deriv</code></td>
<td>
<p> Should derivatives of the GCV and UBRE/AIC scores be calculated?
0, 1 or 2,
indicating the maximum order of differentiation to apply.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_gamma">gamma</code></td>
<td>
<p>The weight given to each degree of freedom in the GCV and UBRE
scores can be varied (usually increased) using this parameter.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_scale">scale</code></td>
<td>
<p>The scale parameter - needed for the UBRE/AIC score.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_printwarn">printWarn</code></td>
<td>
<p>Set to <code>FALSE</code> to suppress some warnings. Useful in
order to ensure that some warnings are only printed if they apply to the final
fitted model, rather than an intermediate used in optimization.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_scoretype">scoreType</code></td>
<td>
<p>specifies smoothing parameter selection criterion to use.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_null.coef">null.coef</code></td>
<td>
<p>coefficients for a model which gives some sort of upper bound on deviance.
This allows immediate divergence problems to be controlled.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_pearson.extra">pearson.extra</code></td>
<td>
<p>Extra component to add to numerator of pearson statistic 
in P-REML/P-ML smoothness selection criteria.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_dev.extra">dev.extra</code></td>
<td>
<p>Extra component to add to deviance for REML/ML type smoothness selection criteria.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_n.true">n.true</code></td>
<td>
<p>Number of data to assume in smoothness selection criteria. &lt;=0 indicates that it should be the 
number of rows of <code>X</code>.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_sl">Sl</code></td>
<td>
<p>A smooth list suitable for passing to gam.fit5. </p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_nei">nei</code></td>
<td>
<p>List specifying neighbourhood structure if NCV used. See <code><a href="#topic+gam">gam</a></code>.</p>
</td></tr>
<tr><td><code id="gam.fit3_+3A_...">...</code></td>
<td>
<p>Other arguments: ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> This routine is basically <code><a href="stats.html#topic+glm.fit">glm.fit</a></code> with some
modifications to allow (i) for quadratic penalties on the log likelihood;
(ii) derivatives of the model coefficients with respect to
log smoothing parameters to be obtained by use of the implicit function theorem and 
(iii) derivatives of the GAM GCV, UBRE/AIC, REML or ML scores to be
evaluated at convergence. 
</p>
<p>In addition the routines apply step halving to any step that increases the
penalized deviance substantially.
</p>
<p>The most costly parts of the calculations are performed by calls to compiled C
code (which in turn calls LAPACK routines) in place of the compiled code that
would usually perform least squares estimation on the working model in the
IRLS iteration. 
</p>
<p>Estimation of smoothing parameters by optimizing GCV scores obtained at
convergence of the P-IRLS iteration was proposed by O'Sullivan et al. (1986),
and is here termed &lsquo;outer&rsquo; iteration. 
</p>
<p>Note that use of non-standard families with this routine requires modification
of the families as described in <code><a href="#topic+fix.family.link">fix.family.link</a></code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>
<p>The routine has been modified from <code>glm.fit</code> in R 2.0.1, written
by the R core (see <code><a href="stats.html#topic+glm.fit">glm.fit</a></code> for further credits).
</p>


<h3>References</h3>

<p>Wood, S.N. (2011) Fast stable restricted maximum likelihood 
and marginal likelihood estimation of semiparametric generalized linear 
models. Journal of the Royal Statistical Society (B) 73(1):3-36
</p>
<p>O 'Sullivan, Yandall and Raynor (1986) Automatic smoothing of regression
functions in generalized linear models. J. Amer. Statist. Assoc. 81:96-103.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam.fit">gam.fit</a></code>,  <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+magic">magic</a></code></p>

<hr>
<h2 id='gam.fit5.post.proc'>Post-processing output of gam.fit5</h2><span id='topic+gam.fit5.post.proc'></span>

<h3>Description</h3>

<p>INTERNAL function for post-processing the output of <code>gam.fit5</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam.fit5.post.proc(object, Sl, L, lsp0, S, off, gamma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gam.fit5.post.proc_+3A_object">object</code></td>
<td>
<p>output of <code>gam.fit5</code>.</p>
</td></tr>
<tr><td><code id="gam.fit5.post.proc_+3A_sl">Sl</code></td>
<td>
<p>penalty object, output of <code>Sl.setup</code>.</p>
</td></tr>
<tr><td><code id="gam.fit5.post.proc_+3A_l">L</code></td>
<td>
<p>matrix mapping the working smoothing parameters.</p>
</td></tr>
<tr><td><code id="gam.fit5.post.proc_+3A_lsp0">lsp0</code></td>
<td>
<p>log smoothing parameters.</p>
</td></tr>
<tr><td><code id="gam.fit5.post.proc_+3A_s">S</code></td>
<td>
<p>penalty matrix.</p>
</td></tr>
<tr><td><code id="gam.fit5.post.proc_+3A_off">off</code></td>
<td>
<p>vector of offsets.</p>
</td></tr>
<tr><td><code id="gam.fit5.post.proc_+3A_gamma">gamma</code></td>
<td>
<p>parameter for increasing model smoothness in fitting.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing: </p>

<ul>
<li><p><code>R</code>: unpivoted Choleski of estimated expected hessian of log-likelihood. 
</p>
</li>
<li><p><code>Vb</code>: the Bayesian covariance matrix of the model parameters. 
</p>
</li>
<li><p><code>Ve</code>: &quot;frequentist&quot; alternative to <code>Vb</code>. 
</p>
</li>
<li><p><code>Vc</code>: corrected covariance matrix. 
</p>
</li>
<li><p><code>F</code>: matrix of effective degrees of freedom (EDF). 
</p>
</li>
<li><p><code>edf</code>:  <code>diag(F)</code>. 
</p>
</li>
<li><p><code>edf2</code>:  <code>diag(2F-FF)</code>.  
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>

<hr>
<h2 id='gam.mh'>Simple posterior simulation with gam fits</h2><span id='topic+gam.mh'></span><span id='topic+posterior.simulation'></span>

<h3>Description</h3>

<p>GAM coefficients can be simulated directly from the Gaussian approximation to the posterior for the coefficients, or using a simple Metropolis Hastings sampler. See also <code><a href="#topic+ginla">ginla</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam.mh(b,ns=10000,burn=1000,t.df=40,rw.scale=.25,thin=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gam.mh_+3A_b">b</code></td>
<td>
<p>a fitted model object from <code><a href="#topic+gam">gam</a></code>. <code><a href="#topic+bam">bam</a></code> fits are not supported.</p>
</td></tr>
<tr><td><code id="gam.mh_+3A_ns">ns</code></td>
<td>
<p>the number of samples to generate.</p>
</td></tr>
<tr><td><code id="gam.mh_+3A_burn">burn</code></td>
<td>
<p>the length of any initial burn in period to discard (in addition to <code>ns</code>).</p>
</td></tr>
<tr><td><code id="gam.mh_+3A_t.df">t.df</code></td>
<td>
<p>degrees of freedom for static multivariate t proposal. Lower for heavier tailed proposals.</p>
</td></tr>
<tr><td><code id="gam.mh_+3A_rw.scale">rw.scale</code></td>
<td>
<p>Factor by which to scale posterior covariance matrix when generating random walk proposals. Negative or non finite to skip the random walk step.  </p>
</td></tr>
<tr><td><code id="gam.mh_+3A_thin">thin</code></td>
<td>
<p>retain only every <code>thin</code> samples.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>Posterior simulation is particularly useful for making inferences about non-linear functions of the model coefficients. Simulate random draws from the posterior, compute the function for each draw, and you have a draw from the posterior for the function. In many cases the Gaussian approximation to the posterior of the model coefficients is accurate, and samples generated from it can be treated as samples from the posterior for the coefficients. See example code below. This approach is computationally very efficient.
</p>
<p>In other cases the Gaussian approximation can become poor. A typical example is in a spatial model with a log or logit link when there is a large area of observations containing only zeroes. In this case the linear predictor is poorly identified and the Gaussian approximation can become useless (an example is provided below). In that case it can sometimes be useful to simulate from the posterior using a Metropolis Hastings sampler. A simple approach alternates fixed proposals, based on the Gaussian approximation to the posterior, with random walk proposals, based on a shrunken version of the approximate posterior covariane matrix. <code>gam.mh</code> implements this. The fixed proposal often promotes rapid mixing, while the random walk component ensures that the chain does not become stuck in regions for which the fixed Gaussian proposal density is much lower than the posterior density.    
</p>
<p>The function reports the acceptance rate of the two types of step. If the random walk acceptance probability is higher than a quarter then <code>rw.step</code> should probably be increased. Similarly if the acceptance rate is too low, it should be decreased. The random walk steps can be turned off altogether (see above), but it is important to check the chains for stuck sections if this is done. 
</p>


<h3>Value</h3>

<p>A list containing the retained simulated coefficients in matrix <code>bs</code> and two entries for the acceptance probabilities.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N.  (2015) Core Statistics, Cambridge
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(3);n &lt;- 400

############################################
## First example: simulated Tweedie model...
############################################

dat &lt;- gamSim(1,n=n,dist="poisson",scale=.2)
dat$y &lt;- rTweedie(exp(dat$f),p=1.3,phi=.5) ## Tweedie response
b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=tw(),
          data=dat,method="REML")

## simulate directly from Gaussian approximate posterior...
br &lt;- rmvn(1000,coef(b),vcov(b))

## Alternatively use MH sampling...
br &lt;- gam.mh(b,thin=2,ns=2000,rw.scale=.15)$bs
## If 'coda' installed, can check effective sample size
## require(coda);effectiveSize(as.mcmc(br))

## Now compare simulation results and Gaussian approximation for
## smooth term confidence intervals...
x &lt;- seq(0,1,length=100)
pd &lt;- data.frame(x0=x,x1=x,x2=x,x3=x)
X &lt;- predict(b,newdata=pd,type="lpmatrix")
par(mfrow=c(2,2))
for(i in 1:4) {
  plot(b,select=i,scale=0,scheme=1)
  ii &lt;- b$smooth[[i]]$first.para:b$smooth[[i]]$last.para
  ff &lt;- X[,ii]%*%t(br[,ii]) ## posterior curve sample
  fq &lt;- apply(ff,1,quantile,probs=c(.025,.16,.84,.975))
  lines(x,fq[1,],col=2,lty=2);lines(x,fq[4,],col=2,lty=2)
  lines(x,fq[2,],col=2);lines(x,fq[3,],col=2)
}

###############################################################
## Second example, where Gaussian approximation is a failure...
###############################################################

y &lt;- c(rep(0, 89), 1, 0, 1, 0, 0, 1, rep(0, 13), 1, 0, 0, 1, 
       rep(0, 10), 1, 0, 0, 1, 1, 0, 1, rep(0,4), 1, rep(0,3),  
       1, rep(0, 3), 1, rep(0, 10), 1, rep(0, 4), 1, 0, 1, 0, 0, 
       rep(1, 4), 0, rep(1, 5), rep(0, 4), 1, 1, rep(0, 46))
set.seed(3);x &lt;- sort(c(0:10*5,rnorm(length(y)-11)*20+100))
b &lt;- gam(y ~ s(x, k = 15),method = 'REML', family = binomial)
br &lt;- gam.mh(b,thin=2,ns=2000,rw.scale=.4)$bs
X &lt;- model.matrix(b)
par(mfrow=c(1,1))
plot(x, y, col = rgb(0,0,0,0.25), ylim = c(0,1))
ff &lt;- X%*%t(br) ## posterior curve sample
linv &lt;- b$family$linkinv
## Get intervals for the curve on the response scale...
fq &lt;- linv(apply(ff,1,quantile,probs=c(.025,.16,.5,.84,.975)))
lines(x,fq[1,],col=2,lty=2);lines(x,fq[5,],col=2,lty=2)
lines(x,fq[2,],col=2);lines(x,fq[4,],col=2)
lines(x,fq[3,],col=4)
## Compare to the Gaussian posterior approximation
fv &lt;- predict(b,se=TRUE)
lines(x,linv(fv$fit))
lines(x,linv(fv$fit-2*fv$se.fit),lty=3)
lines(x,linv(fv$fit+2*fv$se.fit),lty=3)
## ... Notice the useless 95% CI (black dotted) based on the
## Gaussian approximation!
</code></pre>

<hr>
<h2 id='gam.models'>Specifying generalized additive models</h2><span id='topic+gam.models'></span>

<h3>Description</h3>

<p> This page is intended to provide some more information on
how to specify GAMs. A GAM is a GLM in which the linear predictor depends, 
in part, on a sum of smooth functions of predictors and (possibly) linear 
functionals of smooth functions of (possibly dummy) predictors.
</p>
<p>Specifically let <code class="reqn">y_i</code> denote an independent random variable 
with mean <code class="reqn">\mu_i</code> and an exponential family distribution, or failing 
that a known mean variance relationship suitable for use of quasi-likelihood methods. 
Then the the linear predictor of a GAM has a structure something like
</p>
<p style="text-align: center;"><code class="reqn">g(\mu_i) = {\bf X}_i{\beta} + f_1(x_{1i},x_{2i}) + f_2(x_{3i}) + L_i f_3(x_4) + 
\ldots</code>
</p>

<p>where <code class="reqn">g</code> is a known smooth monotonic &lsquo;link&rsquo; function, <code class="reqn">{\bf X}_i\beta</code> 
is the parametric part of the linear predictor, the <code class="reqn">x_j</code> are predictor variables,
the <code class="reqn">f_j</code> are smooth functions and <code class="reqn">L_i</code> is some linear functional of 
<code class="reqn">f_3</code>. There may of course be multiple linear functional terms, or none.
</p>
<p>The key idea here is that the
dependence of the response on the predictors can be represented as a
parametric sub-model plus the sum of some (functionals of) smooth functions of one or
more of the predictor variables. Thus the model is quite flexible
relative to strictly parametric linear or generalized linear models,
but still has much more structure than the completely general model
that says that the response is just some smooth function of all the
covariates.
</p>
<p>Note one important point. In order for the model to be identifiable
the smooth functions usually have to be constrained to have zero mean (usually
taken over the set of covariate values). The constraint is needed if the term involving the 
smooth includes a constant function in its span. <code>gam</code> always applies such constraints 
unless there is a <code>by</code> variable present, in which case an assessment is made of whether 
the constraint is needed or not (see below).
</p>
<p>The following sections discuss specifying model structures for <code>gam</code>. 
Specification of the distribution and link function is done using the <code><a href="stats.html#topic+family">family</a></code> 
argument to <code><a href="#topic+gam">gam</a></code> and works in the same way as for <code><a href="stats.html#topic+glm">glm</a></code>. 
This page therefore concentrates on the model formula for <code>gam</code>.
</p>


<h3>Models with simple smooth terms</h3>

<p>Consider the example model.
</p>
<p style="text-align: center;"><code class="reqn">g(\mu_i) = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + f_1(x_{3i}) + 
f_2(x_{4i},x_{5i})</code>
</p>

<p>where the response variables <code class="reqn">y_i</code> has expectation <code class="reqn">\mu_i</code>
and <code class="reqn">g</code> is a link function.
</p>
<p>The <code>gam</code> formula for this would be <br />
<code>y ~ x1 + x2 + s(x3) + s(x4,x5)</code>. <br />
This would use the default basis for the smooths (a thin plate
regression spline basis for each), with automatic selection of the
effective degrees of freedom for both smooths. The dimension of the
smoothing basis is given a default value as well (the dimension of the
basis sets an upper limit on the maximum possible degrees of
freedom for the basis - the limit is typically one less than basis
dimension). Full details of how to control smooths are given in
<code><a href="#topic+s">s</a></code> and <code><a href="#topic+te">te</a></code>, and further discussion of basis
dimension choice can be found in <code><a href="#topic+choose.k">choose.k</a></code>. 
For the moment suppose that we would like to change
the basis of the first smooth to a cubic regression spline basis with
a dimension of 20, while fixing the second term at 25 degrees of
freedom. The appropriate formula would be:<br />
<code>y ~ x1 + x2 + s(x3,bs="cr",k=20) + s(x4,x5,k=26,fx=TRUE)</code>.
</p>
<p>The above assumes that <code class="reqn">x_{4}</code> and <code class="reqn">x_5</code> are naturally on 
similar scales (e.g. they might be co-ordinates), so that isotropic smoothing 
is appropriate. If this assumption is false then tensor product smoothing might be 
better (see <code><a href="#topic+te">te</a></code>). <br />
<code>y ~ x1 + x2 + s(x3) + te(x4,x5)</code><br />
would generate a tensor product smooth of <code class="reqn">x_{4}</code> and <code class="reqn">x_5</code>. 
By default this smooth would have basis dimension 25 and use cubic regression spline marginals. 
Varying the defaults is easy. For example<br />
<code>y ~ x1 + x2 + s(x3) + te(x4,x5,bs=c("cr","ps"),k=c(6,7))</code><br />
specifies that the tensor product should use a rank 6 cubic regression spline marginal
and a rank 7 P-spline marginal to create a smooth with basis dimension 42.
</p>


<h3>Nested terms/functional ANOVA</h3>

<p>Sometimes it is interesting to specify smooth models with a main effects + interaction structure such as 
</p>
<p style="text-align: center;"><code class="reqn">E(y_i) = f_1(x_i) + f_2(z_i) + f_3(x_i,z_i)</code>
</p>

<p>or
</p>
<p style="text-align: center;"><code class="reqn">E(y_i)=f_1(x_i) + f_2(z_i) + f_3(v_i)
+ f_4(x_i,z_i) + f_5(z_i,v_i) + f_6(z_i,v_i) + f_7(x_i,z_i,v_i) </code>
</p>

<p>for example. Such models should be set up using <code><a href="#topic+ti">ti</a></code> terms in the model formula. For example: <br />
<code>y ~ ti(x) + ti(z) + ti(x,z)</code>, or<br />
<code>y ~ ti(x) + ti(z) + ti(v) + ti(x,z) + ti(x,v) + ti(z,v)+ti(x,z,v)</code>. <br />
The <code>ti</code> terms produce interactions with the component main effects excluded appropriately. (There is in fact no need to use <code>ti</code> terms for the main effects here, <code>s</code> terms could also be used.)
</p>
<p><code>gam</code> allows nesting (or &lsquo;overlap&rsquo;) of <code>te</code> and <code>s</code> smooths, and automatically generates side conditions to 
make such models identifiable, but the resulting models are much less stable and interpretable than those constructed using <code>ti</code> terms. 
</p>


<h3>&lsquo;by&rsquo; variables</h3>

<p><code>by</code> variables are the means for constructing &lsquo;varying-coefficient models&rsquo; (geographic regression models) and 
for letting smooths &lsquo;interact&rsquo; with factors or parametric terms. They are also the key to specifying general linear 
functionals of smooths.
</p>
<p>The <code><a href="#topic+s">s</a></code> and <code><a href="#topic+te">te</a></code> terms used to specify smooths accept an argument <code>by</code>, 
which is a numeric or factor variable of the same dimension as the covariates of the smooth. 
If a <code>by</code> variable is numeric, then its <code class="reqn">i^{th}</code> element multiples the  <code class="reqn">i^{th}</code>
row of the model matrix corresponding to the smooth term concerned. 
</p>
<p>Factor smooth interactions (see also <code><a href="#topic+factor.smooth.interaction">factor.smooth.interaction</a></code>).
If a <code>by</code> variable is a <code><a href="base.html#topic+factor">factor</a></code> then it 
generates an indicator vector for each level 
of the factor, unless it is an <code><a href="base.html#topic+ordered">ordered</a></code> factor.
In the non-ordered case, the model matrix for the smooth term is then replicated for each factor level,
and each copy has its rows multiplied by the corresponding rows of its
indicator variable. The smoothness penalties are also duplicated for each
factor level.  In short a different smooth is generated
for each factor level (the <code>id</code> argument to <code><a href="#topic+s">s</a></code> and <code><a href="#topic+te">te</a></code> can be used to force all 
such smooths to have the same smoothing parameter). <code><a href="base.html#topic+ordered">ordered</a></code> <code>by</code> variables are handled in the same 
way, except that no smooth is generated for the first level of the ordered factor (see <code>b3</code> example below). 
This is useful for setting up 
identifiable models when the same smooth occurs more than once in a model, with different factor <code>by</code> variables.
</p>
<p>As an example, consider the model
</p>
<p style="text-align: center;"><code class="reqn">E(y_i) = \beta_0+ f(x_i)z_i</code>
</p>

<p>where <code class="reqn">f</code> is a smooth function, and <code class="reqn">z_i</code> is a numeric variable.
The appropriate formula is:<br />
<code>y ~ s(x,by=z)</code><br />
- the <code>by</code> argument ensures that the smooth function gets multiplied by
covariate <code>z</code>. Note that when using factor by variables, centering constraints are applied to the smooths,
which usually means that the by variable should be included as a parametric term, as well.
</p>
<p>The example code below also illustrates the use of factor <code>by</code> variables.
</p>
<p><code>by</code> variables may be supplied as numeric matrices as part of specifying general linear functional terms.
</p>
<p>If a <code>by</code> variable is present and numeric (rather than a factor) then the corresponding smooth is only subjected 
to an identifiability constraint if (i) the <code>by</code> variable is a constant vector, or, (ii) for a matrix 
<code>by</code> variable, <code>L</code>, if <code>L%*%rep(1,ncol(L))</code> is constant or (iii) if a user defined smooth constructor 
supplies an identifiability constraint explicitly, and that constraint has an attibute <code>"always.apply"</code>. 
</p>


<h3>Linking smooths with &lsquo;id&rsquo;</h3>

<p>It is sometimes desirable to insist that different smooth terms have the same degree of smoothness. 
This can be done by using the <code>id</code> argument to <code><a href="#topic+s">s</a></code> or <code><a href="#topic+te">te</a></code> terms. Smooths 
which share an <code>id</code> will have the same smoothing parameter. Really this only makes sense if the 
smooths use the same basis functions, and the default behaviour is to force this to happen: all smooths 
sharing an <code>id</code> have the same basis functions as the first smooth occurring with that <code>id</code>. Note 
that if you want exactly the same function for each smooth, then this is best achieved by making use of the 
summation convention covered under &lsquo;linear functional terms&rsquo;. 
</p>
<p>As an example suppose that <code class="reqn">E(y_i)\equiv\mu_i</code> and 
</p>
<p style="text-align: center;"><code class="reqn">g(\mu_i) = f_1(x_{1i}) + f_2(x_{2i},x_{3i}) + 
f_3(x_{4i})</code>
</p>

<p>but that <code class="reqn">f_1</code> and <code class="reqn">f_3</code> should have the same smoothing parameters (and <code class="reqn">x_2</code>
and <code class="reqn">x_3</code> are on different scales). Then 
the <code>gam</code> formula<br />
<code>y ~ s(x1,id=1) + te(x_2,x3) + s(x4,id=1)</code><br />
would achieve the desired result. <code>id</code> can be numbers or character strings. Giving an <code>id</code> to a 
term with a factor <code>by</code> variable causes the smooths at each level of the factor to have the same smoothing 
parameter.
</p>
<p>Smooth term <code>id</code>s are not supported by <code>gamm</code>.
</p>


<h3>Linear functional terms</h3>

<p>General linear functional terms have a long history in the spline literature including in the penalized 
GLM context (see e.g. Wahba 1990). Such terms encompass varying coefficient models/ geographic 
regression, functional GLMs (i.e. GLMs with functional predictors), GLASS models, etc, and allow 
smoothing with respect to aggregated covariate values, for example. 
</p>
<p>Such terms are implemented in <code>mgcv</code> using a simple &lsquo;summation convention&rsquo; for smooth terms: If the covariates of a 
smooth are supplied as matrices, then summation of the evaluated smooth over the columns of the matrices is implied. Each 
covariate matrix and any <code>by</code> variable matrix must be of the same dimension. Consider, for example the term<br />
<code>s(X,Z,by=L)</code><br />
where <code>X</code>, <code>Z</code> and <code>L</code> are <code class="reqn">n \times p</code> matrices. Let <code class="reqn">f</code> denote the thin plate regression 
spline specified. The resulting contibution to the <code class="reqn">i^{\rm th}</code> 
element of the linear predictor is 
</p>
<p style="text-align: center;"><code class="reqn">\sum_{j=1}^p L_{ij}f(X_{ij},Z_{ij})</code>
</p>

<p>If no <code>L</code> is supplied then all its elements are taken as 1. In R code terms, let <code>F</code> denote the <code class="reqn">n \times p</code> 
matrix obtained by evaluating the smooth at the values in <code>X</code> and <code>Z</code>. Then the contribution of the term to the 
linear predictor is <code>rowSums(L*F)</code> (note that it's element by element multiplication here!). 
</p>
<p>The summation convention applies to <code>te</code> terms as well as <code>s</code> terms. More details and examples 
are provided in 
<code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code>. 
</p>


<h3>Random effects</h3>

<p>Random effects can be added to <code>gam</code> models using <code>s(...,bs="re")</code> terms (see 
<code><a href="#topic+smooth.construct.re.smooth.spec">smooth.construct.re.smooth.spec</a></code>), 
or the <code>paraPen</code> argument to <code><a href="#topic+gam">gam</a></code> covered below. See <code><a href="#topic+gam.vcomp">gam.vcomp</a></code>, <code><a href="#topic+random.effects">random.effects</a></code> and 
<code><a href="#topic+smooth.construct.re.smooth.spec">smooth.construct.re.smooth.spec</a></code> for further details. An alternative is to use the approach of <code><a href="#topic+gamm">gamm</a></code>.
</p>


<h3>Penalizing the parametric terms</h3>

<p>In case the ability to add smooth classes, smooth identities, <code>by</code> variables and the summation convention are 
still not sufficient to implement exactly the penalized GLM that you require, <code><a href="#topic+gam">gam</a></code> also allows you to penalize the 
parametric terms in the model formula. This is mostly useful in 
allowing one or more matrix terms to be included in the formula, along with a 
sequence of quadratic penalty matrices for each. 
</p>
<p>Suppose that you have set up a model matrix <code class="reqn">\bf X</code>, and want to penalize the corresponding coefficients, <code class="reqn">\beta</code>
with two penalties <code class="reqn">\beta^T {\bf S}_1 \beta</code> and <code class="reqn">\beta^T {\bf S}_2 \beta</code>. 
Then something like the 
following would be appropriate:<br />
<code>gam(y ~ X - 1,paraPen=list(X=list(S1,S2)))</code><br />
The <code>paraPen</code> argument should be a list with elements having  names corresponding to the terms being penalized. 
Each element of <code>paraPen</code> is itself a list, with optional elements <code>L</code>, <code>rank</code> and <code>sp</code>: all other elements 
must be penalty matrices. If present, <code>rank</code> is a vector giving the rank of each penalty matrix 
(if absent this is determined numerically). <code>L</code> is a matrix that maps underlying log smoothing parameters to the 
log smoothing parameters that actually multiply the individual quadratic penalties: taken as the identity if not supplied.
<code>sp</code> is a vector of (underlying) smoothing parameter values: positive values are taken as fixed, negative to signal that 
the smoothing parameter should be estimated. Taken as all negative if not supplied.
</p>
<p>An obvious application of <code>paraPen</code> is to incorporate random effects, and an example of this is provided below. In this
case the supplied penalty matrices will be (generalized) inverse covariance matrices for the random effects &mdash; i.e. precision 
matrices. The final estimate of the covariance matrix corresponding to one of these penalties is given by the (generalized) 
inverse of the penalty matrix multiplied by the estimated scale parameter and divided by the estimated 
smoothing parameter for the penalty. For example, if you use an identity matrix to penalize some coefficients that are to be viewed as i.i.d. 
Gaussian random effects, then their estimated variance will be the estimated scale parameter divided by the estimate of the 
smoothing parameter, for this penalty. See the &lsquo;rail&rsquo; example below. 
</p>
<p>P-values for penalized parametric terms should be treated with caution. If you must have them, then use the option <code>freq=TRUE</code> in 
<code><a href="#topic+anova.gam">anova.gam</a></code> and <code><a href="#topic+summary.gam">summary.gam</a></code>, which will tend to give reasonable results for random effects implemented this way, 
but not for terms with a rank defficient penalty (or penalties with a wide eigen-spectrum). 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wahba (1990) Spline Models of Observational Data SIAM.
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
set.seed(10)
## simulate date from y = f(x2)*x1 + error
dat &lt;- gamSim(3,n=400)

b&lt;-gam(y ~ s(x2,by=x1),data=dat)
plot(b,pages=1)
summary(b)

## Factor `by' variable example (with a spurious covariate x0)
## simulate data...

dat &lt;- gamSim(4)

## fit model...
b &lt;- gam(y ~ fac+s(x2,by=fac)+s(x0),data=dat)
plot(b,pages=1)
summary(b)

## note that the preceding fit is the same as....
b1&lt;-gam(y ~ s(x2,by=as.numeric(fac==1))+s(x2,by=as.numeric(fac==2))+
            s(x2,by=as.numeric(fac==3))+s(x0)-1,data=dat)
## ... the `-1' is because the intercept is confounded with the 
## *uncentred* smooths here.
plot(b1,pages=1)
summary(b1)

## repeat forcing all s(x2) terms to have the same smoothing param
## (not a very good idea for these data!)
b2 &lt;- gam(y ~ fac+s(x2,by=fac,id=1)+s(x0),data=dat)
plot(b2,pages=1)
summary(b2)

## now repeat with a single reference level smooth, and 
## two `difference' smooths...
dat$fac &lt;- ordered(dat$fac)
b3 &lt;- gam(y ~ fac+s(x2)+s(x2,by=fac)+s(x0),data=dat,method="REML")
plot(b3,pages=1)
summary(b3)


rm(dat)

## An example of a simple random effects term implemented via 
## penalization of the parametric part of the model...

dat &lt;- gamSim(1,n=400,scale=2) ## simulate 4 term additive truth
## Now add some random effects to the simulation. Response is 
## grouped into one of 20 groups by `fac' and each groups has a
## random effect added....
fac &lt;- as.factor(sample(1:20,400,replace=TRUE))
dat$X &lt;- model.matrix(~fac-1)
b &lt;- rnorm(20)*.5
dat$y &lt;- dat$y + dat$X%*%b

## now fit appropriate random effect model...
PP &lt;- list(X=list(rank=20,diag(20)))
rm &lt;- gam(y~ X+s(x0)+s(x1)+s(x2)+s(x3),data=dat,paraPen=PP)
plot(rm,pages=1)
## Get estimated random effects standard deviation...
sig.b &lt;- sqrt(rm$sig2/rm$sp[1]);sig.b 

## a much simpler approach uses "re" terms...

rm1 &lt;- gam(y ~ s(fac,bs="re")+s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="ML")
gam.vcomp(rm1)

## Simple comparison with lme, using Rail data. 
## See ?random.effects for a simpler method
require(nlme)
b0 &lt;- lme(travel~1,data=Rail,~1|Rail,method="ML") 
Z &lt;- model.matrix(~Rail-1,data=Rail,
     contrasts.arg=list(Rail="contr.treatment"))
b &lt;- gam(travel~Z,data=Rail,paraPen=list(Z=list(diag(6))),method="ML")

b0 
(b$reml.scale/b$sp)^.5 ## `gam' ML estimate of Rail sd
b$reml.scale^.5         ## `gam' ML estimate of residual sd

b0 &lt;- lme(travel~1,data=Rail,~1|Rail,method="REML") 
Z &lt;- model.matrix(~Rail-1,data=Rail,
     contrasts.arg=list(Rail="contr.treatment"))
b &lt;- gam(travel~Z,data=Rail,paraPen=list(Z=list(diag(6))),method="REML")

b0 
(b$reml.scale/b$sp)^.5 ## `gam' REML estimate of Rail sd
b$reml.scale^.5         ## `gam' REML estimate of residual sd

################################################################
## Approximate large dataset logistic regression for rare events
## based on subsampling the zeroes, and adding an offset to
## approximately allow for this.
## Doing the same thing, but upweighting the sampled zeroes
## leads to problems with smoothness selection, and CIs.
################################################################
n &lt;- 50000  ## simulate n data 
dat &lt;- gamSim(1,n=n,dist="binary",scale=.33)
p &lt;- binomial()$linkinv(dat$f-6) ## make 1's rare
dat$y &lt;- rbinom(p,1,p)      ## re-simulate rare response

## Now sample all the 1's but only proportion S of the 0's
S &lt;- 0.02                   ## sampling fraction of zeroes
dat &lt;- dat[dat$y==1 | runif(n) &lt; S,] ## sampling

## Create offset based on total sampling fraction
dat$s &lt;- rep(log(nrow(dat)/n),nrow(dat))

lr.fit &lt;- gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+s(x3,bs="cr")+
              offset(s),family=binomial,data=dat,method="REML")

## plot model components with truth overlaid in red
op &lt;- par(mfrow=c(2,2))
fn &lt;- c("f0","f1","f2","f3");xn &lt;- c("x0","x1","x2","x3")
for (k in 1:4) {
       plot(lr.fit,select=k,scale=0)
       ff &lt;- dat[[fn[k]]];xx &lt;- dat[[xn[k]]]
       ind &lt;- sort.int(xx,index.return=TRUE)$ix
       lines(xx[ind],(ff-mean(ff))[ind]*.33,col=2)
}
par(op)
rm(dat)

## A Gamma example, by modify `gamSim' output...
 
dat &lt;- gamSim(1,n=400,dist="normal",scale=1)
dat$f &lt;- dat$f/4 ## true linear predictor 
Ey &lt;- exp(dat$f);scale &lt;- .5 ## mean and GLM scale parameter
## Note that `shape' and `scale' in `rgamma' are almost
## opposite terminology to that used with GLM/GAM...
dat$y &lt;- rgamma(Ey*0,shape=1/scale,scale=Ey*scale)
bg &lt;- gam(y~ s(x0)+ s(x1)+s(x2)+s(x3),family=Gamma(link=log),
          data=dat,method="REML")
plot(bg,pages=1,scheme=1)

</code></pre>

<hr>
<h2 id='gam.outer'>Minimize GCV or UBRE score of a GAM using &lsquo;outer&rsquo; iteration</h2><span id='topic+gam.outer'></span>

<h3>Description</h3>

<p>Estimation of GAM smoothing parameters is most stable if
optimization of the smoothness selection score (GCV, GACV, UBRE/AIC, REML, ML etc) 
is outer to the penalized iteratively
re-weighted least squares scheme used to estimate the model given smoothing 
parameters.
</p>
<p>This routine optimizes a smoothness selection score in this way. Basically the 
score is evaluated for each trial set of smoothing parameters by
estimating the GAM for those smoothing parameters. The score is minimized
w.r.t. the parameters numerically, using <code>newton</code> (default), <code>bfgs</code>, <code>optim</code> or <code>nlm</code>. Exact
(first and second) derivatives of the score can be used by fitting with
<code><a href="#topic+gam.fit3">gam.fit3</a></code>. This
improves efficiency and reliability relative to relying on finite
difference derivatives.  
</p>
<p>Not normally called directly, but rather a service routine for <code><a href="#topic+gam">gam</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam.outer(lsp,fscale,family,control,method,optimizer,
          criterion,scale,gamma,G,start=NULL,nei=NULL,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="gam.outer_+3A_lsp">lsp</code></td>
<td>
<p>The log smoothing parameters.</p>
</td></tr>
<tr><td><code id="gam.outer_+3A_fscale">fscale</code></td>
<td>
<p>Typical scale of the GCV or UBRE/AIC score.</p>
</td></tr>
<tr><td><code id="gam.outer_+3A_family">family</code></td>
<td>
<p>the model family.</p>
</td></tr>
<tr><td><code id="gam.outer_+3A_control">control</code></td>
<td>
<p>control argument to pass to <code><a href="#topic+gam.fit">gam.fit</a></code> if pure
finite differencing is being used.</p>
</td></tr>
<tr><td><code id="gam.outer_+3A_method">method</code></td>
<td>
<p>method argument to <code><a href="#topic+gam">gam</a></code> defining the smoothness criterion to use (but depending on whether or not scale known).</p>
</td></tr>
<tr><td><code id="gam.outer_+3A_optimizer">optimizer</code></td>
<td>
<p>The argument to <code><a href="#topic+gam">gam</a></code> defining the numerical optimization method to use. </p>
</td></tr>
<tr><td><code id="gam.outer_+3A_criterion">criterion</code></td>
<td>
<p>Which smoothness selction criterion to use. One of <code>"UBRE"</code>,
<code>"GCV"</code>, <code>"GACV"</code>, <code>"REML"</code> or <code>"P-REML"</code>.   </p>
</td></tr>
<tr><td><code id="gam.outer_+3A_scale">scale</code></td>
<td>
<p>Supplied scale parameter. Positive indicates known.</p>
</td></tr>
<tr><td><code id="gam.outer_+3A_gamma">gamma</code></td>
<td>
<p> The degree of freedom inflation factor for the GCV/UBRE/AIC score.</p>
</td></tr>
<tr><td><code id="gam.outer_+3A_g">G</code></td>
<td>
<p>List produced by <code>mgcv:::gam.setup</code>, containing most of what's
needed to actually fit a GAM.</p>
</td></tr>
<tr><td><code id="gam.outer_+3A_start">start</code></td>
<td>
<p>starting parameter values.</p>
</td></tr>
<tr><td><code id="gam.outer_+3A_nei">nei</code></td>
<td>
<p>List specifying neighbourhood structure if NCV used. See <code><a href="#topic+gam">gam</a></code>.</p>
</td></tr>
<tr><td><code id="gam.outer_+3A_...">...</code></td>
<td>
<p>other arguments, typically for passing on to <code>gam.fit3</code> (ultimately).</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>See Wood (2008) for full details on &lsquo;outer iteration&rsquo;.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2011) Fast stable restricted maximum likelihood 
and marginal likelihood estimation of semiparametric generalized linear 
models. Journal of the Royal Statistical Society (B) 73(1):3-36
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam.fit3">gam.fit3</a></code>,  <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+magic">magic</a></code></p>

<hr>
<h2 id='gam.reparam'>Finding stable orthogonal re-parameterization of the square root penalty.</h2><span id='topic+gam.reparam'></span>

<h3>Description</h3>

<p>INTERNAL function for finding an orthogonal re-parameterization which avoids &quot;dominant machine zero leakage&quot; between
components of the square root penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam.reparam(rS, lsp, deriv)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gam.reparam_+3A_rs">rS</code></td>
<td>
<p>list of the square root penalties: last entry is root of 
fixed penalty, if <code>fixed.penalty==TRUE</code> (i.e. <code>length(rS)&gt;length(sp)</code>). 
The assumption here is that <code>rS[[i]]</code> are in a null space of total penalty already; 
see e.g. <code>totalPenaltySpace</code> and <code>mini.roots</code>.</p>
</td></tr>
<tr><td><code id="gam.reparam_+3A_lsp">lsp</code></td>
<td>
<p>vector of log smoothing parameters.</p>
</td></tr>
<tr><td><code id="gam.reparam_+3A_deriv">deriv</code></td>
<td>
<p>if <code>deriv==1</code> also the first derivative of the log-determinant of the penalty matrix 
is returned, if <code>deriv&gt;1</code> also the second derivative is returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing </p>

<ul>
<li><p><code>S</code>: the total penalty matrix similarity transformed for stability.
</p>
</li>
<li><p><code>rS</code>: the component square roots, transformed in the same way.
</p>
</li>
<li><p><code>Qs</code>: the orthogonal transformation matrix <code>S = t(Qs)%*%S0%*%Qs</code>, where <code>S0</code> is the
untransformed total penalty implied by <code>sp</code> and <code>rS</code> on input.
</p>
</li>
<li><p><code>det</code>: log|S|.
</p>
</li>
<li><p><code>det1</code>: dlog|S|/dlog(sp) if <code>deriv &gt;0</code>.
</p>
</li>
<li><p><code>det2</code>: hessian of log|S| wrt log(sp) if <code>deriv&gt;1</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>

<hr>
<h2 id='gam.scale'>Scale parameter estimation in GAMs</h2><span id='topic+gam.scale'></span>

<h3>Description</h3>

<p>Scale parameter estimation in <code><a href="#topic+gam">gam</a></code> depends on the type of <code>family</code>. For 
extended families then the RE/ML estimate is used. For conventional exponential families, estimated by the default 
outer iteration, the scale estimator can be controlled using argument <code>scale.est</code> in 
<code><a href="#topic+gam.control">gam.control</a></code>. The options are <code>"fletcher"</code> (default), <code>"pearson"</code> or <code>"deviance"</code>. 
The Pearson estimator is the (weighted) sum of squares of the pearson residuals, divided by the 
effective residual degrees of freedom. The Fletcher (2012) estimator is an improved version of the Pearson estimator.
The deviance estimator simply substitutes deviance residuals for Pearson residuals. 
</p>
<p>Usually the Pearson estimator is recommended for GLMs, since it is asymptotically unbiased. However, it can also be unstable at finite sample sizes, if a few Pearson residuals are very large. For example, a very low Poisson mean with a non zero count can give a huge Pearson residual, even though the deviance residual is much more modest. The Fletcher (2012) estimator is designed to reduce these problems.
</p>
<p>For performance iteration the Pearson estimator is always used.
</p>
<p><code><a href="#topic+gamm">gamm</a></code> uses the estimate of the scale parameter from the underlying call to <code>lme</code>. <code><a href="#topic+bam">bam</a></code> uses the REML estimator if the method is <code>"fREML"</code>. Otherwise the estimator is a Pearson estimator.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> with help from Mark Bravington and David Peel</p>


<h3>References</h3>

<p>Fletcher, David J. (2012) Estimating overdispersion when fitting a generalized linear model to sparse data. Biometrika 99(1), 230-237.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+gam.control">gam.control</a> </code> </p>

<hr>
<h2 id='gam.selection'>Generalized Additive Model Selection</h2><span id='topic+gam.selection'></span>

<h3>Description</h3>

<p> This page is intended to provide some more information on
how to select GAMs. In particular, it gives a brief overview of smoothness 
selection, and then discusses how this can be extended to select inclusion/exclusion 
of terms. Hypothesis testing approaches to the latter problem are also discussed.
</p>


<h3>Smoothness selection criteria</h3>

<p>Given a model structure specified by a gam model formula,
<code>gam()</code> attempts to find the appropriate smoothness for each applicable model 
term using prediction error criteria or likelihood based methods. The prediction error 
criteria used are Generalized (Approximate) Cross Validation (GCV or GACV) when the scale 
parameter is unknown or an Un-Biased Risk Estimator (UBRE) when it is known. UBRE is essentially
scaled AIC (Generalized case) or Mallows' Cp (additive model case). 
GCV and UBRE are covered in Craven and Wahba (1979) and Wahba (1990). 
Alternatively REML of maximum likelihood (ML) may be used for smoothness 
selection, by viewing the smooth components as random effects (in this case the variance component for each 
smooth random effect will be given by the scale parameter divided by the smoothing parameter &mdash; for 
smooths with multiple penalties, there will be multiple variance components). 
The <code>method</code> argument to <code><a href="#topic+gam">gam</a></code> selects the smoothness selection criterion.
</p>
<p>Automatic smoothness selection is unlikely to be successful with few data, particularly with
multiple terms to be selected. In addition GCV and UBRE/AIC score can occasionally
display local minima that can trap the minimisation algorithms. GCV/UBRE/AIC
scores become constant with changing smoothing parameters at very low or very
high smoothing parameters, and on occasion these &lsquo;flat&rsquo; regions can be
separated from regions of lower score by a small &lsquo;lip&rsquo;. This seems to be the
most common form of local minimum, but is usually avoidable by avoiding
extreme smoothing parameters as starting values in optimization, and by
avoiding big jumps in smoothing parameters while optimizing. Never the less, if you are
suspicious of smoothing parameter estimates, try changing fit method (see
<code><a href="#topic+gam">gam</a></code> arguments <code>method</code> and <code>optimizer</code>) and see if the estimates change, or try changing
some or all of the smoothing parameters &lsquo;manually&rsquo; (argument <code>sp</code> of <code><a href="#topic+gam">gam</a></code>,
or <code>sp</code> arguments to <code><a href="#topic+s">s</a></code> or <code><a href="#topic+te">te</a></code>). 
</p>
<p>REML and ML are less prone to local minima than the other criteria, and may therefore be preferable.
</p>


<h3>Automatic term selection</h3>

<p>Unmodified smoothness selection by GCV, AIC, REML etc. will not usually remove 
a smooth from a model. This is because most smoothing penalties view some space
of (non-zero) functions as &lsquo;completely smooth&rsquo; and once a term is penalized heavily 
enough that it is in this space, further penalization does not change it. 
</p>
<p>However it is straightforward to modify smooths so that under heavy penalization they 
are penalized to the zero function and thereby &lsquo;selected out&rsquo; of the model. There are 
two approaches.
</p>
<p>The first approach is to modify the smoothing penalty with an additional shrinkage term. 
Smooth classes<code>cs.smooth</code> and <code>tprs.smooth</code> (specified by <code>"cs"</code> and
<code>"ts"</code> respectively) have smoothness penalties which include a small
shrinkage component, so that for large enough smoothing parameters the smooth 
becomes identically zero. This allows automatic smoothing parameter selection
methods to effectively remove the term from the model altogether. The
shrinkage component of the penalty is set at a level that usually makes negligable
contribution to the penalization of the model, only becoming effective when
the term is effectively &lsquo;completely smooth&rsquo; according to the conventional penalty.
</p>
<p>The second approach leaves the original smoothing penalty unchanged, but constructs an 
additional penalty for each smooth, which penalizes only functions in the null space of the 
original penalty (the &lsquo;completely smooth&rsquo; functions). Hence, if all the smoothing parameters 
for a term tend to infinity, the term will be selected out of the model. This latter approach 
is more expensive computationally, but has the advantage that it can be applied automatically 
to any smooth term. The <code>select</code> argument to <code><a href="#topic+gam">gam</a></code> turns on this method.
</p>
<p>In fact, as implemented, both approaches operate by eigen-decomposiong the original penalty matrix. 
A new penalty is created on the null space: it is the matrix with the same eigenvectors as the
original penalty, but with the originally postive egienvalues set to zero, and the originally 
zero eigenvalues set to something positive. The first approach just addes a multiple of this 
penalty to the original penalty, where the multiple is chosen so that the new penalty can not 
dominate the original. The second approach treats the new penalty as an extra penalty, with its 
own smoothing parameter. 
</p>
<p>Of course, as with all model selection methods, some care must be take to ensure that the 
automatic selection is sensible, and a decision about the effective degrees of freedom at 
which to declare a term &lsquo;negligible&rsquo; has to be made. 
</p>


<h3>Interactive term selection</h3>

<p>In general the most logically consistent method to use for deciding which
terms to include in the model is to compare GCV/UBRE/ML scores for models with 
and without the term (REML scores should not be used to compare models with 
different fixed effects structures). When UBRE is the smoothness selection method this will
give the same result as comparing by <code><a href="stats.html#topic+AIC">AIC</a></code> (the AIC in this case
uses the model EDF in place of the usual model DF). Similarly, comparison via
GCV score and via AIC seldom yields different answers. Note that the negative
binomial with estimated <code>theta</code> parameter is a special case: the GCV
score is not informative, because of the <code>theta</code> estimation
scheme used. More generally the score for the model with a smooth 
term can be compared to the score for the model with 
the smooth term replaced by appropriate parametric terms. Candidates for 
replacement by parametric terms are  smooth terms with estimated 
degrees of freedom close to their minimum possible.
</p>
<p>Candidates for removal can also be identified by reference to the 
approximate p-values provided by <code>summary.gam</code>, and by looking at the
extent to which the confidence band for an estimated term includes the zero 
function. It is perfectly possible to perform backwards selection using p-values
in the usual way: that is by sequentially dropping the single term with the highest 
non-significant p-value from the model and re-fitting, until all terms are 
significant. This suffers from the same problems as stepwise procedures for any
GLM/LM, with the additional caveat that the p-values are only approximate. If adopting 
this approach, it is probably best to use ML smoothness selection.
</p>
<p>Note that GCV and UBRE are not appropriate for comparing models using different families: 
in that case AIC should be used.
</p>


<h3>Caveats/platitudes</h3>

<p>Formal model selection methods are only appropriate for selecting between reasonable models.
If formal model selection is attempted starting from a model that simply doesn't fit the data, 
then it is unlikely to provide meaningful results.  
</p>
<p>The more thought is given to appropriate model structure up front, the more successful model
selection is likely to be. Simply starting with a hugely flexible model with &lsquo;everything in&rsquo; 
and hoping that automatic selection will find the right structure is not often successful.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Marra, G. and S.N. Wood (2011) Practical variable selection for generalized additive models.
Computational Statistics and Data Analysis 55,2372-2387.
</p>
<p>Craven and Wahba (1979) Smoothing Noisy Data with Spline Functions. Numer. Math. 31:377-403
</p>
<p>Venables and Ripley (1999) Modern Applied Statistics with S-PLUS
</p>
<p>Wahba (1990) Spline Models of Observational Data. SIAM.
</p>
<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
</p>
<p>Wood, S.N. (2008) Fast stable direct fitting and smoothness selection for
generalized additive models. J.R.Statist. Soc. B 70(3):495-518
</p>
<p>Wood, S.N. (2011) Fast stable restricted maximum likelihood 
and marginal likelihood estimation of semiparametric generalized linear 
models. Journal of the Royal Statistical Society (B) 73(1):3-36
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+step.gam">step.gam</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## an example of automatic model selection via null space penalization
library(mgcv)
set.seed(3);n&lt;-200
dat &lt;- gamSim(1,n=n,scale=.15,dist="poisson") ## simulate data
dat$x4 &lt;- runif(n, 0, 1);dat$x5 &lt;- runif(n, 0, 1) ## spurious

b&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3)+s(x4)+s(x5),data=dat,
         family=poisson,select=TRUE,method="REML")
summary(b)
plot(b,pages=1)
</code></pre>

<hr>
<h2 id='gam.side'>Identifiability side conditions for a GAM</h2><span id='topic+gam.side'></span>

<h3>Description</h3>

<p> GAM formulae with repeated variables may only correspond to
identifiable models given some side conditions. This routine works 
out appropriate side conditions, based on zeroing redundant parameters.
It is called from <code>mgcv:::gam.setup</code> and is not intended to be called by users. 
</p>
<p>The method identifies nested and repeated variables by their names, but
numerically evaluates which constraints need to be imposed. Constraints are always
applied to smooths of more variables in preference to smooths of fewer
variables. The numerical approach allows appropriate constraints to be
applied to models constructed using any smooths, including user defined smooths.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam.side(sm,Xp,tol=.Machine$double.eps^.5,with.pen=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gam.side_+3A_sm">sm</code></td>
<td>
<p> A list of smooth objects as returned by
<code><a href="#topic+smooth.construct">smooth.construct</a></code>.</p>
</td></tr>
<tr><td><code id="gam.side_+3A_xp">Xp</code></td>
<td>
<p>The model matrix for the strictly parametric model components.</p>
</td></tr>
<tr><td><code id="gam.side_+3A_tol">tol</code></td>
<td>
<p>The tolerance to use when assessing linear dependence of smooths.</p>
</td></tr>
<tr><td><code id="gam.side_+3A_with.pen">with.pen</code></td>
<td>
<p>Should the computation of dependence consider the penalties or not. 
Doing so will lead to fewer constraints.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Models such as  <code>y~s(x)+s(z)+s(x,z)</code> can be estimated by
<code><a href="#topic+gam">gam</a></code>, but require identifiability constraints to be applied, to
make them identifiable. This routine does this, effectively setting redundant parameters
to zero. When the redundancy is between smooths of lower and higher numbers
of variables, the constraint is always applied to the smooth of the higher
number of variables. 
</p>
<p>Dependent smooths are identified symbolically, but which constraints are 
needed to ensure identifiability of these smooths is determined numerically, using
<code><a href="#topic+fixDependence">fixDependence</a></code>. This makes the routine rather general, and not
dependent on any particular basis.
</p>
<p><code>Xp</code> is used to check whether there is a constant term in the model (or 
columns that can be linearly combined to give a constant). This is because 
centred smooths can appear independent, when they would be dependent if there 
is a constant in the model, so dependence testing needs to take account of this.
</p>


<h3>Value</h3>

<p> A list of smooths, with model matrices and penalty matrices adjusted
to automatically impose the required constraints. Any smooth that has been
modified will have an attribute <code>"del.index"</code>, listing the columns of its
model matrix that were deleted. This index is used in the creation of
prediction matrices for the term.
</p>


<h3>WARNINGS </h3>

<p>Much better statistical stability will be obtained by using models like 
<code>y~s(x)+s(z)+ti(x,z)</code> or <code>y~ti(x)+ti(z)+ti(x,z)</code> rather than 
<code>y~s(x)+s(z)+s(x,z)</code>, since the former are designed not to require 
further constraint.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+ti">ti</a></code>, <code><a href="#topic+gam.models">gam.models</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## The first two examples here iluustrate models that cause
## gam.side to impose constraints, but both are a bad way 
## of estimating such models. The 3rd example is the right
## way.... 
set.seed(7)
require(mgcv)
dat &lt;- gamSim(n=400,scale=2) ## simulate data
## estimate model with redundant smooth interaction (bad idea).
b&lt;-gam(y~s(x0)+s(x1)+s(x0,x1)+s(x2),data=dat)
plot(b,pages=1)

## Simulate data with real interation...
dat &lt;- gamSim(2,n=500,scale=.1)
old.par&lt;-par(mfrow=c(2,2))

## a fully nested tensor product example (bad idea)
b &lt;- gam(y~s(x,bs="cr",k=6)+s(z,bs="cr",k=6)+te(x,z,k=6),
       data=dat$data)
plot(b)

old.par&lt;-par(mfrow=c(2,2))
## A fully nested tensor product example, done properly,
## so that gam.side is not needed to ensure identifiability.
## ti terms are designed to produce interaction smooths
## suitable for adding to main effects (we could also have
## used s(x) and s(z) without a problem, but not s(z,x) 
## or te(z,x)).
b &lt;- gam(y ~ ti(x,k=6) + ti(z,k=6) + ti(x,z,k=6),
       data=dat$data)
plot(b)

par(old.par)
rm(dat)
</code></pre>

<hr>
<h2 id='gam.vcomp'>Report gam smoothness estimates as variance components</h2><span id='topic+gam.vcomp'></span>

<h3>Description</h3>

<p>GAMs can be viewed as mixed models, where the smoothing parameters are related to variance 
components. This routine extracts the estimated variance components associated with each smooth term, and if possible returns confidence intervals on the standard deviation scale.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam.vcomp(x,rescale=TRUE,conf.lev=.95)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="gam.vcomp_+3A_x">x</code></td>
<td>
<p> a fitted model object of class <code>gam</code> as produced by <code>gam()</code>.</p>
</td></tr>
<tr><td><code id="gam.vcomp_+3A_rescale">rescale</code></td>
<td>
<p> the penalty matrices for smooths are rescaled before fitting, for numerical 
stability reasons, if <code>TRUE</code> this rescaling is reversed, so that the variance components
are on the original scale.</p>
</td></tr>
<tr><td><code id="gam.vcomp_+3A_conf.lev">conf.lev</code></td>
<td>
<p> when the smoothing parameters are estimated by REML or ML, then confidence intervals 
for the variance components can be obtained from large sample likelihood results. This gives the 
confidence level to work at.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The (pseudo) inverse of the penalty matrix penalizing a term is proportional to the 
covariance matrix of the term's coefficients, when these are viewed as random. For single penalty smooths, 
it is possible to compute the variance component for the smooth (which multiplies the 
inverse penalty matrix to obtain the covariance matrix of the smooth's coefficients). This
variance component is given by the scale parameter divided by the smoothing parameter. 
</p>
<p>This routine computes such variance components, for <code>gam</code> models, and associated confidence intervals, if smoothing parameter estimation was likelihood based. Note that variance components are also returned 
for tensor product smooths, but that their interpretation is not so straightforward. 
</p>
<p>The routine is particularly useful for model fitted by <code><a href="#topic+gam">gam</a></code> in which random effects have 
been incorporated.
</p>


<h3>Value</h3>

<p> Either a vector of variance components for each smooth term (as standard deviations), or a matrix. The first column of the 
matrix gives standard deviations for each term, while the subsequent columns give lower and upper confidence bounds, on the same scale.
</p>
<p>For models in which there are more smoothing parameters than actually estimated (e.g. if some were fixed, or smoothing parameters are linked) then a list is returned. The <code>vc</code> element is as above, the <code>all</code> element is a vector of variance components for all the smoothing parameters (estimated + fixed or replicated).
</p>
<p>The routine prints a table of estimated standard deviations and confidence limits, if these can be 
computed, and reports the numerical rank of the covariance matrix.
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N. (2008) Fast stable direct fitting and smoothness
selection for generalized additive models. Journal of the Royal
Statistical Society (B) 70(3):495-518
</p>
<p>Wood, S.N. (2011) Fast stable restricted maximum likelihood 
and marginal likelihood estimation of semiparametric generalized linear 
models. Journal of the Royal Statistical Society (B) 73(1):3-36
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+smooth.construct.re.smooth.spec">smooth.construct.re.smooth.spec</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> 
  set.seed(3) 
  require(mgcv)
  ## simulate some data, consisting of a smooth truth + random effects

  dat &lt;- gamSim(1,n=400,dist="normal",scale=2)
  a &lt;- factor(sample(1:10,400,replace=TRUE))
  b &lt;- factor(sample(1:7,400,replace=TRUE))
  Xa &lt;- model.matrix(~a-1)    ## random main effects
  Xb &lt;-  model.matrix(~b-1)
  Xab &lt;- model.matrix(~a:b-1) ## random interaction
  dat$y &lt;- dat$y + Xa%*%rnorm(10)*.5 + 
           Xb%*%rnorm(7)*.3 + Xab%*%rnorm(70)*.7
  dat$a &lt;- a;dat$b &lt;- b

  ## Fit the model using "re" terms, and smoother linkage  
  
  mod &lt;- gam(y~s(a,bs="re")+s(b,bs="re")+s(a,b,bs="re")+s(x0,id=1)+s(x1,id=1)+
               s(x2,k=15)+s(x3),data=dat,method="ML")

  gam.vcomp(mod) 

</code></pre>

<hr>
<h2 id='gam2objective'>Objective functions for GAM smoothing parameter estimation</h2><span id='topic+gam2objective'></span><span id='topic+gam2derivative'></span>

<h3>Description</h3>

<p>Estimation of GAM smoothing parameters is most stable if
optimization of the UBRE/AIC or GCV score is outer to the penalized iteratively
re-weighted least squares scheme used to estimate the model given smoothing 
parameters. These functions evaluate the GCV/UBRE/AIC score of a GAM model, given
smoothing parameters, in a manner suitable for use by <code><a href="stats.html#topic+optim">optim</a></code> or <code><a href="stats.html#topic+nlm">nlm</a></code>.
Not normally called directly, but rather service routines for <code><a href="#topic+gam.outer">gam.outer</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gam2objective(lsp,args,...)
gam2derivative(lsp,args,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="gam2objective_+3A_lsp">lsp</code></td>
<td>
<p>The log smoothing parameters.</p>
</td></tr>
<tr><td><code id="gam2objective_+3A_args">args</code></td>
<td>
<p>List of arguments required to call <code><a href="#topic+gam.fit3">gam.fit3</a></code>.</p>
</td></tr>
<tr><td><code id="gam2objective_+3A_...">...</code></td>
<td>
<p>Other arguments for passing to <code>gam.fit3</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

 <p><code>gam2objective</code> and <code>gam2derivative</code> are functions suitable
for calling by <code><a href="stats.html#topic+optim">optim</a></code>, to evaluate the GCV/UBRE/AIC score and its
derivatives w.r.t. log smoothing parameters.
</p>
<p><code>gam4objective</code> is an equivalent to <code>gam2objective</code>, suitable for
optimization by <code><a href="stats.html#topic+nlm">nlm</a></code> - derivatives of the GCV/UBRE/AIC function are
calculated and returned as attributes.
</p>
<p>The basic idea of optimizing smoothing parameters &lsquo;outer&rsquo; to the P-IRLS loop
was first proposed in O'Sullivan et al. (1986).
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2011) Fast stable restricted maximum likelihood 
and marginal likelihood estimation of semiparametric generalized linear 
models. Journal of the Royal Statistical Society (B) 73(1):3-36
</p>
<p>O 'Sullivan, Yandall &amp; Raynor (1986) Automatic smoothing of regression
functions in generalized linear models. J. Amer. Statist. Assoc. 81:96-103.
</p>
<p>Wood, S.N. (2008) Fast stable direct fitting and smoothness selection for generalized
additive models. J.R.Statist.Soc.B 70(3):495-518
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam.fit3">gam.fit3</a></code>,  <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+magic">magic</a></code></p>

<hr>
<h2 id='gamlss.etamu'>Transform derivatives wrt mu to derivatives wrt linear predictor</h2><span id='topic+gamlss.etamu'></span>

<h3>Description</h3>

<p>Mainly intended for internal use in specifying location scale models.
Let <code>g(mu) = lp</code>, where <code>lp</code> is the linear predictor, and <code>g</code> is the link
function. Assume that we have calculated the derivatives of the log-likelihood wrt <code>mu</code>.
This function uses the chain rule to calculate the derivatives of the log-likelihood wrt
<code>lp</code>. See <code><a href="#topic+trind.generator">trind.generator</a></code> for array packing conventions. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gamlss.etamu(l1, l2, l3 = NULL, l4 = NULL, ig1, g2, g3 = NULL,
  g4 = NULL, i2, i3 = NULL, i4 = NULL, deriv = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gamlss.etamu_+3A_l1">l1</code></td>
<td>
<p>array of 1st order derivatives of log-likelihood wrt mu.</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_l2">l2</code></td>
<td>
<p>array of 2nd order derivatives of log-likelihood wrt mu.</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_l3">l3</code></td>
<td>
<p>array of 3rd order derivatives of log-likelihood wrt mu.</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_l4">l4</code></td>
<td>
<p>array of 4th order derivatives of log-likelihood wrt mu.</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_ig1">ig1</code></td>
<td>
<p>reciprocal of the first derivative of the link function wrt the linear predictor.</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_g2">g2</code></td>
<td>
<p>array containing the 2nd order derivative of the link function wrt the linear predictor.</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_g3">g3</code></td>
<td>
<p>array containing the 3rd order derivative of the link function wrt the linear predictor.</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_g4">g4</code></td>
<td>
<p>array containing the 4th order derivative of the link function wrt the linear predictor.</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_i2">i2</code></td>
<td>
<p>two-dimensional index array, such that <code>l2[,i2[i,j]]</code> contains the partial w.r.t. params 
indexed by i,j with no restriction on the index values (except that they are in 1,...,ncol(l1)).</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_i3">i3</code></td>
<td>
<p>third-dimensional index array, such that <code>l3[,i3[i,j,k]]</code> contains the partial w.r.t. params 
indexed by i,j,k.</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_i4">i4</code></td>
<td>
<p>third-dimensional index array, such that <code>l4[,i4[i,j,k,l]]</code> contains the partial w.r.t. params 
indexed by i,j,k,l.</p>
</td></tr>
<tr><td><code id="gamlss.etamu_+3A_deriv">deriv</code></td>
<td>
<p>if <code>deriv==0</code> only first and second order derivatives will be calculated. If
<code>deriv==1</code> the function goes up to 3rd order, and if <code>deriv==2</code> it provides
also 4th order derivatives.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list where the arrays <code>l1</code>, <code>l2</code>, <code>l3</code>, <code>l4</code> contain the derivatives (up
to order four) of the log-likelihood wrt the linear predictor.
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trind.generator">trind.generator</a></code></p>

<hr>
<h2 id='gamlss.gH'>Calculating derivatives of log-likelihood wrt regression coefficients</h2><span id='topic+gamlss.gH'></span>

<h3>Description</h3>

<p> Mainly intended for internal use with location scale model families.
Given the derivatives of the log-likelihood wrt the linear predictor, this function obtains
the derivatives and Hessian wrt the regression coefficients and derivatives of
the Hessian w.r.t. the smoothing parameters. For input derivative array packing conventions see <code><a href="#topic+trind.generator">trind.generator</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gamlss.gH(X, jj, l1, l2, i2, l3 = 0, i3 = 0, l4 = 0, i4 = 0, d1b = 0,
  d2b = 0, deriv = 0, fh = NULL, D = NULL,sandwich=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gamlss.gH_+3A_x">X</code></td>
<td>
<p>matrix containing the model matrices of all the linear predictors.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_jj">jj</code></td>
<td>
<p>list of index vectors such that <code>X[,jj[[i]]]</code> is the model matrix of the i-th linear predictor.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_l1">l1</code></td>
<td>
<p>array of 1st order derivatives of each element of the log-likelihood wrt each parameter.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_l2">l2</code></td>
<td>
<p>array of 2nd order derivatives of each element of the log-likelihood wrt each parameter.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_i2">i2</code></td>
<td>
<p>two-dimensional index array, such that <code>l2[,i2[i,j]]</code> contains the partial w.r.t. params 
indexed by i,j with no restriction on the index values (except that they are in 1,...,ncol(l1)).</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_l3">l3</code></td>
<td>
<p>array of 3rd order derivatives of each element of the log-likelihood wrt each parameter.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_i3">i3</code></td>
<td>
<p>third-dimensional index array, such that <code>l3[,i3[i,j,k]]</code> contains the partial w.r.t. params 
indexed by i,j,k.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_l4">l4</code></td>
<td>
<p>array of 4th order derivatives of each element of the log-likelihood wrt each parameter.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_i4">i4</code></td>
<td>
<p>third-dimensional index array, such that <code>l4[,i4[i,j,k,l]]</code> contains the partial w.r.t. params 
indexed by i,j,k,l.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_d1b">d1b</code></td>
<td>
<p>first derivatives of the regression coefficients wrt the smoothing parameters.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_d2b">d2b</code></td>
<td>
<p>second derivatives of the regression coefficients wrt the smoothing parameters.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_deriv">deriv</code></td>
<td>
<p>if <code>deriv==0</code> only first and second order derivatives will be calculated. If
<code>deriv==1</code> the function return also the diagonal of the first derivative of the Hessian,
if <code>deriv==2</code> it return the full 3rd order derivative and if <code>deriv==3</code> it provides
also 4th order derivatives.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_fh">fh</code></td>
<td>
<p>eigen-decomposition or Cholesky factor of the penalized Hessian.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_d">D</code></td>
<td>
<p>diagonal matrix, used to provide some scaling.</p>
</td></tr>
<tr><td><code id="gamlss.gH_+3A_sandwich">sandwich</code></td>
<td>
<p>set to <code>TRUE</code> to return sandwich estimator 'filling', as opposed to the Hessian, in <code>l2</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing <code>lb</code> - the grad vector w.r.t. coefs; <code>lbb</code> - the Hessian matrix w.r.t. coefs;
<code>d1H</code> - either a list of the derivatives of the Hessian w.r.t. the smoothing parameters, or a single matrix whose columns are the leading diagonals of these dervative matrices; <code>trHid2H</code> - the trace of the inverse Hessian multiplied by the second derivative of the Hessian w.r.t. all combinations of smoothing parameters.
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+trind.generator">trind.generator</a></code></p>

<hr>
<h2 id='gamm'>Generalized Additive Mixed Models</h2><span id='topic+gamm'></span>

<h3>Description</h3>

<p> Fits the specified  generalized additive mixed model (GAMM) to
data, by a call to <code>lme</code> in the normal errors identity link case, or by 
a call to <code>gammPQL</code> (a modification of <code>glmmPQL</code> from the <code>MASS</code> library) otherwise. 
In the latter case estimates are only approximately MLEs. The routine is typically 
slower than <code>gam</code>, and not quite as numerically robust. 
</p>
<p>To use <code>lme4</code> in place of <code>nlme</code> as the underlying fitting engine, 
see <code>gamm4</code> from package <code>gamm4</code>.
</p>
<p>Smooths are specified as in a call to <code><a href="#topic+gam">gam</a></code> as part of the fixed 
effects model formula, but the wiggly components of the smooth are treated as 
random effects. The random effects structures and correlation structures 
available for <code>lme</code> are used to specify other random effects and 
correlations. 
</p>
<p>It is assumed that the random effects and correlation structures are employed 
primarily to model residual correlation in the data and that the prime interest
is in inference about the terms in the fixed effects model formula including 
the smooths. For this reason the routine calculates a posterior covariance 
matrix for the coefficients of all the terms in the fixed effects formula, 
including the smooths.
</p>
<p>To use this function effectively it helps to be quite familiar with the use of
<code><a href="#topic+gam">gam</a></code> and <code><a href="nlme.html#topic+lme">lme</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gamm(formula,random=NULL,correlation=NULL,family=gaussian(),
data=list(),weights=NULL,subset=NULL,na.action,knots=NULL,
control=list(niterEM=0,optimMethod="L-BFGS-B",returnObject=TRUE),
niterPQL=20,verbosePQL=TRUE,method="ML",drop.unused.levels=TRUE,
mustart=NULL, etastart=NULL,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="gamm_+3A_formula">formula</code></td>
<td>
<p> A GAM formula (see also <code><a href="#topic+formula.gam">formula.gam</a></code> and <code><a href="#topic+gam.models">gam.models</a></code>).
This is like the formula for a <code>glm</code> except that smooth terms (<code><a href="#topic+s">s</a></code>, <code><a href="#topic+te">te</a></code> etc.)
can be added to the right hand side of the formula. Note that <code>id</code>s for smooths and fixed smoothing parameters are not supported. Any offset should be specified in the formula.
</p>
</td></tr> 
<tr><td><code id="gamm_+3A_random">random</code></td>
<td>
<p>The (optional) random effects structure as specified in a call to 
<code><a href="nlme.html#topic+lme">lme</a></code>: only the <code>list</code> form is allowed, to facilitate 
manipulation of the random effects structure within <code>gamm</code> in order to deal
with smooth terms. See example below.</p>
</td></tr>
<tr><td><code id="gamm_+3A_correlation">correlation</code></td>
<td>
<p>An optional <code>corStruct</code> object 
(see <code><a href="nlme.html#topic+corClasses">corClasses</a></code>) as used to define correlation 
structures in <code><a href="nlme.html#topic+lme">lme</a></code>. Any grouping factors in the formula for
this object are assumed to be nested within any random effect grouping
factors, without the need to make this explicit in the formula (this is 
slightly different to the behaviour of <code>lme</code>). This 
is a GEE approach to correlation in the generalized case. See examples below.</p>
</td></tr>
<tr><td><code id="gamm_+3A_family">family</code></td>
<td>
<p>A <code>family</code> as used in a call to <code><a href="stats.html#topic+glm">glm</a></code> or <code><a href="#topic+gam">gam</a></code>.
The default <code>gaussian</code> with identity link causes <code>gamm</code> to fit by a direct call to
<code><a href="nlme.html#topic+lme">lme</a></code> provided there is no offset term, otherwise
<code>gammPQL</code> is used.</p>
</td></tr>
<tr><td><code id="gamm_+3A_data">data</code></td>
<td>
<p> A data frame or list containing the model response variable and 
covariates required by the formula.  By default the variables are taken 
from <code>environment(formula)</code>, typically the environment from 
which <code>gamm</code> is called.</p>
</td></tr> 
<tr><td><code id="gamm_+3A_weights">weights</code></td>
<td>
<p>In the generalized case, weights with the same meaning as
<code><a href="stats.html#topic+glm">glm</a></code> weights. An <code>lme</code> type weights argument may only be
used in the identity link gaussian case, with no offset (see documentation for <code>lme</code>
for details of how to use such an argument).</p>
</td></tr>
<tr><td><code id="gamm_+3A_subset">subset</code></td>
<td>
<p> an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td></tr>
<tr><td><code id="gamm_+3A_na.action">na.action</code></td>
<td>
<p> a function which indicates what should happen when the data
contain &lsquo;NA&rsquo;s.  The default is set by the &lsquo;na.action&rsquo; setting
of &lsquo;options&rsquo;, and is &lsquo;na.fail&rsquo; if that is unset.  The
&ldquo;factory-fresh&rdquo; default is &lsquo;na.omit&rsquo;.</p>
</td></tr>
<tr><td><code id="gamm_+3A_knots">knots</code></td>
<td>
<p>this is an optional list containing user specified knot values to be used for basis construction. 
Different terms can use different numbers of knots, unless they share a covariate.
</p>
</td></tr>
<tr><td><code id="gamm_+3A_control">control</code></td>
<td>
<p>A list of fit control parameters for <code><a href="nlme.html#topic+lme">lme</a></code> to replace the 
defaults returned by <code><a href="nlme.html#topic+lmeControl">lmeControl</a></code>. Note the setting 
for the number of EM iterations used by <code>lme</code>: smooths are set up using custom <code>pdMat</code> classes,
which are currently not supported by the EM iteration code. If you supply a list of control values,
it is advisable to include <code>niterEM=0</code>, as well, and only increase from 0
if you want to perturb the starting values used in model fitting
(usually to worse values!). The <code>optimMethod</code> option is only used if your
version of R does not have the <code>nlminb</code> optimizer function.</p>
</td></tr>
<tr><td><code id="gamm_+3A_niterpql">niterPQL</code></td>
<td>
<p>Maximum number of PQL iterations (if any).</p>
</td></tr>
<tr><td><code id="gamm_+3A_verbosepql">verbosePQL</code></td>
<td>
<p>Should PQL report its progress as it goes along?</p>
</td></tr>
<tr><td><code id="gamm_+3A_method">method</code></td>
<td>
<p>Which of <code>"ML"</code> or <code>"REML"</code> to use in the Gaussian
additive mixed model case when <code>lme</code> is called directly. Ignored in the
generalized case (or if the model has an offset), in which case <code>gammPQL</code> is used.</p>
</td></tr>
<tr><td><code id="gamm_+3A_drop.unused.levels">drop.unused.levels</code></td>
<td>
<p>by default unused levels are dropped from factors before fitting. For some smooths 
involving factor variables you might want to turn this off. Only do so if you know what you are doing.</p>
</td></tr>
<tr><td><code id="gamm_+3A_mustart">mustart</code></td>
<td>
<p>starting values for mean if PQL used.</p>
</td></tr>
<tr><td><code id="gamm_+3A_etastart">etastart</code></td>
<td>
<p>starting values for linear predictor if PQL used (over-rides <code>mustart</code> if supplied).</p>
</td></tr>
<tr><td><code id="gamm_+3A_...">...</code></td>
<td>
<p>further arguments for passing on e.g. to <code>lme</code></p>
</td></tr> 
</table>


<h3>Details</h3>

<p> The Bayesian model of spline smoothing introduced by Wahba (1983) and Silverman (1985) opens 
up the possibility of estimating the degree of smoothness of terms in a generalized additive model
as variances of the wiggly components of the smooth terms treated as random effects. Several authors 
have recognised this (see Wang 1998; Ruppert, Wand and Carroll, 2003) and in the normal errors, 
identity link case estimation can 
be performed using general linear mixed effects modelling software such as <code>lme</code>. In the generalized case only 
approximate inference is so far available, for example using the Penalized Quasi-Likelihood approach of Breslow 
and Clayton (1993) as implemented in <code>glmmPQL</code> by Venables and Ripley (2002). 
One advantage of this approach is that it allows correlated errors to be dealt with via random effects 
or the correlation structures available in the <code>nlme</code> library (using correlation structures beyond the 
strictly additive case amounts to using a GEE approach to fitting). 
</p>
<p>Some details of how GAMs are represented as mixed models and estimated using
<code>lme</code> or <code>gammPQL</code> in <code>gamm</code> can be found in Wood (2004 ,2006a,b). In addition <code>gamm</code> obtains a posterior covariance matrix for the parameters of all the fixed effects and the smooth terms. The approach is similar to that described in Lin &amp; Zhang (1999) - the covariance matrix of the data (or pseudodata in the generalized case) implied by the weights, correlation and random effects structure is obtained, based on the estimates of the parameters of these terms and this is used to obtain the posterior covariance matrix of the fixed and smooth effects. 
</p>
<p>The bases used to represent smooth terms are the same as those used in <code><a href="#topic+gam">gam</a></code>, although adaptive
smoothing bases are not available. Prediction from the returned <code>gam</code> object is straightforward using <code><a href="#topic+predict.gam">predict.gam</a></code>, but this will set the random effects to zero. If you want to predict with random effects set to their predicted values then you can adapt the prediction code given in the examples below. 
</p>
<p>In the event of <code>lme</code> convergence failures, consider
modifying <code>options(mgcv.vc.logrange)</code>: reducing it helps to remove
indefiniteness in the likelihood, if that is the problem, but too large a
reduction can force over or undersmoothing. See <code><a href="#topic+notExp2">notExp2</a></code> for more
information on this option. Failing that, you can try increasing the
<code>niterEM</code> option in <code>control</code>: this will perturb the starting values
used in fitting, but usually to values with lower likelihood! Note that this
version of <code>gamm</code> works best with R 2.2.0 or above and <code>nlme</code>, 3.1-62 and above,
since these use an improved optimizer. 
</p>


<h3>Value</h3>

<p> Returns a list with two items:
</p>
<table>
<tr><td><code>gam</code></td>
<td>
<p>an object of class <code>gam</code>, less information relating to
GCV/UBRE model selection. At present this contains enough information to use
<code>predict</code>, <code>summary</code> and <code>print</code> methods and <code>vis.gam</code>,
but not to use e.g. the <code>anova</code> method function to compare models. This is based
on the working model when using <code>gammPQL</code>.</p>
</td></tr>
<tr><td><code>lme</code></td>
<td>
<p>the fitted model object returned by <code>lme</code> or <code>gammPQL</code>. Note that the model formulae and grouping 
structures may appear to be rather bizarre, because of the manner in which the GAMM is split up and the calls to 
<code>lme</code> and <code>gammPQL</code> are constructed.</p>
</td></tr>
</table>


<h3>WARNINGS </h3>

<p><code>gamm</code> has a somewhat different argument list to <code><a href="#topic+gam">gam</a></code>,
<code>gam</code> arguments such as <code>gamma</code> supplied to <code>gamm</code> will
just be ignored.  
</p>
<p><code>gamm</code> performs poorly with binary data, since it uses PQL. It is 
better to use <code>gam</code> with <code>s(...,bs="re")</code> terms, or <code>gamm4</code>.
</p>
<p><code>gamm</code> assumes that you know what you are doing! For example, unlike 
<code>glmmPQL</code> from <code>MASS</code> it will return the complete <code>lme</code> object
from the working model at convergence of the PQL iteration, including the 'log 
likelihood', even though this is not the likelihood of the fitted GAMM. 
</p>
<p>The routine will be very slow and memory intensive if correlation structures
are used for the very large groups of data. e.g. attempting to run the
spatial example in the examples section with many 1000's of data is definitely not 
recommended: often the correlations should only apply within clusters that can
be defined by a grouping factor, and provided these clusters do not get too huge
then fitting is usually possible.
</p>
<p>Models must contain at least one random effect: either a smooth with non-zero
smoothing parameter, or a random effect specified in argument <code>random</code>.
</p>
<p><code>gamm</code> is not as numerically stable as <code>gam</code>: an <code>lme</code> call
will occasionally fail. See details section for suggestions, or try the 
&lsquo;gamm4&rsquo; package.
</p>
<p><code>gamm</code> is usually much slower than <code>gam</code>, and on some platforms you may need to
increase the memory available to R in order to use it with large data sets
(see <code><a href="utils.html#topic+memory.limit">memory.limit</a></code>).
</p>
<p>Note that the weights returned in the fitted GAM object are dummy, and not
those used by the PQL iteration: this makes partial residual plots look odd.
</p>
<p>Note that the <code>gam</code> object part of the returned object is not complete in
the sense of having all the elements defined in <code><a href="#topic+gamObject">gamObject</a></code> and
does not inherit from <code>glm</code>: hence e.g. multi-model <code>anova</code> calls will not work.
It is also based on the working model when PQL is used.
</p>
<p>The parameterization used for the smoothing parameters in <code>gamm</code>, bounds
them above and below by an effective infinity and effective zero. See
<code><a href="#topic+notExp2">notExp2</a></code> for details of how to change this. 
</p>
<p>Linked smoothing parameters and adaptive smoothing are not supported.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Breslow, N. E. and Clayton, D. G. (1993) Approximate inference in generalized linear 
mixed models. Journal of the American Statistical Association 88, 9-25.
</p>
<p>Lin, X and Zhang, D. (1999) Inference in generalized additive mixed models by using smoothing 
splines. JRSSB. 55(2):381-400
</p>
<p>Pinheiro J.C. and Bates, D.M. (2000) Mixed effects Models in S and S-PLUS. Springer
</p>
<p>Ruppert, D., Wand, M.P. and Carroll, R.J. (2003) Semiparametric Regression. 
Cambridge
</p>
<p>Silverman, B.W. (1985) Some aspects of the spline smoothing approach to nonparametric regression.
JRSSB 47:1-52 
</p>
<p>Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics
with S. Fourth edition.  Springer.
</p>
<p>Wahba, G. (1983) Bayesian confidence intervals for the cross validated smoothing spline. 
JRSSB 45:133-150
</p>
<p>Wood, S.N. (2004) Stable and efficient multiple smoothing parameter estimation for
generalized additive models. Journal of the American Statistical Association. 99:673-686
</p>
<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
</p>
<p>Wood, S.N. (2006a) Low rank scale invariant tensor product smooths for
generalized additive mixed models. Biometrics 62(4):1025-1036
</p>
<p>Wood S.N. (2006b) Generalized Additive Models: An Introduction with R. Chapman
and Hall/CRC Press.
</p>
<p>Wang, Y. (1998) Mixed effects smoothing spline analysis of variance. J.R. Statist. Soc. B 60, 159-174
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+magic">magic</a></code> for an alternative for correlated data,
<code><a href="#topic+te">te</a></code>, <code><a href="#topic+s">s</a></code>, 
<code><a href="#topic+predict.gam">predict.gam</a></code>,
<code><a href="#topic+plot.gam">plot.gam</a></code>, <code><a href="#topic+summary.gam">summary.gam</a></code>, <code><a href="#topic+negbin">negbin</a></code>, 
<code><a href="#topic+vis.gam">vis.gam</a></code>,<code><a href="#topic+pdTens">pdTens</a></code>, <code>gamm4</code> (
<a href="https://cran.r-project.org/package=gamm4">https://cran.r-project.org/package=gamm4</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## simple examples using gamm as alternative to gam
set.seed(0) 
dat &lt;- gamSim(1,n=200,scale=2)
b &lt;- gamm(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
plot(b$gam,pages=1)
summary(b$lme) # details of underlying lme fit
summary(b$gam) # gam style summary of fitted model
anova(b$gam) 
gam.check(b$gam) # simple checking plots

b &lt;- gamm(y~te(x0,x1)+s(x2)+s(x3),data=dat) 
op &lt;- par(mfrow=c(2,2))
plot(b$gam)
par(op)
rm(dat)


## Add a factor to the linear predictor, to be modelled as random
dat &lt;- gamSim(6,n=200,scale=.2,dist="poisson")
b2 &lt;- gamm(y~s(x0)+s(x1)+s(x2),family=poisson,
           data=dat,random=list(fac=~1))
plot(b2$gam,pages=1)
fac &lt;- dat$fac
rm(dat)
vis.gam(b2$gam)

## In the generalized case the 'gam' object is based on the working
## model used in the PQL fitting. Residuals for this are not
## that useful on their own as the following illustrates...

gam.check(b2$gam) 

## But more useful residuals are easy to produce on a model
## by model basis. For example...

fv &lt;- exp(fitted(b2$lme)) ## predicted values (including re)
rsd &lt;- (b2$gam$y - fv)/sqrt(fv) ## Pearson residuals (Poisson case)
op &lt;- par(mfrow=c(1,2))
qqnorm(rsd);plot(fv^.5,rsd)
par(op)

## now an example with autocorrelated errors....
n &lt;- 200;sig &lt;- 2
x &lt;- 0:(n-1)/(n-1)
f &lt;- 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
e &lt;- rnorm(n,0,sig)
for (i in 2:n) e[i] &lt;- 0.6*e[i-1] + e[i]
y &lt;- f + e
op &lt;- par(mfrow=c(2,2))
## Fit model with AR1 residuals
b &lt;- gamm(y~s(x,k=20),correlation=corAR1())
plot(b$gam);lines(x,f-mean(f),col=2)
## Raw residuals still show correlation, of course...
acf(residuals(b$gam),main="raw residual ACF")
## But standardized are now fine...
acf(residuals(b$lme,type="normalized"),main="standardized residual ACF")
## compare with model without AR component...
b &lt;- gam(y~s(x,k=20))
plot(b);lines(x,f-mean(f),col=2)

## more complicated autocorrelation example - AR errors
## only within groups defined by `fac'
e &lt;- rnorm(n,0,sig)
for (i in 2:n) e[i] &lt;- 0.6*e[i-1]*(fac[i-1]==fac[i]) + e[i]
y &lt;- f + e
b &lt;- gamm(y~s(x,k=20),correlation=corAR1(form=~1|fac))
plot(b$gam);lines(x,f-mean(f),col=2)
par(op) 

## more complex situation with nested random effects and within
## group correlation 

set.seed(0)
n.g &lt;- 10
n&lt;-n.g*10*4
## simulate smooth part...
dat &lt;- gamSim(1,n=n,scale=2)
f &lt;- dat$f
## simulate nested random effects....
fa &lt;- as.factor(rep(1:10,rep(4*n.g,10)))
ra &lt;- rep(rnorm(10),rep(4*n.g,10))
fb &lt;- as.factor(rep(rep(1:4,rep(n.g,4)),10))
rb &lt;- rep(rnorm(4),rep(n.g,4))
for (i in 1:9) rb &lt;- c(rb,rep(rnorm(4),rep(n.g,4)))
## simulate auto-correlated errors within groups
e&lt;-array(0,0)
for (i in 1:40) {
  eg &lt;- rnorm(n.g, 0, sig)
  for (j in 2:n.g) eg[j] &lt;- eg[j-1]*0.6+ eg[j]
  e&lt;-c(e,eg)
}
dat$y &lt;- f + ra + rb + e
dat$fa &lt;- fa;dat$fb &lt;- fb
## fit model .... 
b &lt;- gamm(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+
  s(x3,bs="cr"),data=dat,random=list(fa=~1,fb=~1),
  correlation=corAR1())
plot(b$gam,pages=1)
summary(b$gam)
vis.gam(b$gam)

## Prediction from gam object, optionally adding 
## in random effects. 

## Extract random effects and make names more convenient...
refa &lt;- ranef(b$lme,level=5)
rownames(refa) &lt;- substr(rownames(refa),start=9,stop=20)
refb &lt;- ranef(b$lme,level=6)
rownames(refb) &lt;- substr(rownames(refb),start=9,stop=20)

## make a prediction, with random effects zero...
p0 &lt;- predict(b$gam,data.frame(x0=.3,x1=.6,x2=.98,x3=.77))

## add in effect for fa = "2" and fb="2/4"...
p &lt;- p0 + refa["2",1] + refb["2/4",1] 

## and a "spatial" example...
library(nlme);set.seed(1);n &lt;- 100
dat &lt;- gamSim(2,n=n,scale=0) ## standard example
attach(dat)
old.par&lt;-par(mfrow=c(2,2))
contour(truth$x,truth$z,truth$f)  ## true function
f &lt;- data$f                       ## true expected response
## Now simulate correlated errors...
cstr &lt;- corGaus(.1,form = ~x+z)  
cstr &lt;- Initialize(cstr,data.frame(x=data$x,z=data$z))
V &lt;- corMatrix(cstr) ## correlation matrix for data
Cv &lt;- chol(V)
e &lt;- t(Cv) %*% rnorm(n)*0.05 # correlated errors
## next add correlated simulated errors to expected values
data$y &lt;- f + e ## ... to produce response
b&lt;- gamm(y~s(x,z,k=50),correlation=corGaus(.1,form=~x+z),
         data=data)
plot(b$gam) # gamm fit accounting for correlation
# overfits when correlation ignored.....  
b1 &lt;- gamm(y~s(x,z,k=50),data=data);plot(b1$gam) 
b2 &lt;- gam(y~s(x,z,k=50),data=data);plot(b2)
par(old.par)

</code></pre>

<hr>
<h2 id='gammals'>Gamma location-scale model family</h2><span id='topic+gammals'></span>

<h3>Description</h3>

<p>The <code>gammals</code> family implements gamma location scale additive models in which 
the log of the mean and the log of the scale parameter (see details) can depend on additive smooth predictors. Useable only with <code><a href="#topic+gam">gam</a></code>, the linear predictors are specified via a list of formulae.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gammals(link=list("identity","log"),b=-7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gammals_+3A_link">link</code></td>
<td>
<p>two item list specifying the link for the mean and the standard deviation. See details for meaning which may not be intuitive.</p>
</td></tr>
<tr><td><code id="gammals_+3A_b">b</code></td>
<td>
<p>The minumum log scale parameter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Used with <code><a href="#topic+gam">gam</a></code> to fit gamma location - scale models parameterized in terms of the log mean and the log scale parameter (the response variance is the squared mean multiplied by the scale parameter). Note that <code>identity</code> links mean that the linear predictors give the log mean and log scale directly. By default the <code>log</code> link for the scale parameter simply forces the log scale parameter to have a lower limit given by argument <code>b</code>: if <code class="reqn">\eta</code> is the linear predictor for the log scale parameter, <code class="reqn">\phi</code>, then <code class="reqn">\log \phi = b + \log(1+e^\eta)</code>. 
</p>
<p><code>gam</code> is called with 
a list containing 2 formulae, the first specifies the response on the left hand side and the structure of the linear predictor for the log mean on the right hand side. The second is one sided, specifying the linear predictor for the log scale on the right hand side. 
</p>
<p>The fitted values for this family will be a two column matrix. The first column is the mean (on original, not log, scale), and the second column is the log scale. Predictions using <code><a href="#topic+predict.gam">predict.gam</a></code> will also produce 2 column matrices for <code>type</code> <code>"link"</code> and <code>"response"</code>. The first column is on the original data scale when <code>type="response"</code> and on the log mean scale of the linear predictor when <code>type="link"</code>. The second column when <code>type="response"</code> is again the log scale parameter, but is on the linear predictor when <code>type="link"</code>.
</p>
<p>The null deviance reported for this family computed by setting the fitted values to the mean response, but using the model estimated scale. 
</p>


<h3>Value</h3>

<p>An object inheriting from class <code>general.family</code>.
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## simulate some data
f0 &lt;- function(x) 2 * sin(pi * x)
f1 &lt;- function(x) exp(2 * x)
f2 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
            (10 * x)^3 * (1 - x)^10
f3 &lt;- function(x) 0 * x
n &lt;- 400;set.seed(9)
x0 &lt;- runif(n);x1 &lt;- runif(n);
x2 &lt;- runif(n);x3 &lt;- runif(n);
mu &lt;- exp((f0(x0)+f2(x2))/5)
th &lt;- exp(f1(x1)/2-2)
y &lt;- rgamma(n,shape=1/th,scale=mu*th)

b1 &lt;- gam(list(y~s(x0)+s(x2),~s(x1)+s(x3)),family=gammals)
plot(b1,pages=1)
summary(b1)
gam.check(b1)
plot(mu,fitted(b1)[,1]);abline(0,1,col=2)
plot(log(th),fitted(b1)[,2]);abline(0,1,col=2)

</code></pre>

<hr>
<h2 id='gamObject'>Fitted gam object</h2><span id='topic+gamObject'></span>

<h3>Description</h3>

<p>A fitted GAM object returned by function <code>gam</code> and of class
<code>"gam"</code> inheriting from classes <code>"glm"</code> and <code>"lm"</code>. Method
functions <code>anova</code>, <code>logLik</code>, <code>influence</code>, <code>plot</code>,
<code>predict</code>, <code>print</code>, <code>residuals</code> and <code>summary</code> exist for
this class.
</p>
<p>All compulsory elements of <code>"glm"</code> and <code>"lm"</code> objects are present,
but the fitting method for a GAM is different to a linear model or GLM, so
that the elements relating to the QR decomposition of the model matrix are
absent.
</p>


<h3>Value</h3>

<p> A <code>gam</code> object has the following elements:
</p>
<table>
<tr><td><code>aic</code></td>
<td>
<p>AIC of the fitted model: bear in mind that the degrees of freedom
used to calculate this are the effective degrees of freedom of the model, and
the likelihood is evaluated at the maximum of the penalized likelihood in most
cases, not at the MLE.</p>
</td></tr>
<tr><td><code>assign</code></td>
<td>
<p>Array whose elements indicate which model term (listed in
<code>pterms</code>) each parameter relates to: applies only to non-smooth terms.</p>
</td></tr>
<tr><td><code>boundary</code></td>
<td>
<p>did parameters end up at boundary of parameter space?</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>the matched call (allows <code>update</code> to be used with <code>gam</code> objects, for example). </p>
</td></tr>
<tr><td><code>cmX</code></td>
<td>
<p>column means of the model matrix (with elements corresponding to smooths set to zero )
&mdash; useful for componentwise CI calculation.</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>the coefficients of the fitted model. Parametric
coefficients are  first, followed  by coefficients for each
spline term in turn.</p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>the <code>gam</code> control list used in the fit.</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>indicates whether or not the iterative fitting method converged.</p>
</td></tr> 
<tr><td><code>data</code></td>
<td>
<p>the original supplied data argument (for class <code>"glm"</code> compatibility). 
Only included if <code><a href="#topic+gam">gam</a></code> <code>control</code> argument element 
<code>keepData</code> is set to <code>TRUE</code> (default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code>db.drho</code></td>
<td>
<p>matrix of first derivatives of model coefficients w.r.t. log smoothing parameters.</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>model deviance (not penalized deviance).</p>
</td></tr>
<tr><td><code>df.null</code></td>
<td>
<p>null degrees of freedom.</p>
</td></tr> 
<tr><td><code>df.residual</code></td>
<td>
<p>effective residual degrees of freedom of the model.</p>
</td></tr>
<tr><td><code>edf</code></td>
<td>
<p>estimated degrees of freedom for each model parameter. Penalization
means that many of these are less than 1.</p>
</td></tr>
<tr><td><code>edf1</code></td>
<td>
<p>similar, but using alternative estimate of EDF. Useful for testing.</p>
</td></tr>
<tr><td><code>edf2</code></td>
<td>
<p>if estimation is by ML or REML then an edf that accounts for smoothing parameter
uncertainty can be computed, this is it. <code>edf1</code> is a heuristic upper bound for <code>edf2</code>.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>family object specifying distribution and link used.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>fitted model predictions of expected value for each
datum.</p>
</td></tr>
<tr><td><code>formula</code></td>
<td>
<p>the model formula.</p>
</td></tr>
<tr><td><code>full.sp</code></td>
<td>
<p>full array of smoothing parameters multiplying penalties (excluding any contribution 
from <code>min.sp</code> argument to <code>gam</code>). May be larger than <code>sp</code> if some terms share 
smoothing parameters, and/or some smoothing parameter values were supplied in the <code>sp</code> argument
of <code><a href="#topic+gam">gam</a></code>.</p>
</td></tr>
<tr><td><code>F</code></td>
<td>
<p>Degrees of freedom matrix. This may be removed at some point, and should probably not be used.</p>
</td></tr>
<tr><td><code>gcv.ubre</code></td>
<td>
<p>The minimized smoothing parameter selection score: GCV, UBRE(AIC), GACV, negative log marginal 
likelihood or negative log restricted likelihood.</p>
</td></tr>
<tr><td><code>hat</code></td>
<td>
<p>array of elements from the leading diagonal of the &lsquo;hat&rsquo; (or &lsquo;influence&rsquo;) matrix. 
Same length as response data vector.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of iterations of P-IRLS taken to get convergence.</p>
</td></tr>
<tr><td><code>linear.predictors</code></td>
<td>
<p>fitted model prediction of link function of
expected value for  each datum.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>One of <code>"GCV"</code> or <code>"UBRE"</code>, <code>"REML"</code>, <code>"P-REML"</code>, <code>"ML"</code>,
<code>"P-ML"</code>, <code>"PQL"</code>, <code>"lme.ML"</code> or <code>"lme.REML"</code>, depending on the fitting
criterion used.</p>
</td></tr>
<tr><td><code>mgcv.conv</code></td>
<td>
<p> A list of convergence diagnostics relating to the
<code>"magic"</code> parts of smoothing parameter estimation - this will not be very meaningful for pure <code>"outer"</code>
estimation of smoothing parameters. The items are: <code>full.rank</code>, The apparent rank of the problem given the model matrix and 
constraints; <code>rank</code>, The numerical rank of the problem;
<code>fully.converged</code>, <code>TRUE</code> is multiple GCV/UBRE converged by meeting 
convergence criteria and <code>FALSE</code> if method stopped with a steepest descent step 
failure; <code>hess.pos.def</code>Was the hessian of the GCV/UBRE score positive definite at 
smoothing parameter estimation convergence?; <code>iter</code> How many iterations were required to find the smoothing parameters?
<code>score.calls</code>, and how many times did the GCV/UBRE score have to be
evaluated?; <code>rms.grad</code>, root mean square of the gradient of the GCV/UBRE score at 
convergence.
</p>
</td></tr> 
<tr><td><code>min.edf</code></td>
<td>
<p>Minimum possible degrees of freedom for whole model.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>model frame containing all variables needed in original model fit.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>The <code><a href="stats.html#topic+na.action">na.action</a></code> used in fitting.</p>
</td></tr>
<tr><td><code>nsdf</code></td>
<td>
<p>number of parametric, non-smooth, model terms including the
intercept.</p>
</td></tr>
<tr><td><code>null.deviance</code></td>
<td>
<p>deviance for single parameter model.</p>
</td></tr> 
<tr><td><code>offset</code></td>
<td>
<p>model offset.</p>
</td></tr>
<tr><td><code>optimizer</code></td>
<td>
<p><code>optimizer</code> argument to <code><a href="#topic+gam">gam</a></code>, or <code>"magic"</code> if it's a pure 
additive model.</p>
</td></tr>
<tr><td><code>outer.info</code></td>
<td>
<p>If &lsquo;outer&rsquo; iteration has been used to fit the model (see
<code><a href="#topic+gam">gam</a></code> argument <code>optimizer</code>) then this is present and contains whatever was
returned by the optimization routine used (currently <code><a href="stats.html#topic+nlm">nlm</a></code> or <code><a href="stats.html#topic+optim">optim</a></code>). </p>
</td></tr>
<tr><td><code>paraPen</code></td>
<td>
<p>If the <code>paraPen</code> argument to <code><a href="#topic+gam">gam</a></code> was used then this provides
information on the parametric penalties. <code>NULL</code> otherwise.</p>
</td></tr>
<tr><td><code>pred.formula</code></td>
<td>
<p>one sided formula containing variables needed for prediction, used by <code>predict.gam</code></p>
</td></tr>
<tr><td><code>prior.weights</code></td>
<td>
<p>prior weights on observations.</p>
</td></tr> 
<tr><td><code>pterms</code></td>
<td>
<p><code>terms</code> object for strictly parametric part of model.</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>Factor R from QR decomposition of weighted model matrix, unpivoted to be in
same column order as model matrix (so need not be upper triangular).</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>apparent rank of fitted model.</p>
</td></tr>
<tr><td><code>reml.scale</code></td>
<td>
<p>The scale (RE)ML scale parameter estimate, if (P-)(RE)ML used 
for smoothness estimation. </p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the working residuals for the fitted model.</p>
</td></tr>
<tr><td><code>rV</code></td>
<td>
<p>If present, <code>rV%*%t(rV)*sig2</code> gives the estimated Bayesian covariance matrix.</p>
</td></tr> 
<tr><td><code>scale</code></td>
<td>
<p>when present, the scale (as <code>sig2</code>)</p>
</td></tr>
<tr><td><code>scale.estimated</code></td>
<td>
 <p><code>TRUE</code> if the scale parameter was estimated, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code>sig2</code></td>
<td>
<p>estimated or supplied variance/scale parameter.</p>
</td></tr>
<tr><td><code>smooth</code></td>
<td>
<p>list of smooth objects, containing the basis information for each term in the 
model formula in the order in which they appear. These smooth objects are what gets returned by
the <code><a href="#topic+smooth.construct">smooth.construct</a></code> objects.</p>
</td></tr>
<tr><td><code>sp</code></td>
<td>
<p>estimated smoothing parameters for the model. These are the underlying smoothing
parameters, subject to optimization. For the full set of smoothing parameters multiplying the 
penalties see <code>full.sp</code>. Divide the scale parameter by the smoothing parameters to get,
variance components, but note that this is not valid for smooths that have used rescaling to 
improve conditioning.</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p><code>terms</code> object of <code>model</code> model frame.</p>
</td></tr>
<tr><td><code>var.summary</code></td>
<td>
<p>A named list of summary information on the predictor variables. If
a parametric variable is a matrix, then the summary is a one row matrix, containing the 
observed data value closest to the column median, for each matrix column. If the variable 
is a factor the then summary is the modal factor level, returned as a factor, with levels 
corresponding to those of the data. For numerics and matrix arguments of smooths, the summary 
is the mean, nearest observed value to median and maximum, as a numeric vector. Used by 
<code><a href="#topic+vis.gam">vis.gam</a></code>, in particular. </p>
</td></tr>
<tr><td><code>Ve</code></td>
<td>
<p>frequentist estimated covariance matrix for the parameter
estimators. Particularly useful for testing whether terms are zero. Not so
useful for CI's as smooths are usually biased.</p>
</td></tr>
<tr><td><code>Vp</code></td>
<td>
<p>estimated covariance matrix for the parameters. This is a Bayesian
posterior covariance matrix that results from adopting a particular Bayesian
model of the smoothing process. Paricularly useful for creating
credible/confidence intervals.</p>
</td></tr>
<tr><td><code>Vc</code></td>
<td>
<p>Under ML or REML smoothing parameter estimation it is possible to correct the covariance
matrix <code>Vp</code> for smoothing parameter uncertainty. This is the corrected version.
</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>final weights used in IRLS iteration.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>response data.</p>
</td></tr>
</table>


<h3>WARNINGS </h3>

<p> This model object is different to that described in
Chambers and Hastie (1993) in order to allow smoothing parameter estimation etc.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>A Key Reference on this implementation:
</p>
<p>Wood, S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
&amp; Hall/ CRC, Boca Raton, Florida
</p>
<p>Key Reference on GAMs generally:
</p>
<p>Hastie (1993) in Chambers and Hastie (1993) Statistical Models in S. Chapman
and Hall.
</p>
<p>Hastie and Tibshirani (1990) Generalized Additive Models. Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam">gam</a></code></p>

<hr>
<h2 id='gamSim'>Simulate example data for GAMs</h2><span id='topic+gamSim'></span>

<h3>Description</h3>

<p> Function used to simulate data sets to illustrate the use of 
<code><a href="#topic+gam">gam</a></code> and <code><a href="#topic+gamm">gamm</a></code>. Mostly used in help files to keep down 
the length of the example code sections.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gamSim(eg=1,n=400,dist="normal",scale=2,verbose=TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gamSim_+3A_eg">eg</code></td>
<td>
<p> numeric value specifying the example required.</p>
</td></tr>
<tr><td><code id="gamSim_+3A_n">n</code></td>
<td>
<p> number of data to simulate.</p>
</td></tr>
<tr><td><code id="gamSim_+3A_dist">dist</code></td>
<td>
<p>character string which may be used to specify the distribution of
the response.</p>
</td></tr>
<tr><td><code id="gamSim_+3A_scale">scale</code></td>
<td>
<p>Used to set noise level.</p>
</td></tr>
<tr><td><code id="gamSim_+3A_verbose">verbose</code></td>
<td>
<p>Should information about simulation type be printed?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the source code for exactly what is simulated in each case. 
</p>

<ol>
<li><p>Gu and Wahba 4 univariate term example.
</p>
</li>
<li><p>A smooth function of 2 variables.
</p>
</li>
<li><p>Example with continuous by variable.
</p>
</li>
<li><p>Example with factor by variable.
</p>
</li>
<li><p>An additive example plus a factor variable.
</p>
</li>
<li><p>Additive + random effect.
</p>
</li>
<li><p>As 1 but with correlated covariates.
</p>
</li></ol>



<h3>Value</h3>

<p> Depends on <code>eg</code>, but usually a dataframe, which may also contain some information on 
the underlying truth. Sometimes a list with more items, including a data frame for model fitting.
See source code or helpfile examples where the function is used for further information.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>See Also</h3>

 <p><code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+gamm">gamm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## see ?gam
</code></pre>

<hr>
<h2 id='gaulss'>Gaussian location-scale model family</h2><span id='topic+gaulss'></span>

<h3>Description</h3>

<p>The <code>gaulss</code> family implements Gaussian location scale additive models in which 
the mean and the logb of the standard deviation (see details) can depend on additive smooth predictors. Useable 
only with <code><a href="#topic+gam">gam</a></code>, the linear predictors are specified via a list of formulae.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaulss(link=list("identity","logb"),b=0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gaulss_+3A_link">link</code></td>
<td>
<p>two item list specifying the link for the mean and the standard deviation. See details.</p>
</td></tr>
<tr><td><code id="gaulss_+3A_b">b</code></td>
<td>
<p>The minumum standard deviation, for the <code>"logb"</code> link.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Used with <code><a href="#topic+gam">gam</a></code> to fit Gaussian location - scale models. <code>gam</code> is called with 
a list containing 2 formulae, the first specifies the response on the left hand side and the structure of the linear predictor for the mean on the right hand side. The second is one sided, specifying the linear predictor for the standard deviation on the right hand side. 
</p>
<p>Link functions <code>"identity"</code>, <code>"inverse"</code>, <code>"log"</code> and <code>"sqrt"</code> are available for the mean. For the standard deviation only the <code>"logb"</code> link is implemented: <code class="reqn">\eta = \log(\sigma - b)</code> and <code class="reqn">\sigma = b + \exp(\eta)</code>. This link is designed to avoid singularities in the likelihood caused by the standard deviation tending to zero. Note that internally the family is parameterized in terms of the <code class="reqn">\tau=\sigma^{-1}</code>, i.e. the standard deviation of the precision, so the link and inverse link are coded to reflect this, however the reltaionships between the linear predictor and the standard deviation are as given above. 
</p>
<p>The fitted values for this family will be a two column matrix. The first column is the mean, and the second column is the inverse of the standard deviation. Predictions using <code><a href="#topic+predict.gam">predict.gam</a></code> will also produce 2 column matrices for <code>type</code> <code>"link"</code> and <code>"response"</code>. The second column when <code>type="response"</code> is again on the reciprocal standard deviation scale (i.e. the square root precision scale). The second column when <code>type="link"</code> is <code class="reqn">\log(\sigma - b)</code>. Also <code><a href="#topic+plot.gam">plot.gam</a></code> will plot smooths relating to <code class="reqn">\sigma</code> on the <code class="reqn">\log(\sigma - b)</code> scale (so high values correspond to high standard deviation and low values to low standard deviation). Similarly the smoothing penalties are applied on the (log) standard deviation scale, not the log precision scale.
</p>
<p>The null deviance reported for this family is the sum of squares of the difference between the response and the mean response divided by the standard deviation of the response according to the model. The deviance is the sum of squares of residuals divided by model standard deviations. 
</p>


<h3>Value</h3>

<p>An object inheriting from class <code>general.family</code>.
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv);library(MASS)
b &lt;- gam(list(accel~s(times,k=20,bs="ad"),~s(times)),
            data=mcycle,family=gaulss())
summary(b) 
plot(b,pages=1,scale=0)
</code></pre>

<hr>
<h2 id='get.var'>Get named variable or evaluate expression from list or data.frame</h2><span id='topic+get.var'></span>

<h3>Description</h3>

<p> This routine takes a text string and a data frame or list. It first sees if the 
string is the name of a variable in the data frame/ list. If it is then the value of this variable is returned. 
Otherwise the routine tries to evaluate the expression within the data.frame/list (but nowhere else) and if 
successful returns the result. If neither step works then <code>NULL</code> is returned. The routine is useful for
processing gam formulae. If the variable is a matrix then it is coerced to a numeric vector, by default.</p>


<h3>Usage</h3>

<pre><code class='language-R'> get.var(txt,data,vecMat=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get.var_+3A_txt">txt</code></td>
<td>
<p>a text string which is either the name of a variable in <code>data</code> or when 
parsed is an expression that can be evaluated in <code>data</code>. It can also be neither in which case the
function returns <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="get.var_+3A_data">data</code></td>
<td>
<p>A data frame or list.</p>
</td></tr> 
<tr><td><code id="get.var_+3A_vecmat">vecMat</code></td>
<td>
<p>Should matrices be coerced to numeric vectors?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The evaluated variable or <code>NULL</code>. May be coerced to a numeric vector if it's a matrix.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> </p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+gam">gam</a> </code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
y &lt;- 1:4;dat&lt;-data.frame(x=5:10)
get.var("x",dat)
get.var("y",dat)
get.var("x==6",dat)
dat &lt;- list(X=matrix(1:6,3,2))
get.var("X",dat)
</code></pre>

<hr>
<h2 id='gevlss'>Generalized Extreme Value location-scale model family</h2><span id='topic+gevlss'></span>

<h3>Description</h3>

<p>The <code>gevlss</code> family implements Generalized Extreme Value location scale additive models in which the location, scale and shape parameters depend on additive smooth predictors. Usable 
only with <code><a href="#topic+gam">gam</a></code>, the linear predictors are specified via a list of formulae.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gevlss(link=list("identity","identity","logit"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gevlss_+3A_link">link</code></td>
<td>
<p>three item list specifying the link for the location scale and shape parameters. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Used with <code><a href="#topic+gam">gam</a></code> to fit Generalized Extreme Value location scale and shape models. <code>gam</code> is called with a list containing 3 formulae: the first specifies the response on the left hand side and the structure of the linear predictor for the location parameter on the right hand side. The second is one sided, specifying the linear predictor for the log scale parameter on the right hand side. The third is one sided specifying the linear predictor for the shape parameter.
</p>
<p>Link functions <code>"identity"</code> and <code>"log"</code> are available for the location (mu) parameter. There is no choice of link for the log scale parameter (<code class="reqn">\rho = \log \sigma</code>). The shape parameter (xi) defaults to a modified logit link restricting its range to (-1,.5), the upper limit is required to ensure finite variance, while the lower limit ensures consistency of the MLE (Smith, 1985).
</p>
<p>The fitted values for this family will be a three column matrix. The first column is the location parameter, the second column is the log scale parameter, the third column is the shape parameter.
</p>
<p>This family does not produce a null deviance. Note that the distribution for <code class="reqn">\xi=0</code> is approximated by setting <code class="reqn">\xi</code> to a small number. 
</p>
<p>The derivative system code for this family is mostly auto-generated, and the family is still somewhat experimental.
</p>
<p>The GEV distribution is rather challenging numerically, and for small datasets or poorly fitting models improved numerical robustness may be obtained by using the extended Fellner-Schall method of Wood and Fasiolo (2017) for smoothing parameter estimation. See examples.
</p>


<h3>Value</h3>

<p>An object inheriting from class <code>general.family</code>.
</p>


<h3>References</h3>

<p>Smith, R.L. (1985) Maximum likelihood estimation in a class of
nonregular cases. Biometrika 72(1):67-90
</p>
<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>
<p>Wood, S.N. and M. Fasiolo (2017) A generalized Fellner-Schall method for smoothing parameter optimization with application to Tweedie location, scale and shape models. Biometrics 73(4): 1071-1081.
<a href="https://doi.org/10.1111/biom.12666">doi:10.1111/biom.12666</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
Fi.gev &lt;- function(z,mu,sigma,xi) {
## GEV inverse cdf.
  xi[abs(xi)&lt;1e-8] &lt;- 1e-8 ## approximate xi=0, by small xi
  x &lt;- mu + ((-log(z))^-xi-1)*sigma/xi
}

## simulate test data...
f0 &lt;- function(x) 2 * sin(pi * x)
f1 &lt;- function(x) exp(2 * x)
f2 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
            (10 * x)^3 * (1 - x)^10
set.seed(1)
n &lt;- 500
x0 &lt;- runif(n);x1 &lt;- runif(n);x2 &lt;- runif(n)
mu &lt;- f2(x2)
rho &lt;- f0(x0)
xi &lt;- (f1(x1)-4)/9
y &lt;- Fi.gev(runif(n),mu,exp(rho),xi)
dat &lt;- data.frame(y,x0,x1,x2);pairs(dat)

## fit model....
b &lt;- gam(list(y~s(x2),~s(x0),~s(x1)),family=gevlss,data=dat)

## same fit using the extended Fellner-Schall method which
## can provide improved numerical robustness... 
b &lt;- gam(list(y~s(x2),~s(x0),~s(x1)),family=gevlss,data=dat,
         optimizer="efs")

## plot and look at residuals...
plot(b,pages=1,scale=0)
summary(b)

par(mfrow=c(2,2))
mu &lt;- fitted(b)[,1];rho &lt;- fitted(b)[,2]
xi &lt;- fitted(b)[,3]
## Get the predicted expected response... 
fv &lt;- mu + exp(rho)*(gamma(1-xi)-1)/xi
rsd &lt;- residuals(b)
plot(fv,rsd);qqnorm(rsd)
plot(fv,residuals(b,"pearson"))
plot(fv,residuals(b,"response"))

</code></pre>

<hr>
<h2 id='gfam'>Grouped families</h2><span id='topic+gfam'></span><span id='topic+grouped+20families'></span>

<h3>Description</h3>

<p>Family for use with <code><a href="#topic+gam">gam</a></code> or <code><a href="#topic+bam">bam</a></code> allowing a univariate response vector to be made up of variables from several different distributions. The response variable is supplied as a 2 column matrix, where the first column contains the response observations and the second column indexes the distribution (family) from which it comes. <code>gfam</code> takes a list of families as its single argument.
</p>
<p>Useful for modelling data from different sources that are linked by a model sharing some components. Smooth model components that are not shared are usually handled with <code>by</code> variables (see <code><a href="#topic+gam.models">gam.models</a></code>).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gfam(fl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gfam_+3A_fl">fl</code></td>
<td>
<p>A list of families. These can be any families inheriting from <code>family</code> or <code>extended.family</code> usable with <code>gam</code>, provided that they do not usually require a matrix response variable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each component function of <code>gfam</code> uses the families supplied in the list <code>fl</code> to obtain the required quantities for that family's subset of data, and combines the results appropriately. For example it provides the total deviance (twice negative log-likelihood) of the model, along with its derivatives, by computing the family specific deviance and derivatives from each family applied to its subset of data, and summing them. Other quantities are computed in the same way.
</p>
<p>Regular exponential families do not compute the same quantities as extended families, so <code>gfam</code> converts what these families produce to <code>extended.family</code> form internally.
</p>
<p>Scale parameters obviously have to be handled separately for each family, and treated as parameters to be estimated, just like other <code>extended.family</code> non-location distribution parameters. Again this is handled internally. This requirement is part of the reason that an <code>extended.family</code> is always produced, even if all elements of <code>fl</code> are standard exponential families. In consequence smoothing parameter estimation is always by REML or NCV. 
</p>
<p>Note that the null deviance is currently computed by assuming a single parameter model for each family, rather than just one parameter, which may slightly lower explained deviances. Note also that residual checking should probably be done by disaggregating the residuals by family. For this reason functions are not provided to facilitate residual checking with <code><a href="#topic+qq.gam">qq.gam</a></code>. 
</p>
<p>Prediction on the response scale requires that a family index vector is supplied, with the name of the response, as part of the new prediction data. However, families such as <code><a href="#topic+ocat">ocat</a></code> which usually produce matrix predictions for prediction type <code>"response"</code>, will not be able to do so when part of <code>gfam</code>. 
</p>
<p><code>gfam</code> relies on the methods in Wood, Pya and Saefken (2016).
</p>


<h3>Value</h3>

<p>An object of class <code>extended.family</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## a mixed family simulator function to play with...
sim.gfam &lt;- function(dist,n=400) {
## dist can be norm, pois, gamma, binom, nbinom, tw, ocat (R assumed 4)
## links used are identitiy, log or logit.
  dat &lt;- gamSim(1,n=n,verbose=FALSE)
  nf &lt;- length(dist) ## how many families
  fin &lt;- c(1:nf,sample(1:nf,n-nf,replace=TRUE)) ## family index
  dat[,6:10] &lt;- dat[,6:10]/5 ## a scale that works for all links used
  y &lt;- dat$y;
  for (i in 1:nf) {
    ii &lt;- which(fin==i) ## index of current family
    ni &lt;- length(ii);fi &lt;- dat$f[ii]
    if (dist[i]=="norm") {
      y[ii] &lt;- fi + rnorm(ni)*.5
    } else if (dist[i]=="pois") {
      y[ii] &lt;- rpois(ni,exp(fi))
    } else if (dist[i]=="gamma") {
      scale &lt;- .5
      y[ii] &lt;- rgamma(ni,shape=1/scale,scale=exp(fi)*scale)
    } else if (dist[i]=="binom") {
      y[ii] &lt;- rbinom(ni,1,binomial()$linkinv(fi))
    } else if (dist[i]=="nbinom") {
      y[ii] &lt;- rnbinom(ni,size=3,mu=exp(fi))
    } else if (dist[i]=="tw") {
      y[ii] &lt;- rTweedie(exp(fi),p=1.5,phi=1.5)
    } else if (dist[i]=="ocat") {
      alpha &lt;- c(-Inf,1,2,2.5,Inf)
      R &lt;- length(alpha)-1
      yi &lt;- fi
      u &lt;- runif(ni)
      u &lt;- yi + log(u/(1-u)) 
      for (j in 1:R) {
        yi[u &gt; alpha[j]&amp;u &lt;= alpha[j+1]] &lt;- j
      }
      y[ii] &lt;- yi
    }
  }
  dat$y &lt;- cbind(y,fin)
  dat
} ## sim.gfam

## some examples

dat &lt;- sim.gfam(c("binom","tw","norm"))
b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),
         family=gfam(list(binomial,tw,gaussian)),data=dat)
predict(b,data.frame(y=1:3,x0=c(.5,.5,.5),x1=c(.3,.2,.3),
        x2=c(.2,.5,.8),x3=c(.1,.5,.9)),type="response",se=TRUE)
summary(b)
plot(b,pages=1)

## set up model so that only the binomial observations depend
## on x0...

dat$id1 &lt;- as.numeric(dat$y[,2]==1)
b1 &lt;- gam(y~s(x0,by=id1)+s(x1)+s(x2)+s(x3),
         family=gfam(list(binomial,tw,gaussian)),data=dat)
plot(b1,pages=1) ## note the CI width increase	 
</code></pre>

<hr>
<h2 id='ginla'>GAM Integrated Nested Laplace Approximation Newton Enhanced</h2><span id='topic+ginla'></span>

<h3>Description</h3>

<p>Apply Integrated Nested Laplace Approximation (INLA, Rue et al. 2009) to models estimable by <code><a href="#topic+gam">gam</a></code> or <code><a href="#topic+bam">bam</a></code>, using the INLA variant described in Wood (2019). Produces marginal posterior densities for each coefficient, selected coefficients or linear transformations of the coefficient vector. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ginla(G,A=NULL,nk=16,nb=100,J=1,interactive=FALSE,integ=0,approx=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ginla_+3A_g">G</code></td>
<td>
<p>A pre-fit gam object, as produced by <code>gam(...,fit=FALSE)</code> or <code>bam(...,discrete=TRUE,fit=FALSE)</code>.</p>
</td></tr>
<tr><td><code id="ginla_+3A_a">A</code></td>
<td>
<p>Either a matrix whose rows are transforms of the coefficients that are of interest (no more rows than columns, full row rank), or an array of indices of the parameters of interest. If <code>NULL</code> then distributions are produced for all coefficients.</p>
</td></tr>
<tr><td><code id="ginla_+3A_nk">nk</code></td>
<td>
<p>Number of values of each coefficient at which to evaluate its log marginal posterior density. These points are then spline interpolated.</p>
</td></tr>
<tr><td><code id="ginla_+3A_nb">nb</code></td>
<td>
<p>Number of points at which to evaluate posterior density of coefficients for returning as a gridded function.</p>
</td></tr>
<tr><td><code id="ginla_+3A_j">J</code></td>
<td>
<p>How many determinant updating steps to take in the log determinant approximation step. Not recommended to increase this. </p>
</td></tr>
<tr><td><code id="ginla_+3A_interactive">interactive</code></td>
<td>
<p>If this is <code>&gt;0</code> or <code>TRUE</code> then every approximate posterior is plotted in red, overlaid on the simple Gaussian approximate posterior. If <code>2</code> then waits for user to press return between each plot. Useful for judging whether anything is gained by using INLA approach. </p>
</td></tr>
<tr><td><code id="ginla_+3A_integ">integ</code></td>
<td>
<p>0 to skip integration and just use the posterior modal smoothing parameter. &gt;0 for integration using the CCD approach proposed in Rue et al. (2009).</p>
</td></tr>
<tr><td><code id="ginla_+3A_approx">approx</code></td>
<td>
<p>0 for full approximation; 1 to update Hessian, but use approximate modes; 2 as 1 and assume constant Hessian. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">\beta</code>, <code class="reqn">\theta</code> and <code class="reqn">y</code> denote the model coefficients, hyperparameters/smoothing parameters and response data, respectively. In principle, INLA employs Laplace approximations for <code class="reqn">\pi(\beta_i|\theta,y)</code> and <code class="reqn">\pi(\theta|y)</code> and then obtains the marginal posterior distribution <code class="reqn">\pi(\beta_i|y)</code> by intergrating the approximations to <code class="reqn">\pi(\beta_i|\theta,y)\pi(\theta|y)</code> w.r.t <code class="reqn">\theta</code> (marginals for the hyperparameters can also be produced). In practice the Laplace approximation for <code class="reqn">\pi(\beta_i|\theta,y)</code> is too expensive to compute for each <code class="reqn">\beta_i</code> and must itself be approximated. To this end, there are two quantities that have to be computed: the posterior mode <code class="reqn">\beta^*|\beta_i</code> and the determinant of the Hessian of the joint log density <code class="reqn">\log \pi(\beta,\theta,y)</code> w.r.t. <code class="reqn">\beta</code> at the mode. Rue et al. (2009) originally approximated the posterior conditional mode by the conditional mode implied by a simple Gaussian approximation to the posterior <code class="reqn">\pi(\beta|y)</code>. They then approximated the log determinant of the Hessian as a function of <code class="reqn">\beta_i</code> using a first order Taylor expansion, which is cheap to compute for the sparse model representaiton that they use, but not when using the dense low rank basis expansions used by <code><a href="#topic+gam">gam</a></code>. They also offer a more expensive alternative approximation based on computing the log determiannt with respect only to those elements of <code class="reqn">\beta</code> with sufficiently high correlation with <code class="reqn">\beta_i</code> according to the simple Gaussian posterior approximation: efficiency again seems to rest on sparsity. Wood (2020) suggests computing the required posterior modes exactly, and basing the log determinant approximation on a BFGS update of the Hessian at the unconditional model. The latter is efficient with or without sparsity, whereas the former is a &lsquo;for free&rsquo; improvement. Both steps are efficient because it is cheap to obtain the Cholesky factor of <code class="reqn">H[-i,-i]</code> from that of <code class="reqn">H</code> - see <code><a href="#topic+choldrop">choldrop</a></code>. This is the approach taken by this routine.
</p>
<p>The <code>approx</code> argument allows two further approximations to speed up computations. For <code>approx==1</code> the exact posterior conditional modes are not used, but instead the conditional modes implied by the simple Gaussian posterior approximation. For <code>approx==2</code> the same approximation is used for the modes and the Hessian is assumed constant. The latter is quite fast as no log joint density gradient evaluations are required.  
</p>
<p>Note that for many models the INLA estimates are very close to the usual Gaussian approximation to the posterior, the <code>interactive</code> argument is useful for investigating this issue.
</p>
<p><code><a href="#topic+bam">bam</a></code> models are only supported with the <code>disrete=TRUE</code> option. The <code>discrete=FALSE</code> approach would be too inefficient. AR1 models are not supported (related arguments are simply ignored). 
</p>


<h3>Value</h3>

<p>A list with elements <code>beta</code> and <code>density</code>, both of which are matrices. Each row relates to one coefficient (or linear coefficient combination) of interest. Both matrices have <code>nb</code> columns. If <code>int!=0</code> then a further element <code>reml</code> gives the integration weights used in the CCD integration, with the central point weight given first.  
</p>


<h3>WARNINGS</h3>

<p>This routine is still somewhat experimental, so details are liable to change. Also currently not all steps are optimally efficient.
</p>
<p>The routine is written for relatively expert users.
</p>
<p><code>ginla</code> is not designed to deal with rank deficient models.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Rue, H, Martino, S. &amp; Chopin, N. (2009) Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations (with discussion). Journal of the Royal Statistical Society, Series B.  71: 319-392.
</p>
<p>Wood (2020) Simplified Integrated Laplace Approximation. Biometrika 107(1): 223-230. [Note: There is an error in the theorem proof - theoretical properties are weaker than claimed - under investigation]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(mgcv); require(MASS)

  ## example using a scale location model for the motorcycle data. A simple
  ## plotting routine is produced first...

  plot.inla &lt;- function(x,inla,k=1,levels=c(.025,.1,.5,.9,.975),
               lcol = c(2,4,4,4,2),lwd = c(1,1,2,1,1),lty=c(1,1,1,1,1),
	       xlab="x",ylab="y",cex.lab=1.5) {
    ## a simple effect plotter, when distributions of function values of
    ## 1D smooths have been computed
    require(splines)
    p &lt;- length(x) 
    betaq &lt;- matrix(0,length(levels),p) ## storage for beta quantiles 
    for (i in 1:p) { ## work through x and betas
      j &lt;- i + k - 1
      p &lt;- cumsum(inla$density[j,])*(inla$beta[j,2]-inla$beta[j,1])
      ## getting quantiles of function values...
      betaq[,i] &lt;- approx(p,y=inla$beta[j,],levels)$y
    }
    xg &lt;- seq(min(x),max(x),length=200)
    ylim &lt;- range(betaq)
    ylim &lt;- 1.1*(ylim-mean(ylim))+mean(ylim)
    for (j in 1:length(levels)) { ## plot the quantiles
      din &lt;- interpSpline(x,betaq[j,])
      if (j==1) {
        plot(xg,predict(din,xg)$y,ylim=ylim,type="l",col=lcol[j],
             xlab=xlab,ylab=ylab,lwd=lwd[j],cex.lab=1.5,lty=lty[j])
      } else lines(xg,predict(din,xg)$y,col=lcol[j],lwd=lwd[j],lty=lty[j])
    }
  } ## plot.inla

  ## set up the model with a `gam' call...

  G &lt;- gam(list(accel~s(times,k=20,bs="ad"),~s(times)),
            data=mcycle,family=gaulss(),fit=FALSE)
  b &lt;- gam(G=G,method="REML") ## regular GAM fit for comparison

  ## Now use ginla to get posteriors of estimated effect values
  ## at evenly spaced times. Create A matrix for this...
  
  rat &lt;- range(mcycle$times)
  pd0 &lt;- data.frame(times=seq(rat[1],rat[2],length=20))
  X0 &lt;- predict(b,newdata=pd0,type="lpmatrix")
  X0[,21:30] &lt;- 0
  pd1 &lt;- data.frame(times=seq(rat[1],rat[2],length=10))
  X1 &lt;- predict(b,newdata=pd1,type="lpmatrix")
  X1[,1:20] &lt;- 0
  A &lt;- rbind(X0,X1) ## A maps coefs to required function values

  ## call ginla. Set integ to 1 for integrated version.
  ## Set interactive = 1 or 2 to plot marginal posterior distributions
  ## (red) and simple Gaussian approximation (black).
 
  inla &lt;- ginla(G,A,integ=0)

  par(mfrow=c(1,2),mar=c(5,5,1,1))
  fv &lt;- predict(b,se=TRUE) ## usual Gaussian approximation, for comparison

  ## plot inla mean smooth effect...
  plot.inla(pd0$times,inla,k=1,xlab="time",ylab=expression(f[1](time))) 

  ## overlay simple Gaussian equivalent (in grey) ...
  points(mcycle$times,mcycle$accel,col="grey")
  lines(mcycle$times,fv$fit[,1],col="grey",lwd=2)
  lines(mcycle$times,fv$fit[,1]+2*fv$se.fit[,1],lty=2,col="grey",lwd=2)
  lines(mcycle$times,fv$fit[,1]-2*fv$se.fit[,1],lty=2,col="grey",lwd=2)

  ## same for log sd smooth...
  plot.inla(pd1$times,inla,k=21,xlab="time",ylab=expression(f[2](time)))
  lines(mcycle$times,fv$fit[,2],col="grey",lwd=2)
  lines(mcycle$times,fv$fit[,2]+2*fv$se.fit[,2],col="grey",lty=2,lwd=2)
  lines(mcycle$times,fv$fit[,2]-2*fv$se.fit[,2],col="grey",lty=2,lwd=2)

  ## ... notice some real differences for the log sd smooth, especially
  ## at the lower and upper ends of the time interval.
</code></pre>

<hr>
<h2 id='gumbls'>Gumbel location-scale model family</h2><span id='topic+gumbls'></span>

<h3>Description</h3>

<p>The <code>gumbls</code> family implements Gumbel location scale additive models in which the location and scale parameters (see details) can depend on additive smooth predictors. Useable only with <code><a href="#topic+gam">gam</a></code>, the linear predictors are specified via a list of formulae.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gumbls(link=list("identity","log"),b=-7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gumbls_+3A_link">link</code></td>
<td>
<p>two item list specifying the link for the location <code class="reqn">\mu</code> and log scale parameter <code class="reqn">\beta</code>. See details for meaning, which may not be intuitive.</p>
</td></tr>
<tr><td><code id="gumbls_+3A_b">b</code></td>
<td>
<p>The minumum log scale parameter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">z = (y-\mu) e^{-\beta}</code>, then the log Gumbel density is <code class="reqn">l = -\beta - z - e^{-z}</code>. The expected value of a Gumbel r.v. is <code class="reqn">\mu + \gamma e^{\beta}</code> where <code class="reqn">\gamma</code> is Eulers constant (about 0.57721566). The corresponding variance is <code class="reqn">\pi^2 e^{2\beta}/6</code>.
</p>
<p><code>gumbls</code> is used with <code><a href="#topic+gam">gam</a></code> to fit Gumbel location - scale models parameterized in terms of location parameter <code class="reqn">\mu</code> and the log scale parameter <code class="reqn">\beta</code>. Note that <code>identity</code> link for the scale parameter means that the corresponding linear predictor gives <code class="reqn">\beta</code> directly. By default the <code>log</code> link for the scale parameter simply forces the log scale parameter to have a lower limit given by argument <code>b</code>: if <code class="reqn">\eta</code> is the linear predictor for the log scale parameter, <code class="reqn">\beta</code>, then <code class="reqn">\beta = b + \log(1+e^\eta)</code>.
</p>
<p><code>gam</code> is called with 
a list containing 2 formulae, the first specifies the response on the left hand side and the structure of the linear predictor for location parameter, <code class="reqn">\mu</code>, on the right hand side. The second is one sided, specifying the linear predictor for the lg scale, <code class="reqn">\beta</code>, on the right hand side. 
</p>
<p>The fitted values for this family will be a two column matrix. The first column is the mean, and the second column is the log scale parameter, <code class="reqn">\beta</code>. Predictions using <code><a href="#topic+predict.gam">predict.gam</a></code> will also produce 2 column matrices for <code>type</code> <code>"link"</code> and <code>"response"</code>. The first column is on the original data scale when <code>type="response"</code> and on the log mean scale of the linear predictor when <code>type="link"</code>. The second column when <code>type="response"</code> is again the log scale parameter, but is on the linear predictor when <code>type="link"</code>.
</p>


<h3>Value</h3>

<p>An object inheriting from class <code>general.family</code>.
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## simulate some data
f0 &lt;- function(x) 2 * sin(pi * x)
f1 &lt;- function(x) exp(2 * x)
f2 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
            (10 * x)^3 * (1 - x)^10
n &lt;- 400;set.seed(9)
x0 &lt;- runif(n);x1 &lt;- runif(n);
x2 &lt;- runif(n);x3 &lt;- runif(n);
mu &lt;- f0(x0)+f1(x1)
beta &lt;- exp(f2(x2)/5)
y &lt;- mu - beta*log(-log(runif(n))) ## Gumbel quantile function

b &lt;- gam(list(y~s(x0)+s(x1),~s(x2)+s(x3)),family=gumbls)
plot(b,pages=1,scale=0)
summary(b)
gam.check(b)

</code></pre>

<hr>
<h2 id='identifiability'>Identifiability constraints</h2><span id='topic+identifiability'></span>

<h3>Description</h3>

<p>Smooth terms are generally only identifiable up to an additive constant. In consequence sum-to-zero identifiability constraints are imposed on most smooth terms. The exceptions are terms with <code>by</code> variables which cause the smooth to be identifiable without constraint (that doesn't include factor <code>by</code> variables), and random effect terms. Alternatively smooths can be set up to pass through zero at a user specified point.    
</p>


<h3>Details</h3>

<p> By default each smooth term is subject to the sum-to-zero constraint </p>
<p style="text-align: center;"><code class="reqn">\sum_i f(x_i) = 0.</code>
</p>

<p>The constraint is imposed by reparameterization. The sum-to-zero constraint causes the term to be orthogonal to the intercept: alternative constraints lead to wider confidence bands for the constrained smooth terms.
</p>
<p>No constraint is used for random effect terms, since the penalty (random effect covariance matrix) anyway ensures identifiability in this case. Also if a <code>by</code> variable means that the smooth is anyway identifiable, then no extra constraint is imposed. Constraints are imposed for factor <code>by</code> variables, so that the main effect of the factor must usually be explicitly added to the model (the example below is an exception). 
</p>
<p>Occasionally it is desirable to substitute the constraint that a particular smooth curve should pass through zero at a particular point: the <code>pc</code> argument to <code><a href="#topic+s">s</a></code>, <code><a href="#topic+te">te</a></code>, <code><a href="#topic+ti">ti</a></code> and  <code><a href="#topic+t2">t2</a></code> allows this: if specified then such constraints are always applied.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood (s.wood@r-project.org)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Example of three groups, each with a different smooth dependence on x
## but each starting at the same value...
require(mgcv)
set.seed(53)
n &lt;- 100;x &lt;- runif(3*n);z &lt;- runif(3*n)
fac &lt;- factor(rep(c("a","b","c"),each=100))
y &lt;- c(sin(x[1:100]*4),exp(3*x[101:200])/10-.1,exp(-10*(x[201:300]-.5))/
       (1+exp(-10*(x[201:300]-.5)))-0.9933071) + z*(1-z)*5 + rnorm(100)*.4

## 'pc' used to constrain smooths to 0 at x=0...
b &lt;- gam(y~s(x,by=fac,pc=0)+s(z)) 
plot(b,pages=1)
</code></pre>

<hr>
<h2 id='in.out'>Which of a set of points lie within a polygon defined region</h2><span id='topic+in.out'></span>

<h3>Description</h3>

<p>Tests whether each of a set of points lie within a region defined by one or more 
(possibly nested) polygons. Points count as &lsquo;inside&rsquo; if they are interior to an odd number of polygons.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>in.out(bnd,x)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="in.out_+3A_bnd">bnd</code></td>
<td>
<p>A two column matrix, the rows of which define the vertices of polygons defining the boundary of a region.
Different polygons should be separated by an <code>NA</code> row, and the polygons are assumed closed. Alternatively can be a lists where <code>bnd[[i]][[1]]</code>, <code>bnd[[i]][[2]]</code> defines the ith boundary loop.</p>
</td></tr>
<tr><td><code id="in.out_+3A_x">x</code></td>
<td>
<p>A two column matrix. Each row is a point to test for inclusion in the region defined by <code>bnd</code>. Can also be a 2-vector, defining a single point.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> The algorithm works by counting boundary crossings (using compiled C code).
</p>


<h3>Value</h3>

<p>A logical vector of length <code>nrow(x)</code>. <code>TRUE</code> if the corresponding row of <code>x</code> is inside 
the boundary and <code>FALSE</code> otherwise.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
data(columb.polys)
bnd &lt;- columb.polys[[2]]
plot(bnd,type="n")
polygon(bnd)
x &lt;- seq(7.9,8.7,length=20)
y &lt;- seq(13.7,14.3,length=20)
gr &lt;- as.matrix(expand.grid(x,y))
inside &lt;- in.out(bnd,gr)
points(gr,col=as.numeric(inside)+1)
</code></pre>

<hr>
<h2 id='influence.gam'>Extract the diagonal of the influence/hat matrix for a GAM </h2><span id='topic+influence.gam'></span>

<h3>Description</h3>

<p> Extracts the leading diagonal of the influence matrix (hat
matrix) of a fitted <code>gam</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
influence(model,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="influence.gam_+3A_model">model</code></td>
<td>
<p> fitted model objects of class <code>gam</code> as produced by <code>gam()</code>.</p>
</td></tr>
<tr><td><code id="influence.gam_+3A_...">...</code></td>
<td>
<p>un-used in this case</p>
</td></tr> 
</table>


<h3>Details</h3>

<p> Simply extracts <code>hat</code> array from fitted model. (More may follow!)
</p>


<h3>Value</h3>

<p> An array (see above).
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>See Also</h3>

  <p><code><a href="#topic+gam">gam</a></code></p>

<hr>
<h2 id='initial.sp'> Starting values for multiple smoothing parameter estimation</h2><span id='topic+initial.sp'></span>

<h3>Description</h3>

<p> Finds initial smoothing parameter guesses for multiple smoothing
parameter estimation. The idea is to find values such that the estimated
degrees of freedom per penalized parameter should be well away from 0 and 1
for each penalized parameter, thus ensuring that the values are in a region of
parameter space where the smoothing parameter estimation criterion is varying
substantially with smoothing parameter value. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initial.sp(X,S,off,expensive=FALSE,XX=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="initial.sp_+3A_x">X</code></td>
<td>
<p>is the model matrix.</p>
</td></tr>
<tr><td><code id="initial.sp_+3A_s">S</code></td>
<td>
<p> is a list of of penalty matrices. <code>S[[i]]</code> is the ith penalty matrix, but note
that it is not stored as a full matrix, but rather as the smallest square matrix including all 
the non-zero elements of the penalty matrix. Element 1,1 of <code>S[[i]]</code>  occupies 
element <code>off[i]</code>, <code>off[i]</code> of the ith penalty matrix. Each <code>S[[i]]</code> must be 
positive semi-definite.  </p>
</td></tr>
<tr><td><code id="initial.sp_+3A_off">off</code></td>
<td>
<p>is an array indicating the first parameter in the parameter vector that is 
penalized by the penalty involving <code>S[[i]]</code>.</p>
</td></tr>
<tr><td><code id="initial.sp_+3A_expensive">expensive</code></td>
<td>
<p>if <code>TRUE</code> then the overall amount of smoothing is
adjusted so that the average degrees of freedom per penalized parameter is
exactly 0.5: this is numerically costly. </p>
</td></tr>
<tr><td><code id="initial.sp_+3A_xx">XX</code></td>
<td>
<p>if <code>TRUE</code> then <code>X</code> contains <code class="reqn">X^TX</code>, rather than <code class="reqn">X</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Basically uses a crude approximation to the estimated degrees of
freedom per model coefficient, to try and find smoothing parameters which
bound these e.d.f.'s away from 0 and 1.
</p>
<p>Usually only called by <code><a href="#topic+magic">magic</a></code> and <code><a href="#topic+gam">gam</a></code>.
</p>


<h3>Value</h3>

<p> An array of initial smoothing parameter estimates.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>See Also</h3>

  
<p><code><a href="#topic+magic">magic</a></code>,
<code><a href="#topic+gam.outer">gam.outer</a></code>,
<code><a href="#topic+gam">gam</a></code>,
</p>

<hr>
<h2 id='inSide'>Are points inside boundary?</h2><span id='topic+inSide'></span>

<h3>Description</h3>

<p>Assesses whether points are inside a boundary. The boundary must enclose the
domain, but may include islands. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inSide(bnd,x,y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inSide_+3A_bnd">bnd</code></td>
<td>
<p>This should have two equal length columns with names matching whatever is 
supplied in <code>x</code> and <code>y</code>. This may contain several sections of boundary separated by
<code>NA</code>. Alternatively <code>bnd</code> may be a list, each element of which 
contains 2 columns named as above.
See below for details.</p>
</td></tr>
<tr><td><code id="inSide_+3A_x">x</code></td>
<td>
<p>x co-ordinates of points to be tested.</p>
</td></tr>
<tr><td><code id="inSide_+3A_y">y</code></td>
<td>
<p>y co-ordinates of points to be tested.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Segments of boundary are separated by <code>NA</code>s, or are in separate list elements.
The boundary co-ordinates are taken to define nodes which are joined by straight line segments in
order to create the boundary. Each segment is assumed to
define a closed loop, and the last point in a segment will be assumed to be
joined to the first. Loops must not intersect (no test is made for
this). 
</p>
<p>The method used is to count how many times a line, in the y-direction from a
point, crosses a boundary segment. An odd number of crossings defines an
interior point. Hence in geographic applications it would be usual to have
an outer boundary loop, possibly with some inner &lsquo;islands&rsquo; completely
enclosed in the outer loop. 
</p>
<p>The routine calls compiled C code and operates by an exhaustive search for
each point in <code>x, y</code>.
</p>


<h3>Value</h3>

<p> The function returns a logical array of the same dimension as <code>x</code> and
<code>y</code>. <code>TRUE</code> indicates that the corresponding <code>x, y</code> point lies
inside the boundary.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
m &lt;- 300;n &lt;- 150
xm &lt;- seq(-1,4,length=m);yn&lt;-seq(-1,1,length=n)
x &lt;- rep(xm,n);y&lt;-rep(yn,rep(m,n))
er &lt;- matrix(fs.test(x,y),m,n)
bnd &lt;- fs.boundary()
in.bnd &lt;- inSide(bnd,x,y)
plot(x,y,col=as.numeric(in.bnd)+1,pch=".")
lines(bnd$x,bnd$y,col=3)
points(x,y,col=as.numeric(in.bnd)+1,pch=".")
## check boundary details ...
plot(x,y,col=as.numeric(in.bnd)+1,pch=".",ylim=c(-1,0),xlim=c(3,3.5))
lines(bnd$x,bnd$y,col=3)
points(x,y,col=as.numeric(in.bnd)+1,pch=".")

</code></pre>

<hr>
<h2 id='interpret.gam'>Interpret a GAM formula</h2><span id='topic+interpret.gam'></span>

<h3>Description</h3>

<p> This is an internal function of package <code>mgcv</code>. It is a service routine for
<code>gam</code> which splits off the strictly parametric part of the model formula, returning 
it as a formula, and interprets the smooth parts of the model formula. 
</p>
<p>Not normally called directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interpret.gam(gf, extra.special = NULL)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interpret.gam_+3A_gf">gf</code></td>
<td>
<p>A GAM formula as supplied to <code><a href="#topic+gam">gam</a></code> or <code><a href="#topic+gamm">gamm</a></code>, or a 
list of such formulae, as supplied for some <code><a href="#topic+gam">gam</a></code> families.</p>
</td></tr>
<tr><td><code id="interpret.gam_+3A_extra.special">extra.special</code></td>
<td>
<p>Name of any extra special in formula in addition to <code>s</code>, <code>te</code>, <code>ti</code> and <code>t2</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>split.gam.formula</code> with the following items:
</p>
<table>
<tr><td><code>pf</code></td>
<td>
<p>A model formula for the strictly parametric part of the model.</p>
</td></tr>
<tr><td><code>pfok</code></td>
<td>
<p>TRUE if there is a <code>pf</code> formula.</p>
</td></tr>
<tr><td><code>smooth.spec</code></td>
<td>
<p>A list of class <code>xx.smooth.spec</code> objects where <code>xx</code> depends on the basis 
specified for the term. (These can be passed to smooth constructor method functions to actually set up
penalties and bases.)</p>
</td></tr>
<tr><td><code>full.formula</code></td>
<td>
<p>An expanded version of the model formula in which the options are fully expanded, and 
the options do not depend on variables which might not be available later.</p>
</td></tr>
<tr><td><code>fake.formula</code></td>
<td>
<p>A formula suitable for use in evaluating a model frame.</p>
</td></tr>
<tr><td><code>response</code></td>
<td>
<p>Name of the response variable.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

   <p><code><a href="#topic+gam">gam</a></code> <code><a href="#topic+gamm">gamm</a></code></p>

<hr>
<h2 id='jagam'>Just Another Gibbs Additive Modeller: JAGS support for mgcv.</h2><span id='topic+jagam'></span><span id='topic+sim2jam'></span>

<h3>Description</h3>

<p>Facilities to auto-generate model specification code and associated data to simulate with GAMs in JAGS (or BUGS). This is useful for inference about models with complex random effects structure best coded in JAGS. It is a very innefficient approach to making inferences about standard GAMs. The idea is that <code>jagam</code> generates template JAGS code, and associated data, for the smooth part of the model. This template is then directly edited to include other stochastic components. After simulation with the resulting model, facilities are provided for plotting and prediction with the model smooth components. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jagam(formula,family=gaussian,data=list(),file,weights=NULL,na.action,
offset=NULL,knots=NULL,sp=NULL,drop.unused.levels=TRUE,
control=gam.control(),centred=TRUE,sp.prior = "gamma",diagonalize=FALSE)

sim2jam(sam,pregam,edf.type=2,burnin=0)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="jagam_+3A_formula">formula</code></td>
<td>
<p> A GAM formula (see <code><a href="#topic+formula.gam">formula.gam</a></code> and also <code><a href="#topic+gam.models">gam.models</a></code>). 
This is exactly like the formula for a GLM except that smooth terms,  <code><a href="#topic+s">s</a></code>, <code><a href="#topic+te">te</a></code>, <code><a href="#topic+ti">ti</a></code> 
and <code><a href="#topic+t2">t2</a></code> can be added to the right hand side to specify that the linear predictor depends on smooth functions of predictors (or linear functionals of these).
</p>
</td></tr> 
<tr><td><code id="jagam_+3A_family">family</code></td>
<td>

<p>This is a family object specifying the distribution and link function to use. 
See <code><a href="stats.html#topic+glm">glm</a></code> and <code><a href="stats.html#topic+family">family</a></code> for more
details. Currently only gaussian, poisson, binomial and Gamma families are supported, 
but the user can easily modify the assumed distribution in the JAGS code.
</p>
</td></tr> 
<tr><td><code id="jagam_+3A_data">data</code></td>
<td>
<p> A data frame or list containing the model response variable and 
covariates required by the formula. By default the variables are taken 
from <code>environment(formula)</code>: typically the environment from 
which <code>jagam</code> is called.</p>
</td></tr> 
<tr><td><code id="jagam_+3A_file">file</code></td>
<td>
<p>Name of the file to which JAGS model specification code should be written. See <code><a href="base.html#topic+setwd">setwd</a></code> for setting and querying the current working directory.</p>
</td></tr>
<tr><td><code id="jagam_+3A_weights">weights</code></td>
<td>
<p> prior weights on the data.</p>
</td></tr>
<tr><td><code id="jagam_+3A_na.action">na.action</code></td>
<td>
<p> a function which indicates what should happen when the data
contain &lsquo;NA&rsquo;s.  The default is set by the &lsquo;na.action&rsquo; setting
of &lsquo;options&rsquo;, and is &lsquo;na.fail&rsquo; if that is unset.  The
&ldquo;factory-fresh&rdquo; default is &lsquo;na.omit&rsquo;.</p>
</td></tr>
<tr><td><code id="jagam_+3A_offset">offset</code></td>
<td>
<p>Can be used to supply a model offset for use in fitting. Note
that this offset will always be completely ignored when predicting, unlike an offset 
included in <code>formula</code>: this conforms to the behaviour of
<code>lm</code> and <code>glm</code>.</p>
</td></tr>
<tr><td><code id="jagam_+3A_control">control</code></td>
<td>
<p>A list of fit control parameters to replace defaults returned by 
<code><a href="#topic+gam.control">gam.control</a></code>. Any control parameters not supplied stay at their default values. 
little effect on <code>jagam</code>.</p>
</td></tr>
<tr><td><code id="jagam_+3A_knots">knots</code></td>
<td>
<p>this is an optional list containing user specified knot values to be used for basis construction. 
For most bases the user simply supplies the knots to be used, which must match up with the <code>k</code> value
supplied (note that the number of knots is not always just <code>k</code>). 
See <code><a href="#topic+tprs">tprs</a></code> for what happens in the <code>"tp"/"ts"</code> case. 
Different terms can use different numbers of knots, unless they share a covariate.
</p>
</td></tr>
<tr><td><code id="jagam_+3A_sp">sp</code></td>
<td>
<p>A vector of smoothing parameters can be provided here.
Smoothing parameters must be supplied in the order that the smooth terms appear in the model 
formula (without forgetting null space penalties). Negative elements indicate that the parameter should be estimated, and hence a mixture 
of fixed and estimated parameters is possible. If smooths share smoothing parameters then <code>length(sp)</code> 
must correspond to the number of underlying smoothing parameters.</p>
</td></tr>
<tr><td><code id="jagam_+3A_drop.unused.levels">drop.unused.levels</code></td>
<td>
<p>by default unused levels are dropped from factors before fitting. For some smooths 
involving factor variables you might want to turn this off. Only do so if you know what you are doing.</p>
</td></tr>
<tr><td><code id="jagam_+3A_centred">centred</code></td>
<td>
<p>Should centring constraints be applied to the smooths, as is usual with GAMS? Only set 
this to <code>FALSE</code> if you know exactly what you are doing. If <code>FALSE</code> there is a (usually global) 
intercept for each smooth.</p>
</td></tr>
<tr><td><code id="jagam_+3A_sp.prior">sp.prior</code></td>
<td>
<p><code>"gamma"</code> or <code>"log.uniform"</code> prior for the smoothing parameters? Do check that the 
default parameters are appropriate for your model in the JAGS code.</p>
</td></tr>
<tr><td><code id="jagam_+3A_diagonalize">diagonalize</code></td>
<td>
<p>Should smooths be re-parameterized to have i.i.d. Gaussian priors (where possible)? For Gaussian data this 
allows efficient conjugate samplers to be used, and it can also work well with GLMs if the JAGS <code>"glm"</code> module is loaded, but otherwise it is often better to update smoothers blockwise, and not do this.</p>
</td></tr>
<tr><td><code id="jagam_+3A_sam">sam</code></td>
<td>
<p>jags sample object, containing at least fields <code>b</code> (coefficients) and <code>rho</code> (log 
smoothing parameters). May also contain field <code>mu</code> containing monitored expected response.</p>
</td></tr>
<tr><td><code id="jagam_+3A_pregam">pregam</code></td>
<td>
<p>standard <code>mgcv</code> GAM setup data, as returned in <code>jagam</code> return list.</p>
</td></tr>
<tr><td><code id="jagam_+3A_edf.type">edf.type</code></td>
<td>
<p>Since EDF is not uniquely defined and may be affected by the stochastic structure added to the JAGS model file, 3 options are offered. See details.</p>
</td></tr>
<tr><td><code id="jagam_+3A_burnin">burnin</code></td>
<td>
<p>the amount of burn in to discard from the simulation chains. Limited to .9 of the chain length.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Smooths are easily incorportated into JAGS models using multivariate normal priors on the smooth coefficients. The smoothing parameters and smoothing penalty matrices directly specifiy the prior multivariate normal precision matrix. Normally a smoothing penalty does not correspond to a full rank precision matrix, implying an improper prior inappropriate for Gibbs sampling. To rectify this problem the null space penalties suggested in Marra and Wood (2011) are added to the usual penalties. 
</p>
<p>In an additive modelling context it is usual to centre the smooths, to avoid the identifiability issues associated with having an intercept for each smooth term (in addition to a global intercept). Under Gibbs sampling with JAGS it is technically possible to omit this centring, since we anyway force propriety on the priors, and this propiety implies formal model identifiability. However, in most situations this formal identifiability is rather artificial and does not imply statistically meaningfull identifiability. Rather it serves only to massively inflate confidence intervals, since the multiple intercept terms are not identifiable from the data, but only from the prior. By default then, <code>jagam</code> imposes standard GAM identifiability constraints on all smooths. The <code>centred</code> argument does allow you to turn this off, but it is not recommended. If you do set <code>centred=FALSE</code> then chain convergence and mixing checks should be particularly stringent. 
</p>
<p>The final technical issue for model setup is the setting of initial conditions for the coefficients and smoothing parameters. The approach taken is to take the default initial smoothing parameter values used elsewhere by <code>mgcv</code>, and to take a single PIRLS fitting step with these smoothing parameters in order to obtain starting values for the smooth coefficients. In the setting of fully conjugate updating the initial values of the coefficients are not critical, and good results are obtained without supplying them. But in the usual setting in which slice sampling is required for at least some of the updates then very poor results can sometimes be obtained without initial values, as the sampler simply fails to find the region of the posterior mode. 
</p>
<p>The <code>sim2jam</code> function takes the partial <code>gam</code> object (<code>pregam</code>) from <code>jagam</code> along with simulation output in standard <code>rjags</code> form and creates a reduced version of a <code>gam</code> object, suitable for plotting and prediction of the model's smooth components. <code>sim2gam</code> computes effective degrees of freedom for each smooth, but it should be noted that there are several possibilites for doing this in the context of a model with a complex random effects structure. The simplest approach (<code>edf.type=0</code>) is to compute the degrees of freedom that the smooth would have had if it had been part of an unweighted Gaussian additive model. One might choose to use this option if the model has been modified so that the response distribution and/or link are not those that were specified to <code>jagam</code>. The second option is (<code>edf.type=1</code>) uses the edf that would have been computed by <code><a href="#topic+gam">gam</a></code> had it produced these estimates - in the context in which the JAGS model modifications have all been about modifying the random effects structure, this is equivalent to simply setting all the random effects to zero for the effective degrees of freedom calculation. The default option (<code>edf.type=2</code>) is to base the EDF on the sample covariance matrix, <code>Vp</code>, of the model coefficients. If the simulation output (<code>sim</code>) includes a <code>mu</code> field, then this will be used to form the weight matrix <code>W</code> in <code>XWX = t(X)%*%W%*%X</code>, where the EDF is computed from <code>rowSums(Vp*XWX)*scale</code>. If <code>mu</code> is not supplied then it is estimated from the the model matrix <code>X</code> and the mean of the simulated coefficients, but the resulting <code>W</code> may not be strictly comaptible with the <code>Vp</code> matrix in this case. In the situation in which the fitted model is very different in structure from the regression model of the template produced by <code>jagam</code> then the default option may make no sense, and indeed it may be best to use option 0.
</p>


<h3>Value</h3>

 
<p>For <code>jagam</code> a three item list containing 
</p>
<table>
<tr><td><code>pregam</code></td>
<td>
<p>standard <code>mgcv</code> GAM setup data.</p>
</td></tr>
<tr><td><code>jags.data</code></td>
<td>
<p>list of arguments to be supplied to JAGS containing information referenced in model specification.</p>
</td></tr>
<tr><td><code>jags.ini</code></td>
<td>
<p>initialization data for smooth coefficients and smoothing parameters.</p>
</td></tr>
</table>
<p>For <code>sim2jam</code> an object of class <code>"jam"</code>: a partial version of an <code>mgcv</code> <code><a href="#topic+gamObject">gamObject</a></code>, suitable for 
plotting and predicting.
</p>


<h3>WARNINGS </h3>

<p>Gibb's sampling is a very slow inferential method for standard GAMs. It is only likely to be worthwhile when complex random effects structures are required above what is possible with direct GAMM methods.
</p>
<p>Check that the parameters of the priors on the parameters are fit for your purpose.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N. (2016) Just Another Gibbs Additive Modeller: Interfacing JAGS and mgcv. Journal of Statistical Software 75(7):1-15 doi:10.18637/jss.v075.i07)
</p>
<p>Marra, G. and S.N. Wood (2011) Practical variable selection for generalized additive models.
Computational Statistics &amp; Data Analysis 55(7): 2372-2387
</p>
<p>Here is a key early reference to smoothing using BUGS (although the approach and smooths used are a bit different to jagam)
</p>
<p>Crainiceanu, C. M. D Ruppert, &amp; M.P. Wand (2005) Bayesian Analysis for Penalized Spline Regression Using WinBUGS Journal of Statistical Software 14.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+gamm">gamm</a></code>, <code><a href="#topic+bam">bam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## the following illustrates a typical workflow. To run the 
## 'Not run' code you need rjags (and JAGS) to be installed.
require(mgcv)
  
set.seed(2) ## simulate some data... 
n &lt;- 400
dat &lt;- gamSim(1,n=n,dist="normal",scale=2)
## regular gam fit for comparison...
b0 &lt;- gam(y~s(x0)+s(x1) + s(x2)+s(x3),data=dat,method="REML")

## Set directory and file name for file containing jags code.
## In real use you would *never* use tempdir() for this. It is
## only done here to keep CRAN happy, and avoid any chance of
## an accidental overwrite. Instead you would use
## setwd() to set an appropriate working directory in which
## to write the file, and just set the file name to what you
## want to call it (e.g. "test.jags" here). 

jags.file &lt;- paste(tempdir(),"/test.jags",sep="") 

## Set up JAGS code and data. In this one might want to diagonalize
## to use conjugate samplers. Usually call 'setwd' first, to set
## directory in which model file ("test.jags") will be written.

jd &lt;- jagam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,file=jags.file,
            sp.prior="gamma",diagonalize=TRUE)

## In normal use the model in "test.jags" would now be edited to add 
## the non-standard stochastic elements that require use of JAGS....

## Not run: 
require(rjags)
load.module("glm") ## improved samplers for GLMs often worth loading
jm &lt;-jags.model(jags.file,data=jd$jags.data,inits=jd$jags.ini,n.chains=1)
list.samplers(jm)
sam &lt;- jags.samples(jm,c("b","rho","scale"),n.iter=10000,thin=10)
jam &lt;- sim2jam(sam,jd$pregam)
plot(jam,pages=1)
jam
pd &lt;- data.frame(x0=c(.5,.6),x1=c(.4,.2),x2=c(.8,.4),x3=c(.1,.1))
fv &lt;- predict(jam,newdata=pd)
## and some minimal checking...
require(coda)
effectiveSize(as.mcmc.list(sam$b))

## End(Not run)

## a gamma example...
set.seed(1); n &lt;- 400
dat &lt;- gamSim(1,n=n,dist="normal",scale=2)
scale &lt;- .5; Ey &lt;- exp(dat$f/2)
dat$y &lt;- rgamma(n,shape=1/scale,scale=Ey*scale)
jd &lt;- jagam(y~s(x0)+te(x1,x2)+s(x3),data=dat,family=Gamma(link=log),
            file=jags.file,sp.prior="log.uniform")

## In normal use the model in "test.jags" would now be edited to add 
## the non-standard stochastic elements that require use of JAGS....

## Not run: 
require(rjags)
## following sets random seed, but note that under JAGS 3.4 many
## models are still not fully repeatable (JAGS 4 should fix this)
jd$jags.ini$.RNG.name &lt;- "base::Mersenne-Twister" ## setting RNG
jd$jags.ini$.RNG.seed &lt;- 6 ## how to set RNG seed
jm &lt;-jags.model(jags.file,data=jd$jags.data,inits=jd$jags.ini,n.chains=1)
list.samplers(jm)
sam &lt;- jags.samples(jm,c("b","rho","scale","mu"),n.iter=10000,thin=10)
jam &lt;- sim2jam(sam,jd$pregam)
plot(jam,pages=1)
jam
pd &lt;- data.frame(x0=c(.5,.6),x1=c(.4,.2),x2=c(.8,.4),x3=c(.1,.1))
fv &lt;- predict(jam,newdata=pd)

## End(Not run)

</code></pre>

<hr>
<h2 id='k.check'>Checking smooth basis dimension </h2><span id='topic+k.check'></span>

<h3>Description</h3>

<p> Takes a fitted <code>gam</code> object produced by <code>gam()</code> and runs 
diagnostic tests of whether the basis dimension choises are adequate. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k.check(b, subsample=5000, n.rep=400)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k.check_+3A_b">b</code></td>
<td>
<p>a fitted <code>gam</code> object as produced by <code><a href="#topic+gam">gam</a>()</code>.</p>
</td></tr>
<tr><td><code id="k.check_+3A_subsample">subsample</code></td>
<td>
<p>above this number of data, testing uses a random sub-sample of data of this size.</p>
</td></tr>
<tr><td><code id="k.check_+3A_n.rep">n.rep</code></td>
<td>
<p>how many re-shuffles to do to get p-value for k testing.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>The test of whether the basis dimension for a smooth is adequate (Wood, 2017, section 5.9) is based on computing an estimate of the residual variance based on differencing residuals that are near neighbours according to the (numeric) covariates of the smooth. This estimate divided by the residual variance is the <code>k-index</code> reported. The further below 1 this is, the more likely it is that there is missed pattern left in the residuals. The <code>p-value</code> is computed by simulation: the residuals are randomly re-shuffled <code>n.rep</code> times to obtain the null distribution of the differencing variance estimator, if there is no pattern in the residuals. For models fitted to more than <code>subsample</code> data, the tests are based of <code>subsample</code> randomly sampled data. Low p-values may indicate that the basis dimension, <code>k</code>, has been set too low, especially if the reported <code>edf</code> is close to <code>k\'</code>, the maximum possible EDF for the term. Note the disconcerting fact that if the test statistic itself is based on random resampling and the null is true, then the associated p-values will of course vary widely from one replicate to the next. Currently smooths of factor variables are not supported and will give an <code>NA</code> p-value.
</p>
<p>Doubling a suspect <code>k</code> and re-fitting is sensible: if the reported <code>edf</code> increases substantially then you may have been missing something in the first fit. Of course p-values can be low for reasons other than a too low <code>k</code>. See <code><a href="#topic+choose.k">choose.k</a></code> for fuller discussion.
</p>


<h3>Value</h3>

<p>A matrix contaning the output of the tests described above.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+choose.k">choose.k</a></code>,  <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+gam.check">gam.check</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(0)
dat &lt;- gamSim(1,n=200)
b&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
plot(b,pages=1)
k.check(b)
</code></pre>

<hr>
<h2 id='ldetS'>Getting log generalized determinant of penalty matrices</h2><span id='topic+ldetS'></span>

<h3>Description</h3>

<p>INTERNAL function calculating the log generalized determinant of penalty matrix S stored blockwise in an Sl list
(which is the output of <code>Sl.setup</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ldetS(Sl, rho, fixed, np, root = FALSE, repara = TRUE,
      nt = 1,deriv=2,sparse=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ldetS_+3A_sl">Sl</code></td>
<td>
<p>the output of <code>Sl.setup</code>.</p>
</td></tr>
<tr><td><code id="ldetS_+3A_rho">rho</code></td>
<td>
<p>the log smoothing parameters.</p>
</td></tr>
<tr><td><code id="ldetS_+3A_fixed">fixed</code></td>
<td>
<p>an array indicating whether the smoothing parameters are fixed (or free).</p>
</td></tr>
<tr><td><code id="ldetS_+3A_np">np</code></td>
<td>
<p>number of coefficients.</p>
</td></tr>
<tr><td><code id="ldetS_+3A_root">root</code></td>
<td>
<p>indicates whether or not to return the matrix square root, <code>E</code>, of the total penalty S_tot.</p>
</td></tr>
<tr><td><code id="ldetS_+3A_repara">repara</code></td>
<td>
<p>if TRUE multi-term blocks will be re-parameterized using <code>gam.reparam</code>, and
a re-parameterization object supplied in the returned object.</p>
</td></tr>
<tr><td><code id="ldetS_+3A_nt">nt</code></td>
<td>
<p>number of parallel threads to use.</p>
</td></tr>
<tr><td><code id="ldetS_+3A_deriv">deriv</code></td>
<td>
<p>order of derivative to use</p>
</td></tr>
<tr><td><code id="ldetS_+3A_sparse">sparse</code></td>
<td>
<p>should <code>E</code> be sparse?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing: </p>

<ul>
<li><p><code>ldetS</code>: the log-determinant of S. 
</p>
</li>
<li><p><code>ldetS1</code>: the gradient of the log-determinant of S. 
</p>
</li>
<li><p><code>ldetS2</code>: the Hessian of the log-determinant of S. 
</p>
</li>
<li><p><code>Sl</code>: with modified rS terms, if needed and rho added to each block 
</p>
</li>
<li><p><code>rp</code>:  a re-parameterization list. 
</p>
</li>
<li><p><code>rp</code>:  E a total penalty square root such that <code>t(E)%*%E = S_tot</code> (if <code>root==TRUE</code>). 
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>

<hr>
<h2 id='ldTweedie'>Log Tweedie density evaluation</h2><span id='topic+ldTweedie'></span>

<h3>Description</h3>

<p>A function to evaluate the log of the Tweedie density for variance powers between 1 and 2, inclusive.
Also evaluates first and second derivatives of log density w.r.t. its scale parameter, <code>phi</code>, and <code>p</code>, 
or w.r.t. <code>rho=log(phi)</code> and <code>theta</code> where <code>p = (a+b*exp(theta))/(1+exp(theta))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ldTweedie(y,mu=y,p=1.5,phi=1,rho=NA,theta=NA,a=1.001,b=1.999,all.derivs=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ldTweedie_+3A_y">y</code></td>
<td>
<p>values at which to evaluate density.</p>
</td></tr>
<tr><td><code id="ldTweedie_+3A_mu">mu</code></td>
<td>
<p>corresponding means (either of same length as <code>y</code> or a single value).</p>
</td></tr>
<tr><td><code id="ldTweedie_+3A_p">p</code></td>
<td>
<p>the variance of <code>y</code> is proportional to its mean to the power <code>p</code>. <code>p</code> must
be between 1 and 2. 1 is Poisson like (exactly Poisson if <code>phi=1</code>), 2 is gamma. </p>
</td></tr>
<tr><td><code id="ldTweedie_+3A_phi">phi</code></td>
<td>
<p>The scale parameter. Variance of <code>y</code> is <code>phi*mu^p</code>.</p>
</td></tr>
<tr><td><code id="ldTweedie_+3A_rho">rho</code></td>
<td>
<p>optional log scale parameter. Over-rides <code>phi</code> if <code>theta</code> also supplied.</p>
</td></tr>
<tr><td><code id="ldTweedie_+3A_theta">theta</code></td>
<td>
<p>parameter such that  <code>p = (a+b*exp(theta))/(1+exp(theta))</code>. Over-rides <code>p</code> if <code>rho</code> 
also supplied.</p>
</td></tr>
<tr><td><code id="ldTweedie_+3A_a">a</code></td>
<td>
<p>lower limit parameter (&gt;1) used in definition of <code>p</code> from <code>theta</code>.</p>
</td></tr>
<tr><td><code id="ldTweedie_+3A_b">b</code></td>
<td>
<p>upper limit parameter (&lt;2) used in definition of <code>p</code> from <code>theta</code>.</p>
</td></tr>
<tr><td><code id="ldTweedie_+3A_all.derivs">all.derivs</code></td>
<td>
<p>if <code>TRUE</code> then derivatives w.r.t. <code>mu</code> are also returned. Only available with <code>rho</code> and
<code>phi</code> parameterization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> A Tweedie random variable with 1&lt;p&lt;2 is a sum of <code>N</code> gamma random variables 
where <code>N</code> has a Poisson distribution. The p=1 case is a generalization of a Poisson distribution and is a discrete 
distribution supported on integer multiples of the scale parameter. For 1&lt;p&lt;2 the distribution is supported on the 
positive reals with a point mass at zero. p=2 is a gamma distribution. As p gets very close to 1 the continuous 
distribution begins to converge on the discretely supported limit at p=1. 
</p>
<p><code>ldTweedie</code> is based on the series evaluation method of Dunn and Smyth (2005). Without 
the restriction on <code>p</code> the calculation of Tweedie densities is less straightforward. If you really need this 
case then the <code>tweedie</code> package is the place to start. 
</p>
<p>The <code>rho</code>, <code>theta</code> parameterization is useful for optimization of <code>p</code> and <code>phi</code>, in order to keep <code>p</code>
bounded well away from 1 and 2, and <code>phi</code> positive. The derivatives near <code>p=1</code> tend to infinity.
</p>
<p>Note that if <code>p</code> and <code>phi</code> (or <code>theta</code> and <code>rho</code>) both contain only a single unique value, then the underlying
code is able to use buffering to avoid repeated calls to expensive log gamma, di-gamma and tri-gamma functions (<code>mu</code> can still be a vector of different values). This is much faster than is possible when these parameters are vectors with different values. 
</p>


<h3>Value</h3>

<p> A matrix with 6 columns, or 10 if <code>all.derivs=TRUE</code>. The first is the log density of <code>y</code> (log probability if <code>p=1</code>). 
The second and third are the first and second derivatives of the log density w.r.t. <code>phi</code>. 4th and 5th 
columns are first and second derivative w.r.t. <code>p</code>, final column is second derivative w.r.t. <code>phi</code> and <code>p</code>.
</p>
<p>If <code>rho</code> and <code>theta</code> were supplied then derivatives are w.r.t. these. In this case, and if <code>all.derivs=TRUE</code> then the 7th colmn is the derivative w.r.t. <code>mu</code>, the 8th is the 2nd derivative w.r.t. <code>mu</code>, the 9th is the mixed derivative w.r.t. <code>theta</code> and<code>mu</code> and the 10th is the mixed derivative w.r.t. <code>rho</code> and <code>mu</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Dunn, P.K. and G.K. Smith (2005) Series evaluation of Tweedie exponential dispersion model densities. 
Statistics and Computing 15:267-280
</p>
<p>Tweedie, M. C. K. (1984). An index which distinguishes between
some important exponential families. Statistics: Applications and
New Directions. Proceedings of the Indian Statistical Institute
Golden Jubilee International Conference (Eds. J. K. Ghosh and J.
Roy), pp. 579-604. Calcutta: Indian Statistical Institute.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(mgcv)
  ## convergence to Poisson illustrated
  ## notice how p&gt;1.1 is OK
  y &lt;- seq(1e-10,10,length=1000)
  p &lt;- c(1.0001,1.001,1.01,1.1,1.2,1.5,1.8,2)
  phi &lt;- .5
  fy &lt;- exp(ldTweedie(y,mu=2,p=p[1],phi=phi)[,1])
  plot(y,fy,type="l",ylim=c(0,3),main="Tweedie density as p changes")
  for (i in 2:length(p)) {
    fy &lt;- exp(ldTweedie(y,mu=2,p=p[i],phi=phi)[,1])
    lines(y,fy,col=i)
  }


</code></pre>

<hr>
<h2 id='linear.functional.terms'>Linear functionals of a smooth in GAMs</h2><span id='topic+linear.functional.terms'></span><span id='topic+function.predictors'></span><span id='topic+signal.regression'></span>

<h3>Description</h3>

<p><code><a href="#topic+gam">gam</a></code> allows the response variable to depend on linear 
functionals of smooth terms. Specifically dependancies of the form
</p>
<p style="text-align: center;"><code class="reqn">g(\mu_i) = \ldots + \sum_j L_{ij} f(x_{ij}) + \ldots </code>
</p>

<p>are allowed, where the <code class="reqn">x_{ij}</code> are covariate values and the <code class="reqn">L_{ij}</code> are 
fixed weights. i.e. the response can depend on the weighted sum of the same smooth 
evaluated at different covariate values. This allows, for example, for the 
response to depend on the derivatives or integrals of a smooth (approximated by finite 
differencing or quadrature, respectively). It also allows dependence on predictor functions 
(sometimes called &lsquo;signal regression&rsquo;).
</p>
<p>The mechanism by which this is achieved is to supply matrices of covariate values to the 
model smooth terms specified by <code><a href="#topic+s">s</a></code> or <code><a href="#topic+te">te</a></code> terms in the model formula.
Each column of the covariate matrix gives rise to a corresponding column of predictions 
from the smooth. Let the resulting matrix of evaluated smooth values be F (F will have 
the same dimension as the covariate matrices). In the absense of a <code>by</code> variable 
then these columns are simply summed and added to the linear predictor. i.e. the contribution of the 
term to the linear predictor is <code>rowSums(F)</code>. If a <code>by</code> variable is present 
then it must be a matrix, L,say, of the same dimension as F (and the covariate matrices), 
and it contains the weights <code class="reqn">L_{ij}</code> in the summation given above. 
So in this case the contribution to the  linear predictor is <code>rowSums(L*F)</code>. 
</p>
<p>Note that if a <code class="reqn">{\bf L1}</code> (i.e. <code>rowSums(L)</code>) is a constant vector, or there is no <code>by</code> 
variable then the smooth will automatically be centred in order to ensure identifiability. Otherwise it 
will not be. Note also that for centred smooths it can be worth replacing the constant term in the model with <code>rowSums(L)</code> 
in order to ensure that predictions are automatically on the right scale.
</p>
<p><code><a href="#topic+predict.gam">predict.gam</a></code> can accept matrix predictors for prediction with such terms, in which case its
<code>newdata</code> argument will need to be a list. However when predicting from the model it is not necessary to provide matrix covariate and <code>by</code> variable values. 
For example to simply examine the underlying smooth function one would use vectors of covariate values and vector 
<code>by</code> variables, with the <code>by</code> variable and equivalent of <code>L1</code>, above, set to vectors of ones. 
</p>
<p>The mechanism is usable with random effect smooths which take factor arguments, by using a trick to create a 2D array of factors. Simply create a factor vector containing the columns of the factor matrix stacked end to end (column major order). Then reset the dimensions of this vector to create the appropriate 2D array: the first dimension should be the number of response data and the second the number of columns of the required factor matrix. You can not use <code>matrix</code> or <code>data.matrix</code> to set up the required matrix of factor levels. See example below.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>### matrix argument `linear operator' smoothing
library(mgcv)
set.seed(0)

###############################
## simple summation example...#
###############################

n&lt;-400
sig&lt;-2
x &lt;- runif(n, 0, .9)
f2 &lt;- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
x1 &lt;- x + .1

f &lt;- f2(x) + f2(x1)  ## response is sum of f at two adjacent x values 
y &lt;- f + rnorm(n)*sig

X &lt;- matrix(c(x,x1),n,2) ## matrix covariate contains both x values
b &lt;- gam(y~s(X))         

plot(b)  ## reconstruction of f
plot(f,fitted(b))

## example of prediction with summation convention...
predict(b,list(X=X[1:3,]))

## example of prediction that simply evaluates smooth (no summation)...
predict(b,data.frame(X=c(.2,.3,.7))) 

######################################################################
## Simple random effect model example.
## model: y[i] = f(x[i]) + b[k[i]] - b[j[i]] + e[i]
## k[i] and j[i] index levels of i.i.d. random effects, b.
######################################################################

set.seed(7)
n &lt;- 200
x &lt;- runif(n) ## a continuous covariate

## set up a `factor matrix'...
fac &lt;- factor(sample(letters,n*2,replace=TRUE))
dim(fac) &lt;- c(n,2)

## simulate data from such a model...
nb &lt;- length(levels(fac))
b &lt;- rnorm(nb)
y &lt;- 20*(x-.3)^4 + b[fac[,1]] - b[fac[,2]] + rnorm(n)*.5

L &lt;- matrix(-1,n,2);L[,1] &lt;- 1 ## the differencing 'by' variable 

mod &lt;- gam(y ~ s(x) + s(fac,by=L,bs="re"),method="REML")
gam.vcomp(mod)
plot(mod,page=1)

## example of prediction using matrices...
dat &lt;- list(L=L[1:20,],fac=fac[1:20,],x=x[1:20],y=y[1:20])
predict(mod,newdata=dat)


######################################################################
## multivariate integral example. Function `test1' will be integrated# 
## (by midpoint quadrature) over 100 equal area sub-squares covering # 
## the unit square. Noise is added to the resulting simulated data.  #
## `test1' is estimated from the resulting data using two alternative#
## smooths.                                                          #
######################################################################

test1 &lt;- function(x,z,sx=0.3,sz=0.4)
  { (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
    0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
  }

## create quadrature (integration) grid, in useful order
ig &lt;- 5 ## integration grid within square
mx &lt;- mz &lt;- (1:ig-.5)/ig
ix &lt;- rep(mx,ig);iz &lt;- rep(mz,rep(ig,ig))

og &lt;- 10 ## observarion grid
mx &lt;- mz &lt;- (1:og-1)/og
ox &lt;- rep(mx,og);ox &lt;- rep(ox,rep(ig^2,og^2))
oz &lt;- rep(mz,rep(og,og));oz &lt;- rep(oz,rep(ig^2,og^2))

x &lt;- ox + ix/og;z &lt;- oz + iz/og ## full grid, subsquare by subsquare

## create matrix covariates...
X &lt;- matrix(x,og^2,ig^2,byrow=TRUE)
Z &lt;- matrix(z,og^2,ig^2,byrow=TRUE)

## create simulated test data...
dA &lt;- 1/(og*ig)^2  ## quadrature square area
F &lt;- test1(X,Z)    ## evaluate on grid
f &lt;- rowSums(F)*dA ## integrate by midpoint quadrature
y &lt;- f + rnorm(og^2)*5e-4 ## add noise
## ... so each y is a noisy observation of the integral of `test1'
## over a 0.1 by 0.1 sub-square from the unit square

## Now fit model to simulated data...

L &lt;- X*0 + dA

## ... let F be the matrix of the smooth evaluated at the x,z values
## in matrices X and Z. rowSums(L*F) gives the model predicted
## integrals of `test1' corresponding to the observed `y'

L1 &lt;- rowSums(L) ## smooths are centred --- need to add in L%*%1

## fit models to reconstruct `test1'....

b &lt;- gam(y~s(X,Z,by=L)+L1-1)   ## (L1 and const are confounded here)
b1 &lt;- gam(y~te(X,Z,by=L)+L1-1) ## tensor product alternative

## plot results...

old.par&lt;-par(mfrow=c(2,2))
x&lt;-runif(n);z&lt;-runif(n);
xs&lt;-seq(0,1,length=30);zs&lt;-seq(0,1,length=30)
pr&lt;-data.frame(x=rep(xs,30),z=rep(zs,rep(30,30)))
truth&lt;-matrix(test1(pr$x,pr$z),30,30)
contour(xs,zs,truth)
plot(b)
vis.gam(b,view=c("X","Z"),cond=list(L1=1,L=1),plot.type="contour")
vis.gam(b1,view=c("X","Z"),cond=list(L1=1,L=1),plot.type="contour")

####################################
## A "signal" regression example...#
####################################

rf &lt;- function(x=seq(0,1,length=100)) {
## generates random functions...
  m &lt;- ceiling(runif(1)*5) ## number of components
  f &lt;- x*0;
  mu &lt;- runif(m,min(x),max(x));sig &lt;- (runif(m)+.5)*(max(x)-min(x))/10
  for (i in 1:m) f &lt;- f+ dnorm(x,mu[i],sig[i])
  f
}

x &lt;- seq(0,1,length=100) ## evaluation points

## example functional predictors...
par(mfrow=c(3,3));for (i in 1:9) plot(x,rf(x),type="l",xlab="x")

## simulate 200 functions and store in rows of L...
L &lt;- matrix(NA,200,100) 
for (i in 1:200) L[i,] &lt;- rf()  ## simulate the functional predictors

f2 &lt;- function(x) { ## the coefficient function
  (0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10)/10 
}

f &lt;- f2(x) ## the true coefficient function

y &lt;- L%*%f + rnorm(200)*20 ## simulated response data

## Now fit the model E(y) = L%*%f(x) where f is a smooth function.
## The summation convention is used to evaluate smooth at each value
## in matrix X to get matrix F, say. Then rowSum(L*F) gives E(y).

## create matrix of eval points for each function. Note that
## `smoothCon' is smart and will recognize the duplication...
X &lt;- matrix(x,200,100,byrow=TRUE) 

b &lt;- gam(y~s(X,by=L,k=20)) 
par(mfrow=c(1,1))
plot(b,shade=TRUE);lines(x,f,col=2)

</code></pre>

<hr>
<h2 id='logLik.gam'>AIC and Log likelihood for a fitted GAM</h2><span id='topic+logLik.gam'></span><span id='topic+AIC.gam'></span>

<h3>Description</h3>

<p> Function to extract the log-likelihood for a fitted <code>gam</code>
model (note that the models are usually fitted by penalized likelihood maximization). 
Used by <code><a href="stats.html#topic+AIC">AIC</a></code>. See details for more information on AIC computation.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
logLik(object,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="logLik.gam_+3A_object">object</code></td>
<td>
<p> fitted model objects of class <code>gam</code> as produced by <code>gam()</code>.</p>
</td></tr>
<tr><td><code id="logLik.gam_+3A_...">...</code></td>
<td>
<p>un-used in this case</p>
</td></tr> 
</table>


<h3>Details</h3>

<p> Modification of <code>logLik.glm</code> which corrects the degrees of
freedom for use with <code>gam</code> objects. 
</p>
<p>The function is provided so that <code><a href="stats.html#topic+AIC">AIC</a></code> functions correctly with
<code>gam</code> objects, and uses the appropriate degrees of freedom (accounting
for penalization). See e.g. Wood, Pya and Saefken (2016) for a derivation of
an appropriate AIC.
</p>
<p>For <code><a href="stats.html#topic+gaussian">gaussian</a></code> family models the MLE of the scale parameter is used. For other families
with a scale parameter the estimated scale parameter is used. This is usually not exactly the MLE, and is not the simple deviance based estimator used with <code><a href="stats.html#topic+glm">glm</a></code> models. This is because the simple deviance based estimator can be badly biased in some cases, for example when a Tweedie distribution is employed with low count data. 
</p>
<p>There are two possibile AIC's that might be considered for use with GAMs. Marginal
AIC is based on the marginal likelihood of the GAM, that is the likelihood based on
treating penalized (e.g. spline) coefficients as random and integrating them out. The
degrees of freedom is then the number of smoothing/variance parameters + the number
of fixed effects. The problem with Marginal AIC is that marginal likelihood
underestimates variance components/oversmooths, so that the approach favours simpler models
excessively (substituting REML does not work, because REML is not comparable between models
with different unpenalized/fixed components). Conditional AIC uses the likelihood of all
the model coefficients, evaluated at the penalized MLE. The degrees of freedom to use then
is the effective degrees of freedom for the model. However, Greven and Kneib (2010) show
that the neglect of smoothing parameter uncertainty can lead to this conditional AIC being
excessively likely to select larger models. Wood, Pya and Saefken (2016) propose a simple
correction to the effective degrees of freedom to fix this problem. <code>mgcv</code> applies this
correction whenever possible: that is when using <code>ML</code> or <code>REML</code> smoothing parameter
selection with <code><a href="#topic+gam">gam</a></code> or <code><a href="#topic+bam">bam</a></code>. The correction
is not computable when using the Extended Fellner Schall or BFGS optimizer (since the correction requires
an estimate of the covariance matrix of the log smoothing parameters). 
</p>


<h3>Value</h3>

<p> Standard <code>logLik</code> object: see <code><a href="stats.html#topic+logLik">logLik</a></code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> based directly on <code>logLik.glm</code></p>


<h3>References</h3>

<p>Greven, S., and Kneib, T. (2010), On the Behaviour of Marginal and
Conditional AIC in Linear Mixed Models, Biometrika, 97, 773-789.
</p>
<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models (with discussion).
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R
(2nd edition). Chapman and Hall/CRC Press. <a href="https://doi.org/10.1201/9781315370279">doi:10.1201/9781315370279</a>
</p>


<h3>See Also</h3>

  <p><code><a href="stats.html#topic+AIC">AIC</a></code></p>

<hr>
<h2 id='ls.size'>Size of list elements</h2><span id='topic+ls.size'></span>

<h3>Description</h3>

<p>Produces a named array giving the size, in bytes, of the elements of a list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ls.size(x)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="ls.size_+3A_x">x</code></td>
<td>
<p> A list.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> A numeric vector giving the size in bytes of each element of the list <code>x</code>. The elements of the array have the 
same names as the elements of the list. If <code>x</code> is not a list then its size in bytes is returned, un-named.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
b &lt;- list(M=matrix(runif(100),10,10),quote=
"The world is ruled by idiots because only an idiot would want to rule the world.",
fam=binomial())
ls.size(b)
</code></pre>

<hr>
<h2 id='magic'> Stable Multiple Smoothing Parameter Estimation by GCV or UBRE</h2><span id='topic+magic'></span>

<h3>Description</h3>

<p>Function to efficiently estimate smoothing parameters in generalized
ridge regression problems with multiple (quadratic) penalties, by GCV 
or UBRE. The function uses Newton's method in multi-dimensions, backed up by 
steepest descent to iteratively adjust the smoothing parameters for each penalty 
(one penalty may have a smoothing parameter fixed at 1). 
</p>
<p>For maximal numerical stability the method is based on orthogonal decomposition methods, 
and attempts to deal with numerical rank deficiency gracefully using a truncated singular 
value decomposition approach. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>magic(y,X,sp,S,off,L=NULL,lsp0=NULL,rank=NULL,H=NULL,C=NULL,
      w=NULL,gamma=1,scale=1,gcv=TRUE,ridge.parameter=NULL,
      control=list(tol=1e-6,step.half=25,rank.tol=
      .Machine$double.eps^0.5),extra.rss=0,n.score=length(y),nthreads=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="magic_+3A_y">y</code></td>
<td>
<p>is the response data vector.</p>
</td></tr>
<tr><td><code id="magic_+3A_x">X</code></td>
<td>
<p>is the model matrix (more columns than rows are allowed).</p>
</td></tr>
<tr><td><code id="magic_+3A_sp">sp</code></td>
<td>
<p>is the array of smoothing parameters. The vector <code>L%*%log(sp)
+ lsp0</code> 
contains the logs of the smoothing parameters that actually multiply the penalty matrices stored in 
<code>S</code> (<code>L</code> is taken as the identity if <code>NULL</code>). 
Any <code>sp</code> values that are negative are autoinitialized, otherwise they are taken as supplying 
starting values. A supplied starting value will be reset to a default starting value if the
gradient of the GCV/UBRE score is too small at the supplied value.  </p>
</td></tr>
<tr><td><code id="magic_+3A_s">S</code></td>
<td>
<p> is a list of of penalty matrices. <code>S[[i]]</code> is the ith penalty matrix, but note
that it is not stored as a full matrix, but rather as the smallest square matrix including all 
the non-zero elements of the penalty matrix. Element 1,1 of <code>S[[i]]</code>  occupies 
element <code>off[i]</code>, <code>off[i]</code> of the ith penalty matrix. Each <code>S[[i]]</code> must be 
positive semi-definite. Set to <code>list()</code> if there are no smoothing
parameters to be estimated. </p>
</td></tr>
<tr><td><code id="magic_+3A_off">off</code></td>
<td>
<p>is an array indicating the first parameter in the parameter vector that is 
penalized by the penalty involving <code>S[[i]]</code>.</p>
</td></tr>
<tr><td><code id="magic_+3A_l">L</code></td>
<td>
<p>is a matrix mapping <code>log(sp)</code> to the log smoothing parameters that actually multiply the
penalties defined by the elemts of <code>S</code>. Taken as the identity, if <code>NULL</code>. See above under <code>sp</code>.</p>
</td></tr>
<tr><td><code id="magic_+3A_lsp0">lsp0</code></td>
<td>
<p>If <code>L</code> is not <code>NULL</code> this is a vector of constants in
the linear transformation from <code>log(sp)</code> to the actual log smoothing
parameters. So the logs of the smoothing parameters multiplying the
<code>S[[i]]</code> are given by <code>L%*%log(sp) + lsp0</code>. Taken as 0 if <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="magic_+3A_rank">rank</code></td>
<td>
<p> is an array specifying the ranks of the penalties. This is useful, but not 
essential, for forming square roots of the penalty matrices.</p>
</td></tr>
<tr><td><code id="magic_+3A_h">H</code></td>
<td>
<p> is the optional offset penalty - i.e. a penalty with a smoothing parameter fixed at 
1. This is useful for allowing regularization of the estimation process, fixed smoothing 
penalties etc.</p>
</td></tr>
<tr><td><code id="magic_+3A_c">C</code></td>
<td>
<p> is the optional matrix specifying any linear equality constraints on the fitting 
problem. If <code class="reqn">\bf b</code> is the parameter vector then the parameters are forced to satisfy 
<code class="reqn"> {\bf Cb} = {\bf 0} </code>. </p>
</td></tr>
<tr><td><code id="magic_+3A_w">w</code></td>
<td>
<p> the regression weights. If this is a matrix then it is taken as being the 
square root of the inverse of the covariance matrix of <code>y</code>, specifically 
<code class="reqn"> {\bf V}_y^{-1} = {\bf w}^\prime{\bf w}</code>. If <code>w</code> is an array then 
it is taken as the diagonal of this matrix, or simply the weight for each element of 
<code>y</code>. See below for an example using this.</p>
</td></tr>
<tr><td><code id="magic_+3A_gamma">gamma</code></td>
<td>
<p>is an inflation factor for the model degrees of freedom in the GCV or UBRE 
score.</p>
</td></tr>
<tr><td><code id="magic_+3A_scale">scale</code></td>
<td>
<p> is the scale parameter for use with UBRE.</p>
</td></tr>
<tr><td><code id="magic_+3A_gcv">gcv</code></td>
<td>
<p> should be set to <code>TRUE</code> if GCV is to be used, <code>FALSE</code> for UBRE.</p>
</td></tr>
<tr><td><code id="magic_+3A_ridge.parameter">ridge.parameter</code></td>
<td>
<p>It is sometimes useful to apply a ridge penalty to the fitting problem, 
penalizing the parameters in the constrained space directly. Setting this parameter to a value 
greater than zero will cause such a penalty to be used, with the magnitude given by the 
parameter value.</p>
</td></tr>
<tr><td><code id="magic_+3A_control">control</code></td>
<td>
<p> is a list of iteration control constants with the following elements:
</p>

<dl>
<dt>tol</dt><dd><p>The tolerance to use in judging convergence.</p>
</dd>
<dt>step.half</dt><dd><p>If a trial step fails then the method tries halving it up to a maximum of 
<code>step.half</code> times.</p>
</dd>
<dt>rank.tol</dt><dd><p>is a constant used to test for numerical rank deficiency of the problem. 
Basically any singular value less than <code>rank_tol</code> multiplied by the largest singular value of 
the  problem is set to zero.</p>
</dd>
</dl>

</td></tr> 
<tr><td><code id="magic_+3A_extra.rss">extra.rss</code></td>
<td>
<p>is a constant to be added to the residual sum of squares
(squared norm) term in the calculation of the GCV, UBRE and scale parameter
estimate. In conjuction with <code>n.score</code>, this is useful for certain methods for dealing with very large data
sets.</p>
</td></tr>
<tr><td><code id="magic_+3A_n.score">n.score</code></td>
<td>
<p>number to use as the number of data in GCV/UBRE score
calculation: usually the actual number of data, but there are methods 
for dealing with very large datasets that change this.</p>
</td></tr>
<tr><td><code id="magic_+3A_nthreads">nthreads</code></td>
<td>
<p><code>magic</code> can make use of multiple threads if this is set to &gt;1.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>The method is a computationally efficient means of applying GCV or UBRE (often approximately 
AIC) to the 
problem of smoothing parameter selection in generalized ridge regression problems 
of the form:
</p>
<p style="text-align: center;"><code class="reqn"> minimise~ \| { \bf W} ({ \bf Xb - y} ) \|^2 + {\bf b}^\prime {\bf Hb} + \sum_{i=1}^m
\theta_i {\bf b^\prime S}_i{\bf b} </code>
</p>

<p>possibly subject to constraints <code class="reqn"> {\bf Cb}={\bf 0}</code>. 
<code class="reqn"> {\bf X}</code> is a design matrix, <code class="reqn">\bf b</code> a parameter vector, 
<code class="reqn">\bf y</code> a data vector, <code class="reqn">\bf W</code> a weight matrix,
<code class="reqn"> {\bf S}_i</code> a positive semi-definite matrix  of coefficients
defining the ith penalty with associated smoothing parameter <code class="reqn">\theta_i</code>, 
<code class="reqn">\bf H</code> is the positive semi-definite offset penalty matrix  and <code class="reqn">\bf C</code> a 
matrix of coefficients defining any linear equality constraints on the problem. 
<code class="reqn"> {\bf X}</code> need not be of full column rank.
</p>
<p>The <code class="reqn">\theta_i</code> are chosen to minimize either the GCV score:
</p>
<p style="text-align: center;"><code class="reqn">V_g = \frac{n\|{\bf W}({\bf y} - {\bf Ay})\|^2}{[tr({\bf I} - \gamma {\bf A})]^2}</code>
</p>

<p>or the UBRE score:
</p>
<p style="text-align: center;"><code class="reqn">V_u=\|{\bf W}({\bf y}-{\bf Ay})\|^2/n-2 \phi tr({\bf I}-\gamma {\bf A})/n + \phi</code>
</p>

<p>where <code class="reqn">\gamma</code> is <code>gamma</code> the inflation factor for degrees of freedom (usually set to 1) and <code class="reqn">\phi</code> 
is <code>scale</code>, the scale parameter. <code class="reqn">\bf A</code> is the hat matrix (influence matrix) for the fitting problem (i.e
the matrix mapping data to fitted values). Dependence of the scores on the smoothing parameters is through <code class="reqn">\bf A</code>. 
</p>
<p>The method operates by  Newton or steepest descent updates of the logs of the 
<code class="reqn">\theta_i</code>. A key aspect of the method is stable and economical calculation of the 
first and second derivatives of the scores w.r.t. the log smoothing parameters. 
Because the GCV/UBRE scores are flat w.r.t. very large or very small <code class="reqn">\theta_i</code>, 
it's important to get good starting parameters, and to be careful not to step into a flat region
of the smoothing parameter space. For this reason the algorithm rescales any Newton step that 
would result in a <code class="reqn">log(\theta_i)</code> change of more than 5. Newton steps are 
only used if the Hessian of the GCV/UBRE is postive definite, otherwise steepest descent is 
used. Similarly steepest descent is used if the Newton step has to be contracted too far 
(indicating that the quadratic model underlying Newton is poor). All initial steepest descent 
steps are scaled so that their largest component is 1. However a step is calculated, 
it is never expanded if it is successful (to avoid flat portions of the objective), 
but steps are successively halved if they do not decrease the GCV/UBRE score, until 
they do, or the direction is deemed to have failed. (Given the smoothing parameters the optimal 
<code class="reqn">\bf b</code> parameters are easily found.)
</p>
<p>The method is coded in <code>C</code> with matrix factorizations performed using LINPACK and LAPACK routines.
</p>


<h3>Value</h3>

<p>The function returns a list with the following items:
</p>
<table>
<tr><td><code>b</code></td>
<td>
<p>The best fit parameters given the estimated smoothing parameters.</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>
<p>the estimated (GCV) or supplied (UBRE) scale parameter.</p>
</td></tr>
<tr><td><code>score</code></td>
<td>
<p>the minimized GCV or UBRE score.</p>
</td></tr>
<tr><td><code>sp</code></td>
<td>
<p>an array of the estimated smoothing parameters.</p>
</td></tr>
<tr><td><code>sp.full</code></td>
<td>
<p>an array of the smoothing parameters that actually multiply the elements of
<code>S</code> (same as <code>sp</code> if <code>L</code> was <code>NULL</code>). This is <code>exp(L%*%log(sp))</code>.</p>
</td></tr>
<tr><td><code>rV</code></td>
<td>
<p>a factored form of the parameter covariance matrix. The (Bayesian)  covariance
matrix of the parametes <code>b</code> is given by <code>rV%*%t(rV)*scale</code>. </p>
</td></tr>
<tr><td><code>gcv.info</code></td>
<td>
<p>is a list of information about the performance of the method with the following elements:
</p>

<dl>
<dt>full.rank</dt><dd><p>The apparent rank of the problem: number of parameters less number of equality constraints.</p>
</dd>
<dt>rank</dt><dd><p>The estimated actual rank of the problem (at the final iteration of the method).</p>
</dd>
<dt>fully.converged</dt><dd><p>is <code>TRUE</code> if the method converged by satisfying the convergence criteria, and <code>FALSE</code> if it coverged 
by failing to decrease the score along the search direction.</p>
</dd>
<dt>hess.pos.def</dt><dd><p>is <code>TRUE</code> if the hessian of the UBRE or GCV score was positive definite at convergence.</p>
</dd>
<dt>iter</dt><dd><p>is the number of Newton/Steepest descent iterations taken.</p>
</dd>
<dt>score.calls</dt><dd><p>is the number of times that the GCV/UBRE score had to be evaluated.</p>
</dd>
<dt>rms.grad</dt><dd><p>is the root mean square of the gradient of the UBRE/GCV score w.r.t. the smoothing parameters.</p>
</dd>
<dt>R</dt><dd><p>The factor R from the QR decomposition of the weighted model matrix. This is un-pivoted so that column 
order corresponds to <code>X</code>. So it may not be upper triangular.</p>
</dd></dl>

</td></tr>
</table>
<p>Note that some further useful quantities can be obtained using <code><a href="#topic+magic.post.proc">magic.post.proc</a></code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2004) Stable and efficient multiple smoothing parameter estimation for
generalized additive models. J. Amer. Statist. Ass. 99:673-686
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

  
<p><code><a href="#topic+magic.post.proc">magic.post.proc</a></code>,<code><a href="#topic+gam">gam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Use `magic' for a standard additive model fit ... 
   library(mgcv)
   set.seed(1);n &lt;- 200;sig &lt;- 1
   dat &lt;- gamSim(1,n=n,scale=sig)
   k &lt;- 30
## set up additive model
   G &lt;- gam(y~s(x0,k=k)+s(x1,k=k)+s(x2,k=k)+s(x3,k=k),fit=FALSE,data=dat)
## fit using magic (and gam default tolerance)
   mgfit &lt;- magic(G$y,G$X,G$sp,G$S,G$off,rank=G$rank,
                  control=list(tol=1e-7,step.half=15))
## and fit using gam as consistency check
   b &lt;- gam(G=G)
   mgfit$sp;b$sp  # compare smoothing parameter estimates
   edf &lt;- magic.post.proc(G$X,mgfit,G$w)$edf # get e.d.f. per param
   range(edf-b$edf)  # compare

## p&gt;n example... fit model to first 100 data only, so more
## params than data...

   mgfit &lt;- magic(G$y[1:100],G$X[1:100,],G$sp,G$S,G$off,rank=G$rank)
   edf &lt;- magic.post.proc(G$X[1:100,],mgfit,G$w[1:100])$edf

## constrain first two smooths to have identical smoothing parameters
   L &lt;- diag(3);L &lt;- rbind(L[1,],L)
   mgfit &lt;- magic(G$y,G$X,rep(-1,3),G$S,G$off,L=L,rank=G$rank,C=G$C)

## Now a correlated data example ... 
    library(nlme)
## simulate truth
    set.seed(1);n&lt;-400;sig&lt;-2
    x &lt;- 0:(n-1)/(n-1)
    f &lt;- 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
## produce scaled covariance matrix for AR1 errors...
    V &lt;- corMatrix(Initialize(corAR1(.6),data.frame(x=x)))
    Cv &lt;- chol(V)  # t(Cv)%*%Cv=V
## Simulate AR1 errors ...
    e &lt;- t(Cv)%*%rnorm(n,0,sig) # so cov(e) = V * sig^2
## Observe truth + AR1 errors
    y &lt;- f + e 
## GAM ignoring correlation
    par(mfrow=c(1,2))
    b &lt;- gam(y~s(x,k=20))
    plot(b);lines(x,f-mean(f),col=2);title("Ignoring correlation")
## Fit smooth, taking account of *known* correlation...
    w &lt;- solve(t(Cv)) # V^{-1} = w'w
    ## Use `gam' to set up model for fitting...
    G &lt;- gam(y~s(x,k=20),fit=FALSE)
    ## fit using magic, with weight *matrix*
    mgfit &lt;- magic(G$y,G$X,G$sp,G$S,G$off,rank=G$rank,C=G$C,w=w)
## Modify previous gam object using new fit, for plotting...    
    mg.stuff &lt;- magic.post.proc(G$X,mgfit,w)
    b$edf &lt;- mg.stuff$edf;b$Vp &lt;- mg.stuff$Vb
    b$coefficients &lt;- mgfit$b 
    plot(b);lines(x,f-mean(f),col=2);title("Known correlation")
</code></pre>

<hr>
<h2 id='magic.post.proc'>Auxilliary information from magic fit</h2><span id='topic+magic.post.proc'></span>

<h3>Description</h3>

<p>Obtains Bayesian parameter covariance matrix, frequentist
parameter estimator covariance matrix, estimated degrees of 
freedom for each parameter and leading diagonal of influence/hat matrix, 
for a penalized regression estimated by <code>magic</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>magic.post.proc(X,object,w=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="magic.post.proc_+3A_x">X</code></td>
<td>
<p> is the model matrix.</p>
</td></tr>
<tr><td><code id="magic.post.proc_+3A_object">object</code></td>
<td>
<p>is the list returned by <code>magic</code> after fitting the 
model with model matrix <code>X</code>.</p>
</td></tr>
<tr><td><code id="magic.post.proc_+3A_w">w</code></td>
<td>
<p>is the weight vector used in fitting, or the weight matrix used 
in fitting (i.e. supplied to <code>magic</code>, if one was.). If <code>w</code> is a vector then its
elements are typically proportional to reciprocal variances (but could even be negative). 
If <code>w</code> is a matrix then 
<code>t(w)%*%w</code> should typically give
the inverse of the covariance matrix of the response data supplied to <code>magic</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

 <p><code>object</code> contains <code>rV</code> (<code class="reqn"> {\bf V}</code>, say), and 
<code>scale</code> (<code class="reqn"> \phi</code>, say) which can be 
used to obtain the require quantities as follows. The Bayesian covariance matrix of 
the parameters is <code class="reqn"> {\bf VV}^\prime \phi</code>. The vector of 
estimated degrees of freedom for each parameter is the leading diagonal of 
<code class="reqn"> {\bf VV}^\prime {\bf X}^\prime {\bf W}^\prime {\bf W}{\bf X}</code> 
where <code class="reqn">\bf{W}</code> is either the 
weight matrix <code>w</code> or the matrix <code>diag(w)</code>. The 
hat/influence  matrix is given by 
<code class="reqn"> {\bf WX}{\bf VV}^\prime {\bf X}^\prime {\bf W}^\prime </code>
.
</p>
<p>The frequentist parameter estimator covariance matrix is 
<code class="reqn"> {\bf VV}^\prime {\bf X}^\prime {\bf W}^\prime {\bf WXVV}^\prime \phi</code>: 
it is sometimes useful for testing terms for equality to zero.
</p>


<h3>Value</h3>

<p> A list with three items:
</p>
<table>
<tr><td><code>Vb</code></td>
<td>
<p>the Bayesian covariance matrix of the model parameters.</p>
</td></tr>
<tr><td><code>Ve</code></td>
<td>
<p>the frequentist covariance matrix for the parameter estimators.</p>
</td></tr>
<tr><td><code>hat</code></td>
<td>
<p>the leading diagonal of the hat (influence) matrix.</p>
</td></tr>
<tr><td><code>edf</code></td>
<td>
<p>the array giving the estimated degrees of freedom associated 
with each parameter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+magic">magic</a></code></p>

<hr>
<h2 id='mgcv.FAQ'>Frequently Asked Questions for package mgcv</h2><span id='topic+mgcv.FAQ'></span>

<h3>Description</h3>

<p> This page provides answers to some of the questions that get asked most often about mgcv</p>


<h3>FAQ list</h3>


<ol>
<li> <p><b>How can I compare gamm models?</b> In the identity link normal errors case, then AIC and hypotheis 
testing based methods are fine. Otherwise it is best to work out a strategy based on the <code><a href="#topic+summary.gam">summary.gam</a></code> 
Alternatively, simple random effects can be fitted with <code><a href="#topic+gam">gam</a></code>, which makes comparison straightforward. 
Package <code>gamm4</code> is an alternative, which allows AIC type model selection for generalized models.
</p>
</li>
<li> <p><b>How do I get the equation of an estimated smooth?</b> This slightly misses the point 
of semi-parametric modelling: the idea is that we estimate the form of the function from data
without assuming that it has a particular simple functional form. Of course for practical computation
the functions do have underlying mathematical representations, but they are not very helpful, when written 
down. If you do need the functional forms then see chapter 5 of Wood (2017). However for most purposes it 
is better to use <code><a href="#topic+predict.gam">predict.gam</a></code> to evaluate the function for whatever argument values you need. 
If derivatives are required then the simplest approach is to use finite differencing (which also allows 
SEs etc to be calculated).
</p>
</li>
<li> <p><b>Some of my smooths are estimated to be straight lines and their confidence intervals vanish at some point in the middle. What is wrong?</b> Nothing. Smooths are subject to sum-to-zero identifiability constraints. If a smooth is estimated to be a straight line then it consequently has one degree of freedom, and there is no choice about where it passes through zero &mdash; so the CI must vanish at that point. 
</p>
</li>
<li> <p><b>How do I test whether a smooth is significantly different from a straight line</b>. See <code><a href="#topic+tprs">tprs</a></code> and the example therein.
</p>
</li>
<li> <p><b>An example from an mgcv helpfile gives an error - is this a bug?</b> It might be, but first please check that the version of mgcv you have loaded into R corresponds to the version from which the helpfile came. Many such problems are caused by trying to run code only supported in a later mgcv version in an earlier version. Another possibility is that you have an object loaded whose name clashes with an mgcv function (for example you are trying to use the mgcv <code>multinom</code> function, but have another object called <code>multinom</code> loaded.)
</p>
</li>
<li> <p><b>Some code from Wood (2006) causes an error: why?</b> The book was written using mgcv version 1.3. To allow for 
REML estimation of smoothing parameters in versions 1.5, some changes had to be made to the syntax. In particular the 
function <code>gam.method</code> no longer exists. The smoothness selection method (GCV, REML etc) is now controlled by the
<code>method</code> argument to <code>gam</code> while the optimizer is selected using the <code>optimizer</code> argument. 
See <code><a href="#topic+gam">gam</a></code> for details.
</p>
</li>
<li> <p><b>Why is a model object saved under a previous mgcv version not usable with the current mgcv version?</b> 
I'm sorry about this issue, I know it's really annoying. Here's my defence. 
Each mgcv version is run through an extensive test suite before release, to ensure that it gives the same results 
as before, unless there are good statistical reasons why not (e.g. improvements to p-value approximation, fixing 
of an error). However it is sometimes necessary to modify the internal structure of model objects in a way that makes an old style object unusable with a newer version. For example, bug fixes or new R features sometimes require changes in the way 
that things are computed which in turn require modification of the object structure. Similarly improvements, such as the 
ability to compute smoothing parameters by RE/ML require object level changes. The only fix to this problem is to access the old object using the original mgcv version (available on CRAN), or to recompute the fit using the current mgcv version.
</p>
</li>
<li> <p><b>When using <code>gamm</code> or <code>gamm4</code>, the reported AIC is different for the <code>gam</code> object and 
the <code>lme</code> or <code>lmer</code> object. Why is this?</b> There are several reasons for this. The most important is that the 
models being used are actually different in the two representations. When treating the GAM as a mixed model, you are 
implicitly assuming that if you gathered a replicate dataset, the smooths in your model would look completely 
different to the smooths from the original model, except for having the same degree of smoothness. Technically you would expect 
the smooths to be drawn afresh from their distribution under the random effects model. When viewing the gam from the 
usual penalized regression perspective, you would expect smooths to look broadly similar under replication of the data. 
i.e. you are really using Bayesian model for the smooths, rather than a random effects model (it's just that the frequentist random 
effects and Bayesian computations happen to coincide for computing the estimates). As a result of the different assumptions 
about the data generating process, AIC model comparisons can give rather different answers depending on the model adopted. 
Which you use should depend on which model you really think is appropriate. In addition the computations of the AICs are 
different. The mixed model AIC uses the marginal liklihood and the corresponding number of model parameters. The gam model 
uses the penalized likelihood and the effective degrees of freedom. 
</p>
</li>
<li> <p><b>What does 'mgcv' stand for?</b> '<b>M</b>ixed <b>G</b>AM <b>C</b>omputation <b>V</b>ehicle', is my current best effort (let me know if you can do better). Originally it stood for &lsquo;Multiple GCV&rsquo;, which has long since ceased to be usefully descriptive, (and I can't really change 'mgcv' now without causing disruption). On a bad inbox day '<b>M</b>ad <b>G</b>AM <b>C</b>omputing <b>V</b>ulture'.
</p>
</li>
<li> <p><b>My new method is failing to beat mgcv, what can I do?</b> If speed is the problem, then make sure that you use the 
slowest basis possible (<code>"tp"</code>) with a large sample size, and experiment with different optimizers to find one that is 
slow for your problem. For prediction error/MSE, then leaving the smoothing basis dimensions at their 
arbitrary defaults, when these are inappropriate for the problem setting, is a good way of reducing performance. 
Similarly, using p-splines in place of derivative penalty based splines will often shave a little more 
from the performance here. Unlike REML/ML, prediction error based smoothness selection criteria such as Mallows Cp and GCV 
often produce a small proportion of severe overfits, so careful choise of smoothness selection method can help further. 
In particular GCV etc. usually result in worse confidence interval and p-value performance than ML or REML. If all this 
fails, try using a really odd simulation setup for which mgcv is clearly not suited: for example poor performance is almost 
guaranteed for small noisy datasets with large numbers of predictors.
</p>
</li></ol>



<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood S.N. (2006) Generalized Additive Models: An Introduction with R. Chapman
and Hall/CRC Press.
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>

<hr>
<h2 id='mgcv.parallel'>Parallel computation in mgcv.
</h2><span id='topic+mgcv.parallel'></span>

<h3>Description</h3>

<p><code>mgcv</code> can make some use of multiple cores or a cluster.
</p>
<p><code><a href="#topic+bam">bam</a></code> can use an openMP based parallelization approach alongside discretisation of covariates to achieve substantial speed ups. This is selected using the <code>discrete=TRUE</code> option to <code>bam</code>, withthe number of threads controlled via the <code>nthreads</code> argument. This is the approach that scales best. See example below.
</p>
<p>Alternatively, function <code><a href="#topic+bam">bam</a></code> can use the facilities provided in the <a href="parallel.html#topic+parallel-package">parallel</a> package. See examples below. Note that most multi-core machines are memory bandwidth limited, so parallel speed up tends to be rather variable. 
</p>
<p>Function <code><a href="#topic+gam">gam</a></code> can use parallel threads on a (shared memory) multi-core 
machine via <code>openMP</code> (where this is supported). To do this, set the desired number of threads by setting <code>nthreads</code> to the number of cores to use, in the <code>control</code> argument of <code><a href="#topic+gam">gam</a></code>. Note that, for the most part, only the dominant <code class="reqn">O(np^2)</code> steps are parallelized (n is number of data, p number of parameters). For additive Gaussian models estimated by GCV, the speed up can be disappointing as these employ an <code class="reqn">O(p^3)</code> SVD step that can also have substantial cost in practice. <code><a href="#topic+magic">magic</a></code> can also use multiple cores, but the same comments apply as for the GCV Gaussian additive model. 
</p>
<p>When using <code><a href="#topic+NCV">NCV</a></code> with <code><a href="#topic+gam">gam</a></code> worthwhile performance improvements are available by setting <code>ncv.threads</code>in <code><a href="#topic+gam.control">gam.control</a></code>.   
</p>
<p>If <code>control$nthreads</code> is set to more than the number of cores detected, then only the number of detected cores is used. Note that using virtual cores usually gives very little speed up, and can even slow computations slightly. For example, many Intel processors reporting 4 cores actually have 2 physical cores, each with 2 virtual cores, so using 2 threads gives a marked increase in speed, while using 4 threads makes little extra difference. 
</p>
<p>Note that on Intel and similar processors the maximum performance is usually achieved by disabling Hyper-Threading in BIOS, and then setting the number of threads to the number of physical cores used. This prevents the operating system scheduler from sending 2 floating point intensive threads to the same physical core, where they have to share a floating point unit (and cache) and therefore slow each other down. The scheduler tends to do this under the manager - worker multi-threading approach used in mgcv, since the manager thread looks very busy up to the point at which the workers are set to work, and at the point of scheduling the sceduler has no way of knowing that the manager thread actually has nothing more to do until the workers are finished. If you are working on a many cored platform where you can not disable hyper-threading then it may be worth setting the number of threads to one less than the number of physical cores, to reduce the frequency of such scheduling problems. 
</p>
<p>mgcv's work splitting always makes the simple assumption that all your cores are equal, and you are not sharing them with other floating point intensive threads. 
</p>
<p>In addition to hyper-threading several features may lead to apparently poor scaling. The first is that many CPUs have a Turbo mode, whereby a few cores can be run at higher frequency, provided the overall power used by the CPU does not exceed design limits, however it is not possible for all cores on the CPU to run at this frequency. So as you add threads eventually the CPU frequency has to be reduced below the Turbo frequency, with the result that you don't get the expected speed up from adding cores. Secondly, most modern CPUs have their frequency set dynamically according to load. You may need to set the system power management policy to favour high performance in order to maximize the chance that all threads run at the speed you were hoping for (you can turn off dynamic power control in BIOS, but then you turn off the possibility of Turbo also). 
</p>
<p>Because the computational burden in <code>mgcv</code> is all in the linear algebra, then parallel computation may provide reduced or no benefit with a tuned BLAS. This is particularly the case if you are using a multi threaded BLAS, but a BLAS that is tuned to make efficient use of a particular cache size may also experience loss of performance if threads have to share the cache.  
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon.wood@r-project.org&gt;
</p>


<h3>References</h3>

<p><a href="https://hpc-tutorials.llnl.gov/openmp/">https://hpc-tutorials.llnl.gov/openmp/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## illustration of multi-threading with gam...

require(mgcv);set.seed(9)
dat &lt;- gamSim(1,n=2000,dist="poisson",scale=.1)
k &lt;- 12;bs &lt;- "cr";ctrl &lt;- list(nthreads=2)

system.time(b1&lt;-gam(y~s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k)
            ,family=poisson,data=dat,method="REML"))[3]

system.time(b2&lt;-gam(y~s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k),
            family=poisson,data=dat,method="REML",control=ctrl))[3]

## Poisson example on a cluster with 'bam'. 
## Note that there is some overhead in initializing the 
## computation on the cluster, associated with loading 
## the Matrix package on each node. Sample sizes are low
## here to keep example quick -- for such a small model
## little or no advantage is likely to be seen.
k &lt;- 13;set.seed(9)
dat &lt;- gamSim(1,n=6000,dist="poisson",scale=.1)

require(parallel)  
nc &lt;- 2   ## cluster size, set for example portability
if (detectCores()&gt;1) { ## no point otherwise
  cl &lt;- makeCluster(nc) 
  ## could also use makeForkCluster, but read warnings first!
} else cl &lt;- NULL
  
system.time(b3 &lt;- bam(y ~ s(x0,bs=bs,k=7)+s(x1,bs=bs,k=7)+s(x2,bs=bs,k=k)
            ,data=dat,family=poisson(),chunk.size=5000,cluster=cl))

fv &lt;- predict(b3,cluster=cl) ## parallel prediction

if (!is.null(cl)) stopCluster(cl)
b3

## Alternative, better scaling example, using the discrete option with bam...

system.time(b4 &lt;- bam(y ~ s(x0,bs=bs,k=7)+s(x1,bs=bs,k=7)+s(x2,bs=bs,k=k)
            ,data=dat,family=poisson(),discrete=TRUE,nthreads=2))

</code></pre>

<hr>
<h2 id='mini.roots'>Obtain square roots of penalty matrices</h2><span id='topic+mini.roots'></span>

<h3>Description</h3>

<p>INTERNAL function to obtain square roots, <code>B[[i]]</code>, of the penalty matrices <code>S[[i]]</code>'s having as few
columns as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mini.roots(S, off, np, rank = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mini.roots_+3A_s">S</code></td>
<td>
<p>a list of penalty matrices, in packed form.</p>
</td></tr>
<tr><td><code id="mini.roots_+3A_off">off</code></td>
<td>
<p>a vector where the i-th element is the offset for the i-th matrix. 
The elements in columns <code>1:off[i]</code> of <code>B[[i]]</code> will be equal to zero.</p>
</td></tr>
<tr><td><code id="mini.roots_+3A_np">np</code></td>
<td>
<p>total number of parameters.</p>
</td></tr>
<tr><td><code id="mini.roots_+3A_rank">rank</code></td>
<td>
<p>here <code>rank[i]</code> is optional supplied rank of <code>S[[i]]</code>. Set <code>rank[i] &lt; 1</code>, or 
<code>rank=NULL</code> to estimate.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of matrix square roots such that <code>S[[i]]=B[[i]]%*%t(B[[i]])</code>.
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>

<hr>
<h2 id='missing.data'>Missing data in GAMs</h2><span id='topic+missing.data'></span>

<h3>Description</h3>

<p>If there are missing values in the response or covariates of a GAM then the default is simply to use only the &lsquo;complete cases&rsquo;. If there are many missing covariates, this can get rather wasteful. One possibility is then to use imputation. Another is to substitute a simple random effects model in which the <code>by</code> variable mechanism is used to set <code>s(x)</code> to zero for any missing <code>x</code>, while a Gaussian random effect is then substituted for the &lsquo;missing&rsquo; s(x). See the example for details of how this works, and <code><a href="#topic+gam.models">gam.models</a></code> for the necessary background on <code>by</code> variables. 
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon.wood@r-project.org&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam.vcomp">gam.vcomp</a></code>, <code><a href="#topic+gam.models">gam.models</a></code>, <code><a href="#topic+s">s</a></code>, 
<code><a href="#topic+smooth.construct.re.smooth.spec">smooth.construct.re.smooth.spec</a></code>,<code><a href="#topic+gam">gam</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## The example takes a couple of minutes to run...

require(mgcv)
par(mfrow=c(4,4),mar=c(4,4,1,1))
for (sim in c(1,7)) { ## cycle over uncorrelated and correlated covariates
  n &lt;- 350;set.seed(2)
  ## simulate data but randomly drop 300 covariate measurements
  ## leaving only 50 complete cases...
  dat &lt;- gamSim(sim,n=n,scale=3) ## 1 or 7
  drop &lt;- sample(1:n,300) ## to
  for (i in 2:5) dat[drop[1:75+(i-2)*75],i] &lt;- NA

  ## process data.frame producing binary indicators of missingness,
  ## mx0, mx1 etc. For each missing value create a level of a factor
  ## idx0, idx1, etc. So idx0 has as many levels as x0 has missing 
  ## values. Replace the NA's in each variable by the mean of the 
  ## non missing for that variable...

  dname &lt;- names(dat)[2:5]
  dat1 &lt;- dat
  for (i in 1:4) {
    by.name &lt;- paste("m",dname[i],sep="") 
    dat1[[by.name]] &lt;- is.na(dat1[[dname[i]]])
    dat1[[dname[i]]][dat1[[by.name]]] &lt;- mean(dat1[[dname[i]]],na.rm=TRUE)
    lev &lt;- rep(1,n);lev[dat1[[by.name]]] &lt;- 1:sum(dat1[[by.name]])
    id.name &lt;- paste("id",dname[i],sep="")
    dat1[[id.name]] &lt;- factor(lev) 
    dat1[[by.name]] &lt;- as.numeric(dat1[[by.name]])
  }

  ## Fit a gam, in which any missing value contributes zero 
  ## to the linear predictor from its smooth, but each 
  ## missing has its own random effect, with the random effect 
  ## variances being specific to the variable. e.g.
  ## for s(x0,by=ordered(!mx0)), declaring the `by' as an ordered
  ## factor ensures that the smooth is centred, but multiplied
  ## by zero when mx0 is one (indicating a missing x0). This means
  ## that any value (within range) can be put in place of the 
  ## NA for x0.  s(idx0,bs="re",by=mx0) produces a separate Gaussian 
  ## random effect for each missing value of x0 (in place of s(x0),
  ## effectively). The `by' variable simply sets the random effect to 
  ## zero when x0 is non-missing, so that we can set idx0 to any 
  ## existing level for these cases.   

  b &lt;- bam(y~s(x0,by=ordered(!mx0))+s(x1,by=ordered(!mx1))+
             s(x2,by=ordered(!mx2))+s(x3,by=ordered(!mx3))+
             s(idx0,bs="re",by=mx0)+s(idx1,bs="re",by=mx1)+
             s(idx2,bs="re",by=mx2)+s(idx3,bs="re",by=mx3)
             ,data=dat1,discrete=TRUE)

  for (i in 1:4) plot(b,select=i) ## plot the smooth effects from b

  ## fit the model to the `complete case' data...
  b2 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML")
  plot(b2) ## plot the complete case results
}

</code></pre>

<hr>
<h2 id='model.matrix.gam'>Extract model matrix from GAM fit</h2><span id='topic+model.matrix.gam'></span>

<h3>Description</h3>

<p>Obtains the model matrix from a fitted <code>gam</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
model.matrix(object, ...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="model.matrix.gam_+3A_object">object</code></td>
<td>
<p> fitted model object of class <code>gam</code> as produced by <code>gam()</code>.</p>
</td></tr>
<tr><td><code id="model.matrix.gam_+3A_...">...</code></td>
<td>
<p> other arguments, passed to <code><a href="#topic+predict.gam">predict.gam</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calls <code><a href="#topic+predict.gam">predict.gam</a></code> with no <code>newdata</code> argument and
<code>type="lpmatrix"</code> in order to obtain the model matrix of <code>object</code>.
</p>


<h3>Value</h3>

<p> A model matrix.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood S.N. (2006b) Generalized Additive Models: An Introduction with R. Chapman
and Hall/CRC Press.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+gam">gam</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> 
require(mgcv)
n &lt;- 15
x &lt;- runif(n)
y &lt;- sin(x*2*pi) + rnorm(n)*.2
mod &lt;- gam(y~s(x,bs="cc",k=6),knots=list(x=seq(0,1,length=6)))
model.matrix(mod)
</code></pre>

<hr>
<h2 id='mono.con'>Monotonicity constraints for a cubic regression spline</h2><span id='topic+mono.con'></span>

<h3>Description</h3>

<p>Finds linear constraints sufficient for monotonicity (and
optionally upper and/or lower boundedness) of a cubic regression
spline. The basis representation assumed is that given by the
<code>gam</code>, <code>"cr"</code> basis: that is the spline has a set of knots,
which have fixed x values, but the y values of which constitute the
parameters of the spline.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mono.con(x,up=TRUE,lower=NA,upper=NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mono.con_+3A_x">x</code></td>
<td>
<p>The array of knot locations.</p>
</td></tr>
<tr><td><code id="mono.con_+3A_up">up</code></td>
<td>
<p>If <code>TRUE</code> then the constraints imply increase, if
<code>FALSE</code> then decrease. </p>
</td></tr>
<tr><td><code id="mono.con_+3A_lower">lower</code></td>
<td>
<p>This specifies the lower bound on the spline unless it is
<code>NA</code> in which case no lower bound is imposed.</p>
</td></tr>
<tr><td><code id="mono.con_+3A_upper">upper</code></td>
<td>
<p>This specifies the upper bound on the spline unless it is
<code>NA</code> in which case no upper bound is imposed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Consider the natural cubic spline passing through the points
<code class="reqn"> \{x_i,p_i:i=1 \ldots n \} </code>.  Then it is possible
to find a relatively small set of linear constraints on <code class="reqn">\mathbf{p}</code>
sufficient to ensure monotonicity (and bounds if required):
<code class="reqn">\mathbf{Ap}\ge\mathbf{b}</code>.
Details are given in Wood (1994).
</p>


<h3>Value</h3>

<p>a list containing constraint matrix <code>A</code> and constraint vector <code>b</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> </p>


<h3>References</h3>

<p>Gill, P.E., Murray, W. and Wright, M.H. (1981)
<em>Practical Optimization</em>. Academic Press, London.
</p>
<p>Wood, S.N. (1994) Monotonic smoothing splines fitted by cross validation.
<em>SIAM Journal on Scientific Computing</em> <b>15</b>(5), 1126&ndash;1133.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+magic">magic</a></code>, <code><a href="#topic+pcls">pcls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see ?pcls
</code></pre>

<hr>
<h2 id='mroot'>Smallest square root of matrix</h2><span id='topic+mroot'></span>

<h3>Description</h3>

<p> Find a square root of a positive semi-definite matrix, 
having as few columns as possible. Uses either pivoted choleski 
decomposition or singular value decomposition to do this.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mroot(A,rank=NULL,method="chol")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mroot_+3A_a">A</code></td>
<td>
<p> The positive semi-definite matrix, a square root of which is 
to be found.</p>
</td></tr>
<tr><td><code id="mroot_+3A_rank">rank</code></td>
<td>
<p>if the rank of the matrix <code>A</code> is known then it should 
be supplied. <code>NULL</code> or &lt;1 imply that it should be estimated.</p>
</td></tr>
<tr><td><code id="mroot_+3A_method">method</code></td>
<td>
 <p><code>"chol"</code> to use pivoted choloeski decompositon, 
which is fast but tends to over-estimate rank. <code>"svd"</code> to use 
singular value decomposition, which is slow, but is the most accurate way 
to estimate rank.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> The function uses SVD, or a pivoted 
Choleski routine. It is primarily of use for turning penalized regression 
problems into ordinary regression problems.</p>


<h3>Value</h3>

<p> A matrix, <code class="reqn"> {\bf B}</code> with as many columns as the rank of 
<code class="reqn"> {\bf A}</code>, and such that <code class="reqn"> {\bf A} = {\bf BB}^\prime</code>.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(mgcv)
  set.seed(0)
  a &lt;- matrix(runif(24),6,4)
  A &lt;- a%*%t(a) ## A is +ve semi-definite, rank 4
  B &lt;- mroot(A) ## default pivoted choleski method
  tol &lt;- 100*.Machine$double.eps
  chol.err &lt;- max(abs(A-B%*%t(B)));chol.err
  if (chol.err&gt;tol) warning("mroot (chol) suspect")
  B &lt;- mroot(A,method="svd") ## svd method
  svd.err &lt;- max(abs(A-B%*%t(B)));svd.err
  if (svd.err&gt;tol) warning("mroot (svd) suspect")  
</code></pre>

<hr>
<h2 id='multinom'>GAM multinomial logistic regression</h2><span id='topic+multinom'></span>

<h3>Description</h3>

<p>Family for use with <code><a href="#topic+gam">gam</a></code>, implementing regression for categorical response data. Categories must be coded 0 to K, where K is a positive integer. <code><a href="#topic+gam">gam</a></code> should be called with a list of K formulae, one for each category except category zero (extra formulae for shared terms may also be supplied: see <code><a href="#topic+formula.gam">formula.gam</a></code>). The first formula also specifies the response variable.      
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinom(K=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multinom_+3A_k">K</code></td>
<td>
<p>There are K+1 categories and K linear predictors. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model has K linear predictors, <code class="reqn">\eta_j</code>, each dependent on smooth functions of predictor variables, in the usual way. If response variable, y, contains the class labels 0,...,K then the likelihood for y&gt;0 is <code class="reqn">\exp(\eta_y)/\{1+\sum_j \exp(\eta_j) \}</code>. If y=0 the likelihood is <code class="reqn">1/\{1+\sum_j \exp(\eta_j) \}</code>. In the two class case this is just a binary logistic regression model. The implementation uses the approach to GAMLSS models described in Wood, Pya and Saefken (2016).
</p>
<p>The residuals returned for this model are simply the square root of -2 times the deviance for each observation, with a positive sign if the observed y is the most probable class for this observation, and a negative sign otherwise. 
</p>
<p>Use <code>predict</code> with <code>type="response"</code> to get the predicted probabilities in each category.
</p>
<p>Note that the model is not completely invariant to category relabelling, even if all linear predictors have the same form. Realistically this model is unlikely to be suitable for problems with large numbers of categories. Missing categories are not supported.   
</p>


<h3>Value</h3>

<p>An object of class <code>general.family</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>, with a variance bug fix from Max Goplerud.
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ocat">ocat</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(6)
## simulate some data from a three class model
n &lt;- 1000
f1 &lt;- function(x) sin(3*pi*x)*exp(-x)
f2 &lt;- function(x) x^3
f3 &lt;- function(x) .5*exp(-x^2)-.2
f4 &lt;- function(x) 1
x1 &lt;- runif(n);x2 &lt;- runif(n)
eta1 &lt;- 2*(f1(x1) + f2(x2))-.5
eta2 &lt;- 2*(f3(x1) + f4(x2))-1
p &lt;- exp(cbind(0,eta1,eta2))
p &lt;- p/rowSums(p) ## prob. of each category 
cp &lt;- t(apply(p,1,cumsum)) ## cumulative prob.
## simulate multinomial response with these probabilities
## see also ?rmultinom
y &lt;- apply(cp,1,function(x) min(which(x&gt;runif(1))))-1
## plot simulated data...
plot(x1,x2,col=y+3)

## now fit the model...
b &lt;- gam(list(y~s(x1)+s(x2),~s(x1)+s(x2)),family=multinom(K=2))
plot(b,pages=1)
gam.check(b)

## now a simple classification plot...
expand.grid(x1=seq(0,1,length=40),x2=seq(0,1,length=40)) -&gt; gr
pp &lt;- predict(b,newdata=gr,type="response")
pc &lt;- apply(pp,1,function(x) which(max(x)==x)[1])-1
plot(gr,col=pc+3,pch=19)

## example sharing a smoother between linear predictors
## ?formula.gam gives more details.
b &lt;- gam(list(y~s(x1),~s(x1),1+2~s(x2)-1),family=multinom(K=2))
plot(b,pages=1)

</code></pre>

<hr>
<h2 id='mvn'>Multivariate normal additive models</h2><span id='topic+mvn'></span>

<h3>Description</h3>

<p>Family for use with <code><a href="#topic+gam">gam</a></code> implementing smooth multivariate Gaussian regression. 
The means for each dimension are given by a separate linear predictor, which may contain smooth components. Extra linear predictors may also be specified giving terms which are shared between components (see <code><a href="#topic+formula.gam">formula.gam</a></code>). The Choleski factor of the response precision matrix is estimated as part of fitting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvn(d=2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvn_+3A_d">d</code></td>
<td>
<p>The dimension of the response (&gt;1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The response is <code>d</code> dimensional multivariate normal, where the covariance matrix is estimated, 
and the means for each dimension have sperate linear predictors. Model sepcification is via a list of gam like 
formulae - one for each dimension. See example.
</p>
<p>Currently the family ignores any prior weights, and is implemented using first derivative information sufficient for BFGS estimation of smoothing parameters. <code>"response"</code> residuals give raw residuals, while <code>"deviance"</code> 
residuals are standardized to be approximately independent standard normal if all is well.
</p>


<h3>Value</h3>

<p>An object of class <code>general.family</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood  <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+gaussian">gaussian</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## simulate some data...
V &lt;- matrix(c(2,1,1,2),2,2)
f0 &lt;- function(x) 2 * sin(pi * x)
f1 &lt;- function(x) exp(2 * x)
f2 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
            (10 * x)^3 * (1 - x)^10
n &lt;- 300
x0 &lt;- runif(n);x1 &lt;- runif(n);
x2 &lt;- runif(n);x3 &lt;- runif(n)
y &lt;- matrix(0,n,2)
for (i in 1:n) {
  mu &lt;- c(f0(x0[i])+f1(x1[i]),f2(x2[i]))
  y[i,] &lt;- rmvn(1,mu,V)
}
dat &lt;- data.frame(y0=y[,1],y1=y[,2],x0=x0,x1=x1,x2=x2,x3=x3)

## fit model...

b &lt;- gam(list(y0~s(x0)+s(x1),y1~s(x2)+s(x3)),family=mvn(d=2),data=dat)
b
summary(b)
plot(b,pages=1)
solve(crossprod(b$family$data$R)) ## estimated cov matrix

</code></pre>

<hr>
<h2 id='NCV'>Neighbourhood Cross Validation</h2><span id='topic+NCV'></span>

<h3>Description</h3>

<p>NCV estimates smoothing parameters by optimizing the average ability of a model to predict subsets of data when subsets of data are omitted from fitting. Usually the predicted subset is a subset of the omitted subset. If both subsets are the same single datapoint, and the average is over all datapoints, then NCV is leave-one-out cross validation. QNCV is a quadratic approximation to NCV, guaranteed finite for any family link combination.
</p>
<p>In detail, suppose that a model is estimated by minimizing a penalized loss
</p>
<p style="text-align: center;"><code class="reqn">\sum_i D(y_i,\theta_i) + \sum_j \lambda_j \beta^{\sf T} {S}_j \beta </code>
</p>

<p>where <code class="reqn">D</code> is a loss (such as a negative log likelihood), dependent on response <code class="reqn">y_i</code> and parameter vector <code class="reqn">\theta_i</code>, which in turn depends on covariates via one or more smooth linear predictors with coefficients <code class="reqn">\beta</code>. The quadratic penalty terms penalize model complexity: <code class="reqn">S_j</code> is a known matrix and <code class="reqn">\lambda_j</code> an unknown smoothing parameter. Given smoothing parameters the penalized loss is readily minimized to estimate <code class="reqn">\beta</code>.
</p>
<p>The smoothing parameters also have to be estimated. To this end, choose <code class="reqn">k = 1,\ldots,m</code> subsets <code class="reqn">\alpha(k)\subset \{1,\ldots,n\}</code> and <code class="reqn">\delta(k)\subset \{1,\ldots,n\}</code>. Usually <code class="reqn">\delta(k)</code> is a subset of (or equal to) <code class="reqn">\alpha(k)</code>. Let <code class="reqn">\theta_i^{\alpha(k)}</code> denote the estimate of <code class="reqn">\theta_i</code> when the points indexed by <code class="reqn">\alpha(k)</code> are omitted from fitting. Then the NCV criterion
</p>
<p style="text-align: center;"><code class="reqn">V = \sum_{k=1}^m \sum_{i \in \alpha(k)} D(y_i,\theta_i^{\alpha(k)})</code>
</p>

<p>is minimized w.r.t. the smoothing parameters, <code class="reqn">\lambda_j</code>. If <code class="reqn">m=n</code> and <code class="reqn">\alpha(k)=\delta(k)=k</code> then ordinary leave-one-out cross validation is recovered. This formulation covers many of the variants of cross validation reviewed in Arlot and Celisse (2010), for example.
</p>
<p>Except for a quadratic loss, <code class="reqn">V</code> can not be computed exactly, but it can be computed to <code class="reqn">O(n^{-2})</code> accuracy (fixed basis size), by taking single Newton optimization steps from the full data <code class="reqn">\beta</code> estimates to the equivalent when each <code class="reqn">\alpha(k)</code> is dropped. This is what <code>mgcv</code> does. The Newton steps require update of the full model Hessian to the equivalent when each datum is dropped. This can be achieved at <code class="reqn">O(p^2)</code> cost, where <code class="reqn">p</code> is the dimension of <code class="reqn">\beta</code>. Hence, for example, the ordinary cross validation criterion is computable at the <code class="reqn">O(np^2)</code> cost of estimating the model given smoothing parameters.
</p>
<p>The NCV score computed in this way is optimized using a BFGS quasi-Newton method, adapted to the case in which smoothing parameters tending to infinity may cause indefiniteness. 
</p>


<h3>Spatial and temporal short range autocorrelation</h3>

<p>A routine applied problem is that smoothing parameters tend to be underestimated in the presence of un-modelled short range autocorrelation, as the smooths try to fit the local excursions in the data caused by the local autocorrelation. Cross validation will tend to 'fit the noise' when there is autocorellation, since a model that fits the noise in the data correlated with an omitted datum, will also tend to closely fit the noise in the omitted datum, because of the correlation. That is autocorrelation works against the avoidance of overfit that cross validation seeks to achieve.
</p>
<p>For short range autocorrelation the problems can be avoided, or at least mitigated, by predicting each datum when all the data in its &lsquo;local&rsquo; neighbourhood are omitted. The neighbourhoods being constructed in order that un-modelled correlation is minimized between the point of interest and points outside its neighbourhood. That is we set <code class="reqn">m=n</code>, <code class="reqn">\delta(k)=k</code> and <code class="reqn">\alpha(k) = {\tt nei}(k)</code>, where <code>nei(k)</code> are the indices of the neighbours of point <code class="reqn">k</code>. This approach has been known for a long time (e.g. Chu and Marron, 1991; Robert et al. 2017), but was previously rather too expensive for regular use for smoothing parameter estimation.</p>


<h3>Specifying the neighbourhoods</h3>

<p>The neighbourhood subsets <code class="reqn">\alpha(k)</code> and <code class="reqn">\delta(k)</code> have to be supplied to <code><a href="#topic+gam">gam</a></code>, and the <code>nei</code> argument does this. It is a list with the following arguments.
</p>

<ul>
<li> <p><code>k</code> is the vector of indices to be dropped for each neighbourhood.
</p>
</li>
<li> <p><code>m</code> gives the end of each neighbourhood. So <code>nei$k[(nei$m[j-1]+1):nei$m[j]]</code> gives the points dropped for the neighbourhood <code>j</code>: that is <code class="reqn">\alpha(j)</code>.
</p>
</li>
<li> <p><code>i</code> is the vector of indices of points to predict.
</p>
</li>
<li> <p><code>mi</code> gives the corresponding endpoints <code>mi</code>. So <code>nei$i[(nei$mi[j-1]+1):nei$mi[j]]</code> indexes the points to predict for neighbourhood j: that is <code class="reqn">\delta(j)</code>.
</p>
</li>
<li> <p><code>jackknife</code> is an optional element. If supplied and <code>TRUE</code> then variance estimates are based on the raw Jackkife estimate, if <code>FALSE</code> then on the standard Bayesian results. If not supplied (usual) then an estimator accounting for the neighbourhood structure is used, that largely accounts for any correlation present within neighbourhoods. <code>jackknife</code> is ignored if NCV is being calculated for a model where another method is used for smoothing parameter selection. 
</p>
</li></ul>

<p>If <code>nei==NULL</code> (or <code>k</code> or <code>m</code> are missing) then leave-one-out cross validation is used. If <code>nei</code> is supplied but NCV is not selected as the smoothing parameter estimation method, then it is simply computed (but not optimized).
</p>


<h3>Numerical issues</h3>

<p>If a model is specified in which some coefficient values, <code class="reqn">\beta</code>, have non-finite likelihood then the NCV criterion computed with single Newton steps could also be non-finite. A simple fix replaces the NCV criterion with a quadratic approximation to the criterion around the full data fit. The quadratic approximation is always finite. This 'QNCV' is essential for some families, such as <code><a href="#topic+gevlss">gevlss</a></code>.
</p>
<p>Although the leading order cost of NCV is the same as REML or GCV, the actual cost is higher because the dominant operations costs are in matrix-vector, rather than matrix-matrix, operations, so BLAS speed ups are small. However multi-core computing is worthwhile for NCV. See the option <code>ncv.threads</code> in <code><a href="#topic+gam.control">gam.control</a></code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Chu and Marron (1991) Comparison of two bandwidth selectors with dependent errors. The Annals of Statistics. 19, 1906-1918
</p>
<p>Arlot, S. and A. Celisse (2010). A survey of cross-validation procedures for model selection. Statistics Surveys 4, 40-79
</p>
<p>Roberts et al. (2017) Cross-validation strategies for data with temporal,
spatial, hierarchical, or phylogenetic structure. Ecography 40(8), 913-929.
</p>
<p>Wood S.N. (2023) On Neighbourhood Cross Validation. in prep.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
nei.cor &lt;- function(h,n) { ## construct nei structure
  nei &lt;- list(mi=1:n,i=1:n)
  nei$m &lt;- cumsum(c((h+1):(2*h+1),rep(2*h+1,n-2*h-2),(2*h+1):(h+1)))
  k0 &lt;- rep(0,0); if (h&gt;0) for (i in 1:h) k0 &lt;- c(k0,1:(h+i))
  k1 &lt;- n-k0[length(k0):1]+1
  nei$k &lt;- c(k0,1:(2*h+1)+rep(0:(n-2*h-1),each=2*h+1),k1)
  nei
}
set.seed(1)
n &lt;- 500;sig &lt;- .6
x &lt;- 0:(n-1)/(n-1)
f &lt;- sin(4*pi*x)*exp(-x*2)*5/2
e &lt;- rnorm(n,0,sig)
for (i in 2:n) e[i] &lt;- 0.6*e[i-1] + e[i]
y &lt;- f + e ## autocorrelated data
nei &lt;- nei.cor(4,n) ## construct neighbourhoods to mitigate 
b0 &lt;- gam(y~s(x,k=40)) ## GCV based fit
gc &lt;- gam.control(ncv.threads=2)
b1 &lt;- gam(y~s(x,k=40),method="NCV",nei=nei,control=gc)
## use "QNCV", which is identical here...
b2 &lt;- gam(y~s(x,k=40),method="QNCV",nei=nei,control=gc)
## plot GCV and NCV based fits...
f &lt;- f - mean(f)
par(mfrow=c(1,2))
plot(b0,rug=FALSE,scheme=1);lines(x,f,col=2)
plot(b1,rug=FALSE,scheme=1);lines(x,f,col=2)
</code></pre>

<hr>
<h2 id='negbin'>GAM negative binomial families</h2><span id='topic+negbin'></span><span id='topic+nb'></span>

<h3>Description</h3>

<p>The <code>gam</code> modelling function is designed to be able to use 
the <code><a href="#topic+negbin">negbin</a></code> family (a modification of MASS library <code>negative.binomial</code> family 
by Venables and Ripley), or the <code><a href="#topic+nb">nb</a></code> function designed for integrated estimation of 
parameter <code>theta</code>. <code class="reqn">\theta</code> is the parameter such that <code class="reqn">var(y) = \mu + \mu^2/\theta</code>, where <code class="reqn">\mu = E(y)</code>.
</p>
<p>Two approaches to estimating <code>theta</code> are available (with <code><a href="#topic+gam">gam</a></code> only):
</p>

<ul>
<li><p> With <code>negbin</code> then if &lsquo;performance iteration&rsquo; is used for smoothing parameter estimation 
(see <code><a href="#topic+gam">gam</a></code>), then smoothing parameters are chosen by GCV and 
<code>theta</code> is chosen in order to ensure that the Pearson estimate of the scale 
parameter is as close as possible to 1, the value that the scale parameter should have.
</p>
</li>
<li><p> If &lsquo;outer iteration&rsquo; is used for smoothing parameter selection with the <code>nb</code> family then 
<code>theta</code> is estimated alongside the smoothing parameters by ML or REML.
</p>
</li></ul>

<p>To use the first option, set the <code>optimizer</code> argument of <code><a href="#topic+gam">gam</a></code> to <code>"perf"</code> (it can sometimes fail to converge).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>negbin(theta = stop("'theta' must be specified"), link = "log")
nb(theta = NULL, link = "log")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="negbin_+3A_theta">theta</code></td>
<td>
<p>Either i) a single value known value of theta or ii) two values of theta specifying the 
endpoints of an interval over which to search for theta (this is an option only for <code>negbin</code>, and is deprecated). 
For <code>nb</code> then a positive supplied <code>theta</code> is treated as a fixed known parameter, otherwise 
it is estimated (the absolute value of a negative <code>theta</code> is taken as a starting value).</p>
</td></tr>
<tr><td><code id="negbin_+3A_link">link</code></td>
<td>
<p>The link function: one of <code>"log"</code>, <code>"identity"</code> or <code>"sqrt"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nb</code> allows estimation of the <code>theta</code> parameter alongside the model smoothing parameters, but is only usable with <code><a href="#topic+gam">gam</a></code> or <code><a href="#topic+bam">bam</a></code> (not <code>gamm</code>).
</p>
<p>For <code>negbin</code>, if a single value of <code>theta</code> is supplied then it is always taken as the known fixed value and this is useable with <code><a href="#topic+bam">bam</a></code> and <code><a href="#topic+gamm">gamm</a></code>. If <code>theta</code> is two 
numbers (<code>theta[2]&gt;theta[1]</code>) then they are taken as specifying the range of values over which to search for 
the optimal theta. This option is deprecated and should only be used with performance iteration estimation (see <code><a href="#topic+gam">gam</a></code> argument <code>optimizer</code>), in which case  the method 
of estimation is to choose <code class="reqn">\hat \theta</code>  so that the GCV (Pearson) estimate 
of the scale parameter is one (since the scale parameter 
is one for the negative binomial). In this case <code class="reqn">\theta</code> estimation is nested within the IRLS loop 
used for GAM fitting. After each call to fit an iteratively weighted additive model to the IRLS pseudodata, 
the <code class="reqn">\theta</code> estimate is updated. This is done by conditioning on all components of the current GCV/Pearson 
estimator of the scale parameter except <code class="reqn">\theta</code> and then searching for the 
<code class="reqn">\hat \theta</code> which equates this conditional  estimator to one. The search is 
a simple bisection search after an initial crude line search to bracket one. The search will 
terminate at the upper boundary of the search region is a Poisson fit would have yielded an estimated 
scale parameter &lt;1.
</p>


<h3>Value</h3>

<p>For <code>negbin</code> an object inheriting from class <code>family</code>, with additional elements
</p>
<table>
<tr><td><code>dvar</code></td>
<td>
<p>the function giving the first derivative of the variance function w.r.t. <code>mu</code>.</p>
</td></tr>
<tr><td><code>d2var</code></td>
<td>
<p>the function giving the second derivative of the variance function w.r.t. <code>mu</code>.</p>
</td></tr>
<tr><td><code>getTheta</code></td>
<td>
<p>A function for retrieving the value(s) of theta. This also useful for retriving the 
estimate of <code>theta</code> after fitting (see example).</p>
</td></tr>
</table>
<p>For <code>nb</code> an object inheriting from class <code>extended.family</code>.
</p>


<h3>WARNINGS</h3>

<p><code><a href="#topic+gamm">gamm</a></code> does not support <code>theta</code> estimation
</p>
<p>The negative binomial functions from the MASS library are no longer supported.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
modified from Venables and Ripley's <code>negative.binomial</code> family.
</p>


<h3>References</h3>

<p>Venables, B. and B.R. Ripley (2002) Modern Applied Statistics in S, Springer.
</p>
<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(3)
n&lt;-400
dat &lt;- gamSim(1,n=n)
g &lt;- exp(dat$f/5)

## negative binomial data... 
dat$y &lt;- rnbinom(g,size=3,mu=g)
## known theta fit ...
b0 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=negbin(3),data=dat)
plot(b0,pages=1)
print(b0)

## same with theta estimation...
b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=nb(),data=dat)
plot(b,pages=1)
print(b)
b$family$getTheta(TRUE) ## extract final theta estimate


## another example...
set.seed(1)
f &lt;- dat$f
f &lt;- f - min(f)+5;g &lt;- f^2/10
dat$y &lt;- rnbinom(g,size=3,mu=g)
b2 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=nb(link="sqrt"),
         data=dat,method="REML") 
plot(b2,pages=1)
print(b2)
rm(dat)
</code></pre>

<hr>
<h2 id='new.name'>Obtain a name for a new variable that is not already in use</h2><span id='topic+new.name'></span>

<h3>Description</h3>

 <p><code><a href="#topic+gamm">gamm</a></code> works by transforming a GAMM into something 
that can be estimated by <code><a href="nlme.html#topic+lme">lme</a></code>, but this involves creating new 
variables, the names of which should not clash with the names of other 
variables on which the model depends. This simple service routine checks a 
suggested name against a list of those in use, and if neccesary modifies it 
so that there is no clash.</p>


<h3>Usage</h3>

<pre><code class='language-R'> new.name(proposed,old.names)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="new.name_+3A_proposed">proposed</code></td>
<td>
<p>a suggested name</p>
</td></tr>
<tr><td><code id="new.name_+3A_old.names">old.names</code></td>
<td>
<p> An array of names that must not be duplicated</p>
</td></tr> 
</table>


<h3>Value</h3>

<p>A name that is not in <code>old.names</code>.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> </p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+gamm">gamm</a> </code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
old &lt;- c("a","tuba","is","tubby")
new.name("tubby",old)
</code></pre>

<hr>
<h2 id='notExp'> Functions for better-than-log positive parameterization</h2><span id='topic+notExp'></span><span id='topic+notLog'></span>

<h3>Description</h3>

<p> It is common practice in statistical optimization to use log-parameterizations when a 
parameter ought to be positive. i.e. if an optimization parameter <code>a</code> should be non-negative then 
we use <code>a=exp(b)</code> and optimize with respect to the unconstrained parameter <code>b</code>. This often works 
well, but it does imply a rather limited working range for <code>b</code>: using 8 byte doubles, for example, 
if <code>b</code>'s magnitude gets much above 700 then <code>a</code> overflows or underflows. This can cause 
problems for numerical optimization methods. 
</p>
<p><code>notExp</code> is a monotonic function for mapping the real line into the positive real line with much less
extreme underflow and overflow behaviour than <code>exp</code>. It is a piece-wise function, but is continuous 
to second derivative: see the source code for the exact definition, and the example below to see what it 
looks like.
</p>
<p><code>notLog</code> is the inverse function of <code>notExp</code>.
</p>
<p>The major use of these functions was originally to provide more robust
<code>pdMat</code> classes for <code>lme</code> for use by <code><a href="#topic+gamm">gamm</a></code>. Currently
the <code><a href="#topic+notExp2">notExp2</a></code> and <code><a href="#topic+notLog2">notLog2</a></code> functions are used in
their place, as a result of changes to the nlme optimization routines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>notExp(x)

notLog(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="notExp_+3A_x">x</code></td>
<td>
<p>Argument array of real numbers (<code>notExp</code>) or positive real numbers (<code>notLog</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p> An array of function values evaluated at the supplied argument values.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+pdTens">pdTens</a></code>, <code><a href="#topic+pdIdnot">pdIdnot</a></code>,  <code><a href="#topic+gamm">gamm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Illustrate the notExp function: 
## less steep than exp, but still monotonic.
require(mgcv)
x &lt;- -100:100/10
op &lt;- par(mfrow=c(2,2))
plot(x,notExp(x),type="l")
lines(x,exp(x),col=2)
plot(x,log(notExp(x)),type="l")
lines(x,log(exp(x)),col=2) # redundancy intended
x &lt;- x/4
plot(x,notExp(x),type="l")
lines(x,exp(x),col=2)
plot(x,log(notExp(x)),type="l")
lines(x,log(exp(x)),col=2) # redundancy intended
par(op)
range(notLog(notExp(x))-x) # show that inverse works!
</code></pre>

<hr>
<h2 id='notExp2'> Alternative to log parameterization for variance components</h2><span id='topic+notExp2'></span><span id='topic+notLog2'></span>

<h3>Description</h3>

 <p><code>notLog2</code> and <code>notExp2</code> are alternatives to <code>log</code>
and <code>exp</code> or <code><a href="#topic+notLog">notLog</a></code> and <code><a href="#topic+notExp">notExp</a></code> for
re-parameterization of variance parameters. They are used by the
<code><a href="#topic+pdTens">pdTens</a></code> and <code><a href="#topic+pdIdnot">pdIdnot</a></code> classes which in turn implement
smooths for <code><a href="#topic+gamm">gamm</a></code>.
</p>
<p>The functions are typically used to ensure that smoothing parameters are
positive, but the <code>notExp2</code> is not monotonic: rather it cycles between
&lsquo;effective zero&rsquo; and &lsquo;effective infinity&rsquo; as its argument changes. The
<code>notLog2</code> is the inverse function of the <code>notExp2</code> only over an
interval centered on zero.
</p>
<p>Parameterizations using these functions ensure that estimated smoothing
parameters remain positive, but also help to ensure that the likelihood is
never indefinite: once a working parameter pushes a smoothing parameter below
&lsquo;effetive zero&rsquo; or above &lsquo;effective infinity&rsquo; the cyclic nature of the
<code>notExp2</code> causes the likelihood to decrease, where otherwise it might
simply have flattened. 
</p>
<p>This parameterization is really just a numerical trick, in order to get
<code>lme</code> to fit <code>gamm</code> models, without failing due to indefiniteness. 
Note in particular that asymptotic results on the likelihood/REML criterion are 
not invalidated by the trick,
unless parameter estimates end up close to the effective zero or effective
infinity: but if this is the case then the asymptotics would also have been invalid
for a conventional monotonic parameterization.
</p>
<p>This reparameterization was made necessary by some modifications to the
underlying optimization method in  <code>lme</code> introduced in nlme 3.1-62. It is
possible that future releases will return to the <code><a href="#topic+notExp">notExp</a></code> parameterization.
</p>
<p>Note that you can reset &lsquo;effective zero&rsquo; and &lsquo;effective infinity&rsquo;: see below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>notExp2(x,d=.Options$mgcv.vc.logrange,b=1/d)

notLog2(x,d=.Options$mgcv.vc.logrange,b=1/d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="notExp2_+3A_x">x</code></td>
<td>
<p>Argument array of real numbers (<code>notExp</code>) or positive real numbers (<code>notLog</code>).</p>
</td></tr>
<tr><td><code id="notExp2_+3A_d">d</code></td>
<td>
<p>the range of <code>notExp2</code> runs from <code>exp(-d)</code> to
<code>exp(d)</code>. To change the range used by <code>gamm</code> reset
<code>mgcv.vc.logrange</code> using <code><a href="base.html#topic+options">options</a></code>.</p>
</td></tr>
<tr><td><code id="notExp2_+3A_b">b</code></td>
<td>
<p>determines the period of the cycle of <code>notExp2</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> An array of function values evaluated at the supplied argument values.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+pdTens">pdTens</a></code>, <code><a href="#topic+pdIdnot">pdIdnot</a></code>,  <code><a href="#topic+gamm">gamm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Illustrate the notExp2 function:
require(mgcv)
x &lt;- seq(-50,50,length=1000)
op &lt;- par(mfrow=c(2,2))
plot(x,notExp2(x),type="l")
lines(x,exp(x),col=2)
plot(x,log(notExp2(x)),type="l")
lines(x,log(exp(x)),col=2) # redundancy intended
x &lt;- x/4
plot(x,notExp2(x),type="l")
lines(x,exp(x),col=2)
plot(x,log(notExp2(x)),type="l")
lines(x,log(exp(x)),col=2) # redundancy intended
par(op)
</code></pre>

<hr>
<h2 id='null.space.dimension'>The basis of the space of un-penalized functions for a TPRS</h2><span id='topic+null.space.dimension'></span>

<h3>Description</h3>

<p> The thin plate spline penalties give zero penalty to some
functions. The space of these functions is spanned by a set of
polynomial terms. <code>null.space.dimension</code> finds the dimension of this space, <code class="reqn">M</code>, given
the number of covariates that the smoother is a function of, <code class="reqn">d</code>,
and the order of the smoothing penalty, <code class="reqn">m</code>. If <code class="reqn">m</code> does not
satisfy <code class="reqn">2m&gt;d</code> then the smallest possible dimension
for the null space is found given <code class="reqn">d</code> and the requirement that
the smooth should be visually smooth.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>null.space.dimension(d,m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="null.space.dimension_+3A_d">d</code></td>
<td>
<p> is a positive integer - the number of variables of which the
t.p.s. is a function. </p>
</td></tr>
<tr><td><code id="null.space.dimension_+3A_m">m</code></td>
<td>
<p> a non-negative integer giving the order of the penalty
functional, or signalling that the default order should be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Thin plate splines are only visually smooth if the order of the
wiggliness penalty, <code class="reqn">m</code>, satisfies <code class="reqn">2m &gt; d+1</code>. If <code class="reqn">2m&lt;d+1</code> then this routine finds the smallest
<code class="reqn">m</code> giving visual smoothness
for the given <code class="reqn">d</code>, otherwise the supplied <code class="reqn">m</code> is used. The null space dimension is given by:
</p>
<p><code class="reqn">M=(m+d-1)!/(d!(m-1)!)</code>
</p>
<p>which is the value returned.
</p>


<h3>Value</h3>

<p>An integer (array), the null space dimension
<code class="reqn">M</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tprs">tprs</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
null.space.dimension(2,0)
</code></pre>

<hr>
<h2 id='ocat'>GAM ordered categorical family</h2><span id='topic+ocat'></span><span id='topic+ordered.categorical'></span>

<h3>Description</h3>

<p>Family for use with <code><a href="#topic+gam">gam</a></code> or <code><a href="#topic+bam">bam</a></code>, implementing regression for ordered categorical data.
A linear predictor provides the expected value of a latent variable following a logistic distribution. The 
probability of this latent variable lying between certain cut-points provides the probability of the ordered 
categorical variable being of the corresponding category. The cut-points are estimated along side the model 
smoothing parameters (using the same criterion). The observed categories are coded 1, 2, 3, ... up to the 
number of categories. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ocat(theta=NULL,link="identity",R=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ocat_+3A_theta">theta</code></td>
<td>
<p>cut point parameter vector (dimension <code>R-2</code>). If supplied and all positive, then taken to be the cut point increments (first cut point is fixed at -1). If any are negative then absolute values are taken as starting values for cutpoint increments. </p>
</td></tr>
<tr><td><code id="ocat_+3A_link">link</code></td>
<td>
<p>The link function: only <code>"identity"</code> allowed at present (possibly for ever).</p>
</td></tr>
<tr><td><code id="ocat_+3A_r">R</code></td>
<td>
<p>the number of catergories.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Such cumulative threshold models are only identifiable up to an intercept, or one of the cut points. 
Rather than remove the intercept, <code>ocat</code> simply sets the first cut point to -1. Use <code><a href="#topic+predict.gam">predict.gam</a></code> with 
<code>type="response"</code> to get the predicted probabilities in each category.
</p>


<h3>Value</h3>

<p>An object of class <code>extended.family</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## Simulate some ordered categorical data...
set.seed(3);n&lt;-400
dat &lt;- gamSim(1,n=n)
dat$f &lt;- dat$f - mean(dat$f)

alpha &lt;- c(-Inf,-1,0,5,Inf)
R &lt;- length(alpha)-1
y &lt;- dat$f
u &lt;- runif(n)
u &lt;- dat$f + log(u/(1-u)) 
for (i in 1:R) {
  y[u &gt; alpha[i]&amp;u &lt;= alpha[i+1]] &lt;- i
}
dat$y &lt;- y

## plot the data...
par(mfrow=c(2,2))
with(dat,plot(x0,y));with(dat,plot(x1,y))
with(dat,plot(x2,y));with(dat,plot(x3,y))

## fit ocat model to data...
b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ocat(R=R),data=dat)
b
plot(b,pages=1)
gam.check(b)
summary(b)
b$family$getTheta(TRUE) ## the estimated cut points

## predict probabilities of being in each category
predict(b,dat[1:2,],type="response",se=TRUE)
</code></pre>

<hr>
<h2 id='one.se.rule'>The one standard error rule for smoother models</h2><span id='topic+one.se.rule'></span>

<h3>Description</h3>

<p> The &lsquo;one standard error rule&rsquo; (see e.g. Hastie, Tibshirani and Friedman, 2009) is a way of producing smoother models than those directly estimated by automatic smoothing parameter selection methods. In the single smoothing parameter case, we select the largest smoothing parameter within one standard error of the optimum of the smoothing parameter selection criterion. This approach can be generalized to multiple smoothing parameters estimated by REML or ML.</p>


<h3>Details</h3>

<p>Under REML or ML smoothing parameter selection an asyptotic distributional approximation is available for the log smoothing parameters. Let <code class="reqn">\rho</code> denote the log smoothing parameters that we want to increase to obtain a smoother model. The large sample distribution of the estimator of <code class="reqn">\rho</code> is <code class="reqn">N(\rho,V)</code> where <code class="reqn">V</code> is the matrix returned by <code><a href="#topic+sp.vcov">sp.vcov</a></code>. Drop any elements of <code class="reqn">\rho</code> that are already at &lsquo;effective infinity&rsquo;, along with the corresponding rows and columns of <code class="reqn">V</code>. The standard errors of the log smoothing parameters can be obtained from the leading diagonal of <code class="reqn">V</code>. Let the vector of these be <code class="reqn">d</code>. Now suppose that we want to increase the estimated log smoothing parameters by an amount <code class="reqn">\alpha d</code>. We choose <code class="reqn">\alpha</code> so that <code class="reqn">\alpha d^T V^{-1}d = \sqrt{2p}</code>, where p is the dimension of d and 2p the variance of a chi-squared r.v. with p degrees of freedom.
</p>
<p>The idea is that we increase the log smoothing parameters in proportion to their standard deviation, until the RE/ML is increased by 1 standard deviation according to its asypmtotic distribution. </p>


<h3>Author(s)</h3>

<p>Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Hastie, T, R. Tibshirani and J. Friedman (2009) The Elements of Statistical Learning 2nd ed. Springer.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+gam">gam</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> 
require(mgcv)
set.seed(2) ## simulate some data...
dat &lt;- gamSim(1,n=400,dist="normal",scale=2)
b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML")
b
## only the first 3 smoothing parameters are candidates for
## increasing here...
V &lt;- sp.vcov(b)[1:3,1:3] ## the approx cov matrix of sps
d &lt;- diag(V)^.5          ## sp se.
## compute the log smoothing parameter step...
d &lt;- sqrt(2*length(d))/d
sp &lt;- b$sp ## extract original sp estimates
sp[1:3] &lt;- sp[1:3]*exp(d) ## apply the step
## refit with the increased smoothing parameters...
b1 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML",sp=sp)
b;b1 ## compare fits
</code></pre>

<hr>
<h2 id='pcls'> Penalized Constrained Least Squares Fitting</h2><span id='topic+pcls'></span>

<h3>Description</h3>

<p>Solves least squares problems with quadratic penalties subject to linear
equality and inequality constraints using quadratic programming.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcls(M)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcls_+3A_m">M</code></td>
<td>
<p>is the single list argument to <code>pcls</code>. It should have  the 
following elements:
</p>

<dl>
<dt>y</dt><dd><p>The response data vector.</p>
</dd>
<dt>w</dt><dd><p>A vector of weights for the data (often proportional to the 
reciprocal of the variance). </p>
</dd>
<dt>X</dt><dd><p>The design matrix for the problem, note that <code>ncol(M$X)</code>
must give the number of model parameters, while <code>nrow(M$X)</code> 
should give the number of data.</p>
</dd>
<dt>C</dt><dd><p>Matrix containing any linear equality constraints 
on the problem (e.g. <code class="reqn"> \bf C</code> in <code class="reqn"> {\bf Cp}={\bf
		c} </code>). If you have no equality constraints
initialize this to a zero by zero matrix. Note that there is no need 
to supply the vector <code class="reqn"> \bf c</code>, it is defined implicitly by the 
initial parameter estimates <code class="reqn"> \bf p</code>.</p>
</dd>
<dt>S</dt><dd><p> A list of penalty matrices. <code>S[[i]]</code> is the smallest contiguous matrix including 
all the non-zero elements of the ith penalty matrix. The first parameter it
penalizes is given by <code>off[i]+1</code> (starting counting at 1). </p>
</dd>
<dt>off</dt><dd><p> Offset values locating the elements of <code>M$S</code> in
the correct location within each penalty coefficient matrix. (Zero
offset implies starting in first location)</p>
</dd>
<dt>sp</dt><dd><p> An array of  smoothing parameter estimates.</p>
</dd>
<dt>p</dt><dd><p>An array of feasible initial parameter estimates - these must
satisfy the constraints, but should avoid satisfying the inequality
constraints as equality constraints.</p>
</dd>
<dt>Ain</dt><dd><p>Matrix for the inequality constraints <code class="reqn"> {\bf A}_{in}
    {\bf p} &gt; {\bf b}_{in}</code>. </p>
</dd>
<dt>bin</dt><dd><p>vector in the inequality constraints. </p>
</dd>
</dl>
 
</td></tr> 
</table>


<h3>Details</h3>

 
<p>This solves the problem:
</p>
<p style="text-align: center;"><code class="reqn"> minimise~ \| { \bf W}^{1/2} ({ \bf Xp - y} ) \|^2  +  \sum_{i=1}^m
\lambda_i {\bf p^\prime S}_i{\bf p} </code>
</p>

<p>subject to constraints <code class="reqn"> {\bf Cp}={\bf c}</code> and <code class="reqn"> {\bf
    A}_{in}{\bf p}&gt;{\bf b}_{in}</code>, w.r.t. <code class="reqn">\bf p</code> given the
smoothing parameters <code class="reqn">\lambda_i</code>.
<code class="reqn"> {\bf X}</code> is a design matrix, <code class="reqn">\bf p</code> a parameter vector, 
<code class="reqn">\bf y</code> a data vector, <code class="reqn">\bf W</code> a diagonal weight matrix,
<code class="reqn"> {\bf S}_i</code> a positive semi-definite matrix  of coefficients
defining the ith penalty and <code class="reqn">\bf C</code> a matrix of coefficients 
defining the linear equality constraints on the problem. The smoothing
parameters are the <code class="reqn">\lambda_i</code>. Note that <code class="reqn"> {\bf X}</code>
must be of full column rank, at least when projected  into the null space
of any equality constraints. <code class="reqn"> {\bf A}_{in}</code> is a matrix of
coefficients defining the inequality constraints, while <code class="reqn"> {\bf
    b}_{in}</code> is a vector involved in defining the inequality constraints.  
</p>
<p>Quadratic programming is used to perform the solution. The method used
is designed for maximum stability with least squares problems:
i.e. <code class="reqn"> {\bf X}^\prime {\bf X}</code> is not formed explicitly. See
Gill et al. 1981.
</p>


<h3>Value</h3>

<p> The function returns an array containing the estimated parameter
vector. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Gill, P.E., Murray, W. and Wright, M.H. (1981) Practical Optimization. Academic
Press, London. 
</p>
<p>Wood, S.N. (1994) Monotonic smoothing splines fitted by cross validation SIAM
Journal on Scientific Computing 15(5):1126-1133
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+magic">magic</a></code>, <code><a href="#topic+mono.con">mono.con</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
# first an un-penalized example - fit E(y)=a+bx subject to a&gt;0
set.seed(0)
n &lt;- 100
x &lt;- runif(n); y &lt;- x - 0.2 + rnorm(n)*0.1
M &lt;- list(X=matrix(0,n,2),p=c(0.1,0.5),off=array(0,0),S=list(),
Ain=matrix(0,1,2),bin=0,C=matrix(0,0,0),sp=array(0,0),y=y,w=y*0+1)
M$X[,1] &lt;- 1; M$X[,2] &lt;- x; M$Ain[1,] &lt;- c(1,0)
pcls(M) -&gt; M$p
plot(x,y); abline(M$p,col=2); abline(coef(lm(y~x)),col=3)

# Penalized example: monotonic penalized regression spline .....

# Generate data from a monotonic truth.
x &lt;- runif(100)*4-1;x &lt;- sort(x);
f &lt;- exp(4*x)/(1+exp(4*x)); y &lt;- f+rnorm(100)*0.1; plot(x,y)
dat &lt;- data.frame(x=x,y=y)
# Show regular spline fit (and save fitted object)
f.ug &lt;- gam(y~s(x,k=10,bs="cr")); lines(x,fitted(f.ug))
# Create Design matrix, constraints etc. for monotonic spline....
sm &lt;- smoothCon(s(x,k=10,bs="cr"),dat,knots=NULL)[[1]]
F &lt;- mono.con(sm$xp);   # get constraints
G &lt;- list(X=sm$X,C=matrix(0,0,0),sp=f.ug$sp,p=sm$xp,y=y,w=y*0+1)
G$Ain &lt;- F$A;G$bin &lt;- F$b;G$S &lt;- sm$S;G$off &lt;- 0

p &lt;- pcls(G);  # fit spline (using s.p. from unconstrained fit)

fv&lt;-Predict.matrix(sm,data.frame(x=x))%*%p
lines(x,fv,col=2)

# now a tprs example of the same thing....

f.ug &lt;- gam(y~s(x,k=10)); lines(x,fitted(f.ug))
# Create Design matrix, constriants etc. for monotonic spline....
sm &lt;- smoothCon(s(x,k=10,bs="tp"),dat,knots=NULL)[[1]]
xc &lt;- 0:39/39 # points on [0,1]  
nc &lt;- length(xc)  # number of constraints
xc &lt;- xc*4-1  # points at which to impose constraints
A0 &lt;- Predict.matrix(sm,data.frame(x=xc)) 
# ... A0%*%p evaluates spline at xc points
A1 &lt;- Predict.matrix(sm,data.frame(x=xc+1e-6)) 
A &lt;- (A1-A0)/1e-6    
##  ... approx. constraint matrix (A%*%p is -ve 
## spline gradient at points xc)
G &lt;- list(X=sm$X,C=matrix(0,0,0),sp=f.ug$sp,y=y,w=y*0+1,S=sm$S,off=0)
G$Ain &lt;- A;    # constraint matrix
G$bin &lt;- rep(0,nc);  # constraint vector
G$p &lt;- rep(0,10); G$p[10] &lt;- 0.1  
# ... monotonic start params, got by setting coefs of polynomial part
p &lt;- pcls(G);  # fit spline (using s.p. from unconstrained fit)

fv2 &lt;- Predict.matrix(sm,data.frame(x=x))%*%p
lines(x,fv2,col=3)

######################################
## monotonic additive model example...
######################################

## First simulate data...

set.seed(10)
f1 &lt;- function(x) 5*exp(4*x)/(1+exp(4*x));
f2 &lt;- function(x) {
  ind &lt;- x &gt; .5
  f &lt;- x*0
  f[ind] &lt;- (x[ind] - .5)^2*10
  f 
}
f3 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 
      10 * (10 * x)^3 * (1 - x)^10
n &lt;- 200
x &lt;- runif(n); z &lt;- runif(n); v &lt;- runif(n)
mu &lt;- f1(x) + f2(z) + f3(v)
y &lt;- mu + rnorm(n)

## Preliminary unconstrained gam fit...
G &lt;- gam(y~s(x)+s(z)+s(v,k=20),fit=FALSE)
b &lt;- gam(G=G)

## generate constraints, by finite differencing
## using predict.gam ....
eps &lt;- 1e-7
pd0 &lt;- data.frame(x=seq(0,1,length=100),z=rep(.5,100),
                  v=rep(.5,100))
pd1 &lt;- data.frame(x=seq(0,1,length=100)+eps,z=rep(.5,100),
                  v=rep(.5,100))
X0 &lt;- predict(b,newdata=pd0,type="lpmatrix")
X1 &lt;- predict(b,newdata=pd1,type="lpmatrix")
Xx &lt;- (X1 - X0)/eps ## Xx %*% coef(b) must be positive 
pd0 &lt;- data.frame(z=seq(0,1,length=100),x=rep(.5,100),
                  v=rep(.5,100))
pd1 &lt;- data.frame(z=seq(0,1,length=100)+eps,x=rep(.5,100),
                  v=rep(.5,100))
X0 &lt;- predict(b,newdata=pd0,type="lpmatrix")
X1 &lt;- predict(b,newdata=pd1,type="lpmatrix")
Xz &lt;- (X1-X0)/eps
G$Ain &lt;- rbind(Xx,Xz) ## inequality constraint matrix
G$bin &lt;- rep(0,nrow(G$Ain))
G$C = matrix(0,0,ncol(G$X))
G$sp &lt;- b$sp
G$p &lt;- coef(b)
G$off &lt;- G$off-1 ## to match what pcls is expecting
## force inital parameters to meet constraint
G$p[11:18] &lt;- G$p[2:9]&lt;- 0
p &lt;- pcls(G) ## constrained fit
par(mfrow=c(2,3))
plot(b) ## original fit
b$coefficients &lt;- p
plot(b) ## constrained fit
## note that standard errors in preceding plot are obtained from
## unconstrained fit

</code></pre>

<hr>
<h2 id='pdIdnot'>Overflow proof pdMat class for multiples of the identity matrix</h2><span id='topic+pdIdnot'></span><span id='topic+pdConstruct.pdIdnot'></span><span id='topic+pdFactor.pdIdnot'></span><span id='topic+pdMatrix.pdIdnot'></span><span id='topic+coef.pdIdnot'></span><span id='topic+corMatrix.pdIdnot'></span><span id='topic+Dim.pdIdnot'></span><span id='topic+logDet.pdIdnot'></span><span id='topic+solve.pdIdnot'></span><span id='topic+summary.pdIdnot'></span>

<h3>Description</h3>

<p> This set of functions is a modification of the  <code>pdMat</code> class <code>pdIdent</code>
from library <code>nlme</code>. The modification is to replace the log parameterization used in <code>pdMat</code>
with a <code><a href="#topic+notLog2">notLog2</a></code> parameterization, since the latter avoids
indefiniteness in the likelihood and associated convergence problems: the
parameters also relate to variances rather than standard deviations, for
consistency with the <code><a href="#topic+pdTens">pdTens</a></code> class. The functions are particularly useful for
working with Generalized Additive Mixed Models where variance parameters/smoothing parameters can
be very large or very small, so that overflow or underflow can be a problem.
</p>
<p>These functions would not normally be called directly, although unlike the 
<code><a href="#topic+pdTens">pdTens</a></code> class it is easy to do so.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pdIdnot(value = numeric(0), form = NULL, 
       nam = NULL, data = sys.frame(sys.parent()))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pdIdnot_+3A_value">value</code></td>
<td>
<p>Initialization values for parameters. Not normally used.</p>
</td></tr>
<tr><td><code id="pdIdnot_+3A_form">form</code></td>
<td>
<p>A one sided formula specifying the random effects structure. </p>
</td></tr>   
<tr><td><code id="pdIdnot_+3A_nam">nam</code></td>
<td>
<p>a names argument, not normally used with this class.</p>
</td></tr>
<tr><td><code id="pdIdnot_+3A_data">data</code></td>
<td>
<p>data frame in which to evaluate formula.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>The following functions are provided: <code>Dim.pdIndot</code>, <code>coef.pdIdnot</code>, <code>corMatrix.pdIdnot</code>, 
<code>logDet.pdIdnot</code>, <code>pdConstruct.pdIdnot</code>, <code>pdFactor.pdIdnot</code>, <code>pdMatrix.pdIdnot</code>,
<code>solve.pdIdnot</code>, <code>summary.pdIdnot</code>. (e.g. <code>mgcv:::coef.pdIdnot</code> to access.)
</p>
<p>Note that while the <code>pdFactor</code> and <code>pdMatrix</code> functions return the inverse of the scaled random 
effect covariance matrix or its factor, the <code>pdConstruct</code> function is initialised with estimates of the 
scaled covariance matrix itself.
</p>


<h3>Value</h3>

<p> A class <code>pdIdnot</code> object, or related quantities. See the <code>nlme</code> documentation for further details.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Pinheiro J.C. and Bates, D.M. (2000) Mixed effects Models in S and S-PLUS. Springer
</p>
<p>The <code>nlme</code> source code.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+te">te</a></code>, <code><a href="#topic+pdTens">pdTens</a></code>, <code><a href="#topic+notLog2">notLog2</a></code>, <code><a href="#topic+gamm">gamm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># see gamm
</code></pre>

<hr>
<h2 id='pdTens'>Functions implementing a pdMat class for tensor product smooths</h2><span id='topic+pdTens'></span><span id='topic+pdConstruct.pdTens'></span><span id='topic+pdFactor.pdTens'></span><span id='topic+pdMatrix.pdTens'></span><span id='topic+coef.pdTens'></span><span id='topic+summary.pdTens'></span>

<h3>Description</h3>

<p>This set of functions implements an <code>nlme</code> library <code>pdMat</code> class to allow
tensor product smooths to be estimated by <code>lme</code> as called by <code>gamm</code>. Tensor product smooths
have a penalty matrix made up of a weighted sum of penalty matrices, where the weights are the smoothing 
parameters. In the mixed model formulation the penalty matrix is the inverse of the covariance matrix for 
the random effects of a term, and the smoothing parameters (times a half) are variance parameters to be estimated. 
It's not possible to transform the problem to make the required random effects covariance matrix look like one of the standard 
<code>pdMat</code> classes: hence the need for the <code>pdTens</code> class. A <code><a href="#topic+notLog2">notLog2</a></code> parameterization ensures that 
the parameters are positive. 
</p>
<p>These functions (<code>pdTens</code>, <code>pdConstruct.pdTens</code>,
<code>pdFactor.pdTens</code>, <code>pdMatrix.pdTens</code>, <code>coef.pdTens</code> and <code>summary.pdTens</code>)
would not normally be called directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pdTens(value = numeric(0), form = NULL, 
       nam = NULL, data = sys.frame(sys.parent()))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pdTens_+3A_value">value</code></td>
<td>
<p>Initialization values for parameters. Not normally used.</p>
</td></tr>
<tr><td><code id="pdTens_+3A_form">form</code></td>
<td>
<p>A one sided formula specifying the random effects structure. The formula should have
an attribute <code>S</code> which is a list of the penalty matrices the weighted sum of which gives the inverse of the 
covariance matrix for these random effects.</p>
</td></tr>   
<tr><td><code id="pdTens_+3A_nam">nam</code></td>
<td>
<p>a names argument, not normally used with this class.</p>
</td></tr>
<tr><td><code id="pdTens_+3A_data">data</code></td>
<td>
<p>data frame in which to evaluate formula.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> If using this class directly note that it is worthwhile scaling the
<code>S</code> matrices to be of &lsquo;moderate size&rsquo;, for example by dividing each
matrix by its largest singular value: this avoids problems with <code>lme</code>
defaults (<code><a href="#topic+smooth.construct.tensor.smooth.spec">smooth.construct.tensor.smooth.spec</a></code> does this automatically).  
</p>
<p>This appears to be the minimum set of functions required to implement a new <code>pdMat</code> class. 
</p>
<p>Note that while the <code>pdFactor</code> and <code>pdMatrix</code> functions return the inverse of the scaled random 
effect covariance matrix or its factor, the <code>pdConstruct</code> function is
sometimes initialised with estimates of the scaled covariance matrix, and
sometimes intialized with its inverse. 
</p>


<h3>Value</h3>

<p> A class <code>pdTens</code> object, or its coefficients or the matrix it
represents or the factor of 
that matrix. <code>pdFactor</code> returns the factor as a vector (packed
column-wise) (<code>pdMatrix</code> always returns a matrix).
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Pinheiro J.C. and Bates, D.M. (2000) Mixed effects Models in S and S-PLUS. Springer
</p>
<p>The <code>nlme</code> source code.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+te">te</a></code>  <code><a href="#topic+gamm">gamm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># see gamm
</code></pre>

<hr>
<h2 id='pen.edf'>Extract the effective degrees of freedom associated with each penalty in a gam fit</h2><span id='topic+pen.edf'></span>

<h3>Description</h3>

<p>Finds the coefficients penalized by each penalty and adds up their effective degrees of freedom.
Very useful for <code><a href="#topic+t2">t2</a></code> terms, but hard to interpret for terms where the penalties penalize 
overlapping sets of parameters (e.g. <code><a href="#topic+te">te</a></code> terms). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pen.edf(x)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="pen.edf_+3A_x">x</code></td>
<td>
<p> an object inheriting from <code>gam</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Useful for models containing <code><a href="#topic+t2">t2</a></code> terms, since it splits the EDF for the term up into 
parts due to different components of the smooth. This is useful for figuring out which interaction terms are 
actually needed in a model.  
</p>


<h3>Value</h3>

<p> A vector of EDFs, named with labels identifying which penalty each EDF relates to.
</p>


<h3>Author(s)</h3>

<p>  Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+t2">t2</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> 
  require(mgcv)
  set.seed(20) 
  dat &lt;- gamSim(1,n=400,scale=2) ## simulate data
  ## following `t2' smooth basically separates smooth 
  ## of x0,x1 into main effects + interaction.... 
  
  b &lt;- gam(y~t2(x0,x1,bs="tp",m=1,k=7)+s(x2)+s(x3),
           data=dat,method="ML")
  pen.edf(b)
  
  ## label "rr" indicates interaction edf (range space times range space)
  ## label "nr" (null space for x0 times range space for x1) is main
  ##            effect for x1.
  ## label "rn" is main effect for x0
  ## clearly interaction is negligible
  
  ## second example with higher order marginals. 
  
  b &lt;- gam(y~t2(x0,x1,bs="tp",m=2,k=7,full=TRUE)
             +s(x2)+s(x3),data=dat,method="ML")
  pen.edf(b)
  
  ## In this case the EDF is negligible for all terms in the t2 smooth
  ## apart from the `main effects' (r2 and 2r). To understand the labels
  ## consider the following 2 examples....
  ## "r1" relates to the interaction of the range space of the first 
  ##      marginal smooth and the first basis function of the null 
  ##      space of the second marginal smooth
  ## "2r" relates to the interaction of the second basis function of 
  ##      the null space of the first marginal smooth with the range 
  ##      space of the second marginal smooth. 
</code></pre>

<hr>
<h2 id='place.knots'> Automatically place a set of knots evenly through covariate values</h2><span id='topic+place.knots'></span>

<h3>Description</h3>

<p>Given a univariate array of covariate values, places a set of knots for a regression spline evenly through the 
covariate values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> place.knots(x,nk)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="place.knots_+3A_x">x</code></td>
<td>
<p>array of covariate values (need not be sorted).</p>
</td></tr> 
<tr><td><code id="place.knots_+3A_nk">nk</code></td>
<td>
<p>integer indicating the required number of knots.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>Places knots evenly throughout a set of covariates. For example, if you had 11 covariate values and wanted 6 knots 
then a knot would be placed at the first (sorted) covariate value and every second (sorted) value thereafter. With 
less convenient numbers of data and knots the knots are placed within intervals between data in order to achieve 
even coverage, where even means having approximately the same number of data between each pair of knots.</p>


<h3>Value</h3>

<p> An array of knot locations.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+smooth.construct.cc.smooth.spec">smooth.construct.cc.smooth.spec</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
x&lt;-runif(30)
place.knots(x,7)
rm(x)
</code></pre>

<hr>
<h2 id='plot.gam'>Default GAM plotting</h2><span id='topic+plot.gam'></span>

<h3>Description</h3>

<p> Takes a fitted <code>gam</code> object produced by <code>gam()</code> and plots the 
component smooth functions that make it up, on the scale of the linear
predictor. Optionally produces term plots for parametric model components
as well.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
plot(x,residuals=FALSE,rug=NULL,se=TRUE,pages=0,select=NULL,scale=-1,
         n=100,n2=40,n3=3,theta=30,phi=30,jit=FALSE,xlab=NULL,
         ylab=NULL,main=NULL,ylim=NULL,xlim=NULL,too.far=0.1,
         all.terms=FALSE,shade=FALSE,shade.col="gray80",shift=0,
         trans=I,seWithMean=FALSE,unconditional=FALSE,by.resids=FALSE,
         scheme=0,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="plot.gam_+3A_x">x</code></td>
<td>
<p> a fitted <code>gam</code> object as produced by <code>gam()</code>.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_residuals">residuals</code></td>
<td>
<p>If <code>TRUE</code> then partial residuals are added to plots of 1-D smooths. If <code>FALSE</code> 
then no residuals are added. If this is an array of the correct length then it is used as the array of 
residuals to be used for producing partial residuals. If <code>TRUE</code> then the
residuals are the working residuals from the IRLS iteration weighted by the (square root)
IRLS weights, in order that they have constant variance if the model is correct. Partial residuals for a smooth term are the
residuals that would be obtained by dropping the term concerned from the model, while leaving all other 
estimates fixed (i.e. the estimates for the term plus the residuals).</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_rug">rug</code></td>
<td>
<p>When <code>TRUE</code> the covariate to which the plot applies is displayed as a rug plot
at the foot of each plot of a 1-d smooth, and the locations of the
covariates are plotted as points on the contour plot representing a 2-d
smooth. The default of <code>NULL</code> sets <code>rug</code> to <code>TRUE</code> when the dataset size
is &lt;= 10000 and <code>FALSE</code> otherwise.</p>
</td></tr> 
<tr><td><code id="plot.gam_+3A_se">se</code></td>
<td>
<p> when TRUE (default) upper and lower lines are added to the
1-d plots at 2 standard errors
above and below the estimate of the smooth being plotted while for
2-d plots, surfaces at +1 and -1 standard errors are contoured
and overlayed on the contour plot for the estimate. If a
positive number is supplied then this number is multiplied by
the standard errors when calculating standard error curves or
surfaces. See also <code>shade</code>, below. </p>
</td></tr>
<tr><td><code id="plot.gam_+3A_pages">pages</code></td>
<td>
<p> (default 0) the number of pages over which to spread the output. For example, 
if <code>pages=1</code> then all terms will be plotted on one page with the layout performed automatically. 
Set to 0 to have the routine leave all graphics settings as they are. </p>
</td></tr>
<tr><td><code id="plot.gam_+3A_select">select</code></td>
<td>
<p>Allows the  plot for a single model term to be selected for printing. e.g. if you just want the plot for the second smooth term set <code>select=2</code>. </p>
</td></tr>
<tr><td><code id="plot.gam_+3A_scale">scale</code></td>
<td>
<p> set to -1 (default) to have the same y-axis scale for each plot, and to 0 for a 
different y axis for each plot. Ignored if <code>ylim</code> supplied.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_n">n</code></td>
<td>
<p> number of points used for each 1-d plot - for a nice smooth plot this needs to be several times the estimated 
degrees of freedom for the smooth. Default value 100.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_n2">n2</code></td>
<td>
<p>Square root of number of points used to grid estimates of 2-d
functions for contouring.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_n3">n3</code></td>
<td>
<p>Square root of number of panels to use when displaying 3 or 4 dimensional functions.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_theta">theta</code></td>
<td>
<p>One of the perspective plot angles.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_phi">phi</code></td>
<td>
<p>The other perspective plot angle.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_jit">jit</code></td>
<td>
<p>Set to TRUE if you want rug plots for 1-d terms to be jittered.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_xlab">xlab</code></td>
<td>
<p>If supplied then this will be used as the x label for all plots.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_ylab">ylab</code></td>
<td>
<p>If supplied then this will be used as the y label for all plots.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_main">main</code></td>
<td>
<p>Used as title (or z axis label) for plots if supplied.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_ylim">ylim</code></td>
<td>
<p>If supplied then this pair of numbers are used as the y limits for each plot.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_xlim">xlim</code></td>
<td>
<p>If supplied then this pair of numbers are used as the x limits for each plot.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_too.far">too.far</code></td>
<td>
<p>If greater than 0 then this is used to determine when a location is too
far from data to be plotted when plotting 2-D smooths. This is useful since smooths tend to go wild away from data.
The data are scaled into the unit square before deciding what to exclude, and <code>too.far</code> is a distance 
within the unit square. Setting to zero can make plotting faster for large datasets, but care then needed with 
interpretation of plots.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_all.terms">all.terms</code></td>
<td>
<p>if set to <code>TRUE</code> then the partial effects of parametric
model components are also plotted, via a call to <code><a href="stats.html#topic+termplot">termplot</a></code>. Only
terms of order 1 can be plotted in this way. Also see warnings.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_shade">shade</code></td>
<td>
<p>Set to <code>TRUE</code> to produce shaded regions as confidence bands
for smooths (not avaliable for parametric terms, which are plotted using <code>termplot</code>).</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_shade.col">shade.col</code></td>
<td>
<p>define the color used for shading confidence bands.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_shift">shift</code></td>
<td>
<p>constant to add to each smooth (on the scale of the linear
predictor) before plotting. Can be useful for some diagnostics, or with <code>trans</code>.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_trans">trans</code></td>
<td>
<p>monotonic function to apply to each smooth  (after any shift), before
plotting. Monotonicity is not checked, but default plot limits assume it.
<code>shift</code> and <code>trans</code> are occasionally
useful as a means for getting plots on the response scale, when the model consists only
of a single smooth. </p>
</td></tr>
<tr><td><code id="plot.gam_+3A_sewithmean">seWithMean</code></td>
<td>
<p>if <code>TRUE</code> the component smooths are shown with confidence 
intervals that include the uncertainty about the overall mean. If <code>FALSE</code> then the 
uncertainty relates purely to the centred smooth itself. If <code>seWithMean=2</code> then the
intervals include the uncertainty in the mean of the fixed effects (but not in the mean of any uncentred smooths or random effects).  Marra and Wood (2012) suggests 
that <code>TRUE</code> results in better coverage performance,
and this is also suggested by simulation.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_unconditional">unconditional</code></td>
<td>
<p>if <code>TRUE</code> then the smoothing parameter uncertainty corrected 
covariance matrix is used to compute uncertainty bands, if available. Otherwise the bands
treat the smoothing parameters as fixed.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_by.resids">by.resids</code></td>
<td>
<p>Should partial residuals be plotted for terms with <code>by</code> variables? 
Usually the answer is no, they would be meaningless.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_scheme">scheme</code></td>
<td>
<p>Integer or integer vector selecting a plotting scheme for each plot. See details.</p>
</td></tr>
<tr><td><code id="plot.gam_+3A_...">...</code></td>
<td>
<p> other graphics parameters to pass on to plotting commands. See details for smooth plot specific options.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Produces default plot showing the smooth components of a
fitted GAM, and optionally parametric terms as well, when these can be
handled by <code><a href="stats.html#topic+termplot">termplot</a></code>.
</p>
<p>For smooth  terms <code>plot.gam</code> actually calls plot method functions depending on the 
class of the smooth. Currently <code><a href="#topic+random.effects">random.effects</a></code>, Markov random fields (<code><a href="#topic+mrf">mrf</a></code>), 
<code><a href="#topic+Spherical.Spline">Spherical.Spline</a></code>  and  <code><a href="#topic+factor.smooth.interaction">factor.smooth.interaction</a></code> terms have special methods 
(documented in their help files), the rest use the defaults described below. 
</p>
<p>For plots of 1-d smooths, the x axis of each plot is labelled 
with the covariate name, while the y axis is labelled <code>s(cov,edf) </code> where <code>cov</code>
is the covariate name, and <code>edf</code> the estimated (or user defined for regression splines) 
degrees of freedom of the smooth. <code>scheme == 0</code> produces a smooth curve with dashed curves 
indicating 2 standard error bounds. <code>scheme == 1</code> illustrates the error bounds using a shaded
region.
</p>
<p>For <code>scheme==0</code>, contour plots are produced for 2-d smooths with the x-axes labelled with the first covariate
name and the y axis with the second covariate name. The main title of
the plot is something like <code>s(var1,var2,edf)</code>, indicating the
variables of which the term is a function, and the estimated degrees of
freedom for the term. When <code>se=TRUE</code>, estimator variability is shown by overlaying
contour plots at plus and minus 1 s.e. relative to the main
estimate. If <code>se</code> is a positive number then contour plots are at plus or minus <code>se</code> multiplied
by the s.e. Contour levels are chosen to try and ensure reasonable
separation of the contours of the different plots, but this is not
always easy to achieve. Note that these plots can not be modified to the same extent as the other plot. 
</p>
<p>For 2-d smooths <code>scheme==1</code> produces a perspective plot, while <code>scheme==2</code> produces a heatmap, 
with overlaid contours and <code>scheme==3</code> a greyscale heatmap (<code>contour.col</code> controls the
contour colour).
</p>
<p>Smooths of 3 and 4 variables are displayed as tiled heatmaps with overlaid contours. In the 3 variable case the third variable is discretized and a contour plot of the first 2 variables is produced for each discrete value. The panels in the lower and upper rows are labelled with the corresponding third variable value. The lowest value is bottom left, and highest at top right. For 4 variables, two of the variables are coarsely discretized and a square array of image plots is produced for each combination of the discrete values. The first two arguments of the smooth are the ones used for the image/contour plots, unless a tensor product term has 2D marginals, in which case the first 2D marginal is image/contour plotted. <code>n3</code> controls the number of panels.
See also <code><a href="#topic+vis.gam">vis.gam</a></code>. 
</p>
<p>Fine control of plots for parametric terms can be obtained by calling
<code><a href="stats.html#topic+termplot">termplot</a></code> directly, taking care to use its <code>terms</code> argument.
</p>
<p>Note that, if <code>seWithMean=TRUE</code>, the confidence bands include the uncertainty about the overall mean. In other words 
although each smooth is shown centred, the confidence bands are obtained as if every other term in the model was 
constrained to have average 0, (average taken over the covariate values), except for the smooth concerned. This seems to correspond more closely to how most users interpret componentwise intervals in practice, and also results in intervals with
close to nominal (frequentist) coverage probabilities by an extension of Nychka's (1988) results presented in Marra and Wood (2012). There are two possible variants of this approach. In the default variant the extra uncertainty is in the mean of all other terms in the model (fixed and random, including uncentred smooths). Alternatively, if <code>seWithMean=2</code> then only the uncertainty in parametric fixed effects is included in the extra uncertainty (this latter option actually tends to lead to wider intervals when the model contains random effects).
</p>
<p>Several smooth plots methods using <code><a href="Matrix.html#topic+image">image</a></code> will accept an <code>hcolors</code> argument, which can be anything documented in <code><a href="grDevices.html#topic+heat.colors">heat.colors</a></code> (in which case something like <code>hcolors=rainbow(50)</code> is appropriate), or the <code><a href="grDevices.html#topic+grey">grey</a></code> function (in which case somthing like <code>hcolors=grey(0:50/50)</code> is needed). Another option is <code>contour.col</code> which will set the contour colour for some plots. These options are useful for producing grey scale pictures instead of colour.
</p>
<p>Sometimes you may want a small change to a default plot, and the arguments to <code>plot.gam</code> just won't let you do it. 
In this case, the quickest option is sometimes to clone the <code>smooth.construct</code> and <code>Predict.matrix</code> methods for 
the smooth concerned, modifying only the returned smoother class (e.g. to <code>foo.smooth</code>). 
Then copy the plot method function for the original class (e.g. <code>mgcv:::plot.mgcv.smooth</code>), modify the source code to plot exactly as you want and rename the plot method function (e.g. <code>plot.foo.smooth</code>). You can then use the cloned 
smooth in models (e.g. <code>s(x,bs="foo")</code>), and it will automatically plot using the modified plotting function. 
</p>


<h3>Value</h3>

<p> The functions main purpose is its side effect of generating plots. It also silently returns 
a list of the data used to produce the plots, which can be used to generate customized plots.
</p>


<h3>WARNING </h3>

<p> Note that the behaviour of this function is not identical to 
<code>plot.gam()</code> in S-PLUS.
</p>
<p>Plotting can be slow for models fitted to large datasets. Set <code>rug=FALSE</code> to improve matters. 
If it's still too slow set <code>too.far=0</code>, but then take care not to overinterpret smooths away from 
supporting data.
</p>
<p>Plots of 2-D smooths with standard error contours shown can not easily be customized.
</p>
<p><code>all.terms</code> uses <code><a href="stats.html#topic+termplot">termplot</a></code> which looks for the original data in the environment of the fitted model object formula. Since <code>gam</code> resets this environment to avoid large saved model objects containing data in hidden environments, this can fail. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>
<p>Henric Nilsson <a href="mailto:henric.nilsson@statisticon.se">henric.nilsson@statisticon.se</a> donated the code for the <code>shade</code> option.
</p>
<p>The design is inspired by the S function of the same name described in
Chambers and Hastie (1993) (but is not a clone).
</p>


<h3>References</h3>

<p>Chambers and Hastie (1993) Statistical Models in S. Chapman &amp; Hall.
</p>
<p>Marra, G and S.N. Wood (2012) Coverage Properties of Confidence Intervals for Generalized Additive
Model Components. Scandinavian Journal of Statistics.
</p>
<p>Nychka (1988) Bayesian Confidence Intervals for Smoothing Splines. 
Journal of the American Statistical Association 83:1134-1143.
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+predict.gam">predict.gam</a></code>, <code><a href="#topic+vis.gam">vis.gam</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(0)
## fake some data...
f1 &lt;- function(x) {exp(2 * x)}
f2 &lt;- function(x) { 
  0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10 
}
f3 &lt;- function(x) {x*0}

n&lt;-200
sig2&lt;-4
x0 &lt;- rep(1:4,50)
x1 &lt;- runif(n, 0, 1)
x2 &lt;- runif(n, 0, 1)
x3 &lt;- runif(n, 0, 1)
e &lt;- rnorm(n, 0, sqrt(sig2))
y &lt;- 2*x0 + f1(x1) + f2(x2) + f3(x3) + e
x0 &lt;- factor(x0)

## fit and plot...
b&lt;-gam(y~x0+s(x1)+s(x2)+s(x3))
plot(b,pages=1,residuals=TRUE,all.terms=TRUE,shade=TRUE,shade.col=2)
plot(b,pages=1,seWithMean=TRUE) ## better coverage intervals

## just parametric term alone...
termplot(b,terms="x0",se=TRUE)

## more use of color...
op &lt;- par(mfrow=c(2,2),bg="blue")
x &lt;- 0:1000/1000
for (i in 1:3) {
  plot(b,select=i,rug=FALSE,col="green",
    col.axis="white",col.lab="white",all.terms=TRUE)
  for (j in 1:2) axis(j,col="white",labels=FALSE)
  box(col="white")
  eval(parse(text=paste("fx &lt;- f",i,"(x)",sep="")))
  fx &lt;- fx-mean(fx)
  lines(x,fx,col=2) ## overlay `truth' in red
}
par(op)

## example with 2-d plots, and use of schemes...
b1 &lt;- gam(y~x0+s(x1,x2)+s(x3))
op &lt;- par(mfrow=c(2,2))
plot(b1,all.terms=TRUE)
par(op) 
op &lt;- par(mfrow=c(2,2))
plot(b1,all.terms=TRUE,scheme=1)
par(op)
op &lt;- par(mfrow=c(2,2))
plot(b1,all.terms=TRUE,scheme=c(2,1))
par(op)

## 3 and 4 D smooths can also be plotted
dat &lt;- gamSim(1,n=400)
b1 &lt;- gam(y~te(x0,x1,x2,d=c(1,2),k=c(5,15))+s(x3),data=dat)

## Now plot. Use cex.lab and cex.axis to control axis label size,
## n3 to control number of panels, n2 to control panel grid size,
## scheme=1 to get greyscale...

plot(b1,pages=1) 

</code></pre>

<hr>
<h2 id='polys.plot'>Plot geographic regions defined as polygons</h2><span id='topic+polys.plot'></span>

<h3>Description</h3>

<p> Produces plots of geographic regions defined by polygons, optionally filling the 
polygons with a color or grey shade dependent on a covariate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polys.plot(pc,z=NULL,scheme="heat",lab="",...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="polys.plot_+3A_pc">pc</code></td>
<td>
<p>A named list of matrices. Each matrix has two columns. The matrix rows each define the 
vertex of a boundary polygon. If a boundary is defined by several polygons, then each of these must be 
separated by an <code>NA</code> row in the matrix. See <code><a href="#topic+mrf">mrf</a></code> for an example.</p>
</td></tr>
<tr><td><code id="polys.plot_+3A_z">z</code></td>
<td>
<p>A vector of values associated with each area (item) of <code>pc</code>. If the vector elements 
have names then these are used to match elements of <code>z</code> to areas defined in <code>pc</code>. Otherwise 
<code>pc</code> and <code>z</code> are assumed to be in the same order. If <code>z</code> is <code>NULL</code> then polygons are not filled. </p>
</td></tr>
<tr><td><code id="polys.plot_+3A_scheme">scheme</code></td>
<td>
<p>One of <code>"heat"</code> or <code>"grey"</code>, indicating how to fill the polygons in accordance with the value
of <code>z</code>.</p>
</td></tr>
<tr><td><code id="polys.plot_+3A_lab">lab</code></td>
<td>
<p>label for plot.</p>
</td></tr>
<tr><td><code id="polys.plot_+3A_...">...</code></td>
<td>
<p>other arguments to pass to plot (currently only if <code>z</code> is <code>NULL</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Any polygon within another polygon counts as a hole in the area. Further nesting is dealt with by 
treating any point that is interior to an odd number of polygons as being within the area, and all other points 
as being exterior. The routine is provided to facilitate plotting with models containing <code><a href="#topic+mrf">mrf</a></code> smooths.
</p>


<h3>Value</h3>

<p>Simply produces a plot.</p>


<h3>Author(s)</h3>

<p>Simon Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mrf">mrf</a></code> and <code><a href="#topic+columb.polys">columb.polys</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see also ?mrf for use of z
require(mgcv)
data(columb.polys)
polys.plot(columb.polys)
</code></pre>

<hr>
<h2 id='predict.bam'>Prediction from fitted Big Additive Model model</h2><span id='topic+predict.bam'></span>

<h3>Description</h3>

<p> In most cases essentially a wrapper for <code><a href="#topic+predict.gam">predict.gam</a></code> for prediction from a 
model fitted by <code><a href="#topic+bam">bam</a></code>. Can compute on a parallel cluster. For models fitted using discrete
methods with <code>discrete=TRUE</code> then discrete prediction methods are used instead. 
</p>
<p>Takes a fitted <code>bam</code> object produced by <code><a href="#topic+bam">bam</a></code> 
and produces predictions given a new set of values for the model covariates 
or the original values used for the model fit. Predictions can be accompanied
by standard errors, based on the posterior distribution of the model
coefficients. The routine can optionally return the matrix by which the model
coefficients must be pre-multiplied in order to yield the values of the linear predictor at
the supplied covariate values: this is useful for obtaining credible regions
for quantities derived from the model (e.g. derivatives of smooths), and for lookup table prediction outside
<code>R</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bam'
predict(object,newdata,type="link",se.fit=FALSE,terms=NULL,
        exclude=NULL,block.size=50000,newdata.guaranteed=FALSE,
        na.action=na.pass,cluster=NULL,discrete=TRUE,n.threads=1,gc.level=0,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="predict.bam_+3A_object">object</code></td>
<td>
<p> a fitted <code>bam</code> object as produced by <code><a href="#topic+bam">bam</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.bam_+3A_newdata">newdata</code></td>
<td>
<p> A data frame or list containing the values of the model covariates at which predictions
are required. If this is not provided then predictions corresponding to the
original data are returned. If <code>newdata</code> is provided then
it should contain all the variables needed for prediction: a
warning is generated if not. </p>
</td></tr> 
<tr><td><code id="predict.bam_+3A_type">type</code></td>
<td>
<p> When this has the value <code>"link"</code> (default) the linear predictor (possibly with
associated standard errors) is returned. When <code>type="terms"</code> each component of the 
linear predictor is returned seperately (possibly with standard errors): this includes 
parametric model components, followed by each smooth component, but excludes
any offset and any intercept. <code>type="iterms"</code> is the same, except that any standard errors 
returned for smooth components will include the uncertainty about the intercept/overall mean.  When 
<code>type="response"</code> predictions 
on the scale of the response are returned (possibly with approximate
standard errors). When <code>type="lpmatrix"</code> then a matrix is returned
which yields the values of the linear predictor (minus any offset) when
postmultiplied by the
parameter vector (in this case <code>se.fit</code> is ignored). The latter
option is most useful for getting variance estimates for quantities derived from
the model: for example integrated quantities, or derivatives of smooths. A
linear predictor matrix can also be used to implement approximate prediction
outside <code>R</code> (see example code, below). </p>
</td></tr>
<tr><td><code id="predict.bam_+3A_se.fit">se.fit</code></td>
<td>
<p> when this is TRUE (not default) standard error estimates are returned for each prediction.</p>
</td></tr>
<tr><td><code id="predict.bam_+3A_terms">terms</code></td>
<td>
<p>if <code>type=="terms"</code> or <code>type="iterms"</code> then only results for the terms (smooth or parametric) named in this array will be returned. Otherwise any terms not named in this array will be set to zero. If <code>NULL</code> then all terms are included. <code>"(Intercept)"</code> is the intercept term.</p>
</td></tr>
<tr><td><code id="predict.bam_+3A_exclude">exclude</code></td>
<td>
<p>if <code>type=="terms"</code> or <code>type="iterms"</code> then terms (smooth or parametric) named in this array
will not be returned. Otherwise any terms named in this array will be set to zero. 
If <code>NULL</code> then no terms are excluded. To avoid supplying covariate values for excluded smooth terms, set <code>newdata.guaranteed=TRUE</code>,
but note that this skips all checks of <code>newdata</code>. </p>
</td></tr>
<tr><td><code id="predict.bam_+3A_block.size">block.size</code></td>
<td>
<p>maximum number of predictions to process per call to underlying
code: larger is quicker, but more memory intensive.</p>
</td></tr>
<tr><td><code id="predict.bam_+3A_newdata.guaranteed">newdata.guaranteed</code></td>
<td>
<p>Set to <code>TRUE</code> to turn off all checking of
<code>newdata</code> except for sanity of factor levels: this can speed things up
for large prediction tasks, but <code>newdata</code> must be complete, with no
<code>NA</code> values for predictors required in the model. </p>
</td></tr>
<tr><td><code id="predict.bam_+3A_na.action">na.action</code></td>
<td>
<p>what to do about <code>NA</code> values in <code>newdata</code>. With the
default <code>na.pass</code>, any row of <code>newdata</code> containing <code>NA</code> values
for required predictors, gives rise to <code>NA</code> predictions (even if the term concerned has no
<code>NA</code> predictors). <code>na.exclude</code> or <code>na.omit</code> result in the
dropping of <code>newdata</code> rows, if they contain any <code>NA</code> values for
required predictors. If <code>newdata</code> is missing then <code>NA</code> handling is 
determined from <code>object$na.action</code>.</p>
</td></tr>
<tr><td><code id="predict.bam_+3A_cluster">cluster</code></td>
<td>
<p><code>predict.bam</code> can compute in parallel using <a href="parallel.html#topic+clusterApply">parLapply</a>
from the <code>parallel</code> package, if it is supplied with a cluster on which to do this (a cluster here can be some cores of a 
single machine). See details and example code for <code><a href="#topic+bam">bam</a></code>. 
</p>
</td></tr>
<tr><td><code id="predict.bam_+3A_discrete">discrete</code></td>
<td>
<p>if <code>TRUE</code> then discrete prediction methods used with model fitted by discrete methods. <code>FALSE</code> for regular prediction. See details.</p>
</td></tr>
<tr><td><code id="predict.bam_+3A_n.threads">n.threads</code></td>
<td>
<p>if <code>se.fit=TRUE</code> and discrete prediction is used then parallel computation can be used to speed up se calcualtion. This specifies number of htreads to use.</p>
</td></tr>
<tr><td><code id="predict.bam_+3A_gc.level">gc.level</code></td>
<td>
<p>increase from 0 to up the level of garbage collection if default does not give enough.</p>
</td></tr>
<tr><td><code id="predict.bam_+3A_...">...</code></td>
<td>
<p> other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The standard errors produced by <code>predict.gam</code> are based on the
Bayesian posterior covariance matrix of the parameters <code>Vp</code> in the fitted
bam object. 
</p>
<p>To facilitate plotting with <code><a href="stats.html#topic+termplot">termplot</a></code>, if <code>object</code> possesses
an attribute <code>"para.only"</code> and <code>type=="terms"</code> then only parametric
terms of order 1 are returned (i.e. those that <code>termplot</code> can handle).
</p>
<p>Note that, in common with other prediction functions, any offset supplied to
<code><a href="#topic+bam">bam</a></code> as an argument is always ignored when predicting, unlike
offsets specified in the bam model formula.
</p>
<p>See the examples in <code><a href="#topic+predict.gam">predict.gam</a></code> for how to use the <code>lpmatrix</code> for obtaining credible
regions for quantities derived from the model.
</p>
<p>When <code>discrete=TRUE</code> the prediction data in <code>newdata</code> is discretized in the same way as is done when using discrete fitting methods with <code>bam</code>. However the discretization grids are not currently identical to those used during fitting. Instead, discretization is done afresh for the prediction data. This means that if you are predicting for a relatively small set of prediction data, or on a regular grid, then the results may in fact be identical to those obtained without discretization. The disadvantage to this approach is that if you make predictions with a large data frame, and then split it into smaller data frames to make the predictions again, the results may differ slightly, because of slightly different discretization errors.    
</p>


<h3>Value</h3>

<p> If <code>type=="lpmatrix"</code> then a matrix is returned which will
give a vector of linear predictor values (minus any offest) at the supplied covariate
values, when applied to the model coefficient vector. 
Otherwise, if <code>se.fit</code> is <code>TRUE</code> then a 2 item list is returned with items (both arrays) <code>fit</code>
and <code>se.fit</code> containing predictions and associated standard error estimates, otherwise an 
array of predictions is returned. The dimensions of the returned arrays depends on whether 
<code>type</code> is <code>"terms"</code> or not: if it is then the array is 2 dimensional with each 
term in the linear predictor separate, otherwise the array is 1 dimensional and contains the 
linear predictor/predicted values (or corresponding s.e.s). The linear predictor returned termwise will 
not include the offset or the intercept.
</p>
<p><code>newdata</code> can be a data frame, list or model.frame: if it's a model frame
then all variables must be supplied.
</p>


<h3>WARNING </h3>

<p>Predictions are likely to be incorrect if data dependent transformations of the covariates
are used within calls to smooths. See examples in <code><a href="#topic+predict.gam">predict.gam</a></code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> 
</p>
<p>The design is inspired by the S function of the same name described in
Chambers and Hastie (1993) (but is not a clone).
</p>


<h3>References</h3>

<p>Chambers and Hastie (1993) Statistical Models in S. Chapman &amp; Hall.
</p>
<p>Marra, G and S.N. Wood (2012) Coverage Properties of Confidence Intervals for Generalized Additive
Model Components. Scandinavian Journal of Statistics.
</p>
<p>Wood S.N. (2006b) Generalized Additive Models: An Introduction with R. Chapman
and Hall/CRC Press.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+bam">bam</a></code>, <code><a href="#topic+predict.gam">predict.gam</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## for parallel computing see examples for ?bam

## for general useage follow examples in ?predict.gam

</code></pre>

<hr>
<h2 id='predict.gam'>Prediction from fitted GAM model</h2><span id='topic+predict.gam'></span>

<h3>Description</h3>

<p> Takes a fitted <code>gam</code> object produced by <code>gam()</code> 
and produces predictions given a new set of values for the model covariates 
or the original values used for the model fit. Predictions can be accompanied
by standard errors, based on the posterior distribution of the model
coefficients. The routine can optionally return the matrix by which the model
coefficients must be pre-multiplied in order to yield the values of the linear predictor at
the supplied covariate values: this is useful for obtaining credible regions
for quantities derived from the model (e.g. derivatives of smooths), and for lookup table prediction outside
<code>R</code> (see example code below).</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
predict(object,newdata,type="link",se.fit=FALSE,terms=NULL,
        exclude=NULL,block.size=NULL,newdata.guaranteed=FALSE,
        na.action=na.pass,unconditional=FALSE,iterms.type=NULL,...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="predict.gam_+3A_object">object</code></td>
<td>
<p> a fitted <code>gam</code> object as produced by <code>gam()</code>.
</p>
</td></tr>
<tr><td><code id="predict.gam_+3A_newdata">newdata</code></td>
<td>
<p> A data frame or list containing the values of the model covariates at which predictions
are required. If this is not provided then predictions corresponding to the
original data are returned. If <code>newdata</code> is provided then
it should contain all the variables needed for prediction: a
warning is generated if not. See details for use with <code>link{linear.functional.terms}</code>. </p>
</td></tr> 
<tr><td><code id="predict.gam_+3A_type">type</code></td>
<td>
<p> When this has the value <code>"link"</code> (default) the linear predictor (possibly with
associated standard errors) is returned. When <code>type="terms"</code> each component of the 
linear predictor is returned seperately (possibly with standard errors): this includes 
parametric model components, followed by each smooth component, but excludes
any offset and any intercept. <code>type="iterms"</code> is the same, except that any standard errors 
returned for smooth components will include the uncertainty about the intercept/overall mean.  When 
<code>type="response"</code> predictions 
on the scale of the response are returned (possibly with approximate
standard errors). When <code>type="lpmatrix"</code> then a matrix is returned
which yields the values of the linear predictor (minus any offset) when
postmultiplied by the
parameter vector (in this case <code>se.fit</code> is ignored). The latter
option is most useful for getting variance estimates for quantities derived from
the model: for example integrated quantities, or derivatives of smooths. A
linear predictor matrix can also be used to implement approximate prediction
outside <code>R</code> (see example code, below). </p>
</td></tr>
<tr><td><code id="predict.gam_+3A_se.fit">se.fit</code></td>
<td>
<p> when this is TRUE (not default) standard error estimates are returned for each prediction.</p>
</td></tr>
<tr><td><code id="predict.gam_+3A_terms">terms</code></td>
<td>
<p>if <code>type=="terms"</code> or <code>type="iterms"</code> then only results for the terms (smooth or parametric) named in this array
will be returned. Otherwise any terms not named in this array will be set to zero. If <code>NULL</code> then all terms are included.</p>
</td></tr>
<tr><td><code id="predict.gam_+3A_exclude">exclude</code></td>
<td>
<p>if <code>type=="terms"</code> or <code>type="iterms"</code> then terms (smooth or parametric) named in this array will not be returned. Otherwise any terms named in this array will be set to zero. 
If <code>NULL</code> then no terms are excluded. Note that this is the term names as it appears in the model summary, see example.
You can avoid providing the covariates for excluded smooth terms by setting <code>newdata.guaranteed=TRUE</code>, which will avoid all
checks on <code>newdata</code> (covariates for parametric terms can not be skipped).</p>
</td></tr>
<tr><td><code id="predict.gam_+3A_block.size">block.size</code></td>
<td>
<p>maximum number of predictions to process per call to underlying
code: larger is quicker, but more memory intensive. Set to &lt; 1 to use total number
of predictions as this. If <code>NULL</code> then block size is 1000 if new data supplied, 
and the number of rows in the model frame otherwise. </p>
</td></tr>
<tr><td><code id="predict.gam_+3A_newdata.guaranteed">newdata.guaranteed</code></td>
<td>
<p>Set to <code>TRUE</code> to turn off all checking of
<code>newdata</code> except for sanity of factor levels: this can speed things up
for large prediction tasks, but <code>newdata</code> must be complete, with no
<code>NA</code> values for predictors required in the model. </p>
</td></tr>
<tr><td><code id="predict.gam_+3A_na.action">na.action</code></td>
<td>
<p>what to do about <code>NA</code> values in <code>newdata</code>. With the
default <code>na.pass</code>, any row of <code>newdata</code> containing <code>NA</code> values
for required predictors, gives rise to <code>NA</code> predictions (even if the term concerned has no
<code>NA</code> predictors). <code>na.exclude</code> or <code>na.omit</code> result in the
dropping of <code>newdata</code> rows, if they contain any <code>NA</code> values for
required predictors. If <code>newdata</code> is missing then <code>NA</code> handling is 
determined from <code>object$na.action</code>.</p>
</td></tr>
<tr><td><code id="predict.gam_+3A_unconditional">unconditional</code></td>
<td>
<p>if <code>TRUE</code> then the smoothing parameter uncertainty corrected covariance
matrix is used, when available, otherwise the covariance matrix conditional on the estimated 
smoothing parameters is used. </p>
</td></tr>
<tr><td><code id="predict.gam_+3A_iterms.type">iterms.type</code></td>
<td>
<p>if <code>type="iterms"</code> then standard errors can either include the uncertainty in the overall mean (default, withfixed and random effects included) or the uncertainty in the mean of the non-smooth fixed effects only (<code>iterms.type=2</code>).</p>
</td></tr>
<tr><td><code id="predict.gam_+3A_...">...</code></td>
<td>
<p> other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The standard errors produced by <code>predict.gam</code> are based on the
Bayesian posterior covariance matrix of the parameters <code>Vp</code> in the fitted
gam object.
</p>
<p>When predicting from models with <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code> then there are two possibilities. If the summation convention is to be used in prediction, as it was in fitting, then <code>newdata</code> should be a list, with named matrix arguments corresponding to any variables that were matrices in fitting. Alternatively one might choose to simply evaluate the constitutent smooths at particular values in which case arguments that were matrices can be replaced by vectors (and <code>newdata</code> can be a dataframe). See <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code> for example code.
</p>
<p>To facilitate plotting with <code><a href="stats.html#topic+termplot">termplot</a></code>, if <code>object</code> possesses
an attribute <code>"para.only"</code> and <code>type=="terms"</code> then only parametric
terms of order 1 are returned (i.e. those that <code>termplot</code> can handle).
</p>
<p>Note that, in common with other prediction functions, any offset supplied to
<code><a href="#topic+gam">gam</a></code> as an argument is always ignored when predicting, unlike
offsets specified in the gam model formula.
</p>
<p>See the examples for how to use the <code>lpmatrix</code> for obtaining credible
regions for quantities derived from the model. 
</p>


<h3>Value</h3>

<p> If <code>type=="lpmatrix"</code> then a matrix is returned which will
give a vector of linear predictor values (minus any offest) at the supplied covariate
values, when applied to the model coefficient vector. 
Otherwise, if <code>se.fit</code> is <code>TRUE</code> then a 2 item list is returned with items (both arrays) <code>fit</code>
and <code>se.fit</code> containing predictions and associated standard error estimates, otherwise an 
array of predictions is returned. The dimensions of the returned arrays depends on whether 
<code>type</code> is <code>"terms"</code> or not: if it is then the array is 2 dimensional with each 
term in the linear predictor separate, otherwise the array is 1 dimensional and contains the 
linear predictor/predicted values (or corresponding s.e.s). The linear predictor returned termwise will 
not include the offset or the intercept.
</p>
<p><code>newdata</code> can be a data frame, list or model.frame: if it's a model frame
then all variables must be supplied.
</p>


<h3>WARNING </h3>

<p>Predictions are likely to be incorrect if data dependent transformations of the covariates
are used within calls to smooths. See examples.
</p>
<p>Note that the behaviour of this function is not identical to 
<code>predict.gam()</code> in Splus.
</p>
<p><code>type=="terms"</code> does not exactly match what <code>predict.lm</code> does for
parametric model components.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> 
</p>
<p>The design is inspired by the S function of the same name described in
Chambers and Hastie (1993) (but is not a clone).
</p>


<h3>References</h3>

<p>Chambers and Hastie (1993) Statistical Models in S. Chapman &amp; Hall.
</p>
<p>Marra, G and S.N. Wood (2012) Coverage Properties of Confidence Intervals for Generalized Additive
Model Components. Scandinavian Journal of Statistics, 39(1), 53-74. <a href="https://doi.org/10.1111/j.1467-9469.2011.00760.x">doi:10.1111/j.1467-9469.2011.00760.x</a>
</p>
<p>Wood S.N. (2017, 2nd ed) Generalized Additive Models: An Introduction with R. Chapman
and Hall/CRC Press.  <a href="https://doi.org/10.1201/9781315370279">doi:10.1201/9781315370279</a>
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+gamm">gamm</a></code>, <code><a href="#topic+plot.gam">plot.gam</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
n &lt;- 200
sig &lt;- 2
dat &lt;- gamSim(1,n=n,scale=sig)

b &lt;- gam(y~s(x0)+s(I(x1^2))+s(x2)+offset(x3),data=dat)

newd &lt;- data.frame(x0=(0:30)/30,x1=(0:30)/30,x2=(0:30)/30,x3=(0:30)/30)
pred &lt;- predict.gam(b,newd)
pred0 &lt;- predict(b,newd,exclude="s(x0)") ## prediction excluding a term
## ...and the same, but without needing to provide x0 prediction data...
newd1 &lt;- newd;newd1$x0 &lt;- NULL ## remove x0 from `newd1'
pred1 &lt;- predict(b,newd1,exclude="s(x0)",newdata.guaranteed=TRUE)

## custom perspective plot...

m1 &lt;- 20;m2 &lt;- 30; n &lt;- m1*m2
x1 &lt;- seq(.2,.8,length=m1);x2 &lt;- seq(.2,.8,length=m2) ## marginal grid points
df &lt;- data.frame(x0=rep(.5,n),x1=rep(x1,m2),x2=rep(x2,each=m1),x3=rep(0,n))
pf &lt;- predict(b,newdata=df,type="terms")
persp(x1,x2,matrix(pf[,2]+pf[,3],m1,m2),theta=-130,col="blue",zlab="")

#############################################
## difference between "terms" and "iterms"
#############################################
nd2 &lt;- data.frame(x0=c(.25,.5),x1=c(.25,.5),x2=c(.25,.5),x3=c(.25,.5))
predict(b,nd2,type="terms",se=TRUE)
predict(b,nd2,type="iterms",se=TRUE)

#########################################################
## now get variance of sum of predictions using lpmatrix
#########################################################

Xp &lt;- predict(b,newd,type="lpmatrix") 

## Xp %*% coef(b) yields vector of predictions

a &lt;- rep(1,31)
Xs &lt;- t(a) %*% Xp ## Xs %*% coef(b) gives sum of predictions
var.sum &lt;- Xs %*% b$Vp %*% t(Xs)


#############################################################
## Now get the variance of non-linear function of predictions
## by simulation from posterior distribution of the params
#############################################################

rmvn &lt;- function(n,mu,sig) { ## MVN random deviates
  L &lt;- mroot(sig);m &lt;- ncol(L);
  t(mu + L%*%matrix(rnorm(m*n),m,n)) 
}

br &lt;- rmvn(1000,coef(b),b$Vp) ## 1000 replicate param. vectors
res &lt;- rep(0,1000)
for (i in 1:1000)
{ pr &lt;- Xp %*% br[i,] ## replicate predictions
  res[i] &lt;- sum(log(abs(pr))) ## example non-linear function
}
mean(res);var(res)

## loop is replace-able by following .... 

res &lt;- colSums(log(abs(Xp %*% t(br))))



##################################################################
## The following shows how to use use an "lpmatrix" as a lookup 
## table for approximate prediction. The idea is to create 
## approximate prediction matrix rows by appropriate linear 
## interpolation of an existing prediction matrix. The additivity 
## of a GAM makes this possible. 
## There is no reason to ever do this in R, but the following 
## code provides a useful template for predicting from a fitted 
## gam *outside* R: all that is needed is the coefficient vector 
## and the prediction matrix. Use larger `Xp'/ smaller `dx' and/or 
## higher order interpolation for higher accuracy.  
###################################################################

xn &lt;- c(.341,.122,.476,.981) ## want prediction at these values
x0 &lt;- 1         ## intercept column
dx &lt;- 1/30      ## covariate spacing in `newd'
for (j in 0:2) { ## loop through smooth terms
  cols &lt;- 1+j*9 +1:9      ## relevant cols of Xp
  i &lt;- floor(xn[j+1]*30)  ## find relevant rows of Xp
  w1 &lt;- (xn[j+1]-i*dx)/dx ## interpolation weights
  ## find approx. predict matrix row portion, by interpolation
  x0 &lt;- c(x0,Xp[i+2,cols]*w1 + Xp[i+1,cols]*(1-w1))
}
dim(x0)&lt;-c(1,28) 
fv &lt;- x0%*%coef(b) + xn[4];fv    ## evaluate and add offset
se &lt;- sqrt(x0%*%b$Vp%*%t(x0));se ## get standard error
## compare to normal prediction
predict(b,newdata=data.frame(x0=xn[1],x1=xn[2],
        x2=xn[3],x3=xn[4]),se=TRUE)

##################################################################
# illustration of unsafe scale dependent transforms in smooths....
##################################################################

b0 &lt;- gam(y~s(x0)+s(x1)+s(x2)+x3,data=dat) ## safe
b1 &lt;- gam(y~s(x0)+s(I(x1/2))+s(x2)+scale(x3),data=dat) ## safe
b2 &lt;- gam(y~s(x0)+s(scale(x1))+s(x2)+scale(x3),data=dat) ## unsafe
pd &lt;- dat; pd$x1 &lt;- pd$x1/2; pd$x3 &lt;- pd$x3/2
par(mfrow=c(1,2))
plot(predict(b0,pd),predict(b1,pd),main="b0 and b1 predictions match")
abline(0,1,col=2)
plot(predict(b0,pd),predict(b2,pd),main="b2 unsafe, doesn't match")
abline(0,1,col=2)


####################################################################
## Differentiating the smooths in a model (with CIs for derivatives)
####################################################################

## simulate data and fit model...
dat &lt;- gamSim(1,n=300,scale=sig)
b&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
plot(b,pages=1)

## now evaluate derivatives of smooths with associated standard 
## errors, by finite differencing...
x.mesh &lt;- seq(0,1,length=200) ## where to evaluate derivatives
newd &lt;- data.frame(x0 = x.mesh,x1 = x.mesh, x2=x.mesh,x3=x.mesh)
X0 &lt;- predict(b,newd,type="lpmatrix") 

eps &lt;- 1e-7 ## finite difference interval
x.mesh &lt;- x.mesh + eps ## shift the evaluation mesh
newd &lt;- data.frame(x0 = x.mesh,x1 = x.mesh, x2=x.mesh,x3=x.mesh)
X1 &lt;- predict(b,newd,type="lpmatrix")

Xp &lt;- (X1-X0)/eps ## maps coefficients to (fd approx.) derivatives
colnames(Xp)      ## can check which cols relate to which smooth

par(mfrow=c(2,2))
for (i in 1:4) {  ## plot derivatives and corresponding CIs
  Xi &lt;- Xp*0 
  Xi[,(i-1)*9+1:9+1] &lt;- Xp[,(i-1)*9+1:9+1] ## Xi%*%coef(b) = smooth deriv i
  df &lt;- Xi%*%coef(b)              ## ith smooth derivative 
  df.sd &lt;- rowSums(Xi%*%b$Vp*Xi)^.5 ## cheap diag(Xi%*%b$Vp%*%t(Xi))^.5
  plot(x.mesh,df,type="l",ylim=range(c(df+2*df.sd,df-2*df.sd)))
  lines(x.mesh,df+2*df.sd,lty=2);lines(x.mesh,df-2*df.sd,lty=2)
}



</code></pre>

<hr>
<h2 id='Predict.matrix'>Prediction methods for smooth terms in a GAM</h2><span id='topic+Predict.matrix'></span><span id='topic+Predict.matrix2'></span>

<h3>Description</h3>

<p> Takes <code>smooth</code> objects produced by <code>smooth.construct</code> methods and obtains the matrix mapping 
the parameters associated with such a smooth to the predicted values of the smooth at a set of new covariate values.
</p>
<p>In practice this method is often called via the wrapper function <code><a href="#topic+PredictMat">PredictMat</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Predict.matrix(object,data)
Predict.matrix2(object,data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Predict.matrix_+3A_object">object</code></td>
<td>
<p> is a smooth object produced by a <code>smooth.construct</code> method function. The object 
contains all the information required to specify the basis for a term of its class, and this information is
used by the appropriate <code>Predict.matrix</code> function to produce a prediction matrix for new covariate values.
Further details are given in <code><a href="#topic+smooth.construct">smooth.construct</a></code>.</p>
</td></tr>
<tr><td><code id="Predict.matrix_+3A_data">data</code></td>
<td>
<p>A data frame containing the values of the (named) covariates at which the smooth term is to be 
evaluated. Exact requirements are as for <code><a href="#topic+smooth.construct">smooth.construct</a></code> and <code>smooth.construct2</code></p>
</td></tr></table>
<p>.
</p>


<h3>Details</h3>

<p> Smooth terms in a GAM formula are turned into smooth specification objects of 
class <code>xx.smooth.spec</code> during processing of the formula. Each of these objects is
converted to a smooth object using an appropriate <code>smooth.construct</code> function. The <code>Predict.matrix</code> 
functions are used to obtain the matrix that will map the parameters associated with a smooth term to
the predicted values for the term at new covariate values.
</p>
<p>Note that new smooth classes can be added by writing a new <code>smooth.construct</code> method function and a 
corresponding <code><a href="#topic+Predict.matrix">Predict.matrix</a></code> method function: see the example code provided for 
<code><a href="#topic+smooth.construct">smooth.construct</a></code> for details.</p>


<h3>Value</h3>

<p> A matrix which will map the parameters associated with the smooth to the vector of values of the smooth 
evaluated at the covariate values given in <code>object</code>. If the smooth class
is one which generates offsets the corresponding offset is returned as
attribute <code>"offset"</code> of the matrix.</p>


<h3>Author(s)</h3>

<p>Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

 
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+gam">gam</a></code>,<code><a href="#topic+gamm">gamm</a></code>,
<code><a href="#topic+smooth.construct">smooth.construct</a></code>, <code><a href="#topic+PredictMat">PredictMat</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># See smooth.construct examples
</code></pre>

<hr>
<h2 id='Predict.matrix.cr.smooth'>Predict matrix method functions</h2><span id='topic+Predict.matrix.cr.smooth'></span><span id='topic+Predict.matrix.cs.smooth'></span><span id='topic+Predict.matrix.cyclic.smooth'></span><span id='topic+Predict.matrix.pspline.smooth'></span><span id='topic+Predict.matrix.tensor.smooth'></span><span id='topic+Predict.matrix.tprs.smooth'></span><span id='topic+Predict.matrix.ts.smooth'></span><span id='topic+Predict.matrix.t2.smooth'></span>

<h3>Description</h3>

<p>The various built in smooth classes for use with <code><a href="#topic+gam">gam</a></code> have associate <code><a href="#topic+Predict.matrix">Predict.matrix</a></code> 
method functions to enable prediction from the fitted model. </p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cr.smooth'
Predict.matrix(object, data)
## S3 method for class 'cs.smooth'
Predict.matrix(object, data)
## S3 method for class 'cyclic.smooth'
Predict.matrix(object, data)
## S3 method for class 'pspline.smooth'
Predict.matrix(object, data)
## S3 method for class 'tensor.smooth'
Predict.matrix(object, data)
## S3 method for class 'tprs.smooth'
Predict.matrix(object, data)
## S3 method for class 'ts.smooth'
Predict.matrix(object, data)
## S3 method for class 't2.smooth'
Predict.matrix(object, data)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="Predict.matrix.cr.smooth_+3A_object">object</code></td>
<td>
<p>a smooth object, usually generated by a <code><a href="#topic+smooth.construct">smooth.construct</a></code> method having 
processed a smooth specification object generated by an <code><a href="#topic+s">s</a></code> or <code><a href="#topic+te">te</a></code> term in a 
<code><a href="#topic+gam">gam</a></code> formula.</p>
</td></tr>
<tr><td><code id="Predict.matrix.cr.smooth_+3A_data">data</code></td>
<td>
<p> A data frame containing the values of the (named) covariates at which the smooth term is to be 
evaluated. Exact requirements are as for <code><a href="#topic+smooth.construct">smooth.construct</a></code> and <code>smooth.construct2</code></p>
</td></tr></table>
<p>.
</p>


<h3>Details</h3>

 
<p>The Predict matrix function is not normally called directly, but is rather used internally by <code><a href="#topic+predict.gam">predict.gam</a></code> etc. 
to predict from a fitted <code><a href="#topic+gam">gam</a></code> model. See <code><a href="#topic+Predict.matrix">Predict.matrix</a></code> for more details, or the specific 
<code>smooth.construct</code> pages for details on a particular smooth class.
</p>


<h3>Value</h3>

<p> A matrix mapping the coeffients for the smooth term to its values at the supplied data values. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see smooth.construct
 

</code></pre>

<hr>
<h2 id='Predict.matrix.soap.film'>Prediction matrix for soap film smooth</h2><span id='topic+Predict.matrix.soap.film'></span><span id='topic+Predict.matrix.sw'></span><span id='topic+Predict.matrix.sf'></span>

<h3>Description</h3>

<p> Creates a prediction matrix for a soap film smooth object,
mapping the coefficients of the smooth to the linear predictor component for
the smooth.  This is the <code><a href="#topic+Predict.matrix">Predict.matrix</a></code> method function required by <code><a href="#topic+gam">gam</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'soap.film'
Predict.matrix(object,data)
## S3 method for class 'sw'
Predict.matrix(object,data)
## S3 method for class 'sf'
Predict.matrix(object,data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Predict.matrix.soap.film_+3A_object">object</code></td>
<td>
<p>A class <code>"soap.film"</code>, <code>"sf"</code> or <code>"sw"</code> object.</p>
</td></tr>
<tr><td><code id="Predict.matrix.soap.film_+3A_data">data</code></td>
<td>
<p>A list list or data frame containing the arguments of the smooth
at which predictions are required.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The smooth object will be largely what is returned from
<code><a href="#topic+smooth.construct.so.smooth.spec">smooth.construct.so.smooth.spec</a></code>, although elements <code>X</code> and
<code>S</code> are not needed, and need not be present, of course.
</p>


<h3>Value</h3>

<p> A matrix. This may have an <code>"offset"</code> attribute corresponding to
the contribution from any known boundary conditions on the smooth.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:s.wood@bath.ac.uk">s.wood@bath.ac.uk</a></p>


<h3>References</h3>

<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+smooth.construct.so.smooth.spec">smooth.construct.so.smooth.spec</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## This is a lower level example. The basis and 
## penalties are obtained explicitly 
## and `magic' is used as the fitting routine...

require(mgcv)
set.seed(66)

## create a boundary...
fsb &lt;- list(fs.boundary())

## create some internal knots...
knots &lt;- data.frame(x=rep(seq(-.5,3,by=.5),4),
                    y=rep(c(-.6,-.3,.3,.6),rep(8,4)))

## Simulate some fitting data, inside boundary...
n&lt;-1000
x &lt;- runif(n)*5-1;y&lt;-runif(n)*2-1
z &lt;- fs.test(x,y,b=1)
ind &lt;- inSide(fsb,x,y) ## remove outsiders
z &lt;- z[ind];x &lt;- x[ind]; y &lt;- y[ind] 
n &lt;- length(z)
z &lt;- z + rnorm(n)*.3 ## add noise

## plot boundary with knot and data locations
plot(fsb[[1]]$x,fsb[[1]]$y,type="l");points(knots$x,knots$y,pch=20,col=2)
points(x,y,pch=".",col=3);

## set up the basis and penalties...
sob &lt;- smooth.construct2(s(x,y,bs="so",k=40,xt=list(bnd=fsb,nmax=100)),
              data=data.frame(x=x,y=y),knots=knots)
## ... model matrix is element `X' of sob, penalties matrices 
## are in list element `S'.

## fit using `magic'
um &lt;- magic(z,sob$X,sp=c(-1,-1),sob$S,off=c(1,1))
beta &lt;- um$b

## produce plots...
par(mfrow=c(2,2),mar=c(4,4,1,1))
m&lt;-100;n&lt;-50 
xm &lt;- seq(-1,3.5,length=m);yn&lt;-seq(-1,1,length=n)
xx &lt;- rep(xm,n);yy&lt;-rep(yn,rep(m,n))

## plot truth...
tru &lt;- matrix(fs.test(xx,yy),m,n) ## truth
image(xm,yn,tru,col=heat.colors(100),xlab="x",ylab="y")
lines(fsb[[1]]$x,fsb[[1]]$y,lwd=3)
contour(xm,yn,tru,levels=seq(-5,5,by=.25),add=TRUE)

## Plot soap, by first predicting on a fine grid...

## First get prediction matrix...
X &lt;- Predict.matrix2(sob,data=list(x=xx,y=yy))

## Now the predictions...
fv &lt;- X%*%beta

## Plot the estimated function...
image(xm,yn,matrix(fv,m,n),col=heat.colors(100),xlab="x",ylab="y")
lines(fsb[[1]]$x,fsb[[1]]$y,lwd=3)
points(x,y,pch=".")
contour(xm,yn,matrix(fv,m,n),levels=seq(-5,5,by=.25),add=TRUE)

## Plot TPRS...
b &lt;- gam(z~s(x,y,k=100))
fv.gam &lt;- predict(b,newdata=data.frame(x=xx,y=yy))
names(sob$sd$bnd[[1]]) &lt;- c("xx","yy","d")
ind &lt;- inSide(sob$sd$bnd,xx,yy)
fv.gam[!ind]&lt;-NA
image(xm,yn,matrix(fv.gam,m,n),col=heat.colors(100),xlab="x",ylab="y")
lines(fsb[[1]]$x,fsb[[1]]$y,lwd=3)
points(x,y,pch=".")
contour(xm,yn,matrix(fv.gam,m,n),levels=seq(-5,5,by=.25),add=TRUE)

</code></pre>

<hr>
<h2 id='print.gam'>Print a Generalized Additive Model object.</h2><span id='topic+print.gam'></span>

<h3>Description</h3>

<p> The default print method for a <code>gam</code> object. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="print.gam_+3A_x">x</code>, <code id="print.gam_+3A_...">...</code></td>
<td>
<p> fitted model objects of class <code>gam</code> as produced by <code>gam()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Prints out the family, model formula, effective degrees of freedom for each smooth term, and optimized 
value of the smoothness selection criterion used. See <code><a href="#topic+gamObject">gamObject</a></code> (or <code>names(x)</code>) for a listing 
of what the object contains. <code><a href="#topic+summary.gam">summary.gam</a></code> provides more detail. 
</p>
<p>Note that the optimized smoothing parameter selection criterion reported is one of GCV, UBRE(AIC), GACV, 
negative log marginal likelihood (ML), or negative log restricted likelihood (REML).
</p>
<p>If rank deficiency of the model was detected then the apparent rank is reported, along with the length of the cofficient vector 
(rank in absense of rank deficieny). Rank deficiency occurs when not all coefficients are identifiable given the data. Although 
the fitting routines (except <code>gamm</code>) deal gracefully with rank deficiency, interpretation of rank deficient models may be difficult. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> </p>


<h3>References</h3>

<p>Wood, S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). CRC/
Chapmand and Hall, Boca Raton, Florida.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

   <p><code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+summary.gam">summary.gam</a></code></p>

<hr>
<h2 id='psum.chisq'>Evaluate the c.d.f. of a weighted sum of chi-squared deviates</h2><span id='topic+psum.chisq'></span>

<h3>Description</h3>

<p> Evaluates the c.d.f. of a weighted sum of chi-squared random variables
by the method of Davies (1973, 1980). That is it computes </p>
<p style="text-align: center;"><code class="reqn">P(q&lt; \sum_{i=1}^r \lambda_i X_i + \sigma_z Z)</code>
</p>
<p> where <code class="reqn">X_j</code> is a chi-squared random variable with <code>df[j]</code> (integer) degrees of freedom and non-centrality parameter <code>nc[j]</code>, while <code class="reqn">Z</code> is a standard normal deviate. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psum.chisq(q,lb,df=rep(1,length(lb)),nc=rep(0,length(lb)),sigz=0,
           lower.tail=FALSE,tol=2e-5,nlim=100000,trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psum.chisq_+3A_q">q</code></td>
<td>
<p> is the vector of quantile values at which to evaluate.</p>
</td></tr>
<tr><td><code id="psum.chisq_+3A_lb">lb</code></td>
<td>
<p> contains <code class="reqn">\lambda_i</code>, the weight for deviate <code>i</code>. Weights can be positive and/or negative.</p>
</td></tr>
<tr><td><code id="psum.chisq_+3A_df">df</code></td>
<td>
<p> is the integer vector of chi-squared degrees of freedom.</p>
</td></tr>
<tr><td><code id="psum.chisq_+3A_nc">nc</code></td>
<td>
<p> is the vector of non-centrality parameters for the chi-squared deviates.</p>
</td></tr>
<tr><td><code id="psum.chisq_+3A_sigz">sigz</code></td>
<td>
<p> is the multiplier for the standard normal deviate. Non- positive to exclude this term. </p>
</td></tr>
<tr><td><code id="psum.chisq_+3A_lower.tail">lower.tail</code></td>
<td>
<p> indicates whether lower of upper tail probabilities are required.</p>
</td></tr>
<tr><td><code id="psum.chisq_+3A_tol">tol</code></td>
<td>
<p> is the numerical tolerance to work to.</p>
</td></tr>
<tr><td><code id="psum.chisq_+3A_nlim">nlim</code></td>
<td>
<p> is the maximum number of integration steps to allow</p>
</td></tr>
<tr><td><code id="psum.chisq_+3A_trace">trace</code></td>
<td>
<p> can be set to <code>TRUE</code> to return some trace information and a fault code as attributes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This calls a C translation of the original Algol60 code from Davies (1980), which numerically inverts the characteristic function of the distribution (see Davies, 1973).  Some modifications have been made to remove <code>goto</code> statements and global variables, to use a slightly more efficient sorting of <code>lb</code> and to use R functions for <code>log(1+x)</code>. In addition the integral and associated error are accumulated in single terms, rather than each being split into 2, since only their sums are ever used. If <code>q</code> is a vector then <code>psum.chisq</code> calls the algorithm separately for each <code>q[i]</code>.
</p>
<p>If the Davies algorithm returns an error then an attempt will be made to use the approximation of Liu et al (2009) and a warning will be issued. If that is not possible then an <code>NA</code> is returned. A warning will also be issued if the algorithm detects that round off errors may be significant.
</p>
<p>If <code>trace</code> is set to <code>TRUE</code> then the result will have two attributes. <code>"ifault"</code> is 0 for no problem, 1 if the desired accuracy can not be obtained, 2 if round-off error may be significant, 3 is invalid parameters have been supplied or 4 if integration parameters can not be located. <code>"trace"</code> is a 7 element vector: 1. absolute value sum; 2. total number of integration terms; 3. number of integrations; 4. integration interval in main integration; 5. truncation point in initial integration; 6. sd of convergence factor term; 7. number of cycles to locate integration parameters. See Davies (1980) for more details. Note that for vector <code>q</code> these attributes relate to the final element of <code>q</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Davies, R. B. (1973). Numerical inversion of a characteristic function. Biometrika, 60(2), 415-417.
</p>
<p>Davies, R. B. (1980) Algorithm AS 155: The Distribution of a Linear Combination of Chi-squared Random Variables. J. R. Statist. Soc. C
29, 323-333
</p>
<p>Liu, H.; Tang, Y. &amp; Zhang, H. H (2009) A new chi-square approximation to the distribution of non-negative definite quadratic forms in non-central normal variables. Computational Statistics &amp; Data Analysis 53,853-856
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(mgcv)
  lb &lt;- c(4.1,1.2,1e-3,-1) ## weights
  df &lt;- c(2,1,1,1) ## degrees of freedom
  nc &lt;- c(1,1.5,4,1) ## non-centrality parameter
  q &lt;- c(1,6,20) ## quantiles to evaluate

  psum.chisq(q,lb,df,nc)

  ## same by simulation...
  
  psc.sim &lt;- function(q,lb,df=lb*0+1,nc=df*0,ns=10000) {
    r &lt;- length(lb);p &lt;- q
    X &lt;- rowSums(rep(lb,each=ns) *
         matrix(rchisq(r*ns,rep(df,each=ns),rep(nc,each=ns)),ns,r))
    apply(matrix(q),1,function(q) mean(X&gt;q))	 
  } ## psc.sim
  
  psum.chisq(q,lb,df,nc)
  psc.sim(q,lb,df,nc,100000)
</code></pre>

<hr>
<h2 id='qq.gam'>QQ plots for gam model residuals</h2><span id='topic+qq.gam'></span>

<h3>Description</h3>

<p>Takes a fitted <code>gam</code> object produced by <code>gam()</code> and produces
QQ plots of its residuals (conditional on the fitted model
coefficients and scale parameter). If the model distributional
assumptions are met then usually these plots should be close to a
straight line (although discrete data can yield marked random
departures from this line). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qq.gam(object, rep=0, level=.9,s.rep=10,
       type=c("deviance","pearson","response"),
       pch=".", rl.col=2, rep.col="gray80", ...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="qq.gam_+3A_object">object</code></td>
<td>
<p> a fitted <code>gam</code> object as produced by <code>gam()</code> (or a <code>glm</code> object).</p>
</td></tr>
<tr><td><code id="qq.gam_+3A_rep">rep</code></td>
<td>
<p>How many replicate datasets to generate to simulate quantiles
of the residual distribution.  <code>0</code> results in an efficient
simulation free method for direct calculation, if this is possible for
the object family.</p>
</td></tr>
<tr><td><code id="qq.gam_+3A_level">level</code></td>
<td>
<p>If simulation is used for the quantiles, then reference intervals can be provided for the QQ-plot, this specifies the level. 
0 or less for no intervals, 1 or more to simply plot the QQ plot for each replicate generated.</p>
</td></tr>
<tr><td><code id="qq.gam_+3A_s.rep">s.rep</code></td>
<td>
<p>how many times to randomize uniform quantiles to data under direct computation.</p>
</td></tr>
<tr><td><code id="qq.gam_+3A_type">type</code></td>
<td>
<p>what sort of residuals should be plotted?  See
<code><a href="#topic+residuals.gam">residuals.gam</a></code>.</p>
</td></tr>
<tr><td><code id="qq.gam_+3A_pch">pch</code></td>
<td>
<p>plot character to use. 19 is good.</p>
</td></tr>
<tr><td><code id="qq.gam_+3A_rl.col">rl.col</code></td>
<td>
<p>color for the reference line on the plot.</p>
</td></tr>
<tr><td><code id="qq.gam_+3A_rep.col">rep.col</code></td>
<td>
<p>color for reference bands or replicate reference plots.</p>
</td></tr>
<tr><td><code id="qq.gam_+3A_...">...</code></td>
<td>
<p>extra graphics parameters to pass to plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>QQ-plots of the the model residuals can be produced in one of two ways. The cheapest method generates reference quantiles by 
associating a quantile of the uniform distribution with each datum, and feeding these uniform quantiles into the quantile function associated with each datum. The resulting quantiles are then used in place of each datum to generate approximate quantiles of residuals.
The residual quantiles are averaged over <code>s.rep</code> randomizations of the uniform quantiles to data. 
</p>
<p>The second method is to use direct simulatation. For each replicate, data are simulated from the fitted model, and the corresponding residuals computed. This is repeated <code>rep</code> times.
Quantiles are readily obtained from the empirical distribution of residuals so obtained. From this method reference bands are also computable.  
</p>
<p>Even if <code>rep</code> is set to zero, the routine will attempt to simulate quantiles if no quantile function is available for the family. If no random deviate generating function family is available (e.g. for the quasi families), then a normal QQ-plot is produced. The routine conditions on the fitted model coefficents and the scale parameter estimate. 
</p>
<p>The plots are very similar to those proposed in Ben and Yohai (2004), but are substantially cheaper to produce (the interpretation of 
residuals for binary data in Ben and Yohai is not recommended).  
</p>
<p>Note that plots for raw residuals from fits to binary data contain almost no useful information 
about model fit. Whether the residual is negative or positive is decided by whether the response is zero or one. The magnitude of the 
residual, given its sign, is determined entirely by the fitted values. In consequence only the most gross violations of the model 
are detectable from QQ-plots of residuals for binary data.
To really check distributional assumptions from residuals for binary data you have to be able to group the data somehow. 
Binomial models other than binary are ok.  
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>N.H. Augustin, E-A Sauleaub, S.N. Wood (2012) On quantile quantile plots for generalized linear models
Computational Statistics &amp; Data Analysis. 56(8), 2404-2409.
</p>
<p>M.G. Ben and V.J. Yohai (2004) JCGS 13(1), 36-47.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+choose.k">choose.k</a></code>,  <code><a href="#topic+gam">gam</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mgcv)
## simulate binomial data...
set.seed(0)
n.samp &lt;- 400
dat &lt;- gamSim(1,n=n.samp,dist="binary",scale=.33)
p &lt;- binomial()$linkinv(dat$f) ## binomial p
n &lt;- sample(c(1,3),n.samp,replace=TRUE) ## binomial n
dat$y &lt;- rbinom(n,n,p)
dat$n &lt;- n

lr.fit &lt;- gam(y/n~s(x0)+s(x1)+s(x2)+s(x3)
             ,family=binomial,data=dat,weights=n,method="REML")

par(mfrow=c(2,2))
## normal QQ-plot of deviance residuals
qqnorm(residuals(lr.fit),pch=19,cex=.3)
## Quick QQ-plot of deviance residuals
qq.gam(lr.fit,pch=19,cex=.3)
## Simulation based QQ-plot with reference bands 
qq.gam(lr.fit,rep=100,level=.9)
## Simulation based QQ-plot, Pearson resids, all
## simulated reference plots shown...  
qq.gam(lr.fit,rep=100,level=1,type="pearson",pch=19,cex=.2)

## Now fit the wrong model and check....

pif &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3)
             ,family=poisson,data=dat,method="REML")
par(mfrow=c(2,2))
qqnorm(residuals(pif),pch=19,cex=.3)
qq.gam(pif,pch=19,cex=.3)
qq.gam(pif,rep=100,level=.9)
qq.gam(pif,rep=100,level=1,type="pearson",pch=19,cex=.2)

## Example of binary data model violation so gross that you see a problem 
## on the QQ plot...

y &lt;- c(rep(1,10),rep(0,20),rep(1,40),rep(0,10),rep(1,40),rep(0,40))
x &lt;- 1:160
b &lt;- glm(y~x,family=binomial)
par(mfrow=c(2,2))
## Note that the next two are not necessarily similar under gross 
## model violation...
qq.gam(b)
qq.gam(b,rep=50,level=1)
## and a much better plot for detecting the problem
plot(x,residuals(b),pch=19,cex=.3)
plot(x,y);lines(x,fitted(b))

## alternative model
b &lt;- gam(y~s(x,k=5),family=binomial,method="ML")
qq.gam(b)
qq.gam(b,rep=50,level=1)
plot(x,residuals(b),pch=19,cex=.3)
plot(b,residuals=TRUE,pch=19,cex=.3)


</code></pre>

<hr>
<h2 id='random.effects'>Random effects in GAMs</h2><span id='topic+random.effects'></span>

<h3>Description</h3>

<p> The smooth components of GAMs can be viewed as random effects for estimation purposes. This means that more conventional 
random effects terms can be incorporated into GAMs in two ways. The first method converts all the smooths into fixed and random components 
suitable for estimation by standard mixed modelling software. Once the GAM is in this form then conventional random effects are easily added,
and the whole model is estimated as a general mixed model. <code><a href="#topic+gamm">gamm</a></code> and <code>gamm4</code> from the <code>gamm4</code> package operate in this way.
</p>
<p>The second method represents the conventional random effects in a GAM in the same way that the smooths are represented &mdash; as penalized 
regression terms. This method can be used with <code><a href="#topic+gam">gam</a></code> by making use of <code>s(...,bs="re")</code> terms in a model: see 
<code><a href="#topic+smooth.construct.re.smooth.spec">smooth.construct.re.smooth.spec</a></code>, for full details. The basic idea is that, e.g., <code>s(x,z,g,bs="re")</code> generates an i.i.d. Gaussian 
random effect with model matrix given by <code>model.matrix(~x:z:g-1)</code> &mdash; in principle such terms can take any number of arguments. This simple 
approach is sufficient for implementing a wide range of commonly used random effect structures. For example if <code>g</code> is a factor then 
<code>s(g,bs="re")</code> produces a random coefficient for each level of <code>g</code>, with the random coefficients all modelled as i.i.d. normal. If <code>g</code> is a factor and <code>x</code> is numeric, then <code>s(x,g,bs="re")</code> produces an i.i.d. normal random slope relating the response to <code>x</code> for each level of <code>g</code>. If <code>h</code> is another factor then <code>s(h,g,bs="re")</code> produces the usual i.i.d. normal <code>g</code> - <code>h</code> interaction. Note that a rather useful approximate test for zero random effect is also implemented for such terms based on Wood (2013). If the precision
matrix is known to within a multiplicative constant, then this can be supplied via the <code>xt</code> argument of <code>s</code>. See <a href="#topic+smooth.construct.re.smooth.spec">smooth.construct.re.smooth.spec</a> for details and example. Some models require differences between different levels of the same random effect: these can be implemented as described in <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code>.
</p>
<p>Alternatively, but less straightforwardly, the <code>paraPen</code> argument to <code><a href="#topic+gam">gam</a></code> can be used: 
see <code><a href="#topic+gam.models">gam.models</a></code>. If smoothing parameter estimation is by ML or REML (e.g. <code>gam(...,method="REML")</code>) then this approach is 
a completely conventional likelihood based treatment of random effects.
</p>
<p><code>gam</code> can be slow for fitting models with large numbers of random effects, because it does not exploit the sparsity that is often a feature
of parametric random effects. It can not be used for models with more coefficients than data. However <code>gam</code> is often faster and more reliable 
than <code>gamm</code> or <code>gamm4</code>, when the number of random effects is modest.
</p>
<p>To facilitate the use of random effects with <code>gam</code>, <code><a href="#topic+gam.vcomp">gam.vcomp</a></code> is a utility routine for converting 
smoothing parameters to variance components. It also provides confidence intervals, if smoothness estimation is by ML or REML. 
</p>
<p>Note that treating random effects as smooths does not remove the usual problems associated with testing variance components for equality to zero: see <code><a href="#topic+summary.gam">summary.gam</a></code> and <code><a href="#topic+anova.gam">anova.gam</a></code>.   
</p>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon.wood@r-project.org&gt;
</p>


<h3>References</h3>

<p>Wood, S.N. (2013) A simple test for random effects in regression models. Biometrika 100:1005-1010
</p>
<p>Wood, S.N. (2011) Fast stable restricted maximum likelihood 
and marginal likelihood estimation of semiparametric generalized linear 
models. Journal of the Royal Statistical Society (B) 73(1):3-36
</p>
<p>Wood, S.N. (2008) Fast stable direct fitting and smoothness
selection for generalized additive models. Journal of the Royal
Statistical Society (B) 70(3):495-518
</p>
<p>Wood, S.N. (2006) Low rank scale invariant tensor product smooths for
generalized additive mixed models. Biometrics 62(4):1025-1036
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam.vcomp">gam.vcomp</a></code>, <code><a href="#topic+gam.models">gam.models</a></code>, <code><a href="#topic+smooth.terms">smooth.terms</a></code>, 
<code><a href="#topic+smooth.construct.re.smooth.spec">smooth.construct.re.smooth.spec</a></code>,
<code><a href="#topic+gamm">gamm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## see also examples for gam.models, gam.vcomp, gamm
## and smooth.construct.re.smooth.spec

## simple comparison of lme and gam
require(mgcv)
require(nlme)
b0 &lt;- lme(travel~1,data=Rail,~1|Rail,method="REML") 

b &lt;- gam(travel~s(Rail,bs="re"),data=Rail,method="REML")

intervals(b0)
gam.vcomp(b)
anova(b)
plot(b)

## simulate example...
dat &lt;- gamSim(1,n=400,scale=2) ## simulate 4 term additive truth

fac &lt;- sample(1:20,400,replace=TRUE)
b &lt;- rnorm(20)*.5
dat$y &lt;- dat$y + b[fac]
dat$fac &lt;- as.factor(fac)

rm1 &lt;- gam(y ~ s(fac,bs="re")+s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="ML")
gam.vcomp(rm1)

fv0 &lt;- predict(rm1,exclude="s(fac)") ## predictions setting r.e. to 0
fv1 &lt;- predict(rm1) ## predictions setting r.e. to predicted values
## prediction setting r.e. to 0 and not having to provide 'fac'...
pd &lt;- dat; pd$fac &lt;- NULL
fv0 &lt;- predict(rm1,pd,exclude="s(fac)",newdata.guaranteed=TRUE)

## Prediction with levels of fac not in fit data.
## The effect of the new factor levels (or any interaction involving them)
## is set to zero.
xx &lt;- seq(0,1,length=10)
pd &lt;- data.frame(x0=xx,x1=xx,x2=xx,x3=xx,fac=c(1:10,21:30))
fv &lt;- predict(rm1,pd)
pd$fac &lt;- NULL
fv0 &lt;- predict(rm1,pd,exclude="s(fac)",newdata.guaranteed=TRUE)

</code></pre>

<hr>
<h2 id='residuals.gam'>Generalized Additive Model residuals</h2><span id='topic+residuals.gam'></span>

<h3>Description</h3>

<p>Returns residuals for a fitted <code>gam</code> model
object. Pearson, deviance, working and response residuals are
available. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
residuals(object, type = "deviance",...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.gam_+3A_object">object</code></td>
<td>
<p> a <code>gam</code> fitted model object. </p>
</td></tr>
<tr><td><code id="residuals.gam_+3A_type">type</code></td>
<td>
<p>the type of residuals wanted. Usually one of 
<code>"deviance"</code>, <code>"pearson"</code>,<code>"scaled.pearson"</code>, 
<code>"working"</code>, or <code>"response"</code>.  </p>
</td></tr>
<tr><td><code id="residuals.gam_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Response residuals are the raw residuals (data minus fitted
values). Scaled Pearson residuals are raw residuals divided by the standard
deviation of the data according to the model mean variance
relationship and estimated scale parameter. Pearson residuals are the same, but multiplied by the square root 
of the scale parameter (so they are independent of the scale parameter):
(<code class="reqn">(y-\mu)/\sqrt{V(\mu)}</code>, where  <code class="reqn">y</code> is data <code class="reqn">\mu</code> 
is model fitted value and <code class="reqn">V</code> is model mean-variance relationship.). Both are provided since not all texts 
agree on the definition of Pearson residuals. Deviance residuals simply
return the deviance residuals defined by the model family. Working
residuals are the residuals returned from model fitting at convergence.
</p>
<p>Families can supply their own residual function, which is used in place of the standard 
function if present, (e.g. <code><a href="#topic+cox.ph">cox.ph</a></code>).
</p>


<h3>Value</h3>

<p> A vector of residuals.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>See Also</h3>

   <p><code><a href="#topic+gam">gam</a></code></p>

<hr>
<h2 id='rig'>Generate inverse Gaussian random deviates</h2><span id='topic+rig'></span>

<h3>Description</h3>

<p>Generates inverse Gaussian random deviates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rig(n,mean,scale)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="rig_+3A_n">n</code></td>
<td>
<p>the number of deviates required. If this has length &gt; 1 then the length is taken as the number of deviates required.</p>
</td></tr>
<tr><td><code id="rig_+3A_mean">mean</code></td>
<td>
<p>vector of mean values.</p>
</td></tr>
<tr><td><code id="rig_+3A_scale">scale</code></td>
<td>
<p>vector of scale parameter values (lambda, see below)</p>
</td></tr>
</table>


<h3>Details</h3>

<p> If x if the returned vector, then E(x) = <code>mean</code> while var(x) = <code>scale*mean^3</code>. For density and distribution functions 
see the <code>statmod</code> package. The algorithm used is Algorithm 5.7 of Gentle (2003), based on Michael et al. (1976). Note that <code>scale</code> 
here is the scale parameter in the GLM sense, which is the reciprocal of the usual &lsquo;lambda&rsquo; parameter. 
</p>


<h3>Value</h3>

 
<p>A vector of inverse Gaussian random deviates.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Gentle, J.E. (2003) Random Number Generation and Monte Carlo Methods (2nd ed.) Springer.
</p>
<p>Michael, J.R., W.R. Schucany &amp; R.W. Hass (1976) Generating random variates using transformations 
with multiple roots. The American Statistician 30, 88-90.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
set.seed(7)
## An inverse.gaussian GAM example, by modify `gamSim' output... 
dat &lt;- gamSim(1,n=400,dist="normal",scale=1)
dat$f &lt;- dat$f/4 ## true linear predictor 
Ey &lt;- exp(dat$f);scale &lt;- .5 ## mean and GLM scale parameter
## simulate inverse Gaussian response...
dat$y &lt;- rig(Ey,mean=Ey,scale=.2)
big &lt;- gam(y~ s(x0)+ s(x1)+s(x2)+s(x3),family=inverse.gaussian(link=log),
          data=dat,method="REML")
plot(big,pages=1)
gam.check(big)
summary(big)
</code></pre>

<hr>
<h2 id='rmvn'>Generate from or evaluate multivariate normal or t densities.</h2><span id='topic+rmvn'></span><span id='topic+dmvn'></span><span id='topic+r.mvt'></span><span id='topic+d.mvt'></span>

<h3>Description</h3>

<p> Generates multivariate normal or t random deviates, and evaluates the corresponding log densities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmvn(n,mu,V)
r.mvt(n,mu,V,df)
dmvn(x,mu,V,R=NULL)
d.mvt(x,mu,V,df,R=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmvn_+3A_n">n</code></td>
<td>
<p>number of simulated vectors required.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_mu">mu</code></td>
<td>
<p>the mean of the vectors: either a single vector of length <code>p=ncol(V)</code> or an <code>n</code> by <code>p</code> matrix.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_v">V</code></td>
<td>
<p>A positive semi definite covariance matrix.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_df">df</code></td>
<td>
<p>The degrees of freedom for a t distribution.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_x">x</code></td>
<td>
<p>A vector or matrix to evaluate the log density of.</p>
</td></tr>
<tr><td><code id="rmvn_+3A_r">R</code></td>
<td>
<p>An optional Cholesky factor of V (not pivoted).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses a &lsquo;square root&rsquo; of <code>V</code> to transform standard normal deviates to multivariate normal with the correct covariance matrix. 
</p>


<h3>Value</h3>

<p> An <code>n</code> row matrix, with each row being a draw from a multivariate normal or t density with covariance matrix <code>V</code> and mean vector <code>mu</code>. Alternatively each row may have a different mean vector if <code>mu</code> is a vector.
</p>
<p>For density functions, a vector of log densities.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ldTweedie">ldTweedie</a></code>, <code><a href="#topic+Tweedie">Tweedie</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
V &lt;- matrix(c(2,1,1,2),2,2) 
mu &lt;- c(1,3)
n &lt;- 1000
z &lt;- rmvn(n,mu,V)
crossprod(sweep(z,2,colMeans(z)))/n ## observed covariance matrix
colMeans(z) ## observed mu
dmvn(z,mu,V)
</code></pre>

<hr>
<h2 id='Rrank'>Find rank of upper triangular matrix</h2><span id='topic+Rrank'></span>

<h3>Description</h3>

 
<p>Finds rank of upper triangular matrix R, by estimating condition
number of upper <code>rank</code> by <code>rank</code> block, and reducing <code>rank</code> 
until this is acceptably low. Assumes R has been computed by a method that uses 
pivoting, usually pivoted QR or Choleski.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Rrank(R,tol=.Machine$double.eps^.9)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rrank_+3A_r">R</code></td>
<td>
<p>An upper triangular matrix, obtained by pivoted QR or pivoted Choleski.</p>
</td></tr>
<tr><td><code id="Rrank_+3A_tol">tol</code></td>
<td>
<p>the tolerance to use for judging rank.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> The method is based on Cline et al. (1979) as described in Golub and van Loan (1996).
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Cline, A.K., C.B. Moler, G.W. Stewart and J.H. Wilkinson (1979) 
An estimate for the condition number of a matrix. 
SIAM J. Num. Anal. 16, 368-375
</p>
<p>Golub, G.H, and C.F. van Loan (1996) 
Matrix Computations 3rd ed. 
Johns Hopkins University Press, Baltimore.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(0)
  n &lt;- 10;p &lt;- 5
  x &lt;- runif(n*(p-1))
  X &lt;- matrix(c(x,x[1:n]),n,p)
  qrx &lt;- qr(X,LAPACK=TRUE)
  Rrank(qr.R(qrx))
</code></pre>

<hr>
<h2 id='rTweedie'>Generate Tweedie random deviates</h2><span id='topic+rTweedie'></span>

<h3>Description</h3>

<p> Generates Tweedie random deviates, for powers between 1 and 2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rTweedie(mu,p=1.5,phi=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rTweedie_+3A_mu">mu</code></td>
<td>
<p>vector of expected values for the deviates to be generated. One deviate generated for each element of <code>mu</code>.</p>
</td></tr>
<tr><td><code id="rTweedie_+3A_p">p</code></td>
<td>
<p>the variance of a deviate is proportional to its mean, <code>mu</code> to the power <code>p</code>. <code>p</code> must
be between 1 and 2. 1 is Poisson like (exactly Poisson if <code>phi=1</code>), 2 is gamma. </p>
</td></tr>
<tr><td><code id="rTweedie_+3A_phi">phi</code></td>
<td>
<p>The scale parameter. Variance of the deviates is given by is <code>phi*mu^p</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> A Tweedie random variable with 1&lt;p&lt;2 is a sum of <code>N</code> gamma random variables 
where <code>N</code> has a Poisson distribution, with mean <code>mu^(2-p)/((2-p)*phi)</code>. The Gamma random variables 
that are summed have shape parameter <code>(2-p)/(p-1)</code> and scale parameter <code>phi*(p-1)*mu^(p-1)</code> (note that 
this scale parameter is different from the scale parameter for a GLM with Gamma errors). 
</p>
<p>This is a restricted, but faster, version of <code>rtweedie</code> from the <code>tweedie</code> package.
</p>


<h3>Value</h3>

<p> A vector of random deviates from a Tweedie distribution, expected value vector <code>mu</code>, variance vector <code>phi*mu^p</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Peter K Dunn (2009). tweedie: Tweedie exponential family models. R
package version 2.0.2. <a href="https://cran.r-project.org/package=tweedie">https://cran.r-project.org/package=tweedie</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ldTweedie">ldTweedie</a></code>, <code><a href="#topic+Tweedie">Tweedie</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> library(mgcv)
 f2 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 *
            (10 * x)^3 * (1 - x)^10
 n &lt;- 300
 x &lt;- runif(n)
 mu &lt;- exp(f2(x)/3+.1);x &lt;- x*10 - 4
 y &lt;- rTweedie(mu,p=1.5,phi=1.3)
 b &lt;- gam(y~s(x,k=20),family=Tweedie(p=1.5))
 b
 plot(b) 

</code></pre>

<hr>
<h2 id='s'>Defining smooths in GAM formulae</h2><span id='topic+s'></span>

<h3>Description</h3>

<p> Function used in definition of smooth terms within
<code>gam</code> model formulae. The function does not evaluate a (spline)
smooth - it exists purely to help set up a model using spline based smooths.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>s(..., k=-1,fx=FALSE,bs="tp",m=NA,by=NA,xt=NULL,id=NULL,sp=NULL,pc=NULL)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="s_+3A_...">...</code></td>
<td>
<p> a list of variables that are the covariates that this
smooth is a function of. Transformations whose form depends on
the values of the data are best avoided here: e.g. <code>s(log(x))</code>
is fine, but <code>s(I(x/sd(x)))</code> is not (see <code><a href="#topic+predict.gam">predict.gam</a></code>).</p>
</td></tr>
<tr><td><code id="s_+3A_k">k</code></td>
<td>
<p> the dimension of the basis used to represent the smooth term.
The default depends on the number of variables that the smooth is a
function of. <code>k</code> should not be less than the dimension of the
null space of the penalty for the term (see
<code><a href="#topic+null.space.dimension">null.space.dimension</a></code>), but will be reset if
it is. See <code><a href="#topic+choose.k">choose.k</a></code> for further information.</p>
</td></tr>
<tr><td><code id="s_+3A_fx">fx</code></td>
<td>
<p>indicates whether the term is a fixed d.f. regression
spline (<code>TRUE</code>) or a penalized regression spline (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="s_+3A_bs">bs</code></td>
<td>
<p>a two letter character string indicating the (penalized) smoothing basis to use.
(eg <code>"tp"</code> for thin plate regression spline, <code>"cr"</code> for cubic regression spline).
see <code><a href="#topic+smooth.terms">smooth.terms</a></code> for an over view of what is available.
</p>
</td></tr>
<tr><td><code id="s_+3A_m">m</code></td>
<td>
<p>The order of the penalty for this term (e.g. 2 for
normal cubic spline penalty with 2nd derivatives when using 
default t.p.r.s basis). <code>NA</code> signals
autoinitialization. Only some smooth classes use this. The <code>"ps"</code> 
class can use a 2 item array giving the basis and penalty order separately.</p>
</td></tr>
<tr><td><code id="s_+3A_by">by</code></td>
<td>
<p>a numeric or factor variable of the same dimension as each covariate. 
In the numeric vector case the elements multiply the smooth, evaluated at the corresponding 
covariate values (a &lsquo;varying coefficient model&rsquo; results). For the numeric <code>by</code> variable case the 
resulting smooth is not usually subject to a centering constraint (so the <code>by variable</code> should 
not be added as an additional main effect). 
In the factor <code>by</code> variable case a replicate of the smooth is produced for
each factor level (these smooths will be centered, so the factor usually needs to be added as 
a main effect as well). See <code><a href="#topic+gam.models">gam.models</a></code> for further details. A <code>by</code> variable may also be a matrix 
if covariates are matrices: in this case implements linear functional of a smooth 
(see <code><a href="#topic+gam.models">gam.models</a></code> and <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code> for details).
</p>
</td></tr>
<tr><td><code id="s_+3A_xt">xt</code></td>
<td>
<p>Any extra information required to set up a particular basis. Used
e.g. to set large data set handling behaviour for <code>"tp"</code> basis. If <code>xt$sumConv</code>
exists and is <code>FALSE</code> then the summation convention for matrix arguments is turned off. </p>
</td></tr>
<tr><td><code id="s_+3A_id">id</code></td>
<td>
<p>A label or integer identifying this term in order to link its smoothing
parameters to others of the same type. If two or more terms have the same 
<code>id</code> then they will have the same smoothing paramsters, and, by default,
the same bases (first occurance defines basis type, but data from all terms 
used in basis construction). An <code>id</code> with a factor <code>by</code> variable causes the smooths
at each factor level to have the same smoothing parameter.</p>
</td></tr> 
<tr><td><code id="s_+3A_sp">sp</code></td>
<td>
<p>any supplied smoothing parameters for this term. Must be an array of the same
length as the number of penalties for this smooth. Positive or zero elements are taken as fixed 
smoothing parameters. Negative elements signal auto-initialization. Over-rides values supplied in 
<code>sp</code> argument to <code><a href="#topic+gam">gam</a></code>. Ignored by <code>gamm</code>.</p>
</td></tr>
<tr><td><code id="s_+3A_pc">pc</code></td>
<td>
<p>If not <code>NULL</code>, signals a point constraint: the smooth should pass through zero at the
point given here (as a vector or list with names corresponding to the smooth names). Never ignored
if supplied. See <code><a href="#topic+identifiability">identifiability</a></code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function does not evaluate the variable arguments. To use this function to specify use of
your own smooths, note the relationships between the inputs and the output object and see the example
in <code><a href="#topic+smooth.construct">smooth.construct</a></code>.
</p>


<h3>Value</h3>

<p> A class <code>xx.smooth.spec</code> object, where <code>xx</code> is a basis identifying code given by
the <code>bs</code> argument of <code>s</code>. These <code>smooth.spec</code> objects define smooths and are turned into
bases and penalties by <code>smooth.construct</code> method functions. 
</p>
<p>The returned object contains the following items:
</p>
<table>
<tr><td><code>term</code></td>
<td>
<p>An array of text strings giving the names of the covariates that 
the term is a function of.</p>
</td></tr>
<tr><td><code>bs.dim</code></td>
<td>
<p>The dimension of the basis used to represent the smooth.</p>
</td></tr>
<tr><td><code>fixed</code></td>
<td>
<p>TRUE if the term is to be treated as a pure regression
spline (with fixed degrees of freedom); FALSE if it is to be treated
as a penalized regression spline</p>
</td></tr>
<tr><td><code>dim</code></td>
<td>
<p>The dimension of the smoother - i.e. the number of
covariates that it is a function of.</p>
</td></tr>
<tr><td><code>p.order</code></td>
<td>
<p>The order of the t.p.r.s. penalty, or 0 for
auto-selection of the penalty order.</p>
</td></tr>
<tr><td><code>by</code></td>
<td>
<p>is the name of any <code>by</code> variable as text (<code>"NA"</code> for none).</p>
</td></tr>
<tr><td><code>label</code></td>
<td>
<p>A suitable text label for this smooth term.</p>
</td></tr>
<tr><td><code>xt</code></td>
<td>
<p>The object passed in as argument <code>xt</code>.</p>
</td></tr>
<tr><td><code>id</code></td>
<td>
<p>An identifying label or number for the smooth, linking it to other
smooths. Defaults to <code>NULL</code> for no linkage. </p>
</td></tr>
<tr><td><code>sp</code></td>
<td>
<p>array of smoothing parameters for the term (negative for
auto-estimation). Defaults to <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+te">te</a></code>, <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+gamm">gamm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># example utilising `by' variables
library(mgcv)
set.seed(0)
n&lt;-200;sig2&lt;-4
x1 &lt;- runif(n, 0, 1);x2 &lt;- runif(n, 0, 1);x3 &lt;- runif(n, 0, 1)
fac&lt;-c(rep(1,n/2),rep(2,n/2)) # create factor
fac.1&lt;-rep(0,n)+(fac==1);fac.2&lt;-1-fac.1 # and dummy variables
fac&lt;-as.factor(fac)
f1 &lt;-  exp(2 * x1) - 3.75887
f2 &lt;-  0.2 * x1^11 * (10 * (1 - x1))^6 + 10 * (10 * x1)^3 * (1 - x1)^10
f&lt;-f1*fac.1+f2*fac.2+x2
e &lt;- rnorm(n, 0, sqrt(abs(sig2)))
y &lt;- f + e
# NOTE: smooths will be centered, so need to include fac in model....
b&lt;-gam(y~fac+s(x1,by=fac)+x2) 
plot(b,pages=1)
</code></pre>

<hr>
<h2 id='scat'>GAM scaled t family for heavy tailed data</h2><span id='topic+scat'></span><span id='topic+t.scaled'></span>

<h3>Description</h3>

<p>Family for use with <code><a href="#topic+gam">gam</a></code> or <code><a href="#topic+bam">bam</a></code>, implementing regression for the heavy tailed response
variables, y, using a scaled t model. The idea is that <code class="reqn">(y-\mu)/\sigma \sim t_\nu </code> where 
<code class="reqn">mu</code> is determined by a linear predictor, while <code class="reqn">\sigma</code> and <code class="reqn">\nu</code> are parameters 
to be estimated alongside the smoothing parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scat(theta = NULL, link = "identity",min.df=3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scat_+3A_theta">theta</code></td>
<td>
<p>the parameters to be estimated <code class="reqn">\nu = b + \exp(\theta_1)</code> (where &lsquo;b&rsquo; is <code>min.df</code>) and 
<code class="reqn">\sigma = \exp(\theta_2)</code>. If supplied and both positive, then taken to be fixed values of 
<code class="reqn">\nu</code> and <code class="reqn">\sigma</code>. If any negative, then absolute values taken as starting values. </p>
</td></tr>
<tr><td><code id="scat_+3A_link">link</code></td>
<td>
<p>The link function: one of <code>"identity"</code>, <code>"log"</code> or <code>"inverse"</code>.</p>
</td></tr>
<tr><td><code id="scat_+3A_min.df">min.df</code></td>
<td>
<p>minimum degrees of freedom. Should not be set to 2 or less as this implies infinite response variance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Useful in place of Gaussian, when data are heavy tailed. <code>min.df</code> can be modified, but lower values can occasionally
lead to convergence problems in smoothing parameter estimation. In any case <code>min.df</code> should be &gt;2, since only then does a t
random variable have finite variance.   
</p>


<h3>Value</h3>

<p>An object of class <code>extended.family</code>.
</p>


<h3>Author(s)</h3>

<p> Natalya Pya (nat.pya@gmail.com)
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## Simulate some t data...
set.seed(3);n&lt;-400
dat &lt;- gamSim(1,n=n)
dat$y &lt;- dat$f + rt(n,df=4)*2

b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=scat(link="identity"),data=dat)

b
plot(b,pages=1)

</code></pre>

<hr>
<h2 id='sdiag'>Extract or modify diagonals of a matrix</h2><span id='topic+sdiag'></span><span id='topic+sdiag+3C-'></span>

<h3>Description</h3>

<p> Extracts or modifies sub- or super- diagonals of a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdiag(A,k=0)
sdiag(A,k=0) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdiag_+3A_a">A</code></td>
<td>
<p>a matrix</p>
</td></tr>
<tr><td><code id="sdiag_+3A_k">k</code></td>
<td>
<p>sub- (negative) or super- (positive) diagonal of a matrix. 0 is the leading diagonal.</p>
</td></tr>
<tr><td><code id="sdiag_+3A_value">value</code></td>
<td>
<p>single value, or vector of the same length as the diagonal.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector containing the requested diagonal, or a matrix with the requested diagonal replaced by <code>value</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
A &lt;- matrix(1:35,7,5)
A
sdiag(A,1) ## first super diagonal
sdiag(A,-1) ## first sub diagonal

sdiag(A) &lt;- 1 ## leading diagonal set to 1
sdiag(A,3) &lt;- c(-1,-2) ## set 3rd super diagonal 

</code></pre>

<hr>
<h2 id='shash'>Sinh-arcsinh location scale and shape model family</h2><span id='topic+shash'></span>

<h3>Description</h3>

<p>The <code>shash</code> family implements the four-parameter sinh-arcsinh (shash) distribution of 
Jones and Pewsey (2009). The location, scale, skewness and kurtosis of the density can depend 
on additive smooth predictors. Useable only with gam, the linear predictors are specified 
via a list of formulae. It is worth carefully considering whether the data are sufficient to support
estimation of such a flexible model before using it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shash(link = list("identity", "logeb", "identity", "identity"), 
      b = 1e-2, phiPen = 1e-3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shash_+3A_link">link</code></td>
<td>
<p>vector of four characters indicating the link function for location, scale, skewness and kurtosis parameters.</p>
</td></tr>
<tr><td><code id="shash_+3A_b">b</code></td>
<td>
<p>positive parameter of the logeb link function, see Details.</p>
</td></tr>
<tr><td><code id="shash_+3A_phipen">phiPen</code></td>
<td>
<p>positive multiplier of a ridge penalty on kurtosis parameter. Do not touch it unless you know what you are doing, see Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The density function of the shash family is 
</p>
<p style="text-align: center;"><code class="reqn">p(y|\mu,\sigma,\epsilon,\delta)= C(z) \exp\{-S(z)^2/2\} \{2\pi(1+z^2)\}^{-1/2}/\sigma,</code>
</p>

<p>where <code class="reqn"> C(z)=\{1+S(z)^2\}^{1/2} </code>, <code class="reqn"> S(z)=\sinh\{\delta \sinh^{-1}(z)-\epsilon\} </code> and
<code class="reqn">z = (y - \mu)/(\sigma \delta)</code>. Here <code class="reqn">\mu</code> and <code class="reqn">\sigma &gt; 0</code> control, respectively, location and 
scale, <code class="reqn">\epsilon</code> determines skewness, while <code class="reqn">\delta &gt; 0</code> controls tailweight.
<code>shash</code> can model skewness to either side, depending on the sign of <code class="reqn">\epsilon</code>. 
Also, shash can have tails that are lighter (<code class="reqn">\delta&gt;1</code>) or heavier (<code class="reqn">0&lt;\delta&lt;1</code>) that a normal.
For fitting purposes, here we are using <code class="reqn">\tau = \log(\sigma)</code> and <code class="reqn">\phi = \log(\delta)</code>.
</p>
<p>The density is based on the expression given on the second line of section 4.1 and equation (2) of Jones and Pewsey (2009), and uses the simple reparameterization given in section 4.3. 
</p>
<p>The link function used for <code class="reqn">\tau</code> is logeb with is <code class="reqn">\eta = \log \{\exp(\tau)-b\}</code> so that the inverse link is
<code class="reqn">\tau = \log(\sigma) = \log\{\exp(\eta)+b\}</code>. The point is that we are don't allow <code class="reqn">\sigma</code> to become smaller
than a small constant b. The likelihood includes a ridge penalty <code class="reqn">- phiPen * \phi^2</code>, which shrinks <code class="reqn">\phi</code> toward zero. When sufficient data is available the ridge penalty does not change the fit much, but it is useful to include it when fitting the model to small data sets, to avoid <code class="reqn">\phi</code> diverging to +infinity (a problem already identified by Jones and Pewsey (2009)). 
</p>


<h3>Value</h3>

<p>An object inheriting from class <code>general.family</code>.
</p>


<h3>Author(s)</h3>

<p>Matteo Fasiolo &lt;matteo.fasiolo@gmail.com&gt; and Simon N. Wood.
</p>


<h3>References</h3>

<p>Jones, M. and A. Pewsey (2009). Sinh-arcsinh distributions. Biometrika 96 (4), 761-780. <a href="https://doi.org/10.1093/biomet/asp053">doi:10.1093/biomet/asp053</a>
</p>
<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
###############
# Shash dataset
###############
##  Simulate some data from shash
set.seed(847)
n &lt;- 1000
x &lt;- seq(-4, 4, length.out = n)

X &lt;- cbind(1, x, x^2)
beta &lt;- c(4, 1, 1)
mu &lt;- X %*% beta 

sigma =  .5+0.4*(x+4)*.5            # Scale
eps = 2*sin(x)                      # Skewness
del = 1 + 0.2*cos(3*x)              # Kurtosis

dat &lt;-  mu + (del*sigma)*sinh((1/del)*asinh(qnorm(runif(n))) + (eps/del))
dataf &lt;- data.frame(cbind(dat, x))
names(dataf) &lt;- c("y", "x")
plot(x, dat, xlab = "x", ylab = "y")

## Fit model
fit &lt;- gam(list(y ~ s(x), # &lt;- model for location 
                  ~ s(x),   # &lt;- model for log-scale
                  ~ s(x),   # &lt;- model for skewness
                  ~ s(x, k = 20)), # &lt;- model for log-kurtosis
           data = dataf, 
           family = shash, # &lt;- new family 
           optimizer = "efs")

## Plotting truth and estimates for each parameters of the density 
muE &lt;- fit$fitted[ , 1]
sigE &lt;- exp(fit$fitted[ , 2])
epsE &lt;- fit$fitted[ , 3]
delE &lt;- exp(fit$fitted[ , 4])

par(mfrow = c(2, 2))
plot(x, muE, type = 'l', ylab = expression(mu(x)), lwd = 2)
lines(x, mu, col = 2, lty = 2, lwd = 2)
legend("top", c("estimated", "truth"), col = 1:2, lty = 1:2, lwd = 2)

plot(x, sigE, type = 'l', ylab = expression(sigma(x)), lwd = 2)
lines(x, sigma, col = 2, lty = 2, lwd = 2)

plot(x, epsE, type = 'l', ylab = expression(epsilon(x)), lwd = 2)
lines(x, eps, col = 2, lty = 2, lwd = 2)

plot(x, delE, type = 'l', ylab = expression(delta(x)), lwd = 2)
lines(x, del, col = 2, lty = 2, lwd = 2)

## Plotting true and estimated conditional density
par(mfrow = c(1, 1))
plot(x, dat, pch = '.', col = "grey", ylab = "y", ylim = c(-35, 70))
for(qq in c(0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.999)){
  est &lt;- fit$family$qf(p=qq, mu = fit$fitted)
  true &lt;- mu + (del * sigma) * sinh((1/del) * asinh(qnorm(qq)) + (eps/del))
  lines(x, est, type = 'l', col = 1, lwd = 2)
  lines(x, true, type = 'l', col = 2, lwd = 2, lty = 2)
}
legend("topleft", c("estimated", "truth"), col = 1:2, lty = 1:2, lwd = 2)

#####################
## Motorcycle example
#####################

# Here shash is overkill, in fact the fit is not good, relative
# to what we would get with mgcv::gaulss
library(MASS)

b &lt;- gam(list(accel~s(times, k=20, bs = "ad"), ~s(times, k = 10), ~1, ~1),
         data=mcycle, family=shash)

par(mfrow = c(1, 1))
xSeq &lt;- data.frame(cbind("accel" = rep(0, 1e3),
                   "times" = seq(2, 58, length.out = 1e3)))
pred &lt;- predict(b, newdata = xSeq)
plot(mcycle$times, mcycle$accel, ylim = c(-180, 100))
for(qq in c(0.1, 0.3, 0.5, 0.7, 0.9)){
  est &lt;- b$family$qf(p=qq, mu = pred)
  lines(xSeq$times, est, type = 'l', col = 2)
}

plot(b, pages = 1, scale = FALSE)
</code></pre>

<hr>
<h2 id='single.index'>Single index models with mgcv</h2><span id='topic+single.index'></span>

<h3>Description</h3>

<p> Single index models contain smooth terms with arguments that are linear combinations 
of other covariates. e.g. <code class="reqn">s(X\alpha)</code> where <code class="reqn">\alpha</code> has to be estimated. For identifiability, assume <code class="reqn">\|\alpha\|=1</code> with positive first element. One simple way to fit such models is to use <code><a href="#topic+gam">gam</a></code> to profile out the smooth model coefficients and smoothing parameters, leaving only the
<code class="reqn">\alpha</code> to be estimated by a general purpose optimizer. 
</p>
<p>Example code is provided below, which can be easily adapted to include multiple single index terms, parametric terms and further smooths. Note the initialization strategy. First estimate <code class="reqn">\alpha</code> without penalization to get starting values and then do the full fit. Otherwise it is easy to get trapped in a local optimum in which the smooth is linear. An alternative is to initialize using fixed penalization (via the <code>sp</code> argument to <code><a href="#topic+gam">gam</a></code>).
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)

si &lt;- function(theta,y,x,z,opt=TRUE,k=10,fx=FALSE) {
## Fit single index model using gam call, given theta (defines alpha). 
## Return ML if opt==TRUE and fitted gam with theta added otherwise.
## Suitable for calling from 'optim' to find optimal theta/alpha.
  alpha &lt;- c(1,theta) ## constrained alpha defined using free theta
  kk &lt;- sqrt(sum(alpha^2))
  alpha &lt;- alpha/kk  ## so now ||alpha||=1
  a &lt;- x%*%alpha     ## argument of smooth
  b &lt;- gam(y~s(a,fx=fx,k=k)+s(z),family=poisson,method="ML") ## fit model
  if (opt) return(b$gcv.ubre) else {
    b$alpha &lt;- alpha  ## add alpha
    J &lt;- outer(alpha,-theta/kk^2) ## compute Jacobian
    for (j in 1:length(theta)) J[j+1,j] &lt;- J[j+1,j] + 1/kk
    b$J &lt;- J ## dalpha_i/dtheta_j 
    return(b)
  }
} ## si

## simulate some data from a single index model...

set.seed(1)
f2 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
            (10 * x)^3 * (1 - x)^10
n &lt;- 200;m &lt;- 3
x &lt;- matrix(runif(n*m),n,m) ## the covariates for the single index part
z &lt;- runif(n) ## another covariate
alpha &lt;- c(1,-1,.5); alpha &lt;- alpha/sqrt(sum(alpha^2))
eta &lt;- as.numeric(f2((x%*%alpha+.41)/1.4)+1+z^2*2)/4
mu &lt;- exp(eta)
y &lt;- rpois(n,mu) ## Poi response 

## now fit to the simulated data...


th0 &lt;- c(-.8,.4) ## close to truth for speed
## get initial theta, using no penalization...
f0 &lt;- nlm(si,th0,y=y,x=x,z=z,fx=TRUE,k=5)
## now get theta/alpha with smoothing parameter selection...
f1 &lt;- nlm(si,f0$estimate,y=y,x=x,z=z,hessian=TRUE,k=10)
theta.est &lt;-f1$estimate 

## Alternative using 'optim'... 

th0 &lt;- rep(0,m-1) 
## get initial theta, using no penalization...
f0 &lt;- optim(th0,si,y=y,x=x,z=z,fx=TRUE,k=5)
## now get theta/alpha with smoothing parameter selection...
f1 &lt;- optim(f0$par,si,y=y,x=x,z=z,hessian=TRUE,k=10)
theta.est &lt;-f1$par 

## extract and examine fitted model...

b &lt;- si(theta.est,y,x,z,opt=FALSE) ## extract best fit model
plot(b,pages=1)
b
b$alpha 
## get sd for alpha...
Vt &lt;- b$J%*%solve(f1$hessian,t(b$J))
diag(Vt)^.5

</code></pre>

<hr>
<h2 id='Sl.inirep'>Re-parametrizing model matrix X</h2><span id='topic+Sl.inirep'></span><span id='topic+Sl.initial.repara'></span>

<h3>Description</h3>

<p>INTERNAL routine to apply initial Sl re-parameterization to model matrix X,
or, if <code>inverse==TRUE</code>, to apply inverse re-parametrization to parameter vector 
or covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sl.inirep(Sl,X,l,r,nt=1)

Sl.initial.repara(Sl, X, inverse = FALSE, both.sides = TRUE, cov = TRUE,
  nt = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sl.inirep_+3A_sl">Sl</code></td>
<td>
<p>the output of <code>Sl.setup</code>.</p>
</td></tr>
<tr><td><code id="Sl.inirep_+3A_x">X</code></td>
<td>
<p>the model matrix.</p>
</td></tr>
<tr><td><code id="Sl.inirep_+3A_l">l</code></td>
<td>
<p>if non-zero apply transform (positive) or inverse transform from left. 1 or -1 of transform, 2 or -2 for transpose.</p>
</td></tr>
<tr><td><code id="Sl.inirep_+3A_r">r</code></td>
<td>
<p>if non-zero apply transform (positive) or inverse transform from right. 1 or -1 of transform, 2 or -2 for transpose.</p>
</td></tr>
<tr><td><code id="Sl.inirep_+3A_inverse">inverse</code></td>
<td>
<p>if <code>TRUE</code> an inverse re-parametrization is performed.</p>
</td></tr>
<tr><td><code id="Sl.inirep_+3A_both.sides">both.sides</code></td>
<td>
<p>if <code>inverse==TRUE</code> and <code>both.sides==FALSE</code> then
the re-parametrization only applied to rhs, as appropriate for a choleski factor.
If <code>both.sides==FALSE</code>, <code>X</code> is a vector and <code>inverse==FALSE</code> then <code>X</code> is
taken as a coefficient vector (so re-parametrization is inverse of that for the model matrix).</p>
</td></tr>
<tr><td><code id="Sl.inirep_+3A_cov">cov</code></td>
<td>
<p>boolean indicating whether <code>X</code> is a covariance matrix.</p>
</td></tr>
<tr><td><code id="Sl.inirep_+3A_nt">nt</code></td>
<td>
<p>number of parallel threads to be used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A re-parametrized version of <code>X</code>.
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>

<hr>
<h2 id='Sl.repara'>Applying re-parameterization from log-determinant of penalty matrix to
model matrix.</h2><span id='topic+Sl.repara'></span>

<h3>Description</h3>

<p>INTERNAL routine to apply re-parameterization from log-determinant of penalty matrix, <code>ldetS</code> to
model matrix, <code>X</code>, blockwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sl.repara(rp, X, inverse = FALSE, both.sides = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sl.repara_+3A_rp">rp</code></td>
<td>
<p>reparametrization.</p>
</td></tr>
<tr><td><code id="Sl.repara_+3A_x">X</code></td>
<td>
<p>if <code>X</code> is a matrix it is assumed to be a model matrix 
whereas if <code>X</code> is a vector it is assumed to be a parameter vector.</p>
</td></tr>
<tr><td><code id="Sl.repara_+3A_inverse">inverse</code></td>
<td>
<p>if <code>TRUE</code> an inverse re-parametrization is performed.</p>
</td></tr>
<tr><td><code id="Sl.repara_+3A_both.sides">both.sides</code></td>
<td>
<p>if <code>inverse==TRUE</code> and <code>both.sides==FALSE</code> then
the re-parametrization only applied to rhs, as appropriate for a choleski factor.
If <code>both.sides==FALSE</code>, <code>X</code> is a vector and <code>inverse==FALSE</code> then <code>X</code> is
taken as a coefficient vector (so re-parametrization is inverse of that for the model matrix).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A re-parametrized version of <code>X</code>.
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>

<hr>
<h2 id='Sl.setup'>Setting up a list representing a block diagonal penalty matrix</h2><span id='topic+Sl.setup'></span>

<h3>Description</h3>

<p>INTERNAL function for setting up a list representing a block diagonal penalty matrix
from the object produced by <code>gam.setup</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sl.setup(G,cholesky=FALSE,no.repara=FALSE,sparse=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sl.setup_+3A_g">G</code></td>
<td>
<p>the output of <code>gam.setup</code>.</p>
</td></tr>
<tr><td><code id="Sl.setup_+3A_cholesky">cholesky</code></td>
<td>
<p>re-parameterize using Cholesky only.</p>
</td></tr>
<tr><td><code id="Sl.setup_+3A_no.repara">no.repara</code></td>
<td>
<p>set to <code>TRUE</code> to turn off all initial reparameterization.</p>
</td></tr>
<tr><td><code id="Sl.setup_+3A_sparse">sparse</code></td>
<td>
<p>sparse setup?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with an element for each block. 
For block, b, <code>Sl[[b]]</code> is a list with the following elements </p>

<ul>
<li><p><code>repara</code>: should re-parameterization be applied to model matrix, etc?
Usually <code>FALSE</code> if non-linear in coefficients.
</p>
</li>
<li><p><code>start, stop</code>: such that <code>start:stop</code> are the indexes of the parameters of this block.
</p>
</li>
<li><p><code>S</code>: a list of penalty matrices for the block (<code>dim = stop-start+1</code>)
If <code>length(S)==1</code> then this will be an identity penalty.
Otherwise it is a multiple penalty, and an <code>rS</code> list of square
root penalty matrices will be added. <code>S</code> (if <code>repara==TRUE</code>) and <code>rS</code> (always)
will be projected into range space of total penalty matrix.
</p>
</li>
<li><p><code>rS</code>: square root of penalty matrices if multiple penalties are used.
</p>
</li>
<li><p><code>D</code>: a reparameterization matrix for the block. Applies to cols/params in <code>start:stop</code>.
If numeric then <code>X[,start:stop]%*%diag(D)</code> is re-parametrization of <code>X[,start:stop]</code>,
and <code>b.orig = D*b.repara</code> (where <code>b.orig</code> is the original parameter vector).
If matrix then <code>X[,start:stop]%*%D</code> is re-parametrization of <code>X[,start:stop]</code>,
and <code>b.orig = D%*%b.repara</code> (where <code>b.orig</code> is the original parameter vector).
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>

<hr>
<h2 id='slanczos'>Compute truncated eigen decomposition of a symmetric matrix</h2><span id='topic+slanczos'></span>

<h3>Description</h3>

<p> Uses Lanczos iteration to find the truncated eigen-decomposition of a symmetric matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slanczos(A,k=10,kl=-1,tol=.Machine$double.eps^.5,nt=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slanczos_+3A_a">A</code></td>
<td>
<p>A symmetric matrix.</p>
</td></tr>
<tr><td><code id="slanczos_+3A_k">k</code></td>
<td>
<p>Must be non-negative. If <code>kl</code> is negative, then the <code>k</code> largest magnitude eigenvalues 
are found, together with the corresponding eigenvectors. If <code>kl</code> is non-negative then the <code>k</code>
highest eigenvalues are found together with their eigenvectors and the <code>kl</code> lowest eigenvalues with
eigenvectors are also returned.</p>
</td></tr>
<tr><td><code id="slanczos_+3A_kl">kl</code></td>
<td>
<p>If <code>kl</code> is non-negative then the <code>kl</code> lowest eigenvalues are returned together with their 
corresponding eigenvectors (in addition to the <code>k</code> highest eignevalues + vectors). 
negative <code>kl</code> signals that the <code>k</code> largest magnitude eigenvalues should be returned, with eigenvectors.</p>
</td></tr>
<tr><td><code id="slanczos_+3A_tol">tol</code></td>
<td>
<p>tolerance to use for convergence testing of eigenvalues. Error in eigenvalues will be less 
than the magnitude of the dominant eigenvalue multiplied by <code>tol</code> (or the machine precision!).</p>
</td></tr>
<tr><td><code id="slanczos_+3A_nt">nt</code></td>
<td>
<p>number of threads to use for leading order iterative multiplication of A by vector. May show no 
speed improvement on two processor machine.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> If <code>kl</code> is non-negative, returns the highest <code>k</code> and lowest <code>kl</code> eigenvalues, 
with their corresponding eigenvectors. If <code>kl</code> is negative, returns the largest magnitude <code>k</code> 
eigenvalues, with corresponding eigenvectors.
</p>
<p>The routine implements Lanczos iteration with full re-orthogonalization as described in Demmel (1997). Lanczos 
iteraction iteratively constructs a tridiagonal matrix, the eigenvalues of which converge to the eigenvalues of <code>A</code>,
as the iteration proceeds (most extreme first). Eigenvectors can also be computed. For small <code>k</code> and <code>kl</code> the 
approach is faster than computing the full symmetric eigendecompostion. The tridiagonal eigenproblems are handled using LAPACK.
</p>
<p>The implementation is not optimal: in particular the inner triadiagonal problems could be handled more efficiently, and 
there would be some savings to be made by not always returning eigenvectors. 
</p>


<h3>Value</h3>

<p> A list with elements <code>values</code> (array of eigenvalues); <code>vectors</code> (matrix with eigenvectors in its columns); 
<code>iter</code> (number of iterations required).
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Demmel, J. (1997) Applied Numerical Linear Algebra. SIAM
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cyclic.p.spline">cyclic.p.spline</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> require(mgcv)
 ## create some x's and knots...
 set.seed(1);
 n &lt;- 700;A &lt;- matrix(runif(n*n),n,n);A &lt;- A+t(A)
 
 ## compare timings of slanczos and eigen
 system.time(er &lt;- slanczos(A,10))
 system.time(um &lt;- eigen(A,symmetric=TRUE))
 
 ## confirm values are the same...
 ind &lt;- c(1:6,(n-3):n)
 range(er$values-um$values[ind]);range(abs(er$vectors)-abs(um$vectors[,ind]))
</code></pre>

<hr>
<h2 id='smooth.construct'>Constructor functions for smooth terms in a GAM</h2><span id='topic+smooth.construct'></span><span id='topic+smooth.construct2'></span><span id='topic+user.defined.smooth'></span>

<h3>Description</h3>

<p>Smooth terms in a GAM formula are turned into smooth specification objects of 
class <code>xx.smooth.spec</code> during processing of the formula. Each of these objects is
converted to a smooth object using an appropriate <code>smooth.construct</code> function. New smooth classes 
can be added by writing a new <code>smooth.construct</code> method function and a corresponding 
<code><a href="#topic+Predict.matrix">Predict.matrix</a></code> method function (see example code below).
</p>
<p>In practice, <code>smooth.construct</code> is usually called via <code>smooth.construct2</code> and the wrapper
function <code><a href="#topic+smoothCon">smoothCon</a></code>, in order to handle <code>by</code> variables and
centering constraints (see the <code><a href="#topic+smoothCon">smoothCon</a></code> documentation if 
you need to handle these things directly, for a user defined smooth class).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smooth.construct(object,data,knots)
smooth.construct2(object,data,knots)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smooth.construct_+3A_object">object</code></td>
<td>
<p> is a smooth specification object, generated by an <code><a href="#topic+s">s</a></code> or <code><a href="#topic+te">te</a></code> term in a GAM 
formula. Objects generated by <code>s</code> terms have class <code>xx.smooth.spec</code> where <code>xx</code> is given by the 
<code>bs</code> argument of <code>s</code> (this convention allows the user to add their own smoothers). 
If <code>object</code> is not class <code>tensor.smooth.spec</code> it will have the following elements:
</p>

<dl>
<dt>term</dt><dd><p>The names of the covariates for this smooth, in an array.</p>
</dd>
<dt>bs.dim</dt><dd><p> Argument <code>k</code> of the <code>s</code> term generating the object. This is the dimension of the basis 
used to represent the term (or, arguably, 1 greater than the basis dimension for <code>cc</code> terms). 
<code>bs.dim&lt;0</code> indicates that the constructor should set this to the default value.</p>
</dd>
<dt>fixed</dt><dd><p><code>TRUE</code> if the term is to be unpenalized, otherwise <code>FALSE</code>.</p>
</dd>
<dt>dim</dt><dd><p>the number covariates of which this smooth is a function.</p>
</dd>
<dt>p.order</dt><dd><p>the order of the smoothness penalty or <code>NA</code> for autoselection of this. This is argument 
<code>m</code> of the <code>s</code> term that generated <code>object</code>.</p>
</dd>
<dt>by</dt><dd><p>the name of any <code>by</code> variable to multiply this term as supplied as an argument to <code>s</code>. 
<code>"NA"</code> if there is no such term.</p>
</dd>
<dt>label</dt><dd><p>A suitable label for use with this term.</p>
</dd>
<dt>xt</dt><dd><p>An object containing information that may be needed for basis setup
(used, e.g. by <code>"tp"</code> smooths to pass optional information on big dataset
handling).</p>
</dd>
<dt>id</dt><dd><p>Any identity associated with this term &mdash; used for linking bases
and smoothing parameters. <code>NULL</code> by default, indicating no linkage.</p>
</dd>
<dt>sp</dt><dd><p>Smoothing parameters for the term. Any negative are estimated,
otherwise they are fixed at the supplied value. Unless <code>NULL</code> (default),
over-rides <code>sp</code> argument to <code><a href="#topic+gam">gam</a></code>.</p>
</dd>
</dl>

<p>If <code>object</code> is of class <code>tensor.smooth.spec</code> then it was generated by a <code>te</code> term in the GAM formula, 
and specifies a smooth of several variables with a basis generated as a tensor product of lower dimensional bases. 
In this case the object will be different and will have the following elements:
</p>

<dl>
<dt>margin</dt><dd><p>is a list of smooth specification objects of the type listed above, defining the bases which have 
their tensor product formed in order to construct this term.</p>
</dd>
<dt>term</dt><dd><p>is the array of names of the covariates that are arguments of the smooth.</p>
</dd>
<dt>by</dt><dd><p>is the name of any <code>by</code> variable, or <code>"NA"</code>.</p>
</dd>
<dt>fx</dt><dd><p>is an array, the elements of which indicate whether (<code>TRUE</code>) any of the margins in the 
tensor product should be unpenalized.</p>
</dd>
<dt>label</dt><dd><p>A suitable label for use with this term.</p>
</dd>
<dt>dim</dt><dd><p>is the number of covariates of which this smooth is a function.</p>
</dd>
<dt>mp</dt><dd><p><code>TRUE</code> if multiple penalties are to be used.</p>
</dd>
<dt>np</dt><dd><p><code>TRUE</code> if 1-D marginal smooths are to be re-parameterized in terms of
function values.</p>
</dd>
<dt>id</dt><dd><p>Any identity associated with this term &mdash; used for linking bases
and smoothing parameters. <code>NULL</code> by default, indicating no linkage.</p>
</dd>
<dt>sp</dt><dd><p>Smoothing parameters for the term. Any negative are estimated,
otherwise they are fixed at the supplied value. Unless <code>NULL</code> (default),
over-rides <code>sp</code> argument to <code><a href="#topic+gam">gam</a></code>.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="smooth.construct_+3A_data">data</code></td>
<td>
<p>For <code>smooth.construct</code> a data frame or list containing the evaluation of the elements of <code>object$term</code>,
with names given by <code>object$term</code>. The last entry will be the <code>by</code> variable, if <code>object$by</code>
is not <code>"NA"</code>. For <code>smooth.construct2</code> <code>data</code> need only be an object within which <code>object$term</code> 
can be evaluated, the variables can be in any order, and there can be irrelevant variables present as well. </p>
</td></tr>
<tr><td><code id="smooth.construct_+3A_knots">knots</code></td>
<td>
<p>an optional data frame or list containing the knots relating to <code>object$term</code>. 
If it is <code>NULL</code> then the knot locations are generated automatically. The structure of <code>knots</code> should
be as for <code>data</code>, depending on whether <code>smooth.construct</code> or <code>smooth.construct2</code> is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> There are built in methods for objects with the following classes:
<code>tp.smooth.spec</code> (thin plate regression splines: see <code><a href="#topic+tprs">tprs</a></code>); 
<code>ts.smooth.spec</code> (thin plate regression splines with shrinkage-to-zero);
<code>cr.smooth.spec</code> (cubic regression splines: see <code><a href="#topic+cubic.regression.spline">cubic.regression.spline</a></code>;
<code>cs.smooth.spec</code> (cubic regression splines with shrinkage-to-zero);
<code>cc.smooth.spec</code> (cyclic cubic regression splines);
<code>ps.smooth.spec</code> (Eilers and Marx (1986) style P-splines: see <code><a href="#topic+p.spline">p.spline</a></code>);
<code>cp.smooth.spec</code> (cyclic P-splines);
<code>ad.smooth.spec</code> (adaptive smooths of 1 or 2 variables: see <code><a href="#topic+adaptive.smooth">adaptive.smooth</a></code>);
<code>re.smooth.spec</code> (simple random effect terms);
<code>mrf.smooth.spec</code> (Markov random field smoothers for smoothing over discrete districts);
<code>tensor.smooth.spec</code> (tensor product smooths).
</p>
<p>There is an implicit assumption that the basis only depends on the knots and/or the set of unique 
covariate combinations; i.e. that the basis is the same whether generated from
the full set of covariates, or just the unique combinations of covariates. 
</p>
<p>Plotting of smooths is handled by plot methods for smooth objects. A default <code>mgcv.smooth</code> method 
is used if there is no more specific method available. Plot methods can be added for specific smooth classes, see 
source code for <code>mgcv:::plot.sos.smooth</code>, <code>mgcv:::plot.random.effect</code>, <code>mgcv:::plot.mgcv.smooth</code> 
for example code.
</p>


<h3>Value</h3>

<p>The input argument <code>object</code>, assigned a new class to indicate what type of smooth it is and with at least the 
following items added:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>The model matrix from this term. This may have an <code>"offset"</code>
attribute:  a vector of length <code>nrow(X)</code> containing any contribution of
the smooth to the model offset term. <code>by</code> variables do not need to be
dealt with here, but if they are then an item <code>by.done</code> must be added to
the <code>object</code>.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>A list of positive semi-definite penalty matrices that apply to this term. The list will be empty 
if the term is to be left un-penalized.</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>An array giving the ranks of the penalties.</p>
</td></tr>
<tr><td><code>null.space.dim</code></td>
<td>
<p>The dimension of the penalty null space (before centering).</p>
</td></tr>
</table>
<p>The following items may be added:
</p>
<table>
<tr><td><code>C</code></td>
<td>
<p>The matrix defining any identifiability constraints on the term, for use when fitting. If this is <code>NULL</code> then
<code>smoothCon</code> will add an identifiability constraint that each term should
sum to zero over the covariate values. Set to a zero row matrix if no
constraints are required. If a supplied <code>C</code> has an attribute <code>"always.apply"</code> then it is never ignored, even if any
<code>by</code> variables of a smooth imply that no constraint is actually needed. Code for creating <code>C</code> should check whether 
the specification object already contains a zero row matrix, and leave this unchanged if it is (since this signifies 
no constraint should be produced). </p>
</td></tr>
<tr><td><code>Cp</code></td>
<td>
<p>An optional matrix supplying alternative identifiability constraints for use when predicting. By default the 
fitting constrants are used. This option is useful when some sort of simple sparse constraint is required for fitting, but the 
usual sum-to-zero constraint is required for prediction so that, e.g. the CIs for model components are as narrow as possible. </p>
</td></tr>
<tr><td><code>no.rescale</code></td>
<td>
<p>if this is non-NULL then the penalty coefficient matrix of the smooth will not be 
rescaled for enhaced numerical stability (rescaling is the default, because <code><a href="#topic+gamm">gamm</a></code> requires it). 
Turning off rescaling is useful if the values of the smoothing parameters should be interpretable in a model, 
for example because they are inverse variance components.  </p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>the degrees of freedom associated with this term (when
unpenalized and unconstrained). If this is null then <code>smoothCon</code> will set it to the basis 
dimension. <code>smoothCon</code> will reduce this by the number of constraints.</p>
</td></tr>
<tr><td><code>te.ok</code></td>
<td>
<p><code>0</code> if this term should not be used as a tensor product marginal, <code>1</code> if 
it can be used and plotted, and <code>2</code> is it can be used but not plotted. Set to <code>1</code> if <code>NULL</code>.</p>
</td></tr>
<tr><td><code>plot.me</code></td>
<td>
<p>Set to <code>FALSE</code> if this smooth should not be plotted by <code><a href="#topic+plot.gam">plot.gam</a></code>.  Set to <code>TRUE</code> if <code>NULL</code>.</p>
</td></tr>
<tr><td><code>side.constrain</code></td>
<td>
<p>Set to <code>FALSE</code> to ensure that the smooth is never subject to side constraints as a result of nesting. </p>
</td></tr>
<tr><td><code>L</code></td>
<td>
<p>smooths may depend on fewer &lsquo;underlying&rsquo; smoothing parameters than there are elements of
<code>S</code>. In this case <code>L</code> is the matrix mapping the vector of underlying log smoothing 
parameters to the vector of logs of the smoothing parameters actually multiplying the <code>S[[i]]</code>. 
<code>L=NULL</code> signifies that there is one smoothing parameter per <code>S[[i]]</code>. </p>
</td></tr>
</table>
<p>Usually the returned object will also include extra information required to define the basis, and used by 
<code><a href="#topic+Predict.matrix">Predict.matrix</a></code> methods to make predictions using the basis. See
the <code>Details</code> section for links to the information included for the built in smooth classes. 
</p>
<p><code>tensor.smooth</code> returned objects will additionally have each element of
the <code>margin</code> list updated in the same way. <code>tensor.smooths</code> also
have a list, <code>XP</code>, containing re-parameterization matrices for any 1-D marginal terms
re-parameterized in terms of function values. This list will have <code>NULL</code>
entries for marginal smooths that are not re-parameterized, and is only long
enough to reach the last re-parameterized marginal in the list.  
</p>


<h3>WARNING</h3>

<p>User defined smooth objects should avoid having attributes names
<code>"qrc"</code> or <code>"nCons"</code> as these are used internally to provide
constraint free parameterizations.</p>


<h3>Author(s)</h3>

<p>Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

 
<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
</p>
<p>Wood, S.N. (2006) Low rank scale invariant tensor product smooths for
generalized additive mixed models. Biometrics 62(4):1025-1036
</p>
<p>The code given in the example is based on the smooths advocated in:
</p>
<p>Ruppert, D., M.P. Wand and R.J. Carroll (2003) Semiparametric Regression. Cambridge 
University Press.
</p>
<p>However if you want p-splines, rather than splines with derivative based penalties,
then the built in &quot;ps&quot; class is probably a marginally better bet. It's based on
</p>
<p>Eilers, P.H.C. and B.D. Marx (1996) Flexible Smoothing with B-splines and Penalties. 
Statistical Science, 11(2):89-121
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+s">s</a></code>,<code><a href="#topic+get.var">get.var</a></code>, <code><a href="#topic+gamm">gamm</a></code>, <code><a href="#topic+gam">gam</a></code>,
<code><a href="#topic+Predict.matrix">Predict.matrix</a></code>,
<code><a href="#topic+smoothCon">smoothCon</a></code>, <code><a href="#topic+PredictMat">PredictMat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Adding a penalized truncated power basis class and methods
## as favoured by Ruppert, Wand and Carroll (2003) 
## Semiparametric regression CUP. (No advantage to actually
## using this, since mgcv can happily handle non-identity 
## penalties.)

smooth.construct.tr.smooth.spec&lt;-function(object,data,knots) {
## a truncated power spline constructor method function
## object$p.order = null space dimension
  m &lt;- object$p.order[1]
  if (is.na(m)) m &lt;- 2 ## default 
  if (m&lt;1) stop("silly m supplied")
  if (object$bs.dim&lt;0) object$bs.dim &lt;- 10 ## default
  nk&lt;-object$bs.dim-m-1 ## number of knots
  if (nk&lt;=0) stop("k too small for m")
  x &lt;- data[[object$term]]  ## the data
  x.shift &lt;- mean(x) # shift used to enhance stability
  k &lt;- knots[[object$term]] ## will be NULL if none supplied
  if (is.null(k)) # space knots through data
  { n&lt;-length(x)
    k&lt;-quantile(x[2:(n-1)],seq(0,1,length=nk+2))[2:(nk+1)]
  }
  if (length(k)!=nk) # right number of knots?
  stop(paste("there should be ",nk," supplied knots"))
  x &lt;- x - x.shift # basis stabilizing shift
  k &lt;- k - x.shift # knots treated the same!
  X&lt;-matrix(0,length(x),object$bs.dim)
  for (i in 1:(m+1)) X[,i] &lt;- x^(i-1)
  for (i in 1:nk) X[,i+m+1]&lt;-(x-k[i])^m*as.numeric(x&gt;k[i])
  object$X&lt;-X # the finished model matrix
  if (!object$fixed) # create the penalty matrix
  { object$S[[1]]&lt;-diag(c(rep(0,m+1),rep(1,nk)))
  }
  object$rank&lt;-nk  # penalty rank
  object$null.space.dim &lt;- m+1  # dim. of unpenalized space
  ## store "tr" specific stuff ...
  object$knots&lt;-k;object$m&lt;-m;object$x.shift &lt;- x.shift
 
  object$df&lt;-ncol(object$X)     # maximum DoF (if unconstrained)
 
  class(object)&lt;-"tr.smooth"  # Give object a class
  object
}

Predict.matrix.tr.smooth&lt;-function(object,data) {
## prediction method function for the `tr' smooth class
  x &lt;- data[[object$term]]
  x &lt;- x - object$x.shift # stabilizing shift
  m &lt;- object$m;     # spline order (3=cubic)
  k&lt;-object$knots    # knot locations
  nk&lt;-length(k)      # number of knots
  X&lt;-matrix(0,length(x),object$bs.dim)
  for (i in 1:(m+1)) X[,i] &lt;- x^(i-1)
  for (i in 1:nk) X[,i+m+1] &lt;- (x-k[i])^m*as.numeric(x&gt;k[i])
  X # return the prediction matrix
}

# an example, using the new class....
require(mgcv)
set.seed(100)
dat &lt;- gamSim(1,n=400,scale=2)
b&lt;-gam(y~s(x0,bs="tr",m=2)+s(x1,bs="ps",m=c(1,3))+
         s(x2,bs="tr",m=3)+s(x3,bs="tr",m=2),data=dat)
plot(b,pages=1)
b&lt;-gamm(y~s(x0,bs="tr",m=2)+s(x1,bs="ps",m=c(1,3))+
         s(x2,bs="tr",m=3)+s(x3,bs="tr",m=2),data=dat)
plot(b$gam,pages=1)
# another example using tensor products of the new class
dat &lt;- gamSim(2,n=400,scale=.1)$data
b &lt;- gam(y~te(x,z,bs=c("tr","tr"),m=c(2,2)),data=dat)
vis.gam(b)
</code></pre>

<hr>
<h2 id='smooth.construct.ad.smooth.spec'>Adaptive smooths in GAMs</h2><span id='topic+smooth.construct.ad.smooth.spec'></span><span id='topic+adaptive.smooth'></span>

<h3>Description</h3>

<p><code><a href="#topic+gam">gam</a></code> can use adaptive smooths of one or two variables, specified 
via terms like <code>s(...,bs="ad",...)</code>. (<code><a href="#topic+gamm">gamm</a></code> can not use such terms &mdash; check out 
package <code>AdaptFit</code> if this is a problem.) The basis for such a term is a (tensor product of) 
p-spline(s) or cubic regression spline(s). Discrete P-spline type penalties are applied directly to the coefficients 
of the basis, but the penalties themselves have a basis representation, allowing the strength of the
penalty to vary with the covariates. The coefficients of the penalty basis are the smoothing parameters.
</p>
<p>When invoking an adaptive smoother the <code>k</code> argument specifies the dimension of the smoothing basis (default 40 in 1D, 15 in 2D), 
while the <code>m</code> argument specifies the dimension of the penalty basis (default 5 in 1D, 3 in 2D). For an adaptive smooth of two 
variables <code>k</code> is taken as the dimension of both marginal bases: different marginal basis dimensions can be 
specified by making <code>k</code> a two element vector. Similarly, in the two dimensional case <code>m</code> is the 
dimension of both marginal bases for the penalties, unless it is a two element vector, which specifies different
basis dimensions for each marginal (If the penalty basis is based on a thin plate spline then <code>m</code> specifies
its dimension directly).
</p>
<p>By default, P-splines are used for the smoothing and penalty bases, but this can be modified by supplying a list as
argument <code>xt</code> with a character vector <code>xt$bs</code> specifying the smoothing basis type. 
Only <code>"ps"</code>, <code>"cp"</code>, <code>"cc"</code> and <code>"cr"</code> may be used for the smoothing basis. The penalty basis is 
always a B-spline, or a cyclic B-spline for cyclic bases.
</p>
<p>The total number of smoothing parameters to be estimated for the term will be the dimension of the penalty basis. 
Bear in mind that adaptive smoothing places quite severe demands on the data. For example, setting <code>m=10</code> for a 
univariate smooth of 200 data is rather like estimating 10 smoothing parameters, each from a data series of length 20.
The problem is particularly serious for smooths of 2 variables, where the number of smoothing parameters required to 
get reasonable flexibility in the penalty can grow rather fast, but it often requires a very large smoothing basis 
dimension to make good use of this flexibility. In short, adaptive smooths should be used sparingly and with care. 
</p>
<p>In practice it is often as effective to simply transform the smoothing covariate as it is to use an adaptive smooth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ad.smooth.spec'
smooth.construct(object, data, knots)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.ad.smooth.spec_+3A_object">object</code></td>
<td>
<p>a smooth specification object, usually generated by a term <code>s(...,bs="ad",...)</code></p>
</td></tr>
<tr><td><code id="smooth.construct.ad.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.ad.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>a list containing any knots supplied for basis setup &mdash; in same order and with same names as <code>data</code>. 
Can be <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p> The constructor is not normally called directly, but is rather used internally by <code><a href="#topic+gam">gam</a></code>. 
To use for basis setup it is recommended to use <code><a href="#topic+smooth.construct2">smooth.construct2</a></code>.  
</p>
<p>This class can not be used as a marginal basis in a tensor product smooth, nor by <code>gamm</code>.
</p>


<h3>Value</h3>

<p> An object of class <code>"pspline.smooth"</code> in the 1D case or <code>"tensor.smooth"</code> in the 2D case.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Comparison using an example taken from AdaptFit
## library(AdaptFit)
require(mgcv)
set.seed(0)
x &lt;- 1:1000/1000
mu &lt;- exp(-400*(x-.6)^2)+5*exp(-500*(x-.75)^2)/3+2*exp(-500*(x-.9)^2)
y &lt;- mu+0.5*rnorm(1000)

##fit with default knots
## y.fit &lt;- asp(y~f(x))

par(mfrow=c(2,2))
## plot(y.fit,main=round(cor(fitted(y.fit),mu),digits=4))
## lines(x,mu,col=2)

b &lt;- gam(y~s(x,bs="ad",k=40,m=5)) ## adaptive
plot(b,shade=TRUE,main=round(cor(fitted(b),mu),digits=4))
lines(x,mu-mean(mu),col=2)
 
b &lt;- gam(y~s(x,k=40))             ## non-adaptive
plot(b,shade=TRUE,main=round(cor(fitted(b),mu),digits=4))
lines(x,mu-mean(mu),col=2)

b &lt;- gam(y~s(x,bs="ad",k=40,m=5,xt=list(bs="cr")))
plot(b,shade=TRUE,main=round(cor(fitted(b),mu),digits=4))
lines(x,mu-mean(mu),col=2)

## A 2D example (marked, 'Not run' purely to reduce
## checking load on CRAN).

par(mfrow=c(2,2),mar=c(1,1,1,1))
x &lt;- seq(-.5, 1.5, length= 60)
z &lt;- x
f3 &lt;- function(x,z,k=15) { r&lt;-sqrt(x^2+z^2);f&lt;-exp(-r^2*k);f}  
f &lt;- outer(x, z, f3)
op &lt;- par(bg = "white")

## Plot truth....
persp(x,z,f,theta=30,phi=30,col="lightblue",ticktype="detailed")

n &lt;- 2000
x &lt;- runif(n)*2-.5
z &lt;- runif(n)*2-.5
f &lt;- f3(x,z)
y &lt;- f + rnorm(n)*.1

## Try tprs for comparison...
b0 &lt;- gam(y~s(x,z,k=150))
vis.gam(b0,theta=30,phi=30,ticktype="detailed")

## Tensor product with non-adaptive version of adaptive penalty
b1 &lt;- gam(y~s(x,z,bs="ad",k=15,m=1),gamma=1.4)
vis.gam(b1,theta=30,phi=30,ticktype="detailed")

## Now adaptive...
b &lt;- gam(y~s(x,z,bs="ad",k=15,m=3),gamma=1.4)
vis.gam(b,theta=30,phi=30,ticktype="detailed")
cor(fitted(b0),f);cor(fitted(b),f)



</code></pre>

<hr>
<h2 id='smooth.construct.bs.smooth.spec'>Penalized B-splines in GAMs</h2><span id='topic+smooth.construct.bs.smooth.spec'></span><span id='topic+Predict.matrix.Bspline.smooth'></span><span id='topic+b.spline'></span><span id='topic+d.spline'></span>

<h3>Description</h3>

<p><code><a href="#topic+gam">gam</a></code> can use smoothing splines based on univariate B-spline bases
with derivative based penalties, specified via terms like <code>s(x,bs="bs",m=c(3,2))</code>. <code>m[1]</code> controls the spline order, with <code>m[1]=3</code> being a cubic spline, <code>m[1]=2</code> being quadratic, and so on. The integrated square of the <code>m[2]</code>th derivative is used as the penalty. So <code>m=c(3,2)</code> is a conventional cubic spline. Any further elements of <code>m</code>, after the first 2, define the order of derivative in further penalties. If <code>m</code> is supplied as a single number, then it is taken to be <code>m[1]</code> and <code>m[2]=m[1]-1</code>, which is only a conventional smoothing spline in the <code>m=3</code>, cubic spline case. Notice that the definition of the spline order in terms of <code>m[1]</code> is intuitive, but differs to that used with the <code><a href="#topic+tprs">tprs</a></code> and <code><a href="#topic+p.spline">p.spline</a></code> bases. See details for options for controlling the interval over which the penalty is evaluated (which can matter if it is necessary to extrapolate).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bs.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'Bspline.smooth'
Predict.matrix(object, data)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.bs.smooth.spec_+3A_object">object</code></td>
<td>
<p>a smooth specification object, usually generated by a term <code>s(x,bs="bs",...)</code></p>
</td></tr>
<tr><td><code id="smooth.construct.bs.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.bs.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>a list containing any knots supplied for basis setup &mdash; in same order and with same names as <code>data</code>. 
Can be <code>NULL</code>. See details for further information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> The basis and penalty are sparse (although sparse matrices are not used to represent them). <code>m[2]&gt;m[1]</code> will generate an error, since in that case the penalty would be based on an undefined derivative of the basis, which makes no sense. The terms can have multiple penalties of different orders, for example <code>s(x,bs="bs",m=c(3,2,1,0))</code> specifies a cubic basis with 3 penalties: a conventional cubic spline penalty, an integrated square of first derivative penalty, and an integrated square of function value penalty.  
</p>
<p>The default basis dimension, <code>k</code>, is the larger of 10 and <code>m[1]</code>. <code>m[1]</code> is the lower limit on basis dimension. If knots are supplied, then  the number of supplied knots should be <code>k + m[1] + 1</code>, and the range of the middle <code>k-m[1]+1</code> knots should include all the covariate values. Alternatively, 2 knots can be supplied, denoting the  lower and upper limits between which the spline can be evaluated (making this range too wide mean that there is no information about some basis coefficients, because the corresponding basis functions have a span that includes no data). Unlike P-splines, splines with derivative based penalties can have uneven knot spacing, without a problem.
</p>
<p>Another option is to supply 4 knots. Then the outer 2 define the interval over which the penalty is to be evaluated, while the inner 2 define an interval within which all but the outermost 2 knots should lie. Normally the outer 2 knots would be the interval over which predictions might be required, while the inner 2 knots define the interval within which the data lie. This option allows the penalty to apply over a wider interval than the data, while still placing most of the basis functions where the data are. This is useful in situations in which it is necessary to extrapolate slightly with a smooth. Only applying the penalty over the interval containing the data amounts to a model in which the function could be less smooth outside the interval than within it, and leads to very wide extrapolation confidence intervals. However the alternative of evaluating the penalty over the whole real line amounts to asserting certainty that the function has some derivative zeroed away from the data, which is equally unreasonable. It is prefereable to build a model in which the same smoothness assumtions apply over both data and extrapolation intervals, but not over the whole real line. See example code for practical illustration.
</p>
<p>Linear extrapolation is used for prediction that requires extrapolation 
(i.e. prediction outside the range of the interior <code>k-m[1]+1</code> knots &mdash; the interval over which the penalty is evaluated). Such extrapolation is not
allowed in basis construction, but is when predicting.
</p>
<p>It is possible to set a <code>deriv</code> flag in a smooth specification or smooth object, so that a model or prediction matrix produces the requested derivative of the spline, rather than evaluating it. 
</p>


<h3>Value</h3>

<p> An object of class <code>"Bspline.smooth"</code>. See <code><a href="#topic+smooth.construct">smooth.construct</a></code>, 
for the elements that this object will contain.
</p>


<h3>WARNING</h3>

<p><code>m[1]</code> directly controls the spline order here, which is intuitively sensible, but different to other bases.</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>. Extrapolation ideas joint with David Miller.</p>


<h3>References</h3>

<p>Wood, S.N. (2017) P-splines with derivative based penalties and tensor product smoothing of unevenly distributed data. Statistics and Computing. 27(4) 985-989 <a href="https://arxiv.org/abs/1605.02446">https://arxiv.org/abs/1605.02446</a> <a href="https://doi.org/10.1007/s11222-016-9666-x">doi:10.1007/s11222-016-9666-x</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+p.spline">p.spline</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(mgcv)
  set.seed(5)
  dat &lt;- gamSim(1,n=400,dist="normal",scale=2)
  bs &lt;- "bs"
  ## note the double penalty on the s(x2) term...
  b &lt;- gam(y~s(x0,bs=bs,m=c(4,2))+s(x1,bs=bs)+s(x2,k=15,bs=bs,m=c(4,3,0))+
           s(x3,bs=bs,m=c(1,0)),data=dat,method="REML")
  plot(b,pages=1)

  ## Extrapolation example, illustrating the importance of considering
  ## the penalty carefully if extrapolating...
  f3 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * (10 * x)^3 * 
              (1 - x)^10 ## test function
  n &lt;- 100;x &lt;- runif(n)
  y &lt;- f3(x) + rnorm(n)*2
  ## first a model with first order penalty over whole real line (red)
  b0 &lt;- gam(y~s(x,m=1,k=20),method="ML")
  ## now a model with first order penalty evaluated over (-.5,1.5) (black)
  op &lt;- options(warn=-1)
  b &lt;- gam(y~s(x,bs="bs",m=c(3,1),k=20),knots=list(x=c(-.5,0,1,1.5)),
           method="ML")
  options(op)
  ## and the equivalent with same penalty over data range only (blue)
  b1 &lt;- gam(y~s(x,bs="bs",m=c(3,1),k=20),method="ML")
  pd &lt;- data.frame(x=seq(-.7,1.7,length=200))
  fv &lt;- predict(b,pd,se=TRUE)
  ul &lt;- fv$fit + fv$se.fit*2; ll &lt;- fv$fit - fv$se.fit*2
  plot(x,y,xlim=c(-.7,1.7),ylim=range(c(y,ll,ul)),main=
  "Order 1 penalties: red tps; blue bs on (0,1); black bs on (-.5,1.5)")
  ## penalty defined on (-.5,1.5) gives plausible predictions and intervals
  ## over this range...
  lines(pd$x,fv$fit);lines(pd$x,ul,lty=2);lines(pd$x,ll,lty=2)
  fv &lt;- predict(b0,pd,se=TRUE)
  ul &lt;- fv$fit + fv$se.fit*2; ll &lt;- fv$fit - fv$se.fit*2
  ## penalty defined on whole real line gives constant width intervals away
  ## from data, as slope there must be zero, to avoid infinite penalty:
  lines(pd$x,fv$fit,col=2)
  lines(pd$x,ul,lty=2,col=2);lines(pd$x,ll,lty=2,col=2)
  fv &lt;- predict(b1,pd,se=TRUE)
  ul &lt;- fv$fit + fv$se.fit*2; ll &lt;- fv$fit - fv$se.fit*2
  ## penalty defined only over the data interval (0,1) gives wild and wide
  ## extrapolation since penalty has been `turned off' outside data range:
  lines(pd$x,fv$fit,col=4)
  lines(pd$x,ul,lty=2,col=4);lines(pd$x,ll,lty=2,col=4)

  ## construct smooth of x. Model matrix sm$X and penalty 
  ## matrix sm$S[[1]] will have many zero entries...
  x &lt;- seq(0,1,length=100)
  sm &lt;- smoothCon(s(x,bs="bs"),data.frame(x))[[1]]

  ## another example checking penalty numerically...
  m &lt;- c(4,2); k &lt;- 15; b &lt;- runif(k)
  sm &lt;- smoothCon(s(x,bs="bs",m=m,k=k),data.frame(x),
                  scale.penalty=FALSE)[[1]]
  sm$deriv &lt;- m[2]
  h0 &lt;- 1e-3; xk &lt;- sm$knots[(m[1]+1):(k+1)]
  Xp &lt;- PredictMat(sm,data.frame(x=seq(xk[1]+h0/2,max(xk)-h0/2,h0)))
  sum((Xp%*%b)^2*h0) ## numerical approximation to penalty
  b%*%sm$S[[1]]%*%b  ## `exact' version

  ## ...repeated with uneven knot spacing...
  m &lt;- c(4,2); k &lt;- 15; b &lt;- runif(k)
  ## produce the required 20 unevenly spaced knots...
  knots &lt;- data.frame(x=c(-.4,-.3,-.2,-.1,-.001,.05,.15,
        .21,.3,.32,.4,.6,.65,.75,.9,1.001,1.1,1.2,1.3,1.4))
  sm &lt;- smoothCon(s(x,bs="bs",m=m,k=k),data.frame(x),
        knots=knots,scale.penalty=FALSE)[[1]]
  sm$deriv &lt;- m[2]
  h0 &lt;- 1e-3; xk &lt;- sm$knots[(m[1]+1):(k+1)]
  Xp &lt;- PredictMat(sm,data.frame(x=seq(xk[1]+h0/2,max(xk)-h0/2,h0)))
  sum((Xp%*%b)^2*h0) ## numerical approximation to penalty
  b%*%sm$S[[1]]%*%b  ## `exact' version

</code></pre>

<hr>
<h2 id='smooth.construct.cr.smooth.spec'>Penalized Cubic regression splines in GAMs</h2><span id='topic+smooth.construct.cr.smooth.spec'></span><span id='topic+smooth.construct.cs.smooth.spec'></span><span id='topic+smooth.construct.cc.smooth.spec'></span><span id='topic+cubic.regression.spline'></span><span id='topic+cyclic.cubic.spline'></span>

<h3>Description</h3>

<p><code><a href="#topic+gam">gam</a></code> can use univariate penalized cubic regression spline smooths, specified via terms like
<code>s(x,bs="cr")</code>. <code>s(x,bs="cs")</code> specifies a penalized cubic regression spline which has had its penalty modified 
to shrink towards zero at high enough smoothing parameters (as the smoothing parameter goes to infinity a normal cubic spline tends to a 
straight line.) <code>s(x,bs="cc")</code> specifies a cyclic penalized cubic regression spline smooth.
</p>
<p>&lsquo;Cardinal&rsquo; spline bases are used: Wood (2017) sections 5.3.1 and 5.3.2 gives full details. These bases have 
very low setup costs. For a given basis dimension, <code>k</code>, they typically perform a little less well 
then thin plate regression splines, but a little better than p-splines. See <code><a href="#topic+te">te</a></code> to use these bases in tensor product smooths of several variables.
</p>
<p>Default <code>k</code> is 10. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cr.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'cs.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'cc.smooth.spec'
smooth.construct(object, data, knots)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.cr.smooth.spec_+3A_object">object</code></td>
<td>
<p>a smooth specification object, usually generated by a term <code>s(...,bs="cr",...)</code>,
<code>s(...,bs="cs",...)</code> or <code>s(...,bs="cc",...)</code></p>
</td></tr>
<tr><td><code id="smooth.construct.cr.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.cr.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>a list containing any knots supplied for basis setup &mdash; in same order and with same names as <code>data</code>. 
Can be <code>NULL</code>. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>The constructor is not normally called directly, but is rather used internally by <code><a href="#topic+gam">gam</a></code>. 
To use for basis setup it is recommended to use <code><a href="#topic+smooth.construct2">smooth.construct2</a></code>.  
</p>
<p>If they are not supplied then the knots  of the spline are placed evenly
throughout the covariate values to which the term refers:  For
example, if fitting 101 data with an 11 knot spline of <code>x</code> then
there would be a knot at every 10th (ordered)  <code>x</code> value. The
parameterization used represents the spline in terms of its
values at the knots. The values at neighbouring knots
are connected by sections of  cubic polynomial constrained to be 
continuous up to and including second derivative at the knots. The resulting curve
is a natural cubic  spline through the values at the knots (given two extra conditions specifying 
that the second derivative of the curve should be zero at the two end 
knots).
</p>
<p>The shrinkage version of the smooth, eigen-decomposes the wiggliness penalty matrix, and sets its 2 zero eigenvalues to small 
multiples of the smallest strictly positive eigenvalue. The penalty is then set to the matrix with eigenvectors corresponding 
to those of the original penalty, but eigenvalues set to the peturbed versions. This penalty matrix has full rank and shrinks 
the curve to zero at high enough smoothing parameters.
</p>
<p>Note that the cyclic smoother will wrap at the smallest and largest covariate values, unless knots are supplied. If only two 
knots are supplied then they are taken as the end points of the smoother (provided all the data lie between them), and the 
remaining knots are generated automatically.
</p>
<p>The cyclic smooth is not 
subject to the condition that second derivatives go to zero at the first and last knots.
</p>


<h3>Value</h3>

<p> An object of class <code>"cr.smooth"</code> <code>"cs.smooth"</code> or <code>"cyclic.smooth"</code>.
In addition to the usual elements of a smooth class documented under <code><a href="#topic+smooth.construct">smooth.construct</a></code>, 
this object will contain:
</p>
<table>
<tr><td><code>xp</code></td>
<td>
<p>giving the knot locations used to generate the basis.</p>
</td></tr>
<tr><td><code>F</code></td>
<td>
<p> For class <code>"cr.smooth"</code> and <code>"cs.smooth"</code> objects  <code>t(F)</code> transforms function values 
at the knots to second derivatives at the knots.</p>
</td></tr>
<tr><td><code>BD</code></td>
<td>
<p>class <code>"cyclic.smooth"</code> objects include matrix <code>BD</code> which transforms function values 
at the knots to second derivatives at the knots.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## cyclic spline example...
  require(mgcv)
  set.seed(6)
  x &lt;- sort(runif(200)*10)
  z &lt;- runif(200)
  f &lt;- sin(x*2*pi/10)+.5
  y &lt;- rpois(exp(f),exp(f)) 

## finished simulating data, now fit model...
  b &lt;- gam(y ~ s(x,bs="cc",k=12) + s(z),family=poisson,
                      knots=list(x=seq(0,10,length=12)))
## or more simply
   b &lt;- gam(y ~ s(x,bs="cc",k=12) + s(z),family=poisson,
                      knots=list(x=c(0,10)))

## plot results...
  par(mfrow=c(2,2))
  plot(x,y);plot(b,select=1,shade=TRUE);lines(x,f-mean(f),col=2)
  plot(b,select=2,shade=TRUE);plot(fitted(b),residuals(b))
  

</code></pre>

<hr>
<h2 id='smooth.construct.ds.smooth.spec'>Low rank Duchon 1977 splines</h2><span id='topic+smooth.construct.ds.smooth.spec'></span><span id='topic+Predict.matrix.duchon.spline'></span><span id='topic+Duchon.spline'></span>

<h3>Description</h3>

<p>Thin plate spline smoothers are a special case of the isotropic splines discussed in Duchon (1977). A subset of this more 
general class can be invoked by terms like <code>s(x,z,bs="ds",m=c(1,.5)</code> in a <code><a href="#topic+gam">gam</a></code> model formula.
In the notation of Duchon (1977) m is given by <code>m[1]</code> (default value 2), while s is given by <code>m[2]</code> (default value 0).
</p>
<p>Duchon's (1977) construction generalizes the usual thin plate spline penalty as follows. The usual TPS penalty is given by the integral 
of the squared Euclidian norm of a vector of mixed partial mth order derivatives of the function w.r.t. its arguments. Duchon re-expresses 
this penalty in the Fourier domain, and then weights the squared norm in the integral by the Euclidean norm of the fourier frequencies, 
raised to the power 2s. s is a user selected constant taking integer values divided by 2. If d is the number of arguments of the smooth, 
then it is required that -d/2 &lt; s &lt; d/2. To obtain continuous functions we further require that m + s &gt; d/2. If s=0 then the usual thin plate 
spline is recovered. 
</p>
<p>The construction is amenable to exactly the low rank approximation method given in Wood (2003) to thin plate splines, with similar 
optimality properties, so this approach to low rank smoothing is used here. For large datasets the same subsampling approach as is used in the 
<code><a href="#topic+tprs">tprs</a></code> case is employed here to reduce computational costs. 
</p>
<p>These smoothers allow the use of lower orders of derivative in the penalty than conventional thin plate splines, 
while still yielding continuous functions. For example, we can set m = 1 and s = d/2 - .5 in order to use first derivative penalization 
for any d (which has the advantage that the dimension of the null space of unpenalized functions is only d+1). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ds.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'duchon.spline'
Predict.matrix(object, data)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.ds.smooth.spec_+3A_object">object</code></td>
<td>
<p>a smooth specification object, usually generated by a term <code>s(...,bs="ds",...)</code>.</p>
</td></tr>
<tr><td><code id="smooth.construct.ds.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.ds.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>a list containing any knots supplied for basis setup &mdash; in same order and with same names as <code>data</code>. 
Can be <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>  The default basis dimension for this class is <code>k=M+k.def</code> where <code>M</code> is the null space dimension 
(dimension of unpenalized function space) and <code>k.def</code> is 10 for dimension 1, 30 for dimension 2 and 100 for higher dimensions. 
This is essentially arbitrary, and should be checked, but as with all penalized regression smoothers, results are statistically 
insensitive to the exact choise, provided it is not so small that it forces oversmoothing (the smoother's 
degrees of freedom are controlled primarily by its smoothing parameter).
</p>
<p>The constructor is not normally called directly, but is rather used internally by <code><a href="#topic+gam">gam</a></code>. 
To use for basis setup it is recommended to use <code><a href="#topic+smooth.construct2">smooth.construct2</a></code>.  
</p>
<p>For these classes the specification <code>object</code> will contain
information on how to handle large datasets in their <code>xt</code> field. The default is to randomly
subsample 2000 &lsquo;knots&rsquo; from which to produce a reduced rank eigen approximation to the full basis, 
if the number of unique predictor variable combinations in excess of 2000. The default can be
modified via the <code>xt</code> argument to <code><a href="#topic+s">s</a></code>. This is supplied as a
list with elements <code>max.knots</code> and <code>seed</code> containing a number
to use in place of 2000, and the random number seed to use (either can be
missing). Note that the random sampling will not effect the state of R's RNG. 
</p>
<p>For these bases <code>knots</code> has two uses. Firstly, as mentioned already, for large datasets 
the calculation of the <code>tp</code> basis can be time-consuming. The user can retain most of the advantages of the 
approach by supplying  a reduced set of covariate values from which to obtain the basis - 
typically the number of covariate values used will be substantially 
smaller than the number of data, and substantially larger than the basis dimension, <code>k</code>. This approach is 
the one taken automatically if the number of unique covariate values (combinations) exceeds <code>max.knots</code>.
The second possibility is to avoid the eigen-decomposition used to find the spline basis altogether and simply use 
the basis implied by the chosen knots: this will happen if the number of knots supplied matches the 
basis dimension, <code>k</code>. For a given basis dimension the second option is 
faster, but gives poorer results (and the user must be quite careful in choosing knot locations).
</p>


<h3>Value</h3>

<p> An object of class <code>"duchon.spline"</code>. In addition to the usual elements of a 
smooth class documented under <code><a href="#topic+smooth.construct">smooth.construct</a></code>, this object will contain:
</p>
<table>
<tr><td><code>shift</code></td>
<td>
<p>A record of the shift applied to each covariate in order to center it around zero and 
avoid any co-linearity problems that might otehrwise occur in the penalty null space basis of the term. </p>
</td></tr>
<tr><td><code>Xu</code></td>
<td>
<p>A matrix of the unique covariate combinations for this smooth (the basis is constructed by first stripping 
out duplicate locations).</p>
</td></tr>
<tr><td><code>UZ</code></td>
<td>
<p>The matrix mapping the smoother parameters back to the parameters of a full Duchon spline.</p>
</td></tr>
<tr><td><code>null.space.dimension</code></td>
<td>
<p>The dimension of the space of functions that have zero wiggliness according to the 
wiggliness penalty for this term.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Duchon, J. (1977) Splines minimizing rotation-invariant semi-norms in Solobev spaces. in W. Shemp and K. Zeller (eds) 
Construction theory of functions of several variables, 85-100, Springer, Berlin.
</p>
<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Spherical.Spline">Spherical.Spline</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
eg &lt;- gamSim(2,n=200,scale=.05)
attach(eg)
op &lt;- par(mfrow=c(2,2),mar=c(4,4,1,1))
b0 &lt;- gam(y~s(x,z,bs="ds",m=c(2,0),k=50),data=data)  ## tps
b &lt;- gam(y~s(x,z,bs="ds",m=c(1,.5),k=50),data=data)  ## first deriv penalty
b1 &lt;- gam(y~s(x,z,bs="ds",m=c(2,.5),k=50),data=data) ## modified 2nd deriv

persp(truth$x,truth$z,truth$f,theta=30) ## truth
vis.gam(b0,theta=30)
vis.gam(b,theta=30)
vis.gam(b1,theta=30)

detach(eg)

</code></pre>

<hr>
<h2 id='smooth.construct.fs.smooth.spec'>Factor smooth interactions in GAMs</h2><span id='topic+smooth.construct.fs.smooth.spec'></span><span id='topic+Predict.matrix.fs.interaction'></span>

<h3>Description</h3>

<p>Simple factor smooth interactions, which are efficient when used with <code><a href="#topic+gamm">gamm</a></code>.
This smooth class allows a separate smooth for each level of a factor, with the same smoothing parameter for all 
smooths. It is an alternative to using factor <code>by</code> variables.
</p>
<p>See <code><a href="#topic+factor.smooth">factor.smooth</a></code> for more genral alternatives for factor smooth interactions (including interactions of tensor product smooths with factors).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fs.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'fs.interaction'
Predict.matrix(object, data)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.fs.smooth.spec_+3A_object">object</code></td>
<td>
<p>For the <code>smooth.construct</code> method a smooth specification object, 
usually generated by a term <code>s(x,...,bs="fs",)</code>. May have a <code>gamm</code> attribute: see details. For the <code>predict.Matrix</code> method 
an object of class <code>"fs.interaction"</code> produced by the <code>smooth.construct</code> method.</p>
</td></tr>
<tr><td><code id="smooth.construct.fs.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code>.</p>
</td></tr> 
<tr><td><code id="smooth.construct.fs.smooth.spec_+3A_knots">knots</code></td>
<td>
<p> a list containing any knots supplied for smooth basis setup.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This class produces a smooth for each level of a single factor variable. Within a <code><a href="#topic+gam">gam</a></code> 
formula this is done with something like <code>s(x,fac,bs="fs")</code>, which is almost equivalent to <code>s(x,by=fac,id=1)</code> 
(with the <code>gam</code> argument <code>select=TRUE</code>). The terms are fully penalized, with separate penalties on each null 
space component: for this reason they are not centred (no sum-to-zero constraint).
</p>
<p>The class is particularly useful for use with <code><a href="#topic+gamm">gamm</a></code>, where estimation efficiently exploits 
the nesting of the smooth within the factor. Note however that: i) <code>gamm</code> only allows one conditioning 
factor for smooths, so <code>s(x)+s(z,fac,bs="fs")+s(v,fac,bs="fs")</code> is OK, but <code>s(x)+s(z,fac1,bs="fs")+s(v,fac2,bs="fs")</code>
is not; ii) all aditional random effects and correlation structures will be treated as nested within the factor
of the smooth factor interaction. To facilitate this the constructor is called from <code><a href="#topic+gamm">gamm</a></code> with an attribute
<code>"gamm"</code> attached to the smooth specification object. The result differs from that resulting from the case where this is
not done. 
</p>
<p>Note that <code>gamm4</code> from the <code>gamm4</code> package suffers from none of the restrictions that apply to <code>gamm</code>, and
<code>"fs"</code> terms can be used without side-effects. Constructor is still called with a smooth specification object having a
<code>"gamm"</code> attribute.
</p>
<p>Any singly penalized basis can be used to smooth at each factor level. The default is <code>"tp"</code>, but alternatives can 
be supplied in the <code>xt</code> argument of <code>s</code> (e.g. <code>s(x,fac,bs="fs",xt="cr")</code> or 
<code>s(x,fac,bs="fs",xt=list(bs="cr")</code>). The <code>k</code> argument to <code>s(...,bs="fs")</code> refers to the basis dimension to 
use for each level of the factor variable.
</p>
<p>Note one computational bottleneck: currently <code><a href="#topic+gamm">gamm</a></code> (or <code>gamm4</code>) will produce the full posterior covariance matrix for the 
smooths, including the smooths at each level of the factor. This matrix can get large and computationally costly if there 
are more than a few hundred levels of the factor. Even at one or two hundred levels, care should be taken to keep 
down <code>k</code>. 
</p>
<p>The plot method for this class has two schemes. <code>scheme==0</code> is in colour, while <code>scheme==1</code> is black and white.
</p>


<h3>Value</h3>

<p> An object of class <code>"fs.interaction"</code> or a matrix mapping the coefficients of the factor smooth interaction to the smooths themselves. The contents of an <code>"fs.interaction"</code> object will depend on whether or not <code>smooth.construct</code> was called with an object with attribute <code>gamm</code>: see below.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> with input from Matteo Fasiolo.</p>


<h3>See Also</h3>

<p><code><a href="#topic+factor.smooth">factor.smooth</a></code>, <code><a href="#topic+gamm">gamm</a></code>, <code><a href="#topic+smooth.construct.sz.smooth.spec">smooth.construct.sz.smooth.spec</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(0)
## simulate data...
f0 &lt;- function(x) 2 * sin(pi * x)
f1 &lt;- function(x,a=2,b=-1) exp(a * x)+b
f2 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
            (10 * x)^3 * (1 - x)^10
n &lt;- 500;nf &lt;- 25
fac &lt;- sample(1:nf,n,replace=TRUE)
x0 &lt;- runif(n);x1 &lt;- runif(n);x2 &lt;- runif(n)
a &lt;- rnorm(nf)*.2 + 2;b &lt;- rnorm(nf)*.5
f &lt;- f0(x0) + f1(x1,a[fac],b[fac]) + f2(x2)
fac &lt;- factor(fac)
y &lt;- f + rnorm(n)*2
## so response depends on global smooths of x0 and 
## x2, and a smooth of x1 for each level of fac.

## fit model...
bm &lt;- gamm(y~s(x0)+ s(x1,fac,bs="fs",k=5)+s(x2,k=20))
plot(bm$gam,pages=1)
summary(bm$gam)

## Also efficient using bam(..., discrete=TRUE)
bd &lt;- bam(y~s(x0)+ s(x1,fac,bs="fs",k=5)+s(x2,k=20),discrete=TRUE)
plot(bd,pages=1)
summary(bd)

## Could also use...
## b &lt;- gam(y~s(x0)+ s(x1,fac,bs="fs",k=5)+s(x2,k=20),method="ML")
## ... but its slower (increasingly so with increasing nf)
## b &lt;- gam(y~s(x0)+ t2(x1,fac,bs=c("tp","re"),k=5,full=TRUE)+
##        s(x2,k=20),method="ML"))
## ... is exactly equivalent. 
</code></pre>

<hr>
<h2 id='smooth.construct.gp.smooth.spec'>Low rank Gaussian process smooths</h2><span id='topic+smooth.construct.gp.smooth.spec'></span><span id='topic+Predict.matrix.gp.smooth'></span><span id='topic+gp.smooth'></span>

<h3>Description</h3>

<p>Gaussian process/kriging models based on simple covariance functions can be written in a very similar form to thin plate and Duchon spline models (e.g. Handcock, Meier, Nychka, 1994), and low rank versions produced by the eigen approximation method of Wood (2003). Kammann and Wand (2003) suggest a particularly simple form of the Matern covariance function with only a single smoothing parameter to estimate, and this class implements this and other similar models.  
</p>
<p>Usually invoked by an <code>s(...,bs="gp")</code> term in a <code>gam</code> formula. Argument <code>m</code> selects the covariance function, sets the range parameter and any power parameter. If <code>m</code> is not supplied then it defaults to <code>NA</code> and the covariance function suggested by Kammann and Wand (2003) along with their suggested range parameter is used. Otherwise <code>abs(m[1])</code> between 1 and 5 selects the correlation function from respectively, spherical, power exponential, and Matern with kappa = 1.5, 2.5 or 3.5. The sign of <code>m[1]</code> determines whether a linear trend in the covariates is added to the Guassian process (positive), or not (negative). The latter ensures stationarity. 
<code>m[2]</code>, if present, specifies the range parameter, with non-positive or absent indicating that the Kammann and Wand estimate should be used. <code>m[3]</code> can be used to specify the power for the power exponential which otherwise defaults to 1. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gp.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'gp.smooth'
Predict.matrix(object, data)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.gp.smooth.spec_+3A_object">object</code></td>
<td>
<p>a smooth specification object, usually generated by a term <code>s(...,bs="ms",...)</code>.</p>
</td></tr>
<tr><td><code id="smooth.construct.gp.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.gp.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>a list containing any knots supplied for basis setup &mdash; in same order and with same names as <code>data</code>. 
Can be <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p> Let <code class="reqn">\rho&gt;0</code> be the range parameter, <code class="reqn">0 &lt; \kappa\le 2 </code> and <code class="reqn">d</code> denote the distance between two points. Then the correlation functions indexed by <code>m[1]</code> are:
</p>

<ol>
<li> <p><code class="reqn">1 - 1.5 d/\rho + 0.5 (d/\rho)^3</code> if <code class="reqn">d \le \rho</code> and 0 otherwise.
</p>
</li>
<li> <p><code class="reqn">\exp(-(d/\rho)^\kappa)</code>.
</p>
</li>
<li> <p><code class="reqn">\exp(-d/\rho)(1+d/\rho)</code>.
</p>
</li>
<li> <p><code class="reqn">\exp(-d/\rho)(1+d/\rho + (d/\rho)^2/3)</code>.
</p>
</li>
<li> <p><code class="reqn">\exp(-d/\rho)(1+d/\rho+2(d/\rho)^2/5 + (d/\rho)^3/15)</code>.
</p>
</li></ol>

<p>See Fahrmeir et al. (2013) section 8.1.6, for example. Note that setting <code>r</code> to too small a value will lead to unpleasant results, as most points become all but independent (especially for the spherical model. Note: Wood 2017, Figure 5.20 right is based on a buggy implementation). 
</p>
<p>The default basis dimension for this class is <code>k=M+k.def</code> where <code>M</code> is the null space dimension (dimension of unpenalized function space) and <code>k.def</code> is 10 for dimension 1, 30 for dimension 2 and 100 for higher dimensions. 
This is essentially arbitrary, and should be checked, but as with all penalized regression smoothers, results are statistically 
insensitive to the exact choise, provided it is not so small that it forces oversmoothing (the smoother's 
degrees of freedom are controlled primarily by its smoothing parameter).
</p>
<p>The constructor is not normally called directly, but is rather used internally by <code><a href="#topic+gam">gam</a></code>. 
To use for basis setup it is recommended to use <code><a href="#topic+smooth.construct2">smooth.construct2</a></code>.  
</p>
<p>For these classes the specification <code>object</code> will contain
information on how to handle large datasets in their <code>xt</code> field. The default is to randomly
subsample 2000 &lsquo;knots&rsquo; from which to produce a reduced rank eigen approximation to the full basis, 
if the number of unique predictor variable combinations in excess of 2000. The default can be
modified via the <code>xt</code> argument to <code><a href="#topic+s">s</a></code>. This is supplied as a
list with elements <code>max.knots</code> and <code>seed</code> containing a number
to use in place of 2000, and the random number seed to use (either can be
missing). Note that the random sampling will not effect the state of R's RNG. 
</p>
<p>For these bases <code>knots</code> has two uses. Firstly, as mentioned already, for large datasets 
the calculation of the <code>tp</code> basis can be time-consuming. The user can retain most of the advantages of the 
approach by supplying  a reduced set of covariate values from which to obtain the basis - 
typically the number of covariate values used will be substantially 
smaller than the number of data, and substantially larger than the basis dimension, <code>k</code>. This approach is 
the one taken automatically if the number of unique covariate values (combinations) exceeds <code>max.knots</code>.
The second possibility is to avoid the eigen-decomposition used to find the spline basis altogether and simply use 
the basis implied by the chosen knots: this will happen if the number of knots supplied matches the 
basis dimension, <code>k</code>. For a given basis dimension the second option is 
faster, but gives poorer results (and the user must be quite careful in choosing knot locations).
</p>


<h3>Value</h3>

<p> An object of class <code>"gp.smooth"</code>. In addition to the usual elements of a 
smooth class documented under <code><a href="#topic+smooth.construct">smooth.construct</a></code>, this object will contain:
</p>
<table>
<tr><td><code>shift</code></td>
<td>
<p>A record of the shift applied to each covariate in order to center it around zero and 
avoid any co-linearity problems that might otherwise occur in the penalty null space basis of the term. </p>
</td></tr>
<tr><td><code>Xu</code></td>
<td>
<p>A matrix of the unique covariate combinations for this smooth (the basis is constructed by first stripping 
out duplicate locations).</p>
</td></tr>
<tr><td><code>UZ</code></td>
<td>
<p>The matrix mapping the smoother parameters back to the parameters of a full GP smooth.</p>
</td></tr>
<tr><td><code>null.space.dimension</code></td>
<td>
<p>The dimension of the space of functions that have zero wiggliness according to the wiggliness penalty for this term.</p>
</td></tr>
<tr><td><code>gp.defn</code></td>
<td>
<p>the type, range parameter and power parameter defining the correlation function. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Fahrmeir, L., T. Kneib, S. Lang and B. Marx (2013) Regression, Springer.
</p>
<p>Handcock, M. S., K. Meier and D. Nychka (1994) Journal of the American Statistical Association, 89: 401-403
</p>
<p>Kammann, E. E. and M.P. Wand (2003) Geoadditive Models. Applied Statistics 52(1):1-18.
</p>
<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
</p>
<p>Wood, S.N. (2017) Generalized Additive Models: an introduction with R (2nd ed). CRC/Taylor and Francis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tprs">tprs</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
eg &lt;- gamSim(2,n=200,scale=.05)
attach(eg)
op &lt;- par(mfrow=c(2,2),mar=c(4,4,1,1))
b0 &lt;- gam(y~s(x,z,k=50),data=data)  ## tps
b &lt;- gam(y~s(x,z,bs="gp",k=50),data=data)  ## Matern spline default range
b1 &lt;- gam(y~s(x,z,bs="gp",k=50,m=c(1,.5)),data=data)  ## spherical 

persp(truth$x,truth$z,truth$f,theta=30) ## truth
vis.gam(b0,theta=30)
vis.gam(b,theta=30)
vis.gam(b1,theta=30)

## compare non-stationary (b1) and stationary (b2)
b2 &lt;- gam(y~s(x,z,bs="gp",k=50,m=c(-1,.5)),data=data)  ## sph stationary 
vis.gam(b1,theta=30);vis.gam(b2,theta=30)
x &lt;- seq(-1,2,length=200); z &lt;- rep(.5,200)
pd &lt;- data.frame(x=x,z=z)
plot(x,predict(b1,pd),type="l");lines(x,predict(b2,pd),col=2)
abline(v=c(0,1))
plot(predict(b1),predict(b2))

detach(eg)

</code></pre>

<hr>
<h2 id='smooth.construct.mrf.smooth.spec'>Markov Random Field Smooths</h2><span id='topic+smooth.construct.mrf.smooth.spec'></span><span id='topic+Predict.matrix.mrf.smooth'></span><span id='topic+mrf'></span>

<h3>Description</h3>

<p>For data observed over discrete spatial units, a simple Markov random field 
smoother is sometimes appropriate. These functions provide such a smoother class for <code>mgcv</code>. 
See details for how to deal with regions with missing data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mrf.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'mrf.smooth'
Predict.matrix(object, data)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.mrf.smooth.spec_+3A_object">object</code></td>
<td>
<p>For the <code>smooth.construct</code> method a smooth specification object, 
usually generated by a term <code>s(x,...,bs="mrf",xt=list(polys=foo))</code>. <code>x</code> is a factor variable giving labels 
for geographic districts, and the <code>xt</code> argument 
is obligatory: see details. For the <code>Predict.Matrix</code> method 
an object of class <code>"mrf.smooth"</code> produced by the <code>smooth.construct</code> method.</p>
</td></tr>
<tr><td><code id="smooth.construct.mrf.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.mrf.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>If there are more geographic areas than data were observed for, then this argument is used to 
provide the labels for all the areas (observed and unobserved). </p>
</td></tr>
</table>


<h3>Details</h3>

<p>A Markov random field smooth over a set of discrete areas is defined using a set of area labels, and 
a neighbourhood structure for the areas. The covariate of the smooth is the vector of area labels 
corresponding to each obervation. This covariate should be a factor, or capable of being coerced to a factor.
</p>
<p>The neighbourhood structure is supplied in the <code>xt</code> argument to <code>s</code>. This must contain at least one of 
the elements <code>polys</code>, <code>nb</code> or <code>penalty</code>. 
</p>

<dl>
<dt>polys</dt><dd><p>contains the polygons defining the geographic areas. 
It is a list with as many elements as there are geographic areas. 
<code>names(polys)</code> must correspond to
the levels of the argument of the smooth, in any order (i.e. it gives the area labels). 
<code>polys[[i]]</code> is a 2 column matrix the rows of which specify the vertices of the polygon(s) 
defining the boundary of the ith area. A boundary may be made up of several closed loops: these must
be separated by <code>NA</code> rows. A polygon within another is treated as a hole. The first polygon in 
any <code>polys[[i]]</code>  should not be a hole.  An example 
of the structure is provided by <code><a href="#topic+columb.polys">columb.polys</a></code> (which contains an artificial hole
in its second element, for illustration). Any list elements with duplicate names are combined into a 
single NA separated matrix.
</p>
<p>Plotting of the smooth is not possible without a <code>polys</code> object.
</p>
<p>If <code>polys</code> is the only element of <code>xt</code> provided, then the neighbourhood structure is 
computed from it automatically. To count as neigbours, polygons must exactly share one of more 
vertices.
</p>
</dd>
<dt>nb</dt><dd><p>is a named list defining the neighbourhood structure. <code>names(nb)</code> must correspond to the 
levels of the covariate of the smooth (i.e. the area labels), but can be in any order. <code>nb[[i]]</code> 
is a numeric vector indexing the neighbours of the ith area (and should not include <code>i</code>).
All indices are relative to <code>nb</code> itself, but can be translated using <code>names(nb)</code>. See example code.
As an alternative each <code>nb[[i]]</code> can be an array of the names of the neighbours, but these will be
converted to the arrays of numeric indices internally. 
</p>
<p>If no <code>penalty</code> is provided then it is computed automatically from this list. The ith row of 
the penalty matrix will be zero everwhere, except in the ith column, which will contain the number 
of neighbours of the ith geographic area, and the columns corresponding to those geographic 
neighbours, which will each contain -1.  
</p>
</dd>
<dt>penalty</dt><dd><p> if this is supplied, then it is used as the penalty matrix. It should be positive semi-definite.
Its row and column names should correspond to the levels of the covariate.</p>
</dd>
</dl>

<p>If no basis dimension is supplied then the constructor produces a full rank MRF, with a coefficient for each 
geographic area. Otherwise a low rank approximation is obtained based on truncation of the parameterization given in
Wood (2017) Section 5.4.2. See Wood (2017, section 5.8.1). 
</p>
<p>Note that smooths of this class have a built in plot method, and that the utility function <code><a href="#topic+in.out">in.out</a></code> 
can be useful for working with discrete area data. The plot method has two schemes, <code>scheme==0</code> is colour, 
<code>scheme==1</code> is grey scale.
</p>
<p>The situation in which there are areas with no data requires special handling. You should set <code>drop.unused.levels=FALSE</code> in 
the model fitting function, <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+bam">bam</a></code> or <code><a href="#topic+gamm">gamm</a></code>, having first ensured that any fixed effect 
factors do not contain unobserved levels. Also make sure that the basis dimension is set to ensure that the total number of 
coefficients is less than the number of observations. 
</p>


<h3>Value</h3>

<p> An object of class <code>"mrf.smooth"</code> or a matrix mapping the coefficients of the MRF smooth 
to the predictions for the areas listed in <code>data</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> and Thomas Kneib
(Fabian Scheipl prototyped the low rank MRF idea) </p>


<h3>References</h3>

<p>Wood S.N. (2017) Generalized additive models: an introduction with R (2nd edition). CRC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+in.out">in.out</a></code>, <code><a href="#topic+polys.plot">polys.plot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## Load Columbus Ohio crime data (see ?columbus for details and credits)
data(columb)       ## data frame
data(columb.polys) ## district shapes list
xt &lt;- list(polys=columb.polys) ## neighbourhood structure info for MRF
par(mfrow=c(2,2))
## First a full rank MRF...
b &lt;- gam(crime ~ s(district,bs="mrf",xt=xt),data=columb,method="REML")
plot(b,scheme=1)
## Compare to reduced rank version...
b &lt;- gam(crime ~ s(district,bs="mrf",k=20,xt=xt),data=columb,method="REML")
plot(b,scheme=1)
## An important covariate added...
b &lt;- gam(crime ~ s(district,bs="mrf",k=20,xt=xt)+s(income),
         data=columb,method="REML")
plot(b,scheme=c(0,1))

## plot fitted values by district
par(mfrow=c(1,1))
fv &lt;- fitted(b)
names(fv) &lt;- as.character(columb$district)
polys.plot(columb.polys,fv)

## Examine an example neighbourhood list - this one auto-generated from
## 'polys' above.

nb &lt;- b$smooth[[1]]$xt$nb 
head(nb) 
names(nb) ## these have to match the factor levels of the smooth
## look at the indices of the neighbours of the first entry,
## named '0'...
nb[['0']] ## by name
nb[[1]]   ## same by index 
## ... and get the names of these neighbours from their indices...
names(nb)[nb[['0']]]
b1 &lt;- gam(crime ~ s(district,bs="mrf",k=20,xt=list(nb=nb))+s(income),
         data=columb,method="REML")
b1 ## fit unchanged
plot(b1) ## but now there is no information with which to plot the mrf
</code></pre>

<hr>
<h2 id='smooth.construct.ps.smooth.spec'>P-splines in GAMs</h2><span id='topic+smooth.construct.ps.smooth.spec'></span><span id='topic+smooth.construct.cp.smooth.spec'></span><span id='topic+p.spline'></span><span id='topic+cyclic.p.spline'></span>

<h3>Description</h3>

<p><code><a href="#topic+gam">gam</a></code> can use univariate P-splines as proposed by Eilers and Marx (1996), 
specified via terms like <code>s(x,bs="ps")</code>. These terms use B-spline bases 
penalized by discrete penalties applied directly to 
the basis coefficients. Cyclic P-splines are specified by model terms like <code>s(x,bs="cp",...)</code>. 
These bases can be used in tensor product smooths (see <code><a href="#topic+te">te</a></code>).
</p>
<p>The advantage of P-splines is the flexible way that penalty and basis order can be mixed (but see also <code><a href="#topic+d.spline">d.spline</a></code>). This often provides a useful way of &lsquo;taming&rsquo; an otherwise poorly behave smooth. However, in regular use, splines with derivative based penalties (e.g. <code>"tp"</code> or <code>"cr"</code> bases) tend to result in slightly better MSE performance, presumably because the good approximation theoretic properties of splines are rather closely connected to the use of derivative penalties. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ps.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'cp.smooth.spec'
smooth.construct(object, data, knots)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.ps.smooth.spec_+3A_object">object</code></td>
<td>
<p>a smooth specification object, usually generated by a term <code>s(x,bs="ps",...)</code> or 
<code>s(x,bs="cp",...)</code></p>
</td></tr>
<tr><td><code id="smooth.construct.ps.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.ps.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>a list containing any knots supplied for basis setup &mdash; in same order and with same names as <code>data</code>. 
Can be <code>NULL</code>. See details for further information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A  smooth term of the form <code>s(x,bs="ps",m=c(2,3))</code> specifies a 2nd order P-spline basis (cubic spline), 
with a third order difference penalty (0th order is a ridge penalty) on the coefficients. If <code>m</code> is a single number then it is taken as the basis order and penalty order. The default is the &lsquo;cubic spline like&rsquo; <code>m=c(2,2)</code>.
</p>
<p>The default basis dimension, <code>k</code>, is the larger of 10 and <code>m[1]+1</code> for a <code>"ps"</code> terms and the larger of 10 and <code>m[1]</code> for a <code>"cp"</code> term. <code>m[1]+1</code> and <code>m[1]</code> are the lower limits on basis dimension for the two types. 
</p>
<p>If knots are supplied, then the number of knots should be one more than the basis dimension 
(i.e. <code>k+1</code>) for a <code>"cp"</code>smooth. For the <code>"ps"</code> basis the number of supplied knots should be <code>k + m[1] + 2</code>, and the range of the middle <code>k-m[1]</code> knots should include all the covariate values. See example.
</p>
<p>Alternatively, for both types of smooth, 2 knots can be supplied, denoting the 
lower and upper limits between which the spline can be evaluated (Don't make this range too wide, however, or you 
can end up with no information about some basis coefficients, because the corresponding basis functions have a 
span that includes no data!). Note that P-splines don't make much sense with uneven knot spacing.
</p>
<p>Linear extrapolation is used for prediction that requires extrapolation 
(i.e. prediction outside the range of the interior <code>k-m[1]</code> knots). Such extrapolation is not
allowed in basis construction, but is when predicting.
</p>
<p>For the <code>"ps"</code> basis it is possible to set flags in the smooth specification object, requesting setup
according to the SCOP-spline monotonic smoother construction of Pya and Wood (2015). As yet this is not 
supported by any modelling functions in <code>mgcv</code> (see package <code>scam</code>). Similarly it is possible to set a 
<code>deriv</code> flag in a smooth specification or smooth object, so that a model or prediction matrix produces the
requested derivative of the spline, rather than evaluating it. See examples below. 
</p>


<h3>Value</h3>

<p> An object of class <code>"pspline.smooth"</code> or <code>"cp.smooth"</code>. See <code><a href="#topic+smooth.construct">smooth.construct</a></code>, 
for the elements that this object will contain.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Eilers, P.H.C. and B.D. Marx (1996) Flexible Smoothing with B-splines and Penalties. 
Statistical Science, 11(2):89-121
</p>
<p>Pya, N., and Wood, S.N. (2015). Shape constrained additive models. Statistics and Computing, 25(3), 543-559.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cSplineDes">cSplineDes</a></code>, <code><a href="#topic+adaptive.smooth">adaptive.smooth</a></code>, <code><a href="#topic+d.spline">d.spline</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## see ?gam
## cyclic example ...
  require(mgcv)
  set.seed(6)
  x &lt;- sort(runif(200)*10)
  z &lt;- runif(200)
  f &lt;- sin(x*2*pi/10)+.5
  y &lt;- rpois(exp(f),exp(f)) 

## finished simulating data, now fit model...
  b &lt;- gam(y ~ s(x,bs="cp") + s(z,bs="ps"),family=poisson)

## example with supplied knot ranges for x and z (can do just one)
  b &lt;- gam(y ~ s(x,bs="cp") + s(z,bs="ps"),family=poisson,
           knots=list(x=c(0,10),z=c(0,1))) 

## example with supplied knots...
  bk &lt;- gam(y ~ s(x,bs="cp",k=12) + s(z,bs="ps",k=13),family=poisson,
                      knots=list(x=seq(0,10,length=13),z=(-3):13/10))

## plot results...
  par(mfrow=c(2,2))
  plot(b,select=1,shade=TRUE);lines(x,f-mean(f),col=2)
  plot(b,select=2,shade=TRUE);lines(z,0*z,col=2)
  plot(bk,select=1,shade=TRUE);lines(x,f-mean(f),col=2)
  plot(bk,select=2,shade=TRUE);lines(z,0*z,col=2)
  
## Example using montonic constraints via the SCOP-spline
## construction, and of computng derivatives...
  x &lt;- seq(0,1,length=100); dat &lt;- data.frame(x)
  sspec &lt;- s(x,bs="ps")
  sspec$mono &lt;- 1
  sm &lt;- smoothCon(sspec,dat)[[1]]
  sm$deriv &lt;- 1
  Xd &lt;- PredictMat(sm,dat)
## generate random coeffients in the unconstrainted 
## parameterization...
  b &lt;- runif(10)*3-2.5
## exponentiate those parameters indicated by sm$g.index 
## to obtain coefficients meeting the constraints...
  b[sm$g.index] &lt;- exp(b[sm$g.index]) 
## plot monotonic spline and its derivative
  par(mfrow=c(2,2))
  plot(x,sm$X%*%b,type="l",ylab="f(x)")
  plot(x,Xd%*%b,type="l",ylab="f'(x)")
## repeat for decrease...
  sspec$mono &lt;- -1
  sm1 &lt;- smoothCon(sspec,dat)[[1]]
  sm1$deriv &lt;- 1
  Xd1 &lt;- PredictMat(sm1,dat)
  plot(x,sm1$X%*%b,type="l",ylab="f(x)")
  plot(x,Xd1%*%b,type="l",ylab="f'(x)")

## Now with sum to zero constraints as well...
  sspec$mono &lt;- 1
  sm &lt;- smoothCon(sspec,dat,absorb.cons=TRUE)[[1]]
  sm$deriv &lt;- 1
  Xd &lt;- PredictMat(sm,dat)
  b &lt;- b[-1] ## dropping first param
  plot(x,sm$X%*%b,type="l",ylab="f(x)")
  plot(x,Xd%*%b,type="l",ylab="f'(x)")
  
  sspec$mono &lt;- -1
  sm1 &lt;- smoothCon(sspec,dat,absorb.cons=TRUE)[[1]]
  sm1$deriv &lt;- 1
  Xd1 &lt;- PredictMat(sm1,dat)
  plot(x,sm1$X%*%b,type="l",ylab="f(x)")
  plot(x,Xd1%*%b,type="l",ylab="f'(x)")
  
</code></pre>

<hr>
<h2 id='smooth.construct.re.smooth.spec'>Simple random effects in GAMs</h2><span id='topic+smooth.construct.re.smooth.spec'></span><span id='topic+Predict.matrix.random.effect'></span>

<h3>Description</h3>

<p><code><a href="#topic+gam">gam</a></code> can deal with simple independent random effects, by exploiting the link 
between smooths and random effects to treat random effects as smooths. <code>s(x,bs="re")</code> implements 
this. Such terms can can have any number of predictors, which can be any mixture of numeric or factor 
variables. The terms produce a parametric interaction of the predictors, and penalize the corresponding 
coefficients with a multiple of the identity matrix, corresponding to an assumption of i.i.d. normality.
See details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 're.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'random.effect'
Predict.matrix(object, data)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.re.smooth.spec_+3A_object">object</code></td>
<td>
<p>For the <code>smooth.construct</code> method a smooth specification object, 
usually generated by a term <code>s(x,...,bs="re",)</code>. For the <code>predict.Matrix</code> method 
an object of class <code>"random.effect"</code> produced by the <code>smooth.construct</code> method.</p>
</td></tr>
<tr><td><code id="smooth.construct.re.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.re.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>generically a list containing any knots supplied for basis setup &mdash; unused at present.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Exactly how the random effects are implemented is best seen by example. Consider the model 
term <code>s(x,z,bs="re")</code>. This will result in the model matrix component corresponding to <code>~x:z-1</code> 
being added to the model matrix for the whole model. The coefficients associated with the model matrix 
component are assumed i.i.d. normal, with unknown variance (to be estimated). This assumption is 
equivalent to an identity penalty matrix (i.e. a ridge penalty) on the coefficients. Because such a 
penalty is full rank, random effects terms do not require centering constraints. 
</p>
<p>If the nature of the random effect specification is not clear, consider a couple more examples: 
<code>s(x,bs="re")</code> results in <code>model.matrix(~x-1)</code> being appended to the overall model matrix, 
while  <code>s(x,v,w,bs="re")</code> would result in  <code>model.matrix(~x:v:w-1)</code> being appended to the model 
matrix. In both cases the corresponding model coefficients are assumed i.i.d. normal, and are hence 
subject to ridge penalties.
</p>
<p>Some models require differences between the coefficients corresponding to different levels of the same random effect. See  <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code> for how to implement this.
</p>
<p>If the random effect precision matrix is of the form <code class="reqn">\sum_j \lambda_j S_j</code> for known matrices <code class="reqn">S_j</code> and unknown parameters <code class="reqn">\lambda_j</code>, then a list containing the <code class="reqn">S_j</code> can be supplied in the <code>xt</code> argument of 
<code><a href="#topic+s">s</a></code>. In this case an array <code>rank</code> should also be supplied in <code>xt</code> giving the ranks of the <code class="reqn">S_j</code> matrices. See simple example below. 
</p>
<p>Note that smooth <code>id</code>s are not supported for random effect terms. Unlike most smooth terms, side 
conditions are never applied to random effect terms in the event of nesting (since they are identifiable 
without side conditions).
</p>
<p>Random effects implemented in this way do not exploit the sparse structure of many random effects, and 
may therefore be relatively inefficient for models with large numbers of random effects, when <code>gamm4</code>
or <code><a href="#topic+gamm">gamm</a></code> may be better alternatives. Note also that <code><a href="#topic+gam">gam</a></code> will not support 
models with more coefficients than data. 
</p>
<p>The situation in which factor variable random effects intentionally have unobserved levels requires special handling. 
You should set <code>drop.unused.levels=FALSE</code> in the model fitting function, <code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+bam">bam</a></code> 
or <code><a href="#topic+gamm">gamm</a></code>, having first ensured that any fixed effect factors do not contain unobserved levels.
</p>
<p>The implementation is designed so that supplying random effect factor levels to <code><a href="#topic+predict.gam">predict.gam</a></code> that were not levels of
the factor when fitting, will result in the corresponding random effect (or interactions involving it) being set to zero (with zero standard error) for prediction. See <code><a href="#topic+random.effects">random.effects</a></code> for an example. This is achieved by the <code>Predict.matrix</code> method zeroing any rows of the prediction matrix involving factors that are <code>NA</code>. <code><a href="#topic+predict.gam">predict.gam</a></code> will set any factor observation to <code>NA</code> if it is a level not present in the fit data. 
</p>


<h3>Value</h3>

<p> An object of class <code>"random.effect"</code> or a matrix mapping the coefficients of the random effect to the random effects themselves.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2008) Fast stable direct fitting and smoothness
selection for generalized additive models. Journal of the Royal
Statistical Society (B) 70(3):495-518
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam.vcomp">gam.vcomp</a></code>, <code><a href="#topic+gamm">gamm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## see ?gam.vcomp

require(mgcv)
## simulate simple random effect example
set.seed(4)
nb &lt;- 50; n &lt;- 400
b &lt;- rnorm(nb)*2 ## random effect
r &lt;- sample(1:nb,n,replace=TRUE) ## r.e. levels
y &lt;- 2 + b[r] + rnorm(n)
r &lt;- factor(r)
## fit model....
b &lt;- gam(y ~ s(r,bs="re"),method="REML")
gam.vcomp(b)

## example with supplied precision matrices...
b &lt;- c(rnorm(nb/2)*2,rnorm(nb/2)*.5) ## random effect now with 2 variances
r &lt;- sample(1:nb,n,replace=TRUE) ## r.e. levels
y &lt;- 2 + b[r] + rnorm(n)
r &lt;- factor(r)
## known precision matrix components...
S &lt;- list(diag(rep(c(1,0),each=nb/2)),diag(rep(c(0,1),each=nb/2)))
b &lt;- gam(y ~ s(r,bs="re",xt=list(S=S,rank=c(nb/2,nb/2))),method="REML")
gam.vcomp(b)
summary(b)
</code></pre>

<hr>
<h2 id='smooth.construct.so.smooth.spec'>Soap film smoother constructer</h2><span id='topic+smooth.construct.so.smooth.spec'></span><span id='topic+smooth.construct.sf.smooth.spec'></span><span id='topic+smooth.construct.sw.smooth.spec'></span><span id='topic+soap'></span>

<h3>Description</h3>

<p> Sets up basis functions and wiggliness penalties for
soap film smoothers (Wood, Bravington and Hedley, 2008). Soap film smoothers are based on the idea of
constructing a 2-D smooth as a film of soap connecting a smoothly varying
closed boundary. Unless smoothing very heavily, the film is distorted towards
the data. The smooths are designed not to smooth across boundary features (peninsulas,
for example).
</p>
<p>The <code>so</code> version sets up the full smooth. The <code>sf</code> version sets up just the boundary interpolating 
soap film, while the <code>sw</code> version sets up the wiggly component of a soap film (zero on the boundary).
The latter two are useful for forming tensor products with soap films, and can be used with <code><a href="#topic+gamm">gamm</a></code> and <code>gamm4</code>. To use these to simply set up a basis, then call via the wrapper  <code><a href="#topic+smooth.construct2">smooth.construct2</a></code> or <code><a href="#topic+smoothCon">smoothCon</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'so.smooth.spec'
smooth.construct(object,data,knots)
## S3 method for class 'sf.smooth.spec'
smooth.construct(object,data,knots)
## S3 method for class 'sw.smooth.spec'
smooth.construct(object,data,knots)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smooth.construct.so.smooth.spec_+3A_object">object</code></td>
<td>
<p>A smooth specification object as produced by a <code>s(...,bs="so",xt=list(bnd=bnd,...))</code> 
term in a <code>gam</code> formula. Note that the <code>xt</code> argument to <code>s</code> *must* be supplied, and 
should be a list, containing at least a boundary specification list (see details). <code>xt</code> may 
also contain various options controlling the boundary smooth (see details), and PDE solution grid. The dimension of the bases for boundary loops is specified via the
<code>k</code> argument of <code>s</code>, either as a single number to be used for each boundary loop, or as a 
vector of different basis dimensions for the various boundary loops.  </p>
</td></tr>
<tr><td><code id="smooth.construct.so.smooth.spec_+3A_data">data</code></td>
<td>
<p>A list or data frame containing the arguments of the smooth.</p>
</td></tr>
<tr><td><code id="smooth.construct.so.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>list or data frame with two named columns specifying the knot locations within 
the boundary. The column names should match the names of the arguments of the smooth. The number 
of knots defines the *interior* basis dimension (i.e. it is *not* supplied via argument <code>k</code> of 
<code>s</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For soap film smooths the following *must* be supplied:
</p>

<dl>
<dt>k</dt><dd><p> the basis dimension for each boundary loop smooth.</p>
</dd>
<dt>xt$bnd</dt><dd><p> the boundary specification for the smooth.</p>
</dd>
<dt>knots</dt><dd><p> the locations of the interior knots for the smooth.</p>
</dd>
</dl>

<p>When used in a GAM then <code>k</code> and <code>xt</code> are supplied via <code>s</code> while <code>knots</code> are 
supplied in the <code>knots</code> argument of <code><a href="#topic+gam">gam</a></code>. 
</p>
<p>The <code>bnd</code> element of the <code>xt</code> list is a list of lists (or data frames), 
specifying the loops that define the boundary. Each boundary loop list must contain 
2 columns giving the co-ordinates of points defining a boundary loop (when joined 
sequentially by line segments). Loops should not intersect (not checked). A point is 
deemed to be in the region of interest if it is interior to an odd number of boundary 
loops. Each boundary loop list may also contain a column <code>f</code> giving known 
boundary conditions on a loop.
</p>
<p>The <code>bndSpec</code> element of <code>xt</code>, if non-NULL, should contain
</p>

<dl>
<dt>bs</dt><dd><p> the type of cyclic smoothing basis to use: one of <code>"cc"</code> and <code>"cp"</code>. 
If not <code>"cc"</code> then a cyclic p-spline is used, and argument 
<code>m</code> must be supplied.</p>
</dd>
<dt>knot.space</dt><dd><p> set to &quot;even&quot; to get even knot spacing with the &quot;cc&quot; basis.</p>
</dd>
<dt>m</dt><dd><p> 1 or 2 element array specifying order of &quot;cp&quot; basis and penalty.</p>
</dd> 
</dl>

<p>Currently the code will not deal with more than one level of nesting of loops, or with 
separate loops without an outer enclosing loop: if there are known boundary conditions 
(identifiability constraints get awkward).
</p>
<p>Note that the function <code><a href="graphics.html#topic+locator">locator</a></code> provides a simple means for defining boundaries
graphically, using something like <code>bnd &lt;-as.data.frame(locator(type="l"))</code>, 
after producing a plot of the domain of interest (right click to stop). If the real boundary is 
very complicated, it is probably better to use a simpler smooth boundary enclosing the true boundary, 
which represents the major boundary features that you don't want to smooth across, but doesn't follow 
every tiny detail.
</p>
<p>Model set up, and prediction, involves evaluating basis functions which are defined as the solution to PDEs. The 
PDEs are solved numerically on a grid using sparse matrix methods, with bilinear interpolation used to obtain 
values at any location within the smoothing domain. The dimension of the PDE solution grid can be controlled 
via element <code>nmax</code> (default 200) of the list supplied as argument <code>xt</code> of <code>s</code> in a <code>gam</code> formula: it gives the number of cells to use on the longest grid side. 
</p>
<p>A little theory: the soap film smooth <code class="reqn">f(x,y)</code> is defined as the solution of
</p>
<p style="text-align: center;"><code class="reqn">f_{xx} + f_{yy} = g</code>
</p>

<p>subject to the condition that <code class="reqn">f=s</code>, on the boundary curve, where
<code class="reqn">s</code> is a smooth function (usually a cyclic penalized regression
spline). The function <code class="reqn">g</code> is defined as the solution of 
</p>
<p style="text-align: center;"><code class="reqn">g_{xx}+g_{yy}=0</code>
</p>

<p>where <code class="reqn">g=0</code> on the boundary curve and
<code class="reqn">g(x_k,y_k)=c_k</code> at the &lsquo;knots&rsquo; of the surface; the
<code class="reqn">c_k</code> are model coefficients. 
</p>
<p>In the simplest case, estimation of the coefficients of <code class="reqn">f</code> (boundary
coefficients plus <code class="reqn">c_k</code>'s) is by minimization of 
</p>
<p style="text-align: center;"><code class="reqn">\|z-f\|^2 + \lambda_s J_s(s) + \lambda_f J_f(f)</code>
</p>

<p>where <code class="reqn">J_s</code> is usually some cubic spline type wiggliness penalty on
the boundary smooth and <code class="reqn">J_f</code> is the integral of
<code class="reqn">(f_xx+f_yy)^2</code> over the interior of the boundary. Both
penalties can be expressed as quadratic forms in the model coefficients. The
<code class="reqn">\lambda</code>'s are smoothing parameters, selectable by GCV, REML, AIC,
etc. <code class="reqn">z</code> represents noisy observations of <code class="reqn">f</code>. 
</p>


<h3>Value</h3>

<p> A list with all the elements of <code>object</code> plus
</p>
<table>
<tr><td><code>sd</code></td>
<td>
<p> A list defining the PDE solution grid and domain boundary, and including the sparse LU
factorization of the PDE coefficient matrix.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p> The model matrix: this will have an <code>"offset"</code> attribute, if there are 
any known boundary conditions.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p> List of smoothing penalty matrices (in smallest non-zero submatrix form).</p>
</td></tr>
<tr><td><code>irng</code></td>
<td>
<p> A vector of scaling factors that have been applied to the model matrix, 
to ensure nice conditioning.</p>
</td></tr>
</table>
<p>In addition there are all the elements usually added by <code>smooth.construct</code> methods. 
</p>


<h3>WARNINGS </h3>

<p>Soap film smooths are quite specialized, and require more setup than most smoothers (e.g. you have to supply the 
boundary and the interior knots, plus the boundary smooth basis dimension(s)). It is worth looking at the reference.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> </p>


<h3>References</h3>

<p>Wood, S.N., M.V. Bravington and S.L. Hedley (2008) &quot;Soap film smoothing&quot;, J.R.Statist.Soc.B 70(5), 931-955.
<a href="https://doi.org/10.1111/j.1467-9868.2008.00665.x">doi:10.1111/j.1467-9868.2008.00665.x</a>
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Predict.matrix.soap.film">Predict.matrix.soap.film</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(mgcv)

##########################
## simple test function...
##########################

fsb &lt;- list(fs.boundary())
nmax &lt;- 100
## create some internal knots...
knots &lt;- data.frame(v=rep(seq(-.5,3,by=.5),4),
                    w=rep(c(-.6,-.3,.3,.6),rep(8,4)))
## Simulate some fitting data, inside boundary...
set.seed(0)
n&lt;-600
v &lt;- runif(n)*5-1;w&lt;-runif(n)*2-1
y &lt;- fs.test(v,w,b=1)
names(fsb[[1]]) &lt;- c("v","w")
ind &lt;- inSide(fsb,x=v,y=w) ## remove outsiders
y &lt;- y + rnorm(n)*.3 ## add noise
y &lt;- y[ind];v &lt;- v[ind]; w &lt;- w[ind] 
n &lt;- length(y)

par(mfrow=c(3,2))
## plot boundary with knot and data locations
plot(fsb[[1]]$v,fsb[[1]]$w,type="l");points(knots,pch=20,col=2)
points(v,w,pch=".");

## Now fit the soap film smoother. 'k' is dimension of boundary smooth.
## boundary supplied in 'xt', and knots in 'knots'...
 
nmax &lt;- 100 ## reduced from default for speed.
b &lt;- gam(y~s(v,w,k=30,bs="so",xt=list(bnd=fsb,nmax=nmax)),knots=knots)

plot(b) ## default plot
plot(b,scheme=1)
plot(b,scheme=2)
plot(b,scheme=3)

vis.gam(b,plot.type="contour")

################################
# Fit same model in two parts...
################################

par(mfrow=c(2,2))
vis.gam(b,plot.type="contour")

b1 &lt;- gam(y~s(v,w,k=30,bs="sf",xt=list(bnd=fsb,nmax=nmax))+
            s(v,w,k=30,bs="sw",xt=list(bnd=fsb,nmax=nmax)) ,knots=knots)
vis.gam(b,plot.type="contour")
plot(b1)
 
##################################################
## Now an example with known boundary condition...
##################################################

## Evaluate known boundary condition at boundary nodes...
fsb[[1]]$f &lt;- fs.test(fsb[[1]]$v,fsb[[1]]$w,b=1,exclude=FALSE)

## Now fit the smooth...
bk &lt;- gam(y~s(v,w,bs="so",xt=list(bnd=fsb,nmax=nmax)),knots=knots)
plot(bk) ## default plot

##########################################
## tensor product example...
##########################################

set.seed(9)
n &lt;- 10000
v &lt;- runif(n)*5-1;w&lt;-runif(n)*2-1
t &lt;- runif(n)
y &lt;- fs.test(v,w,b=1)
y &lt;- y + 4.2
y &lt;- y^(.5+t)
fsb &lt;- list(fs.boundary())
names(fsb[[1]]) &lt;- c("v","w")
ind &lt;- inSide(fsb,x=v,y=w) ## remove outsiders
y &lt;- y[ind];v &lt;- v[ind]; w &lt;- w[ind]; t &lt;- t[ind] 
n &lt;- length(y)
y &lt;- y + rnorm(n)*.05 ## add noise
knots &lt;- data.frame(v=rep(seq(-.5,3,by=.5),4),
                    w=rep(c(-.6,-.3,.3,.6),rep(8,4)))

## notice NULL element in 'xt' list - to indicate no xt object for "cr" basis...
bk &lt;- gam(y~ te(v,w,t,bs=c("sf","cr"),k=c(25,4),d=c(2,1),
          xt=list(list(bnd=fsb,nmax=nmax),NULL))+
          te(v,w,t,bs=c("sw","cr"),k=c(25,4),d=c(2,1),
	  xt=list(list(bnd=fsb,nmax=nmax),NULL)),knots=knots)

par(mfrow=c(3,2))
m&lt;-100;n&lt;-50 
xm &lt;- seq(-1,3.5,length=m);yn&lt;-seq(-1,1,length=n)
xx &lt;- rep(xm,n);yy&lt;-rep(yn,rep(m,n))
tru &lt;- matrix(fs.test(xx,yy),m,n)+4.2 ## truth

image(xm,yn,tru^.5,col=heat.colors(100),xlab="v",ylab="w",
      main="truth")
lines(fsb[[1]]$v,fsb[[1]]$w,lwd=3)
contour(xm,yn,tru^.5,add=TRUE)

vis.gam(bk,view=c("v","w"),cond=list(t=0),plot.type="contour")

image(xm,yn,tru,col=heat.colors(100),xlab="v",ylab="w",
      main="truth")
lines(fsb[[1]]$v,fsb[[1]]$w,lwd=3)
contour(xm,yn,tru,add=TRUE)

vis.gam(bk,view=c("v","w"),cond=list(t=.5),plot.type="contour")

image(xm,yn,tru^1.5,col=heat.colors(100),xlab="v",ylab="w",
      main="truth")
lines(fsb[[1]]$v,fsb[[1]]$w,lwd=3)
contour(xm,yn,tru^1.5,add=TRUE)

vis.gam(bk,view=c("v","w"),cond=list(t=1),plot.type="contour")


#############################
# nested boundary example...
#############################

bnd &lt;- list(list(x=0,y=0),list(x=0,y=0))
seq(0,2*pi,length=100) -&gt; theta
bnd[[1]]$x &lt;- sin(theta);bnd[[1]]$y &lt;- cos(theta)
bnd[[2]]$x &lt;- .3 + .3*sin(theta);
bnd[[2]]$y &lt;- .3 + .3*cos(theta)
plot(bnd[[1]]$x,bnd[[1]]$y,type="l")
lines(bnd[[2]]$x,bnd[[2]]$y)

## setup knots
k &lt;- 8
xm &lt;- seq(-1,1,length=k);ym &lt;- seq(-1,1,length=k)
x=rep(xm,k);y=rep(ym,rep(k,k))
ind &lt;- inSide(bnd,x,y)
knots &lt;- data.frame(x=x[ind],y=y[ind])
points(knots$x,knots$y)

## a test function

f1 &lt;- function(x,y) {
  exp(-(x-.3)^2-(y-.3)^2)
}

## plot the test function within the domain 
par(mfrow=c(2,3))
m&lt;-100;n&lt;-100 
xm &lt;- seq(-1,1,length=m);yn&lt;-seq(-1,1,length=n)
x &lt;- rep(xm,n);y&lt;-rep(yn,rep(m,n))
ff &lt;- f1(x,y)
ind &lt;- inSide(bnd,x,y)
ff[!ind] &lt;- NA
image(xm,yn,matrix(ff,m,n),xlab="x",ylab="y")
contour(xm,yn,matrix(ff,m,n),add=TRUE)
lines(bnd[[1]]$x,bnd[[1]]$y,lwd=2);lines(bnd[[2]]$x,bnd[[2]]$y,lwd=2)

## Simulate data by noisy sampling from test function...

set.seed(1)
x &lt;- runif(300)*2-1;y &lt;- runif(300)*2-1
ind &lt;- inSide(bnd,x,y)
x &lt;- x[ind];y &lt;- y[ind]
n &lt;- length(x)
z &lt;- f1(x,y) + rnorm(n)*.1

## Fit a soap film smooth to the noisy data
nmax &lt;- 60
b &lt;- gam(z~s(x,y,k=c(30,15),bs="so",xt=list(bnd=bnd,nmax=nmax)),
         knots=knots,method="REML")
plot(b) ## default plot
vis.gam(b,plot.type="contour") ## prettier version

## trying out separated fits....
ba &lt;- gam(z~s(x,y,k=c(30,15),bs="sf",xt=list(bnd=bnd,nmax=nmax))+
          s(x,y,k=c(30,15),bs="sw",xt=list(bnd=bnd,nmax=nmax)),
	  knots=knots,method="REML")
plot(ba)
vis.gam(ba,plot.type="contour")
</code></pre>

<hr>
<h2 id='smooth.construct.sos.smooth.spec'>Splines on the sphere</h2><span id='topic+smooth.construct.sos.smooth.spec'></span><span id='topic+Predict.matrix.sos.smooth'></span><span id='topic+Spherical.Spline'></span>

<h3>Description</h3>

<p><code><a href="#topic+gam">gam</a></code> can use isotropic smooths on the sphere, via terms like 
<code>s(la,lo,bs="sos",m=2,k=100)</code>. There must be exactly 2 arguments to such a smooth. 
The first is taken to be latitude (in degrees) and the second longitude (in degrees). 
<code>m</code> (default 0) is an integer in the range -1 to 4 determining the order of the penalty used. 
For <code>m&gt;0</code>, <code>(m+2)/2</code> is the penalty order, with <code>m=2</code> equivalent to the usual second 
derivative penalty. <code>m=0</code> signals to use the 2nd order spline on the sphere, computed by 
Wendelberger's (1981) method. <code>m = -1</code> results in a <code><a href="#topic+Duchon.spline">Duchon.spline</a></code> being 
used (with m=2 and s=1/2), following an unpublished suggestion of Jean Duchon.
</p>
<p><code>k</code> (default 50) is the basis dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sos.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'sos.smooth'
Predict.matrix(object, data)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.sos.smooth.spec_+3A_object">object</code></td>
<td>
<p>a smooth specification object, usually generated by a term <code>s(...,bs="sos",...)</code>.</p>
</td></tr>
<tr><td><code id="smooth.construct.sos.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.sos.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>a list containing any knots supplied for basis setup &mdash; in same order and with same names as <code>data</code>. 
Can be <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p> For <code>m&gt;0</code>, the smooths implemented here are based on the pseudosplines on the sphere of Wahba (1981) 
(there is a correction of table 1 in 1982, but the correction has a misprint in the definition of A &mdash; the A
given in the 1981 paper is correct). For <code>m=0</code> (default) then a second order spline on the sphere is used which is the 
analogue of a second order thin plate spline in 2D: the computation is based on Chapter 4 of Wendelberger, 1981. 
Optimal low rank approximations are obtained using exactly the approach given in Wood (2003). For <code>m = -1</code> a smooth of the general type discussed in Duchon (1977) is used: the sphere is embedded in a 3D Euclidean space, but 
smoothing employs a penalty based on second derivatives (so that locally as the smoothing parameter tends 
to zero we recover a &quot;normal&quot; thin plate spline on the tangent space). This is an unpublished suggestion of Jean
Duchon.  <code>m = -2</code> is the same but with first derivative penalization.
</p>
<p>Note that the null space of the penalty is always the space of constant functions on the sphere, whatever the order of penalty. 
</p>
<p>This class has a plot method, with 3 schemes. <code>scheme==0</code> plots one hemisphere of the sphere, projected onto a circle.
The plotting sphere has the north pole at the top, and the 0 meridian running down the middle of the plot, and towards 
the viewer. The smoothing sphere is rotated within the plotting sphere, by specifying the location of its pole in the 
co-ordinates of the viewing sphere. <code>theta</code>, <code>phi</code> give the longitude and latitude of the smoothing sphere pole
within the plotting sphere (in plotting sphere co-ordinates). (You can visualize the smoothing sphere as a globe, free to 
rotate within the fixed transparent plotting sphere.) The value of the smooth is shown by a heat map overlaid with a 
contour plot. lat, lon gridlines are also plotted. 
</p>
<p><code>scheme==1</code> is as <code>scheme==0</code>, but in black and white, without the image plot. <code>scheme&gt;1</code> calls the default 
plotting method with <code>scheme</code> decremented by 2.
</p>


<h3>Value</h3>

<p> An object of class <code>"sos.smooth"</code>. In addition to the usual elements of a 
smooth class documented under <code><a href="#topic+smooth.construct">smooth.construct</a></code>, this object will contain:
</p>
<table>
<tr><td><code>Xu</code></td>
<td>
<p>A matrix of the unique covariate combinations for this smooth (the basis is constructed by first stripping 
out duplicate locations).</p>
</td></tr>
<tr><td><code>UZ</code></td>
<td>
<p>The matrix mapping the parameters of the reduced rank spline back to the parameters of a full spline.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>, 
with help from Grace Wahba (m=0 case) and Jean Duchon (m = -1 case).</p>


<h3>References</h3>

<p>Wahba, G. (1981) Spline interpolation and smoothing on the sphere. SIAM J. Sci. Stat. Comput. 2(1):5-16
</p>
<p>Wahba, G. (1982) Erratum. SIAM J. Sci. Stat. Comput. 3(3):385-386.
</p>
<p>Wendelberger, J. (1981) PhD Thesis, University of Winsconsin.
</p>
<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Duchon.spline">Duchon.spline</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
set.seed(0)
n &lt;- 400

f &lt;- function(la,lo) { ## a test function...
  sin(lo)*cos(la-.3)
}

## generate with uniform density on sphere...  
lo &lt;- runif(n)*2*pi-pi ## longitude
la &lt;- runif(3*n)*pi-pi/2
ind &lt;- runif(3*n)&lt;=cos(la)
la &lt;- la[ind];
la &lt;- la[1:n]

ff &lt;- f(la,lo)
y &lt;- ff + rnorm(n)*.2 ## test data

## generate data for plotting truth...
lam &lt;- seq(-pi/2,pi/2,length=30)
lom &lt;- seq(-pi,pi,length=60)
gr &lt;- expand.grid(la=lam,lo=lom)
fz &lt;- f(gr$la,gr$lo)
zm &lt;- matrix(fz,30,60)

require(mgcv)
dat &lt;- data.frame(la = la *180/pi,lo = lo *180/pi,y=y)

## fit spline on sphere model...
bp &lt;- gam(y~s(la,lo,bs="sos",k=60),data=dat)

## pure knot based alternative...
ind &lt;- sample(1:n,100)
bk &lt;- gam(y~s(la,lo,bs="sos",k=60),
      knots=list(la=dat$la[ind],lo=dat$lo[ind]),data=dat)

b &lt;- bp

cor(fitted(b),ff)

## plot results and truth...

pd &lt;- data.frame(la=gr$la*180/pi,lo=gr$lo*180/pi)
fv &lt;- matrix(predict(b,pd),30,60)

par(mfrow=c(2,2),mar=c(4,4,1,1))
contour(lom,lam,t(zm))
contour(lom,lam,t(fv))
plot(bp,rug=FALSE)
plot(bp,scheme=1,theta=-30,phi=20,pch=19,cex=.5)

</code></pre>

<hr>
<h2 id='smooth.construct.sz.smooth.spec'>Constrained factor smooth interactions in GAMs</h2><span id='topic+smooth.construct.sz.smooth.spec'></span><span id='topic+Predict.matrix.sz.interaction'></span>

<h3>Description</h3>

<p>Factor smooth interactions constructed to exclude main effects (and lower order factor smooth interactions). A smooth is constucted for each combination of the supplied factor levels. By appropriate application of sum to zero contrasts to equivalent smooth coefficients across factor levels, the required exclusion of lower order effects is achieved.   
</p>
<p>See <a href="#topic+factor.smooth">factor.smooth</a> for alternative factor smooth interactions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sz.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'sz.interaction'
Predict.matrix(object, data)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.sz.smooth.spec_+3A_object">object</code></td>
<td>
<p>For the <code>smooth.construct</code> method a smooth specification object, 
usually generated by a term <code>s(x,...,bs="sz",)</code>. For the <code>predict.Matrix</code> method 
an object of class <code>"sz.interaction"</code> produced by the <code>smooth.construct</code> method.</p>
</td></tr>
<tr><td><code id="smooth.construct.sz.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code>.</p>
</td></tr> 
<tr><td><code id="smooth.construct.sz.smooth.spec_+3A_knots">knots</code></td>
<td>
<p> a list containing any knots supplied for smooth basis setup.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This class produces a smooth for each combination of the levels of the supplied factor variables. <code>s(fac,x,bs="sz")</code> produces a smooth of <code>x</code> for each level of <code>fac</code>, for example. The smooths are constrained to represent deviations from the main effect smooth, so that models such as
</p>
<p style="text-align: center;"><code class="reqn">g(\mu_i) = f(x_i) + f_{k(i)}(x_i)</code>
</p>

<p>can be estimated in an identifiable manner, where <code class="reqn">k(i)</code> indicates the level of some factor that applies for the ith observation. Identifiability in this case is ensured by constraining the coefficients of the splines representing the <code class="reqn">f_{k}</code>. In particular if <code class="reqn">\beta_{ki}</code> is the ith coefficient of <code class="reqn">f_k</code> then the constraints are <code class="reqn">\sum_k \beta_{ki} = 0</code>.
</p>
<p>Such sum to zero constraints are implemented using sum to zero contrasts: identity matrices with an extra row of -1s appended. Consider the case of a single factor first. The model matrix corresponding to a smooth per factor level is the row tensor product (see <code><a href="#topic+tensor.prod.model.matrix">tensor.prod.model.matrix</a></code>) of the model matrix for the factor, and the model matrix for the smooth. The contrast matrix is then the Kronecker product of the sum to zero contrast for the factor, and an identity matrix of dimension determined by the number of coefficients of the smooth.
</p>
<p>If there are multiple factors then the overall model matrix is the row Kronecker product of all the factor model matrices and the smooth, while the contrast is the Kronecker product of all the sum-to-zero contrasts for the factors and a final identity matrix. Notice that this construction means that the main effects (and any interactions) of the factors are included in the factor level dependent smooths. In other words the individual smooths are not each centered. This means that adding main effects or interactions of the factors will lead to a rank deficient model. 
</p>
<p>The terms can have a smoothing parameter per smooth, or a single smoothing parameter for all the smooths. The latter is specified by giving the smooth term an <code>id</code>. e.g. <code>s(fac,x,bs="sz",id=1)</code>.
</p>
<p>The basis for the smooths can be selected by supplying a list as the <code>xt</code> argument to <code><a href="#topic+s">s</a></code>, with a <code>bs</code> item. e.g. <code>s(fac,x,xt=list(bs="cr"))</code> selectes the <code>"cr"</code> basis. The default is <code>"tp"</code>
</p>
<p>The plot method for this class has two schemes. <code>scheme==0</code> is in colour, while <code>scheme==1</code> is black and white. Currently it only works for 1D smooths.
</p>


<h3>Value</h3>

<p> An object of class <code>"sz.interaction"</code> or a matrix mapping the coefficients of the factor smooth interaction to the smooths themselves.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> with input from Matteo Fasiolo.</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam.models">gam.models</a></code>, <code><a href="#topic+gamm">gamm</a></code>, <a href="#topic+factor.smooth">factor.smooth</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(0)
dat &lt;- gamSim(4)

b &lt;- gam(y ~ s(x2)+s(fac,x2,bs="sz")+s(x0),data=dat,method="REML")
plot(b,pages=1)
summary(b)

## Example involving 2 factors

f1 &lt;- function(x2) 2 * sin(pi * x2)
f2 &lt;- function(x2) exp(2 * x2) - 3.75887
f3 &lt;- function(x2) 0.2 * x2^11 * (10 * (1 - x2))^6 + 10 * (10 * x2)^3 * 
            (1 - x2)^10

n &lt;- 600
x &lt;- runif(n)
f1 &lt;- factor(sample(c("a","b","c"),n,replace=TRUE))
f2 &lt;- factor(sample(c("foo","bar"),n,replace=TRUE))

mu &lt;- f3(x)
for (i in 1:3) mu &lt;- mu + exp(2*(2-i)*x)*(f1==levels(f1)[i])
for (i in 1:2) mu &lt;- mu + 10*i*x*(1-x)*(f2==levels(f2)[i])
y &lt;- mu + rnorm(n)
dat &lt;- data.frame(y=y,x=x,f1=f1,f2=f2)
b &lt;- gam(y ~ s(x)+s(f1,x,bs="sz")+s(f2,x,bs="sz")+s(f1,f2,x,bs="sz",id=1),data=dat,method="REML")
plot(b,pages=1,scale=0)



</code></pre>

<hr>
<h2 id='smooth.construct.t2.smooth.spec'>Tensor product smoothing constructor</h2><span id='topic+smooth.construct.t2.smooth.spec'></span>

<h3>Description</h3>

<p>A special <code>smooth.construct</code> method function for creating tensor product smooths from any
combination of single penalty marginal smooths, using the construction of Wood, Scheipl and Faraway (2013). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 't2.smooth.spec'
smooth.construct(object, data, knots)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.t2.smooth.spec_+3A_object">object</code></td>
<td>
<p>a smooth specification object of class <code>t2.smooth.spec</code>, 
usually generated by a term like <code>t2(x,z)</code> in a <code><a href="#topic+gam">gam</a></code> model formula</p>
</td></tr>
<tr><td><code id="smooth.construct.t2.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.t2.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>a list containing any knots supplied for basis setup &mdash; in same order and with same names as <code>data</code>. 
Can be <code>NULL</code>. See details for further information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tensor product smooths are smooths of several variables which allow the degree of smoothing to be different with respect
to different variables. They are useful as smooth interaction terms, as they are invariant to linear rescaling of the covariates,
which means, for example, that they are insensitive to the measurement units of the different covariates. They are also useful 
whenever isotropic smoothing is inappropriate. See <code><a href="#topic+t2">t2</a></code>, <code><a href="#topic+te">te</a></code>, <code><a href="#topic+smooth.construct">smooth.construct</a></code> and 
<code><a href="#topic+smooth.terms">smooth.terms</a></code>. The construction employed here produces tensor smooths for which the smoothing penalties are non-overlapping portions of the identity matrix. This makes their estimation by mixed modelling software rather easy.
</p>


<h3>Value</h3>

<p> An object of class <code>"t2.smooth"</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N., F. Scheipl and J.J. Faraway (2013) Straightforward intermediate rank tensor product smoothing in mixed models.
Statistics and Computing 23: 341-360.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+t2">t2</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## see ?t2

</code></pre>

<hr>
<h2 id='smooth.construct.tensor.smooth.spec'>Tensor product smoothing constructor</h2><span id='topic+smooth.construct.tensor.smooth.spec'></span>

<h3>Description</h3>

<p>A special <code>smooth.construct</code> method function for creating tensor product smooths from any
combination of single penalty marginal smooths.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tensor.smooth.spec'
smooth.construct(object, data, knots)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.tensor.smooth.spec_+3A_object">object</code></td>
<td>
<p>a smooth specification object of class <code>tensor.smooth.spec</code>, 
usually generated by a term like <code>te(x,z)</code> in a <code><a href="#topic+gam">gam</a></code> model formula</p>
</td></tr>
<tr><td><code id="smooth.construct.tensor.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.tensor.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>a list containing any knots supplied for basis setup &mdash; in same order and with same names as <code>data</code>. 
Can be <code>NULL</code>. See details for further information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tensor product smooths are smooths of several variables which allow the degree of smoothing to be different with respect
to different variables. They are useful as smooth interaction terms, as they are invariant to linear rescaling of the covariates,
which means, for example, that they are insensitive to the measurement units of the different covariates. They are also useful 
whenever isotropic smoothing is inappropriate. See <code><a href="#topic+te">te</a></code>, <code><a href="#topic+smooth.construct">smooth.construct</a></code> and 
<code><a href="#topic+smooth.terms">smooth.terms</a></code>.
</p>


<h3>Value</h3>

<p> An object of class <code>"tensor.smooth"</code>. See <code><a href="#topic+smooth.construct">smooth.construct</a></code>, 
for the elements that this object will contain.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2006) Low rank scale invariant tensor product smooths for
generalized additive mixed models. Biometrics 62(4):1025-1036
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cSplineDes">cSplineDes</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## see ?gam

</code></pre>

<hr>
<h2 id='smooth.construct.tp.smooth.spec'>Penalized thin plate regression splines in GAMs</h2><span id='topic+smooth.construct.tp.smooth.spec'></span><span id='topic+smooth.construct.ts.smooth.spec'></span><span id='topic+tprs'></span>

<h3>Description</h3>

<p><code><a href="#topic+gam">gam</a></code> can use isotropic smooths of any number of variables, specified via terms like
<code>s(x,z,bs="tp",m=3)</code> (or just <code>s(x,z)</code> as this is the default basis). These terms are based on thin plate 
regression splines. <code>m</code> specifies the order of the derivatives in the thin plate spline penalty.
</p>
<p>If <code>m</code> is a vector of length 2 and the second element is zero, then the penalty null space of the smooth is not included in the smooth: this is useful if you need to test whether a smooth could be replaced by a linear term, or construct models with odd nesting structures.  
</p>
<p>Thin plate regression splines are constructed by starting with the
basis and penalty for a full thin plate spline and then truncating this basis in
an optimal manner, to obtain a low rank smoother. Details are given in
Wood (2003). One key advantage of the approach is that it avoids
the knot placement problems of conventional regression spline
modelling, but it also has the advantage that smooths of lower rank
are nested within smooths of higher rank, so that it is legitimate to
use conventional hypothesis testing methods to compare models based on
pure regression splines. Note that the basis truncation does not change the 
meaning of the thin plate spline penalty (it penalizes exactly what it 
would have penalized for a full thin plate spline). 
</p>
<p>The t.p.r.s. basis and penalties can become expensive to
calculate for large datasets. For this reason the default behaviour is to
randomly subsample <code>max.knots</code> unique data locations if there are more than <code>max.knots</code>
such, and to use the sub-sample for basis construction. The sampling is
always done with the same random seed to ensure repeatability (does not
reset R RNG). <code>max.knots</code> is 2000, by default. Both seed and
<code>max.knots</code> can be modified using the <code>xt</code> argument to <code>s</code>.
Alternatively the user can supply knots from which to construct a basis. 
</p>
<p>The <code>"ts"</code> smooths are t.p.r.s. with the penalty modified so that the term is shrunk to zero 
for high enough smoothing parameter, rather than being shrunk towards a function in the 
penalty null space (see details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tp.smooth.spec'
smooth.construct(object, data, knots)
## S3 method for class 'ts.smooth.spec'
smooth.construct(object, data, knots)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth.construct.tp.smooth.spec_+3A_object">object</code></td>
<td>
<p>a smooth specification object, usually generated by a term <code>s(...,bs="tp",...)</code> or 
<code>s(...,bs="ts",...)</code></p>
</td></tr>
<tr><td><code id="smooth.construct.tp.smooth.spec_+3A_data">data</code></td>
<td>
<p>a list containing just the data (including any <code>by</code> variable) required by this term, 
with names corresponding to <code>object$term</code> (and <code>object$by</code>). The <code>by</code> variable 
is the last element.</p>
</td></tr> 
<tr><td><code id="smooth.construct.tp.smooth.spec_+3A_knots">knots</code></td>
<td>
<p>a list containing any knots supplied for basis setup &mdash; in same order and with same names as <code>data</code>. 
Can be <code>NULL</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p> The default basis dimension for this class is <code>k=M+k.def</code> where <code>M</code> is the null space dimension 
(dimension of unpenalized function space) and <code>k.def</code> is 8 for dimension 1, 27 for dimension 2 and 100 for higher dimensions. 
This is essentially arbitrary, and should be checked, but as with all penalized regression smoothers, results are statistically 
insensitive to the exact choise, provided it is not so small that it forces oversmoothing (the smoother's 
degrees of freedom are controlled primarily by its smoothing parameter).
</p>
<p>The default is to set <code>m</code> (the order of derivative in the thin plate spline penalty) to the smallest value satisfying <code>2m &gt; d+1</code> where <code>d</code> if the number of covariates of the term: this yields &lsquo;visually smooth&rsquo; functions. In any case <code>2m&gt;d</code> must be satisfied. 
</p>
<p>The constructor is not normally called directly, but is rather used internally by <code><a href="#topic+gam">gam</a></code>. 
To use for basis setup it is recommended to use <code><a href="#topic+smooth.construct2">smooth.construct2</a></code>.  
</p>
<p>For these classes the specification <code>object</code> will contain
information on how to handle large datasets in their <code>xt</code> field. The default is to randomly
subsample 2000 &lsquo;knots&rsquo; from which to produce a tprs basis, if the number of
unique predictor variable combinations in excess of 2000. The default can be
modified via the <code>xt</code> argument to <code><a href="#topic+s">s</a></code>. This is supplied as a
list with elements <code>max.knots</code> and <code>seed</code> containing a number
to use in place of 2000, and the random number seed to use (either can be
missing). 
</p>
<p>For these bases <code>knots</code> has two uses. Firstly, as mentioned already, for large datasets 
the calculation of the <code>tp</code> basis can be time-consuming. The user can retain most of the advantages of the t.p.r.s. 
approach by supplying  a reduced set of covariate values from which to obtain the basis - 
typically the number of covariate values used will be substantially 
smaller than the number of data, and substantially larger than the basis dimension, <code>k</code>. This approach is 
the one taken automatically if the number of unique covariate values (combinations) exceeds <code>max.knots</code>.
The second possibility 
is to avoid the eigen-decomposition used to find the t.p.r.s. basis altogether and simply use 
the basis implied by the chosen knots: this will happen if the number of knots supplied matches the 
basis dimension, <code>k</code>. For a given basis dimension the second option is 
faster, but gives poorer results (and the user must be quite careful in choosing knot locations).
</p>
<p>The shrinkage version of the smooth, eigen-decomposes the wiggliness penalty matrix, and sets its zero eigenvalues to small 
multiples of the smallest strictly positive eigenvalue. The penalty is then set to the matrix with eigenvectors corresponding 
to those of the original penalty, but eigenvalues set to the peturbed versions. This penalty matrix has full rank and shrinks 
the curve to zero at high enough smoothing parameters.
</p>


<h3>Value</h3>

<p> An object of class <code>"tprs.smooth"</code> or <code>"ts.smooth"</code>. In addition to the usual elements of a 
smooth class documented under <code><a href="#topic+smooth.construct">smooth.construct</a></code>, this object will contain:
</p>
<table>
<tr><td><code>shift</code></td>
<td>
<p>A record of the shift applied to each covariate in order to center it around zero and 
avoid any co-linearity problems that might otehrwise occur in the penalty null space basis of the term. </p>
</td></tr>
<tr><td><code>Xu</code></td>
<td>
<p>A matrix of the unique covariate combinations for this smooth (the basis is constructed by first stripping 
out duplicate locations).</p>
</td></tr>
<tr><td><code>UZ</code></td>
<td>
<p>The matrix mapping the t.p.r.s. parameters back to the parameters of a full thin plate spline.</p>
</td></tr>
<tr><td><code>null.space.dimension</code></td>
<td>
<p>The dimension of the space of functions that have zero wiggliness according to the 
wiggliness penalty for this term.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv); n &lt;- 100; set.seed(2)
x &lt;- runif(n); y &lt;- x + x^2*.2 + rnorm(n) *.1

## is smooth significantly different from straight line?
summary(gam(y~s(x,m=c(2,0))+x,method="REML")) ## not quite

## is smooth significatly different from zero?
summary(gam(y~s(x),method="REML")) ## yes!

## Fool bam(...,discrete=TRUE) into (strange) nested
## model fit...
set.seed(2) ## simulate some data... 
dat &lt;- gamSim(1,n=400,dist="normal",scale=2)
dat$x1a &lt;- dat$x1 ## copy x1 so bam allows 2 copies of x1
## Following removes identifiability problem, by removing
## linear terms from second smooth, and then re-inserting
## the one that was not a duplicate (x2)...
b &lt;- bam(y~s(x0,x1)+s(x1a,x2,m=c(2,0))+x2,data=dat,discrete=TRUE)

## example of knot based tprs...
k &lt;- 10; m &lt;- 2
y &lt;- y[order(x)];x &lt;- x[order(x)]
b &lt;- gam(y~s(x,k=k,m=m),method="REML",
         knots=list(x=seq(0,1,length=k)))
X &lt;- model.matrix(b)
par(mfrow=c(1,2))
plot(x,X[,1],ylim=range(X),type="l")
for (i in 2:ncol(X)) lines(x,X[,i],col=i)

## compare with eigen based (default)
b1 &lt;- gam(y~s(x,k=k,m=m),method="REML")
X1 &lt;- model.matrix(b1)
plot(x,X1[,1],ylim=range(X1),type="l")
for (i in 2:ncol(X1)) lines(x,X1[,i],col=i)
## see ?gam
</code></pre>

<hr>
<h2 id='smooth.info'>Generic function to provide extra information about smooth specification</h2><span id='topic+smooth.info'></span>

<h3>Description</h3>

<p>Takes a smooth specification object and adds extra basis specific information to it before smooth constructor called. Default method returns supplied object unmodified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smooth.info(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smooth.info_+3A_object">object</code></td>
<td>
<p> is a smooth specification object</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>Sometimes it is necessary to know something about a smoother before it is constructed, beyond what is in the initial smooth specification object.
For example, some smooth terms could be set up as tensor product smooths and it is useful for <code><a href="#topic+bam">bam</a></code> to take advantage of this when discrete covariate methods are used. However, <code><a href="#topic+bam">bam</a></code> needs to know whether a smoother falls into this category before it is constructed in order to discretize its covariates marginally instead of jointly. Rather than <code><a href="#topic+bam">bam</a></code> having a hard coded list of such smooth classes it is preferable for the smooth specification object to report this themselves. <code>smooth.info</code> method functions are the means for achieving this. When interpreting a gam formula the <code>smooth.info</code> function is applied to each smooth specification object as soon as it is produced (in <code>interpret.gam0</code>). 
</p>


<h3>Value</h3>

<p> A smooth specification object, which may be modified in some way.</p>


<h3>Author(s)</h3>

<p>Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

 
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+bam">bam</a></code>,
<code><a href="#topic+smooth.construct">smooth.construct</a></code>, <code><a href="#topic+PredictMat">PredictMat</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># See smooth.construct examples
spec &lt;- s(a,bs="re")
class(spec)
spec$tensor.possible
spec &lt;- smooth.info(spec)
spec$tensor.possible
</code></pre>

<hr>
<h2 id='smooth.terms'>Smooth terms in GAM</h2><span id='topic+smooth.terms'></span><span id='topic+smooths'></span>

<h3>Description</h3>

<p>Smooth terms are specified in a <code><a href="#topic+gam">gam</a></code> formula using <code><a href="#topic+s">s</a></code>, <code><a href="#topic+te">te</a></code>, <code><a href="#topic+ti">ti</a></code> 
and <code><a href="#topic+t2">t2</a></code> terms. 
Various smooth classes are available, for different modelling tasks, and users can add smooth classes 
(see <code><a href="#topic+user.defined.smooth">user.defined.smooth</a></code>). What defines a smooth class is the basis used to represent 
the smooth function and quadratic penalty (or multiple  penalties) used to penalize 
the basis coefficients in order to control the degree of smoothness. Smooth classes are 
invoked directly by <code>s</code> terms, or as building blocks for tensor product smoothing 
via <code>te</code>, <code>ti</code> or <code>t2</code> terms (only smooth classes with single penalties can be used in tensor products). The smooths
built into the <code>mgcv</code> package are all based one way or another on low rank versions of splines. For the full rank 
versions see Wahba (1990).
</p>
<p>Note that smooths can be used rather flexibly in <code>gam</code> models. In particular the linear predictor of the GAM can 
depend on (a discrete approximation to) any linear functional of a smooth term, using <code>by</code> variables and the 
&lsquo;summation convention&rsquo; explained in <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code>.
</p>
<p>The single penalty built in smooth classes are summarized as follows
</p>

<dl>
<dt>Thin plate regression splines</dt><dd><p><code>bs="tp"</code>. These are low rank isotropic smoothers of any number of covariates. By isotropic is 
meant that rotation of the covariate co-ordinate system will not change the result of smoothing. By low rank is meant 
that they have far fewer coefficients than there are data to smooth. They are reduced rank versions of the thin plate splines and use the thin plate spline penalty. They are the default
smooth for <code>s</code> terms because there is a defined sense in which they are the optimal smoother of any given
basis dimension/rank (Wood, 2003). Thin plate regression splines do not have &lsquo;knots&rsquo; 
(at least not in any conventional sense): a truncated eigen-decomposition is used to achieve the rank reduction.  See <code><a href="#topic+tprs">tprs</a></code> for further details. 
</p>
<p><code>bs="ts"</code> is as <code>"tp"</code> but with a modification to the smoothing penalty, so that the null space is also penalized slightly and the 
whole term can therefore be shrunk to zero.</p>
</dd>
<dt>Duchon splines</dt><dd><p><code>bs="ds"</code>. These generalize thin plate splines. In particular, for any given number of covariates they 
allow lower orders of derivative in the penalty than thin plate splines (and hence a smaller null space).
See <code><a href="#topic+Duchon.spline">Duchon.spline</a></code> for further details. 
</p>
</dd>
<dt>Cubic regression splines</dt><dd><p><code>bs="cr"</code>. 
These have a cubic spline basis defined by a modest sized 
set of knots spread evenly through the 
covariate values. They are penalized by the conventional intergrated square second derivative cubic spline penalty. 
For details see <code><a href="#topic+cubic.regression.spline">cubic.regression.spline</a></code> and e.g. Wood (2017).
</p>
<p><code>bs="cs"</code> specifies a shrinkage version of <code>"cr"</code>.
</p>
<p><code>bs="cc"</code> specifies a cyclic cubic regression splines (see <a href="#topic+cyclic.cubic.spline">cyclic.cubic.spline</a>). i.e. a penalized cubic regression splines whose ends match, up to second 
derivative.</p>
</dd>
<dt>Splines on the sphere</dt><dd><p><code>bs="sos"</code>.
These are two dimensional splines on a sphere. Arguments are latitude and longitude, and they are 
the analogue of thin plate splines for the sphere. Useful for data sampled over a large portion of the globe, 
when isotropy is appropriate. See <code><a href="#topic+Spherical.Spline">Spherical.Spline</a></code> for details.</p>
</dd>
<dt>B-splines</dt><dd><p><code>bs="bs"</code>.
B-spline basis with integrated squared derivative penalties. The order of basis and penalty can be chosen separately, and several penalties of different orders can be applied. Somewhat like a derivative penalty version of P-splines. See <a href="#topic+b.spline">b.spline</a> for details.
</p>
</dd>
<dt>P-splines</dt><dd><p><code>bs="ps"</code>. 
These are P-splines as proposed by Eilers and Marx (1996). They combine a B-spline basis, with a discrete penalty
on the basis coefficients, and any sane combination of penalty and basis order is allowed. Although this penalty has no exact interpretation in terms of function shape, in the way that the derivative penalties do, P-splines perform almost as well as conventional splines in many standard applications, and can perform better in particular cases where it is advantageous to mix different orders of basis and penalty.
</p>
<p><code>bs="cp"</code> gives a cyclic version of a P-spline (see <a href="#topic+cyclic.p.spline">cyclic.p.spline</a>). </p>
</dd>
<dt>Random effects</dt><dd><p><code>bs="re"</code>. These are parametric terms penalized by a ridge penalty (i.e. the identity matrix). When such a smooth has multiple arguments 
then it represents the parametric interaction of these arguments, with the coefficients penalized by a ridge penalty. The ridge penalty is equivalent to an 
assumption that the coefficients are i.i.d. normal random effects. See <code><a href="#topic+smooth.construct.re.smooth.spec">smooth.construct.re.smooth.spec</a></code>.</p>
</dd>
<dt>Markov Random Fields</dt><dd><p><code>bs="mrf"</code>. These are popular when space is split up into discrete contiguous 
geographic units (districts of a town, for example). In this case a simple smoothing penalty is constructed
based on the neighbourhood structure of the geographic units. See <code><a href="#topic+mrf">mrf</a></code> for details and an example.</p>
</dd>
<dt>Gaussian process smooths</dt><dd><p><code>bs="gp"</code>. Gaussian process models with a variety of simple correlation functions can be represented as smooths. See <code><a href="#topic+gp.smooth">gp.smooth</a></code> for details.</p>
</dd>
<dt>Soap film smooths</dt><dd><p><code>bs="so"</code> (actually not single penaltied, but <code>bs="sw"</code> and <code>bs="sf"</code> allows splitting into single penalty components for use in tensor product smoothing). These are finite area smoothers designed to smooth within complicated geographical boundaries, where the boundary matters (e.g. you do not want to smooth across boundary features). See <code><a href="#topic+soap">soap</a></code> for details.</p>
</dd>
</dl>

<p>Broadly speaking the default penalized thin plate regression splines tend to give the best MSE performance, 
but they are slower to set up than the other bases. The knot based penalized cubic regression splines
(with derivative based penalties) usually come next in MSE performance, with the P-splines doing 
just a little worse. However the P-splines are useful in non-standard situations.
</p>
<p>All the preceding classes (and any user defined smooths with single penalties) may be used as marginal 
bases for tensor product smooths specified via <code><a href="#topic+te">te</a></code>, <code><a href="#topic+ti">ti</a></code> or <code><a href="#topic+t2">t2</a></code> terms. Tensor 
product smooths are smooth functions 
of several variables where the basis is built up from tensor products of bases for smooths of fewer (usually one) 
variable(s) (marginal bases). The multiple penalties for these smooths are produced automatically from the
penalties of the marginal smooths. Wood (2006) and Wood, Scheipl and Faraway (2012), give the general recipe for these constructions. 
</p>

<dl>
<dt>te</dt><dd><p><code>te</code> smooths have one penalty per marginal basis, each of which is interpretable in a similar way to the marginal penalty from which it is derived. See Wood (2006).</p>
</dd>
<dt>ti</dt><dd><p><code>ti</code> smooths exclude the basis functions associated with the &lsquo;main effects&rsquo; of the marginal smooths, plus interactions other than the highest order specified. These provide a stable an interpretable way of specifying models with main effects and interactions. For example if we are interested in linear predicto <code class="reqn">f_1(x)+f_2(z)+f_3(x,z)</code>, we might use model formula <code>y~s(x)+s(z)+ti(x,z)</code> or <code>y~ti(x)+ti(z)+ti(x,z)</code>. A similar construction involving <code>te</code> terms instead will be much less statsitically stable.</p>
</dd>   
<dt>t2</dt><dd><p><code>t2</code> uses an alternative tensor product construction that results in more penalties each having a simple non-overlapping structure allowing use with the  <code>gamm4</code> package. It is a natural generalization of the SS-ANOVA construction, but the penalties are a little harder to interpret. See Wood, Scheipl and Faraway (2012/13). </p>
</dd>  
</dl>

<p>Tensor product smooths often perform better than isotropic smooths when the covariates of a smooth are not naturally
on the same scale, so that their relative scaling is arbitrary. For example, if smoothing with repect to time and 
distance, an isotropic smoother will give very different results if the units are cm and minutes compared to if the units are metres and seconds: a tensor product smooth will  give the same answer in both cases (see <code><a href="#topic+te">te</a></code> for an example of this). Note that <code>te</code> terms are knot based, and the thin plate splines seem to offer no advantage over cubic or P-splines as  marginal bases.
</p>
<p>Some further specialist smoothers that are not suitable for use in tensor products are also available.
</p>

<dl>
<dt>Adaptive smoothers</dt><dd><p><code>bs="ad"</code>
Univariate and bivariate adaptive smooths are available (see <code><a href="#topic+adaptive.smooth">adaptive.smooth</a></code>). 
These are appropriate when the degree of smoothing should itself vary with the covariates to be smoothed, and the 
data contain sufficient information to be able to estimate the appropriate variation. Because this flexibility is 
achieved by splitting the penalty into several &lsquo;basis penalties&rsquo; these terms are not suitable as components of tensor 
product smooths, and are not supported by <code>gamm</code>.</p>
</dd>
<dt>Factor smooth interactions</dt><dd><p><code>bs="sz"</code>
Smooth factor interactions (see <a href="#topic+factor.smooth">factor.smooth</a>) are often produced using <code>by</code> variables (see <code><a href="#topic+gam.models">gam.models</a></code>), but it is often desirable to include smooths which represent the deviations from some main effect smooth that apply for each level of a factor (or combination of factors).
See <code><a href="#topic+smooth.construct.sz.smooth.spec">smooth.construct.sz.smooth.spec</a></code> for details.</p>
</dd>
<dt>Random factor smooth interactions</dt><dd><p><code>bs="fs"</code>
A special smoother class (see <code><a href="#topic+smooth.construct.fs.smooth.spec">smooth.construct.fs.smooth.spec</a></code>) is available for the case in which a smooth is required at each of a large number of factor levels (for example a smooth for each patient in a study), and each smooth should have the same smoothing parameter. The <code>"fs"</code> smoothers are set up to be efficient when used with <code><a href="#topic+gamm">gamm</a></code>, and have penalties on each null sapce component (i.e. they are fully &lsquo;random effects&rsquo;). 
</p>
</dd>
</dl>


<h3>Author(s)</h3>

<p>Simon Wood &lt;simon.wood@r-project.org&gt;
</p>


<h3>References</h3>

<p>Eilers, P.H.C. and B.D. Marx (1996) Flexible Smoothing with B-splines and Penalties. 
Statistical Science, 11(2):89-121
</p>
<p>Wahba (1990) Spline Models of Observational Data. SIAM 
</p>
<p>Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114 <a href="https://doi.org/10.1111/1467-9868.00374">doi:10.1111/1467-9868.00374</a>
</p>
<p>Wood, S.N. (2017, 2nd ed) <em>Generalized Additive Models: an introduction with R</em>, CRC <a href="https://doi.org/10.1201/9781315370279">doi:10.1201/9781315370279</a>
</p>
<p>Wood, S.N. (2006) Low rank scale invariant tensor product smooths for
generalized additive mixed models. Biometrics 62(4):1025-1036 <a href="https://doi.org/10.1111/j.1541-0420.2006.00574.x">doi:10.1111/j.1541-0420.2006.00574.x</a>
</p>
<p>Wood, S.N., M.V. Bravington and S.L. Hedley (2008) &quot;Soap film smoothing&quot;, J.R.Statist.Soc.B 70(5), 931-955.
<a href="https://doi.org/10.1111/j.1467-9868.2008.00665.x">doi:10.1111/j.1467-9868.2008.00665.x</a>
</p>
<p>Wood S.N., F. Scheipl and J.J. Faraway (2013) [online 2012] Straightforward intermediate rank tensor product smoothing
in mixed models. Statistics and Computing. 23(3):341-360 <a href="https://doi.org/10.1007/s11222-012-9314-z">doi:10.1007/s11222-012-9314-z</a>
</p>
<p>Wood, S.N. (2017) P-splines with derivative based penalties and tensor product smoothing of unevenly distributed data. Statistics and Computing. 27(4) 985-989 <a href="https://arxiv.org/abs/1605.02446">https://arxiv.org/abs/1605.02446</a> <a href="https://doi.org/10.1007/s11222-016-9666-x">doi:10.1007/s11222-016-9666-x</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+s">s</a></code>, <code><a href="#topic+te">te</a></code>, <code><a href="#topic+t2">t2</a></code>, <code><a href="#topic+tprs">tprs</a></code>, <code><a href="#topic+Duchon.spline">Duchon.spline</a></code>, 
<code><a href="#topic+cubic.regression.spline">cubic.regression.spline</a></code>, <code><a href="#topic+p.spline">p.spline</a></code>, <code><a href="#topic+d.spline">d.spline</a></code>, <code><a href="#topic+mrf">mrf</a></code>, <code><a href="#topic+soap">soap</a></code>, 
<code><a href="#topic+Spherical.Spline">Spherical.Spline</a></code>, <code><a href="#topic+adaptive.smooth">adaptive.smooth</a></code>, <code><a href="#topic+user.defined.smooth">user.defined.smooth</a></code>,
<code><a href="#topic+smooth.construct.re.smooth.spec">smooth.construct.re.smooth.spec</a></code>, <code><a href="#topic+smooth.construct.gp.smooth.spec">smooth.construct.gp.smooth.spec</a></code>, <code><a href="#topic+factor.smooth.interaction">factor.smooth.interaction</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## see examples for gam and gamm
</code></pre>

<hr>
<h2 id='smooth2random'>Convert a smooth to a form suitable for estimating as random effect</h2><span id='topic+smooth2random'></span>

<h3>Description</h3>

<p>A generic function for converting <code>mgcv</code> smooth objects to forms suitable for estimation as random effects by e.g. <code>lme</code>. Exported mostly for use by other package developers. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smooth2random(object,vnames,type=1)


</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="smooth2random_+3A_object">object</code></td>
<td>
<p>an <code>mgcv</code> smooth object.</p>
</td></tr>
<tr><td><code id="smooth2random_+3A_vnames">vnames</code></td>
<td>
<p>a vector of names to avoid as dummy variable names in the random effects form.</p>
</td></tr> 
<tr><td><code id="smooth2random_+3A_type">type</code></td>
<td>
<p><code>1</code> for <code>lme</code>, otherwise <code>lmer</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is a duality between smooths and random effects which means that smooths can be estimated using mixed modelling software. This function converts standard <code>mgcv</code> smooth objects to forms suitable for estimation by <code>lme</code>, for example. A service routine for <code><a href="#topic+gamm">gamm</a></code> exported for use by package developers. See examples for creating prediction matrices for new data, corresponding to the random and fixed effect matrices returned when <code>type=2</code>. 
</p>


<h3>Value</h3>

<p>A list.
</p>
<table>
<tr><td><code>rand</code></td>
<td>
<p> a list of random effects, including grouping factors, and 
a fixed effects matrix. Grouping factors, model matrix and model
matrix name attached as attributes, to each element. Alternatively, for <code>type=2</code>
a list of random effect model matrices, each corresponding to an i.i.d. Gaussian
random effect with a single variance component.</p>
</td></tr>
<tr><td><code>trans.D</code></td>
<td>
<p>A vector, trans.D, that transforms coefs, in order [rand1, rand2,... fix] back to original parameterization. If null, then taken as vector of ones. <code>b.original = trans.U %*% (trans.D*b.fit)</code>.</p>
</td></tr>
<tr><td><code>trans.U</code></td>
<td>
<p>A matrix, trans.U, that transforms coefs, in order [rand1, rand2,... fix] back to original parameterization. If null, then not needed. If null then taken as identity.</p>
</td></tr>
<tr><td><code>Xf</code></td>
<td>
<p>A matrix for the fixed effects, if any.</p>
</td></tr>
<tr><td><code>fixed</code></td>
<td>
<p><code>TRUE/FALSE</code>, indicating if term was unpenalized or not. If unpenalized then other stuff may not be returned (it's not a random effect).</p>
</td></tr>
<tr><td><code>rind</code></td>
<td>
<p>an index vector such that if br is the vector of 
random coefficients for the term, br[rind] is the coefs in 
order for this term. </p>
</td></tr>
<tr><td><code>pen.ind</code></td>
<td>
<p>index of which penalty penalizes each coefficient: 0 for unpenalized.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>.</p>


<h3>References</h3>

<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+gamm">gamm</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Simple type 1 'lme' style...
library(mgcv)
x &lt;- runif(30)
sm &lt;- smoothCon(s(x),data.frame(x=x))[[1]]
smooth2random(sm,"")

## Now type 2 'lme4' style...
z &lt;- runif(30)
dat &lt;- data.frame(x=x,z=z)
sm &lt;- smoothCon(t2(x,z),dat)[[1]]
re &lt;- smooth2random(sm,"",2)
str(re)

## For prediction after fitting we might transform parameters back to
## original parameterization using 'rind', 'trans.D' and 'trans.U',
## and call PredictMat(sm,newdata) to get the prediction matrix to
## multiply these transformed parameters by.
## Alternatively we could obtain fixed and random effect Prediction
## matrices corresponding to the results from smooth2random, which
## can be used with the fit parameters without transforming them.
## The following shows how...

s2rPred &lt;- function(sm,re,data) {
## Function to aid prediction from smooths represented as type==2
## random effects. re must be the result of smooth2random(sm,...,type=2).
  X &lt;- PredictMat(sm,data)   ## get prediction matrix for new data
  ## transform to r.e. parameterization
  if (!is.null(re$trans.U)) X &lt;- X%*%re$trans.U
  X &lt;- t(t(X)*re$trans.D)
  ## re-order columns according to random effect re-ordering...
  X[,re$rind] &lt;- X[,re$pen.ind!=0] 
  ## re-order penalization index in same way  
  pen.ind &lt;- re$pen.ind; pen.ind[re$rind] &lt;- pen.ind[pen.ind&gt;0]
  ## start return object...
  r &lt;- list(rand=list(),Xf=X[,which(re$pen.ind==0),drop=FALSE])
  for (i in 1:length(re$rand)) { ## loop over random effect matrices
    r$rand[[i]] &lt;- X[,which(pen.ind==i),drop=FALSE]
    attr(r$rand[[i]],"s.label") &lt;- attr(re$rand[[i]],"s.label")
  }
  names(r$rand) &lt;- names(re$rand)
  r
} ## s2rPred

## use function to obtain prediction random and fixed effect matrices
## for first 10 elements of 'dat'. Then confirm that these match the
## first 10 rows of the original model matrices, as they should...

r &lt;- s2rPred(sm,re,dat[1:10,])
range(r$Xf-re$Xf[1:10,])
range(r$rand[[1]]-re$rand[[1]][1:10,])

</code></pre>

<hr>
<h2 id='smoothCon'>Prediction/Construction wrapper functions for GAM smooth terms</h2><span id='topic+smoothCon'></span><span id='topic+PredictMat'></span>

<h3>Description</h3>

<p> Wrapper functions for construction of and prediction from smooth
terms in a GAM. The purpose of the wrappers is to allow user-transparant
re-parameterization of smooth terms, in order to allow identifiability
constraints to be absorbed into the parameterization of each term, if required.
The routine also handles &lsquo;by&rsquo; variables and construction of identifiability constraints automatically, 
although this behaviour can be over-ridden.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothCon(object,data,knots=NULL,absorb.cons=FALSE,
          scale.penalty=TRUE,n=nrow(data),dataX=NULL,
          null.space.penalty=FALSE,sparse.cons=0,
          diagonal.penalty=FALSE,apply.by=TRUE,modCon=0)
PredictMat(object,data,n=nrow(data))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothCon_+3A_object">object</code></td>
<td>
<p> is a smooth specification object or a smooth object.</p>
</td></tr>
<tr><td><code id="smoothCon_+3A_data">data</code></td>
<td>
<p>A data frame, model frame or list containing the values of the 
(named) covariates at which the smooth term is to be 
evaluated. If it's a list then <code>n</code> must be supplied.</p>
</td></tr>
<tr><td><code id="smoothCon_+3A_knots">knots</code></td>
<td>
<p>An optional data frame supplying any knot locations to be
supplied for basis construction.</p>
</td></tr>
<tr><td><code id="smoothCon_+3A_absorb.cons">absorb.cons</code></td>
<td>
<p>Set to <code>TRUE</code> in order to have identifiability
constraints absorbed into the basis.</p>
</td></tr>
<tr><td><code id="smoothCon_+3A_scale.penalty">scale.penalty</code></td>
<td>
<p>should the penalty coefficient matrix be scaled to have
approximately the same &lsquo;size&rsquo; as the inner product of the terms model matrix
with itself? This can improve the performance of <code><a href="#topic+gamm">gamm</a></code> fitting.</p>
</td></tr>
<tr><td><code id="smoothCon_+3A_n">n</code></td>
<td>
<p>number of values for each covariate, or if a covariate is a matrix, 
the number of rows in that matrix: must be supplied explicitly if <code>data</code> is a list. </p>
</td></tr>
<tr><td><code id="smoothCon_+3A_datax">dataX</code></td>
<td>
<p>Sometimes the basis should be set up using data in <code>data</code>, but the model matrix
should be constructed with another set of data provided in <code>dataX</code> &mdash; <code>n</code> is assumed to 
be the same for both. Facilitates smooth id's.</p>
</td></tr>
<tr><td><code id="smoothCon_+3A_null.space.penalty">null.space.penalty</code></td>
<td>
<p>Should an extra penalty be added to the smooth which will penalize the 
components of the smooth in the penalty null space: provides a way of penalizing terms out of the model altogether.</p>
</td></tr>
<tr><td><code id="smoothCon_+3A_apply.by">apply.by</code></td>
<td>
<p>set to <code>FALSE</code> to have basis setup exactly as in default case, but to return add an additional 
matrix <code>X0</code> to the return object, containing the model matrix without the <code>by</code> variable, if a <code>by</code> 
variable is present. Useful for <code>bam</code> discrete method setup.</p>
</td></tr>
<tr><td><code id="smoothCon_+3A_sparse.cons">sparse.cons</code></td>
<td>
<p>If <code>0</code> then default sum to zero constraints are used. If <code>-1</code> then sweep and 
drop sum to zero constraints are used (default with <code><a href="#topic+bam">bam</a></code>). If <code>1</code> then one
coefficient is set to zero as constraint for sparse smooths. If <code>2</code> then sparse coefficient sum to zero 
constraints are used for sparse smooths. None of these options has an effect if the smooth supplies its own 
constraint.</p>
</td></tr>
<tr><td><code id="smoothCon_+3A_diagonal.penalty">diagonal.penalty</code></td>
<td>
<p> If <code>TRUE</code> then the smooth is reparameterized to turn the penalty into an identity matrix, 
with the final diagonal elements zeroed (corresponding to the penalty nullspace). May result in a matrix <code>diagRP</code> in the returned object for use by <code>PredictMat</code>.</p>
</td></tr>
<tr><td><code id="smoothCon_+3A_modcon">modCon</code></td>
<td>
<p>force modification of any smooth supplied constraints. 0 - do nothing. 1 - delete supplied constraints, replacing with automatically generated ones. 2 - set fit and predict constraint to predict constraint. 3 - set fit and predict constraint to fit constraint.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> These wrapper functions exist to allow smooths specified using
<code><a href="#topic+smooth.construct">smooth.construct</a></code> and <code><a href="#topic+Predict.matrix">Predict.matrix</a></code> method
functions to be re-parameterized so that identifiability constraints are no
longer required in fitting. This is done in a user transparent
manner, but is typically of no importance in use of GAMs. The routine's 
also handle <code>by</code> variables and will create default identifiability 
constraints.
</p>
<p>If a user defined smooth constructor handles <code>by</code> variables itself, then its 
returned smooth object should contain an object <code>by.done</code>. If this does not exist 
then <code>smoothCon</code> will use the default code. Similarly if a user defined <code>Predict.matrix</code> 
method handles <code>by</code> variables internally then the returned matrix should have a 
<code>"by.done"</code> attribute.
</p>
<p>Default centering constraints, that terms should sum to zero over the covariates, are produced unless 
the smooth constructor includes a matrix <code>C</code> of constraints. To have no constraints (in which case 
you had better have a full rank penalty!) the matrix <code>C</code> should have no rows. There is an option to 
use centering constraint that generate no, or limited infil, if the smoother has a sparse model matrix.
</p>
<p><code>smoothCon</code> returns a list of smooths because factor <code>by</code> variables result in multiple copies 
of a smooth, each multiplied by the dummy variable associated with one factor level. <code>smoothCon</code> modifies 
the smooth object labels in the presence of <code>by</code> variables, to ensure that they are unique, it also stores 
the level of a by variable factor associated with a smooth, for later use by <code>PredictMat</code>.
</p>
<p>The parameterization used by <code><a href="#topic+gam">gam</a></code> can be controlled via
<code><a href="#topic+gam.control">gam.control</a></code>.
</p>


<h3>Value</h3>

<p> From <code>smoothCon</code> a list of <code>smooth</code> objects returned by the
appropriate <code><a href="#topic+smooth.construct">smooth.construct</a></code> method function. If constraints are
to be absorbed then the objects will have  attributes <code>"qrc"</code> and
<code>"nCons"</code>. <code>"nCons"</code> is the number of constraints. <code>"qrc"</code> is
usually the qr decomposition of the constraint matrix (returned by
<code><a href="Matrix.html#topic+qr">qr</a></code>), but if it is a single positive integer it is the index of the 
coefficient to set to zero, and if it is a negative number then this indicates that 
the parameters are to sum to zero. 
</p>
<p>For <code>predictMat</code> a matrix which will map the parameters associated with
the smooth to the vector of values of the smooth evaluated at the covariate
values given in <code>object</code>. 
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

 
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+gam.control">gam.control</a></code>,
<code><a href="#topic+smooth.construct">smooth.construct</a></code>, <code><a href="#topic+Predict.matrix">Predict.matrix</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## example of using smoothCon and PredictMat to set up a basis
## to use for regression and make predictions using the result
library(MASS) ## load for mcycle data.
## set up a smoother...
sm &lt;- smoothCon(s(times,k=10),data=mcycle,knots=NULL)[[1]]
## use it to fit a regression spline model...
beta &lt;- coef(lm(mcycle$accel~sm$X-1))
with(mcycle,plot(times,accel)) ## plot data
times &lt;- seq(0,60,length=200)  ## creat prediction times
## Get matrix mapping beta to spline prediction at 'times'
Xp &lt;- PredictMat(sm,data.frame(times=times))
lines(times,Xp%*%beta) ## add smooth to plot

## Same again but using a penalized regression spline of
## rank 30....
sm &lt;- smoothCon(s(times,k=30),data=mcycle,knots=NULL)[[1]]
E &lt;- t(mroot(sm$S[[1]])) ## square root penalty
X &lt;- rbind(sm$X,0.1*E) ## augmented model matrix
y &lt;- c(mcycle$accel,rep(0,nrow(E))) ## augmented data
beta &lt;- coef(lm(y~X-1)) ## fit penalized regression spline
Xp &lt;- PredictMat(sm,data.frame(times=times)) ## prediction matrix
with(mcycle,plot(times,accel)) ## plot data
lines(times,Xp%*%beta) ## overlay smooth
</code></pre>

<hr>
<h2 id='sp.vcov'>Extract smoothing parameter estimator covariance matrix from (RE)ML GAM fit</h2><span id='topic+sp.vcov'></span>

<h3>Description</h3>

<p> Extracts the estimated covariance matrix for the log smoothing parameter
estimates from a (RE)ML estimated <code>gam</code> object, provided the fit was with a method 
that evaluated the required Hessian.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sp.vcov(x,edge.correct=TRUE,reg=1e-3)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="sp.vcov_+3A_x">x</code></td>
<td>
<p> a fitted model object of class <code>gam</code> as produced by <code>gam()</code>.</p>
</td></tr>
<tr><td><code id="sp.vcov_+3A_edge.correct">edge.correct</code></td>
<td>
<p> if the model was fitted with <code>edge.correct=TRUE</code> (see <code><a href="#topic+gam.control">gam.control</a></code>), then thereturned covariance matrix will be for the edge corrected log smoothing parameters.</p>
</td></tr>
<tr><td><code id="sp.vcov_+3A_reg">reg</code></td>
<td>
<p>regularizer for Hessian - default is equivalent to prior variance of 1000 on log smoothing parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Just extracts the inverse of the hessian matrix of the negative (restricted) log likelihood w.r.t
the log smoothing parameters, if this has been obtained as part of fitting. 
</p>


<h3>Value</h3>

<p> A matrix corresponding to the estimated covariance matrix of the log smoothing parameter estimators,
if this can be extracted, otherwise <code>NULL</code>. If the scale parameter has been (RE)ML estimated (i.e. if the method was <code>"ML"</code> or <code>"REML"</code> and the scale parameter was unknown) then the 
last row and column relate to the log scale parameter. If <code>edge.correct=TRUE</code> and this was used in fitting then the edge corrected smoothing parameters are in attribute <code>lsp</code> of the returned matrix.  
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models (with discussion).
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+gam.vcomp">gam.vcomp</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> 
require(mgcv)
n &lt;- 100
x &lt;- runif(n);z &lt;- runif(n)
y &lt;- sin(x*2*pi) + rnorm(n)*.2
mod &lt;- gam(y~s(x,bs="cc",k=10)+s(z),knots=list(x=seq(0,1,length=10)),
           method="REML")
sp.vcov(mod)
</code></pre>

<hr>
<h2 id='spasm.construct'>Experimental sparse smoothers</h2><span id='topic+spasm.construct'></span><span id='topic+spasm.sp'></span><span id='topic+spasm.smooth'></span>

<h3>Description</h3>

<p>These are experimental sparse smoothing functions, and should 
be left well alone!
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spasm.construct(object,data)
spasm.sp(object,sp,w=rep(1,object$nobs),get.trH=TRUE,block=0,centre=FALSE)
spasm.smooth(object,X,residual=FALSE,block=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spasm.construct_+3A_object">object</code></td>
<td>
<p>sparse smooth object</p>
</td></tr>
<tr><td><code id="spasm.construct_+3A_data">data</code></td>
<td>
<p>data frame</p>
</td></tr>
<tr><td><code id="spasm.construct_+3A_sp">sp</code></td>
<td>
<p>smoothing parameter value</p>
</td></tr>
<tr><td><code id="spasm.construct_+3A_w">w</code></td>
<td>
<p>optional weights</p>
</td></tr>
<tr><td><code id="spasm.construct_+3A_get.trh">get.trH</code></td>
<td>
<p>Should (estimated) trace of sparse smoother matrix be returned</p>
</td></tr>
<tr><td><code id="spasm.construct_+3A_block">block</code></td>
<td>
<p>index of block, 0 for all blocks</p>
</td></tr>
<tr><td><code id="spasm.construct_+3A_centre">centre</code></td>
<td>
<p>should sparse smooth be centred?</p>
</td></tr>
<tr><td><code id="spasm.construct_+3A_x">X</code></td>
<td>
<p>what to smooth</p>
</td></tr>
<tr><td><code id="spasm.construct_+3A_residual">residual</code></td>
<td>
<p>apply residual operation?</p>
</td></tr>
</table>


<h3>WARNING</h3>

<p>It is not recommended to use these yet</p>


<h3>Author(s)</h3>

<p>Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>

<hr>
<h2 id='step.gam'>Alternatives to step.gam</h2><span id='topic+step.gam'></span>

<h3>Description</h3>

<p> There is no <code>step.gam</code> in package <code>mgcv</code>. The
<code>mgcv</code> default for model selection is to use either prediction error criteria such as 
GCV, GACV, Mallows' Cp/AIC/UBRE or the likelihood based methods of REML or ML.  Since the 
smoothness estimation part of model
selection is done in this way it is logically most consistent to perform the rest of model
selection in the same way. i.e. to decide which terms to include
or omit by looking at changes in GCV, AIC, REML etc. 
</p>
<p>To facilitate fully automatic model selection the package implements two smooth
modification techniques which can be used to allow smooths to be shrunk to zero as 
part of smoothness selection.
</p>

<dl>
<dt>Shrinkage smoothers</dt><dd><p>are smoothers in which a small multiple of the identity matrix
is added to the smoothing penalty, so that strong enough penalization will shrink all the 
coefficients of the smooth to zero. Such smoothers can effectively be penalized out of the 
model altogether, as part of smoothing parameter estimation. 2 classes
of these shrinkage smoothers are implemented: <code>"cs"</code> and <code>"ts"</code>, based on 
cubic regression spline and thin plate regression spline smoothers (see <code><a href="#topic+s">s</a></code>)  </p>
</dd>
<dt>Null space penalization</dt><dd><p>An alternative is to construct an extra penalty for each 
smooth which penalizes the space of functions of zero wiggliness according to its existing penalties.
If all the smoothing parameters for such a term tend to infinity then the term is penalized to zero, 
and is effectively dropped from the model. The advantage of this approach is that it can be 
implemented automatically for any smooth. The <code>select</code> argument to <code><a href="#topic+gam">gam</a></code> causes 
this latter approach to be used. Unpenalized terms (e.g. <code>s(x,fx=TRUE)</code>) remain unpenalized. </p>
</dd>
</dl>

<p>REML and ML smoothness selection are equivalent under this approach, and simulation evidence suggests 
that they tend to perform a little better than prediction error criteria, for model selection.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Marra, G. and S.N. Wood (2011) Practical variable selection for generalized additive models
Computational Statistics and Data Analysis 55,2372-2387
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gam.selection">gam.selection</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## an example of GCV based model selection as
## an alternative to stepwise selection, using
## shrinkage smoothers...
library(mgcv)
set.seed(0);n &lt;- 400
dat &lt;- gamSim(1,n=n,scale=2)
dat$x4 &lt;- runif(n, 0, 1)
dat$x5 &lt;- runif(n, 0, 1)
attach(dat)
## Note the increased gamma parameter below to favour
## slightly smoother models...
b&lt;-gam(y~s(x0,bs="ts")+s(x1,bs="ts")+s(x2,bs="ts")+
   s(x3,bs="ts")+s(x4,bs="ts")+s(x5,bs="ts"),gamma=1.4)
summary(b)
plot(b,pages=1)

## Same again using REML/ML
b&lt;-gam(y~s(x0,bs="ts")+s(x1,bs="ts")+s(x2,bs="ts")+
   s(x3,bs="ts")+s(x4,bs="ts")+s(x5,bs="ts"),method="REML")
summary(b)
plot(b,pages=1)

## And once more, but using the null space penalization
b&lt;-gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+
   s(x3,bs="cr")+s(x4,bs="cr")+s(x5,bs="cr"),
   method="REML",select=TRUE)
summary(b)
plot(b,pages=1)


detach(dat);rm(dat)
</code></pre>

<hr>
<h2 id='summary.gam'>Summary for a GAM fit</h2><span id='topic+summary.gam'></span><span id='topic+print.summary.gam'></span>

<h3>Description</h3>

<p> Takes a fitted <code>gam</code> object produced by <code>gam()</code> and produces various useful
summaries from it. (See <code><a href="base.html#topic+sink">sink</a></code> to divert output to a file.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
summary(object, dispersion=NULL, freq=FALSE, re.test=TRUE, ...)

## S3 method for class 'summary.gam'
print(x,digits = max(3, getOption("digits") - 3), 
                  signif.stars = getOption("show.signif.stars"),...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="summary.gam_+3A_object">object</code></td>
<td>
<p> a fitted <code>gam</code> object as produced by <code>gam()</code>.</p>
</td></tr>
<tr><td><code id="summary.gam_+3A_x">x</code></td>
<td>
<p>a <code>summary.gam</code> object produced by <code>summary.gam()</code>.</p>
</td></tr> 
<tr><td><code id="summary.gam_+3A_dispersion">dispersion</code></td>
<td>
<p>A known dispersion parameter. <code>NULL</code> to use estimate or
default (e.g. 1 for Poisson).</p>
</td></tr>
<tr><td><code id="summary.gam_+3A_freq">freq</code></td>
<td>
<p>By default p-values for parametric terms are calculated using the Bayesian estimated
covariance matrix of the parameter estimators. If this is set to <code>TRUE</code> then
the frequentist covariance matrix of the parameters is used instead. </p>
</td></tr>
<tr><td><code id="summary.gam_+3A_re.test">re.test</code></td>
<td>
<p>Should tests be performed for random effect terms (including any term with a zero dimensional null space)?
For large models these tests can be computationally expensive. </p>
</td></tr>
<tr><td><code id="summary.gam_+3A_digits">digits</code></td>
<td>
<p>controls number of digits printed in output.</p>
</td></tr>
<tr><td><code id="summary.gam_+3A_signif.stars">signif.stars</code></td>
<td>
<p>Should significance stars be printed alongside output.</p>
</td></tr>
<tr><td><code id="summary.gam_+3A_...">...</code></td>
<td>
<p> other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Model degrees of freedom are taken as the trace of the influence (or
hat) matrix <code class="reqn"> {\bf A}</code> for the model fit.
Residual degrees of freedom are taken as number of data minus model degrees of
freedom. 
Let <code class="reqn"> {\bf P}_i</code> be the matrix 
giving the parameters of the ith smooth when applied to the data (or pseudodata in the generalized case) and let <code class="reqn"> {\bf X}</code> 
be the design matrix of the model. Then <code class="reqn"> tr({\bf XP}_i )</code> is the edf for the ith term. Clearly this 
definition causes the edf's to add up properly! An alternative version of EDF is more appropriate for p-value computation, and is based on the trace of  <code class="reqn"> 2{\bf A} - {\bf AA}</code>.
</p>
<p><code>print.summary.gam</code> tries to print various bits of summary information useful for term selection in a pretty way.
</p>
<p>P-values for smooth terms are usually based on a 
test statistic motivated by an extension of Nychka's (1988) analysis of the frequentist properties 
of Bayesian confidence intervals for smooths (Marra and Wood, 2012). 
These have better frequentist performance (in terms of power and distribution under the null) 
than the alternative strictly frequentist approximation. When the Bayesian intervals have good 
across the function properties then the p-values have close to the correct null distribution 
and reasonable power (but there are no optimality results for the power). Full details are in Wood (2013b), 
although what is computed is actually a slight variant in which the components of the test statistic are weighted by the iterative fitting weights. 
</p>
<p>Note that for terms with no unpenalized terms (such as Gaussian random effects) the Nychka (1988) requirement for smoothing bias to be substantially less than variance breaks down (see e.g. appendix of Marra and Wood, 2012), and this results in incorrect null distribution for p-values computed using the above approach. In this case it is necessary to use an alternative 
approach designed for random effects variance components, and this is done. See Wood (2013a) for details: the test is based on a likelihood ratio statistic (with the reference distribution appropriate for the null hypothesis on the boundary of the parameter space).
</p>
<p>All p-values are computed without considering uncertainty in the smoothing parameter estimates.
</p>
<p>In simulations the p-values have best behaviour under ML smoothness selection, with REML coming second. In general the p-values behave well, but neglecting smoothing parameter uncertainty means that they may be somewhat too low when smoothing parameters are highly uncertain. High uncertainty happens in particular when smoothing parameters are poorly identified, which can occur with nested smooths or highly correlated covariates (high concurvity).
</p>
<p>By default the p-values for parametric model terms are also based on Wald tests using the Bayesian 
covariance matrix for the coefficients. This is appropriate when there are &quot;re&quot; terms present, and is 
otherwise rather similar to the results using the frequentist covariance matrix (<code>freq=TRUE</code>), since 
the parametric terms themselves are usually unpenalized. Default P-values for parameteric terms that are 
penalized using the <code>paraPen</code> argument will not be good. However if such terms represent conventional 
random effects with full rank penalties, then setting <code>freq=TRUE</code> is appropriate. 
</p>


<h3>Value</h3>

<p><code>summary.gam</code> produces a list of summary information for a fitted <code>gam</code> object. 
</p>
<table>
<tr><td><code>p.coeff</code></td>
<td>
<p>is an array of estimates of the strictly parametric model coefficients.</p>
</td></tr>
<tr><td><code>p.t</code></td>
<td>
<p>is an array of the <code>p.coeff</code>'s divided by their standard errors.</p>
</td></tr>
<tr><td><code>p.pv</code></td>
<td>
<p>is an array of p-values for the null hypothesis that the corresponding parameter is zero. 
Calculated with reference to the t distribution with the estimated residual
degrees of freedom for the model fit if the dispersion parameter has been
estimated, and the standard normal if not.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>The number of smooth terms in the model.</p>
</td></tr>
<tr><td><code>chi.sq</code></td>
<td>
<p>An array of test statistics for assessing the significance of
model smooth terms. See details.</p>
</td></tr>
<tr><td><code>s.pv</code></td>
<td>
<p>An array of approximate p-values for the null hypotheses that each
smooth term is zero. Be warned, these are only approximate.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>array of standard error estimates for all parameter estimates.</p>
</td></tr>
<tr><td><code>r.sq</code></td>
<td>
<p>The adjusted r-squared for the model. Defined as the proportion of variance explained, where original variance and 
residual variance are both estimated using unbiased estimators. This quantity can be negative if your model is worse than a one 
parameter constant model, and can be higher for the smaller of two nested models! The proportion null deviance 
explained is probably more appropriate for non-normal errors. Note that <code>r.sq</code> does not include any offset in the one parameter model.</p>
</td></tr>
<tr><td><code>dev.expl</code></td>
<td>
<p>The proportion of the null deviance explained by the model. The null deviance is computed taking account of any offset, so 
<code>dev.expl</code> can be substantially lower than <code>r.sq</code> when an offset is present.</p>
</td></tr>
<tr><td><code>edf</code></td>
<td>
<p>array of estimated degrees of freedom for the model terms.</p>
</td></tr>
<tr><td><code>residual.df</code></td>
<td>
<p>estimated residual degrees of freedom.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>number of data.</p>
</td></tr>
<tr><td><code>np</code></td>
<td>
<p>number of model coefficients (regression coefficients, not smoothing parameters or other parameters of likelihood).</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>apparent model rank.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The smoothing selection criterion used.</p>
</td></tr>
<tr><td><code>sp.criterion</code></td>
<td>
<p>The minimized value of the smoothness selection criterion. Note that for ML and REML methods, 
what is reported is the negative log marginal likelihood or negative log restricted likelihood. </p>
</td></tr>
<tr><td><code>scale</code></td>
<td>
<p>estimated (or given) scale parameter.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>the family used.</p>
</td></tr>
<tr><td><code>formula</code></td>
<td>
<p>the original GAM formula.</p>
</td></tr>
<tr><td><code>dispersion</code></td>
<td>
<p>the scale parameter.</p>
</td></tr>
<tr><td><code>pTerms.df</code></td>
<td>
<p>the degrees of freedom associated with each parametric term
(excluding the constant).</p>
</td></tr>
<tr><td><code>pTerms.chi.sq</code></td>
<td>
<p>a Wald statistic for testing the null hypothesis that the
each parametric term is zero.</p>
</td></tr>
<tr><td><code>pTerms.pv</code></td>
<td>
<p>p-values associated with the tests that each term is
zero. For penalized fits these are approximate. The reference distribution 
is an appropriate chi-squared when the
scale parameter is known, and is based on an F when it is not.</p>
</td></tr>
<tr><td><code>cov.unscaled</code></td>
<td>
<p>The estimated covariance matrix of the parameters (or
estimators if <code>freq=TRUE</code>), divided
by scale parameter.</p>
</td></tr>
<tr><td><code>cov.scaled</code></td>
<td>
<p>The estimated covariance matrix of the parameters
(estimators if <code>freq=TRUE</code>).</p>
</td></tr>
<tr><td><code>p.table</code></td>
<td>
<p>significance table for parameters</p>
</td></tr>
<tr><td><code>s.table</code></td>
<td>
<p>significance table for smooths</p>
</td></tr>
<tr><td><code>p.Terms</code></td>
<td>
<p>significance table for parametric model terms</p>
</td></tr>
</table>


<h3>WARNING </h3>

 
<p>The p-values are approximate and neglect smoothing parameter uncertainty. They are likely to be somewhat too low 
when smoothing parameter estimates are highly uncertain: do read the details section. If the exact values matter,
read Wood (2013a or b). 
</p>
<p>P-values for terms penalized via &lsquo;paraPen&rsquo; are unlikely to be correct.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> with substantial
improvements by Henric Nilsson.</p>


<h3>References</h3>

<p>Marra, G and S.N. Wood (2012) Coverage Properties of Confidence Intervals for Generalized Additive
Model Components. Scandinavian Journal of Statistics, 39(1), 53-74. <a href="https://doi.org/10.1111/j.1467-9469.2011.00760.x">doi:10.1111/j.1467-9469.2011.00760.x</a>
</p>
<p>Nychka (1988) Bayesian Confidence Intervals for Smoothing Splines. 
Journal of the American Statistical Association 83:1134-1143.
</p>
<p>Wood, S.N. (2013a) A simple test for random effects in regression models. Biometrika 100:1005-1010 <a href="https://doi.org/10.1093/biomet/ast038">doi:10.1093/biomet/ast038</a>
</p>
<p>Wood, S.N. (2013b) On p-values for smooth components of an extended generalized additive model. Biometrika 100:221-228 <a href="https://doi.org/10.1093/biomet/ass048">doi:10.1093/biomet/ass048</a>
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press. <a href="https://doi.org/10.1201/9781315370279">doi:10.1201/9781315370279</a>
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+gam">gam</a></code>, <code><a href="#topic+predict.gam">predict.gam</a></code>,
<code><a href="#topic+gam.check">gam.check</a></code>, <code><a href="#topic+anova.gam">anova.gam</a></code>, <code><a href="#topic+gam.vcomp">gam.vcomp</a></code>, <code><a href="#topic+sp.vcov">sp.vcov</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(0)

dat &lt;- gamSim(1,n=200,scale=2) ## simulate data

b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
plot(b,pages=1)
summary(b)

## now check the p-values by using a pure regression spline.....
b.d &lt;- round(summary(b)$edf)+1 ## get edf per smooth
b.d &lt;- pmax(b.d,3) # can't have basis dimension less than 3!
bc&lt;-gam(y~s(x0,k=b.d[1],fx=TRUE)+s(x1,k=b.d[2],fx=TRUE)+
        s(x2,k=b.d[3],fx=TRUE)+s(x3,k=b.d[4],fx=TRUE),data=dat)
plot(bc,pages=1)
summary(bc)

## Example where some p-values are less reliable...
dat &lt;- gamSim(6,n=200,scale=2)
b &lt;- gam(y~s(x0,m=1)+s(x1)+s(x2)+s(x3)+s(fac,bs="re"),data=dat)
## Here s(x0,m=1) can be penalized to zero, so p-value approximation
## cruder than usual...
summary(b) 

## p-value check - increase k to make this useful!
k&lt;-20;n &lt;- 200;p &lt;- rep(NA,k)
for (i in 1:k)
{ b&lt;-gam(y~te(x,z),data=data.frame(y=rnorm(n),x=runif(n),z=runif(n)),
         method="ML")
  p[i]&lt;-summary(b)$s.p[1]
}
plot(((1:k)-0.5)/k,sort(p))
abline(0,1,col=2)
ks.test(p,"punif") ## how close to uniform are the p-values?

## A Gamma example, by modify `gamSim' output...
 
dat &lt;- gamSim(1,n=400,dist="normal",scale=1)
dat$f &lt;- dat$f/4 ## true linear predictor 
Ey &lt;- exp(dat$f);scale &lt;- .5 ## mean and GLM scale parameter
## Note that `shape' and `scale' in `rgamma' are almost
## opposite terminology to that used with GLM/GAM...
dat$y &lt;- rgamma(Ey*0,shape=1/scale,scale=Ey*scale)
bg &lt;- gam(y~ s(x0)+ s(x1)+s(x2)+s(x3),family=Gamma(link=log),
          data=dat,method="REML")
summary(bg)

</code></pre>

<hr>
<h2 id='t2'>Define alternative tensor product smooths in GAM formulae</h2><span id='topic+t2'></span>

<h3>Description</h3>

<p> Alternative to <code><a href="#topic+te">te</a></code> for defining tensor product smooths
in a <code><a href="#topic+gam">gam</a></code> formula. Results in a construction in which the penalties are 
non-overlapping multiples of identity matrices (with some rows and columns zeroed). 
The construction, which is due to Fabian Scheipl (<code>mgcv</code> implementation, 2010), is analogous to Smoothing Spline ANOVA 
(Gu, 2002), but using low rank penalized regression spline marginals. The main advantage of this construction 
is that it is useable with <code>gamm4</code> from package <code>gamm4</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t2(..., k=NA,bs="cr",m=NA,d=NA,by=NA,xt=NULL,
               id=NULL,sp=NULL,full=FALSE,ord=NULL,pc=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t2_+3A_...">...</code></td>
<td>
<p> a list of variables that are the covariates that this
smooth is a function of.  Transformations whose form depends on
the values of the data are best avoided here: e.g. <code>t2(log(x),z)</code>
is fine, but <code>t2(I(x/sd(x)),z)</code> is not (see <code><a href="#topic+predict.gam">predict.gam</a></code>).</p>
</td></tr>
<tr><td><code id="t2_+3A_k">k</code></td>
<td>
<p> the dimension(s) of the bases used to represent the smooth term.
If not supplied then set to <code>5^d</code>. If supplied as a single number then this 
basis dimension is used for each basis. If supplied as an array then the elements are
the dimensions of the component (marginal) bases of the tensor
product. See <code><a href="#topic+choose.k">choose.k</a></code> for further information.</p>
</td></tr>
<tr><td><code id="t2_+3A_bs">bs</code></td>
<td>
<p>array (or single character string) specifying the type for each 
marginal basis. <code>"cr"</code> for cubic regression spline; <code>"cs"</code> for cubic
regression spline with shrinkage; <code>"cc"</code> for periodic/cyclic 
cubic regression spline; <code>"tp"</code> for thin plate regression spline;
<code>"ts"</code> for t.p.r.s. with extra shrinkage. See <code><a href="#topic+smooth.terms">smooth.terms</a></code> for details 
and full list. User defined bases can 
also be used here (see <code><a href="#topic+smooth.construct">smooth.construct</a></code> for an example). If only one 
basis code is given then this is used for all bases.</p>
</td></tr>
<tr><td><code id="t2_+3A_m">m</code></td>
<td>
<p>The order of the spline and its penalty (for smooth classes that use this) for each term. 
If a single number is given  then it is used for all terms. A vector can be used to 
supply a different <code>m</code> for each margin. For marginals that take vector <code>m</code> 
(e.g. <code><a href="#topic+p.spline">p.spline</a></code> and <code><a href="#topic+Duchon.spline">Duchon.spline</a></code>), then
a list can be supplied, with a vector element for each margin. <code>NA</code> autoinitializes. 
<code>m</code> is ignored by some bases (e.g. <code>"cr"</code>).</p>
</td></tr>
<tr><td><code id="t2_+3A_d">d</code></td>
<td>
<p>array of marginal basis dimensions. For example if you want a smooth for 3 covariates 
made up of a tensor product of a 2 dimensional t.p.r.s. basis and a 1-dimensional basis, then 
set <code>d=c(2,1)</code>. Incompatibilities between built in basis types and dimension will be
resolved by resetting the basis type.</p>
</td></tr>
<tr><td><code id="t2_+3A_by">by</code></td>
<td>
<p>a numeric or factor variable of the same dimension as each covariate. 
In the numeric vector case the elements multiply the smooth evaluated at the corresponding 
covariate values (a &lsquo;varying coefficient model&rsquo; results). 
In the factor case causes a replicate of the smooth to be produced for
each factor level. See <code><a href="#topic+gam.models">gam.models</a></code> for further details. May also be a matrix 
if covariates are matrices: in this case implements linear functional of a smooth 
(see <code><a href="#topic+gam.models">gam.models</a></code> and <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code> for details).</p>
</td></tr>
<tr><td><code id="t2_+3A_xt">xt</code></td>
<td>
<p>Either a single object, providing any extra information to be passed
to each marginal basis constructor, or a list of such objects, one for each
marginal basis. </p>
</td></tr>
<tr><td><code id="t2_+3A_id">id</code></td>
<td>
<p>A label or integer identifying this term in order to link its smoothing
parameters to others of the same type. If two or more smooth terms have the same 
<code>id</code> then they will have the same smoothing paramsters, and, by default,
the same bases (first occurance defines basis type, but data from all terms 
used in basis construction).</p>
</td></tr>
<tr><td><code id="t2_+3A_sp">sp</code></td>
<td>
<p>any supplied smoothing parameters for this term. Must be an array of the same
length as the number of penalties for this smooth. Positive or zero elements are taken as fixed 
smoothing parameters. Negative elements signal auto-initialization. Over-rides values supplied in 
<code>sp</code> argument to <code><a href="#topic+gam">gam</a></code>. Ignored by <code>gamm</code>.</p>
</td></tr>
<tr><td><code id="t2_+3A_full">full</code></td>
<td>
<p>If <code>TRUE</code> then there is a separate penalty for each combination of null space column 
and range space. This gives strict invariance. If <code>FALSE</code> each combination of null space and 
range space generates one penalty, but the coulmns of each null space basis are treated as one group. 
The latter is more parsimonious, but does mean that invariance is only 
achieved by an arbitrary rescaling of null space basis vectors.</p>
</td></tr>
<tr><td><code id="t2_+3A_ord">ord</code></td>
<td>
<p>an array giving the orders of terms to retain. Here order means number of marginal range spaces
used in the construction of the component. <code>NULL</code> to retain everything. </p>
</td></tr>
<tr><td><code id="t2_+3A_pc">pc</code></td>
<td>
<p>If not <code>NULL</code>, signals a point constraint: the smooth should pass through zero at the
point given here (as a vector or list with names corresponding to the smooth names). Never ignored
if supplied. See <code><a href="#topic+identifiability">identifiability</a></code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p> Smooths of several covariates can be constructed from tensor products of the bases
used to represent smooths of one (or sometimes more) of the covariates. To do this &lsquo;marginal&rsquo; bases
are produced with associated model matrices and penalty matrices. These are reparameterized so that the 
penalty is zero everywhere, except for some elements on the leading diagonal, which all have the same non-zero value. 
This reparameterization results in an unpenalized and a penalized subset of parameters, for each marginal basis (see 
e.g. appendix of Wood, 2004, for details).
</p>
<p>The re-parameterized marginal bases are then combined to produce a basis for a single function of all the covariates 
(dimension given by the product of the dimensions of the marginal bases). In this set up there are multiple 
penalty matrices &mdash; all zero, but for a mixture of a constant and zeros on the leading diagonal. No two penalties have 
a non-zero entry in the same place. 
</p>
<p>Essentially the basis for the tensor product can be thought of as being constructed from a set of
products of the penalized (range) or unpenalized (null) space bases of the marginal smooths  (see Gu, 2002, section 2.4). 
To construct one of the set, choose either the 
null space or the range space from each marginal, and from these bases construct a product basis. The result is subject to a ridge 
penalty (unless it happens to be a product entirely of marginal null spaces). The whole basis for the smooth is constructed from 
all the different product bases that can be constructed in this way. The separately penalized components of the smooth basis each
have an interpretation in terms of the ANOVA - decomposition of the term. 
See <code><a href="#topic+pen.edf">pen.edf</a></code> for some further information.
</p>
<p>Note that there are two ways to construct the product. When <code>full=FALSE</code> then the null space bases are treated as a whole in each product,
but when <code>full=TRUE</code> each null space column is treated as a separate null space. The latter results in more penalties, but is the strict 
analog of the SS-ANOVA approach.
</p>
<p>Tensor product smooths are especially useful for representing functions of covariates measured in different units, 
although they are typically not quite as nicely behaved as t.p.r.s. smooths for well scaled covariates.
</p>
<p>Note also that GAMs constructed from lower rank tensor product smooths are
nested within GAMs constructed from higher rank tensor product smooths if the
same marginal bases are used in both cases (the marginal smooths themselves
are just special cases of tensor product smooths.)
</p>
<p>Note that tensor product smooths should not be centred (have identifiability constraints imposed) 
if any marginals would not need centering. The constructor for tensor product smooths 
ensures that this happens.
</p>
<p>The function does not evaluate the variable arguments.
</p>


<h3>Value</h3>

<p> A class <code>t2.smooth.spec</code> object defining a tensor product smooth
to be turned into a basis and penalties by the <code>smooth.construct.tensor.smooth.spec</code> function. 
</p>
<p>The returned object contains the following items:
</p>
<table>
<tr><td><code>margin</code></td>
<td>
<p>A list of <code>smooth.spec</code> objects of the type returned by <code><a href="#topic+s">s</a></code>, 
defining the basis from which the tensor product smooth is constructed.</p>
</td></tr>
<tr><td><code>term</code></td>
<td>
<p>An array of text strings giving the names of the covariates that 
the term is a function of.</p>
</td></tr>
<tr><td><code>by</code></td>
<td>
<p>is the name of any <code>by</code> variable as text (<code>"NA"</code> for none).</p>
</td></tr>
<tr><td><code>fx</code></td>
<td>
<p> logical array with element for each penalty of the term
(tensor product smooths have multiple penalties). <code>TRUE</code> if the penalty is to 
be ignored, <code>FALSE</code>, otherwise. </p>
</td></tr>
<tr><td><code>label</code></td>
<td>
<p>A suitable text label for this smooth term.</p>
</td></tr>
<tr><td><code>dim</code></td>
<td>
<p>The dimension of the smoother - i.e. the number of
covariates that it is a function of.</p>
</td></tr>
<tr><td><code>mp</code></td>
<td>
<p><code>TRUE</code> is multiple penalties are to be used (default).</p>
</td></tr>
<tr><td><code>np</code></td>
<td>
<p><code>TRUE</code> to re-parameterize 1-D marginal smooths in terms of function
values (defualt).</p>
</td></tr>
<tr><td><code>id</code></td>
<td>
<p>the <code>id</code> argument supplied to <code>te</code>.</p>
</td></tr>
<tr><td><code>sp</code></td>
<td>
<p>the <code>sp</code> argument supplied to <code>te</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> and Fabian Scheipl</p>


<h3>References</h3>

<p>Wood S.N., F. Scheipl and J.J. Faraway (2013, online Feb 2012) Straightforward intermediate rank tensor product smoothing in mixed models. Statistics and Computing. 23(3):341-360 <a href="https://doi.org/10.1007/s11222-012-9314-z">doi:10.1007/s11222-012-9314-z</a>
</p>
<p>Gu, C. (2002) Smoothing Spline ANOVA, Springer.
</p>
<p>Alternative approaches to functional ANOVA decompositions, 
*not* implemented by t2 terms, are discussed in:
</p>
<p>Belitz and Lang (2008) Simultaneous selection of variables and smoothing parameters in structured additive regression models. Computational Statistics &amp; Data Analysis, 53(1):61-81
</p>
<p>Lee, D-J and M. Durban (2011) P-spline ANOVA type interaction models for spatio-temporal smoothing. Statistical Modelling, 11:49-69
</p>
<p>Wood, S.N. (2006) Low-Rank Scale-Invariant Tensor Product Smooths for Generalized Additive Mixed Models. Biometrics 62(4): 1025-1036.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+te">te</a></code> <code><a href="#topic+s">s</a></code>,<code><a href="#topic+gam">gam</a></code>,<code><a href="#topic+gamm">gamm</a></code>, 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# following shows how tensor product deals nicely with 
# badly scaled covariates (range of x 5% of range of z )
require(mgcv)
test1&lt;-function(x,z,sx=0.3,sz=0.4)  
{ x&lt;-x*20
  (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
  0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
}
n&lt;-500
old.par&lt;-par(mfrow=c(2,2))
x&lt;-runif(n)/20;z&lt;-runif(n);
xs&lt;-seq(0,1,length=30)/20;zs&lt;-seq(0,1,length=30)
pr&lt;-data.frame(x=rep(xs,30),z=rep(zs,rep(30,30)))
truth&lt;-matrix(test1(pr$x,pr$z),30,30)
f &lt;- test1(x,z)
y &lt;- f + rnorm(n)*0.2
b1&lt;-gam(y~s(x,z))
persp(xs,zs,truth);title("truth")
vis.gam(b1);title("t.p.r.s")
b2&lt;-gam(y~t2(x,z))
vis.gam(b2);title("tensor product")
b3&lt;-gam(y~t2(x,z,bs=c("tp","tp")))
vis.gam(b3);title("tensor product")
par(old.par)

test2&lt;-function(u,v,w,sv=0.3,sw=0.4)  
{ ((pi**sv*sw)*(1.2*exp(-(v-0.2)^2/sv^2-(w-0.3)^2/sw^2)+
  0.8*exp(-(v-0.7)^2/sv^2-(w-0.8)^2/sw^2)))*(u-0.5)^2*20
}
n &lt;- 500
v &lt;- runif(n);w&lt;-runif(n);u&lt;-runif(n)
f &lt;- test2(u,v,w)
y &lt;- f + rnorm(n)*0.2

## tensor product of 2D Duchon spline and 1D cr spline
m &lt;- list(c(1,.5),0)
b &lt;- gam(y~t2(v,w,u,k=c(30,5),d=c(2,1),bs=c("ds","cr"),m=m))

## look at the edf per penalty. "rr" denotes interaction term 
## (range space range space). "rn" is interaction of null space
## for u with range space for v,w...
pen.edf(b) 

## plot results...
op &lt;- par(mfrow=c(2,2))
vis.gam(b,cond=list(u=0),color="heat",zlim=c(-0.2,3.5))
vis.gam(b,cond=list(u=.33),color="heat",zlim=c(-0.2,3.5))
vis.gam(b,cond=list(u=.67),color="heat",zlim=c(-0.2,3.5))
vis.gam(b,cond=list(u=1),color="heat",zlim=c(-0.2,3.5))
par(op)

b &lt;- gam(y~t2(v,w,u,k=c(25,5),d=c(2,1),bs=c("tp","cr"),full=TRUE),
         method="ML")
## more penalties now. numbers in labels like "r1" indicate which 
## basis function of a null space is involved in the term. 
pen.edf(b) 

</code></pre>

<hr>
<h2 id='te'>Define tensor product smooths or tensor product interactions in GAM formulae</h2><span id='topic+te'></span><span id='topic+ti'></span>

<h3>Description</h3>

<p> Functions used for the definition of tensor product smooths and interactions within
<code>gam</code> model formulae. <code>te</code> produces a full tensor product smooth, while <code>ti</code> 
produces a tensor product interaction, appropriate when the main effects (and any lower 
interactions) are also present.
</p>
<p>The functions do not evaluate the
smooth - they exists purely to help set up a model using tensor product 
based smooths. Designed to construct tensor products from any marginal
smooths with a basis-penalty representation (with the restriction that each 
marginal smooth must have only one penalty).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>te(..., k=NA,bs="cr",m=NA,d=NA,by=NA,fx=FALSE,
              np=TRUE,xt=NULL,id=NULL,sp=NULL,pc=NULL)
ti(..., k=NA,bs="cr",m=NA,d=NA,by=NA,fx=FALSE,
              np=TRUE,xt=NULL,id=NULL,sp=NULL,mc=NULL,pc=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="te_+3A_...">...</code></td>
<td>
<p> a list of variables that are the covariates that this
smooth is a function of. Transformations whose form depends on
the values of the data are best avoided here: e.g. <code>te(log(x),z)</code>
is fine, but <code>te(I(x/sd(x)),z)</code> is not (see <code><a href="#topic+predict.gam">predict.gam</a></code>). </p>
</td></tr>
<tr><td><code id="te_+3A_k">k</code></td>
<td>
<p> the dimension(s) of the bases used to represent the smooth term.
If not supplied then set to <code>5^d</code>. If supplied as a single number then this 
basis dimension is used for each basis. If supplied as an array then the elements are
the dimensions of the component (marginal) bases of the tensor
product. See <code><a href="#topic+choose.k">choose.k</a></code> for further information.</p>
</td></tr>
<tr><td><code id="te_+3A_bs">bs</code></td>
<td>
<p>array (or single character string) specifying the type for each 
marginal basis. <code>"cr"</code> for cubic regression spline; <code>"cs"</code> for cubic
regression spline with shrinkage; <code>"cc"</code> for periodic/cyclic 
cubic regression spline; <code>"tp"</code> for thin plate regression spline;
<code>"ts"</code> for t.p.r.s. with extra shrinkage. See <code><a href="#topic+smooth.terms">smooth.terms</a></code> for details 
and full list. User defined bases can 
also be used here (see <code><a href="#topic+smooth.construct">smooth.construct</a></code> for an example). If only one 
basis code is given then this is used for all bases.</p>
</td></tr>
<tr><td><code id="te_+3A_m">m</code></td>
<td>
<p>The order of the spline and its penalty (for smooth classes that use this) for each term. 
If a single number is given  then it is used for all terms. A vector can be used to 
supply a different <code>m</code> for each margin. For marginals that take vector <code>m</code> 
(e.g. <code><a href="#topic+p.spline">p.spline</a></code> and <code><a href="#topic+Duchon.spline">Duchon.spline</a></code>), then
a list can be supplied, with a vector element for each margin. <code>NA</code> autoinitializes. 
<code>m</code> is ignored by some bases (e.g. <code>"cr"</code>).</p>
</td></tr>
<tr><td><code id="te_+3A_d">d</code></td>
<td>
<p>array of marginal basis dimensions. For example if you want a smooth for 3 covariates 
made up of a tensor product of a 2 dimensional t.p.r.s. basis and a 1-dimensional basis, then 
set <code>d=c(2,1)</code>. Incompatibilities between built in basis types and dimension will be
resolved by resetting the basis type.</p>
</td></tr>
<tr><td><code id="te_+3A_by">by</code></td>
<td>
<p>a numeric or factor variable of the same dimension as each covariate. 
In the numeric vector case the elements multiply the smooth evaluated at the corresponding 
covariate values (a &lsquo;varying coefficient model&rsquo; results). 
In the factor case causes a replicate of the smooth to be produced for
each factor level. See <code><a href="#topic+gam.models">gam.models</a></code> for further details. May also be a matrix 
if covariates are matrices: in this case implements linear functional of a smooth 
(see <code><a href="#topic+gam.models">gam.models</a></code> and <code><a href="#topic+linear.functional.terms">linear.functional.terms</a></code> for details).</p>
</td></tr>
<tr><td><code id="te_+3A_fx">fx</code></td>
<td>
<p>indicates whether the term is a fixed d.f. regression
spline (<code>TRUE</code>) or a penalized regression spline (<code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="te_+3A_np">np</code></td>
<td>
 <p><code>TRUE</code> to use the &lsquo;normal parameterization&rsquo; for a tensor
product smooth. This represents any 1-d marginal smooths
via parameters that are function values at &lsquo;knots&rsquo;,
spread evenly through the data. The parameterization makes the penalties
easily interpretable, however it can reduce numerical stability in some cases.</p>
</td></tr>
<tr><td><code id="te_+3A_xt">xt</code></td>
<td>
<p>Either a single object, providing any extra information to be passed
to each marginal basis constructor, or a list of such objects, one for each
marginal basis. </p>
</td></tr>
<tr><td><code id="te_+3A_id">id</code></td>
<td>
<p>A label or integer identifying this term in order to link its smoothing
parameters to others of the same type. If two or more smooth terms have the same 
<code>id</code> then they will have the same smoothing paramsters, and, by default,
the same bases (first occurance defines basis type, but data from all terms 
used in basis construction).</p>
</td></tr>
<tr><td><code id="te_+3A_sp">sp</code></td>
<td>
<p>any supplied smoothing parameters for this term. Must be an array of the same
length as the number of penalties for this smooth. Positive or zero elements are taken as fixed 
smoothing parameters. Negative elements signal auto-initialization. Over-rides values supplied in 
<code>sp</code> argument to <code><a href="#topic+gam">gam</a></code>. Ignored by <code>gamm</code>.</p>
</td></tr>
<tr><td><code id="te_+3A_mc">mc</code></td>
<td>
<p>For <code>ti</code> smooths you can specify which marginals should have centering constraints 
applied, by supplying 0/1 or <code>FALSE</code>/<code>TRUE</code> values for each marginal in this vector. By default
all marginals are constrained, which is what is appropriate for, e.g., functional ANOVA models. Note that
<code>'ti'</code> only applies constraints to the marginals, so if you turn off all marginal constraints the term 
will have no identifiability constraints. Only use this if you really understand how marginal constraints work. 
</p>
</td></tr>
<tr><td><code id="te_+3A_pc">pc</code></td>
<td>
<p>If not <code>NULL</code>, signals a point constraint: the smooth should pass through zero at the
point given here (as a vector or list with names corresponding to the smooth names). Never ignored
if supplied. See <code><a href="#topic+identifiability">identifiability</a></code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p> Smooths of several covariates can be constructed from tensor products of the bases
used to represent smooths of one (or sometimes more) of the covariates. To do this &lsquo;marginal&rsquo; bases
are produced with associated model matrices and penalty matrices, and these are then combined in the
manner described in <code><a href="#topic+tensor.prod.model.matrix">tensor.prod.model.matrix</a></code> and <code><a href="#topic+tensor.prod.penalties">tensor.prod.penalties</a></code>, to produce 
a single model matrix for the smooth, but multiple penalties (one for each marginal basis). The basis dimension 
of the whole smooth is the product of the basis dimensions of the marginal smooths.
</p>
<p>Tensor product smooths are especially useful for representing functions of covariates measured in different units, 
although they are typically not quite as nicely behaved as t.p.r.s. smooths for well scaled covariates.
</p>
<p>It is sometimes useful to investigate smooth models with a main-effects + interactions structure, for example
</p>
<p style="text-align: center;"><code class="reqn">f_1(x)  + f_2(z) + f_3(x,z)</code>
</p>

<p>This functional ANOVA decomposition is supported by <code>ti</code> terms, which produce tensor product interactions from which the main effects have been excluded, under the assumption that they will be included separately. For example the <code>~ ti(x) + ti(z) + ti(x,z)</code> would produce the above main effects + interaction structure. This is much better than attempting the same thing with <code>s</code>or <code>te</code> terms representing the interactions (although mgcv does not forbid it). Technically <code>ti</code> terms are very simple: they simply construct tensor product bases from marginal smooths to which identifiability constraints (usually sum-to-zero) have already been applied: correct nesting is then automatic (as with all interactions in a GLM framework). See Wood (2017, section 5.6.3).
</p>
<p>The &lsquo;normal parameterization&rsquo; (<code>np=TRUE</code>) re-parameterizes the marginal
smooths of a tensor product smooth so that the parameters are function values
at a set of points spread evenly through the range of values of the covariate
of the smooth. This means that the penalty of the tensor product associated
with any particular covariate direction can be interpreted as the penalty of
the appropriate marginal smooth applied in that direction and averaged over
the smooth. Currently this is only done for marginals of a single
variable. This parameterization can reduce numerical stability  when used
with marginal smooths other than <code>"cc"</code>, <code>"cr"</code> and <code>"cs"</code>: if
this causes problems, set <code>np=FALSE</code>.
</p>
<p>Note that tensor product smooths should not be centred (have identifiability constraints imposed) 
if any marginals would not need centering. The constructor for tensor product smooths 
ensures that this happens.
</p>
<p>The function does not evaluate the variable arguments.
</p>


<h3>Value</h3>

<p> A class <code>tensor.smooth.spec</code> object defining a tensor product smooth
to be turned into a basis and penalties by the <code>smooth.construct.tensor.smooth.spec</code> function. 
</p>
<p>The returned object contains the following items:
</p>
<table>
<tr><td><code>margin</code></td>
<td>
<p>A list of <code>smooth.spec</code> objects of the type returned by <code><a href="#topic+s">s</a></code>, 
defining the basis from which the tensor product smooth is constructed.</p>
</td></tr>
<tr><td><code>term</code></td>
<td>
<p>An array of text strings giving the names of the covariates that 
the term is a function of.</p>
</td></tr>
<tr><td><code>by</code></td>
<td>
<p>is the name of any <code>by</code> variable as text (<code>"NA"</code> for none).</p>
</td></tr>
<tr><td><code>fx</code></td>
<td>
<p> logical array with element for each penalty of the term
(tensor product smooths have multiple penalties). <code>TRUE</code> if the penalty is to 
be ignored, <code>FALSE</code>, otherwise. </p>
</td></tr>
<tr><td><code>label</code></td>
<td>
<p>A suitable text label for this smooth term.</p>
</td></tr>
<tr><td><code>dim</code></td>
<td>
<p>The dimension of the smoother - i.e. the number of
covariates that it is a function of.</p>
</td></tr>
<tr><td><code>mp</code></td>
<td>
<p><code>TRUE</code> is multiple penalties are to be used (default).</p>
</td></tr>
<tr><td><code>np</code></td>
<td>
<p><code>TRUE</code> to re-parameterize 1-D marginal smooths in terms of function
values (defualt).</p>
</td></tr>
<tr><td><code>id</code></td>
<td>
<p>the <code>id</code> argument supplied to <code>te</code>.</p>
</td></tr>
<tr><td><code>sp</code></td>
<td>
<p>the <code>sp</code> argument supplied to <code>te</code>.</p>
</td></tr>
<tr><td><code>inter</code></td>
<td>
<p><code>TRUE</code> if the term was generated by <code>ti</code>, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code>mc</code></td>
<td>
<p>the argument <code>mc</code> supplied to <code>ti</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2006) Low rank scale invariant tensor product smooths for
generalized additive mixed models. Biometrics 62(4):1025-1036 <a href="https://doi.org/10.1111/j.1541-0420.2006.00574.x">doi:10.1111/j.1541-0420.2006.00574.x</a>
</p>
<p>Wood S.N. (2017) Generalized Additive Models: An Introduction with R (2nd edition). Chapman
and Hall/CRC Press. <a href="https://doi.org/10.1201/9781315370279">doi:10.1201/9781315370279</a>
</p>
<p><a href="https://www.maths.ed.ac.uk/~swood34/">https://www.maths.ed.ac.uk/~swood34/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+s">s</a></code>,<code><a href="#topic+gam">gam</a></code>,<code><a href="#topic+gamm">gamm</a></code>, 
<code><a href="#topic+smooth.construct.tensor.smooth.spec">smooth.construct.tensor.smooth.spec</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# following shows how tensor pruduct deals nicely with 
# badly scaled covariates (range of x 5% of range of z )
require(mgcv)
test1 &lt;- function(x,z,sx=0.3,sz=0.4) { 
  x &lt;- x*20
  (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
  0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
}
n &lt;- 500
old.par &lt;- par(mfrow=c(2,2))
x &lt;- runif(n)/20;z &lt;- runif(n);
xs &lt;- seq(0,1,length=30)/20;zs &lt;- seq(0,1,length=30)
pr &lt;- data.frame(x=rep(xs,30),z=rep(zs,rep(30,30)))
truth &lt;- matrix(test1(pr$x,pr$z),30,30)
f &lt;- test1(x,z)
y &lt;- f + rnorm(n)*0.2
b1 &lt;- gam(y~s(x,z))
persp(xs,zs,truth);title("truth")
vis.gam(b1);title("t.p.r.s")
b2 &lt;- gam(y~te(x,z))
vis.gam(b2);title("tensor product")
b3 &lt;- gam(y~ ti(x) + ti(z) + ti(x,z))
vis.gam(b3);title("tensor anova")

## now illustrate partial ANOVA decomp...
vis.gam(b3);title("full anova")
b4 &lt;- gam(y~ ti(x) + ti(x,z,mc=c(0,1))) ## note z constrained!
vis.gam(b4);title("partial anova")
plot(b4)

par(old.par)

## now with a multivariate marginal....

test2&lt;-function(u,v,w,sv=0.3,sw=0.4)  
{ ((pi**sv*sw)*(1.2*exp(-(v-0.2)^2/sv^2-(w-0.3)^2/sw^2)+
  0.8*exp(-(v-0.7)^2/sv^2-(w-0.8)^2/sw^2)))*(u-0.5)^2*20
}
n &lt;- 500
v &lt;- runif(n);w&lt;-runif(n);u&lt;-runif(n)
f &lt;- test2(u,v,w)
y &lt;- f + rnorm(n)*0.2
# tensor product of 2D Duchon spline and 1D cr spline
m &lt;- list(c(1,.5),rep(0,0)) ## example of list form of m
b &lt;- gam(y~te(v,w,u,k=c(30,5),d=c(2,1),bs=c("ds","cr"),m=m))
plot(b)

</code></pre>

<hr>
<h2 id='tensor.prod.model.matrix'>Row Kronecker product/ tensor product smooth construction</h2><span id='topic+tensor.prod.model.matrix'></span><span id='topic+tensor.prod.penalties'></span><span id='topic++25.+25'></span>

<h3>Description</h3>

<p>Produce model matrices or penalty matrices for a tensor product smooth from the model matrices or
penalty matrices for the marginal bases of the smooth (marginals and results can be sparse). The model matrix construction uses row Kronecker products.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensor.prod.model.matrix(X)
tensor.prod.penalties(S)
a%.%b
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tensor.prod.model.matrix_+3A_x">X</code></td>
<td>
<p>a list of model matrices for the marginal bases of a smooth. Items can be class <code>"matrix"</code> or <code>"dgCMatrix"</code>, but not a mixture of the two.</p>
</td></tr> 
<tr><td><code id="tensor.prod.model.matrix_+3A_s">S</code></td>
<td>
<p>a list of penalties for the marginal bases of a smooth.</p>
</td></tr>
<tr><td><code id="tensor.prod.model.matrix_+3A_a">a</code></td>
<td>
<p>a matrix with the same number of rows as <code>A</code>.</p>
</td></tr>
<tr><td><code id="tensor.prod.model.matrix_+3A_b">b</code></td>
<td>
<p>a matrix with the same number of rows as <code>B</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> If <code>X[[1]]</code>, <code>X[[2]]</code> ... <code>X[[m]]</code> are the model matrices of the marginal bases of 
a tensor product smooth then the ith row of the model matrix for the whole tensor product smooth is given by
<code>X[[1]][i,]%x%X[[2]][i,]%x% ... X[[m]][i,]</code>, where <code>%x%</code> is the Kronecker product. Of course 
the routine operates column-wise, not row-wise!
</p>
<p><code>A%.%B</code> is the operator form of this &lsquo;row Kronecker product&rsquo;.
</p>
<p>If <code>S[[1]]</code>, <code>S[[2]]</code> ... <code>S[[m]]</code> are  the penalty matrices for the marginal bases, and 
<code>I[[1]]</code>, <code>I[[2]]</code> ... <code>I[[m]]</code> are corresponding identity matrices, each of the same 
dimension as its corresponding penalty, then the tensor product smooth has m associate penalties of the form:
</p>
<p><code>S[[1]]%x%I[[2]]%x% ... I[[m]]</code>, 
</p>
<p><code>I[[1]]%x%S[[2]]%x% ... I[[m]]</code> 
</p>
<p>... 
</p>
<p><code>I[[1]]%x%I[[2]]%x% ... S[[m]]</code>. 
</p>
<p>Of course it's important that the model matrices and penalty matrices are presented in the same order when 
constructing tensor product smooths.
</p>


<h3>Value</h3>

<p> Either a single model matrix for a tensor product smooth (of the same class as the marginals), or a list of penalty terms for a tensor product smooth.   
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2006) Low rank scale invariant tensor product smooths for
Generalized Additive Mixed Models. Biometrics 62(4):1025-1036
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+te">te</a></code>, <code><a href="#topic+smooth.construct.tensor.smooth.spec">smooth.construct.tensor.smooth.spec</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
## Dense row Kronecker product example...
X &lt;- list(matrix(0:3,2,2),matrix(c(5:8,0,0),2,3))
tensor.prod.model.matrix(X)
X[[1]]%.%X[[2]]

## sparse equivalent...
Xs &lt;- lapply(X,as,"dgCMatrix")
tensor.prod.model.matrix(Xs)
Xs[[1]]%.%Xs[[2]]

S &lt;- list(matrix(c(2,1,1,2),2,2),matrix(c(2,1,0,1,2,1,0,1,2),3,3))
tensor.prod.penalties(S)
## Sparse equivalent...
Ss &lt;- lapply(S,as,"dgCMatrix")
tensor.prod.penalties(Ss)
</code></pre>

<hr>
<h2 id='totalPenaltySpace'>Obtaining (orthogonal) basis for null space and range of the penalty matrix</h2><span id='topic+totalPenaltySpace'></span>

<h3>Description</h3>

<p>INTERNAL function to obtain (orthogonal) basis for the null space and
range space of the penalty, and obtain actual null space dimension
components are roughly rescaled to avoid any dominating.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>totalPenaltySpace(S, H, off, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="totalPenaltySpace_+3A_s">S</code></td>
<td>
<p>a list of penalty matrices, in packed form.</p>
</td></tr>
<tr><td><code id="totalPenaltySpace_+3A_h">H</code></td>
<td>
<p>the coefficient matrix of an user supplied fixed quadratic 
penalty on the parameters of the GAM.</p>
</td></tr>
<tr><td><code id="totalPenaltySpace_+3A_off">off</code></td>
<td>
<p>a vector where the i-th element is the offset for the i-th matrix.</p>
</td></tr>
<tr><td><code id="totalPenaltySpace_+3A_p">p</code></td>
<td>
<p>total number of parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of matrix square roots such that <code>S[[i]]=B[[i]]%*%t(B[[i]])</code>.
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>

<hr>
<h2 id='trichol'>Choleski decomposition of a tri-diagonal matrix</h2><span id='topic+trichol'></span>

<h3>Description</h3>

 
<p>Computes Choleski decomposition of a (symmetric positive definite) tri-diagonal matrix stored as a leading diagonal and sub/super diagonal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trichol(ld,sd)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trichol_+3A_ld">ld</code></td>
<td>
<p>leading diagonal of matrix</p>
</td></tr>
<tr><td><code id="trichol_+3A_sd">sd</code></td>
<td>
<p>sub-super diagonal of matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calls <code>dpttrf</code> from <code>LAPACK</code>. The point of this is that it has <code class="reqn">O(n)</code> computational cost, rather than the <code class="reqn">O(n^3)</code> required by dense matrix methods.
</p>


<h3>Value</h3>

<p> A list with elements <code>ld</code> and <code>sd</code>. <code>ld</code> is the leading diagonal and <code>sd</code> is the super diagonal of bidiagonal matrix <code class="reqn">\bf B</code> where <code class="reqn">{\bf B}^T{\bf B} = {\bf T}</code> and <code class="reqn">\bf T</code> is the original tridiagonal matrix. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Anderson, E., Bai, Z., Bischof, C., Blackford, S., Dongarra, J., Du Croz, J., Greenbaum, A., Hammarling, S., McKenney, A. and Sorensen, D., 1999. LAPACK Users' guide (Vol. 9). Siam.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bandchol">bandchol</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)
## simulate some diagonals...
set.seed(19); k &lt;- 7
ld &lt;- runif(k)+1
sd &lt;- runif(k-1) -.5

## get diagonals of chol factor...
trichol(ld,sd)

## compare to dense matrix result...
A &lt;- diag(ld);for (i in 1:(k-1)) A[i,i+1] &lt;- A[i+1,i] &lt;- sd[i]
R &lt;- chol(A)
diag(R);diag(R[,-1])

</code></pre>

<hr>
<h2 id='trind.generator'>Generates index arrays for upper triangular storage</h2><span id='topic+trind.generator'></span>

<h3>Description</h3>

<p>Generates index arrays for upper triangular storage up to order four. Useful when
working with higher order derivatives, which generate symmetric arrays. 
Mainly intended for internal use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trind.generator(K = 2, ifunc=FALSE, reverse= !ifunc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trind.generator_+3A_k">K</code></td>
<td>
<p>positive integer determining the size of the array.</p>
</td></tr>
<tr><td><code id="trind.generator_+3A_ifunc">ifunc</code></td>
<td>
<p>if <code>TRUE</code> index functions are returned in place of index arrays.</p>
</td></tr>
<tr><td><code id="trind.generator_+3A_reverse">reverse</code></td>
<td>
<p>should the reverse indices be computed? Probably not if <code>ifunc==TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose that <code>m=1</code> and you fill an array using code like
<code>for(i in 1:K) for(j in i:K) for(k in j:K) for(l in k:K) 
          {a[,m] &lt;- something; m &lt;- m+1 }</code> and do this because actually the same 
&quot;something&quot; would be stored for any permutation of the indices i,j,k,l.
Clearly in storage we have the restriction l&gt;=k&gt;=j&gt;=i, but for access we 
want no restriction on the indices. <code>i4[i,j,k,l]</code> produces the 
appropriate <code>m</code> for unrestricted indices. <code>i3</code> and <code>i2</code> do the same 
for 3d and 2d arrays. If <code>ifunc==TRUE</code> then <code>i2</code>, <code>i3</code> and <code>i4</code>
are functions, so <code>i4(i,j,k,l)</code> returns appropriate <code>m</code>. For high <code>K</code>
the function versions save storage, but are slower.
</p>
<p>If computed, the reverse indices pick out the unique elements of a symmetric array stored redundantly.
The indices refer to the location of the elements when the redundant array is accessed as its underlying
vector. For example the reverse indices for a 3 by 3 symmetric matrix are 1,2,3,5,6,9.
</p>


<h3>Value</h3>

<p>A list where the entries <code>i1</code> to <code>i4</code> are arrays in up to four dimensions, 
containing K indexes along each dimension. If <code>ifunc==TRUE</code> index functions
are returned in place of index arrays. If <code>reverse==TRUE</code> reverse indices
<code>i1r</code> to <code>i4r</code> are returned (always as arrays).
</p>


<h3>Author(s)</h3>

<p>Simon N. Wood &lt;simon.wood@r-project.org&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
A &lt;- trind.generator(3,reverse=TRUE)

# All permutations of c(1, 2, 3) point to the same index (5)
A$i3[1, 2, 3] 
A$i3[2, 1, 3]
A$i3[2, 3, 1]
A$i3[3, 1, 2]
A$i3[1, 3, 2]

## use reverse indices to pick out unique elements
## just for illustration...
A$i2;A$i2[A$i2r]
A$i3[A$i3r]


## same again using function indices...
A &lt;- trind.generator(3,ifunc=TRUE)
A$i3(1, 2, 3) 
A$i3(2, 1, 3)
A$i3(2, 3, 1)
A$i3(3, 1, 2)
A$i3(1, 3, 2)
</code></pre>

<hr>
<h2 id='Tweedie'>GAM Tweedie families</h2><span id='topic+Tweedie'></span><span id='topic+tw'></span>

<h3>Description</h3>

<p>Tweedie families, designed for use with <code><a href="#topic+gam">gam</a></code> from the <code>mgcv</code> library.
Restricted to variance function powers between 1 and 2. A useful alternative to <code><a href="stats.html#topic+quasi">quasi</a></code> when a
full likelihood is desirable. <code>Tweedie</code> is for use with fixed <code>p</code>. <code>tw</code> is for use when <code>p</code>
is to be estimated during fitting. For fixed <code>p</code> between 1 and 2 the Tweedie is an exponential family 
distribution with variance given by the mean to the power <code>p</code>.
</p>
<p><code>tw</code> is only useable with <code><a href="#topic+gam">gam</a></code> and <code><a href="#topic+bam">bam</a></code> but not <code>gamm</code>. <code>Tweedie</code> works with all three.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Tweedie(p=1, link = power(0))
tw(theta = NULL, link = "log",a=1.01,b=1.99)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tweedie_+3A_p">p</code></td>
<td>
<p>the variance of an observation is proportional to its mean to the power <code>p</code>. <code>p</code> must
be greater than 1 and less than or equal to 2. 1 would be Poisson, 2 is gamma. </p>
</td></tr>
<tr><td><code id="Tweedie_+3A_link">link</code></td>
<td>
<p>The link function: one of <code>"log"</code>, <code>"identity"</code>, <code>"inverse"</code>, <code>"sqrt"</code>, or a 
<code><a href="stats.html#topic+power">power</a></code> link (<code>Tweedie</code> only).</p>
</td></tr>
<tr><td><code id="Tweedie_+3A_theta">theta</code></td>
<td>
<p>Related to the Tweedie power parameter by <code class="reqn">p=(a+b \exp(\theta))/(1+\exp(\theta))</code>. If this is supplied as a positive value then it is taken as the fixed value for <code>p</code>.
If it is a negative values then its absolute value is taken as the initial value for <code>p</code>.</p>
</td></tr>
<tr><td><code id="Tweedie_+3A_a">a</code></td>
<td>
<p>lower limit on <code>p</code> for optimization.</p>
</td></tr>
<tr><td><code id="Tweedie_+3A_b">b</code></td>
<td>
<p>upper limit on <code>p</code> for optimization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> A Tweedie random variable with 1&lt;p&lt;2 is a sum of <code>N</code> gamma random variables 
where <code>N</code> has a Poisson distribution. The p=1 case is a generalization of a Poisson distribution and is a discrete 
distribution supported on integer multiples of the scale parameter. For 1&lt;p&lt;2 the distribution is supported on the 
positive reals with a point mass at zero. p=2 is a gamma distribution. As p gets very close to 1 the continuous 
distribution begins to converge on the discretely supported limit at p=1, and is therefore highly multimodal. 
See <code><a href="#topic+ldTweedie">ldTweedie</a></code> for more on this behaviour.
</p>
<p><code>Tweedie</code> is based partly on the <code><a href="stats.html#topic+poisson">poisson</a></code> family, and partly on <code>tweedie</code> from the 
<code>statmod</code> package. It includes extra components to work with all <code>mgcv</code> GAM fitting methods as well as an <code>aic</code> function. 
</p>
<p>The Tweedie density involves a normalizing constant with no closed form, so this is evaluated using the series 
evaluation method of Dunn and Smyth (2005), with extensions to also compute the derivatives w.r.t. <code>p</code> and the scale parameter. 
Without restricting <code>p</code> to (1,2) the calculation of Tweedie densities is more difficult, and there does not 
currently seem to be an implementation which offers any benefit over <code><a href="stats.html#topic+quasi">quasi</a></code>. If you need  this 
case then the <code>tweedie</code> package is the place to start.
</p>


<h3>Value</h3>

<p>For <code>Tweedie</code>, an object inheriting from class <code>family</code>, with additional elements
</p>
<table>
<tr><td><code>dvar</code></td>
<td>
<p>the function giving the first derivative of the variance function w.r.t. <code>mu</code>.</p>
</td></tr>
<tr><td><code>d2var</code></td>
<td>
<p>the function giving the second derivative of the variance function w.r.t. <code>mu</code>.</p>
</td></tr>
<tr><td><code>ls</code></td>
<td>
<p>A function returning a 3 element array: the saturated log likelihood followed by its first 2 derivatives
w.r.t. the scale parameter.</p>
</td></tr>
</table>
<p>For <code>tw</code>, an object of class <code>extended.family</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>.
</p>


<h3>References</h3>

<p>Dunn, P.K. and G.K. Smyth (2005) Series evaluation of Tweedie exponential dispersion model densities. 
Statistics and Computing 15:267-280
</p>
<p>Tweedie, M. C. K. (1984). An index which distinguishes between
some important exponential families. Statistics: Applications and
New Directions. Proceedings of the Indian Statistical Institute
Golden Jubilee International Conference (Eds. J. K. Ghosh and J.
Roy), pp. 579-604. Calcutta: Indian Statistical Institute.
</p>
<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ldTweedie">ldTweedie</a></code>, <code><a href="#topic+rTweedie">rTweedie</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(3)
n&lt;-400
## Simulate data...
dat &lt;- gamSim(1,n=n,dist="poisson",scale=.2)
dat$y &lt;- rTweedie(exp(dat$f),p=1.3,phi=.5) ## Tweedie response

## Fit a fixed p Tweedie, with wrong link ...
b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=Tweedie(1.25,power(.1)),
         data=dat)
plot(b,pages=1)
print(b)

## Same by approximate REML...
b1 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=Tweedie(1.25,power(.1)),
          data=dat,method="REML")
plot(b1,pages=1)
print(b1)

## estimate p as part of fitting

b2 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=tw(),
          data=dat,method="REML")
plot(b2,pages=1)
print(b2)

rm(dat)
</code></pre>

<hr>
<h2 id='twlss'>Tweedie location scale family</h2><span id='topic+twlss'></span>

<h3>Description</h3>

<p>Tweedie family in which the mean, power and scale parameters can all depend on smooth linear predictors. Restricted to estimation via the extended Fellner Schall method of Wood and Fasiolo (2017). Only usable with <code><a href="#topic+gam">gam</a></code>. Tweedie distributions are exponential family with variance given by <code class="reqn">\phi \mu^p</code> where <code class="reqn">\phi</code> is a scale parameter, <code class="reqn">p</code> a parameter (here between 1 and 2) and <code class="reqn">\mu</code> is the mean. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>twlss(link=list("log","identity","identity"),a=1.01,b=1.99)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="twlss_+3A_link">link</code></td>
<td>
<p>The link function list: currently no choise.</p>
</td></tr>
<tr><td><code id="twlss_+3A_a">a</code></td>
<td>
<p>lower limit on the power parameter relating variance to mean.</p>
</td></tr>
<tr><td><code id="twlss_+3A_b">b</code></td>
<td>
<p>upper limit on power parameter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> A Tweedie random variable with 1&lt;p&lt;2 is a sum of <code>N</code> gamma random variables 
where <code>N</code> has a Poisson distribution. The p=1 case is a generalization of a Poisson distribution and is a discrete 
distribution supported on integer multiples of the scale parameter. For 1&lt;p&lt;2 the distribution is supported on the 
positive reals with a point mass at zero. p=2 is a gamma distribution. As p gets very close to 1 the continuous 
distribution begins to converge on the discretely supported limit at p=1, and is therefore highly multimodal. 
See <code><a href="#topic+ldTweedie">ldTweedie</a></code> for more on this behaviour.
</p>
<p>The Tweedie density involves a normalizing constant with no closed form, so this is evaluated using the series 
evaluation method of Dunn and Smyth (2005), with extensions to also compute the derivatives w.r.t. <code>p</code> and the scale parameter. 
Without restricting <code>p</code> to (1,2) the calculation of Tweedie densities is more difficult, and there does not 
currently seem to be an implementation which offers any benefit over <code><a href="stats.html#topic+quasi">quasi</a></code>. If you need  this 
case then the <code>tweedie</code> package is the place to start.
</p>


<h3>Value</h3>

<p>An object inheriting from class <code>general.family</code>.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>.
</p>


<h3>References</h3>

<p>Dunn, P.K. and G.K. Smyth (2005) Series evaluation of Tweedie exponential dispersion model densities. 
Statistics and Computing 15:267-280
</p>
<p>Tweedie, M. C. K. (1984). An index which distinguishes between
some important exponential families. Statistics: Applications and
New Directions. Proceedings of the Indian Statistical Institute
Golden Jubilee International Conference (Eds. J. K. Ghosh and J.
Roy), pp. 579-604. Calcutta: Indian Statistical Institute.
</p>
<p>Wood, S.N. and Fasiolo, M., (2017). A generalized Fellner-Schall method for smoothing
parameter optimization with application to Tweedie location, scale and shape models. Biometrics, 73(4), pp.1071-1081.
<a href="https://doi.org/10.1111/biom.12666">doi:10.1111/biom.12666</a>
</p>
<p>Wood, S.N., N. Pya and B. Saefken (2016). Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Tweedie">Tweedie</a></code>, <code><a href="#topic+ldTweedie">ldTweedie</a></code>, <code><a href="#topic+rTweedie">rTweedie</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(3)
n&lt;-400
## Simulate data...
dat &lt;- gamSim(1,n=n,dist="poisson",scale=.2)
dat$y &lt;- rTweedie(exp(dat$f),p=1.3,phi=.5) ## Tweedie response

## Fit a fixed p Tweedie, with wrong link ...
b &lt;- gam(list(y~s(x0)+s(x1)+s(x2)+s(x3),~1,~1),family=twlss(),
         data=dat)
plot(b,pages=1)
print(b)

rm(dat)
</code></pre>

<hr>
<h2 id='uniquecombs'>find the unique rows in a matrix </h2><span id='topic+uniquecombs'></span>

<h3>Description</h3>

<p>This routine returns a matrix or data frame containing all the unique rows of the
matrix or data frame supplied as its argument. That is, all the duplicate rows are
stripped out. Note that the ordering of the rows on exit need not be the same
as on entry. It also returns an index attribute for relating the result back 
to the original matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uniquecombs(x,ordered=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="uniquecombs_+3A_x">x</code></td>
<td>
<p> is an <span class="rlang"><b>R</b></span> matrix (numeric), or data frame. </p>
</td></tr>
<tr><td><code id="uniquecombs_+3A_ordered">ordered</code></td>
<td>
<p> set to <code>TRUE</code> to have the rows of the returned object in the same order regardless of input ordering.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Models with more parameters than unique combinations of
covariates are not identifiable. This routine provides a means of
evaluating the number of unique combinations of covariates in a
model. 
</p>
<p>When <code>x</code> has only one column then the routine
uses <code><a href="base.html#topic+unique">unique</a></code> and <code><a href="base.html#topic+match">match</a></code> to get the index. When there are
multiple columns then it uses <code><a href="base.html#topic+paste0">paste0</a></code> to produce labels for each row, 
which should be unique if the row is unique. Then <code>unique</code> and <code>match</code> 
can be used as in the single column case. Obviously the pasting is inefficient, but 
still quicker for large n than the C based code that used to be called by this routine, which 
had O(nlog(n)) cost. In principle a hash table based solution in C 
would be only O(n) and much quicker in the multicolumn case. 
</p>
<p><code><a href="base.html#topic+unique">unique</a></code> and <code><a href="base.html#topic+duplicated">duplicated</a></code>, can be used 
in place of this, if the full index is not needed. Relative performance is variable. 
</p>
<p>If <code>x</code> is not a matrix or data frame on entry then an attempt is made to coerce 
it to a data frame. 
</p>


<h3>Value</h3>

<p>A matrix or data frame consisting of the unique rows of <code>x</code> (in arbitrary order).
</p>
<p>The matrix or data frame has an <code>"index"</code> attribute. <code>index[i]</code> gives the row of the returned 
matrix that contains row i of the original matrix. 
</p>


<h3>WARNINGS </h3>

<p>If a dataframe contains variables of a type other than numeric, logical, factor or character, which
either have no <code>as.character</code> method, or whose <code>as.character</code> method is a many to one mapping,
then the routine is likely to fail.
</p>
<p>If the character representation of a dataframe variable (other than of class factor of character) contains <code>*</code> then in principle the method could fail (but with a warning).
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a> with thanks to Jonathan Rougier</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+unique">unique</a></code>, <code><a href="base.html#topic+duplicated">duplicated</a></code>, <code><a href="base.html#topic+match">match</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(mgcv)

## matrix example...
X &lt;- matrix(c(1,2,3,1,2,3,4,5,6,1,3,2,4,5,6,1,1,1),6,3,byrow=TRUE)
print(X)
Xu &lt;- uniquecombs(X);Xu
ind &lt;- attr(Xu,"index")
## find the value for row 3 of the original from Xu
Xu[ind[3],];X[3,]

## same with fixed output ordering
Xu &lt;- uniquecombs(X,TRUE);Xu
ind &lt;- attr(Xu,"index")
## find the value for row 3 of the original from Xu
Xu[ind[3],];X[3,]


## data frame example...
df &lt;- data.frame(f=factor(c("er",3,"b","er",3,3,1,2,"b")),
      x=c(.5,1,1.4,.5,1,.6,4,3,1.7),
      bb = c(rep(TRUE,5),rep(FALSE,4)),
      fred = c("foo","a","b","foo","a","vf","er","r","g"),
      stringsAsFactors=FALSE)
uniquecombs(df)
</code></pre>

<hr>
<h2 id='vcov.gam'>Extract parameter (estimator) covariance matrix from GAM fit</h2><span id='topic+vcov.gam'></span>

<h3>Description</h3>

<p> Extracts the Bayesian posterior covariance matrix of the
parameters or frequentist covariance matrix of the parameter estimators 
from a fitted <code>gam</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gam'
vcov(object, sandwich=FALSE, freq = FALSE, dispersion = NULL,unconditional=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="vcov.gam_+3A_object">object</code></td>
<td>
<p>fitted model object of class <code>gam</code> as produced by <code>gam()</code>.</p>
</td></tr>
<tr><td><code id="vcov.gam_+3A_sandwich">sandwich</code></td>
<td>
<p>compute sandwich estimate of covariance matrix. Currently expensive for discrete bam fits.</p>
</td></tr>
<tr><td><code id="vcov.gam_+3A_freq">freq</code></td>
<td>
 <p><code>TRUE</code> to return the frequentist covariance matrix of the
parameter estimators, <code>FALSE</code> to return the Bayesian posterior covariance
matrix of the parameters. The latter option includes the expected squared bias
according to the Bayesian smoothing prior.</p>
</td></tr> 
<tr><td><code id="vcov.gam_+3A_dispersion">dispersion</code></td>
<td>
<p> a value for the dispersion parameter: not normally used.</p>
</td></tr>
<tr><td><code id="vcov.gam_+3A_unconditional">unconditional</code></td>
<td>
<p> if <code>TRUE</code> (and <code>freq==FALSE</code>) then the Bayesian 
smoothing parameter 
uncertainty corrected covariance matrix is returned, if available. </p>
</td></tr>
<tr><td><code id="vcov.gam_+3A_...">...</code></td>
<td>
<p> other arguments, currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Basically, just extracts <code>object$Ve</code>, <code>object$Vp</code> or <code>object$Vc</code> (if available) from a <code><a href="#topic+gamObject">gamObject</a></code>, unless <code>sandwich==TRUE</code> in which case the sandwich estimate is computed (with or without the squared bias component). 
</p>


<h3>Value</h3>

<p> A matrix corresponding to the estimated frequentist covariance matrix
of the model parameter estimators/coefficients, or the estimated posterior
covariance matrix of the parameters, depending on the argument <code>freq</code>.
</p>


<h3>Author(s)</h3>

<p> Henric Nilsson. 
Maintained by Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N. (2017) Generalized Additive Models: An Introductio with R (2nd ed) CRC Press
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+gam">gam</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> 
require(mgcv)
n &lt;- 100
x &lt;- runif(n)
y &lt;- sin(x*2*pi) + rnorm(n)*.2
mod &lt;- gam(y~s(x,bs="cc",k=10),knots=list(x=seq(0,1,length=10)))
diag(vcov(mod))
</code></pre>

<hr>
<h2 id='vis.gam'>Visualization of GAM objects</h2><span id='topic+vis.gam'></span><span id='topic+persp.gam'></span>

<h3>Description</h3>

<p> Produces perspective or contour plot views of <code>gam</code> model
predictions, fixing all but the values in <code>view</code> to the  values supplied in <code>cond</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vis.gam(x,view=NULL,cond=list(),n.grid=30,too.far=0,col=NA,
        color="heat",contour.col=NULL,se=-1,type="link",
        plot.type="persp",zlim=NULL,nCol=50,lp=1,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vis.gam_+3A_x">x</code></td>
<td>
<p>a <code>gam</code> object, produced by <code>gam()</code></p>
</td></tr>
<tr><td><code id="vis.gam_+3A_view">view</code></td>
<td>
<p>an array containing the names of the two main effect terms to be displayed on the 
x and y dimensions of the plot. If omitted the first two suitable terms
will be used. Note that variables coerced to factors in the model formula won't work
as view variables, and <code>vis.gam</code> can not detect that this has happened when setting defaults. 
</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_cond">cond</code></td>
<td>
<p>a named list of the values to use for the other predictor terms
(not in <code>view</code>). Variables omitted from this list will have the closest observed value to the median 
for continuous variables, or the most commonly occuring level for factors. Parametric matrix variables have 
all the entries in each column set to the observed column entry closest to the column median.
</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_n.grid">n.grid</code></td>
<td>
<p>The number of grid nodes in each direction used for calculating the 
plotted surface.</p>
</td></tr> 
<tr><td><code id="vis.gam_+3A_too.far">too.far</code></td>
<td>
<p> plot grid nodes that are too far from the points defined by the variables given in <code>view</code> 
can be excluded from the plot. <code>too.far</code> determines what is too far. The grid is scaled into the unit 
square along with the <code>view</code> variables and then grid nodes more than <code>too.far</code> from the predictor variables 
are excluded.</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_col">col</code></td>
<td>
<p>The colours for the facets of the plot. If this is <code>NA</code> then if <code>se</code>&gt;0 the facets are transparent, 
otherwise the colour scheme specified in <code>color</code> is used. If <code>col</code> is not <code>NA</code> then it is used as the facet 
colour.</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_color">color</code></td>
<td>
<p> the colour scheme to use for plots when <code>se</code>&lt;=0. One of <code>"topo"</code>, <code>"heat"</code>, <code>"cm"</code>, 
<code>"terrain"</code>, <code>"gray"</code> or <code>"bw"</code>. Schemes <code>"gray"</code> and
<code>"bw"</code> also modify the colors used when <code>se</code>&gt;0.</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_contour.col">contour.col</code></td>
<td>
<p>sets the colour of contours when using <code>plot.type="contour"</code>. Default scheme used if <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_se">se</code></td>
<td>
<p>if less than or equal to zero then only the predicted surface is plotted, but if greater than zero, then 3 
surfaces are plotted, one at the predicted values minus <code>se</code> standard errors, one at the predicted values and one at
the predicted values plus <code>se</code> standard errors.</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_type">type</code></td>
<td>
<p><code>"link"</code> to plot on linear predictor scale and <code>"response"</code> to plot on the response scale.</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_plot.type">plot.type</code></td>
<td>
<p>one of <code>"contour"</code> or <code>"persp"</code>.</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_zlim">zlim</code></td>
<td>
<p>a two item array giving the lower and upper limits for the z-axis
scale. <code>NULL</code> to choose automatically.</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_ncol">nCol</code></td>
<td>
<p>The number of colors to use in color schemes.</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_lp">lp</code></td>
<td>
<p>selects the linear predictor for models with more than one.</p>
</td></tr>
<tr><td><code id="vis.gam_+3A_...">...</code></td>
<td>
<p>other options to pass on to <code><a href="graphics.html#topic+persp">persp</a></code>,
<code><a href="Matrix.html#topic+image">image</a></code> or <code><a href="graphics.html#topic+contour">contour</a></code>. In particular <code>ticktype="detailed"</code> will add proper axes 
labelling to the plots. </p>
</td></tr>
</table>


<h3>Details</h3>

<p> The x and y limits are determined by the ranges of the terms named in <code>view</code>. If <code>se</code>&lt;=0 then 
a single (height colour coded, by default) surface is produced, otherwise three (by default see-through) meshes are produced at 
mean and +/- <code>se</code> standard errors. Parts of the x-y plane too far from
data can be excluded by setting <code>too.far</code>
</p>
<p>All options to the underlying graphics functions can be reset by passing them
as extra arguments <code>...</code>: such supplied values will always over-ride the
default values used by <code>vis.gam</code>. 
</p>


<h3>Value</h3>

<p>Simply produces a plot.</p>


<h3>WARNINGS</h3>

<p>The routine can not detect that a variable has been coerced to factor within a model formula, 
and will therefore fail if such a variable is used as a <code>view</code> variable. When setting 
default <code>view</code> variables it can not detect this situation either, which can cause failures
if the coerced variables are the first, otherwise suitable, variables encountered.
</p>


<h3>Author(s)</h3>

<p>Simon Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>
<p>Based on an original idea and design by Mike Lonergan.</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+persp">persp</a></code> and <code><a href="#topic+gam">gam</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
set.seed(0)
n&lt;-200;sig2&lt;-4
x0 &lt;- runif(n, 0, 1);x1 &lt;- runif(n, 0, 1)
x2 &lt;- runif(n, 0, 1)
y&lt;-x0^2+x1*x2 +runif(n,-0.3,0.3)
g&lt;-gam(y~s(x0,x1,x2))
old.par&lt;-par(mfrow=c(2,2))
# display the prediction surface in x0, x1 ....
vis.gam(g,ticktype="detailed",color="heat",theta=-35)  
vis.gam(g,se=2,theta=-35) # with twice standard error surfaces
vis.gam(g, view=c("x1","x2"),cond=list(x0=0.75)) # different view 
vis.gam(g, view=c("x1","x2"),cond=list(x0=.75),theta=210,phi=40,
        too.far=.07)
# ..... areas where there is no data are not plotted

# contour examples....
vis.gam(g, view=c("x1","x2"),plot.type="contour",color="heat")
vis.gam(g, view=c("x1","x2"),plot.type="contour",color="terrain")
vis.gam(g, view=c("x1","x2"),plot.type="contour",color="topo")
vis.gam(g, view=c("x1","x2"),plot.type="contour",color="cm")


par(old.par)

# Examples with factor and "by" variables

fac&lt;-rep(1:4,20)
x&lt;-runif(80)
y&lt;-fac+2*x^2+rnorm(80)*0.1
fac&lt;-factor(fac)
b&lt;-gam(y~fac+s(x))

vis.gam(b,theta=-35,color="heat") # factor example

z&lt;-rnorm(80)*0.4   
y&lt;-as.numeric(fac)+3*x^2*z+rnorm(80)*0.1
b&lt;-gam(y~fac+s(x,by=z))

vis.gam(b,theta=-35,color="heat",cond=list(z=1)) # by variable example

vis.gam(b,view=c("z","x"),theta= -135) # plot against by variable

</code></pre>

<hr>
<h2 id='XWXd'>Internal functions for discretized model matrix handling</h2><span id='topic+XWXd'></span><span id='topic+XWyd'></span><span id='topic+Xbd'></span><span id='topic+diagXVXd'></span>

<h3>Description</h3>

<p>Routines for computing with discretized model matrices as described in Wood et al. (2017) and Li and Wood (2019).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>XWXd(X,w,k,ks,ts,dt,v,qc,nthreads=1,drop=NULL,ar.stop=-1,ar.row=-1,ar.w=-1,
     lt=NULL,rt=NULL)
XWyd(X,w,y,k,ks,ts,dt,v,qc,drop=NULL,ar.stop=-1,ar.row=-1,ar.w=-1,lt=NULL)
Xbd(X,beta,k,ks,ts,dt,v,qc,drop=NULL,lt=NULL)
diagXVXd(X,V,k,ks,ts,dt,v,qc,drop=NULL,nthreads=1,lt=NULL,rt=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="XWXd_+3A_x">X</code></td>
<td>
<p>A list of the matrices containing the unique rows of model matrices for terms of a full model matrix, or the model matrices of the terms margins.
if term subsetting arguments <code>lt</code> and <code>rt</code> are non-NULL then this requires an <code>"lpip"</code> attribute: see details. The elements of <code>X</code> may
be sparse matrices of class <code>"dgCMatrix"</code>, in which case the list requires attributes <code>"r"</code> and <code>"off"</code> defining reverse indices (see details).</p>
</td></tr>
<tr><td><code id="XWXd_+3A_w">w</code></td>
<td>
<p>An n-vector of weights</p>
</td></tr>
<tr><td><code id="XWXd_+3A_y">y</code></td>
<td>
<p>n-vector of data.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_beta">beta</code></td>
<td>
<p>coefficient vector.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_k">k</code></td>
<td>
<p>A matrix whose columns are index n-vectors each selecting the rows of an X[[i]] required to create the full matrix.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_ks">ks</code></td>
<td>
<p>The ith term has index vectors <code>ks[i,1]:(ks[i,2]-1)</code>. The corresponing full model matrices are summed over.</p>
</td></tr> 
<tr><td><code id="XWXd_+3A_ts">ts</code></td>
<td>
<p>The element of <code>X</code> at which each model term starts.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_dt">dt</code></td>
<td>
<p>How many elements of <code>X</code> contribute to each term.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_v">v</code></td>
<td>
<p><code>v[[i]]</code> is Householder vector for ith term, if <code>qc[i]&gt;0</code>.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_qc">qc</code></td>
<td>
<p>if <code>qc[i]&gt;0</code> then term has a constraint.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_nthreads">nthreads</code></td>
<td>
<p>number of threads to use</p>
</td></tr>
<tr><td><code id="XWXd_+3A_drop">drop</code></td>
<td>
<p>list of columns of model matrix/parameters to drop</p>
</td></tr>
<tr><td><code id="XWXd_+3A_ar.stop">ar.stop</code></td>
<td>
<p>Negative to ignore. Otherwise sum rows <code>(ar.stop[i-1]+1):ar.stop[i]</code> of the rows selected by <code>ar.row</code> and weighted by <code>ar.w</code> to get ith row of model matrix to use.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_ar.row">ar.row</code></td>
<td>
<p>extract these rows...</p>
</td></tr>
<tr><td><code id="XWXd_+3A_ar.w">ar.w</code></td>
<td>
<p>weight by these weights, and sum up according to <code>ar.stop</code>. Used to implement AR models.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_lt">lt</code></td>
<td>
<p>use only columns of X corresponding to these model matrix terms (for left hand <code>X</code> in <code>XWXd</code>). If <code>NULL</code> set to <code>rt</code>.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_rt">rt</code></td>
<td>
<p>as <code>lt</code> for right hand <code>X</code>. If <code>NULL</code> set to <code>lt</code>. If <code>lt</code> and <code>rt</code> are <code>NULL</code> use all columns.</p>
</td></tr>
<tr><td><code id="XWXd_+3A_v">V</code></td>
<td>
<p>Coefficient covariance matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are really intended to be internal, but are exported so that they can be used in the initialization code of families without problem. They are primarily used by <code><a href="#topic+bam">bam</a></code> to implement the methods given in the references. <code>XWXd</code> produces <code class="reqn">X^TWX</code>, <code>XWy</code> produces <code class="reqn">X^TWy</code>, <code>Xbd</code> produces <code class="reqn">X\beta</code> and <code class="reqn">diagXVXd</code> produces the diagonal of <code class="reqn">XVX^T</code>.
</p>
<p>The <code>"lpip"</code> attribute of <code>X</code> is a list of the coefficient indices for each term. Required if subsetting via <code>lt</code> and <code>rt</code>.
</p>
<p><code>X</code> can be a list of sparse matrices of class <code>"dgCMatrix"</code>, in which case reverse indices are needed, mapping stored matrix rows to rows in the full matrix (that is the reverse of <code>k</code> which maps full matrix rows to the stored unique matrix rows). <code>r</code> is the same dimension as <code>k</code> while <code>off</code> is a list with as many elements as <code>k</code> has columns. <code>r</code> and  <code>off</code> are supplied as attributes to <code>X</code> . For simplicity let <code>r</code> and <code>off</code> denote a single column and element corresponding to each other: then <code>r[off[j]:(off[j+1]-1)]</code> contains the rows of the full matrix corresponding to row <code>j</code> of the stored matrix. The reverse indices are essential for efficient computation with sparse matrices. See the example code for how to create them efficiently from the forward index matrix, <code>k</code>.    
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N., Li, Z., Shaddick, G. &amp; Augustin N.H. (2017) Generalized additive models for gigadata: modelling the UK black smoke network daily data. Journal of the American Statistical Association. 112(519):1199-1210
<a href="https://doi.org/10.1080/01621459.2016.1195744">doi:10.1080/01621459.2016.1195744</a>
</p>
<p>Li, Z &amp; S.N. Wood (2019) Faster model matrix crossproducts for large generalized linear models with discretized covariates. Statistics and Computing.
<a href="https://doi.org/10.1007/s11222-019-09864-2">doi:10.1007/s11222-019-09864-2</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(mgcv);library(Matrix)
  ## simulate some data creating a marginal matrix sequence...
  set.seed(0);n &lt;- 4000
  dat &lt;- gamSim(1,n=n,dist="normal",scale=2)
  dat$x4 &lt;- runif(n)
  dat$y &lt;- dat$y + 3*exp(dat$x4*15-5)/(1+exp(dat$x4*15-5))
  dat$fac &lt;- factor(sample(1:20,n,replace=TRUE))
  G &lt;- gam(y ~ te(x0,x2,k=5,bs="bs",m=1)+s(x1)+s(x4)+s(x3,fac,bs="fs"),
           fit=FALSE,data=dat,discrete=TRUE)
  p &lt;- ncol(G$X)
  ## create a sparse version...
  Xs &lt;- list(); r &lt;- G$kd*0; off &lt;- list()
  for (i in 1:length(G$Xd)) Xs[[i]] &lt;- as(G$Xd[[i]],"dgCMatrix")
  for (j in 1:nrow(G$ks)) { ## create the reverse indices...
    nr &lt;- nrow(Xs[[j]]) ## make sure we always tab to final stored row 
    for (i in G$ks[j,1]:(G$ks[j,2]-1)) {
      r[,i] &lt;- (1:length(G$kd[,i]))[order(G$kd[,i])]
      off[[i]] &lt;- cumsum(c(1,tabulate(G$kd[,i],nbins=nr)))-1
    }
  }
  attr(Xs,"off") &lt;- off;attr(Xs,"r") &lt;- r 

  par(mfrow=c(2,3))

  beta &lt;- runif(p)
  Xb0 &lt;- Xbd(G$Xd,beta,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
  Xb1 &lt;- Xbd(Xs,beta,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
  range(Xb0-Xb1);plot(Xb0,Xb1,pch=".")

  bb &lt;- cbind(beta,beta+runif(p)*.3)
  Xb0 &lt;- Xbd(G$Xd,bb,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
  Xb1 &lt;- Xbd(Xs,bb,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
  range(Xb0-Xb1);plot(Xb0,Xb1,pch=".")
  
  w &lt;- runif(n)
  XWy0 &lt;- XWyd(G$Xd,w,y=dat$y,G$kd,G$ks,G$ts,G$dt,G$v,G$qc) 
  XWy1 &lt;- XWyd(Xs,w,y=dat$y,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
  range(XWy1-XWy0);plot(XWy1,XWy0,pch=".")

  yy &lt;- cbind(dat$y,dat$y+runif(n)-.5)
  XWy0 &lt;- XWyd(G$Xd,w,y=yy,G$kd,G$ks,G$ts,G$dt,G$v,G$qc) 
  XWy1 &lt;- XWyd(Xs,w,y=yy,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
  range(XWy1-XWy0);plot(XWy1,XWy0,pch=".")

  A &lt;- XWXd(G$Xd,w,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
  B &lt;- XWXd(Xs,w,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
  range(A-B);plot(A,B,pch=".")

  V &lt;- crossprod(matrix(runif(p*p),p,p))
  ii &lt;- c(20:30,100:200)
  jj &lt;- c(50:90,150:160)
  V[ii,jj] &lt;- 0;V[jj,ii] &lt;- 0
  d1 &lt;- diagXVXd(G$Xd,V,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
  Vs &lt;- as(V,"dgCMatrix")
  d2 &lt;- diagXVXd(Xs,Vs,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
  range(d1-d2);plot(d1,d2,pch=".")
</code></pre>

<hr>
<h2 id='ziP'>GAM zero-inflated (hurdle) Poisson regression family</h2><span id='topic+ziP'></span>

<h3>Description</h3>

<p>Family for use with <code><a href="#topic+gam">gam</a></code> or <code><a href="#topic+bam">bam</a></code>, implementing regression for zero inflated Poisson data
when the complimentary log log of the zero probability is linearly dependent on the log of the Poisson parameter. Use with great care, noting that simply having many zero response observations is not an indication of zero inflation: the question is whether you have too many zeroes given the specified model.
</p>
<p>This sort of model is really only appropriate when none of your covariates help to explain the zeroes in your data. If your covariates predict which observations are likely to have zero mean then adding a zero inflated model on top of this is likely to lead to identifiability problems. Identifiability problems may lead to fit failures, or absurd values for the linear predictor or predicted values. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ziP(theta = NULL, link = "identity",b=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ziP_+3A_theta">theta</code></td>
<td>
<p>the 2 parameters controlling the slope and intercept of the 
linear transform of the mean controlling the zero inflation rate. If supplied
then treated as fixed parameters (<code class="reqn">\theta_1</code> and <code class="reqn">\theta_2</code>), otherwise estimated.</p>
</td></tr>
<tr><td><code id="ziP_+3A_link">link</code></td>
<td>
<p>The link function: only the <code>"identity"</code> is currently supported.</p>
</td></tr>
<tr><td><code id="ziP_+3A_b">b</code></td>
<td>
<p>a non-negative constant, specifying the minimum dependence of the zero inflation rate on the linear predictor.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probability of a zero count is given by <code class="reqn">1-p</code>, whereas the probability of
count <code class="reqn">y&gt;0</code> is given by the truncated Poisson probability function <code class="reqn">p\mu^y/((\exp(\mu)-1)y!)</code>. The linear predictor 
gives <code class="reqn">\log \mu</code>, while <code class="reqn">\eta = \log(-\log(1-p)) </code> and <code class="reqn">\eta = \theta_1 + \{b+\exp(\theta_2)\} \log \mu </code>. The <code>theta</code> parameters are estimated alongside the smoothing parameters. Increasing the <code>b</code> parameter from zero can greatly reduce identifiability problems, particularly when there are very few non-zero data.  
</p>
<p>The fitted values for this model are the log of the Poisson parameter. Use the <code>predict</code> function with <code>type=="response"</code> to get the predicted expected response. Note that the theta parameters reported in model summaries are <code class="reqn">\theta_1</code> and <code class="reqn">b + \exp(\theta_2)</code>.
</p>
<p>These models should be subject to very careful checking, especially if fitting has not converged. It is quite easy to set up models with identifiability problems, particularly if the data are not really zero inflated, but simply have many zeroes because the mean is very low in some parts of the covariate space. See example for some obvious checks. Take convergence warnings seriously.
</p>


<h3>Value</h3>

<p>An object of class <code>extended.family</code>.
</p>


<h3>WARNINGS </h3>

<p>Zero inflated models are often over-used. Having lots of zeroes in the data does not in itself imply zero inflation. Having too many zeroes *given the model mean* may imply zero inflation. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ziplss">ziplss</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
rzip &lt;- function(gamma,theta= c(-2,.3)) {
## generate zero inflated Poisson random variables, where 
## lambda = exp(gamma), eta = theta[1] + exp(theta[2])*gamma
## and 1-p = exp(-exp(eta)).
   y &lt;- gamma; n &lt;- length(y)
   lambda &lt;- exp(gamma)
   eta &lt;- theta[1] + exp(theta[2])*gamma
   p &lt;- 1- exp(-exp(eta))
   ind &lt;- p &gt; runif(n)
   y[!ind] &lt;- 0
   np &lt;- sum(ind)
   ## generate from zero truncated Poisson, given presence...
   y[ind] &lt;- qpois(runif(np,dpois(0,lambda[ind]),1),lambda[ind])
   y
} 

library(mgcv)
## Simulate some ziP data...
set.seed(1);n&lt;-400
dat &lt;- gamSim(1,n=n)
dat$y &lt;- rzip(dat$f/4-1)

b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ziP(),data=dat)

b$outer.info ## check convergence!!
b
plot(b,pages=1)
plot(b,pages=1,unconditional=TRUE) ## add s.p. uncertainty 
gam.check(b)
## more checking...
## 1. If the zero inflation rate becomes decoupled from the linear predictor, 
## it is possible for the linear predictor to be almost unbounded in regions
## containing many zeroes. So examine if the range of predicted values 
## is sane for the zero cases? 
range(predict(b,type="response")[b$y==0])

## 2. Further plots...
par(mfrow=c(2,2))
plot(predict(b,type="response"),residuals(b))
plot(predict(b,type="response"),b$y);abline(0,1,col=2)
plot(b$linear.predictors,b$y)
qq.gam(b,rep=20,level=1)

## 3. Refit fixing the theta parameters at their estimated values, to check we 
## get essentially the same fit...
thb &lt;- b$family$getTheta()
b0 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ziP(theta=thb),data=dat)
b;b0

## Example fit forcing minimum linkage of prob present and
## linear predictor. Can fix some identifiability problems.
b2 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ziP(b=.3),data=dat)

</code></pre>

<hr>
<h2 id='ziplss'>Zero inflated (hurdle) Poisson location-scale model family</h2><span id='topic+ziplss'></span><span id='topic+zipll'></span>

<h3>Description</h3>

<p>The <code>ziplss</code> family implements a zero inflated (hurdle) Poisson model in which one linear predictor 
controls the probability of presence  and the other controls the mean given presence.
Useable only with <code><a href="#topic+gam">gam</a></code>, the linear predictors are specified via a list of formulae. 
Should be used with care: simply having a large number of zeroes is not an indication of zero inflation. 
</p>
<p>Requires integer count data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ziplss(link=list("identity","identity"))
zipll(y,g,eta,deriv=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ziplss_+3A_link">link</code></td>
<td>
<p>two item list specifying the link - currently only identity links are possible, as 
parameterization is directly in terms of log of Poisson response and complementary log log of probability of presence.</p>
</td></tr>
<tr><td><code id="ziplss_+3A_y">y</code></td>
<td>
<p>response</p>
</td></tr>
<tr><td><code id="ziplss_+3A_g">g</code></td>
<td>
<p>gamma vector</p>
</td></tr>
<tr><td><code id="ziplss_+3A_eta">eta</code></td>
<td>
<p>eta vector</p>
</td></tr>
<tr><td><code id="ziplss_+3A_deriv">deriv</code></td>
<td>
<p>number of derivatives to compute</p>
</td></tr>
</table>


<h3>Details</h3>

 <p><code>ziplss</code> is used with <code><a href="#topic+gam">gam</a></code> to fit 2 stage zero inflated Poisson models. <code>gam</code> is called with 
a list containing 2 formulae, the first specifies the response on the left hand side and the structure of the linear predictor for the Poisson parameter on the right hand side. The second is one sided, specifying the linear predictor for the probability of presence on the right hand side. 
</p>
<p>The fitted values for this family will be a two column matrix. The first column is the log of the Poisson parameter, 
and the second column is the complementary log log of probability of presence.. 
Predictions using <code><a href="#topic+predict.gam">predict.gam</a></code> will also produce 2 column matrices for <code>type</code> 
<code>"link"</code> and <code>"response"</code>.
</p>
<p>The null deviance computed for this model assumes that a single probability of presence and a single Poisson parameter 
are estimated. 
</p>
<p>For data with large areas of covariate space over which the response is zero it may be advisable to use low order penalties to 
avoid problems. For 1D smooths uses e.g. <code>s(x,m=1)</code> and for isotropic smooths use <code><a href="#topic+Duchon.spline">Duchon.spline</a></code>s in place of thin plaste terms with order 1 penalties, e.g <code>s(x,z,m=c(1,.5))</code> &mdash; such smooths penalize towards constants, thereby avoiding extreme estimates when the data are uninformative.
</p>
<p><code>zipll</code> is a function used by <code>ziplss</code>, exported only to allow external use of the <code>ziplss</code> family. It is not usually called directly. 
</p>


<h3>Value</h3>

<p>For <code>ziplss</code> An object inheriting from class <code>general.family</code>.
</p>


<h3>WARNINGS </h3>

<p>Zero inflated models are often over-used. Having many zeroes in the data does not in itself imply zero inflation. Having too many zeroes *given the model mean* may imply zero inflation. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>


<h3>References</h3>

<p>Wood, S.N., N. Pya and B. Saefken (2016), Smoothing parameter and
model selection for general smooth models.
Journal of the American Statistical Association 111, 1548-1575
<a href="https://doi.org/10.1080/01621459.2016.1180986">doi:10.1080/01621459.2016.1180986</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mgcv)
## simulate some data...
f0 &lt;- function(x) 2 * sin(pi * x); f1 &lt;- function(x) exp(2 * x)
f2 &lt;- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
            (10 * x)^3 * (1 - x)^10
n &lt;- 500;set.seed(5)
x0 &lt;- runif(n); x1 &lt;- runif(n)
x2 &lt;- runif(n); x3 &lt;- runif(n)

## Simulate probability of potential presence...
eta1 &lt;- f0(x0) + f1(x1) - 3
p &lt;- binomial()$linkinv(eta1) 
y &lt;- as.numeric(runif(n)&lt;p) ## 1 for presence, 0 for absence

## Simulate y given potentially present (not exactly model fitted!)...
ind &lt;- y&gt;0
eta2 &lt;- f2(x2[ind])/3
y[ind] &lt;- rpois(exp(eta2),exp(eta2))

## Fit ZIP model... 
b &lt;- gam(list(y~s(x2)+s(x3),~s(x0)+s(x1)),family=ziplss())
b$outer.info ## check convergence

summary(b) 
plot(b,pages=1)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
