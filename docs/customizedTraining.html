<!DOCTYPE html><html><head><title>Help for package customizedTraining</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {customizedTraining}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#customizedGlmnet'>
<p>fit glmnet using customized training</p></a></li>
<li><a href='#cv.customizedGlmnet'>
<p>cross validation for customizedGlmnet</p></a></li>
<li><a href='#nonzero'>
<p>return selected variables</p></a></li>
<li><a href='#nonzero.customizedGlmnet'>
<p>return selected variables from a <code>customizedGlmnet</code> object</p></a></li>
<li><a href='#nonzero.singleton'>
<p>return selected variables from a <code>singleton</code> object</p></a></li>
<li><a href='#plot.customizedGlmnet'>
<p>visualize variables selected in each customized training subset</p></a></li>
<li><a href='#plot.cv.customizedGlmnet'>
<p>visualize variables selected in each customized training subset, from a</p>
cross-validated model</a></li>
<li><a href='#predict.customizedGlmnet'>
<p>make predictions from a <code>customizedGlmnet</code> object</p></a></li>
<li><a href='#predict.cv.customizedGlmnet'>
<p>make predictions from a <code>cv.customizedGlmnet</code> object</p></a></li>
<li><a href='#predict.singleton'>
<p>make predictions from a &ldquo;singleton&rdquo; object</p></a></li>
<li><a href='#print.customizedGlmnet'>
<p>print the summary of a fitted <code>customizedGlmnet</code> object</p></a></li>
<li><a href='#print.cv.customizedGlmnet'>
<p>print a &ldquo;cv.customizedGlmnet&rdquo; object</p></a></li>
<li><a href='#Vowel'>
<p>Vowel Recognition</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Customized Training for Lasso and Elastic-Net Regularized
Generalized Linear Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-01-28</td>
</tr>
<tr>
<td>Author:</td>
<td>Scott Powers, Trevor Hastie, Robert Tibshirani</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Scott Powers &lt;saberpowers@gmail.com&gt;</td>
</tr>
<tr>
<td>Imports:</td>
<td>FNN, glmnet</td>
</tr>
<tr>
<td>Description:</td>
<td>Customized training is a simple technique for transductive
    learning, when the test covariates are known at the time of training. The
    method identifies a subset of the training set to serve as the training set
    for each of a few identified subsets in the training set. This package
    implements customized training for the glmnet() and cv.glmnet() functions.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-01-29 02:34:22 UTC; sspowers</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-01-29 08:30:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='customizedGlmnet'>
fit glmnet using customized training
</h2><span id='topic+customizedGlmnet'></span>

<h3>Description</h3>

<p>Fit a regularized lasso model using customized training
</p>


<h3>Usage</h3>

<pre><code class='language-R'>customizedGlmnet(xTrain, yTrain, xTest, groupid = NULL, G = NULL,
    family = c("gaussian", "binomial", "multinomial"), dendrogram = NULL,
    dendrogramTestIndices = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="customizedGlmnet_+3A_xtrain">xTrain</code></td>
<td>

<p>an n-by-p matrix of training covariates
</p>
</td></tr>
<tr><td><code id="customizedGlmnet_+3A_ytrain">yTrain</code></td>
<td>

<p>a length-n vector of training responses. Numeric for family = <code>"gaussian"</code>.
Factor or character for <code>family = "binomial"</code> or
<code>family = "multinomial"</code>
</p>
</td></tr>
<tr><td><code id="customizedGlmnet_+3A_xtest">xTest</code></td>
<td>

<p>an m-by-p matrix of test covariates
</p>
</td></tr>
<tr><td><code id="customizedGlmnet_+3A_groupid">groupid</code></td>
<td>

<p>an optional length-m vector of group memberships for the test set. If
specified, customized training subsets are identified using the union of
nearest neighbor sets for each test group. Either <code>groupid</code> or <code>G</code>
must be specified
</p>
</td></tr>
<tr><td><code id="customizedGlmnet_+3A_g">G</code></td>
<td>

<p>a positive integer indicating the number of clusters for the joint clustering
of the test and training data. Ignored if <code>groupid</code> is specified. Either
<code>groupid</code> or <code>G</code> must be specified
</p>
</td></tr>
<tr><td><code id="customizedGlmnet_+3A_family">family</code></td>
<td>

<p>response type
</p>
</td></tr>
<tr><td><code id="customizedGlmnet_+3A_dendrogram">dendrogram</code></td>
<td>

<p>optional output from <code>hclust</code> on the joint covariate data. Used by
<code>cv.customizedGlmnet</code> so that clustering is not computed redundantly
</p>
</td></tr>
<tr><td><code id="customizedGlmnet_+3A_dendrogramtestindices">dendrogramTestIndices</code></td>
<td>

<p>optional set of indices (corresponding to dendrogram) held out in
cross-validation. Used by <code>cv.customizedGlmnet</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identify customized training subsets of the training data through one of two
methods: (1) If groupid is specified, grouping the test data, then for each
test group find the 10 nearest neighbors of each observation in the group and
use the union of these nearest neighbor sets as the customized training set or
(2) If G is specified, jointly cluster the test and training data using
hierarchical clustering with complete linkage. Within each cluster, the
training data are used as the customized training subset for the test data.
Once the customized training subsets have been identified, use glmnet to fit an
l1-regularized regression model to each.
</p>


<h3>Value</h3>

<p>an object with class <code>customizedGlmnet</code>
</p>
<table>
<tr><td><code>call</code></td>
<td>

<p>the call that produced this object
</p>
</td></tr>
<tr><td><code>CTset</code></td>
<td>

<p>a list containing the customized training subsets for each test group
</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>

<p>a list containing the glmnet fit for each test group
</p>
</td></tr>
<tr><td><code>groupid</code></td>
<td>

<p>a length-m vector containing the group memberships of the test data
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>

<p>a list containing <code>train</code> (which is the input <code>xTrain</code>) and
<code>test</code> (which is the input <code>xTest</code>). Specified in function call
</p>
</td></tr>
<tr><td><code>y</code></td>
<td>

<p>training response vector (specified in function call)
</p>
</td></tr>
<tr><td><code>family</code></td>
<td>

<p>response type (specified in function call)
</p>
</td></tr>
<tr><td><code>standard</code></td>
<td>

<p>the fit of <code>glmnet</code> to the entire training set using standard training
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Robert Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Robert Tibshirani (2015) &quot;Customized training
with an application to mass specrometric imaging of gastric cancer data.&quot;
Annals of Applied Statistics 9, 4:1709-1725.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+print.customizedGlmnet">print.customizedGlmnet</a></code>, <code><a href="#topic+predict.customizedGlmnet">predict.customizedGlmnet</a></code>,
<code><a href="#topic+plot.customizedGlmnet">plot.customizedGlmnet</a></code>, <code><a href="#topic+cv.customizedGlmnet">cv.customizedGlmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(glmnet)

# Simulate synthetic data

n = m = 150
p = 50
q = 5
K = 3
sigmaC = 10
sigmaX = sigmaY = 1
set.seed(5914)

beta = matrix(0, nrow = p, ncol = K)
for (k in 1:K) beta[sample(1:p, q), k] = 1
c = matrix(rnorm(K*p, 0, sigmaC), K, p)
eta = rnorm(K)
pi = (exp(eta)+1)/sum(exp(eta)+1)
z = t(rmultinom(m + n, 1, pi))
x = crossprod(t(z), c) + matrix(rnorm((m + n)*p, 0, sigmaX), m + n, p)
y = rowSums(z*(crossprod(t(x), beta))) + rnorm(m + n, 0, sigmaY)

x.train = x[1:n, ]
y.train = y[1:n]
x.test = x[n + 1:m, ]
y.test = y[n + 1:m]


# Example 1: Use clustering to fit the customized training model to training
# and test data with no predefined test-set blocks

fit1 = customizedGlmnet(x.train, y.train, x.test, G = 3,
    family = "gaussian")

# Print the customized training model fit:
fit1

# Extract nonzero regression coefficients for each group:
nonzero(fit1, lambda = 10)

# Compute test error using the predict function:
mean((y.test - predict(fit1, lambda = 10))^2)

# Plot nonzero coefficients by group:
plot(fit1, lambda = 10)


# Example 2: If the test set has predefined blocks, use these blocks to define
# the customized training sets, instead of using clustering.
group.id = apply(z == 1, 1, which)[n + 1:m]

fit2 = customizedGlmnet(x.train, y.train, x.test, group.id)

# Print the customized training model fit:
fit2

# Extract nonzero regression coefficients for each group:
nonzero(fit2, lambda = 10)

# Compute test error using the predict function:
mean((y.test - predict(fit2, lambda = 10))^2)

# Plot nonzero coefficients by group:
plot(fit2, lambda = 10)
</code></pre>

<hr>
<h2 id='cv.customizedGlmnet'>
cross validation for customizedGlmnet
</h2><span id='topic+cv.customizedGlmnet'></span>

<h3>Description</h3>

<p>Does k-fold cross-validation for customizedGlmnet and returns a values for
<code>G</code> and <code>lambda</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.customizedGlmnet(xTrain, yTrain, xTest = NULL, groupid = NULL, Gs = NULL,
    dendrogram = NULL, dendrogramCV = NULL, lambda = NULL,
    nfolds = 10, foldid = NULL, keep = FALSE,
    family = c("gaussian", "binomial", "multinomial"), verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.customizedGlmnet_+3A_xtrain">xTrain</code></td>
<td>

<p>an n-by-p matrix of training covariates
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_ytrain">yTrain</code></td>
<td>

<p>a length-n vector of training responses. Numeric for family = <code>"gaussian"</code>.
Factor or character for <code>family = "binomial"</code> or
<code>family = "multinomial"</code>
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_xtest">xTest</code></td>
<td>

<p>an m-by-p matrix of test covariates. May be left NULL, in which case cross
validation predictions are made internally on the training set and no test
predictions are returned.
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_groupid">groupid</code></td>
<td>

<p>an optional length-m vector of group memberships for the test set. If
specified, customized training subsets are identified using the union of
nearest neighbor sets for each test group, in which case cross-validation is
used only to select the regularization parameter <code>lambda</code>, not the number
of clusters <code>G</code>. Either <code>groupid</code> or <code>Gs</code> must be specified
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_gs">Gs</code></td>
<td>

<p>a vector of positive integers indicating the numbers of clusters over which to
perform cross-validation to determine the best number. Ignored if <code>groupid</code>
is specified. Either <code>groupid</code> or <code>Gs</code> must be specified
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_dendrogram">dendrogram</code></td>
<td>

<p>optional output from <code>hclust</code> on the joint covariate data. Useful if method
is being used several times to avoid redundancy in calculations
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_dendrogramcv">dendrogramCV</code></td>
<td>

<p>optional output from <code>hclust</code> on the training covariate data. Used as joint
clustering result for cross-validation. Useful to specify in advance if method
is being used several times to avoid redundancy in calculations
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_lambda">lambda</code></td>
<td>

<p>sequence of values to use for the regularization parameter lambda. Recomended
to leave as NULL and allow <code>glmnet</code> to choose automatically.
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_nfolds">nfolds</code></td>
<td>

<p>number of folds &ndash; default is 10. Ignored if foldid is specified
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_foldid">foldid</code></td>
<td>

<p>an optional length-n vector of fold memberships used for cross-validation
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_keep">keep</code></td>
<td>

<p>Should fitted values on the training set from cross validation be included in
output? Default is FALSE.
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_family">family</code></td>
<td>

<p>response type
</p>
</td></tr>
<tr><td><code id="cv.customizedGlmnet_+3A_verbose">verbose</code></td>
<td>

<p>Should progress be printed to console as folds are evaluated during
cross-validation? Default is FALSE.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>cv.customizedGlmnet</code>
</p>
<table>
<tr><td><code>call</code></td>
<td>

<p>the call that produced this object
</p>
</td></tr>
<tr><td><code>G.min</code></td>
<td>

<p>unless groupid is specified, the number of clusters minimizing CV error
</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>

<p>the sequence of values of the regularization parameter <code>lambda</code> considered
</p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>

<p>the value of the regularization parameter <code>lambda</code> minimizing CV error
</p>
</td></tr>
<tr><td><code>error</code></td>
<td>

<p>a matrix containing the CV error for each <code>G</code> and <code>lambda</code>
</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>

<p>a <code>customizedGlmnet</code> object fit using <code>G.min</code> and <code>lambda.min</code>.
Only returned if <code>xTest</code> is not NULL.
</p>
</td></tr>
<tr><td><code>prediction</code></td>
<td>

<p>a length-m vector of predictions for the test set, using the tuning parameters
which minimize cross-validation error. Only returned if <code>xTest</code> is not
NULL.
</p>
</td></tr>
<tr><td><code>selected</code></td>
<td>

<p>a list of nonzero variables for each customized training set, using
<code>G.min</code> and <code>lambda.min</code>. Only returned if <code>xTest</code> is not NULL.
</p>
</td></tr>
<tr><td><code>cv.fit</code></td>
<td>

<p>a array containing fitted values on the training set from cross validation.
Only returned if <code>keep</code> is TRUE.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Robert Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Robert Tibshirani (2015) &quot;Customized training
with an application to mass specrometric imaging of gastric cancer data.&quot;
Annals of Applied Statistics 9, 4:1709-1725.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+customizedGlmnet">customizedGlmnet</a></code>, <code><a href="#topic+plot.cv.customizedGlmnet">plot.cv.customizedGlmnet</a></code>,
<code><a href="#topic+predict.cv.customizedGlmnet">predict.cv.customizedGlmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(glmnet)

# Simulate synthetic data

n = m = 150
p = 50
q = 5
K = 3
sigmaC = 10
sigmaX = sigmaY = 1
set.seed(5914)

beta = matrix(0, nrow = p, ncol = K)
for (k in 1:K) beta[sample(1:p, q), k] = 1
c = matrix(rnorm(K*p, 0, sigmaC), K, p)
eta = rnorm(K)
pi = (exp(eta)+1)/sum(exp(eta)+1)
z = t(rmultinom(m + n, 1, pi))
x = crossprod(t(z), c) + matrix(rnorm((m + n)*p, 0, sigmaX), m + n, p)
y = rowSums(z*(crossprod(t(x), beta))) + rnorm(m + n, 0, sigmaY)

x.train = x[1:n, ]
y.train = y[1:n]
x.test = x[n + 1:m, ]
y.test = y[n + 1:m]
foldid = sample(rep(1:10, length = nrow(x.train)))


# Example 1: Use clustering to fit the customized training model to training
# and test data with no predefined test-set blocks

fit1 = cv.customizedGlmnet(x.train, y.train, x.test, Gs = c(1, 2, 3, 5),
    family = "gaussian", foldid = foldid)

# Print the optimal number of groups and value of lambda:
fit1$G.min
fit1$lambda.min

# Print the customized training model fit:
fit1

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit1))^2)

# Plot nonzero coefficients by group:
plot(fit1)


# Example 2: If the test set has predefined blocks, use these blocks to define
# the customized training sets, instead of using clustering.
foldid = apply(z == 1, 1, which)[1:n]
group.id = apply(z == 1, 1, which)[n + 1:m]

fit2 = cv.customizedGlmnet(x.train, y.train, x.test, group.id, foldid = foldid)

# Print the optimal value of lambda:
fit2$lambda.min

# Print the customized training model fit:
fit2

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit2))^2)

# Plot nonzero coefficients by group:
plot(fit2)


# Example 3: If there is no test set, but the training set is organized into
# blocks, you can do cross validation with these blocks as the basis for the
# customized training sets.

fit3 = cv.customizedGlmnet(x.train, y.train, foldid = foldid)

# Print the optimal value of lambda:
fit3$lambda.min

# Print the customized training model fit:
fit3

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit3))^2)

# Plot nonzero coefficients by group:
plot(fit3)
</code></pre>

<hr>
<h2 id='nonzero'>
return selected variables
</h2><span id='topic+nonzero'></span>

<h3>Description</h3>

<p><code>nonzero</code> is a generic function for returning the set of variables
selected by a model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nonzero(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nonzero_+3A_object">object</code></td>
<td>

<p>a model object for which the set of selected variables is desired
</p>
</td></tr>
<tr><td><code id="nonzero_+3A_...">...</code></td>
<td>

<p>additional arguments, e.g. tuning parameters
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The form of the value returned by <code>nonzero</code> depends on the class of its
argument. See the documentation of the particular methods for details of what
is produced by that method.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nonzero.customizedGlmnet">nonzero.customizedGlmnet</a></code>, <code><a href="#topic+nonzero.singleton">nonzero.singleton</a></code>
</p>

<hr>
<h2 id='nonzero.customizedGlmnet'>
return selected variables from a <code>customizedGlmnet</code> object
</h2><span id='topic+nonzero.customizedGlmnet'></span>

<h3>Description</h3>

<p>Returns a list of vectors of selected variables for each separate glmnet model
fit by a <code>customizedGlmnet</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'customizedGlmnet'
nonzero(object, lambda = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nonzero.customizedGlmnet_+3A_object">object</code></td>
<td>

<p>fitted <code>customizedGlmnet</code> model object
</p>
</td></tr>
<tr><td><code id="nonzero.customizedGlmnet_+3A_lambda">lambda</code></td>
<td>

<p>value of regularization parameter to use. Must be specified
</p>
</td></tr>
<tr><td><code id="nonzero.customizedGlmnet_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of vectors, each vectors representing one of the <code>glmnet</code> models
fit by the <code>customizedGlmnet</code> model. Each vector gives the indices of the
variables selected by the model
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Robert Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Robert Tibshirani (2015) &quot;Customized training
with an application to mass specrometric imaging of gastric cancer data.&quot;
Annals of Applied Statistics 9, 4:1709-1725.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nonzero">nonzero</a></code>, <code><a href="#topic+customizedGlmnet">customizedGlmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(glmnet)

# Simulate synthetic data

n = m = 150
p = 50
q = 5
K = 3
sigmaC = 10
sigmaX = sigmaY = 1
set.seed(5914)

beta = matrix(0, nrow = p, ncol = K)
for (k in 1:K) beta[sample(1:p, q), k] = 1
c = matrix(rnorm(K*p, 0, sigmaC), K, p)
eta = rnorm(K)
pi = (exp(eta)+1)/sum(exp(eta)+1)
z = t(rmultinom(m + n, 1, pi))
x = crossprod(t(z), c) + matrix(rnorm((m + n)*p, 0, sigmaX), m + n, p)
y = rowSums(z*(crossprod(t(x), beta))) + rnorm(m + n, 0, sigmaY)

x.train = x[1:n, ]
y.train = y[1:n]
x.test = x[n + 1:m, ]
y.test = y[n + 1:m]


# Example 1: Use clustering to fit the customized training model to training
# and test data with no predefined test-set blocks

fit1 = customizedGlmnet(x.train, y.train, x.test, G = 3,
    family = "gaussian")

# Extract nonzero regression coefficients for each group:
nonzero(fit1, lambda = 10)


# Example 2: If the test set has predefined blocks, use these blocks to define
# the customized training sets, instead of using clustering.
group.id = apply(z == 1, 1, which)[n + 1:m]

fit2 = customizedGlmnet(x.train, y.train, x.test, group.id)

# Extract nonzero regression coefficients for each group:
nonzero(fit2, lambda = 10)
</code></pre>

<hr>
<h2 id='nonzero.singleton'>
return selected variables from a <code>singleton</code> object
</h2><span id='topic+nonzero.singleton'></span>

<h3>Description</h3>

<p>Returns NULL. Intended for internal use only
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'singleton'
nonzero(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nonzero.singleton_+3A_object">object</code></td>
<td>

<p>an object of class <code>singleton</code>
</p>
</td></tr>
<tr><td><code id="nonzero.singleton_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nonzero">nonzero</a></code>
</p>

<hr>
<h2 id='plot.customizedGlmnet'>
visualize variables selected in each customized training subset
</h2><span id='topic+plot.customizedGlmnet'></span>

<h3>Description</h3>

<p>Produces a plot, with a row for each customized training submodel, showing
the variables selected in the subset, with variables along the horizonal axis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'customizedGlmnet'
plot(x, lambda, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.customizedGlmnet_+3A_x">x</code></td>
<td>

<p>a fitted <code>customizedGlmnet</code> object
</p>
</td></tr>
<tr><td><code id="plot.customizedGlmnet_+3A_lambda">lambda</code></td>
<td>

<p>regularization parameter. Required
</p>
</td></tr>
<tr><td><code id="plot.customizedGlmnet_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Robert Tibshirani
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+plot">plot</a></code>, <code><a href="#topic+customizedGlmnet">customizedGlmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(glmnet)

# Simulate synthetic data

n = m = 150
p = 50
q = 5
K = 3
sigmaC = 10
sigmaX = sigmaY = 1
set.seed(5914)

beta = matrix(0, nrow = p, ncol = K)
for (k in 1:K) beta[sample(1:p, q), k] = 1
c = matrix(rnorm(K*p, 0, sigmaC), K, p)
eta = rnorm(K)
pi = (exp(eta)+1)/sum(exp(eta)+1)
z = t(rmultinom(m + n, 1, pi))
x = crossprod(t(z), c) + matrix(rnorm((m + n)*p, 0, sigmaX), m + n, p)
y = rowSums(z*(crossprod(t(x), beta))) + rnorm(m + n, 0, sigmaY)

x.train = x[1:n, ]
y.train = y[1:n]
x.test = x[n + 1:m, ]
y.test = y[n + 1:m]


# Example 1: Use clustering to fit the customized training model to training
# and test data with no predefined test-set blocks

fit1 = customizedGlmnet(x.train, y.train, x.test, G = 3,
    family = "gaussian")

# Plot nonzero coefficients by group:
plot(fit1, lambda = 10)


# Example 2: If the test set has predefined blocks, use these blocks to define
# the customized training sets, instead of using clustering.
group.id = apply(z == 1, 1, which)[n + 1:m]

fit2 = customizedGlmnet(x.train, y.train, x.test, group.id)

# Plot nonzero coefficients by group:
plot(fit2, lambda = 10)
</code></pre>

<hr>
<h2 id='plot.cv.customizedGlmnet'>
visualize variables selected in each customized training subset, from a
cross-validated model
</h2><span id='topic+plot.cv.customizedGlmnet'></span>

<h3>Description</h3>

<p>Produces a plot, with a row for each customized training submodel, showing
the variables selected in the subset, with variables along the horizonal axis.
The lambda used is the one which minimizes CV error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.customizedGlmnet'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cv.customizedGlmnet_+3A_x">x</code></td>
<td>

<p>a fitted <code>cv.customizedGlmnet</code> object
</p>
</td></tr>
<tr><td><code id="plot.cv.customizedGlmnet_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Robert Tibshirani
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+plot">plot</a></code>, <code><a href="#topic+cv.customizedGlmnet">cv.customizedGlmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(glmnet)

# Simulate synthetic data

n = m = 150
p = 50
q = 5
K = 3
sigmaC = 10
sigmaX = sigmaY = 1
set.seed(5914)

beta = matrix(0, nrow = p, ncol = K)
for (k in 1:K) beta[sample(1:p, q), k] = 1
c = matrix(rnorm(K*p, 0, sigmaC), K, p)
eta = rnorm(K)
pi = (exp(eta)+1)/sum(exp(eta)+1)
z = t(rmultinom(m + n, 1, pi))
x = crossprod(t(z), c) + matrix(rnorm((m + n)*p, 0, sigmaX), m + n, p)
y = rowSums(z*(crossprod(t(x), beta))) + rnorm(m + n, 0, sigmaY)

x.train = x[1:n, ]
y.train = y[1:n]
x.test = x[n + 1:m, ]
y.test = y[n + 1:m]
foldid = sample(rep(1:10, length = nrow(x.train)))


# Example 1: Use clustering to fit the customized training model to training
# and test data with no predefined test-set blocks

fit1 = cv.customizedGlmnet(x.train, y.train, x.test, Gs = c(1, 2, 3, 5),
    family = "gaussian", foldid = foldid)

# Print the optimal number of groups and value of lambda:
fit1$G.min
fit1$lambda.min

# Print the customized training model fit:
fit1

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit1))^2)

# Plot nonzero coefficients by group:
plot(fit1)


# Example 2: If the test set has predefined blocks, use these blocks to define
# the customized training sets, instead of using clustering.
foldid = apply(z == 1, 1, which)[1:n]
group.id = apply(z == 1, 1, which)[n + 1:m]

fit2 = cv.customizedGlmnet(x.train, y.train, x.test, group.id, foldid = foldid)

# Print the optimal value of lambda:
fit2$lambda.min

# Print the customized training model fit:
fit2

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit2))^2)

# Plot nonzero coefficients by group:
plot(fit2)


# Example 3: If there is no test set, but the training set is organized into
# blocks, you can do cross validation with these blocks as the basis for the
# customized training sets.

fit3 = cv.customizedGlmnet(x.train, y.train, foldid = foldid)

# Print the optimal value of lambda:
fit3$lambda.min

# Print the customized training model fit:
fit3

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit3))^2)

# Plot nonzero coefficients by group:
plot(fit3)
</code></pre>

<hr>
<h2 id='predict.customizedGlmnet'>
make predictions from a <code>customizedGlmnet</code> object
</h2><span id='topic+predict.customizedGlmnet'></span>

<h3>Description</h3>

<p>Returns predictions for the test set provided at the time of fitting the
<code>customizedGlmnet</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'customizedGlmnet'
predict(object, lambda,
  type = c('response', 'class'), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.customizedGlmnet_+3A_object">object</code></td>
<td>

<p>a fitted <code>customizedGlmnet</code> object
</p>
</td></tr>
<tr><td><code id="predict.customizedGlmnet_+3A_lambda">lambda</code></td>
<td>

<p>regularization parameter
</p>
</td></tr>
<tr><td><code id="predict.customizedGlmnet_+3A_type">type</code></td>
<td>

<p>Type of prediction, currently only &quot;response&quot; and &quot;class&quot; are supported. Type
&quot;response&quot; returns fitted values for &quot;gaussian&quot; family and fitted probabilities
for &quot;binomial&quot; and &quot;multinomial&quot; families. Type &quot;class&quot; applies only to
&quot;binomial&quot; and &quot;multinomial&quot; families and returns the class with the highest
fitted probability.
</p>
</td></tr>
<tr><td><code id="predict.customizedGlmnet_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of predictions corresponding to the test data input to the model at
the time of fitting
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Robert Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Robert Tibshirani (2015) &quot;Customized training
with an application to mass specrometric imaging of gastric cancer data.&quot;
Annals of Applied Statistics 9, 4:1709-1725.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="#topic+customizedGlmnet">customizedGlmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(glmnet)

# Simulate synthetic data

n = m = 150
p = 50
q = 5
K = 3
sigmaC = 10
sigmaX = sigmaY = 1
set.seed(5914)

beta = matrix(0, nrow = p, ncol = K)
for (k in 1:K) beta[sample(1:p, q), k] = 1
c = matrix(rnorm(K*p, 0, sigmaC), K, p)
eta = rnorm(K)
pi = (exp(eta)+1)/sum(exp(eta)+1)
z = t(rmultinom(m + n, 1, pi))
x = crossprod(t(z), c) + matrix(rnorm((m + n)*p, 0, sigmaX), m + n, p)
y = rowSums(z*(crossprod(t(x), beta))) + rnorm(m + n, 0, sigmaY)

x.train = x[1:n, ]
y.train = y[1:n]
x.test = x[n + 1:m, ]
y.test = y[n + 1:m]


# Example 1: Use clustering to fit the customized training model to training
# and test data with no predefined test-set blocks

fit1 = customizedGlmnet(x.train, y.train, x.test, G = 3,
    family = "gaussian")

# Compute test error using the predict function:
mean((y.test - predict(fit1, lambda = 10))^2)


# Example 2: If the test set has predefined blocks, use these blocks to define
# the customized training sets, instead of using clustering.
group.id = apply(z == 1, 1, which)[n + 1:m]

fit2 = customizedGlmnet(x.train, y.train, x.test, group.id)

# Compute test error using the predict function:
mean((y.test - predict(fit2, lambda = 10))^2)
</code></pre>

<hr>
<h2 id='predict.cv.customizedGlmnet'>
make predictions from a <code>cv.customizedGlmnet</code> object
</h2><span id='topic+predict.cv.customizedGlmnet'></span>

<h3>Description</h3>

<p>Returns predictions for test set provided at time of fitting, using
regulariztion parameter which minimizes CV error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.customizedGlmnet'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cv.customizedGlmnet_+3A_object">object</code></td>
<td>

<p>a fitted <code>cv.customizedGlmnet</code> object
</p>
</td></tr>
<tr><td><code id="predict.cv.customizedGlmnet_+3A_...">...</code></td>
<td>

<p>additional arguments to be passed to <code>predict.customizedGlmnet</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of predictions corresponding to the test set provided when the model
was fit. The results are for the regularization parameter chosen by
cross-validation
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Robert Tibshirani
</p>


<h3>References</h3>

<p>Scott Powers, Trevor Hastie and Robert Tibshirani (2015) &quot;Customized training
with an application to mass specrometric imaging of gastric cancer data.&quot;
Annals of Applied Statistics 9, 4:1709-1725.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="#topic+cv.customizedGlmnet">cv.customizedGlmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(glmnet)

# Simulate synthetic data

n = m = 150
p = 50
q = 5
K = 3
sigmaC = 10
sigmaX = sigmaY = 1
set.seed(5914)

beta = matrix(0, nrow = p, ncol = K)
for (k in 1:K) beta[sample(1:p, q), k] = 1
c = matrix(rnorm(K*p, 0, sigmaC), K, p)
eta = rnorm(K)
pi = (exp(eta)+1)/sum(exp(eta)+1)
z = t(rmultinom(m + n, 1, pi))
x = crossprod(t(z), c) + matrix(rnorm((m + n)*p, 0, sigmaX), m + n, p)
y = rowSums(z*(crossprod(t(x), beta))) + rnorm(m + n, 0, sigmaY)

x.train = x[1:n, ]
y.train = y[1:n]
x.test = x[n + 1:m, ]
y.test = y[n + 1:m]
foldid = sample(rep(1:10, length = nrow(x.train)))


# Example 1: Use clustering to fit the customized training model to training
# and test data with no predefined test-set blocks

fit1 = cv.customizedGlmnet(x.train, y.train, x.test, Gs = c(1, 2, 3, 5),
    family = "gaussian", foldid = foldid)

# Print the optimal number of groups and value of lambda:
fit1$G.min
fit1$lambda.min

# Print the customized training model fit:
fit1

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit1))^2)

# Plot nonzero coefficients by group:
plot(fit1)


# Example 2: If the test set has predefined blocks, use these blocks to define
# the customized training sets, instead of using clustering.
foldid = apply(z == 1, 1, which)[1:n]
group.id = apply(z == 1, 1, which)[n + 1:m]

fit2 = cv.customizedGlmnet(x.train, y.train, x.test, group.id, foldid = foldid)

# Print the optimal value of lambda:
fit2$lambda.min

# Print the customized training model fit:
fit2

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit2))^2)

# Plot nonzero coefficients by group:
plot(fit2)


# Example 3: If there is no test set, but the training set is organized into
# blocks, you can do cross validation with these blocks as the basis for the
# customized training sets.

fit3 = cv.customizedGlmnet(x.train, y.train, foldid = foldid)

# Print the optimal value of lambda:
fit3$lambda.min

# Print the customized training model fit:
fit3

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit3))^2)

# Plot nonzero coefficients by group:
plot(fit3)
</code></pre>

<hr>
<h2 id='predict.singleton'>
make predictions from a &ldquo;singleton&rdquo; object
</h2><span id='topic+predict.singleton'></span>

<h3>Description</h3>

<p>Returns the value stored in the singleton. Intended for internal use only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'singleton'
predict(object, type = c('response', 'class', 'nonzero'),
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.singleton_+3A_object">object</code></td>
<td>

<p>an object of class <code>singleton</code>
</p>
</td></tr>
<tr><td><code id="predict.singleton_+3A_type">type</code></td>
<td>

<p>Type of prediction to be returned, &quot;response&quot; or &quot;class&quot;
</p>
</td></tr>
<tr><td><code id="predict.singleton_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value of the singleton
</p>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Robert Tibshirani
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+predict">predict</a></code>
</p>

<hr>
<h2 id='print.customizedGlmnet'>
print the summary of a fitted <code>customizedGlmnet</code> object
</h2><span id='topic+print.customizedGlmnet'></span>

<h3>Description</h3>

<p>Print the numbers of training observations and test observations in each
submodel of the <code>customizedGlmnet</code> fit
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'customizedGlmnet'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.customizedGlmnet_+3A_x">x</code></td>
<td>

<p>fitted <code>customizedGlmnet</code> object
</p>
</td></tr>
<tr><td><code id="print.customizedGlmnet_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Robert Tibshirani
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+print">print</a></code>, <code><a href="#topic+customizedGlmnet">customizedGlmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(glmnet)

# Simulate synthetic data

n = m = 150
p = 50
q = 5
K = 3
sigmaC = 10
sigmaX = sigmaY = 1
set.seed(5914)

beta = matrix(0, nrow = p, ncol = K)
for (k in 1:K) beta[sample(1:p, q), k] = 1
c = matrix(rnorm(K*p, 0, sigmaC), K, p)
eta = rnorm(K)
pi = (exp(eta)+1)/sum(exp(eta)+1)
z = t(rmultinom(m + n, 1, pi))
x = crossprod(t(z), c) + matrix(rnorm((m + n)*p, 0, sigmaX), m + n, p)
y = rowSums(z*(crossprod(t(x), beta))) + rnorm(m + n, 0, sigmaY)

x.train = x[1:n, ]
y.train = y[1:n]
x.test = x[n + 1:m, ]
y.test = y[n + 1:m]


# Example 1: Use clustering to fit the customized training model to training
# and test data with no predefined test-set blocks

fit1 = customizedGlmnet(x.train, y.train, x.test, G = 3,
    family = "gaussian")

# Print the customized training model fit:
fit1


# Example 2: If the test set has predefined blocks, use these blocks to define
# the customized training sets, instead of using clustering.
group.id = apply(z == 1, 1, which)[n + 1:m]

fit2 = customizedGlmnet(x.train, y.train, x.test, group.id)

# Print the customized training model fit:
fit2
</code></pre>

<hr>
<h2 id='print.cv.customizedGlmnet'>
print a &ldquo;cv.customizedGlmnet&rdquo; object
</h2><span id='topic+print.cv.customizedGlmnet'></span>

<h3>Description</h3>

<p>Print the number of customized training subsets chosen by cross-validation and
the number of variables selected in each training subset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.customizedGlmnet'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.cv.customizedGlmnet_+3A_x">x</code></td>
<td>

<p>a fitted <code>cv.customizedGlmnet</code> object
</p>
</td></tr>
<tr><td><code id="print.cv.customizedGlmnet_+3A_...">...</code></td>
<td>

<p>ignored
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Scott Powers, Trevor Hastie, Robert Tibshirani
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+print">print</a></code>, <code><a href="#topic+cv.customizedGlmnet">cv.customizedGlmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(glmnet)

# Simulate synthetic data

n = m = 150
p = 50
q = 5
K = 3
sigmaC = 10
sigmaX = sigmaY = 1
set.seed(5914)

beta = matrix(0, nrow = p, ncol = K)
for (k in 1:K) beta[sample(1:p, q), k] = 1
c = matrix(rnorm(K*p, 0, sigmaC), K, p)
eta = rnorm(K)
pi = (exp(eta)+1)/sum(exp(eta)+1)
z = t(rmultinom(m + n, 1, pi))
x = crossprod(t(z), c) + matrix(rnorm((m + n)*p, 0, sigmaX), m + n, p)
y = rowSums(z*(crossprod(t(x), beta))) + rnorm(m + n, 0, sigmaY)

x.train = x[1:n, ]
y.train = y[1:n]
x.test = x[n + 1:m, ]
y.test = y[n + 1:m]
foldid = sample(rep(1:10, length = nrow(x.train)))


# Example 1: Use clustering to fit the customized training model to training
# and test data with no predefined test-set blocks

fit1 = cv.customizedGlmnet(x.train, y.train, x.test, Gs = c(1, 2, 3, 5),
    family = "gaussian", foldid = foldid)

# Print the optimal number of groups and value of lambda:
fit1$G.min
fit1$lambda.min

# Print the customized training model fit:
fit1

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit1))^2)

# Plot nonzero coefficients by group:
plot(fit1)


# Example 2: If the test set has predefined blocks, use these blocks to define
# the customized training sets, instead of using clustering.
foldid = apply(z == 1, 1, which)[1:n]
group.id = apply(z == 1, 1, which)[n + 1:m]

fit2 = cv.customizedGlmnet(x.train, y.train, x.test, group.id, foldid = foldid)

# Print the optimal value of lambda:
fit2$lambda.min

# Print the customized training model fit:
fit2

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit2))^2)

# Plot nonzero coefficients by group:
plot(fit2)


# Example 3: If there is no test set, but the training set is organized into
# blocks, you can do cross validation with these blocks as the basis for the
# customized training sets.

fit3 = cv.customizedGlmnet(x.train, y.train, foldid = foldid)

# Print the optimal value of lambda:
fit3$lambda.min

# Print the customized training model fit:
fit3

# Compute test error using the predict function:
mean((y[n + 1:m] - predict(fit3))^2)

# Plot nonzero coefficients by group:
plot(fit3)
</code></pre>

<hr>
<h2 id='Vowel'>
Vowel Recognition
</h2><span id='topic+Vowel'></span>

<h3>Description</h3>

<p>Speaker independent recognition of the eleven steady state vowels
of British English using a specified training set of lpc derived log area
ratios.
</p>


<h3>Format</h3>

<p>A data frame with 990 observations on the following 12 variables.
</p>

<dl>
<dt><code>y</code></dt><dd><p>Class label indicating vowel spoken</p>
</dd>
<dt><code>subset</code></dt><dd><p>a factor with levels <code>test</code> <code>train</code></p>
</dd>
<dt><code>x.1</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.2</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.3</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.4</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.5</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.6</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.7</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.8</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.9</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x.10</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p>The speech signals were low pass filtered at 4.7kHz and then digitised to 12
bits with a 10kHz sampling rate.  Twelfth order linear predictive analysis was
carried out on six 512 sample Hamming windowed segments from the steady part
of the vowel.  The reflection coefficients were used to calculate 10 log area
parameters, giving a 10 dimensional input space.  For a general introduction
to speech processing and an explanation of this technique see Rabiner and
Schafer [RabinerSchafer78].
</p>
<p>Each speaker thus yielded six frames of speech from eleven vowels.  This gave
528 frames from the eight speakers used to train the networks and 462 frames
from the seven speakers used to test the networks.
</p>
<p>The eleven vowels, along with words demonstrating their sound, are:
i (heed)
I (hid)
E (head)
A (had)
a: (hard)
Y (hud)
O (hod)
C: (hoard)
U (hood)
u: (who'd)
3: (heard)
</p>


<h3>Source</h3>

<p>https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/
</p>


<h3>References</h3>

<p>D. H. Deterding, 1989, University of Cambridge, &quot;Speaker Normalisation for Automatic Speech Recognition&quot;, submitted for PhD.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Vowel)
summary(Vowel)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
