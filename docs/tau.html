<!DOCTYPE html><html lang="en"><head><title>Help for package tau</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tau}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#encoding'><p>Adapt the (Declared) Encoding of a Character Vector</p></a></li>
<li><a href='#ligatures'><p>Translate Unicode Latin Ligatures</p></a></li>
<li><a href='#readers'><p>Read Byte or Character Strings</p></a></li>
<li><a href='#textcnt'><p>Term or Pattern Counting of Text Documents</p></a></li>
<li><a href='#util'><p>Preprocessing of Text Documents</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>0.0-26</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Title:</td>
<td>Text Analysis Utilities</td>
</tr>
<tr>
<td>Description:</td>
<td>Utilities for text analysis.</td>
</tr>
<tr>
<td>Suggests:</td>
<td>tm</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-10-15 06:55:07 UTC; hornik</td>
</tr>
<tr>
<td>Author:</td>
<td>Christian Buchta [aut],
  Kurt Hornik <a href="https://orcid.org/0000-0003-4198-9911"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Ingo Feinerer [aut],
  David Meyer [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kurt Hornik &lt;Kurt.Hornik@R-project.org&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-10-15 07:35:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='encoding'>Adapt the (Declared) Encoding of a Character Vector</h2><span id='topic+is.utf8'></span><span id='topic+is.ascii'></span><span id='topic+is.locale'></span><span id='topic+translate'></span><span id='topic+fixEncoding'></span>

<h3>Description</h3>

<p>Functions for testing and adapting the (declared) encoding
of the components of a vector of mode <code>character</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.utf8(x)
is.ascii(x)
is.locale(x)

translate(x, recursive = FALSE, internal = FALSE)
fixEncoding(x, latin1 = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="encoding_+3A_x">x</code></td>
<td>
<p>a vector (of character).</p>
</td></tr>
<tr><td><code id="encoding_+3A_recursive">recursive</code></td>
<td>
<p>option to process list components.</p>
</td></tr>
<tr><td><code id="encoding_+3A_internal">internal</code></td>
<td>
<p>option to use internal translation.</p>
</td></tr>
<tr><td><code id="encoding_+3A_latin1">latin1</code></td>
<td>
<p>option to assume <code>"latin1"</code> if the declared 
encoding is <code>"unknown"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>is.utf8</code> tests if the components of a vector of character
are true UTF-8 strings, i.e. contain one or more valid UTF-8
multi-byte sequence(s).
</p>
<p><code>is.locale</code> tests if the components of a vector of character
are in the encoding of the current locale.
</p>
<p><code>translate</code> encodes the components of a vector of <code>character</code>
in the encoding of the current locale. This includes the <code>names</code>
attribute of vectors of arbitrary mode. If <code>recursive = TRUE</code>
the components of a <code>list</code> are processed. If <code>internal = TRUE</code>
multi-byte sequences that are invalid in the encoding of the current
locale are changed to literal hex numbers (see FIXME).
</p>
<p><code>fixEncoding</code> sets the declared encoding of the components of
a vector of character to their correct or preferred values. If
<code>latin1 = TRUE</code> strings that are not valid UTF-8 strings are
declared to be in <code>"latin1"</code>. On the other hand, strings that
are true UTF-8 strings are declared to be in <code>"UTF-8"</code> encoding.
</p>


<h3>Value</h3>

<p>The same type of object as <code>x</code> with the (declared) encoding
possibly changed.
</p>


<h3>Note</h3>

<p>Currently <code>translate</code> uses <code>iconv</code> and therefore is not
guaranteed to work on all platforms.
</p>


<h3>Author(s)</h3>

<p>Christian Buchta</p>


<h3>References</h3>

<p>FIXME PCRE, RFC 3629</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+Encoding">Encoding</a></code> and <code><a href="base.html#topic+iconv">iconv</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Note that we assume R runs in an UTF-8 locale
text &lt;- c("aa", "a\xe4")
Encoding(text) &lt;- c("unknown", "latin1")
is.utf8(text)
is.ascii(text)
is.locale(text)
## implicit translation
text
##
t1 &lt;- iconv(text, from = "latin1", to = "UTF-8")
Encoding(t1)
## oops
t2 &lt;- iconv(text, from = "latin1", to = "utf-8")
Encoding(t2)
t2
is.locale(t2)
##
t2 &lt;- fixEncoding(t2)
Encoding(t2)
## explicit translation
t3 &lt;- translate(text)
Encoding(t3)
</code></pre>

<hr>
<h2 id='ligatures'>Translate Unicode Latin Ligatures</h2><span id='topic+translate_Unicode_latin_ligatures'></span>

<h3>Description</h3>

<p>Translate Unicode &ldquo;Latin ligature&rdquo; characters to their
respective constituents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>translate_Unicode_latin_ligatures(x)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ligatures_+3A_x">x</code></td>
<td>
<p>a character vector in UTF-8 encoding.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In typography, a ligature occurs where two or more
graphemes are joined as a single glyph.  (See
<a href="https://en.wikipedia.org/wiki/Typographic_ligature">https://en.wikipedia.org/wiki/Typographic_ligature</a> for more
information.)
</p>
<p>Unicode (<a href="http://www.unicode.org/">http://www.unicode.org/</a>) lists the following
&ldquo;Latin&rdquo; ligatures:
</p>

<table>
<tr>
 <td style="text-align: right;">
    Code </td><td style="text-align: left;"> Name </td>
</tr>
<tr>
 <td style="text-align: right;">
    0132 </td><td style="text-align: left;"> LATIN CAPITAL LIGATURE IJ </td>
</tr>
<tr>
 <td style="text-align: right;">
    0133 </td><td style="text-align: left;"> LATIN SMALL LIGATURE IJ </td>
</tr>
<tr>
 <td style="text-align: right;">
    0152 </td><td style="text-align: left;"> LATIN CAPITAL LIGATURE OE </td>
</tr>
<tr>
 <td style="text-align: right;">
    0153 </td><td style="text-align: left;"> LATIN SMALL LIGATURE OE </td>
</tr>
<tr>
 <td style="text-align: right;">
    FB00 </td><td style="text-align: left;"> LATIN SMALL LIGATURE FF </td>
</tr>
<tr>
 <td style="text-align: right;">
    FB01 </td><td style="text-align: left;"> LATIN SMALL LIGATURE FI </td>
</tr>
<tr>
 <td style="text-align: right;">
    FB02 </td><td style="text-align: left;"> LATIN SMALL LIGATURE FL </td>
</tr>
<tr>
 <td style="text-align: right;">
    FB03 </td><td style="text-align: left;"> LATIN SMALL LIGATURE FFI </td>
</tr>
<tr>
 <td style="text-align: right;">
    FB04 </td><td style="text-align: left;"> LATIN SMALL LIGATURE FFL </td>
</tr>
<tr>
 <td style="text-align: right;">
    FB05 </td><td style="text-align: left;"> LATIN SMALL LIGATURE LONG S T </td>
</tr>
<tr>
 <td style="text-align: right;">
    FB06 </td><td style="text-align: left;"> LATIN SMALL LIGATURE ST
  </td>
</tr>

</table>

<p><code>translate_Unicode_latin_ligatures</code> translates these to their
respective constituent characters.
</p>

<hr>
<h2 id='readers'>Read Byte or Character Strings</h2><span id='topic+readBytes'></span><span id='topic+readChars'></span>

<h3>Description</h3>

<p>Read byte or character strings from a connection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readBytes(con)
readChars(con, encoding = "")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="readers_+3A_con">con</code></td>
<td>
<p>a <a href="base.html#topic+connection">connection</a> object or a character string naming a
file.</p>
</td></tr>
<tr><td><code id="readers_+3A_encoding">encoding</code></td>
<td>
<p>encoding to be assumed for input.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Both functions first read the raw bytes from the input connection into
a character string.  <code>readBytes</code> then sets the <a href="base.html#topic+Encoding">Encoding</a> of
this to <code>"bytes"</code>; <code>readChars</code> uses <code><a href="base.html#topic+iconv">iconv</a></code> to
convert from the specified input encoding to UTF-8 (replacing
non-convertible bytes by their hex codes).
</p>


<h3>Value</h3>

<p>For <code>readBytes</code>, a character string marked as <code>"bytes"</code>.
For <code>readChars</code>, a character string marked as <code>"UTF-8"</code> if
containing non-ASCII characters.
</p>


<h3>See Also</h3>

<p><a href="base.html#topic+Encoding">Encoding</a>
</p>

<hr>
<h2 id='textcnt'>Term or Pattern Counting of Text Documents</h2><span id='topic+textcnt'></span><span id='topic+format.textcnt'></span>

<h3>Description</h3>

<p>This function provides a common interface to perform typical term or
pattern counting tasks on text documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textcnt(x, n = 3L, split = "[[:space:][:punct:][:digit:]]+",
        tolower = TRUE, marker = "_", words = NULL, lower = 0L,
        method = c("ngram", "string", "prefix", "suffix"),
        recursive = FALSE, persistent = FALSE, useBytes = FALSE,
        perl = TRUE, verbose = FALSE, decreasing = FALSE)

## S3 method for class 'textcnt'
format(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="textcnt_+3A_x">x</code></td>
<td>
<p>a (list of) vector(s) of character representing one (or more)
text document(s).</p>
</td></tr>
<tr><td><code id="textcnt_+3A_n">n</code></td>
<td>
<p>the maximum number of characters considered in ngram,
prefix, or suffix counting (for word counting see details).</p>
</td></tr>
<tr><td><code id="textcnt_+3A_split">split</code></td>
<td>
<p>the regular expression pattern (PCRE) to be used in word
splitting (if <code>NULL</code>, do nothing).</p>
</td></tr>
<tr><td><code id="textcnt_+3A_tolower">tolower</code></td>
<td>
<p>option to transform the documents to lowercase (after
word splitting).</p>
</td></tr>
<tr><td><code id="textcnt_+3A_marker">marker</code></td>
<td>
<p>the string used to mark word boundaries.</p>
</td></tr>
<tr><td><code id="textcnt_+3A_words">words</code></td>
<td>
<p>the number of words to use from the beginning of a
document (if <code>NULL</code>, all words are used).</p>
</td></tr>
<tr><td><code id="textcnt_+3A_lower">lower</code></td>
<td>
<p>the lower bound for a count to be included in the result
set(s).</p>
</td></tr>
<tr><td><code id="textcnt_+3A_method">method</code></td>
<td>
<p>the type of counts to compute.</p>
</td></tr>
<tr><td><code id="textcnt_+3A_recursive">recursive</code></td>
<td>
<p>option to compute counts for individual documents
(default all documents).</p>
</td></tr>
<tr><td><code id="textcnt_+3A_persistent">persistent</code></td>
<td>
<p>option to count documents incrementally.</p>
</td></tr>
<tr><td><code id="textcnt_+3A_usebytes">useBytes</code></td>
<td>
<p>option to process byte-by-byte instead of
character-by-character.</p>
</td></tr>
<tr><td><code id="textcnt_+3A_perl">perl</code></td>
<td>
<p>option to use PCRE in word splitting.</p>
</td></tr>
<tr><td><code id="textcnt_+3A_verbose">verbose</code></td>
<td>
<p>option to obtain timing statistics.</p>
</td></tr>
<tr><td><code id="textcnt_+3A_decreasing">decreasing</code></td>
<td>
<p>option to return the counts in decreasing order.</p>
</td></tr>
<tr><td><code id="textcnt_+3A_...">...</code></td>
<td>
<p>further (unused) arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following counting methods are currently implemented:
</p>

<dl>
<dt><code>ngram</code></dt><dd><p>Count all word n-grams of order 1,...,<code>n</code>.</p>
</dd>
<dt><code>string</code></dt><dd><p>Count all word sequence n-grams of order <code>n</code>.</p>
</dd>
<dt><code>prefix</code></dt><dd><p>Count all word prefixes of at most length <code>n</code>.</p>
</dd>
<dt><code>suffix</code></dt><dd><p>Count all word suffixes of at most length <code>n</code>.</p>
</dd>
</dl>

<p>The n-grams of a word are defined to be the substrings of length
<code>n = min(length(word), n)</code> starting at positions
1,...,<code>length(word)-n</code>.  Note that the value of <code>marker</code>
is pre- and appended to word before counting. However, the empty word
is never marked and therefore not counted. Note that
<code>marker = "\1"</code> is reserved for counting of an efficient set
of ngrams and <code>marker = "\2"</code> for the set proposed by Cavnar
and Trenkle (see references).
</p>
<p>If <code>method = "string"</code> word-sequences of and only of length
<code>n</code> are counted. Therefore, documents with less than <code>n</code>
words are omitted.
</p>
<p>By default all documents are preprocessed and counted using a single C
function call. For large document collections this may come at the
price of considerable memory consumption. If <code>persistent = TRUE</code> and
<code>recursive = TRUE</code> documents are counted incrementally, i.e., into a
persistent prefix tree using as many C function calls as there are
documents. Further, if <code>persistent = TRUE</code> and <code>recursive = FALSE</code>
the documents are counted using a single call but no result is returned
until the next call with <code>persistent = FALSE</code>. Thus, <code>persistent</code>
acts as a switch with the counts being accumulated until release. Timing
statistics have shown that incremental counting can be order of
magnitudes faster than the default. 
</p>
<p>Be aware that the character strings in the documents are translated
to the encoding of the current locale if the encoding is set (see
<code><a href="base.html#topic+Encoding">Encoding</a></code>). Therefore, with the possibility of <code>"unknown"</code>
encodings when in an <code>"UTF-8"</code> locale, or invalid <code>"UTF-8"</code>
strings declared to be in <code>"UTF-8"</code>, the code checks if each string
is a valid <code>"UTF-8"</code> string and stops if not. Otherwise, strings
are processed bytewise without any checks. However, embedded <code>nul</code>
bytes are always removed from a string. Finally, note that during
incremental counting a change of locale is not allowed (and a change
in method is not recommended).
</p>
<p>Note that the C implementation counts words into a prefix tree.  Whereas this is highly efficient for n-gram, prefix, or suffix counting
it may be less efficient for simple word counting. That is, implementations
which use hash tables may be more efficient if the dictionary is large.
</p>
<p><code>format.textcnt</code> pretty prints a named vector of counts (see below)
including information about the rank and encoding details of the strings.
</p>


<h3>Value</h3>

<p>Either a single vector of counts of mode <code>integer</code> with the names
indexing the patterns counted, or a list of such vectors with the
components corresponding to the individual documents. Note that by
default the counts are in prefix tree (byte) order (for
<code>method = "suffix"</code> this is the order of the reversed strings).
Otherwise, if <code>decreasing = TRUE</code> the counts are sorted in 
decreasing order. Note that the (default) order of ties is preserved
(see <code><a href="base.html#topic+sort">sort</a></code>).
</p>


<h3>Note</h3>

<p>The C functions can be interrupted by <kbd>CTRL-C</kbd>. This is convenient in
interactive mode but comes at the price that the C code cannot clean
up the internal prefix tree. This is a known problem of the R API
and the workaround is to defer the cleanup to the next function call.
</p>
<p>The C code calls <code>translateChar</code> for all input strings which is
documented to release the allocated memory no sooner than when
returning from the <code>.Call</code>/<code>.External</code> interface.
Therefore, in order to avoid excessive memory consumption it is
recommended to either translate the input data to the current locale
or to process the data incrementally.
</p>
<p><code>useBytes</code> may not be fully functional with R versions where
<code>strsplit</code> does not support that argument.
</p>
<p>If <code>useBytes = TRUE</code> the character strings of <code>names</code> will
never be declared to be in an encoding.
</p>


<h3>Author(s)</h3>

<p>Christian Buchta</p>


<h3>References</h3>

<p>W.B. Cavnar and J.M. Trenkle (1994).
N-Gram Based Text Categorization.
In Proceedings of SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, 161&ndash;175.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## the classic
txt &lt;- "The quick brown fox jumps over the lazy dog."

##
textcnt(txt, method = "ngram")
textcnt(txt, method = "prefix", n = 5L)

r &lt;- textcnt(txt, method = "suffix", lower = 1L)
data.frame(counts = unclass(r), size = nchar(names(r)))
format(r)

## word sequences
textcnt(txt, method = "string")

## inefficient
textcnt(txt, split = "", method = "string", n = 1L)

## incremental
textcnt(txt, method = "string", persistent = TRUE, n = 1L)
textcnt(txt, method = "string", n = 1L)

## subset
textcnt(txt, method = "string", words = 5L, n = 1L)

## non-ASCII
txt &lt;- "The quick br\xfcn f\xf6x j\xfbmps \xf5ver the lazy d\xf6\xf8g."
Encoding(txt) &lt;- "latin1"
txt

## implicit translation
r &lt;- textcnt(txt, method = "suffix")
table(Encoding(names(r)))
r
## efficient sets
textcnt("is",     n = 3L, marker = "\1")
textcnt("is",     n = 4L, marker = "\1")
textcnt("corpus", n = 5L, marker = "\1")
## CT sets
textcnt("corpus", n = 5L, marker = "\2")
</code></pre>

<hr>
<h2 id='util'>Preprocessing of Text Documents</h2><span id='topic+tokenize'></span><span id='topic+remove_stopwords'></span>

<h3>Description</h3>

<p>Functions for common preprocessing tasks of text documents,
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize(x, lines = FALSE, eol = "\n")
remove_stopwords(x, words, lines = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="util_+3A_x">x</code></td>
<td>
<p>a vector of character.</p>
</td></tr>
<tr><td><code id="util_+3A_eol">eol</code></td>
<td>
<p>the end-of-line character to use.</p>
</td></tr>
<tr><td><code id="util_+3A_words">words</code></td>
<td>
<p>a vector of character (tokens).</p>
</td></tr>
<tr><td><code id="util_+3A_lines">lines</code></td>
<td>
<p>assume the components are lines of text.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>tokenize</code> is a simple regular expression based parser that
splits the components of a vector of character into tokens while
protecting infix punctuation. If <code>lines = TRUE</code> assume <code>x</code>
was imported with <code>readLines</code> and end-of-line markers need to be
added back to the components.
</p>
<p><code>remove_stopwords</code> removes the tokens given in <code>words</code> from
<code>x</code>. If <code>lines = FALSE</code> assumes the components of both
vectors contain tokens which can be compared using <code>match</code>.
Otherwise, assumes the tokens in <code>x</code> are delimited by word
boundaries (including infix punctuation) and uses regular expression
matching.
</p>


<h3>Value</h3>

<p>The same type of object as <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Christian Buchta</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt &lt;- "\"It's almost noon,\" it@dot.net said."
## split
x &lt;- tokenize(txt)
x
## reconstruct
t &lt;- paste(x, collapse = "")
t

if (require("tm", quietly = TRUE)) {
    words &lt;- readLines(system.file("stopwords", "english.dat",
                       package = "tm"))
    remove_stopwords(x, words)
    remove_stopwords(t, words, lines = TRUE)
} else
    remove_stopwords(t, words = c("it", "it's"), lines = TRUE)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
