<!DOCTYPE html><html><head><title>Help for package fuser</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fuser}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bigeigen'><p>Big eigenvalue calculation</p></a></li>
<li><a href='#fusedL2DescentGLMNet'><p>Optimise the fused L2 model with glmnet (using transformed input data)</p></a></li>
<li><a href='#fusedLassoProximal'><p>Fused lasso optimisation with proximal-gradient method.</p>
(Chen et al. 2010)</a></li>
<li><a href='#fusedLassoProximalIterationsTaken'><p>Following a call to fusedLassoProximal, returns the actual number of iterations taken.</p></a></li>
<li><a href='#generateBlockDiagonalMatrices'><p>Generate block diagonal matrices to allow for fused L2 optimization with glmnet.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Fused Lasso for High-Dimensional Regression over Groups</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Enables high-dimensional penalized regression across heterogeneous 
    subgroups. Fusion penalties are used to share information about the linear 
    parameters across subgroups. The underlying model is described 
    in detail in Dondelinger and Mukherjee (2017) &lt;<a href="https://doi.org/10.48550/arXiv.1611.00953">doi:10.48550/arXiv.1611.00953</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, ggplot2, knitr, rmarkdown</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, irlba, Rcpp, glmnet, RSpectra</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-06-17 19:15:57 UTC; levendis</td>
</tr>
<tr>
<td>Author:</td>
<td>Frank Dondelinger [aut, cre],
  Olivier Wilkinson [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Frank Dondelinger &lt;fdondelinger.work@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-06-17 20:22:54 UTC</td>
</tr>
</table>
<hr>
<h2 id='bigeigen'>Big eigenvalue calculation</h2><span id='topic+bigeigen'></span>

<h3>Description</h3>

<p>Calculate maximal eigenvalue of <code>t(X) %*% X</code> for big
matrices using singular value decomposition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigeigen(X, method = "RSpectra")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bigeigen_+3A_x">X</code></td>
<td>
<p>matrix to be evaluated (can be a Matrix object).</p>
</td></tr>
<tr><td><code id="bigeigen_+3A_method">method</code></td>
<td>
<p>One of 'irlba' or 'RSpectra'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The maximal eigenvalue.
</p>

<hr>
<h2 id='fusedL2DescentGLMNet'>Optimise the fused L2 model with glmnet (using transformed input data)</h2><span id='topic+fusedL2DescentGLMNet'></span>

<h3>Description</h3>

<p>Optimise the fused L2 model with glmnet (using transformed input data)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fusedL2DescentGLMNet(transformed.x, transformed.x.f, transformed.y, groups,
  lambda, gamma = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fusedL2DescentGLMNet_+3A_transformed.x">transformed.x</code></td>
<td>
<p>Transformed covariates (output of generateBlockDiagonalMatrices)</p>
</td></tr>
<tr><td><code id="fusedL2DescentGLMNet_+3A_transformed.x.f">transformed.x.f</code></td>
<td>
<p>Transformed fusion constraints (output of generateBlockDiagonalMatrices)</p>
</td></tr>
<tr><td><code id="fusedL2DescentGLMNet_+3A_transformed.y">transformed.y</code></td>
<td>
<p>Transformed response (output of generateBlockDiagonalMatrices)</p>
</td></tr>
<tr><td><code id="fusedL2DescentGLMNet_+3A_groups">groups</code></td>
<td>
<p>Grouping factors for samples (a vector of size n, with K factor levels)</p>
</td></tr>
<tr><td><code id="fusedL2DescentGLMNet_+3A_lambda">lambda</code></td>
<td>
<p>Sparsity penalty hyperparameter</p>
</td></tr>
<tr><td><code id="fusedL2DescentGLMNet_+3A_gamma">gamma</code></td>
<td>
<p>Fusion penalty hyperparameter</p>
</td></tr>
<tr><td><code id="fusedL2DescentGLMNet_+3A_...">...</code></td>
<td>
<p>Further options passed to glmnet.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix of fitted beta values.
</p>
<p>A matrix with the linear coefficients for each group (p by k).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#' set.seed(123)

# Generate simple heterogeneous dataset
k = 4 # number of groups
p = 100 # number of covariates
n.group = 15 # number of samples per group
sigma = 0.05 # observation noise sd
groups = rep(1:k, each=n.group) # group indicators
# sparse linear coefficients
beta = matrix(0, p, k)
nonzero.ind = rbinom(p*k, 1, 0.025/k) # Independent coefficients
nonzero.shared = rbinom(p, 1, 0.025) # shared coefficients
beta[which(nonzero.ind==1)] = rnorm(sum(nonzero.ind), 1, 0.25)
beta[which(nonzero.shared==1),] = rnorm(sum(nonzero.shared), -1, 0.25)

X = lapply(1:k,
           function(k.i) matrix(rnorm(n.group*p),
                                n.group, p)) # covariates
y = sapply(1:k,
           function(k.i) X[[k.i]] %*% beta[,k.i] +
                           rnorm(n.group, 0, sigma)) # response
X = do.call('rbind', X)

# Pairwise Fusion strength hyperparameters (tau(k,k'))
# Same for all pairs in this example
G = matrix(1, k, k)

# Generate block diagonal matrices
transformed.data = generateBlockDiagonalMatrices(X, y, groups, G)

# Use L2 fusion to estimate betas (with near-optimal information
# sharing among groups)
beta.estimate = fusedL2DescentGLMNet(transformed.data$X,
                                     transformed.data$X.fused,
                                     transformed.data$Y, groups,
                                     lambda=c(0,0.001,0.1,1),
                                     gamma=0.001)
</code></pre>

<hr>
<h2 id='fusedLassoProximal'>Fused lasso optimisation with proximal-gradient method.
(Chen et al. 2010)</h2><span id='topic+fusedLassoProximal'></span>

<h3>Description</h3>

<p>Fused lasso optimisation with proximal-gradient method.
(Chen et al. 2010)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fusedLassoProximal(X, Y, groups, lambda, gamma, G, mu = 1e-04, tol = 1e-06,
  num.it = 1000, lam.max = NULL, c.flag = FALSE, intercept = TRUE,
  penalty.factors = NULL, conserve.memory = p &gt;= 10000, scaling = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fusedLassoProximal_+3A_x">X</code></td>
<td>
<p>matrix of covariates (n by p)</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_y">Y</code></td>
<td>
<p>vector of responses (length n)</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_groups">groups</code></td>
<td>
<p>vector of group indicators (length n)</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_lambda">lambda</code></td>
<td>
<p>Sparsity hyperparameter (accepts scalar value only)</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_gamma">gamma</code></td>
<td>
<p>Fusion hyperparameter (accepts scalar value only)</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_g">G</code></td>
<td>
<p>Matrix of pairwise group information sharing weights (K by K)</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_mu">mu</code></td>
<td>
<p>Smoothness parameter for proximal optimization</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_tol">tol</code></td>
<td>
<p>Tolerance for optimization</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_num.it">num.it</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_lam.max">lam.max</code></td>
<td>
<p>Maximal eigenvalue of <code>t(X) %*% X</code> (will be calculate
if not provided)</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_c.flag">c.flag</code></td>
<td>
<p>Whether to use Rcpp for certain calculations (see Details).</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_intercept">intercept</code></td>
<td>
<p>Whether to include a (group-specific) intercept term.</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_penalty.factors">penalty.factors</code></td>
<td>
<p>Weights for sparsity penalty.</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_conserve.memory">conserve.memory</code></td>
<td>
<p>Whether to calculate XX and XY on the fly, conserving memory
at the cost of speed. (True by default iff p &gt;= 10000)</p>
</td></tr>
<tr><td><code id="fusedLassoProximal_+3A_scaling">scaling</code></td>
<td>
<p>if TRUE, scale the sum-squared loss for each group by 1/n_k
where n_k is the number of samples in group k.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The proximal algorithm uses <code>t(X) %*% X</code> and <code>t(X) %*% Y</code>. The function will attempt to
pre-calculate these values to speed up computation. This may not always be possible due to
memory restrictions; at present this is only done for p &lt; 10,000. When p &gt; 10,000,
crossproducts are calculated explicitly; calculation can be speeded up by using
Rcpp code (setting c.flag=TRUE).
</p>


<h3>Value</h3>

<p>A matrix with the linear coefficients for each group (p by k).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
# Generate simple heterogeneous dataset
k = 4 # number of groups
p = 100 # number of covariates
n.group = 15 # number of samples per group
sigma = 0.05 # observation noise sd
groups = rep(1:k, each=n.group) # group indicators
# sparse linear coefficients
beta = matrix(0, p, k)
nonzero.ind = rbinom(p*k, 1, 0.025/k) # Independent coefficients
nonzero.shared = rbinom(p, 1, 0.025) # shared coefficients
beta[which(nonzero.ind==1)] = rnorm(sum(nonzero.ind), 1, 0.25)
beta[which(nonzero.shared==1),] = rnorm(sum(nonzero.shared), -1, 0.25)

X = lapply(1:k,
           function(k.i) matrix(rnorm(n.group*p),
                                n.group, p)) # covariates
y = sapply(1:k,
           function(k.i) X[[k.i]] %*% beta[,k.i] +
                           rnorm(n.group, 0, sigma)) # response
X = do.call('rbind', X)

# Pairwise Fusion strength hyperparameters (tau(k,k'))
# Same for all pairs in this example
G = matrix(1, k, k)

# Use L1 fusion to estimate betas (with near-optimal sparsity and
# information sharing among groups)
beta.estimate = fusedLassoProximal(X, y, groups, lambda=0.01, tol=3e-3,
                                   gamma=0.01, G, intercept=FALSE,
                                   num.it=500)
</code></pre>

<hr>
<h2 id='fusedLassoProximalIterationsTaken'>Following a call to fusedLassoProximal, returns the actual number of iterations taken.</h2><span id='topic+fusedLassoProximalIterationsTaken'></span>

<h3>Description</h3>

<p>Following a call to fusedLassoProximal, returns the actual number of iterations taken.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fusedLassoProximalIterationsTaken()
</code></pre>


<h3>Value</h3>

<p>Number of iterations performed in the previous call to fusedLassoProximal.
</p>

<hr>
<h2 id='generateBlockDiagonalMatrices'>Generate block diagonal matrices to allow for fused L2 optimization with glmnet.</h2><span id='topic+generateBlockDiagonalMatrices'></span>

<h3>Description</h3>

<p>Generate block diagonal matrices to allow for fused L2 optimization with glmnet.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateBlockDiagonalMatrices(X, Y, groups, G, intercept = FALSE,
  penalty.factors = rep(1, dim(X)[2]), scaling = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateBlockDiagonalMatrices_+3A_x">X</code></td>
<td>
<p>covariates matrix (n by p).</p>
</td></tr>
<tr><td><code id="generateBlockDiagonalMatrices_+3A_y">Y</code></td>
<td>
<p>response vector (length n).</p>
</td></tr>
<tr><td><code id="generateBlockDiagonalMatrices_+3A_groups">groups</code></td>
<td>
<p>vector of group indicators (ideally factors, length n)</p>
</td></tr>
<tr><td><code id="generateBlockDiagonalMatrices_+3A_g">G</code></td>
<td>
<p>matrix representing the fusion strengths between pairs of
groups (K by K). Zero entries are assumed to be independent pairs.</p>
</td></tr>
<tr><td><code id="generateBlockDiagonalMatrices_+3A_intercept">intercept</code></td>
<td>
<p>whether to include an (per-group) intercept in the model</p>
</td></tr>
<tr><td><code id="generateBlockDiagonalMatrices_+3A_penalty.factors">penalty.factors</code></td>
<td>
<p>vector of weights for the penalization of
each covariate (length p)</p>
</td></tr>
<tr><td><code id="generateBlockDiagonalMatrices_+3A_scaling">scaling</code></td>
<td>
<p>Whether to scale each subgroup by its size. See Details for an explanation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We use the <code>glmnet</code> package to perform fused subgroup regression.
In order to achieve this, we need to reformulate the problem as Y' = X'beta',
where Y' is a concatenation of the responses Y and a vector of zeros, X' is a
a matrix consisting of the block-diagonal matrix n by pK matrix X, where each
block contains the covariates for one subgroups, and the choose(K,2)*p by pK
matrix encoding the fusion penalties between pairs of groups. The vector of
parameters beta' of length pK can be rearranged as a p by K matrix giving the
parameters for each subgroup. The lasso penalty on the parameters is handled
by glmnet.
</p>
<p>One weakness of the approach described above is that larger subgroups will
have a larger influence on the global parameters lambda and gamma.
In order to mitigate this, we introduce the <code>scaling</code> parameter. If
<code>scaling=TRUE</code>, then we scale the responses and covariates for each
subgroup by the number of samples in that group.
</p>


<h3>Value</h3>

<p>A list with components X, Y, X.fused and penalty, where
X is a n by pK block-diagonal bigmatrix, Y is a
re-arranged bigvector of length n, and X.fused is a
choose(K,2)*p by pK bigmatrix encoding the fusion penalties.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

# Generate simple heterogeneous dataset
k = 4 # number of groups
p = 100 # number of covariates
n.group = 15 # number of samples per group
sigma = 0.05 # observation noise sd
groups = rep(1:k, each=n.group) # group indicators
# sparse linear coefficients
beta = matrix(0, p, k)
nonzero.ind = rbinom(p*k, 1, 0.025/k) # Independent coefficients
nonzero.shared = rbinom(p, 1, 0.025) # shared coefficients
beta[which(nonzero.ind==1)] = rnorm(sum(nonzero.ind), 1, 0.25)
beta[which(nonzero.shared==1),] = rnorm(sum(nonzero.shared), -1, 0.25)

X = lapply(1:k,
           function(k.i) matrix(rnorm(n.group*p),
                                n.group, p)) # covariates
y = sapply(1:k,
           function(k.i) X[[k.i]] %*% beta[,k.i] +
                           rnorm(n.group, 0, sigma)) # response
X = do.call('rbind', X)

# Pairwise Fusion strength hyperparameters (tau(k,k'))
# Same for all pairs in this example
G = matrix(1, k, k)

# Generate block diagonal matrices
transformed.data = generateBlockDiagonalMatrices(X, y, groups, G)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
