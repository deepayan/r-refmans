<!DOCTYPE html><html><head><title>Help for package kko</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kko}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#generate_data'><p>generate response from nonparametric additive model</p></a></li>
<li><a href='#kko'><p>variable selection for additive model via KKO</p></a></li>
<li><a href='#KO_evaluation'><p>evaluate performance of KKO selection</p></a></li>
<li><a href='#rk_fit'><p>nonparametric additive model seleciton via random kernel</p></a></li>
<li><a href='#rk_subsample'><p>compute selection frequency of rk_fit on subsamples</p></a></li>
<li><a href='#rk_tune'><p>tune random feature number for KKO.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Kernel Knockoffs Selection for Nonparametric Additive Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>A variable selection procedure, dubbed KKO, for nonparametric additive model with finite-sample false discovery rate control guarantee. The method integrates three key components: knockoffs, subsampling for stability, and random feature mapping for nonparametric function approximation. For more information, see the accompanying paper: Dai, X., Lyu, X., &amp; Li, L. (2021). “Kernel Knockoffs Selection for Nonparametric Additive Models”. arXiv preprint &lt;<a href="https://arxiv.org/abs/2105.11659">arXiv:2105.11659</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.3)</td>
</tr>
<tr>
<td>Imports:</td>
<td>grpreg, knockoff, doParallel, parallel, foreach, ExtDist</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, ggplot2</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>false</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Author:</td>
<td>Xiaowu Dai [aut],
  Xiang Lyu [aut, cre],
  Lexin Li [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Xiang Lyu &lt;xianglyu@berkeley.edu&gt;</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-01-31 03:32:30 UTC; xianglyu</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-02-01 09:10:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='generate_data'>generate response from nonparametric additive model</h2><span id='topic+generate_data'></span>

<h3>Description</h3>

<p>The function generate response from additive models of various components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_data(X, reg_coef, model = "linear", err_sd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_data_+3A_x">X</code></td>
<td>
<p>design matrix of additive model; rows are observations and columns are variables.</p>
</td></tr>
<tr><td><code id="generate_data_+3A_reg_coef">reg_coef</code></td>
<td>
<p>regression coefficient vector.</p>
</td></tr>
<tr><td><code id="generate_data_+3A_model">model</code></td>
<td>
<p>types of components. Default is &quot;linear&quot;. Other choices are
</p>

<table>
<tr>
 <td style="text-align: left;">
<code>linear</code>  </td><td style="text-align: left;">  linear regression.  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>poly</code>  </td><td style="text-align: left;">   polynomial of degree sampled from 2 to 4. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>sinpoly</code> </td><td style="text-align: left;"> sum of polynomial of sin and cos. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>sinratio</code>  </td><td style="text-align: left;">  ratio of sin. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>sinmix</code> </td><td style="text-align: left;">   sampled from poly and sinratio. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>
</td></tr>
<tr><td><code id="generate_data_+3A_err_sd">err_sd</code></td>
<td>
<p>standard deviation of regression error.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>reponse vector
</p>


<h3>Author(s)</h3>

<p>Xiaowu Dai, Xiang Lyu, Lexin Li
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=5 # number of predictors
s=2  # sparsity, number of nonzero component functions
sig_mag=100 # signal strength
n= 200 # sample size
model="poly" # component function type
X=matrix(rnorm(n*p),n,p) %*%chol(toeplitz(0.3^(0:(p-1))))   # generate design
reg_coef=c(rep(1,s),rep(0,p-s))  # regression coefficient
reg_coef=reg_coef*(2*(rnorm(p)&gt;0)-1)*sig_mag
y=generate_data(X,reg_coef,model) # reponse vector


</code></pre>

<hr>
<h2 id='kko'>variable selection for additive model via KKO</h2><span id='topic+kko'></span>

<h3>Description</h3>

<p>The function applys KKO to compute importance scores of components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kko(
  X,
  y,
  X_k,
  rfn_range = c(2, 3, 4),
  n_stb_tune = 50,
  n_stb = 100,
  cv_folds = 10,
  frac_stb = 1/2,
  nCores_para = 4,
  rkernel = c("laplacian", "gaussian", "cauchy"),
  rk_scale = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kko_+3A_x">X</code></td>
<td>
<p>design matrix of additive model; rows are observations and columns are variables.</p>
</td></tr>
<tr><td><code id="kko_+3A_y">y</code></td>
<td>
<p>response of addtive model.</p>
</td></tr>
<tr><td><code id="kko_+3A_x_k">X_k</code></td>
<td>
<p>knockoffs matrix of design; the same size as X.</p>
</td></tr>
<tr><td><code id="kko_+3A_rfn_range">rfn_range</code></td>
<td>
<p>a vector of random feature expansion numbers to be tuned.</p>
</td></tr>
<tr><td><code id="kko_+3A_n_stb_tune">n_stb_tune</code></td>
<td>
<p>number of subsampling for tuning random feature numbers.</p>
</td></tr>
<tr><td><code id="kko_+3A_n_stb">n_stb</code></td>
<td>
<p>number of subsampling for computing importance scores.</p>
</td></tr>
<tr><td><code id="kko_+3A_cv_folds">cv_folds</code></td>
<td>
<p>the folds of cross-validation for tuning group lasso penalty.</p>
</td></tr>
<tr><td><code id="kko_+3A_frac_stb">frac_stb</code></td>
<td>
<p>fraction of subsample size.</p>
</td></tr>
<tr><td><code id="kko_+3A_ncores_para">nCores_para</code></td>
<td>
<p>number of cores for parallelizing subsampling.</p>
</td></tr>
<tr><td><code id="kko_+3A_rkernel">rkernel</code></td>
<td>
<p>kernel choices. Default is &quot;laplacian&quot;. Other choices are &quot;cauchy&quot; and &quot;gaussian&quot;.</p>
</td></tr>
<tr><td><code id="kko_+3A_rk_scale">rk_scale</code></td>
<td>
<p>scale parameter of sampling distribution for random feature expansion. For gaussian kernel, it is standard deviation of gaussian sampling distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of selection results.
</p>

<table>
<tr>
 <td style="text-align: left;">
<code>importance_score</code>  </td><td style="text-align: left;">  importance scores of variables for knockoff filtering.  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>selection_frequency</code>  </td><td style="text-align: left;"> a 0/1 matrix of selection results on subsamples.
Rows are subsamples, and columns are variables.
The first half columns are variables of design X, and the latter are knockoffs X_k  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>rfn_tune</code>  </td><td style="text-align: left;">  tuned optimal random feature number.  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>rfn_range</code>  </td><td style="text-align: left;"> range of random feature numbers. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>tune_result</code> </td><td style="text-align: left;"> a list of tuning results.  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Xiaowu Dai, Xiang Lyu, Lexin Li
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(knockoff)
p=4 # number of predictors
sig_mag=100 # signal strength
n= 100 # sample size
rkernel="laplacian" # kernel choice
s=2  # sparsity, number of nonzero component functions
rk_scale=1  # scaling paramtere of kernel
rfn_range=c(2,3,4)  # number of random features
cv_folds=15  # folds of cross-validation in group lasso
n_stb=10 # number of subsampling for importance scores
n_stb_tune=5 # number of subsampling for tuning random feature number
frac_stb=1/2 # fraction of subsample
nCores_para=2 # number of cores for parallelization
X=matrix(rnorm(n*p),n,p)%*%chol(toeplitz(0.3^(0:(p-1))))   # generate design
X_k = create.second_order(X) # generate knockoff
reg_coef=c(rep(1,s),rep(0,p-s))  # regression coefficient
reg_coef=reg_coef*(2*(rnorm(p)&gt;0)-1)*sig_mag
y=X%*% reg_coef + rnorm(n) # response

kko(X,y,X_k,rfn_range,n_stb_tune,n_stb,cv_folds,frac_stb,nCores_para,rkernel,rk_scale)



</code></pre>

<hr>
<h2 id='KO_evaluation'>evaluate performance of KKO selection</h2><span id='topic+KO_evaluation'></span>

<h3>Description</h3>

<p>The function computes {FDP, FPR, TPR} of selection by knockoff filtering on importance scores of KKO.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KO_evaluation(W, reg_coef, fdr_range = 0.2, offset = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KO_evaluation_+3A_w">W</code></td>
<td>
<p>importance scores of variables.</p>
</td></tr>
<tr><td><code id="KO_evaluation_+3A_reg_coef">reg_coef</code></td>
<td>
<p>true regression coefficient.</p>
</td></tr>
<tr><td><code id="KO_evaluation_+3A_fdr_range">fdr_range</code></td>
<td>
<p>FDR control levels of knockoff filter.</p>
</td></tr>
<tr><td><code id="KO_evaluation_+3A_offset">offset</code></td>
<td>
<p>0/1. If 1, knockoff+ filter. Otherwise, knockoff filter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>FDP, FPR, TPR of knockoff filtering at fdr_range.
</p>


<h3>Author(s)</h3>

<p>Xiaowu Dai, Xiang Lyu, Lexin Li
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(knockoff)
p=5 # number of predictors
sig_mag=100 # signal strength
n= 100 # sample size
rkernel="laplacian" # kernel choice
s=2  # sparsity, number of nonzero component functions
rk_scale=1  # scaling paramtere of kernel
rfn_range=c(2,3,4)  # number of random features
cv_folds=15  # folds of cross-validation in group lasso
n_stb=10 # number of subsampling for importance scores
n_stb_tune=5 # number of subsampling for tuning random feature number
frac_stb=1/2 # fraction of subsample
nCores_para=2 # number of cores for parallelization
X=matrix(rnorm(n*p),n,p)%*%chol(toeplitz(0.3^(0:(p-1))))   # generate design
X_k = create.second_order(X) # generate knockoff
reg_coef=c(rep(1,s),rep(0,p-s))  # regression coefficient
reg_coef=reg_coef*(2*(rnorm(p)&gt;0)-1)*sig_mag
y=X%*% reg_coef + rnorm(n) # response

kko_fit=kko(X,y,X_k,rfn_range,n_stb_tune,n_stb,cv_folds,frac_stb,nCores_para,rkernel,rk_scale)
W=kko_fit$importance_score
fdr_range=c(0.2,0.3,0.4,0.5)
KO_evaluation(W,reg_coef,fdr_range,offset=1)


</code></pre>

<hr>
<h2 id='rk_fit'>nonparametric additive model seleciton via random kernel</h2><span id='topic+rk_fit'></span>

<h3>Description</h3>

<p>The function selects additive components via applying group lasso on random feature expansion of data and knockoffs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rk_fit(
  X,
  y,
  X_k,
  rfn,
  cv_folds,
  rkernel = "laplacian",
  rk_scale = 1,
  rseed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rk_fit_+3A_x">X</code></td>
<td>
<p>design matrix of additive model; rows are observations and columns are variables.</p>
</td></tr>
<tr><td><code id="rk_fit_+3A_y">y</code></td>
<td>
<p>response of addtive model.</p>
</td></tr>
<tr><td><code id="rk_fit_+3A_x_k">X_k</code></td>
<td>
<p>knockoffs matrix of design; the same size as X.</p>
</td></tr>
<tr><td><code id="rk_fit_+3A_rfn">rfn</code></td>
<td>
<p>random feature expansion number.</p>
</td></tr>
<tr><td><code id="rk_fit_+3A_cv_folds">cv_folds</code></td>
<td>
<p>the folds of cross-validation for tuning group lasso penalty.</p>
</td></tr>
<tr><td><code id="rk_fit_+3A_rkernel">rkernel</code></td>
<td>
<p>kernel choices. Default is &quot;laplacian&quot;. Other choices are &quot;cauchy&quot; and &quot;gaussian&quot;.</p>
</td></tr>
<tr><td><code id="rk_fit_+3A_rk_scale">rk_scale</code></td>
<td>
<p>scaling parameter of sampling distribution for random feature expansion. For gaussian kernel, it is standard deviation of gaussian sampling distribution.</p>
</td></tr>
<tr><td><code id="rk_fit_+3A_rseed">rseed</code></td>
<td>
<p>seed for random feature expansion.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 0/1 vector indicating selected components.
</p>


<h3>Author(s)</h3>

<p>Xiaowu Dai, Xiang Lyu, Lexin Li
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(knockoff)
p=5 # number of predictors
sig_mag=100 # signal strength
n= 200 # sample size
rkernel="laplacian" # kernel choice
s=2  # sparsity, number of nonzero component functions
rk_scale=1  # scaling paramtere of kernel
rfn= 3  # number of random features
cv_folds=15  # folds of cross-validation in group lasso
X=matrix(rnorm(n*p),n,p)%*%chol(toeplitz(0.3^(0:(p-1))))   # generate design
X_k = create.second_order(X) # generate knockoff
reg_coef=c(rep(1,s),rep(0,p-s))  # regression coefficient
reg_coef=reg_coef*(2*(rnorm(p)&gt;0)-1)*sig_mag
y=X%*% reg_coef + rnorm(n) # response

# the first half is variables of design X, and the latter is knockoffs X_k
rk_fit(X,y,X_k,rfn,cv_folds,rkernel,rk_scale)


</code></pre>

<hr>
<h2 id='rk_subsample'>compute selection frequency of rk_fit on subsamples</h2><span id='topic+rk_subsample'></span>

<h3>Description</h3>

<p>The function applys rk_fit on subsamples and record selection results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rk_subsample(
  X,
  y,
  X_k,
  rfn,
  n_stb,
  cv_folds,
  frac_stb = 1/2,
  nCores_para,
  rkernel = "laplacian",
  rk_scale = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rk_subsample_+3A_x">X</code></td>
<td>
<p>design matrix of additive model; rows are observations and columns are variables.</p>
</td></tr>
<tr><td><code id="rk_subsample_+3A_y">y</code></td>
<td>
<p>response of addtive model.</p>
</td></tr>
<tr><td><code id="rk_subsample_+3A_x_k">X_k</code></td>
<td>
<p>knockoffs matrix of design; the same size as X.</p>
</td></tr>
<tr><td><code id="rk_subsample_+3A_rfn">rfn</code></td>
<td>
<p>random feature expansion number.</p>
</td></tr>
<tr><td><code id="rk_subsample_+3A_n_stb">n_stb</code></td>
<td>
<p>number of subsampling.</p>
</td></tr>
<tr><td><code id="rk_subsample_+3A_cv_folds">cv_folds</code></td>
<td>
<p>the folds of cross-validation for tuning group lasso.</p>
</td></tr>
<tr><td><code id="rk_subsample_+3A_frac_stb">frac_stb</code></td>
<td>
<p>fraction of subsample size.</p>
</td></tr>
<tr><td><code id="rk_subsample_+3A_ncores_para">nCores_para</code></td>
<td>
<p>number of cores for parallelizing subsampling.</p>
</td></tr>
<tr><td><code id="rk_subsample_+3A_rkernel">rkernel</code></td>
<td>
<p>kernel choices. Default is &quot;laplacian&quot;. Other choices are &quot;cauchy&quot; and &quot;gaussian&quot;.</p>
</td></tr>
<tr><td><code id="rk_subsample_+3A_rk_scale">rk_scale</code></td>
<td>
<p>scaling parameter of sampling distribution for random feature expansion. For gaussian kernel, it is standard deviation of gaussian sampling distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 0/1 matrix indicating selection results. Rows are subsamples, and columns are variables.
The first half columns are variables of design X, and the latter are knockoffs X_k.
</p>


<h3>Author(s)</h3>

<p>Xiaowu Dai, Xiang Lyu, Lexin Li
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(knockoff)
p=5 # number of predictors
sig_mag=100 # signal strength
n= 100 # sample size
rkernel="laplacian" # kernel choice
s=2  # sparsity, number of nonzero component functions
rk_scale=1  # scaling paramtere of kernel
rfn= 3  # number of random features
cv_folds=15  # folds of cross-validation in group lasso
n_stb=10 # number of subsampling
frac_stb=1/2 # fraction of subsample
nCores_para=2 # number of cores for parallelization
X=matrix(rnorm(n*p),n,p)%*%chol(toeplitz(0.3^(0:(p-1))))   # generate design
X_k = create.second_order(X) # generate knockoff
reg_coef=c(rep(1,s),rep(0,p-s))  # regression coefficient
reg_coef=reg_coef*(2*(rnorm(p)&gt;0)-1)*sig_mag
y=X%*% reg_coef + rnorm(n) # response

rk_subsample(X,y,X_k,rfn,n_stb,cv_folds,frac_stb,nCores_para,rkernel,rk_scale)


</code></pre>

<hr>
<h2 id='rk_tune'>tune random feature number for KKO.</h2><span id='topic+rk_tune'></span>

<h3>Description</h3>

<p>The function applys KKO with different random feature numbers to tune the optimal number.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rk_tune(
  X,
  y,
  X_k,
  rfn_range,
  n_stb,
  cv_folds,
  frac_stb = 1/2,
  nCores_para = 1,
  rkernel = "laplacian",
  rk_scale = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rk_tune_+3A_x">X</code></td>
<td>
<p>design matrix of additive model; rows are observations and columns are variables.</p>
</td></tr>
<tr><td><code id="rk_tune_+3A_y">y</code></td>
<td>
<p>response of addtive model.</p>
</td></tr>
<tr><td><code id="rk_tune_+3A_x_k">X_k</code></td>
<td>
<p>knockoffs matrix of design; the same size as X.</p>
</td></tr>
<tr><td><code id="rk_tune_+3A_rfn_range">rfn_range</code></td>
<td>
<p>a vector of random feature expansion numbers to be tuned.</p>
</td></tr>
<tr><td><code id="rk_tune_+3A_n_stb">n_stb</code></td>
<td>
<p>number of subsampling in KKO.</p>
</td></tr>
<tr><td><code id="rk_tune_+3A_cv_folds">cv_folds</code></td>
<td>
<p>the folds of cross-validation for tuning group lasso.</p>
</td></tr>
<tr><td><code id="rk_tune_+3A_frac_stb">frac_stb</code></td>
<td>
<p>fraction of subsample.</p>
</td></tr>
<tr><td><code id="rk_tune_+3A_ncores_para">nCores_para</code></td>
<td>
<p>number of cores for parallelizing subsampling.</p>
</td></tr>
<tr><td><code id="rk_tune_+3A_rkernel">rkernel</code></td>
<td>
<p>kernel choices. Default is &quot;laplacian&quot;. Other choices are &quot;cauchy&quot; and &quot;gaussian&quot;.</p>
</td></tr>
<tr><td><code id="rk_tune_+3A_rk_scale">rk_scale</code></td>
<td>
<p>scaling parameter of sampling distribution for random feature expansion. For gaussian kernel, it is standard deviation of gaussian sampling distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of tuning results.
</p>

<table>
<tr>
 <td style="text-align: left;">
<code>rfn_tune</code>  </td><td style="text-align: left;">  tuned optimal random feature number.  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>rfn_range</code>  </td><td style="text-align: left;"> a vector of random feature expansion numbers to be tuned. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>scores</code> </td><td style="text-align: left;"> scores of random feature numbers. rfn_tune has the maximal score. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>Pi_list</code> </td><td style="text-align: left;"> a list of subsample selection results for each random feature number. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Xiaowu Dai, Xiang Lyu, Lexin Li
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(knockoff)
p=5 # number of predictors
sig_mag=100 # signal strength
n= 100 # sample size
rkernel="laplacian" # kernel choice
s=2  # sparsity, number of nonzero component functions
rk_scale=1  # scaling paramtere of kernel
rfn_range= c(2,3,4)  # number of random features
cv_folds=15  # folds of cross-validation in group lasso
n_stb=10 # number of subsampling
frac_stb=1/2 # fraction of subsample
nCores_para=2 # number of cores for parallelization
X=matrix(rnorm(n*p),n,p)%*%chol(toeplitz(0.3^(0:(p-1))))   # generate design
X_k = create.second_order(X) # generate knockoff
reg_coef=c(rep(1,s),rep(0,p-s))  # regression coefficient
reg_coef=reg_coef*(2*(rnorm(p)&gt;0)-1)*sig_mag
y=X%*% reg_coef + rnorm(n) # response

rk_tune(X,y,X_k,rfn_range,n_stb,cv_folds,frac_stb,nCores_para,rkernel,rk_scale)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
