<!DOCTYPE html><html><head><title>Help for package glmpca</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {glmpca}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#glmpca'><p>GLM-PCA</p></a></li>
<li><a href='#predict.glmpca'><p>Predict Method for GLM-PCA Fits</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Dimension Reduction of Non-Normally Distributed Data</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements a generalized version of principal components analysis
    (GLM-PCA) for dimension reduction of non-normally distributed data such as
    counts or binary matrices.
    Townes FW, Hicks SC, Aryee MJ, Irizarry RA (2019) &lt;<a href="https://doi.org/10.1186%2Fs13059-019-1861-6">doi:10.1186/s13059-019-1861-6</a>&gt;.
    Townes FW (2019) &lt;<a href="https://arxiv.org/abs/1907.02647">arXiv:1907.02647</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL (&ge; 3)</a> | file LICENSE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5),</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, methods, stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, ggplot2, knitr, logisticPCA, markdown, Matrix,
testthat,</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/willtownes/glmpca">https://github.com/willtownes/glmpca</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/willtownes/glmpca/issues">https://github.com/willtownes/glmpca/issues</a></td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>false</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-07-18 17:07:28 UTC; townesf</td>
</tr>
<tr>
<td>Author:</td>
<td>F. William Townes [aut, cre, cph],
  Kelly Street [aut],
  Jake Yeung [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>F. William Townes &lt;will.townes@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-07-18 17:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='glmpca'>GLM-PCA</h2><span id='topic+glmpca'></span>

<h3>Description</h3>

<p>Generalized principal components analysis for 
dimension reduction of non-normally distributed data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glmpca(
  Y,
  L,
  fam = c("poi", "nb", "nb2", "binom", "mult", "bern"),
  minibatch = c("none", "stochastic", "memoized"),
  optimizer = c("avagrad", "fisher"),
  ctl = list(),
  sz = NULL,
  nb_theta = NULL,
  X = NULL,
  Z = NULL,
  init = list(factors = NULL, loadings = NULL),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glmpca_+3A_y">Y</code></td>
<td>
<p>matrix-like object of count or binary data with features as rows 
and observations as columns. Sparse matrices from the <code>Matrix</code> 
package are supported. Column-oriented sparsity is preferred.</p>
</td></tr>
<tr><td><code id="glmpca_+3A_l">L</code></td>
<td>
<p>desired number of latent dimensions (positive integer).</p>
</td></tr>
<tr><td><code id="glmpca_+3A_fam">fam</code></td>
<td>
<p>string describing the likelihood to use for the data. Families
include Poisson ('<code>poi</code>'), negative binomial with global 
overdispersion ('<code>nb</code>'), negative binomial with feature-specific 
overdispersion ('<code>nb2</code>'), or binomial ('<code>binom</code>'). Families 
'<code>mult</code>' and '<code>bern</code>' are deprecated as both are special cases of
'<code>binom</code>' with <code>sz</code> set to NULL and 1, respectively. They are 
provided only for backward compatibility. Family '<code>nb2</code>' has not been
thoroughly tested and is considered experimental.</p>
</td></tr>
<tr><td><code id="glmpca_+3A_minibatch">minibatch</code></td>
<td>
<p>string describing whether gradients should be computed with
all observations ('<code>none</code>', the default) or a subset of observations, 
which is useful for larger datasets. Option '<code>stochastic</code>' computes
a noisy estimate of the full gradient using a random sample of observations
at each iteration. Option '<code>memoized</code>' computes the full data 
gradient under memory constraints by caching summary statistics across
batches of observations.</p>
</td></tr>
<tr><td><code id="glmpca_+3A_optimizer">optimizer</code></td>
<td>
<p>string describing whether to use the fast AvaGrad method
('<code>avagrad</code>', the default) or the slower diagonal Fisher scoring 
method ('<code>fisher</code>') that was used in the original glmpca 
implementation.</p>
</td></tr>
<tr><td><code id="glmpca_+3A_ctl">ctl</code></td>
<td>
<p>a list of control parameters. See 'Details'</p>
</td></tr>
<tr><td><code id="glmpca_+3A_sz">sz</code></td>
<td>
<p>numeric vector of size factors for each observation. If NULL 
(the default), colSums are used for family '<code>binom</code>', and 
colMeans are used for families '<code>poi</code>','<code>nb</code>', and '<code>nb2</code>'.</p>
</td></tr>
<tr><td><code id="glmpca_+3A_nb_theta">nb_theta</code></td>
<td>
<p>initial value for negative binomial overdispersion 
parameter(s). Small values lead to more overdispersion. Default: 100. See
<code><a href="MASS.html#topic+negative.binomial">negative.binomial</a></code>. (<code>nb_theta</code>-&gt;<code class="reqn">\infty</code>
equivalent to Poisson).</p>
</td></tr>
<tr><td><code id="glmpca_+3A_x">X</code></td>
<td>
<p>a matrix of column (observations) covariates. Any column with all
same values (eg. 1 for intercept) will be removed. This is because we force
a feature-specific intercept and want to avoid collinearity.</p>
</td></tr>
<tr><td><code id="glmpca_+3A_z">Z</code></td>
<td>
<p>a matrix of row (feature) covariates, usually not needed.</p>
</td></tr>
<tr><td><code id="glmpca_+3A_init">init</code></td>
<td>
<p>a list containing initial estimates for the factors (<code>U</code>) 
and loadings (<code>V</code>) matrices.</p>
</td></tr>
<tr><td><code id="glmpca_+3A_...">...</code></td>
<td>
<p>additional named arguments. Provided only for backward 
compatibility.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic model is <code class="reqn">R = AX'+ZG'+VU'</code>, where <code class="reqn">E[Y] = M
  = linkinv(R)</code>. Regression coefficients are <code>A</code> and <code>G</code>, latent
factors are <code>U</code> and loadings are <code>V</code>. 
The objective is to minimize the deviance between <code>Y</code> 
and <code>M</code>. The deviance quantifies the goodness-of-fit of the GLM-PCA
model to the data (smaller=better). 
Note that <code>glmpca</code> uses a random initialization, 
so for fully reproducible results one may use <code>set.seed</code>.
</p>
<p>The <code>ctl</code> argument accepts any of the following optional components:
</p>

<dl>
<dt>verbose</dt><dd><p>Logical. Should detailed status messages be printed 
during the optimization run? Default: <code>FALSE</code>.</p>
</dd>
<dt>batch_size</dt><dd><p>Positive integer. How many observations should be
included in a minibatch? Larger values use more memory but lead to 
more accurate gradient estimation. Ignored if <code>minibatch='none'</code>.
Default: 1000.</p>
</dd>
<dt>lr</dt><dd><p>Positive scalar. The AvaGrad learning rate. Large values
enable faster convergence but can lead to numerical instability.
Default: 0.1. If a numerical divergence occurs, <code>glmpca</code>
will restart the optimization <code>maxTry</code> times (see below)
and reduce the learning rate by a factor of five each time.</p>
</dd>
<dt>penalty</dt><dd><p>Non-negative scalar. The L2 penalty for the latent 
factors. Default: 1. Regression coefficients are not penalized. Only
used by the Fisher scoring optimizer. Larger values improve numerical
stability but bias the parameter estimates. If a numerical divergence 
occurs, <code>glmpca</code> will restart the optimization <code>maxTry</code> times
(see below) and increase the penalty by a factor of five each time.</p>
</dd>
<dt>maxTry</dt><dd><p>Positive integer. In case of numerical divergence, how
many times should optimization be restarted with a more stable penalty
or learning rate? Default: 10.</p>
</dd>
<dt>minIter</dt><dd><p>Positive integer. Minimum number of iterations (full
passes through the dataset) before checking for numerical convergence.
Default: 30.</p>
</dd>
<dt>maxIter</dt><dd><p>Positive integer. Maximum number of iterations. If
numerical convergence is not achieved by this point, the results may
not be reliable and a warning is issued. Default: 1000.</p>
</dd>
<dt>tol</dt><dd><p>Positive scalar. Relative tolerance for assessing convergence.
Convergence is determined by comparing the deviance at the previous
iteration to the current iteration. Default: 1e-4.</p>
</dd>
<dt>epsilon</dt><dd><p>Positive scalar. AvaGrad hyperparameter. See Savarese et 
al (2020). Default: 0.1.</p>
</dd>
<dt>betas</dt><dd><p>Numeric vector of length two. AvaGrad hyperparameters. 
See Savarese et al (2020). Default: <code>c(0.9, 0.999)</code>.</p>
</dd>
<dt>minDev</dt><dd><p>Scalar. Minimum deviance threshold at which optimization
is terminated. Useful for comparing different algorithms as it avoids
the need to determine numerical convergence. Default: NULL</p>
</dd>
</dl>



<h3>Value</h3>

<p>An S3 object of class <code>glmpca</code> with copies of input components 
<code>optimizer</code>, <code>minibatch</code>, <code>ctl</code>,<code>X</code>, and <code>Z</code>,
along with the following additional fitted components:
</p>

<dl>
<dt>factors</dt><dd><p>a matrix <code>U</code> whose rows match the columns 
(observations) of <code>Y</code>. It is analogous to the principal components
in PCA. Each column of the factors matrix is a different latent 
dimension.</p>
</dd>
<dt>loadings</dt><dd><p>a matrix <code>V</code> whose rows match the rows 
(features/dimensions) of <code>Y</code>. It is analogous to loadings in PCA. 
Each column of the loadings matrix is a different latent dimension.</p>
</dd>
<dt>coefX</dt><dd><p>a matrix <code>A</code> of coefficients for the 
observation-specific covariates matrix <code>X</code>. Each row of coefX 
corresponds to a row of <code>Y</code> and each column corresponds to a 
column of <code>X</code>. The first column of coefX contains feature-specific 
intercepts which are included by default.</p>
</dd>
<dt>coefZ</dt><dd><p>a matrix <code>G</code> of coefficients for the feature-specific 
covariates matrix <code>Z</code>. Each row of coefZ corresponds to a column 
of <code>Y</code> and each column corresponds to a column of <code>Z</code>. By 
default no such covariates are included and this is returned as NULL.</p>
</dd>
<dt>dev</dt><dd><p>a vector of deviance values. The length of the vector is the 
number of iterations it took for GLM-PCA's optimizer to converge. 
The deviance should generally decrease over time. 
If it fluctuates wildly, this often indicates numerical instability, 
which can be improved by decreasing the learning rate or increasing the 
penalty, see <code>ctl</code>.</p>
</dd>
<dt>dev_smooth</dt><dd><p>a locally smoothed version of <code>dev</code> that may be
easier to visualize when <code>minibatch='stochastic'</code>.</p>
</dd>
<dt>glmpca_family</dt><dd><p>an S3 object of class glmpca_family. This is a minor
extension to the <a href="stats.html#topic+family">family</a> or <a href="MASS.html#topic+negative.binomial">negative.binomial</a>
object used by functions like <a href="stats.html#topic+glm">glm</a> and 
<a href="MASS.html#topic+glm.nb">glm.nb</a>. It is basically a list with various internal 
functions and parameters needed to optimize the GLM-PCA objective 
function. For the negative binomial case, it also contains the final 
estimated value of the overdispersion parameter (<code>nb_theta</code>).</p>
</dd>
<dt>offsets</dt><dd><p>For Poisson and negative binomial families, the offsets
are the logarithmically transformed size factors. These are needed to
compute the predicted mean values.</p>
</dd>
</dl>



<h3>References</h3>

<p>Savarese P, McAllester D, Babu S, and Maire M (2020).
Domain-independent Dominance of Adaptive Methods. <em>arXiv</em>
<a href="https://arxiv.org/abs/1912.01823">https://arxiv.org/abs/1912.01823</a>
</p>
<p>Townes FW (2019). Generalized Principal Component Analysis. <em>arXiv</em>
<a href="https://arxiv.org/abs/1907.02647">https://arxiv.org/abs/1907.02647</a>
</p>
<p>Townes FW, Hicks SC, Aryee MJ, and Irizarry RA (2019).
Feature Selection and Dimension Reduction for Single Cell RNA-Seq based on a 
Multinomial Model. <em>Genome Biology</em>
<a href="https://doi.org/10.1186/s13059-019-1861-6">https://doi.org/10.1186/s13059-019-1861-6</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.glmpca">predict.glmpca</a></code>, 
<code><a href="stats.html#topic+prcomp">prcomp</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>,
<code><a href="logisticPCA.html#topic+logisticSVD">logisticSVD</a></code>,
<code>scry::devianceFeatureSelection</code>, 
<code>scry::nullResiduals</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#create a simple dataset with two clusters
mu&lt;-rep(c(.5,3),each=10)
mu&lt;-matrix(exp(rnorm(100*20)),nrow=100)
mu[,1:10]&lt;-mu[,1:10]*exp(rnorm(100))
clust&lt;-rep(c("red","black"),each=10)
Y&lt;-matrix(rpois(prod(dim(mu)),mu),nrow=nrow(mu))
#visualize the latent structure
res&lt;-glmpca(Y, 2)
factors&lt;-res$factors
plot(factors[,1],factors[,2],col=clust,pch=19)

</code></pre>

<hr>
<h2 id='predict.glmpca'>Predict Method for GLM-PCA Fits</h2><span id='topic+predict.glmpca'></span>

<h3>Description</h3>

<p>Predict the mean matrix from a fitted generalized principal
component analysis model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glmpca'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.glmpca_+3A_object">object</code></td>
<td>
<p>a fitted object of class inheriting from <code>glmpca</code>.</p>
</td></tr>
<tr><td><code id="predict.glmpca_+3A_...">...</code></td>
<td>
<p>additional named arguments. Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code>Y</code> be the data matrix originally used to estimate the
parameters in <code>fit</code>. The GLM-PCA model regards each element of 
<code>Y</code> as a random sample from an exponential family distribution 
such as a Poisson, negative binomial, or binomial likelihood. The 
components of a GLM-PCA fit are combined to produce the predicted 
mean of this distribution at each entry of <code>Y</code>. This matrix may be
regarded as a 'denoised' version of the original data.
</p>


<h3>Value</h3>

<p>a dense <code>matrix</code> of predicted mean values.
</p>


<h3>Warning</h3>

<p>The predicted mean matrix returned by this function
will have the same dimensions as the original data matrix and it will be
dense even if the original data were sparse. This can exhaust available
memory for large datasets, so use with caution.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glmpca">glmpca</a></code>, 
<code><a href="stats.html#topic+predict.glm">predict.glm</a></code> with <code>type='response'</code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
