<!DOCTYPE html><html lang="en"><head><title>Help for package transformerForecasting</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {transformerForecasting}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#install_r_dependencies'><p>Install Package Dependencies</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#S_P_500_Close'><p>S&amp;P 500's closing price data</p></a></li>
<li><a href='#TRANSFORMER'><p>Transformer Model for Time Series Forecasting</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Transformer Deep Learning Model for Time Series Forecasting</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>G H Harish Nayak &lt;harishnayak626@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Time series forecasting faces challenges due to the non-stationarity, nonlinearity, and chaotic nature of the data. Traditional deep learning models like Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU) process data sequentially but are inefficient for long sequences. To overcome the limitations of these models, we proposed a transformer-based deep learning architecture utilizing an attention mechanism for parallel processing, enhancing prediction accuracy and efficiency. This paper presents user-friendly code for the implementation of the proposed transformer-based deep learning architecture utilizing an attention mechanism for parallelÂ processing. References:  Nayak et al. (2024) &lt;<a href="https://doi.org/10.1007%2Fs40808-023-01944-7">doi:10.1007/s40808-023-01944-7</a>&gt; and Nayak et al. (2024) &lt;<a href="https://doi.org/10.1016%2Fj.simpa.2024.100716">doi:10.1016/j.simpa.2024.100716</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2, keras, tensorflow, magrittr, reticulate (&ge; 1.20)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>dplyr, knitr, lubridate, readr, rmarkdown, utils</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Author:</td>
<td>G H Harish Nayak [aut, cre],
  Md Wasi Alam [ths],
  B Samuel Naik [ctb],
  G Avinash [ctb],
  Kabilan S [ctb],
  Varshini B S [ctb],
  Mrinmoy Ray [ths],
  Rajeev Ranjan Kumar [ths]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-05 04:28:04 UTC; kabil</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-07 11:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='install_r_dependencies'>Install Package Dependencies</h2><span id='topic+install_r_dependencies'></span>

<h3>Description</h3>

<p>Installs Python dependencies (TensorFlow, Keras, Pandas) in a Conda environment for the transformerForecasting package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_r_dependencies(env_name = "r-reticulate")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="install_r_dependencies_+3A_env_name">env_name</code></td>
<td>
<p>Character string specifying the Conda environment name (default: &quot;r-reticulate&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns NULL after attempting to install dependencies.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  install_r_dependencies()

## End(Not run)
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>magrittr</dt><dd><p><code><a href="magrittr.html#topic+pipe">%&gt;%</a></code></p>
</dd>
</dl>

<hr>
<h2 id='S_P_500_Close'>S&amp;P 500's closing price data</h2><span id='topic+S_P_500_Close'></span>

<h3>Description</h3>

<p>The S&amp;P 500's closing price as data provides an excellent real-world application.
The S&amp;P 500 index, a key benchmark for U.S. stock market performance, is vital for
portfolio management, risk assessment, and market analysis. The original data frame
contains 1340 rows and 2 variables. Here, we have taken only 200 rows and 2 variables
for demonstration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>S_P_500_Close
</code></pre>


<h3>Format</h3>

<p>A data frame with 200 rows and 2 variables:
</p>

<dl>
<dt>Date</dt><dd><p>Formatted date</p>
</dd>
<dt>Price</dt><dd><p>Numerical price values</p>
</dd>
</dl>


<hr>
<h2 id='TRANSFORMER'>Transformer Model for Time Series Forecasting</h2><span id='topic+TRANSFORMER'></span>

<h3>Description</h3>

<p>Transformer model for time series forecasting
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TRANSFORMER(
  df,
  study_variable,
  sequence_size = 10,
  head_size = 512,
  num_heads = 4,
  ff_dim = 4,
  num_transformer_blocks = 4,
  mlp_units = c(128),
  mlp_dropout = 0.4,
  dropout = 0.25,
  epochs = 300,
  batch_size = 64,
  patience = 10
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="TRANSFORMER_+3A_df">df</code></td>
<td>
<p>Input file</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_study_variable">study_variable</code></td>
<td>
<p>The Study_Variable represents the primary variable of interest in the dataset, (Ex:Closing price)</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_sequence_size">sequence_size</code></td>
<td>
<p>Sequence size</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_head_size">head_size</code></td>
<td>
<p>Attention head size</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_num_heads">num_heads</code></td>
<td>
<p>Number of attention heads</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_ff_dim">ff_dim</code></td>
<td>
<p>Size of feed-forward network</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_num_transformer_blocks">num_transformer_blocks</code></td>
<td>
<p>Number of transformer blocks</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_mlp_units">mlp_units</code></td>
<td>
<p>Units for MLP layers</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_mlp_dropout">mlp_dropout</code></td>
<td>
<p>Dropout rate for MLP</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_dropout">dropout</code></td>
<td>
<p>Dropout rate for transformer</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_epochs">epochs</code></td>
<td>
<p>Number of epochs</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_batch_size">batch_size</code></td>
<td>
<p>Batch size</p>
</td></tr>
<tr><td><code id="TRANSFORMER_+3A_patience">patience</code></td>
<td>
<p>Early stopping patience</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates and trains a Transformer-based model for time series
forecasting using the Keras library. It allows customization of key architectural
parameters such as sequence size, attention head size, number of attention heads,
feed-forward network dimensions, number of Transformer blocks, and MLP
(multi-layer perceptron) configurations including units and dropout rates.
</p>
<p>Before running this function, we advise the users to install Python in your system and create the virtual conda environment.
Installation of the modules such as 'tensorflow', 'keras' and 'pandas' are necessary for this package. If the user
does not know about these steps, they can use the <code>install_r_dependencies()</code> function which is available in this package.
</p>
<p>The function begins by generating training sequences from the input data (<code>df</code>)
based on the specified <code>sequence_size</code>. Sliding windows of input sequences are
created as <code>x</code>, while the subsequent values in the series are used as targets (<code>y</code>).
</p>
<p>The model architecture includes an input layer, followed by one or more Transformer
encoder blocks, a global average pooling layer for feature aggregation, and MLP
layers for further processing. The final output layer is designed for the forecasting task.
</p>
<p>The model is compiled using the Adam optimizer and the mean squared error (MSE)
loss function. Training is performed with the specified number of <code>epochs</code>,
<code>batch_size</code>, and early stopping configured through the <code>patience</code> parameter.
During training, 20% of the data is used for validation, and the best model weights
are restored when validation performance stops improving.
</p>
<p>The package requires a dataset with two columns: Date (formatted as dates) and the Close price (numerical).
After loading the data and formatting it appropriately, the TRANSFORMER function
trains a Transformer-based model to predict future closing prices. It outputs
essential performance metrics like RMSE, MAPE, and sMAPE, along with visualizations
such as training loss trends and an actual vs. predicted plot. These features make
it an invaluable tool for understanding and forecasting stock market trendsÂ effectively..
</p>


<h3>Value</h3>

<p>A list containing the following results:
</p>

<ul>
<li> <p><code>PREDICTIONS</code>: The predicted values generated by the model.
</p>
</li>
<li> <p><code>RMSE</code>: Root Mean Squared Error, measuring the average magnitude of the prediction error.
</p>
</li>
<li> <p><code>MAPE</code>: Mean Absolute Percentage Error, representing the prediction accuracy as a percentage.
</p>
</li>
<li> <p><code>MAE</code>: Mean Absolute Error, showing the average absolute difference between actual and predicted values.
</p>
</li>
<li> <p><code>MSE</code>: Mean Squared Error, quantifying the average squared difference between actual and predicted values.
</p>
</li>
<li> <p><code>sMAPE</code>: Symmetric Mean Absolute Percentage Error, a variant of MAPE considering both over- and under-predictions.
</p>
</li>
<li> <p><code>RRMSE</code>: Relative Root Mean Squared Error, RMSE scaled by the mean of the actual values.
</p>
</li>
<li> <p><code>Quantile_Loss</code>: The quantile loss metric for probabilistic forecasting.
</p>
</li>
<li> <p><code>Loss_plot</code>: A ggplot object showing the loss curve over iterations or epochs.
</p>
</li>
<li> <p><code>Actual_vs_Predicted</code>: A ggplot object visualizing the comparison between actual and predicted values.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load sample data
data(S_P_500_Close)
df &lt;- S_P_500_Close

# Run TRANSFORMER (will use mock results if Python is unavailable)
result &lt;- TRANSFORMER(df = df,
  study_variable = "Price",
  sequence_size = 10,
  head_size = 128,
  num_heads = 8,
  ff_dim = 256,
  num_transformer_blocks = 4,
  mlp_units = c(128),
  mlp_dropout = 0.3,
  dropout = 0.2,
  epochs = 2,
  batch_size = 32,
  patience = 15
)

# Display results
result$PREDICTIONS
result$RMSE
result$MAE
result$MAPE
result$sMAPE
result$Quantile_Loss
# Plots are NULL if Python is unavailable
if (!is.null(result$Loss_plot)) result$Loss_plot
if (!is.null(result$Actual_vs_Predicted)) result$Actual_vs_Predicted

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
