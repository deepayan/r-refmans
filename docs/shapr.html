<!DOCTYPE html><html lang="en-US"><head><title>Help for package shapr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {shapr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#shapr-package'><p>shapr: Prediction Explanation with Dependence-Aware Shapley Values</p></a></li>
<li><a href='#additional_regression_setup'><p>Additional setup for regression-based methods</p></a></li>
<li><a href='#aicc_full_cpp'><p>AICc formula for several sets, alternative definition</p></a></li>
<li><a href='#aicc_full_single_cpp'><p>Temp-function for computing the full AICc with several X's etc</p></a></li>
<li><a href='#append_vS_list'><p>Appends the new vS_list to the prev vS_list</p></a></li>
<li><a href='#categorical_to_one_hot_layer'><p>A <code>torch::nn_module()</code> Representing a <code>categorical_to_one_hot_layer</code></p></a></li>
<li><a href='#check_categorical_valid_MCsamp'><p>Check that all explicands has at least one valid MC sample in causal Shapley values</p></a></li>
<li><a href='#check_convergence'><p>Checks the convergence according to the convergence threshold</p></a></li>
<li><a href='#check_groups'><p>Check that the group parameter has the right form and content</p></a></li>
<li><a href='#check_verbose'><p>Function that checks the verbose parameter</p></a></li>
<li><a href='#cli_compute_vS'><p>Printing messages in compute_vS with cli</p></a></li>
<li><a href='#cli_iter'><p>Printing messages in iterative procedure with cli</p></a></li>
<li><a href='#cli_startup'><p>Printing startup messages with cli</p></a></li>
<li><a href='#coalition_matrix_cpp'><p>Get coalition matrix</p></a></li>
<li><a href='#compute_estimates'><p>Computes the the Shapley values and their standard deviation given the <code>v(S)</code></p></a></li>
<li><a href='#compute_MSEv_eval_crit'><p>Mean Squared Error of the Contribution Function <code>v(S)</code></p></a></li>
<li><a href='#compute_shapley'><p>Compute shapley values</p></a></li>
<li><a href='#compute_time'><p>Gathers and computes the timing of the different parts of the explain function.</p></a></li>
<li><a href='#compute_vS'><p>Computes <code>v(S)</code> for all features subsets <code>S</code>.</p></a></li>
<li><a href='#convert_feature_name_to_idx'><p>Convert feature names into feature indices</p></a></li>
<li><a href='#correction_matrix_cpp'><p>Correction term with trace_input in AICc formula</p></a></li>
<li><a href='#create_coalition_table'><p>Define coalitions, and fetch additional information about each unique coalition</p></a></li>
<li><a href='#create_ctree'><p>Build all the conditional inference trees</p></a></li>
<li><a href='#create_marginal_data_cat'><p>Create marginal categorical data for causal Shapley values</p></a></li>
<li><a href='#create_marginal_data_gaussian'><p>Generate marginal Gaussian data using Cholesky decomposition</p></a></li>
<li><a href='#create_marginal_data_training'><p>Function that samples data from the empirical marginal training distribution</p></a></li>
<li><a href='#default_doc_export'><p>Exported documentation helper function.</p></a></li>
<li><a href='#default_doc_internal'><p>Unexported documentation helper function.</p></a></li>
<li><a href='#exact_coalition_table'><p>Get table with all (exact) coalitions</p></a></li>
<li><a href='#explain'><p>Explain the output of machine learning models with dependence-aware (conditional/observational) Shapley values</p></a></li>
<li><a href='#explain_forecast'><p>Explain a forecast from time series models with dependence-aware (conditional/observational) Shapley values</p></a></li>
<li><a href='#finalize_explanation'><p>Gathers the final output to create the explanation object</p></a></li>
<li><a href='#gauss_cat_loss'><p>A <code>torch::nn_module()</code> Representing a <code>gauss_cat_loss</code></p></a></li>
<li><a href='#gauss_cat_parameters'><p>A <code>torch::nn_module()</code> Representing a <code>gauss_cat_parameters</code></p></a></li>
<li><a href='#gauss_cat_sampler_most_likely'><p>A <code>torch::nn_module()</code> Representing a <code>gauss_cat_sampler_most_likely</code></p></a></li>
<li><a href='#gauss_cat_sampler_random'><p>A <code>torch::nn_module()</code> Representing a gauss_cat_sampler_random</p></a></li>
<li><a href='#gaussian_transform'><p>Transforms a sample to standardized normal distribution</p></a></li>
<li><a href='#gaussian_transform_separate'><p>Transforms new data to standardized normal (dimension 1) based on other data transformations</p></a></li>
<li><a href='#get_cov_mat'><p>get_cov_mat</p></a></li>
<li><a href='#get_data_forecast'><p>Set up data for explain_forecast</p></a></li>
<li><a href='#get_data_specs'><p>Fetches feature information from a given data set</p></a></li>
<li><a href='#get_extra_comp_args_default'><p>Gets the default values for the extra estimation arguments</p></a></li>
<li><a href='#get_extra_parameters'><p>This includes both extra parameters and other objects</p></a></li>
<li><a href='#get_feature_specs'><p>Gets the feature specifications form the model</p></a></li>
<li><a href='#get_iterative_args_default'><p>Function to specify arguments of the iterative estimation procedure</p></a></li>
<li><a href='#get_max_n_coalitions_causal'><p>Get the number of coalitions that respects the causal ordering</p></a></li>
<li><a href='#get_model_specs'><p>Fetches feature information from natively supported models</p></a></li>
<li><a href='#get_mu_vec'><p>get_mu_vec</p></a></li>
<li><a href='#get_output_args_default'><p>Gets the default values for the output arguments</p></a></li>
<li><a href='#get_predict_model'><p>Get predict_model function</p></a></li>
<li><a href='#get_S_causal_steps'><p>Get the steps for generating MC samples for coalitions following a causal ordering</p></a></li>
<li><a href='#get_supported_approaches'><p>Gets the implemented approaches</p></a></li>
<li><a href='#get_supported_models'><p>Provides a data.table with the supported models</p></a></li>
<li><a href='#get_valid_causal_coalitions'><p>Get all coalitions satisfying the causal ordering</p></a></li>
<li><a href='#group_forecast_setup'><p>Set up user provided groups for explanation in a forecast model.</p></a></li>
<li><a href='#hat_matrix_cpp'><p>Computing single H matrix in AICc-function using the Mahalanobis distance</p></a></li>
<li><a href='#inv_gaussian_transform_cpp'><p>Transforms new data to a standardized normal distribution</p></a></li>
<li><a href='#lag_data'><p>Lag a matrix of variables a specific number of lags for each variables.</p></a></li>
<li><a href='#mahalanobis_distance_cpp'><p>(Generalized) Mahalanobis distance</p></a></li>
<li><a href='#mcar_mask_generator'><p>Missing Completely at Random (MCAR) Mask Generator</p></a></li>
<li><a href='#memory_layer'><p>A <code>torch::nn_module()</code> Representing a Memory Layer</p></a></li>
<li><a href='#model_checker'><p>Check that the type of model is supported by the native implementation of the model class</p></a></li>
<li><a href='#observation_impute'><p>Generate permutations of training data using test observations</p></a></li>
<li><a href='#observation_impute_cpp'><p>Get imputed data</p></a></li>
<li><a href='#paired_sampler'><p>Sampling Paired Observations</p></a></li>
<li><a href='#plot_MSEv_eval_crit'><p>Plots of the MSEv Evaluation Criterion</p></a></li>
<li><a href='#plot_SV_several_approaches'><p>Shapley value bar plots for several explanation objects</p></a></li>
<li><a href='#plot_vaeac_eval_crit'><p>Plot the training VLB and validation IWAE for <code>vaeac</code> models</p></a></li>
<li><a href='#plot_vaeac_imputed_ggpairs'><p>Plot Pairwise Plots for Imputed and True Data</p></a></li>
<li><a href='#plot.shapr'><p>Plot of the Shapley value explanations</p></a></li>
<li><a href='#predict_model'><p>Generate predictions for input data with specified model</p></a></li>
<li><a href='#prepare_data'><p>Generate data used for predictions and Monte Carlo integration</p></a></li>
<li><a href='#prepare_data_causal'><p>Generate data used for predictions and Monte Carlo integration for causal Shapley values</p></a></li>
<li><a href='#prepare_data_copula_cpp'><p>Generate (Gaussian) Copula MC samples</p></a></li>
<li><a href='#prepare_data_copula_cpp_caus'><p>Generate (Gaussian) Copula MC samples for the causal setup with a single MC sample for each explicand</p></a></li>
<li><a href='#prepare_data_gaussian_cpp'><p>Generate Gaussian MC samples</p></a></li>
<li><a href='#prepare_data_gaussian_cpp_caus'><p>Generate Gaussian MC samples for the causal setup with a single MC sample for each explicand</p></a></li>
<li><a href='#prepare_data_single_coalition'><p>Compute the conditional probabilities for a single coalition for the categorical approach</p></a></li>
<li><a href='#prepare_next_iteration'><p>Prepares the next iteration of the iterative sampling algorithm</p></a></li>
<li><a href='#print_iter'><p>Prints iterative information</p></a></li>
<li><a href='#print.shapr'><p>Print method for shapr objects</p></a></li>
<li><a href='#process_factor_data'><p>Treat factors as numeric values</p></a></li>
<li><a href='#quantile_type7_cpp'><p>Compute the quantiles using quantile type seven</p></a></li>
<li><a href='#reg_forecast_setup'><p>Set up exogenous regressors for explanation in a forecast model.</p></a></li>
<li><a href='#regression.check_namespaces'><p>Check that needed libraries are installed</p></a></li>
<li><a href='#regression.check_parameters'><p>Check regression parameters</p></a></li>
<li><a href='#regression.check_recipe_func'><p>Check <code>regression.recipe_func</code></p></a></li>
<li><a href='#regression.check_sur_n_comb'><p>Check the <code>regression.surrogate_n_comb</code> parameter</p></a></li>
<li><a href='#regression.check_vfold_cv_para'><p>Check the parameters that are sent to <code>rsample::vfold_cv()</code></p></a></li>
<li><a href='#regression.cv_message'><p>Produce message about which batch prepare_data is working on</p></a></li>
<li><a href='#regression.get_string_to_R'><p>Convert the string into an R object</p></a></li>
<li><a href='#regression.get_tune'><p>Get if model is to be tuned</p></a></li>
<li><a href='#regression.get_y_hat'><p>Get the predicted responses</p></a></li>
<li><a href='#regression.surrogate_aug_data'><p>Augment the training data and the explicands</p></a></li>
<li><a href='#regression.train_model'><p>Train a tidymodels model via workflows</p></a></li>
<li><a href='#release_questions'><p>Auxiliary function for the vignettes</p></a></li>
<li><a href='#rss_cpp'><p>Function for computing sigma_hat_sq</p></a></li>
<li><a href='#sample_coalition_table'><p>Get table with sampled coalitions</p></a></li>
<li><a href='#sample_coalitions_cpp_str_paired'><p>We here return a vector of strings/characters, i.e., a CharacterVector,</p>
where each string is a space-separated list of integers.</a></li>
<li><a href='#sample_combinations'><p>Helper function to sample a combination of training and testing rows, which does not risk</p>
getting the same observation twice. Need to improve this help file.</a></li>
<li><a href='#sample_ctree'><p>Sample ctree variables from a given conditional inference tree</p></a></li>
<li><a href='#save_results'><p>Saves the intermediate results to disk</p></a></li>
<li><a href='#setup'><p>check_setup</p></a></li>
<li><a href='#setup_approach'><p>Set up the framework chosen approach</p></a></li>
<li><a href='#shapley_setup'><p>Set up the kernelSHAP framework</p></a></li>
<li><a href='#shapley_weights'><p>Calculate Shapley weight</p></a></li>
<li><a href='#skip_connection'><p>A <code>torch::nn_module()</code> Representing a skip connection</p></a></li>
<li><a href='#specified_masks_mask_generator'><p>A <code>torch::nn_module()</code> Representing a specified_masks_mask_generator</p></a></li>
<li><a href='#specified_prob_mask_generator'><p>A <code>torch::nn_module()</code> Representing a specified_prob_mask_generator</p></a></li>
<li><a href='#test_predict_model'><p>Model testing function</p></a></li>
<li><a href='#testing_cleanup'><p>Cleans out certain output arguments to allow perfect reproducibility of the output</p></a></li>
<li><a href='#vaeac'><p>Initializing a vaeac model</p></a></li>
<li><a href='#vaeac_categorical_parse_params'><p>Creates Categorical Distributions</p></a></li>
<li><a href='#vaeac_check_activation_func'><p>Function that checks the provided activation function</p></a></li>
<li><a href='#vaeac_check_cuda'><p>Function that checks for access to CUDA</p></a></li>
<li><a href='#vaeac_check_epoch_values'><p>Function that checks provided epoch arguments</p></a></li>
<li><a href='#vaeac_check_extra_named_list'><p>Check vaeac.extra_parameters list</p></a></li>
<li><a href='#vaeac_check_logicals'><p>Function that checks logicals</p></a></li>
<li><a href='#vaeac_check_mask_gen'><p>Function that checks the specified masking scheme</p></a></li>
<li><a href='#vaeac_check_masking_ratio'><p>Function that checks that the masking ratio argument is valid</p></a></li>
<li><a href='#vaeac_check_parameters'><p>Function that calls all vaeac parameters check functions</p></a></li>
<li><a href='#vaeac_check_positive_integers'><p>Function that checks positive integers</p></a></li>
<li><a href='#vaeac_check_positive_numerics'><p>Function that checks positive numerics</p></a></li>
<li><a href='#vaeac_check_probabilities'><p>Function that checks probabilities</p></a></li>
<li><a href='#vaeac_check_save_names'><p>Function that checks that the save folder exists and for a valid file name</p></a></li>
<li><a href='#vaeac_check_save_parameters'><p>Function that gives a warning about disk usage</p></a></li>
<li><a href='#vaeac_check_which_vaeac_model'><p>Function that checks for valid <code>vaeac</code> model name</p></a></li>
<li><a href='#vaeac_check_x_colnames'><p>Function that checks the feature names of data and <code>vaeac</code> model</p></a></li>
<li><a href='#vaeac_compute_normalization'><p>Compute Featurewise Means and Standard Deviations</p></a></li>
<li><a href='#vaeac_dataset'><p>Dataset used by the <code>vaeac</code> model</p></a></li>
<li><a href='#vaeac_extend_batch'><p>Extends Incomplete Batches by Sampling Extra Data from Dataloader</p></a></li>
<li><a href='#vaeac_get_current_save_state'><p>Function that extracts additional objects from the environment to the state list</p></a></li>
<li><a href='#vaeac_get_data_objects'><p>Function to set up data loaders and save file names</p></a></li>
<li><a href='#vaeac_get_evaluation_criteria'><p>Extract the Training VLB and Validation IWAE from a list of explanations objects using the vaeac approach</p></a></li>
<li><a href='#vaeac_get_extra_para_default'><p>Function to specify the extra parameters in the <code>vaeac</code> model</p></a></li>
<li><a href='#vaeac_get_full_state_list'><p>Function that extracts the state list objects from the environment</p></a></li>
<li><a href='#vaeac_get_mask_generator_name'><p>Function that determines which mask generator to use</p></a></li>
<li><a href='#vaeac_get_model_from_checkp'><p>Function to load a <code>vaeac</code> model and set it in the right state and mode</p></a></li>
<li><a href='#vaeac_get_n_decimals'><p>Function to get string of values with specific number of decimals</p></a></li>
<li><a href='#vaeac_get_optimizer'><p>Function to create the optimizer used to train <code>vaeac</code></p></a></li>
<li><a href='#vaeac_get_save_file_names'><p>Function that creates the save file names for the <code>vaeac</code> model</p></a></li>
<li><a href='#vaeac_get_val_iwae'><p>Compute the Importance Sampling Estimator (Validation Error)</p></a></li>
<li><a href='#vaeac_get_x_explain_extended'><p>Function to extend the explicands and apply all relevant masks/coalitions</p></a></li>
<li><a href='#vaeac_impute_missing_entries'><p>Impute Missing Values Using Vaeac</p></a></li>
<li><a href='#vaeac_kl_normal_normal'><p>Compute the KL Divergence Between Two Gaussian Distributions.</p></a></li>
<li><a href='#vaeac_normal_parse_params'><p>Creates Normal Distributions</p></a></li>
<li><a href='#vaeac_normalize_data'><p>Normalize mixed data for <code>vaeac</code></p></a></li>
<li><a href='#vaeac_postprocess_data'><p>Postprocess Data Generated by a vaeac Model</p></a></li>
<li><a href='#vaeac_preprocess_data'><p>Preprocess Data for the vaeac approach</p></a></li>
<li><a href='#vaeac_print_train_summary'><p>Function to printout a training summary for the <code>vaeac</code> model</p></a></li>
<li><a href='#vaeac_save_state'><p>Function that saves the state list and the current save state of the <code>vaeac</code> model</p></a></li>
<li><a href='#vaeac_train_model'><p>Train the Vaeac Model</p></a></li>
<li><a href='#vaeac_train_model_auxiliary'><p>Function used to train a <code>vaeac</code> model</p></a></li>
<li><a href='#vaeac_train_model_continue'><p>Continue to Train the vaeac Model</p></a></li>
<li><a href='#vaeac_update_para_locations'><p>Move <code>vaeac</code> parameters to correct location</p></a></li>
<li><a href='#vaeac_update_pretrained_model'><p>Function that checks and adds a pre-trained <code>vaeac</code> model</p></a></li>
<li><a href='#weight_matrix'><p>Calculate weighted matrix</p></a></li>
<li><a href='#weight_matrix_cpp'><p>Calculate weight matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Title:</td>
<td>Prediction Explanation with Dependence-Aware Shapley Values</td>
</tr>
<tr>
<td>Description:</td>
<td>Complex machine learning models are often hard to interpret. However, in 
  many situations it is crucial to understand and explain why a model made a specific 
  prediction. Shapley values is the only method for such prediction explanation framework 
  with a solid theoretical foundation. Previously known methods for estimating the Shapley 
  values do, however, assume feature independence. This package implements methods which accounts for any feature 
  dependence, and thereby produces more accurate estimates of the true Shapley values.
  An accompanying 'Python' wrapper ('shaprpy') is available through the GitHub repository.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://norskregnesentral.github.io/shapr/">https://norskregnesentral.github.io/shapr/</a>,
<a href="https://github.com/NorskRegnesentral/shapr/">https://github.com/NorskRegnesentral/shapr/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/NorskRegnesentral/shapr/issues">https://github.com/NorskRegnesentral/shapr/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, data.table (&ge; 1.15.0), Rcpp (&ge; 0.12.15), Matrix,
future.apply, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>ranger, xgboost, mgcv, testthat (&ge; 3.0.0), knitr, rmarkdown,
roxygen2, ggplot2, gbm, party, partykit, waldo, progressr,
future, ggbeeswarm, vdiffr, forecast, torch, GGally, progress,
coro, parsnip, recipes, workflows, tune, dials, yardstick,
hardhat, rsample, rlang, cli</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>RcppArmadillo, Rcpp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-30 11:54:59 UTC; jullum</td>
</tr>
<tr>
<td>Author:</td>
<td>Martin Jullum <a href="https://orcid.org/0000-0003-3908-5155"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre, aut],
  Lars Henry Berge Olsen
    <a href="https://orcid.org/0009-0006-9360-6993"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Annabelle Redelmeier [aut],
  Jon Lachmann <a href="https://orcid.org/0000-0001-8396-5673"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Nikolai Sellereite
    <a href="https://orcid.org/0000-0002-4671-0337"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Anders Løland [ctb],
  Jens Christian Wahl [ctb],
  Camilla Lingjærde [ctb],
  Norsk Regnesentral [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Martin Jullum &lt;Martin.Jullum@nr.no&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-07 00:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='shapr-package'>shapr: Prediction Explanation with Dependence-Aware Shapley Values</h2><span id='topic+shapr'></span><span id='topic+shapr-package'></span>

<h3>Description</h3>

<p>Complex machine learning models are often hard to interpret. However, in many situations it is crucial to understand and explain why a model made a specific prediction. Shapley values is the only method for such prediction explanation framework with a solid theoretical foundation. Previously known methods for estimating the Shapley values do, however, assume feature independence. This package implements methods which accounts for any feature dependence, and thereby produces more accurate estimates of the true Shapley values. An accompanying 'Python' wrapper ('shaprpy') is available through the GitHub repository.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Martin Jullum <a href="mailto:Martin.Jullum@nr.no">Martin.Jullum@nr.no</a> (<a href="https://orcid.org/0000-0003-3908-5155">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Lars Henry Berge Olsen <a href="mailto:lhbolsen@nr.no">lhbolsen@nr.no</a> (<a href="https://orcid.org/0009-0006-9360-6993">ORCID</a>)
</p>
</li>
<li><p> Annabelle Redelmeier <a href="mailto:ardelmeier@gmail.com">ardelmeier@gmail.com</a>
</p>
</li>
<li><p> Jon Lachmann <a href="mailto:Jon@lachmann.nu">Jon@lachmann.nu</a> (<a href="https://orcid.org/0000-0001-8396-5673">ORCID</a>)
</p>
</li>
<li><p> Nikolai Sellereite <a href="mailto:nikolaisellereite@gmail.com">nikolaisellereite@gmail.com</a> (<a href="https://orcid.org/0000-0002-4671-0337">ORCID</a>)
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Anders Løland <a href="mailto:Anders.Loland@nr.no">Anders.Loland@nr.no</a> [contributor]
</p>
</li>
<li><p> Jens Christian Wahl <a href="mailto:jens.c.wahl@gmail.com">jens.c.wahl@gmail.com</a> [contributor]
</p>
</li>
<li><p> Camilla Lingjærde [contributor]
</p>
</li>
<li><p> Norsk Regnesentral [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://norskregnesentral.github.io/shapr/">https://norskregnesentral.github.io/shapr/</a>
</p>
</li>
<li> <p><a href="https://github.com/NorskRegnesentral/shapr/">https://github.com/NorskRegnesentral/shapr/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/NorskRegnesentral/shapr/issues">https://github.com/NorskRegnesentral/shapr/issues</a>
</p>
</li></ul>


<hr>
<h2 id='additional_regression_setup'>Additional setup for regression-based methods</h2><span id='topic+additional_regression_setup'></span>

<h3>Description</h3>

<p>Additional setup for regression-based methods
</p>


<h3>Usage</h3>

<pre><code class='language-R'>additional_regression_setup(internal, model, predict_model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="additional_regression_setup_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='aicc_full_cpp'>AICc formula for several sets, alternative definition</h2><span id='topic+aicc_full_cpp'></span>

<h3>Description</h3>

<p>AICc formula for several sets, alternative definition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aicc_full_cpp(h, X_list, mcov_list, S_scale_dist, y_list, negative)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aicc_full_cpp_+3A_h">h</code></td>
<td>
<p>numeric specifying the scaling (sigma)</p>
</td></tr>
<tr><td><code id="aicc_full_cpp_+3A_x_list">X_list</code></td>
<td>
<p>List.
Contains matrices with the appropriate features of the training data</p>
</td></tr>
<tr><td><code id="aicc_full_cpp_+3A_mcov_list">mcov_list</code></td>
<td>
<p>List.
Contains the covariance matrices of the matrices in X_list</p>
</td></tr>
<tr><td><code id="aicc_full_cpp_+3A_s_scale_dist">S_scale_dist</code></td>
<td>
<p>Logical.
Indicates whether Mahalanobis distance should be scaled with the number of variables.</p>
</td></tr>
<tr><td><code id="aicc_full_cpp_+3A_y_list">y_list</code></td>
<td>
<p>List.
Contains the appropriate (temporary) response variables.</p>
</td></tr>
<tr><td><code id="aicc_full_cpp_+3A_negative">negative</code></td>
<td>
<p>Logical.
Whether to return the negative of the AICc value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar with the numeric value of the AICc formula
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='aicc_full_single_cpp'>Temp-function for computing the full AICc with several X's etc</h2><span id='topic+aicc_full_single_cpp'></span>

<h3>Description</h3>

<p>Temp-function for computing the full AICc with several X's etc
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aicc_full_single_cpp(X, mcov, S_scale_dist, h, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aicc_full_single_cpp_+3A_x">X</code></td>
<td>
<p>matrix.</p>
</td></tr>
<tr><td><code id="aicc_full_single_cpp_+3A_mcov">mcov</code></td>
<td>
<p>matrix
The covariance matrix of X.</p>
</td></tr>
<tr><td><code id="aicc_full_single_cpp_+3A_s_scale_dist">S_scale_dist</code></td>
<td>
<p>logical.
Indicating whether the Mahalanobis distance should be scaled with the number of variables</p>
</td></tr>
<tr><td><code id="aicc_full_single_cpp_+3A_h">h</code></td>
<td>
<p>numeric specifying the scaling (sigma)</p>
</td></tr>
<tr><td><code id="aicc_full_single_cpp_+3A_y">y</code></td>
<td>
<p>Vector
Representing the (temporary) response variable</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar with the numeric value of the AICc formula.
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='append_vS_list'>Appends the new vS_list to the prev vS_list</h2><span id='topic+append_vS_list'></span>

<h3>Description</h3>

<p>Appends the new vS_list to the prev vS_list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>append_vS_list(vS_list, internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="append_vS_list_+3A_vs_list">vS_list</code></td>
<td>
<p>List
Output from <code><a href="#topic+compute_vS">compute_vS()</a></code></p>
</td></tr>
<tr><td><code id="append_vS_list_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='categorical_to_one_hot_layer'>A <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> Representing a <code>categorical_to_one_hot_layer</code></h2><span id='topic+categorical_to_one_hot_layer'></span>

<h3>Description</h3>

<p>The <code>categorical_to_one_hot_layer</code> module/layer expands categorical features into one-hot vectors,
because multi-layer perceptrons are known to work better with this data representation.
It also replaces NaNs with zeros in order so that further layers may work correctly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>categorical_to_one_hot_layer(
  one_hot_max_sizes,
  add_nans_map_for_columns = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="categorical_to_one_hot_layer_+3A_one_hot_max_sizes">one_hot_max_sizes</code></td>
<td>
<p>A torch tensor of dimension <code>n_features</code> containing the one hot sizes of the <code>n_features</code>
features. That is, if the <code>i</code>th feature is a categorical feature with 5 levels, then <code>one_hot_max_sizes[i] = 5</code>.
While the size for continuous features can either be <code>0</code> or <code>1</code>.</p>
</td></tr>
<tr><td><code id="categorical_to_one_hot_layer_+3A_add_nans_map_for_columns">add_nans_map_for_columns</code></td>
<td>
<p>Optional list which contains indices of columns which is_nan masks are to be appended
to the result tensor. This option is necessary for the full encoder to distinguish whether value is to be
reconstructed or not.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the module works with mixed data represented as 2-dimensional inputs and it
works correctly with missing values in <code>groundtruth</code> as long as they are represented by NaNs.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='check_categorical_valid_MCsamp'>Check that all explicands has at least one valid MC sample in causal Shapley values</h2><span id='topic+check_categorical_valid_MCsamp'></span>

<h3>Description</h3>

<p>Check that all explicands has at least one valid MC sample in causal Shapley values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_categorical_valid_MCsamp(dt, n_explain, n_MC_samples, joint_prob_dt)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_categorical_valid_MCsamp_+3A_dt">dt</code></td>
<td>
<p>Data.table containing the generated MC samples (and conditional values) after each sampling step</p>
</td></tr>
<tr><td><code id="check_categorical_valid_MCsamp_+3A_n_mc_samples">n_MC_samples</code></td>
<td>
<p>Positive integer.
For most approaches, it indicates the maximum number of samples to use in the Monte Carlo integration
of every conditional expectation.
For <code>approach="ctree"</code>, <code>n_MC_samples</code> corresponds to the number of samples
from the leaf node (see an exception related to the <code>ctree.sample</code> argument <code><a href="#topic+setup_approach.ctree">setup_approach.ctree()</a></code>).
For <code>approach="empirical"</code>, <code>n_MC_samples</code> is  the <code class="reqn">K</code> parameter in equations (14-15) of
Aas et al. (2021), i.e. the maximum number of observations (with largest weights) that is used, see also the
<code>empirical.eta</code> argument <code><a href="#topic+setup_approach.empirical">setup_approach.empirical()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For undocumented arguments, see <code><a href="#topic+setup_approach.categorical">setup_approach.categorical()</a></code>.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='check_convergence'>Checks the convergence according to the convergence threshold</h2><span id='topic+check_convergence'></span>

<h3>Description</h3>

<p>Checks the convergence according to the convergence threshold
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_convergence(internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_convergence_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='check_groups'>Check that the group parameter has the right form and content</h2><span id='topic+check_groups'></span>

<h3>Description</h3>

<p>Check that the group parameter has the right form and content
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_groups(feature_names, group)
</code></pre>

<hr>
<h2 id='check_verbose'>Function that checks the verbose parameter</h2><span id='topic+check_verbose'></span>

<h3>Description</h3>

<p>Function that checks the verbose parameter
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_verbose(verbose)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_verbose_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen, Martin Jullum
</p>

<hr>
<h2 id='cli_compute_vS'>Printing messages in compute_vS with cli</h2><span id='topic+cli_compute_vS'></span>

<h3>Description</h3>

<p>Printing messages in compute_vS with cli
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cli_compute_vS(internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cli_compute_vS_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='cli_iter'>Printing messages in iterative procedure with cli</h2><span id='topic+cli_iter'></span>

<h3>Description</h3>

<p>Printing messages in iterative procedure with cli
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cli_iter(verbose, internal, iter)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cli_iter_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
<tr><td><code id="cli_iter_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="cli_iter_+3A_iter">iter</code></td>
<td>
<p>Integer.
The iteration number. Only used internally.</p>
</td></tr>
</table>

<hr>
<h2 id='cli_startup'>Printing startup messages with cli</h2><span id='topic+cli_startup'></span>

<h3>Description</h3>

<p>Printing startup messages with cli
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cli_startup(internal, model_class, verbose)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cli_startup_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="cli_startup_+3A_model_class">model_class</code></td>
<td>
<p>String.
Class of the model as a string</p>
</td></tr>
<tr><td><code id="cli_startup_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
</table>

<hr>
<h2 id='coalition_matrix_cpp'>Get coalition matrix</h2><span id='topic+coalition_matrix_cpp'></span>

<h3>Description</h3>

<p>Get coalition matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coalition_matrix_cpp(coalitions, m)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="coalition_matrix_cpp_+3A_coalitions">coalitions</code></td>
<td>
<p>List.
Each of the elements equals an integer vector representing a valid combination of features/feature groups.</p>
</td></tr>
<tr><td><code id="coalition_matrix_cpp_+3A_m">m</code></td>
<td>
<p>Integer.
Number of features/feature groups.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite, Martin Jullum
</p>

<hr>
<h2 id='compute_estimates'>Computes the the Shapley values and their standard deviation given the <code>v(S)</code></h2><span id='topic+compute_estimates'></span>

<h3>Description</h3>

<p>Computes the the Shapley values and their standard deviation given the <code>v(S)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_estimates(internal, vS_list)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_estimates_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_estimates_+3A_vs_list">vS_list</code></td>
<td>
<p>List
Output from <code><a href="#topic+compute_vS">compute_vS()</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='compute_MSEv_eval_crit'>Mean Squared Error of the Contribution Function <code>v(S)</code></h2><span id='topic+compute_MSEv_eval_crit'></span>

<h3>Description</h3>

<p>Function that computes the Mean Squared Error (MSEv) of the contribution function
v(s) as proposed by <a href="https://arxiv.org/pdf/2006.01272">Frye et al. (2019)</a> and used by
<a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_MSEv_eval_crit(
  internal,
  dt_vS,
  MSEv_uniform_comb_weights,
  MSEv_skip_empty_full_comb = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_MSEv_eval_crit_+3A_internal">internal</code></td>
<td>
<p>List.
Holds all parameters, data, functions and computed objects used within <code><a href="#topic+explain">explain()</a></code>
The list contains one or more of the elements <code>parameters</code>, <code>data</code>, <code>objects</code>, <code>iter_list</code>, <code>timing_list</code>,
<code>main_timing_list</code>, <code>output</code>, and <code>iter_timing_list</code>.</p>
</td></tr>
<tr><td><code id="compute_MSEv_eval_crit_+3A_dt_vs">dt_vS</code></td>
<td>
<p>Data.table of dimension <code>n_coalitions</code> times <code>n_explain + 1</code> containing the contribution function
estimates. The first column is assumed to be named <code>id_coalition</code> and containing the ids of the coalitions.
The last row is assumed to be the full coalition, i.e., it contains the predicted responses for the observations
which are to be explained.</p>
</td></tr>
<tr><td><code id="compute_MSEv_eval_crit_+3A_msev_uniform_comb_weights">MSEv_uniform_comb_weights</code></td>
<td>
<p>Logical.
If <code>TRUE</code> (default), then the function weights the coalitions uniformly when computing the MSEv criterion.
If <code>FALSE</code>, then the function use the Shapley kernel weights to weight the coalitions when computing the MSEv
criterion.
Note that the Shapley kernel weights are replaced by the sampling frequency when not all coalitions are considered.</p>
</td></tr>
<tr><td><code id="compute_MSEv_eval_crit_+3A_msev_skip_empty_full_comb">MSEv_skip_empty_full_comb</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), we exclude the empty and grand
coalitions when computing the MSEv evaluation criterion. This is reasonable as they are identical
for all methods, i.e., their contribution function is independent of the used method as they are special cases not
effected by the used method. If <code>FALSE</code>, we include the empty and grand coalitions. In this situation,
we also recommend setting <code>MSEv_uniform_comb_weights = TRUE</code>, as otherwise the large weights for the empty and
grand coalitions will outweigh all other coalitions and make the MSEv criterion uninformative.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The MSEv evaluation criterion does not rely on access to the true contribution functions nor the
true Shapley values to be computed. A lower value indicates better approximations, however, the
scale and magnitude of the MSEv criterion is not directly interpretable in regard to the precision
of the final estimated Shapley values.
<a href="https://link.springer.com/content/pdf/10.1007/s10618-024-01016-z.pdf">Olsen et al. (2024)</a>
illustrates in Figure 11 a fairly strong linear relationship between the MSEv criterion and the
MAE between the estimated and true Shapley values in a simulation study. Note that explicands
refer to the observations whose predictions we are to explain.
</p>


<h3>Value</h3>

<p>List containing:
</p>

<dl>
<dt><code>MSEv</code></dt><dd><p>A <code><a href="data.table.html#topic+data.table">data.table</a></code> with the overall MSEv evaluation criterion averaged
over both the coalitions and observations/explicands. The <code><a href="data.table.html#topic+data.table">data.table</a></code>
also contains the standard deviation of the MSEv values for each explicand (only averaged over the coalitions)
divided by the square root of the number of explicands.</p>
</dd>
<dt><code>MSEv_explicand</code></dt><dd><p>A <code><a href="data.table.html#topic+data.table">data.table</a></code> with the mean squared error for each
explicand, i.e., only averaged over the coalitions.</p>
</dd>
<dt><code>MSEv_coalition</code></dt><dd><p>A <code><a href="data.table.html#topic+data.table">data.table</a></code> with the mean squared error for each
coalition, i.e., only averaged over the explicands/observations.
The <code><a href="data.table.html#topic+data.table">data.table</a></code> also contains the standard deviation of the MSEv values for
each coalition divided by the square root of the number of explicands.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/pdf/2006.01272">
Frye, C., de Mijolla, D., Begley, T., Cowton, L., Stanley, M., &amp; Feige, I. (2021).
Shapley explainability on the data manifold. In International Conference on Learning Representations.</a>
</p>
</li>
<li> <p><a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">
Olsen, L. H., Glad, I. K., Jullum, M., &amp; Aas, K. (2022). Using Shapley values and variational autoencoders to
explain predictive models with dependent mixed features. Journal of machine learning research, 23(213), 1-51</a>
</p>
</li>
<li> <p><a href="https://link.springer.com/content/pdf/10.1007/s10618-024-01016-z.pdf">
Olsen, L. H. B., Glad, I. K., Jullum, M., &amp; Aas, K. (2024). A comparative study of methods for estimating
model-agnostic Shapley value explanations. Data Mining and Knowledge Discovery, 1-48</a>
</p>
</li></ul>


<hr>
<h2 id='compute_shapley'>Compute shapley values</h2><span id='topic+compute_shapley'></span>

<h3>Description</h3>

<p>Compute shapley values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_shapley(internal, dt_vS)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_shapley_+3A_internal">internal</code></td>
<td>
<p>List.
Holds all parameters, data, functions and computed objects used within <code><a href="#topic+explain">explain()</a></code>
The list contains one or more of the elements <code>parameters</code>, <code>data</code>, <code>objects</code>, <code>iter_list</code>, <code>timing_list</code>,
<code>main_timing_list</code>, <code>output</code>, and <code>iter_timing_list</code>.</p>
</td></tr>
<tr><td><code id="compute_shapley_+3A_dt_vs">dt_vS</code></td>
<td>
<p>The contribution matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.table</code> with Shapley values for each test observation.
</p>

<hr>
<h2 id='compute_time'>Gathers and computes the timing of the different parts of the explain function.</h2><span id='topic+compute_time'></span>

<h3>Description</h3>

<p>Gathers and computes the timing of the different parts of the explain function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_time(internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_time_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='compute_vS'>Computes <code>v(S)</code> for all features subsets <code>S</code>.</h2><span id='topic+compute_vS'></span>

<h3>Description</h3>

<p>Computes <code>v(S)</code> for all features subsets <code>S</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_vS(internal, model, predict_model, method = "future")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_vS_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_vS_+3A_method">method</code></td>
<td>
<p>Character
Indicates whether the lapply method (default) or loop method should be used.
Options other than &quot;future&quot; is only used for testing/debugging.</p>
</td></tr>
</table>

<hr>
<h2 id='convert_feature_name_to_idx'>Convert feature names into feature indices</h2><span id='topic+convert_feature_name_to_idx'></span>

<h3>Description</h3>

<p>Functions that takes a <code>causal_ordering</code> specified using strings and convert these strings to feature indices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_feature_name_to_idx(causal_ordering, labels, feat_group_txt)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="convert_feature_name_to_idx_+3A_causal_ordering">causal_ordering</code></td>
<td>
<p>List.
Not applicable for (regular) non-causal or asymmetric explanations.
<code>causal_ordering</code> is an unnamed list of vectors specifying the components of the
partial causal ordering that the coalitions must respect. Each vector represents
a component and contains one or more features/groups identified by their names
(strings) or indices (integers). If <code>causal_ordering</code> is <code>NULL</code> (default), no causal
ordering is assumed and all possible coalitions are allowed. No causal ordering is
equivalent to a causal ordering with a single component that includes all features
(<code>list(1:n_features)</code>) or groups (<code>list(1:n_groups)</code>) for feature-wise and group-wise
Shapley values, respectively. For feature-wise Shapley values and
<code>causal_ordering = list(c(1, 2), c(3, 4))</code>, the interpretation is that features 1 and 2
are the ancestors of features 3 and 4, while features 3 and 4 are on the same level.
Note: All features/groups must be included in the <code>causal_ordering</code> without any duplicates.</p>
</td></tr>
<tr><td><code id="convert_feature_name_to_idx_+3A_labels">labels</code></td>
<td>
<p>Vector of strings containing (the order of) the feature names.</p>
</td></tr>
<tr><td><code id="convert_feature_name_to_idx_+3A_feat_group_txt">feat_group_txt</code></td>
<td>
<p>String that is either &quot;feature&quot; or &quot;group&quot; based on
if <code>shapr</code> is computing feature- or group-wise Shapley values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>causal_ordering</code> list, but with feature indices (w.r.t. <code>labels</code>) instead of feature names.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='correction_matrix_cpp'>Correction term with trace_input in AICc formula</h2><span id='topic+correction_matrix_cpp'></span>

<h3>Description</h3>

<p>Correction term with trace_input in AICc formula
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correction_matrix_cpp(tr_H, n)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="correction_matrix_cpp_+3A_tr_h">tr_H</code></td>
<td>
<p>numeric
The trace of H</p>
</td></tr>
<tr><td><code id="correction_matrix_cpp_+3A_n">n</code></td>
<td>
<p>numeric
The number of rows in H</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='create_coalition_table'>Define coalitions, and fetch additional information about each unique coalition</h2><span id='topic+create_coalition_table'></span>

<h3>Description</h3>

<p>Define coalitions, and fetch additional information about each unique coalition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_coalition_table(
  m,
  exact = TRUE,
  n_coalitions = 200,
  weight_zero_m = 10^6,
  paired_shap_sampling = TRUE,
  prev_coal_samples = NULL,
  prev_coal_samples_n_unique = NULL,
  n_samps_scale = 10,
  coal_feature_list = as.list(seq_len(m)),
  approach0 = "gaussian",
  kernelSHAP_reweighting = "none",
  dt_valid_causal_coalitions = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_coalition_table_+3A_m">m</code></td>
<td>
<p>Positive integer.
Total number of features/groups.</p>
</td></tr>
<tr><td><code id="create_coalition_table_+3A_exact">exact</code></td>
<td>
<p>Logical.
If <code>TRUE</code> all <code>2^m</code> coalitions are generated, otherwise a subsample of the coalitions is used.</p>
</td></tr>
<tr><td><code id="create_coalition_table_+3A_n_coalitions">n_coalitions</code></td>
<td>
<p>Positive integer.
Note that if <code>exact = TRUE</code>, <code>n_coalitions</code> is ignored.</p>
</td></tr>
<tr><td><code id="create_coalition_table_+3A_weight_zero_m">weight_zero_m</code></td>
<td>
<p>Numeric.
The value to use as a replacement for infinite coalition weights when doing numerical operations.</p>
</td></tr>
<tr><td><code id="create_coalition_table_+3A_paired_shap_sampling">paired_shap_sampling</code></td>
<td>
<p>Logical.
Whether to do paired sampling of coalitions.</p>
</td></tr>
<tr><td><code id="create_coalition_table_+3A_prev_coal_samples">prev_coal_samples</code></td>
<td>
<p>Character vector.
A vector of previously sampled coalitions as characters.
Each string contains a coalition and the feature indices in the coalition is separated by a space.
For example, &quot;1 5 8&quot; is a coalition with features 1, 5, and 8.</p>
</td></tr>
<tr><td><code id="create_coalition_table_+3A_prev_coal_samples_n_unique">prev_coal_samples_n_unique</code></td>
<td>
<p>Positive integer.
The number of unique coalitions in <code>prev_coal_samples</code>.
This is a separate argument to avoid recomputing the number unnecessarily.</p>
</td></tr>
<tr><td><code id="create_coalition_table_+3A_n_samps_scale">n_samps_scale</code></td>
<td>
<p>Positive integer.
Integer that scales the number of coalitions <code>n_coalitions</code> to sample as sampling is cheap,
while checking for <code>n_coalitions</code> unique coalitions is expensive, thus we over sample the
number of coalitions by a factor of <code>n_samps_scale</code> and determine when we have <code>n_coalitions</code> unique
coalitions and only use the coalitions up to this point and throw away the remaining coalitions.</p>
</td></tr>
<tr><td><code id="create_coalition_table_+3A_coal_feature_list">coal_feature_list</code></td>
<td>
<p>List.
A list mapping each coalition to the features it contains.</p>
</td></tr>
<tr><td><code id="create_coalition_table_+3A_approach0">approach0</code></td>
<td>
<p>Character vector.
Contains the approach to be used for estimation of each coalition size. Same as <code>approach</code> in <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="create_coalition_table_+3A_dt_valid_causal_coalitions">dt_valid_causal_coalitions</code></td>
<td>
<p>data.table. Only applicable for asymmetric Shapley
values explanations, and is <code>NULL</code> for symmetric Shapley values.
The data.table contains information about the coalitions that respects the causal ordering.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with info about the coalitions to use
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite, Martin Jullum, Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# All coalitions
x &lt;- create_coalition_table(m = 3)
nrow(x) # Equals 2^3 = 8

# Subsample of coalitions
x &lt;- shapr:::create_coalition_table(m = 10, exact = FALSE, n_coalitions = 1e2)

## End(Not run)
</code></pre>

<hr>
<h2 id='create_ctree'>Build all the conditional inference trees</h2><span id='topic+create_ctree'></span>

<h3>Description</h3>

<p>Build all the conditional inference trees
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_ctree(
  given_ind,
  x_train,
  mincriterion,
  minsplit,
  minbucket,
  use_partykit = "on_error"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_ctree_+3A_given_ind">given_ind</code></td>
<td>
<p>Integer vector.
Indicates which features are conditioned on.</p>
</td></tr>
<tr><td><code id="create_ctree_+3A_x_train">x_train</code></td>
<td>
<p>Data.table with training data.</p>
</td></tr>
<tr><td><code id="create_ctree_+3A_use_partykit">use_partykit</code></td>
<td>
<p>String. In some semi-rare cases <code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code> runs into an error related to the LINPACK
used by R. To get around this problem, one may fall back to using the newer (but slower) <code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code>
function, which is a reimplementation of the same method. Setting this parameter to <code>"on_error"</code> (default)
falls back to  <code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code>, if <code><a href="party.html#topic+ctree">party::ctree()</a></code> fails. Other options are <code>"never"</code>, which always
uses <code><a href="party.html#topic+ctree">party::ctree()</a></code>, and <code>"always"</code>, which always uses <code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code>. A warning message is
created whenever <code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code> is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the documentation of the <code><a href="#topic+setup_approach.ctree">setup_approach.ctree()</a></code> function for undocumented parameters.
</p>


<h3>Value</h3>

<p>List with conditional inference tree and the variables conditioned/not conditioned on.
</p>


<h3>Author(s)</h3>

<p>Annabelle Redelmeier, Martin Jullum
</p>

<hr>
<h2 id='create_marginal_data_cat'>Create marginal categorical data for causal Shapley values</h2><span id='topic+create_marginal_data_cat'></span>

<h3>Description</h3>

<p>This function is used when we generate marginal data for the categorical approach when we have several sampling
steps. We need to treat this separately, as we here in the marginal step CANNOT make feature values such
that the combination of those and the feature values we condition in S are NOT in
<code>categorical.joint_prob_dt</code>. If we do this, then we cannot progress further in the chain of sampling
steps. E.g., X1 in (1,2,3), X2 in (1,2,3), and X3 in (1,2,3).
We know X2 = 2, and let causal structure be X1 -&gt; X2 -&gt; X3. Assume that
P(X1 = 1, X2 = 2, X = 3) = P(X1 = 2, X2 = 2, X = 3) = 1/2. Then there is no point
generating X1 = 3, as we then cannot generate X3.
The solution is only to generate the values which can proceed through the whole
chain of sampling steps. To do that, we have to ensure the the marginal sampling
respects the valid feature coalitions for all sets of conditional features, i.e.,
the features in <code>features_steps_cond_on</code>.
We sample from the valid coalitions using the MARGINAL probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_marginal_data_cat(
  n_MC_samples,
  x_explain,
  Sbar_features,
  S_original,
  joint_prob_dt
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_marginal_data_cat_+3A_n_mc_samples">n_MC_samples</code></td>
<td>
<p>Positive integer.
For most approaches, it indicates the maximum number of samples to use in the Monte Carlo integration
of every conditional expectation.
For <code>approach="ctree"</code>, <code>n_MC_samples</code> corresponds to the number of samples
from the leaf node (see an exception related to the <code>ctree.sample</code> argument <code><a href="#topic+setup_approach.ctree">setup_approach.ctree()</a></code>).
For <code>approach="empirical"</code>, <code>n_MC_samples</code> is  the <code class="reqn">K</code> parameter in equations (14-15) of
Aas et al. (2021), i.e. the maximum number of observations (with largest weights) that is used, see also the
<code>empirical.eta</code> argument <code><a href="#topic+setup_approach.empirical">setup_approach.empirical()</a></code>.</p>
</td></tr>
<tr><td><code id="create_marginal_data_cat_+3A_x_explain">x_explain</code></td>
<td>
<p>Matrix or data.frame/data.table.
Contains the the features, whose predictions ought to be explained.</p>
</td></tr>
<tr><td><code id="create_marginal_data_cat_+3A_sbar_features">Sbar_features</code></td>
<td>
<p>Vector of integers containing the features indices to generate marginal observations for.
That is, if <code>Sbar_features</code> is <code>c(1,4)</code>, then we sample <code>n_MC_samples</code> observations from <code class="reqn">P(X_1, X_4)</code>.
That is, we sample the first and fourth feature values from the same valid feature coalition using
the marginal probability, so we do not break the dependence between them.</p>
</td></tr>
<tr><td><code id="create_marginal_data_cat_+3A_s_original">S_original</code></td>
<td>
<p>Vector of integers containing the features indices of the original coalition <code>S</code>. I.e., not the
features in the current sampling step, but the features are known to us before starting the chain of sampling steps.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For undocumented arguments, see <code><a href="#topic+setup_approach.categorical">setup_approach.categorical()</a></code>.
</p>


<h3>Value</h3>

<p>Data table of dimension <code class="reqn">(`n_MC_samples` * `nrow(x_explain)`) \times `length(Sbar_features)`</code> with the
sampled observations.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='create_marginal_data_gaussian'>Generate marginal Gaussian data using Cholesky decomposition</h2><span id='topic+create_marginal_data_gaussian'></span>

<h3>Description</h3>

<p>Given a multivariate Gaussian distribution, this function creates data from specified marginals of said distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_marginal_data_gaussian(n_MC_samples, Sbar_features, mu, cov_mat)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_marginal_data_gaussian_+3A_n_mc_samples">n_MC_samples</code></td>
<td>
<p>Integer. The number of samples to generate.</p>
</td></tr>
<tr><td><code id="create_marginal_data_gaussian_+3A_sbar_features">Sbar_features</code></td>
<td>
<p>Vector of integers indicating which marginals to sample from.</p>
</td></tr>
<tr><td><code id="create_marginal_data_gaussian_+3A_mu">mu</code></td>
<td>
<p>Numeric vector containing the expected values for all features in the multivariate Gaussian distribution.</p>
</td></tr>
<tr><td><code id="create_marginal_data_gaussian_+3A_cov_mat">cov_mat</code></td>
<td>
<p>Numeric matrix containing the covariance between all features
in the multivariate Gaussian distribution.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='create_marginal_data_training'>Function that samples data from the empirical marginal training distribution</h2><span id='topic+create_marginal_data_training'></span>

<h3>Description</h3>

<p>Sample observations from the empirical distribution P(X) using the training dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_marginal_data_training(
  x_train,
  n_explain,
  Sbar_features,
  n_MC_samples = 1000,
  stable_version = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_marginal_data_training_+3A_x_train">x_train</code></td>
<td>
<p>Data.table with training data.</p>
</td></tr>
<tr><td><code id="create_marginal_data_training_+3A_sbar_features">Sbar_features</code></td>
<td>
<p>Vector of integers containing the features indices to generate marginal observations for.
That is, if <code>Sbar_features</code> is <code>c(1,4)</code>, then we sample <code>n_MC_samples</code> observations from <code class="reqn">P(X_1, X_4)</code> using the
empirical training observations (with replacements). That is, we sample the first and fourth feature values from
the same training observation, so we do not break the dependence between them.</p>
</td></tr>
<tr><td><code id="create_marginal_data_training_+3A_stable_version">stable_version</code></td>
<td>
<p>Logical. If <code>TRUE</code> and <code>n_MC_samples</code> &gt; <code>n_train</code>, then we include each training observation
<code>n_MC_samples %/% n_train</code> times and then sample the remaining <code style="white-space: pre;">&#8288;n_MC_samples %% n_train samples&#8288;</code>. Only the latter is
done when <code>n_MC_samples &lt; n_train</code>. This is done separately for each explicand. If <code>FALSE</code>, we randomly sample the
from the observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data table of dimension <code>n_MC_samples</code> <code class="reqn">\times</code> <code>length(Sbar_features)</code> with the sampled observations.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("airquality")
data &lt;- data.table::as.data.table(airquality)
data &lt;- data[complete.cases(data), ]

x_var &lt;- c("Solar.R", "Wind", "Temp", "Month")
y_var &lt;- "Ozone"

ind_x_explain &lt;- 1:6
x_train &lt;- data[-ind_x_explain, ..x_var]
x_train
shapr:::create_marginal_data_training(
  x_train = x_train,
  Sbar_features = c(1, 4),
  n_MC_samples = 10
)

## End(Not run)

</code></pre>

<hr>
<h2 id='default_doc_export'>Exported documentation helper function.</h2><span id='topic+default_doc_export'></span>

<h3>Description</h3>

<p>Exported documentation helper function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_doc_export(internal, iter, index_features)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="default_doc_export_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="default_doc_export_+3A_iter">iter</code></td>
<td>
<p>Integer.
The iteration number. Only used internally.</p>
</td></tr>
<tr><td><code id="default_doc_export_+3A_index_features">index_features</code></td>
<td>
<p>Positive integer vector. Specifies the id_coalition to
apply to the present method. <code>NULL</code> means all coalitions. Only used internally.</p>
</td></tr>
</table>

<hr>
<h2 id='default_doc_internal'>Unexported documentation helper function.</h2><span id='topic+default_doc_internal'></span>

<h3>Description</h3>

<p>Unexported documentation helper function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_doc_internal(
  internal,
  model,
  predict_model,
  x_explain,
  x_train,
  n_features,
  W_kernel,
  S,
  dt_vS,
  output_size,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="default_doc_internal_+3A_internal">internal</code></td>
<td>
<p>List.
Holds all parameters, data, functions and computed objects used within <code><a href="#topic+explain">explain()</a></code>
The list contains one or more of the elements <code>parameters</code>, <code>data</code>, <code>objects</code>, <code>iter_list</code>, <code>timing_list</code>,
<code>main_timing_list</code>, <code>output</code>, and <code>iter_timing_list</code>.</p>
</td></tr>
<tr><td><code id="default_doc_internal_+3A_model">model</code></td>
<td>
<p>Objects.
The model object that ought to be explained.
See the documentation of <code><a href="#topic+explain">explain()</a></code> for details.</p>
</td></tr>
<tr><td><code id="default_doc_internal_+3A_predict_model">predict_model</code></td>
<td>
<p>Function.
The prediction function used when <code>model</code> is not natively supported.
See the documentation of <code><a href="#topic+explain">explain()</a></code> for details.</p>
</td></tr>
<tr><td><code id="default_doc_internal_+3A_x_explain">x_explain</code></td>
<td>
<p>Data.table with the features of the observation whose
predictions ought to be explained (test data).</p>
</td></tr>
<tr><td><code id="default_doc_internal_+3A_x_train">x_train</code></td>
<td>
<p>Data.table with training data.</p>
</td></tr>
<tr><td><code id="default_doc_internal_+3A_n_features">n_features</code></td>
<td>
<p>Positive integer.
The number of features.</p>
</td></tr>
<tr><td><code id="default_doc_internal_+3A_w_kernel">W_kernel</code></td>
<td>
<p>Numeric matrix. Contains all nonscaled weights between training and test
observations for all coalitions. The dimension equals <code style="white-space: pre;">&#8288;n_train x m&#8288;</code>.</p>
</td></tr>
<tr><td><code id="default_doc_internal_+3A_s">S</code></td>
<td>
<p>Integer matrix of dimension <code style="white-space: pre;">&#8288;n_coalitions x m&#8288;</code>, where <code>n_coalitions</code>
and <code>m</code> equals the total number of sampled/non-sampled coalitions and
the total number of unique features, respectively. Note that <code>m = ncol(x_train)</code>.</p>
</td></tr>
<tr><td><code id="default_doc_internal_+3A_dt_vs">dt_vS</code></td>
<td>
<p>Data.table of dimension <code>n_coalitions</code> times <code>n_explain + 1</code> containing the contribution function
estimates. The first column is assumed to be named <code>id_coalition</code> and containing the ids of the coalitions.
The last row is assumed to be the full coalition, i.e., it contains the predicted responses for the observations
which are to be explained.</p>
</td></tr>
<tr><td><code id="default_doc_internal_+3A_output_size">output_size</code></td>
<td>
<p>Scalar integer.
Specifies the dimension of the output from the prediction model for every observation.</p>
</td></tr>
<tr><td><code id="default_doc_internal_+3A_...">...</code></td>
<td>
<p>Further arguments passed to <code>approach</code>-specific functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>internal</code> list.
It holds all parameters, data, and computed objects used within <code><a href="#topic+explain">explain()</a></code>.
</p>

<hr>
<h2 id='exact_coalition_table'>Get table with all (exact) coalitions</h2><span id='topic+exact_coalition_table'></span>

<h3>Description</h3>

<p>Get table with all (exact) coalitions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exact_coalition_table(
  m,
  dt_valid_causal_coalitions = NULL,
  weight_zero_m = 10^6
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="exact_coalition_table_+3A_m">m</code></td>
<td>
<p>Positive integer.
Total number of features/groups.</p>
</td></tr>
<tr><td><code id="exact_coalition_table_+3A_dt_valid_causal_coalitions">dt_valid_causal_coalitions</code></td>
<td>
<p>data.table. Only applicable for asymmetric Shapley
values explanations, and is <code>NULL</code> for symmetric Shapley values.
The data.table contains information about the coalitions that respects the causal ordering.</p>
</td></tr>
<tr><td><code id="exact_coalition_table_+3A_weight_zero_m">weight_zero_m</code></td>
<td>
<p>Numeric.
The value to use as a replacement for infinite coalition weights when doing numerical operations.</p>
</td></tr>
</table>

<hr>
<h2 id='explain'>Explain the output of machine learning models with dependence-aware (conditional/observational) Shapley values</h2><span id='topic+explain'></span>

<h3>Description</h3>

<p>Computes dependence-aware Shapley values for observations in <code>x_explain</code> from the specified
<code>model</code> by using the method specified in <code>approach</code> to estimate the conditional expectation.
See <a href="https://martinjullum.com/publication/aas-2021-explaining/aas-2021-explaining.pdf">Aas et al. (2021)</a>
for a thorough introduction to dependence-aware prediction explanation with Shapley values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>explain(
  model,
  x_explain,
  x_train,
  approach,
  phi0,
  iterative = NULL,
  max_n_coalitions = NULL,
  group = NULL,
  n_MC_samples = 1000,
  seed = 1,
  verbose = "basic",
  predict_model = NULL,
  get_model_specs = NULL,
  prev_shapr_object = NULL,
  asymmetric = FALSE,
  causal_ordering = NULL,
  confounding = NULL,
  extra_computation_args = list(),
  iterative_args = list(),
  output_args = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="explain_+3A_model">model</code></td>
<td>
<p>Model object.
Specifies the model whose predictions we want to explain.
Run <code><a href="#topic+get_supported_models">get_supported_models()</a></code>
for a table of which models <code>explain</code> supports natively. Unsupported models
can still be explained by passing <code>predict_model</code> and (optionally) <code>get_model_specs</code>,
see details for more information.</p>
</td></tr>
<tr><td><code id="explain_+3A_x_explain">x_explain</code></td>
<td>
<p>Matrix or data.frame/data.table.
Contains the the features, whose predictions ought to be explained.</p>
</td></tr>
<tr><td><code id="explain_+3A_x_train">x_train</code></td>
<td>
<p>Matrix or data.frame/data.table.
Contains the data used to estimate the (conditional) distributions for the features
needed to properly estimate the conditional expectations in the Shapley formula.</p>
</td></tr>
<tr><td><code id="explain_+3A_approach">approach</code></td>
<td>
<p>Character vector of length <code>1</code> or one less than the number of features.
All elements should, either be <code>"gaussian"</code>, <code>"copula"</code>, <code>"empirical"</code>, <code>"ctree"</code>, <code>"vaeac"</code>,
<code>"categorical"</code>, <code>"timeseries"</code>, <code>"independence"</code>, <code>"regression_separate"</code>, or <code>"regression_surrogate"</code>.
The two regression approaches can not be combined with any other approach.
See details for more information.</p>
</td></tr>
<tr><td><code id="explain_+3A_phi0">phi0</code></td>
<td>
<p>Numeric.
The prediction value for unseen data, i.e. an estimate of the expected prediction without conditioning on any
features.
Typically we set this value equal to the mean of the response variable in our training data, but other choices
such as the mean of the predictions in the training data are also reasonable.</p>
</td></tr>
<tr><td><code id="explain_+3A_iterative">iterative</code></td>
<td>
<p>Logical or NULL
If <code>NULL</code> (default), the argument is set to <code>TRUE</code> if there are more than 5 features/groups, and <code>FALSE</code> otherwise.
If eventually <code>TRUE</code>, the Shapley values are estimated iteratively in an iterative manner.
This provides sufficiently accurate Shapley value estimates faster.
First an initial number of coalitions is sampled, then bootsrapping is used to estimate the variance of the Shapley
values.
A convergence criterion is used to determine if the variances of the Shapley values are sufficiently small.
If the variances are too high, we estimate the number of required samples to reach convergence, and thereby add more
coalitions.
The process is repeated until the variances are below the threshold.
Specifics related to the iterative process and convergence criterion are set through <code>iterative_args</code>.</p>
</td></tr>
<tr><td><code id="explain_+3A_max_n_coalitions">max_n_coalitions</code></td>
<td>
<p>Integer.
The upper limit on the number of unique feature/group coalitions to use in the iterative procedure
(if <code>iterative = TRUE</code>).
If <code>iterative = FALSE</code> it represents the number of feature/group coalitions to use directly.
The quantity refers to the number of unique feature coalitions if <code>group = NULL</code>,
and group coalitions if <code>group != NULL</code>.
<code>max_n_coalitions = NULL</code> corresponds to <code>max_n_coalitions=2^n_features</code>.</p>
</td></tr>
<tr><td><code id="explain_+3A_group">group</code></td>
<td>
<p>List.
If <code>NULL</code> regular feature wise Shapley values are computed.
If provided, group wise Shapley values are computed.
<code>group</code> then has length equal to the number of groups.
The list element contains character vectors with the features included in each of the different groups.
See
<a href="https://martinjullum.com/publication/jullum-2021-efficient/jullum-2021-efficient.pdf">Jullum et al. (2021)</a>
for more information on group wise Shapley values.</p>
</td></tr>
<tr><td><code id="explain_+3A_n_mc_samples">n_MC_samples</code></td>
<td>
<p>Positive integer.
For most approaches, it indicates the maximum number of samples to use in the Monte Carlo integration
of every conditional expectation.
For <code>approach="ctree"</code>, <code>n_MC_samples</code> corresponds to the number of samples
from the leaf node (see an exception related to the <code>ctree.sample</code> argument <code><a href="#topic+setup_approach.ctree">setup_approach.ctree()</a></code>).
For <code>approach="empirical"</code>, <code>n_MC_samples</code> is  the <code class="reqn">K</code> parameter in equations (14-15) of
Aas et al. (2021), i.e. the maximum number of observations (with largest weights) that is used, see also the
<code>empirical.eta</code> argument <code><a href="#topic+setup_approach.empirical">setup_approach.empirical()</a></code>.</p>
</td></tr>
<tr><td><code id="explain_+3A_seed">seed</code></td>
<td>
<p>Positive integer.
Specifies the seed before any randomness based code is being run.
If <code>NULL</code> no seed is set in the calling environment.</p>
</td></tr>
<tr><td><code id="explain_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
<tr><td><code id="explain_+3A_predict_model">predict_model</code></td>
<td>
<p>Function.
The prediction function used when <code>model</code> is not natively supported.
(Run <code><a href="#topic+get_supported_models">get_supported_models()</a></code> for a list of natively supported models.)
The function must have two arguments, <code>model</code> and <code>newdata</code> which specify, respectively, the model
and a data.frame/data.table to compute predictions for.
The function must give the prediction as a numeric vector.
<code>NULL</code> (the default) uses functions specified internally.
Can also be used to override the default function for natively supported model classes.</p>
</td></tr>
<tr><td><code id="explain_+3A_get_model_specs">get_model_specs</code></td>
<td>
<p>Function.
An optional function for checking model/data consistency when <code>model</code> is not natively supported.
(Run <code><a href="#topic+get_supported_models">get_supported_models()</a></code> for a list of natively supported models.)
The function takes <code>model</code> as argument and provides a list with 3 elements:
</p>

<dl>
<dt>labels</dt><dd><p>Character vector with the names of each feature.</p>
</dd>
<dt>classes</dt><dd><p>Character vector with the classes of each features.</p>
</dd>
<dt>factor_levels</dt><dd><p>Character vector with the levels for any categorical features.</p>
</dd>
</dl>

<p>If <code>NULL</code> (the default) internal functions are used for natively supported model classes, and the checking is
disabled for unsupported model classes.
Can also be used to override the default function for natively supported model classes.</p>
</td></tr>
<tr><td><code id="explain_+3A_prev_shapr_object">prev_shapr_object</code></td>
<td>
<p><code>shapr</code> object or string.
If an object of class <code>shapr</code> is provided, or string with a path to where intermediate results are stored,
then the function will use the previous object to continue the computation.
This is useful if the computation is interrupted or you want higher accuracy than already obtained, and therefore
want to continue the iterative estimation. See the
<a href="https://norskregnesentral.github.io/shapr/articles/general_usage.html">general usage vignette</a> for examples.</p>
</td></tr>
<tr><td><code id="explain_+3A_asymmetric">asymmetric</code></td>
<td>
<p>Logical.
Not applicable for (regular) non-causal or asymmetric explanations.
If <code>FALSE</code> (default), <code>explain</code> computes regular symmetric Shapley values,
If <code>TRUE</code>, then <code>explain</code> compute asymmetric Shapley values based on the (partial) causal ordering
given by <code>causal_ordering</code>. That is, <code>explain</code> only uses the feature combinations/coalitions that
respect the causal ordering when computing the asymmetric Shapley values. If <code>asymmetric</code> is <code>TRUE</code> and
<code>confounding</code> is <code>NULL</code> (default), then <code>explain</code> computes asymmetric conditional Shapley values as specified in
<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf">
Frye et al. (2020)</a>. If <code>confounding</code> is provided, i.e., not <code>NULL</code>, then <code>explain</code> computes asymmetric causal
Shapley values as specified in
<a href="https://proceedings.neurips.cc/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf">
Heskes et al. (2020)</a>.</p>
</td></tr>
<tr><td><code id="explain_+3A_causal_ordering">causal_ordering</code></td>
<td>
<p>List.
Not applicable for (regular) non-causal or asymmetric explanations.
<code>causal_ordering</code> is an unnamed list of vectors specifying the components of the
partial causal ordering that the coalitions must respect. Each vector represents
a component and contains one or more features/groups identified by their names
(strings) or indices (integers). If <code>causal_ordering</code> is <code>NULL</code> (default), no causal
ordering is assumed and all possible coalitions are allowed. No causal ordering is
equivalent to a causal ordering with a single component that includes all features
(<code>list(1:n_features)</code>) or groups (<code>list(1:n_groups)</code>) for feature-wise and group-wise
Shapley values, respectively. For feature-wise Shapley values and
<code>causal_ordering = list(c(1, 2), c(3, 4))</code>, the interpretation is that features 1 and 2
are the ancestors of features 3 and 4, while features 3 and 4 are on the same level.
Note: All features/groups must be included in the <code>causal_ordering</code> without any duplicates.</p>
</td></tr>
<tr><td><code id="explain_+3A_confounding">confounding</code></td>
<td>
<p>Logical vector.
Not applicable for (regular) non-causal or asymmetric explanations.
<code>confounding</code> is a vector of logicals specifying whether confounding is assumed or not for each component in the
<code>causal_ordering</code>. If <code>NULL</code> (default), then no assumption about the confounding structure is made and <code>explain</code>
computes asymmetric/symmetric conditional Shapley values, depending on the value of <code>asymmetric</code>.
If <code>confounding</code> is a single logical, i.e., <code>FALSE</code> or <code>TRUE</code>, then this assumption is set globally
for all components in the causal ordering. Otherwise, <code>confounding</code> must be a vector of logicals of the same
length as <code>causal_ordering</code>, indicating the confounding assumption for each component. When <code>confounding</code> is
specified, then <code>explain</code> computes asymmetric/symmetric causal Shapley values, depending on the value of
<code>asymmetric</code>. The <code>approach</code> cannot be <code>regression_separate</code> and <code>regression_surrogate</code> as the
regression-based approaches are not applicable to the causal Shapley value methodology.</p>
</td></tr>
<tr><td><code id="explain_+3A_extra_computation_args">extra_computation_args</code></td>
<td>
<p>Named list.
Specifies extra arguments related to the computation of the Shapley values.
See <code><a href="#topic+get_extra_comp_args_default">get_extra_comp_args_default()</a></code> for description of the arguments and their default values.</p>
</td></tr>
<tr><td><code id="explain_+3A_iterative_args">iterative_args</code></td>
<td>
<p>Named list.
Specifies the arguments for the iterative procedure.
See <code><a href="#topic+get_iterative_args_default">get_iterative_args_default()</a></code> for description of the arguments and their default values.</p>
</td></tr>
<tr><td><code id="explain_+3A_output_args">output_args</code></td>
<td>
<p>Named list.
Specifies certain arguments related to the output of the function.
See <code><a href="#topic+get_output_args_default">get_output_args_default()</a></code> for description of the arguments and their default values.</p>
</td></tr>
<tr><td><code id="explain_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+setup_approach.categorical">setup_approach.categorical</a></code>, <code><a href="#topic+setup_approach.copula">setup_approach.copula</a></code>, <code><a href="#topic+setup_approach.ctree">setup_approach.ctree</a></code>, <code><a href="#topic+setup_approach.empirical">setup_approach.empirical</a></code>, <code><a href="#topic+setup_approach.gaussian">setup_approach.gaussian</a></code>, <code><a href="#topic+setup_approach.independence">setup_approach.independence</a></code>, <code><a href="#topic+setup_approach.regression_separate">setup_approach.regression_separate</a></code>, <code><a href="#topic+setup_approach.regression_surrogate">setup_approach.regression_surrogate</a></code>, <code><a href="#topic+setup_approach.timeseries">setup_approach.timeseries</a></code>, <code><a href="#topic+setup_approach.vaeac">setup_approach.vaeac</a></code>
</p>

<dl>
<dt><code>categorical.joint_prob_dt</code></dt><dd><p>Data.table. (Optional)
Containing the joint probability distribution for each combination of feature
values.
<code>NULL</code> means it is estimated from the <code>x_train</code> and <code>x_explain</code>.</p>
</dd>
<dt><code>categorical.epsilon</code></dt><dd><p>Numeric value. (Optional)
If <code>categorical.joint_probability_dt</code> is not supplied, probabilities/frequencies are
estimated using <code>x_train</code>. If certain observations occur in <code>x_explain</code> and NOT in <code>x_train</code>,
then epsilon is used as the proportion of times that these observations occurs in the training data.
In theory, this proportion should be zero, but this causes an error later in the Shapley computation.</p>
</dd>
<dt><code>internal</code></dt><dd><p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</dd>
<dt><code>ctree.mincriterion</code></dt><dd><p>Numeric scalar or vector.
Either a scalar or vector of length equal to the number of features in the model.
The value is equal to 1 - <code class="reqn">\alpha</code> where <code class="reqn">\alpha</code> is the nominal level of the conditional independence tests.
If it is a vector, this indicates which value to use when conditioning on various numbers of features.
The default value is 0.95.</p>
</dd>
<dt><code>ctree.minsplit</code></dt><dd><p>Numeric scalar.
Determines minimum value that the sum of the left and right daughter nodes required for a split.
The default value is 20.</p>
</dd>
<dt><code>ctree.minbucket</code></dt><dd><p>Numeric scalar.
Determines the minimum sum of weights in a terminal node required for a split
The default value is 7.</p>
</dd>
<dt><code>ctree.sample</code></dt><dd><p>Boolean.
If <code>TRUE</code> (default), then the method always samples <code>n_MC_samples</code> observations from the leaf nodes
(with replacement).
If <code>FALSE</code> and the number of observations in the leaf node is less than <code>n_MC_samples</code>,
the method will take all observations in the leaf.
If <code>FALSE</code> and the number of observations in the leaf node is more than <code>n_MC_samples</code>,
the method will sample <code>n_MC_samples</code> observations (with replacement).
This means that there will always be sampling in the leaf unless
<code>sample = FALSE</code> <em>and</em> the number of obs in the node is less than <code>n_MC_samples</code>.</p>
</dd>
<dt><code>empirical.type</code></dt><dd><p>Character. (default = <code>"fixed_sigma"</code>)
Should be equal to either <code>"independence"</code>,<code>"fixed_sigma"</code>, <code>"AICc_each_k"</code> <code>"AICc_full"</code>.
<code>"independence"</code> is deprecated. Use <code>approach = "independence"</code> instead.
<code>"fixed_sigma"</code> uses a fixed bandwidth (set through <code>empirical.fixed_sigma</code>) in the kernel density estimation.
<code>"AICc_each_k"</code> and <code>"AICc_full"</code> optimize the bandwidth using the AICc criterion, with respectively
one bandwidth per coalition size and one bandwidth for all coalition sizes.</p>
</dd>
<dt><code>empirical.eta</code></dt><dd><p>Numeric scalar.
Needs to be <code style="white-space: pre;">&#8288;0 &lt; eta &lt;= 1&#8288;</code>.
The default value is 0.95.
Represents the minimum proportion of the total empirical weight that data samples should use.
If e.g. <code>eta = .8</code> we will choose the <code>K</code> samples with the largest weight so that the sum of the weights
accounts for 80\
<code>eta</code> is the <code class="reqn">\eta</code> parameter in equation (15) of
<a href="https://martinjullum.com/publication/aas-2021-explaining/aas-2021-explaining.pdf">Aas et al. (2021)</a>.</p>
</dd>
<dt><code>empirical.fixed_sigma</code></dt><dd><p>Positive numeric scalar.
The default value is 0.1.
Represents the kernel bandwidth in the distance computation used when conditioning on all different coalitions.
Only used when <code>empirical.type = "fixed_sigma"</code></p>
</dd>
<dt><code>empirical.n_samples_aicc</code></dt><dd><p>Positive integer.
Number of samples to consider in AICc optimization.
The default value is 1000.
Only used for <code>empirical.type</code> is either <code>"AICc_each_k"</code> or <code>"AICc_full"</code>.</p>
</dd>
<dt><code>empirical.eval_max_aicc</code></dt><dd><p>Positive integer.
Maximum number of iterations when optimizing the AICc.
The default value is 20.
Only used for <code>empirical.type</code> is either <code>"AICc_each_k"</code> or <code>"AICc_full"</code>.</p>
</dd>
<dt><code>empirical.start_aicc</code></dt><dd><p>Numeric.
Start value of the <code>sigma</code> parameter when optimizing the AICc.
The default value is 0.1.
Only used for <code>empirical.type</code> is either <code>"AICc_each_k"</code> or <code>"AICc_full"</code>.</p>
</dd>
<dt><code>empirical.cov_mat</code></dt><dd><p>Numeric matrix. (Optional)
The covariance matrix of the data generating distribution used to define the Mahalanobis distance.
<code>NULL</code> means it is estimated from <code>x_train</code>.</p>
</dd>
<dt><code>gaussian.mu</code></dt><dd><p>Numeric vector. (Optional)
Containing the mean of the data generating distribution.
<code>NULL</code> means it is estimated from the <code>x_train</code>.</p>
</dd>
<dt><code>gaussian.cov_mat</code></dt><dd><p>Numeric matrix. (Optional)
Containing the covariance matrix of the data generating distribution.
<code>NULL</code> means it is estimated from the <code>x_train</code>.</p>
</dd>
<dt><code>regression.model</code></dt><dd><p>A <code>tidymodels</code> object of class <code>model_specs</code>. Default is a linear regression model, i.e.,
<code><a href="parsnip.html#topic+linear_reg">parsnip::linear_reg()</a></code>. See <a href="https://www.tidymodels.org/find/parsnip/">tidymodels</a> for all possible models,
and see the vignette for how to add new/own models. Note, to make it easier to call <code>explain()</code> from Python, the
<code>regression.model</code> parameter can also be a string specifying the model which will be parsed and evaluated. For
example, <code style="white-space: pre;">&#8288;"parsnip::rand_forest(mtry = hardhat::tune(), trees = 100, engine = "ranger", mode = "regression")"&#8288;</code>
is also a valid input. It is essential to include the package prefix if the package is not loaded.</p>
</dd>
<dt><code>regression.tune_values</code></dt><dd><p>Either <code>NULL</code> (default), a data.frame/data.table/tibble, or a function.
The data.frame must contain the possible hyperparameter value combinations to try.
The column names must match the names of the tunable parameters specified in <code>regression.model</code>.
If <code>regression.tune_values</code> is a function, then it should take one argument <code>x</code> which is the training data
for the current coalition and returns a data.frame/data.table/tibble with the properties described above.
Using a function allows the hyperparameter values to change based on the size of the coalition See the regression
vignette for several examples.
Note, to make it easier to call <code>explain()</code> from Python, the <code>regression.tune_values</code> can also be a string
containing an R function. For example,
<code>"function(x) return(dials::grid_regular(dials::mtry(c(1, ncol(x)))), levels = 3))"</code> is also a valid input.
It is essential to include the package prefix if the package is not loaded.</p>
</dd>
<dt><code>regression.vfold_cv_para</code></dt><dd><p>Either <code>NULL</code> (default) or a named list containing
the parameters to be sent to <code><a href="rsample.html#topic+vfold_cv">rsample::vfold_cv()</a></code>. See the regression vignette for
several examples.</p>
</dd>
<dt><code>regression.recipe_func</code></dt><dd><p>Either <code>NULL</code> (default) or a function that that takes in a <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>
object and returns a modified <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code> with potentially additional recipe steps. See the regression
vignette for several examples.
Note, to make it easier to call <code>explain()</code> from Python, the <code>regression.recipe_func</code> can also be a string
containing an R function. For example,
<code>"function(recipe) return(recipes::step_ns(recipe, recipes::all_numeric_predictors(), deg_free = 2))"</code> is also
a valid input. It is essential to include the package prefix if the package is not loaded.</p>
</dd>
<dt><code>regression.surrogate_n_comb</code></dt><dd><p>Positive integer.
Specifies the number of unique coalitions to apply to each training observation.
The default is the number of sampled coalitions in the present iteration.
Any integer between 1 and the default is allowed.
Larger values requires more memory, but may improve the surrogate model.
If the user sets a value lower than the maximum, we sample this amount of unique coalitions
separately for each training observations.
That is, on average, all coalitions should be equally trained.</p>
</dd>
<dt><code>timeseries.fixed_sigma</code></dt><dd><p>Positive numeric scalar.
Represents the kernel bandwidth in the distance computation.
The default value is 2.</p>
</dd>
<dt><code>timeseries.bounds</code></dt><dd><p>Numeric vector of length two.
Specifies the lower and upper bounds of the timeseries.
The default is <code>c(NULL, NULL)</code>, i.e. no bounds.
If one or both of these bounds are not <code>NULL</code>, we restrict the sampled time series to be between these bounds.
This is useful if the underlying time series are scaled between 0 and 1, for example.</p>
</dd>
<dt><code>vaeac.depth</code></dt><dd><p>Positive integer (default is <code>3</code>). The number of hidden layers
in the neural networks of the masked encoder, full encoder, and decoder.</p>
</dd>
<dt><code>vaeac.width</code></dt><dd><p>Positive integer (default is <code>32</code>). The number of neurons in each
hidden layer in the neural networks of the masked encoder, full encoder, and decoder.</p>
</dd>
<dt><code>vaeac.latent_dim</code></dt><dd><p>Positive integer (default is <code>8</code>). The number of dimensions in the latent space.</p>
</dd>
<dt><code>vaeac.lr</code></dt><dd><p>Positive numeric (default is <code>0.001</code>). The learning rate used in the <code><a href="torch.html#topic+optim_adam">torch::optim_adam()</a></code> optimizer.</p>
</dd>
<dt><code>vaeac.activation_function</code></dt><dd><p>An <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> representing an activation function such as, e.g.,
<code><a href="torch.html#topic+nn_relu">torch::nn_relu()</a></code> (default), <code><a href="torch.html#topic+nn_leaky_relu">torch::nn_leaky_relu()</a></code>, <code><a href="torch.html#topic+nn_selu">torch::nn_selu()</a></code>, or <code><a href="torch.html#topic+nn_sigmoid">torch::nn_sigmoid()</a></code>.</p>
</dd>
<dt><code>vaeac.n_vaeacs_initialize</code></dt><dd><p>Positive integer (default is <code>4</code>). The number of different vaeac models to initiate
in the start. Pick the best performing one after <code>vaeac.extra_parameters$epochs_initiation_phase</code>
epochs (default is <code>2</code>) and continue training that one.</p>
</dd>
<dt><code>vaeac.epochs</code></dt><dd><p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>vaeac.extra_parameters$epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</dd>
<dt><code>vaeac.extra_parameters</code></dt><dd><p>Named list with extra parameters to the <code>vaeac</code> approach. See
<code><a href="#topic+vaeac_get_extra_para_default">vaeac_get_extra_para_default()</a></code> for description of possible additional parameters and their default values.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>shapr</code> package implements kernelSHAP estimation of dependence-aware Shapley values with
eight different Monte Carlo-based approaches for estimating the conditional distributions of the data.
These are all introduced in the
<a href="https://norskregnesentral.github.io/shapr/articles/general_usage.html">general usage vignette</a>.
(From R: <code>vignette("general_usage", package = "shapr")</code>).
Moreover,
<a href="https://martinjullum.com/publication/aas-2021-explaining/aas-2021-explaining.pdf">Aas et al. (2021)</a>
gives a general introduction to dependence-aware Shapley values, and the three approaches <code>"empirical"</code>,
<code>"gaussian"</code>, <code>"copula"</code>, and also discusses <code>"independence"</code>.
<a href="https://martinjullum.com/publication/redelmeier-2020-explaining/redelmeier-2020-explaining.pdf">
Redelmeier et al. (2020)</a> introduces the approach <code>"ctree"</code>.
<a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a> introduces the <code>"vaeac"</code>
approach.
Approach <code>"timeseries"</code> is discussed in
<a href="https://martinjullum.com/publication/jullum-2021-efficient/jullum-2021-efficient.pdf">Jullum et al. (2021)</a>.
<code>shapr</code> has also implemented two regression-based approaches <code>"regression_separate"</code> and <code>"regression_surrogate"</code>,
as described in <a href="https://link.springer.com/content/pdf/10.1007/s10618-024-01016-z.pdf">Olsen et al. (2024)</a>.
It is also possible to combine the different approaches, see the
<a href="https://norskregnesentral.github.io/shapr/articles/general_usage.html">
general usage</a> for more information.
</p>
<p>The package also supports the computation of causal and asymmetric Shapley values as introduced by
<a href="https://proceedings.neurips.cc/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf">
Heskes et al. (2020)</a> and
<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf">
Frye et al. (2020)</a>.
Asymmetric Shapley values were proposed by
<a href="https://proceedings.neurips.cc/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf">
Heskes et al. (2020)</a> as a way to incorporate causal knowledge in
the real world by restricting the possible feature combinations/coalitions when computing the Shapley values to
those consistent with a (partial) causal ordering.
Causal Shapley values were proposed by
<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf">
Frye et al. (2020)</a> as a way to explain the total effect of features
on the prediction, taking into account their causal relationships, by adapting the sampling procedure in <code>shapr</code>.
</p>
<p>The package allows for parallelized computation with progress updates through the tightly connected
<a href="future.html#topic+future">future::future</a> and <a href="progressr.html#topic+progressr">progressr::progressr</a> packages.
See the examples below.
For iterative estimation (<code>iterative=TRUE</code>), intermediate results may also be printed to the console
(according to the <code>verbose</code> argument).
Moreover, the intermediate results are written to disk.
This combined batch computing of the v(S) values, enables fast and accurate estimation of the Shapley values
in a memory friendly manner.
</p>


<h3>Value</h3>

<p>Object of class <code>c("shapr", "list")</code>. Contains the following items:
</p>

<dl>
<dt><code>shapley_values_est</code></dt><dd><p>data.table with the estimated Shapley values with explained observation in the rows and
features along the columns.
The column <code>none</code> is the prediction not devoted to any of the features (given by the argument <code>phi0</code>)</p>
</dd>
<dt><code>shapley_values_sd</code></dt><dd><p>data.table with the standard deviation of the Shapley values reflecting the uncertainty.
Note that this only reflects the coalition sampling part of the kernelSHAP procedure, and is therefore by
definition 0 when all coalitions is used.
Only present when <code>extra_computation_args$compute_sd=TRUE</code>, which is the default when <code>iterative = TRUE</code></p>
</dd>
<dt><code>internal</code></dt><dd><p>List with the different parameters, data, functions and other output used internally.</p>
</dd>
<dt><code>pred_explain</code></dt><dd><p>Numeric vector with the predictions for the explained observations</p>
</dd>
<dt><code>MSEv</code></dt><dd><p>List with the values of the MSEv evaluation criterion for the approach. See the
<a href="https://norskregnesentral.github.io/shapr/articles/general_usage.html#msev-evaluation-criterion">MSEv evaluation section in the general usage for details</a>.</p>
</dd>
<dt><code>timing</code></dt><dd><p>List containing timing information for the different parts of the computation.
<code>init_time</code> and <code>end_time</code> gives the time stamps for the start and end of the computation.
<code>total_time_secs</code> gives the total time in seconds for the complete execution of <code>explain()</code>.
<code>main_timing_secs</code> gives the time in seconds for the main computations.
<code>iter_timing_secs</code> gives for each iteration of the iterative estimation, the time spent on the different parts
iterative estimation routine.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Martin Jullum, Lars Henry Berge Olsen
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://martinjullum.com/publication/aas-2021-explaining/aas-2021-explaining.pdf">
Aas, K., Jullum, M., &amp; Løland, A. (2021). Explaining individual predictions when features are dependent:
More accurate approximations to Shapley values. Artificial Intelligence, 298, 103502</a>
</p>
</li>
<li> <p><a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf">
Frye, C., Rowat, C., &amp; Feige, I. (2020). Asymmetric Shapley values:
incorporating causal knowledge into model-agnostic explainability.
Advances in neural information processing systems, 33, 1229-1239</a>
</p>
</li>
<li> <p><a href="https://proceedings.neurips.cc/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf">
Heskes, T., Sijben, E., Bucur, I. G., &amp; Claassen, T. (2020). Causal shapley values:
Exploiting causal knowledge to explain individual predictions of complex models.
Advances in neural information processing systems, 33, 4778-4789</a>
</p>
</li>
<li> <p><a href="https://martinjullum.com/publication/jullum-2021-efficient/jullum-2021-efficient.pdf">
Jullum, M., Redelmeier, A. &amp; Aas, K. (2021). Efficient and simple prediction explanations with
groupShapley: A practical perspective. Italian Workshop on Explainable Artificial Intelligence 2021.</a>
</p>
</li>
<li> <p><a href="https://martinjullum.com/publication/redelmeier-2020-explaining/redelmeier-2020-explaining.pdf">
Redelmeier, A., Jullum, M., &amp; Aas, K. (2020). Explaining predictive models with mixed features using Shapley
values and conditional inference trees. In Machine Learning and Knowledge Extraction:
International Cross-Domain Conference, CD-MAKE 2020, Dublin, Ireland, August 25–28, 2020, Proceedings 4
(pp. 117-137). Springer International Publishing.</a>
</p>
</li>
<li> <p><a href="https://www.theoj.org/joss-papers/joss.02027/10.21105.joss.02027.pdf">
Sellereite N., &amp; Jullum, M. (2019). shapr: An R-package for explaining machine learning models with
dependence-aware Shapley values. Journal of Open Source Software, 5(46), 2027</a>
</p>
</li>
<li> <p><a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">
Olsen, L. H., Glad, I. K., Jullum, M., &amp; Aas, K. (2022). Using Shapley values and variational autoencoders to
explain predictive models with dependent mixed features. Journal of machine learning research, 23(213), 1-51</a>
</p>
</li>
<li> <p><a href="https://link.springer.com/content/pdf/10.1007/s10618-024-01016-z.pdf">
Olsen, L. H. B., Glad, I. K., Jullum, M., &amp; Aas, K. (2024). A comparative study of methods for estimating
model-agnostic Shapley value explanations. Data Mining and Knowledge Discovery, 1-48</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/pdf/2410.04883">
Olsen, L. H. B., &amp; Jullum, M. (2024). Improving the Sampling Strategy in KernelSHAP. arXiv e-prints, arXiv-2410</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# Load example data
data("airquality")
airquality &lt;- airquality[complete.cases(airquality), ]
x_var &lt;- c("Solar.R", "Wind", "Temp", "Month")
y_var &lt;- "Ozone"

# Split data into test- and training data
data_train &lt;- head(airquality, -3)
data_explain &lt;- tail(airquality, 3)

x_train &lt;- data_train[, x_var]
x_explain &lt;- data_explain[, x_var]

# Fit a linear model
lm_formula &lt;- as.formula(paste0(y_var, " ~ ", paste0(x_var, collapse = " + ")))
model &lt;- lm(lm_formula, data = data_train)

# Explain predictions
p &lt;- mean(data_train[, y_var])

# (Optionally) enable parallelization via the future package
if (requireNamespace("future", quietly = TRUE)) {
  future::plan("multisession", workers = 2)
}


# (Optionally) enable progress updates within every iteration via the progressr package
if (requireNamespace("progressr", quietly = TRUE)) {
  progressr::handlers(global = TRUE)
}

# Empirical approach
explain1 &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "empirical",
  phi0 = p,
  n_MC_samples = 1e2
)

# Gaussian approach
explain2 &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "gaussian",
  phi0 = p,
  n_MC_samples = 1e2
)

# Gaussian copula approach
explain3 &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "copula",
  phi0 = p,
  n_MC_samples = 1e2
)

# ctree approach
explain4 &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "ctree",
  phi0 = p,
  n_MC_samples = 1e2
)

# Combined approach
approach &lt;- c("gaussian", "gaussian", "empirical")
explain5 &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = approach,
  phi0 = p,
  n_MC_samples = 1e2
)

# Print the Shapley values
print(explain1$shapley_values_est)

# Plot the results
if (requireNamespace("ggplot2", quietly = TRUE)) {
  plot(explain1)
  plot(explain1, plot_type = "waterfall")
}

# Group-wise explanations
group_list &lt;- list(A = c("Temp", "Month"), B = c("Wind", "Solar.R"))

explain_groups &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  group = group_list,
  approach = "empirical",
  phi0 = p,
  n_MC_samples = 1e2
)
print(explain_groups$shapley_values_est)

# Separate and surrogate regression approaches with linear regression models.
explain_separate_lm &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  phi0 = p,
  approach = "regression_separate",
  regression.model = parsnip::linear_reg()
)

explain_surrogate_lm &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  phi0 = p,
  approach = "regression_surrogate",
  regression.model = parsnip::linear_reg()
)

# Iterative estimation
# For illustration purposes only. By default not used for such small dimensions as here

# Gaussian approach
explain_iterative &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "gaussian",
  phi0 = p,
  n_MC_samples = 1e2,
  iterative = TRUE,
  iterative_args = list(initial_n_coalitions = 10)
)

## End(Not run)

</code></pre>

<hr>
<h2 id='explain_forecast'>Explain a forecast from time series models with dependence-aware (conditional/observational) Shapley values</h2><span id='topic+explain_forecast'></span>

<h3>Description</h3>

<p>Computes dependence-aware Shapley values for observations in <code>explain_idx</code> from the specified
<code>model</code> by using the method specified in <code>approach</code> to estimate the conditional expectation.
See
<a href="https://martinjullum.com/publication/aas-2021-explaining/aas-2021-explaining.pdf">Aas, et. al (2021)</a>
for a thorough introduction to dependence-aware prediction explanation with Shapley values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>explain_forecast(
  model,
  y,
  xreg = NULL,
  train_idx = NULL,
  explain_idx,
  explain_y_lags,
  explain_xreg_lags = explain_y_lags,
  horizon,
  approach,
  phi0,
  max_n_coalitions = NULL,
  iterative = NULL,
  group_lags = TRUE,
  group = NULL,
  n_MC_samples = 1000,
  seed = 1,
  predict_model = NULL,
  get_model_specs = NULL,
  verbose = "basic",
  extra_computation_args = list(),
  iterative_args = list(),
  output_args = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="explain_forecast_+3A_model">model</code></td>
<td>
<p>Model object.
Specifies the model whose predictions we want to explain.
Run <code><a href="#topic+get_supported_models">get_supported_models()</a></code>
for a table of which models <code>explain</code> supports natively. Unsupported models
can still be explained by passing <code>predict_model</code> and (optionally) <code>get_model_specs</code>,
see details for more information.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_y">y</code></td>
<td>
<p>Matrix, data.frame/data.table or a numeric vector.
Contains the endogenous variables used to estimate the (conditional) distributions
needed to properly estimate the conditional expectations in the Shapley formula
including the observations to be explained.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_xreg">xreg</code></td>
<td>
<p>Matrix, data.frame/data.table or a numeric vector.
Contains the exogenous variables used to estimate the (conditional) distributions
needed to properly estimate the conditional expectations in the Shapley formula
including the observations to be explained.
As exogenous variables are used contemporaneously when producing a forecast,
this item should contain nrow(y) + horizon rows.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_train_idx">train_idx</code></td>
<td>
<p>Numeric vector.
The row indices in data and reg denoting points in time to use when estimating the conditional expectations in
the Shapley value formula.
If <code>train_idx = NULL</code> (default) all indices not selected to be explained will be used.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_explain_idx">explain_idx</code></td>
<td>
<p>Numeric vector.
The row indices in data and reg denoting points in time to explain.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_explain_y_lags">explain_y_lags</code></td>
<td>
<p>Numeric vector.
Denotes the number of lags that should be used for each variable in <code>y</code> when making a forecast.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_explain_xreg_lags">explain_xreg_lags</code></td>
<td>
<p>Numeric vector.
If <code>xreg != NULL</code>, denotes the number of lags that should be used for each variable in <code>xreg</code> when making a forecast.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_horizon">horizon</code></td>
<td>
<p>Numeric.
The forecast horizon to explain. Passed to the <code>predict_model</code> function.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_approach">approach</code></td>
<td>
<p>Character vector of length <code>1</code> or one less than the number of features.
All elements should, either be <code>"gaussian"</code>, <code>"copula"</code>, <code>"empirical"</code>, <code>"ctree"</code>, <code>"vaeac"</code>,
<code>"categorical"</code>, <code>"timeseries"</code>, <code>"independence"</code>, <code>"regression_separate"</code>, or <code>"regression_surrogate"</code>.
The two regression approaches can not be combined with any other approach.
See details for more information.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_phi0">phi0</code></td>
<td>
<p>Numeric.
The prediction value for unseen data, i.e. an estimate of the expected prediction without conditioning on any
features.
Typically we set this value equal to the mean of the response variable in our training data, but other choices
such as the mean of the predictions in the training data are also reasonable.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_max_n_coalitions">max_n_coalitions</code></td>
<td>
<p>Integer.
The upper limit on the number of unique feature/group coalitions to use in the iterative procedure
(if <code>iterative = TRUE</code>).
If <code>iterative = FALSE</code> it represents the number of feature/group coalitions to use directly.
The quantity refers to the number of unique feature coalitions if <code>group = NULL</code>,
and group coalitions if <code>group != NULL</code>.
<code>max_n_coalitions = NULL</code> corresponds to <code>max_n_coalitions=2^n_features</code>.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_iterative">iterative</code></td>
<td>
<p>Logical or NULL
If <code>NULL</code> (default), the argument is set to <code>TRUE</code> if there are more than 5 features/groups, and <code>FALSE</code> otherwise.
If eventually <code>TRUE</code>, the Shapley values are estimated iteratively in an iterative manner.
This provides sufficiently accurate Shapley value estimates faster.
First an initial number of coalitions is sampled, then bootsrapping is used to estimate the variance of the Shapley
values.
A convergence criterion is used to determine if the variances of the Shapley values are sufficiently small.
If the variances are too high, we estimate the number of required samples to reach convergence, and thereby add more
coalitions.
The process is repeated until the variances are below the threshold.
Specifics related to the iterative process and convergence criterion are set through <code>iterative_args</code>.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_group_lags">group_lags</code></td>
<td>
<p>Logical.
If <code>TRUE</code> all lags of each variable are grouped together and explained as a group.
If <code>FALSE</code> all lags of each variable are explained individually.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_group">group</code></td>
<td>
<p>List.
If <code>NULL</code> regular feature wise Shapley values are computed.
If provided, group wise Shapley values are computed.
<code>group</code> then has length equal to the number of groups.
The list element contains character vectors with the features included in each of the different groups.
See
<a href="https://martinjullum.com/publication/jullum-2021-efficient/jullum-2021-efficient.pdf">Jullum et al. (2021)</a>
for more information on group wise Shapley values.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_n_mc_samples">n_MC_samples</code></td>
<td>
<p>Positive integer.
For most approaches, it indicates the maximum number of samples to use in the Monte Carlo integration
of every conditional expectation.
For <code>approach="ctree"</code>, <code>n_MC_samples</code> corresponds to the number of samples
from the leaf node (see an exception related to the <code>ctree.sample</code> argument <code><a href="#topic+setup_approach.ctree">setup_approach.ctree()</a></code>).
For <code>approach="empirical"</code>, <code>n_MC_samples</code> is  the <code class="reqn">K</code> parameter in equations (14-15) of
Aas et al. (2021), i.e. the maximum number of observations (with largest weights) that is used, see also the
<code>empirical.eta</code> argument <code><a href="#topic+setup_approach.empirical">setup_approach.empirical()</a></code>.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_seed">seed</code></td>
<td>
<p>Positive integer.
Specifies the seed before any randomness based code is being run.
If <code>NULL</code> no seed is set in the calling environment.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_predict_model">predict_model</code></td>
<td>
<p>Function.
The prediction function used when <code>model</code> is not natively supported.
(Run <code><a href="#topic+get_supported_models">get_supported_models()</a></code> for a list of natively supported models.)
The function must have two arguments, <code>model</code> and <code>newdata</code> which specify, respectively, the model
and a data.frame/data.table to compute predictions for.
The function must give the prediction as a numeric vector.
<code>NULL</code> (the default) uses functions specified internally.
Can also be used to override the default function for natively supported model classes.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_get_model_specs">get_model_specs</code></td>
<td>
<p>Function.
An optional function for checking model/data consistency when <code>model</code> is not natively supported.
(Run <code><a href="#topic+get_supported_models">get_supported_models()</a></code> for a list of natively supported models.)
The function takes <code>model</code> as argument and provides a list with 3 elements:
</p>

<dl>
<dt>labels</dt><dd><p>Character vector with the names of each feature.</p>
</dd>
<dt>classes</dt><dd><p>Character vector with the classes of each features.</p>
</dd>
<dt>factor_levels</dt><dd><p>Character vector with the levels for any categorical features.</p>
</dd>
</dl>

<p>If <code>NULL</code> (the default) internal functions are used for natively supported model classes, and the checking is
disabled for unsupported model classes.
Can also be used to override the default function for natively supported model classes.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
<tr><td><code id="explain_forecast_+3A_extra_computation_args">extra_computation_args</code></td>
<td>
<p>Named list.
Specifies extra arguments related to the computation of the Shapley values.
See <code><a href="#topic+get_extra_comp_args_default">get_extra_comp_args_default()</a></code> for description of the arguments and their default values.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_iterative_args">iterative_args</code></td>
<td>
<p>Named list.
Specifies the arguments for the iterative procedure.
See <code><a href="#topic+get_iterative_args_default">get_iterative_args_default()</a></code> for description of the arguments and their default values.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_output_args">output_args</code></td>
<td>
<p>Named list.
Specifies certain arguments related to the output of the function.
See <code><a href="#topic+get_output_args_default">get_output_args_default()</a></code> for description of the arguments and their default values.</p>
</td></tr>
<tr><td><code id="explain_forecast_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+setup_approach.categorical">setup_approach.categorical</a></code>, <code><a href="#topic+setup_approach.copula">setup_approach.copula</a></code>, <code><a href="#topic+setup_approach.ctree">setup_approach.ctree</a></code>, <code><a href="#topic+setup_approach.empirical">setup_approach.empirical</a></code>, <code><a href="#topic+setup_approach.gaussian">setup_approach.gaussian</a></code>, <code><a href="#topic+setup_approach.independence">setup_approach.independence</a></code>, <code><a href="#topic+setup_approach.timeseries">setup_approach.timeseries</a></code>, <code><a href="#topic+setup_approach.vaeac">setup_approach.vaeac</a></code>
</p>

<dl>
<dt><code>categorical.joint_prob_dt</code></dt><dd><p>Data.table. (Optional)
Containing the joint probability distribution for each combination of feature
values.
<code>NULL</code> means it is estimated from the <code>x_train</code> and <code>x_explain</code>.</p>
</dd>
<dt><code>categorical.epsilon</code></dt><dd><p>Numeric value. (Optional)
If <code>categorical.joint_probability_dt</code> is not supplied, probabilities/frequencies are
estimated using <code>x_train</code>. If certain observations occur in <code>x_explain</code> and NOT in <code>x_train</code>,
then epsilon is used as the proportion of times that these observations occurs in the training data.
In theory, this proportion should be zero, but this causes an error later in the Shapley computation.</p>
</dd>
<dt><code>internal</code></dt><dd><p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</dd>
<dt><code>ctree.mincriterion</code></dt><dd><p>Numeric scalar or vector.
Either a scalar or vector of length equal to the number of features in the model.
The value is equal to 1 - <code class="reqn">\alpha</code> where <code class="reqn">\alpha</code> is the nominal level of the conditional independence tests.
If it is a vector, this indicates which value to use when conditioning on various numbers of features.
The default value is 0.95.</p>
</dd>
<dt><code>ctree.minsplit</code></dt><dd><p>Numeric scalar.
Determines minimum value that the sum of the left and right daughter nodes required for a split.
The default value is 20.</p>
</dd>
<dt><code>ctree.minbucket</code></dt><dd><p>Numeric scalar.
Determines the minimum sum of weights in a terminal node required for a split
The default value is 7.</p>
</dd>
<dt><code>ctree.sample</code></dt><dd><p>Boolean.
If <code>TRUE</code> (default), then the method always samples <code>n_MC_samples</code> observations from the leaf nodes
(with replacement).
If <code>FALSE</code> and the number of observations in the leaf node is less than <code>n_MC_samples</code>,
the method will take all observations in the leaf.
If <code>FALSE</code> and the number of observations in the leaf node is more than <code>n_MC_samples</code>,
the method will sample <code>n_MC_samples</code> observations (with replacement).
This means that there will always be sampling in the leaf unless
<code>sample = FALSE</code> <em>and</em> the number of obs in the node is less than <code>n_MC_samples</code>.</p>
</dd>
<dt><code>empirical.type</code></dt><dd><p>Character. (default = <code>"fixed_sigma"</code>)
Should be equal to either <code>"independence"</code>,<code>"fixed_sigma"</code>, <code>"AICc_each_k"</code> <code>"AICc_full"</code>.
<code>"independence"</code> is deprecated. Use <code>approach = "independence"</code> instead.
<code>"fixed_sigma"</code> uses a fixed bandwidth (set through <code>empirical.fixed_sigma</code>) in the kernel density estimation.
<code>"AICc_each_k"</code> and <code>"AICc_full"</code> optimize the bandwidth using the AICc criterion, with respectively
one bandwidth per coalition size and one bandwidth for all coalition sizes.</p>
</dd>
<dt><code>empirical.eta</code></dt><dd><p>Numeric scalar.
Needs to be <code style="white-space: pre;">&#8288;0 &lt; eta &lt;= 1&#8288;</code>.
The default value is 0.95.
Represents the minimum proportion of the total empirical weight that data samples should use.
If e.g. <code>eta = .8</code> we will choose the <code>K</code> samples with the largest weight so that the sum of the weights
accounts for 80\
<code>eta</code> is the <code class="reqn">\eta</code> parameter in equation (15) of
<a href="https://martinjullum.com/publication/aas-2021-explaining/aas-2021-explaining.pdf">Aas et al. (2021)</a>.</p>
</dd>
<dt><code>empirical.fixed_sigma</code></dt><dd><p>Positive numeric scalar.
The default value is 0.1.
Represents the kernel bandwidth in the distance computation used when conditioning on all different coalitions.
Only used when <code>empirical.type = "fixed_sigma"</code></p>
</dd>
<dt><code>empirical.n_samples_aicc</code></dt><dd><p>Positive integer.
Number of samples to consider in AICc optimization.
The default value is 1000.
Only used for <code>empirical.type</code> is either <code>"AICc_each_k"</code> or <code>"AICc_full"</code>.</p>
</dd>
<dt><code>empirical.eval_max_aicc</code></dt><dd><p>Positive integer.
Maximum number of iterations when optimizing the AICc.
The default value is 20.
Only used for <code>empirical.type</code> is either <code>"AICc_each_k"</code> or <code>"AICc_full"</code>.</p>
</dd>
<dt><code>empirical.start_aicc</code></dt><dd><p>Numeric.
Start value of the <code>sigma</code> parameter when optimizing the AICc.
The default value is 0.1.
Only used for <code>empirical.type</code> is either <code>"AICc_each_k"</code> or <code>"AICc_full"</code>.</p>
</dd>
<dt><code>empirical.cov_mat</code></dt><dd><p>Numeric matrix. (Optional)
The covariance matrix of the data generating distribution used to define the Mahalanobis distance.
<code>NULL</code> means it is estimated from <code>x_train</code>.</p>
</dd>
<dt><code>gaussian.mu</code></dt><dd><p>Numeric vector. (Optional)
Containing the mean of the data generating distribution.
<code>NULL</code> means it is estimated from the <code>x_train</code>.</p>
</dd>
<dt><code>gaussian.cov_mat</code></dt><dd><p>Numeric matrix. (Optional)
Containing the covariance matrix of the data generating distribution.
<code>NULL</code> means it is estimated from the <code>x_train</code>.</p>
</dd>
<dt><code>timeseries.fixed_sigma</code></dt><dd><p>Positive numeric scalar.
Represents the kernel bandwidth in the distance computation.
The default value is 2.</p>
</dd>
<dt><code>timeseries.bounds</code></dt><dd><p>Numeric vector of length two.
Specifies the lower and upper bounds of the timeseries.
The default is <code>c(NULL, NULL)</code>, i.e. no bounds.
If one or both of these bounds are not <code>NULL</code>, we restrict the sampled time series to be between these bounds.
This is useful if the underlying time series are scaled between 0 and 1, for example.</p>
</dd>
<dt><code>vaeac.depth</code></dt><dd><p>Positive integer (default is <code>3</code>). The number of hidden layers
in the neural networks of the masked encoder, full encoder, and decoder.</p>
</dd>
<dt><code>vaeac.width</code></dt><dd><p>Positive integer (default is <code>32</code>). The number of neurons in each
hidden layer in the neural networks of the masked encoder, full encoder, and decoder.</p>
</dd>
<dt><code>vaeac.latent_dim</code></dt><dd><p>Positive integer (default is <code>8</code>). The number of dimensions in the latent space.</p>
</dd>
<dt><code>vaeac.lr</code></dt><dd><p>Positive numeric (default is <code>0.001</code>). The learning rate used in the <code><a href="torch.html#topic+optim_adam">torch::optim_adam()</a></code> optimizer.</p>
</dd>
<dt><code>vaeac.activation_function</code></dt><dd><p>An <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> representing an activation function such as, e.g.,
<code><a href="torch.html#topic+nn_relu">torch::nn_relu()</a></code> (default), <code><a href="torch.html#topic+nn_leaky_relu">torch::nn_leaky_relu()</a></code>, <code><a href="torch.html#topic+nn_selu">torch::nn_selu()</a></code>, or <code><a href="torch.html#topic+nn_sigmoid">torch::nn_sigmoid()</a></code>.</p>
</dd>
<dt><code>vaeac.n_vaeacs_initialize</code></dt><dd><p>Positive integer (default is <code>4</code>). The number of different vaeac models to initiate
in the start. Pick the best performing one after <code>vaeac.extra_parameters$epochs_initiation_phase</code>
epochs (default is <code>2</code>) and continue training that one.</p>
</dd>
<dt><code>vaeac.epochs</code></dt><dd><p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>vaeac.extra_parameters$epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</dd>
<dt><code>vaeac.extra_parameters</code></dt><dd><p>Named list with extra parameters to the <code>vaeac</code> approach. See
<code><a href="#topic+vaeac_get_extra_para_default">vaeac_get_extra_para_default()</a></code> for description of possible additional parameters and their default values.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function explains a forecast of length <code>horizon</code>. The argument <code>train_idx</code>
is analogous to x_train in <code>explain()</code>, however, it just contains the time indices of where
in the data the forecast should start for each training sample. In the same way <code>explain_idx</code>
defines the time index (indices) which will precede a forecast to be explained.
</p>
<p>As any autoregressive forecast model will require a set of lags to make a forecast at an
arbitrary point in time, <code>explain_y_lags</code> and <code>explain_xreg_lags</code> define how many lags
are required to &quot;refit&quot; the model at any given time index. This allows the different
approaches to work in the same way they do for time-invariant models.
</p>
<p>See the <a href="https://norskregnesentral.github.io/shapr/articles/general_usage.html#forecasting">
forecasting section of the general usages</a> for further details.
</p>


<h3>Value</h3>

<p>Object of class <code>c("shapr", "list")</code>. Contains the following items:
</p>

<dl>
<dt><code>shapley_values_est</code></dt><dd><p>data.table with the estimated Shapley values with explained observation in the rows and
features along the columns.
The column <code>none</code> is the prediction not devoted to any of the features (given by the argument <code>phi0</code>)</p>
</dd>
<dt><code>shapley_values_sd</code></dt><dd><p>data.table with the standard deviation of the Shapley values reflecting the uncertainty.
Note that this only reflects the coalition sampling part of the kernelSHAP procedure, and is therefore by
definition 0 when all coalitions is used.
Only present when <code>extra_computation_args$compute_sd=TRUE</code>, which is the default when <code>iterative = TRUE</code></p>
</dd>
<dt><code>internal</code></dt><dd><p>List with the different parameters, data, functions and other output used internally.</p>
</dd>
<dt><code>pred_explain</code></dt><dd><p>Numeric vector with the predictions for the explained observations</p>
</dd>
<dt><code>MSEv</code></dt><dd><p>List with the values of the MSEv evaluation criterion for the approach. See the
<a href="https://norskregnesentral.github.io/shapr/articles/general_usage.html#msev-evaluation-criterion">MSEv evaluation section in the general usage for details</a>.</p>
</dd>
<dt><code>timing</code></dt><dd><p>List containing timing information for the different parts of the computation.
<code>init_time</code> and <code>end_time</code> gives the time stamps for the start and end of the computation.
<code>total_time_secs</code> gives the total time in seconds for the complete execution of <code>explain()</code>.
<code>main_timing_secs</code> gives the time in seconds for the main computations.
<code>iter_timing_secs</code> gives for each iteration of the iterative estimation, the time spent on the different parts
iterative estimation routine.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Jon Lachmann, Martin Jullum
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://martinjullum.com/publication/aas-2021-explaining/aas-2021-explaining.pdf">
Aas, K., Jullum, M., &amp; Løland, A. (2021). Explaining individual predictions when features are dependent:
More accurate approximations to Shapley values. Artificial Intelligence, 298, 103502</a>
</p>
</li>
<li> <p><a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf">
Frye, C., Rowat, C., &amp; Feige, I. (2020). Asymmetric Shapley values:
incorporating causal knowledge into model-agnostic explainability.
Advances in neural information processing systems, 33, 1229-1239</a>
</p>
</li>
<li> <p><a href="https://proceedings.neurips.cc/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf">
Heskes, T., Sijben, E., Bucur, I. G., &amp; Claassen, T. (2020). Causal shapley values:
Exploiting causal knowledge to explain individual predictions of complex models.
Advances in neural information processing systems, 33, 4778-4789</a>
</p>
</li>
<li> <p><a href="https://martinjullum.com/publication/jullum-2021-efficient/jullum-2021-efficient.pdf">
Jullum, M., Redelmeier, A. &amp; Aas, K. (2021). Efficient and simple prediction explanations with
groupShapley: A practical perspective. Italian Workshop on Explainable Artificial Intelligence 2021.</a>
</p>
</li>
<li> <p><a href="https://martinjullum.com/publication/redelmeier-2020-explaining/redelmeier-2020-explaining.pdf">
Redelmeier, A., Jullum, M., &amp; Aas, K. (2020). Explaining predictive models with mixed features using Shapley
values and conditional inference trees. In Machine Learning and Knowledge Extraction:
International Cross-Domain Conference, CD-MAKE 2020, Dublin, Ireland, August 25–28, 2020, Proceedings 4
(pp. 117-137). Springer International Publishing.</a>
</p>
</li>
<li> <p><a href="https://www.theoj.org/joss-papers/joss.02027/10.21105.joss.02027.pdf">
Sellereite N., &amp; Jullum, M. (2019). shapr: An R-package for explaining machine learning models with
dependence-aware Shapley values. Journal of Open Source Software, 5(46), 2027</a>
</p>
</li>
<li> <p><a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">
Olsen, L. H., Glad, I. K., Jullum, M., &amp; Aas, K. (2022). Using Shapley values and variational autoencoders to
explain predictive models with dependent mixed features. Journal of machine learning research, 23(213), 1-51</a>
</p>
</li>
<li> <p><a href="https://link.springer.com/content/pdf/10.1007/s10618-024-01016-z.pdf">
Olsen, L. H. B., Glad, I. K., Jullum, M., &amp; Aas, K. (2024). A comparative study of methods for estimating
model-agnostic Shapley value explanations. Data Mining and Knowledge Discovery, 1-48</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/pdf/2410.04883">
Olsen, L. H. B., &amp; Jullum, M. (2024). Improving the Sampling Strategy in KernelSHAP. arXiv e-prints, arXiv-2410</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Load example data
data("airquality")
data &lt;- data.table::as.data.table(airquality)

# Fit an AR(2) model.
model_ar_temp &lt;- ar(data$Temp, order = 2)

# Calculate the zero prediction values for a three step forecast.
p0_ar &lt;- rep(mean(data$Temp), 3)

# Empirical approach, explaining forecasts starting at T = 152 and T = 153.
explain_forecast(
  model = model_ar_temp,
  y = data[, "Temp"],
  train_idx = 2:151,
  explain_idx = 152:153,
  explain_y_lags = 2,
  horizon = 3,
  approach = "empirical",
  phi0 = p0_ar,
  group_lags = FALSE
)

## End(Not run)

</code></pre>

<hr>
<h2 id='finalize_explanation'>Gathers the final output to create the explanation object</h2><span id='topic+finalize_explanation'></span>

<h3>Description</h3>

<p>Gathers the final output to create the explanation object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>finalize_explanation(internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="finalize_explanation_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='gauss_cat_loss'>A <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> Representing a <code>gauss_cat_loss</code></h2><span id='topic+gauss_cat_loss'></span>

<h3>Description</h3>

<p>The <code style="white-space: pre;">&#8288;gauss_cat_loss module&#8288;</code> layer computes the log probability of the <code>groundtruth</code> for each object
given the mask and the distribution parameters. That is, the log-likelihoods of the true/full training observations
based on the generative distributions parameters <code>distr_params</code> inferred by the masked versions of the observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gauss_cat_loss(one_hot_max_sizes, min_sigma = 1e-04, min_prob = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gauss_cat_loss_+3A_one_hot_max_sizes">one_hot_max_sizes</code></td>
<td>
<p>A torch tensor of dimension <code>n_features</code> containing the one hot sizes of the <code>n_features</code>
features. That is, if the <code>i</code>th feature is a categorical feature with 5 levels, then <code>one_hot_max_sizes[i] = 5</code>.
While the size for continuous features can either be <code>0</code> or <code>1</code>.</p>
</td></tr>
<tr><td><code id="gauss_cat_loss_+3A_min_sigma">min_sigma</code></td>
<td>
<p>For stability it might be desirable that the minimal sigma is not too close to zero.</p>
</td></tr>
<tr><td><code id="gauss_cat_loss_+3A_min_prob">min_prob</code></td>
<td>
<p>For stability it might be desirable that the minimal probability is not too close to zero.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the module works with mixed data represented as 2-dimensional inputs and it
works correctly with missing values in <code>groundtruth</code> as long as they are represented by NaNs.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='gauss_cat_parameters'>A <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> Representing a <code>gauss_cat_parameters</code></h2><span id='topic+gauss_cat_parameters'></span>

<h3>Description</h3>

<p>The <code>gauss_cat_parameters</code> module extracts the parameters from the inferred generative Gaussian and
categorical distributions for the continuous and categorical features, respectively.
</p>
<p>If <code>one_hot_max_sizes</code> is <code class="reqn">[4, 1, 1, 2]</code>, then the inferred distribution parameters for one observation is the
vector <code class="reqn">[p_{00}, p_{01}, p_{02}, p_{03}, \mu_1, \sigma_1, \mu_2, \sigma_2, p_{30}, p_{31}]</code>, where
<code class="reqn">\operatorname{Softmax}([p_{00}, p_{01}, p_{02}, p_{03}])</code> and <code class="reqn">\operatorname{Softmax}([p_{30}, p_{31}])</code>
are probabilities of the first and the fourth feature categories respectively in the model generative distribution,
and Gaussian(<code class="reqn">\mu_1, \sigma_1^2</code>) and Gaussian(<code class="reqn">\mu_2, \sigma_2^2</code>) are the model generative distributions
on the second and the third features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gauss_cat_parameters(one_hot_max_sizes, min_sigma = 1e-04, min_prob = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gauss_cat_parameters_+3A_one_hot_max_sizes">one_hot_max_sizes</code></td>
<td>
<p>A torch tensor of dimension <code>n_features</code> containing the one hot sizes of the <code>n_features</code>
features. That is, if the <code>i</code>th feature is a categorical feature with 5 levels, then <code>one_hot_max_sizes[i] = 5</code>.
While the size for continuous features can either be <code>0</code> or <code>1</code>.</p>
</td></tr>
<tr><td><code id="gauss_cat_parameters_+3A_min_sigma">min_sigma</code></td>
<td>
<p>For stability it might be desirable that the minimal sigma is not too close to zero.</p>
</td></tr>
<tr><td><code id="gauss_cat_parameters_+3A_min_prob">min_prob</code></td>
<td>
<p>For stability it might be desirable that the minimal probability is not too close to zero.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='gauss_cat_sampler_most_likely'>A <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> Representing a <code>gauss_cat_sampler_most_likely</code></h2><span id='topic+gauss_cat_sampler_most_likely'></span>

<h3>Description</h3>

<p>The <code>gauss_cat_sampler_most_likely</code> generates the most likely samples from the generative distribution
defined by the output of the vaeac. I.e., the layer will return the mean and most probable class for the Gaussian
(continuous features) and categorical (categorical features) distributions, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gauss_cat_sampler_most_likely(
  one_hot_max_sizes,
  min_sigma = 1e-04,
  min_prob = 1e-04
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gauss_cat_sampler_most_likely_+3A_one_hot_max_sizes">one_hot_max_sizes</code></td>
<td>
<p>A torch tensor of dimension <code>n_features</code> containing the one hot sizes of the <code>n_features</code>
features. That is, if the <code>i</code>th feature is a categorical feature with 5 levels, then <code>one_hot_max_sizes[i] = 5</code>.
While the size for continuous features can either be <code>0</code> or <code>1</code>.</p>
</td></tr>
<tr><td><code id="gauss_cat_sampler_most_likely_+3A_min_sigma">min_sigma</code></td>
<td>
<p>For stability it might be desirable that the minimal sigma is not too close to zero.</p>
</td></tr>
<tr><td><code id="gauss_cat_sampler_most_likely_+3A_min_prob">min_prob</code></td>
<td>
<p>For stability it might be desirable that the minimal probability is not too close to zero.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>gauss_cat_sampler_most_likely</code> object.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='gauss_cat_sampler_random'>A <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> Representing a gauss_cat_sampler_random</h2><span id='topic+gauss_cat_sampler_random'></span>

<h3>Description</h3>

<p>The <code>gauss_cat_sampler_random</code> generates random samples from the generative distribution defined by the
output of the vaeac. The random sample is generated by sampling from the inferred Gaussian and categorical
distributions for the continuous and categorical features, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gauss_cat_sampler_random(
  one_hot_max_sizes,
  min_sigma = 1e-04,
  min_prob = 1e-04
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gauss_cat_sampler_random_+3A_one_hot_max_sizes">one_hot_max_sizes</code></td>
<td>
<p>A torch tensor of dimension <code>n_features</code> containing the one hot sizes of the <code>n_features</code>
features. That is, if the <code>i</code>th feature is a categorical feature with 5 levels, then <code>one_hot_max_sizes[i] = 5</code>.
While the size for continuous features can either be <code>0</code> or <code>1</code>.</p>
</td></tr>
<tr><td><code id="gauss_cat_sampler_random_+3A_min_sigma">min_sigma</code></td>
<td>
<p>For stability it might be desirable that the minimal sigma is not too close to zero.</p>
</td></tr>
<tr><td><code id="gauss_cat_sampler_random_+3A_min_prob">min_prob</code></td>
<td>
<p>For stability it might be desirable that the minimal probability is not too close to zero.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='gaussian_transform'>Transforms a sample to standardized normal distribution</h2><span id='topic+gaussian_transform'></span>

<h3>Description</h3>

<p>Transforms a sample to standardized normal distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_transform(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gaussian_transform_+3A_x">x</code></td>
<td>
<p>Numeric vector.The data which should be transformed to a standard normal distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector of length <code>length(x)</code>
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='gaussian_transform_separate'>Transforms new data to standardized normal (dimension 1) based on other data transformations</h2><span id='topic+gaussian_transform_separate'></span>

<h3>Description</h3>

<p>Transforms new data to standardized normal (dimension 1) based on other data transformations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_transform_separate(yx, n_y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gaussian_transform_separate_+3A_yx">yx</code></td>
<td>
<p>Numeric vector. The first <code>n_y</code> items is the data that is transformed, and last
part is the data with the original transformation.</p>
</td></tr>
<tr><td><code id="gaussian_transform_separate_+3A_n_y">n_y</code></td>
<td>
<p>Positive integer. Number of elements of <code>yx</code> that belongs to the Gaussian data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of back-transformed Gaussian data
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='get_cov_mat'>get_cov_mat</h2><span id='topic+get_cov_mat'></span>

<h3>Description</h3>

<p>get_cov_mat
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_cov_mat(x_train, min_eigen_value = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_cov_mat_+3A_x_train">x_train</code></td>
<td>
<p>Matrix or data.frame/data.table.
Contains the data used to estimate the (conditional) distributions for the features
needed to properly estimate the conditional expectations in the Shapley formula.</p>
</td></tr>
<tr><td><code id="get_cov_mat_+3A_min_eigen_value">min_eigen_value</code></td>
<td>
<p>Numeric
Specifies the smallest allowed eigen value before the covariance matrix of <code>x_train</code> is assumed to not be
positive definite, and <code><a href="Matrix.html#topic+nearPD">Matrix::nearPD()</a></code> is used to find the nearest one.</p>
</td></tr>
</table>

<hr>
<h2 id='get_data_forecast'>Set up data for explain_forecast</h2><span id='topic+get_data_forecast'></span>

<h3>Description</h3>

<p>Set up data for explain_forecast
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_data_forecast(
  y,
  xreg,
  train_idx,
  explain_idx,
  explain_y_lags,
  explain_xreg_lags,
  horizon
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_data_forecast_+3A_y">y</code></td>
<td>
<p>Matrix, data.frame/data.table or a numeric vector.
Contains the endogenous variables used to estimate the (conditional) distributions
needed to properly estimate the conditional expectations in the Shapley formula
including the observations to be explained.</p>
</td></tr>
<tr><td><code id="get_data_forecast_+3A_xreg">xreg</code></td>
<td>
<p>Matrix, data.frame/data.table or a numeric vector.
Contains the exogenous variables used to estimate the (conditional) distributions
needed to properly estimate the conditional expectations in the Shapley formula
including the observations to be explained.
As exogenous variables are used contemporaneously when producing a forecast,
this item should contain nrow(y) + horizon rows.</p>
</td></tr>
<tr><td><code id="get_data_forecast_+3A_train_idx">train_idx</code></td>
<td>
<p>Numeric vector.
The row indices in data and reg denoting points in time to use when estimating the conditional expectations in
the Shapley value formula.
If <code>train_idx = NULL</code> (default) all indices not selected to be explained will be used.</p>
</td></tr>
<tr><td><code id="get_data_forecast_+3A_explain_idx">explain_idx</code></td>
<td>
<p>Numeric vector.
The row indices in data and reg denoting points in time to explain.</p>
</td></tr>
<tr><td><code id="get_data_forecast_+3A_explain_y_lags">explain_y_lags</code></td>
<td>
<p>Numeric vector.
Denotes the number of lags that should be used for each variable in <code>y</code> when making a forecast.</p>
</td></tr>
<tr><td><code id="get_data_forecast_+3A_explain_xreg_lags">explain_xreg_lags</code></td>
<td>
<p>Numeric vector.
If <code>xreg != NULL</code>, denotes the number of lags that should be used for each variable in <code>xreg</code> when making a forecast.</p>
</td></tr>
<tr><td><code id="get_data_forecast_+3A_horizon">horizon</code></td>
<td>
<p>Numeric.
The forecast horizon to explain. Passed to the <code>predict_model</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing
</p>

<ul>
<li><p> The data.frames x_train and x_explain which holds the lagged data examples.
</p>
</li>
<li><p> A numeric, n_endo denoting how many columns are endogenous in x_train and x_explain.
</p>
</li>
<li><p> A list, group with groupings of each variable to explain per variable and not per variable and lag.
</p>
</li></ul>


<hr>
<h2 id='get_data_specs'>Fetches feature information from a given data set</h2><span id='topic+get_data_specs'></span>

<h3>Description</h3>

<p>Fetches feature information from a given data set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_data_specs(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_data_specs_+3A_x">x</code></td>
<td>
<p>data.frame or data.table.
The data to extract feature information from.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to extract the feature information to be checked against the corresponding
information extracted from the model and other data sets.
The function is only called internally
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<dl>
<dt>labels</dt><dd><p>character vector with the feature names to compute Shapley values for</p>
</dd>
<dt>classes</dt><dd><p>a named character vector with the labels as names and the class types as elements</p>
</dd>
<dt>factor_levels</dt><dd><p>a named list with the labels as names and character vectors with the factor levels as elements
(NULL if the feature is not a factor)</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load example data
## Not run: 
data("airquality")
airquality &lt;- airquality[complete.cases(airquality), ]
# Split data into test- and training data
x_train &lt;- head(airquality, -3)
x_explain &lt;- tail(airquality, 3)
# Split data into test- and training data
x_train &lt;- data.table::as.data.table(head(airquality))
x_train[, Temp := as.factor(Temp)]
shapr:::get_data_specs(x_train)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_extra_comp_args_default'>Gets the default values for the extra estimation arguments</h2><span id='topic+get_extra_comp_args_default'></span>

<h3>Description</h3>

<p>Gets the default values for the extra estimation arguments
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_extra_comp_args_default(
  internal,
  paired_shap_sampling = isFALSE(internal$parameters$asymmetric),
  kernelSHAP_reweighting = "on_all_cond",
  compute_sd = isFALSE(internal$parameters$exact),
  n_boot_samps = 100,
  max_batch_size = 10,
  min_n_batches = 10
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_extra_comp_args_default_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="get_extra_comp_args_default_+3A_paired_shap_sampling">paired_shap_sampling</code></td>
<td>
<p>Logical.
If <code>TRUE</code> paired versions of all sampled coalitions are also included in the computation.
That is, if there are 5 features and e.g. coalitions (1,3,5) are sampled, then also coalition (2,4) is used for
computing the Shapley values. This is done to reduce the variance of the Shapley value estimates.
<code>TRUE</code> is the default and is recommended for highest accuracy.
For asymmetric, <code>FALSE</code> is the default and the only legal value.</p>
</td></tr>
<tr><td><code id="get_extra_comp_args_default_+3A_kernelshap_reweighting">kernelSHAP_reweighting</code></td>
<td>
<p>String.
How to reweight the sampling frequency weights in the kernelSHAP solution after sampling.
The aim of this is to reduce the randomness and thereby the variance of the Shapley value estimates.
The options are one of <code>'none'</code>, <code>'on_N'</code>, <code>'on_all'</code>, <code>'on_all_cond'</code> (default).
<code>'none'</code> means no reweighting, i.e. the sampling frequency weights are used as is.
<code>'on_N'</code> means the sampling frequencies are averaged over all coalitions with the same original sampling
probabilities.
<code>'on_all'</code> means the original sampling probabilities are used for all coalitions.
<code>'on_all_cond'</code> means the original sampling probabilities are used for all coalitions, while adjusting for the
probability that they are sampled at least once.
<code>'on_all_cond'</code> is preferred as it performs the best in simulation studies, see
<a href="https://arxiv.org/pdf/2410.04883">Olsen &amp; Jullum (2024)</a>.</p>
</td></tr>
<tr><td><code id="get_extra_comp_args_default_+3A_compute_sd">compute_sd</code></td>
<td>
<p>Logical. Whether to estimate the standard deviations of the Shapley value estimates. This is TRUE
whenever sampling based kernelSHAP is applied (either iteratively or with a fixed number of coalitions).</p>
</td></tr>
<tr><td><code id="get_extra_comp_args_default_+3A_n_boot_samps">n_boot_samps</code></td>
<td>
<p>Integer. The number of bootstrapped samples (i.e. samples with replacement) from the set of all
coalitions used to estimate the standard deviations of the Shapley value estimates.</p>
</td></tr>
<tr><td><code id="get_extra_comp_args_default_+3A_max_batch_size">max_batch_size</code></td>
<td>
<p>Integer. The maximum number of coalitions to estimate simultaneously within each iteration.
A larger numbers requires more memory, but may have a slight computational advantage.</p>
</td></tr>
<tr><td><code id="get_extra_comp_args_default_+3A_min_n_batches">min_n_batches</code></td>
<td>
<p>Integer. The minimum number of batches to split the computation into within each iteration.
Larger numbers gives more frequent progress updates. If parallelization is applied, this should be set no smaller
than the number of parallel workers.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/pdf/2410.04883">
Olsen, L. H. B., &amp; Jullum, M. (2024). Improving the Sampling Strategy in KernelSHAP.
arXiv preprint arXiv:2410.04883.</a>
</p>
</li></ul>


<hr>
<h2 id='get_extra_parameters'>This includes both extra parameters and other objects</h2><span id='topic+get_extra_parameters'></span>

<h3>Description</h3>

<p>This includes both extra parameters and other objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_extra_parameters(internal, type)
</code></pre>

<hr>
<h2 id='get_feature_specs'>Gets the feature specifications form the model</h2><span id='topic+get_feature_specs'></span>

<h3>Description</h3>

<p>Gets the feature specifications form the model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_feature_specs(get_model_specs, model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_feature_specs_+3A_get_model_specs">get_model_specs</code></td>
<td>
<p>Function.
An optional function for checking model/data consistency when <code>model</code> is not natively supported.
(Run <code><a href="#topic+get_supported_models">get_supported_models()</a></code> for a list of natively supported models.)
The function takes <code>model</code> as argument and provides a list with 3 elements:
</p>

<dl>
<dt>labels</dt><dd><p>Character vector with the names of each feature.</p>
</dd>
<dt>classes</dt><dd><p>Character vector with the classes of each features.</p>
</dd>
<dt>factor_levels</dt><dd><p>Character vector with the levels for any categorical features.</p>
</dd>
</dl>

<p>If <code>NULL</code> (the default) internal functions are used for natively supported model classes, and the checking is
disabled for unsupported model classes.
Can also be used to override the default function for natively supported model classes.</p>
</td></tr>
<tr><td><code id="get_feature_specs_+3A_model">model</code></td>
<td>
<p>Model object.
Specifies the model whose predictions we want to explain.
Run <code><a href="#topic+get_supported_models">get_supported_models()</a></code>
for a table of which models <code>explain</code> supports natively. Unsupported models
can still be explained by passing <code>predict_model</code> and (optionally) <code>get_model_specs</code>,
see details for more information.</p>
</td></tr>
</table>

<hr>
<h2 id='get_iterative_args_default'>Function to specify arguments of the iterative estimation procedure</h2><span id='topic+get_iterative_args_default'></span>

<h3>Description</h3>

<p>Function to specify arguments of the iterative estimation procedure
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_iterative_args_default(
  internal,
  initial_n_coalitions = ceiling(min(200, max(5, internal$parameters$n_features,
    (2^internal$parameters$n_features)/10), internal$parameters$max_n_coalitions)),
  fixed_n_coalitions_per_iter = NULL,
  max_iter = 20,
  convergence_tol = 0.02,
  n_coal_next_iter_factor_vec = c(seq(0.1, 1, by = 0.1), rep(1, max_iter - 10))
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_iterative_args_default_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="get_iterative_args_default_+3A_initial_n_coalitions">initial_n_coalitions</code></td>
<td>
<p>Integer. Number of coalitions to use in the first estimation iteration.</p>
</td></tr>
<tr><td><code id="get_iterative_args_default_+3A_fixed_n_coalitions_per_iter">fixed_n_coalitions_per_iter</code></td>
<td>
<p>Integer. Number of <code>n_coalitions</code> to use in each iteration.
<code>NULL</code> (default) means setting it based on estimates based on a set convergence threshold.</p>
</td></tr>
<tr><td><code id="get_iterative_args_default_+3A_max_iter">max_iter</code></td>
<td>
<p>Integer. Maximum number of estimation iterations</p>
</td></tr>
<tr><td><code id="get_iterative_args_default_+3A_convergence_tol">convergence_tol</code></td>
<td>
<p>Numeric. The t variable in the convergence threshold formula on page 6 in the paper
Covert and Lee (2021), 'Improving KernelSHAP: Practical Shapley Value Estimation via Linear Regression'
https://arxiv.org/pdf/2012.01536. Smaller values requires more coalitions before convergence is reached.</p>
</td></tr>
<tr><td><code id="get_iterative_args_default_+3A_n_coal_next_iter_factor_vec">n_coal_next_iter_factor_vec</code></td>
<td>
<p>Numeric vector. The number of <code>n_coalitions</code> that must be used to reach
convergence in the next iteration is estimated.
The number of <code>n_coalitions</code> actually used in the next iteration is set to this estimate multiplied by
<code>n_coal_next_iter_factor_vec[i]</code> for iteration <code>i</code>.
It is wise to start with smaller numbers to avoid using too many <code>n_coalitions</code> due to uncertain estimates in
the first iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions sets default values for the iterative estimation procedure, according to the function
defaults.
If the argument <code>iterative</code> of <code><a href="#topic+explain">explain()</a></code> is FALSE, it sets parameters corresponding to the use of a
non-iterative estimation procedure
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='get_max_n_coalitions_causal'>Get the number of coalitions that respects the causal ordering</h2><span id='topic+get_max_n_coalitions_causal'></span>

<h3>Description</h3>

<p>Get the number of coalitions that respects the causal ordering
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_max_n_coalitions_causal(causal_ordering)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_max_n_coalitions_causal_+3A_causal_ordering">causal_ordering</code></td>
<td>
<p>List.
Not applicable for (regular) non-causal or asymmetric explanations.
<code>causal_ordering</code> is an unnamed list of vectors specifying the components of the
partial causal ordering that the coalitions must respect. Each vector represents
a component and contains one or more features/groups identified by their names
(strings) or indices (integers). If <code>causal_ordering</code> is <code>NULL</code> (default), no causal
ordering is assumed and all possible coalitions are allowed. No causal ordering is
equivalent to a causal ordering with a single component that includes all features
(<code>list(1:n_features)</code>) or groups (<code>list(1:n_groups)</code>) for feature-wise and group-wise
Shapley values, respectively. For feature-wise Shapley values and
<code>causal_ordering = list(c(1, 2), c(3, 4))</code>, the interpretation is that features 1 and 2
are the ancestors of features 3 and 4, while features 3 and 4 are on the same level.
Note: All features/groups must be included in the <code>causal_ordering</code> without any duplicates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the number of coalitions that respects the causal ordering by computing the number
of coalitions in each partial causal component and then summing these. We compute
the number of coalitions in the <code class="reqn">i</code>th a partial causal component by <code class="reqn">2^n - 1</code>,
where <code class="reqn">n</code> is the number of features in the the <code class="reqn">i</code>th partial causal component
and we subtract one as we do not want to include the situation where no features in
the <code class="reqn">i</code>th partial causal component are present. In the end, we add 1 for the
empty coalition.
</p>


<h3>Value</h3>

<p>Integer. The (maximum) number of coalitions that respects the causal ordering.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
get_max_n_coalitions_causal(list(1:10)) # 2^10 = 1024 (no causal order)
get_max_n_coalitions_causal(list(1:3, 4:7, 8:10)) # 30
get_max_n_coalitions_causal(list(1:3, 4:5, 6:7, 8, 9:10)) # 18
get_max_n_coalitions_causal(list(1:3, c(4, 8), c(5, 7), 6, 9:10)) # 18
get_max_n_coalitions_causal(list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) # 11

## End(Not run)

</code></pre>

<hr>
<h2 id='get_model_specs'>Fetches feature information from natively supported models</h2><span id='topic+get_model_specs'></span><span id='topic+get_model_specs.default'></span><span id='topic+get_model_specs.ar'></span><span id='topic+get_model_specs.Arima'></span><span id='topic+get_model_specs.forecast_ARIMA'></span><span id='topic+get_model_specs.glm'></span><span id='topic+get_model_specs.lm'></span><span id='topic+get_model_specs.gam'></span><span id='topic+get_model_specs.ranger'></span><span id='topic+get_model_specs.workflow'></span><span id='topic+get_model_specs.xgb.Booster'></span>

<h3>Description</h3>

<p>This function is used to extract the feature information from the model to be checked against the
corresponding feature information in the data passed to <code><a href="#topic+explain">explain()</a></code>.
</p>
<p>NOTE: You should never need to call this function explicitly.
It is exported just to be easier accessible for users, see details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_model_specs(x)

## Default S3 method:
get_model_specs(x)

## S3 method for class 'ar'
get_model_specs(x)

## S3 method for class 'Arima'
get_model_specs(x)

## S3 method for class 'forecast_ARIMA'
get_model_specs(x)

## S3 method for class 'glm'
get_model_specs(x)

## S3 method for class 'lm'
get_model_specs(x)

## S3 method for class 'gam'
get_model_specs(x)

## S3 method for class 'ranger'
get_model_specs(x)

## S3 method for class 'workflow'
get_model_specs(x)

## S3 method for class 'xgb.Booster'
get_model_specs(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_model_specs_+3A_x">x</code></td>
<td>
<p>Model object for the model to be explained.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If you are explaining a model not supported natively, you may (optionally) enable such checking by
creating this function yourself and passing it on to <code><a href="#topic+explain">explain()</a></code>.
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<dl>
<dt>labels</dt><dd><p>character vector with the feature names to compute Shapley values for</p>
</dd>
<dt>classes</dt><dd><p>a named character vector with the labels as names and the class type as elements</p>
</dd>
<dt>factor_levels</dt><dd><p>a named list with the labels as names and character vectors with the factor levels as elements
(NULL if the feature is not a factor)</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>See Also</h3>

<p>For model classes not supported natively, you NEED to create an analogue to <code><a href="#topic+predict_model">predict_model()</a></code>. See it's
help file for details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load example data
data("airquality")
airquality &lt;- airquality[complete.cases(airquality), ]
# Split data into test- and training data
x_train &lt;- head(airquality, -3)
x_explain &lt;- tail(airquality, 3)
# Fit a linear model
model &lt;- lm(Ozone ~ Solar.R + Wind + Temp + Month, data = x_train)
get_model_specs(model)

</code></pre>

<hr>
<h2 id='get_mu_vec'>get_mu_vec</h2><span id='topic+get_mu_vec'></span>

<h3>Description</h3>

<p>get_mu_vec
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_mu_vec(x_train)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_mu_vec_+3A_x_train">x_train</code></td>
<td>
<p>Matrix or data.frame/data.table.
Contains the data used to estimate the (conditional) distributions for the features
needed to properly estimate the conditional expectations in the Shapley formula.</p>
</td></tr>
</table>

<hr>
<h2 id='get_output_args_default'>Gets the default values for the output arguments</h2><span id='topic+get_output_args_default'></span>

<h3>Description</h3>

<p>Gets the default values for the output arguments
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_output_args_default(
  keep_samp_for_vS = FALSE,
  MSEv_uniform_comb_weights = TRUE,
  saving_path = tempfile("shapr_obj_", fileext = ".rds")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_output_args_default_+3A_keep_samp_for_vs">keep_samp_for_vS</code></td>
<td>
<p>Logical.
Indicates whether the samples used in the Monte Carlo estimation of v_S should be returned (in <code>internal$output</code>).
Not used for <code>approach="regression_separate"</code> or <code>approach="regression_surrogate"</code>.</p>
</td></tr>
<tr><td><code id="get_output_args_default_+3A_msev_uniform_comb_weights">MSEv_uniform_comb_weights</code></td>
<td>
<p>Logical.
If <code>TRUE</code> (default), then the function weights the coalitions uniformly when computing the MSEv criterion.
If <code>FALSE</code>, then the function use the Shapley kernel weights to weight the coalitions when computing the MSEv
criterion.
Note that the Shapley kernel weights are replaced by the sampling frequency when not all coalitions are considered.</p>
</td></tr>
<tr><td><code id="get_output_args_default_+3A_saving_path">saving_path</code></td>
<td>
<p>String.
The path to the directory where the results of the iterative estimation procedure should be saved.
Defaults to a temporary directory.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='get_predict_model'>Get predict_model function</h2><span id='topic+get_predict_model'></span>

<h3>Description</h3>

<p>Get predict_model function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_predict_model(predict_model, model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_predict_model_+3A_predict_model">predict_model</code></td>
<td>
<p>Function.
The prediction function used when <code>model</code> is not natively supported.
See the documentation of <code><a href="#topic+explain">explain()</a></code> for details.</p>
</td></tr>
<tr><td><code id="get_predict_model_+3A_model">model</code></td>
<td>
<p>Objects.
The model object that ought to be explained.
See the documentation of <code><a href="#topic+explain">explain()</a></code> for details.</p>
</td></tr>
</table>

<hr>
<h2 id='get_S_causal_steps'>Get the steps for generating MC samples for coalitions following a causal ordering</h2><span id='topic+get_S_causal_steps'></span>

<h3>Description</h3>

<p>Get the steps for generating MC samples for coalitions following a causal ordering
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_S_causal_steps(S, causal_ordering, confounding, as_string = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_S_causal_steps_+3A_s">S</code></td>
<td>
<p>Integer matrix of dimension <code>n_coalitions_valid x m</code>, where <code>n_coalitions_valid</code> equals
the total number of valid coalitions that respect the causal ordering given in <code>causal_ordering</code> and <code>m</code> equals
the total number of features.</p>
</td></tr>
<tr><td><code id="get_S_causal_steps_+3A_causal_ordering">causal_ordering</code></td>
<td>
<p>List.
Not applicable for (regular) non-causal or asymmetric explanations.
<code>causal_ordering</code> is an unnamed list of vectors specifying the components of the
partial causal ordering that the coalitions must respect. Each vector represents
a component and contains one or more features/groups identified by their names
(strings) or indices (integers). If <code>causal_ordering</code> is <code>NULL</code> (default), no causal
ordering is assumed and all possible coalitions are allowed. No causal ordering is
equivalent to a causal ordering with a single component that includes all features
(<code>list(1:n_features)</code>) or groups (<code>list(1:n_groups)</code>) for feature-wise and group-wise
Shapley values, respectively. For feature-wise Shapley values and
<code>causal_ordering = list(c(1, 2), c(3, 4))</code>, the interpretation is that features 1 and 2
are the ancestors of features 3 and 4, while features 3 and 4 are on the same level.
Note: All features/groups must be included in the <code>causal_ordering</code> without any duplicates.</p>
</td></tr>
<tr><td><code id="get_S_causal_steps_+3A_confounding">confounding</code></td>
<td>
<p>Logical vector.
Not applicable for (regular) non-causal or asymmetric explanations.
<code>confounding</code> is a vector of logicals specifying whether confounding is assumed or not for each component in the
<code>causal_ordering</code>. If <code>NULL</code> (default), then no assumption about the confounding structure is made and <code>explain</code>
computes asymmetric/symmetric conditional Shapley values, depending on the value of <code>asymmetric</code>.
If <code>confounding</code> is a single logical, i.e., <code>FALSE</code> or <code>TRUE</code>, then this assumption is set globally
for all components in the causal ordering. Otherwise, <code>confounding</code> must be a vector of logicals of the same
length as <code>causal_ordering</code>, indicating the confounding assumption for each component. When <code>confounding</code> is
specified, then <code>explain</code> computes asymmetric/symmetric causal Shapley values, depending on the value of
<code>asymmetric</code>. The <code>approach</code> cannot be <code>regression_separate</code> and <code>regression_surrogate</code> as the
regression-based approaches are not applicable to the causal Shapley value methodology.</p>
</td></tr>
<tr><td><code id="get_S_causal_steps_+3A_as_string">as_string</code></td>
<td>
<p>Boolean.
If the returned object is to be a list of lists of integers or a list of vectors of strings.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Depends on the value of the parameter <code>as_string</code>. If a string, then <code>results[j]</code> is a vector specifying
the process of generating the samples for coalition <code>j</code>. The length of <code>results[j]</code> is the number of steps, and
<code>results[j][i]</code> is a string of the form <code>features_to_sample|features_to_condition_on</code>. If the
<code>features_to_condition_on</code> part is blank, then we are to sample from the marginal distribution.
For <code>as_string == FALSE</code>, then we rather return a vector where <code>results[[j]][[i]]</code> contains the elements
<code>Sbar</code> and <code>S</code> representing the features to sample and condition on, respectively.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
m &lt;- 5
causal_ordering &lt;- list(1:2, 3:4, 5)
S &lt;- shapr::feature_matrix_cpp(get_valid_causal_coalitions(causal_ordering = causal_ordering),
  m = m
)
confounding &lt;- c(TRUE, TRUE, FALSE)
get_S_causal_steps(S, causal_ordering, confounding, as_string = TRUE)

# Look at the effect of changing the confounding assumptions
SS1 &lt;- get_S_causal_steps(S, causal_ordering,
  confounding = c(FALSE, FALSE, FALSE),
  as_string = TRUE
)
SS2 &lt;- get_S_causal_steps(S, causal_ordering, confounding = c(TRUE, FALSE, FALSE), as_string = TRUE)
SS3 &lt;- get_S_causal_steps(S, causal_ordering, confounding = c(TRUE, TRUE, FALSE), as_string = TRUE)
SS4 &lt;- get_S_causal_steps(S, causal_ordering, confounding = c(TRUE, TRUE, TRUE), as_string = TRUE)

all.equal(SS1, SS2)
SS1[[2]] # Condition on 1 as there is no confounding in the first component
SS2[[2]] # Do NOT condition on 1 as there is confounding in the first component
SS1[[3]]
SS2[[3]]

all.equal(SS1, SS3)
SS1[[2]] # Condition on 1 as there is no confounding in the first component
SS3[[2]] # Do NOT condition on 1 as there is confounding in the first component
SS1[[5]] # Condition on 3 as there is no confounding in the second component
SS3[[5]] # Do NOT condition on 3 as there is confounding in the second component
SS1[[6]]
SS3[[6]]

all.equal(SS2, SS3)
SS2[[5]]
SS3[[5]]
SS2[[6]]
SS3[[6]]

all.equal(SS3, SS4) # No difference as the last component is a singleton

## End(Not run)
</code></pre>

<hr>
<h2 id='get_supported_approaches'>Gets the implemented approaches</h2><span id='topic+get_supported_approaches'></span>

<h3>Description</h3>

<p>Gets the implemented approaches
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_supported_approaches()
</code></pre>


<h3>Value</h3>

<p>Character vector.
The names of the implemented approaches that can be passed to argument <code>approach</code> in <code><a href="#topic+explain">explain()</a></code>.
</p>

<hr>
<h2 id='get_supported_models'>Provides a data.table with the supported models</h2><span id='topic+get_supported_models'></span>

<h3>Description</h3>

<p>Provides a data.table with the supported models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_supported_models()
</code></pre>

<hr>
<h2 id='get_valid_causal_coalitions'>Get all coalitions satisfying the causal ordering</h2><span id='topic+get_valid_causal_coalitions'></span>

<h3>Description</h3>

<p>This function is only relevant when we are computing asymmetric Shapley values.
For symmetric Shapley values (both regular and causal), all coalitions are allowed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_valid_causal_coalitions(
  causal_ordering,
  sort_features_in_coalitions = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_valid_causal_coalitions_+3A_causal_ordering">causal_ordering</code></td>
<td>
<p>List.
Not applicable for (regular) non-causal or asymmetric explanations.
<code>causal_ordering</code> is an unnamed list of vectors specifying the components of the
partial causal ordering that the coalitions must respect. Each vector represents
a component and contains one or more features/groups identified by their names
(strings) or indices (integers). If <code>causal_ordering</code> is <code>NULL</code> (default), no causal
ordering is assumed and all possible coalitions are allowed. No causal ordering is
equivalent to a causal ordering with a single component that includes all features
(<code>list(1:n_features)</code>) or groups (<code>list(1:n_groups)</code>) for feature-wise and group-wise
Shapley values, respectively. For feature-wise Shapley values and
<code>causal_ordering = list(c(1, 2), c(3, 4))</code>, the interpretation is that features 1 and 2
are the ancestors of features 3 and 4, while features 3 and 4 are on the same level.
Note: All features/groups must be included in the <code>causal_ordering</code> without any duplicates.</p>
</td></tr>
<tr><td><code id="get_valid_causal_coalitions_+3A_sort_features_in_coalitions">sort_features_in_coalitions</code></td>
<td>
<p>Boolean. If <code>TRUE</code>, then the feature indices in the
coalitions are sorted in increasing order. If <code>FALSE</code>, then the function maintains the
order of features within each group given in <code>causal_ordering</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of vectors containing all coalitions that respects the causal ordering.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='group_forecast_setup'>Set up user provided groups for explanation in a forecast model.</h2><span id='topic+group_forecast_setup'></span>

<h3>Description</h3>

<p>Set up user provided groups for explanation in a forecast model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group_forecast_setup(group, horizon_features)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="group_forecast_setup_+3A_group">group</code></td>
<td>
<p>The list of groups to be explained.</p>
</td></tr>
<tr><td><code id="group_forecast_setup_+3A_horizon_features">horizon_features</code></td>
<td>
<p>A list of features per horizon, to split appropriate groups over.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing
</p>

<ul>
<li><p> group The list group with entries that differ per horizon split accordingly.
</p>
</li>
<li><p> horizon_group A list of which groups are applicable per horizon.
</p>
</li></ul>


<hr>
<h2 id='hat_matrix_cpp'>Computing single H matrix in AICc-function using the Mahalanobis distance</h2><span id='topic+hat_matrix_cpp'></span>

<h3>Description</h3>

<p>Computing single H matrix in AICc-function using the Mahalanobis distance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hat_matrix_cpp(X, mcov, S_scale_dist, h)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hat_matrix_cpp_+3A_x">X</code></td>
<td>
<p>matrix.</p>
</td></tr>
<tr><td><code id="hat_matrix_cpp_+3A_mcov">mcov</code></td>
<td>
<p>matrix
The covariance matrix of X.</p>
</td></tr>
<tr><td><code id="hat_matrix_cpp_+3A_s_scale_dist">S_scale_dist</code></td>
<td>
<p>logical.
Indicating whether the Mahalanobis distance should be scaled with the number of variables</p>
</td></tr>
<tr><td><code id="hat_matrix_cpp_+3A_h">h</code></td>
<td>
<p>numeric specifying the scaling (sigma)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix of dimension <code>ncol(X)*ncol(X)</code>
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='inv_gaussian_transform_cpp'>Transforms new data to a standardized normal distribution</h2><span id='topic+inv_gaussian_transform_cpp'></span>

<h3>Description</h3>

<p>Transforms new data to a standardized normal distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inv_gaussian_transform_cpp(z, x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="inv_gaussian_transform_cpp_+3A_z">z</code></td>
<td>
<p>arma::mat.
The data are the Gaussian Monte Carlos samples to transform.</p>
</td></tr>
<tr><td><code id="inv_gaussian_transform_cpp_+3A_x">x</code></td>
<td>
<p>arma::mat.
The data with the original transformation. Used to conduct the transformation of <code>z</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>arma::mat of the same dimension as <code>z</code>
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='lag_data'>Lag a matrix of variables a specific number of lags for each variables.</h2><span id='topic+lag_data'></span>

<h3>Description</h3>

<p>Lag a matrix of variables a specific number of lags for each variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lag_data(x, lags)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lag_data_+3A_x">x</code></td>
<td>
<p>The matrix of variables (one variable per column).</p>
</td></tr>
<tr><td><code id="lag_data_+3A_lags">lags</code></td>
<td>
<p>A numeric vector denoting how many lags each variable should have.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two items
</p>

<ul>
<li><p> A matrix, lagged with the lagged data.
</p>
</li>
<li><p> A list, group, with groupings of the lagged data per variable.
</p>
</li></ul>


<hr>
<h2 id='mahalanobis_distance_cpp'>(Generalized) Mahalanobis distance</h2><span id='topic+mahalanobis_distance_cpp'></span>

<h3>Description</h3>

<p>Used to get the Euclidean distance as well by setting <code>mcov</code> = <code>diag(m)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mahalanobis_distance_cpp(
  featureList,
  Xtrain_mat,
  Xexplain_mat,
  mcov,
  S_scale_dist
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mahalanobis_distance_cpp_+3A_featurelist">featureList</code></td>
<td>
<p>List.
Contains the vectors indicating all factor combinations that should be included in the computations.
Assumes that the first one is empty.</p>
</td></tr>
<tr><td><code id="mahalanobis_distance_cpp_+3A_xtrain_mat">Xtrain_mat</code></td>
<td>
<p>Matrix
Training data in matrix form</p>
</td></tr>
<tr><td><code id="mahalanobis_distance_cpp_+3A_xexplain_mat">Xexplain_mat</code></td>
<td>
<p>Matrix
Explanation data in matrix form.</p>
</td></tr>
<tr><td><code id="mahalanobis_distance_cpp_+3A_mcov">mcov</code></td>
<td>
<p>matrix
The covariance matrix of X.</p>
</td></tr>
<tr><td><code id="mahalanobis_distance_cpp_+3A_s_scale_dist">S_scale_dist</code></td>
<td>
<p>logical.
Indicating whether the Mahalanobis distance should be scaled with the number of variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Array of three dimensions. Contains the squared distance for between all training and test observations for all feature combinations passed to the function.
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='mcar_mask_generator'>Missing Completely at Random (MCAR) Mask Generator</h2><span id='topic+mcar_mask_generator'></span>

<h3>Description</h3>

<p>A mask generator which masks the entries in the input completely at random.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcar_mask_generator(masking_ratio = 0.5, paired_sampling = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mcar_mask_generator_+3A_masking_ratio">masking_ratio</code></td>
<td>
<p>Numeric between 0 and 1. The probability for an entry in the generated mask to be 1 (masked).</p>
</td></tr>
<tr><td><code id="mcar_mask_generator_+3A_paired_sampling">paired_sampling</code></td>
<td>
<p>Boolean. If we are doing paired sampling. So include both S and <code class="reqn">\bar{S}</code>.
If <code>TRUE</code>, then <code>batch</code> must be sampled using <code><a href="#topic+paired_sampler">paired_sampler()</a></code> which ensures that the <code>batch</code> contains
two instances for each original observation. That is, <code>batch</code> <code class="reqn">= [X_1, X_1, X_2, X_2, X_3, X_3, ...]</code>, where
each entry <code class="reqn">X_j</code> is a row of dimension <code class="reqn">p</code> (i.e., the number of features).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The mask generator mask each element in the <code>batch</code> (N x p) using a component-wise independent Bernoulli
distribution with probability <code>masking_ratio</code>. Default values for <code>masking_ratio</code> is 0.5, so all
masks are equally likely to be generated, including the empty and full masks.
The function returns a mask of the same shape as the input <code>batch</code>, and the <code>batch</code> can contain
missing values, indicated by the &quot;NaN&quot; token, which will always be masked.
</p>


<h3>Shape</h3>


<ul>
<li><p> Input: <code class="reqn">(N, p)</code> where N is the number of observations in the <code>batch</code> and <code class="reqn">p</code> is the number of features.
</p>
</li>
<li><p> Output: <code class="reqn">(N, p)</code>, same shape as the input
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
mask_gen &lt;- mcar_mask_generator(masking_ratio = 0.5, paired_sampling = FALSE)
batch &lt;- torch::torch_randn(c(5, 3))
mask_gen(batch)

## End(Not run)

</code></pre>

<hr>
<h2 id='memory_layer'>A <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> Representing a Memory Layer</h2><span id='topic+memory_layer'></span>

<h3>Description</h3>

<p>The layer is used to make skip-connections inside a <code><a href="torch.html#topic+nn_sequential">torch::nn_sequential()</a></code> network
or between several <code><a href="torch.html#topic+nn_sequential">torch::nn_sequential()</a></code> networks without unnecessary code complication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>memory_layer(id, shared_env, output = FALSE, add = FALSE, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="memory_layer_+3A_id">id</code></td>
<td>
<p>A unique id to use as a key in the storage list.</p>
</td></tr>
<tr><td><code id="memory_layer_+3A_shared_env">shared_env</code></td>
<td>
<p>A shared environment for all instances of memory_layer where the inputs are stored.</p>
</td></tr>
<tr><td><code id="memory_layer_+3A_output">output</code></td>
<td>
<p>Boolean variable indicating if the memory layer is to store input in storage or extract from storage.</p>
</td></tr>
<tr><td><code id="memory_layer_+3A_add">add</code></td>
<td>
<p>Boolean variable indicating if the extracted value are to be added or concatenated to the input.
Only applicable when <code>output = TRUE</code>.</p>
</td></tr>
<tr><td><code id="memory_layer_+3A_verbose">verbose</code></td>
<td>
<p>Boolean variable indicating if we want to give printouts to the user.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>output = FALSE</code>, this layer stores its input in the <code>shared_env</code> with the key <code>id</code> and then
passes the input to the next layer. I.e., when memory layer is used in the masked encoder. If <code>output = TRUE</code>, this
layer takes stored tensor from the storage. I.e., when memory layer is used in the decoder. If <code>add = TRUE</code>, it
returns sum of the stored vector and an <code>input</code>, otherwise it returns their concatenation. If the tensor with
specified <code>id</code> is not in storage when the layer with <code>output = TRUE</code> is called, it would cause an exception.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
memory_layer_env &lt;- new.env()
net1 &lt;- torch::nn_sequential(
  memory_layer("#1", shared_env = memory_layer_env),
  memory_layer("#0.1", shared_env = memory_layer_env),
  torch::nn_linear(512, 256),
  torch::nn_leaky_relu(), # Here add cannot be TRUE because the dimensions mismatch
  memory_layer("#0.1", shared_env = memory_layer_env, output = TRUE, add = FALSE),
  torch::nn_linear(768, 256),
  # the dimension after the concatenation with skip-connection is 512 + 256 = 768
)
net2 &lt;- torch::nn_equential(
  torch::nn_linear(512, 512),
  memory_layer("#1", shared_env = memory_layer_env, output = TRUE, add = TRUE),
  ...
)
# Here a and c must be of correct dimensions, e.g., a = torch::torch_ones(1,512).
b &lt;- net1(a)
d &lt;- net2(c) # net2 must be called after net1, otherwise tensor '#1' will not be in storage.

## End(Not run)
</code></pre>

<hr>
<h2 id='model_checker'>Check that the type of model is supported by the native implementation of the model class</h2><span id='topic+model_checker'></span><span id='topic+model_checker.default'></span><span id='topic+model_checker.ar'></span><span id='topic+model_checker.Arima'></span><span id='topic+model_checker.forecast_ARIMA'></span><span id='topic+model_checker.glm'></span><span id='topic+model_checker.lm'></span><span id='topic+model_checker.gam'></span><span id='topic+model_checker.ranger'></span><span id='topic+model_checker.workflow'></span><span id='topic+model_checker.xgb.Booster'></span>

<h3>Description</h3>

<p>The function checks whether the model given by <code>x</code> is supported.
If <code>x</code> is not a supported model the function will return an error message, otherwise it return NULL
(meaning all types of models with this class is supported)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_checker(x)

## Default S3 method:
model_checker(x)

## S3 method for class 'ar'
model_checker(x)

## S3 method for class 'Arima'
model_checker(x)

## S3 method for class 'forecast_ARIMA'
model_checker(x)

## S3 method for class 'glm'
model_checker(x)

## S3 method for class 'lm'
model_checker(x)

## S3 method for class 'gam'
model_checker(x)

## S3 method for class 'ranger'
model_checker(x)

## S3 method for class 'workflow'
model_checker(x)

## S3 method for class 'xgb.Booster'
model_checker(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="model_checker_+3A_x">x</code></td>
<td>
<p>Model object for the model to be explained.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Error or NULL
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+predict_model">predict_model()</a></code> for more information about what type of models <code>shapr</code> currently support.
</p>

<hr>
<h2 id='observation_impute'>Generate permutations of training data using test observations</h2><span id='topic+observation_impute'></span>

<h3>Description</h3>

<p>Generate permutations of training data using test observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>observation_impute(
  W_kernel,
  S,
  x_train,
  x_explain,
  empirical.eta = 0.7,
  n_MC_samples = 1000
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="observation_impute_+3A_w_kernel">W_kernel</code></td>
<td>
<p>Numeric matrix. Contains all nonscaled weights between training and test
observations for all coalitions. The dimension equals <code style="white-space: pre;">&#8288;n_train x m&#8288;</code>.</p>
</td></tr>
<tr><td><code id="observation_impute_+3A_s">S</code></td>
<td>
<p>Integer matrix of dimension <code style="white-space: pre;">&#8288;n_coalitions x m&#8288;</code>, where <code>n_coalitions</code>
and <code>m</code> equals the total number of sampled/non-sampled coalitions and
the total number of unique features, respectively. Note that <code>m = ncol(x_train)</code>.</p>
</td></tr>
<tr><td><code id="observation_impute_+3A_x_train">x_train</code></td>
<td>
<p>Data.table with training data.</p>
</td></tr>
<tr><td><code id="observation_impute_+3A_x_explain">x_explain</code></td>
<td>
<p>Data.table with the features of the observation whose
predictions ought to be explained (test data).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>

<hr>
<h2 id='observation_impute_cpp'>Get imputed data</h2><span id='topic+observation_impute_cpp'></span>

<h3>Description</h3>

<p>Get imputed data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>observation_impute_cpp(index_xtrain, index_s, x_train, x_explain, S)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="observation_impute_cpp_+3A_index_xtrain">index_xtrain</code></td>
<td>
<p>Positive integer. Represents a sequence of row indices from <code>x_train</code>,
i.e. <code>min(index_xtrain) &gt;= 1</code> and <code>max(index_xtrain) &lt;= nrow(x_train)</code>.</p>
</td></tr>
<tr><td><code id="observation_impute_cpp_+3A_index_s">index_s</code></td>
<td>
<p>Positive integer. Represents a sequence of row indices from <code>S</code>,
i.e. <code>min(index_s) &gt;= 1</code> and <code>max(index_s) &lt;= nrow(S)</code>.</p>
</td></tr>
<tr><td><code id="observation_impute_cpp_+3A_x_train">x_train</code></td>
<td>
<p>Matrix.
Contains the training data.</p>
</td></tr>
<tr><td><code id="observation_impute_cpp_+3A_x_explain">x_explain</code></td>
<td>
<p>Matrix with 1 row.
Contains the features of the observation for a single prediction.</p>
</td></tr>
<tr><td><code id="observation_impute_cpp_+3A_s">S</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_coalitions</code>, <code>n_features</code>) containing binary representations of the used coalitions.
S cannot contain the empty or grand coalition, i.e., a row containing only zeros or ones.
This is not a problem internally in shapr as the empty and grand coalitions are treated differently.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>S(i, j) = 1</code> if and only if feature <code>j</code> is present in feature
combination <code>i</code>, otherwise <code>S(i, j) = 0</code>. I.e. if <code>m = 3</code>, there
are <code>2^3 = 8</code> unique ways to combine the features. In this case <code>dim(S) = c(8, 3)</code>.
Let's call the features <code>x1, x2, x3</code> and take a closer look at the combination
represented by <code>s = c(x1, x2)</code>. If this combination is represented by the second row,
the following is true: <code>S[2, 1:3] = c(1, 1, 0)</code>.
</p>
<p>The returned object, <code>X</code>, is a numeric matrix where
<code>dim(X) = c(length(index_xtrain), ncol(x_train))</code>. If feature <code>j</code> is present in
the k-th observation, that is <code>S[index_[k], j] == 1</code>, <code>X[k, j] = x_explain[1, j]</code>.
Otherwise <code>X[k, j] = x_train[index_xtrain[k], j]</code>.
</p>


<h3>Value</h3>

<p>Numeric matrix
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>

<hr>
<h2 id='paired_sampler'>Sampling Paired Observations</h2><span id='topic+paired_sampler'></span>

<h3>Description</h3>

<p>A sampler used to samples the batches where each instances is sampled twice
</p>


<h3>Usage</h3>

<pre><code class='language-R'>paired_sampler(vaeac_dataset_object, shuffle = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="paired_sampler_+3A_vaeac_dataset_object">vaeac_dataset_object</code></td>
<td>
<p>A <code><a href="#topic+vaeac_dataset">vaeac_dataset()</a></code> object containing the data.</p>
</td></tr>
<tr><td><code id="paired_sampler_+3A_shuffle">shuffle</code></td>
<td>
<p>Boolean. If <code>TRUE</code>, then the data is shuffled. If <code>FALSE</code>,
then the data is returned in chronological order.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A sampler object that allows for paired sampling by always including each observation from the
<code><a href="#topic+vaeac_dataset">vaeac_dataset()</a></code> twice. A <code><a href="torch.html#topic+sampler">torch::sampler()</a></code> object can be used with <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code> when creating
batches from a torch dataset <code><a href="torch.html#topic+dataset">torch::dataset()</a></code>. See <a href="https://rdrr.io/cran/torch/src/R/utils-data-sampler.R">https://rdrr.io/cran/torch/src/R/utils-data-sampler.R</a> for
more information. This function does not use batch iterators, which might increase the speed.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Example how to use it combined with mask generators with paired sampling activated
batch_size &lt;- 4
if (batch_size %% 2 == 1) batch_size &lt;- batch_size - 1 # Make sure that batch size is even
n_features &lt;- 3
n_observations &lt;- 5
shuffle &lt;- TRUE
data &lt;- torch_tensor(matrix(rep(seq(n_observations), each = n_features),
  ncol = n_features, byrow = TRUE
))
data
dataset &lt;- vaeac_dataset(data, rep(1, n_features))
dataload &lt;- torch::dataloader(dataset,
  batch_size = batch_size,
  sampler = paired_sampler(dataset,
    shuffle = shuffle
  )
)
dataload$.length() # Number of batches, same as ceiling((2 * n_observations) / batch_size)
mask_generator &lt;- mcar_mask_generator(paired = TRUE)
coro::loop(for (batch in dataload) {
  mask &lt;- mask_generator(batch)
  obs &lt;- mask * batch
  print(torch::torch_cat(c(batch, mask, obs), -1))
})

## End(Not run)
</code></pre>

<hr>
<h2 id='plot_MSEv_eval_crit'>Plots of the MSEv Evaluation Criterion</h2><span id='topic+plot_MSEv_eval_crit'></span>

<h3>Description</h3>

<p>Make plots to visualize and compare the MSEv evaluation criterion for a list of
<code><a href="#topic+explain">explain()</a></code> objects applied to the same data and model. The function creates
bar plots and line plots with points to illustrate the overall MSEv evaluation
criterion, but also for each observation/explicand and coalition by only averaging over
the coalitions and observations/explicands, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_MSEv_eval_crit(
  explanation_list,
  index_x_explain = NULL,
  id_coalition = NULL,
  CI_level = if (length(explanation_list[[1]]$pred_explain) &lt; 20) NULL else 0.95,
  geom_col_width = 0.9,
  plot_type = "overall"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_MSEv_eval_crit_+3A_explanation_list">explanation_list</code></td>
<td>
<p>A list of <code><a href="#topic+explain">explain()</a></code> objects applied to the same data and model.
If the entries in the list are named, then the function use these names. Otherwise, they default to
the approach names (with integer suffix for duplicates) for the explanation objects in <code>explanation_list</code>.</p>
</td></tr>
<tr><td><code id="plot_MSEv_eval_crit_+3A_index_x_explain">index_x_explain</code></td>
<td>
<p>Integer vector.
Which of the test observations to plot. E.g. if you have
explained 10 observations using <code><a href="#topic+explain">explain()</a></code>, you can generate a plot for the first 5
observations by setting <code>index_x_explain = 1:5</code>.</p>
</td></tr>
<tr><td><code id="plot_MSEv_eval_crit_+3A_id_coalition">id_coalition</code></td>
<td>
<p>Integer vector. Which of the coalitions to plot.
E.g. if you used <code>n_coalitions = 16</code> in <code><a href="#topic+explain">explain()</a></code>, you can generate a plot for the
first 5 coalitions and the 10th by setting <code>id_coalition = c(1:5, 10)</code>.</p>
</td></tr>
<tr><td><code id="plot_MSEv_eval_crit_+3A_ci_level">CI_level</code></td>
<td>
<p>Positive numeric between zero and one. Default is <code>0.95</code> if the number of observations to explain is
larger than 20, otherwise <code>CI_level = NULL</code>, which removes the confidence intervals. The level of the approximate
confidence intervals for the overall MSEv and the MSEv_coalition. The confidence intervals are based on that
the MSEv scores are means over the observations/explicands, and that means are approximation normal. Since the
standard deviations are estimated, we use the quantile t from the T distribution with N_explicands - 1 degrees of
freedom corresponding to the provided level. Here, N_explicands is the number of observations/explicands.
MSEv +/- t<em>SD(MSEv)/sqrt(N_explicands). Note that the <code>explain()</code> function already scales the standard deviation by
sqrt(N_explicands), thus, the CI are MSEv \/- t</em>MSEv_sd, where the values MSEv and MSEv_sd are extracted from the
MSEv data.tables in the objects in the <code>explanation_list</code>.</p>
</td></tr>
<tr><td><code id="plot_MSEv_eval_crit_+3A_geom_col_width">geom_col_width</code></td>
<td>
<p>Numeric. Bar width. By default, set to 90% of the <code><a href="ggplot2.html#topic+resolution">ggplot2::resolution()</a></code> of the data.</p>
</td></tr>
<tr><td><code id="plot_MSEv_eval_crit_+3A_plot_type">plot_type</code></td>
<td>
<p>Character vector. The possible options are &quot;overall&quot; (default), &quot;comb&quot;, and &quot;explicand&quot;.
If <code>plot_type = "overall"</code>, then the plot (one bar plot) associated with the overall MSEv evaluation criterion
for each method is created, i.e., when averaging over both the coalitions and observations/explicands.
If <code>plot_type = "comb"</code>, then the plots (one line plot and one bar plot) associated with the MSEv evaluation
criterion for each coalition are created, i.e., when we only average over the observations/explicands.
If <code>plot_type = "explicand"</code>, then the plots (one line plot and one bar plot) associated with the MSEv evaluation
criterion for each observations/explicands are created, i.e., when we only average over the coalitions.
If <code>plot_type</code> is a vector of one or several of &quot;overall&quot;, &quot;comb&quot;, and &quot;explicand&quot;, then the associated plots are
created.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either a single <code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot()</a></code> object of the MSEv criterion when <code>plot_type = "overall"</code>, or a list
of <code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot()</a></code> objects based on the <code>plot_type</code> parameter.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Load necessary libraries
library(xgboost)
library(data.table)
library(shapr)
library(ggplot2)

# Get the data
data("airquality")
data &lt;- data.table::as.data.table(airquality)
data &lt;- data[complete.cases(data), ]

#' Define the features and the response
x_var &lt;- c("Solar.R", "Wind", "Temp", "Month")
y_var &lt;- "Ozone"

# Split data into test and training data set
ind_x_explain &lt;- 1:25
x_train &lt;- data[-ind_x_explain, ..x_var]
y_train &lt;- data[-ind_x_explain, get(y_var)]
x_explain &lt;- data[ind_x_explain, ..x_var]

# Fitting a basic xgboost model to the training data
model &lt;- xgboost::xgboost(
  data = as.matrix(x_train),
  label = y_train,
  nround = 20,
  verbose = FALSE
)

# Specifying the phi_0, i.e. the expected prediction without any features
phi0 &lt;- mean(y_train)

# Independence approach
explanation_independence &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "independence",
  phi0 = phi0,
  n_MC_samples = 1e2
)

# Gaussian 1e1 approach
explanation_gaussian_1e1 &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "gaussian",
  phi0 = phi0,
  n_MC_samples = 1e1
)

# Gaussian 1e2 approach
explanation_gaussian_1e2 &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "gaussian",
  phi0 = phi0,
  n_MC_samples = 1e2
)

# ctree approach
explanation_ctree &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "ctree",
  phi0 = phi0,
  n_MC_samples = 1e2
)

# Combined approach
explanation_combined &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = c("gaussian", "independence", "ctree"),
  phi0 = phi0,
  n_MC_samples = 1e2
)

# Create a list of explanations with names
explanation_list_named &lt;- list(
  "Ind." = explanation_independence,
  "Gaus. 1e1" = explanation_gaussian_1e1,
  "Gaus. 1e2" = explanation_gaussian_1e2,
  "Ctree" = explanation_ctree,
  "Combined" = explanation_combined
)

if (requireNamespace("ggplot2", quietly = TRUE)) {
  # Create the default MSEv plot where we average over both the coalitions and observations
  # with approximate 95% confidence intervals
  plot_MSEv_eval_crit(explanation_list_named, CI_level = 0.95, plot_type = "overall")

  # Can also create plots of the MSEv criterion averaged only over the coalitions or observations.
  MSEv_figures &lt;- plot_MSEv_eval_crit(explanation_list_named,
    CI_level = 0.95,
    plot_type = c("overall", "comb", "explicand")
  )
  MSEv_figures$MSEv_bar
  MSEv_figures$MSEv_coalition_bar
  MSEv_figures$MSEv_explicand_bar

  # When there are many coalitions or observations, then it can be easier to look at line plots
  MSEv_figures$MSEv_coalition_line_point
  MSEv_figures$MSEv_explicand_line_point

  # We can specify which observations or coalitions to plot
  plot_MSEv_eval_crit(explanation_list_named,
    plot_type = "explicand",
    index_x_explain = c(1, 3:4, 6),
    CI_level = 0.95
  )$MSEv_explicand_bar
  plot_MSEv_eval_crit(explanation_list_named,
    plot_type = "comb",
    id_coalition = c(3, 4, 9, 13:15),
    CI_level = 0.95
  )$MSEv_coalition_bar

  # We can alter the figures if other palette schemes or design is wanted
  bar_text_n_decimals &lt;- 1
  MSEv_figures$MSEv_bar +
    ggplot2::scale_x_discrete(limits = rev(levels(MSEv_figures$MSEv_bar$data$Method))) +
    ggplot2::coord_flip() +
    ggplot2::scale_fill_discrete() + #' Default ggplot2 palette
    ggplot2::theme_minimal() + #' This must be set before the other theme call
    ggplot2::theme(
      plot.title = ggplot2::element_text(size = 10),
      legend.position = "bottom"
    ) +
    ggplot2::guides(fill = ggplot2::guide_legend(nrow = 1, ncol = 6)) +
    ggplot2::geom_text(
      ggplot2::aes(label = sprintf(
        paste("%.", sprintf("%d", bar_text_n_decimals), "f", sep = ""),
        round(MSEv, bar_text_n_decimals)
      )),
      vjust = -1.1, # This value must be altered based on the plot dimension
      hjust = 1.1, # This value must be altered based on the plot dimension
      color = "black",
      position = ggplot2::position_dodge(0.9),
      size = 5
    )
}

## End(Not run)

</code></pre>

<hr>
<h2 id='plot_SV_several_approaches'>Shapley value bar plots for several explanation objects</h2><span id='topic+plot_SV_several_approaches'></span>

<h3>Description</h3>

<p>Make plots to visualize and compare the estimated Shapley values for a list of
<code><a href="#topic+explain">explain()</a></code> objects applied to the same data and model. For group-wise Shapley values,
the features values plotted are the mean feature values for all features in each group.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_SV_several_approaches(
  explanation_list,
  index_explicands = NULL,
  index_explicands_sort = FALSE,
  only_these_features = NULL,
  plot_phi0 = FALSE,
  digits = 4,
  add_zero_line = FALSE,
  axis_labels_n_dodge = NULL,
  axis_labels_rotate_angle = NULL,
  horizontal_bars = TRUE,
  facet_scales = "free",
  facet_ncol = 2,
  geom_col_width = 0.85,
  brewer_palette = NULL,
  include_group_feature_means = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_SV_several_approaches_+3A_explanation_list">explanation_list</code></td>
<td>
<p>A list of <code><a href="#topic+explain">explain()</a></code> objects applied to the same data and model.
If the entries in the list are named, then the function use these names. Otherwise, they default to
the approach names (with integer suffix for duplicates) for the explanation objects in <code>explanation_list</code>.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_index_explicands">index_explicands</code></td>
<td>
<p>Integer vector. Which of the explicands (test observations) to plot.
E.g. if you have explained 10 observations using <code><a href="#topic+explain">explain()</a></code>, you can generate a plot for the
first 5 observations/explicands and the 10th by setting <code>index_x_explain = c(1:5, 10)</code>.
The argument <code>index_explicands_sort</code> must be <code>FALSE</code> to plot the explicand
in the order specified in <code>index_x_explain</code>.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_index_explicands_sort">index_explicands_sort</code></td>
<td>
<p>Boolean. If <code>FALSE</code> (default), then <code>shapr</code> plots the explicands in the order
specified in <code>index_explicands</code>. If <code>TRUE</code>, then <code>shapr</code> sort the indices in increasing order based on their id.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_only_these_features">only_these_features</code></td>
<td>
<p>String vector. Containing the names of the features which
are to be included in the bar plots.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_plot_phi0">plot_phi0</code></td>
<td>
<p>Boolean. If we are to include the <code class="reqn">\phi_0</code> in the bar plots or not.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_digits">digits</code></td>
<td>
<p>Integer.
Number of significant digits to use in the feature description.
Applicable for <code>plot_type</code> <code>"bar"</code> and <code>"waterfall"</code></p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_add_zero_line">add_zero_line</code></td>
<td>
<p>Boolean. If we are to add a black line for a feature contribution of 0.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_axis_labels_n_dodge">axis_labels_n_dodge</code></td>
<td>
<p>Integer. The number of rows that
should be used to render the labels. This is useful for displaying labels that would otherwise overlap.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_axis_labels_rotate_angle">axis_labels_rotate_angle</code></td>
<td>
<p>Numeric. The angle of the axis label, where 0 means horizontal, 45 means tilted,
and 90 means vertical. Compared to setting the angle in<code><a href="ggplot2.html#topic+theme">ggplot2::theme()</a></code> / <code><a href="ggplot2.html#topic+element">ggplot2::element_text()</a></code>, this also
uses some heuristics to automatically pick the <code>hjust</code> and <code>vjust</code> that you probably want.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_horizontal_bars">horizontal_bars</code></td>
<td>
<p>Boolean. Flip Cartesian coordinates so that horizontal becomes vertical,
and vertical, horizontal. This is primarily useful for converting geoms and statistics which display
y conditional on x, to x conditional on y. See <code><a href="ggplot2.html#topic+coord_flip">ggplot2::coord_flip()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_facet_scales">facet_scales</code></td>
<td>
<p>Should scales be free (&quot;<code>free</code>&quot;, the default), fixed (&quot;<code>fixed</code>&quot;), or free in one dimension
(&quot;<code>free_x</code>&quot;, &quot;<code>free_y</code>&quot;)? The user has to change the latter manually depending on the value of <code>horizontal_bars</code>.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_facet_ncol">facet_ncol</code></td>
<td>
<p>Integer. The number of columns in the facet grid. Default is <code>facet_ncol = 2</code>.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_geom_col_width">geom_col_width</code></td>
<td>
<p>Numeric. Bar width. By default, set to 85% of the <code><a href="ggplot2.html#topic+resolution">ggplot2::resolution()</a></code> of the data.</p>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_brewer_palette">brewer_palette</code></td>
<td>
<p>String. Name of one of the color palettes from <code><a href="RColorBrewer.html#topic+ColorBrewer">RColorBrewer::RColorBrewer()</a></code>.
If <code>NULL</code>, then the function uses the default <code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot()</a></code> color scheme.
The following palettes are available for use with these scales:
</p>

<dl>
<dt>Diverging</dt><dd><p>BrBG, PiYG, PRGn, PuOr, RdBu, RdGy, RdYlBu, RdYlGn, Spectral</p>
</dd>
<dt>Qualitative</dt><dd><p>Accent, Dark2, Paired, Pastel1, Pastel2, Set1, Set2, Set3</p>
</dd>
<dt>Sequential</dt><dd><p>Blues, BuGn, BuPu, GnBu, Greens, Greys, Oranges,
OrRd, PuBu, PuBuGn, PuRd, Purples, RdPu, Reds, YlGn, YlGnBu, YlOrBr, YlOrRd</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="plot_SV_several_approaches_+3A_include_group_feature_means">include_group_feature_means</code></td>
<td>
<p>Logical. Whether to include the average feature value in a group on the
y-axis or not. If <code>FALSE</code> (default), then no value is shown for the groups. If <code>TRUE</code>, then <code>shapr</code> includes
the mean of the features in each group.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot()</a></code> object.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Load necessary libraries
library(xgboost)
library(data.table)

# Get the data
data("airquality")
data &lt;- data.table::as.data.table(airquality)
data &lt;- data[complete.cases(data), ]

# Define the features and the response
x_var &lt;- c("Solar.R", "Wind", "Temp", "Month")
y_var &lt;- "Ozone"

# Split data into test and training data set
ind_x_explain &lt;- 1:12
x_train &lt;- data[-ind_x_explain, ..x_var]
y_train &lt;- data[-ind_x_explain, get(y_var)]
x_explain &lt;- data[ind_x_explain, ..x_var]

# Fitting a basic xgboost model to the training data
model &lt;- xgboost::xgboost(
  data = as.matrix(x_train),
  label = y_train,
  nround = 20,
  verbose = FALSE
)

# Specifying the phi_0, i.e. the expected prediction without any features
phi0 &lt;- mean(y_train)

# Independence approach
explanation_independence &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "independence",
  phi0 = phi0,
  n_MC_samples = 1e2
)

# Empirical approach
explanation_empirical &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "empirical",
  phi0 = phi0,
  n_MC_samples = 1e2
)

# Gaussian 1e1 approach
explanation_gaussian_1e1 &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "gaussian",
  phi0 = phi0,
  n_MC_samples = 1e1
)

# Gaussian 1e2 approach
explanation_gaussian_1e2 &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "gaussian",
  phi0 = phi0,
  n_MC_samples = 1e2
)

# Combined approach
explanation_combined &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = c("gaussian", "ctree", "empirical"),
  phi0 = phi0,
  n_MC_samples = 1e2
)

# Create a list of explanations with names
explanation_list &lt;- list(
  "Ind." = explanation_independence,
  "Emp." = explanation_empirical,
  "Gaus. 1e1" = explanation_gaussian_1e1,
  "Gaus. 1e2" = explanation_gaussian_1e2,
  "Combined" = explanation_combined
)

if (requireNamespace("ggplot2", quietly = TRUE)) {
  # The function uses the provided names.
  plot_SV_several_approaches(explanation_list)

  # We can change the number of columns in the grid of plots and add other visual alterations
  plot_SV_several_approaches(explanation_list,
    facet_ncol = 3,
    facet_scales = "free_y",
    add_zero_line = TRUE,
    digits = 2,
    brewer_palette = "Paired",
    geom_col_width = 0.6
  ) +
    ggplot2::theme_minimal() +
    ggplot2::theme(legend.position = "bottom", plot.title = ggplot2::element_text(size = 0))


  # We can specify which explicands to plot to get less chaotic plots and make the bars vertical
  plot_SV_several_approaches(explanation_list,
    index_explicands = c(1:2, 5, 10),
    horizontal_bars = FALSE,
    axis_labels_rotate_angle = 45
  )

  # We can change the order of the features by specifying the
  # order using the `only_these_features` parameter.
  plot_SV_several_approaches(explanation_list,
    index_explicands = c(1:2, 5, 10),
    only_these_features = c("Temp", "Solar.R", "Month", "Wind")
  )

  # We can also remove certain features if we are not interested in them
  # or want to focus on, e.g., two features. The function will give a
  # message to if the user specifies non-valid feature names.
  plot_SV_several_approaches(explanation_list,
    index_explicands = c(1:2, 5, 10),
    only_these_features = c("Temp", "Solar.R"),
    plot_phi0 = TRUE
  )
}

## End(Not run)

</code></pre>

<hr>
<h2 id='plot_vaeac_eval_crit'>Plot the training VLB and validation IWAE for <code>vaeac</code> models</h2><span id='topic+plot_vaeac_eval_crit'></span>

<h3>Description</h3>

<p>This function makes (<code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot()</a></code>) figures of the training VLB and the validation IWAE for a list
of <code><a href="#topic+explain">explain()</a></code> objects with <code>approach = "vaeac"</code>. See <code><a href="#topic+setup_approach">setup_approach()</a></code> for more information about the
<code>vaeac</code> approach. Two figures are returned by the function. In the figure, each object in <code>explanation_list</code> gets
its own facet, while in the second figure, we plot the criteria in each facet for all objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_vaeac_eval_crit(
  explanation_list,
  plot_from_nth_epoch = 1,
  plot_every_nth_epoch = 1,
  criteria = c("VLB", "IWAE"),
  plot_type = c("method", "criterion"),
  facet_wrap_scales = "fixed",
  facet_wrap_ncol = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_vaeac_eval_crit_+3A_explanation_list">explanation_list</code></td>
<td>
<p>A list of <code><a href="#topic+explain">explain()</a></code> objects applied to the same data, model, and
<code>vaeac</code> must be the used approach. If the entries in the list is named, then the function use
these names. Otherwise, it defaults to the approach names (with integer suffix for duplicates)
for the explanation objects in <code>explanation_list</code>.</p>
</td></tr>
<tr><td><code id="plot_vaeac_eval_crit_+3A_plot_from_nth_epoch">plot_from_nth_epoch</code></td>
<td>
<p>Integer. If we are only plot the results form the nth epoch and so forth.
The first epochs can be large in absolute value and make the rest of the plot difficult to interpret.</p>
</td></tr>
<tr><td><code id="plot_vaeac_eval_crit_+3A_plot_every_nth_epoch">plot_every_nth_epoch</code></td>
<td>
<p>Integer. If we are only to plot every nth epoch. Usefully to illustrate
the overall trend, as there can be a lot of fluctuation and oscillation in the values between each epoch.</p>
</td></tr>
<tr><td><code id="plot_vaeac_eval_crit_+3A_criteria">criteria</code></td>
<td>
<p>Character vector. The possible options are &quot;VLB&quot;, &quot;IWAE&quot;, &quot;IWAE_running&quot;. Default is the first two.</p>
</td></tr>
<tr><td><code id="plot_vaeac_eval_crit_+3A_plot_type">plot_type</code></td>
<td>
<p>Character vector. The possible options are &quot;method&quot; and &quot;criterion&quot;. Default is to plot both.</p>
</td></tr>
<tr><td><code id="plot_vaeac_eval_crit_+3A_facet_wrap_scales">facet_wrap_scales</code></td>
<td>
<p>String. Should the scales be fixed (&quot;<code>fixed</code>&quot;, the default),
free (&quot;<code>free</code>&quot;), or free in one dimension (&quot;<code>free_x</code>&quot;, &quot;<code>free_y</code>&quot;).</p>
</td></tr>
<tr><td><code id="plot_vaeac_eval_crit_+3A_facet_wrap_ncol">facet_wrap_ncol</code></td>
<td>
<p>Integer. Number of columns in the facet wrap.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a> or the
<a href="https://borea17.github.io/paper_summaries/iwae/">blog post</a> for a summary of the VLB and IWAE.
</p>


<h3>Value</h3>

<p>Either a single <code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot()</a></code> object or a list of <code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot()</a></code> objects based on the
<code>plot_type</code> parameter.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">
Olsen, L. H., Glad, I. K., Jullum, M., &amp; Aas, K. (2022). Using Shapley values and variational autoencoders to
explain predictive models with dependent mixed features. Journal of machine learning research, 23(213), 1-51</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(xgboost)
library(data.table)
library(shapr)

data("airquality")
data &lt;- data.table::as.data.table(airquality)
data &lt;- data[complete.cases(data), ]

x_var &lt;- c("Solar.R", "Wind", "Temp", "Month")
y_var &lt;- "Ozone"

ind_x_explain &lt;- 1:6
x_train &lt;- data[-ind_x_explain, ..x_var]
y_train &lt;- data[-ind_x_explain, get(y_var)]
x_explain &lt;- data[ind_x_explain, ..x_var]

# Fitting a basic xgboost model to the training data
model &lt;- xgboost(data = as.matrix(x_train), label = y_train, nround = 100, verbose = FALSE)

# Specifying the phi_0, i.e. the expected prediction without any features
p0 &lt;- mean(y_train)

# Train vaeac with and without paired sampling
explanation_paired &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = approach,
  phi0 = p0,
  n_MC_samples = 1, # As we are only interested in the training of the vaeac
  vaeac.epochs = 10, # Should be higher in applications.
  vaeac.n_vaeacs_initialize = 1,
  vaeac.width = 16,
  vaeac.depth = 2,
  vaeac.extra_parameters = list(vaeac.paired_sampling = TRUE)
)

explanation_regular &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = approach,
  phi0 = p0,
  n_MC_samples = 1, # As we are only interested in the training of the vaeac
  vaeac.epochs = 10, # Should be higher in applications.
  vaeac.width = 16,
  vaeac.depth = 2,
  vaeac.n_vaeacs_initialize = 1,
  vaeac.extra_parameters = list(vaeac.paired_sampling = FALSE)
)

# Collect the explanation objects in an named list
explanation_list &lt;- list(
  "Regular sampling" = explanation_regular,
  "Paired sampling" = explanation_paired
)

# Call the function with the named list, will use the provided names
plot_vaeac_eval_crit(explanation_list = explanation_list)

# The function also works if we have only one method,
# but then one should only look at the method plot.
plot_vaeac_eval_crit(
  explanation_list = explanation_list[2],
  plot_type = "method"
)

# Can alter the plot
plot_vaeac_eval_crit(
  explanation_list = explanation_list,
  plot_from_nth_epoch = 2,
  plot_every_nth_epoch = 2,
  facet_wrap_scales = "free"
)

# If we only want the VLB
plot_vaeac_eval_crit(
  explanation_list = explanation_list,
  criteria = "VLB",
  plot_type = "criterion"
)

# If we want only want the criterion version
tmp_fig_criterion &lt;-
  plot_vaeac_eval_crit(explanation_list = explanation_list, plot_type = "criterion")

# Since tmp_fig_criterion is a ggplot2 object, we can alter it
# by, e.g,. adding points or smooths with se bands
tmp_fig_criterion + ggplot2::geom_point(shape = "circle", size = 1, ggplot2::aes(col = Method))
tmp_fig_criterion$layers[[1]] &lt;- NULL
tmp_fig_criterion + ggplot2::geom_smooth(method = "loess", formula = y ~ x, se = TRUE) +
  ggplot2::scale_color_brewer(palette = "Set1") +
  ggplot2::theme_minimal()

## End(Not run)

</code></pre>

<hr>
<h2 id='plot_vaeac_imputed_ggpairs'>Plot Pairwise Plots for Imputed and True Data</h2><span id='topic+plot_vaeac_imputed_ggpairs'></span>

<h3>Description</h3>

<p>A function that creates a matrix of plots (<code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>) from
generated imputations from the unconditioned distribution <code class="reqn">p(\boldsymbol{x})</code> estimated by
a <code>vaeac</code> model, and then compares the imputed values with data from the true distribution (if provided).
See <a href="https://www.blopig.com/blog/2019/06/a-brief-introduction-to-ggpairs/">ggpairs</a> for an
introduction to <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>, and the corresponding
<a href="https://ggobi.github.io/ggally/articles/ggally_plots.html">vignette</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_vaeac_imputed_ggpairs(
  explanation,
  which_vaeac_model = "best",
  x_true = NULL,
  add_title = TRUE,
  alpha = 0.5,
  upper_cont = c("cor", "points", "smooth", "smooth_loess", "density", "blank"),
  upper_cat = c("count", "cross", "ratio", "facetbar", "blank"),
  upper_mix = c("box", "box_no_facet", "dot", "dot_no_facet", "facethist",
    "facetdensity", "denstrip", "blank"),
  lower_cont = c("points", "smooth", "smooth_loess", "density", "cor", "blank"),
  lower_cat = c("facetbar", "ratio", "count", "cross", "blank"),
  lower_mix = c("facetdensity", "box", "box_no_facet", "dot", "dot_no_facet",
    "facethist", "denstrip", "blank"),
  diag_cont = c("densityDiag", "barDiag", "blankDiag"),
  diag_cat = c("barDiag", "blankDiag"),
  cor_method = c("pearson", "kendall", "spearman")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_explanation">explanation</code></td>
<td>
<p>Shapr list. The output list from the <code><a href="#topic+explain">explain()</a></code> function.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_which_vaeac_model">which_vaeac_model</code></td>
<td>
<p>String. Indicating which <code>vaeac</code> model to use when generating the samples.
Possible options are always <code>'best'</code>, <code>'best_running'</code>, and <code>'last'</code>. All possible options can be obtained
by calling <code>names(explanation$internal$parameters$vaeac$models)</code>.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_x_true">x_true</code></td>
<td>
<p>Data.table containing the data from the distribution that the <code>vaeac</code> model is fitted to.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_add_title">add_title</code></td>
<td>
<p>Logical. If <code>TRUE</code>, then a title is added to the plot based on the internal description
of the <code>vaeac</code> model specified in <code>which_vaeac_model</code>.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_alpha">alpha</code></td>
<td>
<p>Numeric between <code>0</code> and <code>1</code> (default is <code>0.5</code>). The degree of color transparency.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_upper_cont">upper_cont</code></td>
<td>
<p>String. Type of plot to use in upper triangle for continuous features, see <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>.
Possible options are: <code>'cor'</code> (default), <code>'points'</code>, <code>'smooth'</code>, <code>'smooth_loess'</code>, <code>'density'</code>, and <code>'blank'</code>.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_upper_cat">upper_cat</code></td>
<td>
<p>String. Type of plot to use in upper triangle for categorical features, see <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>.
Possible options are: <code>'count'</code> (default), <code>'cross'</code>, <code>'ratio'</code>, <code>'facetbar'</code>, and <code>'blank'</code>.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_upper_mix">upper_mix</code></td>
<td>
<p>String. Type of plot to use in upper triangle for mixed features, see <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>.
Possible options are: <code>'box'</code> (default), <code>'box_no_facet'</code>, <code>'dot'</code>, <code>'dot_no_facet'</code>, <code>'facethist'</code>,
<code>'facetdensity'</code>, <code>'denstrip'</code>, and <code>'blank'</code></p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_lower_cont">lower_cont</code></td>
<td>
<p>String. Type of plot to use in lower triangle for continuous features, see <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>.
Possible options are: <code>'points'</code> (default), <code>'smooth'</code>, <code>'smooth_loess'</code>, <code>'density'</code>, <code>'cor'</code>, and <code>'blank'</code>.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_lower_cat">lower_cat</code></td>
<td>
<p>String. Type of plot to use in lower triangle for categorical features, see <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>.
Possible options are: <code>'facetbar'</code> (default), <code>'ratio'</code>, <code>'count'</code>, <code>'cross'</code>, and <code>'blank'</code>.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_lower_mix">lower_mix</code></td>
<td>
<p>String. Type of plot to use in lower triangle for mixed features, see <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>.
Possible options are: <code>'facetdensity'</code> (default), <code>'box'</code>, <code>'box_no_facet'</code>, <code>'dot'</code>, <code>'dot_no_facet'</code>,
<code>'facethist'</code>, <code>'denstrip'</code>, and <code>'blank'</code>.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_diag_cont">diag_cont</code></td>
<td>
<p>String. Type of plot to use on the diagonal for continuous features, see <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>.
Possible options are: <code>'densityDiag'</code> (default), <code>'barDiag'</code>, and <code>'blankDiag'</code>.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_diag_cat">diag_cat</code></td>
<td>
<p>String. Type of plot to use on the diagonal for categorical features, see <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>.
Possible options are: <code>'barDiag'</code> (default) and <code>'blankDiag'</code>.</p>
</td></tr>
<tr><td><code id="plot_vaeac_imputed_ggpairs_+3A_cor_method">cor_method</code></td>
<td>
<p>String. Type of correlation measure, see <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code>.
Possible options are: <code>'pearson'</code> (default), <code>'kendall'</code>, and <code>'spearman'</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="GGally.html#topic+ggpairs">GGally::ggpairs()</a></code> figure.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">
Olsen, L. H., Glad, I. K., Jullum, M., &amp; Aas, K. (2022). Using Shapley values and variational autoencoders to
explain predictive models with dependent mixed features. Journal of machine learning research, 23(213), 1-51</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(xgboost)
library(data.table)
library(shapr)

data("airquality")
data &lt;- data.table::as.data.table(airquality)
data &lt;- data[complete.cases(data), ]

x_var &lt;- c("Solar.R", "Wind", "Temp", "Month")
y_var &lt;- "Ozone"

ind_x_explain &lt;- 1:6
x_train &lt;- data[-ind_x_explain, ..x_var]
y_train &lt;- data[-ind_x_explain, get(y_var)]
x_explain &lt;- data[ind_x_explain, ..x_var]

# Fitting a basic xgboost model to the training data
model &lt;- xgboost(
  data = as.matrix(x_train),
  label = y_train,
  nround = 100,
  verbose = FALSE
)

explanation &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "vaeac",
  phi0 = mean(y_train),
  n_MC_samples = 1,
  vaeac.epochs = 10,
  vaeac.n_vaeacs_initialize = 1
)

# Plot the results
figure &lt;- plot_vaeac_imputed_ggpairs(
  explanation = explanation,
  which_vaeac_model = "best",
  x_true = x_train,
  add_title = TRUE
)
figure

# Note that this is an ggplot2 object which we can alter, e.g., we can change the colors.
figure +
  ggplot2::scale_color_manual(values = c("#E69F00", "#999999")) +
  ggplot2::scale_fill_manual(values = c("#E69F00", "#999999"))

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.shapr'>Plot of the Shapley value explanations</h2><span id='topic+plot.shapr'></span>

<h3>Description</h3>

<p>Plots the individual prediction explanations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'shapr'
plot(
  x,
  plot_type = "bar",
  digits = 3,
  index_x_explain = NULL,
  top_k_features = NULL,
  col = NULL,
  bar_plot_phi0 = TRUE,
  bar_plot_order = "largest_first",
  scatter_features = NULL,
  scatter_hist = TRUE,
  include_group_feature_means = FALSE,
  beeswarm_cex = 1/length(index_x_explain)^(1/4),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.shapr_+3A_x">x</code></td>
<td>
<p>An <code>shapr</code> object.
The output from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_plot_type">plot_type</code></td>
<td>
<p>Character.
Specifies the type of plot to produce.
<code>"bar"</code> (the default) gives a regular horizontal bar plot of the Shapley value magnitudes.
<code>"waterfall"</code> gives a waterfall plot indicating the changes in the prediction score due to each features
contribution (their Shapley values).
<code>"scatter"</code> plots the feature values on the x-axis and Shapley values on the y-axis, as well as
(optionally) a background scatter_hist showing the distribution of the feature data.
<code>"beeswarm"</code> summarizes the distribution of the Shapley values along the x-axis for all the features.
Each point gives the shapley value of a given instance, where the points are colored by the feature value
of that instance.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_digits">digits</code></td>
<td>
<p>Integer.
Number of significant digits to use in the feature description.
Applicable for <code>plot_type</code> <code>"bar"</code> and <code>"waterfall"</code></p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_index_x_explain">index_x_explain</code></td>
<td>
<p>Integer vector.
Which of the test observations to plot. E.g. if you have
explained 10 observations using <code><a href="#topic+explain">explain()</a></code>, you can generate a plot for the first 5
observations by setting <code>index_x_explain = 1:5</code>.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_top_k_features">top_k_features</code></td>
<td>
<p>Integer.
How many features to include in the plot.
E.g. if you have 15 features in your model you can plot the 5 most important features,
for each explanation, by setting <code>top_k_features = 1:5</code>.
Applicable for <code>plot_type</code> <code>"bar"</code> and <code>"waterfall"</code></p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_col">col</code></td>
<td>
<p>Character vector (where length depends on plot type).
The color codes (hex codes or other names understood by <code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot()</a></code>) for positive and negative
Shapley values, respectively.
The default is <code>col=NULL</code>, plotting with the default colors respective to the plot type.
For <code>plot_type = "bar"</code> and <code>plot_type = "waterfall"</code>, the default is <code>c("#00BA38","#F8766D")</code>.
For <code>plot_type = "beeswarm"</code>, the default is <code>c("#F8766D","yellow","#00BA38")</code>.
For <code>plot_type = "scatter"</code>, the default is <code>"#619CFF"</code>.
</p>
<p>If you want to alter the colors i the plot, the length of the <code>col</code> vector depends on plot type.
For <code>plot_type = "bar"</code> or <code>plot_type = "waterfall"</code>, two colors should be provided, first for positive and
then for negative Shapley values.
For <code>plot_type = "beeswarm"</code>, either two or three colors can be given.
If two colors are given, then the first color determines the color that points with high feature values will have,
and the second determines the color of points with low feature values.
If three colors are given, then the first colors high feature values, the second colors mid-range feature values,
and the third colors low feature values.
For instance, <code>col = c("red", "yellow", "blue")</code> will make high values red, mid-range values yellow,
and low values blue.
For <code>plot_type = "scatter"</code>, a single color is to be given, which determines the color of the points on the
scatter plot.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_bar_plot_phi0">bar_plot_phi0</code></td>
<td>
<p>Logical.
Whether to include <code>phi0</code> in the plot for  <code>plot_type = "bar"</code>.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_bar_plot_order">bar_plot_order</code></td>
<td>
<p>Character.
Specifies what order to plot the features with respect to the magnitude of the shapley values with
<code>plot_type = "bar"</code>:
<code>"largest_first"</code> (the default) plots the features ordered from largest to smallest absolute Shapley value.
<code>"smallest_first"</code> plots the features ordered from smallest to largest absolute Shapley value.
<code>"original"</code> plots the features in the original order of the data table.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_scatter_features">scatter_features</code></td>
<td>
<p>Integer or character vector.
Only used for <code>plot_type = "scatter"</code>.
Specifies what features to include in (scatter) plot. Can be a numerical vector indicating feature index, or a
character vector, indicating the name(s) of the feature(s) to plot.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_scatter_hist">scatter_hist</code></td>
<td>
<p>Logical.
Only used for <code>plot_type = "scatter"</code>.
Whether to include a scatter_hist indicating the distribution of the data when making the scatter plot. Note
that the bins are scaled so that when all the bins are stacked they fit the span of the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_include_group_feature_means">include_group_feature_means</code></td>
<td>
<p>Logical.
Whether to include the average feature value in a group on the y-axis or not.
If <code>FALSE</code> (default), then no value is shown for the groups. If <code>TRUE</code>, then <code>shapr</code> includes the mean of the
features in each group.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_beeswarm_cex">beeswarm_cex</code></td>
<td>
<p>Numeric.
The cex argument of <code><a href="ggbeeswarm.html#topic+geom_beeswarm">ggbeeswarm::geom_beeswarm()</a></code>, controlling the spacing in the beeswarm plots.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_...">...</code></td>
<td>
<p>Other arguments passed to underlying functions,
like <code><a href="ggbeeswarm.html#topic+geom_beeswarm">ggbeeswarm::geom_beeswarm()</a></code> for <code>plot_type = "beeswarm"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the examples below, or <code>vignette("general_usage", package = "shapr")</code> for an examples of
how you should use the function.
</p>


<h3>Value</h3>

<p>ggplot object with plots of the Shapley value explanations
</p>


<h3>Author(s)</h3>

<p>Martin Jullum, Vilde Ung, Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("airquality")
airquality &lt;- airquality[complete.cases(airquality), ]
x_var &lt;- c("Solar.R", "Wind", "Temp", "Month")
y_var &lt;- "Ozone"

# Split data into test- and training data
data_train &lt;- head(airquality, -50)
data_explain &lt;- tail(airquality, 50)

x_train &lt;- data_train[, x_var]
x_explain &lt;- data_explain[, x_var]

# Fit a linear model
lm_formula &lt;- as.formula(paste0(y_var, " ~ ", paste0(x_var, collapse = " + ")))
model &lt;- lm(lm_formula, data = data_train)

# Explain predictions
p &lt;- mean(data_train[, y_var])

# Empirical approach
x &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "empirical",
  phi0 = p,
  n_MC_samples = 1e2
)

if (requireNamespace("ggplot2", quietly = TRUE) &amp;&amp; requireNamespace("ggbeeswarm", quietly = TRUE)) {
  # The default plotting option is a bar plot of the Shapley values
  # We draw bar plots for the first 4 observations
  plot(x, index_x_explain = 1:4)

  # We can also make waterfall plots
  plot(x, plot_type = "waterfall", index_x_explain = 1:4)
  # And only showing the 2 features with largest contribution
  plot(x, plot_type = "waterfall", index_x_explain = 1:4, top_k_features = 2)

  # Or scatter plots showing the distribution of the shapley values and feature values
  plot(x, plot_type = "scatter")
  # And only for a specific feature
  plot(x, plot_type = "scatter", scatter_features = "Temp")

  # Or a beeswarm plot summarising the Shapley values and feature values for all features
  plot(x, plot_type = "beeswarm")
  plot(x, plot_type = "beeswarm", col = c("red", "black")) # we can change colors

  # Additional arguments can be passed to ggbeeswarm::geom_beeswarm() using the '...' argument.
  # For instance, sometimes the beeswarm plots overlap too much.
  # This can be fixed with the 'corral="wrap" argument.
  # See ?ggbeeswarm::geom_beeswarm for more information.
  plot(x, plot_type = "beeswarm", corral = "wrap")
}

# Example of scatter and beeswarm plot with factor variables
airquality$Month_factor &lt;- as.factor(month.abb[airquality$Month])
airquality &lt;- airquality[complete.cases(airquality), ]
x_var &lt;- c("Solar.R", "Wind", "Temp", "Month_factor")
y_var &lt;- "Ozone"

# Split data into test- and training data
data_train &lt;- airquality
data_explain &lt;- tail(airquality, 50)

x_train &lt;- data_train[, x_var]
x_explain &lt;- data_explain[, x_var]

# Fit a linear model
lm_formula &lt;- as.formula(paste0(y_var, " ~ ", paste0(x_var, collapse = " + ")))
model &lt;- lm(lm_formula, data = data_train)

# Explain predictions
p &lt;- mean(data_train[, y_var])

# Empirical approach
x &lt;- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "ctree",
  phi0 = p,
  n_MC_samples = 1e2
)

if (requireNamespace("ggplot2", quietly = TRUE) &amp;&amp; requireNamespace("ggbeeswarm", quietly = TRUE)) {
  plot(x, plot_type = "scatter")
  plot(x, plot_type = "beeswarm")
}

## End(Not run)

</code></pre>

<hr>
<h2 id='predict_model'>Generate predictions for input data with specified model</h2><span id='topic+predict_model'></span><span id='topic+predict_model.default'></span><span id='topic+predict_model.ar'></span><span id='topic+predict_model.Arima'></span><span id='topic+predict_model.forecast_ARIMA'></span><span id='topic+predict_model.glm'></span><span id='topic+predict_model.lm'></span><span id='topic+predict_model.gam'></span><span id='topic+predict_model.ranger'></span><span id='topic+predict_model.workflow'></span><span id='topic+predict_model.xgb.Booster'></span>

<h3>Description</h3>

<p>Performs prediction of response
<code><a href="stats.html#topic+lm">stats::lm()</a></code>,
<code><a href="stats.html#topic+glm">stats::glm()</a></code>,
<code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code>,
<code><a href="mgcv.html#topic+gam">mgcv::gam()</a></code>,
<code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code> (i.e., <code>tidymodels</code> models), and
<code><a href="xgboost.html#topic+xgb.train">xgboost::xgb.train()</a></code> with binary or continuous
response. See details for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_model(x, newdata, ...)

## Default S3 method:
predict_model(x, newdata, ...)

## S3 method for class 'ar'
predict_model(x, newdata, newreg, horizon, ...)

## S3 method for class 'Arima'
predict_model(
  x,
  newdata,
  newreg,
  horizon,
  explain_idx,
  explain_lags,
  y,
  xreg,
  ...
)

## S3 method for class 'forecast_ARIMA'
predict_model(x, newdata, newreg, horizon, ...)

## S3 method for class 'glm'
predict_model(x, newdata, ...)

## S3 method for class 'lm'
predict_model(x, newdata, ...)

## S3 method for class 'gam'
predict_model(x, newdata, ...)

## S3 method for class 'ranger'
predict_model(x, newdata, ...)

## S3 method for class 'workflow'
predict_model(x, newdata, ...)

## S3 method for class 'xgb.Booster'
predict_model(x, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_model_+3A_x">x</code></td>
<td>
<p>Model object for the model to be explained.</p>
</td></tr>
<tr><td><code id="predict_model_+3A_newdata">newdata</code></td>
<td>
<p>A data.frame/data.table with the features to predict from.</p>
</td></tr>
<tr><td><code id="predict_model_+3A_...">...</code></td>
<td>
<p><code>newreg</code> and <code>horizon</code> parameters used in models passed to <code style="white-space: pre;">&#8288;[explain_forecast()]&#8288;</code></p>
</td></tr>
<tr><td><code id="predict_model_+3A_horizon">horizon</code></td>
<td>
<p>Numeric.
The forecast horizon to explain. Passed to the <code>predict_model</code> function.</p>
</td></tr>
<tr><td><code id="predict_model_+3A_explain_idx">explain_idx</code></td>
<td>
<p>Numeric vector.
The row indices in data and reg denoting points in time to explain.</p>
</td></tr>
<tr><td><code id="predict_model_+3A_y">y</code></td>
<td>
<p>Matrix, data.frame/data.table or a numeric vector.
Contains the endogenous variables used to estimate the (conditional) distributions
needed to properly estimate the conditional expectations in the Shapley formula
including the observations to be explained.</p>
</td></tr>
<tr><td><code id="predict_model_+3A_xreg">xreg</code></td>
<td>
<p>Matrix, data.frame/data.table or a numeric vector.
Contains the exogenous variables used to estimate the (conditional) distributions
needed to properly estimate the conditional expectations in the Shapley formula
including the observations to be explained.
As exogenous variables are used contemporaneously when producing a forecast,
this item should contain nrow(y) + horizon rows.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following models are currently supported:
</p>

<ul>
<li> <p><code><a href="stats.html#topic+lm">stats::lm()</a></code>
</p>
</li>
<li> <p><code><a href="stats.html#topic+glm">stats::glm()</a></code>
</p>
</li>
<li> <p><code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code>
</p>
</li>
<li> <p><code><a href="mgcv.html#topic+gam">mgcv::gam()</a></code>
</p>
</li>
<li> <p><code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code>
</p>
</li>
<li> <p><code><a href="xgboost.html#topic+xgb.train">xgboost::xgb.train()</a></code>
</p>
</li></ul>

<p>If you have a binary classification model we'll always return the probability prediction
for a single class.
</p>
<p>If you are explaining a model not supported natively, you need to create the <code style="white-space: pre;">&#8288;[predict_model()]&#8288;</code> function yourself,
and pass it on to as an argument to <code style="white-space: pre;">&#8288;[explain()]&#8288;</code>.
</p>
<p>For more details on how to explain such non-supported models (i.e. custom models), see the Advanced usage section
of the general usage: <br />
From R: <code>vignette("general_usage", package = "shapr")</code>  <br />
Web: <a href="https://norskregnesentral.github.io/shapr/articles/general_usage.html#explain-custom-models">https://norskregnesentral.github.io/shapr/articles/general_usage.html#explain-custom-models</a>
</p>


<h3>Value</h3>

<p>Numeric. Vector of size equal to the number of rows in <code>newdata</code>.
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load example data
data("airquality")
airquality &lt;- airquality[complete.cases(airquality), ]
# Split data into test- and training data
x_train &lt;- head(airquality, -3)
x_explain &lt;- tail(airquality, 3)
# Fit a linear model
model &lt;- lm(Ozone ~ Solar.R + Wind + Temp + Month, data = x_train)

# Predicting for a model with a standardized format
predict_model(x = model, newdata = x_explain)
</code></pre>

<hr>
<h2 id='prepare_data'>Generate data used for predictions and Monte Carlo integration</h2><span id='topic+prepare_data'></span><span id='topic+prepare_data.categorical'></span><span id='topic+prepare_data.copula'></span><span id='topic+prepare_data.ctree'></span><span id='topic+prepare_data.empirical'></span><span id='topic+prepare_data.gaussian'></span><span id='topic+prepare_data.independence'></span><span id='topic+prepare_data.regression_separate'></span><span id='topic+prepare_data.regression_surrogate'></span><span id='topic+prepare_data.timeseries'></span><span id='topic+prepare_data.vaeac'></span>

<h3>Description</h3>

<p>Generate data used for predictions and Monte Carlo integration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data(internal, index_features = NULL, ...)

## S3 method for class 'categorical'
prepare_data(internal, index_features = NULL, ...)

## S3 method for class 'copula'
prepare_data(internal, index_features, ...)

## S3 method for class 'ctree'
prepare_data(internal, index_features = NULL, ...)

## S3 method for class 'empirical'
prepare_data(internal, index_features = NULL, ...)

## S3 method for class 'gaussian'
prepare_data(internal, index_features, ...)

## S3 method for class 'independence'
prepare_data(internal, index_features = NULL, ...)

## S3 method for class 'regression_separate'
prepare_data(internal, index_features = NULL, ...)

## S3 method for class 'regression_surrogate'
prepare_data(internal, index_features = NULL, ...)

## S3 method for class 'timeseries'
prepare_data(internal, index_features = NULL, ...)

## S3 method for class 'vaeac'
prepare_data(internal, index_features = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_data_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_index_features">index_features</code></td>
<td>
<p>Positive integer vector. Specifies the id_coalition to
apply to the present method. <code>NULL</code> means all coalitions. Only used internally.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table containing simulated data used to estimate
the contribution function by Monte Carlo integration.
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>
<p>Annabelle Redelmeier and Lars Henry Berge Olsen
</p>
<p>Lars Henry Berge Olsen
</p>
<p>Martin Jullum,
</p>

<hr>
<h2 id='prepare_data_causal'>Generate data used for predictions and Monte Carlo integration for causal Shapley values</h2><span id='topic+prepare_data_causal'></span>

<h3>Description</h3>

<p>This function loops over the given coalitions, and for each coalition it extracts the
chain of relevant sampling steps provided in <code>internal$object$S_causal</code>. This chain
can contain sampling from marginal and conditional distributions. We use the approach given by
<code>internal$parameters$approach</code> to generate the samples from the conditional distributions, and
we iteratively call <code>prepare_data()</code> with a modified <code>internal_copy</code> list to reuse code.
However, this also means that chains with the same conditional distributions will retrain a
model of said conditional distributions several times.
For the marginal distribution, we sample from the Gaussian marginals when the approach is
<code>gaussian</code> and from the marginals of the training data for all other approaches. Note that
we could extend the code to sample from the marginal (gaussian) copula, too, when <code>approach</code> is
<code>copula</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data_causal(internal, index_features = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_data_causal_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="prepare_data_causal_+3A_index_features">index_features</code></td>
<td>
<p>Positive integer vector. Specifies the id_coalition to
apply to the present method. <code>NULL</code> means all coalitions. Only used internally.</p>
</td></tr>
<tr><td><code id="prepare_data_causal_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table containing simulated data that respects the (partial) causal ordering and the
the confounding assumptions. The data is used to estimate the contribution function by Monte Carlo integration.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='prepare_data_copula_cpp'>Generate (Gaussian) Copula MC samples</h2><span id='topic+prepare_data_copula_cpp'></span>

<h3>Description</h3>

<p>Generate (Gaussian) Copula MC samples
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data_copula_cpp(
  MC_samples_mat,
  x_explain_mat,
  x_explain_gaussian_mat,
  x_train_mat,
  S,
  mu,
  cov_mat
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_data_copula_cpp_+3A_mc_samples_mat">MC_samples_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_MC_samples</code>, <code>n_features</code>) containing samples from the univariate standard normal.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_+3A_x_explain_mat">x_explain_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_explain</code>, <code>n_features</code>) containing the observations to explain.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_+3A_x_explain_gaussian_mat">x_explain_gaussian_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_explain</code>, <code>n_features</code>) containing the observations to explain after being transformed
using the Gaussian transform, i.e., the samples have been transformed to a standardized normal distribution.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_+3A_x_train_mat">x_train_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_train</code>, <code>n_features</code>) containing the training observations.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_+3A_s">S</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_coalitions</code>, <code>n_features</code>) containing binary representations of the used coalitions.
S cannot contain the empty or grand coalition, i.e., a row containing only zeros or ones.
This is not a problem internally in shapr as the empty and grand coalitions are treated differently.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_+3A_mu">mu</code></td>
<td>
<p>arma::vec.
Vector of length <code>n_features</code> containing the mean of each feature after being transformed using the Gaussian
transform, i.e., the samples have been transformed to a standardized normal distribution.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_+3A_cov_mat">cov_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_features</code>, <code>n_features</code>) containing the pairwise covariance between all pairs of features
after being transformed using the Gaussian transform, i.e., the samples have been transformed to a standardized
normal distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An arma::cube/3D array of dimension (<code>n_MC_samples</code>, <code>n_explain</code> * <code>n_coalitions</code>, <code>n_features</code>), where
the columns (<em>,j,</em>) are matrices of dimension (<code>n_MC_samples</code>, <code>n_features</code>) containing the conditional Gaussian
copula MC samples for each explicand and coalition on the original scale.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='prepare_data_copula_cpp_caus'>Generate (Gaussian) Copula MC samples for the causal setup with a single MC sample for each explicand</h2><span id='topic+prepare_data_copula_cpp_caus'></span>

<h3>Description</h3>

<p>Generate (Gaussian) Copula MC samples for the causal setup with a single MC sample for each explicand
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data_copula_cpp_caus(
  MC_samples_mat,
  x_explain_mat,
  x_explain_gaussian_mat,
  x_train_mat,
  S,
  mu,
  cov_mat
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_data_copula_cpp_caus_+3A_mc_samples_mat">MC_samples_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_MC_samples</code>, <code>n_features</code>) containing samples from the univariate standard normal.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_caus_+3A_x_explain_mat">x_explain_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_explain</code>, <code>n_features</code>) containing the observations to explain.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_caus_+3A_x_explain_gaussian_mat">x_explain_gaussian_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_explain</code>, <code>n_features</code>) containing the observations to explain after being transformed
using the Gaussian transform, i.e., the samples have been transformed to a standardized normal distribution.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_caus_+3A_x_train_mat">x_train_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_train</code>, <code>n_features</code>) containing the training observations.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_caus_+3A_s">S</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_coalitions</code>, <code>n_features</code>) containing binary representations of the used coalitions.
S cannot contain the empty or grand coalition, i.e., a row containing only zeros or ones.
This is not a problem internally in shapr as the empty and grand coalitions are treated differently.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_caus_+3A_mu">mu</code></td>
<td>
<p>arma::vec.
Vector of length <code>n_features</code> containing the mean of each feature after being transformed using the Gaussian
transform, i.e., the samples have been transformed to a standardized normal distribution.</p>
</td></tr>
<tr><td><code id="prepare_data_copula_cpp_caus_+3A_cov_mat">cov_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_features</code>, <code>n_features</code>) containing the pairwise covariance between all pairs of features
after being transformed using the Gaussian transform, i.e., the samples have been transformed to a standardized
normal distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An arma::cube/3D array of dimension (<code>n_MC_samples</code>, <code>n_explain</code> * <code>n_coalitions</code>, <code>n_features</code>), where
the columns (<em>,j,</em>) are matrices of dimension (<code>n_MC_samples</code>, <code>n_features</code>) containing the conditional Gaussian
copula MC samples for each explicand and coalition on the original scale.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='prepare_data_gaussian_cpp'>Generate Gaussian MC samples</h2><span id='topic+prepare_data_gaussian_cpp'></span>

<h3>Description</h3>

<p>Generate Gaussian MC samples
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data_gaussian_cpp(MC_samples_mat, x_explain_mat, S, mu, cov_mat)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_data_gaussian_cpp_+3A_mc_samples_mat">MC_samples_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_MC_samples</code>, <code>n_features</code>) containing samples from the univariate standard normal.</p>
</td></tr>
<tr><td><code id="prepare_data_gaussian_cpp_+3A_x_explain_mat">x_explain_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_explain</code>, <code>n_features</code>) containing the observations to explain.</p>
</td></tr>
<tr><td><code id="prepare_data_gaussian_cpp_+3A_s">S</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_coalitions</code>, <code>n_features</code>) containing binary representations of the used coalitions.
S cannot contain the empty or grand coalition, i.e., a row containing only zeros or ones.
This is not a problem internally in shapr as the empty and grand coalitions are treated differently.</p>
</td></tr>
<tr><td><code id="prepare_data_gaussian_cpp_+3A_mu">mu</code></td>
<td>
<p>arma::vec.
Vector of length <code>n_features</code> containing the mean of each feature.</p>
</td></tr>
<tr><td><code id="prepare_data_gaussian_cpp_+3A_cov_mat">cov_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_features</code>, <code>n_features</code>) containing the covariance matrix of the features.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An arma::cube/3D array of dimension (<code>n_MC_samples</code>, <code>n_explain</code> * <code>n_coalitions</code>, <code>n_features</code>), where
the columns (<em>,j,</em>) are matrices of dimension (<code>n_MC_samples</code>, <code>n_features</code>) containing the conditional Gaussian
MC samples for each explicand and coalition.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='prepare_data_gaussian_cpp_caus'>Generate Gaussian MC samples for the causal setup with a single MC sample for each explicand</h2><span id='topic+prepare_data_gaussian_cpp_caus'></span>

<h3>Description</h3>

<p>Generate Gaussian MC samples for the causal setup with a single MC sample for each explicand
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data_gaussian_cpp_caus(MC_samples_mat, x_explain_mat, S, mu, cov_mat)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_data_gaussian_cpp_caus_+3A_mc_samples_mat">MC_samples_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_MC_samples</code>, <code>n_features</code>) containing samples from the univariate standard normal.</p>
</td></tr>
<tr><td><code id="prepare_data_gaussian_cpp_caus_+3A_x_explain_mat">x_explain_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_explain</code>, <code>n_features</code>) containing the observations to explain.</p>
</td></tr>
<tr><td><code id="prepare_data_gaussian_cpp_caus_+3A_s">S</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_coalitions</code>, <code>n_features</code>) containing binary representations of the used coalitions.
S cannot contain the empty or grand coalition, i.e., a row containing only zeros or ones.
This is not a problem internally in shapr as the empty and grand coalitions are treated differently.</p>
</td></tr>
<tr><td><code id="prepare_data_gaussian_cpp_caus_+3A_mu">mu</code></td>
<td>
<p>arma::vec.
Vector of length <code>n_features</code> containing the mean of each feature.</p>
</td></tr>
<tr><td><code id="prepare_data_gaussian_cpp_caus_+3A_cov_mat">cov_mat</code></td>
<td>
<p>arma::mat.
Matrix of dimension (<code>n_features</code>, <code>n_features</code>) containing the covariance matrix of the features.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An arma::cube/3D array of dimension (<code>n_MC_samples</code>, <code>n_explain</code> * <code>n_coalitions</code>, <code>n_features</code>), where
the columns (<em>,j,</em>) are matrices of dimension (<code>n_MC_samples</code>, <code>n_features</code>) containing the conditional Gaussian
MC samples for each explicand and coalition.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='prepare_data_single_coalition'>Compute the conditional probabilities for a single coalition for the categorical approach</h2><span id='topic+prepare_data_single_coalition'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+prepare_data.categorical">prepare_data.categorical()</a></code> function is slow when evaluated for a single coalition.
This is a bottleneck for Causal Shapley values which call said function a lot with single coalitions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data_single_coalition(internal, index_features)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_data_single_coalition_+3A_internal">internal</code></td>
<td>
<p>List.
Holds all parameters, data, functions and computed objects used within <code><a href="#topic+explain">explain()</a></code>
The list contains one or more of the elements <code>parameters</code>, <code>data</code>, <code>objects</code>, <code>iter_list</code>, <code>timing_list</code>,
<code>main_timing_list</code>, <code>output</code>, and <code>iter_timing_list</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='prepare_next_iteration'>Prepares the next iteration of the iterative sampling algorithm</h2><span id='topic+prepare_next_iteration'></span>

<h3>Description</h3>

<p>Prepares the next iteration of the iterative sampling algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_next_iteration(internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_next_iteration_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='print_iter'>Prints iterative information</h2><span id='topic+print_iter'></span>

<h3>Description</h3>

<p>Prints iterative information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>print_iter(internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print_iter_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='print.shapr'>Print method for shapr objects</h2><span id='topic+print.shapr'></span>

<h3>Description</h3>

<p>Print method for shapr objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'shapr'
print(x, digits = 4, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.shapr_+3A_x">x</code></td>
<td>
<p>A shapr object</p>
</td></tr>
<tr><td><code id="print.shapr_+3A_digits">digits</code></td>
<td>
<p>Scalar Integer.
Number of digits to display to the console</p>
</td></tr>
<tr><td><code id="print.shapr_+3A_...">...</code></td>
<td>
<p>Unused</p>
</td></tr>
</table>

<hr>
<h2 id='process_factor_data'>Treat factors as numeric values</h2><span id='topic+process_factor_data'></span>

<h3>Description</h3>

<p>Factors are given a numeric value above the highest numeric value in the data. The value of the different levels
are sorted by factor and then level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_factor_data(dt, factor_cols)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="process_factor_data_+3A_dt">dt</code></td>
<td>
<p>data.table to plot</p>
</td></tr>
<tr><td><code id="process_factor_data_+3A_factor_cols">factor_cols</code></td>
<td>
<p>Columns that are factors or character</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of a lookup table with each factor and level and its numeric value, a data.table
very similar to the input data, but now with numeric values for factors, and the maximum feature value.
</p>

<hr>
<h2 id='quantile_type7_cpp'>Compute the quantiles using quantile type seven</h2><span id='topic+quantile_type7_cpp'></span>

<h3>Description</h3>

<p>Compute the quantiles using quantile type seven
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quantile_type7_cpp(x, probs)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="quantile_type7_cpp_+3A_x">x</code></td>
<td>
<p>arma::vec.
Numeric vector whose sample quantiles are wanted.</p>
</td></tr>
<tr><td><code id="quantile_type7_cpp_+3A_probs">probs</code></td>
<td>
<p>arma::vec.
Numeric vector of probabilities with values between zero and one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using quantile type number seven from stats::quantile in R.
</p>


<h3>Value</h3>

<p>A vector of length <code>length(probs)</code> with the quantiles is returned.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='reg_forecast_setup'>Set up exogenous regressors for explanation in a forecast model.</h2><span id='topic+reg_forecast_setup'></span>

<h3>Description</h3>

<p>Set up exogenous regressors for explanation in a forecast model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_forecast_setup(x, horizon, group)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reg_forecast_setup_+3A_x">x</code></td>
<td>
<p>A matrix with the exogenous variables.</p>
</td></tr>
<tr><td><code id="reg_forecast_setup_+3A_horizon">horizon</code></td>
<td>
<p>Numeric.
The forecast horizon to explain. Passed to the <code>predict_model</code> function.</p>
</td></tr>
<tr><td><code id="reg_forecast_setup_+3A_group">group</code></td>
<td>
<p>The list of endogenous groups, to append exogenous groups to.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing
</p>

<ul>
<li><p> fcast A matrix containing the exogenous observations needed for each observation.
</p>
</li>
<li><p> group The list group with the exogenous groups appended.
</p>
</li></ul>


<hr>
<h2 id='regression.check_namespaces'>Check that needed libraries are installed</h2><span id='topic+regression.check_namespaces'></span>

<h3>Description</h3>

<p>This function checks that the <code>parsnip</code>, <code>recipes</code>, <code>workflows</code>, <code>tune</code>, <code>dials</code>,
<code>yardstick</code>, <code>hardhat</code>, <code>rsample</code>, and <code>rlang</code> packages are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.check_namespaces()
</code></pre>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='regression.check_parameters'>Check regression parameters</h2><span id='topic+regression.check_parameters'></span>

<h3>Description</h3>

<p>Check regression parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.check_parameters(internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression.check_parameters_+3A_internal">internal</code></td>
<td>
<p>List.
Holds all parameters, data, functions and computed objects used within <code><a href="#topic+explain">explain()</a></code>
The list contains one or more of the elements <code>parameters</code>, <code>data</code>, <code>objects</code>, <code>iter_list</code>, <code>timing_list</code>,
<code>main_timing_list</code>, <code>output</code>, and <code>iter_timing_list</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same <code>internal</code> list, but added logical indicator <code>internal$parameters$regression.tune</code>
if we are to tune the regression model/models.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='regression.check_recipe_func'>Check <code>regression.recipe_func</code></h2><span id='topic+regression.check_recipe_func'></span>

<h3>Description</h3>

<p>Check that regression.recipe_func is a function that returns the
RHS of the formula for arbitrary feature name inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.check_recipe_func(regression.recipe_func, x_explain)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression.check_recipe_func_+3A_regression.recipe_func">regression.recipe_func</code></td>
<td>
<p>Either <code>NULL</code> (default) or a function that that takes in a <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>
object and returns a modified <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code> with potentially additional recipe steps. See the regression
vignette for several examples.
Note, to make it easier to call <code>explain()</code> from Python, the <code>regression.recipe_func</code> can also be a string
containing an R function. For example,
<code>"function(recipe) return(recipes::step_ns(recipe, recipes::all_numeric_predictors(), deg_free = 2))"</code> is also
a valid input. It is essential to include the package prefix if the package is not loaded.</p>
</td></tr>
<tr><td><code id="regression.check_recipe_func_+3A_x_explain">x_explain</code></td>
<td>
<p>Data.table with the features of the observation whose
predictions ought to be explained (test data).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='regression.check_sur_n_comb'>Check the <code>regression.surrogate_n_comb</code> parameter</h2><span id='topic+regression.check_sur_n_comb'></span>

<h3>Description</h3>

<p>Check that <code>regression.surrogate_n_comb</code> is either NULL or a valid integer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.check_sur_n_comb(regression.surrogate_n_comb, n_coalitions)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression.check_sur_n_comb_+3A_regression.surrogate_n_comb">regression.surrogate_n_comb</code></td>
<td>
<p>Positive integer.
Specifies the number of unique coalitions to apply to each training observation.
The default is the number of sampled coalitions in the present iteration.
Any integer between 1 and the default is allowed.
Larger values requires more memory, but may improve the surrogate model.
If the user sets a value lower than the maximum, we sample this amount of unique coalitions
separately for each training observations.
That is, on average, all coalitions should be equally trained.</p>
</td></tr>
<tr><td><code id="regression.check_sur_n_comb_+3A_n_coalitions">n_coalitions</code></td>
<td>
<p>Integer. The number of used coalitions (including the empty and grand coalition).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='regression.check_vfold_cv_para'>Check the parameters that are sent to <code><a href="rsample.html#topic+vfold_cv">rsample::vfold_cv()</a></code></h2><span id='topic+regression.check_vfold_cv_para'></span>

<h3>Description</h3>

<p>Check that <code>regression.vfold_cv_para</code> is either NULL or a named list that only contains recognized parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.check_vfold_cv_para(regression.vfold_cv_para)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression.check_vfold_cv_para_+3A_regression.vfold_cv_para">regression.vfold_cv_para</code></td>
<td>
<p>Either <code>NULL</code> (default) or a named list containing
the parameters to be sent to <code><a href="rsample.html#topic+vfold_cv">rsample::vfold_cv()</a></code>. See the regression vignette for
several examples.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='regression.cv_message'>Produce message about which batch prepare_data is working on</h2><span id='topic+regression.cv_message'></span>

<h3>Description</h3>

<p>Produce message about which batch prepare_data is working on
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.cv_message(
  regression.results,
  regression.grid,
  n_cv = 10,
  current_comb
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression.cv_message_+3A_regression.results">regression.results</code></td>
<td>
<p>The results of the CV procedures.</p>
</td></tr>
<tr><td><code id="regression.cv_message_+3A_regression.grid">regression.grid</code></td>
<td>
<p>Object containing the hyperparameter values.</p>
</td></tr>
<tr><td><code id="regression.cv_message_+3A_n_cv">n_cv</code></td>
<td>
<p>Integer (default is 10) specifying the number of CV hyperparameter configurations to print.</p>
</td></tr>
<tr><td><code id="regression.cv_message_+3A_current_comb">current_comb</code></td>
<td>
<p>Integer vector. The current combination of features, passed to verbosity printing function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='regression.get_string_to_R'>Convert the string into an R object</h2><span id='topic+regression.get_string_to_R'></span>

<h3>Description</h3>

<p>Convert the string into an R object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.get_string_to_R(string)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression.get_string_to_R_+3A_string">string</code></td>
<td>
<p>A character vector/string containing the text to convert into R code.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='regression.get_tune'>Get if model is to be tuned</h2><span id='topic+regression.get_tune'></span>

<h3>Description</h3>

<p>That is, if the regression model contains hyperparameters we are to tune using cross validation.
See <a href="https://www.tidymodels.org/find/parsnip/#model-args">tidymodels</a> for default model hyperparameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.get_tune(regression.model, regression.tune_values, x_train)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression.get_tune_+3A_regression.model">regression.model</code></td>
<td>
<p>A <code>tidymodels</code> object of class <code>model_specs</code>. Default is a linear regression model, i.e.,
<code><a href="parsnip.html#topic+linear_reg">parsnip::linear_reg()</a></code>. See <a href="https://www.tidymodels.org/find/parsnip/">tidymodels</a> for all possible models,
and see the vignette for how to add new/own models. Note, to make it easier to call <code>explain()</code> from Python, the
<code>regression.model</code> parameter can also be a string specifying the model which will be parsed and evaluated. For
example, <code style="white-space: pre;">&#8288;"parsnip::rand_forest(mtry = hardhat::tune(), trees = 100, engine = "ranger", mode = "regression")"&#8288;</code>
is also a valid input. It is essential to include the package prefix if the package is not loaded.</p>
</td></tr>
<tr><td><code id="regression.get_tune_+3A_regression.tune_values">regression.tune_values</code></td>
<td>
<p>Either <code>NULL</code> (default), a data.frame/data.table/tibble, or a function.
The data.frame must contain the possible hyperparameter value combinations to try.
The column names must match the names of the tunable parameters specified in <code>regression.model</code>.
If <code>regression.tune_values</code> is a function, then it should take one argument <code>x</code> which is the training data
for the current coalition and returns a data.frame/data.table/tibble with the properties described above.
Using a function allows the hyperparameter values to change based on the size of the coalition See the regression
vignette for several examples.
Note, to make it easier to call <code>explain()</code> from Python, the <code>regression.tune_values</code> can also be a string
containing an R function. For example,
<code>"function(x) return(dials::grid_regular(dials::mtry(c(1, ncol(x)))), levels = 3))"</code> is also a valid input.
It is essential to include the package prefix if the package is not loaded.</p>
</td></tr>
<tr><td><code id="regression.get_tune_+3A_x_train">x_train</code></td>
<td>
<p>Data.table with training data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A boolean variable indicating if the regression model is to be tuned.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='regression.get_y_hat'>Get the predicted responses</h2><span id='topic+regression.get_y_hat'></span>

<h3>Description</h3>

<p>Get the predicted responses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.get_y_hat(internal, model, predict_model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression.get_y_hat_+3A_internal">internal</code></td>
<td>
<p>List.
Holds all parameters, data, functions and computed objects used within <code><a href="#topic+explain">explain()</a></code>
The list contains one or more of the elements <code>parameters</code>, <code>data</code>, <code>objects</code>, <code>iter_list</code>, <code>timing_list</code>,
<code>main_timing_list</code>, <code>output</code>, and <code>iter_timing_list</code>.</p>
</td></tr>
<tr><td><code id="regression.get_y_hat_+3A_model">model</code></td>
<td>
<p>Objects.
The model object that ought to be explained.
See the documentation of <code><a href="#topic+explain">explain()</a></code> for details.</p>
</td></tr>
<tr><td><code id="regression.get_y_hat_+3A_predict_model">predict_model</code></td>
<td>
<p>Function.
The prediction function used when <code>model</code> is not natively supported.
See the documentation of <code><a href="#topic+explain">explain()</a></code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same <code>internal</code> list, but added vectors <code>internal$data$x_train_y_hat</code> and
<code>internal$data$x_explain_y_hat</code> containing the predicted response of the training and explain data.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='regression.surrogate_aug_data'>Augment the training data and the explicands</h2><span id='topic+regression.surrogate_aug_data'></span>

<h3>Description</h3>

<p>Augment the training data and the explicands
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.surrogate_aug_data(
  internal,
  x,
  y_hat = NULL,
  index_features = NULL,
  augment_masks_as_factor = FALSE,
  augment_include_grand = FALSE,
  augment_add_id_coal = FALSE,
  augment_comb_prob = NULL,
  augment_weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression.surrogate_aug_data_+3A_internal">internal</code></td>
<td>
<p>List.
Holds all parameters, data, functions and computed objects used within <code><a href="#topic+explain">explain()</a></code>
The list contains one or more of the elements <code>parameters</code>, <code>data</code>, <code>objects</code>, <code>iter_list</code>, <code>timing_list</code>,
<code>main_timing_list</code>, <code>output</code>, and <code>iter_timing_list</code>.</p>
</td></tr>
<tr><td><code id="regression.surrogate_aug_data_+3A_x">x</code></td>
<td>
<p>Data.table containing the training data.</p>
</td></tr>
<tr><td><code id="regression.surrogate_aug_data_+3A_y_hat">y_hat</code></td>
<td>
<p>Vector of numerics (optional) containing the predicted responses for the observations in <code>x</code>.</p>
</td></tr>
<tr><td><code id="regression.surrogate_aug_data_+3A_index_features">index_features</code></td>
<td>
<p>Array of integers (optional) containing which coalitions to consider. Must be provided if
<code>x</code> is the explicands.</p>
</td></tr>
<tr><td><code id="regression.surrogate_aug_data_+3A_augment_masks_as_factor">augment_masks_as_factor</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the binary masks are converted
to factors. If <code>FALSE</code>, then the binary masks are numerics.</p>
</td></tr>
<tr><td><code id="regression.surrogate_aug_data_+3A_augment_include_grand">augment_include_grand</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the grand coalition is included.
If <code>index_features</code> are provided, then <code>augment_include_grand</code> has no effect. Note that if we sample the
coalitions then the grand coalition is equally likely to be samples as the other coalitions (or weighted if
<code>augment_comb_prob</code> is provided).</p>
</td></tr>
<tr><td><code id="regression.surrogate_aug_data_+3A_augment_add_id_coal">augment_add_id_coal</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, an additional column is adding containing
which coalition was applied.</p>
</td></tr>
<tr><td><code id="regression.surrogate_aug_data_+3A_augment_comb_prob">augment_comb_prob</code></td>
<td>
<p>Array of numerics (default is <code>NULL</code>). The length of the array must match the number of
coalitions being considered, where each entry specifies the probability of sampling the corresponding coalition.
This is useful if we want to generate more training data for some specific coalitions. One possible choice would be
<code>augment_comb_prob = if (use_Shapley_weights) internal$objects$X$shapley_weight[2:actual_n_coalitions] else NULL</code>.</p>
</td></tr>
<tr><td><code id="regression.surrogate_aug_data_+3A_augment_weights">augment_weights</code></td>
<td>
<p>String (optional). Specifying which type of weights to add to the observations.
If <code>NULL</code> (default), then no weights are added. If <code>"Shapley"</code>, then the Shapley weights for the different
coalitions are added to corresponding observations where the coalitions was applied. If <code>uniform</code>, then
all observations get an equal weight of one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table containing the augmented data.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='regression.train_model'>Train a tidymodels model via workflows</h2><span id='topic+regression.train_model'></span>

<h3>Description</h3>

<p>Function that trains a <code>tidymodels</code> model via <code>workflows</code> based on the provided input parameters.
This function allows for cross validating the hyperparameters of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression.train_model(
  x,
  seed = 1,
  verbose = NULL,
  regression.model = parsnip::linear_reg(),
  regression.tune = FALSE,
  regression.tune_values = NULL,
  regression.vfold_cv_para = NULL,
  regression.recipe_func = NULL,
  regression.response_var = "y_hat",
  regression.surrogate_n_comb = NULL,
  current_comb = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression.train_model_+3A_x">x</code></td>
<td>
<p>Data.table containing the training data.</p>
</td></tr>
<tr><td><code id="regression.train_model_+3A_seed">seed</code></td>
<td>
<p>Positive integer.
Specifies the seed before any randomness based code is being run.
If <code>NULL</code> no seed is set in the calling environment.</p>
</td></tr>
<tr><td><code id="regression.train_model_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
<tr><td><code id="regression.train_model_+3A_regression.model">regression.model</code></td>
<td>
<p>A <code>tidymodels</code> object of class <code>model_specs</code>. Default is a linear regression model, i.e.,
<code><a href="parsnip.html#topic+linear_reg">parsnip::linear_reg()</a></code>. See <a href="https://www.tidymodels.org/find/parsnip/">tidymodels</a> for all possible models,
and see the vignette for how to add new/own models. Note, to make it easier to call <code>explain()</code> from Python, the
<code>regression.model</code> parameter can also be a string specifying the model which will be parsed and evaluated. For
example, <code style="white-space: pre;">&#8288;"parsnip::rand_forest(mtry = hardhat::tune(), trees = 100, engine = "ranger", mode = "regression")"&#8288;</code>
is also a valid input. It is essential to include the package prefix if the package is not loaded.</p>
</td></tr>
<tr><td><code id="regression.train_model_+3A_regression.tune">regression.tune</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then we are to tune the hyperparemeters based on
the values provided in <code>regression.tune_values</code>. Note that no checks are conducted as this is checked earlier in
<code>setup_approach.regression_separate</code> and <code>setup_approach.regression_surrogate</code>.</p>
</td></tr>
<tr><td><code id="regression.train_model_+3A_regression.tune_values">regression.tune_values</code></td>
<td>
<p>Either <code>NULL</code> (default), a data.frame/data.table/tibble, or a function.
The data.frame must contain the possible hyperparameter value combinations to try.
The column names must match the names of the tunable parameters specified in <code>regression.model</code>.
If <code>regression.tune_values</code> is a function, then it should take one argument <code>x</code> which is the training data
for the current coalition and returns a data.frame/data.table/tibble with the properties described above.
Using a function allows the hyperparameter values to change based on the size of the coalition See the regression
vignette for several examples.
Note, to make it easier to call <code>explain()</code> from Python, the <code>regression.tune_values</code> can also be a string
containing an R function. For example,
<code>"function(x) return(dials::grid_regular(dials::mtry(c(1, ncol(x)))), levels = 3))"</code> is also a valid input.
It is essential to include the package prefix if the package is not loaded.</p>
</td></tr>
<tr><td><code id="regression.train_model_+3A_regression.vfold_cv_para">regression.vfold_cv_para</code></td>
<td>
<p>Either <code>NULL</code> (default) or a named list containing
the parameters to be sent to <code><a href="rsample.html#topic+vfold_cv">rsample::vfold_cv()</a></code>. See the regression vignette for
several examples.</p>
</td></tr>
<tr><td><code id="regression.train_model_+3A_regression.recipe_func">regression.recipe_func</code></td>
<td>
<p>Either <code>NULL</code> (default) or a function that that takes in a <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>
object and returns a modified <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code> with potentially additional recipe steps. See the regression
vignette for several examples.
Note, to make it easier to call <code>explain()</code> from Python, the <code>regression.recipe_func</code> can also be a string
containing an R function. For example,
<code>"function(recipe) return(recipes::step_ns(recipe, recipes::all_numeric_predictors(), deg_free = 2))"</code> is also
a valid input. It is essential to include the package prefix if the package is not loaded.</p>
</td></tr>
<tr><td><code id="regression.train_model_+3A_regression.response_var">regression.response_var</code></td>
<td>
<p>String (default is <code>y_hat</code>) containing the name of the response variable.</p>
</td></tr>
<tr><td><code id="regression.train_model_+3A_regression.surrogate_n_comb">regression.surrogate_n_comb</code></td>
<td>
<p>Integer (default is <code>NULL</code>). The number of times each training observations
has been augmented. If <code>NULL</code>, then we assume that we are doing separate regression.</p>
</td></tr>
<tr><td><code id="regression.train_model_+3A_current_comb">current_comb</code></td>
<td>
<p>Integer vector. The current combination of features, passed to verbosity printing function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained <code>tidymodels</code> model based on the provided input parameters.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='release_questions'>Auxiliary function for the vignettes</h2><span id='topic+release_questions'></span>

<h3>Description</h3>

<p>Function that question if the main and vaeac vignette has been built using the
<code>rebuild-long-running-vignette.R</code> function.
This is only useful when using devtools to release <code>shapr</code> to cran.
See <code><a href="devtools.html#topic+release">devtools::release()</a></code> for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>release_questions()
</code></pre>

<hr>
<h2 id='rss_cpp'>Function for computing sigma_hat_sq</h2><span id='topic+rss_cpp'></span>

<h3>Description</h3>

<p>Function for computing sigma_hat_sq
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rss_cpp(H, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rss_cpp_+3A_h">H</code></td>
<td>
<p>Matrix.
Output from <code><a href="#topic+hat_matrix_cpp">hat_matrix_cpp()</a></code></p>
</td></tr>
<tr><td><code id="rss_cpp_+3A_y">y</code></td>
<td>
<p>Vector
Representing the (temporary) response variable</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='sample_coalition_table'>Get table with sampled coalitions</h2><span id='topic+sample_coalition_table'></span>

<h3>Description</h3>

<p>Get table with sampled coalitions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_coalition_table(
  m,
  n_coalitions = 200,
  weight_zero_m = 10^6,
  paired_shap_sampling = TRUE,
  prev_coal_samples = NULL,
  prev_coal_samples_n_unique = NULL,
  kernelSHAP_reweighting,
  n_samps_scale = 10,
  dt_valid_causal_coalitions = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sample_coalition_table_+3A_m">m</code></td>
<td>
<p>Positive integer.
Total number of features/groups.</p>
</td></tr>
<tr><td><code id="sample_coalition_table_+3A_n_coalitions">n_coalitions</code></td>
<td>
<p>Positive integer.
Note that if <code>exact = TRUE</code>, <code>n_coalitions</code> is ignored.</p>
</td></tr>
<tr><td><code id="sample_coalition_table_+3A_weight_zero_m">weight_zero_m</code></td>
<td>
<p>Numeric.
The value to use as a replacement for infinite coalition weights when doing numerical operations.</p>
</td></tr>
<tr><td><code id="sample_coalition_table_+3A_paired_shap_sampling">paired_shap_sampling</code></td>
<td>
<p>Logical.
Whether to do paired sampling of coalitions.</p>
</td></tr>
<tr><td><code id="sample_coalition_table_+3A_prev_coal_samples">prev_coal_samples</code></td>
<td>
<p>Character vector.
A vector of previously sampled coalitions as characters.
Each string contains a coalition and the feature indices in the coalition is separated by a space.
For example, &quot;1 5 8&quot; is a coalition with features 1, 5, and 8.</p>
</td></tr>
<tr><td><code id="sample_coalition_table_+3A_prev_coal_samples_n_unique">prev_coal_samples_n_unique</code></td>
<td>
<p>Positive integer.
The number of unique coalitions in <code>prev_coal_samples</code>.
This is a separate argument to avoid recomputing the number unnecessarily.</p>
</td></tr>
<tr><td><code id="sample_coalition_table_+3A_n_samps_scale">n_samps_scale</code></td>
<td>
<p>Positive integer.
Integer that scales the number of coalitions <code>n_coalitions</code> to sample as sampling is cheap,
while checking for <code>n_coalitions</code> unique coalitions is expensive, thus we over sample the
number of coalitions by a factor of <code>n_samps_scale</code> and determine when we have <code>n_coalitions</code> unique
coalitions and only use the coalitions up to this point and throw away the remaining coalitions.</p>
</td></tr>
<tr><td><code id="sample_coalition_table_+3A_dt_valid_causal_coalitions">dt_valid_causal_coalitions</code></td>
<td>
<p>data.table. Only applicable for asymmetric Shapley
values explanations, and is <code>NULL</code> for symmetric Shapley values.
The data.table contains information about the coalitions that respects the causal ordering.</p>
</td></tr>
</table>

<hr>
<h2 id='sample_coalitions_cpp_str_paired'>We here return a vector of strings/characters, i.e., a CharacterVector,
where each string is a space-separated list of integers.</h2><span id='topic+sample_coalitions_cpp_str_paired'></span>

<h3>Description</h3>

<p>We here return a vector of strings/characters, i.e., a CharacterVector,
where each string is a space-separated list of integers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_coalitions_cpp_str_paired(m, n_coalitions, paired_shap_sampling = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sample_coalitions_cpp_str_paired_+3A_m">m</code></td>
<td>
<p>Positive integer.
Total number of features/groups.</p>
</td></tr>
<tr><td><code id="sample_coalitions_cpp_str_paired_+3A_n_coalitions">n_coalitions</code></td>
<td>
<p>IntegerVector.
The number of features to sample for each feature combination.</p>
</td></tr>
<tr><td><code id="sample_coalitions_cpp_str_paired_+3A_paired_shap_sampling">paired_shap_sampling</code></td>
<td>
<p>Logical.
Whether to do paired sampling of coalitions.</p>
</td></tr>
</table>

<hr>
<h2 id='sample_combinations'>Helper function to sample a combination of training and testing rows, which does not risk
getting the same observation twice. Need to improve this help file.</h2><span id='topic+sample_combinations'></span>

<h3>Description</h3>

<p>Helper function to sample a combination of training and testing rows, which does not risk
getting the same observation twice. Need to improve this help file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_combinations(ntrain, ntest, nsamples, joint_sampling = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sample_combinations_+3A_ntrain">ntrain</code></td>
<td>
<p>Positive integer. Number of training observations to sample from.</p>
</td></tr>
<tr><td><code id="sample_combinations_+3A_ntest">ntest</code></td>
<td>
<p>Positive integer. Number of test observations to sample from.</p>
</td></tr>
<tr><td><code id="sample_combinations_+3A_nsamples">nsamples</code></td>
<td>
<p>Positive integer. Number of samples.</p>
</td></tr>
<tr><td><code id="sample_combinations_+3A_joint_sampling">joint_sampling</code></td>
<td>
<p>Logical. Indicates whether train- and test data should be sampled
separately or in a joint sampling space. If they are sampled separately (which typically
would be used when optimizing more than one distribution at once) we sample with replacement
if <code>nsamples &gt; ntrain</code>. Note that this solution is not optimal. Be careful if you're
doing optimization over every test observation when <code>nsamples &gt; ntrain</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='sample_ctree'>Sample ctree variables from a given conditional inference tree</h2><span id='topic+sample_ctree'></span>

<h3>Description</h3>

<p>Sample ctree variables from a given conditional inference tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_ctree(tree, n_MC_samples, x_explain, x_train, n_features, sample)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sample_ctree_+3A_tree">tree</code></td>
<td>
<p>List. Contains tree which is an object of type ctree built from the party package.
Also contains given_ind, the features to condition upon.</p>
</td></tr>
<tr><td><code id="sample_ctree_+3A_n_mc_samples">n_MC_samples</code></td>
<td>
<p>Scalar integer.
Corresponds to the number of samples from the leaf node.
See an exception when sample = FALSE in <code><a href="#topic+setup_approach.ctree">setup_approach.ctree()</a></code>.</p>
</td></tr>
<tr><td><code id="sample_ctree_+3A_x_explain">x_explain</code></td>
<td>
<p>Data.table with the features of the observation whose
predictions ought to be explained (test data).</p>
</td></tr>
<tr><td><code id="sample_ctree_+3A_x_train">x_train</code></td>
<td>
<p>Data.table with training data.</p>
</td></tr>
<tr><td><code id="sample_ctree_+3A_n_features">n_features</code></td>
<td>
<p>Positive integer.
The number of features.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the documentation of the <code><a href="#topic+setup_approach.ctree">setup_approach.ctree()</a></code> function for undocumented parameters.
</p>


<h3>Value</h3>

<p>data.table with <code>n_MC_samples</code> (conditional) Gaussian samples
</p>


<h3>Author(s)</h3>

<p>Annabelle Redelmeier
</p>

<hr>
<h2 id='save_results'>Saves the intermediate results to disk</h2><span id='topic+save_results'></span>

<h3>Description</h3>

<p>Saves the intermediate results to disk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>save_results(internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="save_results_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='setup'>check_setup</h2><span id='topic+setup'></span>

<h3>Description</h3>

<p>check_setup
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setup(
  x_train,
  x_explain,
  approach,
  phi0,
  output_size = 1,
  max_n_coalitions,
  group,
  n_MC_samples,
  seed,
  feature_specs,
  type = "regular",
  horizon = NULL,
  y = NULL,
  xreg = NULL,
  train_idx = NULL,
  explain_idx = NULL,
  explain_y_lags = NULL,
  explain_xreg_lags = NULL,
  group_lags = NULL,
  verbose,
  iterative = NULL,
  iterative_args = list(),
  is_python = FALSE,
  testing = FALSE,
  init_time = NULL,
  prev_shapr_object = NULL,
  asymmetric = FALSE,
  causal_ordering = NULL,
  confounding = NULL,
  output_args = list(),
  extra_computation_args = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setup_+3A_x_train">x_train</code></td>
<td>
<p>Matrix or data.frame/data.table.
Contains the data used to estimate the (conditional) distributions for the features
needed to properly estimate the conditional expectations in the Shapley formula.</p>
</td></tr>
<tr><td><code id="setup_+3A_x_explain">x_explain</code></td>
<td>
<p>Matrix or data.frame/data.table.
Contains the the features, whose predictions ought to be explained.</p>
</td></tr>
<tr><td><code id="setup_+3A_approach">approach</code></td>
<td>
<p>Character vector of length <code>1</code> or one less than the number of features.
All elements should, either be <code>"gaussian"</code>, <code>"copula"</code>, <code>"empirical"</code>, <code>"ctree"</code>, <code>"vaeac"</code>,
<code>"categorical"</code>, <code>"timeseries"</code>, <code>"independence"</code>, <code>"regression_separate"</code>, or <code>"regression_surrogate"</code>.
The two regression approaches can not be combined with any other approach.
See details for more information.</p>
</td></tr>
<tr><td><code id="setup_+3A_phi0">phi0</code></td>
<td>
<p>Numeric.
The prediction value for unseen data, i.e. an estimate of the expected prediction without conditioning on any
features.
Typically we set this value equal to the mean of the response variable in our training data, but other choices
such as the mean of the predictions in the training data are also reasonable.</p>
</td></tr>
<tr><td><code id="setup_+3A_output_size">output_size</code></td>
<td>
<p>Scalar integer.
Specifies the dimension of the output from the prediction model for every observation.</p>
</td></tr>
<tr><td><code id="setup_+3A_max_n_coalitions">max_n_coalitions</code></td>
<td>
<p>Integer.
The upper limit on the number of unique feature/group coalitions to use in the iterative procedure
(if <code>iterative = TRUE</code>).
If <code>iterative = FALSE</code> it represents the number of feature/group coalitions to use directly.
The quantity refers to the number of unique feature coalitions if <code>group = NULL</code>,
and group coalitions if <code>group != NULL</code>.
<code>max_n_coalitions = NULL</code> corresponds to <code>max_n_coalitions=2^n_features</code>.</p>
</td></tr>
<tr><td><code id="setup_+3A_group">group</code></td>
<td>
<p>List.
If <code>NULL</code> regular feature wise Shapley values are computed.
If provided, group wise Shapley values are computed.
<code>group</code> then has length equal to the number of groups.
The list element contains character vectors with the features included in each of the different groups.
See
<a href="https://martinjullum.com/publication/jullum-2021-efficient/jullum-2021-efficient.pdf">Jullum et al. (2021)</a>
for more information on group wise Shapley values.</p>
</td></tr>
<tr><td><code id="setup_+3A_n_mc_samples">n_MC_samples</code></td>
<td>
<p>Positive integer.
For most approaches, it indicates the maximum number of samples to use in the Monte Carlo integration
of every conditional expectation.
For <code>approach="ctree"</code>, <code>n_MC_samples</code> corresponds to the number of samples
from the leaf node (see an exception related to the <code>ctree.sample</code> argument <code><a href="#topic+setup_approach.ctree">setup_approach.ctree()</a></code>).
For <code>approach="empirical"</code>, <code>n_MC_samples</code> is  the <code class="reqn">K</code> parameter in equations (14-15) of
Aas et al. (2021), i.e. the maximum number of observations (with largest weights) that is used, see also the
<code>empirical.eta</code> argument <code><a href="#topic+setup_approach.empirical">setup_approach.empirical()</a></code>.</p>
</td></tr>
<tr><td><code id="setup_+3A_seed">seed</code></td>
<td>
<p>Positive integer.
Specifies the seed before any randomness based code is being run.
If <code>NULL</code> no seed is set in the calling environment.</p>
</td></tr>
<tr><td><code id="setup_+3A_feature_specs">feature_specs</code></td>
<td>
<p>List. The output from <code><a href="#topic+get_model_specs">get_model_specs()</a></code> or <code><a href="#topic+get_data_specs">get_data_specs()</a></code>.
Contains the 3 elements:
</p>

<dl>
<dt>labels</dt><dd><p>Character vector with the names of each feature.</p>
</dd>
<dt>classes</dt><dd><p>Character vector with the classes of each features.</p>
</dd>
<dt>factor_levels</dt><dd><p>Character vector with the levels for any categorical features.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="setup_+3A_type">type</code></td>
<td>
<p>Character.
Either &quot;regular&quot; or &quot;forecast&quot; corresponding to function <code>setup()</code> is called from,
correspondingly the type of explanation that should be generated.</p>
</td></tr>
<tr><td><code id="setup_+3A_horizon">horizon</code></td>
<td>
<p>Numeric.
The forecast horizon to explain. Passed to the <code>predict_model</code> function.</p>
</td></tr>
<tr><td><code id="setup_+3A_y">y</code></td>
<td>
<p>Matrix, data.frame/data.table or a numeric vector.
Contains the endogenous variables used to estimate the (conditional) distributions
needed to properly estimate the conditional expectations in the Shapley formula
including the observations to be explained.</p>
</td></tr>
<tr><td><code id="setup_+3A_xreg">xreg</code></td>
<td>
<p>Matrix, data.frame/data.table or a numeric vector.
Contains the exogenous variables used to estimate the (conditional) distributions
needed to properly estimate the conditional expectations in the Shapley formula
including the observations to be explained.
As exogenous variables are used contemporaneously when producing a forecast,
this item should contain nrow(y) + horizon rows.</p>
</td></tr>
<tr><td><code id="setup_+3A_train_idx">train_idx</code></td>
<td>
<p>Numeric vector.
The row indices in data and reg denoting points in time to use when estimating the conditional expectations in
the Shapley value formula.
If <code>train_idx = NULL</code> (default) all indices not selected to be explained will be used.</p>
</td></tr>
<tr><td><code id="setup_+3A_explain_idx">explain_idx</code></td>
<td>
<p>Numeric vector.
The row indices in data and reg denoting points in time to explain.</p>
</td></tr>
<tr><td><code id="setup_+3A_explain_y_lags">explain_y_lags</code></td>
<td>
<p>Numeric vector.
Denotes the number of lags that should be used for each variable in <code>y</code> when making a forecast.</p>
</td></tr>
<tr><td><code id="setup_+3A_explain_xreg_lags">explain_xreg_lags</code></td>
<td>
<p>Numeric vector.
If <code>xreg != NULL</code>, denotes the number of lags that should be used for each variable in <code>xreg</code> when making a forecast.</p>
</td></tr>
<tr><td><code id="setup_+3A_group_lags">group_lags</code></td>
<td>
<p>Logical.
If <code>TRUE</code> all lags of each variable are grouped together and explained as a group.
If <code>FALSE</code> all lags of each variable are explained individually.</p>
</td></tr>
<tr><td><code id="setup_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
<tr><td><code id="setup_+3A_iterative">iterative</code></td>
<td>
<p>Logical or NULL
If <code>NULL</code> (default), the argument is set to <code>TRUE</code> if there are more than 5 features/groups, and <code>FALSE</code> otherwise.
If eventually <code>TRUE</code>, the Shapley values are estimated iteratively in an iterative manner.
This provides sufficiently accurate Shapley value estimates faster.
First an initial number of coalitions is sampled, then bootsrapping is used to estimate the variance of the Shapley
values.
A convergence criterion is used to determine if the variances of the Shapley values are sufficiently small.
If the variances are too high, we estimate the number of required samples to reach convergence, and thereby add more
coalitions.
The process is repeated until the variances are below the threshold.
Specifics related to the iterative process and convergence criterion are set through <code>iterative_args</code>.</p>
</td></tr>
<tr><td><code id="setup_+3A_iterative_args">iterative_args</code></td>
<td>
<p>Named list.
Specifies the arguments for the iterative procedure.
See <code><a href="#topic+get_iterative_args_default">get_iterative_args_default()</a></code> for description of the arguments and their default values.</p>
</td></tr>
<tr><td><code id="setup_+3A_is_python">is_python</code></td>
<td>
<p>Logical.
Indicates whether the function is called from the Python wrapper.
Default is FALSE which is never changed when calling the function via <code>explain()</code> in R.
The parameter is later used to disallow running the AICc-versions of the empirical method
as that requires data based optimization, which is not supported in <code>shaprpy</code>.</p>
</td></tr>
<tr><td><code id="setup_+3A_testing">testing</code></td>
<td>
<p>Logical.
Only use to remove random components like timing from the object output when comparing output with testthat.
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="setup_+3A_init_time">init_time</code></td>
<td>
<p>POSIXct object.
The time when the <code>explain()</code> function was called, as outputted by <code>Sys.time()</code>.
Used to calculate the time it took to run the full <code>explain</code> call.</p>
</td></tr>
<tr><td><code id="setup_+3A_prev_shapr_object">prev_shapr_object</code></td>
<td>
<p><code>shapr</code> object or string.
If an object of class <code>shapr</code> is provided, or string with a path to where intermediate results are stored,
then the function will use the previous object to continue the computation.
This is useful if the computation is interrupted or you want higher accuracy than already obtained, and therefore
want to continue the iterative estimation. See the
<a href="https://norskregnesentral.github.io/shapr/articles/general_usage.html">general usage vignette</a> for examples.</p>
</td></tr>
<tr><td><code id="setup_+3A_asymmetric">asymmetric</code></td>
<td>
<p>Logical.
Not applicable for (regular) non-causal or asymmetric explanations.
If <code>FALSE</code> (default), <code>explain</code> computes regular symmetric Shapley values,
If <code>TRUE</code>, then <code>explain</code> compute asymmetric Shapley values based on the (partial) causal ordering
given by <code>causal_ordering</code>. That is, <code>explain</code> only uses the feature combinations/coalitions that
respect the causal ordering when computing the asymmetric Shapley values. If <code>asymmetric</code> is <code>TRUE</code> and
<code>confounding</code> is <code>NULL</code> (default), then <code>explain</code> computes asymmetric conditional Shapley values as specified in
<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf">
Frye et al. (2020)</a>. If <code>confounding</code> is provided, i.e., not <code>NULL</code>, then <code>explain</code> computes asymmetric causal
Shapley values as specified in
<a href="https://proceedings.neurips.cc/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf">
Heskes et al. (2020)</a>.</p>
</td></tr>
<tr><td><code id="setup_+3A_causal_ordering">causal_ordering</code></td>
<td>
<p>List.
Not applicable for (regular) non-causal or asymmetric explanations.
<code>causal_ordering</code> is an unnamed list of vectors specifying the components of the
partial causal ordering that the coalitions must respect. Each vector represents
a component and contains one or more features/groups identified by their names
(strings) or indices (integers). If <code>causal_ordering</code> is <code>NULL</code> (default), no causal
ordering is assumed and all possible coalitions are allowed. No causal ordering is
equivalent to a causal ordering with a single component that includes all features
(<code>list(1:n_features)</code>) or groups (<code>list(1:n_groups)</code>) for feature-wise and group-wise
Shapley values, respectively. For feature-wise Shapley values and
<code>causal_ordering = list(c(1, 2), c(3, 4))</code>, the interpretation is that features 1 and 2
are the ancestors of features 3 and 4, while features 3 and 4 are on the same level.
Note: All features/groups must be included in the <code>causal_ordering</code> without any duplicates.</p>
</td></tr>
<tr><td><code id="setup_+3A_confounding">confounding</code></td>
<td>
<p>Logical vector.
Not applicable for (regular) non-causal or asymmetric explanations.
<code>confounding</code> is a vector of logicals specifying whether confounding is assumed or not for each component in the
<code>causal_ordering</code>. If <code>NULL</code> (default), then no assumption about the confounding structure is made and <code>explain</code>
computes asymmetric/symmetric conditional Shapley values, depending on the value of <code>asymmetric</code>.
If <code>confounding</code> is a single logical, i.e., <code>FALSE</code> or <code>TRUE</code>, then this assumption is set globally
for all components in the causal ordering. Otherwise, <code>confounding</code> must be a vector of logicals of the same
length as <code>causal_ordering</code>, indicating the confounding assumption for each component. When <code>confounding</code> is
specified, then <code>explain</code> computes asymmetric/symmetric causal Shapley values, depending on the value of
<code>asymmetric</code>. The <code>approach</code> cannot be <code>regression_separate</code> and <code>regression_surrogate</code> as the
regression-based approaches are not applicable to the causal Shapley value methodology.</p>
</td></tr>
<tr><td><code id="setup_+3A_output_args">output_args</code></td>
<td>
<p>Named list.
Specifies certain arguments related to the output of the function.
See <code><a href="#topic+get_output_args_default">get_output_args_default()</a></code> for description of the arguments and their default values.</p>
</td></tr>
<tr><td><code id="setup_+3A_extra_computation_args">extra_computation_args</code></td>
<td>
<p>Named list.
Specifies extra arguments related to the computation of the Shapley values.
See <code><a href="#topic+get_extra_comp_args_default">get_extra_comp_args_default()</a></code> for description of the arguments and their default values.</p>
</td></tr>
<tr><td><code id="setup_+3A_...">...</code></td>
<td>
<p>Further arguments passed to specific approaches, see below.</p>
</td></tr>
</table>

<hr>
<h2 id='setup_approach'>Set up the framework chosen approach</h2><span id='topic+setup_approach'></span><span id='topic+setup_approach.combined'></span><span id='topic+setup_approach.categorical'></span><span id='topic+setup_approach.copula'></span><span id='topic+setup_approach.ctree'></span><span id='topic+setup_approach.empirical'></span><span id='topic+setup_approach.gaussian'></span><span id='topic+setup_approach.independence'></span><span id='topic+setup_approach.regression_separate'></span><span id='topic+setup_approach.regression_surrogate'></span><span id='topic+setup_approach.timeseries'></span><span id='topic+setup_approach.vaeac'></span>

<h3>Description</h3>

<p>The different choices of <code>approach</code> take different (optional) parameters,
which are forwarded from <code><a href="#topic+explain">explain()</a></code>.
See the <a href="https://norskregnesentral.github.io/shapr/articles/general_usage.html">general usage vignette</a>
for more information about the different approaches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setup_approach(internal, ...)

## S3 method for class 'combined'
setup_approach(internal, ...)

## S3 method for class 'categorical'
setup_approach(
  internal,
  categorical.joint_prob_dt = NULL,
  categorical.epsilon = 0.001,
  ...
)

## S3 method for class 'copula'
setup_approach(internal, ...)

## S3 method for class 'ctree'
setup_approach(
  internal,
  ctree.mincriterion = 0.95,
  ctree.minsplit = 20,
  ctree.minbucket = 7,
  ctree.sample = TRUE,
  ...
)

## S3 method for class 'empirical'
setup_approach(
  internal,
  empirical.type = "fixed_sigma",
  empirical.eta = 0.95,
  empirical.fixed_sigma = 0.1,
  empirical.n_samples_aicc = 1000,
  empirical.eval_max_aicc = 20,
  empirical.start_aicc = 0.1,
  empirical.cov_mat = NULL,
  model = NULL,
  predict_model = NULL,
  ...
)

## S3 method for class 'gaussian'
setup_approach(internal, gaussian.mu = NULL, gaussian.cov_mat = NULL, ...)

## S3 method for class 'independence'
setup_approach(internal, ...)

## S3 method for class 'regression_separate'
setup_approach(
  internal,
  regression.model = parsnip::linear_reg(),
  regression.tune_values = NULL,
  regression.vfold_cv_para = NULL,
  regression.recipe_func = NULL,
  ...
)

## S3 method for class 'regression_surrogate'
setup_approach(
  internal,
  regression.model = parsnip::linear_reg(),
  regression.tune_values = NULL,
  regression.vfold_cv_para = NULL,
  regression.recipe_func = NULL,
  regression.surrogate_n_comb =
    internal$iter_list[[length(internal$iter_list)]]$n_coalitions - 2,
  ...
)

## S3 method for class 'timeseries'
setup_approach(
  internal,
  timeseries.fixed_sigma = 2,
  timeseries.bounds = c(NULL, NULL),
  ...
)

## S3 method for class 'vaeac'
setup_approach(
  internal,
  vaeac.depth = 3,
  vaeac.width = 32,
  vaeac.latent_dim = 8,
  vaeac.activation_function = torch::nn_relu,
  vaeac.lr = 0.001,
  vaeac.n_vaeacs_initialize = 4,
  vaeac.epochs = 100,
  vaeac.extra_parameters = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setup_approach_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_...">...</code></td>
<td>
<p>Arguments passed to specific classes. See below</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_categorical.joint_prob_dt">categorical.joint_prob_dt</code></td>
<td>
<p>Data.table. (Optional)
Containing the joint probability distribution for each combination of feature
values.
<code>NULL</code> means it is estimated from the <code>x_train</code> and <code>x_explain</code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_categorical.epsilon">categorical.epsilon</code></td>
<td>
<p>Numeric value. (Optional)
If <code>categorical.joint_probability_dt</code> is not supplied, probabilities/frequencies are
estimated using <code>x_train</code>. If certain observations occur in <code>x_explain</code> and NOT in <code>x_train</code>,
then epsilon is used as the proportion of times that these observations occurs in the training data.
In theory, this proportion should be zero, but this causes an error later in the Shapley computation.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_ctree.mincriterion">ctree.mincriterion</code></td>
<td>
<p>Numeric scalar or vector.
Either a scalar or vector of length equal to the number of features in the model.
The value is equal to 1 - <code class="reqn">\alpha</code> where <code class="reqn">\alpha</code> is the nominal level of the conditional independence tests.
If it is a vector, this indicates which value to use when conditioning on various numbers of features.
The default value is 0.95.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_ctree.minsplit">ctree.minsplit</code></td>
<td>
<p>Numeric scalar.
Determines minimum value that the sum of the left and right daughter nodes required for a split.
The default value is 20.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_ctree.minbucket">ctree.minbucket</code></td>
<td>
<p>Numeric scalar.
Determines the minimum sum of weights in a terminal node required for a split
The default value is 7.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_ctree.sample">ctree.sample</code></td>
<td>
<p>Boolean.
If <code>TRUE</code> (default), then the method always samples <code>n_MC_samples</code> observations from the leaf nodes
(with replacement).
If <code>FALSE</code> and the number of observations in the leaf node is less than <code>n_MC_samples</code>,
the method will take all observations in the leaf.
If <code>FALSE</code> and the number of observations in the leaf node is more than <code>n_MC_samples</code>,
the method will sample <code>n_MC_samples</code> observations (with replacement).
This means that there will always be sampling in the leaf unless
<code>sample = FALSE</code> <em>and</em> the number of obs in the node is less than <code>n_MC_samples</code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_empirical.type">empirical.type</code></td>
<td>
<p>Character. (default = <code>"fixed_sigma"</code>)
Should be equal to either <code>"independence"</code>,<code>"fixed_sigma"</code>, <code>"AICc_each_k"</code> <code>"AICc_full"</code>.
<code>"independence"</code> is deprecated. Use <code>approach = "independence"</code> instead.
<code>"fixed_sigma"</code> uses a fixed bandwidth (set through <code>empirical.fixed_sigma</code>) in the kernel density estimation.
<code>"AICc_each_k"</code> and <code>"AICc_full"</code> optimize the bandwidth using the AICc criterion, with respectively
one bandwidth per coalition size and one bandwidth for all coalition sizes.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_empirical.eta">empirical.eta</code></td>
<td>
<p>Numeric scalar.
Needs to be <code style="white-space: pre;">&#8288;0 &lt; eta &lt;= 1&#8288;</code>.
The default value is 0.95.
Represents the minimum proportion of the total empirical weight that data samples should use.
If e.g. <code>eta = .8</code> we will choose the <code>K</code> samples with the largest weight so that the sum of the weights
accounts for 80\
<code>eta</code> is the <code class="reqn">\eta</code> parameter in equation (15) of
<a href="https://martinjullum.com/publication/aas-2021-explaining/aas-2021-explaining.pdf">Aas et al. (2021)</a>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_empirical.fixed_sigma">empirical.fixed_sigma</code></td>
<td>
<p>Positive numeric scalar.
The default value is 0.1.
Represents the kernel bandwidth in the distance computation used when conditioning on all different coalitions.
Only used when <code>empirical.type = "fixed_sigma"</code></p>
</td></tr>
<tr><td><code id="setup_approach_+3A_empirical.n_samples_aicc">empirical.n_samples_aicc</code></td>
<td>
<p>Positive integer.
Number of samples to consider in AICc optimization.
The default value is 1000.
Only used for <code>empirical.type</code> is either <code>"AICc_each_k"</code> or <code>"AICc_full"</code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_empirical.eval_max_aicc">empirical.eval_max_aicc</code></td>
<td>
<p>Positive integer.
Maximum number of iterations when optimizing the AICc.
The default value is 20.
Only used for <code>empirical.type</code> is either <code>"AICc_each_k"</code> or <code>"AICc_full"</code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_empirical.start_aicc">empirical.start_aicc</code></td>
<td>
<p>Numeric.
Start value of the <code>sigma</code> parameter when optimizing the AICc.
The default value is 0.1.
Only used for <code>empirical.type</code> is either <code>"AICc_each_k"</code> or <code>"AICc_full"</code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_empirical.cov_mat">empirical.cov_mat</code></td>
<td>
<p>Numeric matrix. (Optional)
The covariance matrix of the data generating distribution used to define the Mahalanobis distance.
<code>NULL</code> means it is estimated from <code>x_train</code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_model">model</code></td>
<td>
<p>Objects.
The model object that ought to be explained.
See the documentation of <code><a href="#topic+explain">explain()</a></code> for details.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_predict_model">predict_model</code></td>
<td>
<p>Function.
The prediction function used when <code>model</code> is not natively supported.
See the documentation of <code><a href="#topic+explain">explain()</a></code> for details.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_gaussian.mu">gaussian.mu</code></td>
<td>
<p>Numeric vector. (Optional)
Containing the mean of the data generating distribution.
<code>NULL</code> means it is estimated from the <code>x_train</code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_gaussian.cov_mat">gaussian.cov_mat</code></td>
<td>
<p>Numeric matrix. (Optional)
Containing the covariance matrix of the data generating distribution.
<code>NULL</code> means it is estimated from the <code>x_train</code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_regression.model">regression.model</code></td>
<td>
<p>A <code>tidymodels</code> object of class <code>model_specs</code>. Default is a linear regression model, i.e.,
<code><a href="parsnip.html#topic+linear_reg">parsnip::linear_reg()</a></code>. See <a href="https://www.tidymodels.org/find/parsnip/">tidymodels</a> for all possible models,
and see the vignette for how to add new/own models. Note, to make it easier to call <code>explain()</code> from Python, the
<code>regression.model</code> parameter can also be a string specifying the model which will be parsed and evaluated. For
example, <code style="white-space: pre;">&#8288;"parsnip::rand_forest(mtry = hardhat::tune(), trees = 100, engine = "ranger", mode = "regression")"&#8288;</code>
is also a valid input. It is essential to include the package prefix if the package is not loaded.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_regression.tune_values">regression.tune_values</code></td>
<td>
<p>Either <code>NULL</code> (default), a data.frame/data.table/tibble, or a function.
The data.frame must contain the possible hyperparameter value combinations to try.
The column names must match the names of the tunable parameters specified in <code>regression.model</code>.
If <code>regression.tune_values</code> is a function, then it should take one argument <code>x</code> which is the training data
for the current coalition and returns a data.frame/data.table/tibble with the properties described above.
Using a function allows the hyperparameter values to change based on the size of the coalition See the regression
vignette for several examples.
Note, to make it easier to call <code>explain()</code> from Python, the <code>regression.tune_values</code> can also be a string
containing an R function. For example,
<code>"function(x) return(dials::grid_regular(dials::mtry(c(1, ncol(x)))), levels = 3))"</code> is also a valid input.
It is essential to include the package prefix if the package is not loaded.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_regression.vfold_cv_para">regression.vfold_cv_para</code></td>
<td>
<p>Either <code>NULL</code> (default) or a named list containing
the parameters to be sent to <code><a href="rsample.html#topic+vfold_cv">rsample::vfold_cv()</a></code>. See the regression vignette for
several examples.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_regression.recipe_func">regression.recipe_func</code></td>
<td>
<p>Either <code>NULL</code> (default) or a function that that takes in a <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>
object and returns a modified <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code> with potentially additional recipe steps. See the regression
vignette for several examples.
Note, to make it easier to call <code>explain()</code> from Python, the <code>regression.recipe_func</code> can also be a string
containing an R function. For example,
<code>"function(recipe) return(recipes::step_ns(recipe, recipes::all_numeric_predictors(), deg_free = 2))"</code> is also
a valid input. It is essential to include the package prefix if the package is not loaded.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_regression.surrogate_n_comb">regression.surrogate_n_comb</code></td>
<td>
<p>Positive integer.
Specifies the number of unique coalitions to apply to each training observation.
The default is the number of sampled coalitions in the present iteration.
Any integer between 1 and the default is allowed.
Larger values requires more memory, but may improve the surrogate model.
If the user sets a value lower than the maximum, we sample this amount of unique coalitions
separately for each training observations.
That is, on average, all coalitions should be equally trained.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_timeseries.fixed_sigma">timeseries.fixed_sigma</code></td>
<td>
<p>Positive numeric scalar.
Represents the kernel bandwidth in the distance computation.
The default value is 2.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_timeseries.bounds">timeseries.bounds</code></td>
<td>
<p>Numeric vector of length two.
Specifies the lower and upper bounds of the timeseries.
The default is <code>c(NULL, NULL)</code>, i.e. no bounds.
If one or both of these bounds are not <code>NULL</code>, we restrict the sampled time series to be between these bounds.
This is useful if the underlying time series are scaled between 0 and 1, for example.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_vaeac.depth">vaeac.depth</code></td>
<td>
<p>Positive integer (default is <code>3</code>). The number of hidden layers
in the neural networks of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_vaeac.width">vaeac.width</code></td>
<td>
<p>Positive integer (default is <code>32</code>). The number of neurons in each
hidden layer in the neural networks of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_vaeac.latent_dim">vaeac.latent_dim</code></td>
<td>
<p>Positive integer (default is <code>8</code>). The number of dimensions in the latent space.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_vaeac.activation_function">vaeac.activation_function</code></td>
<td>
<p>An <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> representing an activation function such as, e.g.,
<code><a href="torch.html#topic+nn_relu">torch::nn_relu()</a></code> (default), <code><a href="torch.html#topic+nn_leaky_relu">torch::nn_leaky_relu()</a></code>, <code><a href="torch.html#topic+nn_selu">torch::nn_selu()</a></code>, or <code><a href="torch.html#topic+nn_sigmoid">torch::nn_sigmoid()</a></code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_vaeac.lr">vaeac.lr</code></td>
<td>
<p>Positive numeric (default is <code>0.001</code>). The learning rate used in the <code><a href="torch.html#topic+optim_adam">torch::optim_adam()</a></code> optimizer.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_vaeac.n_vaeacs_initialize">vaeac.n_vaeacs_initialize</code></td>
<td>
<p>Positive integer (default is <code>4</code>). The number of different vaeac models to initiate
in the start. Pick the best performing one after <code>vaeac.extra_parameters$epochs_initiation_phase</code>
epochs (default is <code>2</code>) and continue training that one.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_vaeac.epochs">vaeac.epochs</code></td>
<td>
<p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>vaeac.extra_parameters$epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="setup_approach_+3A_vaeac.extra_parameters">vaeac.extra_parameters</code></td>
<td>
<p>Named list with extra parameters to the <code>vaeac</code> approach. See
<code><a href="#topic+vaeac_get_extra_para_default">vaeac_get_extra_para_default()</a></code> for description of possible additional parameters and their default values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>
<p>Lars Henry Berge Olsen
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://martinjullum.com/publication/aas-2021-explaining/aas-2021-explaining.pdf">
Aas, K., Jullum, M., &amp; Løland, A. (2021). Explaining individual predictions when features are dependent:
More accurate approximations to Shapley values. Artificial Intelligence, 298, 103502</a>
</p>
</li></ul>


<hr>
<h2 id='shapley_setup'>Set up the kernelSHAP framework</h2><span id='topic+shapley_setup'></span>

<h3>Description</h3>

<p>Set up the kernelSHAP framework
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapley_setup(internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shapley_setup_+3A_internal">internal</code></td>
<td>
<p>List.
Not used directly, but passed through from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='shapley_weights'>Calculate Shapley weight</h2><span id='topic+shapley_weights'></span>

<h3>Description</h3>

<p>Calculate Shapley weight
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapley_weights(m, N, n_components, weight_zero_m = 10^6)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shapley_weights_+3A_m">m</code></td>
<td>
<p>Positive integer.
Total number of features/groups.</p>
</td></tr>
<tr><td><code id="shapley_weights_+3A_n">N</code></td>
<td>
<p>Positive integer. The number of unique coalitions when sampling <code>n_components</code> features/feature
groups, without replacement, from a sample space consisting of <code>m</code> different features/feature groups.</p>
</td></tr>
<tr><td><code id="shapley_weights_+3A_n_components">n_components</code></td>
<td>
<p>Positive integer. Represents the number of features/feature groups you want to sample from
a feature space consisting of <code>m</code> unique features/feature groups. Note that <code style="white-space: pre;">&#8288; 0 &lt; = n_components &lt;= m&#8288;</code>.</p>
</td></tr>
<tr><td><code id="shapley_weights_+3A_weight_zero_m">weight_zero_m</code></td>
<td>
<p>Numeric.
The value to use as a replacement for infinite coalition weights when doing numerical operations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>

<hr>
<h2 id='skip_connection'>A <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> Representing a skip connection</h2><span id='topic+skip_connection'></span>

<h3>Description</h3>

<p>Skip connection over the sequence of layers in the constructor. The module passes
input data sequentially through these layers and then adds original data to the result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skip_connection(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="skip_connection_+3A_...">...</code></td>
<td>
<p>network modules such as, e.g., <code><a href="torch.html#topic+nn_linear">torch::nn_linear()</a></code>, <code><a href="torch.html#topic+nn_relu">torch::nn_relu()</a></code>,
and <code><a href="#topic+memory_layer">memory_layer()</a></code> objects. See <code><a href="#topic+vaeac">vaeac()</a></code> for more information.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='specified_masks_mask_generator'>A <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> Representing a specified_masks_mask_generator</h2><span id='topic+specified_masks_mask_generator'></span>

<h3>Description</h3>

<p>A mask generator which masks the entries based on sampling provided 1D masks with corresponding probabilities.
Used for Shapley value estimation when only a subset of coalitions are used to compute the Shapley values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>specified_masks_mask_generator(masks, masks_probs, paired_sampling = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="specified_masks_mask_generator_+3A_masks">masks</code></td>
<td>
<p>Matrix/Tensor of possible/allowed 'masks' which we sample from.</p>
</td></tr>
<tr><td><code id="specified_masks_mask_generator_+3A_masks_probs">masks_probs</code></td>
<td>
<p>Array of 'probabilities' for each of the masks specified in 'masks'.
Note that they do not need to be between 0 and 1 (e.g. sampling frequency).
They are scaled, hence, they only need to be positive.</p>
</td></tr>
<tr><td><code id="specified_masks_mask_generator_+3A_paired_sampling">paired_sampling</code></td>
<td>
<p>Boolean. If we are doing paired sampling. So include both S and <code class="reqn">\bar{S}</code>.
If TRUE, then batch must be sampled using 'paired_sampler' which creates batches where
the first half and second half of the rows are duplicates of each other. That is,
<code style="white-space: pre;">&#8288;batch = [row1, row1, row2, row2, row3, row3, ...]&#8288;</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
masks &lt;- torch_tensor(matrix(c(0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1),
  nrow = 3, ncol = 4, byrow = TRUE
))
masks_probs &lt;- c(3, 1, 6)
mask_gen &lt;- specified_masks_mask_generator(masks = masks, masks_probs = masks_probs)
empirical_prob &lt;-
  table(as.array(mask_gen(torch::torch_randn(c(10000, ncol(masks))))$sum(-1)))
empirical_prob / sum(empirical_prob)
masks_probs / sum(masks_probs)

## End(Not run)

</code></pre>

<hr>
<h2 id='specified_prob_mask_generator'>A <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> Representing a specified_prob_mask_generator</h2><span id='topic+specified_prob_mask_generator'></span>

<h3>Description</h3>

<p>A mask generator which masks the entries based on specified probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>specified_prob_mask_generator(masking_probs, paired_sampling = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="specified_prob_mask_generator_+3A_masking_probs">masking_probs</code></td>
<td>
<p>An M+1 numerics containing the probabilities masking 'd' of the (0,...M) entries
for each observation.</p>
</td></tr>
<tr><td><code id="specified_prob_mask_generator_+3A_paired_sampling">paired_sampling</code></td>
<td>
<p>Boolean. If we are doing paired sampling. So include both S and <code class="reqn">\bar{S}</code>.
If TRUE, then batch must be sampled using 'paired_sampler' which creates batches where
the first half and second half of the rows are duplicates of each other. That is,
<code style="white-space: pre;">&#8288;batch = [row1, row1, row2, row2, row3, row3, ...]&#8288;</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A class that takes in the probabilities of having d masked observations.  I.e., for M dimensional data,
masking_probs is of length M+1, where the d'th entry is the probability of having d-1 masked values.
</p>
<p>A mask generator that first samples the number of entries 'd' to be masked in the 'M'-dimensional observation 'x' in
the batch based on the given M+1 probabilities. The 'd' masked are uniformly sampled from the 'M' possible feature
indices. The d'th entry of the probability of having d-1 masked values.
</p>
<p>Note that mcar_mask_generator with p = 0.5 is the same as using <code><a href="#topic+specified_prob_mask_generator">specified_prob_mask_generator()</a></code> with
<code>masking_ratio</code> = choose(M, 0:M), where M is the number of features. This function was initially created to check if
increasing the probability of having a masks with many masked features improved vaeac's performance by focusing more
on these situations during training.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
probs &lt;- c(1, 8, 6, 3, 2)
mask_gen &lt;- specified_prob_mask_generator(probs)
masks &lt;- mask_gen(torch::torch_randn(c(10000, length(probs)) - 1))
empirical_prob &lt;- table(as.array(masks$sum(2)))
empirical_prob / sum(empirical_prob)
probs / sum(probs)

## End(Not run)

</code></pre>

<hr>
<h2 id='test_predict_model'>Model testing function</h2><span id='topic+test_predict_model'></span>

<h3>Description</h3>

<p>Model testing function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_predict_model(x_test, predict_model, model, internal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="test_predict_model_+3A_predict_model">predict_model</code></td>
<td>
<p>Function.
The prediction function used when <code>model</code> is not natively supported.
See the documentation of <code><a href="#topic+explain">explain()</a></code> for details.</p>
</td></tr>
<tr><td><code id="test_predict_model_+3A_model">model</code></td>
<td>
<p>Objects.
The model object that ought to be explained.
See the documentation of <code><a href="#topic+explain">explain()</a></code> for details.</p>
</td></tr>
<tr><td><code id="test_predict_model_+3A_internal">internal</code></td>
<td>
<p>List.
Holds all parameters, data, functions and computed objects used within <code><a href="#topic+explain">explain()</a></code>
The list contains one or more of the elements <code>parameters</code>, <code>data</code>, <code>objects</code>, <code>iter_list</code>, <code>timing_list</code>,
<code>main_timing_list</code>, <code>output</code>, and <code>iter_timing_list</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='testing_cleanup'>Cleans out certain output arguments to allow perfect reproducibility of the output</h2><span id='topic+testing_cleanup'></span>

<h3>Description</h3>

<p>Cleans out certain output arguments to allow perfect reproducibility of the output
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testing_cleanup(output)
</code></pre>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen, Martin Jullum
</p>

<hr>
<h2 id='vaeac'>Initializing a vaeac model</h2><span id='topic+vaeac'></span>

<h3>Description</h3>

<p>Class that represents a vaeac model, i.e., the class creates the neural networks in the vaeac
model and necessary training utilities.
For more details, see <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac(
  one_hot_max_sizes,
  width = 32,
  depth = 3,
  latent_dim = 8,
  activation_function = torch::nn_relu,
  skip_conn_layer = FALSE,
  skip_conn_masked_enc_dec = FALSE,
  batch_normalization = FALSE,
  paired_sampling = FALSE,
  mask_generator_name = c("mcar_mask_generator", "specified_prob_mask_generator",
    "specified_masks_mask_generator"),
  masking_ratio = 0.5,
  mask_gen_coalitions = NULL,
  mask_gen_coalitions_prob = NULL,
  sigma_mu = 10000,
  sigma_sigma = 1e-04
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_+3A_one_hot_max_sizes">one_hot_max_sizes</code></td>
<td>
<p>A torch tensor of dimension <code>n_features</code> containing the one hot sizes of the <code>n_features</code>
features. That is, if the <code>i</code>th feature is a categorical feature with 5 levels, then <code>one_hot_max_sizes[i] = 5</code>.
While the size for continuous features can either be <code>0</code> or <code>1</code>.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_width">width</code></td>
<td>
<p>Integer. The number of neurons in each hidden layer in the neural networks
of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_depth">depth</code></td>
<td>
<p>Integer. The number of hidden layers in the neural networks of the
masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_latent_dim">latent_dim</code></td>
<td>
<p>Integer. The number of dimensions in the latent space.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_activation_function">activation_function</code></td>
<td>
<p>A <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> representing an activation function such as, e.g.,
<code><a href="torch.html#topic+nn_relu">torch::nn_relu()</a></code>, <code><a href="torch.html#topic+nn_leaky_relu">torch::nn_leaky_relu()</a></code>, <code><a href="torch.html#topic+nn_selu">torch::nn_selu()</a></code>,
<code><a href="torch.html#topic+nn_sigmoid">torch::nn_sigmoid()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_skip_conn_layer">skip_conn_layer</code></td>
<td>
<p>Boolean. If we are to use skip connections in each layer, see <code><a href="#topic+skip_connection">skip_connection()</a></code>.
If <code>TRUE</code>, then we add the input to the outcome of each hidden layer, so the output becomes
<code class="reqn">X + \operatorname{activation}(WX + b)</code>. I.e., the identity skip connection.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_skip_conn_masked_enc_dec">skip_conn_masked_enc_dec</code></td>
<td>
<p>Boolean. If we are to apply concatenating skip
connections between the layers in the masked encoder and decoder. The first layer of the masked encoder will be
linked to the last layer of the decoder. The second layer of the masked encoder will be
linked to the second to last layer of the decoder, and so on.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_batch_normalization">batch_normalization</code></td>
<td>
<p>Boolean. If we are to use batch normalization after the activation function.
Note that if <code>skip_conn_layer</code> is TRUE, then the normalization is
done after the adding from the skip connection. I.e, we batch normalize the whole quantity X + activation(WX + b).</p>
</td></tr>
<tr><td><code id="vaeac_+3A_paired_sampling">paired_sampling</code></td>
<td>
<p>Boolean. If we are doing paired sampling. I.e., if we are to include both coalition S
and <code class="reqn">\bar{S}</code> when we sample coalitions during training for each batch.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_mask_generator_name">mask_generator_name</code></td>
<td>
<p>String specifying the type of mask generator to use. Need to be one of
'mcar_mask_generator', 'specified_prob_mask_generator', and 'specified_masks_mask_generator'.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_masking_ratio">masking_ratio</code></td>
<td>
<p>Scalar. The probability for an entry in the generated mask to be 1 (masked).
Not used if <code>mask_gen_coalitions</code> is given.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_mask_gen_coalitions">mask_gen_coalitions</code></td>
<td>
<p>Matrix containing the different coalitions to learn.
Must be given if <code>mask_generator_name = 'specified_masks_mask_generator'</code>.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_mask_gen_coalitions_prob">mask_gen_coalitions_prob</code></td>
<td>
<p>Numerics containing the probabilities
for sampling each mask in <code>mask_gen_coalitions</code>.
Array containing the probabilities for sampling the coalitions in <code>mask_gen_coalitions</code>.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_sigma_mu">sigma_mu</code></td>
<td>
<p>Numeric representing a hyperparameter in the normal-gamma prior used on the masked encoder,
see Section 3.3.1 in <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.</p>
</td></tr>
<tr><td><code id="vaeac_+3A_sigma_sigma">sigma_sigma</code></td>
<td>
<p>Numeric representing a hyperparameter in the normal-gamma prior used on the masked encoder,
see Section 3.3.1 in <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function builds neural networks (masked encoder, full encoder, decoder) given
the list of one-hot max sizes of the features in the dataset we use to train the vaeac model,
and the provided parameters for the networks. It also creates, e.g., reconstruction log probability function,
methods for sampling from the decoder output, and then use these to create the vaeac model.
</p>


<h3>Value</h3>

<p>Returns a list with the neural networks of the masked encoder, full encoder, and decoder together
with reconstruction log probability function, optimizer constructor, sampler from the decoder output,
mask generator, batch size, and scale factor for the stability of the variational lower bound optimization.
</p>


<h3>make_observed</h3>

<p>Apply Mask to Batch to Create Observed Batch
</p>
<p>Compute the parameters for the latent normal distributions inferred by the encoders.
If <code>only_masked_encoder = TRUE</code>, then we only compute the latent normal distributions inferred by the
masked encoder. This is used in the deployment phase when we do not have access to the full observation.
</p>


<h3>make_latent_distributions</h3>

<p>Compute the Latent Distributions Inferred by the Encoders
</p>
<p>Compute the parameters for the latent normal distributions inferred by the encoders.
If <code>only_masked_encoder = TRUE</code>, then we only compute the latent normal distributions inferred by the
masked encoder. This is used in the deployment phase when we do not have access to the full observation.
</p>


<h3>masked_encoder_regularization</h3>

<p>Compute the Regularizes for the Latent Distribution Inferred by the Masked Encoder.
</p>
<p>The masked encoder (prior) distribution regularization in the latent space.
This is used to compute the extended variational lower bound used to train vaeac, see
Section 3.3.1 in <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.
Though regularizing prevents the masked encoder distribution parameters from going to infinity,
the model usually doesn't diverge even without this regularization. It almost doesn't affect
learning process near zero with default regularization parameters which are recommended to be used.
</p>


<h3>batch_vlb</h3>

<p>Compute the Variational Lower Bound for the Observations in the Batch
</p>
<p>Compute differentiable lower bound for the given batch of objects and mask.
Used as the (negative) loss function for training the vaeac model.
</p>


<h3>batch_iwae</h3>

<p>Compute IWAE log likelihood estimate with K samples per object.
</p>
<p>Technically, it is differentiable, but it is recommended to use it for
evaluation purposes inside torch.no_grad in order to save memory. With <code><a href="torch.html#topic+with_no_grad">torch::with_no_grad()</a></code>
the method almost doesn't require extra memory for very large K. The method makes K independent
passes through decoder network, so the batch size is the same as for training with batch_vlb.
IWAE is an abbreviation for Importance Sampling Estimator:
</p>
<p style="text-align: center;"><code class="reqn">
\log p_{\theta, \psi}(x|y) \approx
\log {\frac{1}{K} \sum_{i=1}^K [p_\theta(x|z_i, y) * p_\psi(z_i|y) / q_\phi(z_i|x,y)]} \newline
=
\log {\sum_{i=1}^K \exp(\log[p_\theta(x|z_i, y) * p_\psi(z_i|y) / q_\phi(z_i|x,y)])} - \log(K) \newline
=
\log {\sum_{i=1}^K \exp(\log[p_\theta(x|z_i, y)] + \log[p_\psi(z_i|y)] - \log[q_\phi(z_i|x,y)])} - \log(K) \newline
=
\operatorname{logsumexp}(\log[p_\theta(x|z_i, y)] + \log[p_\psi(z_i|y)] - \log[q_\phi(z_i|x,y)]) - \log(K) \newline
=
\operatorname{logsumexp}(\text{rec}\_\text{loss} + \text{prior}\_\text{log}\_\text{prob} -
 \text{proposal}\_\text{log}\_\text{prob}) - \log(K),</code>
</p>

<p>where <code class="reqn">z_i \sim q_\phi(z|x,y)</code>.
</p>


<h3>generate_samples_params</h3>

<p>Generate the parameters of the generative distributions for samples from the batch.
</p>
<p>The function makes K latent representation for each object from the batch, send these
latent representations through the decoder to obtain the parameters for the generative distributions.
I.e., means and variances for the normal distributions (continuous features) and probabilities
for the categorical distribution (categorical features).
The second axis is used to index samples for an object, i.e. if the batch shape is [n x D1 x D2], then
the result shape is [n x K x D1 x D2]. It is better to use it inside <code><a href="torch.html#topic+with_no_grad">torch::with_no_grad()</a></code> in order to save
memory. With <code><a href="torch.html#topic+with_no_grad">torch::with_no_grad()</a></code> the method doesn't require extra memory except the memory for the result.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_categorical_parse_params'>Creates Categorical Distributions</h2><span id='topic+vaeac_categorical_parse_params'></span>

<h3>Description</h3>

<p>Function that takes in a tensor containing the logits for each of the K classes. Each row corresponds to
an observations. Send each row through the softmax function to convert from logits to probabilities that sum 1 one.
The function also clamps the probabilities between a minimum and maximum probability. Note that we still normalize
them afterward, so the final probabilities can be marginally below or above the thresholds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_categorical_parse_params(params, min_prob = 0, max_prob = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_categorical_parse_params_+3A_params">params</code></td>
<td>
<p>Tensor of dimension <code>batch_size</code> x <code>K</code> containing the logits for each of the <code>K</code> classes and
<code>batch_size</code> observations.</p>
</td></tr>
<tr><td><code id="vaeac_categorical_parse_params_+3A_min_prob">min_prob</code></td>
<td>
<p>For stability it might be desirable that the minimal probability is not too close to zero.</p>
</td></tr>
<tr><td><code id="vaeac_categorical_parse_params_+3A_max_prob">max_prob</code></td>
<td>
<p>For stability it might be desirable that the maximal probability is not too close to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Take a Tensor (e. g. a part of neural network output) and return <code><a href="torch.html#topic+distr_categorical">torch::distr_categorical()</a></code>
distribution. The input tensor after applying softmax over the last axis contains a batch of the categorical
probabilities. So there are no restrictions on the input tensor. Technically, this function treats the last axis as
the categorical probabilities, but Categorical takes only 2D input where the first axis is the batch axis and the
second one corresponds to the probabilities, so practically the function requires 2D input with the batch of
probabilities for one categorical feature. <code>min_prob</code> is the minimal probability for each class.
After clipping the probabilities from below and above they are renormalized in order to be a valid distribution.
This regularization is required for the numerical stability and may be considered as a neural network architecture
choice without any change to the probabilistic model.Note that the softmax function is given by
<code class="reqn">\operatorname{Softmax}(x_i) = (\exp(x_i))/(\sum_{j} \exp(x_j))</code>, where <code class="reqn">x_i</code> are the logits and can
take on any value, negative and positive. The output <code class="reqn">\operatorname{Softmax}(x_i) \in [0,1]</code>
and <code class="reqn">\sum_{j} Softmax(x_i) = 1</code>.
</p>


<h3>Value</h3>

<p>A <a href="torch.html#topic+distr_categorical">torch::distr_categorical</a> distributions with the provided probabilities for each class.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_activation_func'>Function that checks the provided activation function</h2><span id='topic+vaeac_check_activation_func'></span>

<h3>Description</h3>

<p>Function that checks the provided activation function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_activation_func(activation_function)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_activation_func_+3A_activation_function">activation_function</code></td>
<td>
<p>An <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> representing an activation function such as, e.g.,
<code><a href="torch.html#topic+nn_relu">torch::nn_relu()</a></code> (default), <code><a href="torch.html#topic+nn_leaky_relu">torch::nn_leaky_relu()</a></code>, <code><a href="torch.html#topic+nn_selu">torch::nn_selu()</a></code>, or <code><a href="torch.html#topic+nn_sigmoid">torch::nn_sigmoid()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_cuda'>Function that checks for access to CUDA</h2><span id='topic+vaeac_check_cuda'></span>

<h3>Description</h3>

<p>Function that checks for access to CUDA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_cuda(cuda)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_cuda_+3A_cuda">cuda</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the <code>vaeac</code> model will be trained using cuda/GPU.
If <code><a href="torch.html#topic+cuda_is_available">torch::cuda_is_available()</a></code> is <code>FALSE</code>, the we fall back to use CPU. If <code>FALSE</code>, we use the CPU. Using a GPU
for smaller tabular dataset often do not improve the efficiency.
See <code>vignette("installation", package = "torch")</code> fo help to enable running on the GPU (only Linux and Windows).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_epoch_values'>Function that checks provided epoch arguments</h2><span id='topic+vaeac_check_epoch_values'></span>

<h3>Description</h3>

<p>Function that checks provided epoch arguments
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_epoch_values(
  epochs,
  epochs_initiation_phase,
  epochs_early_stopping,
  save_every_nth_epoch
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_epoch_values_+3A_epochs">epochs</code></td>
<td>
<p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_epoch_values_+3A_epochs_initiation_phase">epochs_initiation_phase</code></td>
<td>
<p>Positive integer (default is <code>2</code>). The number of epochs to run each of the
<code>n_vaeacs_initialize</code> <code>vaeac</code> models before continuing to train only the best performing model.</p>
</td></tr>
<tr><td><code id="vaeac_check_epoch_values_+3A_epochs_early_stopping">epochs_early_stopping</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). The training stops if there has been no
improvement in the validation IWAE for <code>epochs_early_stopping</code> epochs. If the user wants the training process
to be solely based on this training criterion, then <code>epochs</code> in <code><a href="#topic+explain">explain()</a></code> should be set to a large
number. If <code>NULL</code>, then <code>shapr</code> will internally set <code>epochs_early_stopping = vaeac.epochs</code> such that early
stopping does not occur.</p>
</td></tr>
<tr><td><code id="vaeac_check_epoch_values_+3A_save_every_nth_epoch">save_every_nth_epoch</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). If provided, then the vaeac model after
every <code>save_every_nth_epoch</code>th epoch will be saved.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_extra_named_list'>Check vaeac.extra_parameters list</h2><span id='topic+vaeac_check_extra_named_list'></span>

<h3>Description</h3>

<p>Check vaeac.extra_parameters list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_extra_named_list(vaeac.extra_parameters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_extra_named_list_+3A_vaeac.extra_parameters">vaeac.extra_parameters</code></td>
<td>
<p>List containing the extra parameters to the <code>vaeac</code> approach</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_logicals'>Function that checks logicals</h2><span id='topic+vaeac_check_logicals'></span>

<h3>Description</h3>

<p>Function that checks logicals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_logicals(named_list_logicals)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_logicals_+3A_named_list_logicals">named_list_logicals</code></td>
<td>
<p>List containing named entries. I.e., <code>list(a = TRUE, b = FALSE)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_mask_gen'>Function that checks the specified masking scheme</h2><span id='topic+vaeac_check_mask_gen'></span>

<h3>Description</h3>

<p>Function that checks the specified masking scheme
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_mask_gen(mask_gen_coalitions, mask_gen_coalitions_prob, x_train)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_mask_gen_+3A_mask_gen_coalitions">mask_gen_coalitions</code></td>
<td>
<p>Matrix (default is <code>NULL</code>). Matrix containing the coalitions that the
<code>vaeac</code> model will be trained on, see <code><a href="#topic+specified_masks_mask_generator">specified_masks_mask_generator()</a></code>. This parameter is used internally
in <code>shapr</code> when we only consider a subset of coalitions, i.e., when
<code>n_coalitions</code> <code class="reqn">&lt; 2^{n_{\text{features}}}</code>, and for group Shapley, i.e.,
when <code>group</code> is specified in <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_mask_gen_+3A_mask_gen_coalitions_prob">mask_gen_coalitions_prob</code></td>
<td>
<p>Numeric array (default is <code>NULL</code>). Array of length equal to the height
of <code>mask_gen_coalitions</code> containing the probabilities of sampling the corresponding coalitions in
<code>mask_gen_coalitions</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_mask_gen_+3A_x_train">x_train</code></td>
<td>
<p>A data.table containing the training data. Categorical data must have class names <code class="reqn">1,2,\dots,K</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_masking_ratio'>Function that checks that the masking ratio argument is valid</h2><span id='topic+vaeac_check_masking_ratio'></span>

<h3>Description</h3>

<p>Function that checks that the masking ratio argument is valid
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_masking_ratio(masking_ratio, n_features)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_masking_ratio_+3A_masking_ratio">masking_ratio</code></td>
<td>
<p>Numeric (default is <code>0.5</code>). Probability of masking a feature in the
<code><a href="#topic+mcar_mask_generator">mcar_mask_generator()</a></code> (MCAR = Missing Completely At Random). The MCAR masking scheme ensures that <code>vaeac</code>
model can do arbitrary conditioning as all coalitions will be trained. <code>masking_ratio</code> will be overruled if
<code>mask_gen_coalitions</code> is specified.</p>
</td></tr>
<tr><td><code id="vaeac_check_masking_ratio_+3A_n_features">n_features</code></td>
<td>
<p>The number of features, i.e., the number of columns in the training data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_parameters'>Function that calls all vaeac parameters check functions</h2><span id='topic+vaeac_check_parameters'></span>

<h3>Description</h3>

<p>Function that calls all vaeac parameters check functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_parameters(
  x_train,
  model_description,
  folder_to_save_model,
  cuda,
  n_vaeacs_initialize,
  epochs_initiation_phase,
  epochs,
  epochs_early_stopping,
  save_every_nth_epoch,
  val_ratio,
  val_iwae_n_samples,
  depth,
  width,
  latent_dim,
  lr,
  batch_size,
  running_avg_n_values,
  activation_function,
  skip_conn_layer,
  skip_conn_masked_enc_dec,
  batch_normalization,
  paired_sampling,
  masking_ratio,
  mask_gen_coalitions,
  mask_gen_coalitions_prob,
  sigma_mu,
  sigma_sigma,
  save_data,
  log_exp_cont_feat,
  which_vaeac_model,
  verbose,
  seed,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_parameters_+3A_x_train">x_train</code></td>
<td>
<p>A data.table containing the training data. Categorical data must have class names <code class="reqn">1,2,\dots,K</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_model_description">model_description</code></td>
<td>
<p>String (default is <code>make.names(Sys.time())</code>). String containing, e.g., the name of the
data distribution or additional parameter information. Used in the save name of the fitted model. If not provided,
then a name will be generated based on <code><a href="base.html#topic+Sys.time">base::Sys.time()</a></code> to ensure a unique name. We use <code><a href="base.html#topic+make.names">base::make.names()</a></code> to
ensure a valid file name for all operating systems.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_folder_to_save_model">folder_to_save_model</code></td>
<td>
<p>String (default is <code><a href="base.html#topic+tempfile">base::tempdir()</a></code>). String specifying a path to a folder where
the function is to save the fitted vaeac model. Note that  the path will be removed from the returned
<code><a href="#topic+explain">explain()</a></code> object if <code>vaeac.save_model = FALSE</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_cuda">cuda</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the <code>vaeac</code> model will be trained using cuda/GPU.
If <code><a href="torch.html#topic+cuda_is_available">torch::cuda_is_available()</a></code> is <code>FALSE</code>, the we fall back to use CPU. If <code>FALSE</code>, we use the CPU. Using a GPU
for smaller tabular dataset often do not improve the efficiency.
See <code>vignette("installation", package = "torch")</code> fo help to enable running on the GPU (only Linux and Windows).</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_n_vaeacs_initialize">n_vaeacs_initialize</code></td>
<td>
<p>Positive integer (default is <code>4</code>). The number of different vaeac models to initiate
in the start. Pick the best performing one after <code>epochs_initiation_phase</code>
epochs (default is <code>2</code>) and continue training that one.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_epochs_initiation_phase">epochs_initiation_phase</code></td>
<td>
<p>Positive integer (default is <code>2</code>). The number of epochs to run each of the
<code>n_vaeacs_initialize</code> <code>vaeac</code> models before continuing to train only the best performing model.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_epochs">epochs</code></td>
<td>
<p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_epochs_early_stopping">epochs_early_stopping</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). The training stops if there has been no
improvement in the validation IWAE for <code>epochs_early_stopping</code> epochs. If the user wants the training process
to be solely based on this training criterion, then <code>epochs</code> in <code><a href="#topic+explain">explain()</a></code> should be set to a large
number. If <code>NULL</code>, then <code>shapr</code> will internally set <code>epochs_early_stopping = vaeac.epochs</code> such that early
stopping does not occur.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_save_every_nth_epoch">save_every_nth_epoch</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). If provided, then the vaeac model after
every <code>save_every_nth_epoch</code>th epoch will be saved.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_val_ratio">val_ratio</code></td>
<td>
<p>Numeric (default is <code>0.25</code>). Scalar between <code>0</code> and <code>1</code> indicating the ratio of
instances from the input data which will be used as validation data. That is, <code>val_ratio = 0.25</code> means
that <code style="white-space: pre;">&#8288;75%&#8288;</code> of the provided data is used as training data, while the remaining <code style="white-space: pre;">&#8288;25%&#8288;</code> is used as validation data.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_val_iwae_n_samples">val_iwae_n_samples</code></td>
<td>
<p>Positive integer (default is <code>25</code>). The number of generated samples used
to compute the IWAE criterion when validating the vaeac model on the validation data.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_depth">depth</code></td>
<td>
<p>Positive integer (default is <code>3</code>). The number of hidden layers
in the neural networks of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_width">width</code></td>
<td>
<p>Positive integer (default is <code>32</code>). The number of neurons in each
hidden layer in the neural networks of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_latent_dim">latent_dim</code></td>
<td>
<p>Positive integer (default is <code>8</code>). The number of dimensions in the latent space.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_lr">lr</code></td>
<td>
<p>Positive numeric (default is <code>0.001</code>). The learning rate used in the <code><a href="torch.html#topic+optim_adam">torch::optim_adam()</a></code> optimizer.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_batch_size">batch_size</code></td>
<td>
<p>Positive integer (default is <code>64</code>). The number of samples to include in each batch
during the training of the vaeac model. Used in <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_running_avg_n_values">running_avg_n_values</code></td>
<td>
<p>running_avg_n_values Positive integer (default is <code>5</code>).
The number of previous IWAE values to include
when we compute the running means of the IWAE criterion.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_activation_function">activation_function</code></td>
<td>
<p>An <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> representing an activation function such as, e.g.,
<code><a href="torch.html#topic+nn_relu">torch::nn_relu()</a></code> (default), <code><a href="torch.html#topic+nn_leaky_relu">torch::nn_leaky_relu()</a></code>, <code><a href="torch.html#topic+nn_selu">torch::nn_selu()</a></code>, or <code><a href="torch.html#topic+nn_sigmoid">torch::nn_sigmoid()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_skip_conn_layer">skip_conn_layer</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, we apply identity skip connections in each
layer, see <code><a href="#topic+skip_connection">skip_connection()</a></code>. That is, we add the input <code class="reqn">X</code> to the outcome of each hidden layer,
so the output becomes <code class="reqn">X + activation(WX + b)</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_skip_conn_masked_enc_dec">skip_conn_masked_enc_dec</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, we apply concatenate skip
connections between the layers in the masked encoder and decoder. The first layer of the masked encoder will be
linked to the last layer of the decoder. The second layer of the masked encoder will be
linked to the second to last layer of the decoder, and so on.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_batch_normalization">batch_normalization</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, we apply batch normalization after the
activation function. Note that if <code>skip_conn_layer = TRUE</code>, then the normalization is applied after the
inclusion of the skip connection. That is, we batch normalize the whole quantity <code class="reqn">X + activation(WX + b)</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_paired_sampling">paired_sampling</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, we apply paired sampling to the training
batches. That is, the training observations in each batch will be duplicated, where the first instance will be masked
by <code class="reqn">S</code> while the second instance will be masked by <code class="reqn">\bar{S}</code>. This ensures that the training of the
<code>vaeac</code> model becomes more stable as the model has access to the full version of each training observation. However,
this will increase the training time due to more complex implementation and doubling the size of each batch. See
<code><a href="#topic+paired_sampler">paired_sampler()</a></code> for more information.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_masking_ratio">masking_ratio</code></td>
<td>
<p>Numeric (default is <code>0.5</code>). Probability of masking a feature in the
<code><a href="#topic+mcar_mask_generator">mcar_mask_generator()</a></code> (MCAR = Missing Completely At Random). The MCAR masking scheme ensures that <code>vaeac</code>
model can do arbitrary conditioning as all coalitions will be trained. <code>masking_ratio</code> will be overruled if
<code>mask_gen_coalitions</code> is specified.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_mask_gen_coalitions">mask_gen_coalitions</code></td>
<td>
<p>Matrix (default is <code>NULL</code>). Matrix containing the coalitions that the
<code>vaeac</code> model will be trained on, see <code><a href="#topic+specified_masks_mask_generator">specified_masks_mask_generator()</a></code>. This parameter is used internally
in <code>shapr</code> when we only consider a subset of coalitions, i.e., when
<code>n_coalitions</code> <code class="reqn">&lt; 2^{n_{\text{features}}}</code>, and for group Shapley, i.e.,
when <code>group</code> is specified in <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_mask_gen_coalitions_prob">mask_gen_coalitions_prob</code></td>
<td>
<p>Numeric array (default is <code>NULL</code>). Array of length equal to the height
of <code>mask_gen_coalitions</code> containing the probabilities of sampling the corresponding coalitions in
<code>mask_gen_coalitions</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_sigma_mu">sigma_mu</code></td>
<td>
<p>Numeric (default is <code>1e4</code>). One of two hyperparameter values in the normal-gamma prior
used in the masked encoder, see Section 3.3.1 in
<a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_sigma_sigma">sigma_sigma</code></td>
<td>
<p>Numeric (default is <code>1e-4</code>). One of two hyperparameter values in the normal-gamma prior
used in the masked encoder, see Section 3.3.1 in
<a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_save_data">save_data</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the data is stored together with
the model. Useful if one are to continue to train the model later using <code><a href="#topic+vaeac_train_model_continue">vaeac_train_model_continue()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_log_exp_cont_feat">log_exp_cont_feat</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If we are to <code class="reqn">\log</code> transform all
continuous features before sending the data to <code><a href="#topic+vaeac">vaeac()</a></code>. The <code>vaeac</code> model creates unbounded Monte Carlo
sample values. Thus, if the continuous features are strictly positive (as for, e.g., the Burr distribution and
Abalone data set), it can be advantageous to <code class="reqn">\log</code> transform the data to unbounded form before using <code>vaeac</code>.
If <code>TRUE</code>, then <code><a href="#topic+vaeac_postprocess_data">vaeac_postprocess_data()</a></code> will take the <code class="reqn">\exp</code> of the results to get back to strictly
positive values when using the <code>vaeac</code> model to impute missing values/generate the Monte Carlo samples.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_which_vaeac_model">which_vaeac_model</code></td>
<td>
<p>String (default is <code>best</code>). The name of the <code>vaeac</code> model (snapshots from different
epochs) to use when generating the Monte Carlo samples. The standard choices are: <code>"best"</code> (epoch with lowest IWAE),
<code>"best_running"</code> (epoch with lowest running IWAE, see <code>vaeac.running_avg_n_values</code>), and <code>last</code> (the last epoch).
Note that additional choices are available if <code>vaeac.save_every_nth_epoch</code> is provided. For example, if
<code>vaeac.save_every_nth_epoch = 5</code>, then <code>vaeac.which_vaeac_model</code> can also take the values <code>"epoch_5"</code>, <code>"epoch_10"</code>,
<code>"epoch_15"</code>, and so on.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_seed">seed</code></td>
<td>
<p>Positive integer (default is <code>1</code>). Seed for reproducibility. Specifies the seed before any randomness
based code is being run.</p>
</td></tr>
<tr><td><code id="vaeac_check_parameters_+3A_...">...</code></td>
<td>
<p>List of extra parameters, currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_positive_integers'>Function that checks positive integers</h2><span id='topic+vaeac_check_positive_integers'></span>

<h3>Description</h3>

<p>Function that checks positive integers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_positive_integers(named_list_positive_integers)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_positive_integers_+3A_named_list_positive_integers">named_list_positive_integers</code></td>
<td>
<p>List containing named entries. I.e., <code>list(a = 1, b = 2)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_positive_numerics'>Function that checks positive numerics</h2><span id='topic+vaeac_check_positive_numerics'></span>

<h3>Description</h3>

<p>Function that checks positive numerics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_positive_numerics(named_list_positive_numerics)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_positive_numerics_+3A_named_list_positive_numerics">named_list_positive_numerics</code></td>
<td>
<p>List containing named entries. I.e., <code>list(a = 0.2, b = 10^3)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_probabilities'>Function that checks probabilities</h2><span id='topic+vaeac_check_probabilities'></span>

<h3>Description</h3>

<p>Function that checks probabilities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_probabilities(named_list_probabilities)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_probabilities_+3A_named_list_probabilities">named_list_probabilities</code></td>
<td>
<p>List containing named entries. I.e., <code>list(a = 0.2, b = 0.9)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_save_names'>Function that checks that the save folder exists and for a valid file name</h2><span id='topic+vaeac_check_save_names'></span>

<h3>Description</h3>

<p>Function that checks that the save folder exists and for a valid file name
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_save_names(folder_to_save_model, model_description)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_save_names_+3A_folder_to_save_model">folder_to_save_model</code></td>
<td>
<p>String (default is <code><a href="base.html#topic+tempfile">base::tempdir()</a></code>). String specifying a path to a folder where
the function is to save the fitted vaeac model. Note that  the path will be removed from the returned
<code><a href="#topic+explain">explain()</a></code> object if <code>vaeac.save_model = FALSE</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_save_names_+3A_model_description">model_description</code></td>
<td>
<p>String (default is <code>make.names(Sys.time())</code>). String containing, e.g., the name of the
data distribution or additional parameter information. Used in the save name of the fitted model. If not provided,
then a name will be generated based on <code><a href="base.html#topic+Sys.time">base::Sys.time()</a></code> to ensure a unique name. We use <code><a href="base.html#topic+make.names">base::make.names()</a></code> to
ensure a valid file name for all operating systems.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_save_parameters'>Function that gives a warning about disk usage</h2><span id='topic+vaeac_check_save_parameters'></span>

<h3>Description</h3>

<p>Function that gives a warning about disk usage
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_save_parameters(
  save_data,
  epochs,
  save_every_nth_epoch,
  x_train_size
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_save_parameters_+3A_save_data">save_data</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the data is stored together with
the model. Useful if one are to continue to train the model later using <code><a href="#topic+vaeac_train_model_continue">vaeac_train_model_continue()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_save_parameters_+3A_epochs">epochs</code></td>
<td>
<p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_save_parameters_+3A_save_every_nth_epoch">save_every_nth_epoch</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). If provided, then the vaeac model after
every <code>save_every_nth_epoch</code>th epoch will be saved.</p>
</td></tr>
<tr><td><code id="vaeac_check_save_parameters_+3A_x_train_size">x_train_size</code></td>
<td>
<p>The object size of the <code>x_train</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_which_vaeac_model'>Function that checks for valid <code>vaeac</code> model name</h2><span id='topic+vaeac_check_which_vaeac_model'></span>

<h3>Description</h3>

<p>Function that checks for valid <code>vaeac</code> model name
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_which_vaeac_model(
  which_vaeac_model,
  epochs,
  save_every_nth_epoch = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_which_vaeac_model_+3A_which_vaeac_model">which_vaeac_model</code></td>
<td>
<p>String (default is <code>best</code>). The name of the <code>vaeac</code> model (snapshots from different
epochs) to use when generating the Monte Carlo samples. The standard choices are: <code>"best"</code> (epoch with lowest IWAE),
<code>"best_running"</code> (epoch with lowest running IWAE, see <code>vaeac.running_avg_n_values</code>), and <code>last</code> (the last epoch).
Note that additional choices are available if <code>vaeac.save_every_nth_epoch</code> is provided. For example, if
<code>vaeac.save_every_nth_epoch = 5</code>, then <code>vaeac.which_vaeac_model</code> can also take the values <code>"epoch_5"</code>, <code>"epoch_10"</code>,
<code>"epoch_15"</code>, and so on.</p>
</td></tr>
<tr><td><code id="vaeac_check_which_vaeac_model_+3A_epochs">epochs</code></td>
<td>
<p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="vaeac_check_which_vaeac_model_+3A_save_every_nth_epoch">save_every_nth_epoch</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). If provided, then the vaeac model after
every <code>save_every_nth_epoch</code>th epoch will be saved.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_check_x_colnames'>Function that checks the feature names of data and <code>vaeac</code> model</h2><span id='topic+vaeac_check_x_colnames'></span>

<h3>Description</h3>

<p>Function that checks the feature names of data and <code>vaeac</code> model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_check_x_colnames(feature_names_vaeac, feature_names_new)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_check_x_colnames_+3A_feature_names_vaeac">feature_names_vaeac</code></td>
<td>
<p>Array of strings containing the feature names of the <code>vaeac</code> model.</p>
</td></tr>
<tr><td><code id="vaeac_check_x_colnames_+3A_feature_names_new">feature_names_new</code></td>
<td>
<p>Array of strings containing the feature names to compare with.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_compute_normalization'>Compute Featurewise Means and Standard Deviations</h2><span id='topic+vaeac_compute_normalization'></span>

<h3>Description</h3>

<p>Returns the means and standard deviations for all continuous features in the data set.
Categorical features get <code class="reqn">mean = 0</code> and <code class="reqn">sd = 1</code> by default.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_compute_normalization(data, one_hot_max_sizes)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_compute_normalization_+3A_data">data</code></td>
<td>
<p>A torch_tensor of dimension <code>n_observation</code> x <code>n_features</code> containing the data.</p>
</td></tr>
<tr><td><code id="vaeac_compute_normalization_+3A_one_hot_max_sizes">one_hot_max_sizes</code></td>
<td>
<p>A torch tensor of dimension <code>n_features</code> containing the one hot sizes of the <code>n_features</code>
features. That is, if the <code>i</code>th feature is a categorical feature with 5 levels, then <code>one_hot_max_sizes[i] = 5</code>.
While the size for continuous features can either be <code>0</code> or <code>1</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the means and the standard deviations of the different features.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_dataset'>Dataset used by the <code>vaeac</code> model</h2><span id='topic+vaeac_dataset'></span>

<h3>Description</h3>

<p>Convert a the data into a <code><a href="torch.html#topic+dataset">torch::dataset()</a></code> which the vaeac model creates batches from.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_dataset(X, one_hot_max_sizes)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_dataset_+3A_x">X</code></td>
<td>
<p>A torch_tensor contain the data of shape N x p, where N and p are the number
of observations and features, respectively.</p>
</td></tr>
<tr><td><code id="vaeac_dataset_+3A_one_hot_max_sizes">one_hot_max_sizes</code></td>
<td>
<p>A torch tensor of dimension <code>n_features</code> containing the one hot sizes of the <code>n_features</code>
features. That is, if the <code>i</code>th feature is a categorical feature with 5 levels, then <code>one_hot_max_sizes[i] = 5</code>.
While the size for continuous features can either be <code>0</code> or <code>1</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a <code><a href="torch.html#topic+dataset">torch::dataset()</a></code> object that represent a map from keys to data samples.
It is used by the <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code> to load data which should be used to extract the
batches for all epochs in the training phase of the neural network. Note that a dataset object
is an R6 instance, see <a href="https://r6.r-lib.org/articles/Introduction.html">https://r6.r-lib.org/articles/Introduction.html</a>, which is classical
object-oriented programming, with self reference. I.e, <code><a href="#topic+vaeac_dataset">vaeac_dataset()</a></code> is a subclass
of type <code><a href="torch.html#topic+dataset">torch::dataset()</a></code>.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
p &lt;- 5
N &lt;- 14
batch_size &lt;- 10
one_hot_max_sizes &lt;- rep(1, p)
vaeac_ds &lt;- vaeac_dataset(
  torch_tensor(matrix(rnorm(p * N), ncol = p),
    dtype = torch_float()
  ),
  one_hot_max_sizes
)
vaeac_ds

vaeac_dl &lt;- torch::dataloader(
  vaeac_ds,
  batch_size = batch_size,
  shuffle = TRUE,
  drop_last = FALSE
)
vaeac_dl$.length()
vaeac_dl$.iter()

vaeac_iterator &lt;- vaeac_dl$.iter()
vaeac_iterator$.next() # batch1
vaeac_iterator$.next() # batch2
vaeac_iterator$.next() # Empty

## End(Not run)
</code></pre>

<hr>
<h2 id='vaeac_extend_batch'>Extends Incomplete Batches by Sampling Extra Data from Dataloader</h2><span id='topic+vaeac_extend_batch'></span>

<h3>Description</h3>

<p>If the height of the <code>batch</code> is less than <code>batch_size</code>, this function extends the <code>batch</code> with
data from the <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code> until the <code>batch</code> reaches the required size.
Note that <code>batch</code> is a tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_extend_batch(batch, dataloader, batch_size)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_extend_batch_+3A_batch">batch</code></td>
<td>
<p>The batch we want to check if has the right size, and if not extend it until it has the right size.</p>
</td></tr>
<tr><td><code id="vaeac_extend_batch_+3A_dataloader">dataloader</code></td>
<td>
<p>A <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code> object from which we can create an iterator object
and load data to extend the batch.</p>
</td></tr>
<tr><td><code id="vaeac_extend_batch_+3A_batch_size">batch_size</code></td>
<td>
<p>Integer. The number of samples to include in each batch.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the extended batch with the correct batch_size.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_get_current_save_state'>Function that extracts additional objects from the environment to the state list</h2><span id='topic+vaeac_get_current_save_state'></span>

<h3>Description</h3>

<p>The function extract the objects that we are going to save together with the <code>vaeac</code> model to make it possible to
train the model further and to evaluate it.
The environment should be the local environment inside the <code><a href="#topic+vaeac_train_model_auxiliary">vaeac_train_model_auxiliary()</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_current_save_state(environment)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_current_save_state_+3A_environment">environment</code></td>
<td>
<p>The <code><a href="base.html#topic+environment">base::environment()</a></code> where the objects are stored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the values of <code>epoch</code>, <code>train_vlb</code>, <code>val_iwae</code>, <code>val_iwae_running</code>,
and the <code>state_dict()</code> of the vaeac model and optimizer.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_get_data_objects'>Function to set up data loaders and save file names</h2><span id='topic+vaeac_get_data_objects'></span>

<h3>Description</h3>

<p>Function to set up data loaders and save file names
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_data_objects(
  x_train,
  log_exp_cont_feat,
  val_ratio,
  batch_size,
  paired_sampling,
  model_description,
  depth,
  width,
  latent_dim,
  lr,
  epochs,
  save_every_nth_epoch,
  folder_to_save_model,
  train_indices = NULL,
  val_indices = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_data_objects_+3A_x_train">x_train</code></td>
<td>
<p>A data.table containing the training data. Categorical data must have class names <code class="reqn">1,2,\dots,K</code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_log_exp_cont_feat">log_exp_cont_feat</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If we are to <code class="reqn">\log</code> transform all
continuous features before sending the data to <code><a href="#topic+vaeac">vaeac()</a></code>. The <code>vaeac</code> model creates unbounded Monte Carlo
sample values. Thus, if the continuous features are strictly positive (as for, e.g., the Burr distribution and
Abalone data set), it can be advantageous to <code class="reqn">\log</code> transform the data to unbounded form before using <code>vaeac</code>.
If <code>TRUE</code>, then <code><a href="#topic+vaeac_postprocess_data">vaeac_postprocess_data()</a></code> will take the <code class="reqn">\exp</code> of the results to get back to strictly
positive values when using the <code>vaeac</code> model to impute missing values/generate the Monte Carlo samples.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_val_ratio">val_ratio</code></td>
<td>
<p>Numeric (default is <code>0.25</code>). Scalar between <code>0</code> and <code>1</code> indicating the ratio of
instances from the input data which will be used as validation data. That is, <code>val_ratio = 0.25</code> means
that <code style="white-space: pre;">&#8288;75%&#8288;</code> of the provided data is used as training data, while the remaining <code style="white-space: pre;">&#8288;25%&#8288;</code> is used as validation data.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_batch_size">batch_size</code></td>
<td>
<p>Positive integer (default is <code>64</code>). The number of samples to include in each batch
during the training of the vaeac model. Used in <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_paired_sampling">paired_sampling</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, we apply paired sampling to the training
batches. That is, the training observations in each batch will be duplicated, where the first instance will be masked
by <code class="reqn">S</code> while the second instance will be masked by <code class="reqn">\bar{S}</code>. This ensures that the training of the
<code>vaeac</code> model becomes more stable as the model has access to the full version of each training observation. However,
this will increase the training time due to more complex implementation and doubling the size of each batch. See
<code><a href="#topic+paired_sampler">paired_sampler()</a></code> for more information.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_model_description">model_description</code></td>
<td>
<p>String (default is <code>make.names(Sys.time())</code>). String containing, e.g., the name of the
data distribution or additional parameter information. Used in the save name of the fitted model. If not provided,
then a name will be generated based on <code><a href="base.html#topic+Sys.time">base::Sys.time()</a></code> to ensure a unique name. We use <code><a href="base.html#topic+make.names">base::make.names()</a></code> to
ensure a valid file name for all operating systems.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_depth">depth</code></td>
<td>
<p>Positive integer (default is <code>3</code>). The number of hidden layers
in the neural networks of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_width">width</code></td>
<td>
<p>Positive integer (default is <code>32</code>). The number of neurons in each
hidden layer in the neural networks of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_latent_dim">latent_dim</code></td>
<td>
<p>Positive integer (default is <code>8</code>). The number of dimensions in the latent space.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_lr">lr</code></td>
<td>
<p>Positive numeric (default is <code>0.001</code>). The learning rate used in the <code><a href="torch.html#topic+optim_adam">torch::optim_adam()</a></code> optimizer.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_epochs">epochs</code></td>
<td>
<p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_save_every_nth_epoch">save_every_nth_epoch</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). If provided, then the vaeac model after
every <code>save_every_nth_epoch</code>th epoch will be saved.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_folder_to_save_model">folder_to_save_model</code></td>
<td>
<p>String (default is <code><a href="base.html#topic+tempfile">base::tempdir()</a></code>). String specifying a path to a folder where
the function is to save the fitted vaeac model. Note that  the path will be removed from the returned
<code><a href="#topic+explain">explain()</a></code> object if <code>vaeac.save_model = FALSE</code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_train_indices">train_indices</code></td>
<td>
<p>Numeric array (optional) containing the indices of the training observations.
There are conducted no checks to validate the indices.</p>
</td></tr>
<tr><td><code id="vaeac_get_data_objects_+3A_val_indices">val_indices</code></td>
<td>
<p>Numeric array (optional) containing the indices of the validation observations.
#' There are conducted no checks to validate the indices.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of objects needed to train the <code>vaeac</code> model
</p>

<hr>
<h2 id='vaeac_get_evaluation_criteria'>Extract the Training VLB and Validation IWAE from a list of explanations objects using the vaeac approach</h2><span id='topic+vaeac_get_evaluation_criteria'></span>

<h3>Description</h3>

<p>Extract the Training VLB and Validation IWAE from a list of explanations objects using the vaeac approach
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_evaluation_criteria(explanation_list)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_evaluation_criteria_+3A_explanation_list">explanation_list</code></td>
<td>
<p>A list of <code><a href="#topic+explain">explain()</a></code> objects applied to the same data, model, and
<code>vaeac</code> must be the used approach. If the entries in the list is named, then the function use
these names. Otherwise, it defaults to the approach names (with integer suffix for duplicates)
for the explanation objects in <code>explanation_list</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table containing the training VLB, validation IWAE, and running validation IWAE at each epoch for
each vaeac model.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_get_extra_para_default'>Function to specify the extra parameters in the <code>vaeac</code> model</h2><span id='topic+vaeac_get_extra_para_default'></span>

<h3>Description</h3>

<p>In this function, we specify the default values for the extra parameters used in <code><a href="#topic+explain">explain()</a></code>
for <code>approach = "vaeac"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_extra_para_default(
  vaeac.model_description = make.names(Sys.time()),
  vaeac.folder_to_save_model = tempdir(),
  vaeac.pretrained_vaeac_model = NULL,
  vaeac.cuda = FALSE,
  vaeac.epochs_initiation_phase = 2,
  vaeac.epochs_early_stopping = NULL,
  vaeac.save_every_nth_epoch = NULL,
  vaeac.val_ratio = 0.25,
  vaeac.val_iwae_n_samples = 25,
  vaeac.batch_size = 64,
  vaeac.batch_size_sampling = NULL,
  vaeac.running_avg_n_values = 5,
  vaeac.skip_conn_layer = TRUE,
  vaeac.skip_conn_masked_enc_dec = TRUE,
  vaeac.batch_normalization = FALSE,
  vaeac.paired_sampling = TRUE,
  vaeac.masking_ratio = 0.5,
  vaeac.mask_gen_coalitions = NULL,
  vaeac.mask_gen_coalitions_prob = NULL,
  vaeac.sigma_mu = 10000,
  vaeac.sigma_sigma = 1e-04,
  vaeac.sample_random = TRUE,
  vaeac.save_data = FALSE,
  vaeac.log_exp_cont_feat = FALSE,
  vaeac.which_vaeac_model = "best",
  vaeac.save_model = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.model_description">vaeac.model_description</code></td>
<td>
<p>String (default is <code>make.names(Sys.time())</code>). String containing, e.g., the name of the
data distribution or additional parameter information. Used in the save name of the fitted model. If not provided,
then a name will be generated based on <code><a href="base.html#topic+Sys.time">base::Sys.time()</a></code> to ensure a unique name. We use <code><a href="base.html#topic+make.names">base::make.names()</a></code> to
ensure a valid file name for all operating systems.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.folder_to_save_model">vaeac.folder_to_save_model</code></td>
<td>
<p>String (default is <code><a href="base.html#topic+tempfile">base::tempdir()</a></code>). String specifying a path to a folder where
the function is to save the fitted vaeac model. Note that the path will be removed from the returned
<code><a href="#topic+explain">explain()</a></code> object if <code>vaeac.save_model = FALSE</code>. Furthermore, the model cannot be moved from its
original folder if we are to use the <code><a href="#topic+vaeac_train_model_continue">vaeac_train_model_continue()</a></code> function to continue training the model.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.pretrained_vaeac_model">vaeac.pretrained_vaeac_model</code></td>
<td>
<p>List or String (default is <code>NULL</code>). 1) Either a list of class
<code>vaeac</code>, i.e., the list stored in <code>explanation$internal$parameters$vaeac</code> where <code>explanation</code> is the returned list
from an earlier call to the <code><a href="#topic+explain">explain()</a></code> function. 2) A string containing the path to where the <code>vaeac</code>
model is stored on disk, for example, <code>explanation$internal$parameters$vaeac$models$best</code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.cuda">vaeac.cuda</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the <code>vaeac</code> model will be trained using cuda/GPU.
If <code><a href="torch.html#topic+cuda_is_available">torch::cuda_is_available()</a></code> is <code>FALSE</code>, the we fall back to use CPU. If <code>FALSE</code>, we use the CPU. Using a GPU
for smaller tabular dataset often do not improve the efficiency.
See <code>vignette("installation", package = "torch")</code> fo help to enable running on the GPU (only Linux and Windows).</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.epochs_initiation_phase">vaeac.epochs_initiation_phase</code></td>
<td>
<p>Positive integer (default is <code>2</code>). The number of epochs to run each of the
<code>vaeac.n_vaeacs_initialize</code> <code>vaeac</code> models before continuing to train only the best performing model.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.epochs_early_stopping">vaeac.epochs_early_stopping</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). The training stops if there has been no
improvement in the validation IWAE for <code>vaeac.epochs_early_stopping</code> epochs. If the user wants the training process
to be solely based on this training criterion, then <code>vaeac.epochs</code> in <code><a href="#topic+explain">explain()</a></code> should be set to a large
number. If <code>NULL</code>, then <code>shapr</code> will internally set <code>vaeac.epochs_early_stopping = vaeac.epochs</code> such that early
stopping does not occur.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.save_every_nth_epoch">vaeac.save_every_nth_epoch</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). If provided, then the vaeac model after
every <code>vaeac.save_every_nth_epoch</code>th epoch will be saved.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.val_ratio">vaeac.val_ratio</code></td>
<td>
<p>Numeric (default is <code>0.25</code>). Scalar between <code>0</code> and <code>1</code> indicating the ratio of
instances from the input data which will be used as validation data. That is, <code>vaeac.val_ratio = 0.25</code> means
that <code style="white-space: pre;">&#8288;75%&#8288;</code> of the provided data is used as training data, while the remaining <code style="white-space: pre;">&#8288;25%&#8288;</code> is used as validation data.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.val_iwae_n_samples">vaeac.val_iwae_n_samples</code></td>
<td>
<p>Positive integer (default is <code>25</code>). The number of generated samples used
to compute the IWAE criterion when validating the vaeac model on the validation data.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.batch_size">vaeac.batch_size</code></td>
<td>
<p>Positive integer (default is <code>64</code>). The number of samples to include in each batch
during the training of the vaeac model. Used in <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.batch_size_sampling">vaeac.batch_size_sampling</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>) The number of samples to include in
each batch when generating the Monte Carlo samples. If <code>NULL</code>, then the function generates the Monte Carlo samples
for the provided coalitions and all explicands sent to <code><a href="#topic+explain">explain()</a></code> at the time.
The number of coalitions are determined by the <code>n_batches</code> used by <code><a href="#topic+explain">explain()</a></code>. We recommend to tweak
<code>extra_computation_args$max_batch_size</code> and <code>extra_computation_args$min_n_batches</code>
rather than <code>vaeac.batch_size_sampling</code>. Larger batch sizes are often much faster provided sufficient memory.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.running_avg_n_values">vaeac.running_avg_n_values</code></td>
<td>
<p>Positive integer (default is <code>5</code>). The number of previous IWAE values to include
when we compute the running means of the IWAE criterion.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.skip_conn_layer">vaeac.skip_conn_layer</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, we apply identity skip connections in each
layer, see <code><a href="#topic+skip_connection">skip_connection()</a></code>. That is, we add the input <code class="reqn">X</code> to the outcome of each hidden layer,
so the output becomes <code class="reqn">X + activation(WX + b)</code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.skip_conn_masked_enc_dec">vaeac.skip_conn_masked_enc_dec</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, we apply concatenate skip
connections between the layers in the masked encoder and decoder. The first layer of the masked encoder will be
linked to the last layer of the decoder. The second layer of the masked encoder will be
linked to the second to last layer of the decoder, and so on.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.batch_normalization">vaeac.batch_normalization</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, we apply batch normalization after the
activation function. Note that if <code>vaeac.skip_conn_layer = TRUE</code>, then the normalization is applied after the
inclusion of the skip connection. That is, we batch normalize the whole quantity <code class="reqn">X + activation(WX + b)</code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.paired_sampling">vaeac.paired_sampling</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, we apply paired sampling to the training
batches. That is, the training observations in each batch will be duplicated, where the first instance will be masked
by <code class="reqn">S</code> while the second instance will be masked by <code class="reqn">\bar{S}</code>. This ensures that the training of the
<code>vaeac</code> model becomes more stable as the model has access to the full version of each training observation. However,
this will increase the training time due to more complex implementation and doubling the size of each batch. See
<code><a href="#topic+paired_sampler">paired_sampler()</a></code> for more information.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.masking_ratio">vaeac.masking_ratio</code></td>
<td>
<p>Numeric (default is <code>0.5</code>). Probability of masking a feature in the
<code><a href="#topic+mcar_mask_generator">mcar_mask_generator()</a></code> (MCAR = Missing Completely At Random). The MCAR masking scheme ensures that <code>vaeac</code>
model can do arbitrary conditioning as all coalitions will be trained. <code>vaeac.masking_ratio</code> will be overruled if
<code>vaeac.mask_gen_coalitions</code> is specified.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.mask_gen_coalitions">vaeac.mask_gen_coalitions</code></td>
<td>
<p>Matrix (default is <code>NULL</code>). Matrix containing the coalitions that the
<code>vaeac</code> model will be trained on, see <code><a href="#topic+specified_masks_mask_generator">specified_masks_mask_generator()</a></code>. This parameter is used internally
in <code>shapr</code> when we only consider a subset of coalitions, i.e., when
<code>n_coalitions</code> <code class="reqn">&lt; 2^{n_{\text{features}}}</code>, and for group Shapley, i.e.,
when <code>group</code> is specified in <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.mask_gen_coalitions_prob">vaeac.mask_gen_coalitions_prob</code></td>
<td>
<p>Numeric array (default is <code>NULL</code>). Array of length equal to the height
of <code>vaeac.mask_gen_coalitions</code> containing the probabilities of sampling the corresponding coalitions in
<code>vaeac.mask_gen_coalitions</code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.sigma_mu">vaeac.sigma_mu</code></td>
<td>
<p>Numeric (default is <code>1e4</code>). One of two hyperparameter values in the normal-gamma prior
used in the masked encoder, see Section 3.3.1 in
<a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.sigma_sigma">vaeac.sigma_sigma</code></td>
<td>
<p>Numeric (default is <code>1e-4</code>). One of two hyperparameter values in the normal-gamma prior
used in the masked encoder, see Section 3.3.1 in
<a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.sample_random">vaeac.sample_random</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, the function generates random Monte Carlo samples
from the inferred generative distributions. If <code>FALSE</code>, the function use the most likely values, i.e., the mean and
class with highest probability for continuous and categorical, respectively.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.save_data">vaeac.save_data</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the data is stored together with
the model. Useful if one are to continue to train the model later using <code><a href="#topic+vaeac_train_model_continue">vaeac_train_model_continue()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.log_exp_cont_feat">vaeac.log_exp_cont_feat</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If we are to <code class="reqn">\log</code> transform all
continuous features before sending the data to <code><a href="#topic+vaeac">vaeac()</a></code>. The <code>vaeac</code> model creates unbounded Monte Carlo
sample values. Thus, if the continuous features are strictly positive (as for, e.g., the Burr distribution and
Abalone data set), it can be advantageous to <code class="reqn">\log</code> transform the data to unbounded form before using <code>vaeac</code>.
If <code>TRUE</code>, then <code><a href="#topic+vaeac_postprocess_data">vaeac_postprocess_data()</a></code> will take the <code class="reqn">\exp</code> of the results to get back to strictly
positive values when using the <code>vaeac</code> model to impute missing values/generate the Monte Carlo samples.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.which_vaeac_model">vaeac.which_vaeac_model</code></td>
<td>
<p>String (default is <code>best</code>). The name of the <code>vaeac</code> model (snapshots from different
epochs) to use when generating the Monte Carlo samples. The standard choices are: <code>"best"</code> (epoch with lowest IWAE),
<code>"best_running"</code> (epoch with lowest running IWAE, see <code>vaeac.running_avg_n_values</code>), and <code>last</code> (the last epoch).
Note that additional choices are available if <code>vaeac.save_every_nth_epoch</code> is provided. For example, if
<code>vaeac.save_every_nth_epoch = 5</code>, then <code>vaeac.which_vaeac_model</code> can also take the values <code>"epoch_5"</code>, <code>"epoch_10"</code>,
<code>"epoch_15"</code>, and so on.</p>
</td></tr>
<tr><td><code id="vaeac_get_extra_para_default_+3A_vaeac.save_model">vaeac.save_model</code></td>
<td>
<p>Boolean. If <code>TRUE</code> (default), the <code>vaeac</code> model will be saved either in a
<code><a href="base.html#topic+tempfile">base::tempdir()</a></code> folder or in a user specified location in <code>vaeac.folder_to_save_model</code>. If <code>FALSE</code>, then
the paths to model and the model will will be deleted from the returned object from <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>vaeac</code> model consists of three neural network (a full encoder, a masked encoder, and a decoder) based
on the provided <code>vaeac.depth</code> and <code>vaeac.width</code>. The encoders map the full and masked input
representations to latent representations, respectively, where the dimension is given by <code>vaeac.latent_dim</code>.
The latent representations are sent to the decoder to go back to the real feature space and
provide a samplable probabilistic representation, from which the Monte Carlo samples are generated.
We use the <code>vaeac</code> method at the epoch with the lowest validation error (IWAE) by default, but
other possibilities are available but setting the <code>vaeac.which_vaeac_model</code> parameter. See
<a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a> for more details.
</p>


<h3>Value</h3>

<p>Named list of the default values <code>vaeac</code> extra parameter arguments specified in this function call.
Note that both <code>vaeac.model_description</code> and <code>vaeac.folder_to_save_model</code> will change with time and R session.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">
Olsen, L. H., Glad, I. K., Jullum, M., &amp; Aas, K. (2022). Using Shapley values and variational autoencoders to
explain predictive models with dependent mixed features. Journal of machine learning research, 23(213), 1-51</a>
</p>
</li></ul>


<hr>
<h2 id='vaeac_get_full_state_list'>Function that extracts the state list objects from the environment</h2><span id='topic+vaeac_get_full_state_list'></span>

<h3>Description</h3>

<p>#' @description
The function extract the objects that we are going to save together with the <code>vaeac</code> model to make it possible to
train the model further and to evaluate it.
The environment should be the local environment inside the <code><a href="#topic+vaeac_train_model_auxiliary">vaeac_train_model_auxiliary()</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_full_state_list(environment)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_full_state_list_+3A_environment">environment</code></td>
<td>
<p>The <code><a href="base.html#topic+environment">base::environment()</a></code> where the objects are stored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the values of <code>norm_mean</code>, <code>norm_std</code>, <code>model_description</code>, <code>folder_to_save_model</code>,
<code>n_train</code>, <code>n_features</code>, <code>one_hot_max_sizes</code>, <code>epochs</code>, <code>epochs_specified</code>, <code>epochs_early_stopping</code>,
<code>early_stopping_applied</code>, <code>running_avg_n_values</code>, <code>paired_sampling</code>, <code>mask_generator_name</code>, <code>masking_ratio</code>,
<code>mask_gen_coalitions</code>, <code>mask_gen_coalitions_prob</code>, <code>val_ratio</code>, <code>val_iwae_n_samples</code>,
<code>n_vaeacs_initialize</code>, <code>epochs_initiation_phase</code>, <code>width</code>, <code>depth</code>, <code>latent_dim</code>, <code>activation_function</code>,
<code>lr</code>, <code>batch_size</code>, <code>skip_conn_layer</code>, <code>skip_conn_masked_enc_dec</code>, <code>batch_normalization</code>, <code>cuda</code>,
<code>train_indices</code>, <code>val_indices</code>, <code>save_every_nth_epoch</code>, <code>sigma_mu</code>,
<code>sigma_sigma</code>, <code>feature_list</code>, <code>col_cat_names</code>, <code>col_cont_names</code>, <code>col_cat</code>, <code>col_cont</code>, <code>cat_in_dataset</code>,
<code>map_new_to_original_names</code>, <code>map_original_to_new_names</code>, <code>log_exp_cont_feat</code>, <code>save_data</code>, <code>verbose</code>,
<code>seed</code>, and <code>vaeac_save_file_names</code>.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_get_mask_generator_name'>Function that determines which mask generator to use</h2><span id='topic+vaeac_get_mask_generator_name'></span>

<h3>Description</h3>

<p>Function that determines which mask generator to use
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_mask_generator_name(
  mask_gen_coalitions,
  mask_gen_coalitions_prob,
  masking_ratio,
  verbose
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_mask_generator_name_+3A_mask_gen_coalitions">mask_gen_coalitions</code></td>
<td>
<p>Matrix (default is <code>NULL</code>). Matrix containing the coalitions that the
<code>vaeac</code> model will be trained on, see <code><a href="#topic+specified_masks_mask_generator">specified_masks_mask_generator()</a></code>. This parameter is used internally
in <code>shapr</code> when we only consider a subset of coalitions, i.e., when
<code>n_coalitions</code> <code class="reqn">&lt; 2^{n_{\text{features}}}</code>, and for group Shapley, i.e.,
when <code>group</code> is specified in <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_mask_generator_name_+3A_mask_gen_coalitions_prob">mask_gen_coalitions_prob</code></td>
<td>
<p>Numeric array (default is <code>NULL</code>). Array of length equal to the height
of <code>mask_gen_coalitions</code> containing the probabilities of sampling the corresponding coalitions in
<code>mask_gen_coalitions</code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_mask_generator_name_+3A_masking_ratio">masking_ratio</code></td>
<td>
<p>Numeric (default is <code>0.5</code>). Probability of masking a feature in the
<code><a href="#topic+mcar_mask_generator">mcar_mask_generator()</a></code> (MCAR = Missing Completely At Random). The MCAR masking scheme ensures that <code>vaeac</code>
model can do arbitrary conditioning as all coalitions will be trained. <code>masking_ratio</code> will be overruled if
<code>mask_gen_coalitions</code> is specified.</p>
</td></tr>
<tr><td><code id="vaeac_get_mask_generator_name_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_get_model_from_checkp'>Function to load a <code>vaeac</code> model and set it in the right state and mode</h2><span id='topic+vaeac_get_model_from_checkp'></span>

<h3>Description</h3>

<p>Function to load a <code>vaeac</code> model and set it in the right state and mode
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_model_from_checkp(checkpoint, cuda, mode_train)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_model_from_checkp_+3A_checkpoint">checkpoint</code></td>
<td>
<p>List. This must be a loaded <code>vaeac</code> save object. That is, <code>torch::torch_load('vaeac_save_path')</code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_model_from_checkp_+3A_cuda">cuda</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the <code>vaeac</code> model will be trained using cuda/GPU.
If <code><a href="torch.html#topic+cuda_is_available">torch::cuda_is_available()</a></code> is <code>FALSE</code>, the we fall back to use CPU. If <code>FALSE</code>, we use the CPU. Using a GPU
for smaller tabular dataset often do not improve the efficiency.
See <code>vignette("installation", package = "torch")</code> fo help to enable running on the GPU (only Linux and Windows).</p>
</td></tr>
<tr><td><code id="vaeac_get_model_from_checkp_+3A_mode_train">mode_train</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the returned <code>vaeac</code> model is set to be in training mode.
If <code>FALSE</code>, the returned <code>vaeac</code> model is set to be in evaluation mode.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>vaeac</code> model with the correct state (based on <code>checkpoint</code>), sent to the desired hardware (based on
<code>cuda</code>), and in the right mode (based on <code>mode_train</code>).
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_get_n_decimals'>Function to get string of values with specific number of decimals</h2><span id='topic+vaeac_get_n_decimals'></span>

<h3>Description</h3>

<p>Function to get string of values with specific number of decimals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_n_decimals(value, n_decimals = 3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_n_decimals_+3A_value">value</code></td>
<td>
<p>The number to get <code>n_decimals</code> for.</p>
</td></tr>
<tr><td><code id="vaeac_get_n_decimals_+3A_n_decimals">n_decimals</code></td>
<td>
<p>Positive integer. The number of decimals. Default is three.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>String of <code>value</code> with <code>n_decimals</code> decimals.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_get_optimizer'>Function to create the optimizer used to train <code>vaeac</code></h2><span id='topic+vaeac_get_optimizer'></span>

<h3>Description</h3>

<p>Only <code><a href="torch.html#topic+optim_adam">torch::optim_adam()</a></code> is currently supported. But it is easy to add an additional option later.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_optimizer(vaeac_model, lr, optimizer_name = "adam")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_optimizer_+3A_vaeac_model">vaeac_model</code></td>
<td>
<p>A <code>vaeac</code> model created using <code><a href="#topic+vaeac">vaeac()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_optimizer_+3A_lr">lr</code></td>
<td>
<p>Positive numeric (default is <code>0.001</code>). The learning rate used in the <code><a href="torch.html#topic+optim_adam">torch::optim_adam()</a></code> optimizer.</p>
</td></tr>
<tr><td><code id="vaeac_get_optimizer_+3A_optimizer_name">optimizer_name</code></td>
<td>
<p>String containing the name of the <code><a href="torch.html#topic+optimizer">torch::optimizer()</a></code> to use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="torch.html#topic+optim_adam">torch::optim_adam()</a></code> optimizer connected to the parameters of the <code>vaeac_model</code>.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_get_save_file_names'>Function that creates the save file names for the <code>vaeac</code> model</h2><span id='topic+vaeac_get_save_file_names'></span>

<h3>Description</h3>

<p>Function that creates the save file names for the <code>vaeac</code> model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_save_file_names(
  model_description,
  n_features,
  n_train,
  depth,
  width,
  latent_dim,
  lr,
  epochs,
  save_every_nth_epoch,
  folder_to_save_model = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_save_file_names_+3A_model_description">model_description</code></td>
<td>
<p>String (default is <code>make.names(Sys.time())</code>). String containing, e.g., the name of the
data distribution or additional parameter information. Used in the save name of the fitted model. If not provided,
then a name will be generated based on <code><a href="base.html#topic+Sys.time">base::Sys.time()</a></code> to ensure a unique name. We use <code><a href="base.html#topic+make.names">base::make.names()</a></code> to
ensure a valid file name for all operating systems.</p>
</td></tr>
<tr><td><code id="vaeac_get_save_file_names_+3A_depth">depth</code></td>
<td>
<p>Positive integer (default is <code>3</code>). The number of hidden layers
in the neural networks of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="vaeac_get_save_file_names_+3A_width">width</code></td>
<td>
<p>Positive integer (default is <code>32</code>). The number of neurons in each
hidden layer in the neural networks of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="vaeac_get_save_file_names_+3A_latent_dim">latent_dim</code></td>
<td>
<p>Positive integer (default is <code>8</code>). The number of dimensions in the latent space.</p>
</td></tr>
<tr><td><code id="vaeac_get_save_file_names_+3A_lr">lr</code></td>
<td>
<p>Positive numeric (default is <code>0.001</code>). The learning rate used in the <code><a href="torch.html#topic+optim_adam">torch::optim_adam()</a></code> optimizer.</p>
</td></tr>
<tr><td><code id="vaeac_get_save_file_names_+3A_epochs">epochs</code></td>
<td>
<p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="vaeac_get_save_file_names_+3A_save_every_nth_epoch">save_every_nth_epoch</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). If provided, then the vaeac model after
every <code>save_every_nth_epoch</code>th epoch will be saved.</p>
</td></tr>
<tr><td><code id="vaeac_get_save_file_names_+3A_folder_to_save_model">folder_to_save_model</code></td>
<td>
<p>String (default is <code><a href="base.html#topic+tempfile">base::tempdir()</a></code>). String specifying a path to a folder where
the function is to save the fitted vaeac model. Note that  the path will be removed from the returned
<code><a href="#topic+explain">explain()</a></code> object if <code>vaeac.save_model = FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Array of string containing the save files to use when training the <code>vaeac</code> model. The first three names
corresponds to the best, best_running, and last epochs, in that order.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_get_val_iwae'>Compute the Importance Sampling Estimator (Validation Error)</h2><span id='topic+vaeac_get_val_iwae'></span>

<h3>Description</h3>

<p>Compute the Importance Sampling Estimator which the vaeac model
uses to evaluate its performance on the validation data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_val_iwae(
  val_dataloader,
  mask_generator,
  batch_size,
  vaeac_model,
  val_iwae_n_samples
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_val_iwae_+3A_val_dataloader">val_dataloader</code></td>
<td>
<p>A torch dataloader which loads the validation data.</p>
</td></tr>
<tr><td><code id="vaeac_get_val_iwae_+3A_mask_generator">mask_generator</code></td>
<td>
<p>A mask generator object that generates the masks.</p>
</td></tr>
<tr><td><code id="vaeac_get_val_iwae_+3A_batch_size">batch_size</code></td>
<td>
<p>Integer. The number of samples to include in each batch.</p>
</td></tr>
<tr><td><code id="vaeac_get_val_iwae_+3A_vaeac_model">vaeac_model</code></td>
<td>
<p>The vaeac model.</p>
</td></tr>
<tr><td><code id="vaeac_get_val_iwae_+3A_val_iwae_n_samples">val_iwae_n_samples</code></td>
<td>
<p>Number of samples to generate for computing the IWAE for each validation sample.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Compute mean IWAE log likelihood estimation of the validation set. IWAE is an abbreviation for Importance
Sampling Estimator </p>
<p style="text-align: center;"><code class="reqn">\log p_{\theta, \psi}(x|y) \approx \log {\frac{1}{S}\sum_{i=1}^S
p_\theta(x|z_i, y) p_\psi(z_i|y) \big/ q_\phi(z_i|x,y),}</code>
</p>
<p> where <code class="reqn">z_i \sim q_\phi(z|x,y)</code>.
For more details, see <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.
</p>


<h3>Value</h3>

<p>The average iwae over all instances in the validation dataset.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_get_x_explain_extended'>Function to extend the explicands and apply all relevant masks/coalitions</h2><span id='topic+vaeac_get_x_explain_extended'></span>

<h3>Description</h3>

<p>Function to extend the explicands and apply all relevant masks/coalitions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_get_x_explain_extended(x_explain, S, index_features)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_get_x_explain_extended_+3A_x_explain">x_explain</code></td>
<td>
<p>Matrix or data.frame/data.table.
Contains the the features, whose predictions ought to be explained.</p>
</td></tr>
<tr><td><code id="vaeac_get_x_explain_extended_+3A_s">S</code></td>
<td>
<p>The <code>internal$objects$S</code> matrix containing the possible coalitions.</p>
</td></tr>
<tr><td><code id="vaeac_get_x_explain_extended_+3A_index_features">index_features</code></td>
<td>
<p>Positive integer vector. Specifies the id_coalition to
apply to the present method. <code>NULL</code> means all coalitions. Only used internally.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The extended version of <code>x_explain</code> where the masks from <code>S</code> with indices <code>index_features</code> have been applied.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_impute_missing_entries'>Impute Missing Values Using Vaeac</h2><span id='topic+vaeac_impute_missing_entries'></span>

<h3>Description</h3>

<p>Impute Missing Values Using Vaeac
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_impute_missing_entries(
  x_explain_with_NaNs,
  n_MC_samples,
  vaeac_model,
  checkpoint,
  sampler,
  batch_size,
  verbose = NULL,
  seed = NULL,
  n_explain = NULL,
  index_features = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_impute_missing_entries_+3A_x_explain_with_nans">x_explain_with_NaNs</code></td>
<td>
<p>A 2D matrix, where the missing entries to impute are represented by <code>NaN</code>.</p>
</td></tr>
<tr><td><code id="vaeac_impute_missing_entries_+3A_n_mc_samples">n_MC_samples</code></td>
<td>
<p>Integer. The number of imputed versions we create for each row in <code>x_explain_with_NaNs</code>.</p>
</td></tr>
<tr><td><code id="vaeac_impute_missing_entries_+3A_vaeac_model">vaeac_model</code></td>
<td>
<p>An initialized <code>vaeac</code> model that we are going to use to generate the MC samples.</p>
</td></tr>
<tr><td><code id="vaeac_impute_missing_entries_+3A_checkpoint">checkpoint</code></td>
<td>
<p>List containing the parameters of the <code>vaeac</code> model.</p>
</td></tr>
<tr><td><code id="vaeac_impute_missing_entries_+3A_sampler">sampler</code></td>
<td>
<p>A sampler object used to sample the MC samples.</p>
</td></tr>
<tr><td><code id="vaeac_impute_missing_entries_+3A_batch_size">batch_size</code></td>
<td>
<p>Positive integer (default is <code>64</code>). The number of samples to include in each batch
during the training of the vaeac model. Used in <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_impute_missing_entries_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
<tr><td><code id="vaeac_impute_missing_entries_+3A_seed">seed</code></td>
<td>
<p>Positive integer (default is <code>1</code>). Seed for reproducibility. Specifies the seed before any randomness
based code is being run.</p>
</td></tr>
<tr><td><code id="vaeac_impute_missing_entries_+3A_n_explain">n_explain</code></td>
<td>
<p>Positive integer. The number of explicands.</p>
</td></tr>
<tr><td><code id="vaeac_impute_missing_entries_+3A_index_features">index_features</code></td>
<td>
<p>Optional integer vector. Used internally in shapr package to index the coalitions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function that imputes the missing values in 2D matrix where each row constitute an individual.
The values are sampled from the conditional distribution estimated by a vaeac model.
</p>


<h3>Value</h3>

<p>A data.table where the missing values (<code>NaN</code>) in <code>x_explain_with_NaNs</code> have been imputed <code>n_MC_samples</code>
times.
The data table will contain extra id columns if <code>index_features</code> and <code>n_explain</code> are provided.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_kl_normal_normal'>Compute the KL Divergence Between Two Gaussian Distributions.</h2><span id='topic+vaeac_kl_normal_normal'></span>

<h3>Description</h3>

<p>Computes the KL divergence between univariate normal distributions using the analytical formula,
see <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions">https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_kl_normal_normal(p, q)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_kl_normal_normal_+3A_p">p</code></td>
<td>
<p>A <code><a href="torch.html#topic+distr_normal">torch::distr_normal()</a></code> object.</p>
</td></tr>
<tr><td><code id="vaeac_kl_normal_normal_+3A_q">q</code></td>
<td>
<p>A <code><a href="torch.html#topic+distr_normal">torch::distr_normal()</a></code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The KL divergence between the two Gaussian distributions.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_normal_parse_params'>Creates Normal Distributions</h2><span id='topic+vaeac_normal_parse_params'></span>

<h3>Description</h3>

<p>Function that takes in the a tensor where the first half of the columns contains the means of the
normal distributions, while the latter half of the columns contains the standard deviations. The standard deviations
are clamped with <code>min_sigma</code> to ensure stable results. If <code>params</code> is of dimensions batch_size x 8, the function
will create 4 independent normal distributions for each of the observation (<code>batch_size</code> observations in total).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_normal_parse_params(params, min_sigma = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_normal_parse_params_+3A_params">params</code></td>
<td>
<p>Tensor of dimension <code>batch_size</code> x <code>2*n_featuers</code> containing the means and standard deviations
to be used in the normal distributions for of the <code>batch_size</code> observations.</p>
</td></tr>
<tr><td><code id="vaeac_normal_parse_params_+3A_min_sigma">min_sigma</code></td>
<td>
<p>For stability it might be desirable that the minimal sigma is not too close to zero.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Take a Tensor (e.g. neural network output) and return a <code><a href="torch.html#topic+distr_normal">torch::distr_normal()</a></code> distribution.
This normal distribution is component-wise independent, and its dimensionality depends on the input shape.
First half of channels is mean (<code class="reqn">\mu</code>) of the distribution, the softplus of the second half is
std (<code class="reqn">\sigma</code>), so there is no restrictions on the input tensor. <code>min_sigma</code> is the minimal value of
<code class="reqn">\sigma</code>. I.e., if the above softplus is less than <code>min_sigma</code>, then <code class="reqn">\sigma</code> is clipped
from below with value <code>min_sigma</code>. This regularization is required for the numerical stability and may
be considered as a neural network architecture choice without any change to the probabilistic model.
</p>


<h3>Value</h3>

<p>A <code><a href="torch.html#topic+distr_normal">torch::distr_normal()</a></code> distribution with the provided means and standard deviations.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_normalize_data'>Normalize mixed data for <code>vaeac</code></h2><span id='topic+vaeac_normalize_data'></span>

<h3>Description</h3>

<p>Compute the mean and std for each continuous feature, while the categorical features will have mean 0 and std 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_normalize_data(
  data_torch,
  one_hot_max_sizes,
  norm_mean = NULL,
  norm_std = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_normalize_data_+3A_one_hot_max_sizes">one_hot_max_sizes</code></td>
<td>
<p>A torch tensor of dimension <code>n_features</code> containing the one hot sizes of the <code>n_features</code>
features. That is, if the <code>i</code>th feature is a categorical feature with 5 levels, then <code>one_hot_max_sizes[i] = 5</code>.
While the size for continuous features can either be <code>0</code> or <code>1</code>.</p>
</td></tr>
<tr><td><code id="vaeac_normalize_data_+3A_norm_mean">norm_mean</code></td>
<td>
<p>Torch tensor (optional). A 1D array containing the means of the columns of <code>x_torch</code>.</p>
</td></tr>
<tr><td><code id="vaeac_normalize_data_+3A_norm_std">norm_std</code></td>
<td>
<p>Torch tensor (optional). A 1D array containing the stds of the columns of <code>x_torch</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the normalized version of <code>x_torch</code>, <code>norm_mean</code> and <code>norm_std</code>.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_postprocess_data'>Postprocess Data Generated by a vaeac Model</h2><span id='topic+vaeac_postprocess_data'></span>

<h3>Description</h3>

<p>vaeac generates numerical values. This function converts categorical features
to from numerics with class labels 1,2,...,K, to factors with the original and class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_postprocess_data(data, vaeac_model_state_list)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_postprocess_data_+3A_data">data</code></td>
<td>
<p>data.table containing the data generated by a vaeac model</p>
</td></tr>
<tr><td><code id="vaeac_postprocess_data_+3A_vaeac_model_state_list">vaeac_model_state_list</code></td>
<td>
<p>List. The returned list from the <code>vaeac_preprocess_data</code> function or
a loaded checkpoint list of a saved vaeac object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table with the generated data from a vaeac model where the categorical features
now have the original class names.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data &lt;- data.table(matrix(rgamma(500 * 3, 2), ncol = 3))
preprocessed &lt;- vaeac_preprocess_data(data)
preprocessed$data_preprocessed
postprocessed &lt;- vaeac_postprocess_data(preprocessed$data_preprocessed, preprocessed)
postprocessed
all.equal(data, postprocessed)

## End(Not run)
</code></pre>

<hr>
<h2 id='vaeac_preprocess_data'>Preprocess Data for the vaeac approach</h2><span id='topic+vaeac_preprocess_data'></span>

<h3>Description</h3>

<p>vaeac only supports numerical values. This function converts categorical features
to numerics with class labels 1,2,...,K, and keeps track of the map between the original and
new class labels. It also computes the one_hot_max_sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_preprocess_data(
  data,
  log_exp_cont_feat = FALSE,
  normalize = TRUE,
  norm_mean = NULL,
  norm_std = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_preprocess_data_+3A_data">data</code></td>
<td>
<p>matrix/data.frame/data.table containing the training data. Only the features and
not the response.</p>
</td></tr>
<tr><td><code id="vaeac_preprocess_data_+3A_log_exp_cont_feat">log_exp_cont_feat</code></td>
<td>
<p>Boolean. If we are to log transform all continuous
features before sending the data to vaeac. vaeac creates unbounded values, so if the continuous
features are strictly positive, as for Burr and Abalone data, it can be advantageous to log-transform
the data to unbounded form before using vaeac. If TRUE, then <code>vaeac_postprocess_data</code> will
take the exp of the results to get back to strictly positive values.</p>
</td></tr>
<tr><td><code id="vaeac_preprocess_data_+3A_norm_mean">norm_mean</code></td>
<td>
<p>Torch tensor (optional). A 1D array containing the means of the columns of <code>x_torch</code>.</p>
</td></tr>
<tr><td><code id="vaeac_preprocess_data_+3A_norm_std">norm_std</code></td>
<td>
<p>Torch tensor (optional). A 1D array containing the stds of the columns of <code>x_torch</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list containing data which can be used in vaeac, maps between original and new class
names for categorical features, one_hot_max_sizes, and list of information about the data.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_print_train_summary'>Function to printout a training summary for the <code>vaeac</code> model</h2><span id='topic+vaeac_print_train_summary'></span>

<h3>Description</h3>

<p>Function to printout a training summary for the <code>vaeac</code> model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_print_train_summary(best_epoch, best_epoch_running, last_state)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_print_train_summary_+3A_best_epoch">best_epoch</code></td>
<td>
<p>Positive integer. The epoch with the lowest validation error.</p>
</td></tr>
<tr><td><code id="vaeac_print_train_summary_+3A_best_epoch_running">best_epoch_running</code></td>
<td>
<p>Positive integer. The epoch with the lowest running validation error.</p>
</td></tr>
<tr><td><code id="vaeac_print_train_summary_+3A_last_state">last_state</code></td>
<td>
<p>The state list (i.e., the saved <code>vaeac</code> object)
of <code>vaeac</code> model at the epoch with the lowest IWAE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function only prints out a message.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_save_state'>Function that saves the state list and the current save state of the <code>vaeac</code> model</h2><span id='topic+vaeac_save_state'></span>

<h3>Description</h3>

<p>Function that saves the state list and the current save state of the <code>vaeac</code> model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_save_state(state_list, file_name, return_state = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_save_state_+3A_state_list">state_list</code></td>
<td>
<p>List containing all the parameters in the state.</p>
</td></tr>
<tr><td><code id="vaeac_save_state_+3A_file_name">file_name</code></td>
<td>
<p>String containing the file path.</p>
</td></tr>
<tr><td><code id="vaeac_save_state_+3A_return_state">return_state</code></td>
<td>
<p>Logical if we are to return the state list or not.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function does not return anything
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_train_model'>Train the Vaeac Model</h2><span id='topic+vaeac_train_model'></span>

<h3>Description</h3>

<p>Function that fits a vaeac model to the given dataset based on the provided parameters,
as described in <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>. Note that
all default parameters specified below origin from <code><a href="#topic+setup_approach.vaeac">setup_approach.vaeac()</a></code> and
<code><a href="#topic+vaeac_get_extra_para_default">vaeac_get_extra_para_default()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_train_model(
  x_train,
  model_description,
  folder_to_save_model,
  cuda,
  n_vaeacs_initialize,
  epochs_initiation_phase,
  epochs,
  epochs_early_stopping,
  save_every_nth_epoch,
  val_ratio,
  val_iwae_n_samples,
  depth,
  width,
  latent_dim,
  lr,
  batch_size,
  running_avg_n_values,
  activation_function,
  skip_conn_layer,
  skip_conn_masked_enc_dec,
  batch_normalization,
  paired_sampling,
  masking_ratio,
  mask_gen_coalitions,
  mask_gen_coalitions_prob,
  sigma_mu,
  sigma_sigma,
  save_data,
  log_exp_cont_feat,
  which_vaeac_model,
  verbose,
  seed,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_train_model_+3A_x_train">x_train</code></td>
<td>
<p>A data.table containing the training data. Categorical data must have class names <code class="reqn">1,2,\dots,K</code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_model_description">model_description</code></td>
<td>
<p>String (default is <code>make.names(Sys.time())</code>). String containing, e.g., the name of the
data distribution or additional parameter information. Used in the save name of the fitted model. If not provided,
then a name will be generated based on <code><a href="base.html#topic+Sys.time">base::Sys.time()</a></code> to ensure a unique name. We use <code><a href="base.html#topic+make.names">base::make.names()</a></code> to
ensure a valid file name for all operating systems.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_folder_to_save_model">folder_to_save_model</code></td>
<td>
<p>String (default is <code><a href="base.html#topic+tempfile">base::tempdir()</a></code>). String specifying a path to a folder where
the function is to save the fitted vaeac model. Note that  the path will be removed from the returned
<code><a href="#topic+explain">explain()</a></code> object if <code>vaeac.save_model = FALSE</code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_cuda">cuda</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the <code>vaeac</code> model will be trained using cuda/GPU.
If <code><a href="torch.html#topic+cuda_is_available">torch::cuda_is_available()</a></code> is <code>FALSE</code>, the we fall back to use CPU. If <code>FALSE</code>, we use the CPU. Using a GPU
for smaller tabular dataset often do not improve the efficiency.
See <code>vignette("installation", package = "torch")</code> fo help to enable running on the GPU (only Linux and Windows).</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_n_vaeacs_initialize">n_vaeacs_initialize</code></td>
<td>
<p>Positive integer (default is <code>4</code>). The number of different vaeac models to initiate
in the start. Pick the best performing one after <code>epochs_initiation_phase</code>
epochs (default is <code>2</code>) and continue training that one.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_epochs_initiation_phase">epochs_initiation_phase</code></td>
<td>
<p>Positive integer (default is <code>2</code>). The number of epochs to run each of the
<code>n_vaeacs_initialize</code> <code>vaeac</code> models before continuing to train only the best performing model.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_epochs">epochs</code></td>
<td>
<p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_epochs_early_stopping">epochs_early_stopping</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). The training stops if there has been no
improvement in the validation IWAE for <code>epochs_early_stopping</code> epochs. If the user wants the training process
to be solely based on this training criterion, then <code>epochs</code> in <code><a href="#topic+explain">explain()</a></code> should be set to a large
number. If <code>NULL</code>, then <code>shapr</code> will internally set <code>epochs_early_stopping = vaeac.epochs</code> such that early
stopping does not occur.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_save_every_nth_epoch">save_every_nth_epoch</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). If provided, then the vaeac model after
every <code>save_every_nth_epoch</code>th epoch will be saved.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_val_ratio">val_ratio</code></td>
<td>
<p>Numeric (default is <code>0.25</code>). Scalar between <code>0</code> and <code>1</code> indicating the ratio of
instances from the input data which will be used as validation data. That is, <code>val_ratio = 0.25</code> means
that <code style="white-space: pre;">&#8288;75%&#8288;</code> of the provided data is used as training data, while the remaining <code style="white-space: pre;">&#8288;25%&#8288;</code> is used as validation data.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_val_iwae_n_samples">val_iwae_n_samples</code></td>
<td>
<p>Positive integer (default is <code>25</code>). The number of generated samples used
to compute the IWAE criterion when validating the vaeac model on the validation data.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_depth">depth</code></td>
<td>
<p>Positive integer (default is <code>3</code>). The number of hidden layers
in the neural networks of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_width">width</code></td>
<td>
<p>Positive integer (default is <code>32</code>). The number of neurons in each
hidden layer in the neural networks of the masked encoder, full encoder, and decoder.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_latent_dim">latent_dim</code></td>
<td>
<p>Positive integer (default is <code>8</code>). The number of dimensions in the latent space.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_lr">lr</code></td>
<td>
<p>Positive numeric (default is <code>0.001</code>). The learning rate used in the <code><a href="torch.html#topic+optim_adam">torch::optim_adam()</a></code> optimizer.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_batch_size">batch_size</code></td>
<td>
<p>Positive integer (default is <code>64</code>). The number of samples to include in each batch
during the training of the vaeac model. Used in <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_running_avg_n_values">running_avg_n_values</code></td>
<td>
<p>running_avg_n_values Positive integer (default is <code>5</code>).
The number of previous IWAE values to include
when we compute the running means of the IWAE criterion.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_activation_function">activation_function</code></td>
<td>
<p>An <code><a href="torch.html#topic+nn_module">torch::nn_module()</a></code> representing an activation function such as, e.g.,
<code><a href="torch.html#topic+nn_relu">torch::nn_relu()</a></code> (default), <code><a href="torch.html#topic+nn_leaky_relu">torch::nn_leaky_relu()</a></code>, <code><a href="torch.html#topic+nn_selu">torch::nn_selu()</a></code>, or <code><a href="torch.html#topic+nn_sigmoid">torch::nn_sigmoid()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_skip_conn_layer">skip_conn_layer</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, we apply identity skip connections in each
layer, see <code><a href="#topic+skip_connection">skip_connection()</a></code>. That is, we add the input <code class="reqn">X</code> to the outcome of each hidden layer,
so the output becomes <code class="reqn">X + activation(WX + b)</code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_skip_conn_masked_enc_dec">skip_conn_masked_enc_dec</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, we apply concatenate skip
connections between the layers in the masked encoder and decoder. The first layer of the masked encoder will be
linked to the last layer of the decoder. The second layer of the masked encoder will be
linked to the second to last layer of the decoder, and so on.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_batch_normalization">batch_normalization</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, we apply batch normalization after the
activation function. Note that if <code>skip_conn_layer = TRUE</code>, then the normalization is applied after the
inclusion of the skip connection. That is, we batch normalize the whole quantity <code class="reqn">X + activation(WX + b)</code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_paired_sampling">paired_sampling</code></td>
<td>
<p>Logical (default is <code>TRUE</code>). If <code>TRUE</code>, we apply paired sampling to the training
batches. That is, the training observations in each batch will be duplicated, where the first instance will be masked
by <code class="reqn">S</code> while the second instance will be masked by <code class="reqn">\bar{S}</code>. This ensures that the training of the
<code>vaeac</code> model becomes more stable as the model has access to the full version of each training observation. However,
this will increase the training time due to more complex implementation and doubling the size of each batch. See
<code><a href="#topic+paired_sampler">paired_sampler()</a></code> for more information.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_masking_ratio">masking_ratio</code></td>
<td>
<p>Numeric (default is <code>0.5</code>). Probability of masking a feature in the
<code><a href="#topic+mcar_mask_generator">mcar_mask_generator()</a></code> (MCAR = Missing Completely At Random). The MCAR masking scheme ensures that <code>vaeac</code>
model can do arbitrary conditioning as all coalitions will be trained. <code>masking_ratio</code> will be overruled if
<code>mask_gen_coalitions</code> is specified.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_mask_gen_coalitions">mask_gen_coalitions</code></td>
<td>
<p>Matrix (default is <code>NULL</code>). Matrix containing the coalitions that the
<code>vaeac</code> model will be trained on, see <code><a href="#topic+specified_masks_mask_generator">specified_masks_mask_generator()</a></code>. This parameter is used internally
in <code>shapr</code> when we only consider a subset of coalitions, i.e., when
<code>n_coalitions</code> <code class="reqn">&lt; 2^{n_{\text{features}}}</code>, and for group Shapley, i.e.,
when <code>group</code> is specified in <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_mask_gen_coalitions_prob">mask_gen_coalitions_prob</code></td>
<td>
<p>Numeric array (default is <code>NULL</code>). Array of length equal to the height
of <code>mask_gen_coalitions</code> containing the probabilities of sampling the corresponding coalitions in
<code>mask_gen_coalitions</code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_sigma_mu">sigma_mu</code></td>
<td>
<p>Numeric (default is <code>1e4</code>). One of two hyperparameter values in the normal-gamma prior
used in the masked encoder, see Section 3.3.1 in
<a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_sigma_sigma">sigma_sigma</code></td>
<td>
<p>Numeric (default is <code>1e-4</code>). One of two hyperparameter values in the normal-gamma prior
used in the masked encoder, see Section 3.3.1 in
<a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_save_data">save_data</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the data is stored together with
the model. Useful if one are to continue to train the model later using <code><a href="#topic+vaeac_train_model_continue">vaeac_train_model_continue()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_log_exp_cont_feat">log_exp_cont_feat</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If we are to <code class="reqn">\log</code> transform all
continuous features before sending the data to <code><a href="#topic+vaeac">vaeac()</a></code>. The <code>vaeac</code> model creates unbounded Monte Carlo
sample values. Thus, if the continuous features are strictly positive (as for, e.g., the Burr distribution and
Abalone data set), it can be advantageous to <code class="reqn">\log</code> transform the data to unbounded form before using <code>vaeac</code>.
If <code>TRUE</code>, then <code><a href="#topic+vaeac_postprocess_data">vaeac_postprocess_data()</a></code> will take the <code class="reqn">\exp</code> of the results to get back to strictly
positive values when using the <code>vaeac</code> model to impute missing values/generate the Monte Carlo samples.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_which_vaeac_model">which_vaeac_model</code></td>
<td>
<p>String (default is <code>best</code>). The name of the <code>vaeac</code> model (snapshots from different
epochs) to use when generating the Monte Carlo samples. The standard choices are: <code>"best"</code> (epoch with lowest IWAE),
<code>"best_running"</code> (epoch with lowest running IWAE, see <code>vaeac.running_avg_n_values</code>), and <code>last</code> (the last epoch).
Note that additional choices are available if <code>vaeac.save_every_nth_epoch</code> is provided. For example, if
<code>vaeac.save_every_nth_epoch = 5</code>, then <code>vaeac.which_vaeac_model</code> can also take the values <code>"epoch_5"</code>, <code>"epoch_10"</code>,
<code>"epoch_15"</code>, and so on.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_seed">seed</code></td>
<td>
<p>Positive integer (default is <code>1</code>). Seed for reproducibility. Specifies the seed before any randomness
based code is being run.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_+3A_...">...</code></td>
<td>
<p>List of extra parameters, currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The vaeac model consists of three neural networks, i.e., a masked encoder, a full encoder, and a decoder.
The networks have shared <code>depth</code>, <code>width</code>, and <code>activation_function</code>. The encoders maps the <code>x_train</code>
to a latent representation of dimension <code>latent_dim</code>, while the decoder maps the latent representations
back to the feature space. See <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">Olsen et al. (2022)</a>
for more details. The function first initiates <code>n_vaeacs_initialize</code> vaeac models with different randomly
initiated network parameter values to remedy poorly initiated values. After <code>epochs_initiation_phase</code> epochs, the
<code>n_vaeacs_initialize</code> vaeac models are compared and the function continues to only train the best performing
one for a total of <code>epochs</code> epochs. The networks are trained using the ADAM optimizer with the learning rate is <code>lr</code>.
</p>


<h3>Value</h3>

<p>A list containing the training/validation errors and paths to where the vaeac models are saved on the disk.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">
Olsen, L. H., Glad, I. K., Jullum, M., &amp; Aas, K. (2022). Using Shapley values and variational autoencoders to
explain predictive models with dependent mixed features. Journal of machine learning research, 23(213), 1-51</a>
</p>
</li></ul>


<hr>
<h2 id='vaeac_train_model_auxiliary'>Function used to train a <code>vaeac</code> model</h2><span id='topic+vaeac_train_model_auxiliary'></span>

<h3>Description</h3>

<p>This function can be applied both in the initialization phase when, we train several initiated <code>vaeac</code> models, and
to keep training the best performing <code>vaeac</code> model for the remaining number of epochs. We are in the former setting
when <code>initialization_idx</code> is provided and the latter when it is <code>NULL</code>. When it is <code>NULL</code>, we save the <code>vaeac</code> models
with lowest VLB, IWAE, running IWAE, and the epochs according to <code>save_every_nth_epoch</code> to disk.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_train_model_auxiliary(
  vaeac_model,
  optimizer,
  train_dataloader,
  val_dataloader,
  val_iwae_n_samples,
  running_avg_n_values,
  verbose,
  cuda,
  epochs,
  save_every_nth_epoch,
  epochs_early_stopping,
  epochs_start = 1,
  progressr_bar = NULL,
  vaeac_save_file_names = NULL,
  state_list = NULL,
  initialization_idx = NULL,
  n_vaeacs_initialize = NULL,
  train_vlb = NULL,
  val_iwae = NULL,
  val_iwae_running = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_train_model_auxiliary_+3A_vaeac_model">vaeac_model</code></td>
<td>
<p>A <code><a href="#topic+vaeac">vaeac()</a></code> object. The <code>vaeac</code> model this function is to train.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_optimizer">optimizer</code></td>
<td>
<p>A <code><a href="torch.html#topic+optimizer">torch::optimizer()</a></code> object. See <code><a href="#topic+vaeac_get_optimizer">vaeac_get_optimizer()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_train_dataloader">train_dataloader</code></td>
<td>
<p>A <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code> containing the training data for the <code>vaeac</code> model.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_val_dataloader">val_dataloader</code></td>
<td>
<p>A <code><a href="torch.html#topic+dataloader">torch::dataloader()</a></code> containing the validation data for the <code>vaeac</code> model.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_val_iwae_n_samples">val_iwae_n_samples</code></td>
<td>
<p>Positive integer (default is <code>25</code>). The number of generated samples used
to compute the IWAE criterion when validating the vaeac model on the validation data.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_running_avg_n_values">running_avg_n_values</code></td>
<td>
<p>running_avg_n_values Positive integer (default is <code>5</code>).
The number of previous IWAE values to include
when we compute the running means of the IWAE criterion.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_cuda">cuda</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the <code>vaeac</code> model will be trained using cuda/GPU.
If <code><a href="torch.html#topic+cuda_is_available">torch::cuda_is_available()</a></code> is <code>FALSE</code>, the we fall back to use CPU. If <code>FALSE</code>, we use the CPU. Using a GPU
for smaller tabular dataset often do not improve the efficiency.
See <code>vignette("installation", package = "torch")</code> fo help to enable running on the GPU (only Linux and Windows).</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_epochs">epochs</code></td>
<td>
<p>Positive integer (default is <code>100</code>). The number of epochs to train the final vaeac model.
This includes <code>epochs_initiation_phase</code>, where the default is <code>2</code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_save_every_nth_epoch">save_every_nth_epoch</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). If provided, then the vaeac model after
every <code>save_every_nth_epoch</code>th epoch will be saved.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_epochs_early_stopping">epochs_early_stopping</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). The training stops if there has been no
improvement in the validation IWAE for <code>epochs_early_stopping</code> epochs. If the user wants the training process
to be solely based on this training criterion, then <code>epochs</code> in <code><a href="#topic+explain">explain()</a></code> should be set to a large
number. If <code>NULL</code>, then <code>shapr</code> will internally set <code>epochs_early_stopping = vaeac.epochs</code> such that early
stopping does not occur.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_epochs_start">epochs_start</code></td>
<td>
<p>Positive integer (default is <code>1</code>). At which epoch the training is starting at.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_progressr_bar">progressr_bar</code></td>
<td>
<p>A <code><a href="progressr.html#topic+progressor">progressr::progressor()</a></code> object (default is <code>NULL</code>) to keep track of progress.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_vaeac_save_file_names">vaeac_save_file_names</code></td>
<td>
<p>Array of strings containing the save file names for the <code>vaeac</code> model.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_state_list">state_list</code></td>
<td>
<p>Named list containing the objects returned from <code><a href="#topic+vaeac_get_full_state_list">vaeac_get_full_state_list()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_initialization_idx">initialization_idx</code></td>
<td>
<p>Positive integer (default is <code>NULL</code>). The index
of the current <code>vaeac</code> model in the initialization phase.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_n_vaeacs_initialize">n_vaeacs_initialize</code></td>
<td>
<p>Positive integer (default is <code>4</code>). The number of different vaeac models to initiate
in the start. Pick the best performing one after <code>epochs_initiation_phase</code>
epochs (default is <code>2</code>) and continue training that one.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_train_vlb">train_vlb</code></td>
<td>
<p>A <code><a href="torch.html#topic+torch_tensor">torch::torch_tensor()</a></code> (default is <code>NULL</code>)
of one dimension containing previous values for the training VLB.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_val_iwae">val_iwae</code></td>
<td>
<p>A <code><a href="torch.html#topic+torch_tensor">torch::torch_tensor()</a></code> (default is <code>NULL</code>)
of one dimension containing previous values for the validation IWAE.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_auxiliary_+3A_val_iwae_running">val_iwae_running</code></td>
<td>
<p>A <code><a href="torch.html#topic+torch_tensor">torch::torch_tensor()</a></code> (default is <code>NULL</code>)
of one dimension containing previous values for the running validation IWAE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Depending on if we are in the initialization phase or not. Then either the trained <code>vaeac</code> model, or
a list of where the <code>vaeac</code> models are stored on disk and the parameters of the model.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_train_model_continue'>Continue to Train the vaeac Model</h2><span id='topic+vaeac_train_model_continue'></span>

<h3>Description</h3>

<p>Function that loads a previously trained vaeac model and continue the training, either
on new data or on the same dataset as it was trained on before. If we are given a new dataset, then
we assume that new dataset has the same distribution and one_hot_max_sizes as the original dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_train_model_continue(
  explanation,
  epochs_new,
  lr_new = NULL,
  x_train = NULL,
  save_data = FALSE,
  verbose = NULL,
  seed = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_train_model_continue_+3A_explanation">explanation</code></td>
<td>
<p>A <code><a href="#topic+explain">explain()</a></code> object and <code>vaeac</code> must be the used approach.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_continue_+3A_epochs_new">epochs_new</code></td>
<td>
<p>Positive integer. The number of extra epochs to conduct.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_continue_+3A_lr_new">lr_new</code></td>
<td>
<p>Positive numeric. If we are to overwrite the old learning rate in the adam optimizer.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_continue_+3A_x_train">x_train</code></td>
<td>
<p>A data.table containing the training data. Categorical data must have class names <code class="reqn">1,2,\dots,K</code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_continue_+3A_save_data">save_data</code></td>
<td>
<p>Logical (default is <code>FALSE</code>). If <code>TRUE</code>, then the data is stored together with
the model. Useful if one are to continue to train the model later using <code><a href="#topic+vaeac_train_model_continue">vaeac_train_model_continue()</a></code>.</p>
</td></tr>
<tr><td><code id="vaeac_train_model_continue_+3A_verbose">verbose</code></td>
<td>
<p>String vector or NULL.
Specifies the verbosity (printout detail level) through one or more of strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and <code>"vS_details"</code>.
<code>"basic"</code> (default) displays basic information about the computation which is being performed.
<code style="white-space: pre;">&#8288;"progress&#8288;</code> displays information about where in the calculation process the function currently is.
#' <code>"convergence"</code> displays information on how close to convergence the Shapley value estimates are
(only when <code>iterative = TRUE</code>) .
<code>"shapley"</code> displays intermediate Shapley value estimates and standard deviations (only when <code>iterative = TRUE</code>)
</p>

<ul>
<li><p> the final estimates.
<code>"vS_details"</code> displays information about the v_S estimates.
This is most relevant for <code style="white-space: pre;">&#8288;approach %in% c("regression_separate", "regression_surrogate", "vaeac"&#8288;</code>).
<code>NULL</code> means no printout.
Note that any combination of four strings can be used.
E.g. <code>verbose = c("basic", "vS_details")</code> will display basic information + details about the v(S)-estimation process.
</p>
</li></ul>
</td></tr>
<tr><td><code id="vaeac_train_model_continue_+3A_seed">seed</code></td>
<td>
<p>Positive integer (default is <code>1</code>). Seed for reproducibility. Specifies the seed before any randomness
based code is being run.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the training/validation errors and paths to where the vaeac models are saved on the disk.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf">
Olsen, L. H., Glad, I. K., Jullum, M., &amp; Aas, K. (2022). Using Shapley values and variational autoencoders to
explain predictive models with dependent mixed features. Journal of machine learning research, 23(213), 1-51</a>
</p>
</li></ul>


<hr>
<h2 id='vaeac_update_para_locations'>Move <code>vaeac</code> parameters to correct location</h2><span id='topic+vaeac_update_para_locations'></span>

<h3>Description</h3>

<p>This function ensures that the main and extra parameters for the <code>vaeac</code>
approach is located at their right locations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_update_para_locations(parameters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_update_para_locations_+3A_parameters">parameters</code></td>
<td>
<p>List. The <code>internal$parameters</code> list created inside the <code><a href="#topic+explain">explain()</a></code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Updated version of <code>parameters</code> where all <code>vaeac</code> parameters are located at the correct location.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='vaeac_update_pretrained_model'>Function that checks and adds a pre-trained <code>vaeac</code> model</h2><span id='topic+vaeac_update_pretrained_model'></span>

<h3>Description</h3>

<p>Function that checks and adds a pre-trained <code>vaeac</code> model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vaeac_update_pretrained_model(parameters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vaeac_update_pretrained_model_+3A_parameters">parameters</code></td>
<td>
<p>List containing the parameters used within <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function adds a valid pre-trained vaeac model to the <code>parameter</code>.
</p>


<h3>Author(s)</h3>

<p>Lars Henry Berge Olsen
</p>

<hr>
<h2 id='weight_matrix'>Calculate weighted matrix</h2><span id='topic+weight_matrix'></span>

<h3>Description</h3>

<p>Calculate weighted matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weight_matrix(X, normalize_W_weights = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weight_matrix_+3A_x">X</code></td>
<td>
<p>data.table.
Output from <code><a href="#topic+create_coalition_table">create_coalition_table()</a></code>.</p>
</td></tr>
<tr><td><code id="weight_matrix_+3A_normalize_w_weights">normalize_W_weights</code></td>
<td>
<p>Logical. Whether to normalize the weights for the coalitions to sum to 1 for
increased numerical stability before solving the WLS (weighted least squares). Applies to all coalitions
except coalition <code>1</code> and <code>2^m</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric matrix. See <code><a href="#topic+weight_matrix_cpp">weight_matrix_cpp()</a></code> for more information.
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite, Martin Jullum
</p>

<hr>
<h2 id='weight_matrix_cpp'>Calculate weight matrix</h2><span id='topic+weight_matrix_cpp'></span>

<h3>Description</h3>

<p>Calculate weight matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weight_matrix_cpp(coalitions, m, n, w)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weight_matrix_cpp_+3A_coalitions">coalitions</code></td>
<td>
<p>List.
Each of the elements equals an integer vector representing a valid combination of features/feature groups.</p>
</td></tr>
<tr><td><code id="weight_matrix_cpp_+3A_m">m</code></td>
<td>
<p>Integer.
Number of features/feature groups.</p>
</td></tr>
<tr><td><code id="weight_matrix_cpp_+3A_n">n</code></td>
<td>
<p>Integer.
Number of combinations.</p>
</td></tr>
<tr><td><code id="weight_matrix_cpp_+3A_w">w</code></td>
<td>
<p>Numeric vector
Should have length <code>n</code>. <code>w[i]</code> equals the Shapley weight of feature/feature group combination <code>i</code>,
represented by <code>coalitions[[i]]</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix of dimension n x m + 1
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite, Martin Jullum
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
