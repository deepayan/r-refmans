<!DOCTYPE html><html><head><title>Help for package shapr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {shapr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aicc_full_cpp'><p>AICc formula for several sets, alternative definition</p></a></li>
<li><a href='#aicc_full_single_cpp'><p>Temp-function for computing the full AICc with several X's etc</p></a></li>
<li><a href='#apply_dummies'><p>Apply dummy variables - this is an internal function intended only to be used in</p>
predict_model.xgb.Booster()</a></li>
<li><a href='#check_features'><p>Checks that two extracted feature lists have exactly the same properties</p></a></li>
<li><a href='#correction_matrix_cpp'><p>correction term with trace_input in AICc formula</p></a></li>
<li><a href='#create_ctree'><p>Make all conditional inference trees</p></a></li>
<li><a href='#explain'><p>Explain the output of machine learning models with more accurately estimated Shapley values</p></a></li>
<li><a href='#feature_combinations'><p>Define feature combinations, and fetch additional information about each unique combination</p></a></li>
<li><a href='#feature_matrix_cpp'><p>Get feature matrix</p></a></li>
<li><a href='#gaussian_transform'><p>Transforms a sample to standardized normal distribution</p></a></li>
<li><a href='#gaussian_transform_separate'><p>Transforms new data to standardized normal (dimension 1) based on other data transformations</p></a></li>
<li><a href='#get_data_specs'><p>Fetches feature information from a given data set</p></a></li>
<li><a href='#get_list_approaches'><p>Helper function used in <code>explain.combined</code></p></a></li>
<li><a href='#get_model_specs'><p>Fetches feature information from a given model object</p></a></li>
<li><a href='#get_supported_models'><p>Provides a data.table with the supported models</p></a></li>
<li><a href='#hat_matrix_cpp'><p>Computing single H matrix in AICc-function using the Mahalanobis distance</p></a></li>
<li><a href='#inv_gaussian_transform'><p>Transforms new data to a standardized normal distribution</p></a></li>
<li><a href='#mahalanobis_distance_cpp'><p>(Generalized) Mahalanobis distance</p></a></li>
<li><a href='#make_dummies'><p>Initiate the making of dummy variables</p></a></li>
<li><a href='#model_checker'><p>Check that the type of model is supported by the explanation method</p></a></li>
<li><a href='#observation_impute'><p>Generate permutations of training data using test observations</p></a></li>
<li><a href='#observation_impute_cpp'><p>Get imputed data</p></a></li>
<li><a href='#plot.shapr'><p>Plot of the Shapley value explanations</p></a></li>
<li><a href='#predict_model'><p>Generate predictions for different model classes</p></a></li>
<li><a href='#prediction'><p>Calculate Shapley weights for test data</p></a></li>
<li><a href='#prepare_data'><p>Generate data used for predictions</p></a></li>
<li><a href='#preprocess_data'><p>Process (check and update) data according to specified feature list</p></a></li>
<li><a href='#rss_cpp'><p>sigma_hat_sq-function</p></a></li>
<li><a href='#sample_combinations'><p>Helper function to sample a combination of training and testing rows, which does not risk</p>
getting the same observation twice. Need to improve this help file.</a></li>
<li><a href='#sample_copula'><p>Sample conditional variables using the Gaussian copula approach</p></a></li>
<li><a href='#sample_ctree'><p>Sample ctree variables from a given conditional inference tree</p></a></li>
<li><a href='#sample_gaussian'><p>Sample conditional Gaussian variables</p></a></li>
<li><a href='#shapley_weights'><p>Calculate Shapley weight</p></a></li>
<li><a href='#shapr'><p>Create an explainer object with Shapley weights for test data.</p></a></li>
<li><a href='#update_data'><p>Updates data by reference according to the updater argument.</p></a></li>
<li><a href='#weight_matrix'><p>Calculate weighted matrix</p></a></li>
<li><a href='#weight_matrix_cpp'><p>Calculate weight matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.2.2</td>
</tr>
<tr>
<td>Title:</td>
<td>Prediction Explanation with Dependence-Aware Shapley Values</td>
</tr>
<tr>
<td>Description:</td>
<td>Complex machine learning models are often hard to interpret. However, in 
  many situations it is crucial to understand and explain why a model made a specific 
  prediction. Shapley values is the only method for such prediction explanation framework 
  with a solid theoretical foundation. Previously known methods for estimating the Shapley 
  values do, however, assume feature independence. This package implements the method 
  described in Aas, Jullum and Løland (2019) &lt;<a href="https://arxiv.org/abs/1903.10464">arXiv:1903.10464</a>&gt;, which accounts for any feature 
  dependence, and thereby produces more accurate estimates of the true Shapley values.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://norskregnesentral.github.io/shapr/">https://norskregnesentral.github.io/shapr/</a>,
<a href="https://github.com/NorskRegnesentral/shapr">https://github.com/NorskRegnesentral/shapr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/NorskRegnesentral/shapr/issues">https://github.com/NorskRegnesentral/shapr/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, data.table, Rcpp (&ge; 0.12.15), condMVNorm, mvnfast,
Matrix</td>
</tr>
<tr>
<td>Suggests:</td>
<td>ranger, xgboost, mgcv, testthat, knitr, rmarkdown, roxygen2,
MASS, ggplot2, caret, gbm, party, partykit</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>RcppArmadillo, Rcpp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-04 12:45:00 UTC; jullum</td>
</tr>
<tr>
<td>Author:</td>
<td>Nikolai Sellereite
    <a href="https://orcid.org/0000-0002-4671-0337"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Martin Jullum <a href="https://orcid.org/0000-0003-3908-5155"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre, aut],
  Annabelle Redelmeier [aut],
  Anders Løland [ctb],
  Jens Christian Wahl [ctb],
  Camilla Lingjærde [ctb],
  Norsk Regnesentral [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Martin Jullum &lt;Martin.Jullum@nr.no&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-04 14:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='aicc_full_cpp'>AICc formula for several sets, alternative definition</h2><span id='topic+aicc_full_cpp'></span>

<h3>Description</h3>

<p>AICc formula for several sets, alternative definition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aicc_full_cpp(h, X_list, mcov_list, S_scale_dist, y_list, negative)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aicc_full_cpp_+3A_h">h</code></td>
<td>
<p>Numeric. Specifies the scaling (sigma)</p>
</td></tr>
<tr><td><code id="aicc_full_cpp_+3A_x_list">X_list</code></td>
<td>
<p>List</p>
</td></tr>
<tr><td><code id="aicc_full_cpp_+3A_mcov_list">mcov_list</code></td>
<td>
<p>List</p>
</td></tr>
<tr><td><code id="aicc_full_cpp_+3A_s_scale_dist">S_scale_dist</code></td>
<td>
<p>Logical. Indicates whether Mahalanobis distance should be scaled with the
number of variables</p>
</td></tr>
<tr><td><code id="aicc_full_cpp_+3A_y_list">y_list</code></td>
<td>
<p>List.</p>
</td></tr>
<tr><td><code id="aicc_full_cpp_+3A_negative">negative</code></td>
<td>
<p>Logical.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar with the numeric value of the AICc formula
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='aicc_full_single_cpp'>Temp-function for computing the full AICc with several X's etc</h2><span id='topic+aicc_full_single_cpp'></span>

<h3>Description</h3>

<p>Temp-function for computing the full AICc with several X's etc
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aicc_full_single_cpp(X, mcov, S_scale_dist, h, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aicc_full_single_cpp_+3A_x">X</code></td>
<td>
<p>matrix with &quot;covariates&quot;</p>
</td></tr>
<tr><td><code id="aicc_full_single_cpp_+3A_mcov">mcov</code></td>
<td>
<p>covariance matrix</p>
</td></tr>
<tr><td><code id="aicc_full_single_cpp_+3A_s_scale_dist">S_scale_dist</code></td>
<td>
<p>logical indicating whether the Mahalanobis distance should be scaled with the number of variables</p>
</td></tr>
<tr><td><code id="aicc_full_single_cpp_+3A_h">h</code></td>
<td>
<p>numeric specifying the scaling (sigma)</p>
</td></tr>
<tr><td><code id="aicc_full_single_cpp_+3A_y">y</code></td>
<td>
<p>vector with the &quot;response variable&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar with the numeric value of the AICc formula
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='apply_dummies'>Apply dummy variables - this is an internal function intended only to be used in
predict_model.xgb.Booster()</h2><span id='topic+apply_dummies'></span>

<h3>Description</h3>

<p>Apply dummy variables - this is an internal function intended only to be used in
predict_model.xgb.Booster()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>apply_dummies(feature_list, testdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="apply_dummies_+3A_feature_list">feature_list</code></td>
<td>
<p>List. The <code>feature_list</code> object in the output object after running
<code><a href="#topic+make_dummies">make_dummies</a></code></p>
</td></tr>
<tr><td><code id="apply_dummies_+3A_testdata">testdata</code></td>
<td>
<p>data.table or data.frame. New data that has the same
feature names, types, and levels as <code>feature_list</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with all features but where the factors in <code>testdata</code> are
one-hot encoded variables as specified in feature_list
</p>


<h3>Author(s)</h3>

<p>Annabelle Redelmeier, Martin Jullum
</p>

<hr>
<h2 id='check_features'>Checks that two extracted feature lists have exactly the same properties</h2><span id='topic+check_features'></span>

<h3>Description</h3>

<p>Checks that two extracted feature lists have exactly the same properties
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_features(f_list_1, f_list_2, use_1_as_truth = T)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_features_+3A_f_list_1">f_list_1</code>, <code id="check_features_+3A_f_list_2">f_list_2</code></td>
<td>
<p>List. As extracted from either <code>get_data_specs</code> or <code>get_model_specs</code>.</p>
</td></tr>
<tr><td><code id="check_features_+3A_use_1_as_truth">use_1_as_truth</code></td>
<td>
<p>Logical. If TRUE, <code>f_list_2</code> is compared to <code>f_list_1</code>, i.e. additional elements
is allowed in <code>f_list_2</code>, and if <code>f_list_1</code>'s feature classes contains NAs, feature class check is
ignored regardless of what is specified in <code>f_list_1</code>. If FALSE, <code>f_list_1</code> and <code>f_list_2</code> are
equated and they need to contain exactly the same elements. Set to TRUE when comparing a model and data, and FALSE
when comparing two data sets.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List. The <code>f_list_1</code> is returned as inserted if there all check are carried out. If some info is
missing from <code>f_list_1</code>, the function continues consistency checking using <code>f_list_2</code> and returns that.
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load example data
if (requireNamespace("MASS", quietly = TRUE)) {
  data("Boston", package = "MASS")
  # Split data into test- and training data
  x_train &lt;- data.table::as.data.table(head(Boston))
  x_train[, rad := as.factor(rad)]
  data_features &lt;- get_data_specs(x_train)
  model &lt;- lm(medv ~ lstat + rm + rad + indus, data = x_train)

  model_features &lt;- get_model_specs(model)
  check_features(model_features, data_features)
}
</code></pre>

<hr>
<h2 id='correction_matrix_cpp'>correction term with trace_input in AICc formula</h2><span id='topic+correction_matrix_cpp'></span>

<h3>Description</h3>

<p>correction term with trace_input in AICc formula
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correction_matrix_cpp(tr_H, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correction_matrix_cpp_+3A_tr_h">tr_H</code></td>
<td>
<p>numeric giving the trace of H</p>
</td></tr>
<tr><td><code id="correction_matrix_cpp_+3A_n">n</code></td>
<td>
<p>numeric given the number of rows in H</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='create_ctree'>Make all conditional inference trees</h2><span id='topic+create_ctree'></span>

<h3>Description</h3>

<p>Make all conditional inference trees
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_ctree(
  given_ind,
  x_train,
  mincriterion,
  minsplit,
  minbucket,
  use_partykit = "on_error"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_ctree_+3A_given_ind">given_ind</code></td>
<td>
<p>Numeric value. Indicates which features are conditioned on.</p>
</td></tr>
<tr><td><code id="create_ctree_+3A_x_train">x_train</code></td>
<td>
<p>Numeric vector. Indicates the specific values of features for individual i.</p>
</td></tr>
<tr><td><code id="create_ctree_+3A_mincriterion">mincriterion</code></td>
<td>
<p>Numeric value or vector equal to 1 - alpha where alpha is the nominal level of the conditional
independence tests.
Can also be a vector equal to the length of the number of features indicating which mincriterion to use
when conditioning on various numbers of features.</p>
</td></tr>
<tr><td><code id="create_ctree_+3A_minsplit">minsplit</code></td>
<td>
<p>Numeric value. Equal to the value that the sum of the left and right daughter nodes need to exceed.</p>
</td></tr>
<tr><td><code id="create_ctree_+3A_minbucket">minbucket</code></td>
<td>
<p>Numeric value. Equal to the minimum sum of weights in a terminal node.</p>
</td></tr>
<tr><td><code id="create_ctree_+3A_use_partykit">use_partykit</code></td>
<td>
<p>String. In some semi-rare cases <code>partyk::ctree</code> runs into an error related to the LINPACK
used by R. To get around this problem, one may fall back to using the newer (but slower) <code>partykit::ctree</code>
function, which is a reimplementation of the same method. Setting this parameter to <code>"on_error"</code> (default)
falls back to  <code>partykit::ctree</code>, if <code>party::ctree</code> fails. Other options are <code>"never"</code>, which always
uses <code>party::ctree</code>, and <code>"always"</code>, which always uses <code>partykit::ctree</code>. A warning message is
created whenever <code>partykit::ctree</code> is used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with conditional inference tree and the variables conditioned/not conditioned on.
</p>


<h3>Author(s)</h3>

<p>Annabelle Redelmeier, Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE) &amp; requireNamespace("party", quietly = TRUE)) {
  m &lt;- 10
  n &lt;- 40
  n_samples &lt;- 50
  mu &lt;- rep(1, m)
  cov_mat &lt;- cov(matrix(rnorm(n * m), n, m))
  x_train &lt;- data.table::data.table(MASS::mvrnorm(n, mu, cov_mat))
  given_ind &lt;- c(4, 7)
  mincriterion &lt;- 0.95
  minsplit &lt;- 20
  minbucket &lt;- 7
  sample &lt;- TRUE
  create_ctree(
    given_ind = given_ind, x_train = x_train,
    mincriterion = mincriterion, minsplit = minsplit,
    minbucket = minbucket, use_partykit = "on_error"
  )
}
</code></pre>

<hr>
<h2 id='explain'>Explain the output of machine learning models with more accurately estimated Shapley values</h2><span id='topic+explain'></span><span id='topic+explain.empirical'></span><span id='topic+explain.gaussian'></span><span id='topic+explain.copula'></span><span id='topic+explain.ctree'></span><span id='topic+explain.combined'></span><span id='topic+explain.ctree_comb_mincrit'></span>

<h3>Description</h3>

<p>Explain the output of machine learning models with more accurately estimated Shapley values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>explain(x, explainer, approach, prediction_zero, ...)

## S3 method for class 'empirical'
explain(
  x,
  explainer,
  approach,
  prediction_zero,
  type = "fixed_sigma",
  fixed_sigma_vec = 0.1,
  n_samples_aicc = 1000,
  eval_max_aicc = 20,
  start_aicc = 0.1,
  w_threshold = 0.95,
  ...
)

## S3 method for class 'gaussian'
explain(
  x,
  explainer,
  approach,
  prediction_zero,
  mu = NULL,
  cov_mat = NULL,
  ...
)

## S3 method for class 'copula'
explain(x, explainer, approach, prediction_zero, ...)

## S3 method for class 'ctree'
explain(
  x,
  explainer,
  approach,
  prediction_zero,
  mincriterion = 0.95,
  minsplit = 20,
  minbucket = 7,
  sample = TRUE,
  ...
)

## S3 method for class 'combined'
explain(
  x,
  explainer,
  approach,
  prediction_zero,
  mu = NULL,
  cov_mat = NULL,
  ...
)

## S3 method for class 'ctree_comb_mincrit'
explain(x, explainer, approach, prediction_zero, mincriterion, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="explain_+3A_x">x</code></td>
<td>
<p>A matrix or data.frame. Contains the the features, whose
predictions ought to be explained (test data).</p>
</td></tr>
<tr><td><code id="explain_+3A_explainer">explainer</code></td>
<td>
<p>An <code>explainer</code> object to use for explaining the observations.
See <code><a href="#topic+shapr">shapr</a></code>.</p>
</td></tr>
<tr><td><code id="explain_+3A_approach">approach</code></td>
<td>
<p>Character vector of length <code>1</code> or <code>n_features</code>.
<code>n_features</code> equals the total number of features in the model. All elements should
either be <code>"gaussian"</code>, <code>"copula"</code>, <code>"empirical"</code>, or <code>"ctree"</code>. See details for more
information.</p>
</td></tr>
<tr><td><code id="explain_+3A_prediction_zero">prediction_zero</code></td>
<td>
<p>Numeric. The prediction value for unseen data, typically equal to the mean of
the response.</p>
</td></tr>
<tr><td><code id="explain_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="#topic+prepare_data">prepare_data</a></code></p>
</td></tr>
<tr><td><code id="explain_+3A_type">type</code></td>
<td>
<p>Character. Should be equal to either <code>"independence"</code>,
<code>"fixed_sigma"</code>, <code>"AICc_each_k"</code> or <code>"AICc_full"</code>.</p>
</td></tr>
<tr><td><code id="explain_+3A_fixed_sigma_vec">fixed_sigma_vec</code></td>
<td>
<p>Numeric. Represents the kernel bandwidth. Note that this argument is only
applicable when <code>approach = "empirical"</code>, and <code>type = "fixed_sigma"</code></p>
</td></tr>
<tr><td><code id="explain_+3A_n_samples_aicc">n_samples_aicc</code></td>
<td>
<p>Positive integer. Number of samples to consider in AICc optimization.
Note that this argument is only applicable when <code>approach = "empirical"</code>, and <code>type</code>
is either equal to <code>"AICc_each_k"</code> or <code>"AICc_full"</code></p>
</td></tr>
<tr><td><code id="explain_+3A_eval_max_aicc">eval_max_aicc</code></td>
<td>
<p>Positive integer. Maximum number of iterations when
optimizing the AICc. Note that this argument is only applicable when
<code>approach = "empirical"</code>, and <code>type</code> is either equal to
<code>"AICc_each_k"</code> or <code>"AICc_full"</code></p>
</td></tr>
<tr><td><code id="explain_+3A_start_aicc">start_aicc</code></td>
<td>
<p>Numeric. Start value of <code>sigma</code> when optimizing the AICc. Note that this argument
is only applicable when <code>approach = "empirical"</code>, and <code>type</code> is either equal to
<code>"AICc_each_k"</code> or <code>"AICc_full"</code></p>
</td></tr>
<tr><td><code id="explain_+3A_w_threshold">w_threshold</code></td>
<td>
<p>Positive integer between 0 and 1.</p>
</td></tr>
<tr><td><code id="explain_+3A_mu">mu</code></td>
<td>
<p>Numeric vector. (Optional) Containing the mean of the data generating distribution.
If <code>NULL</code> the expected values are estimated from the data. Note that this is only used
when <code>approach = "gaussian"</code>.</p>
</td></tr>
<tr><td><code id="explain_+3A_cov_mat">cov_mat</code></td>
<td>
<p>Numeric matrix. (Optional) Containing the covariance matrix of the data
generating distribution. <code>NULL</code> means it is estimated from the data if needed
(in the Gaussian approach).</p>
</td></tr>
<tr><td><code id="explain_+3A_mincriterion">mincriterion</code></td>
<td>
<p>Numeric value or vector where length of vector is the number of features in model.
Value is equal to 1 - alpha where alpha is the nominal level of the conditional
independence tests.
If it is a vector, this indicates which mincriterion to use
when conditioning on various numbers of features.</p>
</td></tr>
<tr><td><code id="explain_+3A_minsplit">minsplit</code></td>
<td>
<p>Numeric value. Equal to the value that the sum of the left and right daughter nodes need to exceed.</p>
</td></tr>
<tr><td><code id="explain_+3A_minbucket">minbucket</code></td>
<td>
<p>Numeric value. Equal to the minimum sum of weights in a terminal node.</p>
</td></tr>
<tr><td><code id="explain_+3A_sample">sample</code></td>
<td>
<p>Boolean. If TRUE, then the method always samples <code>n_samples</code> from the leaf (with replacement).
If FALSE and the number of obs in the leaf is less than <code>n_samples</code>, the method will take all observations
in the leaf. If FALSE and the number of obs in the leaf is more than <code>n_samples</code>, the method will sample
<code>n_samples</code> (with replacement). This means that there will always be sampling in the leaf unless
<code>sample</code> = FALSE AND the number of obs in the node is less than <code>n_samples</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The most important thing to notice is that <code>shapr</code> has implemented four different
approaches for estimating the conditional distributions of the data, namely <code>"empirical"</code>,
<code>"gaussian"</code>, <code>"copula"</code> and <code>"ctree"</code>.
</p>
<p>In addition, the user also has the option of combining the four approaches.
E.g. if you're in a situation where you have trained a model the consists of 10 features,
and you'd like to use the <code>"gaussian"</code> approach when you condition on a single feature,
the <code>"empirical"</code> approach if you condition on 2-5 features, and <code>"copula"</code> version
if you condition on more than 5 features this can be done by simply passing
<code>approach = c("gaussian", rep("empirical", 4), rep("copula", 5))</code>. If
<code>"approach[i]" = "gaussian"</code> it means that you'd like to use the <code>"gaussian"</code> approach
when conditioning on <code>i</code> features.
</p>


<h3>Value</h3>

<p>Object of class <code>c("shapr", "list")</code>. Contains the following items:
</p>

<dl>
<dt>dt</dt><dd><p>data.table</p>
</dd>
<dt>model</dt><dd><p>Model object</p>
</dd>
<dt>p</dt><dd><p>Numeric vector</p>
</dd>
<dt>x_test</dt><dd><p>data.table</p>
</dd>
</dl>

<p>Note that the returned items <code>model</code>, <code>p</code> and <code>x_test</code> are mostly added due
to the implementation of <code>plot.shapr</code>. If you only want to look at the numerical results
it is sufficient to focus on <code>dt</code>. <code>dt</code> is a data.table where the number of rows equals
the number of observations you'd like to explain, and the number of columns equals <code>m +1</code>,
where <code>m</code> equals the total number of features in your model.
</p>
<p>If <code>dt[i, j + 1] &gt; 0</code> it indicates that the j-th feature increased the prediction for
the i-th observation. Likewise, if <code>dt[i, j + 1] &lt; 0</code> it indicates that the j-th feature
decreased the prediction for the i-th observation. The magnitude of the value is also important
to notice. E.g. if <code>dt[i, k + 1]</code> and <code>dt[i, j + 1]</code> are greater than <code>0</code>,
where <code>j != k</code>, and <code>dt[i, k + 1]</code> &gt; <code>dt[i, j + 1]</code> this indicates that feature
<code>j</code> and <code>k</code> both increased the value of the prediction, but that the effect of the k-th
feature was larger than the j-th feature.
</p>
<p>The first column in <code>dt</code>, called 'none', is the prediction value not assigned to any of the features
(<code class="reqn">\phi</code><sub>0</sub>).
It's equal for all observations and set by the user through the argument <code>prediction_zero</code>.
In theory this value should be the expected prediction without conditioning on any features.
Typically we set this value equal to the mean of the response variable in our training data, but other choices
such as the mean of the predictions in the training data are also reasonable.
</p>


<h3>Author(s)</h3>

<p>Camilla Lingjaerde, Nikolai Sellereite, Martin Jullum, Annabelle Redelmeier
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE)) {
  # Load example data
  data("Boston", package = "MASS")

  # Split data into test- and training data
  x_train &lt;- head(Boston, -3)
  x_test &lt;- tail(Boston, 3)

  # Fit a linear model
  model &lt;- lm(medv ~ lstat + rm + dis + indus, data = x_train)

  # Create an explainer object
  explainer &lt;- shapr(x_train, model)

  # Explain predictions
  p &lt;- mean(x_train$medv)

  # Empirical approach
  explain1 &lt;- explain(x_test, explainer,
    approach = "empirical",
    prediction_zero = p, n_samples = 1e2
  )

  # Gaussian approach
  explain2 &lt;- explain(x_test, explainer,
    approach = "gaussian",
    prediction_zero = p, n_samples = 1e2
  )

  # Gaussian copula approach
  explain3 &lt;- explain(x_test, explainer,
    approach = "copula",
    prediction_zero = p, n_samples = 1e2
  )

  # ctree approach
  explain4 &lt;- explain(x_test, explainer,
    approach = "ctree",
    prediction_zero = p
  )

  # Combined approach
  approach &lt;- c("gaussian", "gaussian", "empirical", "empirical")
  explain5 &lt;- explain(x_test, explainer,
    approach = approach,
    prediction_zero = p, n_samples = 1e2
  )

  # Print the Shapley values
  print(explain1$dt)

  # Plot the results
  if (requireNamespace("ggplot2", quietly = TRUE)) {
    plot(explain1)
  }
}
</code></pre>

<hr>
<h2 id='feature_combinations'>Define feature combinations, and fetch additional information about each unique combination</h2><span id='topic+feature_combinations'></span>

<h3>Description</h3>

<p>Define feature combinations, and fetch additional information about each unique combination
</p>


<h3>Usage</h3>

<pre><code class='language-R'>feature_combinations(
  m,
  exact = TRUE,
  n_combinations = 200,
  weight_zero_m = 10^6
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feature_combinations_+3A_m">m</code></td>
<td>
<p>Positive integer. Total number of features.</p>
</td></tr>
<tr><td><code id="feature_combinations_+3A_exact">exact</code></td>
<td>
<p>Logical. If <code>TRUE</code> all <code>2^m</code> combinations are generated, otherwise a
subsample of the combinations is used.</p>
</td></tr>
<tr><td><code id="feature_combinations_+3A_n_combinations">n_combinations</code></td>
<td>
<p>Positive integer. Note that if <code>exact = TRUE</code>,
<code>n_combinations</code> is ignored. However, if <code>m &gt; 12</code> you'll need to add a positive integer
value for <code>n_combinations</code>.</p>
</td></tr>
<tr><td><code id="feature_combinations_+3A_weight_zero_m">weight_zero_m</code></td>
<td>
<p>Numeric. The value to use as a replacement for infinite combination
weights when doing numerical operations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table that contains the following columns:
</p>

<dl>
<dt>id_combination</dt><dd><p>Positive integer. Represents a unique key for each combination. Note that the table
is sorted by <code>id_combination</code>, so that is always equal to <code>x[["id_combination"]] = 1:nrow(x)</code>.</p>
</dd>
<dt>features</dt><dd><p>List. Each item of the list is an integer vector where <code>features[[i]]</code>
represents the indices of the features included in combination <code>i</code>. Note that all the items
are sorted such that <code>features[[i]] == sort(features[[i]])</code> is always true.</p>
</dd>
<dt>n_features</dt><dd><p>Vector of positive integers. <code>n_features[i]</code> equals the number of features in combination
<code>i</code>, i.e. <code>n_features[i] = length(features[[i]])</code>.</p>
</dd></dl>
<p>.
</p>
<dl>
<dt>N</dt><dd><p>Positive integer. The number of unique ways to sample <code>n_features[i]</code> features
from <code>m</code> different features, without replacement.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Nikolai Sellereite, Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'># All combinations
x &lt;- feature_combinations(m = 3)
nrow(x) # Equals 2^3 = 8

# Subsample of combinations
x &lt;- feature_combinations(exact = FALSE, m = 10, n_combinations = 1e2)
</code></pre>

<hr>
<h2 id='feature_matrix_cpp'>Get feature matrix</h2><span id='topic+feature_matrix_cpp'></span>

<h3>Description</h3>

<p>Get feature matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>feature_matrix_cpp(features, m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feature_matrix_cpp_+3A_features">features</code></td>
<td>
<p>List</p>
</td></tr>
<tr><td><code id="feature_matrix_cpp_+3A_m">m</code></td>
<td>
<p>Positive integer. Total number of features</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>

<hr>
<h2 id='gaussian_transform'>Transforms a sample to standardized normal distribution</h2><span id='topic+gaussian_transform'></span>

<h3>Description</h3>

<p>Transforms a sample to standardized normal distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_transform(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gaussian_transform_+3A_x">x</code></td>
<td>
<p>Numeric vector.The data which should be transformed to a standard normal distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector of length <code>length(x)</code>
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='gaussian_transform_separate'>Transforms new data to standardized normal (dimension 1) based on other data transformations</h2><span id='topic+gaussian_transform_separate'></span>

<h3>Description</h3>

<p>Transforms new data to standardized normal (dimension 1) based on other data transformations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_transform_separate(yx, n_y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gaussian_transform_separate_+3A_yx">yx</code></td>
<td>
<p>Numeric vector. The first <code>n_y</code> items is the data that is transformed, and last
part is the data with the original transformation.</p>
</td></tr>
<tr><td><code id="gaussian_transform_separate_+3A_n_y">n_y</code></td>
<td>
<p>Positive integer. Number of elements of <code>yx</code> that belongs to the gaussian data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of back-transformed Gaussian data
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='get_data_specs'>Fetches feature information from a given data set</h2><span id='topic+get_data_specs'></span>

<h3>Description</h3>

<p>Fetches feature information from a given data set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_data_specs(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_data_specs_+3A_x">x</code></td>
<td>
<p>matrix, data.frame or data.table The data to extract feature information from.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to extract the feature information to be checked against the corresponding
information extracted from the model and other data sets. The function is called from
<code><a href="#topic+preprocess_data">preprocess_data</a></code>
and <code><a href="#topic+make_dummies">make_dummies</a></code>
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<dl>
<dt>labels</dt><dd><p>character vector with the feature names to compute Shapley values for</p>
</dd>
<dt>classes</dt><dd><p>a named character vector with the labels as names and the class types as elements</p>
</dd>
<dt>factor_levels</dt><dd><p>a named list with the labels as names and character vectors with the factor levels as elements
(NULL if the feature is not a factor)</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load example data
if (requireNamespace("MASS", quietly = TRUE)) {
  data("Boston", package = "MASS")
  # Split data into test- and training data
  x_train &lt;- data.table::as.data.table(head(Boston))
  x_train[, rad := as.factor(rad)]
  get_data_specs(x_train)
}
</code></pre>

<hr>
<h2 id='get_list_approaches'>Helper function used in <code><a href="#topic+explain.combined">explain.combined</a></code></h2><span id='topic+get_list_approaches'></span>

<h3>Description</h3>

<p>Helper function used in <code><a href="#topic+explain.combined">explain.combined</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_list_approaches(n_features, approach)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_list_approaches_+3A_n_features">n_features</code></td>
<td>
<p>Integer vector. Note that
<code>length(n_features) &lt;= 2^m</code>, where <code>m</code> equals the number
of features.</p>
</td></tr>
<tr><td><code id="get_list_approaches_+3A_approach">approach</code></td>
<td>
<p>Character vector of length <code>m</code>. All elements should be
either <code>"empirical"</code>, <code>"gaussian"</code> or <code>"copula"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>

<hr>
<h2 id='get_model_specs'>Fetches feature information from a given model object</h2><span id='topic+get_model_specs'></span><span id='topic+get_model_specs.default'></span><span id='topic+get_model_specs.lm'></span><span id='topic+get_model_specs.glm'></span><span id='topic+get_model_specs.gam'></span><span id='topic+get_model_specs.ranger'></span><span id='topic+get_model_specs.xgb.Booster'></span>

<h3>Description</h3>

<p>Fetches feature information from a given model object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_model_specs(x)

## Default S3 method:
get_model_specs(x)

## S3 method for class 'lm'
get_model_specs(x)

## S3 method for class 'glm'
get_model_specs(x)

## S3 method for class 'gam'
get_model_specs(x)

## S3 method for class 'ranger'
get_model_specs(x)

## S3 method for class 'xgb.Booster'
get_model_specs(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_model_specs_+3A_x">x</code></td>
<td>
<p>Model object for the model to be explained.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to extract the feature information to be checked against data passed to <code>shapr</code>
and <code>explain</code>. The function is called from <code>preprocess_data</code>.
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<dl>
<dt>labels</dt><dd><p>character vector with the feature names to compute Shapley values for</p>
</dd>
<dt>classes</dt><dd><p>a named character vector with the labels as names and the class type as elements</p>
</dd>
<dt>factor_levels</dt><dd><p>a named list with the labels as names and character vectors with the factor levels as elements
(NULL if the feature is not a factor)</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE)) {
  # Load example data
  data("Boston", package = "MASS")
  # Split data into test- and training data
  x_train &lt;- data.table::as.data.table(head(Boston))
  x_train[, rad := as.factor(rad)]
  model &lt;- lm(medv ~ lstat + rm + rad + indus, data = x_train)

  get_model_specs(model)
}
</code></pre>

<hr>
<h2 id='get_supported_models'>Provides a data.table with the supported models</h2><span id='topic+get_supported_models'></span>

<h3>Description</h3>

<p>Provides a data.table with the supported models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_supported_models()
</code></pre>

<hr>
<h2 id='hat_matrix_cpp'>Computing single H matrix in AICc-function using the Mahalanobis distance</h2><span id='topic+hat_matrix_cpp'></span>

<h3>Description</h3>

<p>Computing single H matrix in AICc-function using the Mahalanobis distance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hat_matrix_cpp(X, mcov, S_scale_dist, h)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hat_matrix_cpp_+3A_x">X</code></td>
<td>
<p>matrix with &quot;covariates&quot;</p>
</td></tr>
<tr><td><code id="hat_matrix_cpp_+3A_mcov">mcov</code></td>
<td>
<p>covariance matrix</p>
</td></tr>
<tr><td><code id="hat_matrix_cpp_+3A_s_scale_dist">S_scale_dist</code></td>
<td>
<p>logical indicating whether the Mahalanobis distance should be scaled with the number of variables</p>
</td></tr>
<tr><td><code id="hat_matrix_cpp_+3A_h">h</code></td>
<td>
<p>numeric specifying the scaling (sigma)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix of dimension <code>ncol(X)*ncol(X)</code>
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='inv_gaussian_transform'>Transforms new data to a standardized normal distribution</h2><span id='topic+inv_gaussian_transform'></span>

<h3>Description</h3>

<p>Transforms new data to a standardized normal distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inv_gaussian_transform(zx, n_z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inv_gaussian_transform_+3A_zx">zx</code></td>
<td>
<p>Numeric vector. The first <code>n_z</code> items are the Gaussian data, and the last part is
the data with the original transformation.</p>
</td></tr>
<tr><td><code id="inv_gaussian_transform_+3A_n_z">n_z</code></td>
<td>
<p>Positive integer. Number of elements of <code>zx</code> that belongs to new data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector of length <code>n_z</code>
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='mahalanobis_distance_cpp'>(Generalized) Mahalanobis distance</h2><span id='topic+mahalanobis_distance_cpp'></span>

<h3>Description</h3>

<p>Used to get the Euclidean distance as well by setting <code>mcov</code> = <code>diag(m)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mahalanobis_distance_cpp(
  featureList,
  Xtrain_mat,
  Xtest_mat,
  mcov,
  S_scale_dist
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mahalanobis_distance_cpp_+3A_featurelist">featureList</code></td>
<td>
<p>List of vectors indicating all factor combinations that should be included in the computations. Assumes that the first one is empty.</p>
</td></tr>
<tr><td><code id="mahalanobis_distance_cpp_+3A_xtrain_mat">Xtrain_mat</code></td>
<td>
<p>Matrix</p>
</td></tr>
<tr><td><code id="mahalanobis_distance_cpp_+3A_xtest_mat">Xtest_mat</code></td>
<td>
<p>Matrix</p>
</td></tr>
<tr><td><code id="mahalanobis_distance_cpp_+3A_mcov">mcov</code></td>
<td>
<p>Matrix. The Sigma-matrix in the Mahalanobis distance formula (<code>stats::cov(Xtrain_mat)</code>) gives Mahalanobis distance,
<code>diag(m)</code> gives the Euclidean distance.</p>
</td></tr>
<tr><td><code id="mahalanobis_distance_cpp_+3A_s_scale_dist">S_scale_dist</code></td>
<td>
<p>Logical indicating</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Array of three dimensions. Contains the squared distance for between all training and test observations for all feature combinations passed to the function.
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='make_dummies'>Initiate the making of dummy variables</h2><span id='topic+make_dummies'></span>

<h3>Description</h3>

<p>Initiate the making of dummy variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_dummies(traindata, testdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_dummies_+3A_traindata">traindata</code></td>
<td>
<p>data.table or data.frame.</p>
</td></tr>
<tr><td><code id="make_dummies_+3A_testdata">testdata</code></td>
<td>
<p>data.table or data.frame. New data that has the same
feature names, types, and levels as <code>traindata</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list that contains the following entries:
</p>

<dl>
<dt>feature_list</dt><dd><p>List. Output from <code>check_features</code></p>
</dd>
<dt>train_dummies</dt><dd><p>A data.frame containing all of the factors in <code>traindata</code> as
one-hot encoded variables.</p>
</dd>
<dt>test_dummies</dt><dd><p>A data.frame containing all of the factors in <code>testdata</code> as
one-hot encoded variables.</p>
</dd>
<dt>traindata_new</dt><dd><p>Original traindata with correct column ordering and factor levels. To be passed to
<code><a href="#topic+shapr">shapr</a>.</code></p>
</dd>
<dt>testdata_new</dt><dd><p>Original testdata with correct column ordering and factor levels. To be passed to
<code><a href="#topic+explain">explain</a>.</code></p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Annabelle Redelmeier, Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE)) {
  data("Boston", package = "MASS")
  x_var &lt;- c("lstat", "rm", "dis", "indus")
  y_var &lt;- "medv"
  x_train &lt;- as.data.frame(Boston[401:411, x_var])
  y_train &lt;- Boston[401:408, y_var]
  x_test &lt;- as.data.frame(Boston[1:4, x_var])

  # convert to factors for illustational purpose
  x_train$rm &lt;- factor(round(x_train$rm))
  x_test$rm &lt;- factor(round(x_test$rm), levels = levels(x_train$rm))

  dummylist &lt;- make_dummies(traindata = x_train, testdata = x_test)
}
</code></pre>

<hr>
<h2 id='model_checker'>Check that the type of model is supported by the explanation method</h2><span id='topic+model_checker'></span><span id='topic+model_checker.default'></span><span id='topic+model_checker.lm'></span><span id='topic+model_checker.glm'></span><span id='topic+model_checker.ranger'></span><span id='topic+model_checker.gam'></span><span id='topic+model_checker.xgb.Booster'></span>

<h3>Description</h3>

<p>The function checks whether the model given by <code>x</code> is supported.
If <code>x</code> is not a supported model the function will return an error message, otherwise it return NULL
(meaning all types of models with this class is supported)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_checker(x)

## Default S3 method:
model_checker(x)

## S3 method for class 'lm'
model_checker(x)

## S3 method for class 'glm'
model_checker(x)

## S3 method for class 'ranger'
model_checker(x)

## S3 method for class 'gam'
model_checker(x)

## S3 method for class 'xgb.Booster'
model_checker(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_checker_+3A_x">x</code></td>
<td>
<p>Model object for the model to be explained.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+predict_model">predict_model</a></code> for more information about
what type of models <code>shapr</code> currently support.
</p>


<h3>Value</h3>

<p>Error or NULL
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE)) {
  # Load example data
  data("Boston", package = "MASS")
  # Split data into test- and training data
  x_train &lt;- head(Boston, -3)
  # Fit a linear model
  model &lt;- lm(medv ~ lstat + rm + dis + indus, data = x_train)

  # Checking the model object
  model_checker(x = model)
}
</code></pre>

<hr>
<h2 id='observation_impute'>Generate permutations of training data using test observations</h2><span id='topic+observation_impute'></span>

<h3>Description</h3>

<p>Generate permutations of training data using test observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>observation_impute(
  W_kernel,
  S,
  x_train,
  x_test,
  w_threshold = 0.7,
  n_samples = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="observation_impute_+3A_w_kernel">W_kernel</code></td>
<td>
<p>Numeric matrix. Contains all nonscaled weights between training and test
observations for all feature combinations. The dimension equals <code>n_train x m</code>.</p>
</td></tr>
<tr><td><code id="observation_impute_+3A_s">S</code></td>
<td>
<p>Integer matrix of dimension <code>n_combinations x m</code>, where <code>n_combinations</code>
and <code>m</code> equals the total number of sampled/non-sampled feature combinations and
the total number of unique features, respectively. Note that <code>m = ncol(x_train)</code>.</p>
</td></tr>
<tr><td><code id="observation_impute_+3A_x_train">x_train</code></td>
<td>
<p>Numeric matrix</p>
</td></tr>
<tr><td><code id="observation_impute_+3A_x_test">x_test</code></td>
<td>
<p>Numeric matrix</p>
</td></tr>
<tr><td><code id="observation_impute_+3A_w_threshold">w_threshold</code></td>
<td>
<p>Numeric vector of length 1, where <code>w_threshold &gt; 0</code> and
<code>w_threshold &lt;= 1</code>. If <code>w_threshold = .8</code> we will choose the <code>K</code> samples with
the largest weight so that the sum of the weights accounts for 80% of the total weight.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>

<hr>
<h2 id='observation_impute_cpp'>Get imputed data</h2><span id='topic+observation_impute_cpp'></span>

<h3>Description</h3>

<p>Get imputed data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>observation_impute_cpp(index_xtrain, index_s, xtrain, xtest, S)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="observation_impute_cpp_+3A_index_xtrain">index_xtrain</code></td>
<td>
<p>Positive integer. Represents a sequence of row indices from <code>xtrain</code>,
i.e. <code>min(index_xtrain) &gt;= 1</code> and <code>max(index_xtrain) &lt;= nrow(xtrain)</code>.</p>
</td></tr>
<tr><td><code id="observation_impute_cpp_+3A_index_s">index_s</code></td>
<td>
<p>Positive integer. Represents a sequence of row indices from <code>S</code>,
i.e. <code>min(index_s) &gt;= 1</code> and <code>max(index_s) &lt;= nrow(S)</code>.</p>
</td></tr>
<tr><td><code id="observation_impute_cpp_+3A_xtrain">xtrain</code></td>
<td>
<p>Numeric matrix.</p>
</td></tr>
<tr><td><code id="observation_impute_cpp_+3A_xtest">xtest</code></td>
<td>
<p>Numeric matrix. Represents a single test observation.</p>
</td></tr>
<tr><td><code id="observation_impute_cpp_+3A_s">S</code></td>
<td>
<p>Integer matrix of dimension <code>n_combinations x m</code>, where <code>n_combinations</code> equals
the total number of sampled/non-sampled feature combinations and <code>m</code> equals
the total number of unique features. Note that <code>m = ncol(xtrain)</code>. See details
for more information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>S(i, j) = 1</code> if and only if feature <code>j</code> is present in feature
combination <code>i</code>, otherwise <code>S(i, j) = 0</code>. I.e. if <code>m = 3</code>, there
are <code>2^3 = 8</code> unique ways to combine the features. In this case <code>dim(S) = c(8, 3)</code>.
Let's call the features <code>x1, x2, x3</code> and take a closer look at the combination
represented by <code>s = c(x1, x2)</code>. If this combination is represented by the second row,
the following is true: <code>S[2, 1:3] = c(1, 1, 0)</code>.
</p>
<p>The returned object, <code>X</code>, is a numeric matrix where
<code>dim(X) = c(length(index_xtrain), ncol(xtrain))</code>. If feature <code>j</code> is present in
the k-th observation, that is <code>S[index_[k], j] == 1</code>, <code>X[k, j] = xtest[1, j]</code>.
Otherwise <code>X[k, j] = xtrain[index_xtrain[k], j]</code>.
</p>


<h3>Value</h3>

<p>Numeric matrix
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>

<hr>
<h2 id='plot.shapr'>Plot of the Shapley value explanations</h2><span id='topic+plot.shapr'></span>

<h3>Description</h3>

<p>Plots the individual prediction explanations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'shapr'
plot(
  x,
  digits = 3,
  plot_phi0 = TRUE,
  index_x_test = NULL,
  top_k_features = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.shapr_+3A_x">x</code></td>
<td>
<p>An <code>shapr</code> object. See <code><a href="#topic+explain">explain</a></code>.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_digits">digits</code></td>
<td>
<p>Integer. Number of significant digits to use in the feature description</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_plot_phi0">plot_phi0</code></td>
<td>
<p>Logical. Whether to include <code>phi0</code> in the plot</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_index_x_test">index_x_test</code></td>
<td>
<p>Integer vector. Which of the test observations to plot. E.g. if you have
explained 10 observations using <code><a href="#topic+explain">explain</a></code>, you can generate a plot for the first 5
observations by setting <code>index_x_test = 1:5</code>.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_top_k_features">top_k_features</code></td>
<td>
<p>Integer. How many features to include in the plot. E.g. if you have 15
features in your model you can plot the 5 most important features, for each explanation, by setting
<code>top_k_features = 1:5</code>.</p>
</td></tr>
<tr><td><code id="plot.shapr_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code>vignette("understanding_shapr", package = "shapr")</code> for an example of
how you should use the function.
</p>


<h3>Value</h3>

<p>ggplot object with plots of the Shapley value explanations
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE)) {
  #' # Load example data
  data("Boston", package = "MASS")

  # Split data into test- and training data
  x_train &lt;- head(Boston, -3)
  x_test &lt;- tail(Boston, 3)

  # Fit a linear model
  model &lt;- lm(medv ~ lstat + rm + dis + indus, data = x_train)

  # Create an explainer object
  explainer &lt;- shapr(x_train, model)

  # Explain predictions
  p &lt;- mean(x_train$medv)

  # Empirical approach
  explanation &lt;- explain(x_test,
    explainer,
    approach = "empirical",
    prediction_zero = p,
    n_samples = 1e2
  )

  if (requireNamespace("ggplot2", quietly = TRUE)) {
    # Plot the explantion (this function)
    plot(explanation)
  }
}
</code></pre>

<hr>
<h2 id='predict_model'>Generate predictions for different model classes</h2><span id='topic+predict_model'></span><span id='topic+predict_model.default'></span><span id='topic+predict_model.lm'></span><span id='topic+predict_model.glm'></span><span id='topic+predict_model.ranger'></span><span id='topic+predict_model.xgb.Booster'></span><span id='topic+predict_model.gam'></span>

<h3>Description</h3>

<p>Performs prediction of response <code><a href="stats.html#topic+lm">lm</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>,
<code><a href="ranger.html#topic+ranger">ranger</a></code>,  <code><a href="mgcv.html#topic+gam">mgcv::gam</a></code> and
<code><a href="xgboost.html#topic+xgb.train">xgboost::xgb.train</a></code> with binary or continuous
response. See details for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_model(x, newdata)

## Default S3 method:
predict_model(x, newdata)

## S3 method for class 'lm'
predict_model(x, newdata)

## S3 method for class 'glm'
predict_model(x, newdata)

## S3 method for class 'ranger'
predict_model(x, newdata)

## S3 method for class 'xgb.Booster'
predict_model(x, newdata)

## S3 method for class 'gam'
predict_model(x, newdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_model_+3A_x">x</code></td>
<td>
<p>Model object for the model to be explained.</p>
</td></tr>
<tr><td><code id="predict_model_+3A_newdata">newdata</code></td>
<td>
<p>A data frame (or matrix) in which to look for variables with which to predict.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following models are currently supported:
</p>

<ul>
<li> <p><code><a href="stats.html#topic+lm">stats::lm</a></code>
</p>
</li>
<li> <p><code><a href="stats.html#topic+glm">stats::glm</a></code>
</p>
</li>
<li> <p><code><a href="ranger.html#topic+ranger">ranger::ranger</a></code>
</p>
</li>
<li> <p><code><a href="mgcv.html#topic+gam">mgcv::gam</a></code>
</p>
</li>
<li> <p><code><a href="xgboost.html#topic+xgb.train">xgboost::xgb.train</a></code>
</p>
</li></ul>

<p>The returned object <code>p</code> always satisfies the following properties:
</p>

<ul>
<li> <p><code>is.atomic(p)</code> equals <code>TRUE</code>
</p>
</li>
<li> <p><code>is.double(p)</code> equals <code>TRUE</code>
</p>
</li></ul>

<p>If you have a binary classification model we'll always return the probability prediction
for a single class.
</p>
<p>For more details on how to explain other types of models (i.e. custom models), see the Advanced usage section
of the vignette: <br />
From R: <code>vignette("understanding_shapr", package = "shapr")</code>  <br />
Web: <a href="https://norskregnesentral.github.io/shapr/articles/understanding_shapr.html#explain-custom-models">https://norskregnesentral.github.io/shapr/articles/understanding_shapr.html#explain-custom-models</a>
</p>


<h3>Value</h3>

<p>Numeric
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE)) {
  # Load example data
  data("Boston", package = "MASS")
  # Split data into test- and training data
  x_train &lt;- head(Boston, -3)
  x_test &lt;- tail(Boston, 3)
  # Fit a linear model
  model &lt;- lm(medv ~ lstat + rm + dis + indus, data = x_train)

  # Predicting for a model with a standardized format
  predict_model(x = model, newdata = x_test)
}
</code></pre>

<hr>
<h2 id='prediction'>Calculate Shapley weights for test data</h2><span id='topic+prediction'></span>

<h3>Description</h3>

<p>This function should only be called internally, and not be used as
a stand-alone function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prediction(dt, prediction_zero, explainer)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prediction_+3A_dt">dt</code></td>
<td>
<p>data.table</p>
</td></tr>
<tr><td><code id="prediction_+3A_prediction_zero">prediction_zero</code></td>
<td>
<p>Numeric. The value to use for <code>phi_0</code>.</p>
</td></tr>
<tr><td><code id="prediction_+3A_explainer">explainer</code></td>
<td>
<p>An object of class <code>explainer</code>. See <code><a href="#topic+shapr">shapr</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>dt</code> does not contain three columns called <code>id</code>, <code>id_combination</code> and <code>w</code>
the function will fail. <code>id</code> represents a unique key for a given test observation,
and <code>id_combination</code> is a unique key for which feature combination the row represents. <code>w</code>
represents the Shapley value of feature combination given by <code>id_combination</code>. In addition
to these three columns, <code>dt</code> should also have columns which matches the variables used
when training the model.
</p>
<p>I.e. you have fitted a linear model using the features <code>x1</code>,
<code>x2</code> and <code>x3</code>, and you want to explain 5 test observations using the exact method, i.e.
setting <code>exact = TRUE</code> in <code><a href="#topic+shapr">shapr</a></code>, the following properties should be satisfied
</p>

<ol>
<li> <p><code>colnames(dt)</code> equals <code>c("x1", "x2", "x3", "id", "id_combination", ""w)</code>
</p>
</li>
<li> <p><code>dt[, max(id)]</code> equals the number of test observations
</p>
</li>
<li> <p><code>dt[, min(id)]</code> equals 1L.
</p>
</li>
<li> <p><code>dt[, max(id_combination)]</code> equals <code>2^m</code> where m equals the number of features.
</p>
</li>
<li> <p><code>dt[, min(id_combination)]</code> equals 1L.
</p>
</li>
<li> <p><code>dt[, type(w)]</code> equals <code>double</code>.
</p>
</li></ol>



<h3>Value</h3>

<p>An object of class <code>c("shapr", "list")</code>. For more details see <code><a href="#topic+explain">explain</a></code>.
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>

<hr>
<h2 id='prepare_data'>Generate data used for predictions</h2><span id='topic+prepare_data'></span><span id='topic+prepare_data.empirical'></span><span id='topic+prepare_data.gaussian'></span><span id='topic+prepare_data.copula'></span><span id='topic+prepare_data.ctree'></span>

<h3>Description</h3>

<p>Generate data used for predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data(x, ...)

## S3 method for class 'empirical'
prepare_data(x, seed = 1, n_samples = 1000, index_features = NULL, ...)

## S3 method for class 'gaussian'
prepare_data(x, seed = 1, n_samples = 1000, index_features = NULL, ...)

## S3 method for class 'copula'
prepare_data(
  x,
  x_test_gaussian = 1,
  seed = 1,
  n_samples = 1000,
  index_features = NULL,
  ...
)

## S3 method for class 'ctree'
prepare_data(
  x,
  seed = 1,
  n_samples = 1000,
  index_features = NULL,
  mc_cores = 1,
  mc_cores_create_ctree = mc_cores,
  mc_cores_sample_ctree = mc_cores,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_data_+3A_x">x</code></td>
<td>
<p>Explainer object. See <code><a href="#topic+explain">explain</a></code> for more information.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_seed">seed</code></td>
<td>
<p>Positive integer. If <code>NULL</code> the seed will be inherited from the calling environment.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_n_samples">n_samples</code></td>
<td>
<p>Integer. The number of obs to sample from the leaf if <code>sample</code> = TRUE or if <code>sample</code>
= FALSE but <code>n_samples</code> is less than the number of obs in the leaf.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_index_features">index_features</code></td>
<td>
<p>List. Default is NULL but if either various methods are being used or various mincriterion are
used for different numbers of conditioned features, this will be a list with the features to pass.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_x_test_gaussian">x_test_gaussian</code></td>
<td>
<p>Matrix. Test data quantile-transformed to standard Gaussian variables. Only applicable if
<code>approach = "empirical"</code>.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_mc_cores">mc_cores</code></td>
<td>
<p>Integer. Only for class <code>ctree</code> currently. The number of cores to use in paralellization of the
tree building (<code>create_ctree</code>) and tree sampling (<code>sample_ctree</code>). Defaults to 1. Note: Uses
parallel::mclapply which relies on forking, i.e. uses only 1 core on Windows systems.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_mc_cores_create_ctree">mc_cores_create_ctree</code></td>
<td>
<p>Integer. Same as <code>mc_cores</code>, but specific for the tree building function
#' Defaults to <code>mc_cores</code>.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_mc_cores_sample_ctree">mc_cores_sample_ctree</code></td>
<td>
<p>Integer. Same as <code>mc_cores</code>, but specific for the tree building prediction
function.
Defaults to <code>mc_cores</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='preprocess_data'>Process (check and update) data according to specified feature list</h2><span id='topic+preprocess_data'></span>

<h3>Description</h3>

<p>Process (check and update) data according to specified feature list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess_data(x, feature_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_data_+3A_x">x</code></td>
<td>
<p>matrix, data.frame or data.table. The data to check input for and update
according to the specification in <code>feature_list</code>.</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_feature_list">feature_list</code></td>
<td>
<p>List. Output from running <code><a href="#topic+get_data_specs">get_data_specs</a></code> or
<code><a href="#topic+get_model_specs">get_model_specs</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes care of all preprocessing and checking of the provided data in <code>x</code> against
the feature_list which is typically the output from <code><a href="#topic+get_model_specs">get_model_specs</a></code>
</p>


<h3>Value</h3>

<p>List with two named elements: <code>x_dt</code>: Checked and updated data <code>x</code> in data.table format, and
<code>update_feature_list</code> the output from <code><a href="#topic+check_features">check_features</a></code>
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load example data
if (requireNamespace("MASS", quietly = TRUE)) {
  data("Boston", package = "MASS")
  # Split data into test- and training data
  x_train &lt;- data.table::as.data.table(head(Boston))
  x_train[, rad := as.factor(rad)]
  data_features &lt;- get_data_specs(x_train)
  model &lt;- lm(medv ~ lstat + rm + rad + indus, data = x_train)

  model_features &lt;- get_model_specs(model)
  preprocess_data(x_train, model_features)
}
</code></pre>

<hr>
<h2 id='rss_cpp'>sigma_hat_sq-function</h2><span id='topic+rss_cpp'></span>

<h3>Description</h3>

<p>sigma_hat_sq-function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rss_cpp(H, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rss_cpp_+3A_h">H</code></td>
<td>
<p>Matrix. Output from <code><a href="#topic+hat_matrix_cpp">hat_matrix_cpp</a></code></p>
</td></tr>
<tr><td><code id="rss_cpp_+3A_y">y</code></td>
<td>
<p>Vector, i.e. representing the response variable</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='sample_combinations'>Helper function to sample a combination of training and testing rows, which does not risk
getting the same observation twice. Need to improve this help file.</h2><span id='topic+sample_combinations'></span>

<h3>Description</h3>

<p>Helper function to sample a combination of training and testing rows, which does not risk
getting the same observation twice. Need to improve this help file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_combinations(ntrain, ntest, nsamples, joint_sampling = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_combinations_+3A_ntrain">ntrain</code></td>
<td>
<p>Positive integer. Number of training observations to sample from.</p>
</td></tr>
<tr><td><code id="sample_combinations_+3A_ntest">ntest</code></td>
<td>
<p>Positive integer. Number of test observations to sample from.</p>
</td></tr>
<tr><td><code id="sample_combinations_+3A_nsamples">nsamples</code></td>
<td>
<p>Positive integer. Number of samples.</p>
</td></tr>
<tr><td><code id="sample_combinations_+3A_joint_sampling">joint_sampling</code></td>
<td>
<p>Logical. Indicates whether train- and test data should be sampled
separately or in a joint sampling space. If they are sampled separately (which typically
would be used when optimizing more than one distribution at once) we sample with replacement
if <code>nsamples &gt; ntrain</code>. Note that this solution is not optimal. Be careful if you're
doing optimization over every test observation when <code>nsamples &gt; ntrain</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='sample_copula'>Sample conditional variables using the Gaussian copula approach</h2><span id='topic+sample_copula'></span>

<h3>Description</h3>

<p>Sample conditional variables using the Gaussian copula approach
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_copula(
  index_given,
  n_samples,
  mu,
  cov_mat,
  m,
  x_test_gaussian,
  x_train,
  x_test
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_copula_+3A_index_given">index_given</code></td>
<td>
<p>Integer vector. The indices of the features to condition upon. Note that
<code>min(index_given) &gt;= 1</code> and <code>max(index_given) &lt;= m</code>.</p>
</td></tr>
<tr><td><code id="sample_copula_+3A_m">m</code></td>
<td>
<p>Positive integer. The total number of features.</p>
</td></tr>
<tr><td><code id="sample_copula_+3A_x_test_gaussian">x_test_gaussian</code></td>
<td>
<p>Numeric matrix. Contains the observation whose predictions ought to be explained (test data),
after quantile-transforming them to standard Gaussian variables.</p>
</td></tr>
<tr><td><code id="sample_copula_+3A_x_test">x_test</code></td>
<td>
<p>Numeric matrix. Contains the features of the observation whose
predictions ought to be explained (test data).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='sample_ctree'>Sample ctree variables from a given conditional inference tree</h2><span id='topic+sample_ctree'></span>

<h3>Description</h3>

<p>Sample ctree variables from a given conditional inference tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_ctree(tree, n_samples, x_test, x_train, p, sample)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_ctree_+3A_tree">tree</code></td>
<td>
<p>List. Contains tree which is an object of type ctree built from the party package.
Also contains given_ind, the features to condition upon.</p>
</td></tr>
<tr><td><code id="sample_ctree_+3A_n_samples">n_samples</code></td>
<td>
<p>Numeric. Indicates how many samples to use for MCMC.</p>
</td></tr>
<tr><td><code id="sample_ctree_+3A_x_test">x_test</code></td>
<td>
<p>Matrix, data.frame or data.table with the features of the observation whose
predictions ought to be explained (test data). Dimension <code>1xp</code> or <code>px1</code>.</p>
</td></tr>
<tr><td><code id="sample_ctree_+3A_x_train">x_train</code></td>
<td>
<p>Matrix, data.frame or data.table with training data.</p>
</td></tr>
<tr><td><code id="sample_ctree_+3A_p">p</code></td>
<td>
<p>Positive integer. The number of features.</p>
</td></tr>
<tr><td><code id="sample_ctree_+3A_sample">sample</code></td>
<td>
<p>Boolean. True indicates that the method samples from the terminal node
of the tree whereas False indicates that the method takes all the observations if it is
less than n_samples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table with <code>n_samples</code> (conditional) Gaussian samples
</p>


<h3>Author(s)</h3>

<p>Annabelle Redelmeier
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE) &amp; requireNamespace("party", quietly = TRUE)) {
  m &lt;- 10
  n &lt;- 40
  n_samples &lt;- 50
  mu &lt;- rep(1, m)
  cov_mat &lt;- cov(matrix(rnorm(n * m), n, m))
  x_train &lt;- data.table::data.table(MASS::mvrnorm(n, mu, cov_mat))
  x_test &lt;- MASS::mvrnorm(1, mu, cov_mat)
  x_test_dt &lt;- data.table::setDT(as.list(x_test))
  given_ind &lt;- c(4, 7)
  dependent_ind &lt;- (1:dim(x_train)[2])[-given_ind]
  x &lt;- x_train[, given_ind, with = FALSE]
  y &lt;- x_train[, dependent_ind, with = FALSE]
  df &lt;- data.table::data.table(cbind(y, x))
  colnames(df) &lt;- c(paste0("Y", 1:ncol(y)), paste0("V", given_ind))
  ynam &lt;- paste0("Y", 1:ncol(y))
  fmla &lt;- as.formula(paste(paste(ynam, collapse = "+"), "~ ."))
  datact &lt;- party::ctree(fmla, data = df, controls = party::ctree_control(
    minbucket = 7,
    mincriterion = 0.95
  ))
  tree &lt;- list(tree = datact, given_ind = given_ind, dependent_ind = dependent_ind)
  shapr:::sample_ctree(
    tree = tree, n_samples = n_samples, x_test = x_test_dt, x_train = x_train,
    p = length(x_test), sample = TRUE
  )
}
</code></pre>

<hr>
<h2 id='sample_gaussian'>Sample conditional Gaussian variables</h2><span id='topic+sample_gaussian'></span>

<h3>Description</h3>

<p>Sample conditional Gaussian variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_gaussian(index_given, n_samples, mu, cov_mat, m, x_test)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_gaussian_+3A_index_given">index_given</code></td>
<td>
<p>Integer vector. The indices of the features to condition upon. Note that
<code>min(index_given) &gt;= 1</code> and <code>max(index_given) &lt;= m</code>.</p>
</td></tr>
<tr><td><code id="sample_gaussian_+3A_m">m</code></td>
<td>
<p>Positive integer. The total number of features.</p>
</td></tr>
<tr><td><code id="sample_gaussian_+3A_x_test">x_test</code></td>
<td>
<p>Numeric matrix. Contains the features of the observation whose
predictions ought to be explained (test data).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>

<hr>
<h2 id='shapley_weights'>Calculate Shapley weight</h2><span id='topic+shapley_weights'></span>

<h3>Description</h3>

<p>Calculate Shapley weight
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapley_weights(m, N, n_features, weight_zero_m = 10^6)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shapley_weights_+3A_m">m</code></td>
<td>
<p>Positive integer. Total number of features.</p>
</td></tr>
<tr><td><code id="shapley_weights_+3A_n">N</code></td>
<td>
<p>Positive integer. The number of unique combinations when sampling <code>n_features</code> features,
without replacement, from a sample space consisting of <code>m</code> different features.</p>
</td></tr>
<tr><td><code id="shapley_weights_+3A_n_features">n_features</code></td>
<td>
<p>Positive integer. Represents the number of features you want to sample from a feature
space consisting of <code>m</code> unique features. Note that <code> 0 &lt; = n_features &lt;= m</code>.</p>
</td></tr>
<tr><td><code id="shapley_weights_+3A_weight_zero_m">weight_zero_m</code></td>
<td>
<p>Positive integer. Represents the Shapley weight for two special
cases, i.e. the case where you have either <code>0</code> or <code>m</code> features.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>

<hr>
<h2 id='shapr'>Create an explainer object with Shapley weights for test data.</h2><span id='topic+shapr'></span>

<h3>Description</h3>

<p>Create an explainer object with Shapley weights for test data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapr(x, model, n_combinations = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shapr_+3A_x">x</code></td>
<td>
<p>Numeric matrix or data.frame/data.table. Contains the data used to estimate the (conditional)
distributions for the features needed to properly estimate the conditional expectations in the Shapley formula.</p>
</td></tr>
<tr><td><code id="shapr_+3A_model">model</code></td>
<td>
<p>The model whose predictions we want to explain. Run
<code><a href="#topic+get_supported_models">shapr:::get_supported_models()</a></code>
for a table of which models <code>shapr</code> supports natively.</p>
</td></tr>
<tr><td><code id="shapr_+3A_n_combinations">n_combinations</code></td>
<td>
<p>Integer. The number of feature combinations to sample. If <code>NULL</code>,
the exact method is used and all combinations are considered. The maximum number of
combinations equals <code>2^ncol(x)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list that contains the following items:
</p>

<dl>
<dt>exact</dt><dd><p>Boolean. Equals <code>TRUE</code> if <code>n_combinations = NULL</code> or
<code>n_combinations &lt; 2^ncol(x)</code>, otherwise <code>FALSE</code>.</p>
</dd>
<dt>n_features</dt><dd><p>Positive integer. The number of columns in <code>x</code></p>
</dd>
<dt>S</dt><dd><p>Binary matrix. The number of rows equals the number of unique combinations, and
the number of columns equals the total number of features. I.e. let's say we have a case with
three features. In that case we have <code>2^3 = 8</code> unique combinations. If the j-th
observation for the i-th row equals <code>1</code> it indicates that the j-th feature is present in
the i-th combination. Otherwise it equals <code>0</code>.</p>
</dd>
<dt>W</dt><dd><p>Second item</p>
</dd>
<dt>X</dt><dd><p>data.table. Returned object from <code><a href="#topic+feature_combinations">feature_combinations</a></code></p>
</dd>
<dt>x_train</dt><dd><p>data.table. Transformed <code>x</code> into a data.table.</p>
</dd>
<dt>feature_list</dt><dd><p>List. The <code>updated_feature_list</code> output from
<code><a href="#topic+preprocess_data">preprocess_data</a></code></p>
</dd>
</dl>

<p>In addition to the items above, <code>model</code> and <code>n_combinations</code> are also present in the returned object.
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("MASS", quietly = TRUE)) {
  # Load example data
  data("Boston", package = "MASS")
  df &lt;- Boston

  # Example using the exact method
  x_var &lt;- c("lstat", "rm", "dis", "indus")
  y_var &lt;- "medv"
  df1 &lt;- df[, x_var]
  model &lt;- lm(medv ~ lstat + rm + dis + indus, data = df)
  explainer &lt;- shapr(df1, model)

  print(nrow(explainer$X))
  # 16 (which equals 2^4)

  # Example using approximation
  y_var &lt;- "medv"
  x_var &lt;- setdiff(colnames(df), y_var)
  model &lt;- lm(medv ~ ., data = df)
  df2 &lt;- df[, x_var]
  explainer &lt;- shapr(df2, model, n_combinations = 1e3)

  print(nrow(explainer$X))

  # Example using approximation where n_combinations &gt; 2^m
  x_var &lt;- c("lstat", "rm", "dis", "indus")
  y_var &lt;- "medv"
  df3 &lt;- df[, x_var]
  model &lt;- lm(medv ~ lstat + rm + dis + indus, data = df)
  explainer &lt;- shapr(df1, model, n_combinations = 1e3)

  print(nrow(explainer$X))
  # 16 (which equals 2^4)
}
</code></pre>

<hr>
<h2 id='update_data'>Updates data by reference according to the updater argument.</h2><span id='topic+update_data'></span>

<h3>Description</h3>

<p><code>data</code> is updated, i.e. unused columns and factor levels are removed as described in
<code>updater</code>. This is done by reference, i.e. updates the object being passed to data even if nothing is
returned by the function itself.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_data(data, updater)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_data_+3A_data">data</code></td>
<td>
<p>data.table. Data that ought to be updated.</p>
</td></tr>
<tr><td><code id="update_data_+3A_updater">updater</code></td>
<td>
<p>List. The object should be the output from
<code><a href="#topic+check_features">check_features</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL.
</p>


<h3>Author(s)</h3>

<p>Martin Jullum
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load example data
if (requireNamespace("MASS", quietly = TRUE)) {
  data("Boston", package = "MASS")
  # Split data into test- and training data
  x_train &lt;- data.table::as.data.table(head(Boston))
  x_train[, rad := as.factor(rad)]
  data_features &lt;- get_data_specs(x_train)
  model &lt;- lm(medv ~ lstat + rm + rad + indus, data = x_train)

  model_features &lt;- get_model_specs(model)
  updater &lt;- check_features(model_features, data_features)
  update_data(x_train, updater)
}
</code></pre>

<hr>
<h2 id='weight_matrix'>Calculate weighted matrix</h2><span id='topic+weight_matrix'></span>

<h3>Description</h3>

<p>Calculate weighted matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weight_matrix(X, normalize_W_weights = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weight_matrix_+3A_x">X</code></td>
<td>
<p>data.table</p>
</td></tr>
<tr><td><code id="weight_matrix_+3A_normalize_w_weights">normalize_W_weights</code></td>
<td>
<p>Logical. Whether to normalize the weights for the combinations to sum to 1 for
increased numerical stability before solving the WLS (weighted least squares). Applies to all combinations
except combination <code>1</code> and <code>2^m</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric matrix. See <code><a href="#topic+weight_matrix_cpp">weight_matrix_cpp</a></code> for more information.
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite, Martin Jullum
</p>

<hr>
<h2 id='weight_matrix_cpp'>Calculate weight matrix</h2><span id='topic+weight_matrix_cpp'></span>

<h3>Description</h3>

<p>Calculate weight matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weight_matrix_cpp(features, m, n, w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weight_matrix_cpp_+3A_features">features</code></td>
<td>
<p>List. Each of the elements equals an integer
vector representing a valid combination of features.</p>
</td></tr>
<tr><td><code id="weight_matrix_cpp_+3A_m">m</code></td>
<td>
<p>Integer. Number of features</p>
</td></tr>
<tr><td><code id="weight_matrix_cpp_+3A_n">n</code></td>
<td>
<p>Integer. Number of combinations</p>
</td></tr>
<tr><td><code id="weight_matrix_cpp_+3A_w">w</code></td>
<td>
<p>Numeric vector of length <code>n</code>, i.e. <code>w[i]</code> equals
the Shapley weight of feature combination <code>i</code>, represented by
<code>features[[i]]</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix of dimension n x m + 1
</p>


<h3>Author(s)</h3>

<p>Nikolai Sellereite
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
