<!DOCTYPE html><html><head><title>Help for package sparklyr.nested</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparklyr.nested}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#sdf_explode'><p>Explode data along a column</p></a></li>
<li><a href='#sdf_nest'><p>Nest data in a Spark Dataframe</p></a></li>
<li><a href='#sdf_schema_json'><p>Work with the schema</p></a></li>
<li><a href='#sdf_select'><p>Select nested items</p></a></li>
<li><a href='#sdf_unnest'><p>Unnest data along a column</p></a></li>
<li><a href='#struct_type'><p>Spark Data Types</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>A 'sparklyr' Extension for Nested Data</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.4</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matt Pollock &lt;mpollock@mitre.org&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A 'sparklyr' extension adding the capability to work easily with nested data.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3)</td>
</tr>
<tr>
<td>Imports:</td>
<td>sparklyr, jsonlite, listviewer, dplyr, rlang, purrr,
tidyselect</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, reactR</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a> | file LICENSE</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Spark: 1.6.x or 2.x</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mitre/sparklyr.nested/issues">https://github.com/mitre/sparklyr.nested/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-02-20 21:39:05 UTC; frodo</td>
</tr>
<tr>
<td>Author:</td>
<td>Matt Pollock [aut, cre],
  The MITRE Corporation [cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-02-20 22:00:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='sdf_explode'>Explode data along a column</h2><span id='topic+sdf_explode'></span>

<h3>Description</h3>

<p>Exploding an array column of length <code>N</code> will replicate the top level record <code>N</code> times.
The i^th replicated record will contain a struct (not an array) corresponding to the i^th element
of the exploded array. Exploding will not promote any fields or otherwise change the schema of
the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_explode(x, column, is_map = FALSE, keep_all = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdf_explode_+3A_x">x</code></td>
<td>
<p>An object (usually a <code>spark_tbl</code>) coercible to a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_explode_+3A_column">column</code></td>
<td>
<p>The field to explode</p>
</td></tr>
<tr><td><code id="sdf_explode_+3A_is_map">is_map</code></td>
<td>
<p>Logical. The (scala) <code>explode</code> method works for both <code>array</code> and <code>map</code>
column types. If the column to explode in an array, then <code>is_map=FALSE</code> will ensure that
the exploded output retains the name of the array column. If however the column to explode is
a map, then the map will have key/value names that will be used if <code>is_map=TRUE</code>.</p>
</td></tr>
<tr><td><code id="sdf_explode_+3A_keep_all">keep_all</code></td>
<td>
<p>Logical. If <code>FALSE</code> then records where the exploded value is empty/null
will be dropped.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two types of exploding are possible. The default method calls the scala <code>explode</code> method.
This operation is supported in both Spark version &gt; 1.6. It will however drop records where the
exploding field is empty/null. Alternatively <code>keep_all=TRUE</code> will use the <code>explode_outer</code>
scala method introduced in spark 2 to not drop any records.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# first get some nested data
iris_tbl &lt;- copy_to(sc, iris, name="iris")
iris_nst &lt;- iris_tbl %&gt;%
  sdf_nest(Sepal_Length, Sepal_Width, Petal_Length, Petal_Width, .key="data") %&gt;%
  group_by(Species) %&gt;%
  summarize(data=collect_list(data))

# then explode it
iris_nst %&gt;% sdf_explode(data)

## End(Not run)
</code></pre>

<hr>
<h2 id='sdf_nest'>Nest data in a Spark Dataframe</h2><span id='topic+sdf_nest'></span>

<h3>Description</h3>

<p>This function is like <code>tidyr::nest</code>. Calling this function will not
aggregate over other columns. Rather the output has the same number of
rows/records as the input. See examples of how to achieve row reduction
by aggregating elements using <code>collect_list</code>, which is a Spark SQL function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_nest(x, ..., .key = "data")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdf_nest_+3A_x">x</code></td>
<td>
<p>A Spark dataframe.</p>
</td></tr>
<tr><td><code id="sdf_nest_+3A_...">...</code></td>
<td>
<p>Columns to nest.</p>
</td></tr>
<tr><td><code id="sdf_nest_+3A_.key">.key</code></td>
<td>
<p>Character. A name for the new column containing nested fields</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# produces a dataframe with an array of characteristics nested under
# each unique species identifier
iris_tbl &lt;- copy_to(sc, iris, name="iris")
iris_tbl %&gt;%
  sdf_nest(Sepal_Length, Sepal_Width, Petal_Length, Petal_Width, .key="data") %&gt;%
  group_by(Species) %&gt;%
  summarize(data=collect_list(data))

## End(Not run)

</code></pre>

<hr>
<h2 id='sdf_schema_json'>Work with the schema</h2><span id='topic+sdf_schema_json'></span><span id='topic+sdf_schema_viewer'></span>

<h3>Description</h3>

<p>These functions support flexible schema inspection both algorithmically and in human-friendly ways.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_schema_json(
  x,
  parse_json = TRUE,
  simplify = FALSE,
  append_complex_type = TRUE
)

sdf_schema_viewer(
  x,
  simplify = TRUE,
  append_complex_type = TRUE,
  use_react = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdf_schema_json_+3A_x">x</code></td>
<td>
<p>An <code>R</code> object wrapping, or containing, a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_schema_json_+3A_parse_json">parse_json</code></td>
<td>
<p>Logical. If <code>TRUE</code> then the JSON return value will be parsed into an R list.</p>
</td></tr>
<tr><td><code id="sdf_schema_json_+3A_simplify">simplify</code></td>
<td>
<p>Logical. If <code>TRUE</code> then the schema will be folded into itself such that
<code>{"name" : "field1", "type" : {"type" : "array", "elementType" : "string", "containsNull" : true},
 "nullable" : true, "metadata" : { } }</code> will be rendered simply <code>{"field1 (array)" : "[string]"}</code></p>
</td></tr>
<tr><td><code id="sdf_schema_json_+3A_append_complex_type">append_complex_type</code></td>
<td>
<p>Logical. This only matters if <code>parse_json=TRUE</code> and <code>simplify=TRUE</code>. 
In that case indicators will be included in the return value for array and struct types.</p>
</td></tr>
<tr><td><code id="sdf_schema_json_+3A_use_react">use_react</code></td>
<td>
<p>Logical. If <code>TRUE</code> schemas will be rendered using <a href="listviewer.html#topic+reactjson">reactjson</a>.
Otherwise they will be rendered using <a href="listviewer.html#topic+jsonedit">jsonedit</a> (the default). Using react works better
in some contexts (e.g. bookdown-rendered HTML) and has a different look &amp; feel. It does however carry
an extra dependency on the <code>reactR</code> package suggested by <code>listviewer</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="sparklyr.html#topic+sdf_schema">sdf_schema</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(testthat)
library(jsonlite)
library(sparklyr)
library(sparklyr.nested)
sample_json &lt;- paste0(
  '{"aircraft_id":["string"],"phase_sequence":["string"],"phases (array)":{"start_point (struct)":',
  '{"segment_phase":["string"],"agl":["double"],"elevation":["double"],"time":["long"],',
  '"latitude":["double"],"longitude":["double"],"altitude":["double"],"course":["double"],',
  '"speed":["double"],"source_point_keys (array)":["[string]"],"primary_key":["string"]},',
  '"end_point (struct)":{"segment_phase":["string"],"agl":["double"],"elevation":["double"],',
  '"time":["long"],"latitude":["double"],"longitude":["double"],"altitude":["double"],',
  '"course":["double"],"speed":["double"],"source_point_keys (array)":["[string]"],',
  '"primary_key":["string"]},"phase":["string"],"primary_key":["string"]},"primary_key":["string"]}'
)

with_mock(
  # I am mocking functions so that the example works without a real spark connection
  spark_read_parquet = function(x, ...){return("this is a spark dataframe")},
  sdf_schema_json = function(x, ...){return(fromJSON(sample_json))},
  spark_connect = function(...){return("this is a spark connection")},
  
  # the meat of the example is here
  sc &lt;- spark_connect(),
  spark_data &lt;- spark_read_parquet(sc, path="path/to/data/*.parquet", name="some_name"),
  sdf_schema_viewer(spark_data)
)

## End(Not run)
</code></pre>

<hr>
<h2 id='sdf_select'>Select nested items</h2><span id='topic+sdf_select'></span>

<h3>Description</h3>

<p>The <code>select</code> function works well for keeping/dropping top level fields. It does not
however support access to nested data. This function will accept complex field names
such as <code>x.y.z</code> where <code>z</code> is a field nested within <code>y</code> which is in turn
nested within <code>x</code>. Since R uses &quot;$&quot; to access nested elements and java/scala use &quot;.&quot;,
<code>sdf_select(data, x.y.z)</code> and <code>sdf_select(data, x$y$z)</code> are equivalent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_select(x, ..., .aliases, .drop_parents = TRUE, .full_name = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdf_select_+3A_x">x</code></td>
<td>
<p>An object (usually a <code>spark_tbl</code>) coercible to a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_select_+3A_...">...</code></td>
<td>
<p>Fields to select</p>
</td></tr>
<tr><td><code id="sdf_select_+3A_.aliases">.aliases</code></td>
<td>
<p>Character. Optional. If provided these names will be matched positionally with
selected fields provided in <code>...</code>. This is more useful when calling from a function and
less natural to use when calling the function directly. It is likely to get you into trouble
if you are using <code>dplyr</code> select helpers. The alternative with direct calls
is to put the alias on the left side of the expression (e.g. <code>sdf_select(df, fld_alias=parent.child.fld)</code>)</p>
</td></tr>
<tr><td><code id="sdf_select_+3A_.drop_parents">.drop_parents</code></td>
<td>
<p>Logical. If <code>TRUE</code> then any field from which nested elements are extracted
will be dropped, even if they were included in the selected <code>...</code>. This better supports using 
<code>dplyr</code> field matching helpers like <code>everything()</code> and <code>starts_with</code>.</p>
</td></tr>
<tr><td><code id="sdf_select_+3A_.full_name">.full_name</code></td>
<td>
<p>Logical. If <code>TRUE</code> then nested field names that are not named (either using
a LHS <code>name=field_name</code> construct or the <code>.aliases</code> argument) will be disambiguated using
the parent field name. For example <code>sdf_select(df, x.y)</code> will return a field named <code>x_y</code>.
If <code>FALSE</code> then the parent field name is dropped unless it is needed to avoid duplicate names.</p>
</td></tr>
</table>


<h3>Selection Helpers</h3>

<p><code>dplyr</code> allows the use of selection helpers (e.g., see <code><a href="dplyr.html#topic+everything">everything</a></code>).
These helpers only work for top level fields however. For now all nested fields that should
be promoted need to be explicitly identified.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# produces a dataframe with an array of characteristics nested under
# each unique species identifier
iris_tbl &lt;- copy_to(sc, iris, name="iris")
iris_nst &lt;- iris_tbl %&gt;%
  sdf_nest(Sepal_Length, Sepal_Width, .key="Sepal") 

# using java-like dot-notation
iris_nst %&gt;%
  sdf_select(Species, Petal_Width, Sepal.Sepal_Width)

# using R-like dollar-sign-notation
iris_nst %&gt;%
  sdf_select(Species, Petal_Width, Sepal$Sepal_Width)
  
# using dplyr selection helpers
iris_nst %&gt;%
  sdf_select(Species, matches("Petal"), Sepal$Sepal_Width)

## End(Not run)
</code></pre>

<hr>
<h2 id='sdf_unnest'>Unnest data along a column</h2><span id='topic+sdf_unnest'></span>

<h3>Description</h3>

<p>Unnesting is an (optional) explode operation coupled with a nested select to promote the sub-fields of
the exploded top level array/map/struct to the top level. Hence, given <code>a</code>, an array with fields
<code>a1, a2, a3</code>, then codesdf_explode(df, a) will produce output with each record replicated
for every element in the <code>a</code> array and with the fields <code>a1, a2, a3</code> (but not <code>a</code>)
at the top level. Similar to <code>tidyr::unnest</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sdf_unnest(x, column, keep_all = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sdf_unnest_+3A_x">x</code></td>
<td>
<p>An object (usually a <code>spark_tbl</code>) coercible to a Spark DataFrame.</p>
</td></tr>
<tr><td><code id="sdf_unnest_+3A_column">column</code></td>
<td>
<p>The field to explode</p>
</td></tr>
<tr><td><code id="sdf_unnest_+3A_keep_all">keep_all</code></td>
<td>
<p>Logical. If <code>FALSE</code> then records where the exploded value is empty/null
will be dropped.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that this is a less precise tool than using <code><a href="#topic+sdf_explode">sdf_explode</a></code> and <code><a href="#topic+sdf_select">sdf_select</a></code>
directly because all fields of the exploded array will be kept and promoted. Direct calls to these
methods allows for more targeted use of <code><a href="#topic+sdf_select">sdf_select</a></code> to promote only those fields that
are wanted to the top level of the data frame.
</p>
<p>Additionally, though <code><a href="#topic+sdf_select">sdf_select</a></code> allows users to reach arbitrarily far into a nested
structure, this function will only reach one layer deep. It may well be that the unnested fields
are themselves nested structures that need to be dealt with accordingly.
</p>
<p>Note that map types are supported, but there is no <code>is_map</code> argument. This is because the
function is doing schema interrogation of the input data anyway to determine whether an explode
operation is required (it is of maps and arrays, but not for bare structs). Given this the result
of the schema interrogation drives the value o <code>is_map</code> provided to <code>sdf_explode</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# first get some nested data
iris_tbl &lt;- copy_to(sc, iris, name="iris")
iris_nst &lt;- iris_tbl %&gt;%
  sdf_nest(Sepal_Length, Sepal_Width, Petal_Length, Petal_Width, .key="data") %&gt;%
  group_by(Species) %&gt;%
  summarize(data=collect_list(data))

# then explode it
iris_nst %&gt;% sdf_unnest(data)

## End(Not run)
</code></pre>

<hr>
<h2 id='struct_type'>Spark Data Types</h2><span id='topic+struct_type'></span><span id='topic+struct_field'></span><span id='topic+array_type'></span><span id='topic+binary_type'></span><span id='topic+boolean_type'></span><span id='topic+byte_type'></span><span id='topic+date_type'></span><span id='topic+double_type'></span><span id='topic+float_type'></span><span id='topic+integer_type'></span><span id='topic+numeric_type'></span><span id='topic+long_type'></span><span id='topic+map_type'></span><span id='topic+string_type'></span><span id='topic+character_type'></span><span id='topic+timestamp_type'></span>

<h3>Description</h3>

<p>These function support supplying a spark read schema. This is particularly useful
when reading data with nested arrays when you are not interested in several of 
the nested fields.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>struct_type(sc, struct_fields)

struct_field(sc, name, data_type, nullable = FALSE)

array_type(sc, data_type, nullable = FALSE)

binary_type(sc)

boolean_type(sc)

byte_type(sc)

date_type(sc)

double_type(sc)

float_type(sc)

integer_type(sc)

numeric_type(sc)

long_type(sc)

map_type(sc, key_type, value_type, nullable = FALSE)

string_type(sc)

character_type(sc)

timestamp_type(sc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="struct_type_+3A_sc">sc</code></td>
<td>
<p>A <code>spark_connection</code></p>
</td></tr>
<tr><td><code id="struct_type_+3A_struct_fields">struct_fields</code></td>
<td>
<p>A vector or fields obtained from <code>struct_field()</code></p>
</td></tr>
<tr><td><code id="struct_type_+3A_name">name</code></td>
<td>
<p>A field name to use in the output struct type</p>
</td></tr>
<tr><td><code id="struct_type_+3A_data_type">data_type</code></td>
<td>
<p>A (java) data type (e.g., <code>string_type()</code> or <code>double_type()</code>)</p>
</td></tr>
<tr><td><code id="struct_type_+3A_nullable">nullable</code></td>
<td>
<p>Logical. Describes whether field can be missing for some rows.</p>
</td></tr>
<tr><td><code id="struct_type_+3A_key_type">key_type</code></td>
<td>
<p>A (java) data type describing the map keys (usually <code>string_type()</code>)</p>
</td></tr>
<tr><td><code id="struct_type_+3A_value_type">value_type</code></td>
<td>
<p>A (java) data type describing the map values</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
