<!DOCTYPE html><html><head><title>Help for package kmed</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kmed}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#barplotnum'><p>Barplot of each cluster for numerical variables data set</p></a></li>
<li><a href='#clust4'><p>4-clustered data set</p></a></li>
<li><a href='#clust5'><p>5-clustered data set</p></a></li>
<li><a href='#clustboot'><p>Bootstrap replications for clustering alorithm</p></a></li>
<li><a href='#clustheatmap'><p>Consensus matrix heatmap from A consensus matrix</p></a></li>
<li><a href='#consensusmatrix'><p>Consensus matrix from A matrix of bootstrap replicates</p></a></li>
<li><a href='#cooccur'><p>Co-occurrence distance for binary/ categorical variables data</p></a></li>
<li><a href='#csv'><p>Centroid shadow value (CSV) index and plot</p></a></li>
<li><a href='#distmix'><p>Distances for mixed variables data set</p></a></li>
<li><a href='#distNumeric'><p>A pair distance for numerical variables</p></a></li>
<li><a href='#fastkmed'><p>Simple and fast k-medoid algorithm</p></a></li>
<li><a href='#globalfood'><p>Global food security index</p></a></li>
<li><a href='#heart'><p>Heart Disease data set</p></a></li>
<li><a href='#inckmed'><p>Increasing number of clusters in k-medoids algorithm</p></a></li>
<li><a href='#matching'><p>A pair distance for binary/ categorical variables</p></a></li>
<li><a href='#msv'><p>Medoid shadow value (MSV) index and plot</p></a></li>
<li><a href='#pcabiplot'><p>Biplot of a PCA object</p></a></li>
<li><a href='#rankkmed'><p>Rank k-medoid algorithm</p></a></li>
<li><a href='#sil'><p>Silhouette index and plot</p></a></li>
<li><a href='#skm'><p>Simple k-medoid algorithm</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Distance-Based k-Medoids</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-08-29</td>
</tr>
<tr>
<td>Description:</td>
<td>Algorithms of distance-based k-medoids clustering: simple and fast 
  k-medoids, ranked k-medoids, and increasing number of clusters in k-medoids. 
  Calculate distances for mixed variable data such as Gower, Podani, Wishart, 
  Huang, Harikumar-PV, and Ahmad-Dey. Cluster validation applies internal and 
  relative criteria. The internal criteria includes silhouette index and shadow 
  values. The relative criterium applies bootstrap procedure producing a heatmap 
  with a flexible reordering matrix algorithm such as complete, ward, or average 
  linkages. The cluster result can be plotted in a marked barplot or pca biplot.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-08-28 22:39:50 UTC; Weksi Budiaji</td>
</tr>
<tr>
<td>Author:</td>
<td>Weksi Budiaji [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Weksi Budiaji &lt;budiaji@untirta.ac.id&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-08-29 06:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='barplotnum'>Barplot of each cluster for numerical variables data set</h2><span id='topic+barplotnum'></span>

<h3>Description</h3>

<p>This function creates a barplot from a cluster result. A barplot
indicates the location and dispersion of each cluster. The x-axis
of the barplot is variable's mean, while the y-axis is the variable's name.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>barplotnum(dataori, clust, nc = 1, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="barplotnum_+3A_dataori">dataori</code></td>
<td>
<p>An original data set.</p>
</td></tr>
<tr><td><code id="barplotnum_+3A_clust">clust</code></td>
<td>
<p>A vector of cluster membership (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="barplotnum_+3A_nc">nc</code></td>
<td>
<p>A number of columns for the plot of all cluster
(see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="barplotnum_+3A_alpha">alpha</code></td>
<td>
<p>A numeric number to set the significant level (between 0 and 0.2).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a marked barplot because some markers are added, i.e.
a significant test, a population mean for each (numerical) variable.
The significance test applies t-test between the population's mean and
cluster's mean in every variable. The alpha is set in between 0 to 20%.
If the population mean differs to the cluster's mean, the bar shade in the
barplot also differs.
</p>
<p><code>clust</code> is a vector with the length equal to the number of objects
(n), or the function will be an error otherwise. <code>nc</code>
controls the layout (grid) of the plot. If <code>nc = 1</code>, the plot of each
cluster is placed in a column. When the number of clusters is 6 and
<code>nc = 2</code>, for example, the plot has a layout of 3-row and 2-column grids.
</p>


<h3>Value</h3>

<p>Function returns a barplot.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>Leisch, F. (2008). Handbook of Data Visualization, Chapter
Visualizing cluster analysis and finite mixture models, pp. 561-587.
Springer Handbooks of Computational Statistics. Springer Verlag.
</p>
<p>Dolnicar, S. and F. Leisch (2014). Using graphical statistics to
better understand market segmentation solutions. International Journal of
Market Research 56, 207-230.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- iris[,1:4]
memb &lt;- cutree(hclust(dist(dat)),3)
barplotnum(dat, memb)
barplotnum(dat, memb, 2)

</code></pre>

<hr>
<h2 id='clust4'>4-clustered data set</h2><span id='topic+clust4'></span>

<h3>Description</h3>

<p>A dataset containing two variables of 300 objects and their class memberships
generated by the <span class="pkg">clusterGeneration</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clust4
</code></pre>


<h3>Format</h3>

<p>A data frame with 300 rows and 3 variables:
</p>

<dl>
<dt>x1</dt><dd><p>X1.</p>
</dd>
<dt>x2</dt><dd><p>X2.</p>
</dd>
<dt>class</dt><dd><p>Class membership.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Data is generated via the <code>genRandomClust</code> function in the
<span class="pkg">clusterGeneration</span> package. The code to generate this data
set is
</p>
<p>set.seed(2016)
</p>
<p>randclust &lt;- clusterGeneration::genRandomClust(4, sepVal = 0.001,
numNonNoisy = 2, numReplicate = 1, clustszind = 3,
clustSizes = as.numeric(table(sample(1:4, 300, replace = TRUE))),
outputDatFlag=FALSE, outputLogFlag=FALSE, outputEmpirical=FALSE,
outputInfo=FALSE)
</p>
<p>clust4 &lt;- as.data.frame(randclust$datList$test_1)
</p>
<p>clust4$class &lt;- randclust$memList$test_1
</p>


<h3>References</h3>

<p>Qiu, W., and H. Joe. 2015. ClusterGeneration: Random Cluster
Generation (with Specified Degree of Separation).
</p>
<p>Qiu, W., and H. Joe. 2006a. Generation of Random Clusters with
Specified Degree of Separation. Journal of Classification 23 pp. 315-34.
</p>
<p>Qiu, W., and H. Joe. 2006b. Separation Index and Partial
Membership for Clustering. Computational Statistics and Data Analysis 50
pp. 585-603.
</p>

<hr>
<h2 id='clust5'>5-clustered data set</h2><span id='topic+clust5'></span>

<h3>Description</h3>

<p>A dataset containing two variables of 800 objects and their class memberships
generated by the <span class="pkg">clusterGeneration</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clust5
</code></pre>


<h3>Format</h3>

<p>A data frame with 800 rows and 3 variables:
</p>

<dl>
<dt>x1</dt><dd><p>X1.</p>
</dd>
<dt>x2</dt><dd><p>X2.</p>
</dd>
<dt>class</dt><dd><p>Class membership.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Data is generated via the <code>genRandomClust</code> function in the
<span class="pkg">clusterGeneration</span> package. The code to generate this data
set is
</p>
<p>set.seed(2016)
</p>
<p>randclust &lt;- clusterGeneration::genRandomClust(5, sepVal = 0.2,
numNonNoisy = 2, numReplicate = 1, clustszind = 3,
clustSizes = as.numeric(table(sample(1:5, 800, replace = TRUE))),
outputDatFlag=FALSE, outputLogFlag=FALSE, outputEmpirical=FALSE,
outputInfo=FALSE)
</p>
<p>clust5 &lt;- as.data.frame(randclust$datList$test_1)
</p>
<p>clust5$class &lt;- randclust$memList$test_1
</p>


<h3>References</h3>

<p>Qiu, W., and H. Joe. 2015. ClusterGeneration: Random Cluster
Generation (with Specified Degree of Separation).
</p>
<p>Qiu, W., and H. Joe. 2006a. Generation of Random Clusters with
Specified Degree of Separation. Journal of Classification 23 pp. 315-34.
</p>
<p>Qiu, W., and H. Joe. 2006b. Separation Index and Partial
Membership for Clustering. Computational Statistics and Data Analysis 50
pp. 585-603.
</p>

<hr>
<h2 id='clustboot'>Bootstrap replications for clustering alorithm</h2><span id='topic+clustboot'></span>

<h3>Description</h3>

<p>This function does bootstrap replications for a clustering
algorithm. Any hard clustering algorithm is valid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clustboot(distdata, nclust = 2, algorithm = fastclust, nboot = 25, diss = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clustboot_+3A_distdata">distdata</code></td>
<td>
<p>A distance matrix (n x n)/ dist object or
a data frame.</p>
</td></tr>
<tr><td><code id="clustboot_+3A_nclust">nclust</code></td>
<td>
<p>A number of clusters.</p>
</td></tr>
<tr><td><code id="clustboot_+3A_algorithm">algorithm</code></td>
<td>
<p>A clustering algorithm function (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="clustboot_+3A_nboot">nboot</code></td>
<td>
<p>A number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="clustboot_+3A_diss">diss</code></td>
<td>
<p>A logical if <code>distdata</code> is a distance matrix/ object or
a data frame.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a function to obtain bootstrap evaluation for cluster results.
The <code>algorithm</code> argument is a function where this function has two input
arguments. The two input arguments are a distance matrix/ object or
a data frame, and number of clusters. Then the output is only
a vector of cluster memberships.
</p>
<p>The default <code>algorithm</code> is <code>fastclust</code> applying the
<code><a href="#topic+fastkmed">fastkmed</a></code> function. The code of the <code>fastclust</code> is
</p>
<p>fastclust &lt;- function(x, nclust) {
</p>
<p>res &lt;- fastkmed(x, nclust, iterate = 50)
</p>
<p>return(res$cluster)
</p>
<p>}
</p>
<p>For other examples, see <strong>Examples</strong>. It applies ward and kmeans
algorithms. When kmeans is applied, for example, <code>diss</code> is set to be
<code>FALSE</code> because the input of the <code>kmclust</code> and
<code><a href="#topic+clustboot">clustboot</a></code> is a data frame instead of a distance.
</p>


<h3>Value</h3>

<p>Function returns a matrix of bootstrap replicates with a dimension of
n x b, where n is the number of objects and b is the
number of bootstrap replicates.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>Dolnicar, S. and Leisch, F. 2010. Evaluation of structure
and reproducibility of cluster solutions using the bootstrap.
Marketing Letters 21 pp. 83-101.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>num &lt;- as.matrix(iris[,1:4])
mrwdist &lt;- distNumeric(num, num, method = "mrw")
ward.D2 &lt;- function(x, nclust) {
res &lt;- hclust(as.dist(x), method = "ward.D2")
member &lt;- cutree(res, nclust)
return(member)
}
kmclust &lt;- function(x, nclust) {
res &lt;- kmeans(x, nclust)
return(res$cluster)
}
irisfast &lt;- clustboot(mrwdist, nclust=3, nboot=7)
head(irisfast)
irisward &lt;- clustboot(mrwdist, nclust=3, algorithm = ward.D2, nboot=7)
head(irisward)
iriskmeans &lt;- clustboot(num, nclust=3, algorithm = kmclust, nboot=7, diss = FALSE)
head(iriskmeans)

</code></pre>

<hr>
<h2 id='clustheatmap'>Consensus matrix heatmap from A consensus matrix</h2><span id='topic+clustheatmap'></span>

<h3>Description</h3>

<p>This function creates a consensus matrix heatmap from a
consensus/ agreement matrix. The values of the consensus/ agreement
matrix are transformed in order to plot the heatmap.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clustheatmap(consmat, title = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clustheatmap_+3A_consmat">consmat</code></td>
<td>
<p>A matrix of consensus/ agreement matrix (see
<strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="clustheatmap_+3A_title">title</code></td>
<td>
<p>A title of the plot.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a function to produce a consensus matrix heatmap from a
consensus/ agreement matrix. A matrix produced by the
<code><a href="#topic+consensusmatrix">consensusmatrix</a></code> function can be directly provided in the
<code>consmat</code> argument. The values of the consensus matrix, <strong>A</strong>,
are then transformed via a non-linear transformation by applying
</p>
<p style="text-align: center;"><code class="reqn">a_{ij}^{trf} = \frac{a_{ij} - min(a_{..})}{max(a_{..}) - min(a_{..})}</code>
</p>

<p>where <code class="reqn">a_{ij}</code> is the value of the consensus matrix in row i and
column j, and <code class="reqn">a_{..}</code> is the all values of the matrix
(<code class="reqn">\forall</code><strong>A</strong>).
</p>


<h3>Value</h3>

<p>Function returns a heatmap plot.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>Monti, S., P. Tamayo, J. Mesirov, and T. Golub. 2003. Consensus
clustering: A resampling-based method for class discovery and visualization
of gene expression microarray data. Machine Learning 52 pp. 91-118.
</p>
<p>Hahsler, M., and Hornik, K., 2011. Dissimilarity plots:
A visual exploration tool for partitional clustering. Journal of
Computational and Graphical Statistics 20(2) pp. 335-354.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>num &lt;- as.matrix(iris[,1:4])
mrwdist &lt;- distNumeric(num, num, method = "mrw")
irisfast &lt;- clustboot(mrwdist, nclust=3, nboot=7)
complete &lt;- function(x, nclust) {
res &lt;- hclust(as.dist(x), method = "complete")
member &lt;- cutree(res, nclust)
return(member)
}
consensuscomplete &lt;- consensusmatrix(irisfast, nclust = 3, reorder = complete)
clustheatmap(consensuscomplete)

</code></pre>

<hr>
<h2 id='consensusmatrix'>Consensus matrix from A matrix of bootstrap replicates</h2><span id='topic+consensusmatrix'></span>

<h3>Description</h3>

<p>This function creates a consensus matrix from a matrix of
bootstrap replicates. It transforms an n x b matrix into an
n x n matrix, where n is the number of objects and b
is the number of bootstrap replicates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>consensusmatrix(bootdata, nclust, reorder = fastclust)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="consensusmatrix_+3A_bootdata">bootdata</code></td>
<td>
<p>A matrix of bootstrap replicate (n x b)
(see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="consensusmatrix_+3A_nclust">nclust</code></td>
<td>
<p>A number of clusters.</p>
</td></tr>
<tr><td><code id="consensusmatrix_+3A_reorder">reorder</code></td>
<td>
<p>Any distance-based clustering algorithm function
(see <strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a function to obtain a consensus matrix from a matrix of
bootstrap replicates to evaluate the clustering result. The
<code>bootdata</code> argument can be supplied directly from a matrix produced
by the <code><a href="#topic+clustboot">clustboot</a></code> function. The values of the consensus matrix,
<strong>A</strong>, are calculated by
</p>
<p style="text-align: center;"><code class="reqn">a_{ij} = a_{ji} = \frac{\#n \:of \:objects \:i \:and \:j
\:in \:the \:same \:cluster}{\#n \:of \:objects \:i \:and \:j
\:sampled \:at \:the \:same \:time}</code>
</p>

<p>where <code class="reqn">a_{ij}</code> is the agreement index between objects i and
j. Note that due to the agreement between objects i and
j equal to the agreement between objects j and i,
the consensus matrix is a symmetric matrix.
</p>
<p>Meanwhile, the <code>reorder</code> argument is a function to reorder the objects
in both the row and column of the consensus matrix such that similar objects
are close to each other. This task can be solved by applying a clustering
algorithm in the consensus matrix. The <code>reorder</code> has to consist of
two input arguments. The two input arguments are a
distance matrix/ object and number of clusters.
The output is only a vector of cluster memberships. Thus,
the algorihtm that can be applied in the <code>reorder</code> argument is the
distance-based algorithm with a distance as the input.
</p>
<p>The default <code>reorder</code> is <code>fastclust</code> applying the
<code><a href="#topic+fastkmed">fastkmed</a></code> function. The code of the <code>fastclust</code> is
</p>
<p>fastclust &lt;- function(x, nclust) {
</p>
<p>res &lt;- fastkmed(x, nclust, iterate = 50)
</p>
<p>return(res$cluster)
</p>
<p>}
</p>
<p>For other examples, see <strong>Examples</strong>. It applies centroid and
complete linkage algorithms.
</p>


<h3>Value</h3>

<p>Function returns a consensus/ agreement matrix of n x n
dimension.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>Monti, S., P. Tamayo, J. Mesirov, and T. Golub. 2003. Consensus
clustering: A resampling-based method for class discovery and visualization
of gene expression microarray data. Machine Learning 52 pp. 91-118.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>num &lt;- as.matrix(iris[,1:4])
mrwdist &lt;- distNumeric(num, num, method = "mrw")
irisfast &lt;- clustboot(mrwdist, nclust=3, nboot=7)
consensusfast &lt;- consensusmatrix(irisfast, nclust = 3)
centroid &lt;- function(x, nclust) {
res &lt;- hclust(as.dist(x), method = "centroid")
member &lt;- cutree(res, nclust)
return(member)
}
consensuscentroid &lt;- consensusmatrix(irisfast, nclust = 3, reorder = centroid)
complete &lt;- function(x, nclust) {
res &lt;- hclust(as.dist(x), method = "complete")
member &lt;- cutree(res, nclust)
return(member)
}
consensuscomplete &lt;- consensusmatrix(irisfast, nclust = 3, reorder = complete)
consensusfast[c(1:5,51:55,101:105),c(1:5,51:55,101:105)]
consensuscentroid[c(1:5,51:55,101:105),c(1:5,51:55,101:105)]
consensuscomplete[c(1:5,51:55,101:105),c(1:5,51:55,101:105)]

</code></pre>

<hr>
<h2 id='cooccur'>Co-occurrence distance for binary/ categorical variables data</h2><span id='topic+cooccur'></span>

<h3>Description</h3>

<p>This function calculates the co-occurrence distance proposed
by Ahmad and Dey (2007).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cooccur(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cooccur_+3A_data">data</code></td>
<td>
<p>A matrix or data frame of binary/ categorical variables
(see <strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes co-occurrence distance, which is a binary/
categorical distance, that based on the other variable's distribution
(see <strong>Examples</strong>).  In the <strong>Examples</strong>, we have a data set:
</p>

<table>
<tr>
 <td style="text-align: left;">
object </td><td style="text-align: right;"> x </td><td style="text-align: right;"> y </td><td style="text-align: right;"> z </td>
</tr>
<tr>
 <td style="text-align: left;">
1 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
2 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 1 </td>
</tr>
<tr>
 <td style="text-align: left;">
3 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
4 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
5 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 1 </td>
</tr>
<tr>
 <td style="text-align: left;">
6 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
7 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2
</td>
</tr>

</table>

<p>The co-occurrence distance transforms each category of binary/ categorical
in a variable based on the distribution of other variables, for example,
the distance between categories 1 and 2 in the x variable can be
different to the distance between categories 1 and 2 in the z
variable. As an example, the transformed distance between categories 1 and 2
in the z variable is presented.
</p>
<p>A cross tabulation between the z and x variables with
corresponding (column) proportion is
</p>

<table>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> ||</td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td>
</tr>
<tr>
 <td style="text-align: right;">
1 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> ||</td><td style="text-align: right;"> 1.0 </td><td style="text-align: right;"> 0.2 </td>
</tr>
<tr>
 <td style="text-align: right;">
2 </td><td style="text-align: right;"> 0 </td><td style="text-align: right;"> 4 </td><td style="text-align: right;"> ||</td><td style="text-align: right;"> 0.0 </td><td style="text-align: right;"> 0.8
</td>
</tr>

</table>

<p>A cross tabulation between the z and y variables with
corresponding (column) proportion is
</p>

<table>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> ||</td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td>
</tr>
<tr>
 <td style="text-align: right;">
1 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 3 </td><td style="text-align: right;"> ||</td><td style="text-align: right;"> 0.5 </td><td style="text-align: right;"> 0.6 </td>
</tr>
<tr>
 <td style="text-align: right;">
2 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> ||</td><td style="text-align: right;"> 0.5 </td><td style="text-align: right;"> 0.4
</td>
</tr>

</table>

<p>Then, the maximum values of the proportion in each row are taken such that
they are 1.0, 0.8, 0.6, and 0.5. The new distance between categories 1 and
2 in the z variable is
</p>
<p style="text-align: center;"><code class="reqn">\delta_{1,2}^z = \frac{(1.0+0.8+0.6+0.5) - 2}{2} = 0.45</code>
</p>

<p>The constant <code class="reqn">2</code> in the formula applies because the z variable
depends on the 2 other variable distributions, i.e the x and y
variables. The new distances of each category in the
for the x and y variables can be calculated in a similar way.
</p>
<p>Thus, the distance between objects 1 and 2 is 0.45. It is only the z
variable counted to calculate the distance between objects 1 and 2
because objects 1 and 2 have similar values in both the x and y
variables.
</p>
<p>The <code>data</code> argument can be supplied with either a matrix or data frame,
in which the class of the element has to be an integer. If it is not
an integer, it will be converted to an integer class. If the <code>data</code>
of a variable only, a simple matching is calculated. The co-occurrence
is absent due to its dependency to the distribution of other variables
and a <code>warning</code> message appears.
</p>


<h3>Value</h3>

<p>Function returns a distance matrix (n x n).
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>Ahmad, A., and Dey, L. 2007. A K-mean clustering algorithm for
mixed numeric and categorical data. Data and Knowledge Engineering 63,
pp. 503-527.
</p>
<p>Harikumar, S., PV, S., 2015. K-medoid clustering for heterogeneous data sets.
JProcedia Computer Science 70, 226-237.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
a &lt;- matrix(sample(1:2, 7*3, replace = TRUE), 7, 3)
cooccur(a)


</code></pre>

<hr>
<h2 id='csv'>Centroid shadow value (CSV) index and plot</h2><span id='topic+csv'></span>

<h3>Description</h3>

<p>This function computes centroid shadow values and shadow value plots of
each cluster. The plot presents the mean of the shadow values as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>csv(distdata, idmedoid, idcluster, title = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="csv_+3A_distdata">distdata</code></td>
<td>
<p>A distance matrix (n x n) or dist object.</p>
</td></tr>
<tr><td><code id="csv_+3A_idmedoid">idmedoid</code></td>
<td>
<p>A vector of id medoids (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="csv_+3A_idcluster">idcluster</code></td>
<td>
<p>A vector of cluster membership (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="csv_+3A_title">title</code></td>
<td>
<p>A title of the plot.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The origin of the centroid shadow value is calculated in the <code>shadow</code>
function of the <span class="pkg">flexclust</span> package, in which it is based on the first and
second closest centroid. The <code>csv</code> function in this package modifies
the centroid into medoid such that the formula to compute shadow value of
object i is
</p>
<p style="text-align: center;"><code class="reqn">csv(i) = \frac{2d(i, m(i))}{d(i, m(i)) + d(i, m'(i))}</code>
</p>

<p>where <code class="reqn">d(i, m(i))</code> is the distance between object i to the first
closest medoid and d(i, m'(i)) is the distance between object
i to the second closest medoid.
</p>
<p>The <code>idmedoid</code> argument corresponds to the <code>idcluster</code> argument.
If the length of <code>idmedoid</code> is 3, for example, the <code>idcluster</code> has
to have 3 unique cluster memberships, or it returns <code>Error</code> otherwise.
The length of the <code>idcluster</code> has also to be equal to n
(the number of objects). In contrast to the silhoutte value,
the centoird shadow value is interpreted that lower value is the better
cluster separation.
</p>


<h3>Value</h3>

<p>Function returns a list with following components:
</p>
<p><code>result</code> is a data frame of the shadow values for all objects
</p>
<p><code>plot</code> is the shadow value plots of each cluster.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>F. Leisch. 2010 Neighborhood graphs, stripes and shadow plots
for cluster visualization. Statistics and Computing. vol. 20, pp. 457-469
</p>
<p>W. Budiaji. 2019 Medoid-based shadow value validation and visualization.
International Journal of Advances in Intelligent Informatics.  Vol 5 No 2
pp. 76-88
</p>


<h3>Examples</h3>

<pre><code class='language-R'>distiris &lt;- as.matrix(dist(iris[,1:4]))
res &lt;- fastkmed(distiris, 3)
sha &lt;- csv(distiris, res$medoid, res$cluster)
sha$result[c(1:3,70:75,101:103),]
sha$plot

</code></pre>

<hr>
<h2 id='distmix'>Distances for mixed variables data set</h2><span id='topic+distmix'></span>

<h3>Description</h3>

<p>This function computes a distance matrix for a mixed
variable data set applying various methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distmix(data, method = "gower", idnum = NULL, idbin = NULL, idcat = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distmix_+3A_data">data</code></td>
<td>
<p>A data frame or matrix object.</p>
</td></tr>
<tr><td><code id="distmix_+3A_method">method</code></td>
<td>
<p>A  method to calculate the mixed variables distance
(see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="distmix_+3A_idnum">idnum</code></td>
<td>
<p>A vector of column index of the numerical variables.</p>
</td></tr>
<tr><td><code id="distmix_+3A_idbin">idbin</code></td>
<td>
<p>A vector of column index of the binary variables.</p>
</td></tr>
<tr><td><code id="distmix_+3A_idcat">idcat</code></td>
<td>
<p>A vector of column index of the categorical variables.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are six methods available to calculate the mixed variable
distance. They are <code>gower</code>, <code>wishart</code>, <code>podani</code>,
<code>huang</code>, <code>harikumar</code>, <code>ahmad</code>.
</p>
<p><code>gower</code>
</p>
<p>The Gower (1971) distance is the most common distance for a mixed variable
data set. Although the Gower distance accommodates missing values, a missing
value is not allowed in this function. If there is a missing value, the Gower
distance from the <code>daisy</code> function in the <span class="pkg">cluster</span> package can be
applied. The Gower distance between objects i and j is
computed by
<code class="reqn">d_{ij} = 1 - s_{ij}</code>, where
</p>
<p style="text-align: center;"><code class="reqn">s_{ij} = \frac{\sum_{l=1}^p \omega_{ijl} s_{ijl}}
{\sum_{l=1}^p \omega_{ijl}}</code>
</p>

<p><code class="reqn">\omega_{ijl}</code> is a weight in variable l that is usually 1 or 0
(for a missing value). If the variable l is a numerical variable,
</p>
<p style="text-align: center;"><code class="reqn">s_{ijl} = 1- \frac{|x_{il} - x_{jl}|}{R_l}</code>
</p>

<p><code class="reqn">s_{ijl} \in</code> {0, 1}, if the variable l is a binary/
categorical variable.
</p>
<p><code>wishart</code>
</p>
<p>Wishart (2003) has proposed a different measure compared to Gower (1971) in
the numerical variable part. Instead of a range, it applies a variance of
the numerical variable in the <code class="reqn">s_{ijl}</code> such that the distance becomes
</p>
<p style="text-align: center;"><code class="reqn">d_{ij} = \sqrt{\sum_{l=1}^p \omega_{ijl} \left(\frac{x_{il} - x_{jl}}
{\delta_{ijl}}\right)^2}</code>
</p>

<p>where <code class="reqn">\delta_{ijl} = s_l</code> when l is a numerical variable and
<code class="reqn">\delta_{ijl} \in</code> {0, 1} when l is a binary/ categorical
variable.
</p>
<p><code>podani</code>
</p>
<p>Podani (1999) has suggested a different method to compute a distance for
a mixed variable data set. The Podani distance is calculated by
</p>
<p style="text-align: center;"><code class="reqn">d_{ij} = \sqrt{\sum_{l=1}^p \omega_{ijl} \left(\frac{x_{il} - x_{jl}}
{\delta_{ijl}}\right)^2}</code>
</p>

<p>where <code class="reqn">\delta_{ijl} = R_l</code> when l is a numerical variable and
<code class="reqn">\delta_{ijl} \in</code> {0, 1} when l is a binary/ categorical
variable.
</p>
<p><code>huang</code>
</p>
<p>The Huang (1997) distance between objects i and j is computed
by
</p>
<p style="text-align: center;"><code class="reqn"> d_{ij} = \sum_{r=1}^{P_n} (x_{ir} - x_{jr})^2 + \gamma
\sum_{s=1}^{P_c} \delta_c (x_{is} - x_{js})</code>
</p>

<p>where <code class="reqn">P_n</code> and <code class="reqn">P_c</code> are the number of numerical and categorical
variables, respectively,
</p>
<p style="text-align: center;"><code class="reqn">\gamma = \frac{\sum_{r=1}^{P_n} s_{r}^2}{P_n} </code>
</p>

<p>and <code class="reqn">\delta_c(x_{is} - x_{js})</code> is the mismatch/ simple matching distance
(see <code><a href="#topic+matching">matching</a></code>) between object i and object
j in the variable s.
</p>
<p><code>harikumar</code>
</p>
<p>Harikumar-PV (2015) has proposed a distance for a mixed variable data set:
</p>
<p style="text-align: center;"><code class="reqn"> d_{ij} = \sum_{r=1}^{P_n} |x_{ir} - x_{jr}| + \sum_{s=1}^{P_c}
\delta_c (x_{is} - x_{js}) + \sum_{t=1}^{p_b} \delta_b (x_{it}, x_{jt})</code>
</p>

<p>where <code class="reqn">P_b</code> is the number of binary variables,
<code class="reqn">\delta_c (x_{is}, x_{js})</code> is the co-occurrence distance (see
<code><a href="#topic+cooccur">cooccur</a></code>), and <code class="reqn">\delta_b (x_{it}, x_{jt})</code> is the
Hamming distance.
</p>
<p><code>ahmad</code>
</p>
<p>Ahmad and Dey (2007) has computed a distance of a mixed variable set via
</p>
<p style="text-align: center;"><code class="reqn"> d_{ij} = \sum_{r=1}^{P_n} (x_{ir} - x_{jr})^2 +
\sum_{s=1}^{P_c} \delta_c (x_{is} - x_{js})</code>
</p>

<p>where <code class="reqn">\delta_c (x_{it}, x_{jt})</code> are the co-occurrence distance
(see <code><a href="#topic+cooccur">cooccur</a></code>). In the Ahmad and Dey distance,
the binary and categorical variables are not separable such that
the co-occurrence distance is based on the combined these two classes,
i.e. binary and categorical variables. Note that this function applies
standard version of Squared Euclidean, i.e without any weight.
</p>
<p>At leas two arguments of the <code>idnum</code>, <code>idbin</code>, and
<code>idcat</code> have to be provided because this function calculates
the mixed distance. If the <code>method</code> is <code>harikumar</code>,
the categorical variables have to be at least two variables such
that the co-occurrence distance can be computed. It also applies when
<code>method = "ahmad"</code>. The <code>idbin</code> + <code>idcat</code> has to
be more than 1 column. It returns to an <code>Error</code> message otherwise.
</p>


<h3>Value</h3>

<p>Function returns a distance matrix (n x n).
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>Ahmad, A., and Dey, L. 2007. A K-mean clustering algorithm for
mixed numeric and categorical data. Data and Knowledge Engineering 63,
pp. 503-527.
</p>
<p>Gower, J., 1971. A general coefficient of similarity and some
of its properties. Biometrics 27, pp. 857-871
</p>
<p>Harikumar, S., PV, S., 2015. K-medoid clustering for
heterogeneous data sets. JProcedia Computer Science 70, pp. 226-237.
</p>
<p>Huang, Z., 1997. Clustering large data sets with mixed numeric
and categorical values, in: The First Pacific-Asia Conference on Knowledge
Discovery and Data Mining, pp. 21-34.
</p>
<p>Podani, J., 1999. Extending gower's general coefficient of
similarity to ordinal characters. Taxon 48, pp. 331-340.
</p>
<p>Wishart, D., 2003. K-means clustering with outlier detection,
mixed variables and missing values, in: Exploratory Data Analysis in
Empirical Research: Proceedings of the 25th Annual Conference of the
Gesellschaft fur Klassifikation e.V., University of Munich, March 14-16,
2001, Springer Berlin Heidelberg, Berlin, Heidelberg. pp. 216-226.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
a &lt;- matrix(sample(1:2, 7*3, replace = TRUE), 7, 3)
a1 &lt;- matrix(sample(1:3, 7*3, replace = TRUE), 7, 3)
mixdata &lt;- cbind(iris[1:7,1:3], a, a1)
colnames(mixdata) &lt;- c(paste(c("num"), 1:3, sep = ""),
                       paste(c("bin"), 1:3, sep = ""),
                       paste(c("cat"), 1:3, sep = ""))
distmix(mixdata, method = "gower", idnum = 1:3, idbin = 4:6, idcat = 7:9)

</code></pre>

<hr>
<h2 id='distNumeric'>A pair distance for numerical variables</h2><span id='topic+distNumeric'></span>

<h3>Description</h3>

<p>This function computes a pairwise numerical distance between
two numerical data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distNumeric(x, y, method = "mrw", xyequal = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distNumeric_+3A_x">x</code></td>
<td>
<p>A first data matrix (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="distNumeric_+3A_y">y</code></td>
<td>
<p>A second data matrix (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="distNumeric_+3A_method">method</code></td>
<td>
<p>A method to calculate the pairwise numerical distance
(see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="distNumeric_+3A_xyequal">xyequal</code></td>
<td>
<p>A logical if <code>x</code> is equal to <code>y</code> (see
<strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>x</code> and <code>y</code> arguments have to be matrices with
the same number of columns where the row indicates the object and the
column is the variable. This function calculate all pairwise distance
between rows in the <code>x</code> and <code>y</code> matrices. Although it calculates
a pairwise distance between two data sets, the default function computes all
distances in the <code>x</code> matrix. If the <code>x</code> matrix is not equal to
the <code>y</code> matrix, the <code>xyequal</code> has to be set <code>FALSE</code>.
</p>
<p>The <code>method</code> available are <code>mrw</code> (Manhattan weighted by range),
<code>sev</code> (squared Euclidean weighted by variance), <code>ser</code>
(squared Euclidean weighted by range), <code>ser.2</code> (squared Euclidean
weighted by squared range) and <code>se</code> (squared Euclidean).
Their formulas are:
</p>
<p style="text-align: center;"><code class="reqn">mrw_{ij} = \sum_{r=1}^{p_n} \frac{|x_{ir} - x_{jr}|}{R_r}</code>
</p>

<p style="text-align: center;"><code class="reqn">sev_{ij} = \sum_{r=1}^{p_n} \frac{(x_{ir} - x_{jr})^2}{s_r^2}</code>
</p>

<p style="text-align: center;"><code class="reqn">ser_{ij} = \sum_{r=1}^{p_n} \frac{(x_{ir} - x_{jr})^2}{ R_r }</code>
</p>

<p style="text-align: center;"><code class="reqn">ser.2_{ij} = \sum_{r=1}^{p_n} \frac{(x_{ir} - x_{jr})^2}{ R_r^2 }</code>
</p>

<p style="text-align: center;"><code class="reqn">se_{ij} = \sum_{r=1}^{p_n} (x_{ir} - x_{jr})^2</code>
</p>

<p>where <code class="reqn">p_n</code> is the number of numerical variables, <code class="reqn">R_r</code> is the range
of the r-th variables, <code class="reqn">s_r^2</code> is the variance of the r-th
variables.
</p>


<h3>Value</h3>

<p>Function returns a distance matrix with the number of rows equal to
the number of objects in the <code>x</code> matrix (<code class="reqn">n_x</code>) and the number of
columns equals to the number of objects in the <code>y</code> matrix (<code class="reqn">n_y</code>).
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>num &lt;- as.matrix(iris[,1:4])
mrwdist &lt;- distNumeric(num, num, method = "mrw")
mrwdist[1:6,1:6]

</code></pre>

<hr>
<h2 id='fastkmed'>Simple and fast k-medoid algorithm</h2><span id='topic+fastkmed'></span>

<h3>Description</h3>

<p>This function runs the simple and fast k-medoid algorithm
proposed by Park and Jun (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fastkmed(distdata, ncluster, iterate = 10, init = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fastkmed_+3A_distdata">distdata</code></td>
<td>
<p>A distance matrix (n x n) or dist object.</p>
</td></tr>
<tr><td><code id="fastkmed_+3A_ncluster">ncluster</code></td>
<td>
<p>A number of clusters.</p>
</td></tr>
<tr><td><code id="fastkmed_+3A_iterate">iterate</code></td>
<td>
<p>A number of iterations for the clustering algorithm.</p>
</td></tr>
<tr><td><code id="fastkmed_+3A_init">init</code></td>
<td>
<p>A vector of initial objects as the cluster medoids
(see <strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The simple and fast k-medoids, which sets a set of medoids as the
cluster centers, adapts the k-means algorithm for medoid up-dating.
The new medoids of each iteration are calculated in the within cluster
only such that it gains speed.
</p>
<p><code>init = NULL</code> is required because the Park and Jun (2009) has
a particular method to select the initial medoids.  The initial medoids
are selected by
</p>
<p style="text-align: center;"><code class="reqn"> v_j = \sum_{i=1}^n \frac{d_{ij}}{\sum_{l=1}^n d_{il}},
\quad j = 1, 2, 3, \ldots, n </code>
</p>

<p>where the first k of the <code class="reqn">v_j</code> is selected if the number of
clusters is k.
</p>
<p><code>init</code> can be provided with a vector of id objects. The length of
the vector has to be equal to the number of clusters. However, assigning
a vector in the <code>init</code> argument, the algorithm is no longer the simple
and fast k-medoids algorithm. The <code><a href="#topic+inckmed">inckmed</a></code> function,
for example, defines a different method to select the initial medoid
though it applies the <code><a href="#topic+fastkmed">fastkmed</a></code> function.
</p>


<h3>Value</h3>

<p>Function returns a list of components:
</p>
<p><code>cluster</code> is the clustering memberships result.
</p>
<p><code>medoid</code> is the id medoids.
</p>
<p><code>minimum_distance</code> is the distance of all objects to their cluster
medoid.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>Park, H., Jun, C., 2009. A simple and fast algorithm for
k-medoids clustering. Expert Systems with Applications 36, pp. 3336-3341.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>num &lt;- as.matrix(iris[,1:4])
mrwdist &lt;- distNumeric(num, num, method = "mrw")
result &lt;- fastkmed(mrwdist, ncluster = 3, iterate = 50)
table(result$cluster, iris[,5])


</code></pre>

<hr>
<h2 id='globalfood'>Global food security index</h2><span id='topic+globalfood'></span>

<h3>Description</h3>

<p>A dataset containing four variables of 113 countries for their food security
index based on panelists evaluation in 2017.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>globalfood
</code></pre>


<h3>Format</h3>

<p>A data frame with 113 rows and 4 variables:
</p>

<dl>
<dt>affordability</dt><dd><p>Index of food affordability.</p>
</dd>
<dt>availability</dt><dd><p>Index of food availability.</p>
</dd>
<dt>safety</dt><dd><p>Index of food quality and safety.</p>
</dd>
<dt>resilience</dt><dd><p>Index of natural resources and resilience.</p>
</dd>
</dl>



<h3>Source</h3>

<p>The original indicator variables consist of 27 variables. Then,
they are summarized into four pillars of food security; they are
affordability, availability, quality and safety, and natural resources
and resilience. Food-security expertise panelists evaluate the score of
each country from 0 to 100, where 0 is the least favorable towards food
security.
</p>
<p><a href="https://impact.economist.com/sustainability/project/food-security-index/">https://impact.economist.com/sustainability/project/food-security-index/</a>
</p>

<hr>
<h2 id='heart'>Heart Disease data set</h2><span id='topic+heart'></span>

<h3>Description</h3>

<p>A mixed variable dataset containing 14 variables of 297 patients for
their heart disease diagnosis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heart
</code></pre>


<h3>Format</h3>

<p>A data frame with 297 rows and 14 variables:
</p>

<dl>
<dt>age</dt><dd><p>Age in years (numerical).</p>
</dd>
<dt>sex</dt><dd><p>Sex: 1 = male, 0 = female (logical).</p>
</dd>
<dt>cp</dt><dd><p>Four chest pain types: (1) typical angina, (2) atypical angina
(3)non-anginal pain, (4) asymptomatic (categorical).</p>
</dd>
<dt>trestbps</dt><dd><p>Resting blood pressure (in mm Hg on admission to
the hospital) (numerical).</p>
</dd>
<dt>chol</dt><dd><p>Serum cholestoral in mg/dl (numerical).</p>
</dd>
<dt>fbs</dt><dd><p>Fasting blood sugar more than 120 mg/dl (logical).</p>
</dd>
<dt>restecg</dt><dd><p>Resting electrocardiographic results: (0) normal,
(1) having ST-T wave abnormality, (2) showing probable or definite
left ventricular hypertrophy by Estes' criteria (categorical).</p>
</dd>
<dt>thalach</dt><dd><p>Maximum heart rate achieved (numerical).</p>
</dd>
<dt>exang</dt><dd><p>Exercise induced angina (logical).</p>
</dd>
<dt>oldpeak</dt><dd><p>ST depression induced by exercise relative to
rest (numerical).</p>
</dd>
<dt>slope</dt><dd><p>The slope of the peak exercise ST segment: (1) upsloping,
(2) flat, (3) downsloping (categorical).</p>
</dd>
<dt>ca</dt><dd><p>Number of major vessels (0-3) colored by flourosopy (numerical).</p>
</dd>
<dt>thal</dt><dd><p>(3) normal, (6) fixed defect, (7) reversable defect
(categorical).</p>
</dd>
<dt>class</dt><dd><p>Diagonosis of heart disease (4 classes). It can be 2 classes
by setting 0 for 0 values and 1 for non-0 values.</p>
</dd>
</dl>



<h3>Source</h3>

<p>The data set is taken from machine learning repository of UCI.
The original data set consists of 303 patients with 6 NA's. Then,
the missing values are omitted such that it reduces into 297 patients.
</p>
<p><a href="https://archive.ics.uci.edu/ml/datasets/Heart+Disease">https://archive.ics.uci.edu/ml/datasets/Heart+Disease</a>
</p>


<h3>References</h3>

<p>Lichman, M. (2013). UCI machine learning repository.
</p>

<hr>
<h2 id='inckmed'>Increasing number of clusters in k-medoids algorithm</h2><span id='topic+inckmed'></span>

<h3>Description</h3>

<p>This function runs the increasing number of  clusters in
the k-medoids algorithm proposed by Yu et. al. (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inckmed(distdata, ncluster, iterate = 10, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inckmed_+3A_distdata">distdata</code></td>
<td>
<p>A distance matrix (n x n) or dist object.</p>
</td></tr>
<tr><td><code id="inckmed_+3A_ncluster">ncluster</code></td>
<td>
<p>A number of clusters.</p>
</td></tr>
<tr><td><code id="inckmed_+3A_iterate">iterate</code></td>
<td>
<p>A number of iterations for the clustering algorithm.</p>
</td></tr>
<tr><td><code id="inckmed_+3A_alpha">alpha</code></td>
<td>
<p>A stretch factor to determine the range of initial medoid
selection (see <strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This algorithm is claimed to manage with the weakness of the
simple and fast-kmedoids (<code><a href="#topic+fastkmed">fastkmed</a></code>). The origin of the
algorithm is a centroid-based algorithm by applying the Euclidean distance.
Then, Bbecause the function is a medoid-based algorithm, the object mean
(centroid) and variance are redefined into medoid and deviation, respectively.
</p>
<p>The <code>alpha</code> argument is a stretch factor, i.e. a constant defined by
the user. It is applied to determine a set of medoid candidates. The medoid
candidates are calculated by
<code class="reqn">O_c = </code>{<code class="reqn">X_i</code>| <code class="reqn">\sigma_i \leq \alpha \sigma,
i = 1, 2, \ldots, n</code> },
where <code class="reqn">\sigma_i</code>  is the average deviation of object i, and
<code class="reqn">\sigma</code> is the average deviation of the data set. They are computed by
</p>
<p style="text-align: center;"><code class="reqn">\sigma = \sqrt{\frac{1}{n-1} \sum_{i=1}^n d(O_i, v_1)}</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma_i = \sqrt{\frac{1}{n-1} \sum_{i=1}^n d(O_i, O_j)}</code>
</p>

<p>where n is the number of objects, <code class="reqn">O_i</code> is the object i,
and <code class="reqn">v_1</code> is the most centrally located object.
</p>


<h3>Value</h3>

<p>Function returns a list of components:
</p>
<p><code>cluster</code> is the clustering memberships result.
</p>
<p><code>medoid</code> is the id medoids.
</p>
<p><code>minimum_distance</code> is the distance of all objects to their cluster
medoid.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>Yu, D., Liu, G., Guo, M., Liu, X., 2018. An improved K-medoids
algorithm based on step increasing and optimizing medoids. Expert Systems
with Applications 92, pp. 464-473.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>num &lt;- as.matrix(iris[,1:4])
mrwdist &lt;- distNumeric(num, num, method = "mrw")
result &lt;- inckmed(mrwdist, ncluster = 3, iterate = 50, alpha = 1.5)
table(result$cluster, iris[,5])


</code></pre>

<hr>
<h2 id='matching'>A pair distance for binary/ categorical variables</h2><span id='topic+matching'></span>

<h3>Description</h3>

<p>This function computes the simple matching distance from
two data frames/ matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matching(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matching_+3A_x">x</code></td>
<td>
<p>A first data frame or matrix (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="matching_+3A_y">y</code></td>
<td>
<p>A second data frame or matrix (see <strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>x</code> and <code>y</code> arguments have to be data frames/
matrices with the same number of columns where the row indicates the object
and the column is the variable. This function calculates all pairwise
distance between rows in the <code>x</code> and <code>y</code> data frames/ matrices.
If the <code>x</code> data frame/ matrix is equal to the <code>y</code> data frame/
matrix, it explicitly calculates all distances in the <code>x</code> data frame/
matrix.
</p>
<p>The simple matching distance between objects i and j is
calculated by
</p>
<p style="text-align: center;"><code class="reqn">d_{ij} = \frac{\sum_{s=1}^{P}(x_{is}-x_{js})}{P}</code>
</p>

<p>where  <code class="reqn">P</code> is the number of variables, and <code class="reqn"> x_{is}-x_{js} \in</code>
{0, 1}. <code class="reqn"> x_{is}-x_{js} = 0</code>, if <code class="reqn"> x_{is}=x_{js}</code> and
<code class="reqn">x_{is}-x_{js} = 1</code>, when <code class="reqn">x_{is} \neq x_{js}</code>.
</p>
<p>As an example, the distance between objects 1 and 2 is presented.
</p>

<table>
<tr>
 <td style="text-align: left;">
object </td><td style="text-align: right;"> x </td><td style="text-align: right;"> y </td><td style="text-align: right;"> z </td>
</tr>
<tr>
 <td style="text-align: left;">
1 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
2 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 1
</td>
</tr>

</table>

<p>The distance between objects 1 and 2 is
</p>
<p style="text-align: center;"><code class="reqn">d_{12} = \frac{\sum_{s=1}^{3}(x_{is}-x_{js})}{3} = \frac{0 + 0 + 1}{3} =
\frac{1}{3} = 0.33</code>
</p>



<h3>Value</h3>

<p>Function returns a distance matrix with the number of rows equal to
the number of objects in the <code>x</code> data frame/ matrix (<code class="reqn">n_x</code>) and
the number of columns equals to the number of objects in the <code>y</code>
data frame/ matrix (<code class="reqn">n_y</code>).
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
a &lt;- matrix(sample(1:2, 7*3, replace = TRUE), 7, 3)
matching(a, a)

</code></pre>

<hr>
<h2 id='msv'>Medoid shadow value (MSV) index and plot</h2><span id='topic+msv'></span>

<h3>Description</h3>

<p>This function computes medoid shadow values and shadow value plots of
each cluster. The plot presents the mean of the shadow values as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msv(distdata, idmedoid, idcluster, title = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msv_+3A_distdata">distdata</code></td>
<td>
<p>A distance matrix (n x n) or dist object.</p>
</td></tr>
<tr><td><code id="msv_+3A_idmedoid">idmedoid</code></td>
<td>
<p>A vector of id medoids (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="msv_+3A_idcluster">idcluster</code></td>
<td>
<p>A vector of cluster membership (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="msv_+3A_title">title</code></td>
<td>
<p>A title of the plot.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The origin of the shadow value is calculated in the <code>shadow</code>
function of the <span class="pkg">flexclust</span> package, in which it is based on the first and
second closest centroid. The <code>msv</code> function in this package modifies
the centroid into medoid such that the formula to compute shadow value of
object i is
</p>
<p style="text-align: center;"><code class="reqn">msv(i) = \frac{d(i, m'(i))-d(i, m(i))}{d(i, m'(i))}</code>
</p>

<p>where <code class="reqn">d(i, m(i))</code> is the distance between object i to the first
closest medoid and d(i, m'(i)) is the distance between object
i to the second closest medoid.
</p>
<p>The <code>idmedoid</code> argument corresponds to the <code>idcluster</code> argument.
If the length of <code>idmedoid</code> is 3, for example, the <code>idcluster</code> has
to have 3 unique cluster memberships, or it returns <code>Error</code> otherwise.
The length of the <code>idcluster</code> has also to be equal to n
(the number of objects). In contrast to the centroid shadow value,
the medoid shadow value is interpreted likewise a silhoutte value,
the higher value the better separation.
</p>


<h3>Value</h3>

<p>Function returns a list with following components:
</p>
<p><code>result</code> is a data frame of the shadow values for all objects
</p>
<p><code>plot</code> is the shadow value plots of each cluster.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>F. Leisch. 2010 Neighborhood graphs, stripes and shadow plots
for cluster visualization. Statistics and Computing. vol. 20, pp. 457-469
</p>
<p>W. Budiaji. 2019 Medoid-based shadow value validation and visualization.
International Journal of Advances in Intelligent Informatics.  Vol 5 No 2
pp. 76-88
</p>


<h3>Examples</h3>

<pre><code class='language-R'>distiris &lt;- as.matrix(dist(iris[,1:4]))
res &lt;- fastkmed(distiris, 3)
sha &lt;- msv(distiris, res$medoid, res$cluster)
sha$result[c(1:3,70:75,101:103),]
sha$plot

</code></pre>

<hr>
<h2 id='pcabiplot'>Biplot of a PCA object</h2><span id='topic+pcabiplot'></span>

<h3>Description</h3>

<p>This function creates a biplot from a pca object, which is
generated by the <code>prcomp</code> function from the <span class="pkg">stats</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcabiplot(
  PC,
  x = "PC1",
  y = "PC2",
  var.line = TRUE,
  colobj = rep(1, nrow(PC$x)),
  o.size = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcabiplot_+3A_pc">PC</code></td>
<td>
<p>A pca object generated by <code>prcomp</code> function.</p>
</td></tr>
<tr><td><code id="pcabiplot_+3A_x">x</code></td>
<td>
<p>X axis (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="pcabiplot_+3A_y">y</code></td>
<td>
<p>Y axis (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="pcabiplot_+3A_var.line">var.line</code></td>
<td>
<p>A logical input, if variable lines are plotted.</p>
</td></tr>
<tr><td><code id="pcabiplot_+3A_colobj">colobj</code></td>
<td>
<p>A vector to provide color in the objects (see
<strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="pcabiplot_+3A_o.size">o.size</code></td>
<td>
<p>A numeric number to set the object size.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a function to plot a pca biplot from a pca object. The x
and y axes can be supplied with any principle component. The length of the
<code>colobj</code> vector has to be equal to the number of objects. This argument
controls the color of the objects and is very convenient to explore the
clustering result. The default value is that all object have the same color.
</p>


<h3>Value</h3>

<p>Function returns a plot of pca.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pcadat &lt;- prcomp(iris[,1:4], scale. = TRUE)
pcabiplot(pcadat)

</code></pre>

<hr>
<h2 id='rankkmed'>Rank k-medoid algorithm</h2><span id='topic+rankkmed'></span>

<h3>Description</h3>

<p>This function runs the rank k-medoids algorithm proposed by
Zadegan et. al. (2013).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rankkmed(distdata, ncluster, m = 3, iterate = 10, init = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rankkmed_+3A_distdata">distdata</code></td>
<td>
<p>A distance matrix (n x n) or dist object.</p>
</td></tr>
<tr><td><code id="rankkmed_+3A_ncluster">ncluster</code></td>
<td>
<p>A number of clusters.</p>
</td></tr>
<tr><td><code id="rankkmed_+3A_m">m</code></td>
<td>
<p>A number of objects to compute hostility (see
<strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="rankkmed_+3A_iterate">iterate</code></td>
<td>
<p>A number of iterations for the clustering algorithm.</p>
</td></tr>
<tr><td><code id="rankkmed_+3A_init">init</code></td>
<td>
<p>A vector of initial objects as the cluster medoids
(see <strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This algorithm is claimed to cope with the local optima problem
of the simple and fast-kmedoids algorithm (<code><a href="#topic+fastkmed">fastkmed</a></code>). The
<code>m</code> argument is defined by the user and has to be <code class="reqn">1 &lt; m \leq n</code>.
The <code>m</code> is a hostility measure computed by
</p>
<p style="text-align: center;"><code class="reqn">m_i = \sum_{X_j \in Y} r_{ij}</code>
</p>

<p>where <code class="reqn">x_j</code> is the object j, Y is the set of objects
as many as m, and <code class="reqn">r_{ij}</code> is the rank distance, i.e. sorted
distance, between object i and j.
</p>
<p><code>init</code> can be provided with a vector of id objects. The length of
the vector has to be equal to the number of clusters. However, assigning
a vector in the <code>init</code> argument, the algorithm is no longer the rank
k-medoids algorithm.
</p>


<h3>Value</h3>

<p>Function returns a list of components:
</p>
<p><code>cluster</code> is the clustering memberships result.
</p>
<p><code>medoid</code> is the id medoids.
</p>
<p><code>minimum_distance</code> is the distance of all objects to their cluster
medoid.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>Zadegan, S.M.R, Mirzaie M, and Sadoughi, F. 2013. Ranked k-medoids: A fast and
accurate rank-based partitioning algorithm for clustering large datasets. Knowledge-Based
Systems 39, 133-143.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>num &lt;- as.matrix(iris[,1:4])
mrwdist &lt;- distNumeric(num, num, method = "mrw")
result &lt;- rankkmed(mrwdist, ncluster = 3, iterate = 50)
table(result$cluster, iris[,5])


</code></pre>

<hr>
<h2 id='sil'>Silhouette index and plot</h2><span id='topic+sil'></span>

<h3>Description</h3>

<p>This function creates silhouette indices and silhouette plots of
each cluster. The plot presents also the mean of the silhouette indices per
cluster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sil(distdata, idmedoid, idcluster, title = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sil_+3A_distdata">distdata</code></td>
<td>
<p>A distance matrix (n x n) or dist object.</p>
</td></tr>
<tr><td><code id="sil_+3A_idmedoid">idmedoid</code></td>
<td>
<p>A vector of id medoids (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="sil_+3A_idcluster">idcluster</code></td>
<td>
<p>A vector of cluster membership (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="sil_+3A_title">title</code></td>
<td>
<p>A title of the plot.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The silhouette index of object i is calculated by
</p>
<p style="text-align: center;"><code class="reqn">si(i)=\frac{b_i-a_i}{max(a_i, b_i)}</code>
</p>

<p>where <code class="reqn">a_i</code> is the average distance of object i to all objects
within the cluster, and <code class="reqn">b_i</code> is the average distance of object i
to all objects within the nearest cluster.
</p>
<p>The <code>idmedoid</code> argument corresponds to the <code>idcluster</code> argument.
If the length of <code>idmedoid</code> is 3, for example, the <code>idcluster</code> has
to have 3 unique memberships, or it returns <code>Error</code> otherwise. The
length of the <code>idcluster</code> has also to be equal to n
(the number of objects).
</p>


<h3>Value</h3>

<p>Function returns a list with following components:
</p>
<p><code>result</code> is a data frame of the silhouette indices for all objects
</p>
<p><code>plot</code> is the silhouette plots of each cluster.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>P. J. Rousseeuw. 1987 Silhouettes: a graphical aid to
the interpretation and validation of cluster analysis.
Journal of Computational and Applied Mathematics, vol. 20, pp. 53-65
</p>


<h3>Examples</h3>

<pre><code class='language-R'>distiris &lt;- as.matrix(dist(iris[,1:4]))
res &lt;- fastkmed(distiris, 3)
silhouette &lt;- sil(distiris, res$medoid, res$cluster)
silhouette$result[c(1:3,70:75,101:103),]
silhouette$plot

</code></pre>

<hr>
<h2 id='skm'>Simple k-medoid algorithm</h2><span id='topic+skm'></span>

<h3>Description</h3>

<p>This function runs the simple k-medoid algorithm
proposed by Budiaji and Leisch (2019).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skm(distdata, ncluster, seeding = 20, iterate = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="skm_+3A_distdata">distdata</code></td>
<td>
<p>A distance matrix (n x n) or dist object.</p>
</td></tr>
<tr><td><code id="skm_+3A_ncluster">ncluster</code></td>
<td>
<p>A number of clusters.</p>
</td></tr>
<tr><td><code id="skm_+3A_seeding">seeding</code></td>
<td>
<p>A number of seedings to run the algorithm
(see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="skm_+3A_iterate">iterate</code></td>
<td>
<p>A number of iterations for each seeding
(see <strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The simple k-medoids, which sets a set of medoids as the
cluster centers, adapts the simple and fast k-medoid algoritm.
The best practice to run the simple and fast k-medoid is by running
the algorithm several times with different random seeding options.
</p>


<h3>Value</h3>

<p>Function returns a list of components:
</p>
<p><code>cluster</code> is the clustering memberships result.
</p>
<p><code>medoid</code> is the id medoids.
</p>
<p><code>minimum_distance</code> is the distance of all objects to their cluster
medoid.
</p>


<h3>Author(s)</h3>

<p>Weksi Budiaji <br /> Contact: <a href="mailto:budiaji@untirta.ac.id">budiaji@untirta.ac.id</a>
</p>


<h3>References</h3>

<p>W. Budiaji, and F. Leisch. 2019. Simple K-Medoids Partitioning
Algorithm for Mixed Variable Data. Algorithms Vol 12(9) 177
</p>


<h3>Examples</h3>

<pre><code class='language-R'>num &lt;- as.matrix(iris[,1:4])
mrwdist &lt;- distNumeric(num, num, method = "mrw")
result &lt;- skm(mrwdist, ncluster = 3, seeding = 50)
table(result$cluster, iris[,5])


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
