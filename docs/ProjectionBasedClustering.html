<!DOCTYPE html><html><head><title>Help for package ProjectionBasedClustering</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ProjectionBasedClustering}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CCA'><p>Curvilinear Component Analysis (CCA)</p></a></li>
<li><a href='#ContTrustMeasure'>
<p>Measure of trustworthiness and continuity for projection</p></a></li>
<li><a href='#DefaultColorSequence'>
<p>Default color sequence for plots</p></a></li>
<li><a href='#Delaunay4Points'>
<p>Adjacency matrix of the delaunay graph for BestMatches of Points</p></a></li>
<li><a href='#DijkstraSSSP'>
<p>Dijkstra SSSP</p></a></li>
<li><a href='#Hepta'>
<p>Hepta is part of the Fundamental Clustering Problem Suit (FCPS) [Thrun/Ultsch, 2020].</p></a></li>
<li><a href='#ICA'><p> Independent Component Analysis (ICA)</p></a></li>
<li><a href='#interactiveClustering'><p>GUI for interactive cluster analysis</p></a></li>
<li><a href='#interactiveGeneralizedUmatrixIsland'><p>GUI for cutting out an Island.</p></a></li>
<li><a href='#interactiveProjectionBasedClustering'>
<p>Interactive Projection-Based Clustering (IPBC)</p></a></li>
<li><a href='#Isomap'><p>Isomap</p></a></li>
<li><a href='#KLMeasure'>
<p>Rank-based smoothed precision/recall measure for projection.</p></a></li>
<li><a href='#KruskalStress'>
<p>Kruskal stress calculation</p></a></li>
<li><a href='#MDS'><p>Multidimensional Scaling (MDS)</p></a></li>
<li><a href='#NeRV'>
<p>Neighbor Retrieval Visualizer (NeRV)</p></a></li>
<li><a href='#PCA'><p>Principal Component Analysis (PCA)</p></a></li>
<li><a href='#PlotProjectedPoints'>
<p>Plot Projected Points</p></a></li>
<li><a href='#PolarSwarm'>
<p>Polar Swarm (Pswarm)</p></a></li>
<li><a href='#Projection2Bestmatches'>
<p>Projection to Bestmatches</p></a></li>
<li><a href='#ProjectionBasedClustering'>
<p>Automatic Projection-based Clustering (PBC) [Thrun/Ultsch, 2020]</p></a></li>
<li><a href='#ProjectionBasedClustering-package'>
<p>Projection Based Clustering</p></a></li>
<li><a href='#ProjectionPursuit'><p> Projection Pursuit</p></a></li>
<li><a href='#SammonsMapping'><p>Sammons Mapping</p></a></li>
<li><a href='#ShortestGraphPathsC'>
<p>Shortest GraphPaths = geodesic distances</p></a></li>
<li><a href='#tSNE'><p>T-distributed Stochastic Neighbor Embedding (t-SNE)</p></a></li>
<li><a href='#UniformManifoldApproximationProjection'>
<p>Uniform Manifold Approximation and Projection</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Projection Based Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-11</td>
</tr>
<tr>
<td>Description:</td>
<td>A clustering approach applicable to every projection method is proposed here. The two-dimensional scatter plot of any projection method can construct a topographic map which displays unapparent data structures by using distance and density information of the data. The generalized U*-matrix renders this visualization in the form of a topographic map, which can be used to automatically define the clusters of high-dimensional data. The whole system is based on Thrun and Ultsch, "Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data" &lt;<a href="https://doi.org/10.1007%2Fs00357-020-09373-2">doi:10.1007/s00357-020-09373-2</a>&gt;. Selecting the correct projection method will result in a visualization in which mountains surround each cluster. The number of clusters can be determined by counting valleys on the topographic map. Most projection methods are wrappers for already available methods in R. By contrast, the neighbor retrieval visualizer (NeRV) is based on C++ source code of the 'dredviz' software package, the t-SNE multicore version is based on C++ source code of Dmitry Ulyanov, and the Curvilinear Component Analysis (CCA) is translated from 'MATLAB' ('SOM Toolbox' 2.0) to R.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, ggplot2, stats, graphics, vegan, deldir, geometry,
GeneralizedUmatrix, shiny, shinyjs, shinythemes, plotly,
grDevices</td>
</tr>
<tr>
<td>Suggests:</td>
<td>DataVisualizations, fastICA, tsne, FastKNN, MASS, pcaPP,
spdep, pracma, grid, mgcv, fields, png, reshape2, Rtsne,
methods, dendextend, umap, uwot, DatabionicSwarm, parallelDist,
parallel</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.deepbionics.org">https://www.deepbionics.org</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/Mthrun/ProjectionBasedClustering/issues">https://github.com/Mthrun/ProjectionBasedClustering/issues</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-11 09:30:58 UTC; MCT</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Thrun [aut, cre, cph],
  Quirin Stier <a href="https://orcid.org/0000-0002-7896-4737"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb, rev],
  Brinkmann Luca [ctb],
  Florian Lerch [aut],
  Felix Pape [aut],
  Tim Schreier [aut],
  Luis Winckelmann [aut],
  Kristian Nybo [cph],
  Jarkko Venna [cph],
  Ulyanov Dimitry [cph],
  van der Maaten Laurens [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Thrun &lt;m.thrun@gmx.net&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-11 18:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='CCA'>Curvilinear Component Analysis (CCA)</h2><span id='topic+CCA'></span>

<h3>Description</h3>

<p>CCA Projects data vectors using Curvilinear Component Analysis [Demartines/Herault, 1995],[Demartines/Herault, 1997].
</p>
<p>Unknown values (NaN's) in the data: projections of vectors with
unknown components tend to drift towards the center of the
projection distribution. Projections of totally unknown vectors are
set to unknown (NaN).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CCA(DataOrDistances,Epochs,OutputDimension=2,method='euclidean',

alpha0 = 0.5, lambda0,PlotIt=FALSE,Cls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CCA_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Numerical matrix defined as either 
</p>
<p><code>Data</code>, i.e., [1:n,1:d], nonsymmetric, and consists of n cases of d-dimensional data points with every case having d attributes, variables or features,
</p>
<p>or
</p>
<p><code>Distances</code>, i.e.,[1:n,1:n], symmetric and consists of n cases, e.g., <code>as.matrix(dist(Data,method))</code>
</p>
</td></tr>
<tr><td><code id="CCA_+3A_epochs">Epochs</code></td>
<td>
<p>Number of eppochs (scalar), i.e, training length</p>
</td></tr>
<tr><td><code id="CCA_+3A_outputdimension">OutputDimension</code></td>
<td>
<p>Number of dimensions in the Outputspace, default=2</p>
</td></tr>
<tr><td><code id="CCA_+3A_method">method</code></td>
<td>
<p>method specified by distance string. One of: 'euclidean','cityblock=manhatten','cosine','chebychev','jaccard','minkowski','manhattan','binary'</p>
</td></tr>
<tr><td><code id="CCA_+3A_alpha0">alpha0</code></td>
<td>
<p>(scalar) initial step size, 0.5 by default</p>
</td></tr>
<tr><td><code id="CCA_+3A_lambda0">lambda0</code></td>
<td>
<p>(scalar) initial radius of influence, 3*max(std(D)) by default</p>
</td></tr>
<tr><td><code id="CCA_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, If TRUE: Plots the projection as a 2d visualization. 
OutputDimension&gt;2: only the first two dimensions will be shown
</p>
</td></tr>
<tr><td><code id="CCA_+3A_cls">Cls</code></td>
<td>
<p>[1:n,1] Optional,: only relevant if PlotIt=TRUE. Numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An short overview of different types of projection methods can be found in [Thrun, 2018, p.42, Fig. 4.1] (<a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>).
</p>


<h3>Value</h3>

<p>A n by OutputDimension matrix containing coordinates of the projected points.</p>


<h3>Note</h3>

<p>Only Transfered from matlab to R. Matlabversion: Contributed to SOM Toolbox 2.0, February 2nd, 2000 by Juha Vesanto.
</p>
<p>You can use the standard <code>Sheparddiagram</code> or the better approach through the <code>ShepardDensityScatter</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Florian Lerch</p>


<h3>References</h3>

<p>[Demartines/Herault, 1997]  Demartines, P., &amp; Herault, J.: Curvilinear component analysis: A self-organizing neural network for nonlinear mapping of data sets, IEEE Transactions on Neural Networks, Vol. 8(1), pp. 148-154. 1997.
</p>
<p>[Demartines/Herault, 1995]  Demartines, P., &amp; Herault, J.: CCA:&quot; Curvilinear component analysis&quot;, Proc. 15 Colloque sur le traitement du signal et des images, Vol. 199, GRETSI, Groupe d'Etudes du Traitement du Signal et des Images, France 18-21 September, 1995.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data

Proj=CCA(Data,Epochs=20)

## Not run: 
PlotProjectedPoints(Proj$ProjectedPoints,Hepta$Cls)

## End(Not run)
</code></pre>

<hr>
<h2 id='ContTrustMeasure'>
Measure of trustworthiness and continuity for projection</h2><span id='topic+ContTrustMeasure'></span>

<h3>Description</h3>

<p>Computes trustworthiness and continuity for projected data (see [Kaski2003]).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ContTrustMeasure(datamat, projmat, lastNeighbor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ContTrustMeasure_+3A_datamat">datamat</code></td>
<td>

<p>numerical matrix of data: n cases in rows, d variables in columns
</p>
</td></tr>
<tr><td><code id="ContTrustMeasure_+3A_projmat">projmat</code></td>
<td>

<p>numerical matrix of projected data: n cases in rows, k variables in columns, where k is the projection output dimension
</p>
</td></tr>
<tr><td><code id="ContTrustMeasure_+3A_lastneighbor">lastNeighbor</code></td>
<td>

<p>scalar, maximal size of neighborhood to be considered</p>
</td></tr>
</table>


<h3>Details</h3>

<p>C++ source code comes from <a href="https://research.cs.aalto.fi/pml/software/dredviz/">https://research.cs.aalto.fi/pml/software/dredviz/</a>
</p>


<h3>Value</h3>

<p>numerical [k,7] matrix, where k is the lastNeighbor value.
The matrix contains the columns:
</p>
<p>Neighborhood size; worst-case trustworthiness; average trustworthiness; best-case trustworthiness; worst-case continuity; average continuity; best-case continuity
</p>
<p>where neighborhood size is the size of the neighberhood considered, which ranges from 1:lastNeighbor
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Kaski2003]: Samuel Kaski, Janne Nikkilä, Merja Oja, Jarkko Venna, Petri Törönen, and Eero Castren. Trustworthiness and metrics in visualizing similarity of gene expression. BMC Bioinformatics, 4:48, 2003.
</p>


<h3>See Also</h3>

<p>An alternative measure is the <a href="#topic+KLMeasure">KLMeasure</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data
res=MDS(Data)
Proj = res$ProjectedPoints
PlotProjectedPoints(res$ProjectedPoints,Hepta$Cls)

ContTrustMeasure(Hepta$Data, Proj, 10)
</code></pre>

<hr>
<h2 id='DefaultColorSequence'>
Default color sequence for plots
</h2><span id='topic+DefaultColorSequence'></span>

<h3>Description</h3>

<p>Defines the default color sequence for plots made within the Projections package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("DefaultColorSequence")</code></pre>


<h3>Format</h3>

<p>A vector with 562 different strings describing colors for plots.
</p>

<hr>
<h2 id='Delaunay4Points'>
Adjacency matrix of the delaunay graph for BestMatches of Points
</h2><span id='topic+Delaunay4Points'></span>

<h3>Description</h3>

<p>Calculates the adjacency matrix of the delaunay graph for BestMatches (BMs) in tiled form if BestMatches are located on a toroid grid
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Delaunay4Points(Points, IsToroid = TRUE,Grid=NULL,PlotIt=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Delaunay4Points_+3A_points">Points</code></td>
<td>
<p>[1:n,1:3] matrix containing the BMKey, X and Y coordinates of the n,  BestMatches NEED NOT BE UNIQUE,  however, there is an edge in the Deaunay between duplicate points!</p>
</td></tr>
<tr><td><code id="Delaunay4Points_+3A_istoroid">IsToroid</code></td>
<td>
<p>OPTIONAL, logical, indicating if BM's are on a toroid grid. Default is True</p>
</td></tr>
<tr><td><code id="Delaunay4Points_+3A_grid">Grid</code></td>
<td>
<p>OPTIONAL, A vector of length 2, containing the number of lines and columns of the Grid</p>
</td></tr>
<tr><td><code id="Delaunay4Points_+3A_plotit">PlotIt</code></td>
<td>
<p>OPTIONAL, bool, Plots the graph</p>
</td></tr>
</table>


<h3>Details</h3>

<p>as
</p>


<h3>Value</h3>

<p>Delaunay[1:n,1:n]               adjacency matrix of the Delaunay-Graph
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun, 2018]  Thrun, M. C.: Projection Based Clustering through Self-Organization and Swarm Intelligence, doctoral dissertation 2017, Springer, ISBN: 978-3-658-20539-3, Heidelberg, 2018.
</p>

<hr>
<h2 id='DijkstraSSSP'>
Dijkstra SSSP
</h2><span id='topic+DijkstraSSSP'></span>

<h3>Description</h3>

<p>Dijkstra's SSSP (Single source shortest path) algorithm:
</p>
<p>gets the shortest path (geodesic distance) from source vertice(point) to all other vertices(points) defined by 
the edges of the adjasency matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DijkstraSSSP(Adj, Costs, source)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DijkstraSSSP_+3A_adj">Adj</code></td>
<td>

<p>[1:n,1:n]         0/1 adjascency matrix, e.g. from delaunay graph or gabriel graph
</p>
</td></tr>
<tr><td><code id="DijkstraSSSP_+3A_costs">Costs</code></td>
<td>

<p>[1:n,1:n]       matrix, distances between n points (normally euclidean)
</p>
</td></tr>
<tr><td><code id="DijkstraSSSP_+3A_source">source</code></td>
<td>

<p>int, vertice(point) from which to calculate the geodesic distance to all other points
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Preallocating space for DataStructures accordingly to the maximum possible number of vertices which is fixed set at 
the number 10001.
</p>


<h3>Value</h3>

<p>ShortestPaths[1:n]   vector, shortest paths (geodesic) to all other vertices including the source vertice itself
</p>


<h3>Note</h3>

<p>runs in O(E*Log(V))
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>uses a changed code which is inspired by Shreyans Sheth 28.05.2015, see 
<a href="https://ideone.com/qkmt31">https://ideone.com/qkmt31</a>
</p>

<hr>
<h2 id='Hepta'>
Hepta is part of the Fundamental Clustering Problem Suit (FCPS) [Thrun/Ultsch, 2020].
</h2><span id='topic+Hepta'></span>

<h3>Description</h3>

<p>clearly defined clusters, different variances
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Hepta")</code></pre>


<h3>Details</h3>

<p>Size 212, Dimensions 3, stored in <code>Hepta$Data</code>
</p>
<p>Classes 7, stored in <code>Hepta$Cls</code>
</p>


<h3>References</h3>

<p>[Thrun/Ultsch, 2020]	Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief,Vol. 30(C), pp. 105501, DOI 10.1016/j.dib.2020.105501 , 2020. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)
str(Hepta)
</code></pre>

<hr>
<h2 id='ICA'> Independent Component Analysis (ICA)</h2><span id='topic+ICA'></span>

<h3>Description</h3>

<p>Independent Component Analysis:
</p>
<p>Negentropie: difference of entropy to a corresponding normally-distributed random variable
J(y)=|E(G(y)-E(G(v)))|^2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICA(Data,OutputDimension=2,Contrastfunction="logcosh",

Alpha=1,Iterations=200,PlotIt=FALSE,Cls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ICA_+3A_data">Data</code></td>
<td>
<p>numerical matrix of n cases in rows, d variables in columns, matrix is not symmetric.</p>
</td></tr>
<tr><td><code id="ICA_+3A_outputdimension">OutputDimension</code></td>
<td>
<p>Number of dimensions in the Outputspace, default=2</p>
</td></tr>
<tr><td><code id="ICA_+3A_contrastfunction">Contrastfunction</code></td>
<td>

<p>Maximierung der Negentropie ueber geeignete geeignete Kontrastfunktion
Default: 'logcosh' G(u)=1/a*log cosh(a*u)
'exp': G(u)=-exp(u^2/2)
</p>
</td></tr>
<tr><td><code id="ICA_+3A_alpha">Alpha</code></td>
<td>

<p>onstant with 1&lt;=alpha&lt;=2 used in approximation to neg-entropy when fun == &quot;logcosh&quot;
</p>
</td></tr>
<tr><td><code id="ICA_+3A_iterations">Iterations</code></td>
<td>

<p>maximum number of iterations to perform.
</p>
</td></tr>
<tr><td><code id="ICA_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, If TRUE: Plots the projection as a 2d visualization. 
OutputDimension&gt;2: only the first two dimensions will be shown
</p>
</td></tr>
<tr><td><code id="ICA_+3A_cls">Cls</code></td>
<td>
<p>[1:n,1] Optional,: only relevant if PlotIt=TRUE. Numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An short overview of different types of projection methods can be found in [Thrun, 2018, p.42, Fig. 4.1] (<a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>ProjectedPoints</code></td>
<td>

<p>[1:n,OutputDimension], n by OutputDimension matrix containing coordinates of the Projectio
</p>
</td></tr>
<tr><td><code>Mixing</code></td>
<td>

<p>[1:OutputDimension,1:d]             Mischungsmatrix s.d gilt Data=MixingMatrix*ProjectedPoints      
</p>
</td></tr>
<tr><td><code>Unmixing</code></td>
<td>
<p>Entmischungsmatrix with Data*Unmixing=ProjectedPoints
</p>
</td></tr>
<tr><td><code>PCMatrix</code></td>
<td>

<p>pre-whitening matrix that projects data onto the first n.comp principal components.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>A wrapper for <code><a href="fastICA.html#topic+fastICA">fastICA</a></code>
</p>
<p>You can use the standard <code>ShepardScatterPlot</code> or the better approach through the <code>ShepardDensityPlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data

Proj=ICA(Data)

## Not run: 
PlotProjectedPoints(Proj$ProjectedPoints,Hepta$Cls)

## End(Not run)
</code></pre>

<hr>
<h2 id='interactiveClustering'>GUI for interactive cluster analysis</h2><span id='topic+interactiveClustering'></span>

<h3>Description</h3>

<p>This tool is an interactive shiny tool that visualizes a given generalized Umatrix and allows the user to select areas and mark them as clusters to improve a projection based clustering.</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="interactiveClustering_+3A_umatrix">Umatrix</code></td>
<td>
<p>[1:Lines,1:Columns] Matrix of Umatrix Heights</p>
</td></tr>
<tr><td><code id="interactiveClustering_+3A_bestmaches">Bestmaches</code></td>
<td>
<p>[1:n,1:2] Array with positions of Bestmatches</p>
</td></tr>
<tr><td><code id="interactiveClustering_+3A_cls">Cls</code></td>
<td>
<p>[1:n] Classification of the Bestmatches</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Clicking on &quot;Quit&quot; returns the Cls vector to the workspace.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>EnlargedUmatrix</code></td>
<td>
<p>[1:Lines,1:Columns] Matrix of Umatrix Heights taken four times and arranged in a square 2x2.</p>
</td></tr>
<tr><td><code>EnlargedBestmaches</code></td>
<td>
<p>[1:n,1:2] Array with positions of Bestmatches according to the enlarged umatrix.</p>
</td></tr>
<tr><td><code>EnlargedCls</code></td>
<td>
<p>[1:n] Classification of the Bestmatches according to the enlarged umatrix.</p>
</td></tr>
<tr><td><code>Umatrix</code></td>
<td>
<p>[1:Lines,1:Columns] Matrix of Umatrix Heights</p>
</td></tr>
<tr><td><code>Bestmaches</code></td>
<td>
<p>[1:n,1:2] Array with positions of Bestmatches</p>
</td></tr>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n] Classification of the Bestmatches</p>
</td></tr>
<tr><td><code>TopView_TopographicMap</code></td>
<td>
<p>Plot of a topographic map.</p>
</td></tr>
</table>


<h3>Note</h3>

<p> If you want to verifiy your clustering result externally, you can use <code>Heatmap</code> or <code>SilhouettePlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Florian Lerch, Michael Thrun</p>


<h3>References</h3>

<p>[Thrun/Ultsch, 2017] Thrun, M.C., Ultsch, A.: Projection based Clustering, Conf. Int. Federation of Classification Societies (IFCS),DOI:10.13140/RG.2.2.13124.53124, Tokyo, 2017.
</p>
<p>[Thrun, 2018]  Thrun, M. C.: Projection Based Clustering through Self-Organization and Swarm Intelligence, doctoral dissertation 2017, Springer, Heidelberg, ISBN: 978-3-658-20539-3, <a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>, 2018. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
#2d projection
# Visualizuation of GeneralizedUmatrix

projectionpoints=NeRV(Hepta$Data)
#Computation of Generalized Umatrix
library(GeneralizedUmatrix)
visualization=GeneralizedUmatrix(Data = Hepta$Data,projectionpoints)

## Semi-Automatic Clustering done interactivly in a  shiny gui
## Not run: 
Cls = interactiveClustering(visualization$Umatrix, visualization$Bestmatches)
##Plotting
plotTopographicMap(visualization$Umatrix,visualization$Bestmatches,Cls)

## End(Not run)
</code></pre>

<hr>
<h2 id='interactiveGeneralizedUmatrixIsland'>GUI for cutting out an Island.</h2><span id='topic+interactiveGeneralizedUmatrixIsland'></span>

<h3>Description</h3>

<p>The toroid Umatrix is usually drawn 4 times, so that connected areas on borders can be seen as a whole. An island is a manual cutout of such a tiled visualization, that is selected such that all connected areas stay intact. This shiny tool allows the user to do this manually.</p>


<h3>Usage</h3>

<pre><code class='language-R'>interactiveGeneralizedUmatrixIsland(Umatrix, Bestmatches=NULL, Cls=NULL, Plotter="plotly")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interactiveGeneralizedUmatrixIsland_+3A_umatrix">Umatrix</code></td>
<td>
<p>[1:Lines,1:Columns] Matrix of Umatrix Heights
</p>
</td></tr>
<tr><td><code id="interactiveGeneralizedUmatrixIsland_+3A_bestmatches">Bestmatches</code></td>
<td>
<p>[1:n, 1:2] Matrix with positions of Bestmatches for n
datapoints, first columns is the position in <code>Lines</code> and second column in
<code>Columns</code>
</p>
</td></tr>
<tr><td><code id="interactiveGeneralizedUmatrixIsland_+3A_cls">Cls</code></td>
<td>
<p>Classification of the Bestmatches
</p>
</td></tr>
<tr><td><code id="interactiveGeneralizedUmatrixIsland_+3A_plotter">Plotter</code></td>
<td>
<p>Choose between plotting frameworks: &quot;plotly&quot; and &quot;ggplot2&quot;
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Clicking on &quot;Quit&quot; returns the Imx matrix to the workspace. Details can bee read in [Thrun et al, 2016, Thrun/Ultsch, 2017].
</p>


<h3>Value</h3>

<p>Boolean Matrix that represents the island within the tiled Umatrix.</p>


<h3>Note</h3>

<p>This function is a depricated version of a function from the Umatrix packages created by Florian Lerch and Michael Thrun
</p>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>[Thrun, et al.,2016] 	Thrun, M. C., Lerch, F., Loetsch, J., Ultsch, A.: Visualization and 3D Printing of Multivariate Data of Biomarkers, in Skala, V. (Ed.), International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision,Plzen, 2016. 
</p>
<p>[Thrun/Ultsch, 2017]	Thrun, M.C., Ultsch, A.: Projection based Clustering, Conf. Int. Federation of Classification Societies (IFCS),&lt;DOI:10.13140/RG.2.2.13124.53124&gt;, Tokyo, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Hepta")
Data=Hepta$Data
Cls=Hepta$Cls
InputDistances=as.matrix(dist(Data))
res=cmdscale(d=InputDistances, k = 2, eig = TRUE, add = FALSE, x.ret = FALSE)
ProjectedPoints=as.matrix(res$points)
#see also ProjectionBasedClustering package for other common projection methods

library(GeneralizedUmatrix)
resUmatrix=GeneralizedUmatrix(Data,ProjectedPoints)
TopviewTopographicMap(resUmatrix$Umatrix,resUmatrix$Bestmatches,Cls)
#or in 3D if rgl package exists
#plotTopographicMap(resUmatrix$Umatrix,resUmatrix$Bestmatches,Cls)

##Interactive Island Generation 
## from a tiled Umatrix (toroidal assumption)

## Not run: 
	Imx = interactiveGeneralizedUmatrixIsland(resUmatrix$Umatrix,

	resUmatrix$Bestmatches)
	plotTopographicMap(resUmatrix$Umatrix,

	resUmatrix$Bestmatches, Imx = Imx)

## End(Not run)
</code></pre>

<hr>
<h2 id='interactiveProjectionBasedClustering'>
Interactive Projection-Based Clustering (IPBC)
</h2><span id='topic+interactiveProjectionBasedClustering'></span><span id='topic+IPBC'></span>

<h3>Description</h3>

<p>An interactive clustering tool published in [Thrun et al., 2020] that uses the topographic map visualizations of the generalized U-matrix and a variety of different projection methods. This function receives a dataset and starts a shiny interface where one is able to choose a projection method and generate a plot.ly visualization of the topograhpic map [Thrun et al., 2016] of the generalized U-matrix [Ultsch/Thrun, 2017] combined with projected points. It includes capabilities for interactive clustering within the interface as well as automatic projection-based clustering based on [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  interactiveProjectionBasedClustering(Data, Cls=NULL)
  
  IPBC(Data, Cls=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interactiveProjectionBasedClustering_+3A_data">Data</code></td>
<td>

<p>The dataset [1:n,1:d] of n cases and d vriables with which the U-matrix and the projection will be calculated. Please see also the note below.
</p>
</td></tr>
<tr><td><code id="interactiveProjectionBasedClustering_+3A_cls">Cls</code></td>
<td>

<p>Optional: Prior Classification of the data for the [1:n] cases of k classes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To cluster data interactively, i.e., select specific data points and create a cluster), first generate the visualization. Thereafter, switch in the menu to clustering, hold the left mouse button and then frame a valley. Simple mouse clicks will not start the lasso functionality of plotly. 
</p>
<p>The resulting clustering is stored in Cls which is a numerical vector of the length n (number of cases) with the integer elements of numbers from 1 to k if k is the number of groups in the data.
Each element of Cls as an unambigous mapping to a case of Data indicating by the rownames of Data. If Data has no rownames a vector from 1:n is generated and then Cls is named by it.
</p>


<h3>Value</h3>

<p>Returns a List of:
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n] numerical vector of the clustering of the dataset for then cases of k clusters</p>
</td></tr>
<tr><td><code>Umatrix</code></td>
<td>

<p>[1:Lines,1:Columns] generalized Umatrix to be plotted, numerical matrix storing the U-heights, see [Thrun, 2018] for definition.
</p>
</td></tr>
<tr><td><code>Bestmatches</code></td>
<td>

<p>[1:n,2]   Matrix of GridConverted Projected Points [1:n, 1:2] called Bestmatches that defines positions for n datapoints, first columns is the position in <code>Lines</code> and second column in <code>Columns</code>
</p>
</td></tr>
<tr><td><code>LastProjectionMethodUsed</code></td>
<td>
<p>name of last projection method that was used as a string</p>
</td></tr>
<tr><td><code>TopView_TopographicMap</code></td>
<td>
<p>The final plot generated by plot.ly when closing the tool</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Some dimensionality reduction methods will assume data without missing values, some other DR methods assume unique data points, i.e., no distance=0 for any two cases(rows) of data.
In these cases the IPBC method will crash.
</p>


<h3>Author(s)</h3>

<p>Tim Schreier, Felix Pape, Luis Winckelmann, Michael Thrun
</p>


<h3>References</h3>

<p>[Ultsch/Thrun, 2017]  Ultsch, A., &amp; Thrun, M. C.: Credible Visualizations for Planar Projections, in Cottrell, M. (Ed.), 12th International Workshop on Self-Organizing Maps and Learning Vector Quantization, Clustering and Data Visualization (WSOM), IEEE Xplore, France, 2017.
</p>
<p>[Thrun/Ultsch, 2017]   Thrun, M. C., &amp; Ultsch, A. : Projection based Clustering, Proc. International Federation of Classification Societies (IFCS), pp. 250-251, Japanese Classification Society (JCS), Tokyo, Japan, 2017. 
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data, Journal of Classification, Springer, DOI: 10.1007/s00357-020-09373-2, 2020. 
</p>
<p>[Thrun et al., 2020]  Thrun, M. C., Pape, F., &amp; Ultsch, A.: Interactive Machine Learning Tool for Clustering in Visual Analytics, 7th IEEE International Conference on Data Science and Advanced Analytics (DSAA 2020), pp. 672-680, DOI 10.1109/DSAA49011.2020.00062, IEEE, Sydney, Australia, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive()){
  data('Hepta')
  Data=Hepta$Data

  V=interactiveProjectionBasedClustering(Data)

  # with prior classification
  Cls=Hepta$Cls
  V=IPBC(Data,Cls)
}
</code></pre>

<hr>
<h2 id='Isomap'>Isomap</h2><span id='topic+Isomap'></span>

<h3>Description</h3>

<p>Isomap procetion as introduced in 2000 by Tenenbaum, de Silva and Langford
</p>
<p>Even with a manifold structure, the sampling must be even and dense so that dissimilarities along a manifold are shorter than across the folds. If data do not have such a manifold structure, the results are very sensitive to parameter values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Isomap(Distances,k,OutputDimension=2,PlotIt=FALSE,Cls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Isomap_+3A_distances">Distances</code></td>
<td>
<p>Symmetric [1:n,1:n] distance matrix, e.g. <code>as.matrix(dist(Data,method))</code></p>
</td></tr>
<tr><td><code id="Isomap_+3A_k">k</code></td>
<td>
<p>number of k nearest neighbors, if the data is fragmented choose an higher k</p>
</td></tr>
<tr><td><code id="Isomap_+3A_outputdimension">OutputDimension</code></td>
<td>
<p>Number of dimensions in the output space, default = 2</p>
</td></tr>
<tr><td><code id="Isomap_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, If TRUE: Plots the projection as a 2d visualization. 
If OutputDimension &gt; 2 only the first two dimensions will be shown.
</p>
</td></tr>
<tr><td><code id="Isomap_+3A_cls">Cls</code></td>
<td>
<p>Optional and only relevant if PlotIt=TRUE. Numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An short overview of different types of projection methods can be found in [Thrun, 2018, p.42, Fig. 4.1] (<a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>).
</p>


<h3>Value</h3>

<p>ProjectedPoints[1:n,OutputDimension]   n by OutputDimension matrix containing coordinates of the Projection: A matrix of the fitted configuration..</p>


<h3>Note</h3>

<p>A wrapper enabling a planar projection of the manifold learning method based on the isomap of the package vegan
</p>
<p>if Data fragmented choose an higher k
</p>
<p>You can use the standard <code>ShepardScatterPlot</code> or the better approach through the <code>ShepardDensityPlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data

Proj=Isomap(as.matrix(dist(Data)),k=7)

## Not run: 
PlotProjectedPoints(Proj$ProjectedPoints,Hepta$Cls)

## End(Not run)
</code></pre>

<hr>
<h2 id='KLMeasure'>
Rank-based smoothed precision/recall measure for projection.
</h2><span id='topic+KLMeasure'></span>

<h3>Description</h3>

<p>Computes rank-based smoothed precision/recall, with cost function based on Kullback-Leibler-divergence (see [Venna2010]).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KLMeasure(Data, pData, NeighborhoodSize = 20L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KLMeasure_+3A_data">Data</code></td>
<td>

<p>numerical matrix of data: n cases in rows, d variables in columns
</p>
</td></tr>
<tr><td><code id="KLMeasure_+3A_pdata">pData</code></td>
<td>

<p>numerical matrix of projected data: n cases in rows, k variables in columns, where k is the projection output dimension
</p>
</td></tr>
<tr><td><code id="KLMeasure_+3A_neighborhoodsize">NeighborhoodSize</code></td>
<td>

<p>Number of points in neighborhood to be considered. Default is 20
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>SmoothedPrecision</code></td>
<td>

<p>Scalar, smoothed precision value
</p>
</td></tr>
<tr><td><code>SmoothedRecall</code></td>
<td>

<p>Scalar, smoothed recall value
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>C++ source code comes from <a href="https://research.cs.aalto.fi/pml/software/dredviz/">https://research.cs.aalto.fi/pml/software/dredviz/</a>
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Venna2010]: Jarkko Venna, Jaakko Peltonen, Kristian Nybo, Helena Aidos, and Samuel Kaski. Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization. Journal of Machine Learning Research, 11:451-490, 2010.
</p>


<h3>See Also</h3>

<p>An alternative measure is the <a href="#topic+ContTrustMeasure">ContTrustMeasure</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data
res=MDS(Data)
Proj = res$ProjectedPoints

kl_m = KLMeasure(Hepta$Data, Proj)
# Smoothed precision
print(kl_m[[1]])
# Smoothed recall
print(kl_m[[2]])

</code></pre>

<hr>
<h2 id='KruskalStress'>
Kruskal stress calculation
</h2><span id='topic+KruskalStress'></span>

<h3>Description</h3>

<p>Calculates the stress as defined by Kruskal for 2 distance matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KruskalStress(InputDistances, OutputDistances)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KruskalStress_+3A_inputdistances">InputDistances</code></td>
<td>

<p>Distance matrix of the original Data
</p>
</td></tr>
<tr><td><code id="KruskalStress_+3A_outputdistances">OutputDistances</code></td>
<td>

<p>Distance matrix of the projected Data
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An short overview of different types of quality measures can be found in [Thrun, 2018, p.68, Fig. 6.3] (<a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>).
</p>


<h3>Value</h3>

<p>A single numerical representing the Kruskal stress of the distance matrices.
</p>


<h3>Author(s)</h3>

<p>Felix Pape
</p>

<hr>
<h2 id='MDS'>Multidimensional Scaling (MDS)</h2><span id='topic+MDS'></span>

<h3>Description</h3>

<p>Classical multidimensional scaling of a data matrix. Also known as principal coordinates analysis</p>


<h3>Usage</h3>

<pre><code class='language-R'>MDS(DataOrDistances,method='euclidean',OutputDimension=2,PlotIt=FALSE,Cls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MDS_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Numerical matrix defined as either 
</p>
<p><code>Data</code>, i.e., [1:n,1:d], nonsymmetric, and consists of n cases of d-dimensional data points with every case having d attributes, variables or features,
</p>
<p>or
</p>
<p><code>Distances</code>, i.e.,[1:n,1:n], symmetric and consists of n cases, e.g., <code>as.matrix(dist(Data,method))</code>
</p>
</td></tr>
<tr><td><code id="MDS_+3A_method">method</code></td>
<td>
<p>method specified by distance string: 'euclidean','cityblock=manhatten','cosine','chebychev','jaccard','minkowski','manhattan','binary'</p>
</td></tr>
<tr><td><code id="MDS_+3A_outputdimension">OutputDimension</code></td>
<td>
<p>Number of dimensions in the Outputspace, default=2</p>
</td></tr>
<tr><td><code id="MDS_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, If TRUE: Plots the projection as a 2d visualization. 
</p>
</td></tr>
<tr><td><code id="MDS_+3A_cls">Cls</code></td>
<td>
<p>[1:n,1] Optional,: only relevant if PlotIt=TRUE. Numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An short overview of different types of projection methods can be found in [Thrun, 2018, p.42, Fig. 4.1] (<a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>ProjectedPoints</code></td>
<td>

<p>[1:n,OutputDimension], n by OutputDimension matrix containing coordinates of the Projection
</p>
</td></tr>
<tr><td><code>Eigenvalues</code></td>
<td>

<p>the eigenvalues of MDSvalues*MDSvalues'
</p>
</td></tr>
<tr><td><code>Stress</code></td>
<td>

<p>Shephard-Kruskal Stress  
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>A wrapper for <code>cmdscale</code>
</p>
<p>You can use the standard <code>ShepardScatterPlot</code> or the better approach through the <code>ShepardDensityPlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data

Proj=MDS(Data)

## Not run: 
PlotProjectedPoints(Proj$ProjectedPoints,Hepta$Cls)

## End(Not run)
</code></pre>

<hr>
<h2 id='NeRV'>
Neighbor Retrieval Visualizer (NeRV) 
</h2><span id='topic+NeRV'></span>

<h3>Description</h3>

<p>Projection is done by the neighbor
retrieval visualizer (NeRV) 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NeRV(Data, lambda = 0.1, neighbors = 20, iterations = 10, 

cg_steps = 2, cg_steps_final = 40, randominit = T, OutputDimension = 2,

PlotIt = FALSE, Cls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NeRV_+3A_data">Data</code></td>
<td>
<p>Numerical matrix of the Data to be projected, [1:n,1:d], nonsymmetric, and consists of n cases of d-dimensional data points with every case having d attributes, variables or features</p>
</td></tr>
<tr><td><code id="NeRV_+3A_lambda">lambda</code></td>
<td>
<p>Optional: Controls the trustworthiness-continuity tradeoff. Default = 0.1</p>
</td></tr>
<tr><td><code id="NeRV_+3A_neighbors">neighbors</code></td>
<td>
<p>Optional: Set the number of nearest neighbours that each point should have. Should be positive. Default = 20</p>
</td></tr>
<tr><td><code id="NeRV_+3A_iterations">iterations</code></td>
<td>
<p>Optional: The number of iterations to perform. Default = 10</p>
</td></tr>
<tr><td><code id="NeRV_+3A_cg_steps">cg_steps</code></td>
<td>
<p>Optional: The number of conjugate gradient steps to perform per iteration in NeRV's optimization scheme. Default = 2</p>
</td></tr>
<tr><td><code id="NeRV_+3A_cg_steps_final">cg_steps_final</code></td>
<td>
<p>Optional: The number of conjugate gradient steps to perform on the final iteration in NeRV's optimization scheme. Default = 40</p>
</td></tr>
<tr><td><code id="NeRV_+3A_randominit">randominit</code></td>
<td>
<p>Optional: TRUE: Random Initialization (default), FALSE: PCA initializiation</p>
</td></tr>
<tr><td><code id="NeRV_+3A_outputdimension">OutputDimension</code></td>
<td>
<p>Optional: Number of dimensions in the Outputspace, default=2</p>
</td></tr>
<tr><td><code id="NeRV_+3A_plotit">PlotIt</code></td>
<td>
<p>Optional: Should the projected points be plotted? Default: FALSE. Note: this is only usefull if OutputDimension = 2.</p>
</td></tr>
<tr><td><code id="NeRV_+3A_cls">Cls</code></td>
<td>
<p>Optional: Vector containing the number of the class for each row in Data. This is only used to color the points according to their classes if PlotIt = T</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the NeRV projection with matrix Data and lambda. Lambda controls the trustworthiness-continuity tradeoff.
</p>
<p>An short overview of different types of projection methods can be found in [Thrun, 2018, p.42, Fig. 4.1] (<a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>).
</p>


<h3>Value</h3>

<p>OutputDimension-dimensional matrix of projected points
</p>


<h3>Note</h3>

<p>PCA initialization changes form the original C++ Sourcecode of <a href="https://research.cs.aalto.fi/pml/software/dredviz/">https://research.cs.aalto.fi/pml/software/dredviz/</a> to the R version of the projections package.
Other changes are made only regarding data types of Rcpp in comparison to the original C++ Source code.
</p>
<p>You can use the standard <code>ShepardScatterPlot</code> or the better approach through the <code>ShepardDensityPlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun, Felix Pape
</p>


<h3>References</h3>

<p>Jarkko Venna, Jaakko Peltonen, Kristian Nybo, Helena Aidos, and Samuel Kaski. Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization. Journal of Machine Learning Research, 11:451-490, 2010.
</p>
<p>Jarkko Venna and Samuel Kaski. Nonlinear Dimensionality Reduction as Information Retrieval. In Marina Meila and Xiaotong Shen, editors, Proceedings of AISTATS 2007, the 11th International Conference on Artificial Intelligence and Statistics. Omnipress, 2007. JMLR Workshop and Conference Proceedings, Volume 2: AISTATS 2007.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data
## Not run: 
Proj=NeRV(Data)
PlotProjectedPoints(Proj$ProjectedPoints,Hepta$Cls)

## End(Not run)


</code></pre>

<hr>
<h2 id='PCA'>Principal Component Analysis (PCA)</h2><span id='topic+PCA'></span>

<h3>Description</h3>

<p>Performs a principal components analysis on the given data matrix projection=SammonsMapping(Data)</p>


<h3>Usage</h3>

<pre><code class='language-R'>PCA(Data,OutputDimension=2,Scale=FALSE,Center=FALSE,PlotIt=FALSE,Cls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PCA_+3A_data">Data</code></td>
<td>
<p>numerical matrix of data: n cases in rows, d variables in columns</p>
</td></tr>
<tr><td><code id="PCA_+3A_outputdimension">OutputDimension</code></td>
<td>
<p>Number of dimensions in the Outputspace, default=2</p>
</td></tr>
<tr><td><code id="PCA_+3A_scale">Scale</code></td>
<td>

<p>a logical value indicating whether the variables should be scaled to have unit variance before the analysis takes place. The default is FALSE for consistency with S, but in general scaling is advisable. Alternatively, a vector of length equal the number of columns of x can be supplied. The value is passed to scale.</p>
</td></tr>
<tr><td><code id="PCA_+3A_center">Center</code></td>
<td>

<p>a logical value indicating whether the variables should be shifted to be zero centered. Alternately, a vector of length equal the number of columns of x can be supplied. The value is passed to scale
</p>
</td></tr>
<tr><td><code id="PCA_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, If TRUE: Plots the projection as a 2d visualization. 
OutputDimension&gt;2: only the first two dimensions will be shown
</p>
</td></tr>
<tr><td><code id="PCA_+3A_cls">Cls</code></td>
<td>
<p>[1:n,1] Optional,: only relevant if PlotIt=TRUE. Numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An short overview of different types of projection methods can be found in [Thrun, 2018, p.42, Fig. 4.1] (<a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>ProjectedPoints</code></td>
<td>

<p>[1:n,OutputDimension], n by OutputDimension matrix containing coordinates of the Projectio
</p>
</td></tr>
<tr><td><code>Rotation</code></td>
<td>

<p>the matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors)
</p>
</td></tr>
<tr><td><code>sDev</code></td>
<td>

<p>the standard deviations of the principal components (i.e., the square roots of 
the eigenvalues of the covariance/correlation matrix, 
though the calculation is actually done with the singular values of the data matrix)
</p>
</td></tr>
<tr><td><code>TransformedData</code></td>
<td>

<p>matrix  with PCA transformed Data
</p>
</td></tr>
<tr><td><code>Center</code></td>
<td>

<p>the centering used, or FALSE
</p>
</td></tr>
<tr><td><code>Scale</code></td>
<td>

<p>the scaling used, or FALSE
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>A wrapper for <code><a href="stats.html#topic+prcomp">prcomp</a></code>
</p>
<p>You can use the standard <code>ShepardScatterPlot</code> or the better approach through the <code>ShepardDensityPlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data

Proj=PCA(Data)

## Not run: 
PlotProjectedPoints(Proj$ProjectedPoints,Hepta$Cls)

## End(Not run)
</code></pre>

<hr>
<h2 id='PlotProjectedPoints'>
Plot Projected Points
</h2><span id='topic+PlotProjectedPoints'></span>

<h3>Description</h3>

<p>plots XY data colored by Cls with ggplot2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotProjectedPoints(Points,Cls,BMUorProjected=F,PlotLegend=FALSE,

xlab='X',ylab='Y',main="Projected Points",PointSize=2.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotProjectedPoints_+3A_points">Points</code></td>
<td>

<p>[1:n,1:2] xy cartesian coordinates of a projection
</p>
</td></tr>
<tr><td><code id="PlotProjectedPoints_+3A_cls">Cls</code></td>
<td>
<p> numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
<tr><td><code id="PlotProjectedPoints_+3A_bmuorprojected">BMUorProjected</code></td>
<td>

<p>Default ==FALSE, If TRUE assuming BestMatches of ESOM instead of Projected Points
</p>
</td></tr>
<tr><td><code id="PlotProjectedPoints_+3A_plotlegend">PlotLegend</code></td>
<td>

<p>...
</p>
</td></tr>
<tr><td><code id="PlotProjectedPoints_+3A_xlab">xlab</code></td>
<td>

<p>Optional: Label of the x axis
</p>
</td></tr>
<tr><td><code id="PlotProjectedPoints_+3A_ylab">ylab</code></td>
<td>

<p>Optional: Label of the y axis
</p>
</td></tr>
<tr><td><code id="PlotProjectedPoints_+3A_main">main</code></td>
<td>

<p>Optional: title
</p>
</td></tr>
<tr><td><code id="PlotProjectedPoints_+3A_pointsize">PointSize</code></td>
<td>

<p>Optional: size of  points
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggobject of ggplot2
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>

<hr>
<h2 id='PolarSwarm'>
Polar Swarm (Pswarm)
</h2><span id='topic+PolarSwarm'></span>

<h3>Description</h3>

<p>Swarm-based Projection method using game theory published in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PolarSwarm(DataOrDistances, method = "euclidean", PlotIt = FALSE, Cls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PolarSwarm_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Numerical matrix defined as either 
</p>
<p><code>Data</code>, i.e., [1:n,1:d], nonsymmetric, and consists of n cases of d-dimensional data points with every case having d attributes, variables or features,
</p>
<p>or
</p>
<p><code>Distances</code>, i.e.,[1:n,1:n], symmetric and consists of n cases, e.g., <code>as.matrix(parallelDist::parallelDist(Data,method))</code>
</p>
</td></tr>
<tr><td><code id="PolarSwarm_+3A_method">method</code></td>
<td>

<p>If <code>Data</code> is given the method to computing the distances can be specified here.
Please see the documentation of package <span class="pkg">parallelDist</span> for the types that are possible.
</p>
</td></tr>
<tr><td><code id="PolarSwarm_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, If TRUE: Plots the projection as a 2d visualization. 
OutputDimension&gt;2: only the first two dimensions will be shown
</p>
</td></tr>
<tr><td><code id="PolarSwarm_+3A_cls">Cls</code></td>
<td>

<p>Optional,: only relevant if PlotIt=TRUE. Numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By exploting swarm intelligence and game theory no parameter have to be set.
</p>


<h3>Value</h3>

<p>List of 
</p>
<table>
<tr><td><code>ProjectedPoints</code></td>
<td>
<p>[1:n,2], n by 2 matrix containing coordinates of the Projection</p>
</td></tr>
<tr><td><code>ModelObject</code></td>
<td>
<p>output of  <code><a href="DatabionicSwarm.html#topic+Pswarm">Pswarm</a></code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Swarm Intelligence for Self-Organized Clustering, Artificial intelligence, Vol. 290, pp. 103237, doi 10.1016/j.artint.2020.103237, 2020.
</p>


<h3>See Also</h3>

<p><code><a href="DatabionicSwarm.html#topic+Pswarm">Pswarm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data

Distances=as.matrix(dist(Data))
Proj=PolarSwarm(Data)
## Not run: 
PlotProjectedPoints(Proj$ProjectedPoints,Hepta$Cls)
## End(Not run)

</code></pre>

<hr>
<h2 id='Projection2Bestmatches'>
Projection to Bestmatches
</h2><span id='topic+Projection2Bestmatches'></span>

<h3>Description</h3>

<p>Transformation of projected points to bestmatches defined by generalized Umatrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Projection2Bestmatches(ProjectedPoints)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Projection2Bestmatches_+3A_projectedpoints">ProjectedPoints</code></td>
<td>

<p>[1:n,1:2] n projected points in two-dimensional space.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that an unambiguous assignment between projected points and data points is given. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>Bestmatches</code></td>
<td>

<p>[1:n,1:2]   Positions of GridConverted Projected Points, which can be used for the generalized Umatrix, to the predefined Grid by Lines and Columns. First Columns has the content of the Line No and second Column of the Column number.
</p>
</td></tr>
<tr><td><code>LC</code></td>
<td>
<p>[1:2] vector if Line No. and ColumnNo. which defines the size of the grid of the generalized Umatrix</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Details of the equations used are written down in [Thrun, 2018, p. 47].
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun, 2018]  Thrun, M. C.: Projection Based Clustering through Self-Organization and Swarm Intelligence, doctoral dissertation 2017, Springer, Heidelberg, ISBN: 978-3-658-20539-3, <a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>, 2018. 
</p>


<h3>See Also</h3>

<p><code><a href="GeneralizedUmatrix.html#topic+XYcoords2LinesColumns">XYcoords2LinesColumns</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
ProjList=MDS(Hepta$Data)
trafo=Projection2Bestmatches(ProjList$ProjectedPoints)
</code></pre>

<hr>
<h2 id='ProjectionBasedClustering'>
Automatic Projection-based Clustering (PBC) [Thrun/Ultsch, 2020]
</h2><span id='topic+ProjectionBasedClustering'></span>

<h3>Description</h3>

<p>Three steps are necessary for PBC. First, a projection method has to be chosen to generate projected points of high-dimensional data points. Second, the generalized U*-matrix has to be applied to the projected points by using a simplified emergent self-organizing map (ESOM) algorithm which is an unsupervised neural network [Thrun, 2018]. The resulting generalized U-matrix can be visualized by the topographic map [Thrun et al., 2016]. Third, the clustering itself is built on top of the generalized U-matrix using the concept of the abstract U-Matrix and shortest graph paths using <code>ShortestGraphPathsC</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ProjectionBasedClustering(k, DataOrDistances, BestMatches, LC,

StructureType = TRUE, PlotIt = FALSE, method = "euclidean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ProjectionBasedClustering_+3A_k">k</code></td>
<td>
<p>number of clusters, how many to you see in the 3d landscape?</p>
</td></tr>
<tr><td><code id="ProjectionBasedClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>Numerical matrix that will be used for clustering with one DataPoint per row, defined as either as
</p>
<p><code>Data</code>, i.e., [1:n,1:d], nonsymmetric, and consists of n cases of d-dimensional data points with every case having d attributes, variables or features,
</p>
<p>or
</p>
<p><code>Distances</code>, i.e.,[1:n,1:n], symmetric and consists of n cases, e.g., <code>as.matrix(dist(Data,method))</code>
</p>
</td></tr>
<tr><td><code id="ProjectionBasedClustering_+3A_bestmatches">BestMatches</code></td>
<td>
<p>[1:n,1:2] Matrix with positions of Bestmatches=ProjectedPoints, one matrix line per data point</p>
</td></tr>
<tr><td><code id="ProjectionBasedClustering_+3A_lc">LC</code></td>
<td>
<p>grid size c(Lines,Columns)</p>
</td></tr>
<tr><td><code id="ProjectionBasedClustering_+3A_structuretype">StructureType</code></td>
<td>
<p>Optional, bool; =TRUE: compact structure of clusters assumed, =FALSE: connected structure of clusters assumed. For the two options vor Clusters, see [Thrun, 2017] or Handl et al. 2006</p>
</td></tr>
<tr><td><code id="ProjectionBasedClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Optional, bool, Plots Dendrogramm</p>
</td></tr>
<tr><td><code id="ProjectionBasedClustering_+3A_method">method</code></td>
<td>
<p>Optional, distance method used in <span class="pkg">parallelDist</span> if <code>Data</code> given.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>ProjectionBasedClustering is a flexible and robust clustering framework based on a chose projection method and
projection method a parameter-free high-dimensional data visualization technique. The visualization combines projected points with a topographic map with hypsometric colors, defined by the generalized U-matrix  (see package GeneralizedUmatrix function GeneralizedUmatrix). 
</p>
<p>The clustering method with no sensitive parameters is done in this function and the algorithm is introduced in detail in [Thrun/Ultsch, 2020]. The clustering can be verified by the visualization and vice versa. If you want to verifiy your clustering result externally, you can use <code>Heatmap</code> or <code>SilhouettePlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>
<p>If <span class="pkg">parallelDist</span> is not installed, function automatically falls back to <code><a href="stats.html#topic+dist">dist</a></code>.
</p>


<h3>Value</h3>

<p>Cls                 [1:n] vector with selected classes of the bestmatches. You can use <code>plotTopographicMap(Umatrix,Bestmatches,Cls)</code> for verification.
</p>


<h3>Note</h3>

<p>Often it is better to mark the outliers  manually after the prozess of clustering; use in this case the visualization <code>plotTopographicMap</code> of the package GeneralizedUmatrix. If you would like to mark the outliers interactivly in the visualization use 
the <code>interactiveClustering</code> function.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun et al., 2016]  Thrun, M. C., Lerch, F., Lötsch, J., &amp; Ultsch, A.: Visualization and 3D Printing of Multivariate Data of Biomarkers, in Skala, V. (Ed.), International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG), Vol. 24, Plzen, http://wscg.zcu.cz/wscg2016/short/A43-full.pdf, 2016.
</p>
<p>[Thrun/Ultsch, 2017] Thrun, M.C., Ultsch, A.: Projection based Clustering, Conf. Int. Federation of Classification Societies (IFCS),DOI:10.13140/RG.2.2.13124.53124, Tokyo, 2017.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data, Journal of Classification, Vol. 38(2), pp. 280-312, Springer, DOI: 10.1007/s00357-020-09373-2, 2020. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
#Step I: 2d projection
projectionpoints=NeRV(Hepta$Data)

#Step II (Optional): Computation of Generalized Umatrix
library(GeneralizedUmatrix)
visualization=GeneralizedUmatrix(Data = Hepta$Data,projectionpoints)
# Visualizuation of GeneralizedUmatrix
library(GeneralizedUmatrix)
TopviewTopographicMap(visualization$Umatrix,visualization$Bestmatches)
#or in 3D if rgl package exists
#plotTopographicMap(visualization$Umatrix,visualization$Bestmatches)

# Step III: Automatic Clustering
trafo=Projection2Bestmatches(projectionpoints)
# number of Cluster from dendrogram  (PlotIt=T) or visualization above
Cls=ProjectionBasedClustering(k=7, Hepta$Data, 

trafo$Bestmatches, trafo$LC,PlotIt=TRUE)

# Verification of Clustering
TopviewTopographicMap(visualization$Umatrix,visualization$Bestmatches,Cls)
#or in 3D if rgl package exists
#plotTopographicMap(visualization$Umatrix,visualization$Bestmatches,Cls)

</code></pre>

<hr>
<h2 id='ProjectionBasedClustering-package'>
Projection Based Clustering
</h2><span id='topic+ProjectionBasedClustering-package'></span>

<h3>Description</h3>

<p>The package is based on a conference talk [Thrun/Ultsch, 2017], see &lt;DOI:10.13140/RG.2.2.13124.53124&gt;. and [Thrun/Ultsch, 2020]. The abstract of the conference talk is as follows:
</p>
<p>Many data mining methods rely on some concept of the dissimilarity between pieces of information encoded in the data of interest. These methods can be used for cluster analysis. However, no generally accepted definition of clusters exists in the literature [Hennig et al., 2015]. Here, consistent with Bouveyron et al., it is assumed that a cluster is a group of similar objects [Bouveyron et al., 2012]. The clusters are called natural because they do not require a dissection; instead, they are clearly separated in the data [Duda et al., 2001,  Theodoridis/Koutroumbas, 2009,  pp. 579, 600]. These clusters can be identified by distance or density based high-dimensional structures. 
Dimensionality reduction techniques are able to reduce the dimensions of the input space to facilitate the exploration of structures in high-dimensional data. If they are used for visualization, they are called projection methods. The generalized U*-matrix technique is applicable for these and can be used to visualize both distance- and density-based structures [Thrun 2018; Ultsch/Thrun, 2017]. The idea that the abstract U*-matrix (AU-matrix) can be used for clustering [Ultsch et al., 2016]. The distances required for hierarchical clustering are defined by the AU-matrix [Lötsch/Ultsch, 2014]. Using this distance we propose a clustering approach for every projection method based on the U*-matrix visualization of a topographic map [Thrun 2018; Thrun/Ultsch, 2017]. The number of clusters and the cluster structure can be estimated by counting the valleys in a topographic map  [Thrun et al., 2016]. If the number of clusters and the clustering method are chosen correctly, then the clusters will be well separated by mountains in the visualization. Outliers are represented as volcanoes and can be interactively marked in the visualization after the automated clustering process.
</p>
<p>Furthermore, [Thrun et al., 2021] presents an interactive parameter-free approach, that incorporates a human-in-the-loop, for projection-based clustering.
</p>


<h3>Details</h3>

<p>A comparison to 32 common clustering algorithms is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Note</h3>

<p>If you want to verifiy your clustering result externally, you can use <code>Heatmap</code> or <code>SilhouettePlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>
<p>Additionally you can use the standard <code>ShepardScatterPlot</code> or the better approach through the <code>ShepardDensityPlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun, Felix Pape, Florian Lerch, Tim Schreier, Luis Winckelmann
</p>


<h3>References</h3>

<p>[Thrun/Ultsch, 2017] Thrun, M.C., Ultsch, A.: Projection based Clustering, Conf. Int. Federation of Classification Societies (IFCS),DOI:10.13140/RG.2.2.13124.53124, Tokyo, 2017.
</p>
<p>[Bouveyron et al., 2012]  Bouveyron, C., Hammer, B., &amp; Villmann, T.: Recent developments in clustering algorithms, Proc. ESANN, Citeseer, 2012.
</p>
<p>[Duda et al., 2001]  Duda, R. O., Hart, P. E., &amp; Stork, D. G.: Pattern classification, (Second Edition ed.), Ney York, USA, John Wiley &amp; Sons, ISBN: 0-471-05669-3, 2001.
</p>
<p>[Hennig et al., 2015]  Hennig, C., Meila, M., Murtagh, F., &amp; Rocci, R.: Handbook of cluster analysis, New York, USA, CRC Press, ISBN: 9781466551893, 2015.
</p>
<p>[Lötsch/Ultsch, 2014]  Lötsch, J., &amp; Ultsch, A.: Exploiting the Structures of the U-Matrix, in Villmann, T., Schleif, F.-M., Kaden, M. &amp; Lange, M. (eds.), Proc. Advances in Self-Organizing Maps and Learning Vector Quantization, pp. 249-257, Springer International Publishing, Mittweida, Germany, 2014.
</p>
<p>[Theodoridis/Koutroumbas, 2009]  Theodoridis, S., &amp; Koutroumbas, K.: Pattern Recognition, (Fourth Edition ed.), Canada, Elsevier, ISBN: 978-1-59749-272-0, 2009.
</p>
<p>[Thrun et al., 2016]  Thrun, M. C., Lerch, F., Lötsch, J., &amp; Ultsch, A.: Visualization and 3D Printing of Multivariate Data of Biomarkers, in Skala, V. (Ed.), International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG), Vol. 24, Plzen, http://wscg.zcu.cz/wscg2016/short/A43-full.pdf, 2016.
</p>
<p>[Ultsch et al., 2016]  Ultsch, A., Behnisch, M., &amp; Lötsch, J.: ESOM Visualizations for Quality Assessment in Clustering, In Merényi, E., Mendenhall, J. M. &amp; O'Driscoll, P. (Eds.), Advances in Self-Organizing Maps and Learning Vector Quantization: Proceedings of the 11th International Workshop WSOM 2016, Houston, Texas, USA, January 6-8, 2016, (10.1007/978-3-319-28518-4_3pp. 39-48), Cham, Springer International Publishing, 2016.
</p>
<p>[Ultsch/Thrun, 2017]  Ultsch, A., &amp; Thrun, M. C.: Credible Visualizations for Planar Projections, in Cottrell, M. (Ed.), 12th International Workshop on Self-Organizing Maps and Learning Vector Quantization, Clustering and Data Visualization (WSOM), IEEE Xplore, France, 2017.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data, Journal of Classification, Vol. 38(2), pp. 280-312, Springer, DOI: 10.1007/s00357-020-09373-2, 2020.
</p>
<p>[Thrun et al., 2021] Thrun, M. C., Pape, F. &amp; Ultsch, A.: Conventional displays of structures in data compared with interactive
projection‑based clustering (IPBC), International Journal of Data Science and Analyitics, Vol. 12(3), pp. 249-271, Springer, DOI: 10.1007/s41060-021-00264-2, 2021
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
#2d projection
# Visualizuation of GeneralizedUmatrix

projectionpoints=NeRV(Hepta$Data)
#Computation of Generalized Umatrix
library(GeneralizedUmatrix)
visualization=GeneralizedUmatrix(Data = Hepta$Data,projectionpoints)
TopviewTopographicMap(visualization$Umatrix,visualization$Bestmatches)
#or in 3D if rgl package exists
#plotTopographicMap(visualization$Umatrix,visualization$Bestmatches)

##Interactive Island Generation 
## from a tiled Umatrix (toroidal assumption)
## Not run: 
	Imx = ProjectionBasedClustering::interactiveGeneralizedUmatrixIsland(visualization$Umatrix,
	visualization$Bestmatches)
	#plotTopographicMap(visualization$Umatrix,visualization$Bestmatches, Imx = Imx)

## End(Not run)

# Automatic Clustering
LC=c(visualization$Lines,visualization$Columns)
# number of Cluster from dendrogram or visualization (PlotIt=TRUE)
Cls=ProjectionBasedClustering(k=7, Hepta$Data, 

visualization$Bestmatches, LC,PlotIt=TRUE)
# Verification
library(GeneralizedUmatrix)
TopviewTopographicMap(visualization$Umatrix,visualization$Bestmatches,Cls)
#or in 3D if rgl package exists
#plotTopographicMap(visualization$Umatrix,visualization$Bestmatches,Cls)

## Sometimes you can improve a Clustering interactivly or mark additional Outliers manually
## Not run: 
Cls2 = interactiveClustering(visualization$Umatrix, visualization$Bestmatches, Cls)

## End(Not run)
</code></pre>

<hr>
<h2 id='ProjectionPursuit'> Projection Pursuit</h2><span id='topic+ProjectionPursuit'></span>

<h3>Description</h3>

<p>In the absence of a generative model for the data the algorithm can be used to find the projection pursuit directions. Projection pursuit is a technique for finding 'interesting' directions in multidimensional  datasets
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ProjectionPursuit(Data,OutputDimension=2,Indexfunction="logcosh",

Alpha=1,Iterations=200,PlotIt=FALSE,Cls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ProjectionPursuit_+3A_data">Data</code></td>
<td>
<p>array of data: n cases in rows, d variables in columns, matrix is not symmetric or distance matrix, in this case matrix has to be symmetric</p>
</td></tr>
<tr><td><code id="ProjectionPursuit_+3A_outputdimension">OutputDimension</code></td>
<td>
<p>Number of dimensions in the Outputspace, default=2</p>
</td></tr>
<tr><td><code id="ProjectionPursuit_+3A_indexfunction">Indexfunction</code></td>
<td>

<p>Criterium for Minimization:
</p>
<p>default: 'logcosh' G(u)=1/a*log cosh(a*u) (ICA)
'exp': G(u)=-exp(u^2/2)
'kernel'  1/(1* pi )*exp(r/2)
</p>
</td></tr>
<tr><td><code id="ProjectionPursuit_+3A_alpha">Alpha</code></td>
<td>

<p>constant with 1&lt;=alpha&lt;=2 used in approximation to neg-entropy when fun == &quot;logcosh&quot;
</p>
</td></tr>
<tr><td><code id="ProjectionPursuit_+3A_iterations">Iterations</code></td>
<td>

<p>maximum number of iterations to perform.
</p>
</td></tr>
<tr><td><code id="ProjectionPursuit_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, If TRUE: Plots the projection as a 2d visualization. 
OutputDimension&gt;2: only the first two dimensions will be shown
</p>
</td></tr>
<tr><td><code id="ProjectionPursuit_+3A_cls">Cls</code></td>
<td>
<p>[1:n,1] Optional,: only relevant if PlotIt=TRUE. Numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An short overview of different types of projection methods can be found in [Thrun, 2018, p.42, Fig. 4.1] (<a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>).
</p>


<h3>Value</h3>

  <table>
<tr><td><code>ProjectedPoints</code></td>
<td>

<p>[1:n,OutputDimension], n by OutputDimension matrix containing coordinates of the Projectio
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>You can use the standard <code>ShepardScatterPlot</code> or the better approach through the <code>ShepardDensityPlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun</p>

<hr>
<h2 id='SammonsMapping'>Sammons Mapping</h2><span id='topic+SammonsMapping'></span>

<h3>Description</h3>

<p>Improved MDS thorugh a normalization of the Input space</p>


<h3>Usage</h3>

<pre><code class='language-R'>SammonsMapping(DataOrDistances,method='euclidean',OutputDimension=2,PlotIt=FALSE,Cls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SammonsMapping_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Numerical matrix defined as either 
</p>
<p><code>Data</code>, i.e., [1:n,1:d], nonsymmetric, and consists of n cases of d-dimensional data points with every case having d attributes, variables or features,
</p>
<p>or
</p>
<p><code>Distances</code>, i.e.,[1:n,1:n], symmetric and consists of n cases, e.g., <code>as.matrix(dist(Data,method))</code>
</p>
</td></tr>
<tr><td><code id="SammonsMapping_+3A_method">method</code></td>
<td>
<p>method specified by distance string: 'euclidean','cityblock=manhatten','cosine','chebychev','jaccard','minkowski','manhattan','binary'</p>
</td></tr>
<tr><td><code id="SammonsMapping_+3A_outputdimension">OutputDimension</code></td>
<td>
<p>Number of dimensions in the Outputspace, default=2</p>
</td></tr>
<tr><td><code id="SammonsMapping_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, If TRUE: Plots the projection as a 2d visualization. 
</p>
</td></tr>
<tr><td><code id="SammonsMapping_+3A_cls">Cls</code></td>
<td>
<p>[1:n,1] Optional,: only relevant if PlotIt=TRUE. Numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An short overview of different types of projection methods can be found in [Thrun, 2018, p.42, Fig. 4.1] (<a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>ProjectedPoints</code></td>
<td>

<p>[1:n,OutputDimension], n by OutputDimension matrix containing coordinates of the Projectio
</p>
</td></tr>
<tr><td><code>Stress</code></td>
<td>

<p>Shephard-Kruskal Stress  
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>A wrapper for <code><a href="MASS.html#topic+sammon">sammon</a></code>
</p>
<p>You can use the standard <code>ShepardScatterPlot</code> or the better approach through the <code>ShepardDensityPlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data

Proj=SammonsMapping(Data)

## Not run: 
PlotProjectedPoints(Proj$ProjectedPoints,Hepta$Cls)

## End(Not run)
</code></pre>

<hr>
<h2 id='ShortestGraphPathsC'>
Shortest GraphPaths = geodesic distances
</h2><span id='topic+ShortestGraphPathsC'></span>

<h3>Description</h3>

<p>Dijkstra's SSSP (Single source shortest path) algorithm, from all points to all points
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ShortestGraphPathsC(Adj, Cost)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ShortestGraphPathsC_+3A_adj">Adj</code></td>
<td>

<p>[1:n,1:n]         0/1 adjascency matrix, e.g. from delaunay graph or gabriel graph
</p>
</td></tr>
<tr><td><code id="ShortestGraphPathsC_+3A_cost">Cost</code></td>
<td>

<p>[1:n,1:n]       matrix, distances between n points (normally euclidean)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Vertices are the points, edges have the costs defined by weights (normally a distance).
The algorithm runs in runs in O(n*E*Log(V)), see also [Jungnickel, 2013, p. 87]. Further details can be foubd in 
[Jungnickel, 2013, p. 83-87].
</p>


<h3>Value</h3>

<p>ShortestPaths[1:n,1:n]   
vector, shortest paths (geodesic) to all other vertices including the source vertice itself
from al vertices to all vertices, stored as a matrix
</p>


<h3>Note</h3>

<p>require C++11 standard (set flag in Compiler, if not set automatically)
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Dijkstra, 1959]	Dijkstra, E. W.: A note on two problems in connexion with graphs, Numerische mathematik, Vol. 1(1), pp. 269-271. 1959.
</p>
<p>[Jungnickel, 2013]	Jungnickel, D.: Graphs, networks and algorithms, (4th ed ed. Vol. 5), Berlin, Heidelberg, Germany, Springer, ISBN: 978-3-642-32278-5, 2013.
</p>
<p>[Thrun/Ultsch, 2017]	Thrun, M.C., Ultsch, A.: Projection based Clustering, Conf. Int. Federation of Classification Societies (IFCS),DOI:10.13140/RG.2.2.13124.53124, Tokyo, 2017.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+DijkstraSSSP">DijkstraSSSP</a></code>
</p>

<hr>
<h2 id='tSNE'>T-distributed Stochastic Neighbor Embedding (t-SNE)</h2><span id='topic+tSNE'></span>

<h3>Description</h3>

<p>T-distributed Stochastic Neighbor Embedding   res = tSNE(Data, KNN=30,OutputDimension=2) 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tSNE(DataOrDistances,k,OutputDimension=2,Algorithm='tsne_cpp',

method="euclidean",Whitening=FALSE, Iterations=1000,PlotIt=FALSE,Cls,num_threads=1,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tSNE_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Numerical matrix defined as either 
</p>
<p><code>Data</code>, i.e., [1:n,1:d], nonsymmetric, and consists of n cases of d-dimensional data points with every case having d attributes, variables or features,
</p>
<p>or
</p>
<p><code>Distances</code>, i.e.,[1:n,1:n], symmetric and consists of n cases, e.g., <code>as.matrix(dist(Data,method))</code>
</p>
</td></tr>
<tr><td><code id="tSNE_+3A_k">k</code></td>
<td>
<p>number of k nearest neighbors=number of effective nearest neighbors(&quot;perplexity&quot;); Important parameter. If not given, settings of packages of t-SNE will be used depending <code>Algorithm</code></p>
</td></tr>
<tr><td><code id="tSNE_+3A_outputdimension">OutputDimension</code></td>
<td>
<p>Number of dimensions in the Outputspace, default=2</p>
</td></tr>
<tr><td><code id="tSNE_+3A_algorithm">Algorithm</code></td>
<td>

<p>'tsne_cpp': T-Distributed Stochastic Neighbor Embedding using a Barnes-HutImplementation in C++ of <span class="pkg">Rtsne</span>. Requires Version &gt;= 0.15 of <span class="pkg">Rtsne</span> for multicore parallelisation.
</p>
<p>'tsne_opt_cpp': T-Distributed Stochastic Neighbor Embedding with automated optimized parameters using a Barnes-HutImplementation in C++ of [Ulyanov, 2016].
</p>
<p>'tsne_r': pure R implementation of the t-SNE algorithm of of <span class="pkg">tsne</span>
</p>
</td></tr>
<tr><td><code id="tSNE_+3A_method">method</code></td>
<td>
<p>	                    method specified by distance string: 
'euclidean','cityblock=manhatten','cosine','chebychev','jaccard','minkowski','manhattan','binary' 
</p>
</td></tr>
<tr><td><code id="tSNE_+3A_whitening">Whitening</code></td>
<td>
<p>A boolean value indicating whether the matrix data should be whitened (tsne_r) or if pca should be used priorly 
(tsne_cpp)</p>
</td></tr>
<tr><td><code id="tSNE_+3A_iterations">Iterations</code></td>
<td>
<p> maximum number of iterations to perform.</p>
</td></tr>
<tr><td><code id="tSNE_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, If TRUE: Plots the projection as a 2d visualization. 
OutputDimension&gt;2: only the first two dimensions will be shown
</p>
</td></tr>
<tr><td><code id="tSNE_+3A_cls">Cls</code></td>
<td>
<p>[1:n,1] Optional,: only relevant if PlotIt=TRUE. Numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
<tr><td><code id="tSNE_+3A_num_threads">num_threads</code></td>
<td>

<p>Number of threads for parallel computation, only usable for Algorithm='tsne_cpp' or 'tsne_opt_cpp'
</p>
</td></tr>
<tr><td><code id="tSNE_+3A_...">...</code></td>
<td>

<p>Further arguments passed on to either 'Rtsne' or 'tsne' 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An short overview of different types of projection methods can be found in [Thrun, 2018, p.42, Fig. 4.1], <a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>.
</p>


<h3>Value</h3>

<p>List of 
</p>
<table>
<tr><td><code>ProjectedPoints</code></td>
<td>
<p>[1:n,OutputDimension], n by OutputDimension matrix containing coordinates of the Projection</p>
</td></tr>
<tr><td><code>ModelObject</code></td>
<td>
<p>NULL for tsne_r, further information if tsne_cpp is selected </p>
</td></tr>
</table>


<h3>Note</h3>

<p>A wrapper for <code><a href="Rtsne.html#topic+Rtsne">Rtsne</a></code> (Algorithm='tsne_cpp'),
</p>
<p><a href="https://github.com/omiq-ai/Multicore-opt-SNE">Multicore-opt-tSNE</a> (Algorithm='tsne_opt_cpp'),
</p>
<p>or  for <code><a href="tsne.html#topic+tsne">tsne</a></code> (Algorithm='tsne_r')
</p>
<p>You can use the standard <code>ShepardScatterPlot</code> or the better approach through the <code>ShepardDensityPlot</code> of the CRAN package <code>DataVisualizations</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun, Luca Brinkmann</p>


<h3>References</h3>

<p>Anna C. Belkina, Christopher O. Ciccolella, Rina Anno, Josef Spidlen, Richard Halpert, Jennifer Snyder-Cappione: Automated optimal parameters for T-distributed stochastic neighbor embedding improve visualization and allow analysis of large datasets, bioRxiv 451690, doi: https://doi.org/10.1101/451690, 2018.
</p>
<p>L.J.P van der Maaten: Accelerating t-SNE using tree-based algorithms, Journal of Machine Learning Research 15.1:3221-3245, 2014.
</p>
<p>Ulyanov, Dmitry: Multicore-TSNE, GitHub repository URL <a href="https://github.com/DmitryUlyanov/Multicore-TSNE">https://github.com/DmitryUlyanov/Multicore-TSNE</a>, 2016.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data

## Not run: 
Proj=tSNE(Data,k=7)

PlotProjectedPoints(Proj$ProjectedPoints,Hepta$Cls)

## End(Not run)

</code></pre>

<hr>
<h2 id='UniformManifoldApproximationProjection'>
Uniform Manifold Approximation and Projection
</h2><span id='topic+UniformManifoldApproximationProjection'></span>

<h3>Description</h3>

<p>Uniform manifold approximation and projection is a technique for dimension reduction. The algorithm was described by [McInnes et al., 2018].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>UniformManifoldApproximationProjection(DataOrDistances, k,

Epochs,OutputDimension=2,Algorithm='umap_pkg',PlotIt=FALSE,Cls,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="UniformManifoldApproximationProjection_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Numerical matrix defined as either 
</p>
<p><code>Data</code>, i.e., [1:n,1:d], nonsymmetric, and consists of n cases of d-dimensional data points with every case having d attributes, variables or features,
</p>
<p>or
</p>
<p><code>Distances</code>, i.e.,[1:n,1:n], symmetric and consists of n cases, e.g., <code>as.matrix(dist(Data,method))</code>
</p>
</td></tr>
<tr><td><code id="UniformManifoldApproximationProjection_+3A_k">k</code></td>
<td>

<p>number of k nearest neighbors, Important parameter, if not given, settings of package <span class="pkg">umap</span> will be used, default of package <span class="pkg">umap</span> is currently 15
</p>
</td></tr>
<tr><td><code id="UniformManifoldApproximationProjection_+3A_epochs">Epochs</code></td>
<td>

<p>Number of eppochs (scalar), i.e, training length, default of package <span class="pkg">umap</span> is currently 200
</p>
</td></tr>
<tr><td><code id="UniformManifoldApproximationProjection_+3A_outputdimension">OutputDimension</code></td>
<td>

<p>Number of dimensions in the Outputspace, default=2
</p>
</td></tr>
<tr><td><code id="UniformManifoldApproximationProjection_+3A_algorithm">Algorithm</code></td>
<td>

<p><code>"umap_pkg"</code>: provides an interface for two implementations. One is written from scratch other one requires python <span class="pkg">umap</span>
</p>
<p><code>"uwot_pkg"</code>: complete re-implementation in R (and C++, via the 'Rcpp' package) of <span class="pkg">uwot</span>
</p>
</td></tr>
<tr><td><code id="UniformManifoldApproximationProjection_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, If TRUE: Plots the projection as a 2d visualization. 
OutputDimension&gt;2: only the first two dimensions will be shown
</p>
</td></tr>
<tr><td><code id="UniformManifoldApproximationProjection_+3A_cls">Cls</code></td>
<td>

<p>Optional,: only relevant if PlotIt=TRUE. Numeric vector, given Classification in numbers: every element is the cluster number of a certain corresponding element of data.
</p>
</td></tr>
<tr><td><code id="UniformManifoldApproximationProjection_+3A_...">...</code></td>
<td>

<p>one of the other 21 parameters that can be specified, please see <code><a href="umap.html#topic+umap.defaults">umap.defaults</a></code> of package <span class="pkg">umap</span> for details or parameters to be set in package <span class="pkg">uwot</span> depending on the choice of <code>Algorithm</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To the knowledge of the author of this function no peer-reviewed publication of the method exists. Use with greate care.
</p>


<h3>Value</h3>

<p>List of 
</p>
<table>
<tr><td><code>ProjectedPoints</code></td>
<td>
<p>[1:n,OutputDimension], n by OutputDimension matrix containing coordinates of the Projection</p>
</td></tr>
<tr><td><code>ModelObject</code></td>
<td>
<p>output of  <code><a href="umap.html#topic+umap">umap</a></code> or of package <span class="pkg">uwot</span> depending on <code>Algorithm</code></p>
</td></tr>
<tr><td><code>Setting</code></td>
<td>
<p>specific settings used in <code>UniformManifoldApproximationProjection</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p>Uniform Manifold Approximation and Projection and U-matrix [Ultsch/Siemon, 1990] are both sometimes abbreviated with Umap. Hence the abbreveviation is omitted here.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[McInnes et al., 2018]  McInnes, L., Healy, J., &amp; Melville, J.: Umap: Uniform manifold approximation and projection for dimension reduction, arXiv preprint arXiv:1802.03426, 2018.
</p>
<p>[Ultsch/Siemon, 1990]  Ultsch, A., &amp; Siemon, H. P.: Kohonen's Self Organizing Feature Maps for Exploratory Data Analysis, International Neural Network Conference, pp. 305-308, Kluwer Academic Press, Paris, France, 1990.
</p>


<h3>See Also</h3>

<p><code><a href="umap.html#topic+umap">umap</a></code> of <span class="pkg">umap</span>
</p>
<p><code><a href="uwot.html#topic+umap">umap</a></code> of <span class="pkg">uwot</span>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data=Hepta$Data

Proj=UniformManifoldApproximationProjection(Data)

## Not run: 
PlotProjectedPoints(Proj$ProjectedPoints,Hepta$Cls)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
