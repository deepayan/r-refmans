<!DOCTYPE html><html lang="en"><head><title>Help for package micer</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {micer}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#biData'><p>Example binary classification dataset</p></a></li>
<li><a href='#compareData'><p>Data for multiclass classification comparison</p></a></li>
<li><a href='#mcData'><p>Example multiclass classification dataset</p></a></li>
<li><a href='#mice'><p>mice</p></a></li>
<li><a href='#miceCI'><p>miceCI</p></a></li>
<li><a href='#miceCM'><p>miceCM</p></a></li>
<li><a href='#miceCompare'><p>miceCompare</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Map Image Classification Efficacy</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-05</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Aaron Maxwell &lt;Aaron.Maxwell@mail.wvu.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Map image classification efficacy (MICE) adjusts the accuracy rate relative to a random classification baseline (Shao et al. (2021)&lt;<a href="https://doi.org/10.1109%2FACCESS.2021.3116526">doi:10.1109/ACCESS.2021.3116526</a>&gt; and Tang et al. (2024)&lt;<a href="https://doi.org/10.1109%2FTGRS.2024.3446950">doi:10.1109/TGRS.2024.3446950</a>&gt;). Only the proportions from the reference labels are considered, as opposed to the proportions from the reference and predictions, as is the case for the Kappa statistic. This package offers means to calculate MICE and adjusted versions of class-level user's accuracy (i.e., precision) and producer's accuracy (i.e., recall) and F1-scores. Class-level metrics are aggregated using macro-averaging. Functions are also made available to estimate confidence intervals using bootstrapping and statistically compare two classification results. </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr (&ge; 1.1.3),</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/maxwell-geospatial/micer">https://github.com/maxwell-geospatial/micer</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/maxwell-geospatial/micer/issues">https://github.com/maxwell-geospatial/micer/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-18 02:30:32 UTC; vidcg</td>
</tr>
<tr>
<td>Author:</td>
<td>Aaron Maxwell [aut, cre, cph],
  Sarah Farhadpour [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-20 21:00:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='biData'>Example binary classification dataset</h2><span id='topic+biData'></span>

<h3>Description</h3>

<p>Example binary classification dataset. &quot;Mine&quot; is the positive case and
&quot;Not Mine&quot; is the background class. There are 178 samples from the &quot;Mine&quot; class and
4,822 samples from the &quot;Not Mine&quot; class. Counts are relative to reference labels.
Class proportions are based on landscape proportions. There are a total of 5,000 samples.
</p>


<h3>Format</h3>


<dl>
<dt>ref</dt><dd><p>reference label</p>
</dd>
<dt>pred</dt><dd><p>predicted label</p>
</dd>
</dl>



<h3>References</h3>

<p>Maxwell, A.E., Bester, M.S., Guillen, L.A., Ramezan, C.A., Carpinello, D.J., Fan, Y.,
Hartley, F.M., Maynard, S.M. and Pyron, J.L., 2020. Semantic segmentation deep learning for extracting
surface mine extents from historic topographic maps. Remote Sensing, 12(24), p.4145.
</p>

<hr>
<h2 id='compareData'>Data for multiclass classification comparison</h2><span id='topic+compareData'></span>

<h3>Description</h3>

<p>Example multiclass classification dataset with the following wetland-related classes:
&quot;PFO&quot;, &quot;PEM&quot;, &quot;RLP&quot;, and &quot;Not&quot;. PFO = Palustrine Forested; PEM = Palustrine Emergent;
RLP = River, Lake, Pond; Not = Not Wetland. There are 600 examples from each class relative to the
reference labels.
</p>


<h3>Format</h3>


<dl>
<dt>ref</dt><dd><p>correct label</p>
</dd>
<dt>rfRred</dt><dd><p>random forest prediction</p>
</dd>
<dt>dfRred</dt><dd><p>single decision tree prediction</p>
</dd>
</dl>



<h3>References</h3>

<p>These data are unpublished
</p>

<hr>
<h2 id='mcData'>Example multiclass classification dataset</h2><span id='topic+mcData'></span>

<h3>Description</h3>

<p>Example multiclass classification dataset with the following classes (counts relative to reference labels):
&quot;Barren&quot; (n=163), &quot;Forest&quot; (n=20,807), &quot;Impervious&quot; (n=426), &quot;Low Vegetation&quot; (n=3,182), &quot;Mixed Dev&quot; (n=520),
and &quot;Water&quot; (n=200). There are a total of 25,298 samples.
</p>


<h3>Format</h3>


<dl>
<dt>ref</dt><dd><p>reference label</p>
</dd>
<dt>pred</dt><dd><p>predicted label</p>
</dd>
</dl>



<h3>References</h3>

<p>Maxwell, A.E., Strager, M.P., Warner, T.A., Ramezan, C.A., Morgan, A.N. and Pauley, C.E.,
2019. Large-area, high spatial resolution land cover mapping using random forests, GEOBIA, and NAIP
orthophotography: Findings and recommendations. Remote Sensing, 11(12), p.1409.
</p>

<hr>
<h2 id='mice'>mice</h2><span id='topic+mice'></span>

<h3>Description</h3>

<p>Calculate map image classification efficacy (MICE) and other metrics using columns/vectors of reference and predicted classes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mice(
  reference,
  prediction,
  mappings = levels(as.factor(reference)),
  multiclass = TRUE,
  positiveIndex = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mice_+3A_reference">reference</code></td>
<td>
<p>column/vector of reference labels as factor data type.</p>
</td></tr>
<tr><td><code id="mice_+3A_prediction">prediction</code></td>
<td>
<p>column/vector of predicted labels as factor data type.</p>
</td></tr>
<tr><td><code id="mice_+3A_mappings">mappings</code></td>
<td>
<p>names of classes (if not provided, factor levels are used).</p>
</td></tr>
<tr><td><code id="mice_+3A_multiclass">multiclass</code></td>
<td>
<p>TRUE or FALSE. If TRUE, treats classification as multiclass. If FALSE, treats classification as binary. Default is TRUE.</p>
</td></tr>
<tr><td><code id="mice_+3A_positiveindex">positiveIndex</code></td>
<td>
<p>index for positive case for binary classification. Ignored for multiclass classification. Default is 1 or first factor level.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For multiclass classification, returns a list object with the following items:
$Mappings = class names;
$confusionMatrix = confusion matrix where columns represent the reference data and rows represent the classification result;
$referenceCounts = count of samples in each reference class;
$predictionCounts = count of predictions in each class;
$overallAccuracy = overall accuracy;
$MICE = map image classification efficacy;
$usersAccuracies = class-level user's accuracies (1 - commission error);
$CTBICEs = classification-total-based image classification efficacies (adjusted user's accuracies);
$producersAccuracies = class-level producer's accuracies (1 - omission error);
$RTBICEs = reference-total-based image classification efficacies (adjusted producer's accuracies);
$F1Scores = class-level harmonic mean of user's and producer's accuracies;
$F1Efficacies = F1-score efficacies;
$macroPA = class-aggregated, macro-averaged producer's accuracy;
$macroRTBICE = class-aggregated, macro-averaged reference-total-based image classification efficacy;
$macroUA = class-aggregated, macro-averaged user's accuracy;
$macroCTBICE = class-aggregated, macro-averaged classification-total-based image classification efficacy;
$macroF1 = class-aggregated, macro-averaged F1-score;
$macroF1Efficacy = class-aggregated, macro-averaged F1 efficacy;
</p>
<p>For binary classification, returns a list object with the following items:
$Mappings = class names;
$confusionMatrix = confusion matrix where columns represent the reference data and rows represent the classification result;
$referenceCounts = count of samples in each reference class;
$predictionCounts = count of predictions in each class;
$postiveCase = name or mapping for the positive case;
$overallAccuracy = overall accuracy;
$MICE = map image classification efficacy;
$Precision = precision (1 - commission error relative to positive case);
$precisionEfficacy = precision efficacy;
$NPV = negative predictive value (1 - commission error relative to negative case);
$npvEfficacy = negative predictive value efficacy;
$Recall = recall (1 - omission error relative to positive case);
$recallEfficacy = recall efficacy;
$specificity = specificity (1 - omission error relative to negative case);
$specificityEfficacy = specificity efficacy;
$f1Score = harmonic mean of precision and recall;
$f1Efficacy = F1-score efficacy;
</p>


<h3>Value</h3>

<p>multiclass or binary assessment metrics in a list object. See details for description of generated metrics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Multiclass example
data(mcData)
mice(mcData$ref,
mcData$pred,
mappings=c("Barren", "Forest", "Impervious", "Low
Vegetation", "Mixed Dev", "Water"),
multiclass=TRUE)

#Binary example
data(biData)
mice(biData$ref,
biData$pred,
mappings = c("Mined", "Not Mined"),
multiclass=FALSE,
positiveIndex=1)
</code></pre>

<hr>
<h2 id='miceCI'>miceCI</h2><span id='topic+miceCI'></span>

<h3>Description</h3>

<p>Calculate confidence intervals (CIs) for MICE and associated metrics using bootstrap sampling and the percentile method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>miceCI(
  reps = 200,
  frac = 0.7,
  lowPercentile,
  highPercentile,
  reference,
  prediction,
  mappings = levels(as.factor(reference)),
  multiclass = TRUE,
  positiveIndex = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="miceCI_+3A_reps">reps</code></td>
<td>
<p>number of bootstrap replicates to use. Default is 200.</p>
</td></tr>
<tr><td><code id="miceCI_+3A_frac">frac</code></td>
<td>
<p>proportion of samples to include in each bootstrap sample. Default is 0.7.</p>
</td></tr>
<tr><td><code id="miceCI_+3A_lowpercentile">lowPercentile</code></td>
<td>
<p>lower percentile for confidence interval. Default is 0.025 for a 95% CI.</p>
</td></tr>
<tr><td><code id="miceCI_+3A_highpercentile">highPercentile</code></td>
<td>
<p>upper percentile for confidence interval. Default is 0.975 for a 95% CI.</p>
</td></tr>
<tr><td><code id="miceCI_+3A_reference">reference</code></td>
<td>
<p>column of reference labels as factor data type.</p>
</td></tr>
<tr><td><code id="miceCI_+3A_prediction">prediction</code></td>
<td>
<p>column of predicted labels as factor data type.</p>
</td></tr>
<tr><td><code id="miceCI_+3A_mappings">mappings</code></td>
<td>
<p>names of classes (if not provided, factor levels are used).</p>
</td></tr>
<tr><td><code id="miceCI_+3A_multiclass">multiclass</code></td>
<td>
<p>TRUE or FALSE. If TRUE, treats classification as multiclass. If FALSE, treats classification as binary. Default is TRUE.</p>
</td></tr>
<tr><td><code id="miceCI_+3A_positiveindex">positiveIndex</code></td>
<td>
<p>index for positive case for binary classification. Ignored for multiclass classification. Default is 1 or first factor level</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Confidence intervals are estimated for overall accuracy, MICE, and all class-aggregated, macro-averaged metrics produced by mice() or miceCM().
Returns metric name, mean metric value, median metric value, lower confidence interval bounds (low.ci), and upper confidence interval bounds
(upper.ci) as a dataframe object.
</p>


<h3>Value</h3>

<p>dataframe object of metric name and estimated mean value, median value, and lower and upper CIs.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Multiclass example
data(mcData)
ciResultsMC &lt;- miceCI(rep=100,
frac=.7,
mcData$ref,
mcData$pred,
lowPercentile=0.025,
highPercentile=0.975,
mappings=c("Barren", "Forest", "Impervious", "Low Vegetation", "Mixed Dev", "Water"),
multiclass=TRUE)

print(ciResultsMC)

#Binary example
data(biData)
ciResultsBi &lt;- miceCI(rep=100,
frac=.7,
biData$ref,
biData$pred,
lowPercentile=0.025,
highPercentile=0.975,
mappings = c("Mined", "Not Mined"),
multiclass=FALSE,
positiveIndex=1)

print(ciResultsBi)
</code></pre>

<hr>
<h2 id='miceCM'>miceCM</h2><span id='topic+miceCM'></span>

<h3>Description</h3>

<p>Calculate map image classification efficacy (MICE) and other metrics using confusion matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>miceCM(
  cm,
  mappings = levels(as.factor(row.names(cm))),
  multiclass = TRUE,
  positiveIndex = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="miceCM_+3A_cm">cm</code></td>
<td>
<p>confusion matrix as table object where rows define predictions and columns define reference labels.</p>
</td></tr>
<tr><td><code id="miceCM_+3A_mappings">mappings</code></td>
<td>
<p>names of classes (if not provided, factor levels are used).</p>
</td></tr>
<tr><td><code id="miceCM_+3A_multiclass">multiclass</code></td>
<td>
<p>TRUE or FALSE. If TRUE, treats classification as multiclass. If FALSE, treats classification as binary. Default is TRUE.</p>
</td></tr>
<tr><td><code id="miceCM_+3A_positiveindex">positiveIndex</code></td>
<td>
<p>index for positive case for binary classification. Ignored for multiclass classification. Default is 1 or first factor level.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For multiclass classification, returns a list object with the following items:
$Mappings = class names;
$confusionMatrix = confusion matrix where columns represent the reference data and rows represent the classification result;
$referenceCounts = count of samples in each reference class;
$predictionCounts = count of predictions in each class;
$overallAccuracy = overall accuracy;
$MICE = map image classification efficacy;
$usersAccuracies = class-level user's accuracies (1 - commission error);
$CTBICEs = classification-total-based image classification efficacies (adjusted user's accuracies);
$producersAccuracies = class-level producer's accuracies (1 - omission error);
$RTBICEs = reference-total-based image classification efficacies (adjusted producer's accuracies);
$F1Scores = class-level harmonic mean of user's and producer's accuracies;
$F1Efficacies = F1-score efficacies;
$macroPA = class-aggregated, macro-averaged producer's accuracy;
$macroRTBICE = class-aggregated, macro-averaged reference-total-based image classification efficacy;
$macroUA = class-aggregated, macro-averaged user's accuracy;
$macroCTBICE = class-aggregated, macro-averaged classification-total-based image classification efficacy;
$macroF1 = class-aggregated, macro-averaged F1-score;
$macroF1Efficacy = class-aggregated, macro-averaged F1 efficacy;
</p>
<p>For binary classification, returns a list object with the following items:
$Mappings = class names;
$confusionMatrix = confusion matrix where columns represent the reference data and rows represent the classification result;
$referenceCounts = count of samples in each reference class;
$predictionCounts = count of predictions in each class;
$postiveCase = name or mapping for the positive case;
$overallAccuracy = overall accuracy;
$MICE = map image classification efficacy;
$Precision = precision (1 - commission error relative to positive case);
$precisionEfficacy = precision efficacy;
$NPV = negative predictive value (1 - commission error relative to negative case);
$npvEfficacy = negative predictive value efficacy;
$Recall = recall (1 - omission error relative to positive case);
$recallEfficacy = recall efficacy;
$specificity = specificity (1 - omission error relative to negative case);
$specificityEfficacy = specificity efficacy;
$f1Score = harmonic mean of precision and recall;
$f1Efficacy = F1-score efficacy;
</p>


<h3>Value</h3>

<p>multiclass or binary assessment metrics in a list object. See details for description of generated metrics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Multiclass example
data(mcData)
cmMC &lt;- table(mcData$pred, mcData$ref)
miceCM(cmMC,
mappings=c("Barren", "Forest", "Impervious", "Low Vegetation", "Mixed Dev", "Water"),
multiclass=TRUE)

#Binary example
data(biData)
cmB &lt;- table(biData$pred, biData$ref)
miceMCResult &lt;- miceCM(cmB,
mappings=c("Mined", "Not Mined"),
multiclass=FALSE,
positiveIndex=1)
print(miceMCResult)
</code></pre>

<hr>
<h2 id='miceCompare'>miceCompare</h2><span id='topic+miceCompare'></span>

<h3>Description</h3>

<p>Statistically compare two models using a paired t-test and bootstrap samples of the assessment results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>miceCompare(ref, result1, result2, reps, frac)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="miceCompare_+3A_ref">ref</code></td>
<td>
<p>column of reference labels as factor data type.</p>
</td></tr>
<tr><td><code id="miceCompare_+3A_result1">result1</code></td>
<td>
<p>column of predicted labels as factor data type (first result to compare).</p>
</td></tr>
<tr><td><code id="miceCompare_+3A_result2">result2</code></td>
<td>
<p>column of predicted labels as factor data type (second result to compare).</p>
</td></tr>
<tr><td><code id="miceCompare_+3A_reps">reps</code></td>
<td>
<p>number of bootstrap replicates to use. Default is 200.</p>
</td></tr>
<tr><td><code id="miceCompare_+3A_frac">frac</code></td>
<td>
<p>proportion of samples to include in each bootstrap sample. Default is 0.7.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>paired t-test results including t-statistic, degrees of freedom, p-value, 95% confidence interval, and mean difference
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(compareData)
compareResult &lt;- miceCompare(ref=compareData$ref,
result1=compareData$rfPred,
result2=compareData$dtPred,
reps=100,
frac=.7)
print(compareResult)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
