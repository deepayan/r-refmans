<!DOCTYPE html><html lang="en"><head><title>Help for package leapgp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {leapgp}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#leapGP'><p>Localized Ensemble of Approximate Gaussian Processes</p></a></li>
<li><a href='#predict_leapGP'><p>Predict Method for leapGP</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Localized Ensemble of Approximate Gaussian Processes</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Kellin Rumsey [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kellin Rumsey &lt;knrumsey@lanl.gov&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An emulator designed for rapid sequential emulation (e.g., Markov chain Monte Carlo applications). Works via extension of the 'laGP' approach by Gramacy and Apley (2015 &lt;<a href="https://doi.org/10.1080%2F10618600.2014.914442">doi:10.1080/10618600.2014.914442</a>&gt;). Details are given in Rumsey et al. (2023 &lt;<a href="https://doi.org/10.1002%2Fsta4.576">doi:10.1002/sta4.576</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>laGP, RANN, cluster</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat, lhs, tictoc, RColorBrewer</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-07-30 18:09:00 UTC; knrumsey</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-08-01 09:30:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='leapGP'>Localized Ensemble of Approximate Gaussian Processes</h2><span id='topic+leapGP'></span>

<h3>Description</h3>

<p>Function to train or initialize a leapGP model, as described in Rumsey et al. (2023).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leapGP(
  X,
  y,
  M0 = ceiling(sqrt(length(y))),
  rho = NA,
  scale = FALSE,
  n = ceiling(sqrt(length(y))),
  start = NA,
  verbose = FALSE,
  justdoit = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="leapGP_+3A_x">X</code></td>
<td>
<p>a matrix of training locations (1 row for each training instance)</p>
</td></tr>
<tr><td><code id="leapGP_+3A_y">y</code></td>
<td>
<p>a vector of training responses (<code>length(y)</code> should equal <code>nrow(X)</code>)</p>
</td></tr>
<tr><td><code id="leapGP_+3A_m0">M0</code></td>
<td>
<p>the number of prediction hubs desired. Defaults to <code>ceiling(sqrt(length(Y)))</code>.</p>
</td></tr>
<tr><td><code id="leapGP_+3A_rho">rho</code></td>
<td>
<p>(optional). The parameter controlling time-accuracy tradeoff. Can also be specified during prediction.</p>
</td></tr>
<tr><td><code id="leapGP_+3A_scale">scale</code></td>
<td>
<p>logical. Do we want the scale parameter to be returned for predictions? If TRUE,
the matrix <code class="reqn">K^{-1}</code> will be stored for each hub.</p>
</td></tr>
<tr><td><code id="leapGP_+3A_n">n</code></td>
<td>
<p>local neighborhood size (for laGP)</p>
</td></tr>
<tr><td><code id="leapGP_+3A_start">start</code></td>
<td>
<p>number of starting points for neighborhood (between 6 and n inclusive)</p>
</td></tr>
<tr><td><code id="leapGP_+3A_verbose">verbose</code></td>
<td>
<p>logical. Should status be printed? Deault is FALSE</p>
</td></tr>
<tr><td><code id="leapGP_+3A_justdoit">justdoit</code></td>
<td>
<p>logical. Force leapGP to run using specified parameters (may take a long time and/or cause R to crash).</p>
</td></tr>
<tr><td><code id="leapGP_+3A_...">...</code></td>
<td>
<p>optional arguments to be passed to <code>laGP()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The leapGP is extends the laGP framework of Gramacy &amp; Apley (2015). The methods are equivalent for <code>rho=1</code>,
but leapGP trades memory for speed when <code>rho &lt; 1</code>. The method is described in Rumsey et al. (2023) where they demonstrate
that leapGP is faster than laGP for sequential predictions and is also generally more accurate for some settings of <code>rho</code>.
</p>


<h3>Value</h3>

<p>an object of class <code>leapGP</code> with fields <code>X</code>, <code>y</code>, and <code>hubs</code>.  Also returns scale parameter if <code>scale=TRUE</code>
</p>


<h3>References</h3>

<p>Gramacy, R. B., &amp; Apley, D. W. (2015). Local Gaussian process approximation for large computer experiments. Journal of Computational and Graphical Statistics, 24(2), 561-578.
</p>
<p>Rumsey, K. N., Huerta, G., &amp; Derek Tucker, J. (2023). A localized ensemble of approximate Gaussian processes for fast sequential emulation. Stat, 12(1), e576.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate data
f &lt;- function(x){
   1.3356*(1.5*(1-x[1]) + exp(2*x[1] - 1)*sin(3*pi*(x[1] - 0.6)^2) +
   exp(3*(x[2]-0.5))*sin(4*pi*(x[2] - 0.9)^2))
}
X &lt;- matrix(runif(200), ncol=2)
y &lt;- apply(X, 1, f)

# Generate data for prediction
Xtest &lt;- matrix(runif(200), ncol=2)
ytest &lt;- apply(Xtest, 1, f)

# Train initial model
mod &lt;- leapGP(X, y, M0 = 30)
# Make sequential predictions
pred &lt;- rep(NA, 100)
for(i in 1:100){
  mod &lt;- predict_leapGP(mod, matrix(Xtest[i,], nrow=1), rho=0.9)
  pred[i] &lt;- mod$mean
}
</code></pre>

<hr>
<h2 id='predict_leapGP'>Predict Method for leapGP</h2><span id='topic+predict_leapGP'></span>

<h3>Description</h3>

<p>Predict method for an object of class leapGP.
Returns a (possibly modified) leapGP object as well as a prediction (with uncertainty, if requested).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_leapGP(
  object,
  newdata,
  rho = 0.95,
  scale = FALSE,
  n = ceiling(sqrt(length(y))),
  start = NA,
  M_max = Inf,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_leapGP_+3A_object">object</code></td>
<td>
<p>An object of class <code>leapGP</code></p>
</td></tr>
<tr><td><code id="predict_leapGP_+3A_newdata">newdata</code></td>
<td>
<p>New data</p>
</td></tr>
<tr><td><code id="predict_leapGP_+3A_rho">rho</code></td>
<td>
<p>parameter controlling time-accuracy tradeoff (default is <code>rho=0.95</code>)</p>
</td></tr>
<tr><td><code id="predict_leapGP_+3A_scale">scale</code></td>
<td>
<p>logical. Do we want the scale parameter to be returned for predictions? If TRUE,
the matrix <code class="reqn">K^{-1}</code> will be stored for each hub.</p>
</td></tr>
<tr><td><code id="predict_leapGP_+3A_n">n</code></td>
<td>
<p>local neighborhood size</p>
</td></tr>
<tr><td><code id="predict_leapGP_+3A_start">start</code></td>
<td>
<p>number of starting points for neighborhood (between 6 and n inclusive)</p>
</td></tr>
<tr><td><code id="predict_leapGP_+3A_m_max">M_max</code></td>
<td>
<p>the maximum number of hubs allowed (used to upper bound the run time)</p>
</td></tr>
<tr><td><code id="predict_leapGP_+3A_...">...</code></td>
<td>
<p>optional arguments to be passed to <code>laGP()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The leapGP is extends the laGP framework of Gramacy &amp; Apley (2015). The methods are equivalent for <code>rho=1</code>,
but leapGP trades memory for speed when <code>rho &lt; 1</code>. The method is described in Rumsey et al. (2023) where they demonstrate
that leapGP is faster than laGP for sequential predictions and is also generally more accurate for some settings of <code>rho</code>.
</p>


<h3>Value</h3>

<p>A list containing values <code>mean</code>, <code>hubs</code> <code>X</code> and <code>y</code>. If <code>scale=TRUE</code> the list also contains field <code>sd</code>.
</p>


<h3>References</h3>

<p>Gramacy, R. B., &amp; Apley, D. W. (2015). Local Gaussian process approximation for large computer experiments. Journal of Computational and Graphical Statistics, 24(2), 561-578.
</p>
<p>Rumsey, K. N., Huerta, G., &amp; Derek Tucker, J. (2023). A localized ensemble of approximate Gaussian processes for fast sequential emulation. Stat, 12(1), e576.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate data
f &lt;- function(x){
   1.3356*(1.5*(1-x[1]) + exp(2*x[1] - 1)*sin(3*pi*(x[1] - 0.6)^2) +
   exp(3*(x[2]-0.5))*sin(4*pi*(x[2] - 0.9)^2))
}
X &lt;- matrix(runif(200), ncol=2)
y &lt;- apply(X, 1, f)

# Generate data for prediction
Xtest &lt;- matrix(runif(200), ncol=2)
ytest &lt;- apply(Xtest, 1, f)

# Train initial model
mod &lt;- leapGP(X, y, M0 = 30)
# Make sequential predictions
pred &lt;- rep(NA, 100)
for(i in 1:100){
  mod &lt;- predict_leapGP(mod, matrix(Xtest[i,], nrow=1), rho=0.9)
  pred[i] &lt;- mod$mean
}
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
