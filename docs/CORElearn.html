<!DOCTYPE html><html><head><title>Help for package CORElearn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {CORElearn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CORElearn-package'><p>R port of CORElearn</p></a></li>
<li><a href='#attrEval'><p> Attribute evaluation</p></a></li>
<li><a href='#auxTest'><p>Test functions for manual usage</p></a></li>
<li><a href='#calibrate'><p> Calibration of  probabilities according to the given prior.</p></a></li>
<li><a href='#classDataGen'><p>Artificial data for testing classification algorithms</p></a></li>
<li><a href='#classPrototypes'><p> The typical instances of each class - class prototypes</p></a></li>
<li><a href='#CORElearn-internal'><p>Internal structures of CORElearn C++ part</p></a></li>
<li><a href='#CoreModel'><p>Build a classification or regression model</p></a></li>
<li><a href='#cvGen'><p> Cross-validation and stratified cross-validation</p></a></li>
<li><a href='#destroyModels'><p>Destroy single model or all CORElearn models</p></a></li>
<li><a href='#discretize'><p> Discretization of numeric attributes</p></a></li>
<li><a href='#display.CoreModel'><p> Displaying decision and regression trees</p></a></li>
<li><a href='#getCoreModel'><p> Conversion of model to a list</p></a></li>
<li><a href='#getRFsizes'><p> Get sizes of the trees in RF</p></a></li>
<li><a href='#getRpartModel'><p> Conversion of a CoreModel tree into a rpart.object</p></a></li>
<li><a href='#helpCore'><p> Description of parameters.</p></a></li>
<li><a href='#infoCore'><p> Description of certain CORElearn parameters</p></a></li>
<li><a href='#modelEval'><p> Statistical evaluation of predictions</p></a></li>
<li><a href='#noEqualRows'><p> Number of equal rows in two data sets</p></a></li>
<li><a href='#ordDataGen'><p>Artificial data for testing ordEval algorithms</p></a></li>
<li><a href='#ordEval'><p> Evaluation of ordered attributes</p></a></li>
<li><a href='#paramCoreIO'><p> Input/output of parameters from/to file</p></a></li>
<li><a href='#plot.CoreModel'><p> Visualization of CoreModel models</p></a></li>
<li><a href='#plot.ordEval'><p> Visualization of ordEval results</p></a></li>
<li><a href='#predict.CoreModel'><p> Prediction using constructed model</p></a></li>
<li><a href='#preparePlot'><p> Prepare graphics device</p></a></li>
<li><a href='#regDataGen'><p>Artificial data for testing regression algorithms</p></a></li>
<li><a href='#reliabilityPlot'><p>Plots reliability plot of probabilities</p></a></li>
<li><a href='#rfAttrEval'><p> Attribute evaluation with random forest</p></a></li>
<li><a href='#rfClustering'><p>Random forest based clustering</p></a></li>
<li><a href='#rfOOB'><p> Out-of-bag performance estimation for random forests</p></a></li>
<li><a href='#rfOutliers'><p> Random forest based outlier detection</p></a></li>
<li><a href='#rfProximity'><p> A random forest based proximity function</p></a></li>
<li><a href='#saveRF'><p> Saves/loads random forests model to/from file</p></a></li>
<li><a href='#testCore'><p>Verification of the CORElearn installation</p></a></li>
<li><a href='#versionCore'><p> Package version</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Classification, Regression and Feature Evaluation</td>
</tr>
<tr>
<td>Version:</td>
<td>1.57.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-11-18</td>
</tr>
<tr>
<td>Author:</td>
<td>Marko Robnik-Sikonja and Petr Savicky</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>"Marko Robnik-Sikonja" &lt;marko.robnik@fri.uni-lj.si&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A suite of machine learning algorithms written in C++ with the R 
 interface contains several learning techniques for classification and regression.
 Predictive models include e.g., classification and regression trees with
 optional constructive induction and models in the leaves, random forests, kNN, 
 naive Bayes, and locally weighted regression. All predictions obtained with these
 models can be explained and visualized with the 'ExplainPrediction' package.  
 This package is especially strong in feature evaluation where it contains several variants of
 Relief algorithm and many impurity based attribute evaluation functions, e.g., Gini, 
 information gain, MDL, and DKM. These methods can be used for feature selection 
 or discretization of numeric attributes.
 The OrdEval algorithm and its visualization is used for evaluation
 of data sets with ordinal features and class, enabling analysis according to the 
 Kano model of customer satisfaction. 
 Several algorithms support parallel multithreaded execution via OpenMP.  
 The top-level documentation is reachable through ?CORElearn.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://lkm.fri.uni-lj.si/rmarko/software/">http://lkm.fri.uni-lj.si/rmarko/software/</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>cluster,stats,nnet,plotrix,rpart.plot</td>
</tr>
<tr>
<td>Suggests:</td>
<td>lattice,MASS,ExplainPrediction</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-18 09:55:04 UTC; marko</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-11-18 14:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='CORElearn-package'>R port of CORElearn </h2><span id='topic+CORElearn-package'></span><span id='topic+CORElearn'></span>

<h3>Description</h3>

<p>The package CORElearn is an R port of CORElearn data mining system.
It provides various classification and regression models
as well as algorithms for feature selection and evaluation.
Several algorithms support parallel multithreaded execution via OpenMP (see details in function descriptions)., 
It is possible to run many functions outside the R environment.
The description and source code is available on the package web site
<a href="http://lkm.fri.uni-lj.si/rmarko/software/">http://lkm.fri.uni-lj.si/rmarko/software/</a>.
</p>


<h3>Details</h3>

<p>The main functions are 
</p>

<ul>
<li> <p><code><a href="#topic+CoreModel">CoreModel</a></code> which constructs classification  or regression model.
</p>

<ul>
<li><p> Classification models available:
</p>

<ul>
<li><p> random forests with optional local weighing of basic models
</p>
</li>
<li><p> decision tree with optional constructive induction in the inner nodes and/or models in the leaves
</p>
</li>
<li><p> kNN and kNN with Gaussian kernel, 
</p>
</li>
<li><p> naive Bayes.
</p>
</li></ul>

</li>
<li><p> Regression models:
</p>

<ul>
<li><p> regression trees with optional constructive induction in the inner nodes and/or models in the leaves, 
</p>
</li>
<li><p> linear models with pruning techniques
</p>
</li>
<li><p> locally weighted regression
</p>
</li>
<li><p> kNN and kNN with Gaussian kernel.
</p>
</li></ul>

</li></ul>

</li>
<li> <p><code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>  predicts with classification model labels and probabilities of new instances.
For regression models it returns the predicted function value.
</p>
</li>
<li> <p><code><a href="#topic+plot.CoreModel">plot.CoreModel</a></code> graphically visualizes trees and random forest models
</p>
</li>
<li> <p><code><a href="#topic+modelEval">modelEval</a></code> computes some statistics from predictions  
</p>
</li>
<li> <p><code><a href="#topic+attrEval">attrEval</a></code>  evaluates the quality of the attributes (dependent variables)
with the selected heuristic method.  
Feature evaluation algorithms are various variants of Relief algorithms (ReliefF, RReliefF,
cost-sensitive ReliefF, etc), gain ratio, gini-index, MDL, DKM, information gain, MSE, MAE, etc.
</p>
</li>
<li> <p><code><a href="#topic+ordEval">ordEval</a></code> evaluates ordinal attributes with ordEval algorithm and visualizes them with <code><a href="#topic+plot.ordEval">plot.ordEval</a></code>, 
</p>
</li>
<li> <p><code><a href="#topic+infoCore">infoCore</a></code>  outputs certain information about CORElearn methods,
</p>
</li>
<li> <p><code><a href="#topic+helpCore">helpCore</a></code> prints short description of a given parameter,
</p>
</li>
<li> <p><code><a href="#topic+paramCoreIO">paramCoreIO</a></code> reads/writes parameters for given model from/to file, 
</p>
</li>
<li> <p><code><a href="#topic+versionCore">versionCore</a></code> outputs version of the package from underlying C++ library.
</p>
</li></ul>

<p>Some of the internal structures of the C++ part are described in <code><a href="#topic+CORElearn-internal">CORElearn-internal</a></code>.
</p>
<p>For an automatically generated list of functions use <code>help(package=CORElearn)</code> or 
<code>library(help=CORElearn)</code>.
</p>
<p>ut this feature is currently not supported on all platforms and may interfere with other means of parallelization used in R, like package paralell.
It is tested to works on Windows, Linux, and Mac.
</p>
<p>For certain platforms multithreaded execution is not supported, since current set of compilers 
at CRAN do not fully support OpenMP. Also note that OpenMP execution may interfere with other means of parallelization on certain platforms. E.g.,
interference with package parallel is reported on Windows which can be prevented by setting parameter maxThreads=1.
For platforms other than Linux, Windows, and OsX to support multithreading it is possible to recompile the package with appropriate tools and compilers
(modify Makefile or Makefile.win in src folder, or consult authors).   
</p>


<h3>Author(s)</h3>

<p>Marko Robnik-Sikonja, Petr Savicky
</p>


<h3>References</h3>

<p>Marko Robnik-Sikonja, Igor Kononenko: Theoretical and Empirical Analysis of ReliefF and RReliefF.
<em>Machine Learning Journal</em>, 53:23-69, 2003
</p>
<p>Marko Robnik-Sikonja: Improving Random Forests. 
In J.-F. Boulicaut et al.(Eds): <em>ECML 2004, LNAI 3210</em>, Springer, Berlin, 2004, pp. 359-370
</p>
<p>Marko Robnik-Sikonja, Koen Vanhoof: Evaluation of ordinal attributes at value level. 
<em>Knowledge Discovery and Data Mining</em>, 14:225-243, 2007    
</p>
<p>Marko Robnik-Sikonja: Experiments with Cost-sensitive Feature Evaluation. 
In Lavrac et al.(eds): <em>Machine Learning, Proceedings of ECML 2003</em>, Springer, Berlin, 2003, pp. 325-336
</p>
<p>Majority of these references are available also from <a href="http://lkm.fri.uni-lj.si/rmarko/papers/">http://lkm.fri.uni-lj.si/rmarko/papers/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>,
<code><a href="#topic+plot.CoreModel">plot.CoreModel</a></code>,
<code><a href="#topic+modelEval">modelEval</a></code>,
<code><a href="#topic+attrEval">attrEval</a></code>,
<code><a href="#topic+ordEval">ordEval</a></code>,
<code><a href="#topic+plot.ordEval">plot.ordEval</a></code>,
<code><a href="#topic+helpCore">helpCore</a></code>, 
<code><a href="#topic+paramCoreIO">paramCoreIO</a></code>,
<code><a href="#topic+infoCore">infoCore</a></code>,
<code><a href="#topic+versionCore">versionCore</a></code>,
<code><a href="#topic+CORElearn-internal">CORElearn-internal</a></code>,
<code><a href="#topic+classDataGen">classDataGen</a></code>,
<code><a href="#topic+regDataGen">regDataGen</a></code>,
<code><a href="#topic+ordDataGen">ordDataGen</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the package
library(CORElearn) 
cat(versionCore(),"\n")

# use iris data set
trainIdxs &lt;- sample(x=nrow(iris), size=0.7*nrow(iris), replace=FALSE)
testIdxs &lt;- c(1:nrow(iris))[-trainIdxs]

# build random forests model with certain parameters
# setting maxThreads to 0 or more than 1 forces 
# utilization of several processor cores 
modelRF &lt;- CoreModel(Species ~ ., iris[trainIdxs,], model="rf",
              selectionEstimator="MDL",minNodeWeightRF=5,
              rfNoTrees=100, maxThreads=1)

print(modelRF) # simple visualization, test also others with function plot

# prediction on testing set
pred &lt;- predict(modelRF, iris[testIdxs,], type="both") 

# compute statistics
mEval &lt;- modelEval(modelRF, iris[["Species"]][testIdxs], pred$class, pred$prob)
print(mEval) 

## Not run: 
# explain predictions on the level of model and individual instances
require(ExplainPrediction)
explainVis(modelRF, iris[trainIdxs,], iris[testIdxs,], method="EXPLAIN",
           visLevel="model", problemName="iris", fileType="none", 
           classValue=1, displayColor="color") 
# turn on the history in visualization window to see all instances
explainVis(modelRF, iris[trainIdxs,], iris[testIdxs,], method="EXPLAIN",
           visLevel="instance", problemName="iris", fileType="none", 
           classValue=1, displayColor="color") 

## End(Not run)
# Clean up, otherwise the memory is still taken
destroyModels(modelRF) # clean up

 
# evaluate features in given data set with selected method
# instead of formula interface one can provide just 
# the name or index of target variable
estReliefF &lt;- attrEval("Species", iris, 
                       estimator="ReliefFexpRank", ReliefIterations=30)
print(estReliefF)
    
# evaluate ordered features with ordEval
profiles &lt;- ordDataGen(200)
est &lt;- ordEval(class ~ ., profiles, ordEvalNoRandomNormalizers=100)
# print(est)  

</code></pre>

<hr>
<h2 id='attrEval'> Attribute evaluation </h2><span id='topic+attrEval'></span>

<h3>Description</h3>

<p>The method evaluates the quality of the features/attributes/dependent variables
specified by the formula with the selected heuristic method.  
Feature evaluation algorithms available for classification problems 
are various variants of Relief and ReliefF algorithms (ReliefF,
cost-sensitive ReliefF, ...), and impurity-based algorithms (information gain, gain ratio, gini-index, MDL, DKM, etc).
For regression problems there are RREliefF, MSEofMean, MSEofModel, MAEofModel, ...
Parallel execution on several cores is supported for speedup.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  attrEval(formula, data, estimator, costMatrix = NULL, 
           outputNumericSplits=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="attrEval_+3A_formula">formula</code></td>
<td>
<p> Either a formula specifying the attributes to be evaluated and the target variable, 
or a name of target variable, or an index of target variable. </p>
</td></tr>
<tr><td><code id="attrEval_+3A_data">data</code></td>
<td>
<p> Data frame with evaluation data. </p>
</td></tr>
<tr><td><code id="attrEval_+3A_estimator">estimator</code></td>
<td>
<p> The name of the evaluation method.</p>
</td></tr>
<tr><td><code id="attrEval_+3A_costmatrix">costMatrix</code></td>
<td>
<p> Optional cost matrix used with certain estimators. </p>
</td></tr>
<tr><td><code id="attrEval_+3A_outputnumericsplits">outputNumericSplits</code></td>
<td>
<p> Controls of the output contain the best split point for numeric attributes. 
This is only sensible for impurity based estimators (like information gain, gini, MDL, gain ratio, etc. in classification, 
and MSEofMean in regression). The default value of parameter 
<code>binaryEvaluateNumericAttributes = TRUE</code> shall not be modified in this case. If the value of 
<code>outputNumericSplits = TRUE</code>, the output is a list with attribute evaluations and numeric attributes' splits 
(instead of a single vector with evaaluations). See the returned value description.</p>
</td></tr>
<tr><td><code id="attrEval_+3A_...">...</code></td>
<td>
<p> Additional options used by specific evaluation methods as described in <code><a href="#topic+helpCore">helpCore</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameter <code>formula</code> can be interpreted in three ways, where the formula interface is the most elegant one, 
but inefficient and inappropriate for large data sets.   See also examples below. As <code>formula</code> one can specify:
</p>
 
<dl>
<dt>an object of class <code>formula</code></dt><dd><p>used as a mechanism to select features (attributes) 
and prediction variable (class). Only simple terms can be used and 
interaction expressed in formula syntax are not supported. The simplest way is
to specify just response variable: <code>class ~ .</code>.
In this case all other attributes in the data set are evaluated. Note that formula interface is not appropriate for data sets with
large number of variables.</p>
</dd>
<dt>a character vector</dt><dd><p>specifying the name of target variable, all the other columns in data frame <code>data</code> are used as predictors.</p>
</dd>
<dt>an integer</dt><dd><p>specifying the index of of target variable in data frame <code>data</code>, all the other columns are used as predictors.</p>
</dd>
</dl>
  
<p>The optional parameter <b> costMatrix </b> can provide nonuniform cost matrix to certain
cost-sensitive measures (ReliefFexpC, ReliefFavgC, ReliefFpe, ReliefFpa, ReliefFsmp,GainRatioCost,     
DKMcost, ReliefKukar, and MDLsmp). For other measures this parameter is ignored.    
The format of the matrix is costMatrix(true class, predicted class). 
By default a uniform costs are assumed, i.e.,  costMatrix(i, i) = 0, and costMatrix(i, j) = 1, for i not equal to j.
</p>
<p>The <b>estimator</b> parameter selects the evaluation heuristics.  For classification problem it 
must be one of the names returned by <code>infoCore(what="attrEval")</code> and for 
regression problem it must be one of the names returned by <code>infoCore(what="attrEvalReg")</code>
Majority of these feature evaluation measures are described in the references given below,
here only a short description is given. For classification problem they are
</p>

<dl>
<dt>&quot;ReliefFequalK&quot;</dt><dd><p>ReliefF algorithm where k nearest instances have equal weight. </p>
</dd>
<dt>&quot;ReliefFexpRank&quot;</dt><dd><p>ReliefF algorithm where k nearest instances have weight exponentially decreasing with 
increasing rank. Rank of nearest instance is determined by the increasing (Manhattan) distance from the selected instance.
This is a default choice for methods taking conditional dependencies among the attributes into account. </p>
</dd>
<dt>&quot;ReliefFbestK&quot;</dt><dd><p>ReliefF algorithm where all possible k (representing k nearest instances)
are tested and for each feature the highest score is returned. Nearest instances have equal weights. </p>
</dd>
<dt>&quot;Relief&quot;</dt><dd><p>Original algorithm of Kira and Rendel (1991) working on two class problems. </p>
</dd>
<dt>&quot;InfGain&quot;</dt><dd><p>Information gain. </p>
</dd>
<dt>&quot;GainRatio&quot;</dt><dd><p>Gain ratio, which is normalized information gain to prevent bias to multi-valued attributes.</p>
</dd>
<dt>&quot;MDL&quot;</dt><dd><p>Acronym for Minimum Description Length, presents method introduced in (Kononenko, 1995) 
with favorable bias for multi-valued and multi-class problems. Might be the best method
among those not taking conditional dependencies into account. </p>
</dd>
<dt>&quot;Gini&quot;</dt><dd><p>Gini-index. </p>
</dd>
<dt>&quot;MyopicReliefF&quot;</dt><dd><p>Myopic version of ReliefF resulting from assumption of no local dependencies and 
attribute dependencies upon class. </p>
</dd>
<dt>&quot;Accuracy&quot;</dt><dd><p>Accuracy of resulting split. </p>
</dd>
<dt>&quot;ReliefFmerit&quot; </dt><dd><p>ReliefF algorithm where for each random instance the merit of each attribute is 
normalized by the sum of differences in all attributes.</p>
</dd>
<dt>&quot;ReliefFdistance&quot;</dt><dd><p>ReliefF algorithm where k nearest instances are weighed  directly with its 
inverse distance from the selected instance. Usually using ranks instead of distance
as in <code>ReliefFexpRank</code> is more effective. </p>
</dd>
<dt>&quot;ReliefFsqrDistance&quot;</dt><dd><p>ReliefF algorithm where k nearest instances are weighed  with its 
inverse square distance from the selected instance. </p>
</dd> 
<dt>&quot;DKM&quot;</dt><dd><p>Measure named after Dietterich, Kearns, and Mansour who proposed it in 1996. </p>
</dd>
<dt>&quot;ReliefFexpC&quot;</dt><dd><p>Cost-sensitive ReliefF algorithm with expected costs. </p>
</dd>
<dt>&quot;ReliefFavgC&quot;</dt><dd><p>Cost-sensitive ReliefF algorithm with average costs. </p>
</dd>
<dt>&quot;ReliefFpe&quot;</dt><dd><p>Cost-sensitive ReliefF algorithm with expected probability. </p>
</dd>
<dt>&quot;ReliefFpa&quot;</dt><dd><p>Cost-sensitive ReliefF algorithm with average probability. </p>
</dd>
<dt>&quot;ReliefFsmp&quot;</dt><dd><p>Cost-sensitive ReliefF algorithm with cost sensitive sampling. </p>
</dd>
<dt>&quot;GainRatioCost&quot; </dt><dd><p>Cost-sensitive variant of GainRatio. </p>
</dd>
<dt>&quot;DKMcost&quot; </dt><dd><p>Cost-sensitive variant of DKM. </p>
</dd>
<dt>&quot;ReliefKukar&quot;</dt><dd><p>Cost-sensitive Relief algorithm introduced by Kukar in 1999. </p>
</dd>
<dt>&quot;MDLsmp&quot; </dt><dd><p>Cost-sensitive variant of MDL where costs are introduced through sampling.</p>
</dd>
<dt>&quot;ImpurityEuclid&quot;</dt><dd><p>Euclidean distance as impurity function on within node class distributions.</p>
</dd>
<dt>&quot;ImpurityHellinger&quot;</dt><dd><p>Hellinger distance as impurity function on within node class distributions.</p>
</dd>
<dt>&quot;UniformDKM&quot;</dt><dd><p>Dietterich-Kearns-Mansour (DKM) with uniform priors. </p>
</dd>
<dt>&quot;UniformGini&quot;</dt><dd><p>Gini index with uniform priors.</p>
</dd>                      
<dt>&quot;UniformInf&quot;</dt><dd><p>Information gain with uniform priors.</p>
</dd> 
<dt>&quot;UniformAccuracy&quot;</dt><dd><p>Accuracy with uniform priors. </p>
</dd>               
<dt>&quot;EqualDKM&quot;</dt><dd><p>Dietterich-Kearns-Mansour (DKM) with equal weights for splits. </p>
</dd>
<dt>&quot;EqualGini&quot;</dt><dd><p>Gini index with equal weights for splits.</p>
</dd>                      
<dt>&quot;EqualInf&quot;</dt><dd><p>Information gain with equal weights for splits. </p>
</dd>              
<dt>&quot;EqualHellinger&quot;</dt><dd><p>Two equally weighted splits based Hellinger distance.</p>
</dd> 
<dt>&quot;DistHellinger&quot;</dt><dd><p>Hellinger distance between class distributions in branches.</p>
</dd>
<dt>&quot;DistAUC&quot;</dt><dd><p>AUC distance between splits.</p>
</dd>               
<dt>&quot;DistAngle&quot;</dt><dd><p>Cosine of angular distance between splits.</p>
</dd>              
<dt>&quot;DistEuclid&quot;</dt><dd><p>Euclidean distance between splits.</p>
</dd>              
</dl>
    
<p>For regression problem the implemented measures are:
</p>

<dl>
<dt>&quot;RReliefFequalK&quot;</dt><dd><p>RReliefF algorithm where k nearest instances have equal weight. </p>
</dd>
<dt>&quot;ReliefFexpRank&quot;</dt><dd><p>RReliefF algorithm where k nearest instances have weight exponentially decreasing with 
increasing rank. Rank of nearest instance is determined by the increasing (Manhattan) distance from the selected instance.
This is a default choice for methods taking conditional dependencies among the attributes into account. </p>
</dd>
<dt>&quot;RReliefFbestK&quot;</dt><dd><p>RReliefF algorithm where all possible k (representing k nearest instances)
are tested and for each feature the highest score is returned. Nearest instances have equal weights. </p>
</dd>
<dt>&quot;RReliefFwithMSE&quot;</dt><dd><p>A combination of RReliefF and MSE algorithms. </p>
</dd>
<dt>&quot;MSEofMean&quot;  </dt><dd><p>Mean Squared Error as heuristic used to measure error by mean predicted value after split on the feature.</p>
</dd>
<dt>&quot;MSEofModel&quot;  </dt><dd><p>Mean Squared Error of an arbitrary model used on splits resulting from the feature. 
The model is chosen with parameter <code>modelTypeReg</code>. </p>
</dd>
<dt>&quot;MAEofModel&quot;  </dt><dd><p>Mean Absolute Error of an arbitrary model used on splits resulting from the feature. 
The model is chosen with parameter <code>modelTypeReg</code>. If we use median as the model, we get robust equivalent 
to <code>MSEofMean</code>.</p>
</dd>
<dt>&quot;RReliefFdistance&quot;</dt><dd><p>RReliefF algorithm where k nearest instances are weighed  directly with its 
inverse distance from the selected instance. Usually using ranks instead of distance
as in <code>RReliefFexpRank</code> is more effective. </p>
</dd>
<dt>&quot;RReliefFsqrDistance&quot;</dt><dd><p>RReliefF algorithm where k nearest instances are weighed  with its 
inverse square distance from the selected instance. </p>
</dd> 
</dl>
        
<p>There are some additional parameters <b>... </b> available which are used by specific evaluation heuristics.
Their list and short description is available by calling <code><a href="#topic+helpCore">helpCore</a></code>. See Section on attribute evaluation.
</p>
<p>The attributes can also be evaluated via random forest out-of-bag set with function <code><a href="#topic+rfAttrEval">rfAttrEval</a></code>.
</p>
<p>Evaluation and visualization of ordered attributes is covered in function <code><a href="#topic+ordEval">ordEval</a></code>.  
</p>


<h3>Value</h3>

<p>The method returns a vector of evaluations for the features in the order specified by the formula.
In case of parameter <code>binaryEvaluateNumericAttributes=TRUE</code> the method returns a list with two components:
<code>attrEval</code> and <code>splitPointNum</code>. The <code>attrEval</code> contains 
a vector of evaluations for the features in the order specified by the formula. The <code>splitPointNum</code>
contains the split points of numeric attributes which produced the given attribute evaluation scores.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja </p>


<h3>References</h3>

 
<p>Marko Robnik-Sikonja, Igor Kononenko: Theoretical and Empirical Analysis of ReliefF and RReliefF.
<em>Machine Learning Journal</em>, 53:23-69, 2003
</p>
<p>Marko Robnik-Sikonja: Experiments with Cost-sensitive Feature Evaluation. 
In Lavrac et al.(eds): <em>Machine Learning, Proceedings of ECML 2003</em>, Springer, Berlin, 2003, pp. 325-336
</p>
<p>Igor Kononenko: On Biases in Estimating Multi-Valued Attributes.
In <em>Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI'95)</em>, 
pp. 1034-1040, 1995 
</p>
<p>Some of these references are available also from <a href="http://lkm.fri.uni-lj.si/rmarko/papers/">http://lkm.fri.uni-lj.si/rmarko/papers/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+rfAttrEval">rfAttrEval</a></code>,
<code><a href="#topic+ordEval">ordEval</a></code>,
<code><a href="#topic+helpCore">helpCore</a></code>, 
<code><a href="#topic+infoCore">infoCore</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use iris data

# run method ReliefF with exponential rank distance  
estReliefF &lt;- attrEval(Species ~ ., iris, 
                       estimator="ReliefFexpRank", ReliefIterations=30)
print(estReliefF)

# alternatively and more appropriate for large data sets 
# one can specify just the target variable
# estReliefF &lt;- attrEval("Species", iris, estimator="ReliefFexpRank",
#                        ReliefIterations=30)

# print all available estimators
infoCore(what="attrEval")
</code></pre>

<hr>
<h2 id='auxTest'>Test functions for manual usage</h2><span id='topic+testTime'></span><span id='topic+testClassPseudoRandom'></span>

<h3>Description</h3>

<p>Test functions for the current state of the development.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testTime()
testClassPseudoRandom(s, k, m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auxTest_+3A_s">s</code></td>
<td>
<p>Seed.</p>
</td></tr>
<tr><td><code id="auxTest_+3A_k">k</code></td>
<td>
<p>Length of required output.</p>
</td></tr>
<tr><td><code id="auxTest_+3A_m">m</code></td>
<td>
<p>number of streams.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>testTime()</code> determines the current time.
<code>testClassPseudoRandom(s, k, m)</code> tests the functionality of multiple streams of RNGs.
</p>


<h3>Value</h3>

<p>Depends on the function.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja, Petr Savicky </p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>testTime()
</code></pre>

<hr>
<h2 id='calibrate'> Calibration of  probabilities according to the given prior.</h2><span id='topic+calibrate'></span><span id='topic+applyCalibration'></span>

<h3>Description</h3>

<p>Given probability scores <code>predictedProb</code> as provided for example by a call to <code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code> 
and using one of available methods given by <code>methods</code> the function calibrates predicted probabilities  so that they 
match the actual probabilities of a binary class 1 provided by <code>correctClass</code>. 
The computed calibration can be applied to the scores returned by that model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
calibrate(correctClass, predictedProb, class1=1, 
          method = c("isoReg","binIsoReg","binning","mdlMerge"), 
          weight=NULL, noBins=10, assumeProbabilities=FALSE)
          
applyCalibration(predictedProb, calibration)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calibrate_+3A_correctclass">correctClass</code></td>
<td>
<p> A vector of correct class labels for a binary classification problem. </p>
</td></tr>
<tr><td><code id="calibrate_+3A_predictedprob">predictedProb</code></td>
<td>
<p> A vector of predicted class 1 (probability) scores. In <code>calibrate</code> method it should be of the same length as <code>correctClass</code>. </p>
</td></tr>
<tr><td><code id="calibrate_+3A_class1">class1</code></td>
<td>
<p>A class value (factor) or an index of the class value to be taken as a class to be calibrated.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_method">method</code></td>
<td>
<p> One of <code>isoReg</code>, <code>binIsoReg</code>, <code>binning</code>, or <code>mdlMerge</code>. See details below.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_weight">weight</code></td>
<td>
<p> If specified, should be of the same length as <code>correctClass</code> and gives the weights for all the instances, 
otherwise a default weight of 1 for each instance is assumed. </p>
</td></tr>
<tr><td><code id="calibrate_+3A_nobins">noBins</code></td>
<td>
<p>The value of parameter depends on the parameter <code>method</code> and specifies desired or initial number of bins. See details below.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_assumeprobabilities">assumeProbabilities</code></td>
<td>
<p> If <code>assumeProbabilities=TRUE</code> the values in <code>predictedProb</code> are expected to be in [0,1] range i.e., probability estimates. 
<code>assumeProbabilities=FALSE</code> the algorithm can be used as ordinary (isotonic) regression    </p>
</td></tr>  
<tr><td><code id="calibrate_+3A_calibration">calibration</code></td>
<td>
<p>The list resulting from a call to <code>calibration</code> and subsequently applied to probability scores returned by the same model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Depending on the specified <code>method</code> one of the following calibration methods is executed.
</p>

<ul>
<li> <p><code>"isoReg"</code> isotonic regression calibration based on pair-adjacent violators (PAV) algorithm.
</p>
</li>
<li> <p><code>"binning"</code> calibration into a pre-specified number of bands given by <code>noBins</code> parameter, trying to make bins of equal weight.
</p>
</li>
<li> <p><code>"binIsoReg"</code> first binning method is executed, following by a isotonic regression calibration.
</p>
</li>
<li> <p><code>"mdlMerge"</code> first intervals are merged by a MDL gain criterion into a prespecified number of intervals, following by the isotonic regression calibration.
</p>
</li></ul>
  
<p>If <code>model="binning"</code> the parameter <code>noBins</code> specifies the desired number of bins i.e., calibration bands;
if <code>model="binIsoReg"</code> the parameter <code>noBins</code> specifies the number of initial bins that are formed by binning before isotonic regression is applied;
if <code>model="mdlMerge"</code>  the parameter <code>noBins</code> specifies the number of bins formed after first applying isotonic regression. The most similar bins are merged using MDL criterion.
</p>


<h3>Value</h3>

<p>A function returns a list with two vector components of the same length:
</p>
<table>
<tr><td><code>interval</code></td>
<td>
<p>The boundaries of the intervals. Lower boundary 0 is not explicitly included but should be taken into account.</p>
</td></tr>
<tr><td><code>calProb</code></td>
<td>
<p>The calibrated probabilities for each corresponding interval.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>References</h3>

 
<p>I. Kononenko, M. Kukar: <em>Machine Learning and Data Mining: Introduction to Principles and Algorithms. </em> Horwood, 2007
</p>
<p>A. Niculescu-Mizil, R. Caruana: Predicting Good Probabilities With Supervised Learning. <em>Proceedings of the 22nd International Conference on Machine Learning (ICML'05)</em>, 2005
</p>


<h3>See Also</h3>

<p><code><a href="#topic+reliabilityPlot">reliabilityPlot</a></code>,
<code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>
.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate data set separately for training the model, 
#   calibration of probabilities and testing
train &lt;-classDataGen(noInst=200)
cal &lt;-classDataGen(noInst=200)
test &lt;- classDataGen(noInst=200)

# build random forests model with default parameters
modelRF &lt;- CoreModel(class~., train, model="rf", maxThreads=1)

# prediction 
predCal &lt;- predict(modelRF, cal, rfPredictClass=FALSE)
predTest &lt;- predict(modelRF, test, rfPredictClass=FALSE)
destroyModels(modelRF) # clean up, model not needed anymore

# calibrate for a chosen class1 and method
class1&lt;-1
calibration &lt;- calibrate(cal$class, predCal$prob[,class1], class1=class1, 
                         method="isoReg",assumeProbabilities=TRUE)

# apply the calibration to the testing set
calibratedProbs &lt;- applyCalibration(predTest$prob[,class1], calibration)
# the calibration of probabilities can be visualized with 
# reliabilityPlot function

</code></pre>

<hr>
<h2 id='classDataGen'>Artificial data for testing classification algorithms</h2><span id='topic+classDataGen'></span>

<h3>Description</h3>

<p>The generator produces classification data with 2 classes, 7 discrete and 3 numeric attributes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  classDataGen(noInst, t1=0.7, t2=0.9, t3=0.34, t4=0.32, 
               p1=0.5, classNoise=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classDataGen_+3A_noinst">noInst</code></td>
<td>
<p>Number of instances to generate.</p>
</td></tr>
<tr><td><code id="classDataGen_+3A_t1">t1</code>, <code id="classDataGen_+3A_t2">t2</code>, <code id="classDataGen_+3A_t3">t3</code></td>
<td>
<p> Parameters, which control the hardness of the discrete attributes.</p>
</td></tr>
<tr><td><code id="classDataGen_+3A_t4">t4</code></td>
<td>
<p> Parameter, which controls the hardness of the numeric attributes..</p>
</td></tr>
<tr><td><code id="classDataGen_+3A_p1">p1</code></td>
<td>
<p> Probability of class 1.</p>
</td></tr>
<tr><td><code id="classDataGen_+3A_classnoise">classNoise</code></td>
<td>
<p>Proportion of noise in the class variable for classification or virtual class variable for regression.</p>
</td></tr>   
</table>


<h3>Details</h3>

<p>Class probabilities are <code>p1</code> and <code>1 - p1</code>, respectively. The conditional distribution of attributes
under each of the classes depends on parameters <code>t1, t2, t3, t4</code> from [0,1].
Attributes a7 and x3 are irrelevant for all values of parameters.
</p>
<p>Examples of extreme settings of the parameters.
</p>

<ul>
<li><p> Setting satisfying t1*t2 = t3 implies no difference between the distributions
of individual discrete attributes among the two classes. However, if t1 &lt; 1, then
the joint distribution of them is different for the two classes.
</p>
</li>
<li><p> Setting t1 = 1 and t2 = t3 implies no difference between the joint distribution
of the discrete attributes among the two classes.
</p>
</li>
<li><p> Setting t1 = 1, t2 = 1, t3 = 0 implies disjoint supports of the distributions
of a1, a2, a4, a5, so this allows exact classification.
</p>
</li>
<li><p> Setting t4 = 1 implies no difference between the distribution of x1, x2 between
the classes. Setting t4 = 0 allows correct classification with probability one
only using x1 and x2.
</p>
</li></ul>

<p>For class 1 the attributes have distributions
</p>

<table>
<tr>
 <td style="text-align: left;">
(a1, a2, a3) </td><td style="text-align: left;">  <code class="reqn">D_1(t1, t2)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
a4, a5, a6   </td><td style="text-align: left;">  <code class="reqn">D_2(t3)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
a7           </td><td style="text-align: left;">  irrelevant attribute, probabilities of {a,b,c,d} are (1/2, 1/6, 1/6, 1/6) </td>
</tr>
<tr>
 <td style="text-align: left;">
x1, x2, x3   </td><td style="text-align: left;">  independent normal variables with mean 0 and standard  deviation 1, t4, 1 </td>
</tr>
<tr>
 <td style="text-align: left;">
x4, x5      </td><td style="text-align: left;"> independent uniformly distributed variables on [0,1] 
</td>
</tr>

</table>

<p>For class 2 the attributes have distributions
</p>

<table>
<tr>
 <td style="text-align: left;">
a1, a2, a3   </td><td style="text-align: left;"> <code class="reqn">D_2(t3)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
(a4, a5, a6) </td><td style="text-align: left;">  <code class="reqn">D_1(t1, t2)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
a7           </td><td style="text-align: left;"> irrelevant attribute, probabilities of {a,b,c,d} are (1/2, 1/6, 1/6, 1/6) </td>
</tr>
<tr>
 <td style="text-align: left;">
x1, x2, x3   </td><td style="text-align: left;"> independent normal variables with mean 0 and st. dev. t4, 1, 1 </td>
</tr>
<tr>
 <td style="text-align: left;">
x4, x5       </td><td style="text-align: left;"> independent uniformly distributed variables on [0,1] 
</td>
</tr>

</table>

<p>x3 is irrelevant for classification, since it has the same distribution under both classes.
</p>
<p>Attributes in a bracket are mutually dependent. Otherwise, the attributes
are conditionally independent for each of the two classes. This means
that if we consider groups of the attributes such that the attributes in each
of the two brackets form a group and each of the remaining attributes forms a
group with one element, then for each class, we have 7 groups, which are
conditionally independent for the given class. Note that the splitting into
groups differs for class 1 and 2.
</p>
<p>Distribution <code class="reqn">D_1(t1,t2)</code> consists of three dependent attributes. The
distribution of individual attributes depends only on t1*t2. For a given t1*t2,
the level of dependence decreases with t1 and increases with t2. There are
two extreme settings:
Setting t1 = 1, t2 = t1*t2 has the largest t1 and the smallest t2 and all three
attributes are independent.
Setting t1 = t1*t2, t2 = 1 has the smallest t1 and the largest t2 and also the
largest dependence between attributes.
</p>
<p>Distribution <code class="reqn">D_2(t3)</code> is equal to <code class="reqn">D_1(1, t3)</code>, so it contains three independent
attributes, whose distributions are the same as in <code class="reqn">D_1(t1,t2)</code> for every
setting satifying t1*t2 = t3.
</p>
<p>In other words, if t3 = t1*t2, then the distributions <code class="reqn">D_1(t1, t2)</code> and <code class="reqn">D_2(t3)</code>
have the same distributions of individual attributes and may differ only
in the dependences. There are no in <code class="reqn">D_2(t3)</code> and there are some in <code class="reqn">D_1(t1, t2)</code>
if t1 &lt; 1.
</p>
<p><em>Hardness of the discrete part</em>
</p>
<p>Setting t1 = 1 and t2 = t3 implies no difference between the discrete
attributes among the two classes.
</p>
<p>Setting satisfying t1*t2 = t3 implies no difference between the distributions
of individual discrete attributes among the two classes. However, there may
be a difference in dependences.
</p>
<p>Setting t1 = 1, t2 = 1, t3 = 0 implies disjoint supports of the distributions
of a1, a2, a4, a5, so this allows exact classification.
</p>
<p><em>Hardness of the continuous part</em>
</p>
<p>Depends monotonically on t4. Setting t4 = 1 implies no difference between the
classes. Setting t4 = 0 allows correct classification with probability one.
</p>


<h3>Value</h3>

<p>The method <code>classDataGen</code> returns a <code><a href="base.html#topic+data.frame">data.frame</a></code> with <code>noInst</code> rows and 11 columns. 
Range of values of the attributes and class are
</p>
<table>
<tr><td><code>a1</code></td>
<td>
<p> 0,1</p>
</td></tr>
<tr><td><code>a2</code></td>
<td>
<p> 0,1</p>
</td></tr>
<tr><td><code>a3</code></td>
<td>
<p> a,b,c,d</p>
</td></tr>
<tr><td><code>a4</code></td>
<td>
<p> 0,1</p>
</td></tr>
<tr><td><code>a5</code></td>
<td>
<p> 0,1</p>
</td></tr>
<tr><td><code>a6</code></td>
<td>
<p> a,b,c,d</p>
</td></tr>
<tr><td><code>a7</code></td>
<td>
<p> a,b,c,d</p>
</td></tr>
<tr><td><code>x1</code></td>
<td>
<p> numeric</p>
</td></tr>
<tr><td><code>x2</code></td>
<td>
<p> numeric</p>
</td></tr>
<tr><td><code>x3</code></td>
<td>
<p> numeric</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p> 1,2 </p>
</td></tr>
</table>
<p>For detailed specification of attributes (columns) see details section below.
</p>


<h3>Author(s)</h3>

<p> Petr Savicky </p>


<h3>See Also</h3>

<p><code><a href="#topic+regDataGen">regDataGen</a></code>, <code><a href="#topic+ordDataGen">ordDataGen</a></code>,<code><a href="#topic+CoreModel">CoreModel</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#prepare a classification data set
classData &lt;-classDataGen(noInst=200)

# build random forests model with certain parameters
modelRF &lt;- CoreModel(class~., classData, model="rf",
              selectionEstimator="MDL", minNodeWeightRF=5,
              rfNoTrees=100, maxThreads=1)
print(modelRF)
destroyModels(modelRF) # clean up
</code></pre>

<hr>
<h2 id='classPrototypes'> The typical instances of each class - class prototypes</h2><span id='topic+classPrototypes'></span>

<h3>Description</h3>

<p>For each class the most typical instances are returned based on the highest predicted probability for each class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classPrototypes(model, dataset, noPrototypes=10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classPrototypes_+3A_model">model</code></td>
<td>
<p> a <code><a href="#topic+CoreModel">CoreModel</a></code> model.</p>
</td></tr>
<tr><td><code id="classPrototypes_+3A_dataset">dataset</code></td>
<td>
<p> a dataset from which to get prototypes.</p>
</td></tr>
<tr><td><code id="classPrototypes_+3A_noprototypes">noPrototypes</code></td>
<td>
<p>number of instances of each class to return</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses <code>predict.CoreModel(model, dataset)</code> for prediction of the <code>dataset</code> with
<code>model</code>. Based on the returned probabilities, it selects the <code>noPrototypes</code> instances with highest probabilities for each class to be 
typical representatives of that class, i.e., prototypes. The prototypes can be
visualized by calling e.g., <br />
<code>plot(model, dataset, rfGraphType="prototypes", noPrototypes = 10)</code>.
</p>


<h3>Value</h3>

<p>A list with the most typical <code>noPrototypes</code> instances is returned. The list has the following attributes.
</p>
<table>
<tr><td><code>prototypes</code></td>
<td>
<p>vector with indexes of the most typical instances</p>
</td></tr>
<tr><td><code>clustering</code></td>
<td>
<p>vector with class assignments for typical instances in vector <code>instances</code> </p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>the names of the class values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> John Adeyanju Alao (as a part of his BSc thesis) and Marko Robnik-Sikonja (thesis supervisor)</p>


<h3>References</h3>

<p>Leo Breiman: Random Forests. <em>Machine Learning Journal</em>, 45:5-32, 2001
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>,
<code><a href="#topic+plot.CoreModel">plot.CoreModel</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dataset &lt;- iris
md &lt;- CoreModel(Species ~ ., dataset, model="rf", rfNoTrees=30,maxThreads=1)
typical &lt;- classPrototypes(md, dataset, 10)
destroyModels(md) # clean up
</code></pre>

<hr>
<h2 id='CORElearn-internal'>Internal structures of CORElearn C++ part</h2><span id='topic+CORElearn-internal'></span>

<h3>Description</h3>

<p>The package CORElearn is an R port of CORElearn data mining system.
This document is a short description of the C++ part which can also
serve as a standalone Linux or Windows data mining system,
its organization and main classes and data structures.
</p>


<h3>Details</h3>

<p>The C++ part is called from R functions collected in file <code>Rinterface.R</code>.
The C++ functions called from R and providing interface to R are collected in <code>Rfront.cpp</code>
and <code>Rconvert.cpp</code>. The front end for standalone version is in file <code>frontend.cpp</code>.
For many parts of the code there are two variants, classification and regression one.
Regression part usually has <code>Reg</code> somewhere in its name.
The main classes are 
</p>

<ul>
<li> <p><code>marray, mmatrix</code> are templates for storing vectors and matrixes
</p>
</li>
<li> <p><code>dataStore</code>  contains data storage and data manipulation methods, of which the most important are
</p>

<ul>
<li>   <p><code>mmatrix&lt;int&gt; DiscData, DiscPredictData</code> contain values of discrete attributes and class for training and prediction (optional).
In classification column 0 always    stores class values.
</p>
</li>
<li> <p><code>mmatrix&lt;double&gt; ContData, ContPredictData</code> contain values of numeric attribute and prediction values for training and prediction (optional). 
In regression column 0 always  stores target values.
</p>
</li>
<li> <p><code>marray&lt;attribute&gt; AttrDesc</code> with information about attributes' types, number of values, min, max, column index in DiscData or ContData, ...
</p>
</li></ul>

</li>
<li> <p><code>estimation, estimationReg</code> evaluate attributes with different purposes: decision/regression tree splitting, binarization, 
discretization, constructive induction, feature selection, etc. Because of efficiency these classes store its own data in
</p>

<ul>
<li>   <p><code>mmatrix&lt;int&gt; DiscValues</code> containing discrete attributes and class values,
</p>
</li>
<li> <p><code>mmatrix&lt;double&gt; ContValues</code> containing numeric attribute and prediction values.
</p>
</li></ul>

</li>
<li> <p><code>Options</code> stores and handles all the parameters of the system.
</p>
</li>
<li> <p><code>featureTree, regressionTree</code> build all the models, predict with them, and create output.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Marko Robnik-Sikonja
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>, <code><a href="#topic+CoreModel">CoreModel</a></code>, <code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>,
<code><a href="#topic+modelEval">modelEval</a></code>, <code><a href="#topic+attrEval">attrEval</a></code>, <code><a href="#topic+ordEval">ordEval</a></code>,
<code><a href="#topic+plot.ordEval">plot.ordEval</a></code>, <code><a href="#topic+helpCore">helpCore</a></code>, <code><a href="#topic+paramCoreIO">paramCoreIO</a></code>,
<code><a href="#topic+infoCore">infoCore</a></code>, <code><a href="#topic+versionCore">versionCore</a></code>.
</p>

<hr>
<h2 id='CoreModel'>Build a classification or regression model </h2><span id='topic+CoreModel'></span><span id='topic+cvCoreModel'></span>

<h3>Description</h3>

<p>Builds a classification or regression model from the <code>data</code> and <code>formula</code> with given parameters.
Classification models available are
</p>

<ul>
<li><p> random forests, possibly with local weighing of basic models (parallel execution on several cores),
</p>
</li>
<li><p> decision tree with constructive induction in the inner nodes and/or models in the leaves,
</p>
</li>
<li><p> kNN and weighted kNN with Gaussian kernel,
</p>
</li>
<li><p> naive Bayesian classifier.
</p>
</li></ul>

<p>Regression models:
</p>

<ul>
<li><p> regression trees with constructive induction in the inner nodes and/or models in the leaves,
</p>
</li>
<li><p> linear models with pruning techniques,
</p>
</li>
<li><p> locally weighted regression,
</p>
</li>
<li><p> kNN and weighted kNN with Gaussian kernel.
</p>
</li></ul>

<p>Function <code>cvCoreModel</code> applies cross-validation to estimate predictive performance of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  CoreModel(formula, data,
       model=c("rf","rfNear","tree","knn","knnKernel","bayes","regTree"),
       costMatrix=NULL,...)
  cvCoreModel(formula, data,
       model=c("rf","rfNear","tree","knn","knnKernel","bayes","regTree"),
       costMatrix=NULL, folds=10, stratified=TRUE, returnModel=TRUE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CoreModel_+3A_formula">formula</code></td>
<td>
<p> Either a formula specifying the attributes to be evaluated and the target variable, or a name of target variable, or an index of target variable. </p>
</td></tr>
<tr><td><code id="CoreModel_+3A_data">data</code></td>
<td>
<p> Data frame with training data. </p>
</td></tr>
<tr><td><code id="CoreModel_+3A_model">model</code></td>
<td>
<p> The type of model to be learned. </p>
</td></tr>
<tr><td><code id="CoreModel_+3A_costmatrix">costMatrix</code></td>
<td>
<p> Optional misclassification cost matrix used with certain models. </p>
</td></tr>
<tr><td><code id="CoreModel_+3A_folds">folds</code></td>
<td>
<p> An integer, specifying the number of folds to use in cross-validation of model. </p>
</td></tr>
<tr><td><code id="CoreModel_+3A_stratified">stratified</code></td>
<td>
<p> A boolean specifying if cross-valiadation is to be stratified fpr classification problems, i.e. shall all folds have the same distribution of class values. </p>
</td></tr>
<tr><td><code id="CoreModel_+3A_returnmodel">returnModel</code></td>
<td>
<p> If <code>TRUE</code> the function <code>cvCoreModel</code> estimates predictive performance using cross-validation and returns the model build on the whole data set. 
If <code>returnModel=FALSE</code> the function only evaluates the model using cross-validation.  </p>
</td></tr>
<tr><td><code id="CoreModel_+3A_...">...</code></td>
<td>
<p> Options for building the model. See <code><a href="#topic+helpCore">helpCore</a></code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameter <code>formula</code> can be interpreted in three ways, where the formula interface is the most elegant one, 
but inefficient and inappropriate for large data sets. See also examples below. As <code>formula</code> one can specify:
</p>
 
<dl>
<dt>an object of class <code>formula</code></dt><dd><p>used as a mechanism to select features (attributes) 
and prediction variable (class). Only simple terms can be used and 
interaction expressed in formula syntax are not supported. The simplest way is
to specify just response variable: <code>class ~ .</code>.
In this case all other attributes in the data set are evaluated. Note that formula interface is not appropriate for data sets with
large number of variables.</p>
</dd>
<dt>a character vector</dt><dd><p>specifying the name of target variable, all the other columns in data frame <code>data</code> are used as predictors.</p>
</dd>
<dt>an integer</dt><dd><p>specifying the index of of target variable in data frame <code>data</code>, all the other columns are used as predictors.</p>
</dd>
</dl>

<p>Parameter <b>model</b> controls the type of the constructed model. There are several possibilities:
</p>

<dl>
<dt><code>"rf"</code></dt><dd><p> random forests classifier as defined by (Breiman, 2001) with some extensions, </p>
</dd>
<dt><code>"rfNear"</code></dt><dd><p> random forests classifier with basic models weighted locally (Robnik-Sikonja, 2005), </p>
</dd>
<dt><code>"tree"</code></dt><dd><p> decision tree with constructive induction in the inner nodes and/or models in the leaves, </p>
</dd>
<dt><code>"knn"</code></dt><dd><p> k nearest neighbors classifier, </p>
</dd>
<dt><code>"knnKernel"</code></dt><dd><p> weighted k nearest neighbors classifier with distance taken into account through Gaussian kernel, </p>
</dd>
<dt><code>"bayes"</code></dt><dd><p> naive Bayesian classifier, </p>
</dd>
<dt><code>"regTree"</code></dt><dd><p> regression trees with constructive induction in inner nodes and/or models in leaves
controlled by modelTypeReg parameter.
Models used in leaves of the regression tree can also be used as stand-alone regression models 
using option minNodeWeightTree=Inf (see examples below):
</p>

<ul>
<li><p> linear models with pruning techniques
</p>
</li>
<li><p> locally weighted regression
</p>
</li>
<li><p> kNN and kNN with Gaussian kernel.
</p>
</li></ul>

</dd>
</dl>

<p>There are many additional parameters <b>... </b> available which are used by different models.
Their list and description is available by calling <code><a href="#topic+helpCore">helpCore</a></code>. Evaluation of attributes is covered
in function <code><a href="#topic+attrEval">attrEval</a></code>.
</p>
<p>The optional parameter <b> costMatrix </b> can provide nonuniform cost matrix for classification problems. For regression
problem this parameter is ignored. The format of the matrix is costMatrix(true class, predicted class).
By default uniform costs are assumed, i.e.,  costMatrix(i, i) = 0, and costMatrix(i, j) = 1, for i not equal to j.
</p>


<h3>Value</h3>

<p>The created model is not returned as a R structure. It is stored internally
in the package memory space and only its pointer (index) is returned.
The maximum  number of models that can be stored simultaneously
is a parameter of the initialization function <code>initCore</code> and
defaults to 16384. Models, which are not needed, may be deleted in order
to free the memory using function <code>destroyModels</code>.
By referencing the returned model, any of the stored models may be
used for prediction with <code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>.
What the function actually returns is a list with components:
</p>
<table>
<tr><td><code>modelID</code></td>
<td>
<p> index of internally stored model, </p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p> description of prediction variables and response, </p>
</td></tr>
<tr><td><code>class.lev</code></td>
<td>
<p> class values for classification problem, null for regression problem, </p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p> the type of model used, see parameter <code>model</code>, </p>
</td></tr>
<tr><td><code>formula</code></td>
<td>
<p> the <code>formula</code> parameter passed. </p>
</td></tr>
</table>
<p>The function <code>cvCoreModel</code> evaluates the model using cross-validation and function <code><a href="#topic+modelEval">modelEval</a></code> to return
these additional components:
</p>
<table>
<tr><td><code>avgs</code></td>
<td>
<p>A vector with average values of each evaluation metric obtained from <code><a href="#topic+modelEval">modelEval</a></code>. </p>
</td></tr>
<tr><td><code>stds</code></td>
<td>
<p>A vector with standard deviations of each evaluation metric from <code><a href="#topic+modelEval">modelEval</a></code>. </p>
</td></tr>
<tr><td><code>evalList</code></td>
<td>
<p>A list, where each component is an evaluation metric from <code><a href="#topic+modelEval">modelEval</a></code>. Each component contains results of cross-validated runs.</p>
</td></tr>
</table>
<p>In case <code>returnModel=FALSE</code> the function only returns the above three components are keeps no model.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja, Petr Savicky </p>


<h3>References</h3>

<p>Marko Robnik-Sikonja, Igor Kononenko: Theoretical and Empirical Analysis of ReliefF and RReliefF.
<em>Machine Learning Journal</em>, 53:23-69, 2003
</p>
<p>Leo Breiman: Random Forests. <em>Machine Learning Journal</em>, 45:5-32, 2001
</p>
<p>Marko Robnik-Sikonja: Improving Random Forests.
In J.-F. Boulicaut et al.(Eds): <em>ECML 2004, LNAI 3210</em>, Springer, Berlin, 2004, pp. 359-370
</p>
<p>Marko Robnik-Sikonja: CORE - a system that predicts continuous variables.
<em>Proceedings of ERK'97</em> , Portoroz, Slovenia, 1997
</p>
<p>Marko Robnik-Sikonja, Igor Kononenko: Discretization of continuous attributes using ReliefF.
<em>Proceedings of ERK'95</em>, B149-152, Ljubljana, 1995
</p>
<p>Majority of these references are available from <a href="http://lkm.fri.uni-lj.si/rmarko/papers/">http://lkm.fri.uni-lj.si/rmarko/papers/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>,
<code><a href="#topic+modelEval">modelEval</a></code>,
<code><a href="#topic+attrEval">attrEval</a></code>,
<code><a href="#topic+helpCore">helpCore</a></code>,
<code><a href="#topic+paramCoreIO">paramCoreIO</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use iris data set
trainIdxs &lt;- sample(x=nrow(iris), size=0.7*nrow(iris), replace=FALSE)
testIdxs &lt;- c(1:nrow(iris))[-trainIdxs]

# build random forests model with certain parameters
# setting maxThreads to 0 or more than 1 forces 
# utilization of several processor cores 
modelRF &lt;- CoreModel(Species ~ ., iris[trainIdxs,], model="rf",
              selectionEstimator="MDL",minNodeWeightRF=5,
              rfNoTrees=100, maxThreads=1)
print(modelRF) # simple visualization, test also others with function plot
# prediction on testing set
pred &lt;- predict(modelRF, iris[testIdxs,], type="both") 
mEval &lt;- modelEval(modelRF, iris[["Species"]][testIdxs], pred$class, pred$prob)
print(mEval) # evaluation of the model
# visualization of individual predictions and the model
## Not run: 
require(ExplainPrediction)
explainVis(modelRF, iris[trainIdxs,], iris[testIdxs,], method="EXPLAIN",
           visLevel="model", problemName="iris", fileType="none", 
           classValue=1, displayColor="color") 
# turn on the history in visualization window to see all instances
explainVis(modelRF, iris[trainIdxs,], iris[testIdxs,], method="EXPLAIN",
           visLevel="instance", problemName="iris", fileType="none", 
           classValue=1, displayColor="color") 

## End(Not run)
destroyModels(modelRF) # clean up


# build decision tree with naive Bayes in the leaves
# more appropriate for large data sets one can specify just the target variable
modelDT &lt;- CoreModel("Species", iris, model="tree", modelType=4)
print(modelDT)
destroyModels(modelDT) # clean up


# build regression tree similar to CART
instReg &lt;- regDataGen(200)
modelRT &lt;- CoreModel(response~., instReg, model="regTree", modelTypeReg=1)
print(modelRT)
destroyModels(modelRT) # clean up

# build kNN kernel regressor by preventing tree splitting
modelKernel &lt;- CoreModel(response~., instReg, model="regTree",
                    modelTypeReg=7, minNodeWeightTree=Inf)
print(modelKernel)
destroyModels(modelKernel) # clean up

## Not run: 
# A more complex example 
# Test accuracy of random forest predictor with 20 trees on iris data
# using 10-fold cross-validation.
ncases &lt;- nrow(iris)
ind &lt;- ceiling(10*(1:ncases)/ncases)
ind &lt;- sample(ind,length(ind))
pred &lt;- rep(NA,ncases)
fit &lt;- NULL
for (i in unique(ind)) {
    # Delete the previous model, if there is one.
    fit &lt;- CoreModel(Species ~ ., iris[ind!=i,], model="rf", 
                     rfNoTrees=20, maxThreads=1)
    pred[ind==i] &lt;- predict(fit, iris[ind==i,], type="class")
    if (!is.null(fit)) destroyModels(fit) # dispose model no longer needed
 
}
table(pred,iris$Species)

## End(Not run)
# a simpler way to estimate performance using cross-validation
model &lt;- cvCoreModel(Species ~ ., iris, model="rf", rfNoTrees=20, 
                    folds=10, stratified=TRUE, returnModel=TRUE,
                    maxThreads=1)    
model$avgs           

</code></pre>

<hr>
<h2 id='cvGen'> Cross-validation and stratified cross-validation</h2><span id='topic+cvGen'></span><span id='topic+cvGenStratified'></span><span id='topic+gatherFromList'></span>

<h3>Description</h3>

<p>Generate indices for cross-validation and stratified cross-validation </p>


<h3>Usage</h3>

<pre><code class='language-R'>cvGen(n, k) 
cvGenStratified(classVal,k) 
gatherFromList(lst)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvGen_+3A_n">n</code></td>
<td>
<p> The number of instances in a data set.</p>
</td></tr>
<tr><td><code id="cvGen_+3A_k">k</code></td>
<td>
<p> The number of folds in cross-validation.</p>
</td></tr>
<tr><td><code id="cvGen_+3A_classval">classVal</code></td>
<td>
<p> A vector of factors representing class values.</p>
</td></tr>
<tr><td><code id="cvGen_+3A_lst">lst</code></td>
<td>
<p>A list of lists from which we collect results of the same components.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions <code>cvGen</code> and <code>cvGenStratified</code> generate indices of instances from a data set which can be used in cross-validation. 
The function <code>cvGenStratified</code> generates the same distribution of class values in each fold.
The function <code>gatherFromList</code> is an auxiliary function helping in collection of results, see the example below.
</p>


<h3>Value</h3>

<p>The functions <code>cvGen</code> and <code>cvGenStratified</code> return a vector of indices indicating fold membership i.e. from 1:k.
The function codegatherFromList returns a list with components containing elements of the same name.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- iris
folds &lt;- 10
foldIdx &lt;- cvGen(nrow(data), k=folds)
evalCore&lt;-list()
for (j in 1:folds) {
	dTrain &lt;- data[foldIdx!=j,]
	dTest  &lt;- data[foldIdx==j,]
	modelCore &lt;- CoreModel(Species~., dTrain, model="rf") 
	predCore &lt;- predict(modelCore, dTest)
	evalCore[[j]] &lt;- modelEval(modelCore, correctClass=dTest$Species,
	          predictedClass=predCore$class, predictedProb=predCore$prob ) 
	destroyModels(modelCore)
}
results &lt;- gatherFromList(evalCore)
sapply(results, mean)
</code></pre>

<hr>
<h2 id='destroyModels'>Destroy single model or all CORElearn models </h2><span id='topic+destroyModels'></span>

<h3>Description</h3>

<p>Destroys internal representation of a given model or all constructed models.
As side effect the memory used by the model(s) is freed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>destroyModels(model=NULL) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="destroyModels_+3A_model">model</code></td>
<td>
<p> The model structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>. The default value of NULL represents all generated models.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function destroys the <code>model</code> structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>.
Subsequent work with this model is no longer possible. 
If parameter <code>model=NULL</code> (default value) all generated models are destroyed and
memory used by their internal representation is freed.
</p>


<h3>Value</h3>

<p>There is no return value.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja, Petr Savicky </p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use iris data set

# build random forests model with certain parameters
model &lt;- CoreModel(Species ~ ., iris, model="rf", 
              selectionEstimator="MDL", minNodeWeightRF=5,
              rfNoTrees=100, maxThreads=1)

# prediction 
pred &lt;- predict(model, iris, rfPredictClass=FALSE)
# print(pred)

# destruction of model's internal representation
destroyModels(model)

</code></pre>

<hr>
<h2 id='discretize'> Discretization of numeric attributes </h2><span id='topic+discretize'></span><span id='topic+applyDiscretization'></span><span id='topic+intervalMidPoint'></span>

<h3>Description</h3>

<p>The method <code>discretize</code> returns discretization bounds for numeric attributes and two auxiliary functions. 
Discretization can be obtained with one of the three discretization methods: 
greedy search using given feature evaluation heuristics, equal width of intervals, or equal number of instances in each interval.
The attributes and target variable are specified using formula interface, target variable name or index.
Feature evaluation algorithms available for classification problems
are various variants of Relief and ReliefF algorithms, gain ratio, gini-index, MDL, DKM, information gain, etc.
For regression problems there are RREliefF, MSEofMean, MSEofModel, MAEofMode, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> discretize(formula, data, method=c("greedy", "equalFrequency", "equalWidth"), 
            estimator, discretizationLookahead=3, discretizationSample=0, 
            maxBins=0, equalDiscBins=4, ...)
 
 applyDiscretization(data, boundsList, noDecimalsInValueName=2)
    
 intervalMidPoint(data, boundsList, 
                  midPointMethod=c("equalFrequency", "equalWidth")) 
         
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discretize_+3A_formula">formula</code></td>
<td>
<p> Either a formula specifying the attributes to be evaluated and the target variable, or a name of target variable, or an index of target variable. </p>
</td></tr>
<tr><td><code id="discretize_+3A_data">data</code></td>
<td>
<p> Data frame with data. </p>
</td></tr>
<tr><td><code id="discretize_+3A_method">method</code></td>
<td>
<p> Three discretization methods are available. With <code>method="greedy"</code> 
greedy search using given feature evaluation heuristics is selected, while <code>"equalFrequency"</code> and <code>"equalWidth"</code>
select equal frequency (the same number of instances in each interval) and equal width discretization, respectively.  </p>
</td></tr>
<tr><td><code id="discretize_+3A_estimator">estimator</code></td>
<td>
<p> The name of the evaluation method.</p>
</td></tr>
<tr><td><code id="discretize_+3A_discretizationlookahead">discretizationLookahead</code></td>
<td>
<p>Discretization is performed with a greedy algorithm which adds a new boundary, until there is no
improvement in evaluation function for <code>discretizationLookahead</code> number of times
(0=try all possibilities). Candidate boundaries are chosen from a random sample of boundaries,
whose size is <code>discretizationSample</code>.</p>
</td></tr>
<tr><td><code id="discretize_+3A_discretizationsample">discretizationSample</code></td>
<td>
<p>Maximal number of points to try discretization (0=all sensible). Binarization of multivalued discrete features with
<code class="reqn">k</code> values is performed exhaustively, if <code class="reqn">2^k - 1</code> is at most <code>discretizationSample</code>. Otherwise binarization
is done greedily starting from the best separation of a single value.
For ReliefF-type measures, binarization of numeric features is performed with <code>discretizationSample</code> randomly
chosen splits. For other measures, the split is searched exhaustively among all possible splits.</p>
</td></tr>
<tr><td><code id="discretize_+3A_maxbins">maxBins</code></td>
<td>
<p>The maximal number of discrete bins for numeric attributes used for greedy discretization (0=don't care). 
This shall be an integer vector of length 
equal to the number of numeric attributes or an integer which applies to all numeric attributes. The default value of
0 means that the number of bins will be determined greedily taking into account <code>discretizationLookahead</code>.</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="discretize_+3A_equaldiscbins">equalDiscBins</code></td>
<td>
<p>The number of bins used in equal frequency and equal width discretization.
This shall be an integer vector of length 
equal to the number of numeric attributes or an integer which applies to all numeric attributes. The default value is 4.</p>
</td></tr>
<tr><td><code id="discretize_+3A_...">...</code></td>
<td>
<p> Additional options used by specific evaluation methods as described in <code><a href="#topic+helpCore">helpCore</a></code>.</p>
</td></tr>
<tr><td><code id="discretize_+3A_boundslist">boundsList</code></td>
<td>
<p>A list of numeric bounds which is applied to numeric attributes in <code>data</code> to produce
discrete attributes of type <code>factor</code>. Numeric bounds can be obtained by calling <code>discretize</code> function.</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="discretize_+3A_nodecimalsinvaluename">noDecimalsInValueName</code></td>
<td>
<p>With how many decimal places will the numeric feature values be presented in description (i.e., levels)
of feature values. The default value is 2, but will be increased if this is necessary to avoid the same description of feature values.</p>
</td></tr>
<tr><td><code id="discretize_+3A_midpointmethod">midPointMethod</code></td>
<td>
<p> Two methods to determine the middle points of discretization intervals are available. 
The <code>"equalFrequency"</code> method select the middle point so that each half-interval
contains equal number of instances.  The <code>"equalWidth"</code> methods sets middle point to be equally distant from the boundaries.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In method <code>discretize</code> the parameter <code>formula</code> can be interpreted in three ways, where the formula interface is the most elegant one, 
but inefficient and inappropriate for large data sets. See <code><a href="#topic+CoreModel">CoreModel</a></code> for details.
</p>
<p>The <b>estimator</b> parameter selects the evaluation heuristics.  For classification problem it 
must be one of the names returned by <code>infoCore(what="attrEval")</code> and for 
regression problem it must be one of the names returned by <code>infoCore(what="attrEvalReg")</code>.
For details see their description in <code><a href="#topic+attrEval">attrEval</a></code>.
</p>
<p>If the number of supplied vector in <code>maxBins</code> and <code>equalDiscBins</code> is shorter than the number of numeric attributes, the
vector is coerced to the required length.
</p>
<p>There are some additional parameters <b>... </b> available which are used by specific evaluation heuristics.
Their list and short description is available by calling <code><a href="#topic+helpCore">helpCore</a></code>. See Section on attribute evaluation.
</p>
<p>The function <code>applyDiscretization</code> takes the discretization bounds obtain with function <code>discretize</code> and transforms
numeric features in a data set into discrete features.
</p>
<p>The function <code>intervalMidPoint</code> takes discretization bounds provided by function <code>discretize</code> and returns
middle points of discretization intervals for numeric attributes. The middle points are computed from the data;
for lowest/highest interval the minimum/maximum of the values in the <code>data</code> for particular attribute
are implicitly taken as an additional left/right boundary point. 
</p>


<h3>Value</h3>

<p>The method <code>discretize</code> returns a list of discretization bounds for numeric attributes. One component of a list contains bounds for one attribute.
If an attribute has all values equal, value NA is returned. If an attribute has all values equal to NA, it is skipped in the returned list.
</p>
<p>The function <code>applyDiscretization</code> returns a data set where all numeric attributes are replaced with their discrete versions. 
</p>
<p>The function <code>intervalMidPoint</code> returns a list of vectors where each vector contains middle point of discretized intevals. 
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja </p>


<h3>References</h3>

 
<p>Marko Robnik-Sikonja, Igor Kononenko: Theoretical and Empirical Analysis of ReliefF and RReliefF.
<em>Machine Learning Journal</em>, 53:23-69, 2003
</p>
<p>Marko Robnik-Sikonja, Igor Kononenko: Discretization of continuous attributes using ReliefF. <em>Proceedings of ERK'95</em> , Portoroz, Slovenia, 1995.
</p>
<p>Some of these references are available also from <a href="http://lkm.fri.uni-lj.si/rmarko/papers/">http://lkm.fri.uni-lj.si/rmarko/papers/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+attrEval">attrEval</a></code>,
<code><a href="#topic+helpCore">helpCore</a></code>, 
<code><a href="#topic+infoCore">infoCore</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use iris data
# run method using estimator ReliefF with exponential rank distance  
discBounds &lt;- discretize(Species ~ ., iris, method="greedy", 
                         estimator="ReliefFexpRank")
print(discBounds)
discreteIris &lt;- applyDiscretization(iris, discBounds)
prototypePoints &lt;- intervalMidPoint(iris, discBounds, 
                                    midPointMethod="equalFrequency")

regData &lt;- regDataGen(200)
discretize(response ~ ., regData, method="greedy", estimator="RReliefFequalK", 
           maxBins=2)
 
# print all available estimators
#infoCore(what="attrEval")
#infoCore(what="attrEvalReg")


</code></pre>

<hr>
<h2 id='display.CoreModel'> Displaying decision and regression trees </h2><span id='topic+display.CoreModel'></span><span id='topic+display'></span>

<h3>Description</h3>

<p>The method <code>display</code> prints the tree models returned by <code>CoreModel()</code>
function. Depending of parameter <code>format</code> the output is prepared for either screen or in dot format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    ## S3 method for class 'CoreModel'
display(x, format=c("screen","dot"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="display.CoreModel_+3A_x">x</code></td>
<td>
<p>The model structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>.</p>
</td></tr>          
<tr><td><code id="display.CoreModel_+3A_format">format</code></td>
<td>
<p> The type of output, i.e., prepared for screen display or in dot language</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The tree based models returned by function <code><a href="#topic+CoreModel">CoreModel</a></code> are visualized.
Only tree based models supported, including the trees which include other prediction models in their leaves.
Tree based models available are decision trees (obtained by using parameter <code>model="tree"</code> in CoreModel),
and regression trees (with <code>model="regTree"</code>). 
</p>
<p>Models in the leaves of decision trees
can be set using parameter <code>modelType</code> in <code><a href="#topic+CoreModel">CoreModel</a></code>. 
At the moment naive Bayes and kNN are available, for details see <a href="#topic+helpCore">helpCore</a>.  
</p>
<p>Models in the leaves of regression trees can be set using parameter <code>modelTypeReg</code> in <code><a href="#topic+CoreModel">CoreModel</a></code>. 
At the moment kNN, kernel regression, and several types of linear models  are available,
for details see <a href="#topic+helpCore">helpCore</a>. 
</p>
<p>The output in dot language can be used with graphViz visualization software to create model
visualization in various formats. 
</p>


<h3>Value</h3>

<p>The method invisibly returns a printed character vector.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>See Also</h3>

<p><code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+plot.CoreModel">plot.CoreModel</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># decision tree
dataset &lt;- CO2
md &lt;- CoreModel(Plant ~ ., dataset, model="tree")
display(md)
destroyModels(md) #clean up

# regression tree
dataset &lt;- CO2
mdr &lt;- CoreModel(uptake ~ ., dataset, model="regTree")
display(mdr, format="dot")
destroyModels(mdr) # clean up

</code></pre>

<hr>
<h2 id='getCoreModel'> Conversion of model to a list  </h2><span id='topic+getCoreModel'></span>

<h3>Description</h3>

<p>Function converts given model from internal structures in C++ to R's data structures.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getCoreModel(model) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getCoreModel_+3A_model">model</code></td>
<td>
<p> The model structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function converts the model referenced by <code>model</code> from C++ internal structures 
to R's lists. Currently it is implemented only for random forests models.
</p>


<h3>Value</h3>

<p>For random forest a resulting list contains first all the information on the forest level, 
followed by the list of trees. For each tree the nodes are recursively nested with indication 
of node type (leaf or internal node) and than required information for that data type.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>See Also</h3>

<p><code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+CORElearn">CORElearn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># uses iris data set

# build random forests model with certain parameters, 
# do not make too many and too large trees
modelRF &lt;- CoreModel(Species ~ ., iris, model="rf", 
              selectionEstimator="MDL", minNodeWeightRF=50,
              rfNoTrees=5, maxThreads=1)
print(modelRF)

# get the structure of the forest 
forest &lt;- getCoreModel(modelRF) 
# forest

destroyModels(modelRF) # clean up
 
</code></pre>

<hr>
<h2 id='getRFsizes'> Get sizes of the trees in RF </h2><span id='topic+getRFsizes'></span>

<h3>Description</h3>

<p>Get numerical characteristics of the trees in a RF model related to the size
and depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getRFsizes(model, type=c("size", "sumdepth")) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getRFsizes_+3A_model">model</code></td>
<td>
<p> The model structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>. </p>
</td></tr>
<tr><td><code id="getRFsizes_+3A_type">type</code></td>
<td>
<p> The required characteristics. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Size is the number of leaves. The sum of depths means the sum of the depth
of all leaves.
</p>


<h3>Value</h3>

<p>Numerical vector of the length equal to the number of trees in RF.
</p>


<h3>Author(s)</h3>

<p> Petr Savicky</p>


<h3>See Also</h3>

<p><code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+CORElearn">CORElearn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># uses iris data set

# build random forests model with certain parameters, 
# do not make too many and too large trees
modelRF &lt;- CoreModel(Species ~ ., iris, model="rf", 
              selectionEstimator="MDL", minNodeWeightRF=50, 
              rfNoTrees=50, maxThreads=1)

getRFsizes(modelRF) 

destroyModels(modelRF) # clean up
</code></pre>

<hr>
<h2 id='getRpartModel'> Conversion of a CoreModel tree into a rpart.object </h2><span id='topic+getRpartModel'></span>

<h3>Description</h3>

<p> The function converts a given CoreModel model (decision or regression tree) 
into a <code>rpart.object</code> prepared for visualization with <code>plot</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  getRpartModel(model, dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getRpartModel_+3A_model">model</code></td>
<td>
<p> A tree model produced by <code><a href="#topic+CoreModel">CoreModel</a></code> </p>
</td></tr>
<tr><td><code id="getRpartModel_+3A_dataset">dataset</code></td>
<td>
<p> A data set which was used in learning of the <code>model</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The conversion creates <code>rpart.object</code> and copies CORElearn internal structures contained in memory controlled by 
dynamic link library written in C++. 
</p>
<p>An alternative visualization is accessible via function <code><a href="#topic+display">display</a></code>, which outputs tree structure formatted 
for screen or in dot format.
</p>


<h3>Value</h3>

<p> Function returns a <code>rpart.object</code>.</p>


<h3>Author(s)</h3>

<p> Initial version by John Adeyanju Alao, improvements by Marko Robnik-Sikonja.</p>


<h3>See Also</h3>

<p><code><a href="#topic+CoreModel">CoreModel</a></code>, <code><a href="#topic+plot.CoreModel">plot.CoreModel</a></code>,<code>rpart.object</code>, <code><a href="#topic+display">display</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># plot a decision tree directly
dataset &lt;- CO2
md&lt;-CoreModel(Plant ~ ., dataset, model="tree")
plot(md, dataset)

# or indirectly 
rpm &lt;- getRpartModel(md, dataset)
# set angle to tan(0.5)=45 (degrees) and length of branches at least 5 
plot(rpm, branch=0.5, minbranch=5, compress=TRUE)
# pretty=0 prints full names of attributes, 
# numbers to 3 decimals, try to make a dendrogram more compact
text(rpm, pretty=0, digits=3)
destroyModels(md) # clean up

# an alternative is to use fancier rpart.plot package
# rpart.plot(rpm) # rpart.plot has many parameters controlling the output
# but it cannot plot models in tree leaves 
</code></pre>

<hr>
<h2 id='helpCore'> Description of parameters. </h2><span id='topic+helpCore'></span><span id='topic+help.Core'></span>

<h3>Description</h3>

<p>The behavior of CORElearn is controlled by several parameters. This is a short overview.  
</p>


<h3>Details</h3>

<p>There are many different parameters available. Some are general and can be used in many
learning, or feature evaluation algorithms. All the values actually used by 
the classifier / regressor can be written to file (or read from it) using
<code><a href="#topic+paramCoreIO">paramCoreIO</a></code>.
The parameters for the methods are split into several groups and documented below.
</p>


<h3>Attribute/feature evaluation</h3>

<p>The parameters in this group may be used inside model construction 
via <code><a href="#topic+CoreModel">CoreModel</a></code> and feature evaluation in <code><a href="#topic+attrEval">attrEval</a></code>. See <code><a href="#topic+attrEval">attrEval</a></code>
for description of relevant evaluation methods. 
</p>
<p>Parameters <code>attrEvaluationInstances</code>, <code>binaryEvaluation</code>, <br />
<code>binarySplitNumericAttributes</code>
are applicable to all attribute evaluation methods. In models which need feature evaluation (e.g., trees,
random forests) they affect the selection of splits in the nodes.
Other parameters may be used only in context sensitive measures, i.e., ReliefF in classification
and RReliefF in regression and their variants.  
</p>

<dl>
<dt>binaryEvaluation</dt><dd><p>type: logical, default value: FALSE <br />
Shall we treat all attributes as binary and binarize them before evaluation if necessary.
If <code>TRUE</code>, then for all multivalued discrete and all numeric features a search for the
best binarization is performed. The evaluation of the best binarization found is reported.
If <code>FALSE</code>, then multivalued discrete features are evaluated &quot;as is&quot; with multivalued versions
of estimators. With ReliefF-type measures, numeric features are also evaluated &quot;as is&quot;. For evaluation
of numeric features with other (non-ReliefF-type) measures, they are first binarized or discretized.
The choice between binarization and discretization is controlled by 
<code>binaryEvaluateNumericAttributes</code>. Due to performance reasons it is recommended that 
<code>binaryEvaluation=FALSE</code> is used.
See also <code>discretizationSample</code>. </p>
</dd>
<dt>binaryEvaluateNumericAttributes</dt><dd><p>type: logical, default value: TRUE <br />
ReliefF like measures can evaluate numeric attributes intrinsically, others
have to discretize or binarize them before evaluation; for those measures
this parameter selects binarization (default) or discretization (computationally more demanding). </p>
</dd>
<dt>multiclassEvaluation</dt><dd><p>type: integer, default value: 1, value range: 1, 4 <br /> 
multi-class extension for two-class-only evaluation measures 
(1-average of all-pairs, 2-best of all-pairs, 
3-average of one-against-all, 4-best of one-against-all). </p>
</dd>
<dt>attrEvaluationInstances</dt><dd><p>type: integer, default value: 0, value range: 0, Inf <br />
number of instances for attribute evaluation (0=all available). </p>
</dd>
<dt>minNodeWeightEst</dt><dd><p>type: numeric, default value: 2, value range: 0, Inf <br />
minimal number of instances (weight) in resulting split to take it in consideration. </p>
</dd>          
<dt>ReliefIterations</dt><dd><p>type: integer, default value: 0, value range: -2, Inf <br /> 
number of iterations for all variants of Relief (0=DataSize, -1=ln(DataSize) -2=sqrt(DataSize)). </p>
</dd>
<dt>numAttrProportionEqual</dt><dd><p>type: numeric, default value: 0.04, value range: 0, 1 <br />
used in ramp function, proportion of numerical attribute's range to consider two values equal. </p>
</dd>
<dt>numAttrProportionDifferent</dt><dd><p>type: numeric, default value: 0.1, value range: 0, 1 <br />
used in ramp function, proportion of numerical attribute's range to consider two values different. </p>
</dd>
<dt>kNearestEqual</dt><dd><p>type: integer, default value: 10, value range: 0, Inf <br />
number of neighbors to consider in equal k-nearest attribute evaluation. </p>
</dd>
<dt>kNearestExpRank</dt><dd><p>type: integer, default value: 70, value range: 0, Inf <br />
number of neighbors to consider in exponential rank distance attribute evaluation. </p>
</dd>
<dt>quotientExpRankDistance</dt><dd><p>type: numeric, default value: 20, value range: 0, Inf <br />
quotient in exponential rank distance attribute evaluation. </p>
</dd>
</dl>



<h3>Decision/regression tree construction</h3>

<p>There are several parameters controlling a construction of the tree model. Some are described here,
but also attribute evaluation, stop building, model, constructive induction, discretization, 
and pruning options described in this document are applicable.    
Splits in trees are always binary, however, the option <code>binaryEvaluation</code> has influence on the
feature selection for the split. Namely, selecting the best feature for the split is done with the given
value of <code>binaryEvaluation</code>. If <code>binaryEvaluation=FALSE</code>, the features are first evaluated and
the best one is finally binarized. If <code>binaryEvaluation=TRUE</code>, the features are binarized before
selection. In this case, a search for the best binarization for all considered features is performed and
the best binarizations found are used for splits. The latter option is computationally more intensive,
but typically does not produce better trees. 
</p>

<dl>
<dt>selectionEstimator</dt><dd><p>type: character, default value: &quot;MDL&quot;, possible values: all from <code><a href="#topic+attrEval">attrEval</a></code>, section classification <br />
estimator for selection of attributes and binarization in classification. </p>
</dd>
<dt>selectionEstimatorReg</dt><dd><p>type: character, default value: &quot;RReliefFexpRank&quot;, possible values: all from <code><a href="#topic+attrEval">attrEval</a></code>, section regression <br />
estimator for selection of attributes and binarization in regression. </p>
</dd>
<dt>minReliefEstimate</dt><dd><p>type: numeric, default value: 0, value range: -1, 1 <br />
for all variants of Relief attribute estimator: the minimal evaluation of attribute to consider the attribute useful in further processing. </p>
</dd>
<dt>minInstanceWeight</dt><dd><p>type: numeric, default value: 0.05, value range: 0, 1 <br />  minimal weight of an instance to use it further in splitting. </p>
</dd>
</dl>



<h3>Stop tree building</h3>

<p>During tree construction the node is recursively split, until certain condition is fulfilled.
</p>

<dl>
<dt>minNodeWeightTree</dt><dd><p>type: numeric, default value: 5, value range: 0, Inf <br />
minimal number of instances (weight) of a leaf in the decision or regression tree model. </p>
</dd>
<dt>minNodeWeightRF</dt><dd><p>type: numeric, default value: 2, value range: 0, Inf <br />
minimal number of instances (weight) of a leaf in the random forest tree. </p>
</dd>
<dt>relMinNodeWeight</dt><dd><p>type: numeric, default value: 0, value range: 0, 1 <br />
minimal proportion of training instances in a tree node to split it further. </p>
</dd>
<dt>majorClassProportion</dt><dd><p>type: numeric, default value: 1, value range: 0, 1 <br />
proportion of majority class in a classification tree node to stop splitting it. </p>
</dd>
<dt>rootStdDevProportion</dt><dd><p>type: numeric, default value: 0, value range: 0, 1 <br />
proportion of root's standard deviation in a regression tree node to stop splitting it. </p>
</dd>
</dl>



<h3>Models in the tree leaves</h3>

<p>In leaves of the tree model there can be various prediction models controlling prediction. For example instead of classification with 
majority of class values one can use naive Bayes in classification, or a linear model in regression, thereby expanding 
expressive power of the tree model. 
</p>

<dl>
<dt>modelType</dt><dd><p>type: integer, default value: 1, value range: 1, 4 <br />
type of models used in classification tree leaves (1=majority class, 2=k-nearest neighbors, 3=k-nearest neighbors with kernel, 4=naive Bayes). </p>
</dd>
<dt>modelTypeReg</dt><dd><p>type: integer, default value: 5, value range: 1, 8 <br />
type of models used in regression tree leaves (1=mean predicted value, 2=median predicted value, 3=linear by MSE, 
4=linear by MDL, 5=linear reduced as in M5, 6=kNN, 7=Gaussian kernel regression, 8=locally weighted linear regression). </p>
</dd>
<dt>kInNN</dt><dd><p>type: integer, default value: 10, value range: 0, Inf <br />
number of neighbors in k-nearest neighbors models (0=all). </p>
</dd>
<dt>nnKernelWidth</dt><dd><p>type: numeric, default value: 2, value range: 0, Inf <br />  kernel width in k-nearest neighbors models. </p>
</dd>
<dt>bayesDiscretization</dt><dd><p>type: integer, default value: 2, value range: 1, 3 <br />
type of discretization for naive Bayesian models (1=greedy with selection estimator, 2=equal frequency, 3=equal width). </p>
</dd>
<dt>discretizationIntervals</dt><dd><p>type: integer, default value: 4, value range: 1, Inf <br />
number of intervals in equal frequency or equal width discretizations. </p>
</dd>
</dl>



<h3>Constructive induction aka. feature construction</h3>

<p>The expressive power of tree models can be increased by incorporating additional types of splits. Operator based
constructive induction is implemented in both classification and regression. The best construct is searched with beam search. 
At each step new constructs are evaluated with selected feature evaluation measure.
With different types of operators one can control expressions in the interior tree nodes.
</p>

<dl>
<dt>constructionMode</dt><dd><p>type: integer, default value: 15, value range: 1, 15 <br /> 
sum of constructive operators (1=single attributes, 2=conjunction, 4=addition, 8=multiplication); all=1+2+4+8=15 </p>
</dd>
<dt>constructionDepth</dt><dd><p>type: integer, default value: 0, value range: 0, Inf <br />
maximal depth of the tree for constructive induction (0=do not do construction, 1=only at root, ...). </p>
</dd>
<dt>noCachedInNode</dt><dd><p>type: integer, default value: 5, value range: 0, Inf <br />
number of cached attributes in each node where construction was performed. </p>
</dd>
<dt>constructionEstimator</dt><dd><p>type: character, default value: &quot;MDL&quot;, possible values: all from <code><a href="#topic+attrEval">attrEval</a></code>, section classification <br />
estimator for constructive induction in classification. </p>
</dd>
<dt>constructionEstimatorReg</dt><dd><p>type: character, default value: &quot;RReliefFexpRank&quot;, possible values: all from <code><a href="#topic+attrEval">attrEval</a></code>, section regression <br />
estimator for constructive induction in regression. </p>
</dd>
<dt>beamSize</dt><dd><p>type: integer, default value: 20, value range: 1, Inf <br />     size of the beam in search for best feature in constructive induction. </p>
</dd>
<dt>maxConstructSize</dt><dd><p>type: integer, default value: 3, value range: 1, Inf <br />
maximal size of constructs in constructive induction. </p>
</dd>
</dl>



<h3>Attribute discretization and binarization</h3>

<p>Some algorithms cannot deal with numeric attributes directly, so we have to discretize them. Also the tree models use
binary splits in nodes. The discretization  algorithm  evaluates split candidates and forms intervals of values. 
Note that setting <code>discretizationSample=1</code> will force random selection of splitting point, which will speed-up the algorithm
and may be perfectly acceptable for random forest ensembles.
</p>
<p>CORElearn builds binary trees so multivalued  discrete attributes have to be binarized i.e., values have to be split into 
twoa subset, one going left and the other going right in a node. The method used depends on the parameters 
and the number of attribute values. Possible methods are exhaustive (if the number of attribute values is less or equal  
<code>maxValues4Exhaustive</code>), greedy ((if the number of attribute values is less or equal <code>maxValues4Greedy</code>)
and random ((if the number of attribute values is more than <code>maxValues4Exhaustive</code>). 
Setting <code>maxValues4Greedy=2</code> will always randomly selet splitting point. 
</p>

<dl>
<dt>discretizationLookahead</dt><dd><p>type: integer, default value: 3, value range: 0, Inf <br />
Discretization is performed with a greedy algorithm which adds a new boundary, until there is no
improvement in evaluation function for <code>discretizationLookahead</code> number of times
(0=try all possibilities). Candidate boundaries are chosen from a random sample of boundaries,
whose size is <code>discretizationSample</code>. </p>
</dd>
<dt>discretizationSample</dt><dd><p>type: integer, default value: 50, value range: 0, Inf <br />
Maximal number of points to try discretization (0=all sensible). 
For ReliefF-type measures, binarization of numeric features is performed with <code>discretizationSample</code> randomly
chosen splits. For other measures, the split is searched among all possible splits.
</p>
</dd>
<dt>maxValues4Exhaustive</dt><dd><p>type: integer, default value: 7, value range: 2, Inf <br />
Maximal number of values of a discrete attribute to try finding split exhaustively.
If the attribute has more values the split will be searched greedily or selected ranomly based on the 
value of parameter <code>maxValues4Greedy</code>.
</p>
</dd>
<dt>maxValues4Greedy</dt><dd><p>type: integer, default value: 30, value range: 2, Inf <br />
Maximal number of values of a discrete attribute to try finding split greedily.
If the attribute has more values the split will be selected ranomly. Setting this parameter to 2 will
force random but balanced selection of splits which may be acceptable for random forest ensembles and will greatly
speed-up tree construction.
</p>
</dd>     
</dl>



<h3>Tree pruning</h3>

<p>After the tree is constructed, to reduce noise it is beneficial to prune it.
</p>

<dl>
<dt>selectedPruner</dt><dd><p>type: integer, default value: 1, value range: 0, 1 <br />  decision tree pruning method used (0=none, 1=with m-estimate). </p>
</dd>
<dt>selectedPrunerReg</dt><dd><p>type: integer, default value: 2, value range: 0, 4 <br />
regression tree pruning method used (0=none, 1=MDL, 2=with m-estimate, 3=as in M5, 4=error complexity as in CART (fixed alpha)). </p>
</dd>
<dt>mdlModelPrecision</dt><dd><p>type: numeric, default value: 0.1, value range: 0, Inf <br /> precision of model coefficients in MDL tree pruning. </p>
</dd>
<dt>mdlErrorPrecision</dt><dd><p>type: numeric, default value: 0.01, value range: 0, Inf <br /> precision of errors in MDL tree pruning. </p>
</dd>
<dt>mEstPruning</dt><dd><p>type: numeric, default value: 2, value range: 0, Inf <br /> m-estimate for pruning with m-estimate. </p>
</dd>
<dt>alphaErrorComplexity</dt><dd><p>type: numeric, default value: 0, value range: 0, Inf <br /> alpha for error complexity pruning. </p>
</dd>
</dl>



<h3>Prediction</h3>

<p>For some models (decision trees, random forests, naive Bayes, and regression trees) one can smoothe the output predictions.
In classification models output probabilities are smoothed and in case of regression prediction value is smoothed. 
</p>

<dl>
<dt>smoothingType</dt><dd><p>type: integer, default value: 0, value range: 0, 4 <br /> default value 0
means no smoothing  (in case classification one gets relative frequencies),
value 1 stands for additive smoothing, 2 is pure Laplace's smoothing, 3 is m-estimate smoothing, and 4 means
Zadrozny-Elkan type of m-estimate smoothing where <code>smoothingValue</code> is interpreted as 
<code class="reqn">m\cdot p_c</code> and <code class="reqn">p_c</code> is the prior probability of
the least probable class value; 
for regression <code>smoothingType</code> has no effect, as the smoothing is controlled solely by <code>smoothingValue</code>.
</p>
</dd>
<dt>smoothingValue</dt><dd><p>type: numeric, default value: 0, value range: 0, Inf <br /> 
additional parameter for some sorts of smoothing; in classification it is needed for additive, m-estimate, 
and Zadrozny-Elkan type of smoothing; 
in case of regression trees 0 means no smoothing and values larger than 0 change prediction value towards
the prediction of the models in ascendant nodes.
</p>
</dd>
</dl>



<h3>Random forests</h3>

<p>Random forest is quite complex model, whose construction one can control with several parameters.
Momentarily only classification version of the algorithm is implemented.
Besides parameters in this section one can apply majority of parameters for control of decision trees (except constructive induction and tree pruning).  
</p>

<dl>
<dt>rfNoTrees</dt><dd><p>type: integer, default value: 100, value range: 1, Inf <br /> number of trees in the random forest. </p>
</dd>
<dt>rfNoSelAttr</dt><dd><p>type: integer, default value: 0, value range: -2, Inf <br /> 
number of randomly selected attributes in the node (0=sqrt(numOfAttr), -1=log2(numOfAttr)+1, -2=all). </p>
</dd>
<dt>rfMultipleEst</dt><dd><p>type: logical, default value: FALSE <br />
use multiple attribute estimators in the forest? If TRUE the algorithm uses some preselected attribute evaluation measures on different trees. </p>
</dd>
<dt>rfkNearestEqual</dt><dd><p>type: integer, default value: 30, value range: 0, Inf <br />
number of nearest intances for weighted random forest classification (0=no weighing). </p>
</dd>
<dt>rfPropWeightedTrees</dt><dd><p>type: numeric, default value: 0, value range: 0, 1 <br />
Proportion of trees where attribute probabilities are weighted with their quality. As attribute weighting might reduce the variance between the models, 
the default value switches the weighing off. </p>
</dd>
<dt>rfPredictClass</dt><dd><p>type: logical, default value: FALSE <br />
shall individual trees predict with majority class (otherwise with class distribution). </p>
</dd>     
</dl>



<h3>General tree ensembles</h3>

<p>In the same manner as random forests more general tree ensembles can be constructed. Additional options control sampling,
tree size and regularization. 
</p>

<dl>
<dt>rfSampleProp</dt><dd><p>type: numeric, default value: 0, value range: 0, 1 <br />
proportion of the training set to be used in learning (0=bootstrap replication). </p>
</dd>
<dt>rfNoTerminals</dt><dd><p>type: integer, default value: 0, value range: 0, Inf <br />  maximal number of leaves in each tree (0=build the whole tree). </p>
</dd>
<dt>rfRegType</dt><dd><p>type: integer, default value: 2, value range: 0, 2 <br />
type of regularization (0=no regularization, 1=global regularization, 2=local regularization). </p>
</dd>
<dt>rfRegLambda</dt><dd><p>type: numeric, default value: 0, value range: 0, Inf <br /> regularization parameter lambda (0=no regularization). </p>
</dd>
</dl>



<h3>Read data directly from files</h3>

<p>In case of very large data sets it is useful to bypass <span class="rlang"><b>R</b></span> and read data directly from files as the standalone learning system CORElearn 
does. Supported file formats are C4.5, M5, and native format of CORElearn. See documentation at <a href="http://lkm.fri.uni-lj.si/rmarko/software/">http://lkm.fri.uni-lj.si/rmarko/software/</a>.
</p>

<dl>
<dt>domainName</dt><dd><p>type: character, <br /> name of a problem to read from files with suffixes .dsc, .dat, .names, .data, .cm, and .costs </p>
</dd>
<dt>dataDirectory</dt><dd><p>type: character, <br /> folder where data files are stored. </p>
</dd>   
<dt>NAstring</dt><dd><p>type: character, default value: &quot;?&quot; <br />
character string which represents missing and NA values in the data files. </p>
</dd>     
</dl>
  


<h3>Miscellaneous</h3>


<dl>
<dt>maxThreads</dt><dd><p>type: integer, default value: 0, value range: 0, Inf <br /> maximal number of active threads (0=allow OpenMP to set its defaults).
<br /> As side effect, this parameter changes the number of active threads in all subsequent execution (till <code>maxThreads</code> is set again). </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja, Petr Savicky </p>


<h3>References</h3>

<p>B. Zadrozny, C. Elkan. Learning and making decisions when costs and probabilities are both unknown.
In Proceedings of the Seventh International Conference on Knowledge Discovery and Data Mining, 2001.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>,
<code><a href="#topic+attrEval">attrEval</a></code>,
<code><a href="#topic+ordEval">ordEval</a></code>,
<code><a href="#topic+paramCoreIO">paramCoreIO</a></code>.
</p>

<hr>
<h2 id='infoCore'> Description of certain CORElearn parameters</h2><span id='topic+infoCore'></span>

<h3>Description</h3>

<p>Depending on parameter <code>what</code> the function prints some information on CORElearn,
for example codes of available classification (or regression) attribute evaluation heuristics.
For more complete description of the parameters see <code><a href="#topic+helpCore">helpCore</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>infoCore(what=c("attrEval","attrEvalReg"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="infoCore_+3A_what">what</code></td>
<td>
<p>Selects the info to be printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Depending on the parameter <code>what</code> the function some information on CORElearn.
</p>

<dl>
<dt>&quot;attrEval&quot;</dt><dd><p>Prints codes of all available classification attribute evaluation heuristics. 
These codes can be used as parameters for attribute evaluation methods in learning. 
It is internally used for validation of parameters. For more complete 
information see <code><a href="#topic+attrEval">attrEval</a></code>. </p>
</dd>
<dt>&quot;attrEvalReg&quot;</dt><dd><p>prints codes of all available regression attribute evaluation heuristics. These codes can be used as parameters for attribute evaluation 
methods in learning. It is internally used for validation of parameters. For more complete 
information see <code><a href="#topic+attrEval">attrEval</a></code>.</p>
</dd>
</dl>



<h3>Value</h3>

<p>For <code>what="attrEval"</code> or <code>"attrEvalReg"</code> function returns vector of codes for all 
implemented classification or regression attribute evaluation heuristics, respectively. 
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>See Also</h3>

<p><code><a href="#topic+attrEval">attrEval</a></code>,
<code><a href="#topic+helpCore">helpCore</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>estClass &lt;- infoCore(what="attrEval")
print(estClass)
infoCore(what="attrEvalReg")

</code></pre>

<hr>
<h2 id='modelEval'> Statistical evaluation of predictions </h2><span id='topic+modelEval'></span>

<h3>Description</h3>

<p>Using predictions of given model produced by <code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code> and correct labels,
computes  some statistics evaluating the quality of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelEval(model=NULL, correctClass, predictedClass, 
          predictedProb=NULL, costMatrix=NULL, 
          priorClProb = NULL, avgTrainPrediction = NULL, beta = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modelEval_+3A_model">model</code></td>
<td>
<p> The model structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>, or NULL if some other predictions are evaluated. </p>
</td></tr>
<tr><td><code id="modelEval_+3A_correctclass">correctClass</code></td>
<td>
<p> A vector of correct class labels for classification problem and function values for regression problem. </p>
</td></tr>
<tr><td><code id="modelEval_+3A_predictedclass">predictedClass</code></td>
<td>
<p> A vector of predicted class labels for classification problem and function values for regression problem. </p>
</td></tr>
<tr><td><code id="modelEval_+3A_predictedprob">predictedProb</code></td>
<td>
<p> An optional matrix of predicted class probabilities for classification.</p>
</td></tr>
<tr><td><code id="modelEval_+3A_costmatrix">costMatrix</code></td>
<td>
<p> Optional cost matrix can provide nonuniform costs for classification problems. </p>
</td></tr>
<tr><td><code id="modelEval_+3A_priorclprob">priorClProb</code></td>
<td>
<p> If <code>model=NULL</code> a vector of prior class probabilities shall be provided in case of classification. </p>
</td></tr>
<tr><td><code id="modelEval_+3A_avgtrainprediction">avgTrainPrediction</code></td>
<td>
<p> If <code>model=NULL</code> mean of prediction values on training set shall be provided in case of regression. </p>
</td></tr>
<tr><td><code id="modelEval_+3A_beta">beta</code></td>
<td>
<p>For two class problems <code>beta</code> controls the relative importance of precision and recall in F-measure.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses the <code>model</code> structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>,
<code>predictedClass</code> and <code>predictedProb</code> returned by 
<code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>. Predicted values are compared with true values 
and some statistics are computed  measuring the quality of predictions.
In classification only one of the <code>predictedClass</code> and <code>predictedProb</code> can be NULL
(one of them is computed from the other under assumption that class label is assigned to the most probable class). 
Some of the returned statistics  are defined only for two class problems, for which the 
confusion matrix specifying the number of instances of true/predicted class is
defined as follows,
</p>

<table>
<tr>
 <td style="text-align: left;">
         true/predicted class </td><td style="text-align: center;">  positive           </td><td style="text-align: center;"> negative     </td>
</tr>
<tr>
 <td style="text-align: left;">
                   positive   </td><td style="text-align: center;"> true positive (TP)  </td><td style="text-align: center;"> false negative (FN) </td>
</tr>
<tr>
 <td style="text-align: left;">
                   negative   </td><td style="text-align: center;"> false positive (FP) </td><td style="text-align: center;"> true negative (TN)
       </td>
</tr>

</table>

<p>Optional cost matrix can provide nonuniform costs for classification problems. For regression
problem this parameter is ignored. The costs can be different from the ones used for building the model 
in <code><a href="#topic+CoreModel">CoreModel</a></code> and prediction with the model in <code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>.
If no costs are supplied, uniform costs are assumed. 
The format of the matrix is <code>costMatrix(true_class, predicted_class)</code>. 
By default a uniform costs are assumed, i.e.,  <code>costMatrix(i, i) = 0</code>, and <code>costMatrix(i, j) = 1</code>, 
for <code>i</code> not equal to <code>j</code>. See the example below.
</p>
<p>If a non-CORElearn model is evaluated, one should set <code>model=NULL</code>, and  a vector of prior of class 
probabilities <code>priorClProb</code> shall be provided in case of classification,
and in case of regression <code>avgTrainPrediction</code> shall be the mean of prediction values 
(estimated on a e.g., training set).
</p>


<h3>Value</h3>

<p>For classification problem function returns list with the components
</p>
<table>
<tr><td><code>accuracy</code></td>
<td>
<p>classification accuracy, for two class problems this would equal 
</p>
<p style="text-align: center;"><code class="reqn">\rm{accuracy}=\frac{TP+TN}{TP+FN+FP+TN}</code>
</p>
 </td></tr>
<tr><td><code>averageCost</code></td>
<td>
<p>average classification cost</p>
</td></tr>
<tr><td><code>informationScore</code></td>
<td>
<p>information score statistics measuring information contents in the predicted probabilities</p>
</td></tr>
<tr><td><code>AUC</code></td>
<td>
<p>Area under the ROC curve</p>
</td></tr>
<tr><td><code>predictionMatrix</code></td>
<td>
<p>matrix of miss-classifications also confusion matrix</p>
</td></tr>
<tr><td><code>sensitivity</code></td>
<td>
<p>sensitivity for two class problems (also called accuracy of the positive class, i.e., acc+, or true positive rate),
</p>
<p style="text-align: center;"><code class="reqn">rm{sensitivity} = \frac{TP}{TP+FN}</code>
</p>
</td></tr>
<tr><td><code>specificity</code></td>
<td>
<p>specificity for two class problems (also called accuracy of the negative class, i.e., acc-, or true negative rate),
</p>
<p style="text-align: center;"><code class="reqn">\rm{specificity} = \frac{TN}{TN+FP}</code>
</p>
</td></tr>
<tr><td><code>brierScore</code></td>
<td>
<p>Brier score of predicted probabilities (the original Brier's definition which scores all the classes not only the correct one)</p>
</td></tr>
<tr><td><code>kappa</code></td>
<td>
<p>Cohen's kappa statistics measuring randomness of the predictions; for perfect predictions kappa=1, for completely random predictions kappa=0 </p>
</td></tr>
<tr><td><code>precision</code></td>
<td>
<p>precision for two class problems
</p>
<p style="text-align: center;"><code class="reqn">\rm{precision} = \frac{TP}{TP+FP}</code>
</p>
</td></tr>
<tr><td><code>recall</code></td>
<td>
<p>recall for two class problems (the same as sensitivity) </p>
</td></tr>
<tr><td><code>F-measure</code></td>
<td>
<p>F-measure giving a weighted score of precision and recall for two class problems
</p>
<p style="text-align: center;"><code class="reqn">F= \frac{(1+\beta^2)\cdot \rm{recall} \cdot \rm{precision}}{\beta^2 \cdot \rm{recall} + \rm{precision}}</code>
</p>
 </td></tr>
<tr><td><code>G-mean</code></td>
<td>
<p>geometric mean of positive and negative accuracy,
</p>
<p style="text-align: center;"><code class="reqn">G=\sqrt{\rm{senstivity} \cdot \rm{specificity}} </code>
</p>
</td></tr>
<tr><td><code>KS</code></td>
<td>
<p>Kolmogorov-Smirnov statistics defined for binary classification problems, reports the distance between the probability distributions of positive class
for positive and negative instances, see (Hand, 2005), value 0 means no separation, and value 1 means perfect separation,
</p>
<p style="text-align: center;"><code class="reqn">KS = \max_t |TPR(t)-FPR(t)|</code>
</p>
 
<p>see definitions of TPR and FPR below</p>
</td></tr>
<tr><td><code>TPR</code></td>
<td>
<p>true positive rate <code class="reqn">TPR = \frac{TP}{TP+FN}</code> at maximal value of <code>KS</code> statistics</p>
</td></tr>   
<tr><td><code>FPR</code></td>
<td>
<p>false positive rate <code class="reqn">FPR = \frac{FP}{FP+TN}</code> at maximal value of <code>KS</code> statistics</p>
</td></tr>   
</table>
<p>For regression problem the returned list has components
</p>
<table>
<tr><td><code>MSE</code></td>
<td>
<p>square root of Mean Squared Error</p>
</td></tr>
<tr><td><code>RMSE</code></td>
<td>
<p>Relative Mean Squared Error</p>
</td></tr> 
<tr><td><code>MAE</code></td>
<td>
<p>Mean Absolute Error</p>
</td></tr>
<tr><td><code>RMAE</code></td>
<td>
<p>Relative Mean Absolute Error</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>References</h3>

 
<p>Igor Kononenko, Matjaz Kukar: <em>Machine Learning and Data Mining: Introduction to Principles and Algorithms. </em>
Horwood, 2007
</p>
<p>David J.Hand: Good practice in retail credit scorecard assesment. <em>Journal of Operational Research Society</em>, 56:1109-1117, 2005)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+predict.CoreModel">predict.CoreModel</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use iris data

# build random forests model with certain parameters
model &lt;- CoreModel(Species ~ ., iris, model="rf", 
              selectionEstimator="MDL",minNodeWeightRF=5,
              rfNoTrees=100, maxThreads=1)

# prediction with node distribution
pred &lt;- predict(model, iris, rfPredictClass=FALSE)

# Model evaluation
mEval &lt;- modelEval(model, iris[["Species"]], pred$class, pred$prob)
print(mEval)

# use nonuniform cost matrix
noClasses &lt;- length(levels(iris[["Species"]]))
costMatrix &lt;- 1 - diag(noClasses)
costMatrix[3,1] &lt;- costMatrix[3,2] &lt;- 5 # assume class 3 is more valuable  
mEvalCost &lt;- modelEval(model, iris[["Species"]], pred$class, pred$prob, 
                       costMatrix=costMatrix)
print(mEvalCost)

destroyModels(model) # clean up

</code></pre>

<hr>
<h2 id='noEqualRows'> Number of equal rows in two data sets </h2><span id='topic+noEqualRows'></span>

<h3>Description</h3>

<p>Counts number of equal rows in two data sets. The two data sets shall have equal number of columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> noEqualRows(data1, data2, tolerance=1e-5, countOnce=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="noEqualRows_+3A_data1">data1</code></td>
<td>
<p> The first data set.</p>
</td></tr>
<tr><td><code id="noEqualRows_+3A_data2">data2</code></td>
<td>
<p> The second data set.</p>
</td></tr>
<tr><td><code id="noEqualRows_+3A_tolerance">tolerance</code></td>
<td>
<p> Tolerated difference between two rows.</p>
</td></tr>
<tr><td><code id="noEqualRows_+3A_countonce">countOnce</code></td>
<td>
<p> Shall each equal row in data1 be counted just once, or number of rows it is equal to in data2.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Rows are compared using column-wise comparisons. The sum of differences up to a given tolerance are tolerated.
</p>


<h3>Value</h3>

<p>Integer value giving the count of equal instances.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># uses two randomly generated data sets
set.seed(12345)
d1 &lt;- classDataGen(100)
d2 &lt;- classDataGen(100)
noEqualRows(d1, d2, tolerance=1e-4)
</code></pre>

<hr>
<h2 id='ordDataGen'>Artificial data for testing ordEval algorithms</h2><span id='topic+ordDataGen'></span>

<h3>Description</h3>

<p>The generator produces ordinal data simulating different profiles of attributes:
basic, performance, excitement and irrelevant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ordDataGen(noInst, classNoise=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ordDataGen_+3A_noinst">noInst</code></td>
<td>
<p>Number of instances to generate.</p>
</td></tr>
<tr><td><code id="ordDataGen_+3A_classnoise">classNoise</code></td>
<td>
<p>Proportion of randomly determined values in the class variable.</p>
</td></tr>   
</table>


<h3>Details</h3>

<p>Problem is
described by six important and two
irrelevant features. The important features correspond to different
feature types from the marketing theory:  two basic features
(<code class="reqn">B_{weak}</code> and <code class="reqn">B_{strong}</code>), two performance features (<code class="reqn">P_{weak}</code>
and <code class="reqn">P_{strong}</code>), two excitement features (<code class="reqn">E_{weak}</code> and
<code class="reqn">E_{strong}</code>), and two irrelevant features (<code class="reqn">I_{uniform}</code> and
<code class="reqn">I_{normal}</code>). The values of all features are randomly generated
integer values from 1 to 5, indicating for example score assigned to
each of the features by the survey's respondent. The dependent
variable for each instance (class) is the sum of its features'
effects, which we scale to the uniform distribution of integers 1-5,
indicating, for example, an overall score assigned by the respondent.
</p>
<p style="text-align: center;"><code class="reqn">%
 C=b_w(B_{weak})+b_s(B_{strong})+p_w(P_{weak})+p_s(P_{strong})+e_w(E_{weak})+e_s(E_{strong})%
 </code>
</p>



<h3>Value</h3>

<p>The method returns a <code><a href="base.html#topic+data.frame">data.frame</a></code> with <code>noInst</code> rows and 9 columns. 
Range of values of the attributes and class are integers in [1,5]
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja </p>


<h3>See Also</h3>

<p><code><a href="#topic+classDataGen">classDataGen</a></code>,
<code><a href="#topic+regDataGen">regDataGen</a></code>,
<code><a href="#topic+ordEval">ordEval</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#prepare a data set
dat &lt;- ordDataGen(200)

# evaluate ordered features with ordEval
est &lt;- ordEval(class ~ ., dat, ordEvalNoRandomNormalizers=100)
# print(est)  
plot(est)
</code></pre>

<hr>
<h2 id='ordEval'> Evaluation of ordered attributes </h2><span id='topic+ordEval'></span><span id='topic+OrdEval'></span>

<h3>Description</h3>

<p>The method evaluates the quality of ordered attributes
specified by the formula with ordEval algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ordEval(formula, data, file=NULL, rndFile=NULL, 
        variant=c("allNear","attrDist1","classDist1"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ordEval_+3A_formula">formula</code></td>
<td>
<p> Either a formula specifying the attributes to be evaluated and the target variable, or a name of target variable, or an index of target variable. </p>
</td></tr>
<tr><td><code id="ordEval_+3A_data">data</code></td>
<td>
<p> Data frame with evaluation data. </p>
</td></tr>
<tr><td><code id="ordEval_+3A_file">file</code></td>
<td>
<p> Name of file where evaluation results will be written to. </p>
</td></tr>
<tr><td><code id="ordEval_+3A_rndfile">rndFile</code></td>
<td>
<p> Name of file where evaluation of random normalizing attributes will be written to. </p>
</td></tr>
<tr><td><code id="ordEval_+3A_variant">variant</code></td>
<td>
<p> Name of the variant of ordEval algorithm. Can be any of <code>"allNear", "attrDist1"</code>, or <code>"classDist1"</code>. </p>
</td></tr>
<tr><td><code id="ordEval_+3A_...">...</code></td>
<td>
<p> Other options specific to ordEval or common to other context-sensitive evaluation methods (e.g., ReliefF). </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameter <code>formula</code> can be interpreted in three ways, where the formula interface is the most elegant one, 
but inefficient and inappropriate for large data sets.   See also examples below. As <code>formula</code> one can specify:
</p>
 
<dl>
<dt>an object of class <code>formula</code></dt><dd><p>used as a mechanism to select features (attributes) 
and prediction variable (class). Only simple terms can be used and 
interaction expressed in formula syntax are not supported. The simplest way is
to specify just response variable: <code>class ~ .</code>.
In this case all other attributes in the data set are evaluated. Note that formula interface is not appropriate for data sets with
large number of variables.</p>
</dd>
<dt>a character vector</dt><dd><p>specifying the name of target variable, all the other columns in data frame <code>data</code> are used as predictors.</p>
</dd>
<dt>an integer</dt><dd><p>specifying the index of of target variable in data frame <code>data</code>, all the other columns are used as predictors.</p>
</dd>
</dl>

<p>In the data frame <code>data</code> take care to supply the ordinal data as factors and to provide equal levels for them
(this is not necessary what one gets with <code><a href="utils.html#topic+read.table">read.table</a></code>).
See example below.
</p>
<p>The output can be optionally written to files <code>file</code> and <code>rndFile</code>,
in a format used by visualization methods in <code><a href="#topic+plotOrdEval">plotOrdEval</a></code>.
</p>
<p>The variant of the algorithm actually used is controlled with <code>variant</code> parameter
which can have values &quot;allNear&quot;, &quot;attrDist1&quot;, and &quot;classDist1&quot;. The default value
is &quot;allNear&quot; which takes all nearest neighbors into account in evaluation of attributes.
Variant &quot;attrDist1&quot; takes only  neighbors with attribute value at most 1 different from
current case into account (for each attribute separately). This makes sense when we want to 
see the thresholds of reinforcement, and therefore observe just small change up or down
(it makes sense to combine this with <code>equalUpDown=TRUE</code> in <code><a href="#topic+plot.ordEval">plot.ordEval</a></code> function).   
The &quot;classDist1&quot; variant takes only  neighbors with class value at most 1 different from
current case into account. This makes sense if we want to observe strictly small
changes in upward/downward reinforcement and has little effect in practical applications.
</p>
<p>There are some additional parameters (note <b>... </b>) some of which are common with other context-sensitive evaluation methods (e.g., ReliefF). 
Their list of common parameters is available in <code><a href="#topic+helpCore">helpCore</a></code> (see subsection on attribute evaluation therein).
The parameters specific to <code><a href="#topic+ordEval">ordEval</a></code> are: 
</p>

<dl>
<dt>ordEvalNoRandomNormalizers</dt><dd><p>type: integer, default value: 0, value range: 0, Inf, <br />
number of randomly shuffled attributes for normalization of each attribute (0=no normalization). This parameter should be set to 
a reasonably high value (e.g., 200) in order to produce reliable confidence intervals with <code><a href="#topic+plot.ordEval">plot.ordEval</a></code>. The parameters 
<code>ordEvalBootstrapNormalize</code> and <code>ordEvalNormalizingPercentile</code> only make sense if this parameter is larger than 0.</p>
</dd>
<dt>ordEvalBootstrapNormalize</dt><dd><p>type: logical, default value: FALSE <br />
are features used for normalization constructed with bootstrap sampling or random permutation. </p>
</dd>
<dt>ordEvalNormalizingPercentile</dt><dd><p>type: numeric, default value: 0.025, value range: 0, 0.5 <br />
percentile defines the length of confidence interval obtained with random normalization. Percentile <code>t</code> forms
interval by taking the <code class="reqn">n\cdot t</code> and <code class="reqn">n(1-t)</code> random evaluation as the confidence interval boundaries, thereby forming 
<code class="reqn">100(1-2t)</code>%  confidence interval (<code>t</code>=0.025 gives 95% confidence interval). The value <code class="reqn">n</code> is set by 
<code>ordEvalNoRandomNormalizers</code> parameter.</p>
</dd>
<dt>attrWeights</dt><dd><p>type: character, <br />
a character vector representing a list of attribute weights in the ordEval distance measure. </p>
</dd>
</dl>

<p>Evaluation of attributes without specifics of ordered attributes is covered in function <code><a href="#topic+attrEval">attrEval</a></code>.  
</p>


<h3>Value</h3>

<p>The method returns a list with following components:
</p>
<table>
<tr><td><code>reinfPosAV</code></td>
<td>
<p>a matrix of positive reinforcement for attributes' values, </p>
</td></tr>
<tr><td><code>reinfNegAV</code></td>
<td>
<p>a matrix of negative reinforcement for attributes' values, </p>
</td></tr>
<tr><td><code>anchorAV</code></td>
<td>
<p>a matrix of anchoring for attributes' values, </p>
</td></tr>
<tr><td><code>noAV</code></td>
<td>
<p>a matrix containing count for each value of each attribute, </p>
</td></tr>
<tr><td><code>reinfPosAttr</code></td>
<td>
<p>a vector of positive reinforcement for attributes, </p>
</td></tr>
<tr><td><code>reinfNegAttr</code></td>
<td>
<p>a matrix of negative reinforcement for attributes, </p>
</td></tr>
<tr><td><code>anchorAttr</code></td>
<td>
<p>a matrix of anchoring for attributes, </p>
</td></tr>
<tr><td><code>noAVattr</code></td>
<td>
<p>a vector containing count of valid values of each attribute, </p>
</td></tr>
<tr><td><code>rndReinfPosAV</code></td>
<td>
<p>a three dimensional array of statistics for random normalizing attributes'  positive reinforcement for attributes' values, </p>
</td></tr>
<tr><td><code>rndReinfPosAV</code></td>
<td>
<p>a three dimensional array of statistics for random normalizing attributes' negative reinforcement for attributes' values, </p>
</td></tr>
<tr><td><code>rndAnchorAV</code></td>
<td>
<p>a three dimensional array of statistics for random normalizing attributes' anchoring for attributes' values, </p>
</td></tr>
<tr><td><code>rndReinfPosAttr</code></td>
<td>
<p>a three dimensional array of statistics for random normalizing attributes'  positive reinforcement for attributes, </p>
</td></tr>
<tr><td><code>rndReinfPosAttr</code></td>
<td>
<p>a three dimensional array of statistics for random normalizing attributes' negative reinforcement for attributes, </p>
</td></tr>
<tr><td><code>rndAnchorAttr</code></td>
<td>
<p>a three dimensional array of statistics for random normalizing attributes' anchoring for attributes. </p>
</td></tr>
<tr><td><code>attrNames</code></td>
<td>
<p>the names of attributes</p>
</td></tr>
<tr><td><code>valueNames</code></td>
<td>
<p>the values of attributes</p>
</td></tr>
<tr><td><code>noAttr</code></td>
<td>
<p>number of attributes</p>
</td></tr>
<tr><td><code>ordVal</code></td>
<td>
<p>maximal number of attribute values</p>
</td></tr>
<tr><td><code>variant</code></td>
<td>
<p>the variant of the algorithm used</p>
</td></tr>
<tr><td><code>file</code></td>
<td>
<p>the file to store the results</p>
</td></tr>
<tr><td><code>rndFile</code></td>
<td>
<p>the file to store random normalizations</p>
</td></tr>
</table>
<p>The statistics used are median, 1st quartile, 3rd quartile, low and high percentile selected by <br />
<code>ordEvalNormalizingPercentile</code>, mean, standard deviation, and expected probability according to value distribution.  
With these statistics we can visualize significance of reinforcements using adapted box and whiskers plot.  
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja </p>


<h3>References</h3>

 
<p>Marko Robnik-Sikonja, Koen Vanhoof: Evaluation of ordinal attributes at value level. 
<em>Knowledge Discovery and Data Mining</em>, 14:225-243, 2007    
</p>
<p>Marko Robnik-Sikonja, Igor Kononenko: Theoretical and Empirical Analysis of ReliefF and RReliefF.
<em>Machine Learning Journal</em>, 53:23-69, 2003
</p>
<p>Some of the references are available also from <a href="http://lkm.fri.uni-lj.si/rmarko/papers/">http://lkm.fri.uni-lj.si/rmarko/papers/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.ordEval">plot.ordEval</a></code>,
<code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+helpCore">helpCore</a></code>, 
<code><a href="#topic+infoCore">infoCore</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#prepare a data set
dat &lt;- ordDataGen(200)

# evaluate ordered features with ordEval
est &lt;- ordEval(class ~ ., dat, ordEvalNoRandomNormalizers=100)
# print(est)
printOrdEval(est)  
plot(est)

</code></pre>

<hr>
<h2 id='paramCoreIO'> Input/output of parameters from/to file </h2><span id='topic+paramCoreIO'></span>

<h3>Description</h3>

<p>All the parameters of the given model are written directly to file,
or read from file into model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>paramCoreIO(model, fileName, io=c("read","write")) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="paramCoreIO_+3A_model">model</code></td>
<td>
<p> The model structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>. </p>
</td></tr>
<tr><td><code id="paramCoreIO_+3A_filename">fileName</code></td>
<td>
<p> Name of the parameter file. </p>
</td></tr>
<tr><td><code id="paramCoreIO_+3A_io">io</code></td>
<td>
<p> Controls weather the parameters will be read or written. </p>
</td></tr> 
</table>


<h3>Details</h3>

<p>The function uses the <code>model</code> structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code> and
reads or writes all its parameters from/to file.
If parameter <code>io="read"</code> parameters are read from file <code>filename</code>.
If parameter <code>io="write"</code> parameters are written to file <code>filename</code>.
</p>


<h3>Value</h3>

<p>Returns invisible list with parameters passed to C function:
<code>list(modelID, filename, io</code>.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+helpCore">helpCore</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use iris data
# build random forests model with certain parameters
modelRF &lt;- CoreModel(Species ~ ., iris, model="rf", 
              selectionEstimator="MDL",minNodeWeightRF=5,
              rfNoTrees=50, maxThreads=1)

# writes all the used parameters to file
paramCoreIO(modelRF, "parameters.par", io="write")
# and reads them back into the model
paramCoreIO(modelRF, "parameters.par", io="read")

# clean up for the sake of R package checks
file.remove("parameters.par")

destroyModels(modelRF) # clean up

</code></pre>

<hr>
<h2 id='plot.CoreModel'> Visualization of CoreModel models </h2><span id='topic+plot.CoreModel'></span>

<h3>Description</h3>

<p>The method <code>plot</code> visualizes the models returned by CoreModel()
function or summaries obtained by applying these models to data.
Different plots can be produced depending on the type of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    ## S3 method for class 'CoreModel'
plot(x, trainSet, rfGraphType=c("attrEval", "outliers", "scaling",
    "prototypes", "attrEvalCluster"), clustering=NULL, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.CoreModel_+3A_x">x</code></td>
<td>
<p>The model structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>.</p>
</td></tr>          
<tr><td><code id="plot.CoreModel_+3A_trainset">trainSet</code></td>
<td>
<p> The data frame containing training data which produced the model <code>x</code>. </p>
</td></tr>
<tr><td><code id="plot.CoreModel_+3A_rfgraphtype">rfGraphType</code></td>
<td>
<p> The type of the graph to produce for random forest models. See details.</p>
</td></tr>
<tr><td><code id="plot.CoreModel_+3A_clustering">clustering</code></td>
<td>
<p>The clustering of the training instances used in some model types. See details.</p>
</td></tr>
<tr><td><code id="plot.CoreModel_+3A_...">...</code></td>
<td>
<p> Other options controlling graphical output passed to additional graphical functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output of function <code><a href="#topic+CoreModel">CoreModel</a></code> is visualized. Depending on the model type, different visualizations 
are produced. Currently, classification tree, regression tree, and random forests are supported 
(models &quot;tree&quot;, &quot;regTree&quot;, &quot;rf&quot;, and &quot;rfNear&quot;).
</p>
<p>For classification and regression trees (models &quot;tree&quot; and &quot;regTree&quot;) the visualization produces a graph 
representing structure
of classification and regression tree, respectively. This process exploits graphical capabilities of 
<code><a href="rpart.plot.html#topic+rpart.plot">rpart.plot</a></code> package. Internal structures of 
<code>CoreModel</code> are converted to <code>rpart.object</code> and then visualized by calling
<code><a href="rpart.plot.html#topic+rpart.plot">rpart.plot</a></code> using default parameters. Any additional parameters are passed on to this function. For further 
control use the <code><a href="#topic+getRpartModel">getRpartModel</a></code> function and call the function <code><a href="rpart.plot.html#topic+rpart.plot">rpart.plot</a></code> 
or <code>plot.rpart</code> with different parameters.  
Note that <code>rpart.plot</code> can only display a single value in a leaf, which is not appropriate for model trees using e.g., 
linear regression in the leaves. For these cases function <code><a href="#topic+display">display</a></code> is a better alternative.
</p>
<p>For random forest models (models &quot;rf&quot; and &quot;rfNear&quot;) different types of visualizations can be produced depending on the
<code>graphType</code> parameter:
</p>

<ul>
<li> <p><code>"attrEval"</code> the attributes are evaluated with random forest model and the importance scores are then
visualized. For details see <code><a href="#topic+rfAttrEval">rfAttrEval</a></code>.
</p>
</li>
<li> <p><code>"attrEvalClustering"</code> similarly to the <code>"attrEval"</code> the attributes are evaluated with random forest 
model and the importance scores are then visualized, but the importance scores are generated
for each cluster separately. The parameter <code>clustering</code> provides clustering information on
the <code>trainSet</code>. If <code>clustering</code> parameter is set to NULL, the class values are used as 
clustering information and visualization of attribute importance for each class separately is
generated.
For details see <code><a href="#topic+rfAttrEvalClustering">rfAttrEvalClustering</a></code>.                         
</p>
</li>
<li> <p><code>"outliers"</code> the random forest proximity measure of training instances in <code>trainSet</code>
is visualized and outliers for each class separately can be detected.
For details see <code><a href="#topic+rfProximity">rfProximity</a></code> and <code><a href="#topic+rfOutliers">rfOutliers</a></code>.  
</p>
</li>
<li> <p><code>"prototypes"</code> typical instances are found based on predicted class probabilities
and their values are visualized (see <code><a href="#topic+classPrototypes">classPrototypes</a></code>). 
</p>
</li>
<li> <p><code>"scaling"</code> returns a scaling plot of training instances in a two dimensional space using
random forest based proximity as the distance (see <code><a href="#topic+rfProximity">rfProximity</a></code> 
and a scaling function <code><a href="stats.html#topic+cmdscale">cmdscale</a></code>).
</p>
</li></ul>
       


<h3>Value</h3>

<p>The method returns no value.
</p>


<h3>Author(s)</h3>

<p> John Adeyanju Alao (initial implementation) and Marko Robnik-Sikonja (integration, improvements)</p>


<h3>References</h3>

<p>Leo Breiman: Random Forests. <em>Machine Learning Journal</em>, 45:5-32, 2001
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+rfProximity">rfProximity</a></code>,
<code><a href="cluster.html#topic+pam">pam</a></code>,
<code><a href="#topic+rfClustering">rfClustering</a></code>,
<code><a href="#topic+rfAttrEvalClustering">rfAttrEvalClustering</a></code>,
<code><a href="#topic+rfOutliers">rfOutliers</a></code>,
<code><a href="#topic+classPrototypes">classPrototypes</a></code>,
<code><a href="stats.html#topic+cmdscale">cmdscale</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># decision tree
dataset &lt;- iris
md &lt;- CoreModel(Species ~ ., dataset, model="tree")
plot(md, dataset) # additional parameters are passed directly to rpart.plot

# Additional visualizations can be obtained by explicit conversion to rpart.object 
#rpm &lt;- getRpartModel(md,dataset)
# and than setting graphical parameters in plot.rpart and text.rpart
#require(rpart)
# E.g., set angle to tan(0.5)=45 (degrees) and length of branches at least 5, 
# try to make a dendrogram more compact
#plot(rpm, branch=0.5, minbranch=5, compress=TRUE)
#(pretty=0) full names of attributes, numbers to 3 decimals, 
#text(rpm, pretty=0, digits=3)

destroyModels(md) # clean up

# regression tree
dataset &lt;- CO2
mdr &lt;- CoreModel(uptake ~ ., dataset, model="regTree")
plot(mdr, dataset)
destroyModels(mdr) # clean up

#random forests
dataset &lt;- iris
mdRF &lt;- CoreModel(Species ~ ., dataset, model="rf", rfNoTrees=30, maxThreads=1)
plot(mdRF, dataset, rfGraphType="attrEval")
plot(mdRF, dataset, rfGraphType="outliers")
plot(mdRF, dataset, rfGraphType="scaling")
plot(mdRF, dataset, rfGraphType="prototypes")
plot(mdRF, dataset, rfGraphType="attrEvalCluster", clustering=NULL)
destroyModels(mdRF) # clean up

</code></pre>

<hr>
<h2 id='plot.ordEval'> Visualization of ordEval results </h2><span id='topic+plotOrdEval'></span><span id='topic+plot.ordEval'></span><span id='topic+printOrdEval'></span>

<h3>Description</h3>

<p>The method <code>plot</code> visualizes the results of ordEval algorithm with an adapted 
box-and-whiskers plots. The method <code>printOrdEval</code> prints summary of the results 
in a text format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    plotOrdEval(file, rndFile, ...) 
    
    ## S3 method for class 'ordEval'
plot(x, graphType=c("avBar", "attrBar", "avSlope"), ...)
    
    printOrdEval(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ordEval_+3A_x">x</code></td>
<td>
<p>The object containing results of ordEval algorithm obtained by calling  <code><a href="#topic+ordEval">ordEval</a></code>.
If this object is not given, it has to be constructed from files <code>file</code> and <code>rndFile</code>.</p>
</td></tr>
<tr><td><code id="plot.ordEval_+3A_file">file</code></td>
<td>
<p> Name of file where evaluation results of ordEval algorithm were written to. </p>
</td></tr>
<tr><td><code id="plot.ordEval_+3A_rndfile">rndFile</code></td>
<td>
<p> Name of file where evaluation of random normalizing attributes by ordEval algorithm were written to. </p>
</td></tr>
<tr><td><code id="plot.ordEval_+3A_graphtype">graphType</code></td>
<td>
<p> The type of the graph to produce. Can be any of <code>"avBar", "attrBar", "avSlope"</code>. </p>
</td></tr>
<tr><td><code id="plot.ordEval_+3A_...">...</code></td>
<td>
<p> Other options controlling graphical output, used by specific graphical methods. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output of function <code><a href="#topic+ordEval">ordEval</a></code> either returned directly or stored in files <code>file</code> and <code>rndFile</code>
is read and visualized. The type of graph produced is controlled by <code>graphType</code> parameter:
</p>

<ul>
<li> <p><code>avBar</code> the positive and negative reinforcement of each value of each attribute is visualized 
as the length of the bar. For each value also a normalizing modified box and whiskers plot
is produced above it, showing the confidence interval of the same attribute value under the assumption 
that the attribute contains no information. If the length of the bar is outside the normalizing whiskers, this 
is a statistically significant indication that the value is important.
</p>
</li>
<li> <p><code>attrBar</code> the positive and negative reinforcement for each attribute is visualized 
as the length of the bar. This reinforcement is weighted sum of contributions of individual
values visualized with <code>avBar</code> graph type. 
</p>
</li>
<li> <p><code>avSlope</code> the positive and negative reinforcement of each value of each attribute is visualized 
as the slope of the line segment connecting consequent values
</p>
</li></ul>
    
<p>The <code>avBar</code> and <code>avSlope</code> produce several graphs (one for each attribute). In order to see them all on
an interactive device use <code><a href="grDevices.html#topic+devAskNewPage">devAskNewPage</a></code>. On some platforms or in RStudio environment the graphical window stores the
history and one can browse through recent pages. Alternatively use any of non-interactive devices
such as <code><a href="grDevices.html#topic+pdf">pdf</a></code> or <code><a href="grDevices.html#topic+postscript">postscript</a></code>. Some support for opening and handling of these devices is provided 
by function <code><a href="#topic+preparePlot">preparePlot</a></code>. The user should take care to call <code><a href="grDevices.html#topic+dev.off">dev.off</a></code> after completion of the operations.  
</p>
<p>There are some additional optional parameters <b>... </b> which are important to all or for some graph types.
</p>

<ul>
<li> <p><code>ciType</code> The type of the confidence interval in &quot;avBar&quot; and &quot;attrBar&quot; graph types. Can be <code>"two.sided"</code>, <code>"upper"</code>, <code>"lower"</code>, or <code>"none"</code>. 
Together with <code>ordEvalNormalizingPercentile</code> parameter in <code><a href="#topic+ordEval">ordEval</a></code>, <code>ciType</code>, <code>ciDisplay</code>, 
and <code>ciDecorate</code>  controls the type, length and display of confidence intervals for each value.    
</p>
</li>
<li> <p><code>ciDisplay</code> The way how confidence intervals are displayed. Can be <code>"box"</code> or <code>"color"</code>. The value <code>"box"</code> displays confidence interval as box and whiskers plot above the actual value with whiskers representing confidence percentiles.
The value <code>"color"</code> displays only the upper limit of confidence interval, namely the value 
(represented with a length of the bar) beyond the confidence interval is displayed with more intensive color or shade. 
</p>
</li>
<li> <p><code>ciDecorate</code> controls if the reinforcement factors stretching outside the confidence intervals of possible random effects are decorated by being circled with an ellipse. The default value NULL means that there are no decorations, other values are interpreted as colors in the function <code>draw.elipse</code>, e.g., <code>ciDecorate="red"</code> draws red ellipses around statisticaly significant reinforcemnets.
</p>
</li>
<li> <p><code>equalUpDown</code> a boolean specifying if upward and downward reinforcement of the same value are to be displayed 
side by side on the same level; it usually makes sense to 
set this parameter to <code>TRUE</code> when specifying a single value differences by setting <code>variant="attrDist1"</code> in 
<code><a href="#topic+ordEval">ordEval</a></code> function.  
</p>
</li>
<li> <p><code>graphTitle</code> specifies text to incorporate into the title.
</p>
</li>
<li> <p><code>attrIdx</code> displays plot for a single attribute with specified index.
</p>
</li>
<li> <p><code>xlabel</code>  label of lower horizontal axis. 
</p>
</li>
<li> <p><code>ylabLeft</code> label of the left-hand vertical axis.
</p>
</li>
<li> <p><code>ylabRight</code> label of the right-hand vertical axis.
</p>
</li>
<li> <p><code>colors</code> a vector with four colors specifying colors of reinforcement bars for down, down_beyond, up, and up_beyond, respectively. If set to NULL this produces black and white graph with shades of gray.
The colors down_beyond and up_beyond depict the confidence interval if parameter <code>ciDisplay="color"</code>. <br />
The default values are <code>colors=c("green","lightgreen","blue","lightblue")</code>.
</p>
</li></ul>
   


<h3>Value</h3>

<p>The method returns no value.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>References</h3>

 
<p>Marko Robnik-Sikonja, Koen Vanhoof: Evaluation of ordinal attributes at value level. 
<em>Knowledge Discovery and Data Mining</em>, 14:225-243, 2007    
</p>
<p>Marko Robnik-Sikonja, Igor Kononenko: Theoretical and Empirical Analysis of ReliefF and RReliefF.
<em>Machine Learning Journal</em>, 53:23-69, 2003
</p>
<p>Some of the references are available also from <a href="http://lkm.fri.uni-lj.si/rmarko/papers/">http://lkm.fri.uni-lj.si/rmarko/papers/</a>    
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ordEval">ordEval</a></code>,
<code><a href="#topic+helpCore">helpCore</a></code>,
<code><a href="#topic+preparePlot">preparePlot</a></code>,
<code><a href="#topic+CORElearn">CORElearn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # prepare a data set
    dat &lt;- ordDataGen(200)

    # evaluate ordered features with ordEval
    oe &lt;- ordEval(class ~ ., dat, ordEvalNoRandomNormalizers=200)
    plot(oe)
    # printOrdEval(oe)
    
    # the same effect we achieve by storing results to files
    tmp &lt;- ordEval(class ~ ., dat, file="profiles.oe", 
                  rndFile="profiles.oer", ordEvalNoRandomNormalizers=200)   
    plotOrdEval(file="profiles.oe", rndFile="profiles.oer",
                graphType="attrBar")
    # clean up for the sake of R package checks
    file.remove("profiles.oe")
    file.remove("profiles.oer")

</code></pre>

<hr>
<h2 id='predict.CoreModel'> Prediction using constructed model </h2><span id='topic+predict.CoreModel'></span><span id='topic+predict'></span>

<h3>Description</h3>

<p>Using a previously built model and new data, predicts the class value and probabilities for 
classification problem and function value for regression problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CoreModel'
predict(object, newdata, ..., costMatrix=NULL, 
                            type=c("both","class","probability"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.CoreModel_+3A_object">object</code></td>
<td>
<p> The model structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>. </p>
</td></tr>
<tr><td><code id="predict.CoreModel_+3A_newdata">newdata</code></td>
<td>
<p> Data frame with fresh data. </p>
</td></tr>
<tr><td><code id="predict.CoreModel_+3A_costmatrix">costMatrix</code></td>
<td>
<p> Optional cost matrix can provide nonuniform costs for classification problems. </p>
</td></tr>
<tr><td><code id="predict.CoreModel_+3A_type">type</code></td>
<td>
<p> Controls what will be return value in case of classification. </p>
</td></tr> 
<tr><td><code id="predict.CoreModel_+3A_...">...</code></td>
<td>
<p> Other model dependent options for prediction. See <code><a href="#topic+helpCore">helpCore</a></code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses the <code>object</code> structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code> and
applies it on the data frame <code>newdata</code>. The <code>newdata</code> must be transformable
using the formula specified for building  the model (with dependent variable removed). If the dependent
variable is present in <code>newdata</code>, it is ignored. 
</p>
<p>Optional cost matrix can provide nonuniform costs for classification problems. For regression
problem this parameter is ignored. The costs can be different from the ones used for building the model 
in <code><a href="#topic+CoreModel">CoreModel</a></code>.
</p>


<h3>Value</h3>

<p>For regression model a vector of predicted values for given input instances. For classification
problem the parameter <code>type</code> controls what is returned. With default value <code>"both"</code> 
function returns a list with two components <code>class</code>
and <code>probabilities</code> containing predicted class values and probabilities for all class values, respectively.
With <code>type</code> set to  <code>"class"</code> or <code>"probability"</code> the function returns only the selected component 
as vector or matrix.   
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja, Petr Savicky </p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+modelEval">modelEval</a></code>,
<code><a href="#topic+helpCore">helpCore</a></code>, 
<code><a href="#topic+paramCoreIO">paramCoreIO</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use iris data set

# build random forests model with certain parameters
modelRF &lt;- CoreModel(Species ~ ., iris, model="rf", 
              selectionEstimator="MDL",minNodeWeightRF=5,rfNoTrees=100)
print(modelRF)

# prediction with node distribution
pred &lt;- predict(modelRF, iris, rfPredictClass=FALSE, type="both")
# print(pred)

destroyModels(modelRF) # clean up

</code></pre>

<hr>
<h2 id='preparePlot'> Prepare graphics device </h2><span id='topic+preparePlot'></span><span id='topic+preparePlot.Core'></span>

<h3>Description</h3>

<p>Based on provided <code>fileName</code> opens and sets appropriate graphical device: pdf, postscript, 
interactive graphical window, or (only on windows) windows metafile,.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preparePlot(fileName="Rplot",...)
 </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preparePlot_+3A_filename">fileName</code></td>
<td>
<p> Name of the file to store the output to. </p>
</td></tr>
<tr><td><code id="preparePlot_+3A_...">...</code></td>
<td>
<p> Further parameters passed to device. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function opens the graphical output device based on <code>fileName</code> extension. 
The extensions .pdf, .ps, .jpg, .bmp, .tif, .png, .tiff or none select  <code><a href="grDevices.html#topic+pdf">pdf</a></code>, <code><a href="grDevices.html#topic+postscript">postscript</a></code>,
<code><a href="grDevices.html#topic+jpeg">jpeg</a></code>, <code><a href="grDevices.html#topic+bmp">bmp</a></code>, <code><a href="grDevices.html#topic+tiff">tiff</a></code>,<code><a href="grDevices.html#topic+png">png</a></code>, <code><a href="grDevices.html#topic+bitmap">bitmap</a></code>
or a default (interactive) graphical device.
</p>
<p>On Windows also .emf extension is supported which opens <code>win.metafile</code>    
and creates vector graphics in windows enhanced metafile format.      
</p>
<p>The extension .tiff opens <code><a href="grDevices.html#topic+bitmap">bitmap</a></code> device which produces bitmap via <code><a href="grDevices.html#topic+postscript">postscript</a></code> device.
Therefore it requires Ghostscript to be installed and on the executable path.
</p>
<p>Some sensible default values are passed to created devices, but further options can be
passed via <code>...</code>.  
</p>


<h3>Value</h3>

<p>A plot device is opened and nothing is returned to the R interpreter.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja </p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+plot.ordEval">plot.ordEval</a></code>,
<code><a href="grDevices.html#topic+pdf">pdf</a></code>, 
<code><a href="grDevices.html#topic+postscript">postscript</a></code>,
<code><a href="grDevices.html#topic+jpeg">jpeg</a></code>, 
<code><a href="grDevices.html#topic+bmp">bmp</a></code>, 
<code><a href="grDevices.html#topic+tiff">tiff</a></code>,
<code><a href="grDevices.html#topic+png">png</a></code>,
<code><a href="grDevices.html#topic+Devices">Devices</a></code>    
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   # prepare a data set
    dat &lt;- ordDataGen(200)
    # evaluate ordered features with ordEval
    oe &lt;- ordEval(class ~ ., dat, ordEvalNoRandomNormalizers=200)
    
    # the folowing line if uncommented will create a separate 
    # postscript file with given filename for each attribute   
    # preparePlot("myGraph%03d.ps") 
    
    plot(oe)
    dev.off()
</code></pre>

<hr>
<h2 id='regDataGen'>Artificial data for testing regression algorithms</h2><span id='topic+regDataGen'></span>

<h3>Description</h3>

<p>The generator produces regression data data with 4 discrete and 7 numeric attributes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  regDataGen(noInst, t1=0.8, t2=0.5, noise=0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regDataGen_+3A_noinst">noInst</code></td>
<td>
<p>Number of instances to generate.</p>
</td></tr>
<tr><td><code id="regDataGen_+3A_t1">t1</code>, <code id="regDataGen_+3A_t2">t2</code></td>
<td>
<p> Parameters controlling the shape of the distribution.</p>
</td></tr>
<tr><td><code id="regDataGen_+3A_noise">noise</code></td>
<td>
<p>Parameter controlling the amount of noise. If <code>noise=0</code>, there is no noise. If noise = 1, then the level
of the signal and noise are the same.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The response variable is derived from x4, x5, x6 using two different
functions. The choice depends on a hidden variable, which determines weather the 
response value would follow a linear dependency <code class="reqn">f=x_4-2x_5+3x_6</code>,
or a nonlinear one <code class="reqn">f=cos(4\pi x_4)(2x_5-3x_6)</code>.
</p>
<p>Attributes a1, a2, x1, x2 carry some information on the hidden
variables depending on parameters t1, t2. Extreme values of the
parameters are t1=0.5 and t2=1, when there is no information.
On the other hand, if t1=0 or t1=1 then each of the attributes
a1, a2 carries full information. If t2=0, then each of x1, x2
carries full information on the hidden variable.
</p>
<p>The attributes x4, x5, x6 are available with a noise level depending
on parameter <code>noise</code>. If <code>noise=0</code>, there is no noise. If <code>noise=1</code>, then the level
of the signal and noise are the same.
</p>


<h3>Value</h3>

<p>Returns a <code><a href="base.html#topic+data.frame">data.frame</a></code> with <code>noInst</code> rows and 11 columns. 
Range of values of the attributes and response are
</p>
<table>
<tr><td><code>a1</code></td>
<td>
<p>0,1</p>
</td></tr>
<tr><td><code>a2</code></td>
<td>
<p> a,b,c,d</p>
</td></tr>
<tr><td><code>a3</code></td>
<td>
<p> 0,1 (irrelevant)</p>
</td></tr>
<tr><td><code>a4</code></td>
<td>
<p> a,b,c,d (irrelevant)</p>
</td></tr>
<tr><td><code>x1</code></td>
<td>
<p> numeric (gaussian with different sd for each class)</p>
</td></tr>
<tr><td><code>x2</code></td>
<td>
<p> numeric (gaussian with different sd for each class)</p>
</td></tr>
<tr><td><code>x3</code></td>
<td>
<p> numeric (gaussian, irrelevant)</p>
</td></tr>
<tr><td><code>x4</code></td>
<td>
<p> numeric from [0,1]</p>
</td></tr>
<tr><td><code>x5</code></td>
<td>
<p> numeric from [0,1]</p>
</td></tr>
<tr><td><code>x6</code></td>
<td>
<p> numeric from [0,1]</p>
</td></tr>
<tr><td><code>response</code></td>
<td>
<p> numeric</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Petr Savicky </p>


<h3>See Also</h3>

<p><code><a href="#topic+classDataGen">classDataGen</a></code>,<code><a href="#topic+ordDataGen">ordDataGen</a></code>,<code><a href="#topic+CoreModel">CoreModel</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#prepare a regression data set
regData &lt;-regDataGen(noInst=200)

# build regression tree similar to CART
modelRT &lt;- CoreModel(response ~ ., regData, model="regTree", modelTypeReg=1)
print(modelRT)

destroyModels(modelRT) # clean up

</code></pre>

<hr>
<h2 id='reliabilityPlot'>Plots reliability plot of probabilities</h2><span id='topic+reliabilityPlot'></span>

<h3>Description</h3>

<p>Given probability scores <code>probScore</code> and  true probabilities <code>trueProb</code> the methods plots one against the other using a selected boxing method 
which groups scores and probabilities to show calibration of probabilities in given probability bands.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reliabilityPlot(probScore, trueProb, titleText="", boxing="equipotent", 
                noBins=10, classValue = 1, printWeight=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reliabilityPlot_+3A_probscore">probScore</code></td>
<td>
<p> A vector of predicted probabilities for a given class <code>classValue</code>. </p>
</td></tr>
<tr><td><code id="reliabilityPlot_+3A_trueprob">trueProb</code></td>
<td>
<p> A vector of true probabilities for a given <code>classValue</code>, should be of the same length as <code>probScore</code>. </p>
</td></tr>
<tr><td><code id="reliabilityPlot_+3A_titletext">titleText</code></td>
<td>
<p>The text of the graph title.</p>
</td></tr>
<tr><td><code id="reliabilityPlot_+3A_boxing">boxing</code></td>
<td>
<p> One of <code>"unique"</code>, <code>"equidistant"</code> or <code>"equipotent"</code>, determines the grouping of probabilities. See details below.</p>
</td></tr>
<tr><td><code id="reliabilityPlot_+3A_nobins">noBins</code></td>
<td>
<p>The value of parameter depends on the parameter <code>boxing</code> and specifies the number of bins. See details below.</p>
</td></tr>
<tr><td><code id="reliabilityPlot_+3A_classvalue">classValue</code></td>
<td>
<p>A class value (factor) or an index of the class value (integer) for which reliability plot is made.</p>
</td></tr>
<tr><td><code id="reliabilityPlot_+3A_printweight">printWeight</code></td>
<td>
<p> A boolean specifying if box weights are to be printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Depending on the specified <code>boxing</code> the probability scores are grouped in one of three possible ways
</p>

<ul>
<li> <p><code>"unique"</code> each unique probability score forms its own box.
</p>
</li>
<li> <p><code>"equidistant"</code> forms <code>noBins</code> equally wide boxes.
</p>
</li>
<li> <p><code>"equipotent"</code> forms <code>noBins</code> boxes with equal number of scores in each box.
</p>
</li></ul>
  
<p>The parameter <code>trueProb</code> can represent either probabilities (in [0, 1] range, in most cases these will be 0s or 1s), 
or the true class values from which the method will form 0 and 1 values corresponding to probabilities for class value <code>classValue</code>.
</p>


<h3>Value</h3>

<p>A function returns a graph containing reliability plot on a current graphical device.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+calibrate">calibrate</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate data consisting from 3 parts:
#  one part for training, one part for calibration, one part for testing
train &lt;-classDataGen(noInst=200)
cal &lt;-classDataGen(noInst=200)
test &lt;- classDataGen(noInst=200)

# build random forests model with default parameters
modelRF &lt;- CoreModel(class~., train, model="rf")
# prediction of calibration and test set
predCal &lt;- predict(modelRF, cal, rfPredictClass=FALSE)
predTest &lt;- predict(modelRF, test, rfPredictClass=FALSE)
destroyModels(modelRF) # no longer needed, clean up

# show reliability plot of uncalibrated test set
class1&lt;-1
par(mfrow=c(1,2))
reliabilityPlot(predTest$prob[,class1], test$class, 
                titleText="Uncalibrated probabilities", classValue=class1) 

# calibrate for a chosen class1 and method using calibration set
calibration &lt;- calibrate(cal$class, predCal$prob[,class1], class1=1, 
                         method="isoReg", assumeProbabilities=TRUE)
calTestProbs &lt;- applyCalibration(predTest$prob[,class1], calibration)
# display calibrated probabilities
reliabilityPlot(calTestProbs, test$class, 
                titleText="Calibrated probabilities", classValue=class1) 

</code></pre>

<hr>
<h2 id='rfAttrEval'> Attribute evaluation with random forest </h2><span id='topic+rfAttrEval'></span><span id='topic+rfAttrEvalClustering'></span>

<h3>Description</h3>

<p>The method evaluates the quality of the features/attributes/dependent variables
used in the given random forest <code>model</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rfAttrEval(model) 
rfAttrEvalClustering(model, dataset, clustering=NULL)  </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rfAttrEval_+3A_model">model</code></td>
<td>
<p> The model of type <code>rf</code> or <code>rfNear</code> as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>. </p>
</td></tr>
<tr><td><code id="rfAttrEval_+3A_dataset">dataset</code></td>
<td>
<p> Training instances that produced random forest <code>model</code>.</p>
</td></tr>
<tr><td><code id="rfAttrEval_+3A_clustering">clustering</code></td>
<td>
<p> A clustering vector of <code>dataset</code> training instances used in <code>model</code>.</p>
</td></tr>   
</table>


<h3>Details</h3>

<p>The attributes are evaluated via provided random forest's out-of-bag sets. Values for each attribute in turn
are randomly shuffled and classified with random forest. The difference between average margin of
non-shuffled and shuffled instances serves as a quality estimate of the attribute.
The function <code>rfAttrEvalClustering</code> uses a clustering of the training instances to produce 
importance score of attributes 
for each cluster separately. If parameter <code>clustering</code> is set to <code>NULL</code> 
the actual class values of the instances are used as clusters thereby  producing the evaluation of attributes 
specific for each of the class values.  
</p>


<h3>Value</h3>

<p>In case of <code>rfAttrEval</code> a vector of evaluations for the features in the order specified by the formula used to generate the provided <code>model</code>.
In case of <code>rfAttrEvalClustering</code> a matrix is returned, where each row contains evaluations for one of the clusters. 
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja (thesis supervisor) and John Adeyanju Alao (as a part of his BSc thesis)</p>


<h3>References</h3>

 
<p>Marko Robnik-Sikonja: Improving Random Forests. In J.-F. Boulicaut et al.(Eds): ECML 2004, 
LNAI 3210, Springer, Berlin, 2004, pp. 359-370
Available also from <a href="http://lkm.fri.uni-lj.si/rmarko/papers/">http://lkm.fri.uni-lj.si/rmarko/papers/</a>
</p>
<p>Leo Breiman: Random Forests. Machine Learning Journal, 2001, 45, 5-32
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+attrEval">attrEval</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># build random forests model with certain parameters
modelRF &lt;- CoreModel(Species ~ ., iris, model="rf", 
              selectionEstimator="MDL", minNodeWeightRF=5, 
              rfNoTrees=100, maxThreads=1)
rfAttrEval(modelRF) # feature evaluations

x &lt;- rfAttrEval(modelRF) # feature evaluations for each class
print(x)

destroyModels(modelRF) # clean up


</code></pre>

<hr>
<h2 id='rfClustering'>Random forest based clustering</h2><span id='topic+rfClustering'></span>

<h3>Description</h3>

<p>Creates a clustering of random forest training instances. Random forest provides proximity of its training instances based on their out-of-bag classification.
This information is usually passed to visualizations (e.g., scaling) and attribute importance measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rfClustering(model, noClusters=4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rfClustering_+3A_model">model</code></td>
<td>
<p> a random forest model returned by <code><a href="#topic+CoreModel">CoreModel</a></code></p>
</td></tr>
<tr><td><code id="rfClustering_+3A_noclusters">noClusters</code></td>
<td>
<p>number of clusters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The method calls <code><a href="cluster.html#topic+pam">pam</a></code> function for clustering, initializing its distance matrix with random forest based similarity by calling
<code><a href="#topic+rfProximity">rfProximity</a></code> with argument <code>model</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>pam</code> representing the clustering (see <code>?pam.object</code> for details),
the most important being a vector of cluster assignments (named <code>cluster</code>) to training instances used to generate the <code>model</code>.
</p>


<h3>Author(s)</h3>

<p> John Adeyanju Alao (as a part of his BSc thesis) and Marko Robnik-Sikonja (thesis supervisor)</p>


<h3>References</h3>

<p>Leo Breiman: Random Forests. <em>Machine Learning Journal</em>, 45:5-32, 2001
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CoreModel">CoreModel</a></code>
<code><a href="#topic+rfProximity">rfProximity</a></code>
<code><a href="cluster.html#topic+pam">pam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set&lt;-iris
md&lt;-CoreModel(Species ~ ., set, model="rf", rfNoTrees=30, maxThreads=1)
mdCluster&lt;-rfClustering(md, 5)

destroyModels(md) # clean up

</code></pre>

<hr>
<h2 id='rfOOB'> Out-of-bag performance estimation for random forests</h2><span id='topic+rfOOB'></span>

<h3>Description</h3>

<p>The method returns internal out-of-bag performance evaluation for given random forests <code>model</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  rfOOB(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rfOOB_+3A_model">model</code></td>
<td>
<p> The model of type <code>rf</code> or <code>rfNear</code> as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>. </p>
</td></tr> 
</table>


<h3>Details</h3>

<p>The method returns random forest performance estimations obtained via its out-of-bag sets. 
The performance measures returned are classification accuracy, average classification margin, and correlation between trees in the forest.
The classification margin is defined as the difference between probability of the correct class and probability of the most probable incorrect class.
The correlation between models is estimated as the ratio between classification margin variance and 
variance of the forest as defined in (Breiman, 2001).   
</p>


<h3>Value</h3>

<p>The list containing three performance measures computed with out-of-bag instances is returned:
</p>
<table>
<tr><td><code>accuracy</code></td>
<td>
<p>the classification accuracy of the forest,</p>
</td></tr>
<tr><td><code>margin</code></td>
<td>
<p>the average margin of classification with the forest,</p>
</td></tr>
<tr><td><code>correlation</code></td>
<td>
<p>the correlation between trees in the forest.</p>
</td></tr>  
</table>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja.</p>


<h3>References</h3>

 
<p>Leo Breiman: Random Forests. Machine Learning Journal, 2001, 45, 5-32
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># build random forests model with certain parameters
modelRF &lt;- CoreModel(Species ~ ., iris, model="rf", 
              selectionEstimator="MDL", minNodeWeightRF=5, 
              rfNoTrees=100, maxThreads=1)
rfOOB(modelRF) 

destroyModels(modelRF) # clean up

</code></pre>

<hr>
<h2 id='rfOutliers'> Random forest based outlier detection </h2><span id='topic+rfOutliers'></span>

<h3>Description</h3>

<p>Based on random forest instance proximity measure detects training cases which are different to all other cases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rfOutliers(model, dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rfOutliers_+3A_model">model</code></td>
<td>
<p>a random forest model returned by <code><a href="#topic+CoreModel">CoreModel</a></code></p>
</td></tr>
<tr><td><code id="rfOutliers_+3A_dataset">dataset</code></td>
<td>
<p>a training set used to generate the <code>model</code></p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>Strangeness is defined using the random forest model via a proximity matrix (see <code><a href="#topic+rfProximity">rfProximity</a></code>).
If the number is greater than 10, the case can be considered an outlier according to Breiman 2001.
</p>


<h3>Value</h3>

<p>For each instance from a <code>dataset</code> the function returns a numeric score of its strangeness to other cases.</p>


<h3>Author(s)</h3>

<p> John Adeyanju Alao (as a part of his BSc thesis) and Marko Robnik-Sikonja (thesis supervisor)</p>


<h3>References</h3>

<p>Leo Breiman: Random Forests. <em>Machine Learning Journal</em>, 45:5-32, 2001
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+rfProximity">rfProximity</a></code>,
<code><a href="#topic+rfClustering">rfClustering</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#first create a random forest tree using CORElearn
dataset &lt;- iris
md &lt;- CoreModel(Species ~ ., dataset, model="rf", rfNoTrees=30, 
                maxThreads=1)
outliers &lt;- rfOutliers(md, dataset)
plot(abs(outliers))
#for a nicer display try 
plot(md, dataset, rfGraphType="outliers")

destroyModels(md) # clean up

</code></pre>

<hr>
<h2 id='rfProximity'> A random forest based proximity function </h2><span id='topic+rfProximity'></span>

<h3>Description</h3>

<p>Random forest computes similarity between instances with classification of out-of-bag instances. 
If two out-of-bag cases are classified in the same tree leaf the proximity between them is incremented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rfProximity(model, outProximity=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rfProximity_+3A_model">model</code></td>
<td>
<p>a <code>CORElearn</code> model of type random forest.</p>
</td></tr>
<tr><td><code id="rfProximity_+3A_outproximity">outProximity</code></td>
<td>
<p>if <code>TRUE</code>, function returns a proximity matrix, else it returns a distance matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A proximity is transformed into distance with expression <code>distance=sqrt(1-proximity)</code>.
</p>


<h3>Value</h3>

<p>Function returns an M by M matrix where M is the number of training instances.
Returned matrix is used as an input to other function (see <code><a href="#topic+rfOutliers">rfOutliers</a></code>
and <code><a href="#topic+rfClustering">rfClustering</a></code>).
</p>


<h3>Author(s)</h3>

<p> John Adeyanju Alao (as a part of his BSc thesis) and Marko Robnik-Sikonja (thesis supervisor)</p>


<h3>References</h3>

<p>Leo Breiman: Random Forests. <em>Machine Learning Journal</em>, 45:5-32, 2001
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CoreModel">CoreModel</a></code>,
<code><a href="#topic+rfOutliers">rfOutliers</a></code>,
<code><a href="stats.html#topic+cmdscale">cmdscale</a></code>,
<code><a href="#topic+rfClustering">rfClustering</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>md &lt;- CoreModel(Species ~ ., iris, model="rf", rfNoTrees=30, maxThreads=1)
pr &lt;- rfProximity(md, outProximity=TRUE)
# visualization
require(lattice)
levelplot(pr)

destroyModels(md) # clean up

</code></pre>

<hr>
<h2 id='saveRF'> Saves/loads random forests model to/from file </h2><span id='topic+saveRF'></span><span id='topic+loadRF'></span>

<h3>Description</h3>

<p><code>saveRF</code>: the internal structure of given random forests model is saved to file.
<code>loadRF</code>: the internal structure of random forests model is loaded from given file and a model is created and returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>saveRF(model, fileName) 
loadRF(fileName) 
 </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="saveRF_+3A_model">model</code></td>
<td>
<p> The model structure as returned by <code><a href="#topic+CoreModel">CoreModel</a></code>. </p>
</td></tr>
<tr><td><code id="saveRF_+3A_filename">fileName</code></td>
<td>
<p> Name of the file to save/load the model to/from. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>saveRF</code> saves the internal structure of given random forests model to file. 
The structures from C++ code are stored to the file with specified file, while internal structures
from R are stored to file named <code>fileName.Rda</code>.   
The <code>model</code> must be a valid structure returned by <code><a href="#topic+CoreModel">CoreModel</a></code>.
</p>
<p>The function <code>loadRF</code> loads the internal structure of random forests saved in a specified files and
returns access to it. 
</p>


<h3>Value</h3>

<p><code>saveRF</code> invisibly returns some debugging information, while <code>loadRF</code>
returns a loaded model as a list, similarly to <code><a href="#topic+CoreModel">CoreModel</a></code>.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja</p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>,
<code><a href="#topic+CoreModel">CoreModel</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use iris data set


# build random forests model with certain parameters
modelRF &lt;- CoreModel(Species ~ ., iris, model="rf", 
              selectionEstimator="MDL",minNodeWeightRF=5,
              rfNoTrees=100, maxThreads=1)
print(modelRF)

# prediction with node distribution
pred &lt;- predict(modelRF, iris, rfPredictClass=FALSE, type="both")
# print(pred)


# saves the random forests model to file
saveRF(modelRF, "tempRF.txt")

# restore the model to another model
loadedRF = loadRF("tempRF.txt")

# prediction should be the same
predLoaded &lt;- predict(loadedRF, iris, rfPredictClass=FALSE, type="both")
# print(predLoaded)
# sum of differences should be zero subject to numeric imprecision 
sum(pred$probabilities - predLoaded$probabilities) 

cat("Are predicted classes of original and retrieved models equal? ", 
                           all(pred$class == predLoaded$class), "\n" ) 
# cat("Are predicted probabilities of original and retrieved model equal? ", 
#                 all(pred$probabilities == predLoaded$probabilities), "\n" ) 

# clean up the models when no longer needed
destroyModels(modelRF) 
destroyModels(loadedRF) 

# clean up for the sake of R package checks
file.remove("tempRF.txt")
file.remove("tempRF.txt.Rda")



</code></pre>

<hr>
<h2 id='testCore'>Verification of the CORElearn installation</h2><span id='topic+testCore'></span><span id='topic+testCoreClass'></span><span id='topic+testCoreAttrEval'></span><span id='topic+testCoreReg'></span><span id='topic+testCoreOrdEval'></span><span id='topic+testCoreNA'></span><span id='topic+testCoreRPORT'></span><span id='topic+testCoreRand'></span><span id='topic+allTests'></span>

<h3>Description</h3>

<p>Performs a partial check of the classification part of CORElearn.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testCoreClass(continue=TRUE)
testCoreAttrEval(continue=TRUE)
testCoreReg(continue=TRUE)
testCoreOrdEval(continue=TRUE)
testCoreNA(continue=TRUE)
testCoreRPORT(continue=TRUE)
testCoreRand(continue=TRUE)
allTests(continue=TRUE, timed=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="testCore_+3A_continue">continue</code></td>
<td>
<p> Logical. Whether a warning or an error should be generated
when a test fails.</p>
</td></tr>
<tr><td><code id="testCore_+3A_timed">timed</code></td>
<td>
<p>Logical. Whether the time usage should be printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Functions <code>testCoreClass()</code>, <code>testCoreAttrEval()</code>, <code>testCoreReg()</code> evaluate
functions <code>CoreModel()</code>, <code>predict.CoreModel()</code>, <code>modelEval()</code>, and
<code>attrEval()</code> and perform a partial check of the obtained results.
</p>
<p>Function <code>testNA()</code> performs a test of consistency NA and NaN between R and CORElearn.
</p>
<p>Functions <code>testCoreRPORT()</code> and <code>testCoreRand()</code> test, whether the
<code>R_PORT</code> directive is defined in C code and whether R random number
generator is used. These tests are mostly used for debugging.
</p>
<p>Function <code>allTests()</code> calls all the above functions and prints a table
of the results. If an error is found, a more detailed information
is printed and the continuation of the tests depends on the argument
<code>continue</code>.
</p>


<h3>Value</h3>

<p>The functions have no output value. The result OK or FAILED is printed.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja, Petr Savicky </p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>allTests() # run all tests and generate an error, if any of the tests fails
</code></pre>

<hr>
<h2 id='versionCore'> Package version</h2><span id='topic+versionCore'></span>

<h3>Description</h3>

<p>Prints package version obtained from C code.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>versionCore()
</code></pre>


<h3>Arguments</h3>

<p>None.</p>


<h3>Details</h3>

<p>The function returns the information about the current version obtained from underlying C library <code>link{CORElearn}</code>.
</p>


<h3>Value</h3>

<p>Character string with information about the version.
</p>


<h3>Author(s)</h3>

<p> Marko Robnik-Sikonja, Petr Savicky </p>


<h3>See Also</h3>

<p><code><a href="#topic+CORElearn">CORElearn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load the package
library(CORElearn)

# print its version 
versionCore()
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
