<!DOCTYPE html><html><head><title>Help for package PRIMME</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PRIMME}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#eigs_sym'><p>Find a few eigenvalues and vectors on large, sparse Hermitian matrix</p></a></li>
<li><a href='#svds'><p>Find a few singular values and vectors on large, sparse matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Eigenvalues and Singular Values and Vectors from Large Matrices</td>
</tr>
<tr>
<td>Version:</td>
<td>3.2-6</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-10</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Eloy Romero &lt;eloy@cs.wm.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>
    R interface to 'PRIMME' <a href="https://www.cs.wm.edu/~andreas/software/">https://www.cs.wm.edu/~andreas/software/</a>, a C library for computing a few
    eigenvalues and their corresponding eigenvectors of a real symmetric or complex
    Hermitian matrix, or generalized Hermitian eigenproblem.  It can also compute
    singular values and vectors of a square or rectangular matrix. 'PRIMME' finds
    largest, smallest, or interior singular/eigenvalues and can use preconditioning
    to accelerate convergence. General description of the methods are provided in the papers
    Stathopoulos (2010, &lt;<a href="https://doi.org/10.1145%2F1731022.1731031">doi:10.1145/1731022.1731031</a>&gt;) and Wu (2017, &lt;<a href="https://doi.org/10.1137%2F16M1082214">doi:10.1137/16M1082214</a>&gt;).
    See 'citation("PRIMME")' for details.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.cs.wm.edu/~andreas/software/">https://www.cs.wm.edu/~andreas/software/</a>
<a href="https://github.com/primme/primme">https://github.com/primme/primme</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/primme/primme/issues">https://github.com/primme/primme/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.4), Matrix</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, Matrix</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>A POSIX system. Currently Linux and OS X are known
to work. GNU make.</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-09 13:39:04 UTC; eromero</td>
</tr>
<tr>
<td>Author:</td>
<td>Eloy Romero [aut, cre],
  Andreas Stathopoulos [aut],
  Lingfei Wu [aut],
  College of William &amp; Mary [cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-09 14:00:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='eigs_sym'>Find a few eigenvalues and vectors on large, sparse Hermitian matrix</h2><span id='topic+eigs_sym'></span>

<h3>Description</h3>

<p>Compute a few eigenpairs from a specified region (the largest, the smallest,
the closest to a point) on a symmetric/Hermitian matrix using PRIMME [1].
Generalized symmetric/Hermitian problem is also supported.
Only the matrix-vector product of the matrix is required. The used method is
usually faster than a direct method (such as <code><a href="base.html#topic+eigen">eigen</a></code>) if
seeking a few eigenpairs and the matrix-vector product is cheap. For
accelerating the convergence consider to use preconditioning and/or educated
initial guesses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eigs_sym(
  A,
  NEig = 1,
  which = "LA",
  targetShifts = NULL,
  tol = 1e-06,
  x0 = NULL,
  ortho = NULL,
  prec = NULL,
  isreal = NULL,
  B = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eigs_sym_+3A_a">A</code></td>
<td>
<p>symmetric/Hermitian matrix or a function with signature f(x) that
returns <code>A %*% x</code>.</p>
</td></tr>
<tr><td><code id="eigs_sym_+3A_neig">NEig</code></td>
<td>
<p>number of eigenvalues and vectors to seek.</p>
</td></tr>
<tr><td><code id="eigs_sym_+3A_which">which</code></td>
<td>
<p>which eigenvalues to find:
</p>

<dl>
<dt><code>"LA"</code></dt><dd><p>the largest (rightmost) values;</p>
</dd>
<dt><code>"SA"</code></dt><dd><p>the smallest (leftmost) values;</p>
</dd>
<dt><code>"LM"</code></dt><dd><p>the farthest from <code>targetShifts</code>;</p>
</dd>
<dt><code>"SM"</code></dt><dd><p>the closest to <code>targetShifts</code>;</p>
</dd>
<dt><code>"CLT"</code></dt><dd><p>the closest but left to <code>targetShifts</code>;</p>
</dd>
<dt><code>"CGT"</code></dt><dd><p>the closest but greater than <code>targetShifts</code>;</p>
</dd>
<dt>vector of numbers</dt><dd><p>the closest values to these points.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="eigs_sym_+3A_targetshifts">targetShifts</code></td>
<td>
<p>return the closest eigenvalues to these points as
indicated by <code>target</code>.</p>
</td></tr>
<tr><td><code id="eigs_sym_+3A_tol">tol</code></td>
<td>
<p>the convergence tolerance:
<code class="reqn">\|A x - x\lambda\| \le tol\|A\|</code>.</p>
</td></tr>
<tr><td><code id="eigs_sym_+3A_x0">x0</code></td>
<td>
<p>matrix whose columns are educated guesses of the eigenvectors to
to find.</p>
</td></tr>
<tr><td><code id="eigs_sym_+3A_ortho">ortho</code></td>
<td>
<p>find eigenvectors orthogonal to the space spanned by the
columns of this matrix; useful to avoid finding some eigenvalues or
to find new solutions.</p>
</td></tr>
<tr><td><code id="eigs_sym_+3A_prec">prec</code></td>
<td>
<p>preconditioner used to accelerated the convergence; usually it
is an approximation of the inverse of <code class="reqn">A - \sigma I</code> if finding
the closest eigenvalues to <code class="reqn">\sigma</code>. If it is a matrix
it is used as prec %*% x; otherwise it is used as prec(x).</p>
</td></tr>
<tr><td><code id="eigs_sym_+3A_isreal">isreal</code></td>
<td>
<p>whether A %*% x always returns real number and not complex.</p>
</td></tr>
<tr><td><code id="eigs_sym_+3A_b">B</code></td>
<td>
<p>symmetric/Hermitian positive definite matrix or a function with
signature f(x) that returns <code>B %*% x</code>. If given, the function
returns the eigenpairs of (A,B).</p>
</td></tr>
<tr><td><code id="eigs_sym_+3A_...">...</code></td>
<td>
<p>other PRIMME options (see details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Optional arguments to pass to PRIMME eigensolver (see further details at [2]):
</p>

<dl>
<dt><code>method</code></dt><dd><p> used by the solver, one of:
</p>

<dl>
<dt><code>"DYNAMIC"</code></dt><dd><p>                  switches dynamically between DEFAULT_MIN_TIME and DEFAULT_MIN_MATVECS</p>
</dd>
<dt><code>"DEFAULT_MIN_TIME"</code></dt><dd><p>         best method for light matrix-vector product</p>
</dd>
<dt><code>"DEFAULT_MIN_MATVECS"</code></dt><dd><p>      best method for heavy matrix-vector product or preconditioner</p>
</dd>
<dt><code>"Arnoldi"</code></dt><dd><p>                  an Arnoldi not implemented efficiently</p>
</dd>
<dt><code>"GD"</code></dt><dd><p>                       classical block Generalized Davidson </p>
</dd>
<dt><code>"GD_plusK"</code></dt><dd><p>                 GD+k block GD with recurrence restarting</p>
</dd>
<dt><code>"GD_Olsen_plusK"</code></dt><dd><p>           GD+k with approximate Olsen preconditioning</p>
</dd>
<dt><code>"JD_Olsen_plusK"</code></dt><dd><p>           GD+k, exact Olsen (two preconditioner applications per step)</p>
</dd>
<dt><code>"RQI"</code></dt><dd><p>                      Rayleigh Quotient Iteration, also Inverse Iteration
if <code>targetShifts</code> is provided</p>
</dd>
<dt><code>"JDQR"</code></dt><dd><p>                     original block, Jacobi Davidson</p>
</dd>
<dt><code>"JDQMR"</code></dt><dd><p>                    our block JDQMR method (similar to JDCG)</p>
</dd>
<dt><code>"JDQMR_ETol"</code></dt><dd><p>               slight, but efficient JDQMR modification</p>
</dd>
<dt><code>"STEEPEST_DESCENT"</code></dt><dd><p>         equivalent to GD(<code>maxBlockSize</code>,2*<code>maxBlockSize</code>)</p>
</dd>
<dt><code>"LOBPCG_OrthoBasis"</code></dt><dd><p>        equivalent to GD(<code>neig</code>,3*<code>neig</code>)+<code>neig</code></p>
</dd>
<dt><code>"LOBPCG_OrthoBasis_Window"</code></dt><dd><p> equivalent to GD(<code>maxBlockSize</code>,3*<code>maxBlockSize</code>)+<code>maxBlockSize</code> when neig&gt;<code>maxBlockSize</code></p>
</dd>
</dl>
</dd>
<dt><code>aNorm</code></dt><dd><p>estimation of norm-2 of A, used in convergence test (if not
provided, it is estimated as the largest eigenvalue in magnitude
seen).</p>
</dd>
<dt><code>maxBlockSize</code></dt><dd><p>maximum block size (like in subspace iteration or
LOBPCG).</p>
</dd>
<dt><code>printLevel</code></dt><dd><p>message level reporting, from 0 (no output) to 5 (show all).</p>
</dd> 
<dt><code>locking</code></dt><dd><p>1, hard locking; 0, soft locking.</p>
</dd>
<dt><code>maxBasisSize</code></dt><dd><p>maximum size of the search subspace.</p>
</dd>
<dt><code>minRestartSize</code></dt><dd><p> minimum Ritz vectors to keep in restarting.</p>
</dd>
<dt><code>maxMatvecs</code></dt><dd><p> maximum number of matrix vector multiplications.</p>
</dd>
<dt><code>maxit</code></dt><dd><p> maximum number of outer iterations.</p>
</dd>
<dt><code>scheme</code></dt><dd><p> the restart scheme (thick restart by default).</p>
</dd>
<dt><code>maxPrevRetain</code></dt><dd><p> number of approximate eigenvectors retained from
previous iteration, that are kept after restart.</p>
</dd>
<dt><code>robustShifts</code></dt><dd><p> set to true to avoid stagnation.</p>
</dd>
<dt><code>maxInnerIterations</code></dt><dd><p> maximum number of inner QMR iterations.</p>
</dd>
<dt><code>LeftQ</code></dt><dd><p> use the locked vectors in the left projector.</p>
</dd>
<dt><code>LeftX</code></dt><dd><p> use the approx. eigenvector in the left projector.</p>
</dd>
<dt><code>RightQ</code></dt><dd><p> use the locked vectors in the right projector.</p>
</dd>
<dt><code>RightX</code></dt><dd><p> use the approx. eigenvector in the right projector.</p>
</dd>
<dt><code>SkewQ</code></dt><dd><p> use the preconditioned locked vectors in the right projector.</p>
</dd>
<dt><code>SkewX</code></dt><dd><p> use the preconditioned approximate eigenvector in the right
projector.</p>
</dd>
<dt><code>relTolBase</code></dt><dd><p> a legacy from classical JDQR (recommend not use).</p>
</dd>
<dt><code>iseed</code></dt><dd><p> an array of four numbers used as a random seed.</p>
</dd>
</dl>



<h3>Value</h3>

<p>list with the next elements
</p>

<dl>
<dt><code>values</code></dt><dd><p>the eigenvalues <code class="reqn">\lambda_i</code></p>
</dd>
<dt><code>vectors</code></dt><dd><p>the eigenvectors <code class="reqn">x_i</code></p>
</dd>
<dt><code>rnorms</code></dt><dd><p>the residual vector norms
<code class="reqn">\|A x_i - \lambda_i B x_i\|</code>.</p>
</dd>
<dt><code>stats$numMatvecs</code></dt><dd><p>number of matrix-vector products performed</p>
</dd>
<dt><code>stats$numPreconds</code></dt><dd><p>number of preconditioner applications performed</p>
</dd>
<dt><code>stats$elapsedTime</code></dt><dd><p>time expended by the eigensolver</p>
</dd>
<dt><code>stats$timeMatvec</code></dt><dd><p>time expended in the matrix-vector products</p>
</dd>
<dt><code>stats$timePrecond</code></dt><dd><p>time expended in applying the preconditioner</p>
</dd>
<dt><code>stats$timeOrtho</code></dt><dd><p>time expended in orthogonalizing</p>
</dd>
<dt><code>stats$estimateMinEval</code></dt><dd><p>estimation of the smallest eigenvalue of A</p>
</dd>
<dt><code>stats$estimateMaxEval</code></dt><dd><p>estimation of the largest eigenvalue of A</p>
</dd>
<dt><code>stats$estimateANorm</code></dt><dd><p>estimation of the norm of A</p>
</dd>
</dl>



<h3>References</h3>

<p>[1] A. Stathopoulos and J. R. McCombs <em>PRIMME: PReconditioned Iterative
MultiMethod Eigensolver: Methods and software description</em>, ACM
Transaction on Mathematical Software Vol. 37, No. 2, (2010)
21:1-21:30.
</p>
<p>[2] <a href="https://www.cs.wm.edu/~andreas/software/doc/primmec.html#parameters-guide">https://www.cs.wm.edu/~andreas/software/doc/primmec.html#parameters-guide</a>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+eigen">eigen</a></code> for computing all values;
<code><a href="#topic+svds">svds</a></code> for computing a few singular values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- diag(1:10)  # the eigenvalues of this matrix are 1:10 and the
                 # eigenvectors are the columns of diag(10)
r &lt;- eigs_sym(A, 3);
r$values  # the three largest eigenvalues on diag(1:10)
r$vectors # the corresponding approximate eigenvectors
r$rnorms # the corresponding residual norms
r$stats$numMatvecs # total matrix-vector products spend

r &lt;- eigs_sym(A, 3, 'SA') # compute the three smallest values

r &lt;- eigs_sym(A, 3, 2.5) # compute the three closest values to 2.5

r &lt;- eigs_sym(A, 3, 2.5, tol=1e-3); # compute the values with
r$rnorms                                    # residual norm &lt;= 1e-3*||A||

B &lt;- diag(rev(1:10));
r &lt;- eigs_sym(A, 3, B=B); # compute the 3 largest eigenpairs of
                          # the generalized problem (A,B)

# Build a Jacobi preconditioner (too convenient for a diagonal matrix!)
# and see how reduce the number matrix-vector products
A &lt;- diag(1:1000)   # we use a larger matrix to amplify the difference
P &lt;- diag(diag(A) - 2.5)
eigs_sym(A, 3, 2.5, tol=1e-3)$stats$numMatvecs
eigs_sym(A, 3, 2.5, tol=1e-3, prec=P)$stats$numMatvecs

# Passing A and the preconditioner as functions
Af &lt;- function(x) (1:100) * x; # = diag(1:100) %*% x
Pf &lt;- function(x) x / (1:100 - 2.5); # = solve(diag(1:100 - 2.5), x)
r &lt;- eigs_sym(Af, 3, 2.5, tol=1e-3, prec=Pf, n=100)

# Passing initial guesses
A &lt;- diag(1:1000)   # we use a larger matrix to amplify the difference
x0 &lt;- diag(1,1000,4) + matrix(rnorm(4000), 1000, 4)/100;
eigs_sym(A, 4, "SA", tol=1e-3)$stats$numMatvecs
eigs_sym(A, 4, "SA", tol=1e-3, x0=x0)$stats$numMatvecs

# Passing orthogonal constrain, in this case, already compute eigenvectors
r &lt;- eigs_sym(A, 4, "SA", tol=1e-3); r$values
eigs_sym(A, 4, "SA", tol=1e-3, ortho=r$vectors)$values

</code></pre>

<hr>
<h2 id='svds'>Find a few singular values and vectors on large, sparse matrix</h2><span id='topic+svds'></span>

<h3>Description</h3>

<p>Compute a few singular triplets from a specified region (the largest, the
smallest, the closest to a point) on a matrix using PRIMME [1].
Only the matrix-vector product of the matrix is required. The used method is
usually faster than a direct method (such as <code><a href="base.html#topic+svd">svd</a></code>) if
seeking few singular values and the matrix-vector product is cheap. For
accelerating the convergence consider to use preconditioning  and/or
educated initial guesses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svds(
  A,
  NSvals,
  which = "L",
  tol = 1e-06,
  u0 = NULL,
  v0 = NULL,
  orthou = NULL,
  orthov = NULL,
  prec = NULL,
  isreal = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svds_+3A_a">A</code></td>
<td>
<p>matrix or a function with signature f(x, trans) that returns
<code>A %*% x</code> when <code>trans == "n"</code> and
<code>t(Conj(A)) %*% x</code> when <code>trans == "c"</code>.</p>
</td></tr>
<tr><td><code id="svds_+3A_nsvals">NSvals</code></td>
<td>
<p>number of singular triplets to seek.</p>
</td></tr>
<tr><td><code id="svds_+3A_which">which</code></td>
<td>
<p>which singular values to find:
</p>

<dl>
<dt><code>"L"</code></dt><dd><p>the largest values;</p>
</dd>
<dt><code>"S"</code></dt><dd><p>the smallest values;</p>
</dd>
<dt>vector of numbers</dt><dd><p>the closest values to these points.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="svds_+3A_tol">tol</code></td>
<td>
<p>a triplet <code class="reqn">(\sigma,u,v)</code> is marked as converged when
<code class="reqn">\sqrt{\|Av - \sigma u\|^2+\|A^*u - \sigma v\|^2} \le tol\|A\|</code>
is smaller than <code class="reqn">tol*||A||</code>, or close to the minimum tolerance that
the selected method can achieve.</p>
</td></tr>
<tr><td><code id="svds_+3A_u0">u0</code></td>
<td>
<p>matrix whose columns are educated guesses of the left singular
vectors to find.</p>
</td></tr>
<tr><td><code id="svds_+3A_v0">v0</code></td>
<td>
<p>matrix whose columns are educated guesses of the right singular
vectors to find.</p>
</td></tr>
<tr><td><code id="svds_+3A_orthou">orthou</code></td>
<td>
<p>find left singular vectors orthogonal to the space spanned by
the columns of this matrix; useful to avoid finding some triplets or
to find new solutions.</p>
</td></tr>
<tr><td><code id="svds_+3A_orthov">orthov</code></td>
<td>
<p>find right singular vectors orthogonal to the space spanned by
the columns of this matrix.</p>
</td></tr>
<tr><td><code id="svds_+3A_prec">prec</code></td>
<td>
<p>preconditioner used to accelerated the convergence; it is a named
list of matrices or functions such as <code>solve(prec[[mode]],x)</code> or
<code>prec[[mode]](x)</code> return an approximation of <code class="reqn">OP^{-1} x</code>,
where
</p>

<table>
<tr>
 <td style="text-align: center;">
  <code>mode</code>  </td><td style="text-align: center;"> <code class="reqn">OP</code> </td>
</tr>
<tr>
 <td style="text-align: center;">
  <code>"AHA"</code> </td><td style="text-align: center;"> <code class="reqn">A^*A</code> </td>
</tr>
<tr>
 <td style="text-align: center;">
  <code>"AAH"</code> </td><td style="text-align: center;"> <code class="reqn">A A^*</code> </td>
</tr>
<tr>
 <td style="text-align: center;">
  <code>"aug"</code> </td><td style="text-align: center;"> <code class="reqn">[0 A; A^* 0]</code>
</td>
</tr>

</table>

<p>The three values haven't to be set. It is recommended to set
<code>"AHA"</code> for matrices with nrow &gt; ncol; <code>"AAH"</code> for
matrices with nrow &lt; ncol; and additionally <code>"aug"</code> for
<code>tol</code> &lt; 1e-8.</p>
</td></tr>
<tr><td><code id="svds_+3A_isreal">isreal</code></td>
<td>
<p>whether A %*% x always returns real number and not complex.</p>
</td></tr>
<tr><td><code id="svds_+3A_...">...</code></td>
<td>
<p>other PRIMME options (see details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Optional arguments to pass to PRIMME eigensolver (see further details at
[2]):
</p>

<dl>
<dt><code>aNorm</code></dt><dd><p>estimation of norm-2 of A, used in convergence test
(if not provided, it is estimated as the largest eigenvalue in 
magnitude seen)</p>
</dd>
<dt><code>maxBlockSize</code></dt><dd><p>maximum block size (like in subspace iteration
or LOBPCG)</p>
</dd>
<dt><code>printLevel</code></dt><dd><p>message level reporting, from 0 (no output) to 5
(show all)</p>
</dd> 
<dt><code>locking</code></dt><dd><p>1, hard locking; 0, soft locking</p>
</dd>
<dt><code>maxBasisSize</code></dt><dd><p>maximum size of the search subspace</p>
</dd>
<dt><code>minRestartSize</code></dt><dd><p> minimum Ritz vectors to keep in restarting</p>
</dd>
<dt><code>maxMatvecs</code></dt><dd><p> maximum number of matrix vector multiplications</p>
</dd>
<dt><code>iseed</code></dt><dd><p> an array of four numbers used as a random seed</p>
</dd>
<dt><code>method</code></dt><dd><p>which equivalent eigenproblem to solve
</p>

<dl>
<dt><code>"primme_svds_normalequation"</code></dt><dd><p><code class="reqn">A^*A</code> or <code class="reqn">AA^*</code></p>
</dd>
<dt><code>"primme_svds_augmented"</code></dt><dd> <p><code class="reqn">[0 A^*;A 0]</code></p>
</dd>
<dt><code>"primme_svds_hybrid"</code></dt><dd><p> first normal equations and
then augmented (default)</p>
</dd>
</dl>
                   
</dd>
<dt><code>locking</code></dt><dd><p>1, hard locking; 0, soft locking</p>
</dd>
<dt><code>primmeStage1, primmeStage2</code></dt><dd><p>list with options for the first
and the second stage solver; see <code><a href="#topic+eigs_sym">eigs_sym</a></code></p>
</dd>
</dl>

<p>If <code>method</code> is <code>"primme_svds_normalequation"</code>, the minimum
tolerance that can be achieved is <code class="reqn">\|A\|\epsilon/\sigma</code>, where <code class="reqn">\epsilon</code>
is the machine precision. If <code>method</code> is <code>"primme_svds_augmented"</code>
or <code>"primme_svds_hybrid"</code>, the minimum tolerance is <code class="reqn">\|A\|\epsilon</code>.
However it may not return triplets with singular values smaller than
<code class="reqn">\|A\|\epsilon</code>.
</p>


<h3>Value</h3>

<p>list with the next elements
</p>

<dl>
<dt><code>d</code></dt><dd><p>the singular values <code class="reqn">\sigma_i</code></p>
</dd>
<dt><code>u</code></dt><dd><p>the left singular vectors <code class="reqn">u_i</code></p>
</dd>
<dt><code>v</code></dt><dd><p>the right singular vectors <code class="reqn">v_i</code></p>
</dd>
<dt><code>rnorms</code></dt><dd><p>the residual vector norms
<code class="reqn">\sqrt{\|Av - \sigma u\|^2+\|A^*u - \sigma v\|^2}</code></p>
</dd>
<dt><code>stats$numMatvecs</code></dt><dd><p>matrix-vector products performed</p>
</dd>
<dt><code>stats$numPreconds</code></dt><dd><p>number of preconditioner applications performed</p>
</dd>
<dt><code>stats$elapsedTime</code></dt><dd><p>time expended by the eigensolver</p>
</dd>
<dt><code>stats$timeMatvec</code></dt><dd><p>time expended in the matrix-vector products</p>
</dd>
<dt><code>stats$timePrecond</code></dt><dd><p>time expended in applying the preconditioner</p>
</dd>
<dt><code>stats$timeOrtho</code></dt><dd><p>time expended in orthogonalizing</p>
</dd>
<dt><code>stats$estimateANorm</code></dt><dd><p>estimation of the norm of A</p>
</dd>
</dl>



<h3>References</h3>

<p>[1]  L. Wu, E. Romero and A. Stathopoulos, <em>PRIMME_SVDS: A High-Performance
Preconditioned SVD Solver for Accurate Large-Scale Computations</em>,
J. Sci. Comput., Vol. 39, No. 5, (2017), S248&ndash;S271.
</p>
<p>[2] <a href="https://www.cs.wm.edu/~andreas/software/doc/svdsc.html#parameters-guide">https://www.cs.wm.edu/~andreas/software/doc/svdsc.html#parameters-guide</a>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+svd">svd</a></code> for computing all singular triplets;
<code><a href="#topic+eigs_sym">eigs_sym</a></code> for computing a few eigenvalues and vectors
from a symmetric/Hermitian matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- diag(1:5,10,5)  # the singular values of this matrix are 1:10 and the
                        # left and right singular vectors are the columns of
                        # diag(1,100,10) and diag(10), respectively
r &lt;- svds(A, 3);
r$d # the three largest singular values on A
r$u # the corresponding approximate left singular vectors
r$v # the corresponding approximate right singular vectors
r$rnorms # the corresponding residual norms
r$stats$numMatvecs # total matrix-vector products spend

r &lt;- svds(A, 3, "S") # compute the three smallest values

r &lt;- svds(A, 3, 2.5) # compute the three closest values to 2.5

A &lt;- diag(1:500,500,100)   # we use a larger matrix to amplify the difference
r &lt;- svds(A, 3, 2.5, tol=1e-3); # compute the values with 
r$rnorms                               # residual norm &lt;= 1e-3*||A||

# Build the diagonal squared preconditioner
# and see how reduce the number matrix-vector products
P &lt;- diag(colSums(A^2))
svds(A, 3, "S", tol=1e-3)$stats$numMatvecs
svds(A, 3, "S", tol=1e-3, prec=list(AHA=P))$stats$numMatvecs

# Passing A and the preconditioner as functions
Af &lt;- function(x,mode) if (mode == "n") A%*%x else crossprod(A,x);
P = colSums(A^2);
PAHAf &lt;- function(x) x / P;
r &lt;- svds(Af, 3, "S", tol=1e-3, prec=list(AHA=PAHAf), m=500, n=100)

# Passing initial guesses
v0 &lt;- diag(1,100,4) + matrix(rnorm(400), 100, 4)/100;
svds(A, 4, "S", tol=1e-3)$stats$numMatvecs
svds(A, 4, "S", tol=1e-3, v0=v0)$stats$numMatvecs

# Passing orthogonal constrain, in this case, already compute singular vectors
r &lt;- svds(A, 4, "S", tol=1e-3); r$d
svds(A, 4, "S", tol=1e-3, orthov=r$v)$d

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
