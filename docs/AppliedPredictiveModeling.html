<!DOCTYPE html><html><head><title>Help for package AppliedPredictiveModeling</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {AppliedPredictiveModeling}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#abalone'><p>Abalone Data</p></a></li>
<li><a href='#AlzheimerDisease'><p>Alzheimer's Disease CSF Data</p></a></li>
<li><a href='#AppliedPredictiveModeling-package'>
<p>Data, Functions and Scripts for 'scriptLocation'</p></a></li>
<li><a href='#bookTheme'>
<p>Lattice Themes</p></a></li>
<li><a href='#caret-internal'><p>Internal Functions</p></a></li>
<li><a href='#ChemicalManufacturingProcess'><p>Chemical Manufacturing Process Data</p></a></li>
<li><a href='#concrete'><p>Compressive Strength of Concrete from Yeh (1998)</p></a></li>
<li><a href='#FuelEconomy'><p>Fuel Economy Data</p></a></li>
<li><a href='#getPackages'>
<p>Install Packages for Each Chapter</p></a></li>
<li><a href='#hepatic'><p>Hepatic Injury Data</p></a></li>
<li><a href='#logisticCreditPredictions'><p>Logistic Regression Predictions for the Credit Data</p></a></li>
<li><a href='#permeability'><p>Permeability Data</p></a></li>
<li><a href='#permuteRelief'>
<p>Permutation Statistics for the Relief Algorithm</p></a></li>
<li><a href='#quadBoundaryFunc'>
<p>Functions for Simulating Data</p></a></li>
<li><a href='#schedulingData'><p>HPC Job Scheduling Data</p></a></li>
<li><a href='#scriptLocation'>
<p>Find Chapter Script Files</p></a></li>
<li><a href='#segmentationOriginal'><p>Cell Body Segmentation</p></a></li>
<li><a href='#solubility'><p>Solubility Data</p></a></li>
<li><a href='#twoClassData'><p>Two Class Example Data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Functions and Data Sets for 'Applied Predictive Modeling'</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1-7</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-05-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Max Kuhn, Kjell Johnson</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Max Kuhn &lt;mxkuhn@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A few functions and several data set for the Springer book 'Applied Predictive Modeling'.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://appliedpredictivemodeling.com/">http://appliedpredictivemodeling.com/</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>CORElearn, MASS, plyr, reshape2, lattice, ellipse</td>
</tr>
<tr>
<td>Suggests:</td>
<td>caret (&ge; 6.0-22)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-05-22 19:03:00 UTC; max</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-05-22 19:14:21 UTC</td>
</tr>
</table>
<hr>
<h2 id='abalone'>Abalone Data</h2><span id='topic+abalone'></span>

<h3>Description</h3>

<p>The Abalone data consist of data from 4177 abalones. The data consist of measurements of the type (male, female and infant), the longest shell measurement, the diameter, height and several weights (whole, shucked, viscera and shell). The outcome is the number of rings. The age of the abalone is the number of rings plus 1.5. 
</p>
<p>The data are taken from the UCI database (<a href="http://archive.ics.uci.edu/ml/datasets/Abalone">http://archive.ics.uci.edu/ml/datasets/Abalone</a>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(abalone)</code></pre>


<h3>Value</h3>

<table>
<tr><td><code>abalone</code></td>
<td>
<p>a data frame with 4177 rows and 9 columns</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(abalone)
</code></pre>

<hr>
<h2 id='AlzheimerDisease'>Alzheimer's Disease CSF Data</h2><span id='topic+diagnosis'></span><span id='topic+predictors'></span>

<h3>Description</h3>

<p>Washington University conducted a clinical study to determine if biological measurements made from cerebrospinal fluid (CSF) can be used to diagnose or predict Alzheimer's disease (Craig-Schapiro et al. 2011). These data are a modified version of the values used for the publication.
</p>
<p>The R factor vector <code>diagnosis</code> contains the outcome data for 333 of the subjects. The demographic and laboratory results are collected in the data frame <code>predictors</code>.
</p>
<p>One important indicator of Alzheimer's disease is the genetic background of a subject. In particular, what versions of the Apolipoprotein E gene inherited from one's parents has an association with the disease. There are three variants of the gene: E2, E3 and E4. Since a child inherits a version of the gene from each parent, there are six possible combinations (e.g. E2/E2, E2/E3, and so on). This data is contained in the predictor column named <code>Genotype</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(AlzheimerDisease)</code></pre>


<h3>Value</h3>

<table>
<tr><td><code>diagnosis</code></td>
<td>
<p>labels for the patients, either &quot;Impaired&quot; or &quot;Control&quot;. </p>
</td></tr>
<tr><td><code>predictors</code></td>
<td>
<p>predictors for demographic data (eg. age, gender), genotype and assay results.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Craig-Schapiro, R., Kuhn, M., Xiong, C., Pickering, E. H., Liu, J., Misko, T. P., Perrin, R. J., et al. (2011). Multiplexed Immunoassay Panel Identifies Novel CSF Biomarkers for Alzheimer's Disease Diagnosis and Prognosis. PLoS ONE, 6(4), e18850.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(AlzheimerDisease)
</code></pre>

<hr>
<h2 id='AppliedPredictiveModeling-package'>
Data, Functions and Scripts for 'scriptLocation'
</h2><span id='topic+AppliedPredictiveModeling-package'></span><span id='topic+AppliedPredictiveModeling'></span>

<h3>Description</h3>

<p>This package can be used to reproduce the analyses in the text. Scripts for each chapter are located in the &quot;chapters&quot; directory. Use <code>scriptLocation()</code> to find their exact location.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> AppliedPredictiveModeling</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.1-1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2013-05-29</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Max Kuhn
</p>
<p>Maintainer: Max Kuhn &lt;mkuhn@gmail.com&gt;
</p>


<h3>References</h3>

<p>Kuhn M and Johnson K (2013) Applied Predictive Modeling, Springer, NY
</p>

<hr>
<h2 id='bookTheme'>
Lattice Themes
</h2><span id='topic+bookTheme'></span><span id='topic+transparentTheme'></span>

<h3>Description</h3>

<p>Two <span class="pkg">lattice</span> themes used throughout the book.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bookTheme(set = TRUE)

transparentTheme(set = TRUE, pchSize = 1, trans = 0.2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bookTheme_+3A_set">set</code></td>
<td>

<p>a logical: should these settings be applied to the current device?
</p>
</td></tr>
<tr><td><code id="bookTheme_+3A_pchsize">pchSize</code></td>
<td>

<p>the size of the plot symbols
</p>
</td></tr>
<tr><td><code id="bookTheme_+3A_trans">trans</code></td>
<td>

<p>the amount of transparency (via the alpha channel). Note that transparency is not supported by all graphics devices.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When using these functions to save a plot, make sure to invoke them after the device has been opened (e.g. after calls such as <code>pdf()</code>.
</p>


<h3>Value</h3>

<p>Each function returns a list of theme parameters. See Sarkar (2008) or <code><a href="lattice.html#topic+trellis.par.get">trellis.par.get</a></code> for specific details. 
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Some of the colors are based on values from ColorBrewer <a href="http://www.colorbrewer.org">http://www.colorbrewer.org</a>.
</p>
<p>Sarkar, D. (2008). Lattice: Multivariate Data Visualization with R. UseR! (1st ed. p. 286). Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lattice)

example &lt;- quadBoundaryFunc(100)

bookTheme(set = TRUE)
xyplot(X2 ~ X1, data = example, groups = class, auto.key = TRUE)

transparentTheme(set = TRUE, trans = .6)
xyplot(X2 ~ X1, data = example, groups = class, auto.key = TRUE)
</code></pre>

<hr>
<h2 id='caret-internal'>Internal Functions</h2><span id='topic+lowerp'></span><span id='topic+upperp'></span>

<h3>Description</h3>

<p>Internal functions</p>


<h3>Usage</h3>

<pre><code class='language-R'>lowerp(...)
upperp(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="caret-internal_+3A_...">...</code></td>
<td>

<p>optional arguments to pass to internal functions
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Max Kuhn</p>

<hr>
<h2 id='ChemicalManufacturingProcess'>Chemical Manufacturing Process Data</h2><span id='topic+ChemicalManufacturingProcess'></span>

<h3>Description</h3>

<p>This data set contains information about a chemical manufacturing
process, in which the goal is to understand the relationship between
the process and the resulting final product yield.  Raw material in
this process is put through a sequence of 27 steps to generate the
final pharmaceutical product.  The starting material is generated from
a biological unit and has a range of quality and characteristics.  The
objective in this project was to develop a model to predict percent
yield of the manufacturing process.  The data set consisted of 177
samples of biological material for which 57 characteristics were
measured.  Of the 57 characteristics, there were 12 measurements of
the biological starting material, and 45 measurements of the
manufacturing process.  The process variables included measurements
such as temperature, drying time, washing time, and concentrations of
by&ndash;products at various steps.  Some of the process measurements can
be controlled, while others are observed.  Predictors are continuous,
count, categorical; some are correlated, and some contain missing
values.  Samples are not independent because sets of samples come from
the same batch of biological starting material.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ChemicalManufacturingProcess)</code></pre>


<h3>Value</h3>

<p><code>ChemicalManufacturingProcess</code>: a data frame with columns for the outcome (<code>Yield</code>) and the predictors (<code>BiologicalMaterial01</code> though <code>BiologicalMaterial12</code> and <code>ManufacturingProcess01</code> though <code>ManufacturingProcess45</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ChemicalManufacturingProcess)
</code></pre>

<hr>
<h2 id='concrete'>Compressive Strength of Concrete from Yeh (1998)</h2><span id='topic+concrete'></span><span id='topic+mixtures'></span>

<h3>Description</h3>

<p>Yeh (1998) describes a collection of data sets from different sources that can be used for modeling the compressive strength of concrete formulations as a functions of their ingredients and age. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(concrete)</code></pre>


<h3>Details</h3>

<p>The data are from Yeh (1998) and taken from the UCI ML website <a href="http://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength">http://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength</a>.
</p>
<p>There are 1030 data points from the UCI website, but the paper states that approximately 1,000 samples were made, but only 727 were analyzed in the source material. It is unclear which samples were excluded.
</p>


<h3>Value</h3>

<table>
<tr><td><code>concrete</code></td>
<td>
<p>data frame of data with predictor columns <code>Cement</code>, <code>BlastFurnaceSlag</code>, <code>FlyAsh</code>, <code>Water</code>, <code>Superplasticizer</code>, <code>CoarseAggregate</code>, <code>FineAggregate</code> and <code>Age</code> with response column <code>CompressiveStrength</code>. These are the amounts.</p>
</td></tr>
<tr><td><code>mixtures</code></td>
<td>
<p>The same data where all the ingredients have been converted to proportions of the total amounts.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Yeh, I. C. (1998). Modeling of strength of high-performance concrete using artificial neural networks. <em>Cement and Concrete Research</em>, 28(12), 1797-1808. Elsevier.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(concrete)

library(caret)

### Split used in the book:
set.seed(975)
inTrain &lt;- createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training &lt;- mixtures[ inTrain,]
testing  &lt;- mixtures[-inTrain,]

</code></pre>

<hr>
<h2 id='FuelEconomy'>Fuel Economy Data</h2><span id='topic+cars2010'></span><span id='topic+cars2011'></span><span id='topic+cars2012'></span>

<h3>Description</h3>

<p>The <a href="http://fueleconomy.gov">http://fueleconomy.gov</a> website, run by the U.S. Department of Energy's Office of Energy Efficiency and Renewable Energy and the U.S. Environmental Protection Agency, lists different estimates of fuel economy for passenger cars and trucks. For each vehicle, various characteristics are recorded such as the engine displacement or number of cylinders. Along with these values, laboratory measurements are made for the city and highway miles per gallon (MPG) of the car.
</p>
<p>Predictors extracted from the website include: <code>EngDispl</code>, <code>NumCyl</code>, <code>Transmission</code>, <code>AirAspirationMethod</code>, <code>NumGears</code>, <code>TransLockup</code>, <code>TransCreeperGear</code>, <code>DriveDesc</code>, <code>IntakeValvePerCyl</code>, <code>ExhaustValvesPerCyl</code>, <code>CarlineClassDesc</code>, <code>VarValveTiming</code> and <code>VarValveLift</code>. The outcome used in the book is in column  <code>FE</code> and is the unadjusted highway data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(FuelEconomy)</code></pre>


<h3>Value</h3>

<table>
<tr><td><code>cars2010</code></td>
<td>
<p>data in cars from model year 2010. </p>
</td></tr>
<tr><td><code>cars2011</code></td>
<td>
<p>cars introduced in 2011 that were not in the model year 2010 data.</p>
</td></tr>
<tr><td><code>cars2012</code></td>
<td>
<p>cars introduced in 2012 that were not in the model year 2010 or 2011 data </p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(FuelEconomy)

library(lattice)

### Plot shown in the text:

cars2010 &lt;- cars2010[order(cars2010$EngDispl),]
cars2011 &lt;- cars2011[order(cars2011$EngDispl),]

cars2010a &lt;- cars2010
cars2010a$Year &lt;- "2010 Model Year"
cars2011a &lt;- cars2011
cars2011a$Year &lt;- "2011 Model Year"

plotData &lt;- rbind(cars2010a, cars2011a)

plotTheme &lt;- bookTheme(FALSE)
plotTheme$plot.symbol$col &lt;- rgb(.2, .2, .2, .5)
plotTheme$plot.symbol$cex &lt;- 0.7
trellis.par.set(plotTheme)

xyplot(FE ~ EngDispl|Year, plotData,
       xlab = "Engine Displacement",
       ylab = "Fuel Efficiency (MPG)",
       between = list(x = 1.2))

</code></pre>

<hr>
<h2 id='getPackages'>
Install Packages for Each Chapter
</h2><span id='topic+getPackages'></span>

<h3>Description</h3>

<p>This function identifies the physical location on the user's computer where the chapter R scripts are located.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getPackages(chapter, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getPackages_+3A_chapter">chapter</code></td>
<td>
<p>an integer vector (or character versions of the integer) for the chapter number. See Details below:</p>
</td></tr>
<tr><td><code id="getPackages_+3A_...">...</code></td>
<td>
<p>options to pass to <code><a href="utils.html#topic+install.packages">install.packages</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Chapter names and packages. about dependencies. 
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
getPackages(2)
getPackages(2:3)
getPackages("4")

## End(Not run)
</code></pre>

<hr>
<h2 id='hepatic'>Hepatic Injury Data</h2><span id='topic+bio'></span><span id='topic+chem'></span><span id='topic+injury'></span>

<h3>Description</h3>

<p>This  data set was used to develop a model for predicting compounds' probability of causing hepatic injury (i.e. liver damage). This data set consisted of 281 unique compounds; 376 predictors were measured or computed for each. The response was categorical (either &quot;None&quot;, &quot;Mild&quot; or &quot;Severe&quot; ),and was highly unbalanced.
</p>
<p>This kind of response often occurs in pharmaceutical data because companies steer away from creating molecules that have undesirable characteristics. Therefore, well-behaved molecules often greatly outnumber undesirable molecules. The predictors consisted of measurements from 184 biological screens and 192 chemical feature predictors. The biological predictors represent activity for each screen and take values between 0 and 10 with a mode of 4. The chemical feature predictors represent counts of important sub-structures as well as measures of physical properties that are thought to be associated with hepatic injury. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hepatic)</code></pre>


<h3>Value</h3>

<table>
<tr><td><code>bio</code></td>
<td>
<p>Biological screen results. </p>
</td></tr>
<tr><td><code>chem</code></td>
<td>
<p>Chemical fingerprints for sub-structures.</p>
</td></tr>
<tr><td><code>injury</code></td>
<td>
<p>A factor vector of outcomes.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(hepatic)
</code></pre>

<hr>
<h2 id='logisticCreditPredictions'>Logistic Regression Predictions for the Credit Data</h2><span id='topic+logisticCreditPredictions'></span>

<h3>Description</h3>

<p>add some notes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(solubility)</code></pre>


<h3>Value</h3>

<p>A data frame with columns
</p>
<table>
<tr><td><code>Bad</code></td>
<td>
<p>The predicted class probability for bad credit. </p>
</td></tr>
<tr><td><code>Good</code></td>
<td>
<p>The predicted class probability for good credit.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>The predicted class. </p>
</td></tr>
<tr><td><code>obs</code></td>
<td>
<p>The observed class </p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## show code to make the predictions
</code></pre>

<hr>
<h2 id='permeability'>Permeability Data</h2><span id='topic+permeability'></span><span id='topic+fingerprints'></span>

<h3>Description</h3>

<p>This pharmaceutical data set was used to develop a model for predicting compounds' permeability.  In short, permeability is the measure of a molecule's ability to cross a membrane.  The body, for example, has notable membranes between the body and brain, known as the blood-brain barrier, and between the gut and body in the intestines.  These membranes help the body guard critical regions from receiving undesirable or detrimental substances.  For an orally taken drug to be effective in the brain, it first must pass through the intestinal wall and then must pass through the blood-brain barrier in order to be present for the desired neurological target.  Therefore, a compound's ability to permeate relevant biological membranes is critically important to understand early in the drug discovery process.  Compounds that appear to be effective for a particular disease in research screening experiments, but appear to be poorly permeable may need to be altered in order improve permeability, and thus the compound's ability to reach the desired target.  Identifying permeability problems can help guide chemists towards better molecules.
</p>
<p>Permeability assays such as PAMPA and Caco-2 have been developed to help measure compounds' permeability (Kansy et al, 1998).  These screens are effective at quantifying a compound's permeability, but the assay is expensive labor intensive.  Given a sufficient number of compounds that have been screened, we could develop a predictive model for permeability in an attempt to potentially reduce the need for the assay.  In this project there were 165 unique compounds; 1107 molecular fingerprints were determined for each.  A molecular fingerprint is a binary sequence of numbers that represents the presence or absence of a specific molecular sub-structure.  The response is highly skewed, the predictors are sparse (15.5 percent are present), and many predictors are strongly associated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(permeability)</code></pre>


<h3>Value</h3>

<table>
<tr><td><code>permeability</code></td>
<td>
<p>permeability values for each compound. </p>
</td></tr>
<tr><td><code>fingerprints</code></td>
<td>
<p>a matrix of binary fingerprint indicator variables.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kansy, M., Senner, F., and Gubernator, K. (1998). Physicochemical High Throughput Screening:  Parallel Artificial Membrane Permeation Assay in the Description of Passive Absorption Processes. J. Med. Chem, 41(7), 1007-1010.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(permeability)

hist(permeability)

summary(apply(fingerprints, 2, mean))
</code></pre>

<hr>
<h2 id='permuteRelief'>
Permutation Statistics for the Relief Algorithm
</h2><span id='topic+permuteRelief'></span>

<h3>Description</h3>

<p>This function uses a permutation approach to determining the relative magnitude of Relief scores (Kira and Rendell, 1992 and Kononenko, 1994).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permuteRelief(x, y, nperm = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="permuteRelief_+3A_x">x</code></td>
<td>

<p>a data frame of predictor data
</p>
</td></tr>
<tr><td><code id="permuteRelief_+3A_y">y</code></td>
<td>

<p>a vector of outcomes
</p>
</td></tr>
<tr><td><code id="permuteRelief_+3A_nperm">nperm</code></td>
<td>

<p>the number of random permutations of the data
</p>
</td></tr>
<tr><td><code id="permuteRelief_+3A_...">...</code></td>
<td>

<p>options to pass to <code><a href="CORElearn.html#topic+attrEval">attrEval</a></code>, such as the exact Relief algorithm, to use
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The scores for each predictor are computed using the original data and after outcome data are randomly scrambled (<code>nprem</code> times). The mean and standard deviation of the permuted values are determined and a standardized version of the observed scores are determined by subtracting the permuted means from the original values, then dividing each by the corresponding standard deviation. 
</p>


<h3>Value</h3>

<p>a list with elements
</p>
<table>
<tr><td><code>standardized</code></td>
<td>
<p>a vector of standardized predictor scores</p>
</td></tr>
<tr><td><code>permutations</code></td>
<td>
<p>the values of the permuted scores, for plotting to assess the permutation distribution</p>
</td></tr>
<tr><td><code>observed</code></td>
<td>
<p>the observed scores</p>
</td></tr>
<tr><td><code>options</code></td>
<td>
<p>a list of options passed using ...</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Kira, K., &amp; Rendell, L. (1992). The feature selection problem: Traditional methods and a new algorithm. <em>Proceedings of the Eleventh International Conference on Machine Learning</em>, 129-129.
</p>
<p>Kononenko, I. (1994). Estimating attributes: analysis and extensions of RELIEF. Machine Learning: ECML-94, 171-182.
</p>


<h3>See Also</h3>

<p><code><a href="CORElearn.html#topic+attrEval">attrEval</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(874)
reliefEx3 &lt;- easyBoundaryFunc(500)
reliefEx3$X1 &lt;- scale(reliefEx3$X1)
reliefEx3$X2 &lt;- scale(reliefEx3$X2)
reliefEx3$prob &lt;- NULL

standardized &lt;- permuteRelief(reliefEx3[, 1:2], reliefEx3$class, 
                              ## For efficiency, a small number of
                              ## permutations are used here.
                              nperm = 50,           
                              estimator="ReliefFequalK", 
                              ReliefIterations= 50)

</code></pre>

<hr>
<h2 id='quadBoundaryFunc'>
Functions for Simulating Data
</h2><span id='topic+quadBoundaryFunc'></span><span id='topic+easyBoundaryFunc'></span>

<h3>Description</h3>

<p>These functions simulate data that are used in the text. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quadBoundaryFunc(n)

easyBoundaryFunc(n, intercept = 0, interaction = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quadBoundaryFunc_+3A_n">n</code></td>
<td>
<p>the sample size</p>
</td></tr>
<tr><td><code id="quadBoundaryFunc_+3A_intercept">intercept</code></td>
<td>
<p>the coefficient for the logistic regression intercept term</p>
</td></tr>
<tr><td><code id="quadBoundaryFunc_+3A_interaction">interaction</code></td>
<td>
<p>the coefficient for the logistic regression interaction term</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>quadBoundaryFunc</code> function creates a class boundary that is a function of both predictors. The probability values are based on a logistic regression model with model equation: <code class="reqn">-1-2X_1 -0.2X_1^2 + 2X_2^2</code>. The predictors here are multivariate normal with mean (1, 0) and a moderate degree of positive correlation.
</p>
<p>Similarly, the <code>easyBoundaryFunc</code> uses a logistic regression model with model equation: <code class="reqn">intercept -4X_1 + 4X_2 + interaction \times X_1 \times X_2</code>. The predictors here are multivariate normal with mean (1, 0) and a strong positive correlation.
</p>


<h3>Value</h3>

<p>Both functions return data frames with columns
</p>
<table>
<tr><td><code>X1</code></td>
<td>
<p>numeric predictor value</p>
</td></tr>
<tr><td><code>X2</code></td>
<td>
<p>numeric predictor value</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>numeric value reflecting the true probability of the first class</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>a factor variable with levels 'Class1' and 'Class2'</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## in Chapter 11, 'Measuring Performance in Classification Model'
set.seed(975)
training &lt;- quadBoundaryFunc(500)
testing &lt;- quadBoundaryFunc(1000)
 

## in Chapter 20, 'Factors That Can Affect Model Performance'
set.seed(615)
dat &lt;- easyBoundaryFunc(200, interaction = 3, intercept = 3)
dat$X1 &lt;- scale(dat$X1)
dat$X2 &lt;- scale(dat$X2)
dat$Data &lt;- "Original"
dat$prob &lt;- NULL

## in Chapter X, 'An Introduction to Feature Selection'

set.seed(874)
reliefEx3 &lt;- easyBoundaryFunc(500)
reliefEx3$X1 &lt;- scale(reliefEx3$X1)
reliefEx3$X2 &lt;- scale(reliefEx3$X2)
reliefEx3$prob &lt;- NULL

</code></pre>

<hr>
<h2 id='schedulingData'>HPC Job Scheduling Data</h2><span id='topic+schedulingData'></span>

<h3>Description</h3>

<p>These data consist of information on 4331 jobs in a high performance computing environment. Seven attributes were recorded for each job along with a discrete class describing the execution time. 
</p>
<p>The predictors are: <code>Protocol</code> (the type of computation), <code>Compounds</code> (the number of data points for each jobs), <code>InputFields</code> (the number of characteristic being estimated), <code>Iterations</code> (maximum number of iterations for the computations), <code>NumPending</code> (the number of other jobs pending at the time of launch), <code>Hour</code> (decimal hour of day for launch time) and <code>Day</code> (of launch time).
</p>
<p>The classes are: <code>VF</code> (very fast), <code>F</code> (fast), <code>M</code> (moderate) and <code>L</code> (long).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(schedulingData)</code></pre>


<h3>Value</h3>

<table>
<tr><td><code>schedulingData</code></td>
<td>
<p>a data frame with 4331 rows and 8 columns</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(schedulingData)

library(caret)

set.seed(1104)
inTrain &lt;- createDataPartition(schedulingData$Class, p = .8, list = FALSE)

schedulingData$NumPending &lt;- schedulingData$NumPending + 1

trainData &lt;- schedulingData[ inTrain,]
testData  &lt;- schedulingData[-inTrain,]

modForm &lt;- as.formula(Class ~ Protocol + log10(Compounds) +
                      log10(InputFields)+ log10(Iterations) +
                      log10(NumPending) + Hour + Day)


</code></pre>

<hr>
<h2 id='scriptLocation'>
Find Chapter Script Files
</h2><span id='topic+scriptLocation'></span>

<h3>Description</h3>

<p>This function identifies the physical location on the user's computer where the chapter R scripts are located.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scriptLocation()
</code></pre>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>Examples</h3>

<pre><code class='language-R'>scriptLocation()
</code></pre>

<hr>
<h2 id='segmentationOriginal'>Cell Body Segmentation</h2><span id='topic+segmentationOriginal'></span>

<h3>Description</h3>

<p>Hill, LaPan, Li and Haney (2007) develop models to predict which cells in a high content screen were well segmented. 
The data consists of 119 imaging measurements on 2019. The original analysis used 1009 for training and 1010 as a test set (see the column called <code>Case</code>). 
</p>
<p>The outcome class is contained in a factor variable called <code>Class</code> with levels &quot;PS&quot; for poorly segmented and &quot;WS&quot; for well segmented.
</p>
<p>A pre-processed version of these data can be found in the <span class="pkg">caret</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(segmentationOriginal)</code></pre>


<h3>Value</h3>

<table>
<tr><td><code>segmentationOriginal</code></td>
<td>
<p>data frame of cells</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Hill, LaPan, Li and Haney (2007). Impact of image segmentation on high-content screening data quality for SK-BR-3 cells, 
<em>BMC Bioinformatics</em>, Vol. 8, pg. 340, <a href="http://www.biomedcentral.com/1471-2105/8/340">http://www.biomedcentral.com/1471-2105/8/340</a>.
</p>

<hr>
<h2 id='solubility'>Solubility Data</h2><span id='topic+trainX'></span><span id='topic+solTestXtrans'></span><span id='topic+solTrainY'></span><span id='topic+solTestX'></span><span id='topic+solTrainX'></span><span id='topic+solTrainXtrans'></span><span id='topic+solTestY'></span>

<h3>Description</h3>

<p>Tetko et al. (2001) and Huuskonen (2000) investigated a set of compounds with corresponding experimental solubility values using complex sets of descriptors. They used linear regression and neural network models to estimate the relationship between chemical structure and solubility. For our analyses, we will use 1267 compounds and a set of more understandable descriptors that fall into one of three groups: 208 binary &quot;fingerprints&quot; that indicate the presence or absence of a particular chemical sub-structure, 16 count descriptors (such as the number of bonds or the number of Bromine atoms) and 4 continuous descriptors (such as molecular weight or surface area).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(solubility)</code></pre>


<h3>Value</h3>

<table>
<tr><td><code>solTrainX</code></td>
<td>
<p>training set predictors in their natural units. </p>
</td></tr>
<tr><td><code>solTrainXtrans</code></td>
<td>
<p>training set predictors after transformations for skewness and centering/scaling.</p>
</td></tr>
<tr><td><code>solTrainY</code></td>
<td>
<p>a vector of log10 solubility values for the training set. </p>
</td></tr>
<tr><td><code>solTestX</code></td>
<td>
<p>test set predictors in their natural units. </p>
</td></tr>
<tr><td><code>solTestXtrans</code></td>
<td>
<p>test set predictors after the same transformations used on the training set are applied.</p>
</td></tr>
<tr><td><code>solTestY</code></td>
<td>
<p>a vector of log10 solubility values for the training set. </p>
</td></tr>
</table>


<h3>Source</h3>

<p>Tetko, I., Tanchuk, V., Kasheva, T., and Villa, A. (2001). Estimation of aqueous solubility of chemical compounds using E-state indices. <em>Journal of Chemical Information and Computer Sciences</em>, 41(6), 1488-1493.
</p>
<p>Huuskonen, J. (2000). Estimation of aqueous solubility for a diverse set of organic compounds based on molecular topology. <em>Journal of Chemical Information and Computer Sciences</em>, 40(3), 773-777.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(solubility)

library(caret)

### Cross-validation splits used in the book:
set.seed(100)
indx &lt;- createFolds(solTrainY, returnTrain = TRUE)

### To re-create the transformed version of the data:
## Not run: 
## Find the predictors that are not fingerprints
contVars &lt;- names(solTrainX)[!grepl("FP", names(solTrainX))]
## Some have zero values, so we need to add one to them so that
## we can use the Box-Cox transformation. Alternatively, we could 
## use the Yeo-Johnson transformation without altering the data.
contPredTrain &lt;- solTrainX[,contVars] + 1
contPredTest  &lt;-  solTestX[,contVars] + 1

pp &lt;- preProcess(contPredTrain, method = "BoxCox")
contPredTrain &lt;- predict(pp, contPredTrain)
contPredTest  &lt;- predict(pp, contPredTest)

## Reassemble the fingerprint data with the transformed values.
trainXtrans &lt;- cbind(solTrainX[,grep("FP", names(solTrainX))], contPredTrain)
testXtrans  &lt;- cbind( solTestX[,grep("FP", names(solTestX))],  contPredTest)

all.equal(trainXtrans, solTrainXtrans)
all.equal(testXtrans, solTestXtrans)
	
## End(Not run)

</code></pre>

<hr>
<h2 id='twoClassData'>Two Class Example Data</h2><span id='topic+twoClassData'></span><span id='topic+classes'></span>

<h3>Description</h3>

<p>These data contain two predictors measured for 208 samples. Of these, 111 samples are 
labeled as <code>Class1</code> and the remaining 97 are <code>Class2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(twoClassData)</code></pre>


<h3>Value</h3>

<table>
<tr><td><code>predictors</code></td>
<td>
<p>data frame of two predictors</p>
</td></tr>
<tr><td><code>classes</code></td>
<td>
<p>a factor vector of class labeled</p>
</td></tr>  
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(twoClassData)

library(lattice)
xyplot(PredictorB ~ PredictorA, 
       data = predictors, 
       groups = classes, 
       auto.key = TRUE)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
