<!DOCTYPE html><html><head><title>Help for package text2map</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {text2map}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#text2map-package'><p>Text2Map</p></a></li>
<li><a href='#anchor_lists'><p>A dataset of anchor lists</p></a></li>
<li><a href='#CMDist'><p>Calculate Concept Mover's Distance</p></a></li>
<li><a href='#CoCA'><p>Performs Concept Class Analysis (CoCA)</p></a></li>
<li><a href='#doc_centrality'><p>Find a specified document centrality metric</p></a></li>
<li><a href='#doc_similarity'><p>Find a similarities between documents</p></a></li>
<li><a href='#dtm_builder'><p>A fast unigram DTM builder</p></a></li>
<li><a href='#dtm_melter'><p>Melt a DTM into a triplet data frame</p></a></li>
<li><a href='#dtm_resampler'><p>Resamples an input DTM to generate new DTMs</p></a></li>
<li><a href='#dtm_stats'><p>Gets DTM summary statistics</p></a></li>
<li><a href='#dtm_stopper'><p>Removes terms from a DTM based on rules</p></a></li>
<li><a href='#find_projection'><p>Find the 'projection matrix' to a semantic vector</p></a></li>
<li><a href='#find_rejection'><p>Find the 'rejection matrix' from a semantic vector</p></a></li>
<li><a href='#find_transformation'><p>Find a specified matrix transformation</p></a></li>
<li><a href='#ft_wv_sample'><p>Sample of fastText embeddings</p></a></li>
<li><a href='#get_anchors'><p>Gets anchor terms from precompiled anchor lists</p></a></li>
<li><a href='#get_centroid'><p>Word embedding semantic centroid extractor</p></a></li>
<li><a href='#get_direction'><p>Word embedding semantic direction extractor</p></a></li>
<li><a href='#get_regions'><p>Word embedding semantic region extractor</p></a></li>
<li><a href='#get_stoplist'><p>Gets stoplist from precompiled lists</p></a></li>
<li><a href='#jfk_speech'><p>Full Text of JFK's Rice Speech</p></a></li>
<li><a href='#Matrix'><p>Import Matrix</p></a></li>
<li><a href='#meta_shakespeare'><p>Metadata for Shakespeare's First Folio</p></a></li>
<li><a href='#perm_tester'><p>Monte Carlo Permutation Tests for Model P-Values</p></a></li>
<li><a href='#plot.CoCA'><p>Plot CoCA</p></a></li>
<li><a href='#print.CoCA'><p>Prints CoCA class information</p></a></li>
<li><a href='#rancor_builder'><p>Build a Random Corpus</p></a></li>
<li><a href='#rancors_builder'><p>Build Multiple Random Corpora</p></a></li>
<li><a href='#seq_builder'><p>Represent Documents as Token-Integer Sequences</p></a></li>
<li><a href='#stoplists'><p>A dataset of stoplists</p></a></li>
<li><a href='#test_anchors'><p>Evaluate anchor sets in defining semantic directions</p></a></li>
<li><a href='#tiny_gender_tagger'><p>A very tiny &quot;gender&quot; tagger</p></a></li>
<li><a href='#vocab_builder'><p>A fast unigram vocabulary builder</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>R Tools for Text Matrices, Embeddings, and Networks</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>This is a collection of functions optimized for working with
             with various kinds of text matrices. Focusing on 
             the text matrix as the primary object - represented 
             either as a base R dense matrix or a 'Matrix' package sparse 
             matrix - allows for a consistent and intuitive interface 
             that stays close to the underlying mathematical foundation 
             of computational text analysis. In particular, the package
             includes functions for working with word embeddings, 
             text networks, and document-term matrices. Methods developed in 
             Stoltz and Taylor (2019) &lt;<a href="https://doi.org/10.1007%2Fs42001-019-00048-6">doi:10.1007/s42001-019-00048-6</a>&gt;, 
             Taylor and Stoltz (2020) &lt;<a href="https://doi.org/10.1007%2Fs42001-020-00075-8">doi:10.1007/s42001-020-00075-8</a>&gt;, 
             Taylor and Stoltz (2020) &lt;<a href="https://doi.org/10.15195%2Fv7.a23">doi:10.15195/v7.a23</a>&gt;, and
             Stoltz and Taylor (2021) &lt;<a href="https://doi.org/10.1016%2Fj.poetic.2021.101567">doi:10.1016/j.poetic.2021.101567</a>&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://gitlab.com/culturalcartography/text2map">https://gitlab.com/culturalcartography/text2map</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://gitlab.com/culturalcartography/text2map/-/issues">https://gitlab.com/culturalcartography/text2map/-/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), Matrix (&ge; 1.4.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>text2vec, parallel, doParallel, foreach, stringi, dplyr, kit,
fastmatch, methods, qgraph (&ge; 1.6.9), igraph (&ge; 1.2.6),
rlang, ClusterR, tibble, rsvd, permute</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0), tm, quanteda</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>rmarkdown</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-10 18:28:40 UTC; dss</td>
</tr>
<tr>
<td>Author:</td>
<td>Dustin Stoltz <a href="https://orcid.org/0000-0002-4774-0765"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Marshall Taylor <a href="https://orcid.org/0000-0002-7440-0723"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dustin Stoltz &lt;dss219@lehigh.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-11 08:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='text2map-package'>Text2Map</h2><span id='topic+text2map'></span><span id='topic+text2map-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>This is a collection of functions optimized for working with with various kinds of text matrices. Focusing on the text matrix as the primary object - represented either as a base R dense matrix or a 'Matrix' package sparse matrix - allows for a consistent and intuitive interface that stays close to the underlying mathematical foundation of computational text analysis. In particular, the package includes functions for working with word embeddings, text networks, and document-term matrices. Methods developed in Stoltz and Taylor (2019) <a href="https://doi.org/10.1007/s42001-019-00048-6">doi:10.1007/s42001-019-00048-6</a>, Taylor and Stoltz (2020) <a href="https://doi.org/10.1007/s42001-020-00075-8">doi:10.1007/s42001-020-00075-8</a>, Taylor and Stoltz (2020) <a href="https://doi.org/10.15195/v7.a23">doi:10.15195/v7.a23</a>, and Stoltz and Taylor (2021) <a href="https://doi.org/10.1016/j.poetic.2021.101567">doi:10.1016/j.poetic.2021.101567</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Dustin Stoltz <a href="mailto:dss219@lehigh.edu">dss219@lehigh.edu</a> (<a href="https://orcid.org/0000-0002-4774-0765">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Marshall Taylor <a href="mailto:mtaylor2@nmsu.edu">mtaylor2@nmsu.edu</a> (<a href="https://orcid.org/0000-0002-7440-0723">ORCID</a>)
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://gitlab.com/culturalcartography/text2map">https://gitlab.com/culturalcartography/text2map</a>
</p>
</li>
<li><p> Report bugs at <a href="https://gitlab.com/culturalcartography/text2map/-/issues">https://gitlab.com/culturalcartography/text2map/-/issues</a>
</p>
</li></ul>


<hr>
<h2 id='anchor_lists'>A dataset of anchor lists</h2><span id='topic+anchor_lists'></span>

<h3>Description</h3>

<p>A dataset containing juxtaposing pairs of English words
for 26 semantic relations. These anchors are used with
the <code><a href="#topic+get_anchors">get_anchors()</a></code> function, which can then be used with
the <code><a href="#topic+get_direction">get_direction()</a></code> function. These have been collected
from previously published articles and should be used
as a starting point for defining a given relation in
a word embedding model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anchor_lists
</code></pre>


<h3>Format</h3>

<p>A data frame with 303 rows and 4 variables.
</p>


<h3>Variables</h3>

<p>Variables:
</p>

<ul>
<li><p> add. words to be added (or the positive direction)
</p>
</li>
<li><p> subtract. words to be subtract (or the negative direction)
</p>
</li>
<li><p> relation. the relation to be extracted, 26 relations available
</p>
</li>
<li><p> domain. 6 broader categories within which each relation falls
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+CoCA">CoCA</a>, <a href="#topic+get_direction">get_direction</a>, <a href="#topic+get_centroid">get_centroid</a>, <a href="#topic+get_anchors">get_anchors</a>
</p>

<hr>
<h2 id='CMDist'>Calculate Concept Mover's Distance</h2><span id='topic+CMDist'></span><span id='topic+cmdist'></span>

<h3>Description</h3>

<p>Concept Mover's Distance classifies documents of any length along a
continuous measure of engagement with a given concept of interest
using word embeddings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CMDist(
  dtm,
  cw = NULL,
  cv = NULL,
  wv,
  missing = "stop",
  scale = TRUE,
  sens_interval = FALSE,
  alpha = 1,
  n_iters = 20L,
  parallel = FALSE,
  threads = 2L,
  setup_timeout = 120L
)

cmdist(
  dtm,
  cw = NULL,
  cv = NULL,
  wv,
  missing = "stop",
  scale = TRUE,
  sens_interval = FALSE,
  alpha = 1,
  n_iters = 20L,
  parallel = FALSE,
  threads = 2L,
  setup_timeout = 120L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CMDist_+3A_dtm">dtm</code></td>
<td>
<p>Document-term matrix with words as columns. Works with DTMs
produced by any popular text analysis package, or using the
<code>dtm_builder()</code> function.</p>
</td></tr>
<tr><td><code id="CMDist_+3A_cw">cw</code></td>
<td>
<p>Vector with concept word(s) (e.g., <code>c("love", "money")</code>,
<code>c("critical thinking")</code>)</p>
</td></tr>
<tr><td><code id="CMDist_+3A_cv">cv</code></td>
<td>
<p>Concept vector(s) as output from <code><a href="#topic+get_direction">get_direction()</a></code>,
<code><a href="#topic+get_centroid">get_centroid()</a></code>, or <code><a href="#topic+get_regions">get_regions()</a></code></p>
</td></tr>
<tr><td><code id="CMDist_+3A_wv">wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as words.</p>
</td></tr>
<tr><td><code id="CMDist_+3A_missing">missing</code></td>
<td>
<p>Indicates what action to take if words are not in embeddings.
If <code>action = "stop"</code> (default), the function is stopped
and an error messages states which words are missing.
If <code>action = "remove"</code>,  output is the same as terms but
missing words or rows with missing words are removed.
Missing words will be printed as a message.</p>
</td></tr>
<tr><td><code id="CMDist_+3A_scale">scale</code></td>
<td>
<p>Logical (default = <code>FALSE</code>) uses <code>scale()</code> on output. This will
set zero to the mean of the estimates, and scale by the
standard deviation of the estimates. Document estimates will,
therefore, be relative to other documents within that specific
run, but not necessarily across discrete runs.</p>
</td></tr>
<tr><td><code id="CMDist_+3A_sens_interval">sens_interval</code></td>
<td>
<p>logical (default = <code>FALSE</code>), if <code>TRUE</code> several CMDs
will be estimate on N resampled DTMs, sensitivity
intervals are produced by returning the 2.5 and 97.5
percentiles of estimated CMDs for a given concept word
or concept vector.</p>
</td></tr>
<tr><td><code id="CMDist_+3A_alpha">alpha</code></td>
<td>
<p>If <code>sens_interval = TRUE</code>, a number indicating the proportion
of the document length to be resampled for sensitivity intervals.
Default is 1 or 100 percent of each documents' length.</p>
</td></tr>
<tr><td><code id="CMDist_+3A_n_iters">n_iters</code></td>
<td>
<p>If <code>sens_interval = TRUE</code>, integer (default = 20L) indicates
the number of resampled DTMs to produced for
sensitivity intervals</p>
</td></tr>
<tr><td><code id="CMDist_+3A_parallel">parallel</code></td>
<td>
<p>Logical (default = <code>FALSE</code>), whether to parallelize estimate</p>
</td></tr>
<tr><td><code id="CMDist_+3A_threads">threads</code></td>
<td>
<p>If <code>parallel = TRUE</code>, an integer indicating
attempts to connect to master before failing.</p>
</td></tr>
<tr><td><code id="CMDist_+3A_setup_timeout">setup_timeout</code></td>
<td>
<p>If <code>parallel = TRUE</code>, maximum number of seconds a worker
attempts to connect to master before failing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>CMDist()</code> requires three things: a (1) document-term matrix (DTM), a (2)
matrix of word embedding vectors, and (3) concept words or concept vectors.
The function uses <em>word counts</em> from the DTM and  <em>word similarities</em>
from the cosine similarity of their respective word vectors in a
word embedding model. The &quot;cost&quot; of transporting all the words in a
document to a single vector or a few vectors (denoting a
concept of interest) is the measure of engagement, with higher costs
indicating less engagement. For intuitiveness the output of <code>CMDist()</code>
is inverted such that higher numbers will indicate <em>more engagement</em>
with a concept of interest.
</p>
<p>The vector, or vectors, of the concept are specified in several ways.
The simplest involves selecting a single word from the word embeddings, the
analyst can also specify the concept by indicating a few words. The algorithm
then splits the overall flow between each concept word (roughly) depending on
which word in the document is nearest. The words need not be in the DTM, but
they must be in the word embeddings (the function will either stop or remove
words not in the embeddings).
</p>
<p>Instead of selecting a word already in the embedding space, the function can
also take a vector extracted from the embedding space in the form of a
centroid (which averages the vectors of several words) ,a direction (which
uses the offset of several juxtaposing words), or a region (which is built
by clustering words into $k$ regions). The <code><a href="#topic+get_centroid">get_centroid()</a></code>,
<code><a href="#topic+get_direction">get_direction()</a></code>, and <code><a href="#topic+get_regions">get_regions()</a></code> functions will extract these.
</p>


<h3>Value</h3>

<p>Returns a data frame with the first column as document ids and each
subsequent column as the CMD engagement corresponding to each
concept word or concept vector. The upper and lower bound
estimates will follow each unique CMD if <code>sens_interval = TRUE</code>.
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz and Marshall Taylor
</p>


<h3>References</h3>

<p>Stoltz, Dustin S., and Marshall A. Taylor. (2019)
'Concept Mover's Distance' <em>Journal of Computational
Social Science</em> 2(2):293-313.
<a href="https://doi.org/10.1007/s42001-019-00048-6">doi:10.1007/s42001-019-00048-6</a>.<br />
Taylor, Marshall A., and Dustin S. Stoltz. (2020) 'Integrating semantic
directions with concept mover's distance to measure binary concept
engagement.' <em>Journal of Computational Social Science</em> 1-12.
<a href="https://doi.org/10.1007/s42001-020-00075-8">doi:10.1007/s42001-020-00075-8</a>.<br />
Taylor, Marshall A., and Dustin S. Stoltz.
(2020) 'Concept Class Analysis: A Method for Identifying Cultural
Schemas in Texts.' <em>Sociological Science</em> 7:544-569.
<a href="https://doi.org/10.15195/v7.a23">doi:10.15195/v7.a23</a>.<br />
</p>


<h3>See Also</h3>

<p><a href="#topic+CoCA">CoCA</a>, <a href="#topic+get_direction">get_direction</a>, <a href="#topic+get_centroid">get_centroid</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# load example word embeddings
data(ft_wv_sample)

# load example text
data(jfk_speech)

# minimal preprocessing
jfk_speech$sentence &lt;- tolower(jfk_speech$sentence)
jfk_speech$sentence &lt;- gsub("[[:punct:]]+", " ", jfk_speech$sentence)

# create DTM
dtm &lt;- dtm_builder(jfk_speech, sentence, sentence_id)

# example 1
cm.dists &lt;- CMDist(dtm,
  cw = "space",
  wv = ft_wv_sample
)

# example 2
space &lt;- c("spacecraft", "rocket", "moon")
cen &lt;- get_centroid(anchors = space, wv = ft_wv_sample)

cm.dists &lt;- CMDist(dtm,
  cv = cen,
  wv = ft_wv_sample
)
</code></pre>

<hr>
<h2 id='CoCA'>Performs Concept Class Analysis (CoCA)</h2><span id='topic+CoCA'></span><span id='topic+coca'></span>

<h3>Description</h3>

<p>CoCA outputs schematic classes derived from documents' engagement
with multiple bi-polar concepts (in a Likert-style fashion).
The function requires a (1) DTM of a corpus which can be obtained using any
popular text analysis package, or from the <code><a href="#topic+dtm_builder">dtm_builder()</a></code> function, and (2)
semantic directions as output from the <code><a href="#topic+get_direction">get_direction()</a></code>.
<code><a href="#topic+CMDist">CMDist()</a></code> works under the hood. Code modified from the <code>corclass</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CoCA(
  dtm,
  wv = NULL,
  directions = NULL,
  filter_sig = TRUE,
  filter_value = 0.05,
  zero_action = c("drop", "ownclass")
)

coca(
  dtm,
  wv = NULL,
  directions = NULL,
  filter_sig = TRUE,
  filter_value = 0.05,
  zero_action = c("drop", "ownclass")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CoCA_+3A_dtm">dtm</code></td>
<td>
<p>Document-term matrix with words as columns. Works with DTMs
produced by any popular text analysis package, or you can use the
<code>dtm_builder()</code> function.</p>
</td></tr>
<tr><td><code id="CoCA_+3A_wv">wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as words.</p>
</td></tr>
<tr><td><code id="CoCA_+3A_directions">directions</code></td>
<td>
<p>direction vectors output from get_direction()</p>
</td></tr>
<tr><td><code id="CoCA_+3A_filter_sig">filter_sig</code></td>
<td>
<p>logical (default = TRUE), sets 'insignificant'
ties to 0 to decrease noise and increase stability</p>
</td></tr>
<tr><td><code id="CoCA_+3A_filter_value">filter_value</code></td>
<td>
<p>Minimum significance cutoff.
Absolute row correlations below
this value will be set to 0</p>
</td></tr>
<tr><td><code id="CoCA_+3A_zero_action">zero_action</code></td>
<td>
<p>If 'drop', CCA drops rows with
0 variance from the analyses (default).
If 'ownclass', the correlations between 0-variance
rows and all other rows is set 0, and the correlations
between all pairs of 0-var rows are set to 1</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a named list object of class <code>CoCA</code>. List elements include:
</p>

<ul>
<li><p> membership: document memberships
</p>
</li>
<li><p> modules: schematic classes
</p>
</li>
<li><p> cormat: correlation matrix
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Dustin Stoltz and Marshall Taylor
</p>


<h3>References</h3>

<p>Taylor, Marshall A., and Dustin S. Stoltz.
(2020) 'Concept Class Analysis: A Method for Identifying Cultural
Schemas in Texts.' <em>Sociological Science</em> 7:544-569.
<a href="https://doi.org/10.15195/v7.a23">doi:10.15195/v7.a23</a>.<br />
Boutyline, Andrei. 'Improving the measurement of shared cultural
schemas with correlational class analysis: Theory and method.'
Sociological Science 4.15 (2017): 353-393.
<a href="https://doi.org/10.15195/v4.a15">doi:10.15195/v4.a15</a><br />
</p>


<h3>See Also</h3>

<p><a href="#topic+CMDist">CMDist</a>, <a href="#topic+get_direction">get_direction</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#' # load example word embeddings
data(ft_wv_sample)

# load example text
data(jfk_speech)

# minimal preprocessing
jfk_speech$sentence &lt;- tolower(jfk_speech$sentence)
jfk_speech$sentence &lt;- gsub("[[:punct:]]+", " ", jfk_speech$sentence)

# create DTM
dtm &lt;- dtm_builder(jfk_speech, sentence, sentence_id)

# create semantic directions
gen &lt;- data.frame(
  add = c("woman"),
  subtract = c("man")
)

die &lt;- data.frame(
  add = c("alive"),
  subtract = c("die")
)

gen_dir &lt;- get_direction(anchors = gen, wv = ft_wv_sample)
die_dir &lt;- get_direction(anchors = die, wv = ft_wv_sample)

sem_dirs &lt;- rbind(gen_dir, die_dir)

classes &lt;- CoCA(
  dtm = dtm,
  wv = ft_wv_sample,
  directions = sem_dirs,
  filter_sig = TRUE,
  filter_value = 0.05,
  zero_action = "drop"
)

print(classes)
</code></pre>

<hr>
<h2 id='doc_centrality'>Find a specified document centrality metric</h2><span id='topic+doc_centrality'></span>

<h3>Description</h3>

<p>Given a document-term matrix or a document-similarity matrix,
this function returns specified text network-based centrality measures.
Currently, this includes degree, eigenvector, betweenness, and
spanning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>doc_centrality(mat, method, alpha = 1L, two_mode = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="doc_centrality_+3A_mat">mat</code></td>
<td>
<p>Document-term matrix with terms as columns or a
document-similarity matrix with documents as rows and columns.</p>
</td></tr>
<tr><td><code id="doc_centrality_+3A_method">method</code></td>
<td>
<p>Character vector indicating centrality method, including
&quot;degree&quot;, &quot;eigen&quot;, &quot;span&quot;, and &quot;between&quot;.</p>
</td></tr>
<tr><td><code id="doc_centrality_+3A_alpha">alpha</code></td>
<td>
<p>Number (default = 1) indicating the tuning parameter for
weighted metrics.</p>
</td></tr>
<tr><td><code id="doc_centrality_+3A_two_mode">two_mode</code></td>
<td>
<p>Logical (default = TRUE), indicating whether the input matrix
is two mode (i.e. a document-term matrix) or one-mode
(i.e. document-similarity matrix)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a document-term matrix is provided, the function obtains the one-mode
document-level projection to get the document-similarity matrix using
<code>tcrossprod()</code>. If a one-mode document-similarity matrix is provided, then
this step is skipped. This way document similiarities may be obtained
using other methods, such as Word-Mover's Distance (see <code>doc_similarity</code>).
The diagonal is ignored in all calculations.
</p>
<p>Document centrality methods include:
</p>

<ul>
<li><p> degree: Opsahl's weighted degree centrality with tuning parameter &quot;alpha&quot;
</p>
</li>
<li><p> between: vertex betweenness centrality using Brandes' method
</p>
</li>
<li><p> eigen: eigenvector centrality using Freeman's method
</p>
</li>
<li><p> span: Modified Burt's constraint following Stoltz and Taylor's method,
uses a tuning parameter &quot;alpha&quot; and the output is scaled.
</p>
</li></ul>



<h3>Value</h3>

<p>A dataframe with two columns
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>


<h3>References</h3>

<p>Brandes, Ulrik
(2000) 'A faster algorithm for betweenness centrality'
<em>Journal of Mathematical Sociology</em>. 25(2):163-177
<a href="https://doi.org/10.1080/0022250X.2001.9990249">doi:10.1080/0022250X.2001.9990249</a>.<br />
Opsahl, Tore, et al.
(2010) 'Node centrality in weighted networks: Generalizing degree
and shortest paths.' <em>Social Networks</em>. 32(3)245:251
<a href="https://doi.org/10.1016/j.socnet.2010.03.006">doi:10.1016/j.socnet.2010.03.006</a><br />
Stoltz, Dustin; Taylor, Marshall
(2019) 'Textual Spanning: Finding Discursive Holes in Text Networks'
<em>Socius</em>. <a href="https://doi.org/10.1177/2378023119827674">doi:10.1177/2378023119827674</a><br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load example text
data(jfk_speech)

# minimal preprocessing
jfk_speech$sentence &lt;- tolower(jfk_speech$sentence)
jfk_speech$sentence &lt;- gsub("[[:punct:]]+", " ", jfk_speech$sentence)

# create DTM
dtm &lt;- dtm_builder(jfk_speech, sentence, sentence_id)

ddeg &lt;- doc_centrality(dtm, method = "degree")
deig &lt;- doc_centrality(dtm, method = "eigen")
dbet &lt;- doc_centrality(dtm, method = "between")
dspa &lt;- doc_centrality(dtm, method = "span")

# with a document-similarity matrix (dsm)

dsm &lt;- doc_similarity(dtm, method = "cosine")
ddeg &lt;- doc_centrality(dsm, method = "degree", two_mode = FALSE)

</code></pre>

<hr>
<h2 id='doc_similarity'>Find a similarities between documents</h2><span id='topic+doc_similarity'></span>

<h3>Description</h3>

<p>Given a document-term matrix (DTM) this function returns the
similarities between documents using a specified method (see details).
The result is a square document-by-document similarity matrix (DSM),
equivalent to a weighted adjacency matrix in network analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>doc_similarity(x, y = NULL, method, wv = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="doc_similarity_+3A_x">x</code></td>
<td>
<p>Document-term matrix with terms as columns.</p>
</td></tr>
<tr><td><code id="doc_similarity_+3A_y">y</code></td>
<td>
<p>Optional second matrix (default = <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="doc_similarity_+3A_method">method</code></td>
<td>
<p>Character vector indicating similarity method, including
projection, cosine, wmd, and centroid (see Details).</p>
</td></tr>
<tr><td><code id="doc_similarity_+3A_wv">wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as words. Required for &quot;wmd&quot; and &quot;centroid&quot;
similarities.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Document similarity methods include:
</p>

<ul>
<li><p> projection: finds the one-mode projection matrix from the two-mode DTM
using <code>tcrossprod()</code> which measures the shared vocabulary overlap
</p>
</li>
<li><p> cosine: compares row vectors using cosine  similarity
</p>
</li>
<li><p> jaccard: compares proportion of common words to unique words in
both documents
</p>
</li>
<li><p> wmd: word mover's distance to compare documents (requires word
embedding vectors), using linear-complexity relaxed word mover's distance
</p>
</li>
<li><p> centroid: represents each document as a centroid of their respective
vocabulary, then uses cosine similarity to compare centroid vectors
(requires word embedding vectors)
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load example word embeddings
data(ft_wv_sample)

# load example text
data(jfk_speech)

# minimal preprocessing
jfk_speech$sentence &lt;- tolower(jfk_speech$sentence)
jfk_speech$sentence &lt;- gsub("[[:punct:]]+", " ", jfk_speech$sentence)

# create DTM
dtm &lt;- dtm_builder(jfk_speech, sentence, sentence_id)

dsm_prj &lt;- doc_similarity(dtm, method = "projection")
dsm_cos &lt;- doc_similarity(dtm, method = "cosine")
dsm_wmd &lt;- doc_similarity(dtm, method = "wmd", wv = ft_wv_sample)
dsm_cen &lt;- doc_similarity(dtm, method = "centroid", wv = ft_wv_sample)

</code></pre>

<hr>
<h2 id='dtm_builder'>A fast unigram DTM builder</h2><span id='topic+dtm_builder'></span>

<h3>Description</h3>

<p>A streamlined function to take raw texts from a column of a data.frame and
produce a sparse Document-Term Matrix (of generic class &quot;dgCMatrix&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_builder(
  data,
  text,
  doc_id = NULL,
  vocab = NULL,
  chunk = NULL,
  dense = FALSE,
  omit_empty = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dtm_builder_+3A_data">data</code></td>
<td>
<p>Data.frame with column of texts and column of document ids</p>
</td></tr>
<tr><td><code id="dtm_builder_+3A_text">text</code></td>
<td>
<p>Name of the column with documents' text</p>
</td></tr>
<tr><td><code id="dtm_builder_+3A_doc_id">doc_id</code></td>
<td>
<p>Name of the column with documents' unique ids.</p>
</td></tr>
<tr><td><code id="dtm_builder_+3A_vocab">vocab</code></td>
<td>
<p>Default is <code>NULL</code>, if a list of terms is provided, the function
will return a DTM with terms restricted to this vocabulary.
Columns will also be in the same order as the list of terms.</p>
</td></tr>
<tr><td><code id="dtm_builder_+3A_chunk">chunk</code></td>
<td>
<p>Default is <code>NULL</code>, if an integer is provided, the function will
&quot;re-chunk&quot; the corpus into new documents of a particular length.
For example, <code>100L</code> will divide the corpus into new documents
with 100 terms (with the final document likely including
slightly less than 100).</p>
</td></tr>
<tr><td><code id="dtm_builder_+3A_dense">dense</code></td>
<td>
<p>The default (<code>FALSE</code>) is to return a matrix of class
&quot;dgCMatrix&quot; as DTMs typically have
mostly zero cells. This is much more memory efficient.
Setting dense to <code>TRUE</code> will return a normal base <code>R</code> matrix.</p>
</td></tr>
<tr><td><code id="dtm_builder_+3A_omit_empty">omit_empty</code></td>
<td>
<p>Logical (default = <code>FALSE</code>) indicating whether to omit rows
that are empty after stopping any terms.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is fast because it has few bells and whistles:
</p>

<ul>
<li><p> No weighting schemes other than raw counts
</p>
</li>
<li><p> Tokenizes by the fixed, single whitespace
</p>
</li>
<li><p> Only tokenizes unigrams. No bigrams, trigrams, etc...
</p>
</li>
<li><p> Columns are in the order unique terms are discovered
</p>
</li>
<li><p> No preprocessing during building
</p>
</li>
<li><p> Outputs a basic sparse Matrix or dense matrix
</p>
</li></ul>

<p>Weighting or stopping terms can be done efficiently after the fact with
simple matrix operations, rather than achieved implicitly within the
function itself. For example, using the <code>dtm_stopper()</code> function.
Prior to creating the DTM, texts should have whitespace trimmed, if
desired, punctuation removed and terms lowercased.
</p>
<p>Like <code>tidytext</code>'s DTM functions, <code>dtm_builder()</code> is optimized for use
in a pipeline, but unlike <code>tidytext</code>, it does not build an intermediary
tripletlist, so <code>dtm_builder()</code> is faster and far more memory
efficient.
</p>
<p>The function can also <code>chunk</code> the corpus into documents of a given length
(default is <code>NULL</code>). If the integer provided is <code>200L</code>, this will divide
the corpus into new documents with 200 terms (with the final document
likely including slightly less than 200). If the total terms in the
corpus were less than or equal to <code>chunk</code> integer, this would produce
a DTM with one document (most will probably not want this).
</p>
<p>If the vocabulary is already known, or standardizing vocabulary across
several DTMs is desired, a list of terms can be provided to the <code>vocab</code>
argument. Columns of the DTM will be in the order of the list of terms.
</p>


<h3>Value</h3>

<p>returns a document-term matrix of class &quot;dgCMatrix&quot;
or class &quot;matrix&quot;
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

my_corpus &lt;- data.frame(
  text = c(
    "I hear babies crying I watch them grow",
    "They'll learn much more than I'll ever know",
    "And I think to myself",
    "What a wonderful world",
    "Yes I think to myself",
    "What a wonderful world"
  ),
  line_id = paste0("line", seq_len(6))
)
## some text preprocessing
my_corpus$clean_text &lt;- tolower(gsub("'", "", my_corpus$text))

# example 1 with R 4.1 pipe

dtm &lt;- my_corpus |&gt;
  dtm_builder(clean_text, line_id)


# example 2 without pipe
dtm &lt;- dtm_builder(
  data = my_corpus,
  text = clean_text,
  doc_id = line_id
)

# example 3 with dplyr pipe and mutate

dtm &lt;- my_corpus %&gt;%
  mutate(
    clean_text = gsub("'", "", text),
    clean_text = tolower(clean_text)
  ) %&gt;%
  dtm_builder(clean_text, line_id)

# example 4 with dplyr and chunk of 3 terms
dtm &lt;- my_corpus %&gt;%
  dtm_builder(clean_text,
    line_id,
    chunk = 3L
  )


# example 5 with user defined vocabulary
my.vocab &lt;- c("wonderful", "world", "haiku", "think")

dtm &lt;- dtm_builder(
  data = my_corpus,
  text = clean_text,
  doc_id = line_id,
  vocab = my.vocab
)
</code></pre>

<hr>
<h2 id='dtm_melter'>Melt a DTM into a triplet data frame</h2><span id='topic+dtm_melter'></span>

<h3>Description</h3>

<p>Converts a DTM into a data frame with three columns:
documents, terms, frequency. Each row is a unique
document by term frequency. This is akin to <code>reshape2</code>
packages <code>melt</code> function, but works on a sparse matrix.
The resulting data frame is also equivalent to the
<code>tidytext</code> triplet tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_melter(dtm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dtm_melter_+3A_dtm">dtm</code></td>
<td>
<p>Document-term matrix with terms as columns. Works with DTMs
produced by any popular text analysis package, or using the
<code>dtm_builder()</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns data frame with three columns: doc_id, term, freq
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>

<hr>
<h2 id='dtm_resampler'>Resamples an input DTM to generate new DTMs</h2><span id='topic+dtm_resampler'></span>

<h3>Description</h3>

<p>Takes any DTM and randomly resamples from each row, creating a new DTM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_resampler(dtm, alpha = NULL, n = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dtm_resampler_+3A_dtm">dtm</code></td>
<td>
<p>Document-term matrix with terms as columns. Works with DTMs
produced by any popular text analysis package, or you can use the
<code>dtm_builder()</code> function.</p>
</td></tr>
<tr><td><code id="dtm_resampler_+3A_alpha">alpha</code></td>
<td>
<p>Number indicating proportion of document lengths, e.g.,
<code>alpha = 1</code> returns resampled rows that are the same lengths
as the original DTM.</p>
</td></tr>
<tr><td><code id="dtm_resampler_+3A_n">n</code></td>
<td>
<p>Integer indicating the length of documents to be returned, e.g.,
<code>n = 100L</code> will bring documents shorter than 100 tokens up to 100,
while bringing documents longer than 100 tokens down to 100.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using the row counts as probabilities, each document's tokens are resampled
with replacement up to a certain proportion of the row count (set by alpha).
This function can be used with iteration to &quot;bootstrap&quot; a DTM without
returning to the raw text. It does not iterate, however, so operations can
be performed on one DTM at a time without storing multiple DTMs in memory.
</p>
<p>If <code>alpha</code> is less than 1, then a proportion of each documents' lengths is
returned. For example, <code>alpha = 0.50</code> will return a resampled DTM where each
row has half the tokens of the original DTM. If <code>alpha = 2</code>, than each row in
the resampled DTM twice the number of tokens of the original DTM.
If an integer is provided to <code>n</code> then all documents will be resampled to that
length. For example, <code>n = 2000L</code> will resample each document until they are
2000 tokens long &ndash; meaning those shorter than 2000 will be increased in
length, while those longer than 2000 will be decreased in length. <code>alpha</code>
and <code>n</code> should not be specified at the same time.
</p>


<h3>Value</h3>

<p>returns a document-term matrix of class &quot;dgCMatrix&quot;
</p>

<hr>
<h2 id='dtm_stats'>Gets DTM summary statistics</h2><span id='topic+dtm_stats'></span>

<h3>Description</h3>

<p><code>dtm_stats()</code> provides a summary of corpus-level statistics
using any document-term matrix. These include (1) basic information
on size (total documents, total unique terms, total tokens),
(2) lexical richness, (3) distribution information,
(4) central tendency, and (5) character-level information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_stats(
  dtm,
  richness = TRUE,
  distribution = TRUE,
  central = TRUE,
  character = TRUE,
  simplify = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dtm_stats_+3A_dtm">dtm</code></td>
<td>
<p>Document-term matrix with terms as columns. Works with DTMs
produced by any popular text analysis package, or you can use the
<code>dtm_builder()</code> function.</p>
</td></tr>
<tr><td><code id="dtm_stats_+3A_richness">richness</code></td>
<td>
<p>Logical (default = TRUE), whether to include statistics
about lexical richness, i.e. terms that occur once,
twice, and three times (hapax, dis, tris), and the total
type-token ratio.</p>
</td></tr>
<tr><td><code id="dtm_stats_+3A_distribution">distribution</code></td>
<td>
<p>Logical (default = TRUE), whether to include statistics
about the distribution, i.e. min, max st. dev, skewness,
kurtosis.</p>
</td></tr>
<tr><td><code id="dtm_stats_+3A_central">central</code></td>
<td>
<p>Logical (default = TRUE), whether to include statistics
about the central tendencies i.e. mean and median for
types and tokens.</p>
</td></tr>
<tr><td><code id="dtm_stats_+3A_character">character</code></td>
<td>
<p>Logical (default = TRUE), whether to include statistics
about the character lengths of terms, i.e. min, max, mean</p>
</td></tr>
<tr><td><code id="dtm_stats_+3A_simplify">simplify</code></td>
<td>
<p>Logical (default = FALSE), whether to return statistics as a
data frame where each statistic is a column. Default returns
a list of small data frames.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of one to five data frames with summary statistics (if
<code>simplify=FALSE</code>), otherwise a single data frame where each
statistic is a column.
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>

<hr>
<h2 id='dtm_stopper'>Removes terms from a DTM based on rules</h2><span id='topic+dtm_stopper'></span>

<h3>Description</h3>

<p><code>dtm_stopper</code> will &quot;stop&quot; terms from the analysis by removing columns in a
DTM based on stop rules. Rules include matching terms in a precompiled or
custom list, terms meeting an upper or lower document frequency threshold,
or terms meeting an upper or lower term frequency threshold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_stopper(
  dtm,
  stop_list = NULL,
  stop_termfreq = NULL,
  stop_termrank = NULL,
  stop_termprop = NULL,
  stop_docfreq = NULL,
  stop_docprop = NULL,
  stop_hapax = FALSE,
  stop_null = FALSE,
  omit_empty = FALSE,
  dense = FALSE,
  ignore_case = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dtm_stopper_+3A_dtm">dtm</code></td>
<td>
<p>Document-term matrix with terms as columns. Works with DTMs
produced by any popular text analysis package, or you can use the
<code>dtm_builder</code> function.</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_stop_list">stop_list</code></td>
<td>
<p>Vector of terms, from a precompiled stoplist or
custom list such as <code>c("never", "gonna", "give")</code>.</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_stop_termfreq">stop_termfreq</code></td>
<td>
<p>Vector of two numbers indicating the lower and upper
threshold for exclusion (see details). Use <code>Inf</code>
for max or min, respectively.</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_stop_termrank">stop_termrank</code></td>
<td>
<p>Single integer indicating upper term rank threshold
for exclusion (see details).</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_stop_termprop">stop_termprop</code></td>
<td>
<p>Vector of two numbers indicating the lower and upper
threshold for exclusion (see details). Use <code>Inf</code>
for max or min, respectively.</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_stop_docfreq">stop_docfreq</code></td>
<td>
<p>Vector of two numbers indicating the lower and upper
threshold for exclusion (see details). Use <code>Inf</code>
for max or min, respectively.</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_stop_docprop">stop_docprop</code></td>
<td>
<p>Vector of two numbers indicating the lower and upper
threshold for exclusion (see details). Use <code>Inf</code>
for max or min, respectively.</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_stop_hapax">stop_hapax</code></td>
<td>
<p>Logical (default = FALSE) indicating whether to remove
terms occurring one time (or zero times), a.k.a.
hapax legomena</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_stop_null">stop_null</code></td>
<td>
<p>Logical (default = FALSE) indicating whether to remove terms
that occur zero times in the DTM.</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_omit_empty">omit_empty</code></td>
<td>
<p>Logical (default = FALSE) indicating whether to omit rows
that are empty after stopping any terms.</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_dense">dense</code></td>
<td>
<p>The default (<code>FALSE</code>) is to return a matrix of class
&quot;dgCMatrix&quot;. Setting dense to <code>TRUE</code> will return a
normal base <code>R</code> dense matrix.</p>
</td></tr>
<tr><td><code id="dtm_stopper_+3A_ignore_case">ignore_case</code></td>
<td>
<p>Logical (default = TRUE) indicating whether to ignore
capitalization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Stopping terms by removing their respective columns in the DTM is
significantly more efficient than searching raw text with string matching
and deletion rules. Behind the scenes, the function relies on
the <code>fastmatch</code> package to quickly match/not-match terms.
</p>
<p>The <code>stop_list</code> arguments takes a list of terms which are matched and
removed from the DTM. If <code>ignore_case = TRUE</code> (the default) then word
case will be ignored.
</p>
<p>The <code>stop_termfreq</code> argument provides rules based on a term's occurrences
in the DTM as a whole &ndash; regardless of its within document frequency. If
real numbers between 0 and 1 are provided then terms will be removed by
corpus proportion. For example <code>c(0.01, 0.99)</code>, terms that are either below
1% of the total tokens or above 99% of the total tokens will be removed. If
integers are provided then terms will be removed by total count. For example
<code>c(100, 9000)</code>, occurring less than 100 or more than 9000 times in the
corpus will be removed. This also means that if <code>c(0, 1)</code> is provided, then
the will only <em>keep</em> terms occurring once.
</p>
<p>The <code>stop_termrank</code> argument provides the upper threshold for a terms' rank
in the corpus. For example, <code>5L</code> will remove the five most frequent terms.
</p>
<p>The <code>stop_docfreq</code> argument provides rules based on a term's document
frequency &ndash; i.e. the number of documents within which it occurs, regardless
of how many times it occurs. If real numbers between 0 and 1 are provided
then terms will be removed by corpus proportion. For example <code>c(0.01, 0.99)</code>,
terms in more than 99% of all documents or terms that are in less than 1% of
all documents. For example <code>c(100, 9000)</code>, then words occurring in less than
100 documents or more than 9000 documents will be removed. This means that if
<code>c(0, 1)</code> is provided, then the function will only <em>keep</em> terms occurring in
exactly one document, and remove terms in more than one.
</p>
<p>The <code>stop_hapax</code> argument is a shortcut for removing terms occurring just one
time in the corpus &ndash; called hapax legomena. Typically, a size-able portion
of the corpus tends to be hapax terms, and removing them is a quick solution
to reducing the dimensions of a DTM. The DTM must be frequency counts (not
relative frequencies).
</p>
<p>The <code>stop_null</code> argument removes terms that do not occur at all.
In other words, there is a column for the term, but the entire column
is zero. This can occur for a variety of reasons, such as starting with
a predefined vocabulary (e.g., using <a href="#topic+dtm_builder">dtm_builder</a>'s <code>vocab</code> argument) or
through some cleaning processes.
</p>
<p>The <code>omit_empty</code> argument will remove documents that are empty
</p>


<h3>Value</h3>

<p>returns a document-term matrix of class &quot;dgCMatrix&quot;
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create corpus and DTM
my_corpus &lt;- data.frame(
  text = c(
    "I hear babies crying I watch them grow",
    "They'll learn much more than I'll ever know",
    "And I think to myself",
    "What a wonderful world",
    "Yes I think to myself",
    "What a wonderful world"
  ),
  line_id = paste0("line", seq_len(6))
)
## some text preprocessing
my_corpus$clean_text &lt;- tolower(gsub("'", "", my_corpus$text))

dtm &lt;- dtm_builder(
  data = my_corpus,
  text = clean_text,
  doc_id = line_id
)

## example 1 with R 4.1 pipe

dtm_st &lt;- dtm |&gt;
  dtm_stopper(stop_list = c("world", "babies"))


## example 2 without pipe
dtm_st &lt;- dtm_stopper(
  dtm,
  stop_list = c("world", "babies")
)

## example 3 precompiled stoplist
dtm_st &lt;- dtm_stopper(
  dtm,
  stop_list = get_stoplist("snowball2014")
)

## example 4, stop top 2
dtm_st &lt;- dtm_stopper(
  dtm,
  stop_termrank = 2L
)

## example 5, stop docfreq
dtm_st &lt;- dtm_stopper(
  dtm,
  stop_docfreq = c(2, 5)
)

</code></pre>

<hr>
<h2 id='find_projection'>Find the 'projection matrix' to a semantic vector</h2><span id='topic+find_projection'></span>

<h3>Description</h3>

<p>&quot;Project&quot; each word in a word embedding matrix of <code class="reqn">D</code> dimension along a
vector of <code class="reqn">D</code> dimensions, extracted from the same embedding space.
The vector can be a single word, or a concept vector obtained from
<code><a href="#topic+get_centroid">get_centroid()</a></code>, <code><a href="#topic+get_direction">get_direction()</a></code>, or <code><a href="#topic+get_regions">get_regions()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_projection(wv, vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_projection_+3A_wv">wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as words.</p>
</td></tr>
<tr><td><code id="find_projection_+3A_vec">vec</code></td>
<td>
<p>Vector extracted from the embeddings</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All the vectors in the matrix <code class="reqn">A</code> are projected onto the a vector,
<code class="reqn">v</code>, to find the projection matrix, <code class="reqn">P</code>, defined as:
</p>
<p style="text-align: center;"><code class="reqn">P = \frac{A \cdot v}{v \cdot v} * v</code>
</p>



<h3>Value</h3>

<p>A new word embedding matrix,
each row of which is parallel to vector.
</p>

<hr>
<h2 id='find_rejection'>Find the 'rejection matrix' from a semantic vector</h2><span id='topic+find_rejection'></span>

<h3>Description</h3>

<p>&quot;Reject&quot; each word in a word embedding matrix of <code class="reqn">D</code> dimension
from a vector of <code class="reqn">D</code> dimensions, extracted from the same
embedding space. The vector can be a single word, or a concept
vector obtained from <code><a href="#topic+get_centroid">get_centroid()</a></code>, <code><a href="#topic+get_direction">get_direction()</a></code>,
or <code><a href="#topic+get_regions">get_regions()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_rejection(wv, vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_rejection_+3A_wv">wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as words.</p>
</td></tr>
<tr><td><code id="find_rejection_+3A_vec">vec</code></td>
<td>
<p>Vector extracted from the embeddings</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new word embedding matrix,
each row of which is rejected from vector.
</p>

<hr>
<h2 id='find_transformation'>Find a specified matrix transformation</h2><span id='topic+find_transformation'></span>

<h3>Description</h3>

<p>Given a matrix, <code class="reqn">B</code>, of word embedding vectors (source) with
terms as rows, this function finds a transformed matrix following a
specified operation. These include: centering (i.e.
translation) and normalization (i.e. scaling). In the first, <code class="reqn">B</code> is
centered by subtracting column means. In the second, <code class="reqn">B</code> is
normalized by the L2 norm. Both have been found to improve
word embedding representations. The function also finds a transformed
matrix that approximately aligns <code class="reqn">B</code>, with another matrix,
<code class="reqn">A</code>, of word embedding vectors (reference), using Procrustes
transformation (see details). Finally, given a term-co-occurrence matrix
built on a local corpus, the function can &quot;retrofit&quot; pretrained
embeddings to better match the local corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_transformation(
  wv,
  ref = NULL,
  method = c("align", "norm", "center", "retrofit")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_transformation_+3A_wv">wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as terms (the source matrix to be transformed).</p>
</td></tr>
<tr><td><code id="find_transformation_+3A_ref">ref</code></td>
<td>
<p>If <code>method = "align"</code>, this is the reference matrix
toward which the source matrix is to be aligned.</p>
</td></tr>
<tr><td><code id="find_transformation_+3A_method">method</code></td>
<td>
<p>Character vector indicating the method to use for
the transformation. Current methods include: &quot;align&quot;,
&quot;norm&quot;, &quot;center&quot;, and &quot;refrofit&quot; &ndash; see details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Aligning a source matrix of word embedding vectors, <code class="reqn">B</code>, to a
reference matrix, <code class="reqn">A</code>, has primarily been used as a post-processing step
for embeddings trained on longitudinal corpora for diachronic analysis
or for cross-lingual embeddings. Aligning preserves internal (cosine)
distances, while orient the source embeddings to minimize the sum of squared
distances (and is therefore a Least Squares problem).
Alignment is accomplished with the following steps:
</p>

<ul>
<li><p> translation: centering by column means
</p>
</li>
<li><p> scaling: scale (normalizes) by the L2 Norm
</p>
</li>
<li><p> rotation/reflection: rotates and a reflects to minimize
sum of squared differences, using singular value decomposition
</p>
</li></ul>

<p>Alignment is asymmetrical, and only outputs the transformed source matrix,
<code class="reqn">B</code>. Therefore, it is typically recommended to align <code class="reqn">B</code> to <code class="reqn">A</code>,
and then <code class="reqn">A</code> to <code class="reqn">B</code>. However, simplying centering and norming
<code class="reqn">A</code> after may be sufficient.
</p>


<h3>Value</h3>

<p>A new word embedding matrix,
transformed using the specified method.
</p>


<h3>References</h3>

<p>Mikel Artetxe, Gorka Labaka, and Eneko Agirre. (2018).
'A robust self-learning method for fully unsupervised
cross-lingual mappings of word embeddings.' <em>In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics</em>. 789-798<br />
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2019.
'An effective approach to unsupervised machine translation.'
<em>In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>. 194-203<br />
Hamilton, William L., Jure Leskovec, and Dan Jurafsky. (2018).
'Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.'
<a href="https://arxiv.org/abs/1605.09096v6">https://arxiv.org/abs/1605.09096v6</a>.<br />
Lin, Zefeng, Xiaojun Wan, and Zongming Guo. (2019).
'Learning Diachronic Word Embeddings with Iterative Stable
Information Alignment.' <em>Natural Language Processing and
Chinese Computing</em>. 749-60. <a href="https://doi.org/10.1007/978-3-030-32233-5_58">doi:10.1007/978-3-030-32233-5_58</a>.<br />
Schlechtweg et al. (2019). 'A Wind of Change: Detecting and
Evaluating Lexical Semantic Change across Times and Domains.'
<a href="https://arxiv.org/abs/1906.02979v1">https://arxiv.org/abs/1906.02979v1</a>.
Shoemark et a. (2019). 'Room to Glo: A Systematic Comparison
of Semantic Change Detection Approaches with Word Embeddings.'
<em>Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing</em>. 66-76. <a href="https://doi.org/10.18653/v1/D19-1007">doi:10.18653/v1/D19-1007</a>
Borg and Groenen. (1997). <em>Modern Multidimensional Scaling</em>.
New York: Springer. 340-342
</p>

<hr>
<h2 id='ft_wv_sample'>Sample of fastText embeddings</h2><span id='topic+ft_wv_sample'></span>

<h3>Description</h3>

<p>These are a sample of the English fastText embeddings
including 770 words matching those used in the <code>jfk_speech</code>.
These are intended to be used for example code.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ft_wv_sample
</code></pre>


<h3>Format</h3>

<p>A matrix of 770 rows and 300 columns
</p>

<hr>
<h2 id='get_anchors'>Gets anchor terms from precompiled anchor lists</h2><span id='topic+get_anchors'></span>

<h3>Description</h3>

<p>Produces a data.frame of juxtaposed word pairs used to extract
a semantic direction from word embeddings. Can be used as input
to <code><a href="#topic+get_direction">get_direction()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_anchors(relation)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_anchors_+3A_relation">relation</code></td>
<td>
<p>String indicating a semantic relation, 26 relations are
available in the dataset (see details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sets of juxtaposed &quot;anchor&quot; pairs are adapted from published work
and associated with a particular semantic relation. These should
be used as a starting point, not as a &quot;ground truth.&quot;
</p>
<p>Available relations include:
</p>

<ul>
<li><p> activity
</p>
</li>
<li><p> affluence
</p>
</li>
<li><p> age
</p>
</li>
<li><p> attractiveness
</p>
</li>
<li><p> borders
</p>
</li>
<li><p> concreteness
</p>
</li>
<li><p> cultivation
</p>
</li>
<li><p> dominance
</p>
</li>
<li><p> education
</p>
</li>
<li><p> gender
</p>
</li>
<li><p> government
</p>
</li>
<li><p> purity
</p>
</li>
<li><p> safety
</p>
</li>
<li><p> sexuality
</p>
</li>
<li><p> skills
</p>
</li>
<li><p> status
</p>
</li>
<li><p> valence
</p>
</li>
<li><p> whiteness
</p>
</li></ul>



<h3>Value</h3>

<p>returns a tibble with two columns
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
gen &lt;- get_anchors(relation = "gender")
</code></pre>

<hr>
<h2 id='get_centroid'>Word embedding semantic centroid extractor</h2><span id='topic+get_centroid'></span>

<h3>Description</h3>

<p>The function outputs an averaged vector from a set of anchor terms' word
vectors. This average is roughly equivalent to the intersection of the
contexts in which each word is used. This semantic centroid can be used
for a variety of ends, and specifically as input to <code><a href="#topic+CMDist">CMDist()</a></code>.
<code>get_centroid()</code> requires a list of terms, string of terms, data.frame
or matrix. In the latter two cases, the first column will be used. The
vectors are aggregated using the simple average. Terms can be repeated,
and are therefore &quot;weighted&quot; by their counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_centroid(anchors, wv, missing = "stop")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_centroid_+3A_anchors">anchors</code></td>
<td>
<p>List of terms to be averaged</p>
</td></tr>
<tr><td><code id="get_centroid_+3A_wv">wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as words.</p>
</td></tr>
<tr><td><code id="get_centroid_+3A_missing">missing</code></td>
<td>
<p>what action to take if terms are not in embeddings.
If action = &quot;stop&quot; (default), the function is stopped
and an error messages states which terms are missing.
If action = &quot;remove&quot;,  missing terms or rows with missing
terms are removed. Missing terms will be printed as a message.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a one row matrix
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load example word embeddings
data(ft_wv_sample)

space1 &lt;- c("spacecraft", "rocket", "moon")

cen1 &lt;- get_centroid(anchors = space1, wv = ft_wv_sample)

space2 &lt;- c("spacecraft rocket moon")
cen2 &lt;- get_centroid(anchors = space2, wv = ft_wv_sample)

identical(cen1, cen2)
</code></pre>

<hr>
<h2 id='get_direction'>Word embedding semantic direction extractor</h2><span id='topic+get_direction'></span>

<h3>Description</h3>

<p><code>get_direction()</code> outputs a vector corresponding to one pole of a
&quot;semantic direction&quot; built from sets of antonyms or juxtaposed terms.
The output can be used as an input to <code><a href="#topic+CMDist">CMDist()</a></code> and <code><a href="#topic+CoCA">CoCA()</a></code>. Anchors
must be a two-column data.frame or a list of length == 2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_direction(anchors, wv, method = "paired", missing = "stop", n_dirs = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_direction_+3A_anchors">anchors</code></td>
<td>
<p>A data frame or list of juxtaposed 'anchor' terms</p>
</td></tr>
<tr><td><code id="get_direction_+3A_wv">wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as terms.</p>
</td></tr>
<tr><td><code id="get_direction_+3A_method">method</code></td>
<td>
<p>Indicates the method used to generate vector offset.
Default is 'paired'. See details.</p>
</td></tr>
<tr><td><code id="get_direction_+3A_missing">missing</code></td>
<td>
<p>what action to take if terms are not in embeddings.
If action = &quot;stop&quot; (default), the function is stopped
and an error messages states which terms are missing.
If action = &quot;remove&quot;,  missing terms or rows with missing
terms are removed. Missing terms will be printed as a message.</p>
</td></tr>
<tr><td><code id="get_direction_+3A_n_dirs">n_dirs</code></td>
<td>
<p>If <code>method = "PCA"</code>, an integer indicating how many directions
to return. Default = <code>1L</code>, indicating a single,
bipolar, direction.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Semantic directions can be estimated in using a few methods:
</p>

<ul>
<li><p> 'paired' (default): each individual term is subtracted from exactly one
other paired term. there must be the same number of
terms for each side of the direction (although one
word may be used more than once).
</p>
</li>
<li><p> 'pooled': terms corresponding to one side of a direction are first
averaged, and then these averaged vectors are subtracted.
A different number of terms can be used for each side of
the direction.
</p>
</li>
<li><p> 'L2': the vector is calculated the same as with 'pooled'
but is then divided by the L2 'Euclidean' norm
</p>
</li>
<li><p> 'PCA': vector offsets are calculated for each pair of terms,
as with 'paired', and if <code>n_dirs = 1L</code> (the default)
then the direction is the first principal component.
Users can return more than one direction by increasing
the <code>n_dirs</code> parameter.
</p>
</li></ul>



<h3>Value</h3>

<p>returns a one row matrix
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>


<h3>References</h3>

<p>Bolukbasi, T., Chang, K. W., Zou, J., Saligrama, V., and Kalai, A. (2016).
Quantifying and reducing stereotypes in word embeddings. arXiv preprint
<a href="https://arxiv.org/abs/1606.06121v1">https://arxiv.org/abs/1606.06121v1</a>.<br />
Bolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama,
Adam Kalai (2016). 'Man Is to Computer Programmer as Woman Is to Homemaker?
Debiasing Word Embeddings.' Proceedings of the 30th International Conference
on Neural Information Processing Systems. 4356-4364.
<a href="https://dl.acm.org/doi/10.5555/3157382.3157584">https://dl.acm.org/doi/10.5555/3157382.3157584</a>.<br />
Taylor, Marshall A., and Dustin S. Stoltz. (2020)
'Concept Class Analysis: A Method for Identifying Cultural
Schemas in Texts.' <em>Sociological Science</em> 7:544-569.
<a href="https://doi.org/10.15195/v7.a23">doi:10.15195/v7.a23</a>.<br />
Taylor, Marshall A., and Dustin S. Stoltz. (2020) 'Integrating semantic
directions with concept mover's distance to measure binary concept
engagement.' <em>Journal of Computational Social Science</em> 1-12.
<a href="https://doi.org/10.1007/s42001-020-00075-8">doi:10.1007/s42001-020-00075-8</a>.<br />
Kozlowski, Austin C., Matt Taddy, and James A. Evans. (2019). 'The geometry
of culture: Analyzing the meanings of class through word embeddings.'
<em>American Sociological Review</em> 84(5):905-949.
<a href="https://doi.org/10.1177/0003122419877135">doi:10.1177/0003122419877135</a>.<br />
Arseniev-Koehler, Alina, and Jacob G. Foster. (2020). 'Machine learning
as a model for cultural learning: Teaching an algorithm what it means to
be fat.' arXiv preprint <a href="https://arxiv.org/abs/2003.12133v2">https://arxiv.org/abs/2003.12133v2</a>.<br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load example word embeddings
data(ft_wv_sample)

# create anchor list
gen &lt;- data.frame(
  add = c("woman"),
  subtract = c("man")
)

dir &lt;- get_direction(anchors = gen, wv = ft_wv_sample)

dir &lt;- get_direction(
  anchors = gen, wv = ft_wv_sample,
  method = "PCA", n = 1L
)
</code></pre>

<hr>
<h2 id='get_regions'>Word embedding semantic region extractor</h2><span id='topic+get_regions'></span>

<h3>Description</h3>

<p>Given a set of word embeddings of <code class="reqn">d</code> dimensions and <code class="reqn">v</code> vocabulary,
<code><a href="#topic+get_regions">get_regions()</a></code> finds <code class="reqn">k</code> semantic regions in <code class="reqn">d</code> dimensions.
This, in effect, learns latent topics from an embedding space (a.k.a.
topic modeling), which are directly comparable to both terms (with
cosine similarity) and documents (with Concept Mover's distance
using <code><a href="#topic+CMDist">CMDist()</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_regions(wv, k_regions = 5L, max_iter = 20L, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_regions_+3A_wv">wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as words.</p>
</td></tr>
<tr><td><code id="get_regions_+3A_k_regions">k_regions</code></td>
<td>
<p>Integer indicating the k number of regions to return</p>
</td></tr>
<tr><td><code id="get_regions_+3A_max_iter">max_iter</code></td>
<td>
<p>Integer indicating the maximum number of iterations
before k-means terminates.</p>
</td></tr>
<tr><td><code id="get_regions_+3A_seed">seed</code></td>
<td>
<p>Integer indicating a random seed. Default is 0, which calls
'std::time(NULL)'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To group words into more encompassing &quot;semantic regions&quot; we use <code class="reqn">k</code>-means
clustering. We choose <code class="reqn">k</code>-means primarily for it's ubiquity and the wide
range of available diagnostic tools for <code class="reqn">k</code>-means cluster.
</p>
<p>A word embedding matrix of <code class="reqn">d</code> dimensions and <code class="reqn">v</code> vocabulary is
&quot;clustered&quot; into <code class="reqn">k</code> semantic regions which have <code class="reqn">d</code> dimensions.
Each region is represented by a single point defined by the <code class="reqn">d</code>
dimensional vector. The process discretely assigns all word vectors are
assigned to a given region so as to minimize some error function, however
as the resulting regions are in the same dimensions as the word embeddings,
we can measure each terms similarity to each region. This, in effect,
is a mixed membership topic model similar to topic modeling by Latent
Dirichlet Allocation.
</p>
<p>We use the <code>KMeans_arma</code> function from the <code>ClusterR</code> package which
uses the Armadillo library.
</p>


<h3>Value</h3>

<p>returns a matrix of class &quot;dgCMatrix&quot; with k rows and d dimensions
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>


<h3>References</h3>

<p>Butnaru, Andrei M., and Radu Tudor Ionescu.  (2017)
'From image to text classification: A novel approach
based on clustering word embeddings.'
<em>Procedia computer science</em>. 112:1783-1792.
<a href="https://doi.org/10.1016/j.procs.2017.08.211">doi:10.1016/j.procs.2017.08.211</a>.<br />
Zhang, Yi, Jie Lu, Feng Liu, Qian Liu, Alan Porter,
Hongshu Chen, and Guangquan Zhang. (2018).
'Does Deep Learning Help Topic Extraction? A Kernel
K-Means Clustering Method with Word Embedding.'
<em>Journal of Informetrics</em>. 12(4):1099-1117.
<a href="https://doi.org/10.1016/j.joi.2018.09.004">doi:10.1016/j.joi.2018.09.004</a>.<br />
Arseniev-Koehler, Alina and Cochran, Susan D and
Mays, Vickie M and Chang, Kai-Wei and Foster,
Jacob Gates (2021) 'Integrating topic modeling
and word embedding to characterize violent deaths'
<a href="https://doi.org/10.31235/osf.io/nkyaq">doi:10.31235/osf.io/nkyaq</a><br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load example word embeddings
data(ft_wv_sample)

my.regions &lt;- get_regions(
  wv = ft_wv_sample,
  k_regions = 10L,
  max_iter = 10L,
  seed = 01984
)
</code></pre>

<hr>
<h2 id='get_stoplist'>Gets stoplist from precompiled lists</h2><span id='topic+get_stoplist'></span>

<h3>Description</h3>

<p>Provides access to 8 precompiled stoplists, including the most commonly used
stoplist from the Snowball stemming package (&quot;snowball2014&quot;), <code>text2map</code>'s
tiny stoplist (&quot;tiny2020&quot;), a few historically important stop lists. This
aims to be a transparent and well-document collection of stoplists. Only
includes English language stoplists at the moment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_stoplist(source = "tiny2020", language = "en", tidy = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_stoplist_+3A_source">source</code></td>
<td>
<p>Character indicating source, default = <code>"tiny2020"</code></p>
</td></tr>
<tr><td><code id="get_stoplist_+3A_language">language</code></td>
<td>
<p>Character (default = &quot;en&quot;) indicating language of stopwords
by ISO 639-1 code, currently only English is supported.</p>
</td></tr>
<tr><td><code id="get_stoplist_+3A_tidy">tidy</code></td>
<td>
<p>logical (default = <code>FALSE</code>), returns a tibble</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is no such thing as a <em>stopword</em>! But, there are <strong>tons</strong> of
precompiled lists of words that someone thinks we should remove from
our texts. (See for example: https://github.com/igorbrigadir/stopwords)
One of the first stoplists is from C.J. van Rijsbergen's &quot;Information
retrieval: theory and practice&quot; (1979) and includes 250 words.
<code>text2map</code>'s very own stoplist <code>tiny2020</code> is a lean 34 words.
</p>
<p>Below are stoplists available with <a href="#topic+get_stoplist">get_stoplist</a>:
</p>

<ul>
<li><p> &quot;tiny2020&quot;: Tiny (2020) list of 33 words (Default)
</p>
</li>
<li><p> &quot;snowball2001&quot;: Snowball stemming package's (2001) list of 127 words
</p>
</li>
<li><p> &quot;snowball2014&quot;: Updated Snowball (2014) list of 175 words
</p>
</li>
<li><p> &quot;van1979&quot;: C. J. van Rijsbergen's (1979) list of 250 words
</p>
</li>
<li><p> &quot;fox1990&quot;: Christopher Fox's (1990) list of 421 words
</p>
</li>
<li><p> &quot;smart1993&quot;: Original SMART (1993) list of 570 words
</p>
</li>
<li><p> &quot;onix2000&quot;: ONIX (2000) list of 196 words
</p>
</li>
<li><p> &quot;nltk2001&quot;: Python's NLTK (2009) list of 179 words
</p>
</li></ul>

<p>The Snowball (2014) stoplist is likely the most commonly, it is the default
in the <code>stopwords</code> package, which is used by <code>quanteda</code>, <code>tidytext</code> and
<code>tokenizers</code> packages, followed closely by the Smart (1993) stoplist,
the default in the <code>tm</code> package. The word counts for SMART (1993) and
ONIX (2000) are slightly different than in other places because of
duplicate words.
</p>


<h3>Value</h3>

<p>Character vector of words to be stopped,
if tidy = TRUE, a tibble is returned
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>

<hr>
<h2 id='jfk_speech'>Full Text of JFK's Rice Speech</h2><span id='topic+jfk_speech'></span>

<h3>Description</h3>

<p>This is a data frame for the text of JFK's Rice Speech &quot;We choose to go to
the moon.&quot; Each row is a 10 word string of the speech &ndash; roughly a sentence.
This is intended to be used for example code.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jfk_speech
</code></pre>


<h3>Format</h3>

<p>A data frame with 2 columns
</p>


<h3>Variables</h3>

<p>Variables:
</p>

<ul>
<li><p> sentence_id. Order and unique ID for the sentence
</p>
</li>
<li><p> sentence. The text of a sentence
</p>
</li></ul>


<hr>
<h2 id='Matrix'>Import Matrix</h2><span id='topic+Matrix'></span>

<h3>Description</h3>

<p>Get methods and classes from Matrix
</p>

<hr>
<h2 id='meta_shakespeare'>Metadata for Shakespeare's First Folio</h2><span id='topic+meta_shakespeare'></span>

<h3>Description</h3>

<p>Metadata related to Shakespeare's First Folio
including the IDs to download the plays from
Project Gutenberg, and a count of the number of
deaths in each play (body count).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meta_shakespeare
</code></pre>


<h3>Format</h3>

<p>A matrix of 37 rows and 8 columns
</p>


<h3>Variables</h3>

<p>Variables:
</p>

<ul>
<li><p> short_title.
</p>
</li>
<li><p> gutenberg_title.
</p>
</li>
<li><p> gutenberg_id.
</p>
</li>
<li><p> genre.
</p>
</li>
<li><p> year.
</p>
</li>
<li><p> body_count.
</p>
</li>
<li><p> boas_problem_plays.
</p>
</li>
<li><p> death.
</p>
</li></ul>


<hr>
<h2 id='perm_tester'>Monte Carlo Permutation Tests for Model P-Values</h2><span id='topic+perm_tester'></span>

<h3>Description</h3>

<p><code>perm_tester()</code> carries out Monte Carlo permutation tests for model
p-values from two-tailed, left-tailed, and/or right-tailed hypothesis
testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perm_tester(
  data,
  model,
  perm_var = NULL,
  strat_var = NULL,
  statistic,
  perm_n = 1000,
  alternative = "all",
  alpha = 0.05,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perm_tester_+3A_data">data</code></td>
<td>
<p>The dataframe from which the model is estimated.</p>
</td></tr>
<tr><td><code id="perm_tester_+3A_model">model</code></td>
<td>
<p>The model which will be estimated and re-estimated.</p>
</td></tr>
<tr><td><code id="perm_tester_+3A_perm_var">perm_var</code></td>
<td>
<p>The variable in the model that will be permuted.
Default is <code>NULL</code> which takes the first <code>Y</code>n term
in the formula of the model</p>
</td></tr>
<tr><td><code id="perm_tester_+3A_strat_var">strat_var</code></td>
<td>
<p>Categorical variable for within-stratum permutations.
Defaults to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="perm_tester_+3A_statistic">statistic</code></td>
<td>
<p>The name of the model statistic you want to &quot;grab&quot; after
re-running the model with each permutation to compare to
the original model statistic.</p>
</td></tr>
<tr><td><code id="perm_tester_+3A_perm_n">perm_n</code></td>
<td>
<p>The total number of permutations. Defaults to 1000.</p>
</td></tr>
<tr><td><code id="perm_tester_+3A_alternative">alternative</code></td>
<td>
<p>The alternative hypothesis. One of <code>"two.sided"</code>
(default),
<code>"left"</code>, <code>"right"</code>, and <code>"all"</code>. Defaults to <code>"all"</code>,
which reports the p-value statistics for all three
alternative hypotheses.</p>
</td></tr>
<tr><td><code id="perm_tester_+3A_alpha">alpha</code></td>
<td>
<p>Alpha level for the hypothesis test. Defaults to 0.05.</p>
</td></tr>
<tr><td><code id="perm_tester_+3A_seed">seed</code></td>
<td>
<p>Optional seed for reproducibility of the p-value statistics.
Defaults to null.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>perm_tester()</code> can be used to derive p-values under the randomization
model of inference. There are various reasons one might want to do this&mdash;
with text data, and observational data more generally, this might be
because the corpus/sample is not a random sample from a target population.
In such cases, population model p-values might not make much sense since
the asymptotically-derived standard errors from which they are constructed
themselves do not make sense. We might therefore want to make inferences
on the basis of whether or not randomness, as a data-generating mechanism,
might reasonably account for a statistic at least as extreme as the one
we observed. <code>perm_tester()</code> works from this idea.
</p>
<p><code>perm_tester()</code> works like this. First, the model (supplied the <code>model</code>
parameter) is run on the observed data. Second, we take some statistic of
interest, which we indicate with the <code>statistic</code> parameter, and set it to
the side. Third, a variable, <code>perm_var</code>, is permuted&mdash;meaning the observed
values for the rows of <code>data</code> on <code>perm_var</code> are randomly reshuffled. Fourth,
we estimate the model again, this time with the permuted <code>perm_var</code>. Fifth,
we get grab that same <code>statistic</code>. We repeat steps two through
five a total of <code>perm_n</code> times, each time tallying the number of times the
<code>statistic</code> from the permutation-derived model is greater than or equal to
(for a right-tailed test), less-than or equal to (for a left-tailed test),
and/or has an absolute value greater than or equal to (for a two-tailed test)
the <code>statistic</code> from the &quot;real&quot; model.
</p>
<p>If we divide those tallies by the total number of permutations, then we
get randomization-based p-values. This is what <code>perm_tester()</code> does. The
null hypothesis is that randomness could likely generate the statistic
that we observe. The alternative hypothesis is that randomness alone likely
can't account for the observed statistic.
</p>
<p>We then reject the null hypothesis if the p-value is below a threshold indicated
with <code>alpha</code>, which, as in population-based inference, is the probability
below which we are willing to reject the null hypothesis when it is actually
true. So if the p-value is below, say, <code>alpha</code> = 0.05 and we're performing,
a right-tailed test, then fewer than 5% of the statistics derived from the
permutation-based models are greater than or equal to our observed
statistic. We would then reject the null, as it is unlikely (based on our <code>alpha</code>
threshold), that randomness as a data-generating mechanism can account
for a test statistic at least as large the one we observed.
</p>
<p>In most cases, analysts probably cannot expect to perform &quot;exact&quot; permutation
tests where every possible permutation is accounted for&mdash;i.e., where
<code>perm_n</code> equals the total number of possible permutations. Instead, we
can take random samples of the &quot;population&quot; of permutations. <code>perm_tester()</code>
does this, and reports the standard errors and (1 - <code>alpha</code>) confidence
intervals for the p-values.
</p>
<p><code>perm_tester()</code> can also perform stratified permutation tests, where the observed
<code>perm_var</code> variables within groups. This can be done by setting the <code>strat_var</code>
variable to be he grouping variable.
</p>


<h3>Value</h3>

<p>Returns a data frame with the observed statistic (<code>stat</code>), the
p-values (<code>P_left</code>, for left-tailed, <code>P_right</code> for right-tailed,
and/or
<code>P_two</code> for two-tailed), and the standard errors and confidence
intervals for those p-values, respectively.
</p>


<h3>Author(s)</h3>

<p>Marshall Taylor and Dustin Stoltz
</p>


<h3>References</h3>

<p>Taylor, Marshall A. (2020)
'Visualization Strategies for Regression Estimates with Randomization
Inference' <em>Stata Journal</em> 20(2):309-335.
<a href="https://doi.org/10.1177/1536867X20930999">doi:10.1177/1536867X20930999</a>.<br />
</p>
<p>#' Darlington, Richard B. and Andrew F. Hayes (2016)
<em>Regression analysis and linear models: Concepts, applications, and implementation</em>.
Guilford Publications.<br />
</p>
<p>Ernst, Michael D. (2004)
'permutation methods: a basis for exact inference' <em>Statistical Scicence</em>
19(4):676-685.
<a href="https://doi.org/10.1214/088342304000000396">doi:10.1214/088342304000000396</a>.<br />
</p>
<p>Manly, Bryan F. J. (2007)
<em>Randomization, Bootstrap and Monte Carlo Methods in Biology</em>.
Chapman and Hall/CRC.
<a href="https://doi.org/10.1201/9781315273075">doi:10.1201/9781315273075</a>.<br />
</p>


<h3>See Also</h3>

<p><a href="#topic+CMDist">CMDist</a>, <a href="#topic+CoCA">CoCA</a>, <a href="#topic+get_direction">get_direction</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- text2map::meta_shakespeare

model &lt;- lm(body_count ~ boas_problem_plays + year + genre, data = data)

# without stratified permutations, two-sided test
out1 &lt;- perm_tester(
  data = data,
  model = model,
  statistic = "coefficients",
  perm_n = 40,
  alternative = "two.sided",
  alpha = .01,
  seed = 8675309
)

# with stratified permutations, two-sided test
out2 &lt;- perm_tester(
  data = data,
  model = model,
  strat_var = "boas_problem_plays",
  statistic = "coefficients",
  perm_n = 40,
  alternative = "two.sided",
  alpha = .01,
  seed = 8675309
)


</code></pre>

<hr>
<h2 id='plot.CoCA'>Plot CoCA</h2><span id='topic+plot.CoCA'></span>

<h3>Description</h3>

<p>Plot CoCA
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CoCA'
plot(
  x,
  module = NULL,
  cutoff = 0.05,
  repulse = 1.86,
  min = 0.15,
  max = 1,
  main = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.CoCA_+3A_x">x</code></td>
<td>
<p>CoCA object returned by <code><a href="#topic+CoCA">CoCA()</a></code></p>
</td></tr>
<tr><td><code id="plot.CoCA_+3A_module">module</code></td>
<td>
<p>index for which module to plot (default = NULL)</p>
</td></tr>
<tr><td><code id="plot.CoCA_+3A_cutoff">cutoff</code></td>
<td>
<p>minimum absolute value of correlations to plot</p>
</td></tr>
<tr><td><code id="plot.CoCA_+3A_repulse">repulse</code></td>
<td>
<p>repulse radius in the spring layout</p>
</td></tr>
<tr><td><code id="plot.CoCA_+3A_min">min</code></td>
<td>
<p>edges with absolute weights under this value are
not shown (default = 0.15)</p>
</td></tr>
<tr><td><code id="plot.CoCA_+3A_max">max</code></td>
<td>
<p>highest weight to scale the edge widths too (default = 1)</p>
</td></tr>
<tr><td><code id="plot.CoCA_+3A_main">main</code></td>
<td>
<p>title for plot (default = NULL)</p>
</td></tr>
<tr><td><code id="plot.CoCA_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns <code>qgraph</code> object
</p>

<hr>
<h2 id='print.CoCA'>Prints CoCA class information</h2><span id='topic+print.CoCA'></span>

<h3>Description</h3>

<p>Prints CoCA class information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CoCA'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.CoCA_+3A_x">x</code></td>
<td>
<p>CoCA object returned by <code>CoCA()</code></p>
</td></tr>
<tr><td><code id="print.CoCA_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>prints a message indicating the classes and sizes
</p>

<hr>
<h2 id='rancor_builder'>Build a Random Corpus</h2><span id='topic+rancor_builder'></span>

<h3>Description</h3>

<p><code>rancor_builder()</code> generates a random corpus (rancor) based on a user
defined term probabilities and vocabulary. Users can set the number of
documents, as well as the mean, standard deviation, minimum, and maximum
document lengths (i.e., number of tokens) of the parent normal distribution
from which the document lengths are randomly sampled. The output is a single
document-term matrix. To produce multiple random corpora, use
<code>rancors_builder()</code> (note the plural). Term probabilities/vocabulary can
come from a users own corpus, or a pre-compiled frequency list, such
as the one derived from the Google Book N-grams corpus
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rancor_builder(
  data,
  vocab,
  probs,
  n_docs = 100L,
  len_mean = 500,
  len_var = 10L,
  len_min = 20L,
  len_max = 1000L,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rancor_builder_+3A_data">data</code></td>
<td>
<p>Data.frame containing vocabulary and probabilities</p>
</td></tr>
<tr><td><code id="rancor_builder_+3A_vocab">vocab</code></td>
<td>
<p>Name of the column containing vocabulary</p>
</td></tr>
<tr><td><code id="rancor_builder_+3A_probs">probs</code></td>
<td>
<p>Name of the column containing probabilities</p>
</td></tr>
<tr><td><code id="rancor_builder_+3A_n_docs">n_docs</code></td>
<td>
<p>Integer indicating the number of documents to be returned</p>
</td></tr>
<tr><td><code id="rancor_builder_+3A_len_mean">len_mean</code></td>
<td>
<p>Integer indicating the mean of the document lengths in the
parent normal sampling distribution</p>
</td></tr>
<tr><td><code id="rancor_builder_+3A_len_var">len_var</code></td>
<td>
<p>Integer indicating the standard deviation of the
document lengths in the parent normal sampling distribution</p>
</td></tr>
<tr><td><code id="rancor_builder_+3A_len_min">len_min</code></td>
<td>
<p>Integer indicating the minimum of the document lengths
in the parent normal sampling distribution</p>
</td></tr>
<tr><td><code id="rancor_builder_+3A_len_max">len_max</code></td>
<td>
<p>Integer indicating the maximum of the document lengths
in the parent normal sampling distribution</p>
</td></tr>
<tr><td><code id="rancor_builder_+3A_seed">seed</code></td>
<td>
<p>Optional seed for reproducibility</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dustin Stoltz and Marshall Taylor
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create corpus and DTM
my_corpus &lt;- data.frame(
  text = c(
    "I hear babies crying I watch them grow",
    "They'll learn much more than I'll ever know",
    "And I think to myself",
    "What a wonderful world",
    "Yes I think to myself",
    "What a wonderful world"
  ),
  line_id = paste0("line", seq_len(6))
)
## some text preprocessing
my_corpus$clean_text &lt;- tolower(gsub("'", "", my_corpus$text))

dtm &lt;- dtm_builder(
  data = my_corpus,
  text = clean_text,
  doc_id = line_id
)

# use colSums to get term frequencies
df &lt;- data.frame(
  terms = colnames(dtm),
  freqs = colSums(dtm)
)
# convert to probabilities
df$probs &lt;- df$freqs / sum(df$freqs)

# create random DTM
rDTM &lt;- df |&gt;
  rancor_builder(terms, probs)

</code></pre>

<hr>
<h2 id='rancors_builder'>Build Multiple Random Corpora</h2><span id='topic+rancors_builder'></span>

<h3>Description</h3>

<p><code>rancors_builder()</code> generates multiple random corpus (rancor) based on a user
defined term probabilities and vocabulary. sers can set the number of
documents, as well as the mean, standard deviation, minimum, and maximum
document lengths (i.e., number of tokens) of the parent normal distribution
from which the document lengths are randomly sampled. The output is a list of
document-term matrices. To produce a <em>single</em> random corpus, use
<code>rancor_builder()</code> (note the singular).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rancors_builder(
  data,
  vocab,
  probs,
  n_cors,
  n_docs,
  len_mean,
  len_var,
  len_min,
  len_max,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rancors_builder_+3A_data">data</code></td>
<td>
<p>Data.frame containing vocabulary and probabilities</p>
</td></tr>
<tr><td><code id="rancors_builder_+3A_vocab">vocab</code></td>
<td>
<p>Name of the column containing vocabulary</p>
</td></tr>
<tr><td><code id="rancors_builder_+3A_probs">probs</code></td>
<td>
<p>Name of the column containing probabilities</p>
</td></tr>
<tr><td><code id="rancors_builder_+3A_n_cors">n_cors</code></td>
<td>
<p>Integer indicating the number of corpora to build</p>
</td></tr>
<tr><td><code id="rancors_builder_+3A_n_docs">n_docs</code></td>
<td>
<p>Integer(s) indicating the number of documents to be returned
If two numbers are provide, number will be randomly sampled
within the range for each corpora.</p>
</td></tr>
<tr><td><code id="rancors_builder_+3A_len_mean">len_mean</code></td>
<td>
<p>Integer(s) indicating the mean of the document lengths
in the parent normal sampling distribution. If two
numbers are provided, number will be randomly sampled
within the range for each corpora.</p>
</td></tr>
<tr><td><code id="rancors_builder_+3A_len_var">len_var</code></td>
<td>
<p>Integer(s) indicating the standard deviation of the
document lengths in the parent normal sampling distribution.
If two numbers are provided, number will be randomly sampled
within the range for each corpora.</p>
</td></tr>
<tr><td><code id="rancors_builder_+3A_len_min">len_min</code></td>
<td>
<p>Integer(s) indicating the minimum of the document lengths
in the parent normal sampling distribution. If two numbers
are provided, number will be randomly sampled within the
range for each corpora.</p>
</td></tr>
<tr><td><code id="rancors_builder_+3A_len_max">len_max</code></td>
<td>
<p>Integer(s) indicating the maximum of the document lengths
in the parent normal sampling distribution. If two numbers
are provided, number will be randomly sampled within the
range for each corpora.</p>
</td></tr>
<tr><td><code id="rancors_builder_+3A_seed">seed</code></td>
<td>
<p>Optional seed for reproducibility</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dustin Stoltz and Marshall Taylor
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# create corpus and DTM
my_corpus &lt;- data.frame(
  text = c(
    "I hear babies crying I watch them grow",
    "They'll learn much more than I'll ever know",
    "And I think to myself",
    "What a wonderful world",
    "Yes I think to myself",
    "What a wonderful world"
  ),
  line_id = paste0("line", seq_len(6))
)
## some text preprocessing
my_corpus$clean_text &lt;- tolower(gsub("'", "", my_corpus$text))

dtm &lt;- dtm_builder(
  data = my_corpus,
  text = clean_text,
  doc_id = line_id
)

# use colSums to get term frequencies
df &lt;- data.frame(
  vocab = colnames(dtm),
  freqs = colSums(dtm)
)
# convert to probabilities
df$probs &lt;- df$freqs / sum(df$freqs)

# create random DTM
ls_dtms &lt;- df |&gt;
  rancors_builder(vocab,
    probs,
    n_cors = 20,
    n_docs = 100,
    len_mean = c(50, 200),
    len_var = 5,
    len_min = 20,
    len_max = 1000,
    seed = 59801
  )
length(ls_dtms)

</code></pre>

<hr>
<h2 id='seq_builder'>Represent Documents as Token-Integer Sequences</h2><span id='topic+seq_builder'></span>

<h3>Description</h3>

<p>First, each token in the vocabulary is mapped to an integer
in a lookup dictionary. Next, documents are converted to sequences
of integers where each integer is an index of the token
from the dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>seq_builder(
  data,
  text,
  doc_id = NULL,
  vocab = NULL,
  maxlen = NULL,
  matrix = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="seq_builder_+3A_data">data</code></td>
<td>
<p>Data.frame with column of texts and column of document ids</p>
</td></tr>
<tr><td><code id="seq_builder_+3A_text">text</code></td>
<td>
<p>Name of the column with documents' text</p>
</td></tr>
<tr><td><code id="seq_builder_+3A_doc_id">doc_id</code></td>
<td>
<p>Name of the column with documents' unique ids.</p>
</td></tr>
<tr><td><code id="seq_builder_+3A_vocab">vocab</code></td>
<td>
<p>Default is <code>NULL</code>, if a list of terms is provided,
the function will return a DTM with terms restricted
to this vocabulary. Columns will also be in the same
order as the list of terms.</p>
</td></tr>
<tr><td><code id="seq_builder_+3A_maxlen">maxlen</code></td>
<td>
<p>Integer indicating the maximum document length.
If NULL (default), the length of the longest document is used.</p>
</td></tr>
<tr><td><code id="seq_builder_+3A_matrix">matrix</code></td>
<td>
<p>Logical, <code>TRUE</code> (default) returns a matrix, <code>FALSE</code> a list</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function will return a matrix of integer sequences by default.
The columns will be the length of the longest document or
<code>maxlen</code>, with shorter documents padded with zeros. The
dictionary will be an attribute of the matrix accessed with
<code>attr(seq, "dic")</code>. If <code>matrix = FALSE</code>, the function will
return a list of integer sequences. The vocabulary will either
be each unique token in the corpus, or a the list of words
provided to the <code>vocab</code> argument. This kind of text
representation is used in <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer">tensorflow</a>
and <a href="https://tensorflow.rstudio.com/reference/keras/texts_to_sequences">keras</a>.
</p>


<h3>Value</h3>

<p>returns a matrix or list
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>

<hr>
<h2 id='stoplists'>A dataset of stoplists</h2><span id='topic+stoplists'></span>

<h3>Description</h3>

<p>A dataset containing eight English stoplist. Is used
with the <code><a href="#topic+get_stoplist">get_stoplist()</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stoplists
</code></pre>


<h3>Format</h3>

<p>A data frame with 1775 rows and 2 variables.
</p>


<h3>Details</h3>

<p>The stoplists include:
</p>

<ul>
<li><p> &quot;tiny2020&quot;: Tiny (2020) list of 33 words (Default)
</p>
</li>
<li><p> &quot;snowball2001&quot;: Snowball (2001) list of 127 words
</p>
</li>
<li><p> &quot;snowball2014&quot;: Updated Snowball (2014) list of 175 words
</p>
</li>
<li><p> &quot;van1979&quot;: van Rijsbergen's (1979) list of 250 words
</p>
</li>
<li><p> &quot;fox1990&quot;: Christopher Fox's (1990) list of 421 words
</p>
</li>
<li><p> &quot;smart1993&quot;: Original SMART (1993) list of 570 words
</p>
</li>
<li><p> &quot;onix2000&quot;: ONIX (2000) list of 196 words
</p>
</li>
<li><p> &quot;nltk2001&quot;: Python's NLTK (2009) list of 179 words
</p>
</li></ul>

<p>Tiny 2020, is a very small stop list of the most frequent
English conjunctions, articles, prepositions, and
demonstratives (N=17). Also includes the 8 forms of the
copular verb &quot;to be&quot; and the 8 most frequent personal
(singular and plural) pronouns (minus gendered
and possessive pronouns).
</p>
<p>No contractions are included.
</p>


<h3>Variables</h3>

<p>Variables:
</p>

<ul>
<li><p> words. words to be stopped
</p>
</li>
<li><p> source. source of the list
</p>
</li></ul>


<hr>
<h2 id='test_anchors'>Evaluate anchor sets in defining semantic directions</h2><span id='topic+test_anchors'></span>

<h3>Description</h3>

<p>This function evaluates how well an anchor set
defines a semantic direction. Anchors must be a
two-column data.frame or a list of length == 2.
Currently, the function only implements the &quot;PairDir&quot; metric
developed by Boutyline and Johnston (2023).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_anchors(anchors, wv, method = c("pairdir"), all = FALSE, summarize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_anchors_+3A_anchors">anchors</code></td>
<td>
<p>A data frame or list of juxtaposed 'anchor' terms</p>
</td></tr>
<tr><td><code id="test_anchors_+3A_wv">wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as terms.</p>
</td></tr>
<tr><td><code id="test_anchors_+3A_method">method</code></td>
<td>
<p>Which metric used to evaluate (currently only pairdir)</p>
</td></tr>
<tr><td><code id="test_anchors_+3A_all">all</code></td>
<td>
<p>Logical (default <code>FALSE</code>). Whether to evaluate all possible
pairwise combinations of two sets of anchors. If <code>FALSE</code> only
the input pairs are used in evaluation and anchor sets must be
of equal lengths.</p>
</td></tr>
<tr><td><code id="test_anchors_+3A_summarize">summarize</code></td>
<td>
<p>Logical (default <code>TRUE</code>). Returns a dataframe with AVERAGE
scores for input pairs along with each pairs' contribution.
If <code>summarize = FALSE</code>, returns a list with each
offset matrix, each contribution, and the average score.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>According to Boutyline and Johnston (2023):
</p>
<p>&quot;We find that  PairDir &ndash; a measure of parallelism between the offset
vectors (and thus of the internal reliability of the estimated relation)
&ndash; consistently outperforms other reliability metrics in
explaining axis accuracy.&quot;
</p>
<p>Boutyline and Johnston only consider analyst specified pairs. However,
if <code>all = TRUE</code>, all pairwise combinations of terms between each set
are evaluated. This can allow for unequal sets of anchors, however this
increases computational complexity considerably.
</p>


<h3>Value</h3>

<p>dataframe or list
</p>


<h3>References</h3>

<p>Boutyline, Andrei, and Ethan Johnston. 2023.
Forging Better Axes: Evaluating and Improving
the Measurement of Semantic Dimensions in Word Embeddings.
<a href="https://doi.org/10.31235/osf.io/576h3">doi:10.31235/osf.io/576h3</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# load example word embeddings
data(ft_wv_sample)

df_anchors &lt;- data.frame(
  a = c("rest", "rested", "stay", "stand"),
  z = c("coming", "embarked", "fast", "move")
)

test_anchors(df_anchors, ft_wv_sample)

test_anchors(df_anchors, ft_wv_sample, all = TRUE)

</code></pre>

<hr>
<h2 id='tiny_gender_tagger'>A very tiny &quot;gender&quot; tagger</h2><span id='topic+tiny_gender_tagger'></span>

<h3>Description</h3>

<p>Provides a small dictionary which matches common English pronouns
and nouns to conventional gender categories (&quot;masculine&quot; or
&quot;feminine&quot;). There are 20 words in each category.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tiny_gender_tagger()
</code></pre>


<h3>Value</h3>

<p>returns a tibble with two columns
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>

<hr>
<h2 id='vocab_builder'>A fast unigram vocabulary builder</h2><span id='topic+vocab_builder'></span>

<h3>Description</h3>

<p>A streamlined function to take raw texts from a column of a data.frame and
produce a list of all the unique tokens. Tokenizes by the fixed,
single whitespace, and then extracts the unique tokens. This can be used as
input to <code>dtm_builder()</code> to standardize the vocabulary (i.e. the columns)
across multiple DTMs. Prior to building the vocabulary, texts should have
whitespace trimmed, if desired, punctuation removed and terms lowercased.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vocab_builder(data, text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vocab_builder_+3A_data">data</code></td>
<td>
<p>Data.frame with one column of texts</p>
</td></tr>
<tr><td><code id="vocab_builder_+3A_text">text</code></td>
<td>
<p>Name of the column with documents' text</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list of unique terms in a corpus
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
