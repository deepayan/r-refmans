<!DOCTYPE html><html lang="en-US"><head><title>Help for package sacRebleu</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sacRebleu}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#sacRebleu'><p>sacRebleu: An R package for calculating BLEU scores</p></a></li>
<li><a href='#bleu_corpus_ids'><p>Computes BLEU score (Papineni et al., 2002).</p></a></li>
<li><a href='#bleu_sentence_ids'><p>Computes BLEU-Score (Papineni et al., 2002).</p></a></li>
<li><a href='#validate_arguments'><p>Validate Arguments</p></a></li>
<li><a href='#validate_references'><p>Validate References</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Metrics for Assessing the Quality of Generated Text</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-19</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of the BLEU-Score in 'C++' to evaluate the
    quality of generated text. The BLEU-Score, introduced by Papineni et al. (2002)
    &lt;<a href="https://doi.org/10.3115%2F1073083.1073135">doi:10.3115/1073083.1073135</a>&gt;, is a metric for evaluating the quality of
    generated text. It is based on the n-gram overlap between the generated
    text and reference texts. Additionally, the package provides some smoothing
    methods as described in Chen and Cherry (2014) &lt;<a href="https://doi.org/10.3115%2Fv1%2FW14-3346">doi:10.3115/v1/W14-3346</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>libclang/llvm-config</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>checkmate, Rcpp (&ge; 1.0.12)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/LazerLambda/sacRebleu">https://github.com/LazerLambda/sacRebleu</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/LazerLambda/sacRebleu/issues">https://github.com/LazerLambda/sacRebleu/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0), vctrs, withr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>Config/rextendr/version:</td>
<td>0.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-21 18:15:29 UTC; philko</td>
</tr>
<tr>
<td>Author:</td>
<td>Philipp Koch [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Philipp Koch &lt;PhillKoch@protonmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-22 08:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='sacRebleu'>sacRebleu: An R package for calculating BLEU scores</h2><span id='topic+sacRebleu-package'></span><span id='topic+sacRebleu'></span>

<h3>Description</h3>

<p>This package provides functions for calculating BLEU scores,
a common metric for evaluating machine translation models.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Philipp Koch <a href="mailto:PhillKoch@protonmail.com">PhillKoch@protonmail.com</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/LazerLambda/sacRebleu">https://github.com/LazerLambda/sacRebleu</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/LazerLambda/sacRebleu/issues">https://github.com/LazerLambda/sacRebleu/issues</a>
</p>
</li></ul>


<hr>
<h2 id='bleu_corpus_ids'>Computes BLEU score (Papineni et al., 2002).</h2><span id='topic+bleu_corpus_ids'></span>

<h3>Description</h3>

<p>'bleu_sentence_ids' computes the BLEU score for a corpus and its respective reference sentences.
The sentences must be tokenized before so they are represented as integer vectors.
Akin to 'sacrebleu' ('Python'), the function allows the application of different smoothing methods.
Epsilon- and add-k-smoothing are available. Epsilon-smoothing is equivalent to 'floor'
smoothing in the sacreBLEU implementation.
The different smoothing techniques are described in Chen et al., 2014
(https://aclanthology.org/W14-3346/).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bleu_corpus_ids(
  references,
  candidates,
  n = 4,
  weights = NULL,
  smoothing = NULL,
  epsilon = 0.1,
  k = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bleu_corpus_ids_+3A_references">references</code></td>
<td>
<p>A list of a list of reference sentences ('list(list(c(1,2,...)), list(c(3,5,...)))').</p>
</td></tr>
<tr><td><code id="bleu_corpus_ids_+3A_candidates">candidates</code></td>
<td>
<p>A list of candidate sentences ('list(c(1,2,...), c(3,5,...))').</p>
</td></tr>
<tr><td><code id="bleu_corpus_ids_+3A_n">n</code></td>
<td>
<p>N-gram for BLEU score (default is set to 4).</p>
</td></tr>
<tr><td><code id="bleu_corpus_ids_+3A_weights">weights</code></td>
<td>
<p>Weights for the n-grams (default is set to 1/n for each entry).</p>
</td></tr>
<tr><td><code id="bleu_corpus_ids_+3A_smoothing">smoothing</code></td>
<td>
<p>Smoothing method for BLEU score (default is set to 'standard', 'floor', 'add-k' available)</p>
</td></tr>
<tr><td><code id="bleu_corpus_ids_+3A_epsilon">epsilon</code></td>
<td>
<p>Epsilon value for epsilon-smoothing (default is set to 0.1).</p>
</td></tr>
<tr><td><code id="bleu_corpus_ids_+3A_k">k</code></td>
<td>
<p>K value for add-k-smoothing (default is set to 1).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The BLEU score for the candidate sentence.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cand_corpus &lt;- list(c(1,2,3), c(1,2))
ref_corpus &lt;- list(list(c(1,2,3), c(2,3,4)), list(c(1,2,6), c(781, 21, 9), c(7, 3)))
bleu_corpus_ids_standard &lt;- bleu_corpus_ids(ref_corpus, cand_corpus)
bleu_corpus_ids_floor &lt;- bleu_corpus_ids(ref_corpus, cand_corpus, smoothing="floor", epsilon=0.01)
bleu_corpus_ids_add_k &lt;- bleu_corpus_ids(ref_corpus, cand_corpus, smoothing="add-k", k=1)
</code></pre>

<hr>
<h2 id='bleu_sentence_ids'>Computes BLEU-Score (Papineni et al., 2002).</h2><span id='topic+bleu_sentence_ids'></span>

<h3>Description</h3>

<p>'bleu_sentence_ids' computes the BLEU score for a single candidate sentence and a list of reference sentences.
The sentences must be tokenized before so they are represented as integer vectors.
Akin to 'sacrebleu' ('Python'), the function allows the application of different smoothing methods.
Epsilon- and add-k-smoothing are available. Epsilon-smoothing is equivalent to 'floor'
smoothing in the sacrebleu implementation.
The different smoothing techniques are described in Chen et al., 2014
(https://aclanthology.org/W14-3346/).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bleu_sentence_ids(
  references,
  candidate,
  n = 4,
  weights = NULL,
  smoothing = NULL,
  epsilon = 0.1,
  k = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bleu_sentence_ids_+3A_references">references</code></td>
<td>
<p>A list of reference sentences.</p>
</td></tr>
<tr><td><code id="bleu_sentence_ids_+3A_candidate">candidate</code></td>
<td>
<p>A candidate sentence.</p>
</td></tr>
<tr><td><code id="bleu_sentence_ids_+3A_n">n</code></td>
<td>
<p>N-gram for BLEU score (default is set to 4).</p>
</td></tr>
<tr><td><code id="bleu_sentence_ids_+3A_weights">weights</code></td>
<td>
<p>Weights for the n-grams (default is set to 1/n for each entry).</p>
</td></tr>
<tr><td><code id="bleu_sentence_ids_+3A_smoothing">smoothing</code></td>
<td>
<p>Smoothing method for BLEU score (default is set to 'standard', 'floor', 'add-k' available)</p>
</td></tr>
<tr><td><code id="bleu_sentence_ids_+3A_epsilon">epsilon</code></td>
<td>
<p>Epsilon value for epsilon-smoothing (default is set to 0.1).</p>
</td></tr>
<tr><td><code id="bleu_sentence_ids_+3A_k">k</code></td>
<td>
<p>K value for add-k-smoothing (default is set to 1).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The BLEU score for the candidate sentence.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ref_corpus &lt;- list(c(1,2,3,4))
cand_corpus &lt;- c(1,2,3,5)
bleu_standard &lt;- bleu_sentence_ids(ref_corpus, cand_corpus)
bleu_floor &lt;- bleu_sentence_ids(ref_corpus, cand_corpus, smoothing="floor", epsilon=0.01)
bleu_add_k &lt;- bleu_sentence_ids(ref_corpus, cand_corpus, smoothing="add-k", k=1)
</code></pre>

<hr>
<h2 id='validate_arguments'>Validate Arguments</h2><span id='topic+validate_arguments'></span>

<h3>Description</h3>

<p>Validate Arguments
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_arguments(weights, smoothing, n)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_arguments_+3A_weights">weights</code></td>
<td>
<p>Weight vector for 'bleu_corpus_ids' and 'bleu_sentence_ids' functions</p>
</td></tr>
<tr><td><code id="validate_arguments_+3A_smoothing">smoothing</code></td>
<td>
<p>Smoothing method for 'bleu_corpus_ids' and 'bleu_sentence_ids' functions</p>
</td></tr>
<tr><td><code id="validate_arguments_+3A_n">n</code></td>
<td>
<p>N-gram for 'bleu_corpus_ids' and 'bleu_sentence_ids' functions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the validated arguments (weights and smoothing)
</p>

<hr>
<h2 id='validate_references'>Validate References</h2><span id='topic+validate_references'></span>

<h3>Description</h3>

<p>Validate References
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_references(references, target)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_references_+3A_references">references</code></td>
<td>
<p>A list of reference sentences.</p>
</td></tr>
<tr><td><code id="validate_references_+3A_target">target</code></td>
<td>
<p>A vector of target lengths.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A boolean value indicating if the references are valid.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
