<!DOCTYPE html><html lang="en"><head><title>Help for package anticlust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {anticlust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#anticlustering'><p>Anticlustering</p></a></li>
<li><a href='#balanced_clustering'><p>Create balanced clusters of equal size</p></a></li>
<li><a href='#bicriterion_anticlustering'><p>Bicriterion iterated local search heuristic</p></a></li>
<li><a href='#categorical_sampling'><p>Random sampling employing a categorical constraint</p></a></li>
<li><a href='#categories_to_binary'><p>Get binary representation of categorical variables</p></a></li>
<li><a href='#dispersion_objective'><p>Cluster dispersion</p></a></li>
<li><a href='#diversity_objective'><p>(Anti)cluster editing &quot;diversity&quot; objective</p></a></li>
<li><a href='#fast_anticlustering'><p>Fast anticlustering</p></a></li>
<li><a href='#generate_exchange_partners'><p>Get exchange partners for fast_anticlustering()</p></a></li>
<li><a href='#generate_partitions'><p>Generate all partitions of same cardinality</p></a></li>
<li><a href='#kplus_anticlustering'><p>K-plus anticlustering</p></a></li>
<li><a href='#kplus_moment_variables'><p>Compute k-plus variables</p></a></li>
<li><a href='#matching'><p>Matching</p></a></li>
<li><a href='#mean_sd_tab'><p>Means and standard deviations by group variable formatted in table</p></a></li>
<li><a href='#n_partitions'><p>Number of equal sized partitions</p></a></li>
<li><a href='#optimal_anticlustering'><p>Optimal (&quot;exact&quot;) algorithms for anticlustering</p></a></li>
<li><a href='#optimal_dispersion'><p>Maximize dispersion for K groups</p></a></li>
<li><a href='#plot_clusters'><p>Visualize a cluster analysis</p></a></li>
<li><a href='#plot_similarity'><p>Plot similarity objective by cluster</p></a></li>
<li><a href='#schaper2019'><p>Ratings for 96 words</p></a></li>
<li><a href='#three_phase_search_anticlustering'><p>Three phase search with dynamic population size heuristic</p></a></li>
<li><a href='#variance_objective'><p>Objective value for the variance criterion</p></a></li>
<li><a href='#wce'><p>Exact weighted cluster editing</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Subset Partitioning via Anticlustering</td>
</tr>
<tr>
<td>Version:</td>
<td>0.8.10</td>
</tr>
<tr>
<td>Author:</td>
<td>Martin Papenberg <a href="https://orcid.org/0000-0002-9900-4268"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Meik Michalke [ctb] (centroid based clustering algorithm),
  Gunnar W. Klau [ths],
  Juliane V. Nagel [ctb] (package logo),
  Martin Breuer [ctb] (Bicriterion algorithm by Brusco et al.),
  Marie L. Schaper [ctb] (Example data set),
  Max Diekhoff [ctb] (Optimal maximum dispersion algorithm),
  Hannah Hengelbrock [ctb] (TPSDP heuristic by Yang et al.)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Martin Papenberg &lt;martin.papenberg@hhu.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The method of anticlustering partitions a pool of elements into groups (i.e., anticlusters) with the goal of maximizing between-group similarity or within-group heterogeneity. The anticlustering approach thereby reverses the logic of cluster analysis that strives for high within-group homogeneity and clear separation between groups.  Computationally, anticlustering is accomplished by maximizing instead of minimizing a clustering objective function, such as the intra-cluster variance (used in k-means clustering) or the sum of pairwise distances within clusters. The main function anticlustering() gives access to optimal and heuristic anticlustering methods described in Papenberg and Klau (2021; &lt;<a href="https://doi.org/10.1037%2Fmet0000301">doi:10.1037/met0000301</a>&gt;), Brusco et al. (2020; &lt;<a href="https://doi.org/10.1111%2Fbmsp.12186">doi:10.1111/bmsp.12186</a>&gt;), Papenberg (2024; &lt;<a href="https://doi.org/10.1111%2Fbmsp.12315">doi:10.1111/bmsp.12315</a>&gt;), and Papenberg et al. (2025; &lt;<a href="https://doi.org/10.1101%2F2025.03.03.641320">doi:10.1101/2025.03.03.641320</a>&gt;). The optimal algorithms require that an integer linear programming solver is installed. This package will install 'lpSolve' (<a href="https://cran.r-project.org/package=lpSolve">https://cran.r-project.org/package=lpSolve</a>) as a default solver, but it is also possible to use the package 'Rglpk' (<a href="https://cran.r-project.org/package=Rglpk">https://cran.r-project.org/package=Rglpk</a>), which requires the GNU linear programming kit (<a href="https://www.gnu.org/software/glpk/glpk.html">https://www.gnu.org/software/glpk/glpk.html</a>), the package 'Rsymphony' (<a href="https://cran.r-project.org/package=Rsymphony">https://cran.r-project.org/package=Rsymphony</a>), which requires the SYMPHONY ILP solver (<a href="https://github.com/coin-or/SYMPHONY">https://github.com/coin-or/SYMPHONY</a>), or the commercial solver Gurobi, which provides its own R package that is not available via CRAN (<a href="https://www.gurobi.com/downloads/">https://www.gurobi.com/downloads/</a>). 'Rglpk', 'Rsymphony', 'gurobi' and their system dependencies have to be manually installed by the user because they are only suggested dependencies. Full access to the bicriterion anticlustering method proposed by Brusco et al. (2020) is given via the function bicriterion_anticlustering(), while kplus_anticlustering() implements the full functionality of the k-plus anticlustering approach proposed by Papenberg (2024). Some other functions are available to solve classical clustering problems. The function balanced_clustering() applies a cluster analysis under size constraints, i.e., creates equal-sized clusters. The function matching() can be used for (unrestricted, bipartite, or K-partite) matching. The function wce() can be used optimally solve the (weighted) cluster editing problem, also known as correlation clustering, clique partitioning problem or transitivity clustering.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/m-Py/anticlust">https://github.com/m-Py/anticlust</a>,
<a href="https://m-py.github.io/anticlust/">https://m-py.github.io/anticlust/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/m-Py/anticlust/issues">https://github.com/m-Py/anticlust/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, RANN (&ge; 2.6.0), lpSolve</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, palmerpenguins, Rglpk, rmarkdown, Rsymphony, tinytest,
gurobi</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Rendering the vignette requires pandoc
(&lt;https://pandoc.org/&gt;).</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-13 12:50:52 UTC; martinp</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-13 23:20:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='anticlustering'>Anticlustering</h2><span id='topic+anticlustering'></span>

<h3>Description</h3>

<p>Partition a pool of elements into groups (i.e., anticlusters) with
the aim of creating high within-group heterogeneity and high
between-group similarity.  Anticlustering is accomplished by
maximizing instead of minimizing a clustering objective function.
Implements anticlustering methods as described in Papenberg and
Klau (2021; &lt;doi:10.1037/met0000301&gt;), Brusco et al. 
(2020; &lt;doi:10.1111/bmsp.12186&gt;), Papenberg (2024; 
&lt;doi:10.1111/bmsp.12315&gt;), and Papenberg et al. (2025; 
&lt;doi:10.1101/2025.03.03.641320&gt;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anticlustering(
  x,
  K,
  objective = "diversity",
  method = "exchange",
  preclustering = FALSE,
  categories = NULL,
  repetitions = NULL,
  standardize = FALSE,
  cannot_link = NULL,
  must_link = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="anticlustering_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A
feature matrix where rows correspond to elements and columns
correspond to variables (a single numeric variable can be
passed as a vector). (2) An N x N matrix dissimilarity matrix;
can be an object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_k">K</code></td>
<td>
<p>How many anticlusters should be created. Alternatively:
(a) A vector describing the size of each group, or (b) a vector
of length <code>nrow(x)</code> describing how elements are assigned
to anticlusters before the optimization starts.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_objective">objective</code></td>
<td>
<p>The objective to be maximized. The options
&quot;diversity&quot; (default; previously called &quot;distance&quot;, which is
still supported), &quot;average-diversity&quot;, &quot;variance&quot;, &quot;kplus&quot; and &quot;dispersion&quot; are
natively supported. May also be a user-defined function. See
Details.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_method">method</code></td>
<td>
<p>One of &quot;exchange&quot; (default) , &quot;local-maximum&quot;,
&quot;brusco&quot;, &quot;ilp&quot;, or &quot;2PML&quot;.  See Details.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_preclustering">preclustering</code></td>
<td>
<p>Boolean. Should a preclustering be conducted
before anticlusters are created? Defaults to <code>FALSE</code>. See
Details.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_categories">categories</code></td>
<td>
<p>A vector, data.frame or matrix representing one
or several categorical variables whose distribution should be similar 
between groups. See Details.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_repetitions">repetitions</code></td>
<td>
<p>The number of times a search heuristic is
initiated when using <code>method = "exchange"</code>, <code>method =
"local-maximum"</code>, <code>method = "brusco"</code>, or <code>method = "2PML"</code>. 
In the end, the best objective found across the repetitions is returned.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_standardize">standardize</code></td>
<td>
<p>Boolean. If <code>TRUE</code> and <code>x</code> is a
feature matrix, the data is standardized through a call to
<code><a href="base.html#topic+scale">scale</a></code> before the optimization starts. This
argument is silently ignored if <code>x</code> is a distance matrix.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_cannot_link">cannot_link</code></td>
<td>
<p>A 2 column matrix where each row has the indices 
of two elements that must not be assigned to the same anticluster.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_must_link">must_link</code></td>
<td>
<p>A numeric vector of length <code>nrow(x)</code>. Elements having 
the same value in this vector are assigned to the same anticluster.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to solve anticlustering. That is, the data
input is divided into <code>K</code> groups in such a way that elements
within groups are heterogeneous and the different groups are
similar. Anticlustering is accomplished by maximizing instead of
minimizing a clustering objective function. The maximization of
five objectives is natively supported (other
functions can also defined by the user as described below):
</p>

<ul>
<li><p>the diversity, setting 
<code>objective = "diversity"</code> (this is the default objective)
</p>
</li>
<li><p>the average diversity, which normalizes the diversity by cluster size, 
setting <code>objective = "average-diversity"</code>
</p>
</li>
<li><p>the k-means (or &quot;variance&quot;) objective, setting <code>objective = "variance"</code>
</p>
</li>
<li><p>the k-plus objective, an extension of the k-means objective,
setting <code>objective = "kplus"</code>
</p>
</li>
<li><p>the dispersion, which is the minimum distance between 
any two elements within the same cluster (setting 
<code>objective = "dispersion"</code>)
</p>
</li></ul>

<p>The k-means objective is the within-group variance&mdash;that is, the
sum of the squared distances between each element and its cluster
center (see <code><a href="#topic+variance_objective">variance_objective</a></code>). K-means
anticlustering focuses on minimizing differences with regard to the
means of the input variables (that is, the columns in <code>x</code>), but it ignores any other distribution
characterstics such as the variance / standard deviation. K-plus anticlustering
(using <code>objective = "kplus"</code>) is an extension of the k-means criterion that also
minimizes differences with regard to the standard
deviations between groups (for details see <code><a href="#topic+kplus_anticlustering">kplus_anticlustering</a></code>). K-plus
anticlustering can also be extended towards higher order moments such as skew and kurtosis; 
to consider these additional distribution characteristics, use the function
<code><a href="#topic+kplus_anticlustering">kplus_anticlustering</a></code>. Setting <code>objective = "kplus"</code> in 
<code>anticlustering</code> function will only consider means 
and standard deviations (in my experience, this is what users usually want). 
It is strongly recommended to set the argument <code>standardize = TRUE</code> 
when using the k-plus objective.
</p>
<p>The &quot;diversity&quot; objective is the sum of pairwise
distances of elements within the same groups (see
<code><a href="#topic+diversity_objective">diversity_objective</a></code>). Hence, anticlustering using the diversity 
criterion maximizes between-group similarity
by maximizing within-group heterogeneity (represented as the sum of all pairwise distances). 
If it is computed on the basis of the Euclidean distance (which is the default
behaviour if <code>x</code> is a feature matrix), the diversity is an all rounder objective that 
tends to equate all distribution 
characteristics between groups (such as means, variances, ...). 
Note that the equivalence of within-group heterogeneity and between-group similarity only
holds for equal-sized groups. For unequal-sized groups, it is recommended to
use a different objective when striving for overall between-group similarity,
e.g., the k-plus objective or the <code>"average-diversity"</code>. The average diversity
was introduced in version 0.8.6, and it is more useful if groups are not 
equal-sized. The average diversity normalizes the sum of intra-cluster distances 
by group size. If all groups are equal-sized, it is equivalent to the 
regular diversity. In the publication that introduces
the <code>anticlust</code> package (Papenberg &amp; Klau, 2021), we used the term &quot;anticluster 
editing&quot; to refer to the maximization of the diversity, because the reversed 
procedure - minimizing the diversity - is also known as &quot;cluster editing&quot;. 
</p>
<p>The &quot;dispersion&quot; is the minimum distance between any two elements
that are part of the same cluster; maximization of this objective
ensures that any two elements within the same group are as
dissimilar from each other as possible. Applications that require
high within-group heterogeneity often require to maximize the
dispersion. Oftentimes, it is useful to also consider the diversity
and not only the dispersion; to optimize both objectives at the
same time, see the function
<code><a href="#topic+bicriterion_anticlustering">bicriterion_anticlustering</a></code>.
</p>
<p>If the data input <code>x</code> is a feature matrix (that is: each row
is a &quot;case&quot; and each column is a &quot;variable&quot;) and the option
<code>objective = "diversity"</code> or <code>objective = "dispersion"</code> is used, 
the Euclidean distance is computed as the basic unit of the objectives. If
a different measure of dissimilarity is preferred, you may pass a
self-generated dissimilarity matrix via the argument <code>x</code>.
</p>
<p>In the standard case, groups of equal size are generated. Adjust
the argument <code>K</code> to create groups of different size (see
Examples).
</p>
<p><strong>Algorithms for anticlustering</strong>
</p>
<p>By default, a heuristic method is employed for anticlustering: the
exchange method (<code>method = "exchange"</code>). First, elements are
randomly assigned to anticlusters (It is also possible to
explicitly specify the initial assignment using the argument
<code>K</code>; in this case, <code>K</code> has length <code>nrow(x)</code>.) Based
on the initial assignment, elements are systematically swapped
between anticlusters in such a way that each swap improves the
objective value. For an element, each possible swap with elements
in other clusters is simulated; then, the one swap is performed
that improves the objective the most, but a swap is only conducted
if there is an improvement at all. This swapping procedure is
repeated for each element. When using <code>method =
"local-maximum"</code>, the exchange method does not terminate after the
first iteration over all elements; instead, the swapping continues
until a local maximum is reached. This method corresponds to the algorithm 
&quot;LCW&quot; by Weitz and Lakshminarayanan (1998). This means that after the
exchange process has been conducted once for each data point, the
algorithm restarts with the first element and proceeds to conduct
exchanges until the objective cannot be improved.
</p>
<p>When setting <code>preclustering = TRUE</code>, only the <code>K - 1</code>
most similar elements serve as exchange partners for each element,
which can speed up the optimization (more information
on the preclustering heuristic follows below). If the <code>categories</code> argument
is used, only elements having the same value in <code>categories</code> serve as exchange
partners.
</p>
<p>Using <code>method = "brusco"</code> implements the local bicriterion
iterated local search (BILS) heuristic by Brusco et al. (2020) and
returns the partition that best optimized either the diversity or
the dispersion during the optimization process. The function
<code><a href="#topic+bicriterion_anticlustering">bicriterion_anticlustering</a></code> can also be used to run
the algorithm by Brusco et al., but it returns multiple partitions
that approximate the optimal pareto efficient set according to both
objectives (diversity and dispersion). Thus, to fully utilize the
BILS algorithm, use the function
<code><a href="#topic+bicriterion_anticlustering">bicriterion_anticlustering</a></code>.
</p>
<p><strong>Optimal anticlustering</strong>
</p>
<p>Usually, heuristics are employed to tackle anticlustering problems,
and their performance is generally very satisfying.  However,
heuristics do not investigate all possible group assignments and
therefore do not (necessarily) find the
&quot;globally optimal solution&quot;, i.e., a partitioning that has the best
possible value with regard to the objective that is optimized.  Enumerating
all possible partitions in order to find the best solution,
however, quickly becomes impossible with increasing N, and
therefore it is not possible to find a global optimum this
way. Because all anticlustering problems considered here are also
NP-hard, there is also no (known) clever algorithm that might
identify the best solution without considering all possibilities -
at least in the worst case. Integer linear programming (ILP) is an
approach for tackling NP hard problems that nevertheless tries to
be clever when finding optimal solutions: It does not necessarily
enumerate all possibilities but is still guaranteed to return the
optimal solution. Still, for NP hard problems such as
anticlustering, ILP methods will also fail at some point (i.e.,
when N increases).
</p>
<p><code>anticlust</code> implements optimal solution algorithms via integer
linear programming. In order to use the ILP methods, set
<code>method = "ilp"</code>. The integer linear program optimizing the
diversity was described in Papenberg &amp; Klau, (2021; (8) -
(12)). It can also be used to optimize the k-means and k-plus objectives,
but you actually have to use the function <code><a href="#topic+optimal_anticlustering">optimal_anticlustering</a></code>
for these objectives. The documentation of the function
<code><a href="#topic+optimal_dispersion">optimal_dispersion</a></code> and <code><a href="#topic+optimal_anticlustering">optimal_anticlustering</a></code>
contain more information on the optimal anticlustering algorithms.
</p>
<p><strong>Categorical variables</strong>
</p>
<p>There are two ways to balance categorical variables among anticlusters (also 
see the package vignette &quot;Using categorical variables with anticlustering&quot;).
The first way is to treat them as &quot;hard constraints&quot; via the argument 
<code>categories</code> (see Papenberg &amp; Klau, 2021). If done so, balancing the 
categorical variable is accomplished via <code>categorical_sampling</code> through
a stratified split before the anticlustering optimization. After that, the 
balance is never changed when the algorithm runs (hence, it is a &quot;hard constraint&quot;). 
When <code>categories</code> has multiple columns (i.e., there are multiple 
categorical variables), each combination of categories is treated as a
distinct category by the exchange method (i.e., the multiple columns
are &quot;merged&quot; into a single column). This behaviour may lead
to less than optimal results on the level of each single categorical variable.
In this case, it may be useful to treat the categorical variables as part of 
the numeric data, i.e., the first argument <code>x</code> via binary coding 
(e.g. using <code><a href="#topic+categories_to_binary">categories_to_binary</a></code>). The examples show how to do this 
when using the bicriterion algorithm by Bruso et al. Using the argument 
<code>categories</code> is only available for the classical exchange procedures, 
that is, for <code>method = "exchange"</code> and <code>method = "local-maximum"</code>. 
</p>
<p><strong>Anticlustering with constraints</strong>
</p>
<p>Versions 0.8.6 and 0.8.7 of anticlust introduced the possibility to induce
cannot-link and must-link constraints with anticlustering with 
the arguments <code>cannot_link</code> and <code>must_link</code>, respectively.
Cannot-link constraints ensure that pairs of items are assigned to different
clusters. They are given as a 2-column matrix, where each row has the indices
of two elements, which must not be assigned to the same cluster. It is possible
that a set of cannot-link constraints cannot be fulfilled. To verify whether
the constraints cannot be fulfilled (and to actually assign elements 
while respecting the constraints), a graph coloring algorithm algorithm is used.
This algorithm is is actually the same method as used in <code><a href="#topic+optimal_dispersion">optimal_dispersion</a></code>. 
The graph coloring algorithm uses an ILP solver and it greatly profits (that is,
it may be much faster) from the Rsymphony package, which is not installed as 
a necessary dependency with anticlust. It is therefore recommended to 
manually install the Rsymphony package, which is then automatically 
selected as solver when using the <code>must_link</code> argument. If you have
access to the gurobi solver and have the gurobi R package installed, it will
be selected as solver (which is even faster than Symphony).
</p>
<p>Must-link constraints are passed as a single vector of length <code>nrow(x)</code>.
Positions that have the same numeric index are assigned to the same anticluster 
(if the constraints can be fulfilled). When including must-link constraints, 
<code>method = "2PML"</code> performs a specialized search heuristic that potentially
yields better results than <code>method = "local-maximum"</code>. The must-link 
functionality and the 2PML algorithm was introduced in Papenberg et al. (2025).
</p>
<p>The examples illustrate the usage of the <code>must_link</code> and <code>cannot_link</code>
arguments. Currently, the different kinds of constraints (arguments <code>must_link</code>, 
<code>cannot_link</code>, and <code>categories</code>) cannot be used together, but this 
may change in future versions. 
</p>
<p><strong>Preclustering</strong>
</p>
<p>A useful heuristic for anticlustering is to form small groups of
very similar elements and assign these to different groups. This
logic is used as a preprocessing when setting <code>preclustering =
TRUE</code>. That is, before the anticlustering objective is optimized, a
cluster analysis identifies small groups of similar elements (pairs
if <code>K = 2</code>, triplets if <code>K = 3</code>, and so forth). The
optimization of the anticlustering objective is then conducted
under the constraint that these matched elements cannot be assigned
to the same group. When using the exchange algorithm, preclustering
is conducted using a call to <code><a href="#topic+matching">matching</a></code>. When using
<code>method = "ilp"</code>, the preclustering optimally finds groups of
minimum pairwise distance by solving the integer linear program
described in Papenberg and Klau (2021; (8) - (10), (12) - (13)).
Note that when combining preclustering restrictions with <code>method = "ilp"</code>,
the anticlustering result is no longer guaranteed to be globally optimal, but
only optimal given the preclustering restrictions.
</p>
<p><strong>Optimize a custom objective function</strong>
</p>
<p>It is possible to pass a <code>function</code> to the argument
<code>objective</code>. See <code><a href="#topic+dispersion_objective">dispersion_objective</a></code> for an
example. If <code>objective</code> is a function, the exchange method
assigns elements to anticlusters in such a way that the return
value of the custom function is maximized (hence, the function
should return larger values when the between-group similarity is
higher). The custom function has to take two arguments: the first
is the data argument, the second is the clustering assignment. That
is, the argument <code>x</code> will be passed down to the user-defined
function as first argument. <strong>However, only after</strong>
<code><a href="base.html#topic+as.matrix">as.matrix</a></code> has been called on <code>x</code>. This implies
that in the function body, columns of the data set cannot be
accessed using <code>data.frame</code> operations such as
<code>$</code>. Objects of class <code>dist</code> will be converted to matrix
as well.
</p>


<h3>Value</h3>

<p>A vector of length N that assigns a group (i.e, a number
between 1 and <code>K</code>) to each input element.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Brusco, M. J., Cradit, J. D., &amp; Steinley, D. (2020). Combining
diversity and dispersion criteria for anticlustering: A bicriterion
approach. British Journal of Mathematical and Statistical
Psychology, 73, 275-396. https://doi.org/10.1111/bmsp.12186
</p>
<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>
<p>Papenberg, M. (2024). K-plus Anticlustering: An Improved k-means Criterion for 
Maximizing Between-Group Similarity. British Journal of Mathematical and 
Statistical Psychology, 77(1), 80-102. https://doi.org/10.1111/bmsp.12315
</p>
<p>Papenberg, M., Wang, C., Diop, M., Bukhari, S. H., Oskotsky, B., Davidson, 
B. R., Vo, K. C., Liu, B., Irwin, J. C., Combes, A., Gaudilliere, B., 
Li, J., Stevenson, D. K., Klau, G. W., Giudice, L. C., Sirota, M., 
&amp; Oskotsky, T. T. (2025). Anticlustering for sample allocation to minimize 
batch effects. bioRxiv. https://doi.org/10.1101/2025.03.03.641320
</p>
<p>Späth, H. (1986). Anticlustering: Maximizing the variance criterion.
Control and Cybernetics, 15, 213-218.
</p>
<p>Weitz, R. R., &amp; Lakshminarayanan, S. (1998). An empirical comparison of 
heuristic methods for creating maximally diverse groups. Journal of the 
Operational Research Society, 49(6), 635-646. https://doi.org/10.1057/palgrave.jors.2600510
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Use default method ("exchange") and the default diversity criterion, also include
# a categorical variable via argument `categories`:
anticlusters &lt;- anticlustering(
  schaper2019[, 3:6],
  K = 3,
  categories = schaper2019$room
)
# Compare feature means and standard deviations by anticluster
mean_sd_tab(schaper2019[, 3:6], anticlusters)
# Verify that the "room" is balanced across anticlusters:
table(anticlusters, schaper2019$room)

# Use multiple starts of the algorithm to improve the objective and
# optimize the k-means criterion ("variance")
anticlusters &lt;- anticlustering(
  schaper2019[, 3:6],
  objective = "variance",
  K = 3,
  categories = schaper2019$room,
  method = "local-maximum", # better search algorithm
  repetitions = 20 # multiple restarts of the algorithm 
)
# Compare means and standard deviations by anticluster
mean_sd_tab(schaper2019[, 3:6], anticlusters)

# Use different group sizes and optimize the extended k-means
# criterion ("kplus")
anticlusters &lt;- anticlustering(
  schaper2019[, 3:6],
  objective = "kplus",
  K = c(24, 24, 48),
  categories = schaper2019$room,
  repetitions = 20,
  method = "local-maximum",
  standardize = TRUE # ususally recommended
)

# Use cannot_link constraints: Element 1 must not be linked with elements 2 to 10:
cl_matrix &lt;- matrix(c(rep(1, 9), 2:10), ncol = 2)
cl &lt;- anticlustering(
  schaper2019[, 3:6],
  K = 10,
  cannot_link = cl_matrix
)
all(cl[1] != cl[2:10])

# Use cannot_link constraints: Element 1 must be linked with elements 2 to 10.
# Element 11 must be linked with elements 12-20.
must_link &lt;- rep(NA, nrow(schaper2019))
must_link[1:10] &lt;- 1
must_link[11:20] &lt;- 2
cl &lt;- anticlustering(
  schaper2019[, 3:6],
  K = 3,
  must_link = must_link
)
cl[1:10]
cl[11:20]

# Use the heuristic by Brusco et al. (2020) for k-plus anticlustering
# Include categorical variable as part of the optimization criterion rather 
# than the argument categories!
anticlusters &lt;- anticlustering(
  cbind(
    kplus_moment_variables(schaper2019[, 3:6], 2), 
    categories_to_binary(schaper2019$room)
  ),
  objective = "variance", # k-plus anticlustering because of the input above!
  K = 3,
  repetitions = 20,
  method = "brusco"
)

mean_sd_tab(schaper2019[, 3:6], anticlusters)
table(anticlusters, schaper2019$room)

</code></pre>

<hr>
<h2 id='balanced_clustering'>Create balanced clusters of equal size</h2><span id='topic+balanced_clustering'></span>

<h3>Description</h3>

<p>Create balanced clusters of equal size
</p>


<h3>Usage</h3>

<pre><code class='language-R'>balanced_clustering(x, K, method = "centroid", solver = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="balanced_clustering_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A feature
matrix where rows correspond to elements and columns correspond
to variables (a single numeric variable can be passed as a
vector). (2) An N x N matrix dissimilarity matrix; can be an
object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="balanced_clustering_+3A_k">K</code></td>
<td>
<p>How many clusters should be created.</p>
</td></tr>
<tr><td><code id="balanced_clustering_+3A_method">method</code></td>
<td>
<p>One of &quot;centroid&quot; or &quot;ilp&quot;. See Details.</p>
</td></tr>
<tr><td><code id="balanced_clustering_+3A_solver">solver</code></td>
<td>
<p>Optional. The solver used to obtain the optimal method 
if <code>method  = "ilp"</code>. Currently supports &quot;glpk&quot; and &quot;symphony&quot;. 
Is ignored for <code>method = "centroid"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function partitions a set of elements into <code>K</code>
equal-sized clusters. The function offers two methods: a heuristic
and an exact method. The heuristic (<code>method = "centroid"</code>)
first computes the centroid of all data points. If the input is a
feature matrix, the centroid is defined as the mean vector of all
columns. If the input is a dissimilarity matrix, the most central
element acts as the centroid; the most central element is defined
as the element having the minimum maximal distance to all other
elements. After identifying the centroid, the algorithm proceeds as
follows: The element having the highest distance from the centroid
is clustered with its <code>(N/K) - 1</code> nearest neighbours
(neighbourhood is defined according to the Euclidean distance if
the data input is a feature matrix). From the remaining elements,
again the element farthest to the centroid is selected and
clustered with its <code>(N/K) - 1</code> neighbours; the procedure is
repeated until all elements are part of a cluster.
</p>
<p>An exact method (<code>method = "ilp"</code>) can be used to solve
equal-sized weighted cluster editing optimally (implements the
integer linear program described in Papenberg and Klau, 2020; 
(8) - (10), (12) - (13)). The cluster editing objective is the 
sum of pairwise distances
within clusters; clustering is accomplished by minimizing this
objective. If the argument <code>x</code> is a features matrix, the
Euclidean distance is computed as the basic unit of the cluster
editing objective. If another distance measure is preferred, users
may pass a self-computed dissimiliarity matrix via the argument
<code>x</code>. 
</p>
<p>The optimal <code>method = "ilp"</code> uses a &quot;solver&quot; to optimize
the clustering objective. See <code><a href="#topic+optimal_anticlustering">optimal_anticlustering</a></code>
for an overview of the solvers that are available.
</p>


<h3>Value</h3>

<p>An integer vector representing the cluster affiliation of 
each data point
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>
<p>Meik Michalke <a href="mailto:meik.michalke@hhu.de">meik.michalke@hhu.de</a>
</p>


<h3>Source</h3>

<p>The centroid method was originally developed and contributed by
Meik Michalke. It was later rewritten by Martin Papenberg, who
also implemented the integer linear programming method.
</p>


<h3>References</h3>

<p>Grötschel, M., &amp; Wakabayashi, Y. (1989). A cutting plane algorithm
for a clustering problem. Mathematical Programming, 45, 59–96.
</p>
<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Cluster a data set and visualize results
N &lt;- 1000
lds &lt;- data.frame(f1 = rnorm(N), f2 = rnorm(N))
cl &lt;- balanced_clustering(lds, K = 10)
plot_clusters(lds, clusters = cl)

# Repeat using a distance matrix as input
cl2 &lt;- balanced_clustering(dist(lds), K = 10)
plot_clusters(lds, clusters = cl2)

</code></pre>

<hr>
<h2 id='bicriterion_anticlustering'>Bicriterion iterated local search heuristic</h2><span id='topic+bicriterion_anticlustering'></span>

<h3>Description</h3>

<p>This function implements the bicriterion algorithm BILS for
anticlustering by Brusco et al. (2020;
&lt;doi:10.1111/bmsp.12186&gt;). The description of their algorithm is
given in Section 3 of their paper (in particular, see the
Pseudocode in their Figure 2). As of anticlust version 0.8.6, this
function also includes some extensions to the BILS algorithm that
are implemented through the optional arguments
<code>dispersion_distances</code>, <code>average_diversity</code>,
<code>init_partitions</code>, and <code>return</code>. If these arguments are
not changed, the function performs the &quot;vanilla&quot; BILS as described
in Brusco et al.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bicriterion_anticlustering(
  x,
  K,
  R = NULL,
  W = c(1e-06, 1e-05, 1e-04, 0.001, 0.01, 0.1, 0.5, 0.99, 0.999, 0.999999),
  Xi = c(0.05, 0.1),
  dispersion_distances = NULL,
  average_diversity = FALSE,
  init_partitions = NULL,
  return = "paretoset"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bicriterion_anticlustering_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A
feature matrix where rows correspond to elements and columns
correspond to variables (a single numeric variable can be
passed as a vector). (2) An N x N matrix dissimilarity matrix;
can be an object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_k">K</code></td>
<td>
<p>How many anticlusters should be created. Alternatively:
(a) A vector describing the size of each group, or (b) a vector
of length <code>nrow(x)</code> describing how elements are assigned
to anticlusters before the optimization starts.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_r">R</code></td>
<td>
<p>The desired number of restarts for the algorithm. By
default, both phases (MBPI + ILS) of the algorithm are
performed once.  See details.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_w">W</code></td>
<td>
<p>Optional argument, a vector of weights defining the
relative importance of dispersion and diversity (0 &lt;= W &lt;=
1). See details.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_xi">Xi</code></td>
<td>
<p>Optional argument, specifies probability of swapping
elements during the iterated local search. See examples.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_dispersion_distances">dispersion_distances</code></td>
<td>
<p>A distance matrix used to compute the
dispersion if the dispersion should not be computed on the
basis of argument <code>x</code>.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_average_diversity">average_diversity</code></td>
<td>
<p>Boolean. Compute the diversity not as a
global sum across all pairwise within-group distances, but as
the sum of the average of within-group distances.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_init_partitions">init_partitions</code></td>
<td>
<p>A matrix of initial partitions (rows =
partitions; columns = elements) that serve as starting
partitions during the iterations of the first phase of the BILS
(i.e., the MBPI).  If not passed, a new random partition is
generated at the start of each iteration (which is the default
behaviour).</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_return">return</code></td>
<td>
<p>Either &quot;paretoset&quot; (default), &quot;best-diversity&quot;, 
&quot;best-average-diversity&quot;, &quot;best-dispersion&quot;. See below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The bicriterion algorithm by Brusco et al. (2020) aims to
simultaneously optimize two anticlustering criteria: the
<code><a href="#topic+diversity_objective">diversity_objective</a></code> and the
<code><a href="#topic+dispersion_objective">dispersion_objective</a></code>. It returns a list of partitions
that approximate the pareto set of efficient solutions across both
criteria. By considering both the diversity and dispersion, this
algorithm is well-suited for maximizing overall within-group
heterogeneity. To select a partition among the approximated pareto
set, it is reasonable to plot the objectives for each partition
(see Examples).
</p>
<p>The arguments <code>R</code>, <code>W</code> and <code>Xi</code> are named for
consistency with Brusco et al. (2020). The argument <code>K</code> is
used for consistency with other functions in anticlust; Brusco et
al. used 'G' to denote the number of groups. However, note that
<code>K</code> can not only be used to denote the number of equal-sized
groups, but also to specify group sizes, as in
<code><a href="#topic+anticlustering">anticlustering</a></code>.
</p>
<p>This function implements the combined bicriterion algorithm BILS,
which consists of two phases: The multistart bicriterion pairwise
interchange heuristic (MBPI, which is a local maximum search
heuristic similar to <code>method = "local-maximum"</code> in
<code><a href="#topic+anticlustering">anticlustering</a></code>), and the iterated local search (ILS),
which is an improvement procedure that tries to overcome local
optima.  The argument <code>R</code> denotes the number of restarts of
the two phases of the algorithm. If <code>R</code> has length 1, half of
the repetitions perform the first phase MBPI), the other half
perform the ILS.  If <code>R</code> has length 2, the first entry
indicates the number of restarts of MBPI the second entry indicates
the number of restarts of ILS.  The argument <code>W</code> denotes the
relative weight given to the diversity and dispersion criterion in
a given run of the search heuristic. In each run, the a weight is
randomly selected from the vector <code>W</code>. As default values, we
use the weights that Brusco et al. used in their analyses. All
values in <code>W</code> have to be in [0, 1]; larger values indicate
that diversity is more important, whereas smaller values indicate
that dispersion is more important; <code>w = .5</code> implies the same
weight for both criteria. The argument <code>Xi</code> is the probability
that an element is swapped during the iterated local search
(specifically, Xi has to be a vector of length 2, denoting the
range of a uniform distribution from which the probability of
swapping is selected). For <code>Xi</code>, the default is selected
consistent with the analyses by Brusco et al.
</p>
<p>If the data input <code>x</code> is a feature matrix (that is: each row
is a &quot;case&quot; and each column is a &quot;variable&quot;), a matrix of the
Euclidean distances is computed as input to the algorithm. If a
different measure of dissimilarity is preferred, you may pass a
self-generated dissimilarity matrix via the argument <code>x</code>.  The
argument <code>dispersion_distances</code> can additionally be used if
the dispersion should be computed on the basis of a different
distance matrix.
</p>
<p>If multiple <code>init_partitions</code> are given, ensure that each
partition (i.e., each row of<code>init_partitions</code>) has the exact
same output of <code><a href="base.html#topic+table">table</a></code>.
</p>


<h3>Value</h3>

<p>By default, a <code>matrix</code> of anticlustering partitions
(i.e., the approximated pareto set). Each row corresponds to a
partition, each column corresponds to an input element. If the
argument <code>return</code> is set to either &quot;best-diversity&quot;, 
&quot;best-average-diversity&quot;, or &quot;best-dispersion&quot;, it only returns one 
partition (as a vector), that maximizes the respective objective.
</p>


<h3>Note</h3>

<p>For technical reasons, the pareto set returned by this function has
a limit of 500 partitions. Usually however, the
algorithm usually finds much fewer partitions. There is one following exception:
We do not recommend to use this method when the input data is
one-dimensional where the algorithm may identify too many
equivalent partitions causing it to run very slowly (see section 5.6 in 
Breuer, 2020).
</p>


<h3>Author(s)</h3>

<p>Martin Breuer <a href="mailto:M.Breuer@hhu.de">M.Breuer@hhu.de</a>, Martin Papenberg
<a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Brusco, M. J., Cradit, J. D., &amp; Steinley, D. (2020). Combining
diversity and dispersion criteria for anticlustering: A bicriterion
approach. British Journal of Mathematical and Statistical
Psychology, 73, 275-396. https://doi.org/10.1111/bmsp.12186
</p>
<p>Breuer (2020). Using anticlustering to maximize diversity and dispersion:
Comparing exact and heuristic approaches. Bachelor thesis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate some random data
M &lt;- 3
N &lt;- 80
K &lt;- 4
data &lt;- matrix(rnorm(N * M), ncol = M)

# Perform bicriterion algorithm, use 200 repetitions
pareto_set &lt;- bicriterion_anticlustering(data, K = K, R = 200)

# Compute objectives for all solutions
diversities_pareto &lt;- apply(pareto_set, 1, diversity_objective, x = data)
dispersions_pareto &lt;- apply(pareto_set, 1, dispersion_objective, x = data)

# Plot the pareto set
plot(
  diversities_pareto,
  dispersions_pareto,
  col = "blue",
  cex = 2,
  pch = as.character(1:NROW(pareto_set))
)

# Get some random solutions for comparison
rnd_solutions &lt;- t(replicate(n = 200, sample(pareto_set[1, ])))

# Compute objectives for all random solutions
diversities_rnd &lt;- apply(rnd_solutions, 1, diversity_objective, x = data)
dispersions_rnd &lt;- apply(rnd_solutions, 1, dispersion_objective, x = data)

# Plot random solutions and pareto set. Random solutions are far away 
# from the good solutions in pareto set
plot(
  diversities_rnd, dispersions_rnd, 
  col = "red",
  xlab = "Diversity",
  ylab = "Dispersion",
  ylim = c(
    min(dispersions_rnd, dispersions_pareto), 
    max(dispersions_rnd, dispersions_pareto)
  ),
  xlim = c(
    min(diversities_rnd, diversities_pareto), 
    max(diversities_rnd, diversities_pareto)
  )
)

# Add approximated pareto set from bicriterion algorithm:
points(diversities_pareto, dispersions_pareto, col = "blue", cex = 2, pch = 19)

</code></pre>

<hr>
<h2 id='categorical_sampling'>Random sampling employing a categorical constraint</h2><span id='topic+categorical_sampling'></span>

<h3>Description</h3>

<p>This function can be used to obtain a stratified split of a data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>categorical_sampling(categories, K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="categorical_sampling_+3A_categories">categories</code></td>
<td>
<p>A matrix or vector of one or more categorical variables.</p>
</td></tr>
<tr><td><code id="categorical_sampling_+3A_k">K</code></td>
<td>
<p>The number of groups that are returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to obtain a stratified split of a data set. 
Using this function is like calling <code><a href="#topic+anticlustering">anticlustering</a></code> with 
argument 'categories', but without optimizing a clustering objective. The
categories are just evenly split between samples. Apart from the restriction 
that categories are balanced between samples, the split is random.
</p>


<h3>Value</h3>

<p>A vector representing the sample each element was assigned to.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(schaper2019)
categories &lt;- schaper2019$room
groups &lt;- categorical_sampling(categories, K = 6)
table(groups, categories)

# Unequal sized groups
groups &lt;- categorical_sampling(categories, K = c(24, 24, 48))
table(groups, categories)

# Heavily unequal sized groups, is harder to balance the groups
groups &lt;- categorical_sampling(categories, K = c(51, 19, 26))
table(groups, categories)

</code></pre>

<hr>
<h2 id='categories_to_binary'>Get binary representation of categorical variables</h2><span id='topic+categories_to_binary'></span>

<h3>Description</h3>

<p>Get binary representation of categorical variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>categories_to_binary(categories, use_combinations = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="categories_to_binary_+3A_categories">categories</code></td>
<td>
<p>A vector, data.frame or matrix representing one
or several categorical variables</p>
</td></tr>
<tr><td><code id="categories_to_binary_+3A_use_combinations">use_combinations</code></td>
<td>
<p>Logical, should the output also include columns representing
the combination / interaction of the categories (defaults to <code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The conversion of categorical variables to binary variables is done via
<code><a href="stats.html#topic+model.matrix">model.matrix</a></code>. Since version 0.8.9, each category
of a categorical variable is coded by a separate variable. So this is not
'dummy' coding, which is often used to encode predictors in statistical 
analysis. Dummy coding uses a reference category that has only zeros for 
each variable, while all other categories consist of a 1 and otherwise zeros. 
This implies that there is a different distance to the reference category 
than among the other categories, which is unwarranted in anticlustering.
</p>
<p>This function can be used to include categorical variables as part of the 
optimization criterion in anticlustering, rather than including them as hard constraints as done when using the 
argument <code>categories</code> in <code><a href="#topic+anticlustering">anticlustering</a></code> (or <code><a href="#topic+fast_anticlustering">fast_anticlustering</a></code>). 
This way, categorical variables are treated as numeric variables, 
which can be useful when there are several
categorical variables or when the group sizes are unequal (or both).
See examples. Please see the vignette 'Using categorical variables with anticlustering'
for more information on this approach.
</p>


<h3>Value</h3>

<p>A matrix encoding the categorical variable(s) in binary form.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Papenberg, M. (2024). K-plus Anticlustering: An Improved k-means Criterion for 
Maximizing Between-Group Similarity. British Journal of Mathematical and 
Statistical Psychology, 77(1), 80&ndash;102. https://doi.org/10.1111/bmsp.12315
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# How to encode a categorical variable with three levels:
unique(iris$Species)
categories_to_binary(iris$Species)[c(1, 51, 101), ]

# Use Schaper data set for anticlustering example
data(schaper2019)
features &lt;- schaper2019[, 3:6]
K &lt;- 3
N &lt;- nrow(features) 

# - Generate data input for k-means anticlustering -
# We conduct k-plus anticlustering by first generating k-plus variables, 
# and also include the categorical variable as "numeric" input for the 
# k-means optimization (rather than as input for the argument \code{categories})

input_data &lt;- cbind(
  kplus_moment_variables(features, T = 2), 
  categories_to_binary(schaper2019$room) 
)

kplus_groups &lt;- anticlustering(
  input_data, 
  K = K,
  objective = "variance",
  method = "local-maximum", 
  repetitions = 10
)
mean_sd_tab(features, kplus_groups)
table(kplus_groups, schaper2019$room) # argument categories was not used!

 
</code></pre>

<hr>
<h2 id='dispersion_objective'>Cluster dispersion</h2><span id='topic+dispersion_objective'></span>

<h3>Description</h3>

<p>Compute the dispersion objective for a given clustering (i.e., the
minimum distance between two elements within the same cluster).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dispersion_objective(x, clusters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dispersion_objective_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A
feature matrix where rows correspond to elements and columns
correspond to variables (a single numeric variable can be
passed as a vector). (2) An N x N matrix dissimilarity matrix;
can be an object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="dispersion_objective_+3A_clusters">clusters</code></td>
<td>
<p>A vector representing (anti)clusters (e.g.,
returned by <code><a href="#topic+anticlustering">anticlustering</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dispersion is the minimum distance between two elements within
the same cluster. When the input <code>x</code> is a feature matrix, the
Euclidean distance is used as the distance unit. Maximizing the
dispersion maximizes the minimum heterogeneity within clusters and
is an anticlustering task.
</p>


<h3>References</h3>

<p>Brusco, M. J., Cradit, J. D., &amp; Steinley, D. (2020). Combining
diversity and dispersion criteria for anticlustering: A bicriterion
approach. British Journal of Mathematical and Statistical
Psychology, 73, 275-396. https://doi.org/10.1111/bmsp.12186
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N &lt;- 50 # number of elements
M &lt;- 2  # number of variables per element
K &lt;- 5  # number of clusters
random_data &lt;- matrix(rnorm(N * M), ncol = M)
random_clusters &lt;- sample(rep_len(1:K, N))
dispersion_objective(random_data, random_clusters)

# Maximize the dispersion 
optimized_clusters &lt;- anticlustering(
  random_data,
  K = random_clusters, 
  objective = dispersion_objective
)
dispersion_objective(random_data, optimized_clusters)

</code></pre>

<hr>
<h2 id='diversity_objective'>(Anti)cluster editing &quot;diversity&quot; objective</h2><span id='topic+diversity_objective'></span>

<h3>Description</h3>

<p>Compute the diversity for a given clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diversity_objective(x, clusters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diversity_objective_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A data matrix
where rows correspond to elements and columns correspond to
features (a single numeric feature can be passed as a vector). (2)
An N x N matrix dissimilarity matrix; can be an object of class
<code>dist</code> (e.g., returned by <code><a href="stats.html#topic+dist">dist</a></code> or
<code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code> where the entries of
the upper and lower triangular matrix represent the pairwise
dissimilarities.</p>
</td></tr>
<tr><td><code id="diversity_objective_+3A_clusters">clusters</code></td>
<td>
<p>A vector representing (anti)clusters (e.g.,
returned by <code><a href="#topic+anticlustering">anticlustering</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The objective function used in (anti)cluster editing is the
diversity, i.e., the sum of the pairwise distances between elements
within the same groups. When the input <code>x</code> is a feature
matrix, the Euclidean distance is computed as the basic distance
unit of this objective.
</p>


<h3>Value</h3>

<p>The cluster editing objective
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Brusco, M. J., Cradit, J. D., &amp; Steinley, D. (2020). Combining
diversity and dispersion criteria for anticlustering: A bicriterion
approach. British Journal of Mathematical and Statistical
Psychology, 73, 275-396. https://doi.org/10.1111/bmsp.12186
</p>
<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
distances &lt;- dist(iris[1:60, -5])
## Clustering
clusters &lt;- balanced_clustering(distances, K = 3)
# This is low:
diversity_objective(distances, clusters)
## Anticlustering
anticlusters &lt;- anticlustering(distances, K = 3)
# This is higher:
diversity_objective(distances, anticlusters)

</code></pre>

<hr>
<h2 id='fast_anticlustering'>Fast anticlustering</h2><span id='topic+fast_anticlustering'></span>

<h3>Description</h3>

<p>Increasing the speed of (k-means / k-plus) anticlustering by (1) 
conducting fewer exchanges during the optimization and (2) using an alternative
formulation of the k-means objective. Makes anticlustering applicable to 
quite large data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_anticlustering(
  x,
  K,
  k_neighbours = Inf,
  categories = NULL,
  exchange_partners = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fast_anticlustering_+3A_x">x</code></td>
<td>
<p>A numeric vector, matrix or data.frame of data points.
Rows correspond to elements and columns correspond to
features. A vector represents a single numeric feature.</p>
</td></tr>
<tr><td><code id="fast_anticlustering_+3A_k">K</code></td>
<td>
<p>How many anticlusters should be created. Alternatively:
(a) A vector describing the size of each group, or (b) a vector
of length <code>nrow(x)</code> describing how elements are assigned
to anticlusters before the optimization starts.</p>
</td></tr>
<tr><td><code id="fast_anticlustering_+3A_k_neighbours">k_neighbours</code></td>
<td>
<p>The number of nearest neighbours that serve as
exchange partner for each element. See details.</p>
</td></tr>
<tr><td><code id="fast_anticlustering_+3A_categories">categories</code></td>
<td>
<p>A vector, data.frame or matrix representing one
or several categorical constraints.</p>
</td></tr>
<tr><td><code id="fast_anticlustering_+3A_exchange_partners">exchange_partners</code></td>
<td>
<p>Optional argument. A list of length
<code>NROW(x)</code> specifying for each element the indices of the
elements that serve as exchange partners. If used, this
argument overrides the <code>k_neighbours</code> argument. See
examples.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function was created to make anticlustering applicable to
large data sets (e.g., several 100,000 elements). It optimizes the
k-means objective because computing all pairwise distances as is
done when optimizing the &quot;diversity&quot; (i.e., the default in
<code><a href="#topic+anticlustering">anticlustering</a></code>) is not feasible for very large data
sets (for about N &gt; 20000 on my personal computer). Using
<code>fast_anticlustering</code> for k-plus anticlustering is also
possible by applying <code><a href="#topic+kplus_moment_variables">kplus_moment_variables</a></code> on the
input (and possibly by using the argument <code>exchange_partners</code>,
see Examples).
</p>
<p>The function <code>fast_anticlustering</code> employs a speed-optimized
exchange method, which is basically equivalent to <code>method =
"exchange"</code> in <code><a href="#topic+anticlustering">anticlustering</a></code>, but may reduce the number
of exchanges that are investigated for each input element. The number of 
exchange partners per element has to be set using the argument <code>k_neighbours</code>. By
default, it is set to <code>Inf</code>, meaning that all possible swaps are
tested. If <code>k_neighbours</code> is set differently (which is usually recommended when running 
this function), the default behaviour is to generate exchange partners using a
nearest neighbour search (using the function <code><a href="RANN.html#topic+nn2">nn2</a></code>
from the <code>RANN</code> package). Using more exchange partners can improve the quality of
the results, but also increase run time. Note that for very large
data sets, anticlustering generally becomes &quot;easier&quot; (even a random
split may yield satisfactory results), so using few exchange
partners is usually not a problem. 
</p>
<p>It is possible to specify custom exchange partners using the
argument <code>exchange_partners</code> instead of relying on the default
nearest neighbour search.  When using <code>exchange_partners</code>, it
is not necessary that each element has the same number of exchange
partners; this is why the argument <code>exchange_partners</code> has to
be a <code>list</code> instead of a <code>matrix</code> or
<code>data.frame</code>. Exchange partners can for example be generated
by <code><a href="#topic+generate_exchange_partners">generate_exchange_partners</a></code> (see Examples), but a
custom list may also be used. Note that categorical constraints
induced via <code>categories</code> may not be respected during the
optimization if the <code>exchange_partners</code> argument allows
exchanges between members of different categories, so care must be
taken when combining the arguments <code>exchange_partners</code> and
<code>categories</code>.
</p>
<p>In <code>anticlustering(..., objective = "variance")</code>, the run time
of computing the k-means objective is in O(M N), where N is the
total number of elements and M is the number of variables. This is
because the variance is computed as the sum of squared distances
between all data points and their cluster centers.  The function
<code>fast_anticlustering</code> uses a different - but equivalent -
formulation of the k-means objective, where the re-computation of
the objective only depends and M but no longer on N. In
particular, this variant of k-means anticlustering minimizes 
the weighted sum of squared distances between
cluster centroids and the overall data centroid; the distances
between all individual data points and their cluster center are not
computed (Späth, 1986). Using the different objective formulation 
reduces the run time by an
order of magnitude and makes k-means anticlustering applicable to
very large data sets (even in the millions). For a fixed number of
exchange partners (specified using the argument
<code>k_neighbours</code>), the approximate run time of
<code>fast_anticlustering</code> is in O(M N). The algorithm
<code>method = "exchange"</code> in <code><a href="#topic+anticlustering">anticlustering</a></code> with
<code>objective = "variance"</code> has a run time of O(M N^3). 
Thus, <code>fast_anticlustering</code> can improve the run time
by two orders of magnitude as compared to the standard exchange
algorithm. The nearest neighbour search, which is done in the
beginning usually does not strongly contribute to the overall
run time. It is nevertheless possible to suppress the nearest
neighbour search by using the <code>exchange_partners</code> argument.
</p>
<p>When setting the <code>categories</code> argument, exchange partners
(i.e., nearest neighbours) will be generated from the same
category. Note that when <code>categories</code> has multiple columns,
each combination of these categories is treated as a distinct
category by the exchange method. You can also use
<code><a href="#topic+categories_to_binary">categories_to_binary</a></code> to potentially improve results
for several categorical variables, instead of using the argument
<code>categories</code>.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>
<p>Papenberg, M. (2024). K-plus Anticlustering: An Improved k-means Criterion for 
Maximizing Between-Group Similarity. British Journal of Mathematical and 
Statistical Psychology, 77(1), 80&ndash;102. https://doi.org/10.1111/bmsp.12315
</p>
<p>Späth, H. (1986). Anticlustering: Maximizing the variance criterion.
Control and Cybernetics, 15, 213-218.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anticlustering">anticlustering</a></code>
</p>
<p><code><a href="#topic+kplus_moment_variables">kplus_moment_variables</a></code>
</p>
<p><code><a href="#topic+categories_to_binary">categories_to_binary</a></code>
</p>
<p><code><a href="#topic+variance_objective">variance_objective</a></code>
</p>
<p><code><a href="#topic+generate_exchange_partners">generate_exchange_partners</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Use fewer or more exchange partners to adjust speed (vs. quality tradeoff)
features &lt;- iris[, - 5]
N &lt;- nrow(features)
init &lt;- sample(rep_len(1:3, N)) # same starting point for all calls:
groups1 &lt;- fast_anticlustering(features, K = init) # default: all exchanges
groups2 &lt;- fast_anticlustering(features, K = init, k_neighbours = 20) 
groups3 &lt;- fast_anticlustering(features, K = init, k_neighbours = 2)

variance_objective(features, groups1)
variance_objective(features, groups2)
variance_objective(features, groups3)

# K-plus anticlustering is straight forward when sticking with the default
# for k_neighbours
kplus_anticlusters &lt;- fast_anticlustering(
  kplus_moment_variables(features, T = 2), 
  K = 3
)
mean_sd_tab(features, kplus_anticlusters)

# Some care is needed when applying k-plus using with this function 
# while using a reduced number of exchange partners generated in the 
# nearest neighbour search. Then we:
# 1) Use kplus_moment_variables() on the numeric input
# 2) Generate custom exchange_partners because otherwise nearest 
#    neighbours are internally selected based on the extended k-plus 
#    variables returned by kplus_moment_variables() 
#    (which does not really make sense)
kplus_anticlusters &lt;- fast_anticlustering(
  kplus_moment_variables(features, T = 2), 
  K = 3,
  exchange_partners = generate_exchange_partners(120, features = features, method = "RANN")
 )
mean_sd_tab(features, kplus_anticlusters)
# Or we use random exchange partners: 
kplus_anticlusters &lt;- fast_anticlustering(
  kplus_moment_variables(features, T = 2), 
  K = 3,
  exchange_partners = generate_exchange_partners(120, N = nrow(features), method = "random")
)
mean_sd_tab(features, kplus_anticlusters)


# Working on several 1000 elements is very fast (Here n = 10000, m = 2)
data &lt;- matrix(rnorm(10000 * 2), ncol = 2)
start &lt;- Sys.time()
groups &lt;- fast_anticlustering(data, K = 5, k_neighbours = 5)
Sys.time() - start 

</code></pre>

<hr>
<h2 id='generate_exchange_partners'>Get exchange partners for fast_anticlustering()</h2><span id='topic+generate_exchange_partners'></span>

<h3>Description</h3>

<p>Get exchange partners for fast_anticlustering()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_exchange_partners(
  n_exchange_partners,
  N = NULL,
  features = NULL,
  method = "random",
  categories = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_exchange_partners_+3A_n_exchange_partners">n_exchange_partners</code></td>
<td>
<p>The number of exchange partners per
element</p>
</td></tr>
<tr><td><code id="generate_exchange_partners_+3A_n">N</code></td>
<td>
<p>The number of elements for which exchange partners; can be
<code>NULL</code> if <code>features</code> is passed (it is ignored if
<code>features</code> is passed).</p>
</td></tr>
<tr><td><code id="generate_exchange_partners_+3A_features">features</code></td>
<td>
<p>The features for which nearest neighbours are
sought if <code>method = "RANN"</code>.  May be NULL if random
exchange partners are generated.</p>
</td></tr>
<tr><td><code id="generate_exchange_partners_+3A_method">method</code></td>
<td>
<p>Currently supports &quot;random&quot; (default), &quot;RANN&quot; and
&quot;restricted_random&quot;. See details.</p>
</td></tr>
<tr><td><code id="generate_exchange_partners_+3A_categories">categories</code></td>
<td>
<p>A vector, data.frame or matrix representing one
or several categorical constraints.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>method = "RANN"</code> generates exchange partners using a
nearest neighbour search via <code><a href="RANN.html#topic+nn2">nn2</a></code> from the
<code>RANN</code> package; <code>methode = "restricted_random"</code> generates
random exchange partners but ensures that for each element, no
duplicates are generated and that the element itself does not occur
as exchange partner (this is the slowest method, and I would not
recommend it for large N); <code>method = "random"</code> (default) does
not impose these restrictions and generates unrescricted random
partners (it may therefore generate duplicates and the element
itself as exchange partner).
</p>
<p>When setting the <code>categories</code> argument and using <code>method
= "RANN"</code>, exchange partners (i.e., nearest neighbours) will be
generated from the same category; <code>methode =
"restricted_random"</code> will also adhere to categorical constraints
induced via <code>categories</code> (i.e. each element only receives
exchange partners from the same category as itself); <code>methode
= "random"</code> cannot incoorporate categorical restrictions.
</p>


<h3>Value</h3>

<p>A list of length <code>N</code>. Is usually used as input to the
argument <code>exchange_partners</code> in
<code><a href="#topic+fast_anticlustering">fast_anticlustering</a></code>.  Then, the i'th element of
the list contains the indices of the exchange partners that are
used for the i'th element.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Restricted random method generates no duplicates per element and cannot return 
# the element itself as exchange partner
generate_exchange_partners(5, N = 10, method = "restricted_random")
# "random" simply randomizes with replacement and without restrictions
# (categorical restrictions are also not possible; is much faster for large data sets)
generate_exchange_partners(5, N = 10, method = "random")
# May return less than 5 exchange partners if there are not enough members 
# of the same category: 
generate_exchange_partners(
  5, N = 10, 
  method = "restricted_random", 
  categories = cbind(schaper2019$room, schaper2019$frequency)
)
# using nearest neighbour search (unlike RANN::nn2, this does not 
# return the ID of the element itself as neighbour)
generate_exchange_partners(5, features = schaper2019[, 3:5], method = "RANN")[1:3]
# compare with RANN directly:
RANN::nn2(schaper2019[, 3:5], k = 6)$nn.idx[1:3, ] # note k = 6

</code></pre>

<hr>
<h2 id='generate_partitions'>Generate all partitions of same cardinality</h2><span id='topic+generate_partitions'></span>

<h3>Description</h3>

<p>Generate all partitions of same cardinality
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_partitions(N, K, generate_permutations = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_partitions_+3A_n">N</code></td>
<td>
<p>The total N. <code>K</code> has to be dividble
by <code>N</code>.</p>
</td></tr>
<tr><td><code id="generate_partitions_+3A_k">K</code></td>
<td>
<p>How many partitions</p>
</td></tr>
<tr><td><code id="generate_partitions_+3A_generate_permutations">generate_permutations</code></td>
<td>
<p>If TRUE, all permutations are returned,
resulting in duplicate partitions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In principle, anticlustering can be solved to optimality by
generating all possible partitions of N items into K groups.
The example code below illustrates how to do this.
However, this approach only works for small N because the
number of partitions grows exponentially with N.
</p>
<p>The partition c(1, 2, 2, 1)
is the same as the partition c(2, 1, 1, 2) but they correspond
to different permutations of the elements [1, 1, 2, 2]. If the argument
<code>generate_permutations</code> is <code>TRUE</code>, all permutations are
returned. To solve balanced anticlustering exactly, it is sufficient
to inspect all partitions while ignoring duplicated permutations.
</p>


<h3>Value</h3>

<p>A list of all partitions (or permutations if
<code>generate_permutations</code> is <code>TRUE</code>).
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Generate all partitions to solve k-means anticlustering
## to optimality.

N &lt;- 14
K &lt;- 2
features &lt;- matrix(sample(N * 2, replace = TRUE), ncol = 2)
partitions &lt;- generate_partitions(N, K)
length(partitions) # number of possible partitions

## Create an objective function that takes the partition
## as first argument (then, we can use sapply to compute
## the objective for each partition)
var_obj &lt;- function(clusters, features) {
  variance_objective(features, clusters)
}

all_objectives &lt;- sapply(
  partitions,
  FUN = var_obj,
  features = features
)

## Check out distribution of the objective over all partitions:
hist(all_objectives) # many large, few low objectives
## Get best k-means anticlustering objective:
best_obj &lt;- max(all_objectives)
## It is possible that there are multiple best solutions:
sum(all_objectives == best_obj)
## Select one best partition:
best_anticlustering &lt;- partitions[all_objectives == best_obj][[1]]
## Look at mean for each partition:
by(features, best_anticlustering, function(x) round(colMeans(x), 2))


## Get best k-means clustering objective:
min_obj &lt;- min(all_objectives)
sum(all_objectives == min_obj)
## Select one best partition:
best_clustering &lt;- partitions[all_objectives == min_obj][[1]]

## Plot minimum and maximum objectives:
user_par &lt;- par("mfrow")
par(mfrow = c(1, 2))
plot_clusters(
  features,
  best_anticlustering,
  illustrate_variance = TRUE,
  main = "Maximum variance"
)
plot_clusters(
  features,
  best_clustering,
  illustrate_variance = TRUE,
  main = "Minimum variance"
)
par(mfrow = user_par)

</code></pre>

<hr>
<h2 id='kplus_anticlustering'>K-plus anticlustering</h2><span id='topic+kplus_anticlustering'></span>

<h3>Description</h3>

<p>Perform anticlustering using the k-plus objective to maximize between-group 
similarity. This function implements the k-plus anticlustering method described 
in Papenberg (2024; &lt;doi:10.1111/bmsp.12315&gt;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kplus_anticlustering(
  x,
  K,
  variance = TRUE,
  skew = FALSE,
  kurtosis = FALSE,
  covariances = FALSE,
  T = NULL,
  standardize = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kplus_anticlustering_+3A_x">x</code></td>
<td>
<p>A feature matrix where rows correspond to elements and columns
correspond to variables (a single numeric variable can be
passed as a vector).</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_k">K</code></td>
<td>
<p>How many anticlusters should be created. Alternatively:
(a) A vector describing the size of each group, or (b) a vector
of length <code>nrow(x)</code> describing how elements are assigned
to anticlusters before the optimization starts.</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_variance">variance</code></td>
<td>
<p>Boolean: Should the k-plus objective include a term to 
maximize between-group similarity with regard to the variance? 
(Default = TRUE)</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_skew">skew</code></td>
<td>
<p>Boolean: Should the k-plus objective include a term to 
maximize between-group similarity with regard to skewness? 
(Default = FALSE)</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_kurtosis">kurtosis</code></td>
<td>
<p>Boolean: Should the k-plus objective include a term to 
maximize between-group similarity with regard to kurtosis? 
(Default = FALSE)</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_covariances">covariances</code></td>
<td>
<p>Boolean: Should the k-plus objective include a term to 
maximize between-group similarity with regard to covariance structure? 
(Default = FALSE)</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_t">T</code></td>
<td>
<p>Optional argument: An integer specifying how many
distribution moments should be equalized between groups.</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_standardize">standardize</code></td>
<td>
<p>Boolean. If <code>TRUE</code>, the data is standardized through 
a call to <code><a href="base.html#topic+scale">scale</a></code> before the optimization starts. 
Defaults to TRUE. See details.</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_...">...</code></td>
<td>
<p>Arguments passed down to <code><a href="#topic+anticlustering">anticlustering</a></code>. All of the 
arguments are supported except for <code>objective</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the unweighted sum approach for k-plus 
anticlustering. Details are given in Papenberg (2024). 
</p>
<p>The optional argument <code>T</code> denotes the number of distribution 
moments that are considered in the anticlustering process. For example,
<code>T = 4</code> will lead to similar means, variances, skew and kurtosis. 
For the first four moments, it is also possible to use the boolean
convenience arguments <code>variance</code>, <code>skew</code> and <code>kurtosis</code>; the
mean (the first moment) is always included and cannot be &quot;turned off&quot;.
If the argument <code>T</code> is used, it overrides the arguments
<code>variance</code>, <code>skew</code> and <code>kurtosis</code> (corresponding to
the second, third and fourth moment), ignoring their values.
</p>
<p>The <code>standardization</code> is applied to all original features and the 
additional k-plus features that are appended to the data set in order 
to optimize the k-plus criterion. When using standardization, 
all criteria such as means, variances and skewness receive a comparable
weight during the optimization. It is usually recommended not
to change the default setting <code>standardization = TRUE</code>.
</p>
<p>This function can use any arguments that are also possible in 
<code><a href="#topic+anticlustering">anticlustering</a></code>
(except for 'objective' because the objective optimized here
is the k-plus objective; to use a different objective, 
call <code><a href="#topic+anticlustering">anticlustering</a></code> directly). Any arguments that are
not explicitly changed here (i.e., <code>standardize = TRUE</code>) receive the 
default given in <code><a href="#topic+anticlustering">anticlustering</a></code> 
(e.g., <code>method = "exchange"</code>.)
</p>


<h3>References</h3>

<p>Papenberg, M. (2024). K-plus Anticlustering: An Improved k-means Criterion for 
Maximizing Between-Group Similarity. British Journal of Mathematical and 
Statistical Psychology, 77(1), 80&ndash;102. https://doi.org/10.1111/bmsp.12315
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate some data
N &lt;- 180
M &lt;- 4
features &lt;- matrix(rnorm(N * M), ncol = M)
# standard k-plus anticlustering: optimize similarity with regard to mean and variance:
cl &lt;- kplus_anticlustering(features, K = 3, method = "local-maximum")
mean_sd_tab(features, cl)
# Visualize an anticlustering solution:
plot(features, col = palette()[2:4][cl], pch = c(16:18)[cl])

# Also optimize with regard to skewness and kurtosis
cl2 &lt;- kplus_anticlustering(
  features, 
  K = 3, 
  method = "local-maximum", 
  skew = TRUE, 
  kurtosis = TRUE
)

# The following two calls are equivalent: 
init_clusters &lt;- sample(rep_len(1:3, nrow(features)))
# 1.
x1 &lt;- kplus_anticlustering(
  features, 
  K = init_clusters, 
  variance = TRUE,
  skew = TRUE
)
# 2. 
x2 &lt;- kplus_anticlustering(
  features, 
  K = init_clusters, 
  T = 3
)
# Verify: 
all(x1 == x2)

</code></pre>

<hr>
<h2 id='kplus_moment_variables'>Compute k-plus variables</h2><span id='topic+kplus_moment_variables'></span>

<h3>Description</h3>

<p>Compute k-plus variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kplus_moment_variables(x, T, standardize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kplus_moment_variables_+3A_x">x</code></td>
<td>
<p>A vector, matrix or data.frame of data points. Rows
correspond to elements and columns correspond to features. A
vector represents a single feature.</p>
</td></tr>
<tr><td><code id="kplus_moment_variables_+3A_t">T</code></td>
<td>
<p>The number of distribution moments for which variables are generated.</p>
</td></tr>
<tr><td><code id="kplus_moment_variables_+3A_standardize">standardize</code></td>
<td>
<p>Logical, should all columns of the output be standardized
(defaults to TRUE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The k-plus criterion is an extension of the k-means criterion
(i.e., the &quot;variance&quot;, see <code><a href="#topic+variance_objective">variance_objective</a></code>).
In <code><a href="#topic+kplus_anticlustering">kplus_anticlustering</a></code>, equalizing means and variances
simultaneously (and possibly additional distribution moments) is
accomplished by internally appending new variables to the data
input <code>x</code>. When using only the  variance as additional criterion, the
new variables represent the squared difference of each data point to
the mean of the respective column. All columns are then included&mdash;in
addition to the original data&mdash;in standard k-means
anticlustering. The logic is readily extended towards higher order moments,
see Papenberg (2024). This function gives users the possibility to generate
k-plus variables themselves, which offers some additional flexibility when
conducting k-plus anticlustering.
</p>


<h3>Value</h3>

<p>A matrix containing all columns of <code>x</code> and all additional
columns of k-plus variables. If <code>x</code> has M columns, the output matrix
has M * T columns.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Papenberg, M. (2024). K-plus Anticlustering: An Improved k-means Criterion for 
Maximizing Between-Group Similarity. British Journal of Mathematical and 
Statistical Psychology, 77(1), 80&ndash;102. https://doi.org/10.1111/bmsp.12315
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Use Schaper data set for example
data(schaper2019)
features &lt;- schaper2019[, 3:6]
K &lt;- 3
N &lt;- nrow(features)

# Some equivalent ways of doing k-plus anticlustering:

init_groups &lt;- sample(rep_len(1:3, N))
table(init_groups)

kplus_groups1 &lt;- anticlustering(
  features,
  K = init_groups,
  objective = "kplus",
  standardize = TRUE,
  method = "local-maximum"
)

kplus_groups2 &lt;- anticlustering(
  kplus_moment_variables(features, T = 2), # standardization included by default
  K = init_groups,
  objective = "variance", # (!)
  method = "local-maximum"
)

# this function uses standardization by default unlike anticlustering():
kplus_groups3 &lt;- kplus_anticlustering(
  features, 
  K = init_groups,
  method = "local-maximum"
)

all(kplus_groups1 == kplus_groups2)
all(kplus_groups1 == kplus_groups3)
all(kplus_groups2 == kplus_groups3)

</code></pre>

<hr>
<h2 id='matching'>Matching</h2><span id='topic+matching'></span>

<h3>Description</h3>

<p>Conduct K-partite or unrestricted (minimum distance) matching to
find pairs or groups of similar elements. By default, finding
matches is based on the Euclidean distance between data points, but
a custom dissimilarity measure can also be employed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matching(
  x,
  p = 2,
  match_between = NULL,
  match_within = NULL,
  match_extreme_first = TRUE,
  target_group = NULL,
  sort_output = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matching_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A feature
matrix where rows correspond to elements and columns correspond
to variables (a single numeric variable can be passed as a
vector). (2) An N x N matrix dissimilarity matrix; can be an
object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="matching_+3A_p">p</code></td>
<td>
<p>The size of the groups; the default is 2, in which case
the function returns pairs.</p>
</td></tr>
<tr><td><code id="matching_+3A_match_between">match_between</code></td>
<td>
<p>An optional vector, <code>data.frame</code> or
matrix representing one or several categorical constraints. If
passed, the argument <code>p</code> is ignored and matches are sought
between elements of different categories.</p>
</td></tr>
<tr><td><code id="matching_+3A_match_within">match_within</code></td>
<td>
<p>An optional vector, <code>data.frame</code> or matrix
representing one or several categorical constraints. If passed,
matches are sought between elements of the same category.</p>
</td></tr>
<tr><td><code id="matching_+3A_match_extreme_first">match_extreme_first</code></td>
<td>
<p>Logical: Determines if matches are first
sought for extreme elements first or for central
elements. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="matching_+3A_target_group">target_group</code></td>
<td>
<p>Currently, the options &quot;none&quot;,
smallest&quot; and &quot;diverse&quot; are supported. See Details.</p>
</td></tr>
<tr><td><code id="matching_+3A_sort_output">sort_output</code></td>
<td>
<p>Boolean. If <code>TRUE</code> (default), the output clusters 
are sorted by similarity. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the data input <code>x</code> is a feature matrix, matching is based
on the Euclidean distance between data points. If the argument
<code>x</code> is a dissimilarity matrix, matching is based on the
user-specified dissimilarities. To find matches, the algorithm
proceeds by selecting a target element and then searching its
nearest neighbours. Critical to the behaviour or the algorithm is
the order in which target elements are selected. By default, the
most extreme elements are selected first, i.e., elements with the
highest distance to the centroid of the data set (see
<code><a href="#topic+balanced_clustering">balanced_clustering</a></code> that relies on the same
algorithm). Set the argument <code>match_extreme_first</code> to
<code>FALSE</code>, to enforce that elements close to the centroid are
first selected as targets.
</p>
<p>If the argument <code>match_between</code> is passed and the groups
specified via this argument are of different size, target elements
are selected from the smallest group by default (because in this
group, all elements can be matched). However, it is also possible
to specify how matches are selected through the option
<code>target_group</code>. When specifying <code>"none"</code>, matches are
always selected from extreme elements, irregardless of the group
sizes (or from central elements first if <code>match_extreme_first
= FALSE</code>). With option <code>"smallest"</code>, matches are selected from
the smallest group. With option <code>"diverse"</code>, matches are
selected from the most heterogenous group according to the sum of
pairwise distances within groups.
</p>
<p>The output is an integer vector encoding which elements have been
matched. The grouping numbers are sorted by similarity. That is,
elements with the grouping number »1« have the highest intra-group
similarity, followed by 2 etc (groups having the same similarity
index are still assigned a different grouping number,
though). Similarity is measured as the sum of pairwise (Euclidean)
distances within groups (see <code><a href="#topic+diversity_objective">diversity_objective</a></code>). To 
prevent sorting by similarity (this is some extra computational burden),
set <code>sort_output = FALSE</code>. Some unmatched elements may be <code>NA</code>. 
This happens if it is not
possible to evenly split the item pool evenly into groups of size
<code>p</code> or if the categories described by the argument
<code>match_between</code> are of different size.
</p>


<h3>Value</h3>

<p>An integer vector encoding the matches. See Details for
more information.
</p>


<h3>Note</h3>

<p>It is possible to specify grouping restrictions via
<code>match_between</code> and <code>match_within</code> at the same time.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Find triplets
N &lt;- 120
lds &lt;- data.frame(f1 = rnorm(N), f2 = rnorm(N))
triplets &lt;- matching(lds, p = 3)
plot_clusters(
  lds,
  clusters = triplets,
  within_connection = TRUE
)

# Bipartite matching with unequal-sized groups:
# Only selects matches for some elements
N &lt;- 100
data &lt;- matrix(rnorm(N), ncol = 1)
groups &lt;- sample(1:2, size = N, replace = TRUE, prob = c(0.8, 0.2))
matched &lt;- matching(data[, 1], match_between = groups)
plot_clusters(
  cbind(groups, data), 
  clusters = matched, 
  within_connection = TRUE
)

# Match objects from the same category only
matched &lt;- matching(
  schaper2019[, 3:6], 
  p = 3, 
  match_within = schaper2019$room
)
head(table(matched, schaper2019$room))

# Match between different plant species in the »iris« data set
species &lt;- iris$Species != "versicolor"
matched &lt;- matching(
  iris[species, 1], 
  match_between = iris[species, 5]
)
# Adjust `match_extreme_first` argument
matched2 &lt;- matching(
  iris[species, 1], 
  match_between = iris[species, 5],
  match_extreme_first = FALSE
)
# Plot the matching results
user_par &lt;- par("mfrow")
par(mfrow = c(1, 2))
data &lt;- data.frame(
  Species = as.numeric(iris[species, 5]),
  Sepal.Length = iris[species, 1]
)
plot_clusters(
  data,
  clusters = matched,
  within_connection = TRUE,
  main = "Extreme elements matched first"
)
plot_clusters(
  data,
  clusters = matched2,
  within_connection = TRUE,
  main = "Central elements matched first"
)
par(mfrow = user_par)


</code></pre>

<hr>
<h2 id='mean_sd_tab'>Means and standard deviations by group variable formatted in table</h2><span id='topic+mean_sd_tab'></span>

<h3>Description</h3>

<p>Means and standard deviations by group variable formatted in table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_sd_tab(features, groups, decimals = 2, na.rm = FALSE, return_diff = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mean_sd_tab_+3A_features">features</code></td>
<td>
<p>A data frame of features</p>
</td></tr>
<tr><td><code id="mean_sd_tab_+3A_groups">groups</code></td>
<td>
<p>A grouping vector</p>
</td></tr>
<tr><td><code id="mean_sd_tab_+3A_decimals">decimals</code></td>
<td>
<p>The number of decimals</p>
</td></tr>
<tr><td><code id="mean_sd_tab_+3A_na.rm">na.rm</code></td>
<td>
<p>Should NAs be removed prior to computing stats 
(Default = FALSE)</p>
</td></tr>
<tr><td><code id="mean_sd_tab_+3A_return_diff">return_diff</code></td>
<td>
<p>Boolean. Should an additional row be printed that 
contains the difference between minimum and maximum</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A table that illustrates means and standard deviations (in brackets)
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
mean_sd_tab(iris[, -5], iris[, 5])

</code></pre>

<hr>
<h2 id='n_partitions'>Number of equal sized partitions</h2><span id='topic+n_partitions'></span>

<h3>Description</h3>

<p>Number of equal sized partitions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>n_partitions(N, K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="n_partitions_+3A_n">N</code></td>
<td>
<p>How many elements</p>
</td></tr>
<tr><td><code id="n_partitions_+3A_k">K</code></td>
<td>
<p>How many partitions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The number of partitions
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n_partitions(20, 2)

</code></pre>

<hr>
<h2 id='optimal_anticlustering'>Optimal (&quot;exact&quot;) algorithms for anticlustering</h2><span id='topic+optimal_anticlustering'></span>

<h3>Description</h3>

<p>Wrapper function that gives access to all optimal algorithms for anticlustering 
that are available in anticlust.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal_anticlustering(x, K, objective, solver = NULL, time_limit = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optimal_anticlustering_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A feature
matrix where rows correspond to elements and columns correspond
to variables (a single numeric variable can be passed as a
vector). (2) An N x N matrix dissimilarity matrix; can be an
object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="optimal_anticlustering_+3A_k">K</code></td>
<td>
<p>How many anticlusters should be created or alternatively:
(a) A vector describing the size of each group (the latter
currently only works for <code>objective = "dispersion")</code>.</p>
</td></tr>
<tr><td><code id="optimal_anticlustering_+3A_objective">objective</code></td>
<td>
<p>The anticlustering objective, can be &quot;diversity&quot;,
&quot;variance&quot;, &quot;kplus&quot; or &quot;dispersion&quot;.</p>
</td></tr>
<tr><td><code id="optimal_anticlustering_+3A_solver">solver</code></td>
<td>
<p>Optional. The solver used to obtain the optimal
method.  Currently supports &quot;glpk&quot;, &quot;symphony&quot;, &quot;lpSolve&quot; and &quot;gurobi&quot;. 
See details.</p>
</td></tr>
<tr><td><code id="optimal_anticlustering_+3A_time_limit">time_limit</code></td>
<td>
<p>Time limit in seconds, given to the solver.
Default is there is no time limit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a wrapper for all optimal methods supported in anticlust
(currently and in the future).  As compared to
<code><a href="#topic+anticlustering">anticlustering</a></code>, it allows to specify the solver to
obtain an optimal solution and it can be used to obtain optimal
solutions for all supported anticlustering objectives (variance,
diversity, k-plus, dispersion). For the objectives &quot;variance&quot;,
&quot;diversity&quot; and &quot;kplus&quot;, the optimal ILP method in Papenberg and
Klau (2021) is used, which maximizes the sum of all pairwise
intra-cluster distances (given user specified number of clusters,
for equal-sized clusters).  To employ k-means anticlustering
(i.e. set <code>objective = "variance"</code>), the squared Euclidean
distance is used. For k-plus anticlustering, the squared Euclidean
distance based on the extended k-plus data matrix is used (see
<code><a href="#topic+kplus_moment_variables">kplus_moment_variables</a></code>).  For the diversity (and the
dispersion), the Euclidean distance is used by default, but any
user-defined dissimilarity matrix is possible.
</p>
<p>The dispersion is solved optimal using the approach described in
<code><a href="#topic+optimal_dispersion">optimal_dispersion</a></code>.
</p>
<p>The optimal methods make use of &quot;solvers&quot; that actually implement
the algorithm for finding optimal solutions. The package anticlust
supports three solvers:
</p>

<ul>
<li><p>The default solver lpSolve (&lt;https://sourceforge.net/projects/lpsolve/&gt;).
</p>
</li>
<li><p>GNU linear programming kit (&lt;http://www.gnu.org/software/glpk/&gt;), 
available from the package Rglpk and requested using <code>solver = "glpk"</code>.
The R package Rglpk has to be installed manually if this solver should be used.
</p>
</li>
<li><p>The Symphony solver (&lt;https://github.com/coin-or/SYMPHONY&gt;),
available from the package Rsymphony and requested using <code>solver = "symphony"</code>.
(The package Rsymphony has to be installed manually if this solver should be used).
</p>
</li>
<li><p>The commercial gurobi solver, see https://www.gurobi.com/downloads/.
</p>
</li></ul>

<p>For the maximum dispersion problem, it seems that the Symphony
solver is fastest, while the lpSolve solver seems to be good for
maximum diversity. However, note that in general the dispersion can
be solved optimally for much larger data sets than the diversity.
</p>
<p>If a <code>time_limit</code> is set and the function cannot find in the optimal
objective in the given time, it will throw an error.
</p>


<h3>Value</h3>

<p>A vector of length N that assigns a group (i.e, a number
between 1 and <code>K</code>) to each input element.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- matrix(rnorm(24), ncol = 2)

# These calls are equivalent for k-means anticlustering:
optimal_anticlustering(data, K = 2, objective = "variance")
optimal_anticlustering(dist(data)^2, K = 2, objective = "diversity")

# These calls are equivalent for k-plus anticlustering:
optimal_anticlustering(data, K = 2, objective = "kplus")
optimal_anticlustering(dist(kplus_moment_variables(data, 2))^2, K = 2, objective = "diversity")

</code></pre>

<hr>
<h2 id='optimal_dispersion'>Maximize dispersion for K groups</h2><span id='topic+optimal_dispersion'></span>

<h3>Description</h3>

<p>Maximize dispersion for K groups
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal_dispersion(
  x,
  K,
  solver = NULL,
  max_dispersion_considered = NULL,
  min_dispersion_considered = NULL,
  npartitions = 1,
  time_limit = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optimal_dispersion_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A
feature matrix where rows correspond to elements and columns
correspond to variables (a single numeric variable can be
passed as a vector). (2) An N x N matrix dissimilarity matrix;
can be an object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="optimal_dispersion_+3A_k">K</code></td>
<td>
<p>The number of groups or a vector describing the size of
each group.</p>
</td></tr>
<tr><td><code id="optimal_dispersion_+3A_solver">solver</code></td>
<td>
<p>Optional argument; currently supports &quot;lpSolve&quot;, 
&quot;glpk&quot;, &quot;symphony&quot;, and &quot;gurobi&quot;. See <code><a href="#topic+optimal_anticlustering">optimal_anticlustering</a></code>.</p>
</td></tr>
<tr><td><code id="optimal_dispersion_+3A_max_dispersion_considered">max_dispersion_considered</code></td>
<td>
<p>Optional argument used for early
stopping. If the dispersion found is equal to or exceeds this
value, a solution having the previous best dispersion is
returned.</p>
</td></tr>
<tr><td><code id="optimal_dispersion_+3A_min_dispersion_considered">min_dispersion_considered</code></td>
<td>
<p>Optional argument used for
speeding up the algorithm computation.  If passed, the
dispersion is optimized starting from this value instead the
global minimum distance.</p>
</td></tr>
<tr><td><code id="optimal_dispersion_+3A_npartitions">npartitions</code></td>
<td>
<p>The number of groupings that are returned, each
having an optimal dispersion value (defaults to 1).</p>
</td></tr>
<tr><td><code id="optimal_dispersion_+3A_time_limit">time_limit</code></td>
<td>
<p>Time limit in seconds, given to the solver.
Default is there is no time limit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dispersion is the minimum distance between two elements
within the same group. This function implements an optimal method
to maximize the dispersion. If the data input <code>x</code> is a
feature matrix and not a dissimilarity matrix, the pairwise
Euclidean distance is used. It uses the algorithm presented in
Max Diekhoff's Bachelor thesis at the Computer Science Department
at Heinrich Heine University Düsseldorf.
</p>
<p>To find out which items are not allowed to be grouped in the same
cluster for maximum dispersion, the algorithm sequentially builds
instances of a graph coloring problem, using an integer linear
programming (ILP) representation (also see Fernandez et al.,
2013).  It is possible to specify the ILP solver via the argument
<code>solver</code> (See <code><a href="#topic+optimal_anticlustering">optimal_anticlustering</a></code> for more
information on this argument). Optimally solving the maximum
dispersion problem is NP-hard for K &gt; 2 and therefore
computationally infeasible for larger data sets. For K = 3 and K
= 4, it seems that this approach scales up to several 100
elements, or even &gt; 1000 for K = 3 (at least when using the
Symphony solver).  For larger data sets, use the heuristic
approaches in <code><a href="#topic+anticlustering">anticlustering</a></code> or
<code><a href="#topic+bicriterion_anticlustering">bicriterion_anticlustering</a></code>. However, note that for
K = 2, the optimal approach is usually much faster than the
heuristics.
</p>
<p>In the output, the element <code>edges</code> defines which elements
must be in separate clusters in order to achieve maximum
dispersion. All elements not listed here can be changed
arbitrarily between clusters without reducing the dispersion.  If
the maximum possible dispersion corresponds to the minimum
dispersion in the data set, the output elements <code>edges</code> and
<code>groups</code> are set to <code>NULL</code> because all possible
groupings have the same value of dispersion.  In this case the
output element <code>dispersions_considered</code> has length 1.
</p>
<p>If a <code>time_limit</code> is set and the function cannot find in the optimal
dispersion in the given time, it will throw an error.
</p>


<h3>Value</h3>

<p>A list with four elements:  
</p>
<table role = "presentation">
<tr><td><code>dispersion</code></td>
<td>
<p>The optimal dispersion</p>
</td></tr>
<tr><td><code>groups</code></td>
<td>
<p>An assignment of elements to groups (vector)</p>
</td></tr>
<tr><td><code>edges</code></td>
<td>
<p>A matrix of 2 columns. Each row contains the indices of 
elements that had to be investigated to find the dispersion (i.e., each pair
of elements cannot be part of the same group in order to achieve maximum 
dispersion).</p>
</td></tr>
<tr><td><code>dispersions_considered</code></td>
<td>
<p>All distances that were tested until the dispersion was found.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If the SYMPHONY solver is used, an unfortunate &quot;message&quot; is
printed to the console when this function terminates:
</p>
<p>sym_get_col_solution(): No solution has been stored!
</p>
<p>This message is no reason to worry and instead is a direct result
of the algorithm finding the optimal value for the dispersion.
Unfortunately, this message is generated in the C code underlying
the SYMPHONY library (via the printing function <code>printf</code>),
which cannot be prevented in R.
</p>


<h3>Author(s)</h3>

<p>Max Diekhoff
</p>
<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Diekhoff (2023). Maximizing dispersion for anticlustering. Retrieved from 
https://www.cs.hhu.de/fileadmin/redaktion/Fakultaeten/Mathematisch-Naturwissenschaftliche_Fakultaet/Informatik/Algorithmische_Bioinformatik/Bachelor-_Masterarbeiten/2831963_ba_ifo_AbschlArbeit_klau_mapap102_madie120_20230203_1815.pdf  
</p>
<p>Fernández, E., Kalcsics, J., &amp; Nickel, S. (2013). The maximum dispersion 
problem. Omega, 41(4), 721–730. https://doi.org/10.1016/j.omega.2012.09.005
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dispersion_objective">dispersion_objective</a></code> <code><a href="#topic+anticlustering">anticlustering</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N &lt;- 30
M &lt;- 5
K &lt;- 3
data &lt;- matrix(rnorm(N*M), ncol = M)
distances &lt;- dist(data)

opt &lt;- optimal_dispersion(distances, K = K)
opt

# Compare to bicriterion heuristic:
groups_heuristic &lt;- anticlustering(
  distances, 
  K = K,
  method = "brusco", 
  objective = "dispersion", 
  repetitions = 100
)
c(
  OPT = dispersion_objective(distances, opt$groups),
  HEURISTIC = dispersion_objective(distances, groups_heuristic)
)

# Different group sizes are possible:
table(optimal_dispersion(distances, K = c(15, 10, 5))$groups)

# Induce cannot-link constraints by maximizing the dispersion:
solvable &lt;- matrix(1, ncol = 6, nrow = 6)
solvable[2, 1] &lt;- -1
solvable[3, 1] &lt;- -1
solvable[4, 1] &lt;- -1
solvable &lt;- as.dist(solvable)
solvable

# An optimal solution has to put item 1 in a different group than 
# items 2, 3 and 4 -&gt; this is possible for K = 2
optimal_dispersion(solvable, K = 2)$groups

# It no longer works when item 1 can also not be linked with item 5:
# (check out output!)
unsolvable &lt;- as.matrix(solvable)
unsolvable[5, 1] &lt;- -1
unsolvable &lt;- as.dist(unsolvable)
unsolvable
optimal_dispersion(unsolvable, K = 2)
# But:
optimal_dispersion(unsolvable, K = c(2, 4)) # group sizes, not number of groups

</code></pre>

<hr>
<h2 id='plot_clusters'>Visualize a cluster analysis</h2><span id='topic+plot_clusters'></span>

<h3>Description</h3>

<p>Visualize a cluster analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_clusters(
  features,
  clusters,
  within_connection = FALSE,
  between_connection = FALSE,
  illustrate_variance = FALSE,
  show_axes = FALSE,
  xlab = NULL,
  ylab = NULL,
  xlim = NULL,
  ylim = NULL,
  main = "",
  cex = 1.2,
  cex.axis = 1.2,
  cex.lab = 1.2,
  lwd = 1.5,
  lty = 2,
  frame.plot = FALSE,
  cex_centroid = 2
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_clusters_+3A_features">features</code></td>
<td>
<p>A data.frame or matrix representing the features that
are plotted. Must have two columns.</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_clusters">clusters</code></td>
<td>
<p>A vector representing the clustering</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_within_connection">within_connection</code></td>
<td>
<p>Boolean. Connect the elements within each
clusters through lines? Useful to illustrate a graph structure.</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_between_connection">between_connection</code></td>
<td>
<p>Boolean. Connect the elements between each
clusters through lines? Useful to illustrate a graph structure.
(This argument only works for two clusters).</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_illustrate_variance">illustrate_variance</code></td>
<td>
<p>Boolean. Illustrate the variance criterion
in the plot?</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_show_axes">show_axes</code></td>
<td>
<p>Boolean, display values on the x and y-axis? Defaults
to 'FALSE'.</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_xlab">xlab</code></td>
<td>
<p>The label for the x-axis</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_ylab">ylab</code></td>
<td>
<p>The label for the y-axis</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_xlim">xlim</code></td>
<td>
<p>The limits for the x-axis</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_ylim">ylim</code></td>
<td>
<p>The limits for the y-axis</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_main">main</code></td>
<td>
<p>The title of the plot</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_cex">cex</code></td>
<td>
<p>The size of the plotting symbols, see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_cex.axis">cex.axis</code></td>
<td>
<p>The size of the values on the axes</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_cex.lab">cex.lab</code></td>
<td>
<p>The size of the labels of the axes</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_lwd">lwd</code></td>
<td>
<p>The width of the lines connecting elements.</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_lty">lty</code></td>
<td>
<p>The line type of the lines connecting elements
(see <code><a href="graphics.html#topic+par">par</a></code>).</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_frame.plot">frame.plot</code></td>
<td>
<p>a logical indicating whether a box should be drawn
around the plot.</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_cex_centroid">cex_centroid</code></td>
<td>
<p>The size of the cluster center symbol (has an
effect only if <code>illustrate_variance</code> is <code>TRUE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In most cases, the argument <code>clusters</code> is a vector
returned by one of the functions <code><a href="#topic+anticlustering">anticlustering</a></code>,
<code><a href="#topic+balanced_clustering">balanced_clustering</a></code> or <code><a href="#topic+matching">matching</a></code>. 
However, the plotting function can also be used to plot the results 
of other cluster functions such as <code><a href="stats.html#topic+kmeans">kmeans</a></code>. This function
is usually just used to get a fast impression of the results of an 
(anti)clustering assignment, but limited in its functionality. 
It is useful for depicting the intra-cluster connections using 
argument <code>within_connection</code>.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N &lt;- 15
features &lt;- matrix(runif(N * 2), ncol = 2)
K &lt;- 3
clusters &lt;- balanced_clustering(features, K = K)
anticlusters &lt;- anticlustering(features, K = K)
user_par &lt;- par("mfrow")
par(mfrow = c(1, 2))
plot_clusters(features, clusters, main = "Cluster editing", within_connection = TRUE)
plot_clusters(features, anticlusters, main = "Anticluster editing", within_connection = TRUE)
par(mfrow = user_par)

</code></pre>

<hr>
<h2 id='plot_similarity'>Plot similarity objective by cluster</h2><span id='topic+plot_similarity'></span>

<h3>Description</h3>

<p>Plot similarity objective by cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_similarity(x, groups)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_similarity_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A data matrix
where rows correspond to elements and columns correspond to
features (a single numeric feature can be passed as a vector). (2)
An N x N matrix dissimilarity matrix; can be an object of class
<code>dist</code> (e.g., returned by <code><a href="stats.html#topic+dist">dist</a></code> or
<code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code> where the entries of
the upper and lower triangular matrix represent the pairwise
dissimilarities.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_groups">groups</code></td>
<td>
<p>A grouping vector of length N, usually the output
of <code><a href="#topic+matching">matching</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots the sum of pairwise distances by group.
</p>


<h3>Value</h3>

<p>The diversity (sum of distances) by group.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+diversity_objective">diversity_objective</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Match elements and plot similarity by match
N &lt;- 100
lds &lt;- data.frame(f1 = rnorm(N), f2 = rnorm(N))
pairs &lt;- matching(lds, p = 2)
plot_similarity(lds, pairs)

</code></pre>

<hr>
<h2 id='schaper2019'>Ratings for 96 words</h2><span id='topic+schaper2019'></span>

<h3>Description</h3>

<p>A stimulus set that was used in experiments by Schaper, Kuhlmann and
Bayen (2019a; 2019b). The item pool consists of 96 German words. 
Each word represents an object that is either 
typically found in a bathroom or in a kitchen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>schaper2019
</code></pre>


<h3>Format</h3>

<p>A data frame with 96 rows and 7 variables
</p>

<dl>
<dt>item</dt><dd><p>The name of an object (in German)</p>
</dd>
<dt>room</dt><dd><p>The room in which the item is typically found; can be
'kitchen' or 'bathroom'</p>
</dd>
<dt>rating_consistent</dt><dd><p>How expected would it
be to find the <code>item</code> in the typical <code>room</code></p>
</dd>
<dt>rating_inconsistent</dt><dd><p>How expected would it
be to find the <code>item</code> in the atypical <code>room</code></p>
</dd>
<dt>syllables</dt><dd><p>The number of syllables in the object name</p>
</dd>
<dt>frequency</dt><dd><p>A value indicating the relative frequency of the
object name in German language (lower values indicate higher
frequency)</p>
</dd>
<dt>list</dt><dd><p>Represents the set affiliation of the <code>item</code> as
realized in experiments by Schaper et al.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Courteously provided by Marie Lusia Schaper and Ute Bayen.
</p>


<h3>References</h3>

<p>Schaper, M. L., Kuhlmann, B. G., &amp; Bayen, U. J. (2019a). Metacognitive expectancy
effects in source monitoring: Beliefs, in-the-moment experiences, or both? Journal
of Memory and Language, 107, 95–110. https://doi.org/10.1016/j.jml.2019.03.009
</p>
<p>Schaper, M. L., Kuhlmann, B. G., &amp; Bayen, U. J. (2019b). Metamemory expectancy illusion
and schema-consistent guessing in source monitoring. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 45, 470.
https://doi.org/10.1037/xlm0000602
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
head(schaper2019)
features &lt;- schaper2019[, 3:6]

# Optimize the variance criterion
# (tends to maximize similarity in feature means)
anticlusters &lt;- anticlustering(
  features,
  K = 3,
  objective = "variance",
  categories = schaper2019$room,
  method = "exchange"
)

# Means are quite similar across sets:
by(features, anticlusters, function(x) round(colMeans(x), 2))
# Check differences in standard deviations:
by(features, anticlusters, function(x) round(apply(x, 2, sd), 2))
# Room is balanced between the three sets:
table(Room = schaper2019$room, Set = anticlusters)

# Maximize the diversity criterion
ac_dist &lt;- anticlustering(
  features,
  K = 3,
  objective = "diversity",
  categories = schaper2019$room,
  method = "exchange"
)
# With the distance criterion, means tend to be less similar,
# but standard deviations tend to be more similar:
by(features, ac_dist, function(x) round(colMeans(x), 2))
by(features, ac_dist, function(x) round(apply(x, 2, sd), 2))


</code></pre>

<hr>
<h2 id='three_phase_search_anticlustering'>Three phase search with dynamic population size heuristic</h2><span id='topic+three_phase_search_anticlustering'></span>

<h3>Description</h3>

<p>This function implements the three phase search algorithm TPSPD for
anticlustering by Yang et al. (2022; &lt;doi.org/10.1016/j.ejor.2022.02.003&gt;).
The description of their algorithm is
given in Section 2 of their paper (in particular, see the
Pseudocode in Algorithm 1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>three_phase_search_anticlustering(
  x,
  K,
  N,
  objective = "diversity",
  number_iterations = 50,
  clusters = NULL,
  upper_bound = NULL,
  lower_bound = NULL,
  beta_max = 15,
  theta_max = NULL,
  theta_min = NULL,
  beta_min = NULL,
  eta_max = 3,
  alpha = 0.05
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="three_phase_search_anticlustering_+3A_x">x</code></td>
<td>
<p>The data input, as in <code><a href="#topic+anticlustering">anticlustering</a></code>.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_k">K</code></td>
<td>
<p>Number of anticlusters to be formed.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_n">N</code></td>
<td>
<p>Number of elements.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_objective">objective</code></td>
<td>
<p>The anticlustering objective, can be &quot;diversity&quot; or &quot;dispersion&quot;.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_number_iterations">number_iterations</code></td>
<td>
<p>A number that defines how many times the steps in the search algorithm are repeated.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_clusters">clusters</code></td>
<td>
<p>A vector of length K that specifies the number of elements each cluster can contain. 
If this vector is not NULL, the lower and upper bounds will be disregarded.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_upper_bound">upper_bound</code></td>
<td>
<p>Maximum number of elements in each anticluster. By default, anticlusters are of equal size,
calculated as the total number of items divided by the number of clusters.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_lower_bound">lower_bound</code></td>
<td>
<p>Minimum number of elements in each anticluster. By default, anticlusters are of equal size,
calculated as the total number of items divided by the number of clusters.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_beta_max">beta_max</code></td>
<td>
<p>The algorithm begins with a pool of random initial solutions of size beta_max. 
Over time, the size of the solution pool decreases linearly until it reaches beta_min.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_theta_max">theta_max</code></td>
<td>
<p>Parameter for the strength of undirected perturbation, 
which decreases linearly over time from theta_max to theta_min.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_theta_min">theta_min</code></td>
<td>
<p>Parameter for the strength of undirected perturbation, 
which decreases linearly over time from theta_max to theta_min.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_beta_min">beta_min</code></td>
<td>
<p>The minimum solution pool size the algorithm should reach before making a determination.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_eta_max">eta_max</code></td>
<td>
<p>Parameter that specifies how many times the steps in the direct perturbation are executed.</p>
</td></tr>
<tr><td><code id="three_phase_search_anticlustering_+3A_alpha">alpha</code></td>
<td>
<p>Parameter for weighing the discrimination of a slightly worse local optimal child solution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Details of the implementation of the algorithm can be found 
in the pseudocode of the paper Yang et al. (2022). However, we performed one change
as compared to the original description of the algorithm: Instead of 
setting a time limit, we define the number of iterations the algorithm 
performs before terminating (via argument <code>number_iterations</code>).
</p>


<h3>Value</h3>

<p>A vector of length N that assigns a group (i.e, a number
between 1 and <code>K</code>) to each input element
</p>


<h3>Author(s)</h3>

<p>Hannah Hengelbrock <a href="mailto:Hannah.Hengelbrock@hhu.de">Hannah.Hengelbrock@hhu.de</a>, 
Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Xiao Yang et al. “A three-phase search approach with dynamic population size for solving 
the maximally diverse grouping problem”. In: European Journal of Operational Research
302.3 (2022) &lt;doi:10.1016/j.ejor.2022.02.003&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate some random data
N &lt;- 120
M &lt;- 5
K &lt;- 4
dat &lt;- matrix(rnorm(N * M), ncol = M)
distances &lt;- dist(dat)

# Perform three hase serach algorithm
result1 &lt;- three_phase_search_anticlustering(dat, K, N)

# Compute objectives function
diversity_objective(distances, result1)

# Standard algorithm:
result2 &lt;- anticlustering(distances, K=K, method="local-maximum", repetitions = 10)
diversity_objective(distances, result2)


</code></pre>

<hr>
<h2 id='variance_objective'>Objective value for the variance criterion</h2><span id='topic+variance_objective'></span>

<h3>Description</h3>

<p>Compute the k-means variance objective for a given clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>variance_objective(x, clusters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="variance_objective_+3A_x">x</code></td>
<td>
<p>A vector, matrix or data.frame of data points. Rows
correspond to elements and columns correspond to features. A
vector represents a single feature.</p>
</td></tr>
<tr><td><code id="variance_objective_+3A_clusters">clusters</code></td>
<td>
<p>A vector representing (anti)clusters (e.g., returned
by <code><a href="#topic+anticlustering">anticlustering</a></code> or
<code><a href="#topic+balanced_clustering">balanced_clustering</a></code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The variance objective is given by the sum of the squared
errors between cluster centers and individual data points. It is the
objective function used in k-means clustering, see
<code><a href="stats.html#topic+kmeans">kmeans</a></code>.
</p>


<h3>Value</h3>

<p>The total within-cluster variance
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Jain, A. K. (2010). Data clustering: 50 years beyond k-means.
Pattern Recognition Letters, 31, 651–666.
</p>
<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>
<p>Späth, H. (1986). Anticlustering: Maximizing the variance criterion.
Control and Cybernetics, 15, 213–218.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
## Clustering
clusters &lt;- balanced_clustering(
  iris[, -5],
  K = 3
)
# This is low:
variance_objective(
  iris[, -5],
  clusters
)
## Anticlustering
anticlusters &lt;- anticlustering(
  iris[, -5],
  K = 3,
  objective = "variance"
)
# This is higher:
variance_objective(
  iris[, -5],
  anticlusters
)

# Illustrate variance objective
N &lt;- 18
data &lt;- matrix(rnorm(N * 2), ncol = 2)
cl &lt;- balanced_clustering(data, K = 3)
plot_clusters(data, cl, illustrate_variance = TRUE)
</code></pre>

<hr>
<h2 id='wce'>Exact weighted cluster editing</h2><span id='topic+wce'></span>

<h3>Description</h3>

<p>Optimally solves weighted cluster editing (also known as »correlation clustering« or
»clique partitioning problem«).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wce(x, solver = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wce_+3A_x">x</code></td>
<td>
<p>A N x N similarity matrix. Larger values indicate stronger
agreement / similarity between a pair of data points</p>
</td></tr>
<tr><td><code id="wce_+3A_solver">solver</code></td>
<td>
<p>Optional argument; if passed, has to be either &quot;glpk&quot; or
&quot;symphony&quot;. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Finds the clustering that maximizes the sum of pairwise similarities within clusters. 
In the input some similarities should be negative (indicating dissimilarity) because 
otherwise the maximum sum of similarities is obtained by simply joining all elements 
within a single big cluster. The function uses a &quot;solver&quot; to optimize
the clustering objective. See <code><a href="#topic+optimal_anticlustering">optimal_anticlustering</a></code>
for an overview of the solvers that are available.
</p>


<h3>Value</h3>

<p>An integer vector representing the cluster affiliation of each data point
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Bansal, N., Blum, A., &amp; Chawla, S. (2004). Correlation clustering. 
Machine Learning, 56, 89–113. 
</p>
<p>Böcker, S., &amp; Baumbach, J. (2013). Cluster editing. In Conference on 
Computability in Europe (pp. 33–44).
</p>
<p>Grötschel, M., &amp; Wakabayashi, Y. (1989). A cutting plane algorithm
for a clustering problem. Mathematical Programming, 45, 59-96.
</p>
<p>Wittkop, T., Emig, D., Lange, S., Rahmann, S., Albrecht, M., Morris, J. H., ..., Baumbach,
J. (2010). Partitioning biological data with transitivity clustering. Nature Methods, 7,
419–420.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
features &lt;- swiss
distances &lt;- dist(scale(swiss))
hist(distances)
# Define agreement as being close enough to each other.
# By defining low agreement as -1 and high agreement as +1, we
# solve *unweighted* cluster editing
agreements &lt;- ifelse(as.matrix(distances) &lt; 3, 1, -1)
clusters &lt;- wce(agreements)
plot(swiss, col = clusters, pch = 19)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
