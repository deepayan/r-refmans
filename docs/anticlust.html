<!DOCTYPE html><html><head><title>Help for package anticlust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {anticlust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#anticlust'><p>anticlust: Subset Partitioning via Anticlustering</p></a></li>
<li><a href='#anticlustering'><p>Anticlustering</p></a></li>
<li><a href='#balanced_clustering'><p>Create balanced clusters of equal size</p></a></li>
<li><a href='#bicriterion_anticlustering'><p>Bicriterion iterated local search heuristic</p></a></li>
<li><a href='#categorical_sampling'><p>Random sampling employing a categorical constraint</p></a></li>
<li><a href='#categories_to_binary'><p>Get binary representation of categorical variables</p></a></li>
<li><a href='#dispersion_objective'><p>Cluster dispersion</p></a></li>
<li><a href='#diversity_objective'><p>(Anti)cluster editing &quot;diversity&quot; objective</p></a></li>
<li><a href='#fast_anticlustering'><p>Fast anticlustering</p></a></li>
<li><a href='#generate_partitions'><p>Generate all partitions of same cardinality</p></a></li>
<li><a href='#kplus_anticlustering'><p>K-plus anticlustering</p></a></li>
<li><a href='#kplus_moment_variables'><p>Compute k-plus variables</p></a></li>
<li><a href='#matching'><p>Matching</p></a></li>
<li><a href='#mean_sd_tab'><p>Means and standard deviations by group variable formatted in table</p></a></li>
<li><a href='#n_partitions'><p>Number of equal sized partitions</p></a></li>
<li><a href='#optimal_dispersion'><p>Maximize dispersion for K groups</p></a></li>
<li><a href='#plot_clusters'><p>Visualize a cluster analysis</p></a></li>
<li><a href='#plot_similarity'><p>Plot similarity objective by cluster</p></a></li>
<li><a href='#schaper2019'><p>Ratings for 96 words</p></a></li>
<li><a href='#variance_objective'><p>Objective value for the variance criterion</p></a></li>
<li><a href='#wce'><p>Exact weighted cluster editing</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Subset Partitioning via Anticlustering</td>
</tr>
<tr>
<td>Version:</td>
<td>0.8.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Martin Papenberg <a href="https://orcid.org/0000-0002-9900-4268"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Meik Michalke [ctb] (centroid based clustering algorithm),
  Gunnar W. Klau [ths],
  Juliane V. Nagel [ctb] (package logo),
  Martin Breuer [ctb] (Bicriterion algorithm by Brusco et al.),
  Marie L. Schaper [ctb] (Example data set),
  Max Diekhoff [ctb] (Optimal maximum dispersion algorithm)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Martin Papenberg &lt;martin.papenberg@hhu.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The method of anticlustering partitions a pool of elements
    into groups (i.e., anticlusters) with the goal of maximizing
    between-group similarity or within-group heterogeneity.  The
    anticlustering approach thereby reverses the logic of cluster analysis
    that strives for high within-group homogeneity and clear separation
    between groups.  Computationally, anticlustering is accomplished by
    maximizing instead of minimizing a clustering objective function, such
    as the intra-cluster variance (used in k-means clustering) or the sum
    of pairwise distances within clusters. The main function
    anticlustering() gives access to exact and heuristic anticlustering
    methods described in Papenberg and Klau (2021;
    &lt;<a href="https://doi.org/10.1037%2Fmet0000301">doi:10.1037/met0000301</a>&gt;), Brusco et al. (2020;
    &lt;<a href="https://doi.org/10.1111%2Fbmsp.12186">doi:10.1111/bmsp.12186</a>&gt;), and Papenberg (2023;
    &lt;<a href="https://doi.org/10.1111%2Fbmsp.12315">doi:10.1111/bmsp.12315</a>&gt;). The exact algorithms require that an
    integer linear programming solver is installed, either the GNU linear
    programming kit (<a href="https://www.gnu.org/software/glpk/glpk.html">https://www.gnu.org/software/glpk/glpk.html</a>)
    together with the interface package 'Rglpk'
    (<a href="https://cran.R-project.org/package=Rglpk">https://cran.R-project.org/package=Rglpk</a>), or the SYMPHONY ILP
    solver (<a href="https://github.com/coin-or/SYMPHONY">https://github.com/coin-or/SYMPHONY</a>) together with the
    interface package 'Rsymphony'
    (<a href="https://cran.r-project.org/package=Rsymphony">https://cran.r-project.org/package=Rsymphony</a>). Full access to the
    bicriterion anticlustering method proposed by Brusco et al. (2020) is
    given via the function bicriterion_anticlustering(), while
    kplus_anticlustering() implements the full functionality of the k-plus
    anticlustering approach proposed by Papenberg (2023). Some other
    functions are available to solve classical clustering problems. The
    function balanced_clustering() applies a cluster analysis under size
    constraints, i.e., creates equal-sized clusters. The function
    matching() can be used for (unrestricted, bipartite, or K-partite)
    matching. The function wce() can be used optimally solve the
    (weighted) cluster editing problem, also known as correlation
    clustering, clique partitioning problem or transitivity clustering.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/m-Py/anticlust">https://github.com/m-Py/anticlust</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/m-Py/anticlust/issues">https://github.com/m-Py/anticlust/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, RANN (&ge; 2.6.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, palmerpenguins, Rglpk, rmarkdown, Rsymphony, testthat</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>The exact (anti)clustering algorithms require that
either the GNU linear programming kit (GLPK library) is
installed (&lt;http://www.gnu.org/software/glpk/&gt;) or the SYMPHONY
open source MILP solver
(&lt;https://github.com/coin-or/SYMPHONY&gt;). Rendering the vignette
requires pandoc (&lt;https://pandoc.org/&gt;).</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-26 07:19:38 UTC; martin</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-26 20:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='anticlust'>anticlust: Subset Partitioning via Anticlustering</h2><span id='topic+anticlust'></span>

<h3>Description</h3>

<p>The method of anticlustering partitions a pool of elements into
groups (i.e., anticlusters) in such a way that the between-group
similarity is maximized and &ndash; at the same time &ndash; the within-group
heterogeneity is maximized. This reverses the logic of cluster
analysis that strives for high within-group homogeneity and low
similarity of the different groups. Computationally, anticlustering
is accomplished by maximizing instead of minimizing a clustering
objective function, such as the intra-cluster variance (used in
k-means clustering) or the sum of pairwise distances within
clusters.  The function anticlustering() implements exact and
heuristic anticlustering algorithms as described in Papenberg and
Klau (2020; &lt;doi:10.1037/met0000301&gt;). The exact approach requires
that the GNU linear programming kit
(&lt;https://www.gnu.org/software/glpk/glpk.html&gt;) is available and
the R package 'Rglpk' (&lt;https://cran.R-project.org/package=Rglpk&gt;)
is installed. Some other functions are available to solve classical
clustering problems. The function balanced_clustering() applies a
cluster analysis under size constraints, i.e., creates equal-sized
clusters. The function matching() can be used for (unrestricted,
bipartite, or K-partite) matching. The function wce() can be used
optimally solve the (weighted) cluster editing problem, also known
as correlation clustering, clique partitioning problem or
transitivity clustering.
</p>


<h3>Primary functions</h3>

<p><code><a href="#topic+anticlustering">anticlustering</a></code> 
<code><a href="#topic+balanced_clustering">balanced_clustering</a></code> 
<code><a href="#topic+matching">matching</a></code> 
<code><a href="#topic+categorical_sampling">categorical_sampling</a></code> 
<code><a href="#topic+wce">wce</a></code>
</p>

<hr>
<h2 id='anticlustering'>Anticlustering</h2><span id='topic+anticlustering'></span>

<h3>Description</h3>

<p>Partition a pool of elements into groups (i.e., anticlusters) with
the aim of creating high within-group heterogeneity and high
between-group similarity.  Anticlustering is accomplished by
maximizing instead of minimizing a clustering objective function.
Implements anticlustering methods as described in Papenberg and
Klau (2021; &lt;doi:10.1037/met0000301&gt;), Brusco et al. 
(2020; &lt;doi:10.1111/bmsp.12186&gt;), and Papenberg (2023; 
&lt;doi:10.1111/bmsp.12315&gt;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anticlustering(
  x,
  K,
  objective = "diversity",
  method = "exchange",
  preclustering = FALSE,
  categories = NULL,
  repetitions = NULL,
  standardize = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anticlustering_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A
feature matrix where rows correspond to elements and columns
correspond to variables (a single numeric variable can be
passed as a vector). (2) An N x N matrix dissimilarity matrix;
can be an object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_k">K</code></td>
<td>
<p>How many anticlusters should be created. Alternatively:
(a) A vector describing the size of each group, or (b) a vector
of length <code>nrow(x)</code> describing how elements are assigned
to anticlusters before the optimization starts.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_objective">objective</code></td>
<td>
<p>The objective to be maximized. The options
&quot;diversity&quot; (default; previously called &quot;distance&quot;, which is
still supported), &quot;variance&quot;, &quot;kplus&quot; and &quot;dispersion&quot; are
natively supported. May also be a user-defined function. See
Details.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_method">method</code></td>
<td>
<p>One of &quot;exchange&quot; (default) , &quot;local-maximum&quot;,
&quot;brusco&quot;, or &quot;ilp&quot;.  See Details.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_preclustering">preclustering</code></td>
<td>
<p>Boolean. Should a preclustering be conducted
before anticlusters are created? Defaults to <code>FALSE</code>. See
Details.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_categories">categories</code></td>
<td>
<p>A vector, data.frame or matrix representing one
or several categorical variables whose distribution should be similar 
between groups. See Details.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_repetitions">repetitions</code></td>
<td>
<p>The number of times a search heuristic is
initiated when using <code>method = "exchange"</code>, <code>method =
"local-maximum"</code>, or <code>method = "brusco"</code>. In the end, the
best objective found across the repetitions is returned.</p>
</td></tr>
<tr><td><code id="anticlustering_+3A_standardize">standardize</code></td>
<td>
<p>Boolean. If <code>TRUE</code> and <code>x</code> is a
feature matrix, the data is standardized through a call to
<code><a href="base.html#topic+scale">scale</a></code> before the optimization starts. This
argument is silently ignored if <code>x</code> is a distance matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to solve anticlustering. That is, the data
input is divided into <code>K</code> groups in such a way that elements
within groups are heterogeneous and the different groups are
similar. Anticlustering is accomplished by maximizing instead of
minimizing a clustering objective function. The maximization of
four clustering objective functions is natively supported (other
functions can also defined by the user as described below):
</p>

<ul>
<li><p>the 'diversity', setting 
<code>objective = "diversity"</code> (this is the default objective)
</p>
</li>
<li><p>k-means 'variance' objective, setting <code>objective = "variance"</code>
</p>
</li>
<li><p>'k-plus' objective, an extension of the k-means variance criterion,
setting <code>objective = "kplus"</code>
</p>
</li>
<li><p>the 'dispersion' objective is the minimum distance between 
any two elements within the same cluster (setting 
<code>objective = "dispersion"</code>)
</p>
</li></ul>

<p>The k-means objective is the within-group variance&mdash;that is, the
sum of the squared distances between each element and its cluster
center (see <code><a href="#topic+variance_objective">variance_objective</a></code>). K-means
anticlustering focuses on minimizing differences with regard to the
means of the input variables (that is, the columns in <code>x</code>), but it ignores any other distribution
characterstics such as the variance / standard deviation. K-plus anticlustering
(using <code>objective = "kplus"</code>) is an extension of the k-means criterion that also
minimizes differences with regard to the standard
deviations between groups (for details see <code><a href="#topic+kplus_anticlustering">kplus_anticlustering</a></code>). K-plus
anticlustering can also be extended towards higher order moments such as skew and kurtosis; 
to consider these additional distribution characteristics, use the function
<code><a href="#topic+kplus_anticlustering">kplus_anticlustering</a></code>. Setting <code>objective = "kplus"</code> in 
<code>anticlustering</code> function will only consider means 
and standard deviations (in my experience, this is what users usually want). 
It is strongly recommended to set the argument <code>standardize = TRUE</code> 
when using the k-plus objective.
</p>
<p>The &quot;diversity&quot; objective is the sum of pairwise
distances of elements within the same groups (see
<code><a href="#topic+diversity_objective">diversity_objective</a></code>). Hence, anticlustering using the diversity 
criterion maximizes between-group similarity
by maximizing within-group heterogeneity (represented as the sum of all pairwise distances). 
The diversity is an all rounder objective that tends to equalize all distribution 
characteristics between groups (such as means, variances, ...). 
Note that the equivalence of within-group heterogeneity and between-group similarity only
holds for equal-sized groups. For unequal-sized groups, it is recommended to
use a different objective when striving for overall between-group similarity
(e.g., the k-plus objective). In previous versions of this
package, <code>objective = "distance"</code> was used (and is still
supported) to refer to the diversity objective, but now <code>objective =
"diversity"</code> is preferred because there are several clustering
objectives based on pairwise distances (e.g., see
<code><a href="#topic+dispersion_objective">dispersion_objective</a></code>). In the publication that introduces
the <code>anticlust</code> package (Papenberg &amp; Klau, 2021), we used the term &quot;anticluster 
editing&quot; to refer to the maximization of the diversity, because the reversed 
procedure - minimizing the diversity - is also known as &quot;cluster editing&quot;. 
</p>
<p>The &quot;dispersion&quot; is the minimum distance between any two elements
that are part of the same cluster; maximization of this objective
ensures that any two elements within the same group are as
dissimilar from each other as possible. Applications that require
high within-group heterogeneity often require to maximize the
dispersion. Oftentimes, it is useful to also consider the diversity
and not only the dispersion; to optimize both objectives at the
same time, see the function
<code><a href="#topic+bicriterion_anticlustering">bicriterion_anticlustering</a></code>.
</p>
<p>If the data input <code>x</code> is a feature matrix (that is: each row
is a &quot;case&quot; and each column is a &quot;variable&quot;) and the option
<code>objective = "diversity"</code> is used, the Euclidean distance is
computed as the basic unit of the diversity and dispersion objectives. If
a different measure of dissimilarity is preferred, you may pass a
self-generated dissimiliarity matrix via the argument <code>x</code>.
</p>
<p>In the standard case, groups of equal size are generated. Adjust
the argument <code>K</code> to create groups of different size (see
Examples).
</p>
<p><strong>Algorithms for anticlustering</strong>
</p>
<p>By default, a heuristic method is employed for anticlustering: the
exchange method (<code>method = "exchange"</code>). First, elements are
randomly assigned to anticlusters (It is also possible to
explicitly specify the initial assignment using the argument
<code>K</code>; in this case, <code>K</code> has length <code>nrow(x)</code>.) Based
on the initial assignment, elements are systematically swapped
between anticlusters in such a way that each swap improves the
objective value. For an element, each possible swap with elements
in other clusters is simulated; then, the one swap is performed
that improves the objective the most, but a swap is only conducted
if there is an improvement at all. This swapping procedure is
repeated for each element. When using <code>method =
"local-maximum"</code>, the exchange method does not terminate after the
first iteration over all elements; instead, the swapping continues
until a local maximum is reached. This means that after the
exchange process has been conducted once for each data point, the
algorithm restarts with the first element and proceeds to conduct
exchanges until the objective cannot be improved.
</p>
<p>When setting <code>preclustering = TRUE</code>, only the <code>K - 1</code>
most similar elements serve as exchange partners for each element,
which can speed up the optimization (more information
on the preclustering heuristic follows below). If the <code>categories</code> argument
is used, only elements having the same value in <code>categories</code> serve as exchange
partners.
</p>
<p>Using <code>method = "brusco"</code> implements the local bicriterion
iterated local search (BILS) heuristic by Brusco et al. (2020) and
returns the partition that best optimized either the diversity or
the dispersion during the optimization process. The function
<code><a href="#topic+bicriterion_anticlustering">bicriterion_anticlustering</a></code> can also be used to run
the algorithm by Brusco et al., but it returns multiple partitions
that approximate the optimal pareto efficient set according to both
objectives (diversity and dispersion). Thus, to fully utilize the
BILS algorithm, use the function
<code><a href="#topic+bicriterion_anticlustering">bicriterion_anticlustering</a></code>.
</p>
<p><strong>Optimal anticlustering</strong>
</p>
<p>Usually, heuristics are employed to tackle anticlustering problems,
and their performance is generally very satisfying.  However,
heuristics do not investigate all possible group assignments and
therefore do not (necessarily) find the
&quot;globally optimal solution&quot;, i.e., a partitioning that has the best
possible value with regard to the objective that is optimized.  Enumerating
all possible partitions in order to find the best solution,
however, quickly becomes impossible with increasing N, and
therefore it is not possible to find a global optimum this
way. Because all anticlustering problems considered here are also
NP-hard, there is also no (known) clever algorithm that might
identify the best solution without considering all possibilities -
at least in the worst case. Integer linear programming (ILP) is an
approach for tackling NP hard problems that nevertheless tries to
be clever when finding optimal solutions: It does not necessarily
enumerate all possibilities but is still guaranteed to return the
optimal solution. Still, for NP hard problems such as
anticlustering, ILP methods will also fail at some point (i.e.,
when N increases).
</p>
<p>For the objectives <code>diversity</code> and <code>dispersion</code>,
<code>anticlust</code> implements optimal solution algorithms via integer
linear programming. In order to use the ILP methods, set
<code>method = "ilp"</code>. The integer linear program optimizing the
diversity was described in Papenberg &amp; Klau, (2021; (8) -
(12)). The documentation of the function
<code><a href="#topic+optimal_dispersion">optimal_dispersion</a></code> has more information on the
optimal maximization of the dispersion (this is the function that is called internally by
anticlustering() when using <code>objective = "dispersion"</code> and
<code>method = "ilp"</code>). The ILP methods either require the R
package <code>Rglpk</code> and the GNU linear programming kit
(&lt;http://www.gnu.org/software/glpk/&gt;), or the R package
<code>Rsymphony</code> and the COIN-OR SYMPHONY solver libraries
(&lt;https://github.com/coin-or/SYMPHONY&gt;). The function will try to
find the GLPK or SYMPHONY solver and throw an error if none is
available (it prioritizes using SYMPHONY if both are available).
</p>
<p>Optimally maximizing the diversity only works for rather small N
and K; N = 20 and K = 2 is usually solved within some seconds, but
the run time quickly increases with increasing N (or K). The
maximum dispersion problem can be solved for much larger instances,
especially for K = 2 (which in theory is not even NP hard; note
that for the diversity, K = 2 is already NP hard). For K = 3, and K
= 4, several 100 elements can usually be processed, especially when
installing the SYMPHONY solver.
</p>
<p><strong>Preclustering</strong>
</p>
<p>A useful heuristic for anticlustering is to form small groups of
very similar elements and assign these to different groups. This
logic is used as a preprocessing when setting <code>preclustering =
TRUE</code>. That is, before the anticlustering objective is optimized, a
cluster analysis identifies small groups of similar elements (pairs
if <code>K = 2</code>, triplets if <code>K = 3</code>, and so forth). The
optimization of the anticlustering objective is then conducted
under the constraint that these matched elements cannot be assigned
to the same group. When using the exchange algorithm, preclustering
is conducted using a call to <code><a href="#topic+matching">matching</a></code>. When using
<code>method = "ilp"</code>, the preclustering optimally finds groups of
minimum pairwise distance by solving the integer linear program
described in Papenberg and Klau (2021; (8) - (10), (12) - (13)).
Note that when combining preclustering restrictions with <code>method = "ilp"</code>,
the anticlustering result is no longer guaranteed to be globally optimal, but
only optimal given the preclustering restrictions.
</p>
<p><strong>Categorical variables</strong>
</p>
<p>The argument <code>categories</code> may induce categorical constraints,
i.e., can be used to distribute categorical variables evenly
between sets.  The grouping variables indicated by
<code>categories</code> will be balanced out across anticlusters. This
functionality is only available for the classical exchange
procedures, that is, for <code>method = "exchange"</code> and
<code>method = "local-maximum"</code>. When <code>categories</code> has multiple columns 
(i.e., there are multiple categorical variables), each combination of categories is
treated as a distinct category by the exchange method (i.e., the multiple columns
are &quot;merged&quot; into a single column). This behaviour may lead
to less than optimal results on the level of each single categorical variable.
</p>
<p><strong>Optimize a custom objective function</strong>
</p>
<p>It is possible to pass a <code>function</code> to the argument
<code>objective</code>. See <code><a href="#topic+dispersion_objective">dispersion_objective</a></code> for an
example. If <code>objective</code> is a function, the exchange method
assigns elements to anticlusters in such a way that the return
value of the custom function is maximized (hence, the function
should return larger values when the between-group similarity is
higher). The custom function has to take two arguments: the first
is the data argument, the second is the clustering assignment. That
is, the argument <code>x</code> will be passed down to the user-defined
function as first argument. <strong>However, only after</strong>
<code><a href="base.html#topic+as.matrix">as.matrix</a></code> has been called on <code>x</code>. This implies
that in the function body, columns of the data set cannot be
accessed using <code>data.frame</code> operations such as
<code>$</code>. Objects of class <code>dist</code> will be converted to matrix
as well.
</p>


<h3>Value</h3>

<p>A vector of length N that assigns a group (i.e, a number
between 1 and <code>K</code>) to each input element.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Brusco, M. J., Cradit, J. D., &amp; Steinley, D. (2020). Combining
diversity and dispersion criteria for anticlustering: A bicriterion
approach. British Journal of Mathematical and Statistical
Psychology, 73, 275-396. https://doi.org/10.1111/bmsp.12186
</p>
<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>
<p>Papenberg, M. (2023). K-plus Anticlustering: An Improved k-means Criterion 
for Maximizing Between-Group Similarity. British Journal of Mathematical 
and Statistical Psychology. Advance online publication. 
https://doi.org/10.1111/bmsp.12315
</p>
<p>Späth, H. (1986). Anticlustering: Maximizing the variance criterion.
Control and Cybernetics, 15, 213-218.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Optimize the default diversity criterion
anticlusters &lt;- anticlustering(
  schaper2019[, 3:6],
  K = 3,
  categories = schaper2019$room
)
# Compare feature means by anticluster
by(schaper2019[, 3:6], anticlusters, function(x) round(colMeans(x), 2))
# Compare standard deviations by anticluster
by(schaper2019[, 3:6], anticlusters, function(x) round(apply(x, 2, sd), 2))
# check that the "room" is balanced across anticlusters:
table(anticlusters, schaper2019$room)

# Use multiple starts of the algorithm to improve the objective and
# optimize the k-means criterion ("variance")
anticlusters &lt;- anticlustering(
  schaper2019[, 3:6],
  objective = "variance",
  K = 3,
  categories = schaper2019$room,
  method = "local-maximum",
  repetitions = 2
)
# Compare means and standard deviations by anticluster
by(schaper2019[, 3:6], anticlusters, function(x) round(colMeans(x), 2))
by(schaper2019[, 3:6], anticlusters, function(x) round(apply(x, 2, sd), 2))

# Use different group sizes and optimize the extended k-means
# criterion ("kplus")
anticlusters &lt;- anticlustering(
  schaper2019[, 3:6],
  objective = "kplus",
  K = c(24, 24, 48),
  categories = schaper2019$room,
  repetitions = 10,
  method = "local-maximum",
  standardize = TRUE
)

table(anticlusters, schaper2019$room)
# Compare means and standard deviations by anticluster
by(schaper2019[, 3:6], anticlusters, function(x) round(colMeans(x), 2))
by(schaper2019[, 3:6], anticlusters, function(x) round(apply(x, 2, sd), 2))


</code></pre>

<hr>
<h2 id='balanced_clustering'>Create balanced clusters of equal size</h2><span id='topic+balanced_clustering'></span>

<h3>Description</h3>

<p>Create balanced clusters of equal size
</p>


<h3>Usage</h3>

<pre><code class='language-R'>balanced_clustering(x, K, method = "centroid")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="balanced_clustering_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A feature
matrix where rows correspond to elements and columns correspond
to variables (a single numeric variable can be passed as a
vector). (2) An N x N matrix dissimilarity matrix; can be an
object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="balanced_clustering_+3A_k">K</code></td>
<td>
<p>How many clusters should be created.</p>
</td></tr>
<tr><td><code id="balanced_clustering_+3A_method">method</code></td>
<td>
<p>One of &quot;centroid&quot; or &quot;ilp&quot;. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function partitions a set of elements into <code>K</code>
equal-sized clusters. The function offers two methods: a heuristic
and an exact method. The heuristic (<code>method = "centroid"</code>)
first computes the centroid of all data points. If the input is a
feature matrix, the centroid is defined as the mean vector of all
columns. If the input is a dissimilarity matrix, the most central
element acts as the centroid; the most central element is defined
as the element having the minimum maximal distance to all other
elements. After identifying the centroid, the algorithm proceeds as
follows: The element having the highest distance from the centroid
is clustered with its <code>(N/K) - 1</code> nearest neighbours
(neighbourhood is defined according to the Euclidean distance if
the data input is a feature matrix). From the remaining elements,
again the element farthest to the centroid is selected and
clustered with its <code>(N/K) - 1</code> neighbours; the procedure is
repeated until all elements are part of a cluster.
</p>
<p>An exact method (<code>method = "ilp"</code>) can be used to solve
equal-sized weighted cluster editing optimally (implements the
integer linear program described in Papenberg and Klau, 2020; 
(8) - (10), (12) - (13)). The cluster editing objective is the 
sum of pairwise distances
within clusters; clustering is accomplished by minimizing this
objective. If the argument <code>x</code> is a features matrix, the
Euclidean distance is computed as the basic unit of the cluster
editing objective. If another distance measure is preferred, users
may pass a self-computed dissimiliarity matrix via the argument
<code>x</code>. The optimal cluster editing objective can be found via
integer linear programming. To obtain an optimal solution, the open
source GNU linear programming kit (available from
https://www.gnu.org/software/glpk/glpk.html) and the R package
<code>Rglpk</code> must be installed.
</p>


<h3>Value</h3>

<p>An integer vector representing the cluster affiliation of 
each data point
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>
<p>Meik Michalke <a href="mailto:meik.michalke@hhu.de">meik.michalke@hhu.de</a>
</p>


<h3>Source</h3>

<p>The centroid method was originally developed and contributed by
Meik Michalke. It was later rewritten by Martin Papenberg, who
also implemented the integer linear programming method.
</p>


<h3>References</h3>

<p>Grötschel, M., &amp; Wakabayashi, Y. (1989). A cutting plane algorithm
for a clustering problem. Mathematical Programming, 45, 59–96.
</p>
<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Cluster a data set and visualize results
N &lt;- 1000
lds &lt;- data.frame(f1 = rnorm(N), f2 = rnorm(N))
cl &lt;- balanced_clustering(lds, K = 10)
plot_clusters(lds, clusters = cl)

# Repeat using a distance matrix as input
cl2 &lt;- balanced_clustering(dist(lds), K = 10)
plot_clusters(lds, clusters = cl2)

</code></pre>

<hr>
<h2 id='bicriterion_anticlustering'>Bicriterion iterated local search heuristic</h2><span id='topic+bicriterion_anticlustering'></span>

<h3>Description</h3>

<p>This function implements the bicriterion for anticlustering by Brusco, 
Cradit, and Steinley (2020; &lt;doi:10.1111/bmsp.12186&gt;). The description of 
the algorithm is given in Section 3 of their paper (in particular, see the 
pseudocode in their Figure 2).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bicriterion_anticlustering(
  x,
  K,
  R = NULL,
  W = c(1e-06, 1e-05, 1e-04, 0.001, 0.01, 0.1, 0.5, 0.99, 0.999, 0.999999),
  Xi = c(0.05, 0.1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bicriterion_anticlustering_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A
feature matrix where rows correspond to elements and columns
correspond to variables (a single numeric variable can be
passed as a vector). (2) An N x N matrix dissimilarity matrix;
can be an object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_k">K</code></td>
<td>
<p>How many anticlusters should be created. Alternatively:
(a) A vector describing the size of each group, or (b) a vector
of length <code>nrow(x)</code> describing how elements are assigned
to anticlusters before the optimization starts.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_r">R</code></td>
<td>
<p>The desired number of restarts for the algorithm. By
default, both phases (MBPI + BILS) of the algorithm are performed once.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_w">W</code></td>
<td>
<p>Optional argument, a vector of weights defining the
relative importance of dispersion and diversity (0 &lt;= W &lt;=
1). See details.</p>
</td></tr>
<tr><td><code id="bicriterion_anticlustering_+3A_xi">Xi</code></td>
<td>
<p>Optional argument, specifies probability of swapping
elements during the iterated local search. See examples.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The bicriterion algorithm by Brusco, Cradit, and Steinley (2020)
aims to simultaneously optimize two anticlustering criteria:
the <code><a href="#topic+diversity_objective">diversity_objective</a></code> and the
<code><a href="#topic+dispersion_objective">dispersion_objective</a></code>. It returns a list of partitions
that approximate the pareto set of efficient solutions across both
criteria. By considering both the diversity and dispersion, this
algorithm is well-suited for maximizing overall within-group
heterogeneity. To select a partition among the approximated pareto
set, it is reasonable to plot the objectives for each partition
(see Examples).
</p>
<p>The arguments <code>R</code>, <code>W</code> and <code>Xi</code> are named for
consistency with Brusco et al. (2020). The argument <code>K</code> is
used for consistency with other functions in anticlust; Brusco et
al. used 'G' to denote the number of groups. However, note that
<code>K</code> can not only be used to denote the number of equal-sized
groups, but also to specify group sizes, as in
<code><a href="#topic+anticlustering">anticlustering</a></code>.
</p>
<p>This function implements the combined bicriterion algorithm MBPI + BILS.
The argument <code>R</code> denotes the number of restarts of the search
heuristic. Half of the repetitions perform MBPI and the other half perform 
BILS, as suggested by Brusco et al. The argument <code>W</code> denotes the possible
weights given
to the diversity criterion in a given run of the search
heuristic. In each run, the a weight is randomly selected from the
vector <code>W</code>. As default values, we use the weights that Brusco
et al. used in their analyses. All values in <code>w</code> have to be in
[0, 1]; larger values indicate that diversity is more important,
whereas smaller values indicate that dispersion is more important;
<code>w = .5</code> implies the same weight for both criteria. The
argument <code>Xi</code> is the probability that an element is swapped
during the iterated local search (specifically, Xi has to be a
vector of length 2, denoting the range of a uniform distribution
from which the probability of swapping is selected). For <code>Xi</code>, the default
is selected consistent with the analyses by Brusco et al. 
</p>
<p>If the data input <code>x</code> is a feature matrix (that is: each row
is a &quot;case&quot; and each column is a &quot;variable&quot;), a matrix of the
Euclidean distances is computed as input to the algorithm. If a
different measure of dissimilarity is preferred, you may pass a
self-generated dissimilarity matrix via the argument <code>x</code>.
</p>


<h3>Value</h3>

<p>A <code>matrix</code> of anticlustering partitions (i.e., the
approximated pareto set). Each row corresponds to a partition,
each column corresponds to an input element.
</p>


<h3>Note</h3>

<p>For technical reasons, the pareto set returned by this function has
a limit of 500 partitions. Usually however, the
algorithm usually finds much fewer partitions. There is one following exception:
We do not recommend to use this method when the input data is
one-dimensional where the algorithm may identify too many
equivalent partitions causing it to run very slowly (see section 5.6 in 
Breuer, 2020).
</p>


<h3>Author(s)</h3>

<p>Martin Breuer <a href="mailto:M.Breuer@hhu.de">M.Breuer@hhu.de</a>, Martin Papenberg
<a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Brusco, M. J., Cradit, J. D., &amp; Steinley, D. (2020). Combining
diversity and dispersion criteria for anticlustering: A bicriterion
approach. British Journal of Mathematical and Statistical
Psychology, 73, 275-396. https://doi.org/10.1111/bmsp.12186
</p>
<p>Breuer (2020). Using anticlustering to maximize diversity and dispersion:
Comparing exact and heuristic approaches. Bachelor thesis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate some random data
M &lt;- 3
N &lt;- 80
K &lt;- 4
data &lt;- matrix(rnorm(N * M), ncol = M)

# Perform bicriterion algorithm, use 200 repetitions
pareto_set &lt;- bicriterion_anticlustering(data, K = K, R = 200)

# Compute objectives for all solutions
diversities_pareto &lt;- apply(pareto_set, 1, diversity_objective, x = data)
dispersions_pareto &lt;- apply(pareto_set, 1, dispersion_objective, x = data)

# Plot the pareto set
plot(
  diversities_pareto,
  dispersions_pareto,
  col = "blue",
  cex = 2,
  pch = as.character(1:NROW(pareto_set))
)

# Get some random solutions for comparison
rnd_solutions &lt;- t(replicate(n = 200, sample(pareto_set[1, ])))

# Compute objectives for all random solutions
diversities_rnd &lt;- apply(rnd_solutions, 1, diversity_objective, x = data)
dispersions_rnd &lt;- apply(rnd_solutions, 1, dispersion_objective, x = data)

# Plot random solutions and pareto set. Random solutions are far away 
# from the good solutions in pareto set
plot(
  diversities_rnd, dispersions_rnd, 
  col = "red",
  xlab = "Diversity",
  ylab = "Dispersion",
  ylim = c(
    min(dispersions_rnd, dispersions_pareto), 
    max(dispersions_rnd, dispersions_pareto)
  ),
  xlim = c(
    min(diversities_rnd, diversities_pareto), 
    max(diversities_rnd, diversities_pareto)
  )
)

# Add approximated pareto set from bicriterion algorithm:
points(diversities_pareto, dispersions_pareto, col = "blue", cex = 2, pch = 19)

</code></pre>

<hr>
<h2 id='categorical_sampling'>Random sampling employing a categorical constraint</h2><span id='topic+categorical_sampling'></span>

<h3>Description</h3>

<p>This function can be used to obtain a stratified split of a data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>categorical_sampling(categories, K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="categorical_sampling_+3A_categories">categories</code></td>
<td>
<p>A matrix or vector of one or more categorical variables.</p>
</td></tr>
<tr><td><code id="categorical_sampling_+3A_k">K</code></td>
<td>
<p>The number of groups that are returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to obtain a stratified split of a data set. 
Using this function is like calling <code><a href="#topic+anticlustering">anticlustering</a></code> with 
argument 'categories', but without optimizing a clustering objective. The
categories are just evenly split between samples. Apart from the restriction 
that categories are balanced between samples, the split is random.
</p>


<h3>Value</h3>

<p>A vector representing the sample each element was assigned to.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(schaper2019)
categories &lt;- schaper2019$room
groups &lt;- categorical_sampling(categories, K = 6)
table(groups, categories)

# Unequal sized groups
groups &lt;- categorical_sampling(categories, K = c(24, 24, 48))
table(groups, categories)

# Heavily unequal sized groups, is harder to balance the groups
groups &lt;- categorical_sampling(categories, K = c(51, 19, 26))
table(groups, categories)

</code></pre>

<hr>
<h2 id='categories_to_binary'>Get binary representation of categorical variables</h2><span id='topic+categories_to_binary'></span>

<h3>Description</h3>

<p>Get binary representation of categorical variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>categories_to_binary(categories, use_combinations = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="categories_to_binary_+3A_categories">categories</code></td>
<td>
<p>A vector, data.frame or matrix representing one
or several categorical variables</p>
</td></tr>
<tr><td><code id="categories_to_binary_+3A_use_combinations">use_combinations</code></td>
<td>
<p>Logical, should the output also include columns representing
the combination / interaction of the categories (defaults to <code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The conversion of categorical variable to binary variables is done via
<code><a href="stats.html#topic+model.matrix">model.matrix</a></code>. This function can be used to include 
categorical variables as part of the optimization criterion in k-means / 
k-plus anticlustering, rather than including them as hard constraints as 
done in <code><a href="#topic+anticlustering">anticlustering</a></code>. This can be useful when there are several
categorical variables or when the group sizes are unequal (or both).
See examples.
</p>


<h3>Value</h3>

<p>A matrix representing the categorical variables in binary form (&quot;dummy coding&quot;)
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Papenberg, M. (2023). K-plus Anticlustering: An Improved k-means Criterion 
for Maximizing Between-Group Similarity. British Journal of Mathematical 
and Statistical Psychology. Advance online publication. 
https://doi.org/10.1111/bmsp.12315
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Use Schaper data set for example
data(schaper2019)
features &lt;- schaper2019[, 3:6]
K &lt;- 3
N &lt;- nrow(features) 

# - Generate data input for k-means anticlustering -
# We conduct k-plus anticlustering by first generating k-plus variables, 
# and also include the categorical variable as "numeric" input for the 
# k-means optimization (rather than as input for the argument `categories`)

input_data &lt;- cbind(
  kplus_moment_variables(features, T = 2), 
  categories_to_binary(schaper2019$room) 
)

kplus_groups &lt;- anticlustering(
  input_data, 
  K = K,
  objective = "variance",
  method = "local-maximum", 
  repetitions = 10
)
mean_sd_tab(features, kplus_groups)
table(kplus_groups, schaper2019$room) # argument categories was not used!

 
</code></pre>

<hr>
<h2 id='dispersion_objective'>Cluster dispersion</h2><span id='topic+dispersion_objective'></span>

<h3>Description</h3>

<p>Compute the dispersion objective for a given clustering (i.e., the
minimum distance between two elements within the same cluster).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dispersion_objective(x, clusters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dispersion_objective_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A
feature matrix where rows correspond to elements and columns
correspond to variables (a single numeric variable can be
passed as a vector). (2) An N x N matrix dissimilarity matrix;
can be an object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="dispersion_objective_+3A_clusters">clusters</code></td>
<td>
<p>A vector representing (anti)clusters (e.g.,
returned by <code><a href="#topic+anticlustering">anticlustering</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dispersion is the minimum distance between two elements within
the same cluster. When the input <code>x</code> is a feature matrix, the
Euclidean distance is used as the distance unit. Maximizing the
dispersion maximizes the minimum heterogeneity within clusters and
is an anticlustering task.
</p>


<h3>References</h3>

<p>Brusco, M. J., Cradit, J. D., &amp; Steinley, D. (2020). Combining
diversity and dispersion criteria for anticlustering: A bicriterion
approach. British Journal of Mathematical and Statistical
Psychology, 73, 275-396. https://doi.org/10.1111/bmsp.12186
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N &lt;- 50 # number of elements
M &lt;- 2  # number of variables per element
K &lt;- 5  # number of clusters
random_data &lt;- matrix(rnorm(N * M), ncol = M)
random_clusters &lt;- sample(rep_len(1:K, N))
dispersion_objective(random_data, random_clusters)

# Maximize the dispersion 
optimized_clusters &lt;- anticlustering(
  random_data,
  K = random_clusters, 
  objective = dispersion_objective
)
dispersion_objective(random_data, optimized_clusters)

</code></pre>

<hr>
<h2 id='diversity_objective'>(Anti)cluster editing &quot;diversity&quot; objective</h2><span id='topic+diversity_objective'></span>

<h3>Description</h3>

<p>Compute the diversity for a given clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diversity_objective(x, clusters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diversity_objective_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A data matrix
where rows correspond to elements and columns correspond to
features (a single numeric feature can be passed as a vector). (2)
An N x N matrix dissimilarity matrix; can be an object of class
<code>dist</code> (e.g., returned by <code><a href="stats.html#topic+dist">dist</a></code> or
<code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code> where the entries of
the upper and lower triangular matrix represent the pairwise
dissimilarities.</p>
</td></tr>
<tr><td><code id="diversity_objective_+3A_clusters">clusters</code></td>
<td>
<p>A vector representing (anti)clusters (e.g.,
returned by <code><a href="#topic+anticlustering">anticlustering</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The objective function used in (anti)cluster editing is the
diversity, i.e., the sum of the pairwise distances between elements
within the same groups. When the input <code>x</code> is a feature
matrix, the Euclidean distance is computed as the basic distance
unit of this objective.
</p>


<h3>Value</h3>

<p>The cluster editing objective
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Brusco, M. J., Cradit, J. D., &amp; Steinley, D. (2020). Combining
diversity and dispersion criteria for anticlustering: A bicriterion
approach. British Journal of Mathematical and Statistical
Psychology, 73, 275-396. https://doi.org/10.1111/bmsp.12186
</p>
<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
distances &lt;- dist(iris[1:60, -5])
## Clustering
clusters &lt;- balanced_clustering(distances, K = 3)
# This is low:
diversity_objective(distances, clusters)
## Anticlustering
anticlusters &lt;- anticlustering(distances, K = 3)
# This is higher:
diversity_objective(distances, anticlusters)

</code></pre>

<hr>
<h2 id='fast_anticlustering'>Fast anticlustering</h2><span id='topic+fast_anticlustering'></span>

<h3>Description</h3>

<p>Anticlustering via optimizing the k-means variance criterion with an
adjusted exchange method where the number of exchange partners can be 
specified. Note that this function is no longer the fastest way to solve
anticlustering, because the exchange method used in <code><a href="#topic+anticlustering">anticlustering</a></code>
and <code><a href="#topic+kplus_anticlustering">kplus_anticlustering</a></code> has been reimplemented in C,
while <code>fast_anticlustering</code> still uses a plain R implementation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_anticlustering(x, K, k_neighbours = Inf, categories = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fast_anticlustering_+3A_x">x</code></td>
<td>
<p>A numeric vector, matrix or data.frame of data
points.  Rows correspond to elements and columns correspond to
features. A vector represents a single numeric feature.</p>
</td></tr>
<tr><td><code id="fast_anticlustering_+3A_k">K</code></td>
<td>
<p>How many anticlusters should be created.</p>
</td></tr>
<tr><td><code id="fast_anticlustering_+3A_k_neighbours">k_neighbours</code></td>
<td>
<p>The number of neighbours that serve as exchange
partner for each element. Defaults to Inf, i.e., each element
is exchanged with each element in other groups.</p>
</td></tr>
<tr><td><code id="fast_anticlustering_+3A_categories">categories</code></td>
<td>
<p>A vector, data.frame or matrix representing one or
several categorical constraints.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function was created to make anticlustering applicable
to large data sets (e.g., 100,000 elements). It optimizes the k-means
variance objective because computing all pairwise as is done when optimizing 
the diversity is not feasible for very large data sets (like for about N &gt; 30000). 
Additionally, this function employs a
speed-optimized exchange method. For each element, the potential
exchange partners are generated using a nearest neighbor search with the
function <code><a href="RANN.html#topic+nn2">nn2</a></code> from the <code>RANN</code> package. The nearest
neighbors then serve as exchange partners. This approach is inspired by the
preclustering heuristic according to which good solutions are found
when similar elements are in different sets&mdash;by swapping nearest
neighbors, this will often be the case. The number of exchange partners
per element has to be set using the argument <code>k_neighbours</code>; by
default, it is set to <code>Inf</code>, meaning that all possible swaps are
tested. This default must be changed by the user for large data sets.
More exchange partners generally improve the output, but also increase
run time.
</p>
<p>When setting the <code>categories</code> argument, exchange partners will
be generated from the same category. Note that when
<code>categories</code> has multiple columns (i.e., each element is
assigned to multiple columns), each combination of categories is
treated as a distinct category by the exchange method.
</p>
<p>Note that in the recent versions of anticlust, the function <code><a href="#topic+anticlustering">anticlustering</a></code>
is actually faster than <code>fast_anticlustering</code> because the exchange method
there has been implemented in C instead of plain R. In most cases it is therefore
not recommended to call <code>fast_anticlustering</code>, instead use <code><a href="#topic+anticlustering">anticlustering</a></code>
or <code><a href="#topic+kplus_anticlustering">kplus_anticlustering</a></code>.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anticlustering">anticlustering</a></code>
</p>
<p><code><a href="#topic+variance_objective">variance_objective</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

features &lt;- iris[, - 5]

start &lt;- Sys.time()
ac_exchange &lt;- fast_anticlustering(features, K = 3)
Sys.time() - start

## The following call is equivalent to the call above:
start &lt;- Sys.time()
ac_exchange &lt;- anticlustering(features, K = 3, objective = "variance")
Sys.time() - start

## Improve run time by using fewer exchange partners:
start &lt;- Sys.time()
ac_fast &lt;- fast_anticlustering(features, K = 3, k_neighbours = 10)
Sys.time() - start

by(features, ac_exchange, function(x) round(colMeans(x), 2))
by(features, ac_fast, function(x) round(colMeans(x), 2))

</code></pre>

<hr>
<h2 id='generate_partitions'>Generate all partitions of same cardinality</h2><span id='topic+generate_partitions'></span>

<h3>Description</h3>

<p>Generate all partitions of same cardinality
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_partitions(N, K, generate_permutations = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_partitions_+3A_n">N</code></td>
<td>
<p>The total N. <code>K</code> has to be dividble
by <code>N</code>.</p>
</td></tr>
<tr><td><code id="generate_partitions_+3A_k">K</code></td>
<td>
<p>How many partitions</p>
</td></tr>
<tr><td><code id="generate_partitions_+3A_generate_permutations">generate_permutations</code></td>
<td>
<p>If TRUE, all permutations are returned,
resulting in duplicate partitions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In principle, anticlustering can be solved to optimality by
generating all possible partitions of N items into K groups.
The example code below illustrates how to do this.
However, this approach only works for small N because the
number of partitions grows exponentially with N.
</p>
<p>The partition c(1, 2, 2, 1)
is the same as the partition c(2, 1, 1, 2) but they correspond
to different permutations of the elements [1, 1, 2, 2]. If the argument
<code>generate_permutations</code> is <code>TRUE</code>, all permutations are
returned. To solve balanced anticlustering exactly, it is sufficient
to inspect all partitions while ignoring duplicated permutations.
</p>


<h3>Value</h3>

<p>A list of all partitions (or permutations if
<code>generate_permutations</code> is <code>TRUE</code>).
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Generate all partitions to solve k-means anticlustering
## to optimality.

N &lt;- 14
K &lt;- 2
features &lt;- matrix(sample(N * 2, replace = TRUE), ncol = 2)
partitions &lt;- generate_partitions(N, K)
length(partitions) # number of possible partitions

## Create an objective function that takes the partition
## as first argument (then, we can use sapply to compute
## the objective for each partition)
var_obj &lt;- function(clusters, features) {
  variance_objective(features, clusters)
}

all_objectives &lt;- sapply(
  partitions,
  FUN = var_obj,
  features = features
)

## Check out distribution of the objective over all partitions:
hist(all_objectives) # many large, few low objectives
## Get best k-means anticlustering objective:
best_obj &lt;- max(all_objectives)
## It is possible that there are multiple best solutions:
sum(all_objectives == best_obj)
## Select one best partition:
best_anticlustering &lt;- partitions[all_objectives == best_obj][[1]]
## Look at mean for each partition:
by(features, best_anticlustering, function(x) round(colMeans(x), 2))


## Get best k-means clustering objective:
min_obj &lt;- min(all_objectives)
sum(all_objectives == min_obj)
## Select one best partition:
best_clustering &lt;- partitions[all_objectives == min_obj][[1]]

## Plot minimum and maximum objectives:
user_par &lt;- par("mfrow")
par(mfrow = c(1, 2))
plot_clusters(
  features,
  best_anticlustering,
  illustrate_variance = TRUE,
  main = "Maximum variance"
)
plot_clusters(
  features,
  best_clustering,
  illustrate_variance = TRUE,
  main = "Minimum variance"
)
par(mfrow = user_par)

</code></pre>

<hr>
<h2 id='kplus_anticlustering'>K-plus anticlustering</h2><span id='topic+kplus_anticlustering'></span>

<h3>Description</h3>

<p>Perform anticlustering using the k-plus objective to maximize between-group 
similarity. This function implements the k-plus anticlustering method described 
in Papenberg (2023; &lt;doi:10.1111/bmsp.12315&gt;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kplus_anticlustering(
  x,
  K,
  variance = TRUE,
  skew = FALSE,
  kurtosis = FALSE,
  covariances = FALSE,
  T = NULL,
  standardize = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kplus_anticlustering_+3A_x">x</code></td>
<td>
<p>A feature matrix where rows correspond to elements and columns
correspond to variables (a single numeric variable can be
passed as a vector).</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_k">K</code></td>
<td>
<p>How many anticlusters should be created. Alternatively:
(a) A vector describing the size of each group, or (b) a vector
of length <code>nrow(x)</code> describing how elements are assigned
to anticlusters before the optimization starts.</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_variance">variance</code></td>
<td>
<p>Boolean: Should the k-plus objective include a term to 
maximize between-group similarity with regard to the variance? 
(Default = TRUE)</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_skew">skew</code></td>
<td>
<p>Boolean: Should the k-plus objective include a term to 
maximize between-group similarity with regard to skewness? 
(Default = FALSE)</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_kurtosis">kurtosis</code></td>
<td>
<p>Boolean: Should the k-plus objective include a term to 
maximize between-group similarity with regard to kurtosis? 
(Default = FALSE)</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_covariances">covariances</code></td>
<td>
<p>Boolean: Should the k-plus objective include a term to 
maximize between-group similarity with regard to covariance structure? 
(Default = FALSE)</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_t">T</code></td>
<td>
<p>Optional argument: An integer specifying how many
distribution moments should be equalized between groups.</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_standardize">standardize</code></td>
<td>
<p>Boolean. If <code>TRUE</code>, the data is standardized through 
a call to <code><a href="base.html#topic+scale">scale</a></code> before the optimization starts. 
Defaults to TRUE. See details.</p>
</td></tr>
<tr><td><code id="kplus_anticlustering_+3A_...">...</code></td>
<td>
<p>Arguments passed down to <code><a href="#topic+anticlustering">anticlustering</a></code>. All of the 
arguments are supported except for <code>objective</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the unweighted sum approach for k-plus 
anticlustering. Details are given in Papenberg (2023). 
</p>
<p>The optional argument <code>T</code> denotes the number of distribution 
moments that are considered in the anticlustering process. For example,
<code>T = 4</code> will lead to similar means, variances, skew and kurtosis. 
For the first four moments, it is also possible to use the boolean
convenience arguments <code>variance</code>, <code>skew</code> and <code>kurtosis</code>; the
mean (the first moment) is always included and cannot be &quot;turned off&quot;.
If the argument <code>T</code> is used, it overrides the arguments
<code>variance</code>, <code>skew</code> and <code>kurtosis</code> (corresponding to
the second, third and fourth moment), ignoring their values.
</p>
<p>The <code>standardization</code> is applied to all original features and the 
additional k-plus features that are appended to the data set in order 
to optimize the k-plus criterion. When using standardization, 
all criteria such as means, variances and skewness receive a comparable
weight during the optimization. It is usually recommended not
to change the default setting <code>standardization = TRUE</code>.
</p>
<p>This function can use any arguments that are also possible in 
<code><a href="#topic+anticlustering">anticlustering</a></code>
(except for 'objective' because the objective optimized here
is the k-plus objective; to use a different objective, 
call <code><a href="#topic+anticlustering">anticlustering</a></code> directly). Any arguments that are
not explicitly changed here (i.e., <code>standardize = TRUE</code>) receive the 
default given in <code><a href="#topic+anticlustering">anticlustering</a></code> 
(e.g., <code>method = "exchange"</code>.)
</p>


<h3>References</h3>

<p>Papenberg, M. (2023). K-plus Anticlustering: An Improved k-means Criterion 
for Maximizing Between-Group Similarity. British Journal of Mathematical 
and Statistical Psychology. Advance online publication. 
https://doi.org/10.1111/bmsp.12315
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate some data
N &lt;- 180
M &lt;- 4
features &lt;- matrix(rnorm(N * M), ncol = M)
# standard k-plus anticlustering: optimize similarity with regard to mean and variance:
cl &lt;- kplus_anticlustering(features, K = 3, method = "local-maximum")
mean_sd_tab(features, cl)
# Visualize an anticlustering solution:
plot(features, col = palette()[2:4][cl], pch = c(16:18)[cl])

# Also optimize with regard to skewness and kurtosis
cl2 &lt;- kplus_anticlustering(
  features, 
  K = 3, 
  method = "local-maximum", 
  skew = TRUE, 
  kurtosis = TRUE
)

# The following two calls are equivalent: 
init_clusters &lt;- sample(rep_len(1:3, nrow(features)))
# 1.
x1 &lt;- kplus_anticlustering(
  features, 
  K = init_clusters, 
  variance = TRUE,
  skew = TRUE
)
# 2. 
x2 &lt;- kplus_anticlustering(
  features, 
  K = init_clusters, 
  T = 3
)
# Verify: 
all(x1 == x2)

</code></pre>

<hr>
<h2 id='kplus_moment_variables'>Compute k-plus variables</h2><span id='topic+kplus_moment_variables'></span>

<h3>Description</h3>

<p>Compute k-plus variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kplus_moment_variables(x, T, standardize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kplus_moment_variables_+3A_x">x</code></td>
<td>
<p>A vector, matrix or data.frame of data points. Rows
correspond to elements and columns correspond to features. A
vector represents a single feature.</p>
</td></tr>
<tr><td><code id="kplus_moment_variables_+3A_t">T</code></td>
<td>
<p>The number of distribution moments for which variables are generated.</p>
</td></tr>
<tr><td><code id="kplus_moment_variables_+3A_standardize">standardize</code></td>
<td>
<p>Logical, should all columns of the output be standardized
(defaults to TRUE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The k-plus criterion is an extension of the k-means criterion
(i.e., the &quot;variance&quot;, see <code><a href="#topic+variance_objective">variance_objective</a></code>).
In <code><a href="#topic+kplus_anticlustering">kplus_anticlustering</a></code>, equalizing means and variances
simultaneously (and possibly additional distribution moments) is
accomplished by internally appending new variables to the data
input <code>x</code>. When using only the  variance as additional criterion, the
new variables represent the squared difference of each data point to
the mean of the respective column. All columns are then included&mdash;in
addition to the original data&mdash;in standard k-means
anticlustering. The logic is readily extended towards higher order moments,
see Papenberg (2023). This function gives users the possibility to generate
k-plus variables themselves, which offers some additional flexibility when
conducting k-plus anticlustering.
</p>


<h3>Value</h3>

<p>A matrix containing all columns of <code>x</code> and all additional
columns of k-plus variables. If <code>x</code> has M columns, the output matrix
has M * T columns.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Papenberg, M. (2023). K-plus Anticlustering: An Improved k-means Criterion 
for Maximizing Between-Group Similarity. British Journal of Mathematical 
and Statistical Psychology. Advance online publication. 
https://doi.org/10.1111/bmsp.12315
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Use Schaper data set for example
data(schaper2019)
features &lt;- schaper2019[, 3:6]
K &lt;- 3
N &lt;- nrow(features)

# Some equivalent ways of doing k-plus anticlustering:

init_groups &lt;- sample(rep_len(1:3, N))
table(init_groups)

kplus_groups1 &lt;- anticlustering(
  features,
  K = init_groups,
  objective = "kplus",
  standardize = TRUE,
  method = "local-maximum"
)

kplus_groups2 &lt;- anticlustering(
  kplus_moment_variables(features, T = 2), # standardization included by default
  K = init_groups,
  objective = "variance", # (!)
  method = "local-maximum"
)

# this function uses standardization by default unlike anticlustering():
kplus_groups3 &lt;- kplus_anticlustering(
  features, 
  K = init_groups,
  method = "local-maximum"
)

all(kplus_groups1 == kplus_groups2)
all(kplus_groups1 == kplus_groups3)
all(kplus_groups2 == kplus_groups3)

</code></pre>

<hr>
<h2 id='matching'>Matching</h2><span id='topic+matching'></span>

<h3>Description</h3>

<p>Conduct K-partite or unrestricted (minimum distance) matching to
find pairs or groups of similar elements. By default, finding
matches is based on the Euclidean distance between data points, but
a custom dissimilarity measure can also be employed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matching(
  x,
  p = 2,
  match_between = NULL,
  match_within = NULL,
  match_extreme_first = TRUE,
  target_group = NULL,
  sort_output = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matching_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A feature
matrix where rows correspond to elements and columns correspond
to variables (a single numeric variable can be passed as a
vector). (2) An N x N matrix dissimilarity matrix; can be an
object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="matching_+3A_p">p</code></td>
<td>
<p>The size of the groups; the default is 2, in which case
the function returns pairs.</p>
</td></tr>
<tr><td><code id="matching_+3A_match_between">match_between</code></td>
<td>
<p>An optional vector, <code>data.frame</code> or
matrix representing one or several categorical constraints. If
passed, the argument <code>p</code> is ignored and matches are sought
between elements of different categories.</p>
</td></tr>
<tr><td><code id="matching_+3A_match_within">match_within</code></td>
<td>
<p>An optional vector, <code>data.frame</code> or matrix
representing one or several categorical constraints. If passed,
matches are sought between elements of the same category.</p>
</td></tr>
<tr><td><code id="matching_+3A_match_extreme_first">match_extreme_first</code></td>
<td>
<p>Logical: Determines if matches are first
sought for extreme elements first or for central
elements. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="matching_+3A_target_group">target_group</code></td>
<td>
<p>Currently, the options &quot;none&quot;,
smallest&quot; and &quot;diverse&quot; are supported. See Details.</p>
</td></tr>
<tr><td><code id="matching_+3A_sort_output">sort_output</code></td>
<td>
<p>Boolean. If <code>TRUE</code> (default), the output clusters 
are sorted by similarity. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the data input <code>x</code> is a feature matrix, matching is based
on the Euclidean distance between data points. If the argument
<code>x</code> is a dissimilarity matrix, matching is based on the
user-specified dissimilarities. To find matches, the algorithm
proceeds by selecting a target element and then searching its
nearest neighbours. Critical to the behaviour or the algorithm is
the order in which target elements are selected. By default, the
most extreme elements are selected first, i.e., elements with the
highest distance to the centroid of the data set (see
<code><a href="#topic+balanced_clustering">balanced_clustering</a></code> that relies on the same
algorithm). Set the argument <code>match_extreme_first</code> to
<code>FALSE</code>, to enforce that elements close to the centroid are
first selected as targets.
</p>
<p>If the argument <code>match_between</code> is passed and the groups
specified via this argument are of different size, target elements
are selected from the smallest group by default (because in this
group, all elements can be matched). However, it is also possible
to specify how matches are selected through the option
<code>target_group</code>. When specifying <code>"none"</code>, matches are
always selected from extreme elements, irregardless of the group
sizes (or from central elements first if <code>match_extreme_first
= FALSE</code>). With option <code>"smallest"</code>, matches are selected from
the smallest group. With option <code>"diverse"</code>, matches are
selected from the most heterogenous group according to the sum of
pairwise distances within groups.
</p>
<p>The output is an integer vector encoding which elements have been
matched. The grouping numbers are sorted by similarity. That is,
elements with the grouping number »1« have the highest intra-group
similarity, followed by 2 etc (groups having the same similarity
index are still assigned a different grouping number,
though). Similarity is measured as the sum of pairwise (Euclidean)
distances within groups (see <code><a href="#topic+diversity_objective">diversity_objective</a></code>). To 
prevent sorting by similarity (this is some extra computational burden),
set <code>sort_output = FALSE</code>. Some unmatched elements may be <code>NA</code>. 
This happens if it is not
possible to evenly split the item pool evenly into groups of size
<code>p</code> or if the categories described by the argument
<code>match_between</code> are of different size.
</p>


<h3>Value</h3>

<p>An integer vector encoding the matches. See Details for
more information.
</p>


<h3>Note</h3>

<p>It is possible to specify grouping restrictions via
<code>match_between</code> and <code>match_within</code> at the same time.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Find triplets
N &lt;- 120
lds &lt;- data.frame(f1 = rnorm(N), f2 = rnorm(N))
triplets &lt;- matching(lds, p = 3)
plot_clusters(
  lds,
  clusters = triplets,
  within_connection = TRUE
)

# Bipartite matching with unequal-sized groups:
# Only selects matches for some elements
N &lt;- 100
data &lt;- matrix(rnorm(N), ncol = 1)
groups &lt;- sample(1:2, size = N, replace = TRUE, prob = c(0.8, 0.2))
matched &lt;- matching(data[, 1], match_between = groups)
plot_clusters(
  cbind(groups, data), 
  clusters = matched, 
  within_connection = TRUE
)

# Match objects from the same category only
matched &lt;- matching(
  schaper2019[, 3:6], 
  p = 3, 
  match_within = schaper2019$room
)
head(table(matched, schaper2019$room))

# Match between different plant species in the »iris« data set
species &lt;- iris$Species != "versicolor"
matched &lt;- matching(
  iris[species, 1], 
  match_between = iris[species, 5]
)
# Adjust `match_extreme_first` argument
matched2 &lt;- matching(
  iris[species, 1], 
  match_between = iris[species, 5],
  match_extreme_first = FALSE
)
# Plot the matching results
user_par &lt;- par("mfrow")
par(mfrow = c(1, 2))
data &lt;- data.frame(
  Species = as.numeric(iris[species, 5]),
  Sepal.Length = iris[species, 1]
)
plot_clusters(
  data,
  clusters = matched,
  within_connection = TRUE,
  main = "Extreme elements matched first"
)
plot_clusters(
  data,
  clusters = matched2,
  within_connection = TRUE,
  main = "Central elements matched first"
)
par(mfrow = user_par)


</code></pre>

<hr>
<h2 id='mean_sd_tab'>Means and standard deviations by group variable formatted in table</h2><span id='topic+mean_sd_tab'></span>

<h3>Description</h3>

<p>Means and standard deviations by group variable formatted in table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_sd_tab(features, groups, decimals = 2, na.rm = FALSE, return_diff = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean_sd_tab_+3A_features">features</code></td>
<td>
<p>A data frame of features</p>
</td></tr>
<tr><td><code id="mean_sd_tab_+3A_groups">groups</code></td>
<td>
<p>A grouping vector</p>
</td></tr>
<tr><td><code id="mean_sd_tab_+3A_decimals">decimals</code></td>
<td>
<p>The number of decimals</p>
</td></tr>
<tr><td><code id="mean_sd_tab_+3A_na.rm">na.rm</code></td>
<td>
<p>Should NAs be removed prior to computing stats 
(Default = FALSE)</p>
</td></tr>
<tr><td><code id="mean_sd_tab_+3A_return_diff">return_diff</code></td>
<td>
<p>Boolean. Should an additional row be printed that 
contains the difference between minimum and maximum</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A table that illustrates means and standard deviations (in brackets)
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
mean_sd_tab(iris[, -5], iris[, 5])

</code></pre>

<hr>
<h2 id='n_partitions'>Number of equal sized partitions</h2><span id='topic+n_partitions'></span>

<h3>Description</h3>

<p>Number of equal sized partitions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>n_partitions(N, K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="n_partitions_+3A_n">N</code></td>
<td>
<p>How many elements</p>
</td></tr>
<tr><td><code id="n_partitions_+3A_k">K</code></td>
<td>
<p>How many partitions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The number of partitions
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n_partitions(20, 2)

</code></pre>

<hr>
<h2 id='optimal_dispersion'>Maximize dispersion for K groups</h2><span id='topic+optimal_dispersion'></span>

<h3>Description</h3>

<p>Maximize dispersion for K groups
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal_dispersion(x, K, solver = NULL, max_dispersion_considered = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimal_dispersion_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A
feature matrix where rows correspond to elements and columns
correspond to variables (a single numeric variable can be
passed as a vector). (2) An N x N matrix dissimilarity matrix;
can be an object of class <code>dist</code> (e.g., returned by
<code><a href="stats.html#topic+dist">dist</a></code> or <code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code>
where the entries of the upper and lower triangular matrix
represent pairwise dissimilarities.</p>
</td></tr>
<tr><td><code id="optimal_dispersion_+3A_k">K</code></td>
<td>
<p>The number of groups or a vector describing the size of each group.</p>
</td></tr>
<tr><td><code id="optimal_dispersion_+3A_solver">solver</code></td>
<td>
<p>Optional argument; if passed, has to be either &quot;glpk&quot; or
&quot;symphony&quot;. See details.</p>
</td></tr>
<tr><td><code id="optimal_dispersion_+3A_max_dispersion_considered">max_dispersion_considered</code></td>
<td>
<p>Optional argument used for early stopping. If the dispersion found
is equal to or exceeds this value, a solution having the previous best dispersion 
is returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dispersion is the minimum distance between two elements
within the same group. This function implements an optimal method
to maximize the dispersion. If the data input <code>x</code> is a feature
matrix and not a dissimilarity matrix, the pairwise Euclidean
distance is used. It uses the algorithm presented in Max
Diekhoff's Bachelor thesis at the Computer Science Department at
the Heinrich Heine University Düsseldorf.
</p>
<p>To find out which items are not allowed to be grouped in the same
cluster for maximum dispersion, the algorithm sequentially builds
instances of a graph coloring problem, using an integer linear
programming (ILP) representation (also see Fernandez et al.,
2013).  It is possible to specify the ILP solver via the argument
<code>solver</code>. This function either requires the R package
<code>Rglpk</code> and the GNU linear programming kit
(&lt;http://www.gnu.org/software/glpk/&gt;) or the R package
<code>Rsymphony</code> and the COIN-OR SYMPHONY solver libraries
(&lt;https://github.com/coin-or/SYMPHONY&gt;). If the argument
<code>solver</code> is not specified, the function will try to find the
GLPK or SYMPHONY solver by itself (it prioritizes using SYMPHONY if
both are available). The GNU linear programming kit (<code>solver =
  "glpk"</code>) seems to be considerably slower for K &gt;= 3 than the
SYMPHPONY solver (<code>solver = "symphony"</code>).
</p>
<p>Optimally solving the maximum dispersion problem is NP-hard for K
&gt; 2 and therefore computationally infeasible for larger data
sets. For K = 3 and K = 4, it seems that this approach scales up to several 100 elements, 
or even &gt; 1000 for K = 3 (at least when using the Symphony solver). 
For larger data sets, use the heuristic approaches in <code><a href="#topic+anticlustering">anticlustering</a></code> or
<code><a href="#topic+bicriterion_anticlustering">bicriterion_anticlustering</a></code>. However, note that for K = 2, 
the optimal approach is usually much faster than the heuristics.
</p>
<p>In the output, the element <code>edges</code> defines which elements must be in separate 
clusters in order to achieve maximum dispersion. All elements not listed here
can be changed arbitrarily between clusters without reducing the dispersion.
If the maximum possible dispersion corresponds to the minimum dispersion
in the data set, the output elements <code>edges</code> and <code>groups</code> are set to
<code>NULL</code> because all possible groupings have the same value of dispersion.
In this case the output element <code>dispersions_considered</code> has length 1.
</p>


<h3>Value</h3>

<p>A list with four elements:  
</p>
<table>
<tr><td><code>dispersion</code></td>
<td>
<p>The optimal dispersion</p>
</td></tr>
<tr><td><code>groups</code></td>
<td>
<p>An assignment of elements to groups (vector)</p>
</td></tr>
<tr><td><code>edges</code></td>
<td>
<p>A matrix of 2 columns. Each row contains the indices of 
elements that had to be investigated to find the dispersion (i.e., each pair
of elements cannot be part of the same group in order to achieve maximum 
dispersion).</p>
</td></tr>
<tr><td><code>dispersions_considered</code></td>
<td>
<p>All distances that were tested until the dispersion was found.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If the SYMPHONY solver is used, an unfortunate
&quot;message&quot; is printed to the console when this function terminates: 
</p>
<p>sym_get_col_solution(): No solution has been stored!
</p>
<p>This message is no reason to worry and instead is a direct result
of the algorithm finding the optimal value for the dispersion.
Unfortunately, this message is generated in the C code underlying the 
SYMPHONY library (via the printing function <code>printf</code>), which cannot be
prevented in R.
</p>


<h3>Author(s)</h3>

<p>Max Diekhoff
</p>
<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Diekhoff (2023). Maximizing dispersion for anticlustering. Retrieved from 
https://www.cs.hhu.de/fileadmin/redaktion/Fakultaeten/Mathematisch-Naturwissenschaftliche_Fakultaet/Informatik/Algorithmische_Bioinformatik/Bachelor-_Masterarbeiten/2831963_ba_ifo_AbschlArbeit_klau_mapap102_madie120_20230203_1815.pdf  
</p>
<p>Fernández, E., Kalcsics, J., &amp; Nickel, S. (2013). The maximum dispersion 
problem. Omega, 41(4), 721–730. https://doi.org/10.1016/j.omega.2012.09.005
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dispersion_objective">dispersion_objective</a></code> <code><a href="#topic+anticlustering">anticlustering</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N &lt;- 30
M &lt;- 5
K &lt;- 3
data &lt;- matrix(rnorm(N*M), ncol = M)
distances &lt;- dist(data)

opt &lt;- optimal_dispersion(distances, K = K)
opt

# Compare to bicriterion heuristic:
groups_heuristic &lt;- anticlustering(
  distances, 
  K = K,
  method = "brusco", 
  objective = "dispersion", 
  repetitions = 100
)
c(
  OPT = dispersion_objective(distances, opt$groups),
  HEURISTIC = dispersion_objective(distances, groups_heuristic)
)

# Different group sizes are possible:
table(optimal_dispersion(distances, K = c(15, 10, 5))$groups)

# Induce cannot-link constraints by maximizing the dispersion:
solvable &lt;- matrix(1, ncol = 6, nrow = 6)
solvable[2, 1] &lt;- -1
solvable[3, 1] &lt;- -1
solvable[4, 1] &lt;- -1
solvable &lt;- as.dist(solvable)
solvable

# An optimal solution has to put item 1 in a different group than 
# items 2, 3 and 4 -&gt; this is possible for K = 2
optimal_dispersion(solvable, K = 2)$groups

# It no longer works when item 1 can also not be linked with item 5:
# (check out output!)
unsolvable &lt;- as.matrix(solvable)
unsolvable[5, 1] &lt;- -1
unsolvable &lt;- as.dist(unsolvable)
unsolvable
optimal_dispersion(unsolvable, K = 2)


</code></pre>

<hr>
<h2 id='plot_clusters'>Visualize a cluster analysis</h2><span id='topic+plot_clusters'></span>

<h3>Description</h3>

<p>Visualize a cluster analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_clusters(
  features,
  clusters,
  within_connection = FALSE,
  between_connection = FALSE,
  illustrate_variance = FALSE,
  show_axes = FALSE,
  xlab = NULL,
  ylab = NULL,
  xlim = NULL,
  ylim = NULL,
  main = "",
  cex = 1.2,
  cex.axis = 1.2,
  cex.lab = 1.2,
  lwd = 1.5,
  lty = 2,
  frame.plot = FALSE,
  cex_centroid = 2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_clusters_+3A_features">features</code></td>
<td>
<p>A data.frame or matrix representing the features that
are plotted. Must have two columns.</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_clusters">clusters</code></td>
<td>
<p>A vector representing the clustering</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_within_connection">within_connection</code></td>
<td>
<p>Boolean. Connect the elements within each
clusters through lines? Useful to illustrate a graph structure.</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_between_connection">between_connection</code></td>
<td>
<p>Boolean. Connect the elements between each
clusters through lines? Useful to illustrate a graph structure.
(This argument only works for two clusters).</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_illustrate_variance">illustrate_variance</code></td>
<td>
<p>Boolean. Illustrate the variance criterion
in the plot?</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_show_axes">show_axes</code></td>
<td>
<p>Boolean, display values on the x and y-axis? Defaults
to 'FALSE'.</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_xlab">xlab</code></td>
<td>
<p>The label for the x-axis</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_ylab">ylab</code></td>
<td>
<p>The label for the y-axis</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_xlim">xlim</code></td>
<td>
<p>The limits for the x-axis</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_ylim">ylim</code></td>
<td>
<p>The limits for the y-axis</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_main">main</code></td>
<td>
<p>The title of the plot</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_cex">cex</code></td>
<td>
<p>The size of the plotting symbols, see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_cex.axis">cex.axis</code></td>
<td>
<p>The size of the values on the axes</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_cex.lab">cex.lab</code></td>
<td>
<p>The size of the labels of the axes</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_lwd">lwd</code></td>
<td>
<p>The width of the lines connecting elements.</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_lty">lty</code></td>
<td>
<p>The line type of the lines connecting elements
(see <code><a href="graphics.html#topic+par">par</a></code>).</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_frame.plot">frame.plot</code></td>
<td>
<p>a logical indicating whether a box should be drawn
around the plot.</p>
</td></tr>
<tr><td><code id="plot_clusters_+3A_cex_centroid">cex_centroid</code></td>
<td>
<p>The size of the cluster center symbol (has an
effect only if <code>illustrate_variance</code> is <code>TRUE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In most cases, the argument <code>clusters</code> is a vector
returned by one of the functions <code><a href="#topic+anticlustering">anticlustering</a></code>,
<code><a href="#topic+balanced_clustering">balanced_clustering</a></code> or <code><a href="#topic+matching">matching</a></code>. 
However, the plotting function can also be used to plot the results 
of other cluster functions such as <code><a href="stats.html#topic+kmeans">kmeans</a></code>. This function
is usually just used to get a fast impression of the results of an 
(anti)clustering assignment, but limited in its functionality. 
It is useful for depicting the intra-cluster connections using 
argument <code>within_connection</code>.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N &lt;- 15
features &lt;- matrix(runif(N * 2), ncol = 2)
K &lt;- 3
clusters &lt;- balanced_clustering(features, K = K)
anticlusters &lt;- anticlustering(features, K = K)
user_par &lt;- par("mfrow")
par(mfrow = c(1, 2))
plot_clusters(features, clusters, main = "Cluster editing", within_connection = TRUE)
plot_clusters(features, anticlusters, main = "Anticluster editing", within_connection = TRUE)
par(mfrow = user_par)

</code></pre>

<hr>
<h2 id='plot_similarity'>Plot similarity objective by cluster</h2><span id='topic+plot_similarity'></span>

<h3>Description</h3>

<p>Plot similarity objective by cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_similarity(x, groups)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_similarity_+3A_x">x</code></td>
<td>
<p>The data input. Can be one of two structures: (1) A data matrix
where rows correspond to elements and columns correspond to
features (a single numeric feature can be passed as a vector). (2)
An N x N matrix dissimilarity matrix; can be an object of class
<code>dist</code> (e.g., returned by <code><a href="stats.html#topic+dist">dist</a></code> or
<code><a href="stats.html#topic+as.dist">as.dist</a></code>) or a <code>matrix</code> where the entries of
the upper and lower triangular matrix represent the pairwise
dissimilarities.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_groups">groups</code></td>
<td>
<p>A grouping vector of length N, usually the output
of <code><a href="#topic+matching">matching</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots the sum of pairwise distances by group.
</p>


<h3>Value</h3>

<p>The diversity (sum of distances) by group.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+diversity_objective">diversity_objective</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Match elements and plot similarity by match
N &lt;- 100
lds &lt;- data.frame(f1 = rnorm(N), f2 = rnorm(N))
pairs &lt;- matching(lds, p = 2)
plot_similarity(lds, pairs)

</code></pre>

<hr>
<h2 id='schaper2019'>Ratings for 96 words</h2><span id='topic+schaper2019'></span>

<h3>Description</h3>

<p>A stimulus set that was used in experiments by Schaper, Kuhlmann and
Bayen (2019a; 2019b). The item pool consists of 96 German words. 
Each word represents an object that is either 
typically found in a bathroom or in a kitchen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>schaper2019
</code></pre>


<h3>Format</h3>

<p>A data frame with 96 rows and 7 variables
</p>

<dl>
<dt>item</dt><dd><p>The name of an object (in German)</p>
</dd>
<dt>room</dt><dd><p>The room in which the item is typically found; can be
'kitchen' or 'bathroom'</p>
</dd>
<dt>rating_consistent</dt><dd><p>How expected would it
be to find the <code>item</code> in the typical <code>room</code></p>
</dd>
<dt>rating_inconsistent</dt><dd><p>How expected would it
be to find the <code>item</code> in the atypical <code>room</code></p>
</dd>
<dt>syllables</dt><dd><p>The number of syllables in the object name</p>
</dd>
<dt>frequency</dt><dd><p>A value indicating the relative frequency of the
object name in German language (lower values indicate higher
frequency)</p>
</dd>
<dt>list</dt><dd><p>Represents the set affiliation of the <code>item</code> as
realized in experiments by Schaper et al.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Courteously provided by Marie Lusia Schaper and Ute Bayen.
</p>


<h3>References</h3>

<p>Schaper, M. L., Kuhlmann, B. G., &amp; Bayen, U. J. (2019a). Metacognitive expectancy
effects in source monitoring: Beliefs, in-the-moment experiences, or both? Journal
of Memory and Language, 107, 95–110. https://doi.org/10.1016/j.jml.2019.03.009
</p>
<p>Schaper, M. L., Kuhlmann, B. G., &amp; Bayen, U. J. (2019b). Metamemory expectancy illusion
and schema-consistent guessing in source monitoring. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 45, 470.
https://doi.org/10.1037/xlm0000602
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
head(schaper2019)
features &lt;- schaper2019[, 3:6]

# Optimize the variance criterion
# (tends to maximize similarity in feature means)
anticlusters &lt;- anticlustering(
  features,
  K = 3,
  objective = "variance",
  categories = schaper2019$room,
  method = "exchange"
)

# Means are quite similar across sets:
by(features, anticlusters, function(x) round(colMeans(x), 2))
# Check differences in standard deviations:
by(features, anticlusters, function(x) round(apply(x, 2, sd), 2))
# Room is balanced between the three sets:
table(Room = schaper2019$room, Set = anticlusters)

# Maximize the diversity criterion
ac_dist &lt;- anticlustering(
  features,
  K = 3,
  objective = "diversity",
  categories = schaper2019$room,
  method = "exchange"
)
# With the distance criterion, means tend to be less similar,
# but standard deviations tend to be more similar:
by(features, ac_dist, function(x) round(colMeans(x), 2))
by(features, ac_dist, function(x) round(apply(x, 2, sd), 2))


</code></pre>

<hr>
<h2 id='variance_objective'>Objective value for the variance criterion</h2><span id='topic+variance_objective'></span>

<h3>Description</h3>

<p>Compute the k-means variance objective for a given clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>variance_objective(x, clusters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="variance_objective_+3A_x">x</code></td>
<td>
<p>A vector, matrix or data.frame of data points. Rows
correspond to elements and columns correspond to features. A
vector represents a single feature.</p>
</td></tr>
<tr><td><code id="variance_objective_+3A_clusters">clusters</code></td>
<td>
<p>A vector representing (anti)clusters (e.g., returned
by <code><a href="#topic+anticlustering">anticlustering</a></code> or
<code><a href="#topic+balanced_clustering">balanced_clustering</a></code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The variance objective is given by the sum of the squared
errors between cluster centers and individual data points. It is the
objective function used in k-means clustering, see
<code><a href="stats.html#topic+kmeans">kmeans</a></code>.
</p>


<h3>Value</h3>

<p>The total within-cluster variance
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Jain, A. K. (2010). Data clustering: 50 years beyond k-means.
Pattern Recognition Letters, 31, 651–666.
</p>
<p>Papenberg, M., &amp; Klau, G. W. (2021). Using anticlustering to partition 
data sets into equivalent parts. Psychological Methods, 26(2), 
161–174. https://doi.org/10.1037/met0000301.
</p>
<p>Späth, H. (1986). Anticlustering: Maximizing the variance criterion.
Control and Cybernetics, 15, 213–218.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
## Clustering
clusters &lt;- balanced_clustering(
  iris[, -5],
  K = 3
)
# This is low:
variance_objective(
  iris[, -5],
  clusters
)
## Anticlustering
anticlusters &lt;- anticlustering(
  iris[, -5],
  K = 3,
  objective = "variance"
)
# This is higher:
variance_objective(
  iris[, -5],
  anticlusters
)

# Illustrate variance objective
N &lt;- 18
data &lt;- matrix(rnorm(N * 2), ncol = 2)
cl &lt;- balanced_clustering(data, K = 3)
plot_clusters(data, cl, illustrate_variance = TRUE)
</code></pre>

<hr>
<h2 id='wce'>Exact weighted cluster editing</h2><span id='topic+wce'></span>

<h3>Description</h3>

<p>Optimally solves weighted cluster editing (also known as »correlation clustering« or
»clique partitioning problem«).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wce(x, solver = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wce_+3A_x">x</code></td>
<td>
<p>A N x N similarity matrix. Larger values indicate stronger
agreement / similarity between a pair of data points</p>
</td></tr>
<tr><td><code id="wce_+3A_solver">solver</code></td>
<td>
<p>Optional argument; if passed, has to be either &quot;glpk&quot; or
&quot;symphony&quot;. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Finds the clustering that maximizes the sum of pairwise similarities within clusters. 
In the input some similarities should be negative (indicating dissimilarity) because 
otherwise the maximum sum of similarities is obtained by simply joining all elements 
within a single big cluster. If the argument <code>solver</code> is not specified, the function
will try to find the GLPK or SYMPHONY solver by itself (it prioritizes using SYMPHONY if both are
available).
</p>


<h3>Value</h3>

<p>An integer vector representing the cluster affiliation of each data point
</p>


<h3>Note</h3>

<p>This function either requires the R package <code>Rglpk</code> and the GNU linear 
programming kit (&lt;http://www.gnu.org/software/glpk/&gt;) or the R package 
<code>Rsymphony</code> and the COIN-OR SYMPHONY solver libraries 
(&lt;https://github.com/coin-or/SYMPHONY&gt;).
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Bansal, N., Blum, A., &amp; Chawla, S. (2004). Correlation clustering. 
Machine Learning, 56, 89–113. 
</p>
<p>Böcker, S., &amp; Baumbach, J. (2013). Cluster editing. In Conference on 
Computability in Europe (pp. 33–44).
</p>
<p>Grötschel, M., &amp; Wakabayashi, Y. (1989). A cutting plane algorithm
for a clustering problem. Mathematical Programming, 45, 59-96.
</p>
<p>Wittkop, T., Emig, D., Lange, S., Rahmann, S., Albrecht, M., Morris, J. H., ..., Baumbach,
J. (2010). Partitioning biological data with transitivity clustering. Nature Methods, 7,
419–420.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
features &lt;- swiss
distances &lt;- dist(scale(swiss))
hist(distances)
# Define agreement as being close enough to each other.
# By defining low agreement as -1 and high agreement as +1, we
# solve *unweighted* cluster editing
agreements &lt;- ifelse(as.matrix(distances) &lt; 3, 1, -1)
clusters &lt;- wce(agreements)
plot(swiss, col = clusters, pch = 19)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
