<!DOCTYPE html><html lang="en"><head><title>Help for package klaR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {klaR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.dmvnorm'><p>Density of a Multivariate Normal Distribution</p></a></li>
<li><a href='#b.scal'><p>Calculation of beta scaling parameters</p></a></li>
<li><a href='#B3'><p>West German Business Cycles 1955-1994</p></a></li>
<li><a href='#benchB3'><p>Benchmarking on B3 data</p></a></li>
<li><a href='#betascale'><p>Scale membership values according to a beta scaling</p></a></li>
<li><a href='#calc.trans'><p>Calculation of transition probabilities</p></a></li>
<li><a href='#centerlines'><p>Lines from classborders to the center</p></a></li>
<li><a href='#classscatter'><p>Classification scatterplot matrix</p></a></li>
<li><a href='#cond.index'><p>Calculation of Condition Indices for Linear Regression</p></a></li>
<li><a href='#corclust'><p>Function to identify groups of highly correlated variables</p>
for removing correlated features from the data for further analysis.</a></li>
<li><a href='#countries'><p>Socioeconomic data for the most populous countries.</p></a></li>
<li><a href='#cvtree'><p>Extracts variable cluster IDs</p></a></li>
<li><a href='#distmirr'><p>Internal function to convert a distance structure to a matrix</p></a></li>
<li><a href='#dkernel'><p>Estimate density of a given kernel</p></a></li>
<li><a href='#drawparti'><p>Plotting the 2-d partitions of classification methods</p></a></li>
<li><a href='#e.scal'><p>Function to calculate e- or softmax scaled membership values</p></a></li>
<li><a href='#EDAM'><p>Computation of an Eight Direction Arranged Map</p></a></li>
<li><a href='#errormatrix'><p>Tabulation of prediction errors by classes</p></a></li>
<li><a href='#friedman.data'><p>Friedman's classification benchmark data</p></a></li>
<li><a href='#GermanCredit'><p>Statlog German Credit</p></a></li>
<li><a href='#greedy.wilks'><p>Stepwise forward variable selection for classification</p></a></li>
<li><a href='#hmm.sop'><p>Calculation of HMM Sum of Path</p></a></li>
<li><a href='#kmodes'>
<p>K-Modes Clustering</p></a></li>
<li><a href='#loclda'><p>Localized Linear Discriminant Analysis (LocLDA)</p></a></li>
<li><a href='#locpvs'><p>Pairwise variable selection for classification in local models</p></a></li>
<li><a href='#meclight.default'><p>Minimal Error Classification</p></a></li>
<li><a href='#NaiveBayes'><p>Naive Bayes Classifier</p></a></li>
<li><a href='#nm'><p>Nearest Mean Classification</p></a></li>
<li><a href='#partimat'><p>Plotting the 2-d partitions of classification methods</p></a></li>
<li><a href='#plineplot'><p>Plotting marginal posterior class probabilities</p></a></li>
<li><a href='#plot.NaiveBayes'><p>Naive Bayes Plot</p></a></li>
<li><a href='#plot.woe'><p>Plot information values</p></a></li>
<li><a href='#predict.loclda'><p>Localized Linear Discriminant Analysis (LocLDA)</p></a></li>
<li><a href='#predict.locpvs'><p>predict method for locpvs objects</p></a></li>
<li><a href='#predict.meclight'><p>Prediction of Minimal Error Classification</p></a></li>
<li><a href='#predict.NaiveBayes'><p>Naive Bayes Classifier</p></a></li>
<li><a href='#predict.pvs'><p>predict method for pvs objects</p></a></li>
<li><a href='#predict.rda'><p>Regularized Discriminant Analysis (RDA)</p></a></li>
<li><a href='#predict.sknn'><p>Simple k Nearest Neighbours Classification</p></a></li>
<li><a href='#predict.svmlight'><p>Interface to SVMlight</p></a></li>
<li><a href='#predict.woe'><p>Weights of evidence</p></a></li>
<li><a href='#pvs'><p>Pairwise variable selection for classification</p></a></li>
<li><a href='#quadplot'><p>Plotting of 4 dimensional membership representation simplex</p></a></li>
<li><a href='#quadtrafo'><p>Transforming of 4 dimensional values in a barycentric coordinate system.</p></a></li>
<li><a href='#rda'><p>Regularized Discriminant Analysis (RDA)</p></a></li>
<li><a href='#rerange'><p>Linear transformation of data</p></a></li>
<li><a href='#shardsplot'><p>Plotting Eight Direction Arranged Maps or Self-Organizing Maps</p></a></li>
<li><a href='#sknn'><p>Simple k nearest Neighbours</p></a></li>
<li><a href='#stepclass'><p>Stepwise variable selection for classification</p></a></li>
<li><a href='#svmlight'><p>Interface to SVMlight</p></a></li>
<li><a href='#TopoS'><p>Computation of criterion S of a visualization</p></a></li>
<li><a href='#triframe'><p>Barycentric plots</p></a></li>
<li><a href='#trigrid'><p>Barycentric plots</p></a></li>
<li><a href='#triperplines'><p>Barycentric plots</p></a></li>
<li><a href='#triplot'><p>Barycentric plots</p></a></li>
<li><a href='#tripoints'><p>Barycentric plots</p></a></li>
<li><a href='#tritrafo'><p>Barycentric plots</p></a></li>
<li><a href='#ucpm'><p>Uschi's classification performance measures</p></a></li>
<li><a href='#woe'><p>Weights of evidence</p></a></li>
<li><a href='#xtractvars'><p>Variable clustering based variable selection</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.7-3</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-13</td>
</tr>
<tr>
<td>Title:</td>
<td>Classification and Visualization</td>
</tr>
<tr>
<td>Author:</td>
<td>Christian Roever, Nils Raabe, Karsten Luebke, Uwe Ligges, Gero Szepannek, Marc Zentgraf, David Meyer</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Uwe Ligges &lt;ligges@statistik.tu-dortmund.de&gt;</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>SVMlight</td>
</tr>
<tr>
<td>Suggests:</td>
<td>scatterplot3d (&ge; 0.3-22), som, mlbench, rpart, e1071</td>
</tr>
<tr>
<td>Enhances:</td>
<td>clustMixType, randomForest, ClustVarLV</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.0), MASS</td>
</tr>
<tr>
<td>Imports:</td>
<td>combinat, questionr, grDevices, stats, utils, graphics</td>
</tr>
<tr>
<td>Description:</td>
<td>Miscellaneous functions for classification and visualization,
     e.g. regularized discriminant analysis, sknn() kernel-density naive Bayes, 
     an interface to 'svmlight' and stepclass() wrapper variable selection 
     for supervised classification, partimat() visualization of classification rules 
         and shardsplot() of cluster results as well as kmodes() clustering for categorical data, 
     corclust() variable clustering, variable extraction from different variable clustering models 
         and weight of evidence preprocessing.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://statistik.tu-dortmund.de">https://statistik.tu-dortmund.de</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-13 16:51:28 UTC; ligges</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-13 22:30:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='.dmvnorm'>Density of a Multivariate Normal Distribution</h2><span id='topic+.dmvnorm'></span>

<h3>Description</h3>

<p>Intended for internal use only.</p>


<h3>Usage</h3>

<pre><code class='language-R'>.dmvnorm(x, mu = NA, inv.sigma = NA, logDetCov = NA)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".dmvnorm_+3A_x">x</code></td>
<td>
</td></tr>
<tr><td><code id=".dmvnorm_+3A_mu">mu</code></td>
<td>
</td></tr>
<tr><td><code id=".dmvnorm_+3A_inv.sigma">inv.sigma</code></td>
<td>
</td></tr>
<tr><td><code id=".dmvnorm_+3A_logdetcov">logDetCov</code></td>
<td>
</td></tr>
</table>

<hr>
<h2 id='b.scal'>Calculation of beta scaling parameters</h2><span id='topic+b.scal'></span>

<h3>Description</h3>

<p>Calculates the scaling parameter for <code><a href="#topic+betascale">betascale</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>b.scal(member, grouping, dis = FALSE, eps = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="b.scal_+3A_member">member</code></td>
<td>
<p>Membership values of an argmax classification method. 
Eg. posterior probabilities of <code><a href="MASS.html#topic+lda">lda</a></code>. 
Row-wise values must sum up to 1 and must be in the interval [0,1].</p>
</td></tr>
<tr><td><code id="b.scal_+3A_grouping">grouping</code></td>
<td>
<p>Class vector.</p>
</td></tr>
<tr><td><code id="b.scal_+3A_dis">dis</code></td>
<td>
<p>Logical, whether to optimize the dispersion parameter in <code><a href="stats.html#topic+pbeta">pbeta</a></code>.</p>
</td></tr>
<tr><td><code id="b.scal_+3A_eps">eps</code></td>
<td>
<p>Minimum variation of membership values. If variance is smaller than <code>eps</code>,
the values are treated as one point.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>With <code><a href="#topic+betascale">betascale</a></code> and <code>b.scal</code>, membership values of an argmax classifier 
are scaled in such a way, that the mean membership value of those values which are assigned 
to each class reflect the mean correctness rate of that values.
This is done via <code><a href="stats.html#topic+qbeta">qbeta</a></code> and <code><a href="stats.html#topic+pbeta">pbeta</a></code> with the appropriate shape parameters. 
If <code>dis</code> is <code>TRUE</code>, it is tried that the variation of membership values 
is optimal for the accuracy relative to the correctness rate. 
If the variation of the membership values is less than <code>eps</code>,
they are treated as one point and shifted towards the correctness rate.
</p>


<h3>Value</h3>

<p>A list containing
</p>
<table role = "presentation">
<tr><td><code>model</code></td>
<td>
<p>Estimated parameters for <code><a href="#topic+betascale">betascale</a></code>.</p>
</td></tr>
<tr><td><code>eps</code></td>
<td>
<p>Value of <code>eps</code> from the call.</p>
</td></tr>
<tr><td><code>member</code></td>
<td>
<p>Scaled membership values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Karsten Luebke (<a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a>), Uwe Ligges</p>


<h3>References</h3>

<p>Garczarek, Ursula Maria (2002): Classification rules in standardized partition spaces.
Dissertation, University of Dortmund. 
URL <a href="http://hdl.handle.net/2003/2789">http://hdl.handle.net/2003/2789</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+betascale">betascale</a></code>, <code><a href="#topic+e.scal">e.scal</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MASS)
data(B3)
pB3 &lt;- predict(lda(PHASEN ~ ., data = B3))$posterior
pbB3 &lt;- b.scal(pB3, B3$PHASEN, dis = TRUE)
ucpm(pB3, B3$PHASEN)
ucpm(pbB3$member, B3$PHASEN)
</code></pre>

<hr>
<h2 id='B3'>West German Business Cycles 1955-1994</h2><span id='topic+B3'></span>

<h3>Description</h3>

<p>West German Business Cycles 1955-1994</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(B3)</code></pre>


<h3>Format</h3>

<p>A data frame with 157 observations on the following 14 variables.
</p>

<dl>
<dt>PHASEN</dt><dd><p>a factor with levels <code>1</code> (upswing), 
<code>2</code> (upper turning points), <code>3</code> (downswing), and <code>4</code> (lower turning points).</p>
</dd>
<dt>BSP91JW</dt><dd><p>GNP (y)</p>
</dd>
<dt>CP91JW</dt><dd><p>Private Consumption (y)</p>
</dd>
<dt>DEFRATE</dt><dd><p>Government deficit (percent of GNP)</p>
</dd>
<dt>EWAJW</dt><dd><p>Wage and salary earners (y)</p>
</dd>
<dt>EXIMRATE</dt><dd><p>Net exports as (percent of GNP)</p>
</dd>
<dt>GM1JW</dt><dd><p>Money supply M1 (y)</p>
</dd>
<dt>IAU91JW</dt><dd><p>Investment in equipment (y)</p>
</dd>
<dt>IB91JW</dt><dd><p>Investment in construction (y)</p>
</dd>
<dt>LSTKJW</dt><dd><p>Unit labor cost (y)</p>
</dd>
<dt>PBSPJW</dt><dd><p>GNP price deflator (y)</p>
</dd>
<dt>PCPJW</dt><dd><p>Consumer price index (y)</p>
</dd>
<dt>ZINSK</dt><dd><p>Short term interest rate (nominal)</p>
</dd>
<dt>ZINSLR</dt><dd><p>Long term interest rate (real)</p>
</dd>
</dl>

<p>where (y) stands for &ldquo;yearly growth rates&rdquo;.
</p>
<p>Note that years and corresponding year quarters are given in the row names of the data frame,
e.g. &ldquo;1988,3&rdquo; for the third quarter in 1988. 
</p>


<h3>Details</h3>

<p>The West German Business Cycles data (1955-1994) is analyzed by the project <em>B3</em>
of the SFB475 (Collaborative Research Centre 
&ldquo;Reduction of Complexity for Multivariate Data Structures&rdquo;),
supported by the Deutsche Forschungsgemeinschaft.
</p>


<h3>Source</h3>

<p>RWI (Rheinisch Westfälisches Institut für Wirtschaftsforschung), Essen, Germany.
</p>


<h3>References</h3>

<p>Heilemann, U. and Münch, H.J. (1996): 
West German Business Cycles 1963-1994: A Multivariate Discriminant Analysis.
<em>CIRET&ndash;Conference in Singapore, CIRET&ndash;Studien</em> 50.
</p>


<h3>See Also</h3>

<p>For benchmarking on this data see also <code><a href="#topic+benchB3">benchB3</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(B3)
summary(B3)
</code></pre>

<hr>
<h2 id='benchB3'>Benchmarking on B3 data</h2><span id='topic+benchB3'></span>

<h3>Description</h3>

<p>Evaluates the performance of a classification method on the <code><a href="#topic+B3">B3</a></code> data. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>benchB3(method, prior = rep(1/4, 4), sv = "4", scale = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="benchB3_+3A_method">method</code></td>
<td>
<p>classification method to use</p>
</td></tr>
<tr><td><code id="benchB3_+3A_prior">prior</code></td>
<td>
<p>prior probabilities of classes</p>
</td></tr>
<tr><td><code id="benchB3_+3A_sv">sv</code></td>
<td>
<p>class of the start of a business cycle</p>
</td></tr>
<tr><td><code id="benchB3_+3A_scale">scale</code></td>
<td>
<p>logical, whether to use <code><a href="base.html#topic+scale">scale</a></code> first</p>
</td></tr>
<tr><td><code id="benchB3_+3A_...">...</code></td>
<td>
<p>furhter arguments passed to <code>method</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The performance of classification methods on cyclic data can be measured by a special form 
of cross-validation: Leave-One-Cycle-Out. That means that a complete cycle is used as test data 
and the others are used as training data. This is repeated for all complete cycles in the data. 
</p>


<h3>Value</h3>

<p>A list with elements
</p>
<table role = "presentation">
<tr><td><code>MODEL</code></td>
<td>
<p>list with the model returned by <code>method</code> of the training data</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>vector of test error rates in cycles</p>
</td></tr>
<tr><td><code>l1co.error</code></td>
<td>
<p>leave-one-cycle-out error rate</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+B3">B3</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>perLDA &lt;- benchB3("lda")
## Not run: 
## due to parameter optimization rda takes a while 
perRDA &lt;- benchB3("rda")
library(rpart)
## rpart will not work with prior argument:
perRpart &lt;- benchB3("rpart", prior = NULL)

## End(Not run)
</code></pre>

<hr>
<h2 id='betascale'>Scale membership values according to a beta scaling</h2><span id='topic+betascale'></span>

<h3>Description</h3>

<p>Performs the scaling for beta scaling learned by <code><a href="#topic+b.scal">b.scal</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>betascale(betaobj, member)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="betascale_+3A_betaobj">betaobj</code></td>
<td>
<p>A model learned by <code><a href="#topic+b.scal">b.scal</a></code>.</p>
</td></tr>
<tr><td><code id="betascale_+3A_member">member</code></td>
<td>
<p>Membership values to be scaled.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+b.scal">b.scal</a></code>.
</p>


<h3>Value</h3>

<p>A matrix with the scaled membership values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+b.scal">b.scal</a></code>, <code><a href="#topic+e.scal">e.scal</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MASS)
data(B3)
pB3 &lt;- predict(lda(PHASEN ~ ., data = B3))$posterior
pbB3 &lt;- b.scal(pB3, B3$PHASEN)
betascale(pbB3)
</code></pre>

<hr>
<h2 id='calc.trans'>Calculation of transition probabilities</h2><span id='topic+calc.trans'></span>

<h3>Description</h3>

<p>Function to estimate the probabilities of a time series to stay or change the state.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc.trans(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc.trans_+3A_x">x</code></td>
<td>
<p>(factor) vector of states</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To estimate the transition probabilities the empirical frequencies are counted.
</p>


<h3>Value</h3>

<p>The transition probabilities matrix. 
<code>x[i,j]</code> is the probability to change from state <code>i</code> to state <code>j</code>.
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(B3)
calc.trans(B3$PHASEN)
</code></pre>

<hr>
<h2 id='centerlines'>Lines from classborders to the center</h2><span id='topic+centerlines'></span>

<h3>Description</h3>

<p>Function which constructs the lines from the borders between 
two classes to the center. To be used in connection with
<code><a href="#topic+triplot">triplot</a></code> and <code><a href="#topic+quadplot">quadplot</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>centerlines(n)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="centerlines_+3A_n">n</code></td>
<td>
<p>number of classes. Meaningful are <code>3</code> or <code>4</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with <code>n</code>-columns. 
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+triplot">triplot</a></code>, <code><a href="#topic+quadplot">quadplot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>centerlines(3)
centerlines(4)
</code></pre>

<hr>
<h2 id='classscatter'>Classification scatterplot matrix</h2><span id='topic+classscatter'></span>

<h3>Description</h3>

<p>Function to plot a scatterplot matrix with a classification result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classscatter(formula, data, method, col.correct = "black", 
    col.wrong = "red", gs = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="classscatter_+3A_formula">formula</code></td>
<td>
<p> formula of the form <code>groups ~ x1 + x2 + ...</code>. 
That is, the response is the grouping factor and the right hand side specifies the (non-factor) discriminators.  </p>
</td></tr>
<tr><td><code id="classscatter_+3A_data">data</code></td>
<td>
<p> Data frame from which variables specified in formula are preferentially to be taken.</p>
</td></tr>
<tr><td><code id="classscatter_+3A_method">method</code></td>
<td>
<p>character, name of classification function 
(e.g. &ldquo;<code><a href="MASS.html#topic+lda">lda</a></code>&rdquo;).</p>
</td></tr>
<tr><td><code id="classscatter_+3A_col.correct">col.correct</code></td>
<td>
<p>color to use for correct classified objects.</p>
</td></tr>
<tr><td><code id="classscatter_+3A_col.wrong">col.wrong</code></td>
<td>
<p>color to use for missclassified objects.</p>
</td></tr>
<tr><td><code id="classscatter_+3A_gs">gs</code></td>
<td>
<p>group symbol (plot character), must have the same length as the data. 
If <code>NULL</code>, <code>as.character(groups)</code> is the default.</p>
</td></tr>
<tr><td><code id="classscatter_+3A_...">...</code></td>
<td>
<p>further arguments passed to the underlying classification method or plot functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The actual error rate.
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+plot">plot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(B3)
library(MASS)
classscatter(PHASEN ~ BSP91JW + EWAJW + LSTKJW, 
    data = B3, method = "lda")
</code></pre>

<hr>
<h2 id='cond.index'>Calculation of Condition Indices for Linear Regression</h2><span id='topic+cond.index'></span>

<h3>Description</h3>

<p>Diagnosis of collinearity in X
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cond.index(formula, data, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cond.index_+3A_formula">formula</code></td>
<td>
<p>formula of the form &lsquo;<code>groups ~ x1 + x2 + ...</code>&rsquo;</p>
</td></tr>
<tr><td><code id="cond.index_+3A_data">data</code></td>
<td>
<p>data frame (or matrix) containing the explanatory variables</p>
</td></tr>
<tr><td><code id="cond.index_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to <code>lm</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Collinearities can inflate the variance of the estimated regression coefficients and numerical stability. The condition indices are calculated by the eigenvalues of the crossproduct matrix of the scaled but uncentered explanatory variables.
Indices &gt; 30 may indicate collinearity.
</p>


<h3>Value</h3>

<p>A vector of the condition indices.
</p>


<h3>Author(s)</h3>

<p> Andrea Preusser, Karsten Luebke (<a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a>)</p>


<h3>References</h3>

<p>Belsley, D. , Kuh, E. and Welsch, R. E. (1979), <em>Regression Diagnostics: Identifying Influential Data and Sources of Collinearity</em>, 
John Wiley (New York) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+stepclass">stepclass</a></code>, <code><a href="stats.html#topic+manova">manova</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Boston)
condition_medv &lt;- cond.index(medv ~ ., data = Boston)
condition_medv
</code></pre>

<hr>
<h2 id='corclust'>Function to identify groups of highly correlated variables 
for removing correlated features from the data for further analysis.</h2><span id='topic+corclust'></span><span id='topic+plot.corclust'></span>

<h3>Description</h3>

<p>A hierarchical clustering of variables using <code>hclust</code> is performed using 
1 - the absolute correlation as a distance measure between tow variables.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corclust(x, cl = NULL, method = "complete")
## S3 method for class 'corclust'
plot(x, selection = "both", mincor = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="corclust_+3A_x">x</code></td>
<td>
<p>Either a data frame or a matrix consisting of numerical attributes.</p>
</td></tr>
<tr><td><code id="corclust_+3A_cl">cl</code></td>
<td>
<p>Optional vector of ty factor indicating class levels, if class specific correlations should to be considered.</p>
</td></tr>
<tr><td><code id="corclust_+3A_method">method</code></td>
<td>
<p>Linkage to be used for clustering. Default is <code>complete</code> linkage.</p>
</td></tr>
<tr><td><code id="corclust_+3A_selection">selection</code></td>
<td>
<p>If <code>"numeric"</code>, &lsquo;1 - average absolute correlation within cluster&rsquo; is plotted, 
if <code>"factor"</code>, &lsquo;1 - minimum Cramer's V within cluster&rsquo; is plotted. The default, <code>"both"</code>, 
generates both variations.
</p>
</td></tr>
<tr><td><code id="corclust_+3A_mincor">mincor</code></td>
<td>
<p>Adds a horizontal line for this correlation.</p>
</td></tr>
<tr><td><code id="corclust_+3A_...">...</code></td>
<td>
<p>passed to underlying plot functions.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>Each cluster consists of a set of correlated variables according to the chosen clustering criterion. 
The default criterion is &lsquo;<code>complete</code>&rsquo;. This choice is meaningful as it represents the 
<em>minimum absolute correlation</em> between all variables of a cluster.<br />
The data set is split into numerics and factors two separate clustering models are built, depending on the variable type. 
For factors distances are computed based on 1-Cramer's V statistic using <code><a href="stats.html#topic+chisq.test">chisq.test</a></code>. 
For a large number of factor variables this might take some time. 
The resulting trees can be plotted using <code>plot</code>.<br />
Further proceeding would consist in chosing one variable of each cluster to obtain a 
subset of rather uncorrelated variables for further analysis. 
An automatic variable selection can be done using <code><a href="#topic+cvtree">cvtree</a></code> and <code><a href="#topic+xtractvars">xtractvars</a></code>.<br />
If an additional class vector <code>cl</code> is given to the function for any two variables their minimum correlation over all classes is used.  
</p>


<h3>Value</h3>

<p>Object of class <code>corclust</code>.
</p>
<table role = "presentation">
<tr><td><code>cor</code></td>
<td>
<p>Correlation matrix of numeric variables.</p>
</td></tr>
<tr><td><code>crv</code></td>
<td>
<p>Matrix of Cramer's V for factor variables.</p>
</td></tr>
<tr><td><code>cluster.numerics</code></td>
<td>
<p>Resulting hierarchical <code>hclust</code> model for numeric variables.</p>
</td></tr>
<tr><td><code>cluster.factors</code></td>
<td>
<p>Resulting hierarchical <code>hclust</code> model for factor variables.</p>
</td></tr>
<tr><td><code>id.numerics</code></td>
<td>
<p>Variable IDs of numeric variables in <code>x</code>.</p>
</td></tr>
<tr><td><code>id.factors</code></td>
<td>
<p>Variable IDs of factor variables <code>x</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gero Szepannek</p>


<h3>References</h3>

<p>Roever, C. and Szepannek, G. (2005): Application of a genetic algorithm
to variable selection in fuzzy clustering. In C. Weihs and W. Gaul (eds), Classification 
- The Ubiquitous Challenge, 674-681, Springer.</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.corclust">plot.corclust</a></code> and <code><a href="stats.html#topic+hclust">hclust</a></code> for details on the clustering algorithm, and 
<code><a href="#topic+cvtree">cvtree</a></code>, <code><a href="#topic+xtractvars">xtractvars</a></code> for postprocessing.</p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(iris)
    classes &lt;- iris$Species
    variables &lt;- iris[,1:4]
    ccres &lt;- corclust(variables, classes)
    plot(ccres, mincor = 0.6)
</code></pre>

<hr>
<h2 id='countries'>Socioeconomic data for the most populous countries.</h2><span id='topic+countries'></span>

<h3>Description</h3>

<p>Socioeconomic data for the most populous countries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(countries)</code></pre>


<h3>Format</h3>

<p>A data frame with 42 observations on the following 7 variables.
</p>

<dl>
<dt>Country</dt><dd><p>name of the country.</p>
</dd>
<dt>Popul</dt><dd><p>population.</p>
</dd>
<dt>PopDens</dt><dd><p>population density.</p>
</dd>
<dt>GDPpp</dt><dd><p>GDP per inhabitant.</p>
</dd>
<dt>LifeEx</dt><dd><p>mean life expectation</p>
</dd>
<dt>InfMor</dt><dd><p>infant mortality</p>
</dd>
<dt>Illit</dt><dd><p>illiteracy rate</p>
</dd>
</dl>



<h3>Source</h3>

<p>CIA World Factbook <a href="https://www.cia.gov/the-world-factbook/">https://www.cia.gov/the-world-factbook/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(countries)
summary(countries)
</code></pre>

<hr>
<h2 id='cvtree'>Extracts variable cluster IDs</h2><span id='topic+cvtree'></span>

<h3>Description</h3>

<p>Extracts cluster IDs for variables according to a dendrogram from object of class <code><a href="#topic+cvtree">cvtree</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvtree(object, k = 2, mincor = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cvtree_+3A_object">object</code></td>
<td>
<p>Object of class <code><a href="#topic+corclust">corclust</a></code>.</p>
</td></tr>
<tr><td><code id="cvtree_+3A_k">k</code></td>
<td>
<p>Number of clusters to be extracted from dendrogram.</p>
</td></tr>
<tr><td><code id="cvtree_+3A_mincor">mincor</code></td>
<td>
<p>Minimum within cluster correlation. Can be specified alternatively to <code>k</code>.</p>
</td></tr>
<tr><td><code id="cvtree_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>Like in <code><a href="#topic+corclust">corclust</a></code> for correlation comparison numerics and factors are considered separately. 
For factors Cramer's V statistic is used.
</p>


<h3>Value</h3>

<p>Object of class <code>cvtree</code> with elements:
</p>
<table role = "presentation">
<tr><td><code>cluster</code></td>
<td>
<p>Vector of cluster IDs.</p>
</td></tr>
<tr><td><code>correlations</code></td>
<td>
<p>Matrix of average within cluster correlations and average corrleation to all variables of the closest cluster as well as the ID of the closest cluster. For factor variables Cramer's V is computed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gero Szepannek</p>


<h3>References</h3>

<p>Roever, C. and Szepannek, G. (2005): Application of a genetic algorithm
to variable selection in fuzzy clustering. In C. Weihs and W. Gaul (eds), Classification 
- The Ubiquitous Challenge, 674-681, Springer.</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+corclust">corclust</a></code>,  <code><a href="#topic+plot.corclust">plot.corclust</a></code> and <code><a href="stats.html#topic+hclust">hclust</a></code> for details on the clustering algorithm.</p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(B3)
    ccres &lt;- corclust(B3)
    plot(ccres)
    cvtree(ccres, k = 3)
</code></pre>

<hr>
<h2 id='distmirr'>Internal function to convert a distance structure to a matrix</h2><span id='topic+distmirr'></span>

<h3>Description</h3>

<p>For internal use only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distmirr(dis)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="distmirr_+3A_dis">dis</code></td>
<td>
<p>a distance structure as computed by <code><a href="stats.html#topic+dist">dist</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A symmetric distance matrix is returned.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dist">dist</a></code></p>

<hr>
<h2 id='dkernel'>Estimate density of a given kernel</h2><span id='topic+dkernel'></span>

<h3>Description</h3>

<p>Given an estimated kernel density this function estimates the density of a new vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dkernel(x, kernel = density(x), interpolate = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dkernel_+3A_x">x</code></td>
<td>
<p>vector of which the density should be estimated</p>
</td></tr>
<tr><td><code id="dkernel_+3A_kernel">kernel</code></td>
<td>
<p>object of <code>class</code> <code><a href="stats.html#topic+density">density</a></code></p>
</td></tr>
<tr><td><code id="dkernel_+3A_interpolate">interpolate</code></td>
<td>
<p>Interpolate or use <code><a href="stats.html#topic+density">density</a></code> of nearest point?</p>
</td></tr>
<tr><td><code id="dkernel_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Denstiy of <code>x</code> in <code>kernel</code>.
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+density">density</a></code>, <code><a href="#topic+NaiveBayes">NaiveBayes</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>kern &lt;- density(rnorm(50))
x &lt;- seq(-3, 3, len = 100)
y &lt;- dkernel(x, kern)
plot(x, y, type = "l")
</code></pre>

<hr>
<h2 id='drawparti'>Plotting the 2-d partitions of classification methods</h2><span id='topic+drawparti'></span>

<h3>Description</h3>

<p>Plot showing the classification of observations based on 
classification methods (e.g. <code>lda</code>, <code>qda</code>) for two variables. 
Moreover, the classification borders are displayed and the apparent error rates are given in each title.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>drawparti(grouping, x, y, method = "lda", prec = 100, xlab = NULL, 
    ylab = NULL, col.correct = "black", col.wrong = "red", 
    col.mean = "black", col.contour = "darkgrey", 
    gs = as.character(grouping), pch.mean = 19, cex.mean = 1.3, 
    print.err = 0.7, legend.err = FALSE, legend.bg = "white",
    imageplot = TRUE, image.colors = cm.colors(nc), 
    plot.control = list(), ...)        
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="drawparti_+3A_grouping">grouping</code></td>
<td>
<p>factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_x">x</code></td>
<td>
<p>first explanatory vector.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_y">y</code></td>
<td>
<p>second explanatory vector.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_method">method</code></td>
<td>
<p>the method the classification is based on, currently supported are:
<code><a href="MASS.html#topic+lda">lda</a></code>, <code><a href="MASS.html#topic+qda">qda</a></code>, <code><a href="rpart.html#topic+rpart">rpart</a></code>, <code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code>, 
<code><a href="#topic+rda">rda</a></code>, <code><a href="#topic+sknn">sknn</a></code> and <code><a href="#topic+svmlight">svmlight</a></code></p>
</td></tr></table>
<p>.
</p>
<table role = "presentation">
<tr><td><code id="drawparti_+3A_prec">prec</code></td>
<td>
<p>precision used to draw the classification borders (the higher the more precise; default: 100).</p>
</td></tr>
<tr><td><code id="drawparti_+3A_xlab">xlab</code></td>
<td>
<p>a title for the x axis.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_ylab">ylab</code></td>
<td>
<p>a title for the y axis.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_col.correct">col.correct</code></td>
<td>
<p>color for correct classified objects.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_col.wrong">col.wrong</code></td>
<td>
<p>color for wrong classified objects.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_col.mean">col.mean</code></td>
<td>
<p>color for class means (only for methods <code>lda</code> and <code>qda</code>).</p>
</td></tr>
<tr><td><code id="drawparti_+3A_col.contour">col.contour</code></td>
<td>
<p>color of the contour lines (if <code>imageplot = FALSE</code>).</p>
</td></tr>
<tr><td><code id="drawparti_+3A_gs">gs</code></td>
<td>
<p>group symbol (plot character), must have the same length as <code>grouping</code>.</p>
</td></tr>            
<tr><td><code id="drawparti_+3A_pch.mean">pch.mean</code></td>
<td>
<p>plot character for class means (only for methods <code>lda</code> and <code>qda</code>).</p>
</td></tr>
<tr><td><code id="drawparti_+3A_cex.mean">cex.mean</code></td>
<td>
<p>character expansion for class means (only for methods <code>lda</code> and <code>qda</code>).</p>
</td></tr>
<tr><td><code id="drawparti_+3A_print.err">print.err</code></td>
<td>
<p>character expansion for text specifying the apparent error rate. 
If <code>print.err = 0</code>, nothing is printed.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_legend.err">legend.err</code></td>
<td>
<p>logical; whether to plot the apparent error rate above the plot (if <code>FALSE</code>),
or into a legend into the upper right corner of the plot (if <code>TRUE</code>). 
This argument is ignored, if <code>print.err = 0</code>, i.e. if no error rate is printed.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_legend.bg">legend.bg</code></td>
<td>
<p>Backgound colour to use for the legend.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_imageplot">imageplot</code></td>
<td>
<p>logical; whether to use an <code><a href="graphics.html#topic+image">image</a></code> plot or <code><a href="graphics.html#topic+contour">contour</a></code> lines.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_image.colors">image.colors</code></td>
<td>
<p>colors used for the <code>imageplot</code>, if TRUE.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_plot.control">plot.control</code></td>
<td>
<p>A list containing further arguments passed to the underlying plot functions.</p>
</td></tr>
<tr><td><code id="drawparti_+3A_...">...</code></td>
<td>
<p>Further arguments passed to the classification <code>method</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a>, Uwe Ligges, Irina Czogiel</p>


<h3>See Also</h3>

<p><code><a href="#topic+partimat">partimat</a></code></p>

<hr>
<h2 id='e.scal'>Function to calculate e- or softmax scaled membership values</h2><span id='topic+e.scal'></span>

<h3>Description</h3>

<p>Calculates the e- or softmax scaled membership values of an argmax based classification rule.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>e.scal(x, k = 1, tc = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="e.scal_+3A_x">x</code></td>
<td>
<p>matrix of membership values</p>
</td></tr>
<tr><td><code id="e.scal_+3A_k">k</code></td>
<td>
<p>parameter for e-scaling (1 for softmax)</p>
</td></tr>
<tr><td><code id="e.scal_+3A_tc">tc</code></td>
<td>
<p>vector of true classes (required if <code>k</code> has to be optimized)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For any  membership vector y <code class="reqn">\exp(y\cdot k) / \sum\exp(y\cdot k)</code> is calculated. 
If <code>k=1</code>, the classical softmax scaling is used. If the true classes are given, <code>k</code> is optimized 
so that the apparent error rate is minimized.
</p>


<h3>Value</h3>

<p>A list containing elements
</p>
<table role = "presentation">
<tr><td><code>sv</code></td>
<td>
<p>Scaled values</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>Optimal <code>k</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>References</h3>

<p>Garczarek, Ursula Maria (2002): Classification rules in standardized partition spaces.
Dissertation, University of Dortmund. 
URL <a href="http://hdl.handle.net/2003/2789">http://hdl.handle.net/2003/2789</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MASS)
data(iris)
ldaobj &lt;- lda(Species ~ ., data = iris)
ldapred &lt;- predict(ldaobj)$posterior
e.scal(ldapred)
e.scal(ldapred, tc = iris$Species)
</code></pre>

<hr>
<h2 id='EDAM'>Computation of an Eight Direction Arranged Map</h2><span id='topic+EDAM'></span>

<h3>Description</h3>

<p>Produces an object of class <code>EDAM</code> which is a two dimensional representation 
of data in a rectangular, equally spaced 
grid as known from Self-Organizing Maps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EDAM(EV0, nzx = 0, iter.max = 10, random = TRUE, standardize = FALSE, 
    wghts = 0, classes = 0, sa = TRUE, temp.in = 0.5, temp.fin = 1e-07, 
    temp.gamma = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EDAM_+3A_ev0">EV0</code></td>
<td>
<p> either a symmetric dissimilarity matrix or a matrix of arbitrary dimensions whose 
n rows correspond to cases and whose k columns correspond to variables. </p>
</td></tr>
<tr><td><code id="EDAM_+3A_nzx">nzx</code></td>
<td>
<p> an integer specifying the number of vertical bars in the grid. By default, 
<code>nzx</code> is chosen automatically, so that the grid
gets closest do a square. If n is no multiple of <code>nzx</code>, all surplus objects are skipped.</p>
</td></tr>
<tr><td><code id="EDAM_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer giving the maxmimum number of iterations to perform for the same neighborhood size.</p>
</td></tr>
<tr><td><code id="EDAM_+3A_random">random</code></td>
<td>
<p>logical. If <code>TRUE</code>, the initital order is drawn from a uniform distribution.</p>
</td></tr>
<tr><td><code id="EDAM_+3A_standardize">standardize</code></td>
<td>
<p>logical. If <code>TRUE</code>, the measurements in <code>EV0</code> are standardized before 
calculating Euclidean distances. 
Measurements are standardized for each variable by dividing by the variable's 
standard deviation. Meaningless if <code>EV0</code> is a dissimilarity matrix. </p>
</td></tr>
<tr><td><code id="EDAM_+3A_wghts">wghts</code></td>
<td>
<p> an optional vector of length k giving relative weights of the variables in 
computing Euclidean distances. Meaningless if <code>EV0</code> is a dissimilarity matrix. </p>
</td></tr>
<tr><td><code id="EDAM_+3A_classes">classes</code></td>
<td>
<p> an optional vector of length n specifying the membership to classes for all objects.</p>
</td></tr>
<tr><td><code id="EDAM_+3A_sa">sa</code></td>
<td>
<p>logical. If <code>TRUE</code>, the optimization is obtained by Simulated Annealing.</p>
</td></tr>
<tr><td><code id="EDAM_+3A_temp.in">temp.in</code></td>
<td>
<p>numeric giving the initial temperature, if <code>sa</code> is set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="EDAM_+3A_temp.fin">temp.fin</code></td>
<td>
<p> numeric giving the final temperature, if <code>sa</code> is set to <code>TRUE</code>. 
Meaningless if <code>temp.gamma</code> is greater than 0.</p>
</td></tr>
<tr><td><code id="EDAM_+3A_temp.gamma">temp.gamma</code></td>
<td>
<p> numeric giving the relative change of the temperature from one iteration to the other, 
if <code>sa</code> is set to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data given by <code>EV0</code> is visualized by the EDAM-algorithm. This method approximates the best visualization where
goodness is measured by <code>S</code>, a transformation of the criterion <code>stress</code> as i.e. 
known from <code><a href="MASS.html#topic+sammon">sammon</a></code>. 
The target space of the visualization is restricted to a grid so the problem has a discrete solution space. 
Originally this restriction was made to make the results
comparable to those of Kohonen Self-Organizing Maps. But it turns out that also for reasons of a clear arrangement the
representation in a grid can be more favorable than in the hole plane.
</p>
<p>During the computation of EDAM 3 values indicating its progress are given online. The first is the number of the actual
iteration, the second the maximum number of overall performed iterations. The latter may reduce during computation,
since the neighborhood reduces in case of convergence before the last iteration. 
The last number gives the actual criterion S.
The default plot method <code>plot.edam</code> for objects of class <code>EDAM</code> is <code><a href="#topic+shardsplot">shardsplot</a></code>.
</p>


<h3>Value</h3>

<p>EDAM returns an object of <code>class</code> <code>EDAM</code>, which is a list containing the following components:
</p>
<table role = "presentation">
<tr><td><code>preimages</code></td>
<td>
<p>the re-ordered data; the position of the i-th object is where <code>Z</code> equals i.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>a matrix representing the positions of the <code>preimages</code> in the grid by their numbers.</p>
</td></tr>
<tr><td><code>Z.old.terms</code></td>
<td>
<p>a matrix representing the positions of the data in original order in the grid by their numbers.</p>
</td></tr>
<tr><td><code>cl.ord</code></td>
<td>
<p>a vector giving the re-ordered classes. All elements equal 1 if argument <code>classes</code> is undefined.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the criterion of the map</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Nils Raabe</p>


<h3>References</h3>

<p>Raabe, N. (2003).
<em>Vergleich von Kohonen Self-Organizing-Maps mit einem nichtsimultanen 
Klassifikations- und Visualisierungsverfahren</em>.
Diploma Thesis, Department of Statistics, University of Dortmund.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+shardsplot">shardsplot</a></code>, <code><a href="#topic+TopoS">TopoS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute an Eight Directions Arranged Map for a random sample 
# of the iris data.
data(iris)
set.seed(1234)
iris.sample &lt;- sample(150, 42)
irisEDAM &lt;- EDAM(iris[iris.sample, 1:4], classes = iris[iris.sample, 5], 
    standardize = TRUE, iter.max = 3)
plot(irisEDAM, vertices = FALSE)
legend(3, 5, col = rainbow(3), legend = levels(iris[,5]), pch = 16)
print(irisEDAM)

# Construct clusters within the phases of the german business data 
# and visualize the centroids by EDAM.
data(B3)
phasemat &lt;- lapply(1:4, function(x) B3[B3[,1] == x, 2:14])
subclasses &lt;- lapply(phasemat, 
    function(x) cutree(hclust(dist(x)), k = round(nrow(x) / 4.47)))
centroids &lt;- lapply(1:4, 
    function(y) apply(phasemat[[y]], 2, 
        function(x) by(x, subclasses[[y]], mean)))
centmat &lt;- matrix(unlist(sapply(centroids, t)), ncol = 13, 
    byrow = TRUE, dimnames = list(NULL, colnames(centroids[[1]])))
centclasses &lt;- unlist(lapply(1:4, 
    function(x) rep(x, unlist(lapply(centroids, nrow))[x])))
B3EDAM &lt;- EDAM(centmat, classes = centclasses, standardize = TRUE, 
    iter.max = 6, rand = FALSE)
plot(B3EDAM, standardize = TRUE)
opar &lt;- par(xpd = NA)
legend(4, 5.1, col = rainbow(4), pch = 16, xjust = 0.5, yjust = 0,
    ncol = 2, legend = c("upswing", "upper turning point", 
                         "downswing", "lower turning point"))
print(B3EDAM)
par(opar)
</code></pre>

<hr>
<h2 id='errormatrix'>Tabulation of prediction errors by classes</h2><span id='topic+errormatrix'></span>

<h3>Description</h3>

<p>Cross-tabulates true and predicted classes 
with the option to show relative frequencies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>errormatrix(true, predicted, relative = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="errormatrix_+3A_true">true</code></td>
<td>
<p>Vector of true classes.</p>
</td></tr>
<tr><td><code id="errormatrix_+3A_predicted">predicted</code></td>
<td>
<p>Vector of predicted classes.</p>
</td></tr>
<tr><td><code id="errormatrix_+3A_relative">relative</code></td>
<td>
<p>Logical. If <code>TRUE</code> rows are normalized
to show relative frequencies (see below).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given vectors of true and predicted classes, a (symmetric) 
table of misclassifications is constructed.
</p>
<p>Element [i,j] shows the number of objects of class i that 
were classified as class j; so the main diagonal shows the 
correct classifications. The last row and column show the
corresponding sums of misclassifications, the lower right 
element is the total sum of misclassifications.
</p>
<p>If &lsquo;<code>relative</code>&rsquo; is <code>TRUE</code>, the <em>rows</em> are 
normalized so they show relative frequencies instead. The 
lower right element now shows the total error rate, and the
remaining last row sums up to one, so it shows &ldquo;where the
misclassifications went&rdquo;.
</p>


<h3>Value</h3>

<p>A (named) matrix.
</p>


<h3>Note</h3>

<p>Concerning the case that &lsquo;<code>relative</code>&rsquo; is <code>TRUE</code>:
</p>
<p>If a prior distribution over the classes is given, the 
misclassification rate that is returned as the lower right 
element (which is only the fraction of misclassified 
<em>data</em>) is not an estimator for the expected 
misclassification rate.
</p>
<p>In that case you have to multiply the individual error rates for 
each class (returned in the last column) with the corresponding
prior probabilities and sum these up (see example below).
</p>
<p>Both error rate estimates are equal, if the fractions of classes 
in the data are equal to the prior probabilities.
</p>


<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+table">table</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
library(MASS)
x &lt;- lda(Species ~ Sepal.Length + Sepal.Width, data=iris)
y &lt;- predict(x, iris)

# absolute numbers: 
errormatrix(iris$Species, y$class)

# relative frequencies: 
errormatrix(iris$Species, y$class, relative = TRUE)

# percentages: 
round(100 * errormatrix(iris$Species, y$class, relative = TRUE), 0)

# expected error rate in case of class prior: 
indiv.rates &lt;- errormatrix(iris$Species, y$class, relative = TRUE)[1:3, 4]
prior &lt;- c("setosa" = 0.2, "versicolor" = 0.3, "virginica" = 0.5)
total.rate &lt;- t(indiv.rates) %*% prior
total.rate
</code></pre>

<hr>
<h2 id='friedman.data'>Friedman's classification benchmark data</h2><span id='topic+friedman.data'></span>

<h3>Description</h3>

<p>Function to generate 3-class classification benchmarking data
as introduced by J.H. Friedman (1989)</p>


<h3>Usage</h3>

<pre><code class='language-R'>friedman.data(setting = 1, p = 6, samplesize = 40, asmatrix = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="friedman.data_+3A_setting">setting</code></td>
<td>
<p>the problem setting (integer 1,2,...,6).</p>
</td></tr>
<tr><td><code id="friedman.data_+3A_p">p</code></td>
<td>
<p>number of variables (6, 10, 20 or 40).</p>
</td></tr>
<tr><td><code id="friedman.data_+3A_samplesize">samplesize</code></td>
<td>
<p>sample size (number of observations, &gt;=6).</p>
</td></tr>
<tr><td><code id="friedman.data_+3A_asmatrix">asmatrix</code></td>
<td>
<p>if <code>TRUE</code>, results are returned as a matrix,
otherwise as a data frame (default).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When J.H. Friedman introduced the Regularized Discriminant Analysis
(<code><a href="#topic+rda">rda</a></code>) in 1989, he used artificially generated data
to test the procedure and to examine its performance in comparison to
Linear and Quadratic Discriminant Analysis
(see also <code><a href="MASS.html#topic+lda">lda</a></code> and <code><a href="MASS.html#topic+qda">qda</a></code>).
</p>
<p>6 different settings were considered to demonstrate potential strengths
and weaknesses of the new method:
</p>

<ol>
<li><p> equal spherical covariance matrices,
</p>
</li>
<li><p> unequal spherical covariance matrices,
</p>
</li>
<li><p> equal, highly ellipsoidal covariance matrices with mean
differences in low-variance subspace,
</p>
</li>
<li><p> equal, highly ellipsoidal covariance matrices with mean
differences in high-variance subspace,
</p>
</li>
<li><p> unequal, highly ellipsoidal covariance matrices with zero mean
differences and
</p>
</li>
<li><p> unequal, highly ellipsoidal covariance matrices with nonzero mean
differences.
</p>
</li></ol>

<p>For each of the 6 settings data was generated with 6, 10, 20 and 40
variables.
</p>
<p>Classification performance was then measured by repeatedly creating
training-datasets of 40 observations and estimating the
misclassification rates by test sets of 100 observations.
</p>
<p>The number of classes is always 3, class labels are assigned randomly
(with equal probabilities) to observations, so the contributions of
classes to the data differs from dataset to dataset. To make sure
covariances can be estimated at all, there are always at least two
observations from each class in a dataset.
</p>


<h3>Value</h3>

<p>Depending on <code>asmatrix</code> either a data frame or a matrix with
<code>samplesize</code> rows and <code>p+1</code> columns, the first column containing
the class labels, the remaining columns being the variables.
</p>


<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>References</h3>

<p>Friedman, J.H. (1989): Regularized Discriminant Analysis.
In: <em>Journal of the American Statistical Association</em> 84, 165-175.</p>


<h3>See Also</h3>

<p><code><a href="#topic+rda">rda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Reproduce the 1st setting with 6 variables.
# Error rate should be somewhat near 9 percent.
training &lt;- friedman.data(1, 6, 40)
x &lt;- rda(class ~ ., data = training, gamma = 0.74, lambda = 0.77)
test &lt;- friedman.data(1, 6, 100)
y &lt;- predict(x, test[,-1])
errormatrix(test[,1], y$class)
</code></pre>

<hr>
<h2 id='GermanCredit'>Statlog German Credit</h2><span id='topic+GermanCredit'></span>

<h3>Description</h3>

<p>The dataset contains data of past credit applicants. The applicants are rated
as <em>good</em> or <em>bad</em>. Models of this data can be used to determine if
new applicants present a <em>good</em> or <em>bad</em> credit risk. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("GermanCredit")</code></pre>


<h3>Format</h3>

<p>A data frame containing 1,000 observations on 21 variables.
</p>

<dl>
<dt>status</dt><dd><p>factor variable indicating the status of the existing checking account, with levels <code>... &lt; 100 DM</code>, <code>0 &lt;= ... &lt; 200 DM</code>, <code>... &gt;= 200 DM/salary for at least 1 year</code> and <code>no checking account</code>.</p>
</dd>
<dt>duration</dt><dd><p>duration in months.</p>
</dd>  
<dt>credit_history</dt><dd><p>factor variable indicating credit history, with levels <code>no credits taken/all credits paid back duly</code>, <code>all credits at this bank paid back duly</code>,  <code>existing credits paid back duly till now</code>, <code>delay in paying off in the past</code> and <code>critical account/other credits existing</code>.</p>
</dd>    
<dt>purpose</dt><dd><p>factor variable indicating the credit's purpose, with levels <code>car (new)</code>, <code>car (used)</code>, <code>furniture/equipment</code>, <code>radio/television</code>, <code>domestic appliances</code>, <code>repairs</code>, <code>education</code>,  <code>retraining</code>, <code>business</code> and <code>others</code>.</p>
</dd>  
<dt>amount</dt><dd><p>credit amount.</p>
</dd>    
<dt>savings</dt><dd><p>factor. savings account/bonds, with levels <code>... &lt; 100 DM</code>, <code>100 &lt;= ... &lt; 500 DM</code>, <code>500 &lt;= ... &lt; 1000 DM</code>, <code>... &gt;= 1000 DM</code> and <code>unknown/no savings account</code>.</p>
</dd>  
<dt>employment_duration</dt><dd><p>ordered factor indicating the duration of the current employment, with levels <code>unemployed</code>, <code>... &lt; 1 year</code>, <code>1 &lt;= ... &lt; 4 years</code>, <code>4 &lt;= ... &lt; 7 years</code> and <code>... &gt;= 7 years</code>.</p>
</dd>    
<dt>installment_rate</dt><dd><p>installment rate in percentage of disposable income.</p>
</dd>  
<dt>personal_status_sex</dt><dd><p>factor variable indicating personal status and sex, with levels 
<code>male:divorced/separated</code>, <code>female:divorced/separated/married</code>, 
<code>male:single</code>, <code>male:married/widowed</code> and <code>female:single</code>.</p>
</dd>    
<dt>other_debtors</dt><dd><p>factor. Other debtors, with levels <code>none</code>, <code>co-applicant</code> and <code>guarantor</code>.</p>
</dd>      
<dt>present_residence</dt><dd><p>present residence since?</p>
</dd>    
<dt>property</dt><dd><p>factor variable indicating the client's highest valued property, with levels <code>real estate</code>, <code>building society savings agreement/life insurance</code>, <code>car or other</code> and <code>unknown/no property</code>.</p>
</dd>  
<dt>age</dt><dd><p>client's age.</p>
</dd>    
<dt>other_installment_plans</dt><dd><p>factor variable indicating other installment plans, with levels <code>bank</code>, <code>stores</code> and <code>none</code>.</p>
</dd>     
<dt>housing</dt><dd><p>factor variable indicating housing, with levels <code>rent</code>, <code>own</code> and <code>for free</code>.</p>
</dd>    
<dt>number_credits</dt><dd><p>number of existing credits at this bank.</p>
</dd>  
<dt>job</dt><dd><p>factor indicating employment status, with levels <code>unemployed/unskilled - non-resident</code>, <code>unskilled - resident</code>, <code>skilled employee/official</code> and <code>management/self-employed/highly qualified employee/officer</code>.</p>
</dd>    
<dt>people_liable</dt><dd><p>Number of people being liable to provide maintenance.</p>
</dd>     
<dt>telephone</dt><dd><p>binary variable indicating if the customer has a registered telephone number.</p>
</dd>
<dt>foreign_worker</dt><dd><p>binary variable indicating if the customer is a foreign worker.</p>
</dd>
<dt>credit_risk</dt><dd><p>binary variable indicating credit risk, with levels <code>good</code> and <code>bad</code>.</p>
</dd>
</dl>



<h3>Source</h3>

<p>The original data was provided by:
</p>
<p>Professor Dr. Hans Hofmann,  
Institut fuer Statistik und Oekonometrie,
Universitaet Hamburg,  
FB Wirtschaftswissenschaften,  
Von-Melle-Park 5,    
2000 Hamburg 13
</p>
<p>The dataset has been taken from the UCI Repository Of Machine Learning Databases at <a href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a>.
</p>
<p>It was published this way in CRAN package evtree (maintainer: Thomas Grubinger)
that has been archived from CRAN on May 31, 2014. Afterwards the exactly same data object has been copied from the evtree package to klaR.
</p>

<hr>
<h2 id='greedy.wilks'>Stepwise forward variable selection for classification</h2><span id='topic+greedy.wilks'></span><span id='topic+greedy.wilks.default'></span><span id='topic+greedy.wilks.formula'></span><span id='topic+print.greedy.wilks'></span>

<h3>Description</h3>

<p>Performs a stepwise forward variable/model selection using the Wilk's Lambda criterion.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>greedy.wilks(X, ...)
## Default S3 method:
greedy.wilks(X, grouping, niveau = 0.2, ...)
## S3 method for class 'formula'
greedy.wilks(formula, data = NULL, ...) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="greedy.wilks_+3A_x">X</code></td>
<td>
<p>matrix or data frame (rows=cases, columns=variables) </p>
</td></tr>
<tr><td><code id="greedy.wilks_+3A_grouping">grouping</code></td>
<td>
<p>class indicator vector</p>
</td></tr>
<tr><td><code id="greedy.wilks_+3A_formula">formula</code></td>
<td>
<p>formula of the form &lsquo;<code>groups ~ x1 + x2 + ...</code>&rsquo;</p>
</td></tr>
<tr><td><code id="greedy.wilks_+3A_data">data</code></td>
<td>
<p>data frame (or matrix) containing the explanatory variables</p>
</td></tr>
<tr><td><code id="greedy.wilks_+3A_niveau">niveau</code></td>
<td>
<p>level for the approximate F-test decision</p>
</td></tr>
<tr><td><code id="greedy.wilks_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to the default method, e.g. <code>niveau</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>A stepwise forward variable selection is performed. The initial model is defined by starting with the
variable which separates the groups most. The model is then extended by including further variables depending 
on the Wilk's lambda criterion: Select the one which minimizes the Wilk's lambda of the model including the
variable if its p-value still shows statistical significance.
</p>


<h3>Value</h3>

<p>A list of two components, a <code>formula</code> of the form &lsquo;<code>response ~ list + of + selected + variables</code>&rsquo;,
and a data.frame <code>results</code> containing the following variables:
</p>
<table role = "presentation">
<tr><td><code>vars</code></td>
<td>
<p>the names of the variables in the final model in the order of selection.</p>
</td></tr>
<tr><td><code>Wilks.lambda</code></td>
<td>
<p>the appropriate Wilks' lambda for the selected variables.</p>
</td></tr>
<tr><td><code>F.statistics.overall</code></td>
<td>
<p>the approximated F-statistic for the so far selected model.</p>
</td></tr>
<tr><td><code>p.value.overall</code></td>
<td>
<p>the appropriate p-value of the F-statistic.</p>
</td></tr>
<tr><td><code>F.statistics.diff</code></td>
<td>
<p>the approximated F-statistic of the partial Wilks's lambda (for comparing the model 
including the new variable with the model not including it).</p>
</td></tr>
<tr><td><code>p.value.diff</code></td>
<td>
<p>the appropriate p-value of the F-statistic of the partial Wilk's lambda.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Andrea Preusser, Karsten Luebke (<a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a>)</p>


<h3>References</h3>

<p>Mardia, K. V. , Kent, J. T. and Bibby, J. M. (1979), <em>Multivariate analysis</em>, 
Academic Press (New York; London) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+stepclass">stepclass</a></code>, <code><a href="stats.html#topic+manova">manova</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(B3)
gw_obj &lt;- greedy.wilks(PHASEN ~ ., data = B3, niveau = 0.1)
gw_obj
## now you can say stuff like
## lda(gw_obj$formula, data = B3)
</code></pre>

<hr>
<h2 id='hmm.sop'>Calculation of HMM Sum of Path</h2><span id='topic+hmm.sop'></span>

<h3>Description</h3>

<p>A Hidden Markov Model for the classification of states in a time series. 
Based on the transition probabilities and the so called emission probabilities
(<code class="reqn">p(class|x)</code>) the &lsquo;prior probablilities&rsquo; of states (classes) in time period <em>t</em> 
given all past information in time period <em>t</em> are calculated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmm.sop(sv, trans.matrix, prob.matrix)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hmm.sop_+3A_sv">sv</code></td>
<td>
<p>state at time 0</p>
</td></tr>
<tr><td><code id="hmm.sop_+3A_trans.matrix">trans.matrix</code></td>
<td>
<p>matrix of transition probabilities</p>
</td></tr>
<tr><td><code id="hmm.sop_+3A_prob.matrix">prob.matrix</code></td>
<td>
<p>matrix of <code class="reqn">p(class|x)</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the &lsquo;prior probablilities&rsquo; of states. 
</p>


<h3>Author(s)</h3>

<p>Daniel Fischer, Reinald Oetsch</p>


<h3>References</h3>

<p>Garczarek, Ursula Maria (2002): Classification rules in standardized partition spaces.
Dissertation, University of Dortmund. 
URL <a href="http://hdl.handle.net/2003/2789">http://hdl.handle.net/2003/2789</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+calc.trans">calc.trans</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MASS)
data(B3)
trans.matrix &lt;- calc.trans(B3$PHASEN)

# Calculate posterior prob. for the classes via lda
prob.matrix &lt;- predict(lda(PHASEN ~ ., data = B3))$post
errormatrix(B3$PHASEN, apply(prob.matrix, 1, which.max))
prior.prob &lt;- hmm.sop("2", trans.matrix, prob.matrix)
errormatrix(B3$PHASEN, apply(prior.prob, 1, which.max))
</code></pre>

<hr>
<h2 id='kmodes'>
K-Modes Clustering
</h2><span id='topic+kmodes'></span><span id='topic+print.kmodes'></span>

<h3>Description</h3>

<p>Perform k-modes clustering on categorical data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmodes(data, modes, iter.max = 10, weighted = FALSE, fast = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kmodes_+3A_data">data</code></td>
<td>
<p>A matrix or data frame of categorical data. Objects have to be in rows, variables in columns.</p>
</td></tr>
<tr><td><code id="kmodes_+3A_modes">modes</code></td>
<td>
<p>Either the number of modes or a set of initial
(distinct) cluster modes.  If a number, a random set of (distinct)
rows in <code>data</code> is chosen as the initial modes.</p>
</td></tr>
<tr><td><code id="kmodes_+3A_iter.max">iter.max</code></td>
<td>
<p>The maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="kmodes_+3A_weighted">weighted</code></td>
<td>
<p>Whether usual simple-matching distance between objects is used, or a weighted version of this distance.</p>
</td></tr>
<tr><td><code id="kmodes_+3A_fast">fast</code></td>
<td>
<p>Logical Whether a fast version of the algorithm should be applied.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">k</code>-modes algorithm (Huang, 1997) an extension of the k-means algorithm by MacQueen (1967).
</p>
<p>The data given by <code>data</code> is clustered by the <code class="reqn">k</code>-modes method (Huang, 1997)
which aims to partition the objects into <code class="reqn">k</code> groups such that the
distance from objects to the assigned cluster modes is minimized.
</p>
<p>By default simple-matching distance is used to determine the dissimilarity of two objects. It is computed by counting the number of mismatches in all variables.
Alternative this distance is weighted by the frequencies of the categories in data (see Huang, 1997, for details).
</p>
<p>If an initial matrix of modes is supplied, it is possible that
no object will be closest to one or more modes. In this case less cluster than supplied modes will be returned 
and a warning is given.
</p>
<p>If called using <code>fast = TRUE</code> the reassignment of the data to clusters is done for the entire data set before recomputation of the modes is done. For computational reasons this option should be chosen unless moderate data sizes.   
</p>
<p>For clustering mixed type data it is referred to <code><a href="clustMixType.html#topic+kproto">kproto</a></code>.
</p>


<h3>Value</h3>

<p>An object of class <code>"kmodes"</code> which is a list with components:
</p>
<table role = "presentation">
<tr><td><code>cluster</code></td>
<td>
<p>A vector of integers indicating the cluster to which each object is allocated.</p>
</td></tr> 
<tr><td><code>size</code></td>
<td>
<p>The number of objects in each cluster.</p>
</td></tr>
<tr><td><code>modes</code></td>
<td>
<p>A matrix of cluster modes.</p>
</td></tr>
<tr><td><code>withindiff</code></td>
<td>
<p>The within-cluster simple-matching distance for each cluster.</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>The number of iterations the algorithm has run.</p>
</td></tr>
<tr><td><code>weighted</code></td>
<td>
<p>Whether weighted distances were used or not.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Neumann, <a href="mailto:christian2.neumann@tu-dortmund.de">christian2.neumann@tu-dortmund.de</a>, Gero Szepannek, <a href="mailto:gero.szepannek@web.de">gero.szepannek@web.de</a></p>


<h3>References</h3>

<p>Huang, Z. (1997)  A Fast Clustering Algorithm to Cluster Very Large Categorical Data Sets in Data Mining.
in <em>KDD: Techniques and Applications</em> (H. Lu, H. Motoda and H. Luu, Eds.), pp. 21-34, World Scientific, Singapore.
</p>
<p>MacQueen, J. (1967)  Some methods for classification and analysis of
multivariate observations. In <em>Proceedings of the Fifth Berkeley
Symposium on  Mathematical Statistics and  Probability</em>,
eds L. M. Le Cam &amp; J. Neyman,
<b>1</b>, pp. 281-297. Berkeley, CA: University of California Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### a 5-dimensional toy-example:

## generate data set with two groups of data:
set.seed(1)
x &lt;- rbind(matrix(rbinom(250, 2, 0.25), ncol = 5),
           matrix(rbinom(250, 2, 0.75), ncol = 5))
colnames(x) &lt;- c("a", "b", "c", "d", "e")

## run algorithm on x:
(cl &lt;- kmodes(x, 2))

## and visualize with some jitter:
plot(jitter(x), col = cl$cluster)
points(cl$modes, col = 1:5, pch = 8)
</code></pre>

<hr>
<h2 id='loclda'>Localized Linear Discriminant Analysis (LocLDA)</h2><span id='topic+loclda'></span><span id='topic+loclda.default'></span><span id='topic+loclda.formula'></span><span id='topic+loclda.matrix'></span><span id='topic+loclda.data.frame'></span><span id='topic+print.loclda'></span>

<h3>Description</h3>

<p>A localized version of Linear Discriminant Analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loclda(x, ...)

## S3 method for class 'formula'
loclda(formula, data, ..., subset, na.action)

## Default S3 method:
loclda(x, grouping, weight.func = function(x) 1/exp(x), 
    k = nrow(x), weighted.apriori = TRUE, ...)

## S3 method for class 'data.frame'
 loclda(x, ...)

## S3 method for class 'matrix'
loclda(x, grouping, ..., subset, na.action)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loclda_+3A_formula">formula</code></td>
<td>
<p>Formula of the form &lsquo;<code>groups ~ x1 + x2 + ...</code>&rsquo;.</p>
</td></tr>
<tr><td><code id="loclda_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in <code>formula</code> are to be taken.</p>
</td></tr>
<tr><td><code id="loclda_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the explanatory variables
(required, if <code>formula</code> is not given).</p>
</td></tr>
<tr><td><code id="loclda_+3A_grouping">grouping</code></td>
<td>
<p>(required if no <code>formula</code> principal argument is given.) 
A factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="loclda_+3A_weight.func">weight.func</code></td>
<td>
<p>Function used to compute local weights. 
Must be finite over the interval [0,1]. See Details below.</p>
</td></tr>
<tr><td><code id="loclda_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbours used to construct localized classification rules. 
See Details below.</p>
</td></tr>
<tr><td><code id="loclda_+3A_weighted.apriori">weighted.apriori</code></td>
<td>
<p>Logical: if <code>TRUE</code>, class prior probabilities are computed 
using local weights (see Details below). If <code>FALSE</code>, equal priors for all classes 
actually occurring in the train data are used.</p>
</td></tr>
<tr><td><code id="loclda_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the training sample.</p>
</td></tr>
<tr><td><code id="loclda_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are found. 
The default action is for the procedure to fail. An alternative is <code>na.omit</code> 
which leads to rejection of cases with missing values on any required variable.</p>
</td></tr>
<tr><td><code id="loclda_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>loclda.default</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is an approach to apply the concept of localization described by Tutz and Binder (2005) 
to Linear Discriminant Analysis. The function <code>loclda</code> generates an object of class <code>loclda</code> 
(see Value below). As localization makes it necessary to build an 
individual decision rule for each test observation, 
this rule construction has to be handled by <code><a href="#topic+predict.loclda">predict.loclda</a></code>. 
For convenience, the rule building procedure is still described here.
</p>
<p>To classify a test observation <code class="reqn">x_s</code>, only the <code>k</code> nearest neighbours of 
<code class="reqn">x_s</code> within the train data are used. Each of these k train observations 
<code class="reqn">x_i, i = 1,\dots,k</code>, is assigned a weight <code class="reqn">w_i</code> according to 
</p>
<p style="text-align: center;"><code class="reqn">w_i = K\left(\frac{||x_i-x_s||}{d_k}\right), i=1,\dots,k</code>
</p>
 
<p>where K is the weighting function given by <code>weight.func</code>, <code class="reqn">||x_i-x_s||</code> 
is the euclidian distance of <code class="reqn">x_i</code> and <code class="reqn">x_s</code> 
and <code class="reqn">d_k</code> is the euclidian distance of <code class="reqn">x_s</code> 
to its <code class="reqn">k</code>-th nearest neighbour. 
With these weights for each class <code class="reqn">A_g, g=1,\dots,G</code>, 
its weighted empirical mean <code class="reqn">\hat{\mu}_g</code> and weighted empirical 
covariance matrix are computed. The estimated pooled (weighted) covariance matrix 
<code class="reqn">\hat{\Sigma}</code> is then calculated from the individual weighted 
empirical class covariance matrices. If <code>weighted.apriori</code> is <code>TRUE</code> (the default), 
prior class probabilities are estimated according to: 
</p>
<p style="text-align: center;"><code class="reqn">prior_g := \frac{\sum_{i=1}^k \left(w_i \cdot I (x_i \in A_g)\right)}{\sum_{i=1}^k \left( w_i \right)}</code>
</p>
 
<p>where I is the indicator function. If <code>FALSE</code>, equal priors for all classes are used. 
In analogy to Linear Discriminant Analysis, the decision rule for <code class="reqn">x_s</code> is 
</p>
<p style="text-align: center;"><code class="reqn">\hat{A} := argmax_{g \in 1,\dots,G} (posterior_g)</code>
</p>
 
<p>where </p>
<p style="text-align: center;"><code class="reqn">posterior_g := prior_g \cdot \exp{\left( (-\frac{1}{2}) t(x_s-\hat{\mu}_g)\hat{\Sigma}^{-1}(x_s-\hat{\mu}_g)\right)} </code>
</p>

<p>If <code class="reqn">posterior_g &lt; 10^{(-150)} \forall g \in \{1,\dots,G\}</code>, 
<code class="reqn">posterior_g</code> is set to <code class="reqn">\frac{1}{G}</code> for all <code class="reqn">g \in 1,\dots,G</code> 
and the test observation <code class="reqn">x_s</code> is simply assigned to the class whose weighted mean has the lowest 
euclidian distance to <code class="reqn">x_s</code>.
</p>


<h3>Value</h3>

<p>A list of class <code>loclda</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td></tr>
<tr><td><code>learn</code></td>
<td>
<p>Matrix containing the values of the explanatory variables for all train observations.</p>
</td></tr>
<tr><td><code>grouping</code></td>
<td>
<p>Factor specifying the class for each train observation.</p>
</td></tr>
<tr><td><code>weight.func</code></td>
<td>
<p>Value of the argument <code>weight.func</code>.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>Value of the argument <code>k</code>.</p>
</td></tr>
<tr><td><code>weighted.apriori</code></td>
<td>
<p>Value of the argument <code>weighted.apriori</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marc Zentgraf (<a href="mailto:marc-zentgraf@gmx.de">marc-zentgraf@gmx.de</a>) and Karsten Luebke (<a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a>)</p>


<h3>References</h3>

<p>Tutz, G. and Binder, H. (2005): Localized classification. <em>Statistics and Computing</em> 15, 155-166.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.loclda">predict.loclda</a></code>,
<code><a href="MASS.html#topic+lda">lda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>benchB3("lda")$l1co.error
benchB3("loclda")$l1co.error
</code></pre>

<hr>
<h2 id='locpvs'>Pairwise variable selection for classification in local models</h2><span id='topic+locpvs'></span>

<h3>Description</h3>

<p>Performs pairwise variable selection on subclasses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>locpvs(x, subclasses, subclass.labels, prior=NULL, method="lda", 
    vs.method = c("ks.test", "stepclass", "greedy.wilks"),
    niveau=0.05, fold=10, impr=0.1, direct="backward", out=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="locpvs_+3A_x">x</code></td>
<td>
<p>matrix or data frame containing the explanatory variables. x must consist of numerical data only. </p>
</td></tr>
<tr><td><code id="locpvs_+3A_subclasses">subclasses</code></td>
<td>
<p>vector indicating the subclasses (a factor) </p>
</td></tr>
<tr><td><code id="locpvs_+3A_subclass.labels">subclass.labels</code></td>
<td>
<p>must be a matrix with 2 coloumns, where the first coloumn specifies the subclass and the second coloumn the according upper class </p>
</td></tr>
<tr><td><code id="locpvs_+3A_prior">prior</code></td>
<td>
<p>prior probabilites for the classes. If not specified the prior probabilities will be set according to proportion in &ldquo;subclasses&rdquo;. If specified the order of prior 
probabilities must be the same as in &ldquo;subclasses&rdquo;. </p>
</td></tr>
<tr><td><code id="locpvs_+3A_method">method</code></td>
<td>
<p>character, name of classification function (e.g. &ldquo;<code><a href="MASS.html#topic+lda">lda</a></code>&rdquo; (default)).</p>
</td></tr>
<tr><td><code id="locpvs_+3A_vs.method">vs.method</code></td>
<td>
<p>character, name of variable selection method. Must be one of &ldquo;<code><a href="stats.html#topic+ks.test">ks.test</a></code>&rdquo; (default),
&ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; or &ldquo;<code><a href="#topic+greedy.wilks">greedy.wilks</a></code>&rdquo;. </p>
</td></tr>
<tr><td><code id="locpvs_+3A_niveau">niveau</code></td>
<td>
<p>used niveau for &ldquo;<code><a href="stats.html#topic+ks.test">ks.test</a></code>&rdquo;</p>
</td></tr>
<tr><td><code id="locpvs_+3A_fold">fold</code></td>
<td>
<p>parameter for cross-validation, if &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; is chosen &lsquo;<code>vs.method</code>&rsquo;</p>
</td></tr>                                            
<tr><td><code id="locpvs_+3A_impr">impr</code></td>
<td>
<p>least improvement of performance measure desired to include or exclude any variable (&lt;=1), if &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; is chosen &lsquo;<code>vs.method</code>&rsquo; </p>
</td></tr>  
<tr><td><code id="locpvs_+3A_direct">direct</code></td>
<td>
<p>direction of variable selection, if &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; is chosen &lsquo;<code>vs.method</code>&rsquo;. 
Must be one if &ldquo;<code>forward</code>&rdquo;, &ldquo;<code>backward</code>&rdquo; (default) or &ldquo;<code>both</code>&rdquo;. </p>
</td></tr>
<tr><td><code id="locpvs_+3A_out">out</code></td>
<td>
<p>indicator (logical) for textoutput during computation (slows down computation!), if &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; is chosen &lsquo;<code>vs.method</code>&rsquo; </p>
</td></tr>
<tr><td><code id="locpvs_+3A_...">...</code></td>
<td>
<p>further parameters passed to classification function (&lsquo;<code>method</code>&rsquo;) or variable selection method (&lsquo;<code>vs.method</code>&rsquo;) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>A call on <code><a href="#topic+pvs">pvs</a></code> is performed using &ldquo;subclasses&rdquo; as grouping variable. See <code><a href="#topic+pvs">pvs</a></code> for further details.
</p>


<h3>Value</h3>

<p>An object of class &lsquo;<code>locpvs</code>&rsquo; containing the following components:
</p>
<table role = "presentation">
<tr><td><code>pvs.result</code></td>
<td>
<p>the complete output of the call to <code><a href="#topic+pvs">pvs</a></code> (see <code><a href="#topic+pvs">pvs</a></code> for further details</p>
</td></tr>
<tr><td><code>subclass.labels</code></td>
<td>
<p>the subclass.labels as specified in function call</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gero Szepannek, <a href="mailto:szepannek@statistik.tu-dortmund.de">szepannek@statistik.tu-dortmund.de</a>, Christian Neumann</p>


<h3>References</h3>

<p>Szepannek, G. and Weihs, C. (2006) Local Modelling in Classification on Different Feature Subspaces. 
In <em>Advances in Data Mining.</em>, ed Perner, P., LNAI 4065, pp. 226-234. Springer, Heidelberg.</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.locpvs">predict.locpvs</a></code> for predicting &lsquo;<code>locpvs</code>&rsquo; models and <code><a href="#topic+pvs">pvs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## this example might be a bit artificial, but it sufficiently shows how locpvs has to be used

## learn a locpvs-model on the Vehicle dataset

library("mlbench")
data("Vehicle")

subclass &lt;- Vehicle$Class # use four car-types in dataset as subclasses
## aggregate "bus" and "van" to upper-class "big" and "saab" and "opel" to upper-class "small"
subclass_class &lt;- matrix(c("bus","van","saab","opel","big","big","small","small"),ncol=2) 

## learn now a locpvs-model for the subclasses:
model &lt;- locpvs(Vehicle[,1:18], subclass, subclass_class) 
model # short summary, showing the class-pairs of the submodels 
# together with the selected variables and the relation of sub- to upperclasses

## predict:
pred &lt;- predict(model, Vehicle[,1:18])

## now you can look at the predicted classes:
pred$class
## or at the posterior probabilities:
pred$posterior
## or at the posterior probabilities for the subclasses:
pred$subclass.posteriors

</code></pre>

<hr>
<h2 id='meclight.default'>Minimal Error Classification</h2><span id='topic+meclight'></span><span id='topic+meclight.default'></span><span id='topic+meclight.formula'></span><span id='topic+meclight.matrix'></span><span id='topic+meclight.data.frame'></span><span id='topic+print.meclight'></span>

<h3>Description</h3>

<p>Computer intensive method for linear dimension reduction that minimizes the classification error directly.</p>


<h3>Usage</h3>

<pre><code class='language-R'>meclight(x, ...)

## Default S3 method:
meclight(x, grouping, r = 1, fold = 10, ...)
## S3 method for class 'formula'
meclight(formula, data = NULL, ..., subset, na.action = na.fail)
## S3 method for class 'data.frame'
meclight(x, ...)
## S3 method for class 'matrix'
meclight(x, grouping, ..., subset, na.action = na.fail)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="meclight.default_+3A_x">x</code></td>
<td>
<p>(required if no formula is given as the principal argument.) A matrix or data frame 
containing the explanatory variables.</p>
</td></tr>
<tr><td><code id="meclight.default_+3A_grouping">grouping</code></td>
<td>
<p>(required if no formula principal argument is given.) A factor specifying the class for each observation.</p>
</td></tr>  
<tr><td><code id="meclight.default_+3A_r">r</code></td>
<td>
<p>Dimension of projected subspace.</p>
</td></tr>
<tr><td><code id="meclight.default_+3A_fold">fold</code></td>
<td>
<p>Number of Bootstrap samples.</p>
</td></tr>
<tr><td><code id="meclight.default_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code>. That is, the response is the grouping factor and 
the right hand side specifies the (non-factor) discriminators. </p>
</td></tr>
<tr><td><code id="meclight.default_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in formula are preferentially to be taken.</p>
</td></tr>
<tr><td><code id="meclight.default_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the training sample. 
(NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="meclight.default_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if NAs are found. 
The default action is for the procedure to fail. 
An alternative is <code>na.omit</code>, 
which leads to rejection of cases with missing values on any required variable. 
(NOTE: If given, this argument must be named.) </p>
</td></tr>
<tr><td><code id="meclight.default_+3A_...">...</code></td>
<td>
<p>Further arguments passed to <code><a href="MASS.html#topic+lda">lda</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computer intensive method for linear dimension reduction that minimizes the classification error in the projected
subspace directly. Classification is done by <code><a href="MASS.html#topic+lda">lda</a></code>. In contrast to the reference function minimization is
done by Nelder-Mead in <code><a href="stats.html#topic+optim">optim</a></code>.</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>method.model</code></td>
<td>
<p>An object of class &lsquo;lda&rsquo;.</p>
</td></tr>
<tr><td><code>Proj.matrix</code></td>
<td>
<p>Projection matrix.</p>
</td></tr>
<tr><td><code>B.error</code></td>
<td>
<p>Estimated bootstrap error rate.</p>
</td></tr>
<tr><td><code>B.impro</code></td>
<td>
<p>Improvement in <code><a href="MASS.html#topic+lda">lda</a></code> error rate.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Maria Eveslage, Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>References</h3>

<p>Roehl, M.C., Weihs, C., and Theis, W. (2002): 
Direct Minimization in Multivariate Classification. <em>Computational Statistics</em>, 17, 29-46.</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.meclight">predict.meclight</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
meclight.obj &lt;- meclight(Species ~ ., data = iris)
meclight.obj
</code></pre>

<hr>
<h2 id='NaiveBayes'>Naive Bayes Classifier</h2><span id='topic+NaiveBayes'></span><span id='topic+NaiveBayes.default'></span><span id='topic+NaiveBayes.formula'></span>

<h3>Description</h3>

<p>Computes the conditional a-posterior probabilities of a categorical
class variable given independent predictor variables using
the Bayes rule.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
NaiveBayes(formula, data, ..., subset, na.action = na.pass)
## Default S3 method:
NaiveBayes(x, grouping, prior, usekernel = FALSE, fL = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NaiveBayes_+3A_x">x</code></td>
<td>
<p>a numeric matrix, or a data frame of categorical and/or
numeric variables.</p>
</td></tr>
<tr><td><code id="NaiveBayes_+3A_grouping">grouping</code></td>
<td>
<p>class vector (a factor).</p>
</td></tr>
<tr><td><code id="NaiveBayes_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>class ~ x1 + x2 +
      ...</code>. Interactions are not allowed.</p>
</td></tr>
<tr><td><code id="NaiveBayes_+3A_data">data</code></td>
<td>
<p>a data frame of predictors (categorical and/or
numeric).</p>
</td></tr>
<tr><td><code id="NaiveBayes_+3A_prior">prior</code></td>
<td>
<p>the prior probabilities of class membership. If unspecified, 
the class proportions for the training set are used. If present, 
the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="NaiveBayes_+3A_usekernel">usekernel</code></td>
<td>
<p>if <code>TRUE</code> a kernel density estimate (<code><a href="stats.html#topic+density">density</a></code>)
is used for density estimation. If <code>FALSE</code> a normal density is estimated.</p>
</td></tr>
<tr><td><code id="NaiveBayes_+3A_fl">fL</code></td>
<td>
<p>Factor for Laplace correction, default factor is 0, i.e. no correction.</p>
</td></tr>
<tr><td><code id="NaiveBayes_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="stats.html#topic+density">density</a></code>.</p>
</td></tr>
<tr><td><code id="NaiveBayes_+3A_subset">subset</code></td>
<td>
<p>for data given in a data frame, an index vector
specifying the cases to be used in the
training sample.  (NOTE: If given, this argument must be
named.)</p>
</td></tr>
<tr><td><code id="NaiveBayes_+3A_na.action">na.action</code></td>
<td>
<p>a function to specify the action to be taken if <code>NA</code>s are
found. The default action is not to count them for the
computation of the probability factors. An
alternative is na.omit, which leads to rejection of cases
with missing values on any required variable.  (NOTE: If
given, this argument must be named.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implementation of Naive Bayes as well as this help is based on the code by 
David Meyer in the package e1071 but extended for kernel estimated densities and user 
specified <code>prior</code> probabilities. 
The standard naive Bayes classifier (at least this implementation)
assumes independence of the predictor
variables.
</p>


<h3>Value</h3>

<p>An object of class <code>"NaiveBayes"</code> including components:
</p>
<table role = "presentation">
<tr><td><code>apriori</code></td>
<td>
<p>Class distribution for the dependent variable.</p>
</td></tr>
<tr><td><code>tables</code></td>
<td>
<p>A list of tables, one for each predictor variable. For each
categorical variable a table giving, for each attribute level, the conditional
probabilities given the target class. For each numeric variable, a
table giving, for each target class, mean and standard deviation of
the (sub-)variable or a object of <code>class</code> <code><a href="stats.html#topic+density">density</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.NaiveBayes">predict.NaiveBayes</a></code>,<code><a href="#topic+plot.NaiveBayes">plot.NaiveBayes</a></code>,<code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code>,<code><a href="MASS.html#topic+qda">qda</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
m &lt;- NaiveBayes(Species ~ ., data = iris)
</code></pre>

<hr>
<h2 id='nm'>Nearest Mean Classification</h2><span id='topic+nm'></span><span id='topic+nm.default'></span><span id='topic+nm.formula'></span><span id='topic+nm.matrix'></span><span id='topic+nm.data.frame'></span>

<h3>Description</h3>

<p>Function for nearest mean classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nm(x, ...)

## Default S3 method:
nm(x, grouping, gamma = 0, ...)
## S3 method for class 'data.frame'
nm(x, ...)
## S3 method for class 'matrix'
nm(x, grouping, ..., subset, na.action = na.fail)
## S3 method for class 'formula'
nm(formula, data = NULL, ..., subset, na.action = na.fail)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nm_+3A_x">x</code></td>
<td>
<p>matrix or data frame containing the explanatory variables 
(required, if <code>formula</code> is not given)</p>
</td></tr>
<tr><td><code id="nm_+3A_grouping">grouping</code></td>
<td>
<p>factor specifying the class for each observation 
(required, if <code>formula</code> is not given)</p>
</td></tr>
<tr><td><code id="nm_+3A_formula">formula</code></td>
<td>
<p>formula of the form <code>groups ~ x1 + x2 + ...</code>.
That is, the response is the grouping factor and the right hand side specifies the (non-factor) discriminators</p>
</td></tr>
<tr><td><code id="nm_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in <code>formula</code> are preferentially to be taken</p>
</td></tr>
<tr><td><code id="nm_+3A_gamma">gamma</code></td>
<td>
<p>gamma parameter for rbf weight of the distance to mean. If <code>gamma=0</code> the posterior is 1 for the
nearest class (mean) and 0 else.</p>
</td></tr>
<tr><td><code id="nm_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample. (Note: If given, this argument must be named!)</p>
</td></tr>
<tr><td><code id="nm_+3A_na.action">na.action</code></td>
<td>
<p>specify the action to be taken if <code>NA</code>s are
found. The default action is for the procedure to fail. An
alternative is <code><a href="stats.html#topic+na.omit">na.omit</a></code>, which leads to rejection of cases with
missing values on any required variable. (Note: If given, this
argument must be named.) </p>
</td></tr>
<tr><td><code id="nm_+3A_...">...</code></td>
<td>
<p>further arguments passed to the underlying <code><a href="#topic+sknn">sknn</a></code> function</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nm</code> is calling <code><a href="#topic+sknn">sknn</a></code> with the class means as observations.
If <code>gamma&gt;0</code> a gaussian like density is used to weight the distance to the class means
<code>weight=exp(-gamma*distance)</code>. This is similar to an rbf kernel. 
If the distances are large it may be useful to <code><a href="base.html#topic+scale">scale</a></code> the data first.
</p>


<h3>Value</h3>

<p>A list containing the function call and the class means (<code>learn</code>)). 
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+sknn">sknn</a></code>, <code><a href="#topic+rda">rda</a></code>, <code><a href="class.html#topic+knn">knn</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(B3)
x &lt;- nm(PHASEN ~ ., data = B3)
x$learn
x &lt;- nm(PHASEN ~ ., data = B3, gamma = 0.1)
predict(x)$post
</code></pre>

<hr>
<h2 id='partimat'>Plotting the 2-d partitions of classification methods</h2><span id='topic+partimat'></span><span id='topic+partimat.default'></span><span id='topic+partimat.formula'></span><span id='topic+partimat.data.frame'></span><span id='topic+partimat.matrix'></span>

<h3>Description</h3>

<p>Provides a multiple figure array which shows the classification of observations based on 
classification methods (e.g. <code>lda</code>, <code>qda</code>) for every combination of two variables. 
Moreover, the classification borders are displayed and the apparent error rates are given in each title.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partimat(x,...)

## Default S3 method:
partimat(x, grouping, method = "lda", prec = 100, 
    nplots.vert, nplots.hor, main = "Partition Plot", name, mar, 
    plot.matrix = FALSE, plot.control = list(), ...)
## S3 method for class 'data.frame'
partimat(x, ...)
## S3 method for class 'matrix'
partimat(x, grouping, ..., subset, na.action = na.fail)
## S3 method for class 'formula'
partimat(formula, data = NULL, ..., subset, na.action = na.fail)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="partimat_+3A_x">x</code></td>
<td>
<p>matrix or data frame containing the explanatory variables (required, if <code>formula</code> is not given).</p>
</td></tr>
<tr><td><code id="partimat_+3A_grouping">grouping</code></td>
<td>
<p>factor specifying the class for each observation (required, if <code>formula</code> is not given).</p>
</td></tr>
<tr><td><code id="partimat_+3A_formula">formula</code></td>
<td>
<p>formula of the form <code>groups ~ x1 + x2 + ...</code>. 
That is, the response is the grouping factor and the right hand side specifies the (non-factor) discriminators. </p>
</td></tr>
<tr><td><code id="partimat_+3A_method">method</code></td>
<td>
<p>the method the classification is based on, currently supported are:
<code><a href="MASS.html#topic+lda">lda</a></code>, <code><a href="MASS.html#topic+qda">qda</a></code>, <code><a href="rpart.html#topic+rpart">rpart</a></code>, <code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code>, 
<code><a href="#topic+rda">rda</a></code>, <code><a href="#topic+sknn">sknn</a></code> and <code><a href="#topic+svmlight">svmlight</a></code></p>
</td></tr></table>
<p>.
</p>
<table role = "presentation">
<tr><td><code id="partimat_+3A_prec">prec</code></td>
<td>
<p>precision used to draw the classification borders (the higher the more precise; default: 100).</p>
</td></tr>
<tr><td><code id="partimat_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in formula are preferentially to be taken.</p>
</td></tr>
<tr><td><code id="partimat_+3A_nplots.vert">nplots.vert</code></td>
<td>
<p>number of rows in the multiple figure array</p>
</td></tr>
<tr><td><code id="partimat_+3A_nplots.hor">nplots.hor</code></td>
<td>
<p>number of columns in the multiple figure array</p>
</td></tr>
<tr><td><code id="partimat_+3A_subset">subset</code></td>
<td>
<p>index vector specifying the cases to be used in the
training sample. (Note: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="partimat_+3A_na.action">na.action</code></td>
<td>
<p>specify the action to be taken if <code>NA</code>s are
found. The default action is for the procedure to fail. An
alternative is <code><a href="stats.html#topic+na.omit">na.omit</a></code>, which leads to rejection of cases with
missing values on any required variable. (Note: If given, this argument must be named.) </p>
</td></tr>
<tr><td><code id="partimat_+3A_main">main</code></td>
<td>
<p>title</p>
</td></tr>
<tr><td><code id="partimat_+3A_name">name</code></td>
<td>
<p>Variable names to be printed at the axis / into the diagonal.</p>
</td></tr>
<tr><td><code id="partimat_+3A_mar">mar</code></td>
<td>
<p>numerical vector of the form <code>c(bottom, left, top, right)</code>
which gives the lines of margin to be specified on the four sides of the plot.
Defaults are <code>rep(0, 4)</code> if <code>plot.matrix = TRUE</code>, <code>c(5, 4, 2, 1) + 0.1</code> otherwise.</p>
</td></tr>
<tr><td><code id="partimat_+3A_plot.matrix">plot.matrix</code></td>
<td>
<p>logical; if <code>TRUE</code>, like a scatterplot matrix; 
if <code>FALSE</code> (default) uses less space and arranges the plots &ldquo;optimal&rdquo; 
(using a fuzzy algorithm) in an array by plotting each pair of variables once.</p>
</td></tr>
<tr><td><code id="partimat_+3A_plot.control">plot.control</code></td>
<td>
<p>A list containing further arguments passed to the underlying 
plot functions (and to <code><a href="#topic+drawparti">drawparti</a></code>).</p>
</td></tr>
<tr><td><code id="partimat_+3A_...">...</code></td>
<td>
<p>Further arguments passed to the classification <code>method</code> (through <code><a href="#topic+drawparti">drawparti</a></code>).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Warnings such as  &lsquo;parameter &ldquo;xyz&rdquo; couldn't be set in high-level plot function&rsquo; are expected,
if making use of <code>...</code>.</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a>, Uwe Ligges, Irina Czogiel</p>


<h3>See Also</h3>

<p>for much more fine tuning see <code><a href="#topic+drawparti">drawparti</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MASS)
data(iris)
partimat(Species ~ ., data = iris, method = "lda")
## Not run: 
partimat(Species ~ ., data = iris, method = "lda", 
    plot.matrix = TRUE, imageplot = FALSE) # takes some time ...

## End(Not run)
</code></pre>

<hr>
<h2 id='plineplot'>Plotting marginal posterior class probabilities</h2><span id='topic+plineplot'></span>

<h3>Description</h3>

<p>For a given variable the posteriori probabilities of the classes given by a 
classification method are plotted. The variable need not be used for the actual
classifcation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plineplot(formula, data, method, x, col.wrong = "red", 
          ylim = c(0, 1), loo = FALSE, mfrow, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plineplot_+3A_formula">formula</code></td>
<td>
<p>formula of the form <code>groups ~ x1 + x2 + ...</code>. 
That is, the response is the grouping factor and the right hand side specifies the (non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="plineplot_+3A_data">data</code></td>
<td>
<p> Data frame from which variables specified in formula are preferentially to be taken.</p>
</td></tr>
<tr><td><code id="plineplot_+3A_method">method</code></td>
<td>
<p>character, name of classification function 
(e.g. &ldquo;<code><a href="MASS.html#topic+lda">lda</a></code>&rdquo;).</p>
</td></tr>
<tr><td><code id="plineplot_+3A_x">x</code></td>
<td>
<p>variable that should be plotted. See examples.</p>
</td></tr>
<tr><td><code id="plineplot_+3A_col.wrong">col.wrong</code></td>
<td>
<p>color to use for missclassified objects.</p>
</td></tr>
<tr><td><code id="plineplot_+3A_ylim">ylim</code></td>
<td>
<p><code>ylim</code> for the plot.</p>
</td></tr>
<tr><td><code id="plineplot_+3A_loo">loo</code></td>
<td>
<p>logical, whether leave-one-out estimate is used for prediction</p>
</td></tr>
<tr><td><code id="plineplot_+3A_mfrow">mfrow</code></td>
<td>
<p>number of rows and columns in the graphics device, see <code><a href="graphics.html#topic+par">par</a></code>. 
If missing, number of rows equals number of classes, and 1 column.</p>
</td></tr>
<tr><td><code id="plineplot_+3A_...">...</code></td>
<td>
<p>further arguments passed to the underlying classification method or plot functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The actual error rate.
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+partimat">partimat</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MASS)

# The name of the variable can be used for x
data(B3)
plineplot(PHASEN ~ ., data = B3, method = "lda", 
    x = "EWAJW", xlab = "EWAJW")

# The plotted variable need not be in the data
data(iris)
iris2 &lt;- iris[ , c(1,3,5)]
plineplot(Species ~ ., data = iris2, method = "lda", 
    x = iris[ , 4], xlab = "Petal.Width")
</code></pre>

<hr>
<h2 id='plot.NaiveBayes'>Naive Bayes Plot</h2><span id='topic+plot.NaiveBayes'></span>

<h3>Description</h3>

<p>Visualizes the marginal probabilities of predictor variables 
given the class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'NaiveBayes'
plot(x, vars, n = 1000, legendplot = TRUE, lty, col,
    ylab = "Density", main = "Naive Bayes Plot", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.NaiveBayes_+3A_x">x</code></td>
<td>
<p>an object of class <code><a href="#topic+NaiveBayes">NaiveBayes</a></code></p>
</td></tr>
<tr><td><code id="plot.NaiveBayes_+3A_vars">vars</code></td>
<td>
<p>variables to be plotted. If missing, all predictor variables are plotted.</p>
</td></tr>
<tr><td><code id="plot.NaiveBayes_+3A_n">n</code></td>
<td>
<p>number of points used to plot the density line.</p>
</td></tr>
<tr><td><code id="plot.NaiveBayes_+3A_legendplot">legendplot</code></td>
<td>
<p>logical, whether to print a <code><a href="graphics.html#topic+legend">legend</a></code></p>
</td></tr>
<tr><td><code id="plot.NaiveBayes_+3A_lty">lty</code></td>
<td>
<p>line type for different classes, defaults to the first <code>length(x$apriori)</code> 
colors of the current palette in use.</p>
</td></tr>
<tr><td><code id="plot.NaiveBayes_+3A_col">col</code></td>
<td>
<p>color for different classes, defaults to <code>rainbow(length(x$apriori))</code>.</p>
</td></tr>
<tr><td><code id="plot.NaiveBayes_+3A_ylab">ylab</code></td>
<td>
<p>label for y-axis.</p>
</td></tr>
<tr><td><code id="plot.NaiveBayes_+3A_main">main</code></td>
<td>
<p>title of the plots.</p>
</td></tr>
<tr><td><code id="plot.NaiveBayes_+3A_...">...</code></td>
<td>
<p>furhter arguments passed to the underlying plot functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For metric variables the estimated density is plotted. For categorial variables <code><a href="graphics.html#topic+mosaicplot">mosaicplot</a> is called.</code>
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+NaiveBayes">NaiveBayes</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
mN &lt;- NaiveBayes(Species ~ ., data = iris)
plot(mN)

mK &lt;- NaiveBayes(Species ~ ., data = iris, usekernel = TRUE)
plot(mK)
</code></pre>

<hr>
<h2 id='plot.woe'>Plot information values</h2><span id='topic+plot.woe'></span>

<h3>Description</h3>

<p>Barplot of information values to compare dicriminator of the transformed variables.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'woe'
plot(x, type = c("IV", "woes"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.woe_+3A_x">x</code></td>
<td>
<p>An object of class <code>woe</code>.</p>
</td></tr>
<tr><td><code id="plot.woe_+3A_type">type</code></td>
<td>
<p>Character to specify the plot type, see below. Either <code>"IV"</code> (default) or <code>"woes"</code>.</p>
</td></tr>
<tr><td><code id="plot.woe_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the barplot function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>type=="IV"</code> a barplot of information values for all transformed variables. 
A thumb rule of interpretation is that Values above 0.3 are considered as strongly discrimative where values below 0.02 are considered to characterize unpredictive variables. 
For <code>type=="woes"</code> for each variable the relative frequencies of all transformed levels are plotted.
</p>


<h3>Value</h3>

<p>No value is returned.</p>


<h3>Author(s)</h3>

<p>Gero Szepannek</p>


<h3>References</h3>

<p>Good, I. (1950): <em>Probability and the Weighting of Evidences.</em> Charles Griffin, London.
</p>
<p>Kullback, S. (1959): <em>Information Theory and Statistics.</em> Wiley, New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+woe">woe</a></code>, <code><a href="#topic+predict.woe">predict.woe</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># see examples in ?woe
</code></pre>

<hr>
<h2 id='predict.loclda'>Localized Linear Discriminant Analysis (LocLDA)</h2><span id='topic+predict.loclda'></span>

<h3>Description</h3>

<p>Classifies new observations using parameters determined by
the <code>loclda</code>-function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'loclda'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.loclda_+3A_object">object</code></td>
<td>
<p>Object of class <code>loclda</code>.</p>
</td></tr>
<tr><td><code id="predict.loclda_+3A_newdata">newdata</code></td>
<td>
<p>Data frame of cases to be classified.</p>
</td></tr>
<tr><td><code id="predict.loclda_+3A_...">...</code></td>
<td>
<p>Further arguments are ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table role = "presentation">
<tr><td><code>class</code></td>
<td>
<p>Vector (of class <code>factor</code>) of classifications.</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>Posterior probabilities for the classes. 
For details of computation see <code><a href="#topic+loclda">loclda</a></code> 
(+ normalization so posterior-values add up to 1 for each observation).</p>
</td></tr>
<tr><td><code>all.zero</code></td>
<td>
<p>Vector (of class <code>integer</code>) indicating for which rows of
<code>newdata</code> all corresponding posterior-values are <code class="reqn">&lt; 10^{-150}</code> before normalization. 
Those observations are assigned to the class to whose (locally weighted) centroid they have the 
lowest euclidian distance.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marc Zentgraf (<a href="mailto:marc-zentgraf@gmx.de">marc-zentgraf@gmx.de</a>) and Karsten Luebke (<a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a>)</p>


<h3>See Also</h3>

<p><code><a href="#topic+loclda">loclda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(B3)
x &lt;- loclda(PHASEN ~ ., data = B3, subset = 1:80)
predict(x, B3[-(1:80),])
</code></pre>

<hr>
<h2 id='predict.locpvs'>predict method for locpvs objects</h2><span id='topic+predict.locpvs'></span>

<h3>Description</h3>

<p>Prediction of class membership and posterior probabilities in local models using pairwise variable selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'locpvs'
predict(object,newdata, quick = FALSE, return.subclass.prediction = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.locpvs_+3A_object">object</code></td>
<td>
<p>an object of class &lsquo;<code>locpvs</code>&rsquo;, as that created by the function &ldquo;<code><a href="#topic+locpvs">locpvs</a></code>&rdquo;</p>
</td></tr>
<tr><td><code id="predict.locpvs_+3A_newdata">newdata</code></td>
<td>
<p>a data frame or matrix containing new data. If not given the same datas as used for training the &lsquo;<code>pvs</code>&rsquo;-model are used. </p>
</td></tr>
<tr><td><code id="predict.locpvs_+3A_quick">quick</code></td>
<td>
<p>indicator (logical), whether a quick, but less accurate computation of posterior probabalities should be used or not.</p>
</td></tr>
<tr><td><code id="predict.locpvs_+3A_return.subclass.prediction">return.subclass.prediction</code></td>
<td>
<p>indicator (logical), whether the returned object includes posterior probabilities for each date in each subclass </p>
</td></tr>
<tr><td><code id="predict.locpvs_+3A_...">...</code></td>
<td>
<p>Further arguments are passed to underlying <code>predict</code> calls.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Posterior probabilities are predicted as if object is a standard &lsquo;<code>pvs</code>&rsquo;-model with the subclasses as classes. Then the posterior probabalities are summed over all subclasses for each class. The class with the highest value becomes the prediction.
</p>
<p>If &ldquo;<code>quick=FALSE</code>&rdquo; the posterior probabilites for each case are computed using the pairwise coupling algorithm presented by Hastie, Tibshirani (1998). If &ldquo;<code>quick=FALSE</code>&rdquo; a much quicker solution is used, which leads to less accurate posterior probabalities. In almost all cases it doesn't has a negative effect on the classification result.
</p>


<h3>Value</h3>

<p>a list with components:
</p>
<table role = "presentation">
<tr><td><code>class</code></td>
<td>
<p>the predicted (upper) classes</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>posterior probabilities for the (upper) classes</p>
</td></tr>
<tr><td><code>subclass.posteriors</code></td>
<td>
<p>(only if &ldquo;<code>return.subclass.prediction=TRUE</code>&rdquo;. A matrix containing posterior probabalities for the subclasses. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gero Szepannek, <a href="mailto:szepannek@statistik.tu-dortmund.de">szepannek@statistik.tu-dortmund.de</a>, Christian Neumann</p>


<h3>References</h3>

<p>Szepannek, G. and Weihs, C. (2006) Local Modelling in Classification on Different Feature Subspaces. 
In <em>Advances in Data Mining.</em>, ed Perner, P., LNAI 4065, pp. 226-234. Springer, Heidelberg.</p>


<h3>See Also</h3>

<p><code><a href="#topic+locpvs">locpvs</a></code> for learning &lsquo;<code>locpvs</code>&rsquo;-models and examples for applying this predict method, <code><a href="#topic+pvs">pvs</a></code> for pairwise variable selection without modeling subclasses, <code><a href="#topic+predict.pvs">predict.pvs</a></code> for predicting &lsquo;<code>pvs</code>&rsquo;-models 
</p>

<hr>
<h2 id='predict.meclight'>Prediction of Minimal Error Classification</h2><span id='topic+predict.meclight'></span>

<h3>Description</h3>

<p>Classify multivariate observations in conjunction with <code><a href="#topic+meclight">meclight</a></code> and
<code><a href="MASS.html#topic+lda">lda</a></code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'meclight'
predict(object, newdata,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.meclight_+3A_object">object</code></td>
<td>
<p>Object of class <code><a href="#topic+meclight">meclight</a></code>.</p>
</td></tr>
<tr><td><code id="predict.meclight_+3A_newdata">newdata</code></td>
<td>
<p>Data frame of cases to be classified or, 
if object has a formula, a data frame with columns of the same names
as the variables used. A vector will be interpreted as a row
vector.</p>
</td></tr>
<tr><td><code id="predict.meclight_+3A_...">...</code></td>
<td>
<p>currently ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Classify multivariate observations in conjunction with <code><a href="#topic+meclight">meclight</a></code> and
<code><a href="MASS.html#topic+lda">lda</a></code>. 
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>class</code></td>
<td>
<p>The estimated class (<code>factor</code>).</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>Posterior probabilities for the classes.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>References</h3>

<p>Roehl, M.C., Weihs, C., and Theis, W. (2002): 
Direct Minimization in Multivariate Classification. <em>Computational Statistics</em>, 17, 29-46.</p>


<h3>See Also</h3>

<p><code><a href="#topic+meclight">meclight</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
meclight.obj &lt;- meclight(Species ~ ., data = iris)
predict(meclight.obj, iris)
</code></pre>

<hr>
<h2 id='predict.NaiveBayes'>Naive Bayes Classifier</h2><span id='topic+predict.NaiveBayes'></span>

<h3>Description</h3>

<p>Computes the conditional a-posterior probabilities of a categorical
class variable given independent predictor variables using
the Bayes rule.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'NaiveBayes'
predict(object, newdata, threshold = 0.001, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.NaiveBayes_+3A_object">object</code></td>
<td>
<p>An object of class <code>"naiveBayes"</code>.</p>
</td></tr>
<tr><td><code id="predict.NaiveBayes_+3A_newdata">newdata</code></td>
<td>
<p>A dataframe with new predictors.</p>
</td></tr>
<tr><td><code id="predict.NaiveBayes_+3A_threshold">threshold</code></td>
<td>
<p>Value replacing cells with 0 probabilities.</p>
</td></tr>
<tr><td><code id="predict.NaiveBayes_+3A_...">...</code></td>
<td>
<p>passed to <code><a href="#topic+dkernel">dkernel</a></code> function if neccessary.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implementation of Naive Bayes as well as this help is based on the code by 
David Meyer in the package e1071 but extended for kernel estimated densities.
The standard naive Bayes classifier (at least this implementation)
assumes independence of the predictor
variables. For attributes with missing values, the
corresponding table entries are omitted for prediction.
</p>


<h3>Value</h3>

<p>A list with the conditional a-posterior
probabilities for each class and the estimated class are returned.
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+NaiveBayes">NaiveBayes</a></code>,<code><a href="#topic+dkernel">dkernel</a></code><code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code>,<code><a href="MASS.html#topic+qda">qda</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
m &lt;- NaiveBayes(Species ~ ., data = iris)
predict(m)
</code></pre>

<hr>
<h2 id='predict.pvs'>predict method for pvs objects</h2><span id='topic+predict.pvs'></span>

<h3>Description</h3>

<p>Prediction of class membership and posterior probabilities using pairwise variable selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pvs'
predict(object, newdata, quick = FALSE, detail = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.pvs_+3A_object">object</code></td>
<td>
<p>an object of class &lsquo;<code>pvs</code>&rsquo;, as that created by the function &ldquo;<code><a href="#topic+pvs">pvs</a></code>&rdquo; </p>
</td></tr>
<tr><td><code id="predict.pvs_+3A_newdata">newdata</code></td>
<td>
<p>a data frame or matrix containing new data. If not given the same datas as used for training the &lsquo;<code>pvs</code>&rsquo;-model are used. </p>
</td></tr>
<tr><td><code id="predict.pvs_+3A_quick">quick</code></td>
<td>
<p>indicator (logical), whether a quick, but less accurate computation of posterior probabalities should be used or not.</p>
</td></tr>
<tr><td><code id="predict.pvs_+3A_detail">detail</code></td>
<td>
<p>indicator (logical), whether the returned object includes additional information about the posterior probabilities for each date in each submodel.</p>
</td></tr>
<tr><td><code id="predict.pvs_+3A_...">...</code></td>
<td>
<p>Further arguments are passed to underlying <code>predict</code> calls.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If &ldquo;<code>quick=FALSE</code>&rdquo; the posterior probabilites for each case are computed using the pairwise coupling algorithm presented by Hastie, Tibshirani (1998). 
If &ldquo;<code>quick=FALSE</code>&rdquo; a much quicker solution is used, which leads to less accurate posterior probabalities. 
In almost all cases it doesn't has a negative effect on the classification result.
</p>


<h3>Value</h3>

<p>a list with components:
</p>
<table role = "presentation">
<tr><td><code>class</code></td>
<td>
<p>the predicted classes</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>posterior probabilities for the classes</p>
</td></tr>
<tr><td><code>details</code></td>
<td>
<p>(only if &ldquo;<code>details=TRUE</code>&rdquo;. A list containing matrices of posterior probabalities computated by the classification method for each case and classpair. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gero Szepannek, <a href="mailto:szepannek@statistik.tu-dortmund.de">szepannek@statistik.tu-dortmund.de</a>, Christian Neumann</p>


<h3>References</h3>

<p>Szepannek, G. and Weihs, C. (2006)  Variable Selection for Classification of More than Two 
Classes Where the Data are Sparse. In <em>From Data and Information Analysis to Kwnowledge Engineering.</em>,
eds Spiliopolou, M., Kruse, R., Borgelt, C., Nuernberger, A. and Gaul, W. pp. 700-708. Springer, Heidelberg.</p>


<h3>See Also</h3>

<p>For more details and examples how to use this predict method, see <code><a href="#topic+pvs">pvs</a></code>.
</p>

<hr>
<h2 id='predict.rda'>Regularized Discriminant Analysis (RDA)</h2><span id='topic+predict.rda'></span>

<h3>Description</h3>

<p>Classifies new observations using parameters determined by 
the <code>rda</code>-function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rda'
predict(object, newdata, posterior = TRUE, 
    aslist = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.rda_+3A_object">object</code></td>
<td>
<p>Object of class <code>rda</code>.</p>
</td></tr>
<tr><td><code id="predict.rda_+3A_newdata">newdata</code></td>
<td>
<p>Data frame (or matrix) of cases to be classified.</p>
</td></tr>
<tr><td><code id="predict.rda_+3A_posterior">posterior</code></td>
<td>
<p>Logical; indicates whether a matrix of 
posterior probabilites over all classes for each observation 
shall be returned in addition to classifications.</p>
</td></tr>
<tr><td><code id="predict.rda_+3A_aslist">aslist</code></td>
<td>
<p>Logical; if <code>TRUE</code>, a list
containing classifications and posterior probabilities is returned, 
otherwise a vector with an attribute &lsquo;<code>posterior</code>&rsquo;.</p>
</td></tr>
<tr><td><code id="predict.rda_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Depends on the value of argument &lsquo;<code>aslist</code>&rsquo;:
</p>
<p>Either a vector (of class <code>factor</code>) of classifications 
that (optionally) has an attribute &lsquo;<code>posterior</code>&rsquo; 
containing the posterior probability matrix, or
</p>
<p>A list with elements &lsquo;<code>class</code>&rsquo; and &lsquo;<code>posterior</code>&rsquo;.
</p>


<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+rda">rda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- rda(Species ~ ., data = iris, gamma = 0.05, lambda = 0.2)
predict(x, iris[, 1:4])
</code></pre>

<hr>
<h2 id='predict.sknn'>Simple k Nearest Neighbours Classification</h2><span id='topic+predict.sknn'></span>

<h3>Description</h3>

<p>Classifies new observations using the sknn learned by 
the <code>sknn</code>-function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sknn'
predict(object, newdata,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.sknn_+3A_object">object</code></td>
<td>
<p>Object of class <code><a href="#topic+sknn">sknn</a></code>.</p>
</td></tr>
<tr><td><code id="predict.sknn_+3A_newdata">newdata</code></td>
<td>
<p>Data frame (or matrix) of cases to be classified.</p>
</td></tr>
<tr><td><code id="predict.sknn_+3A_...">...</code></td>
<td>
<p>...</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements &lsquo;<code>class</code>&rsquo; and &lsquo;<code>posterior</code>&rsquo;.
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+sknn">sknn</a></code>, <code><a href="class.html#topic+knn">knn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- sknn(Species ~ ., data = iris)
predict(x, iris)
x &lt;- sknn(Species ~ ., gamma = 10, kn = 10, data = iris)
predict(x, iris)
</code></pre>

<hr>
<h2 id='predict.svmlight'>Interface to SVMlight</h2><span id='topic+predict.svmlight'></span>

<h3>Description</h3>

<p>Predicts new observations using the SVM learned by 
the <code>svmlight</code>-function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'svmlight'
predict(object, newdata, scal = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.svmlight_+3A_object">object</code></td>
<td>
<p>Object of class <code><a href="#topic+svmlight">svmlight</a></code>.</p>
</td></tr>
<tr><td><code id="predict.svmlight_+3A_newdata">newdata</code></td>
<td>
<p>Data frame (or matrix) of cases to be predicted.</p>
</td></tr>
<tr><td><code id="predict.svmlight_+3A_scal">scal</code></td>
<td>
<p>Logical, whether to scale membership values via <code><a href="#topic+e.scal">e.scal</a></code>.</p>
</td></tr>
<tr><td><code id="predict.svmlight_+3A_...">...</code></td>
<td>
<p>...</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If a classification is learned (<code>type="C"</code>) in <code><a href="#topic+svmlight">svmlight</a></code> a 
list with elements &lsquo;<code>class</code>&rsquo; and &lsquo;<code>posterior</code>&rsquo; (scaled, if <code>scal = TRUE</code>).
</p>
<p>If a Regression is learned (<code>type="R"</code>) in <code><a href="#topic+svmlight">svmlight</a></code> the predicted values.
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+svmlight">svmlight</a></code>, <code><a href="e1071.html#topic+svm">svm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(iris)
x &lt;- svmlight(Species ~ ., data = iris)
predict(x, iris)

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.woe'>Weights of evidence</h2><span id='topic+predict.woe'></span>

<h3>Description</h3>

<p>Applies weight of evidence transform of factor variables for binary classification based on a model of class <code><a href="#topic+woe">woe</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'woe'
predict(object, newdata, replace = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.woe_+3A_object">object</code></td>
<td>
<p>Object resulting from a call of <code>woe</code>.</p>
</td></tr>
<tr><td><code id="predict.woe_+3A_newdata">newdata</code></td>
<td>
<p>A matrix or data frame where WOE transform should be applied of the same dimension as the data used for training the <code>woe</code> object.</p>
</td></tr>
<tr><td><code id="predict.woe_+3A_replace">replace</code></td>
<td>
<p>Logical flag specifying whether the original factor variables should be kept in the output.</p>
</td></tr>
<tr><td><code id="predict.woe_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>        
</table>


<h3>Value</h3>

<p>Data frame including the transformed numeric <code>woe</code> variables. 
</p>


<h3>Author(s)</h3>

<p>Gero Szepannek</p>


<h3>References</h3>

<p>Good, I. (1950): <em>Probability and the Weighting of Evidences.</em> Charles Griffin, London.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+woe">woe</a></code>, <code><a href="#topic+plot.woe">plot.woe</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># see examples in ?woe
</code></pre>

<hr>
<h2 id='pvs'>Pairwise variable selection for classification</h2><span id='topic+pvs'></span><span id='topic+pvs.default'></span><span id='topic+pvs.formula'></span><span id='topic+print.pvs'></span>

<h3>Description</h3>

<p>Pairwise variable selection for numerical data, allowing the use of different classifiers and different variable selection methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pvs(x, ...)

## Default S3 method:
pvs(x, grouping, prior=NULL, method="lda", 
    vs.method=c("ks.test","stepclass","greedy.wilks"), niveau=0.05, 
    fold=10, impr=0.1, direct="backward", out=FALSE, ...)
    
## S3 method for class 'formula'
pvs(formula, data = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pvs_+3A_x">x</code></td>
<td>
<p>matrix or data frame containing the explanatory variables 
(required, if <code>formula</code> is not given). x must consist of numerical data only. </p>
</td></tr>
<tr><td><code id="pvs_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code>. 
That is, the response is the grouping factor (the classes) and the right hand side 
specifies the (numerical) discriminators. 
Interaction terms are not supported.</p>
</td></tr>
<tr><td><code id="pvs_+3A_data">data</code></td>
<td>
<p>data matrix (rows=cases, columns=variables)</p>
</td></tr>
<tr><td><code id="pvs_+3A_grouping">grouping</code></td>
<td>
<p>class indicator vector (a factor)</p>
</td></tr>
<tr><td><code id="pvs_+3A_prior">prior</code></td>
<td>
<p>prior probabilites for the classes. If not specified the prior probabilities will be set according to proportion in &ldquo;grouping&rdquo;. If specified the order of prior 
probabilities must be the same as in &ldquo;grouping&rdquo;. </p>
</td></tr>
<tr><td><code id="pvs_+3A_method">method</code></td>
<td>
<p>character, name of classification function (e.g. &ldquo;<code><a href="MASS.html#topic+lda">lda</a></code>&rdquo; (default)).</p>
</td></tr>
<tr><td><code id="pvs_+3A_vs.method">vs.method</code></td>
<td>
<p>character, name of variable selection method. Must be one of &ldquo;<code><a href="stats.html#topic+ks.test">ks.test</a></code>&rdquo; (default),
&ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; or &ldquo;<code><a href="#topic+greedy.wilks">greedy.wilks</a></code>&rdquo;. </p>
</td></tr>
<tr><td><code id="pvs_+3A_niveau">niveau</code></td>
<td>
<p>used niveau for &ldquo;<code><a href="stats.html#topic+ks.test">ks.test</a></code>&rdquo;</p>
</td></tr>
<tr><td><code id="pvs_+3A_fold">fold</code></td>
<td>
<p>parameter for cross-validation, if &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; is chosen &lsquo;<code>vs.method</code>&rsquo;</p>
</td></tr>                                            
<tr><td><code id="pvs_+3A_impr">impr</code></td>
<td>
<p>least improvement of performance measure desired to include or exclude any variable (&lt;=1), if &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; is chosen &lsquo;<code>vs.method</code>&rsquo; </p>
</td></tr>  
<tr><td><code id="pvs_+3A_direct">direct</code></td>
<td>
<p>direction of variable selection, if &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; is chosen &lsquo;<code>vs.method</code>&rsquo;. 
Must be one if &ldquo;<code>forward</code>&rdquo;, &ldquo;<code>backward</code>&rdquo; (default) or &ldquo;<code>both</code>&rdquo;. </p>
</td></tr>
<tr><td><code id="pvs_+3A_out">out</code></td>
<td>
<p>indicator (logical) for textoutput during computation (slows down computation!), if &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; is chosen &lsquo;<code>vs.method</code>&rsquo; </p>
</td></tr>
<tr><td><code id="pvs_+3A_...">...</code></td>
<td>
<p>further parameters passed to classification function (&lsquo;<code>method</code>&rsquo;) or variable selection method (&lsquo;<code>vs.method</code>&rsquo;) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The classification &ldquo;method&rdquo; (e.g. &lsquo;<code><a href="MASS.html#topic+lda">lda</a></code>&rsquo;) must have its own 
&lsquo;<code>predict</code>&rsquo; method (like &lsquo;<code><a href="MASS.html#topic+predict.lda">predict.lda</a></code>&rsquo; for &lsquo;<code>lda</code>&rsquo;) 
returns a list with an element &lsquo;<code>posterior</code>&rsquo; containing the posterior probabilties. It must be able to deal with matrices as in <code>method(x, grouping, ...)</code>. 
Examples of such classification methods are &lsquo;<code><a href="MASS.html#topic+lda">lda</a></code>&rsquo;, &lsquo;<code><a href="MASS.html#topic+qda">qda</a></code>&rsquo;, &lsquo;<code><a href="#topic+rda">rda</a></code>&rsquo;, 
&lsquo;<code><a href="#topic+NaiveBayes">NaiveBayes</a></code>&rsquo; or &lsquo;<code><a href="#topic+sknn">sknn</a></code>&rsquo;.\
For the classification methods &ldquo;<code><a href="e1071.html#topic+svm">svm</a></code>&rdquo; and &ldquo;<code><a href="randomForest.html#topic+randomForest">randomForest</a></code>&rdquo; there are special routines implemented, to make them work with &lsquo;<code>pvs</code>&rsquo; method even though their &lsquo;<code>predict</code>&rsquo; methods don't provide the demanded posteriors. However those two classfiers can not be used together with variable selection method &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo;.
</p>
<p>&lsquo;<code>pvs</code>&rsquo; performs a variable selection using the selection method chosen in &lsquo;<code>vs.method</code>&rsquo; for each pair of classes in &lsquo;<code>x</code>&rsquo;. 
Then for each pair of classes a submodel using &lsquo;<code>method</code>&rsquo; is trained (using only the earlier selected variables for this class-pair).
</p>
<p>If &lsquo;<code>method</code>&rsquo; is &ldquo;<code><a href="stats.html#topic+ks.test">ks.test</a></code>&rdquo;, then for each variable the empirical distribution functions of the cases of both classes are compared via &ldquo;<code><a href="stats.html#topic+ks.test">ks.test</a></code>&rdquo;. Only variables with a p-values below &lsquo;<code>niveau</code>&rsquo; are used for training the submodel for this pair of classes.
</p>
<p>If &lsquo;<code>method</code>&rsquo; is &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; the variable selection is performed using the &ldquo;<code><a href="#topic+stepclass">stepclass</a></code>&rdquo; method.
</p>
<p>If &lsquo;<code>method</code>&rsquo; is &ldquo;<code><a href="#topic+greedy.wilks">greedy.wilks</a></code>&rdquo; the variable selection is performed using Wilk's lambda criterion.
</p>


<h3>Value</h3>

<p>An object of class &lsquo;<code>pvs</code>&rsquo; containing the following components:
</p>
<table role = "presentation">
<tr><td><code>classes</code></td>
<td>
<p>the classes in grouping</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>used prior probabilities</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>name of used classification function</p>
</td></tr>
<tr><td><code>vs.method</code></td>
<td>
<p>name of used function for variable selection</p>
</td></tr>
<tr><td><code>submodels</code></td>
<td>
<p>containing a list of submodels. For each pair of classes there is a list element being another list of 3 containing the class-pair of this submodel, the selected variables 
for the subspace of classes and the result of the trained classification function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the (matched) function call</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gero Szepannek, <a href="mailto:szepannek@statistik.tu-dortmund.de">szepannek@statistik.tu-dortmund.de</a>, Christian Neumann</p>


<h3>References</h3>


<ul>
<li><p> Szepannek, G. and Weihs, C. (2006)  Variable Selection for Classification of More than Two 
Classes Where the Data are Sparse. In <em>From Data and Information Analysis to Kwnowledge Engineering.</em>,
eds Spiliopolou, M., Kruse, R., Borgelt, C., Nuernberger, A. and Gaul, W. pp. 700-708. Springer, Heidelberg.
</p>
</li>
<li><p> Szepannek, G. (2008): Different Subspace Classification - Datenanalyse, -interpretation, -visualisierung und 
Vorhersage in hochdimensionalen Raeumen, ISBN 978-3-8364-6302-7, vdm, Saarbruecken.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.pvs">predict.pvs</a></code> for predicting &lsquo;<code>pvs</code>&rsquo; models and <code><a href="#topic+locpvs">locpvs</a></code> for pairwisevariable selection in local models of several subclasses
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example 1: learn an "lda" model on the waveform data using pairwise variable 
## selection (pvs) using "ks.test" and compare it to using lda without pvs 

library("mlbench")
trainset &lt;- mlbench.waveform(300) 
pvsmodel &lt;- pvs(trainset$x, trainset$classes, niveau=0.05) # default: using method="lda"
## short summary, showing the class-pairs of the submodels and the selected variables
pvsmodel
 
testset &lt;-  mlbench.waveform(500)
## prediction of the test data set: 
prediction &lt;- predict(pvsmodel, testset$x)

## calculating the test error rate
1-sum(testset$classes==prediction$class)/length(testset$classes)
## Bayes error is 0.149

## comparison to performance of simple lda
ldamodel &lt;- lda(trainset$x, trainset$classes)
LDAprediction &lt;- predict(ldamodel, testset$x)

## test error rate
1-sum(testset$classes==LDAprediction$class)/length(testset$classes)


## Example 2: learn a "qda" model with pvs on half of the Satellite dataset, 
## using "ks.test" 


library("mlbench")
data("Satellite")

## takes few seconds as exact KS tests are calculated here:
model &lt;- pvs(classes ~ ., Satellite[1:3218,], method="qda", vs.method="ks.test")
## short summary, showing the class-pairs of the submodels and the selected variables
model 

## now predict on the rest of the data set:
## pred &lt;- predict(model,Satellite[3219:6435,]) # takes some time
pred &lt;- predict(model,Satellite[3219:6435,], quick=TRUE) # that's much quicker

## now you can look at the predicted classes:
pred$class 
## or the posterior probabilities:
pred$posterior

</code></pre>

<hr>
<h2 id='quadplot'>Plotting of 4 dimensional membership representation simplex</h2><span id='topic+quadplot'></span>

<h3>Description</h3>

<p>For a 4 class discrimination problem the membership values of each class are visualized in
a 3 dimensional barycentric coordinate system.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quadplot(e = NULL, f = NULL, g = NULL, h = NULL, angle = 75, 
    scale.y = 0.6, label = 1:4, labelcol = rainbow(4), 
    labelpch = 19, labelcex = 1.5, main = "", s3d.control = list(), 
    simplex.control = list(), legend.control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="quadplot_+3A_e">e</code></td>
<td>
<p>either a matrix with 4 columns represanting the membership values or a 
vector with the membership values of the first class</p>
</td></tr>
<tr><td><code id="quadplot_+3A_f">f</code></td>
<td>
<p>vector with the membership values of the second class</p>
</td></tr>
<tr><td><code id="quadplot_+3A_g">g</code></td>
<td>
<p>vector with the membership values of the third class</p>
</td></tr>
<tr><td><code id="quadplot_+3A_h">h</code></td>
<td>
<p>vector with the membership values of the forth class</p>
</td></tr>
<tr><td><code id="quadplot_+3A_angle">angle</code></td>
<td>
<p>angle between x and y axis </p>
</td></tr>
<tr><td><code id="quadplot_+3A_scale.y">scale.y</code></td>
<td>
<p>scale of y axis related to x- and z axis</p>
</td></tr>
<tr><td><code id="quadplot_+3A_label">label</code></td>
<td>
<p>label for the classes </p>
</td></tr>
<tr><td><code id="quadplot_+3A_labelcol">labelcol</code></td>
<td>
<p>colors to use for the labels</p>
</td></tr>
<tr><td><code id="quadplot_+3A_labelpch">labelpch</code></td>
<td>
<p><code>pch</code> for the labels</p>
</td></tr>
<tr><td><code id="quadplot_+3A_labelcex">labelcex</code></td>
<td>
<p><code>cex</code> for the labels</p>
</td></tr>
<tr><td><code id="quadplot_+3A_main">main</code></td>
<td>
<p>main title of the plot</p>
</td></tr>
<tr><td><code id="quadplot_+3A_s3d.control">s3d.control</code></td>
<td>
<p>a <em>list</em> with further arguments passed to the underlying 
<code><a href="scatterplot3d.html#topic+scatterplot3d">scatterplot3d</a></code> function call that sets up the plot</p>
</td></tr>
<tr><td><code id="quadplot_+3A_simplex.control">simplex.control</code></td>
<td>
<p>a <em>list</em> with further arguments passed to the underlying 
function call that draws the barycentric coordinate system</p>
</td></tr>
<tr><td><code id="quadplot_+3A_legend.control">legend.control</code></td>
<td>
<p>a <em>list</em> with further arguments passed to the underlying 
function call that adds the <code><a href="graphics.html#topic+legend">legend</a></code></p>
</td></tr>
<tr><td><code id="quadplot_+3A_...">...</code></td>
<td>
<p>further arguments passed to the underlying <code>plot</code> function that draws the data points</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The membership values are calculated with <code><a href="#topic+quadtrafo">quadtrafo</a></code> and plotted 
with <code><a href="scatterplot3d.html#topic+scatterplot3d">scatterplot3d</a></code>.
</p>


<h3>Value</h3>

<p>A <code><a href="scatterplot3d.html#topic+scatterplot3d">scatterplot3d</a></code> object.
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a>, and Uwe Ligges</p>


<h3>References</h3>

<p>Garczarek, Ursula Maria (2002): Classification rules in standardized partition spaces.
Dissertation, University of Dortmund. 
URL <a href="http://hdl.handle.net/2003/2789">http://hdl.handle.net/2003/2789</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+triplot">triplot</a></code>, <code><a href="scatterplot3d.html#topic+scatterplot3d">scatterplot3d</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library("MASS")
data(B3)
opar &lt;- par(mfrow = c(1, 2), pty = "s")
posterior &lt;- predict(lda(PHASEN ~ ., data = B3))$post
s3d &lt;- quadplot(posterior, col = rainbow(4)[B3$PHASEN], 
        labelpch = 22:25, labelcex = 0.8,
        pch = (22:25)[apply(posterior, 1, which.max)], 
        main = "LDA posterior assignments")
quadlines(centerlines(4), sp = s3d, lty = "dashed")

posterior &lt;- predict(qda(PHASEN ~ ., data = B3))$post
s3d &lt;- quadplot(posterior, col = rainbow(4)[B3$PHASEN], 
        labelpch = 22:25, labelcex = 0.8,
        pch = (22:25)[apply(posterior, 1, which.max)],
        main = "QDA posterior assignments")
quadlines(centerlines(4), sp = s3d, lty = "dashed")
par(opar)
</code></pre>

<hr>
<h2 id='quadtrafo'>Transforming of 4 dimensional values in a barycentric coordinate system.</h2><span id='topic+quadtrafo'></span><span id='topic+quadpoints'></span><span id='topic+quadlines'></span>

<h3>Description</h3>

<p>Transforming of 4 dimensional values in a barycentric coordinate system.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quadtrafo(e, f = NULL, g = NULL, h = NULL)
        
quadlines(e, f = NULL, g = NULL, h = NULL, sp, ...)

quadpoints(e, f = NULL, g = NULL, h = NULL, sp, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="quadtrafo_+3A_e">e</code></td>
<td>
<p>either a matrix with 4 columns represanting the membership values or a 
vector with the membership values of the first class</p>
</td></tr>
<tr><td><code id="quadtrafo_+3A_f">f</code></td>
<td>
<p>vector with the membership values of the second class</p>
</td></tr>
<tr><td><code id="quadtrafo_+3A_g">g</code></td>
<td>
<p>vector with the membership values of the third class</p>
</td></tr>
<tr><td><code id="quadtrafo_+3A_h">h</code></td>
<td>
<p>vector with the membership values of the forth class</p>
</td></tr>
<tr><td><code id="quadtrafo_+3A_sp">sp</code></td>
<td>
<p><code>scatterplot3d</code> object to which <code>points</code> or <code>lines</code> should be plotted</p>
</td></tr>
<tr><td><code id="quadtrafo_+3A_...">...</code></td>
<td>
<p>further arguments passed to the underlyind <code>plot</code> functions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>quadtrafo</code> the 3 dimensional values in the barycentrix coordinate system. 
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+triplot">triplot</a></code>, <code><a href="#topic+quadplot">quadplot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MASS)
data(B3)
posterior &lt;- predict(lda(PHASEN ~ ., data = B3))$post
quadtrafo(posterior)
</code></pre>

<hr>
<h2 id='rda'>Regularized Discriminant Analysis (RDA)</h2><span id='topic+rda'></span><span id='topic+rda.default'></span><span id='topic+rda.formula'></span><span id='topic+plot.rda'></span><span id='topic+print.rda'></span>

<h3>Description</h3>

<p>Builds a classification rule using regularized group covariance 
matrices that are supposed to be more robust against 
multicollinearity in the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rda(x, ...)

## Default S3 method:
rda(x, grouping = NULL, prior = NULL, gamma = NA, 
    lambda = NA, regularization = c(gamma = gamma, lambda = lambda), 
    crossval = TRUE, fold = 10, train.fraction = 0.5, 
    estimate.error = TRUE, output = FALSE, startsimplex = NULL, 
    max.iter = 100, trafo = TRUE, simAnn = FALSE, schedule = 2, 
    T.start = 0.1, halflife = 50, zero.temp = 0.01, alpha = 2, 
    K = 100, ...)
## S3 method for class 'formula'
rda(formula, data, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rda_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the explanatory variables 
(required, if <code>formula</code> is not given).</p>
</td></tr>
<tr><td><code id="rda_+3A_formula">formula</code></td>
<td>
<p>Formula of the form &lsquo;<code>groups ~ x1 + x2 + ...</code>&rsquo;.</p>
</td></tr>
<tr><td><code id="rda_+3A_data">data</code></td>
<td>
<p>A data frame (or matrix) containing the explanatory 
variables.</p>
</td></tr>
<tr><td><code id="rda_+3A_grouping">grouping</code></td>
<td>
<p>(Optional) a vector specifying the class for 
each observation; if not specified, the first column of 
&lsquo;<code>data</code>&rsquo; is taken.</p>
</td></tr>
<tr><td><code id="rda_+3A_prior">prior</code></td>
<td>
<p>(Optional) prior probabilities for the classes. 
Default: proportional to training sample sizes. 
&ldquo;<code>prior=1</code>&rdquo; indicates equally likely classes.</p>
</td></tr>
<tr><td><code id="rda_+3A_gamma">gamma</code>, <code id="rda_+3A_lambda">lambda</code>, <code id="rda_+3A_regularization">regularization</code></td>
<td>

<p>One or both of the rda-parameters may be fixed manually. 
Unspecified parameters are determined by minimizing the 
estimated error rate (see below).</p>
</td></tr>
<tr><td><code id="rda_+3A_crossval">crossval</code></td>
<td>
<p>Logical. If <code>TRUE</code>, in the optimization 
step the error rate is estimated by Cross-Validation, 
otherwise by drawing several training- and test-samples.</p>
</td></tr>
<tr><td><code id="rda_+3A_fold">fold</code></td>
<td>
<p>The number of Cross-Validation- or Bootstrap-samples
to be drawn.</p>
</td></tr>
<tr><td><code id="rda_+3A_train.fraction">train.fraction</code></td>
<td>
<p>In case of Bootstrapping: the fraction of 
the data to be used for training in each Bootstrap-sample; 
the remainder is used to estimate the misclassification rate.</p>
</td></tr>
<tr><td><code id="rda_+3A_estimate.error">estimate.error</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the apparent 
error rate for the final parameter set is estimated.</p>
</td></tr>
<tr><td><code id="rda_+3A_output">output</code></td>
<td>
<p>Logical flag to indicate whether text output 
during computation is desired.</p>
</td></tr>
<tr><td><code id="rda_+3A_startsimplex">startsimplex</code></td>
<td>
<p>(Optional) a starting simplex for the 
Nelder-Mead-minimization.</p>
</td></tr>
<tr><td><code id="rda_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations for Nelder-Mead.</p>
</td></tr>
<tr><td><code id="rda_+3A_trafo">trafo</code></td>
<td>
<p>Logical; indicates whether minimization is carrried 
out using transformed parameters.</p>
</td></tr>
<tr><td><code id="rda_+3A_simann">simAnn</code></td>
<td>
<p>Logical; indicates whether Simulated Annealing 
shall be used.</p>
</td></tr>
<tr><td><code id="rda_+3A_schedule">schedule</code></td>
<td>
<p>Annealing schedule 1 or 2 (exponential or polynomial).</p>
</td></tr>
<tr><td><code id="rda_+3A_t.start">T.start</code></td>
<td>
<p>Starting temperature for Simulated Annealing.</p>
</td></tr>
<tr><td><code id="rda_+3A_halflife">halflife</code></td>
<td>
<p>Number of iterations until temperature is reduced to a half  (schedule 1).</p>
</td></tr>
<tr><td><code id="rda_+3A_zero.temp">zero.temp</code></td>
<td>
<p>Temperature at which it is set to zero  (schedule 1).</p>
</td></tr> 
<tr><td><code id="rda_+3A_alpha">alpha</code></td>
<td>
<p>Power of temperature reduction (linear, quadratic, cubic,...)  (schedule 2).</p>
</td></tr>
<tr><td><code id="rda_+3A_k">K</code></td>
<td>
<p>Number of iterations until temperature = 0  (schedule 2).</p>
</td></tr>
<tr><td><code id="rda_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>J.H. Friedman (see references below) suggested a method to fix 
almost singular covariance matrices in discriminant analysis. 
Basically, individual covariances as in QDA are used, but 
depending on two parameters (<code class="reqn">\gamma</code> and 
<code class="reqn">\lambda</code>), these can be shifted towards a 
diagonal matrix and/or the pooled covariance 
matrix. For (<code class="reqn">\gamma=0</code>, <code class="reqn">\lambda=0</code>) it equals QDA, 
for (<code class="reqn">\gamma=0</code>, <code class="reqn">\lambda=1</code>) it equals LDA.
</p>
<p>You may fix these parameters at certain values or leave it to 
the function to try to find &ldquo;optimal&rdquo; values. If one 
parameter is given, the other one is determined using the 
R-function &lsquo;<code><a href="stats.html#topic+optimize">optimize</a></code>&rsquo;. If no parameter is 
given, both are determined numerically by a 
Nelder-Mead-(Simplex-)algorithm with the option of using 
Simulated Annealing.
The goal function to be minimized is the (estimated) 
misclassification rate; the misclassification rate is estimated 
either by Cross-Validation or by repeatedly dividing the data 
into training- and test-sets (Boostrapping).
</p>
<p><em>Warning</em>: If these sets are small, optimization is expected 
to produce almost random results. We recommend to adjust the 
parameters manually in such a case.
In all other cases it is recommended to run the optimization 
several times in order to see whether stable results are gained.
</p>
<p>Since the Nelder-Mead-algorithm is actually intended for 
<em>continuous</em> functions while the observed error rate 
by its nature is <em>discrete</em>, a greater number of 
Boostrap-samples might improve the optimization by increasing 
the smoothness of the response surface (and, of course, by 
reducing variance and bias). 
If a set of parameters leads to singular covariance 
matrices, a penalty term is added to the misclassification rate 
which will hopefully help to maneuver back out of singularity
(so do not worry about error rates greater than one during 
optimization).
</p>


<h3>Value</h3>

<p>A list of class <code>rda</code> containing the following 
components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td></tr>
<tr><td><code>regularization</code></td>
<td>
<p>vector containing the two regularization 
parameters (gamma, lambda)</p>
</td></tr>
<tr><td><code>classes</code></td>
<td>
<p>the names of the classes</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>the prior probabilities for the classes</p>
</td></tr>
<tr><td><code>error.rate</code></td>
<td>
<p>apparent error rate (if computation 
was not suppressed), and, if any optimization took place, the
final (cross-validated or bootstrapped) error rate estimate as 
well.</p>
</td></tr>
<tr><td><code>means</code></td>
<td>
<p>Group means.</p>
</td></tr>
<tr><td><code>covariances</code></td>
<td>
<p>Array of group covariances.</p>
</td></tr>
<tr><td><code>covpooled</code></td>
<td>
<p>Pooled covariance.</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>(Logical) indicator of convergence (only for 
Nelder-Mead).</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of iterations actually performed (only for 
Nelder-Mead).</p>
</td></tr>
</table>


<h3>More details</h3>

<p>The explicit defintion of <code class="reqn">\gamma</code>,
<code class="reqn">\lambda</code> and the resulting covariance estimates
is as follows:
</p>
<p>The pooled covariance estimate <code class="reqn">\hat{\Sigma}</code> is 
given as well as the individual covariance estimates 
<code class="reqn">\hat{\Sigma}_k</code> for each group.
</p>
<p>First, using <code class="reqn">\lambda</code>, a convex combination of 
these two is computed:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\Sigma}_k (\lambda) := (1-\lambda) \hat{\Sigma}_k + \lambda \hat{\Sigma}.</code>
</p>

<p>Then, another convex combination is constructed using the 
above estimate and a (scaled) identity matrix:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\Sigma}_k (\lambda,\gamma) = (1-\gamma)\hat{\Sigma}_k(\lambda)+
\gamma\frac{1}{d}\mathrm{tr}[\hat{\Sigma}_k(\lambda)]\mathrm{I}.</code>
</p>

<p>The factor 
<code class="reqn">\frac{1}{d}\mathrm{tr}[\hat{\Sigma}_k(\lambda)]</code>
in front of the identity matrix I is the mean of the diagonal 
elements of 
<code class="reqn">\hat{\Sigma}_k(\lambda)</code>, so it is 
the mean variance of all <code class="reqn">d</code> variables assuming the group 
covariance <code class="reqn">\hat{\Sigma}_k(\lambda)</code>.
</p>
<p>For the four extremes of (<code class="reqn">\gamma</code>,<code class="reqn">\lambda</code>) 
the covariance structure reduces to special cases:
</p>

<ul>
<li><p> (<code class="reqn">\gamma=0</code>, <code class="reqn">\lambda=0</code>): 
QDA - individual covariance for each group.
</p>
</li>
<li><p> (<code class="reqn">\gamma=0</code>, <code class="reqn">\lambda=1</code>): 
LDA - a common covariance matrix.
</p>
</li>
<li><p> (<code class="reqn">\gamma=1</code>, <code class="reqn">\lambda=0</code>): 
Conditional independent variables - similar to Naive Bayes,
but variable variances within group (main diagonal elements) 
are equal.
</p>
</li>
<li><p> (<code class="reqn">\gamma=1</code>, <code class="reqn">\lambda=1</code>):
Classification using euclidean distance - as in previous case, 
but variances are the same for all groups. Objects are assigned 
to group with nearest mean.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>References</h3>

<p>Friedman, J.H. (1989): Regularized Discriminant Analysis.
In: <em>Journal of the American Statistical Association</em> 84, 
165-175.
</p>
<p>Press, W.H., Flannery, B.P., Teukolsky, S.A., Vetterling, W.T. (1992): 
<em>Numerical Recipes in C</em>.  Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.rda">predict.rda</a></code>,
<code><a href="MASS.html#topic+lda">lda</a></code>, <code><a href="MASS.html#topic+qda">qda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- rda(Species ~ ., data = iris, gamma = 0.05, lambda = 0.2)
predict(x, iris)
</code></pre>

<hr>
<h2 id='rerange'>Linear transformation of data</h2><span id='topic+rerange'></span>

<h3>Description</h3>

<p>The function performs a linear transformation of the data, such that
afterwards <code>range(data)=c(theMin,theMax)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rerange(data, min.goal = 0, max.goal = 1, min.data =
    min(data), max.data = max(data), center = NA)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rerange_+3A_data">data</code></td>
<td>
<p> vector with the <code>data</code> to transform </p>
</td></tr>
<tr><td><code id="rerange_+3A_min.goal">min.goal</code></td>
<td>
<p> new minimum value </p>
</td></tr>
<tr><td><code id="rerange_+3A_max.goal">max.goal</code></td>
<td>
<p> new maximum value </p>
</td></tr>
<tr><td><code id="rerange_+3A_min.data">min.data</code></td>
<td>
<p> old minimum value </p>
</td></tr>
<tr><td><code id="rerange_+3A_max.data">max.data</code></td>
<td>
<p> old maximum value </p>
</td></tr>
<tr><td><code id="rerange_+3A_center">center</code></td>
<td>
<p>which old value should become the new center <code>(max.goal + min.goal) / 2)</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the transformed data
</p>


<h3>Author(s)</h3>

<p>Dominik Reusser</p>


<h3>Examples</h3>

<pre><code class='language-R'>   klaR:::rerange(data=1:20)
   klaR:::rerange(data=1:30, center=5)
</code></pre>

<hr>
<h2 id='shardsplot'>Plotting Eight Direction Arranged Maps or Self-Organizing Maps</h2><span id='topic+shardsplot'></span><span id='topic+level_shardsplot'></span><span id='topic+plot.EDAM'></span>

<h3>Description</h3>

<p>Plotting method for objects of <code>class</code> <code><a href="#topic+EDAM">EDAM</a></code> or <code><a href="som.html#topic+som">som</a></code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shardsplot(object, plot.type = c("eight", "four", "points", "n"), 
    expand = 1, stck = TRUE, grd = FALSE, standardize = FALSE, 
    data.or = NA, label = FALSE, plot = TRUE, classes = 0, 
    vertices = TRUE, classcolors = "rainbow", wghts = 0, 
    xlab = "Dimension 1", ylab = "Dimension 2", xaxs = "i", 
    yaxs = "i", plot.data.column = NA,
    log.classes = FALSE, revert.colors = FALSE, ...)

level_shardsplot(object, par.names, rows = 1:NCOL(object$data),
    centers = rep(NA, length(par.names)), class.labels = NA,
    revert.colors = rep(FALSE, length(par.names)), 
    log.classes = rep(FALSE, length(par.names)),
    centeredcolors = colorRamp(c("red", "white", "blue")),
    mfrow = c(2, 2), plot.type = c("eight", "four", "points", "n"),
    expand = 1, stck = TRUE, grd = FALSE, standardize = FALSE,
    label = FALSE, plot = TRUE, vertices = TRUE, classcolors = "topo",
    wghts = 0, xlab = "Dimension 1", ylab = "Dimension 2",
    xaxs = "i", yaxs = "i", ...)

## S3 method for class 'EDAM'
plot(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shardsplot_+3A_object">object</code></td>
<td>
<p>an object of class <code>EDAM</code> or <code>som</code>.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_par.names">par.names</code></td>
<td>
<p>names used to lable the data columns</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_rows">rows</code></td>
<td>
<p>vector with indices of colomns to be plotted</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_centers">centers</code></td>
<td>
<p>vector of type numeric defining the class centers for the data. NA if data does not have a center.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_class.labels">class.labels</code></td>
<td>
<p>matrix of type text and <code>dimension(3, NROW(object$data))</code> defining the lables to be used for maximum, minimum and central value.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_centeredcolors">centeredcolors</code></td>
<td>
<p>colors to represent the classes with a central value</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_mfrow">mfrow</code></td>
<td>
<p>parameter defining number of plots on a page. see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="shardsplot_+3A_plot.type">plot.type</code></td>
<td>
<p>a character giving the shape of the shards. 
Available are &ldquo;<code>eight</code>&rdquo; and &ldquo;<code>four</code>&rdquo; for octagons resp. rectangles, 
and &ldquo;<code>points</code>&rdquo; for points. If <code>plot.type</code> is &ldquo;<code>n</code>&rdquo;, 
no shards are plotted at all.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_expand">expand</code></td>
<td>
<p>a numeric giving the relative expansion of the axes. 
A value greater than one implies smaller shards. Varying <code>expand</code>
can be sensible for visual reasons.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_stck">stck</code></td>
<td>
<p>logical. If <code>TRUE</code> the cells are varied continously corresponding to 
the differences of direct neighbors in the origin space. 
Within this variation the relative order of the cells is always preserved.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_grd">grd</code></td>
<td>
<p>logical. If <code>TRUE</code> (which automatically sets <code>stck</code> to <code>TRUE</code>), 
the variation of cells is restricted to their original discrete values.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_standardize">standardize</code></td>
<td>
<p>logical. If <code>TRUE</code>, then the measurements in <code>object$preimages</code> 
are standardized before calculating Euclidean distances. 
Measurements are standardized for each variable by dividing by the variable's 
standard deviation. Meaningless if <code>object$preimages</code> is a dissimilarity matrix. </p>
</td></tr>
<tr><td><code id="shardsplot_+3A_data.or">data.or</code></td>
<td>
<p>original data and classes where the first k columns are variables and the (k+1)-th column are the classes.
If defined and class of <code>object</code> is <code>som</code>, <code>data.or</code> is used to assign a class to each codebook. There
a codebook receives the class, from which the majority of its assigned objects origins.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_label">label</code></td>
<td>
<p>logical. If <code>TRUE</code>, the shards are labeled by the rownames of the preimages.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_plot">plot</code></td>
<td>
<p>logical. If <code>FALSE</code>, all graphical output is suppressed.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_classes">classes</code></td>
<td>
<p>a vector giving alternative classes for objects of class <code>EDAM</code>; <code>classes</code> have to be given in 
the original order of the data to which <code><a href="#topic+EDAM">EDAM</a></code> was applied.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_vertices">vertices</code></td>
<td>
<p>logical. If <code>TRUE</code> the grid is drawn.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_classcolors">classcolors</code></td>
<td>
<p>colors to represent the classes, or a character giving the <em>colorscale</em> for the classes. 
Since now available scales are <code>rainbow</code>, <code>topo</code> and <code>gray</code>.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_wghts">wghts</code></td>
<td>
<p> an optional vector of length k giving relative weights of the variables 
in computing Euclidean distances. Meaningless if <code>object$preimages</code> is a dissimilarity matrix.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_xaxs">xaxs</code></td>
<td>
<p>see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="shardsplot_+3A_yaxs">yaxs</code></td>
<td>
<p>see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="shardsplot_+3A_xlab">xlab</code></td>
<td>
<p>see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="shardsplot_+3A_ylab">ylab</code></td>
<td>
<p>see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="shardsplot_+3A_...">...</code></td>
<td>
<p>further plotting parameters.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_plot.data.column">plot.data.column</code></td>
<td>
<p>column index defining from <code>data.or</code> providing the data used to calculate the coloring of the cells.</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_log.classes">log.classes</code></td>
<td>
<p>boolean indicating that the data should be transformed with the logarithmic function before calculating the cell coloring</p>
</td></tr>
<tr><td><code id="shardsplot_+3A_revert.colors">revert.colors</code></td>
<td>
<p>boolean indicating that the colorscale should be reverted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>level_shardsplot</code> uses multiple <code>shardsplot</code> representations of a SOM in order to depict how 
the data used to calculate the SOM is distribution across the map. 
Two representations are possible for the data, first with a single color ramp from the minimum 
value to the maximum value. The second representation is usefull for data for which a basic 
value exists some where between minimum and maximum for which a special color representation should be used 
(e.g. 0 is indicated with white).
</p>
<p>If <code>plot.type</code> is &ldquo;<code>four</code>&rdquo; or &ldquo;<code>eight</code>&rdquo;, the shape of each shard depends 
on the relative distances of the actual object 
or codebook to its up to eight neighbours. If <code>plot.type</code> is &ldquo;<code>eight</code>&rdquo;, <code>shardsplot</code>
corresponds to the representation method
suggested by Cottrell and de Bodt (1996) for Kohonen Self-Organizing Maps. 
If <code>plot.type</code> is &ldquo;<code>points</code>&rdquo;, <code>shardsplot</code> reduces to a usual scatter plot.
</p>


<h3>Value</h3>

<p>The following list is (invisibly) returned:
</p>
<table role = "presentation">
<tr><td><code>Cells.ex</code></td>
<td>
<p>the images of the visualized data</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>the criterion of the visualization</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nils Raabe, <code>level_shardsplot</code> function from Dominik Reusser</p>


<h3>References</h3>

<p>Cottrell, M., and de Bodt, E. (1996).
A Kohonen Map Representation to Avoid Misleading Interpretations.
<em>Proceedings of the European Symposium on Atrificial Neural Networks</em>, D-Facto, pp. 103&ndash;110.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EDAM">EDAM</a></code>, <code><a href="#topic+TopoS">TopoS</a></code>, <code><a href="som.html#topic+som">som</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute clusters and an Eight Directions Arranged Map for the 
# country data. Plotting the result.
data(countries)
logcount &lt;- log(countries[,2:7])
sdlogcount &lt;- apply(logcount, 2, sd)
logstand &lt;- t((t(logcount) / sdlogcount) * c(1,2,6,5,5,3))
cclasses &lt;- cutree(hclust(dist(logstand)), k = 6)
countryEDAM &lt;- EDAM(logstand, classes = cclasses, sa = FALSE, 
    iter.max = 10, random = FALSE)
plot(countryEDAM, vertices = FALSE, label = TRUE, stck = FALSE)

# Compute and plot a Self-Organizing Map for the iris data
data(iris)
library(som)
irissom &lt;- som(iris[,1:4], xdim = 6, ydim = 14)
shardsplot(irissom, data.or = iris, vertices = FALSE)
opar &lt;- par(xpd = NA)
legend(7.5, 6.1, col = rainbow(3), xjust = 0.5, yjust = 0,
    legend = levels(iris[, 5]), pch = 16, horiz = TRUE)
par(opar)    

level_shardsplot(irissom, par.names = names(iris), 
    class.labels = NA, mfrow = c(2,2))
</code></pre>

<hr>
<h2 id='sknn'>Simple k nearest Neighbours</h2><span id='topic+sknn'></span><span id='topic+sknn.default'></span><span id='topic+sknn.formula'></span><span id='topic+sknn.matrix'></span><span id='topic+sknn.data.frame'></span>

<h3>Description</h3>

<p>Function for simple knn classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sknn(x, ...)

## Default S3 method:
sknn(x, grouping, kn = 3, gamma=0, ...)
## S3 method for class 'data.frame'
sknn(x, ...)
## S3 method for class 'matrix'
sknn(x, grouping, ..., subset, na.action = na.fail)
## S3 method for class 'formula'
sknn(formula, data = NULL, ..., subset, na.action = na.fail)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sknn_+3A_x">x</code></td>
<td>
<p>matrix or data frame containing the explanatory variables 
(required, if <code>formula</code> is not given).</p>
</td></tr>
<tr><td><code id="sknn_+3A_grouping">grouping</code></td>
<td>
<p>factor specifying the class for each observation 
(required, if <code>formula</code> is not given).</p>
</td></tr>
<tr><td><code id="sknn_+3A_formula">formula</code></td>
<td>
<p>formula of the form <code>groups ~ x1 + x2 + ...</code>. 
That is, the response is the grouping factor and the right hand side specifies the (non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="sknn_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in <code>formula</code> are preferentially to be taken.</p>
</td></tr>
<tr><td><code id="sknn_+3A_kn">kn</code></td>
<td>
<p>Number of nearest neighbours to use.</p>
</td></tr>
<tr><td><code id="sknn_+3A_gamma">gamma</code></td>
<td>
<p>gamma parameter for rbf in knn. If <code>gamma=0</code> ordinary knn classification is used.</p>
</td></tr>
<tr><td><code id="sknn_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample. (Note: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="sknn_+3A_na.action">na.action</code></td>
<td>
<p>specify the action to be taken if <code>NA</code>s are
found. The default action is for the procedure to fail. An
alternative is <code><a href="stats.html#topic+na.omit">na.omit</a></code>, which leads to rejection of cases with
missing values on any required variable. (Note: If given, this
argument must be named.) </p>
</td></tr>
<tr><td><code id="sknn_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>gamma&gt;0</code> an gaussian like density is used to weight the classes of the <code>kn</code> nearest neighbors. 
<code>weight=exp(-gamma*distance)</code>. This is similar to an rbf kernel. 
If the distances are large it may be useful to <code><a href="base.html#topic+scale">scale</a></code> the data first.
</p>


<h3>Value</h3>

<p>A list containing the function call. 
</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.sknn">predict.sknn</a></code>, <code><a href="class.html#topic+knn">knn</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- sknn(Species ~ ., data = iris)
x &lt;- sknn(Species ~ ., gamma = 4, data = iris)
</code></pre>

<hr>
<h2 id='stepclass'>Stepwise variable selection for classification</h2><span id='topic+stepclass'></span><span id='topic+stepclass.default'></span><span id='topic+stepclass.formula'></span><span id='topic+print.stepclass'></span><span id='topic+plot.stepclass'></span>

<h3>Description</h3>

<p>Forward/backward variable selection for classification using any specified 
classification function and selecting by estimated classification performance measure from <code><a href="#topic+ucpm">ucpm</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stepclass(x, ...)

## Default S3 method:
stepclass(x, grouping, method, improvement = 0.05, maxvar = Inf, 
    start.vars = NULL, direction = c("both", "forward", "backward"), 
    criterion = "CR",  fold = 10, cv.groups = NULL, output = TRUE, 
    min1var = TRUE, ...)
## S3 method for class 'formula'
stepclass(formula, data, method, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stepclass_+3A_x">x</code></td>
<td>
<p>matrix or data frame containing the explanatory variables 
(required, if <code>formula</code> is not given).</p>
</td></tr>
<tr><td><code id="stepclass_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code>. 
That is, the response is the grouping factor and the right hand side 
specifies the (non-factor) discriminators. 
Interaction terms are not supported.</p>
</td></tr>
<tr><td><code id="stepclass_+3A_data">data</code></td>
<td>
<p>data matrix (rows=cases, columns=variables)</p>
</td></tr>
<tr><td><code id="stepclass_+3A_grouping">grouping</code></td>
<td>
<p>class indicator vector (a factor)</p>
</td></tr>
<tr><td><code id="stepclass_+3A_method">method</code></td>
<td>
<p>character, name of classification function 
(e.g. &ldquo;<code><a href="MASS.html#topic+lda">lda</a></code>&rdquo;).</p>
</td></tr>
<tr><td><code id="stepclass_+3A_improvement">improvement</code></td>
<td>
<p>least improvement of performance measure desired 
to include or exclude any variable (&lt;=1)</p>
</td></tr>
<tr><td><code id="stepclass_+3A_maxvar">maxvar</code></td>
<td>
<p>maximum number of variables in model</p>
</td></tr>
<tr><td><code id="stepclass_+3A_start.vars">start.vars</code></td>
<td>
<p>set variables to start with (indices or names). 
Default is no variables if &lsquo;<code>direction</code>&rsquo; is 
&ldquo;<code>forward</code>&rdquo; or &ldquo;<code>both</code>&rdquo;, 
and all variables if &lsquo;<code>direction</code>&rsquo; is &ldquo;<code>backward</code>&rdquo;.</p>
</td></tr>
<tr><td><code id="stepclass_+3A_direction">direction</code></td>
<td>
<p>&ldquo;<code>forward</code>&rdquo;, &ldquo;<code>backward</code>&rdquo; or 
&ldquo;<code>both</code>&rdquo; (default)</p>
</td></tr>
<tr><td><code id="stepclass_+3A_criterion">criterion</code></td>
<td>
<p>performance measure taken from <code><a href="#topic+ucpm">ucpm</a></code>.</p>
</td></tr>
<tr><td><code id="stepclass_+3A_fold">fold</code></td>
<td>
<p>parameter for cross-validation; omitted if &lsquo;<code>cv.groups</code>&rsquo; is specified.</p>
</td></tr>                                            
<tr><td><code id="stepclass_+3A_cv.groups">cv.groups</code></td>
<td>
<p>vector of group indicators for cross-validation. 
By default assigned automatically.</p>
</td></tr>
<tr><td><code id="stepclass_+3A_output">output</code></td>
<td>
<p>indicator (logical) for textoutput during computation (slows down computation!)</p>
</td></tr>
<tr><td><code id="stepclass_+3A_min1var">min1var</code></td>
<td>
<p>logical, whether to include at least one variable in the model, 
even if the prior itself already is a reasonable model.</p>
</td></tr>
<tr><td><code id="stepclass_+3A_...">...</code></td>
<td>
<p>further parameters passed to classification function (&lsquo;<code>method</code>&rsquo;), e.g. priors etc.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The classification &ldquo;method&rdquo; (e.g. &lsquo;<code><a href="MASS.html#topic+lda">lda</a></code>&rsquo;) must have its own 
&lsquo;<code>predict</code>&rsquo; method (like &lsquo;<code><a href="MASS.html#topic+predict.lda">predict.lda</a></code>&rsquo; for &lsquo;<code>lda</code>&rsquo;) 
that either returns a matrix of posterior probabilities or a list with an element &lsquo;<code>posterior</code>&rsquo; containing 
that matrix instead. It must be able to deal with matrices as in <code>method(x, grouping, ...)</code>
</p>
<p>Then a stepwise variable selection is performed. 
The initial model is defined by the provided starting variables; 
in every step new models are generated by including every single 
variable that is not in the model, and by excluding every single 
variable that is in the model. The resulting performance measure for these 
models are estimated (by cross-validation), and if the maximum value of the chosen
criterion is better than &lsquo;<code>improvement</code>&rsquo; plus the value so far, the 
corresponding variable is in- or excluded. The procedure stops, if
the new best value is not good enough, or if the specified maximum 
number of variables is reached.
</p>
<p>If &lsquo;<code>direction</code>&rsquo; is &ldquo;<code>forward</code>&rdquo;, the model is only extended (by including 
further variables), if &lsquo;<code>direction</code>&rsquo; is &ldquo;<code>backward</code>&rdquo;, the model is only 
reduced (by excluding variables from the model).
</p>


<h3>Value</h3>

<p>An object of class &lsquo;<code>stepclass</code>&rsquo; containing the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the (matched) function call.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>name of classification function used (e.g. &ldquo;<code>lda</code>&rdquo;).</p>
</td></tr>
<tr><td><code>start.variables</code></td>
<td>
<p>vector of starting variables.</p>
</td></tr>
<tr><td><code>process</code></td>
<td>
<p>data frame showing selection process (included/excluded variables and performance measure).</p>
</td></tr> 
<tr><td><code>model</code></td>
<td>
<p>the final model: data frame with 2 columns; indices and names of variables.</p>
</td></tr>
<tr><td><code>perfomance.measure</code></td>
<td>
<p>value of the criterion used by <code><a href="#topic+ucpm">ucpm</a></code></p>
</td></tr>
<tr><td><code>formula</code></td>
<td>
<p>formula of the form &lsquo;<code>response ~ list + of + selected + variables</code>&rsquo;</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a>, Irina Czogiel</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+step">step</a></code>, <code><a href="MASS.html#topic+stepAIC">stepAIC</a></code>, 
and <code><a href="#topic+greedy.wilks">greedy.wilks</a></code> for stepwise variable selection according to Wilk's lambda
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
library(MASS)
iris.d &lt;- iris[,1:4]  # the data    
iris.c &lt;- iris[,5]    # the classes 
sc_obj &lt;- stepclass(iris.d, iris.c, "lda", start.vars = "Sepal.Width")
sc_obj
plot(sc_obj)

## or using formulas:
sc_obj &lt;- stepclass(Species ~ ., data = iris, method = "qda", 
    start.vars = "Sepal.Width", criterion = "AS")  # same as above 
sc_obj
## now you can say stuff like
## qda(sc_obj$formula, data = B3)
</code></pre>

<hr>
<h2 id='svmlight'>Interface to SVMlight</h2><span id='topic+svmlight'></span><span id='topic+svmlight.default'></span><span id='topic+svmlight.formula'></span><span id='topic+svmlight.matrix'></span><span id='topic+svmlight.data.frame'></span>

<h3>Description</h3>

<p>Function to call SVMlight from R for classification. Multiple group
classification is done with the one-against-rest partition of data.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svmlight(x, ...)

## Default S3 method:
svmlight(x, grouping, temp.dir = NULL, pathsvm = NULL, 
    del = TRUE, type = "C", class.type = "oaa", svm.options = NULL, 
    prior = NULL, out = FALSE, ...)
## S3 method for class 'data.frame'
svmlight(x, ...)
## S3 method for class 'matrix'
svmlight(x, grouping, ..., subset, na.action = na.fail)
## S3 method for class 'formula'
svmlight(formula, data = NULL, ..., subset, 
    na.action = na.fail)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="svmlight_+3A_x">x</code></td>
<td>
<p>matrix or data frame containing the explanatory variables 
(required, if <code>formula</code> is not given).</p>
</td></tr>
<tr><td><code id="svmlight_+3A_grouping">grouping</code></td>
<td>
<p>factor specifying the class for each observation 
(required, if <code>formula</code> is not given).</p>
</td></tr>
<tr><td><code id="svmlight_+3A_formula">formula</code></td>
<td>
<p>formula of the form <code>groups ~ x1 + x2 + ...</code>. 
That is, the response is the grouping factor and the right hand side specifies the (non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="svmlight_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in <code>formula</code> are preferentially to be taken.</p>
</td></tr>
<tr><td><code id="svmlight_+3A_temp.dir">temp.dir</code></td>
<td>
<p>directory for temporary files.</p>
</td></tr>
<tr><td><code id="svmlight_+3A_pathsvm">pathsvm</code></td>
<td>
<p>Path to SVMlight binaries (required, if path is unknown by the OS).</p>
</td></tr>
<tr><td><code id="svmlight_+3A_del">del</code></td>
<td>
<p>Logical: whether to delete temporary files</p>
</td></tr>
<tr><td><code id="svmlight_+3A_type">type</code></td>
<td>
<p>Perform <code>"C"</code>=Classification or <code>"R"</code>=Regression</p>
</td></tr>
<tr><td><code id="svmlight_+3A_class.type">class.type</code></td>
<td>
<p>Multiclass scheme to use. See details.</p>
</td></tr>
<tr><td><code id="svmlight_+3A_svm.options">svm.options</code></td>
<td>
<p>Optional parameters to SVMlight.
</p>
<p>For further details see: &ldquo;How to use&rdquo; on <a href="http://svmlight.joachims.org/">http://svmlight.joachims.org/</a>.
</p>
</td></tr>
<tr><td><code id="svmlight_+3A_prior">prior</code></td>
<td>
<p>A Priori probabilities of classes.</p>
</td></tr>
<tr><td><code id="svmlight_+3A_out">out</code></td>
<td>
<p>Logical: whether SVMlight output ahouild be printed on console 
(only for Windows OS.)</p>
</td></tr>
<tr><td><code id="svmlight_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample. (Note: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="svmlight_+3A_na.action">na.action</code></td>
<td>
<p>specify the action to be taken if <code>NA</code>s are
found. The default action is for the procedure to fail. An
alternative is <code><a href="stats.html#topic+na.omit">na.omit</a></code>, which leads to rejection of cases with
missing values on any required variable. (Note: If given, this
argument must be named.) </p>
</td></tr>
<tr><td><code id="svmlight_+3A_...">...</code></td>
<td>
<p>currently unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function to call SVMlight from R for classification (<code>type="C"</code>). 
SVMlight is an implementation of Vapnik's Support Vector Machine. It
is written in C by Thorsten Joachims. On the homepage (see below) the
source-code and several binaries for SVMlight are available. If more
then two classes are given the SVM is learned by the one-against-all
scheme (<code>class.type="oaa"</code>). That means that each class is trained against the other K-1
classes. The class with the highest decision function in the SVM
wins. So K SVMs have to be learned.
If <code>class.type="oao"</code> each class is tested against every other and the final class is elected
by a majority vote.
</p>
<p>If <code>type="R"</code> a SVM Regression is performed.
</p>


<h3>Value</h3>

<p>A list containing the function call and the result of SVMlight. 
</p>


<h3>Requirements</h3>

<p>SVMlight (<a href="http://svmlight.joachims.org/">http://svmlight.joachims.org/</a>) must be installed before using this interface.</p>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a>, Andrea Preusser</p>


<h3>References</h3>

<p><a href="http://svmlight.joachims.org/">http://svmlight.joachims.org/</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.svmlight">predict.svmlight</a></code>,<code><a href="e1071.html#topic+svm">svm</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Only works if the svmlight binaries are in the path.
data(iris)
x &lt;- svmlight(Species ~ ., data = iris)
## Using RBF-Kernel with gamma=0.1:
data(B3)
x &lt;- svmlight(PHASEN ~ ., data = B3, svm.options = "-t 2 -g 0.1")

## End(Not run)
</code></pre>

<hr>
<h2 id='TopoS'>Computation of criterion S of a visualization</h2><span id='topic+TopoS'></span>

<h3>Description</h3>

<p><code>TopoS</code> computes one version of the criterion <code>stress</code> as i.e. 
known from <code><a href="MASS.html#topic+sammon">sammon</a></code> for a given visualization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TopoS(EV.dist, Cells.dist)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="TopoS_+3A_ev.dist">EV.dist</code></td>
<td>
<p>a symmetric distance matrix consisting of distances in the origin space (<code>dx</code>)</p>
</td></tr>
<tr><td><code id="TopoS_+3A_cells.dist">Cells.dist</code></td>
<td>
<p>a symmetric distance matrix consisting of distances in the target space (<code>dy</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>TopoS first performs a homogeneous linear regression where <code>dx</code> is predicted by <code>dy</code>. 
Then the residual sum of squares of this regression is computed and divided by the sum of squared <code>dx</code>. 
Finally one minus the square root of the latter result defines <code>S</code>.
</p>


<h3>Value</h3>

<p><code>TopoS</code> returns a numeric between zero and one which is the criterion <code>S</code> of the given visualization.
</p>


<h3>Author(s)</h3>

<p>Nils Raabe</p>


<h3>See Also</h3>

<p><code><a href="#topic+EDAM">EDAM</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute S for the MDS visualization of the german business data
data(B3)
plot(cmdscale(dist(B3[, 2:14])), col = rainbow(4)[B3[, 1]], pch = 16)
TopoS(dist(B3[, 2:14]), dist(cmdscale(dist(B3[, 2:14]))))
</code></pre>

<hr>
<h2 id='triframe'>Barycentric plots</h2><span id='topic+triframe'></span>

<h3>Description</h3>

<p>Function to add a frame to an existing (barycentric) plot.</p>


<h3>Usage</h3>

<pre><code class='language-R'>triframe(label = 1:3, label.col = 1, cex = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="triframe_+3A_label">label</code></td>
<td>
<p>labels for the three corners of the plot.</p>
</td></tr>
<tr><td><code id="triframe_+3A_label.col">label.col</code></td>
<td>
<p>text color for labels.</p>
</td></tr>
<tr><td><code id="triframe_+3A_cex">cex</code></td>
<td>
<p>Magnification factor for label text relative to the default.</p>
</td></tr>
<tr><td><code id="triframe_+3A_...">...</code></td>
<td>
<p>Further graphical parameters passed to <code><a href="#topic+trilines">trilines</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+triplot">triplot</a></code>, <code><a href="#topic+trilines">trilines</a></code>, <code><a href="#topic+trigrid">trigrid</a></code>, <code><a href="#topic+centerlines">centerlines</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>triplot(grid = TRUE, frame = FALSE)     # plot without frame 
some.triangle &lt;- rbind(c(0, 0.65, 0.35), c(0.53, 0.47, 0), 
                       c(0.72, 0, 0.28))[c(1:3, 1), ]
trilines(some.triangle, col = "red", pch = 16, type = "b")
triframe(label = c("left", "top", "right"), col = "blue", 
         label.col = "green3")          # frame on top of points 
</code></pre>

<hr>
<h2 id='trigrid'>Barycentric plots</h2><span id='topic+trigrid'></span>

<h3>Description</h3>

<p>Function to add a grid to an existing (barycentric) plot.</p>


<h3>Usage</h3>

<pre><code class='language-R'>trigrid(x = seq(0.1, 0.9, by = 0.1), y = NULL, z = NULL, 
    lty = "dashed", col = "grey", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trigrid_+3A_x">x</code></td>
<td>
<p>Values along which to draw grid lines for first dimension 
(or all dimensions if <code>y</code> and <code>z</code> omitted). 
For NO grid lines in some dimensions just supply an <code>NA</code>.</p>
</td></tr>
<tr><td><code id="trigrid_+3A_y">y</code></td>
<td>
<p>Grid lines for second dimension.</p>
</td></tr>
<tr><td><code id="trigrid_+3A_z">z</code></td>
<td>
<p>Grid lines for third dimension.</p>
</td></tr>
<tr><td><code id="trigrid_+3A_lty">lty</code></td>
<td>
<p>Line type (see <code><a href="graphics.html#topic+par">par</a></code>).</p>
</td></tr>
<tr><td><code id="trigrid_+3A_col">col</code></td>
<td>
<p>Line colour (see <code><a href="graphics.html#topic+par">par</a></code>).</p>
</td></tr>
<tr><td><code id="trigrid_+3A_...">...</code></td>
<td>
<p>Further graphical parameters passed to <code><a href="#topic+trilines">trilines</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Grid lines illustrate the set of points for which one of the dimensions is held constant; 
e.g. horizontal lines contain all points with a certain value y for the second dimension, 
connecting the two extreme points (0,y,1-y) and (1-y,y,0).
</p>
<p>Grids may be designed more flexible than with <code>triplot</code>'s <code>grid</code> option.</p>


<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+triplot">triplot</a></code>, <code><a href="#topic+trilines">trilines</a></code>, <code><a href="#topic+triframe">triframe</a></code>, <code><a href="#topic+centerlines">centerlines</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>triplot(grid = FALSE)
trigrid(c(1/3, 0.5)) # same grid for all 3 dimensions 

triplot(grid = c(1/3, 0.5))  # (same effect) 

triplot(grid = FALSE)
# different grids for all dimensions:
trigrid(x = 1/3, y = 0.5, z = seq(0.2, 0.8, by=0.2))  

triplot(grid = FALSE)
# grid for third dimension only:
trigrid(x = NA, y = NA, z = c(0.1, 0.2, 0.4, 0.8))  
</code></pre>

<hr>
<h2 id='triperplines'>Barycentric plots</h2><span id='topic+triperplines'></span>

<h3>Description</h3>

<p>Function to add a point and the corresponding perpendicular lines to all three 
sides to an existing (barycentric) plot.</p>


<h3>Usage</h3>

<pre><code class='language-R'>triperplines(x, y = NULL, z = NULL, lcol = "red", pch = 17, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="triperplines_+3A_x">x</code></td>
<td>
<p>fraction of first component
OR 3-element vector (for all three components, omitting <code>y</code> and <code>z</code>).</p>
</td></tr>
<tr><td><code id="triperplines_+3A_y">y</code></td>
<td>
<p>(optional) fraction of second component.</p>
</td></tr>
<tr><td><code id="triperplines_+3A_z">z</code></td>
<td>
<p>(optional) fraction of third component.</p>
</td></tr>
<tr><td><code id="triperplines_+3A_lcol">lcol</code></td>
<td>
<p>line color</p>
</td></tr>
<tr><td><code id="triperplines_+3A_pch">pch</code></td>
<td>
<p>plotting character. <code>pch = 0</code> for no point</p>
</td></tr>
<tr><td><code id="triperplines_+3A_...">...</code></td>
<td>
<p>Further graphical parameters (see <code><a href="graphics.html#topic+points">points</a></code>, <code><a href="graphics.html#topic+lines">lines</a></code> and <code><a href="graphics.html#topic+par">par</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Adds a (single!) point and lines to an existing plot (generated by <code><a href="#topic+triplot">triplot</a></code>).
The lines originate from the point and run (perpendicular) towards all three sides. 
The lengths (and proportions) of these lines are identical to those of <code>x</code>, <code>y</code> and <code>z</code>.
</p>


<h3>Value</h3>

<p>a 2-column-matrix containing plot coordinates.
</p>


<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+triplot">triplot</a></code>, <code><a href="#topic+tripoints">tripoints</a></code>, <code><a href="#topic+trilines">trilines</a></code>, <code><a href="#topic+tritrafo">tritrafo</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>triplot()  # empty plot 
triperplines(1/2, 1/3, 1/6)
</code></pre>

<hr>
<h2 id='triplot'>Barycentric plots</h2><span id='topic+triplot'></span>

<h3>Description</h3>

<p>Function to produce triangular (barycentric) plots 
illustrating proportions of 3 components, 
e.g. discrete 3D-distributions or mixture fractions that sum up to 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>triplot(x = NULL, y = NULL, z = NULL, main = "", frame = TRUE, 
    label = 1:3, grid = seq(0.1, 0.9, by = 0.1), center = FALSE, 
    set.par = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="triplot_+3A_x">x</code></td>
<td>
<p>Vector of fractions of first component
OR 3-column matrix containing all three components (omitting <code>y</code> and <code>z</code>)
OR 3-element vector (for all three components, omitting <code>y</code> and <code>z</code>).</p>
</td></tr>
<tr><td><code id="triplot_+3A_y">y</code></td>
<td>
<p>(Optional) vector of fractions of second component.</p>
</td></tr>
<tr><td><code id="triplot_+3A_z">z</code></td>
<td>
<p>(Optional) vector of fractions of third component.</p>
</td></tr>
<tr><td><code id="triplot_+3A_main">main</code></td>
<td>
<p>Main title</p>
</td></tr>
<tr><td><code id="triplot_+3A_frame">frame</code></td>
<td>
<p>Controls whether a frame (triangle) and labels are drawn.</p>
</td></tr>
<tr><td><code id="triplot_+3A_label">label</code></td>
<td>
<p>(Character) vector of labels for the three corners.</p>
</td></tr>
<tr><td><code id="triplot_+3A_grid">grid</code></td>
<td>
<p>Values along which grid lines are to be drawn (or <code>FALSE</code> for no grid at all). 
Default is steps of 10 percent.</p>
</td></tr>
<tr><td><code id="triplot_+3A_center">center</code></td>
<td>
<p>Controls whether or not to draw centerlines at which there is a 
&lsquo;tie&rsquo; between any two dimensions (see also <code><a href="#topic+centerlines">centerlines</a></code>).</p>
</td></tr>
<tr><td><code id="triplot_+3A_set.par">set.par</code></td>
<td>
<p>Controls whether graphical parameter <code>mar</code> is set so 
the plot fills the window (see <code><a href="graphics.html#topic+par">par</a></code>).</p>
</td></tr>
<tr><td><code id="triplot_+3A_...">...</code></td>
<td>
<p>Further graphical parameters passed to <code><a href="#topic+trilines">trilines</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The barycentric plot illustrates the set of points (x,y,z) with x,y,z between 0 and 1 and x+y+z=1; 
that is, the triangle spanned by (1,0,0), (0,1,0) and (0,0,1) in 3-dimensional space.
The three dimensions x, y and z correspond to lower left, upper and lower right corner of the plot.
The greater the share of x in the proportion, the closer the point is to the lower left corner;
Points on the opposite (upper right) side have a zero x-fraction.
The grid lines show the points at which one dimension is held constant, horizontal lines for 
example contain points with a constant second dimension.
</p>


<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+tripoints">tripoints</a></code>, <code><a href="#topic+trilines">trilines</a></code>, <code><a href="#topic+triperplines">triperplines</a></code>, <code><a href="#topic+trigrid">trigrid</a></code>, 
<code><a href="#topic+triframe">triframe</a></code> for points, lines and layout, <code><a href="#topic+tritrafo">tritrafo</a></code> for placing labels,
and <code><a href="#topic+quadplot">quadplot</a></code> for the same in 4 dimensions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># illustrating probabilities: 
triplot(label = c("1, 2 or 3", "4 or 5", "6"), 
    main = "die rolls: probabilities", pch = 17)
triperplines(1/2, 1/3, 1/6)

# expected... 
triplot(1/2, 1/3, 1/6, label = c("1, 2 or 3", "4 or 5", "6"), 
    main = "die rolls: expected and observed frequencies", pch = 17)
# ... and observed frequencies. 
dierolls &lt;- matrix(sample(1:3, size = 50*20, prob = c(1/2, 1/3, 1/6), 
                          replace = TRUE), ncol = 50)
frequencies &lt;- t(apply(dierolls, 1, 
    function(x)(summary(factor(x, levels = 1:3)))) / 50)
tripoints(frequencies)

# LDA classification posterior: 
data(iris)
require(MASS)
pred &lt;- predict(lda(Species ~ ., data = iris),iris)
plotchar &lt;- rep(1,150)
plotchar[pred$class != iris$Species] &lt;- 19
triplot(pred$posterior, label = colnames(pred$posterior), 
        main = "LDA posterior assignments", center = TRUE, 
        pch = plotchar, col = rep(c("blue", "green3", "red"), rep(50, 3)), 
        grid = TRUE)
legend(x = -0.6, y = 0.7, col = c("blue", "green3", "red"), 
    pch = 15, legend = colnames(pred$posterior))
</code></pre>

<hr>
<h2 id='tripoints'>Barycentric plots</h2><span id='topic+tripoints'></span><span id='topic+trilines'></span>

<h3>Description</h3>

<p>Function to add points or lines to an existing (barycentric) plot.</p>


<h3>Usage</h3>

<pre><code class='language-R'>tripoints(x, y = NULL, z = NULL, ...)
trilines(x, y = NULL, z = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tripoints_+3A_x">x</code></td>
<td>
<p>Vector of fractions of first component
OR 3-column matrix containing all three components (omitting <code>y</code> and <code>z</code>)
OR 3-element vector (for all three components, omitting <code>y</code> and <code>z</code>).</p>
</td></tr>
<tr><td><code id="tripoints_+3A_y">y</code></td>
<td>
<p>(optional) vector of fractions of second component.</p>
</td></tr>
<tr><td><code id="tripoints_+3A_z">z</code></td>
<td>
<p>(optional) vector of fractions of third component.</p>
</td></tr>
<tr><td><code id="tripoints_+3A_...">...</code></td>
<td>
<p>Further graphical parameters (see <code><a href="graphics.html#topic+points">points</a></code> and <code><a href="graphics.html#topic+par">par</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Adds points or lines to an existing plot (generated by <code><a href="#topic+triplot">triplot</a></code>).
</p>


<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+points">points</a></code>, <code><a href="graphics.html#topic+lines">lines</a></code>, <code><a href="#topic+triplot">triplot</a></code>, <code><a href="#topic+tritrafo">tritrafo</a></code>, <code><a href="#topic+centerlines">centerlines</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>triplot()  # empty plot 
tripoints(0.1, 0.2, 0.7)                        # a point 
tripoints(c(0.2, 0.6), c(0.3, 0.3), c(0.5, 0.1),
    pch = c(2, 6))                              # two points
trilines(c(0.1, 0.6), c(0.2, 0.3), c(0.7, 0.1), 
    col = "blue", lty = "dotted")               # a line 

trilines(centerlines(3))
</code></pre>

<hr>
<h2 id='tritrafo'>Barycentric plots</h2><span id='topic+tritrafo'></span>

<h3>Description</h3>

<p>Function to carry out the transformation into 2D space
for <code><a href="#topic+triplot">triplot</a></code>, <code><a href="#topic+trilines">trilines</a></code> etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tritrafo(x, y = NULL, z = NULL, check = TRUE, tolerance = 0.0001)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tritrafo_+3A_x">x</code></td>
<td>
<p>Vector of fractions of first component
OR 3-column matrix containing all three components (omitting <code>y</code> and <code>z</code>)
OR 3-element vector (for all three components, omitting <code>y</code> and <code>z</code>).</p>
</td></tr>
<tr><td><code id="tritrafo_+3A_y">y</code></td>
<td>
<p>(optional) vector of fractions of second component.</p>
</td></tr>
<tr><td><code id="tritrafo_+3A_z">z</code></td>
<td>
<p>(optional) vector of fractions of third component.</p>
</td></tr>
<tr><td><code id="tritrafo_+3A_check">check</code></td>
<td>
<p>if <code>TRUE</code>, it is checked whether <code>x+y+z=1</code> and <code>x,y,z&gt;=0</code> for all cases.</p>
</td></tr>
<tr><td><code id="tritrafo_+3A_tolerance">tolerance</code></td>
<td>
<p>tolerance for above sum check.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Projects the mixture given by <code>x</code>, <code>y</code>, and <code>z</code> 
with <code>x</code>, <code>y</code>, <code>z</code> between one and zero and <code>x+y+z=1</code> into
a two-dimensional space.
</p>
<p>For further details see <code><a href="#topic+triplot">triplot</a></code>.
</p>


<h3>Value</h3>

<p>A matrix with two columns corresponding to the two dimensions.
</p>


<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+triplot">triplot</a></code>, <code><a href="#topic+tripoints">tripoints</a></code>, <code><a href="#topic+trilines">trilines</a></code>, <code><a href="#topic+trigrid">trigrid</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>tritrafo(0.1, 0.2, 0.7)
tritrafo(0.1, 0.2, 0.6) # warning 

triplot()
points(tritrafo(0.1, 0.2, 0.7), col="red")
tripoints(0.1, 0.2, 0.7, col="green")  # the same

tritrafo(c(0.1,0.2), c(0.3,0.4), c(0.6,0.4))
tritrafo(diag(3))

point &lt;- c(0.25,0.6,0.15)
triplot(point, pch=16)
text(tritrafo(point), "(0.25, 0.60, 0.15)", adj=c(0.5,2)) # add a label 
</code></pre>

<hr>
<h2 id='ucpm'>Uschi's classification performance measures</h2><span id='topic+ucpm'></span>

<h3>Description</h3>

<p>Function to calculate the Correctness Rate, the Accuracy, the Ability to Seperate and the Confidence of 
a classification rule.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ucpm(m, tc, ec = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ucpm_+3A_m">m</code></td>
<td>
<p>matrix of (scaled) membership values</p>
</td></tr>
<tr><td><code id="ucpm_+3A_tc">tc</code></td>
<td>
<p>vector of true classes</p>
</td></tr>
<tr><td><code id="ucpm_+3A_ec">ec</code></td>
<td>
<p>vector of estimated classes (only required if scaled membership values are used)</p>
</td></tr>
</table>


<h3>Details</h3>

 

<ul>
<li><p> The <em>correctness rate</em> is the estimator for the correctness of a classification rule (1-error rate). 
</p>
</li>
<li><p> The <em>accuracy</em> is based on the euclidean distances between (scaled) membership vectors and the vectors 
representing the true class corner. These distances are standardized so that a measure of 1 is achieved
if all vectors lie in the correct corners and 0 if they all lie in the center. 
</p>
</li>
<li><p> Analougously, the <em>ability to seperate</em> is based on the distances between (scaled) membership 
vectors and the vector representing the corresponding assigned class corner. 
</p>
</li>
<li><p> The <em>confidence</em> is the mean of the membership values of the assigned classes. 
</p>
</li></ul>



<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>CR</code></td>
<td>
<p>Correctness Rate</p>
</td></tr>
<tr><td><code>AC</code></td>
<td>
<p>Accuracy</p>
</td></tr>
<tr><td><code>AS</code></td>
<td>
<p>Ability to Seperate</p>
</td></tr>
<tr><td><code>CF</code></td>
<td>
<p>Confidence</p>
</td></tr>
<tr><td><code>CFvec</code></td>
<td>
<p>Confidence for each (true) class</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Karsten Luebke, <a href="mailto:karsten.luebke@fom.de">karsten.luebke@fom.de</a></p>


<h3>References</h3>

<p>Garczarek, Ursula Maria (2002): Classification rules in standardized partition spaces.
Dissertation, University of Dortmund. 
URL <a href="http://hdl.handle.net/2003/2789">http://hdl.handle.net/2003/2789</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MASS)
data(iris)
ucpm(predict(lda(Species ~ ., data = iris))$posterior, iris$Species)
</code></pre>

<hr>
<h2 id='woe'>Weights of evidence</h2><span id='topic+woe'></span><span id='topic+woe.default'></span><span id='topic+woe.formula'></span><span id='topic+print.woe'></span>

<h3>Description</h3>

<p>Computes weight of evidence transform of factor variables for binary classification.</p>


<h3>Usage</h3>

<pre><code class='language-R'>woe(x, ...)
## Default S3 method:
woe(x, grouping, weights = NULL, zeroadj = 0, ids = NULL, 
                      appont = TRUE, ...)
## S3 method for class 'formula'
woe(formula, data = NULL, weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="woe_+3A_x">x</code></td>
<td>
<p>A matrix or data frame containing the explanatory variables.</p>
</td></tr>
<tr><td><code id="woe_+3A_grouping">grouping</code></td>
<td>
<p>A factor specifying the binary class for each observation.</p>
</td></tr>
<tr><td><code id="woe_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>grouping ~ x1 + x2 + ...</code> That is, the response is the grouping factor and the right hand side specifies the discriminators.</p>
</td></tr>
<tr><td><code id="woe_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in formula are to be taken.</p>
</td></tr>
<tr><td><code id="woe_+3A_weights">weights</code></td>
<td>
<p>Vector with observation weights. For call <code>woe(formula, data, weights)</code> also a a character is possible that specifies the name of the weight coloumn in data.</p>
</td></tr> 
<tr><td><code id="woe_+3A_zeroadj">zeroadj</code></td>
<td>
<p>Additive constant to be added for a level with 0 observations in a class.</p>
</td></tr>  
<tr><td><code id="woe_+3A_ids">ids</code></td>
<td>
<p>Vector of either indices or variable names that specifies the variables to be transformed.</p>
</td></tr>
<tr><td><code id="woe_+3A_appont">appont</code></td>
<td>
<p>Application on training data: logical indicating whether the transformed values for the training data should be returned by recursive calling of <code>predict.woe</code>.</p>
</td></tr>
<tr><td><code id="woe_+3A_...">...</code></td>
<td>
<p>For <code>woe.formula</code>: Further arguments passed to function <code>woe.default</code> such as <code>ids</code>. For <code>woe.default</code>: <code>replace = FALSE</code> can be passed to recursive call of <code>predict.woe</code> if <code>appont</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To each factor level <code class="reqn">x</code> a numeric value <code class="reqn">WOE(x) = ln(f(x|1)/f(x|2))</code> is assigned where 1 and 2 denote the class labels. The WOE transform is motivated for subsequent modelling by logistic regression. Note that the frequencies of the classes should be investigated before. Information values heuristically quantify the discriminatory power of a variable by <code class="reqn">IV = (f(x|1)-f(x|2)) ln(f(x|1)/f(x|2))</code>.
</p>


<h3>Value</h3>

<p>Returns an object of class <em>woe</em> that can be applied to new data. 
</p>
<table role = "presentation">
<tr><td><code>woe</code></td>
<td>
<p>WOE coefficients for factor2numeric transformation of each (specified) variable.</p>
</td></tr>
<tr><td><code>IV</code></td>
<td>
<p>Vector of information values of all transformed variables.</p>
</td></tr>
<tr><td><code>newx</code></td>
<td>
<p>Data frame of transformed data if <code>appont</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gero Szepannek</p>


<h3>References</h3>

<p>Good, I. (1950): <em>Probability and the Weighting of Evidences.</em> Charles Griffin, London.
</p>
<p>Kullback, S. (1959): <em>Information Theory and Statistics.</em> Wiley, New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.woe">predict.woe</a></code>, <code><a href="#topic+plot.woe">plot.woe</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## load German credit data
data("GermanCredit")

## training/validation split
train &lt;- sample(nrow(GermanCredit), round(0.6*nrow(GermanCredit)))

woemodel &lt;- woe(credit_risk~., data = GermanCredit[train,], zeroadj=0.5, applyontrain = TRUE)
woemodel

## plot variable information values and woes
plot(woemodel)
plot(woemodel, type = "woes")

## apply woes 
traindata &lt;- predict(woemodel, GermanCredit[train,], replace = TRUE)
str(traindata)

## fit logistic regression model
glmodel     &lt;- glm(credit_risk~., traindata, family=binomial)
summary(glmodel)
pred.trn &lt;- predict(glmodel, traindata, type = "response")

## predict validation data
validata &lt;- predict(woemodel, GermanCredit[-train,], replace = TRUE)
pred.val &lt;- predict(glmodel, validata, type = "response")

</code></pre>

<hr>
<h2 id='xtractvars'>Variable clustering based variable selection</h2><span id='topic+xtractvars'></span>

<h3>Description</h3>

<p>Applies variable selection to data based on variable clusterings as resulting from <code><a href="#topic+corclust">corclust</a></code> or  <code><a href="ClustVarLV.html#topic+CLV">CLV</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>xtractvars(object, data, thres = 0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xtractvars_+3A_object">object</code></td>
<td>
<p>Object of class <code><a href="#topic+cvtree">cvtree</a></code> applied to a <code><a href="#topic+corclust">corclust</a></code> object or the <code>summary()</code> of a <code>clv</code> object as created by <code><a href="ClustVarLV.html#topic+CLV">CLV</a></code>.</p>
</td></tr>
<tr><td><code id="xtractvars_+3A_data">data</code></td>
<td>
<p>Data where variables are to be selected. Coloumn names must be identical to those used in corclust model.</p>
</td></tr>
<tr><td><code id="xtractvars_+3A_thres">thres</code></td>
<td>
<p>Maximum accepted average within cluster correlation for selection of a variable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Of each cluster the first variable is selected as well as all other variables with an average within cluster correlation below <code>thres</code>. 
</p>


<h3>Value</h3>

<p>The data is returned where unselected coloumns are removed.
</p>


<h3>Author(s)</h3>

<p>Gero Szepannek</p>


<h3>References</h3>

<p>Roever, C. and Szepannek, G. (2005): Application of a genetic algorithm
to variable selection in fuzzy clustering. In C. Weihs and W. Gaul (eds), Classification 
- The Ubiquitous Challenge, 674-681, Springer.</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+corclust">corclust</a></code>, <code><a href="#topic+cvtree">cvtree</a></code> and  <code><a href="ClustVarLV.html#topic+CLV">CLV</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(B3)
    ccres &lt;- corclust(B3)
    plot(ccres)
    cvtres &lt;- cvtree(ccres, k = 3)
    newdata &lt;- xtractvars(cvtres, B3, thres = 0.5) 
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
