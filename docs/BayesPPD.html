<!DOCTYPE html><html lang="en"><head><title>Help for package BayesPPD</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BayesPPD}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BayesPPD-package'><p>Bayesian sample size determination using the power and normalized power prior for generalized linear models</p></a></li>
<li><a href='#actg019'><p>AIDS Clinical Trial ACTG019 (1990).</p></a></li>
<li><a href='#actg036'><p>AIDS Clinical Trial ACTG036 (1991).</p></a></li>
<li><a href='#glm.fixed.a0'><p>Model fitting for generalized linear models with fixed a0</p></a></li>
<li><a href='#glm.random.a0'><p>Model fitting for generalized linear models with random a0</p></a></li>
<li><a href='#normalizing.constant'><p>Function for approximating the normalizing constant for generalized linear models with random a0</p></a></li>
<li><a href='#power.glm.fixed.a0'><p>Power/type I error calculation for generalized linear models with fixed a0</p></a></li>
<li><a href='#power.glm.random.a0'><p>Power/type I error calculation for generalized linear models with random a0</p></a></li>
<li><a href='#power.two.grp.fixed.a0'><p>Power/type I error calculation for data with two groups (treatment and control group, no covariates) with fixed a0</p></a></li>
<li><a href='#power.two.grp.random.a0'><p>Power/type I error calculation for two groups (treatment and control group, no covariates) with random a0</p></a></li>
<li><a href='#two.grp.fixed.a0'><p>Model fitting for two groups (treatment and control group, no covariates) with fixed a0</p></a></li>
<li><a href='#two.grp.random.a0'><p>Model fitting for two groups (treatment and control group, no covariates) with random a0</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Bayesian Power Prior Design</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-03</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian power/type I error calculation and model fitting using 
  the power prior and the normalized power prior for generalized linear models.
  Detailed examples of applying the package are available at &lt;<a href="https://doi.org/10.32614%2FRJ-2023-016">doi:10.32614/RJ-2023-016</a>&gt;.
  Models for time-to-event outcomes are implemented in the R package 'BayesPPDSurv'.
  The Bayesian clinical trial design methodology is described in Chen et al. (2011) 
  &lt;<a href="https://doi.org/10.1111%2Fj.1541-0420.2011.01561.x">doi:10.1111/j.1541-0420.2011.01561.x</a>&gt;, and Psioda and Ibrahim (2019) 
  &lt;<a href="https://doi.org/10.1093%2Fbiostatistics%2Fkxy009">doi:10.1093/biostatistics/kxy009</a>&gt;. The normalized power prior is described in Duan et al. (2006) 
  &lt;<a href="https://doi.org/10.1002%2Fenv.752">doi:10.1002/env.752</a>&gt; and Ibrahim et al. (2015) &lt;<a href="https://doi.org/10.1002%2Fsim.6728">doi:10.1002/sim.6728</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppEigen, RcppNumerical</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr, testthat (&ge; 3.0.0), ggplot2, kableExtra</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-11 03:12:29 UTC; angie</td>
</tr>
<tr>
<td>Author:</td>
<td>Yueqi Shen [aut, cre],
  Matthew A. Psioda [aut],
  Joseph G. Ibrahim [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yueqi Shen &lt;angieshen6@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-13 19:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='BayesPPD-package'>Bayesian sample size determination using the power and normalized power prior for generalized linear models</h2><span id='topic+BayesPPD-package'></span>

<h3>Description</h3>

<p>The <span class="pkg">BayesPPD</span> (Bayesian Power Prior Design) package provides two categories of functions:
functions for Bayesian power/type I error calculation and functions for model fitting.
Supported distributions include normal, binary (Bernoulli/binomial), Poisson and exponential.
The power parameter <code class="reqn">a_0</code> can be fixed or modeled as random using a normalized power prior.
</p>


<h3>Details</h3>

<p>Following Chen et al.(2011), for two group models (i.e., treatment and control group with no covariates), denote the parameter for the treatment group by <code class="reqn">\mu_t</code>
and the parameter for the control group by <code class="reqn">\mu_c</code>. Suppose there are <code class="reqn">K</code> historical datasets <code class="reqn">D_0 = (D_{01},\cdots, D_{0K})'</code>. We consider the following normalized power prior
for <code class="reqn">\mu_c</code> given multiple historical datasets <code class="reqn">D_0</code>
</p>
<p style="text-align: center;"><code class="reqn">\pi(\mu_c|D_0,a_0) = \frac{1}{C(a_0)}\prod_{k=1}^K \left[L(\mu_c|D_{0k})^{a_{0k}}\right]\pi_0(\mu_c)</code>
</p>

<p>where <code class="reqn">a_0 = (a_{01},\cdots,a_{0K})'</code>, <code class="reqn">0\le a_{0k} \le 1</code> for <code class="reqn">k=1,\cdots,K</code>, <code class="reqn">L(\mu_c|D_{0k})</code> is the historical data likelihood,
<code class="reqn">\pi_0(\mu_c)</code> is an initial prior, and <code class="reqn">C(a_0)=\int \prod_{k=1}^K [L(\mu_c|D_{0k})^{a_{0k}}]\pi_0(\mu_c)d\mu_c</code>. When <code class="reqn">a_0</code> is fixed,
the normalized power prior is equivalent to the power prior
</p>
<p style="text-align: center;"><code class="reqn">\pi(\mu_c|D_0,a_0) = \prod_{k=1}^K \left[L(\mu_c|D_{0k})^{a_{0k}}\right]\pi_0(\mu_c).</code>
</p>

<p>By default, the power/type I error calculation algorithm assumes the null and alternative hypotheses are given by
</p>
<p style="text-align: center;"><code class="reqn">H_0: \mu_t - \mu_c \ge \delta</code>
</p>
<p> and </p>
<p style="text-align: center;"><code class="reqn">H_1: \mu_t - \mu_c &lt; \delta,</code>
</p>
<p> where <code class="reqn">\delta</code> is a prespecified constant. To test hypotheses of
the opposite direction, i.e., <code class="reqn">H_0: \mu_t - \mu_c \le \delta</code> and <code class="reqn">H_1: \mu_t - \mu_c &gt; \delta</code> , one can set the parameter <code>nullspace.ineq</code> to &quot;&lt;&quot;.
To determine Bayesian sample size, we estimate the quantity </p>
<p style="text-align: center;"><code class="reqn">\beta_{sj}^{(n)}=E_s[I\{P(\mu_t-\mu_c&lt;\delta|y^{(n)}, \pi^{(f)})\ge \gamma\}]</code>
</p>

<p>where <code class="reqn">\gamma &gt; 0</code> is a prespecified posterior probability threshold for rejecting the null hypothesis (e.g., <code class="reqn">0.975</code>), the probability is computed with respect to the posterior distribution given the data
<code class="reqn">y^{(n)}</code> and the fitting prior <code class="reqn">\pi^{(f)}</code>, and the expectation is taken with respect to the marginal distribution of <code class="reqn">y^{(n)}</code>
defined based on the sampling prior <code class="reqn">\pi^{(s)}(\theta)</code>, where <code class="reqn">\theta=(\mu_t, \mu_c, \eta)</code> and <code class="reqn">\eta</code> denotes any nuisance parameter in the model.
Let <code class="reqn">\Theta_0</code> and <code class="reqn">\Theta_1</code> denote the parameter spaces corresponding to <code class="reqn">H_0</code> and <code class="reqn">H_1</code>.
Let <code class="reqn">\pi_0^{(s)}(\theta)</code> denote a sampling prior that puts mass in the null region, i.e., <code class="reqn">\theta \subset \Theta_0</code>.
Let <code class="reqn">\pi_1^{(s)}(\theta)</code> denote a sampling prior that puts mass in the alternative region, i.e., <code class="reqn">\theta \subset \Theta_1</code>.
Then <code class="reqn">\beta_{s0}^{(n)}</code> corresponding to <code class="reqn">\pi^{(s)}(\theta)=\pi_0^{(s)}(\theta)</code> is a Bayesian type I error,
while <code class="reqn">\beta_{s1}^{(n)}</code> corresponding to <code class="reqn">\pi^{(s)}(\theta)=\pi_1^{(s)}(\theta)</code> is a Bayesian power.
We compute <code class="reqn">n_{\alpha_0} = \min\{n: \beta_{s0}^{(n)} \le \alpha_0\}</code> and <code class="reqn">n_{\alpha_1} = \min\{n: \beta_{s1}^{(n)} \ge 1-\alpha_1\}</code>.
Then Bayesian sample size is max<code class="reqn">\{n_{\alpha_0}, n_{\alpha_1}\}</code>. Choosing <code class="reqn">\alpha_0=0.05</code> and <code class="reqn">\alpha_1=0.2</code>
guarantees that the Bayesian type I error rate is at most <code class="reqn">0.05</code> and the Bayesian power is at least <code class="reqn">0.8</code>.
</p>
<p>To compute <code class="reqn">\beta_{sj}^{(n)}</code>, the following algorithm is used:
</p>

<dl>
<dt>Step 1:</dt><dd><p>Generate <code class="reqn">\theta \sim \pi_j^{(s)}(\theta)</code></p>
</dd>
<dt>Step 2:</dt><dd><p>Generate <code class="reqn">y^{(n)} \sim f(y^{(n)}|\theta)</code></p>
</dd>
<dt>Step 3:</dt><dd><p>Compute <code class="reqn">P(\mu_t &lt; \mu_c + \delta|y^{(n)}, \pi^{(f)})</code></p>
</dd>
<dt>Step 4:</dt><dd><p>Check whether <code class="reqn">P(\mu_t &lt; \mu_c + \delta|y^{(n)}, \pi^{(f)}) \ge \gamma</code></p>
</dd>
<dt>Step 5:</dt><dd><p>Repeat Steps 1-4 <code class="reqn">N</code> times</p>
</dd>
<dt>Step 6:</dt><dd><p>Compute the proportion of times that <code class="reqn">\{\mu_t &lt; \mu_c + \delta|y^{(n)}, \pi^{(f)} \ge \gamma\}</code> is true out of the <code class="reqn">N</code> simulated datasets, which gives an estimate of <code class="reqn">\beta_{sj}^{(n)}</code>.</p>
</dd>
</dl>

<p>For positive continuous data assumed to follow exponential distribution, the hypotheses are given by
</p>
<p style="text-align: center;"><code class="reqn">H_0: \mu_t/\mu_c \ge \delta</code>
</p>
<p> and </p>
<p style="text-align: center;"><code class="reqn">H_1: \mu_t/\mu_c &lt; \delta,</code>
</p>
<p> where <code class="reqn">\mu_t</code> and <code class="reqn">\mu_c</code> are the hazards for the treatment and the control group, respectively.
The definition of <code class="reqn">\beta_{sj}^{(n)}</code> and the algorithm change accordingly.
</p>
<p>If there are covariates to adjust for, we assume the first column of the covariate matrix is the treatment indicator,
and the corresponding parameter is <code class="reqn">\beta_1</code>, which, for example, corresponds to a difference in means for the linear regression model and a log hazard ratio for the exponential regression model.
The hypotheses are given by
</p>
<p style="text-align: center;"><code class="reqn">H_0: \beta_1 \ge \delta</code>
</p>
<p> and </p>
<p style="text-align: center;"><code class="reqn">H_1: \beta_1 &lt; \delta.</code>
</p>

<p>The definition of <code class="reqn">\beta_{sj}^{(n)}</code> and the algorithm change accordingly.
</p>
<p>By default, the package assumes the historical data is
composed of control group subjects only. If the user wants to use historical data to inform treatment effect, one can set <code>borrow.treat=TRUE</code>
and include the treatment indicator in the historical covariate matrix.
</p>
<p>This implementation of the method does not assume any particular distribution for the sampling priors.
The user is allowed to specify a vector or matrix of samples for <code class="reqn">\theta</code> (matrix if <code class="reqn">\theta</code> is of dimension &gt;1) from any distribution, and the algorithm samples with replacement
from the vector or matrix at each iteration of data simulation. In order to accurately approximate a joint distribution
for multiple parameters, the number of iterations should be large (e.g., 10,000).
</p>
<p>Gibbs sampling is used for normally distributed data. Slice sampling is used for all other data distributions.
For two group models with fixed <code class="reqn">a_0</code>,
numerical integration using the <span class="pkg">RcppNumerical</span> package is used.
</p>


<h3>References</h3>

<p>Chen, Ming-Hui, et al. &quot;Bayesian design of noninferiority trials for medical devices using historical data.&quot; Biometrics 67.3 (2011): 1163-1170.
</p>

<hr>
<h2 id='actg019'>AIDS Clinical Trial ACTG019 (1990).</h2><span id='topic+actg019'></span>

<h3>Description</h3>

<p>A dataset containing the ACTG019 clinical trial placebo group data (1990) in adults with asymptomatic HIV.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>actg019
</code></pre>


<h3>Format</h3>

<p>A data frame with 404 rows and 4 variables:
</p>

<dl>
<dt>outcome</dt><dd><p>binary variable with 1 indicating death, development of AIDS or ARC and 0 otherwise</p>
</dd>
<dt>age</dt><dd><p>patient age in years</p>
</dd>
<dt>race</dt><dd><p>binary variable with 1 indicating white and 0 otherwise</p>
</dd>
<dt>T4count</dt><dd><p>CD4 cell count (cell count per cubicmillimetre of serum)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Chen, Ming-Hui, et al. &quot;Prior Elicitation, Variable Selection and Bayesian Computation for Logistic Regression Models.&quot; Journal of the Royal Statistical Society. Series B, vol. 61, no. 1, 1999, pp. 223-242.
</p>

<hr>
<h2 id='actg036'>AIDS Clinical Trial ACTG036 (1991).</h2><span id='topic+actg036'></span>

<h3>Description</h3>

<p>A dataset containing the ACTG036 clinical trial data (1991) comparing zidovudine (AZT) with a placebo in asymptomatic patients with hereditary coagulation disorders and HIV infection.
The ACTG036 trial had the same response variable and covariates as the ACTG019 study. The ATCG019 data can be used as a historical dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>actg036
</code></pre>


<h3>Format</h3>

<p>A data frame with 183 rows and 5 variables:
</p>

<dl>
<dt>outcome</dt><dd><p>binary variable with 1 indicating death, development of AIDS or ARC and 0 otherwise</p>
</dd>
<dt>treat</dt><dd><p>binary variable with 1 indicating Zidovudine (AZT) treatment and 0 indicating placebo</p>
</dd>
<dt>age</dt><dd><p>patient age in years</p>
</dd>
<dt>race</dt><dd><p>binary variable with 1 indicating white and 0 otherwise</p>
</dd>
<dt>T4count</dt><dd><p>CD4 cell count (cell count per cubicmillimetre of serum)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Chen, Ming-Hui, et al. &quot;Prior Elicitation, Variable Selection and Bayesian Computation for Logistic Regression Models.&quot; Journal of the Royal Statistical Society. Series B, vol. 61, no. 1, 1999, pp. 223-242.
</p>

<hr>
<h2 id='glm.fixed.a0'>Model fitting for generalized linear models with fixed a0</h2><span id='topic+glm.fixed.a0'></span>

<h3>Description</h3>

<p>Model fitting using power priors for generalized linear models with fixed <code class="reqn">a_0</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm.fixed.a0(
  data.type,
  data.link,
  y = 0,
  x = matrix(),
  n = 1,
  borrow.treat = FALSE,
  historical = list(),
  lower.limits = rep(-100, 50),
  upper.limits = rep(100, 50),
  slice.widths = rep(1, 50),
  nMC = 10000,
  nBI = 250,
  current.data = TRUE,
  prior.beta.var = rep(10, 50)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="glm.fixed.a0_+3A_data.type">data.type</code></td>
<td>
<p>Character string specifying the type of response. The options are &quot;Normal&quot;, &quot;Bernoulli&quot;, &quot;Binomial&quot;, &quot;Poisson&quot; and &quot;Exponential&quot;.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_data.link">data.link</code></td>
<td>
<p>Character string specifying the link function. The options are &quot;Logistic&quot;, &quot;Probit&quot;, &quot;Log&quot;, &quot;Identity-Positive&quot;, &quot;Identity-Probability&quot; and &quot;Complementary Log-Log&quot;. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_y">y</code></td>
<td>
<p>Vector of responses.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_x">x</code></td>
<td>
<p>Matrix of covariates. The first column should be the treatment indicator with 1 indicating treatment group. The number of rows should equal the length of the response vector <code>y</code>.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_n">n</code></td>
<td>
<p>(For binomial data only) vector of integers specifying the number of subjects who have a particular value of the covariate vector. If the data is binary and all covariates are discrete, collapsing Bernoulli data into a binomial structure can make the slice sampler much faster.
The length of <code>n</code> should be equal to the number of rows of <code>x</code>.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_borrow.treat">borrow.treat</code></td>
<td>
<p>Logical value indicating whether the historical information is used to inform the treatment effect parameter. The default value is FALSE. If TRUE, the first column of the historical covariate matrix must be the treatment indicator.
If FALSE, the historical covariate matrix must NOT have the treatment indicator, since the historical data is assumed to be from the control group only.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_historical">historical</code></td>
<td>
<p>(Optional) list of historical dataset(s). East historical dataset is stored in a list which contains three <em>named</em> elements: <code>y0</code>, <code>x0</code> and <code>a0</code>.
</p>

<ul>
<li> <p><code>y0</code> is a vector of responses.
</p>
</li>
<li> <p><code>x0</code> is a matrix of covariates. If <code>borrow.treat</code> is FALSE (the default), <code>x0</code> should NOT have the treatment indicator. Apart from missing the treatment indicator, <code>x0</code> should have the same set of covariates in the same order as <code>x</code>.
If <code>borrow.treat</code> is TRUE, <code>x0</code> should have the same set of covariates in the same order as <code>x</code>, where the first column of <code>x0</code> must be the treatment indicator.
</p>
</li>
<li> <p><code>a0</code> is a number between 0 and 1 indicating the discounting parameter value for that historical dataset.
</p>
</li></ul>

<p>For binomial data, an additional element <code>n0</code> is required.
</p>

<ul>
<li> <p><code>n0</code> is vector of integers specifying the number of subjects who have a particular value of the covariate vector.
The length of <code>n0</code> should be equal to the number of rows of <code>x0</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_lower.limits">lower.limits</code></td>
<td>
<p>Vector of lower limits for parameters to be used by the slice sampler. The length of the vector should be equal to the total number of parameters, i.e. P+1 where P is the number of covariates. The default is -100 for all parameters (may not be appropriate for all situations). Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_upper.limits">upper.limits</code></td>
<td>
<p>Vector of upper limits for parameters to be used by the slice sampler. The length of the vector should be equal to the total number of parameters, i.e. P+1 where P is the number of covariates. The default is 100 for all parameters (may not be appropriate for all situations). Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_slice.widths">slice.widths</code></td>
<td>
<p>Vector of initial slice widths for parameters to be used by the slice sampler. The length of the vector should be equal to the total number of parameters, i.e. P+1 where P is the number of covariates. The default is 1 for all parameter (may not be appropriate for all situations). Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_nmc">nMC</code></td>
<td>
<p>Number of iterations (excluding burn-in samples) for the slice sampler or Gibbs sampler. The default is 10,000.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_nbi">nBI</code></td>
<td>
<p>Number of burn-in samples for the slice sampler or Gibbs sampler. The default is 250.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_current.data">current.data</code></td>
<td>
<p>Logical value indicating whether current data is included. The default is TRUE. If FALSE, only historical data is included in the analysis,
and the posterior samples can be used as a discrete approximation to the sampling prior in <code><a href="#topic+power.glm.fixed.a0">power.glm.fixed.a0</a></code>.</p>
</td></tr>
<tr><td><code id="glm.fixed.a0_+3A_prior.beta.var">prior.beta.var</code></td>
<td>
<p>Only applies if current.data = FALSE. If no current data is provided, the initial priors used for <code class="reqn">\beta</code> are i.i.d. normal distributions with mean zero and variance equal to <code>prior.beta.var</code>.
The length of the vector should be equal to the length of <code class="reqn">\beta</code>. The default variance is 10.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>data.type</code> is &quot;Normal&quot;, the response <code class="reqn">y_i</code> is assumed to follow <code class="reqn">N(x_i'\beta, \tau^{-1})</code> where <code class="reqn">x_i</code> is the vector of covariates for subject <code class="reqn">i</code>.
Each historical dataset <code class="reqn">D_{0k}</code> is assumed to have a different precision parameter <code class="reqn">\tau_k</code>.
The initial prior for <code class="reqn">\tau</code> is the Jeffery's prior, <code class="reqn">\tau^{-1}</code>, and the initial prior for <code class="reqn">\tau_k</code> is <code class="reqn">\tau_k^{-1}</code>.
The initial prior for <code class="reqn">\beta</code> is the uniform improper prior. Posterior samples are obtained through Gibbs sampling.
</p>
<p>For all other data types, posterior samples are obtained through slice sampling.
The default lower limits for the parameters are -100. The default upper limits
for the parameters are 100. The default slice widths for the parameters are 1.
The defaults may not be appropriate for all situations, and the user can specify the appropriate limits
and slice width for each parameter.
</p>
<p>When <code>current.data</code> is set to FALSE, only historical data is included in the analysis,
and the posterior samples can be used as a discrete approximation to the sampling prior in <code><a href="#topic+power.glm.fixed.a0">power.glm.fixed.a0</a></code>.
</p>


<h3>Value</h3>

<p>The function returns a S3 object with a <code>summary</code> method. If <code>data.type</code> is &quot;Normal&quot;, posterior samples of <code class="reqn">\beta</code>, <code class="reqn">\tau</code> and <code class="reqn">\tau_k</code>'s (if historical data is given) are returned.
For all other data types, a matrix of posterior samples of <code class="reqn">\beta</code> is returned. The first column contains posterior samples of the intercept.
The second column contains posterior samples of <code class="reqn">\beta_1</code>, the parameter for the treatment indicator.
</p>


<h3>References</h3>

<p>Neal, Radford M. Slice sampling. Ann. Statist. 31 (2003), no. 3, 705&ndash;767.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+power.glm.fixed.a0">power.glm.fixed.a0</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.type &lt;- "Bernoulli"
data.link &lt;- "Logistic"

# Simulate current data
set.seed(1)
p &lt;- 3
n_total &lt;- 100
y &lt;- rbinom(n_total,size=1,prob=0.6)
# The first column of x is the treatment indicator.
x &lt;- cbind(rbinom(n_total,size=1,prob=0.5),
           matrix(rnorm(p*n_total),ncol=p,nrow=n_total))

# Simulate two historical datasets
# Note that x0 does not have the treatment indicator
historical &lt;- list(list(y0=rbinom(n_total,size=1,prob=0.2),
                        x0=matrix(rnorm(p*n_total),ncol=p,nrow=n_total), a0=0.2),
                   list(y0=rbinom(n_total, size=1, prob=0.5),
                        x0=matrix(rnorm(p*n_total),ncol=p,nrow=n_total), a0=0.3))

# Set parameters of the slice sampler
lower.limits &lt;- rep(-100, 5) # The dimension is the number of columns of x plus 1 (intercept)
upper.limits &lt;- rep(100, 5)
slice.widths &lt;- rep(1, 5)

nMC &lt;- 1000 # nMC should be larger in practice
nBI &lt;- 250
result &lt;- glm.fixed.a0(data.type=data.type, data.link=data.link, y=y, x=x, historical=historical,
                       lower.limits=lower.limits, upper.limits=upper.limits,
                       slice.widths=slice.widths, nMC=nMC, nBI=nBI)

summary(result)

</code></pre>

<hr>
<h2 id='glm.random.a0'>Model fitting for generalized linear models with random a0</h2><span id='topic+glm.random.a0'></span>

<h3>Description</h3>

<p>Model fitting using normalized power priors for generalized linear models with random <code class="reqn">a_0</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm.random.a0(
  data.type,
  data.link,
  y,
  x,
  n = 1,
  borrow.treat = FALSE,
  historical,
  prior.beta.var = rep(10, 50),
  prior.a0.shape1 = rep(1, 10),
  prior.a0.shape2 = rep(1, 10),
  a0.coefficients,
  lower.limits = NULL,
  upper.limits = NULL,
  slice.widths = rep(0.1, 50),
  nMC = 10000,
  nBI = 250
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="glm.random.a0_+3A_data.type">data.type</code></td>
<td>
<p>Character string specifying the type of response. The options are &quot;Normal&quot;, &quot;Bernoulli&quot;, &quot;Binomial&quot;, &quot;Poisson&quot; and &quot;Exponential&quot;.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_data.link">data.link</code></td>
<td>
<p>Character string specifying the link function. The options are &quot;Logistic&quot;, &quot;Probit&quot;, &quot;Log&quot;, &quot;Identity-Positive&quot;, &quot;Identity-Probability&quot; and &quot;Complementary Log-Log&quot;. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_y">y</code></td>
<td>
<p>Vector of responses.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_x">x</code></td>
<td>
<p>Matrix of covariates. The first column should be the treatment indicator with 1 indicating treatment group. The number of rows should equal the length of the response vector <code>y</code>.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_n">n</code></td>
<td>
<p>(For binomial data only) vector of integers specifying the number of subjects who have a particular value of the covariate vector. If the data is binary and all covariates are discrete, collapsing Bernoulli data into a binomial structure can make the slice sampler much faster.
The sum of <code>n</code> should be equal to <code>data.size</code>. The length of <code>n</code> should be equal to the number of rows of <code>x0</code>.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_borrow.treat">borrow.treat</code></td>
<td>
<p>Logical value indicating whether the historical information is used to inform the treatment effect parameter. The default value is FALSE. If TRUE, the first column of the historical covariate matrix must be the treatment indicator.
If FALSE, the historical covariate matrix must NOT have the treatment indicator, since the historical data is assumed to be from the control group only.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_historical">historical</code></td>
<td>
<p>List of historical dataset(s). East historical dataset is stored in a list which contains two <em>named</em> elements: <code>y0</code> and <code>x0</code>.
</p>

<ul>
<li> <p><code>y0</code> is a vector of responses.
</p>
</li>
<li> <p><code>x0</code> is a matrix of covariates. If <code>borrow.treat</code> is FALSE (the default), <code>x0</code> should NOT have the treatment indicator. Apart from missing the treatment indicator, <code>x0</code> should have the same set of covariates in the same order as <code>x</code>.
If <code>borrow.treat</code> is TRUE, <code>x0</code> should have the same set of covariates in the same order as <code>x</code>, where the first column of <code>x0</code> must be the treatment indicator.
</p>
</li></ul>

<p>For binomial data, an additional element <code>n0</code> is required.
</p>

<ul>
<li> <p><code>n0</code> is vector of integers specifying the number of subjects who have a particular value of the covariate vector.
The length of <code>n0</code> should be equal to the number of rows of <code>x0</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_prior.beta.var">prior.beta.var</code></td>
<td>
<p>Vector of variances of the independent normal initial priors on <code class="reqn">\beta</code> with mean zero. The length of the vector should be equal to the length of <code class="reqn">\beta</code>. The default variance is 10.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_prior.a0.shape1">prior.a0.shape1</code></td>
<td>
<p>Vector of the first shape parameters of the independent beta priors for <code class="reqn">a_0</code>. The length of the vector should be equal to the number of historical datasets. The default is a vector of one's.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_prior.a0.shape2">prior.a0.shape2</code></td>
<td>
<p>Vector of the second shape parameters of the independent beta priors for <code class="reqn">a_0</code>. The length of the vector should be equal to the number of historical datasets. The default is a vector of one's.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_a0.coefficients">a0.coefficients</code></td>
<td>
<p>Vector of coefficients for <code class="reqn">a_0</code> returned by the function <code><a href="#topic+normalizing.constant">normalizing.constant</a></code>. This is necessary for estimating the normalizing constant for the normalized power prior. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_lower.limits">lower.limits</code></td>
<td>
<p>Vector of lower limits for parameters to be used by the slice sampler. If <code>data.type</code> is &quot;Normal&quot;, slice sampling is used for <code class="reqn">a_0</code>, and the length of the vector should be equal to the number of historical datasets.
For all other data types, slice sampling is used for <code class="reqn">\beta</code> and <code class="reqn">a_0</code>. The first P+1 elements apply to the sampling of <code class="reqn">\beta</code> and the rest apply to the sampling of <code class="reqn">a_0</code>.
The length of the vector should be equal to the sum of the total number of parameters (i.e. P+1 where P is the number of covariates) and the number of historical datasets.
The default is -100 for <code class="reqn">\beta</code> and 0 for <code class="reqn">a_0</code> (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_upper.limits">upper.limits</code></td>
<td>
<p>Vector of upper limits for parameters to be used by the slice sampler. If <code>data.type</code> is &quot;Normal&quot;, slice sampling is used for <code class="reqn">a_0</code>, and the length of the vector should be equal to the number of historical datasets.
For all other data types, slice sampling is used for <code class="reqn">\beta</code> and <code class="reqn">a_0</code>. The first P+1 elements apply to the sampling of <code class="reqn">\beta</code> and the rest apply to the sampling of <code class="reqn">a_0</code>.
The length of the vector should be equal to the sum of the total number of parameters (i.e. P+1 where P is the number of covariates) and the number of historical datasets.
The default is 100 for <code class="reqn">\beta</code> and 1 for <code class="reqn">a_0</code>  (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_slice.widths">slice.widths</code></td>
<td>
<p>Vector of initial slice widths used by the slice sampler. If <code>data.type</code> is &quot;Normal&quot;, slice sampling is used for <code class="reqn">a_0</code>, and the length of the vector should be equal to the number of historical datasets.
For all other data types, slice sampling is used for <code class="reqn">\beta</code> and <code class="reqn">a_0</code>. The first P+1 elements apply to the sampling of <code class="reqn">\beta</code> and the rest apply to the sampling of <code class="reqn">a_0</code>.
The length of the vector should be equal to the sum of the total number of parameters (i.e. P+1 where P is the number of covariates) and the number of historical datasets.
The default is 0.1 for all parameter (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_nmc">nMC</code></td>
<td>
<p>Number of iterations (excluding burn-in samples) for the slice sampler or Gibbs sampler. The default is 10,000.</p>
</td></tr>
<tr><td><code id="glm.random.a0_+3A_nbi">nBI</code></td>
<td>
<p>Number of burn-in samples for the slice sampler or Gibbs sampler. The default is 250.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The user should use the function <code><a href="#topic+normalizing.constant">normalizing.constant</a></code> to obtain <code>a0.coefficients</code> (does not apply if <code>data.type</code> is &quot;Normal&quot;).
</p>
<p>If <code>data.type</code> is &quot;Normal&quot;, the response <code class="reqn">y_i</code> is assumed to follow <code class="reqn">N(x_i'\beta, \tau^{-1})</code> where <code class="reqn">x_i</code> is the vector of covariates for subject <code class="reqn">i</code>.
Historical datasets are assumed to have the same precision parameter as the current dataset for computational simplicity.
The initial prior for <code class="reqn">\tau</code> is the Jeffery's prior, <code class="reqn">\tau^{-1}</code>.
Independent normal priors with mean zero and variance <code>prior.beta.var</code> are used for <code class="reqn">\beta</code> to ensure the propriety of the normalized power prior. Posterior samples for <code class="reqn">\beta</code> and <code class="reqn">\tau</code> are obtained through Gibbs sampling.
Independent beta(<code>prior.a0.shape1</code>, <code>prior.a0.shape1</code>) priors are used for <code class="reqn">a_0</code>. Posterior samples for <code class="reqn">a_0</code> are obtained through slice sampling.
</p>
<p>For all other data types, posterior samples are obtained through slice sampling.
The default lower limits are -100 for <code class="reqn">\beta</code> and 0 for <code class="reqn">a_0</code>. The default upper limits
for the parameters are 100 for <code class="reqn">\beta</code> and 1 for <code class="reqn">a_0</code>. The default slice widths for the parameters are 0.1.
The defaults may not be appropriate for all situations, and the user can specify the appropriate limits
and slice width for each parameter.
</p>


<h3>Value</h3>

<p>The function returns a S3 object with a <code>summary</code> method. If <code>data.type</code> is &quot;Normal&quot;, posterior samples of <code class="reqn">\beta</code>, <code class="reqn">\tau</code> and <code class="reqn">a_0</code> are returned.
For all other data types, posterior samples of <code class="reqn">\beta</code> and <code class="reqn">a_0</code> are returned.
The first column of the matrix of posterior samples of <code class="reqn">\beta</code> contains posterior samples of the intercept.
The second column contains posterior samples of <code class="reqn">\beta_1</code>, the parameter for the treatment indicator.
</p>


<h3>References</h3>

<p>Neal, Radford M. Slice sampling. Ann. Statist. 31 (2003), no. 3, 705&ndash;767.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normalizing.constant">normalizing.constant</a></code> and <code><a href="#topic+power.glm.random.a0">power.glm.random.a0</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.type &lt;- "Bernoulli"
data.link &lt;- "Logistic"

# Simulate current data
set.seed(1)
p &lt;- 3
n_total &lt;- 100
y &lt;- rbinom(n_total,size=1,prob=0.6)
# The first column of x is the treatment indicator.
x &lt;- cbind(rbinom(n_total,size=1,prob=0.5),
           matrix(rnorm(p*n_total),ncol=p,nrow=n_total))

# Simulate two historical datasets
# Note that x0 does not have the treatment indicator
historical &lt;- list(list(y0=rbinom(n_total,size=1,prob=0.2),
                        x0=matrix(rnorm(p*n_total),ncol=p,nrow=n_total)),
                   list(y0=rbinom(n_total, size=1, prob=0.5),
                        x0=matrix(rnorm(p*n_total),ncol=p,nrow=n_total)))

# Please see function "normalizing.constant" for how to obtain a0.coefficients
# Here, suppose one-degree polynomial regression is chosen by the "normalizing.constant"
# function. The coefficients are obtained for the intercept, a0_1 and a0_2.
a0.coefficients &lt;- c(1, 0.5, -1)

# Set parameters of the slice sampler
# The dimension is the number of columns of x plus 1 (intercept)
# plus the number of historical datasets
lower.limits &lt;- c(rep(-100, 5), rep(0, 2))
upper.limits &lt;- c(rep(100, 5), rep(1, 2))
slice.widths &lt;- rep(0.1, 7)

nMC &lt;- 500 # nMC should be larger in practice
nBI &lt;- 100
result &lt;- glm.random.a0(data.type=data.type, data.link=data.link, y=y, x=x,
                        historical=historical, a0.coefficients=a0.coefficients,
                        lower.limits=lower.limits, upper.limits=upper.limits,
                        slice.widths=slice.widths, nMC=nMC, nBI=nBI)
summary(result)

</code></pre>

<hr>
<h2 id='normalizing.constant'>Function for approximating the normalizing constant for generalized linear models with random a0</h2><span id='topic+normalizing.constant'></span>

<h3>Description</h3>

<p>This function returns a vector of coefficients that defines a function <code class="reqn">f(a_0)</code> that approximates the normalizing constant for generalized linear models with random <code class="reqn">a_0</code>.
The user should input the values returned to <code><a href="#topic+glm.random.a0">glm.random.a0</a></code> or <code><a href="#topic+power.glm.random.a0">power.glm.random.a0</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalizing.constant(
  grid,
  historical,
  data.type,
  data.link,
  prior.beta.var = rep(10, 50),
  lower.limits = rep(-100, 50),
  upper.limits = rep(100, 50),
  slice.widths = rep(1, 50),
  nMC = 10000,
  nBI = 250
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normalizing.constant_+3A_grid">grid</code></td>
<td>
<p>Matrix of potential values for <code class="reqn">a_0</code>, where the number of columns should equal the number of historial datasets. Note that the algorithm may fail if some grid values are close to zero. See <em>Details</em> below.</p>
</td></tr>
<tr><td><code id="normalizing.constant_+3A_historical">historical</code></td>
<td>
<p>List of historical dataset(s). East historical dataset is stored in a list which constains two <em>named</em> elements: <code>y0</code> and <code>x0</code>.
</p>

<ul>
<li> <p><code>y0</code> is a vector of responses.
</p>
</li>
<li> <p><code>x0</code> is a matrix of covariates.
</p>
</li></ul>

<p>For binomial data, an additional element <code>n0</code> is required.
</p>

<ul>
<li> <p><code>n0</code> is vector of integers specifying the number of subjects who have a particular value of the covariate vector.
</p>
</li></ul>
</td></tr>
<tr><td><code id="normalizing.constant_+3A_data.type">data.type</code></td>
<td>
<p>Character string specifying the type of response. The options are &quot;Bernoulli&quot;, &quot;Binomial&quot;, &quot;Poisson&quot; and &quot;Exponential&quot;.</p>
</td></tr>
<tr><td><code id="normalizing.constant_+3A_data.link">data.link</code></td>
<td>
<p>Character string specifying the link function. The options are &quot;Logistic&quot;, &quot;Probit&quot;, &quot;Log&quot;, &quot;Identity-Positive&quot;, &quot;Identity-Probability&quot; and &quot;Complementary Log-Log&quot;. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="normalizing.constant_+3A_prior.beta.var">prior.beta.var</code></td>
<td>
<p>Vector of variances of the independent normal initial priors on <code class="reqn">\beta</code> with mean zero. The length of the vector should be equal to the length of <code class="reqn">\beta</code>. The default variance is 10.</p>
</td></tr>
<tr><td><code id="normalizing.constant_+3A_lower.limits">lower.limits</code></td>
<td>
<p>Vector of lower limits for parameters to be used by the slice sampler. The length of the vector should be equal to the total number of parameters, i.e. P+1 where P is the number of covariates. The default is -100 for all parameters (may not be appropriate for all situations). Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="normalizing.constant_+3A_upper.limits">upper.limits</code></td>
<td>
<p>Vector of upper limits for parameters to be used by the slice sampler. The length of the vector should be equal to the total number of parameters, i.e. P+1 where P is the number of covariates. The default is 100 for all parameters (may not be appropriate for all situations). Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="normalizing.constant_+3A_slice.widths">slice.widths</code></td>
<td>
<p>Vector of initial slice widths for parameters to be used by the slice sampler. The length of the vector should be equal to the total number of parameters, i.e. P+1 where P is the number of covariates. The default is 1 for all parameter (may not be appropriate for all situations). Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="normalizing.constant_+3A_nmc">nMC</code></td>
<td>
<p>Number of iterations (excluding burn-in samples) for the slice sampler or Gibbs sampler. The default is 10,000.</p>
</td></tr>
<tr><td><code id="normalizing.constant_+3A_nbi">nBI</code></td>
<td>
<p>Number of burn-in samples for the slice sampler or Gibbs sampler. The default is 250.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs the following steps:
</p>

<ol>
<li><p>	Suppose there are K historical datasets. The user inputs a grid of M rows and K columns of potential values for <code class="reqn">a_0</code>. For example, one can choose the vector <code>v = c(0.1, 0.25, 0.5, 0.75, 1)</code>
and use <code>expand.grid(a0_1=v, a0_2=v, a0_3=v)</code> when <code class="reqn">K=3</code> to get a grid with <code class="reqn">M=5^3=125</code> rows and 3 columns. If there are more than three historical datasets, the dimension of <code>v</code> can be reduced
to limit the size of the grid. A large grid will increase runtime.
</p>
</li>
<li><p>	For each row of <code class="reqn">a_0</code> values in the grid, obtain <code class="reqn">M</code> samples for <code class="reqn">\beta</code> from the power prior associated with the current values of <code class="reqn">a_0</code> using the slice sampler.
</p>
</li>
<li><p>	For each of the M sets of posterior samples, execute the PWK algorithm (Wang et al., 2018) to estimate the log of normalizing constant <code class="reqn">d_1,...,d_M</code> for the normalized power prior.
</p>
</li>
<li><p>	At this point, one has a dataset with outcomes <code class="reqn">d_1,...,d_M</code> and predictors corresponding to the rows of the <code class="reqn">a_0</code> grid matrix. A polynomial regression is applied to estimate a function <code class="reqn">d=f(a0)</code>.
The degree of the polynomial regression is determined by the algorithm to ensure <code class="reqn">R^2 &gt; 0.99</code>.
</p>
</li>
<li><p>	The vector of coefficients from the polynomial regression model is returned by the function, which the user must input into <code><a href="#topic+glm.random.a0">glm.random.a0</a></code> or <code><a href="#topic+power.glm.random.a0">power.glm.random.a0</a></code>.
</p>
</li></ol>

<p>When a row of the <code>grid</code> contains elements that are close to zero, the resulting power prior will be flat and estimates of normalizing constants may be inaccurate.
Therefore, it is recommended that <code>grid</code> values should be at least 0.05.
</p>
<p>If one encounters the error message &quot;some coefficients are not defined because of singularities&quot;,
it could be due to the following factors: number of <code>grid</code> rows too large or too small, insufficient sample size of the historical data, insufficient number of iterations for the slice sampler,
or near-zero <code>grid</code> values.
</p>
<p>Note that due to computational intensity, the <code>normalizing.constant</code> function has not been evaluated for accuracy for high dimensional <code class="reqn">\beta</code> (e.g., dimension &gt; 10) or high dimensional <code class="reqn">a_0</code> (e.g., dimension &gt; 5).
</p>


<h3>Value</h3>

<p>Vector of coefficients for <code class="reqn">a_0</code> that defines a function <code class="reqn">f(a_0)</code> that approximates the normalizing constant, necessary for functions <code><a href="#topic+glm.random.a0">glm.random.a0</a></code> and <code><a href="#topic+power.glm.random.a0">power.glm.random.a0</a></code>.
The length of the vector is equal to 1+K*L where K is the number of historical datasets and L is the degree of the polynomial regression determined by the algorithm.
</p>


<h3>References</h3>

<p>Wang, Yu-Bo; Chen, Ming-Hui; Kuo, Lynn; Lewis, Paul O. A New Monte Carlo Method for Estimating Marginal Likelihoods. Bayesian Anal. 13 (2018), no. 2, 311&ndash;333.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.random.a0">glm.random.a0</a></code> and <code><a href="#topic+power.glm.random.a0">power.glm.random.a0</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.type &lt;- "Bernoulli"
data.link &lt;- "Logistic"
data.size &lt;- 50

# Simulate two historical datasets
p &lt;- 1
set.seed(111)
x1 &lt;- matrix(rnorm(p*data.size),ncol=p,nrow=data.size)
set.seed(222)
x2 &lt;- matrix(rnorm(p*data.size),ncol=p,nrow=data.size)
beta &lt;- c(1,2)
mean1 &lt;- exp(x1*beta)/(1+exp(x1*beta))
mean2 &lt;- exp(x2*beta)/(1+exp(x2*beta))
historical &lt;- list(list(y0=rbinom(data.size,size=1,prob=mean1),x0=x1),
                   list(y0=rbinom(data.size, size=1, prob=mean2),x0=x2))

# Create grid of possible values of a0 with two columns corresponding to a0_1 and a0_2
g &lt;- c(0.1, 0.25, 0.5, 0.75, 1)
grid &lt;- expand.grid(a0_1=g, a0_2=g)

nMC &lt;- 100 # nMC should be larger in practice
nBI &lt;- 50
result &lt;- normalizing.constant(grid=grid, historical=historical,
                               data.type=data.type, data.link=data.link,
                               nMC=nMC, nBI=nBI)
</code></pre>

<hr>
<h2 id='power.glm.fixed.a0'>Power/type I error calculation for generalized linear models with fixed a0</h2><span id='topic+power.glm.fixed.a0'></span>

<h3>Description</h3>

<p>Power/type I error calculation for generalized linear models with fixed <code class="reqn">a_0</code> using power priors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.glm.fixed.a0(
  data.type,
  data.link = "",
  data.size,
  n = 1,
  borrow.treat = FALSE,
  treat.assign.prob = 0.5,
  historical = list(),
  nullspace.ineq = "&gt;",
  x.samples = matrix(),
  samp.prior.beta,
  samp.prior.var = 0,
  lower.limits = rep(-100, 50),
  upper.limits = rep(100, 50),
  slice.widths = rep(1, 50),
  delta = 0,
  gamma = 0.95,
  nMC = 10000,
  nBI = 250,
  N = 10000,
  approximate = FALSE,
  nNR = 10000,
  tol = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="power.glm.fixed.a0_+3A_data.type">data.type</code></td>
<td>
<p>Character string specifying the type of response. The options are &quot;Normal&quot;, &quot;Bernoulli&quot;, &quot;Binomial&quot;, &quot;Poisson&quot; and &quot;Exponential&quot;.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_data.link">data.link</code></td>
<td>
<p>Character string specifying the link function. The options are &quot;Logistic&quot;, &quot;Probit&quot;, &quot;Log&quot;, &quot;Identity-Positive&quot;, &quot;Identity-Probability&quot; and &quot;Complementary Log-Log&quot;. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_data.size">data.size</code></td>
<td>
<p>Sample size of the simulated datasets.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_n">n</code></td>
<td>
<p>(For binomial data only) vector of integers specifying the number of subjects who have a particular value of the covariate vector. If the data is binary and all covariates are discrete, collapsing Bernoulli data into a binomial structure can make the slice sampler much faster.
The sum of <code>n</code> should be equal to <code>data.size</code>. The length of <code>n</code> should be equal to the number of rows of <code>x0</code>.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_borrow.treat">borrow.treat</code></td>
<td>
<p>Logical value indicating whether the historical information is used to inform the treatment effect parameter. The default value is FALSE. If TRUE, the first column of the historical covariate matrix must be the treatment indicator.
If FALSE, the historical covariate matrix must NOT have the treatment indicator, since the historical data is assumed to be from the control group only.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_treat.assign.prob">treat.assign.prob</code></td>
<td>
<p>Probability of being assigned to the treatment group. The default value is 0.5. Only applies if <code>borrow.treat=FALSE</code>.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_historical">historical</code></td>
<td>
<p>(Optional) list of historical dataset(s). East historical dataset is stored in a list which contains three <em>named</em> elements: <code>y0</code>, <code>x0</code> and <code>a0</code>.
</p>

<ul>
<li> <p><code>y0</code> is a vector of responses.
</p>
</li>
<li> <p><code>x0</code> is a matrix of covariates. If <code>borrow.treat</code> is FALSE (the default), <code>x0</code> should NOT have the treatment indicator.
If <code>borrow.treat</code> is TRUE, the first column of <code>x0</code> must be the treatment indicator.
</p>
</li>
<li> <p><code>a0</code> is a number between 0 and 1 indicating the discounting parameter value for that historical dataset.
</p>
</li></ul>

<p>For binomial data, an additional element <code>n0</code> is required.
</p>

<ul>
<li> <p><code>n0</code> is vector of integers specifying the number of subjects who have a particular value of the covariate vector.
The length of <code>n0</code> should be equal to the number of rows of <code>x0</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_nullspace.ineq">nullspace.ineq</code></td>
<td>
<p>Character string specifying the inequality of the null hypothesis. The options are &quot;&gt;&quot; and &quot;&lt;&quot;. If &quot;&gt;&quot; is specified, the null hypothesis is <code class="reqn">H_0</code>: <code class="reqn">\beta_1</code> <code class="reqn">\ge</code> <code class="reqn">\delta</code>. If &quot;&lt;&quot; is specified, the null hypothesis is <code class="reqn">H_0</code>: <code class="reqn">\beta_1</code> <code class="reqn">\le</code> <code class="reqn">\delta</code>. The default choice is &quot;&gt;&quot;.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_x.samples">x.samples</code></td>
<td>
<p>(Only applies when there is no historical dataset) matrix of possible values of covariates from which covariate vectors are sampled with replacement.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_samp.prior.beta">samp.prior.beta</code></td>
<td>
<p>Matrix of possible values of <code class="reqn">\beta</code> to sample (with replacement) from. Each row is a possible <code class="reqn">\beta</code> vector (a realization from the sampling prior for <code class="reqn">\beta</code>), where the first element is the coefficient for the intercept and the second element is the coefficient for the treatment indicator.
The length of the vector should be equal to the total number of parameters. If P is the number of columns of <code>x0</code> in <code>historical</code>, the total number of parameters is P+2 if <code>borrow.treat=FALSE</code>, and is P+1 if <code>borrow.treat=TRUE</code>.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_samp.prior.var">samp.prior.var</code></td>
<td>
<p>Vector of possible values of <code class="reqn">\sigma^2</code> to sample (with replacement) from. Only applies if <code>data.type</code> is &quot;Normal&quot;. The vector contains realizations from the sampling prior (e.g. inverse-gamma distribution) for <code class="reqn">\sigma^2</code>.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_lower.limits">lower.limits</code></td>
<td>
<p>Vector of lower limits for parameters to be used by the slice sampler. The length of the vector should be equal to the total number of parameters, i.e. P+1 where P is the number of covariates. The default is -100 for all parameters (may not be appropriate for all situations). Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_upper.limits">upper.limits</code></td>
<td>
<p>Vector of upper limits for parameters to be used by the slice sampler. The length of the vector should be equal to the total number of parameters, i.e. P+1 where P is the number of covariates. The default is 100 for all parameters (may not be appropriate for all situations). Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_slice.widths">slice.widths</code></td>
<td>
<p>Vector of initial slice widths for parameters to be used by the slice sampler. The length of the vector should be equal to the total number of parameters, i.e. P+1 where P is the number of covariates. The default is 1 for all parameter (may not be appropriate for all situations). Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_delta">delta</code></td>
<td>
<p>Prespecified constant that defines the boundary of the null hypothesis. The default is zero.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_gamma">gamma</code></td>
<td>
<p>Posterior probability threshold for rejecting the null. The null hypothesis is rejected if posterior probability is greater <code>gamma</code>. The default is 0.95.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_nmc">nMC</code></td>
<td>
<p>Number of iterations (excluding burn-in samples) for the slice sampler or Gibbs sampler. The default is 10,000.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_nbi">nBI</code></td>
<td>
<p>Number of burn-in samples for the slice sampler or Gibbs sampler. The default is 250.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_n">N</code></td>
<td>
<p>Number of simulated datasets to generate. The default is 10,000.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_approximate">approximate</code></td>
<td>
<p>Logical value indicating whether the approximation method based on asymptotic theory is used. The default is FALSE. If TRUE, an approximation method based on the Newton-Raphson algorithm (assuming canonical links) is used.
This feature helps users quickly obtain a rough estimate of the sample size required for the desired level of power or type I error rate.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_nnr">nNR</code></td>
<td>
<p>(Only applies if <code>approximate=TRUE</code>) number of iterations of the Newton-Raphson algorithm. The default value is 10,000.</p>
</td></tr>
<tr><td><code id="power.glm.fixed.a0_+3A_tol">tol</code></td>
<td>
<p>(Only applies if <code>approximate=TRUE</code>) absolute tolerance of the Newton-Raphson algorithm. The default value is 0.00001.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If historical datasets are provided, the algorithm samples with replacement from the historical covariates to construct the simulated datasets.
Otherwise, the algorithm samples with replacement from <code>x.samples</code>. One of the arguments <code>historical</code> and <code>x.samples</code> must be provided.
</p>
<p>The sampling prior for the treatment parameter can be generated from a normal distribution (see examples).
For example, suppose one wants to compute the power for the hypotheses <code class="reqn">H_0: \beta_1 \ge 0</code> and <code class="reqn">H_1: \beta_1 &lt; 0.</code>
To approximate the sampling prior for <code class="reqn">\beta_1</code>, one can simply sample from a normal distribution with negative mean,
so that the mass of the prior falls in the alternative space. Conversely, to compute the type I error rate, one can
sample from a normal distribution with positive mean, so that the mass of the prior falls in the null space.
The sampling prior for the other parameters can be generated by using the <code>glm.fixed.a0</code> function with <code>current.data</code> set to FALSE.
The posterior samples based on only historical data can be used as a discrete approximation to the sampling prior.
</p>
<p><code>samp.prior.var</code> is necessary for generating normally distributed data.
</p>
<p>If <code>data.type</code> is &quot;Normal&quot;, the response <code class="reqn">y_i</code> is assumed to follow <code class="reqn">N(x_i'\beta, \tau^{-1})</code> where <code class="reqn">x_i</code> is the vector of covariates for subject <code class="reqn">i</code>.
Each historical dataset <code class="reqn">D_{0k}</code> is assumed to have a different precision parameter <code class="reqn">\tau_k</code>.
The initial prior for <code class="reqn">\tau</code> is the Jeffery's prior, <code class="reqn">\tau^{-1}</code>, and the initial prior for <code class="reqn">\tau_k</code> is <code class="reqn">\tau_k^{-1}</code>.
The initial prior for <code class="reqn">\beta</code> is the uniform improper prior. Posterior samples are obtained through Gibbs sampling.
</p>
<p>For all other data types, posterior samples are obtained through slice sampling.
The default lower limits for the parameters are -100. The default upper limits
for the parameters are 100. The default slice widths for the parameters are 1.
The defaults may not be appropriate for all situations, and the user can specify the appropriate limits
and slice width for each parameter.
</p>
<p>If a sampling prior with support in the null space is used, the value returned is a Bayesian type I error rate.
If a sampling prior with support in the alternative space is used, the value returned is a Bayesian power.
</p>
<p>Because running <code>power.glm.fixed.a0()</code> and <code>power.glm.random.a0()</code> is potentially time-consuming,
an approximation method based on asymptotic theory has been implemented for the model with fixed <code class="reqn">a_0</code>.
In order to attain the exact sample size needed for the desired power, the user can start with the approximation
to get a rough estimate of the sample size required, using <code>power.glm.fixed.a0()</code> with <code>approximate=TRUE</code>.
</p>


<h3>Value</h3>

<p>The function returns a S3 object with a <code>summary</code> method. Power or type I error is returned, depending on the sampling prior used.
The posterior probabilities of the alternative hypothesis are returned.
The average posterior mean of <code class="reqn">\beta</code> and its corresponding bias are returned.
If <code>data.type</code> is &quot;Normal&quot;, average posterior means of <code class="reqn">\tau</code> and <code class="reqn">\tau_k</code>'s (if historical data is given) are also returned.
The first column of <code class="reqn">\beta</code> contains posterior samples of the intercept. The second column contains posterior samples of <code class="reqn">\beta_1</code>, the parameter for the treatment indicator.
</p>


<h3>References</h3>

<p>Chen, Ming-Hui, et al. &quot;Bayesian design of noninferiority trials for medical devices using historical data.&quot; Biometrics 67.3 (2011): 1163-1170.
</p>
<p>Neal, Radford M. Slice sampling. Ann. Statist. 31 (2003), no. 3, 705&ndash;767.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.fixed.a0">glm.fixed.a0</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.type &lt;- "Bernoulli"
data.link &lt;- "Logistic"
data.size &lt;- 100

# Simulate two historical datasets
p &lt;- 3
historical &lt;- list(list(y0=rbinom(data.size,size=1,prob=0.2),
                        x0=matrix(rnorm(p*data.size),ncol=p,nrow=data.size), a0=0.2),
                   list(y0=rbinom(data.size, size=1, prob=0.5),
                        x0=matrix(rnorm(p*data.size),ncol=p,nrow=data.size), a0=0.3))

# Generate sampling priors

# The null hypothesis here is H0: beta_1 &gt;= 0. To calculate power,
# we can provide samples of beta_1 such that the mass of beta_1 &lt; 0.
# To calculate type I error, we can provide samples of beta_1 such that
# the mass of beta_1 &gt;= 0.
samp.prior.beta1 &lt;- rnorm(100, mean=-3, sd=1)
# Here, mass is put on the alternative region, so power is calculated.
samp.prior.beta &lt;- cbind(rnorm(100), samp.prior.beta1, matrix(rnorm(100*p), 100, p))

nMC &lt;- 100 # nMC should be larger in practice
nBI &lt;- 50
N &lt;- 5 # N should be larger in practice
result &lt;- power.glm.fixed.a0(data.type=data.type, data.link=data.link,
                             data.size=data.size, historical=historical,
                             samp.prior.beta=samp.prior.beta,
                             delta=0, nMC=nMC, nBI=nBI, N=N)
summary(result)

</code></pre>

<hr>
<h2 id='power.glm.random.a0'>Power/type I error calculation for generalized linear models with random a0</h2><span id='topic+power.glm.random.a0'></span>

<h3>Description</h3>

<p>Power/type I error calculation using normalized power priors for generalized linear models with random <code class="reqn">a_0</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.glm.random.a0(
  data.type,
  data.link,
  data.size,
  n = 1,
  treat.assign.prob = 0.5,
  borrow.treat = FALSE,
  historical,
  nullspace.ineq = "&gt;",
  samp.prior.beta,
  samp.prior.var,
  prior.beta.var = rep(10, 50),
  prior.a0.shape1 = rep(1, 10),
  prior.a0.shape2 = rep(1, 10),
  a0.coefficients,
  lower.limits = NULL,
  upper.limits = NULL,
  slice.widths = rep(0.1, 50),
  delta = 0,
  gamma = 0.95,
  nMC = 10000,
  nBI = 250,
  N = 10000
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="power.glm.random.a0_+3A_data.type">data.type</code></td>
<td>
<p>Character string specifying the type of response. The options are &quot;Normal&quot;, &quot;Bernoulli&quot;, &quot;Binomial&quot;, &quot;Poisson&quot; and &quot;Exponential&quot;.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_data.link">data.link</code></td>
<td>
<p>Character string specifying the link function. The options are &quot;Logistic&quot;, &quot;Probit&quot;, &quot;Log&quot;, &quot;Identity-Positive&quot;, &quot;Identity-Probability&quot; and &quot;Complementary Log-Log&quot;. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_data.size">data.size</code></td>
<td>
<p>Sample size of the simulated datasets.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_n">n</code></td>
<td>
<p>(For binomial data only) vector of integers specifying the number of subjects who have a particular value of the covariate vector. If the data is binary and all covariates are discrete, collapsing Bernoulli data into a binomial structure can make the slice sampler much faster.
The sum of <code>n</code> should be equal to <code>data.size</code>. The length of <code>n</code> should be equal to the number of rows of <code>x0</code>.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_treat.assign.prob">treat.assign.prob</code></td>
<td>
<p>Probability of being assigned to the treatment group. The default value is 0.5. Only applies if <code>borrow.treat=FALSE</code>.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_borrow.treat">borrow.treat</code></td>
<td>
<p>Logical value indicating whether the historical information is used to inform the treatment effect parameter. The default value is FALSE. If TRUE, the first column of the historical covariate matrix must be the treatment indicator.
If FALSE, the historical covariate matrix must NOT have the treatment indicator, since the historical data is assumed to be from the control group only.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_historical">historical</code></td>
<td>
<p>List of historical dataset(s). East historical dataset is stored in a list which contains two <em>named</em> elements: <code>y0</code> and <code>x0</code>.
</p>

<ul>
<li> <p><code>y0</code> is a vector of responses.
</p>
</li>
<li> <p><code>x0</code> is a matrix of covariates. If <code>borrow.treat</code> is FALSE (the default), <code>x0</code> should NOT have the treatment indicator.
If <code>borrow.treat</code> is TRUE, the first column of <code>x0</code> must be the treatment indicator.
</p>
</li></ul>

<p>For binomial data, an additional element <code>n0</code> is required.
</p>

<ul>
<li> <p><code>n0</code> is vector of integers specifying the number of subjects who have a particular value of the covariate vector.
The length of <code>n0</code> should be equal to the number of rows of <code>x0</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_nullspace.ineq">nullspace.ineq</code></td>
<td>
<p>Character string specifying the inequality of the null hypothesis. The options are &quot;&gt;&quot; and &quot;&lt;&quot;. If &quot;&gt;&quot; is specified, the null hypothesis is <code class="reqn">H_0</code>: <code class="reqn">\beta_1</code> <code class="reqn">\ge</code> <code class="reqn">\delta</code>. If &quot;&lt;&quot; is specified, the null hypothesis is <code class="reqn">H_0</code>: <code class="reqn">\beta_1</code> <code class="reqn">\le</code> <code class="reqn">\delta</code>. The default choice is &quot;&gt;&quot;.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_samp.prior.beta">samp.prior.beta</code></td>
<td>
<p>Matrix of possible values of <code class="reqn">\beta</code> to sample (with replacement) from. Each row is a possible <code class="reqn">\beta</code> vector (a realization from the sampling prior for <code class="reqn">\beta</code>), where the first element is the coefficient for the intercept and the second element is the coefficient for the treatment indicator.
The length of the vector should be equal to the total number of parameters. If P is the number of columns of <code>x0</code> in <code>historical</code>, the total number of parameters is P+2 if <code>borrow.treat=FALSE</code>, and is P+1 if <code>borrow.treat=TRUE</code>.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_samp.prior.var">samp.prior.var</code></td>
<td>
<p>Vector of possible values of <code class="reqn">\sigma^2</code> to sample (with replacement) from. Only applies if <code>data.type</code> is &quot;Normal&quot;. The vector contains realizations from the sampling prior (e.g. inverse-gamma distribution) for <code class="reqn">\sigma^2</code>.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_prior.beta.var">prior.beta.var</code></td>
<td>
<p>Vector of variances of the independent normal initial priors on <code class="reqn">\beta</code> with mean zero. The length of the vector should be equal to the length of <code class="reqn">\beta</code>. The default variance is 10.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_prior.a0.shape1">prior.a0.shape1</code></td>
<td>
<p>Vector of the first shape parameters of the independent beta priors for <code class="reqn">a_0</code>. The length of the vector should be equal to the number of historical datasets. The default is a vector of one's.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_prior.a0.shape2">prior.a0.shape2</code></td>
<td>
<p>Vector of the second shape parameters of the independent beta priors for <code class="reqn">a_0</code>. The length of the vector should be equal to the number of historical datasets. The default is a vector of one's.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_a0.coefficients">a0.coefficients</code></td>
<td>
<p>Vector of coefficients for <code class="reqn">a_0</code> returned by the function <code><a href="#topic+normalizing.constant">normalizing.constant</a></code>. This is necessary for estimating the normalizing constant for the normalized power prior. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_lower.limits">lower.limits</code></td>
<td>
<p>Vector of lower limits for parameters to be used by the slice sampler. If <code>data.type</code> is &quot;Normal&quot;, slice sampling is used for <code class="reqn">a_0</code>, and the length of the vector should be equal to the number of historical datasets.
For all other data types, slice sampling is used for <code class="reqn">\beta</code> and <code class="reqn">a_0</code>. The first P+1 elements apply to the sampling of <code class="reqn">\beta</code> and the rest apply to the sampling of <code class="reqn">a_0</code>.
The length of the vector should be equal to the sum of the total number of parameters (i.e. P+1 where P is the number of covariates) and the number of historical datasets.
The default is -100 for <code class="reqn">\beta</code> and 0 for <code class="reqn">a_0</code> (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_upper.limits">upper.limits</code></td>
<td>
<p>Vector of upper limits for parameters to be used by the slice sampler. If <code>data.type</code> is &quot;Normal&quot;, slice sampling is used for <code class="reqn">a_0</code>, and the length of the vector should be equal to the number of historical datasets.
For all other data types, slice sampling is used for <code class="reqn">\beta</code> and <code class="reqn">a_0</code>. The first P+1 elements apply to the sampling of <code class="reqn">\beta</code> and the rest apply to the sampling of <code class="reqn">a_0</code>.
The length of the vector should be equal to the sum of the total number of parameters (i.e. P+1 where P is the number of covariates) and the number of historical datasets.
The default is 100 for <code class="reqn">\beta</code> and 1 for <code class="reqn">a_0</code>  (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_slice.widths">slice.widths</code></td>
<td>
<p>Vector of initial slice widths used by the slice sampler. If <code>data.type</code> is &quot;Normal&quot;, slice sampling is used for <code class="reqn">a_0</code>, and the length of the vector should be equal to the number of historical datasets.
For all other data types, slice sampling is used for <code class="reqn">\beta</code> and <code class="reqn">a_0</code>. The first P+1 elements apply to the sampling of <code class="reqn">\beta</code> and the rest apply to the sampling of <code class="reqn">a_0</code>.
The length of the vector should be equal to the sum of the total number of parameters (i.e. P+1 where P is the number of covariates) and the number of historical datasets.
The default is 0.1 for all parameter (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_delta">delta</code></td>
<td>
<p>Prespecified constant that defines the boundary of the null hypothesis. The default is zero.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_gamma">gamma</code></td>
<td>
<p>Posterior probability threshold for rejecting the null. The null hypothesis is rejected if posterior probability is greater <code>gamma</code>. The default is 0.95.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_nmc">nMC</code></td>
<td>
<p>Number of iterations (excluding burn-in samples) for the slice sampler or Gibbs sampler. The default is 10,000.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_nbi">nBI</code></td>
<td>
<p>Number of burn-in samples for the slice sampler or Gibbs sampler. The default is 250.</p>
</td></tr>
<tr><td><code id="power.glm.random.a0_+3A_n">N</code></td>
<td>
<p>Number of simulated datasets to generate. The default is 10,000.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The user should use the function <code><a href="#topic+normalizing.constant">normalizing.constant</a></code> to obtain <code>a0.coefficients</code> (does not apply if <code>data.type</code> is &quot;Normal&quot;).
</p>
<p>The sampling prior for the treatment parameter can be generated from a normal distribution (see examples).
For example, suppose one wants to compute the power for the hypotheses <code class="reqn">H_0: \beta_1 \ge 0</code> and <code class="reqn">H_1: \beta_1 &lt; 0.</code>
To approximate the sampling prior for <code class="reqn">\beta_1</code>, one can simply sample from a normal distribution with negative mean,
so that the mass of the prior falls in the alternative space. Conversely, to compute the type I error rate, one can
sample from a normal distribution with positive mean, so that the mass of the prior falls in the null space.
The sampling prior for the other parameters can be generated by using the <code>glm.fixed.a0</code> function with <code>current.data</code> set to FALSE.
The posterior samples based on only historical data can be used as a discrete approximation to the sampling prior.
</p>
<p><code>samp.prior.var</code> is necessary for generating normally distributed data.
</p>
<p>If <code>data.type</code> is &quot;Normal&quot;, the response <code class="reqn">y_i</code> is assumed to follow <code class="reqn">N(x_i'\beta, \tau^{-1})</code> where <code class="reqn">x_i</code> is the vector of covariates for subject <code class="reqn">i</code>.
Historical datasets are assumed to have the same precision parameter as the current dataset for computational simplicity.
The initial prior for <code class="reqn">\tau</code> is the Jeffery's prior, <code class="reqn">\tau^{-1}</code>.
Independent normal priors with mean zero and variance <code>prior.beta.var</code> are used for <code class="reqn">\beta</code> to ensure the propriety of the normalized power prior. Posterior samples for <code class="reqn">\beta</code> and <code class="reqn">\tau</code> are obtained through Gibbs sampling.
Independent beta(<code>prior.a0.shape1</code>, <code>prior.a0.shape1</code>) priors are used for <code class="reqn">a_0</code>. Posterior samples for <code class="reqn">a_0</code> are obtained through slice sampling.
</p>
<p>For all other data types, posterior samples are obtained through slice sampling.
The default lower limits are -100 for <code class="reqn">\beta</code> and 0 for <code class="reqn">a_0</code>. The default upper limits
for the parameters are 100 for <code class="reqn">\beta</code> and 1 for <code class="reqn">a_0</code>. The default slice widths for the parameters are 0.1.
The defaults may not be appropriate for all situations, and the user can specify the appropriate limits
and slice width for each parameter.
</p>
<p>If a sampling prior with support in the null space is used, the value returned is a Bayesian type I error rate.
If a sampling prior with support in the alternative space is used, the value returned is a Bayesian power.
</p>
<p>Because running <code>power.glm.fixed.a0()</code> and <code>power.glm.random.a0()</code> is potentially time-consuming,
an approximation method based on asymptotic theory has been implemented for the model with fixed <code class="reqn">a_0</code>.
In order to attain the exact sample size needed for the desired power, the user can start with the approximation
to get a rough estimate of the sample size required, using <code>power.glm.fixed.a0()</code> with <code>approximate=TRUE</code>.
</p>


<h3>Value</h3>

<p>The function returns a S3 object with a <code>summary</code> method. Power or type I error is returned, depending on the sampling prior used.
The posterior probabilities of the alternative hypothesis are returned.
The average posterior mean of <code class="reqn">\beta</code> and its corresponding bias are returned.
The average posterior mean of <code class="reqn">a_0</code> is returned.
If <code>data.type</code> is &quot;Normal&quot;, the average posterior mean of <code class="reqn">\tau</code> is also returned.
The first element of the average posterior means of <code class="reqn">\beta</code> is the average posterior mean of the intercept.
The second element is the average posterior mean of <code class="reqn">\beta_1</code>, the parameter for the treatment indicator.
</p>


<h3>References</h3>

<p>Chen, Ming-Hui, et al. &quot;Bayesian design of noninferiority trials for medical devices using historical data.&quot; Biometrics 67.3 (2011): 1163-1170.
</p>
<p>Neal, Radford M. Slice sampling. Ann. Statist. 31 (2003), no. 3, 705&ndash;767.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normalizing.constant">normalizing.constant</a></code> and <code><a href="#topic+glm.random.a0">glm.random.a0</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.type &lt;- "Bernoulli"
data.link &lt;- "Logistic"
data.size &lt;- 100

# Simulate two historical datasets
p &lt;- 3
historical &lt;- list(list(y0=rbinom(data.size,size=1,prob=0.2),
                        x0=matrix(rnorm(p*data.size),ncol=p,nrow=data.size)),
                   list(y0=rbinom(data.size, size=1, prob=0.5),
                        x0=matrix(rnorm(p*data.size),ncol=p,nrow=data.size)))

# Generate sampling priors

# The null hypothesis here is H0: beta_1 &gt;= 0. To calculate power,
# we can provide samples of beta_1 such that the mass of beta_1 &lt; 0.
# To calculate type I error, we can provide samples of beta_1 such that
# the mass of beta_1 &gt;= 0.
samp.prior.beta1 &lt;- rnorm(100, mean=-3, sd=1)
# Here, mass is put on the alternative region, so power is calculated.
samp.prior.beta &lt;- cbind(rnorm(100), samp.prior.beta1, matrix(rnorm(100*p), 100, p))

# Please see function "normalizing.constant" for how to obtain a0.coefficients
# Here, suppose one-degree polynomial regression is chosen by the "normalizing.constant"
# function. The coefficients are obtained for the intercept, a0_1 and a0_2.
a0.coefficients &lt;- c(1, 0.5, -1)

nMC &lt;- 100 # nMC should be larger in practice
nBI &lt;- 50
N &lt;- 3 # N should be larger in practice
result &lt;- power.glm.random.a0(data.type=data.type, data.link=data.link,
                              data.size=data.size, historical=historical,
                              samp.prior.beta=samp.prior.beta, a0.coefficients=a0.coefficients,
                              delta=0, nMC=nMC, nBI=nBI, N=N)
summary(result)

</code></pre>

<hr>
<h2 id='power.two.grp.fixed.a0'>Power/type I error calculation for data with two groups (treatment and control group, no covariates) with fixed a0</h2><span id='topic+power.two.grp.fixed.a0'></span>

<h3>Description</h3>

<p>Power/type I error calculation for data with two groups (treatment and control group, no covariates) with fixed <code class="reqn">a_0</code> using power priors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.two.grp.fixed.a0(
  data.type,
  n.t,
  n.c,
  historical = matrix(0, 1, 4),
  nullspace.ineq = "&gt;",
  samp.prior.mu.t,
  samp.prior.mu.c,
  samp.prior.var.t,
  samp.prior.var.c,
  prior.mu.t.shape1 = 1,
  prior.mu.t.shape2 = 1,
  prior.mu.c.shape1 = 1,
  prior.mu.c.shape2 = 1,
  delta = 0,
  gamma = 0.95,
  nMC = 10000,
  nBI = 250,
  N = 10000
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="power.two.grp.fixed.a0_+3A_data.type">data.type</code></td>
<td>
<p>Character string specifying the type of response. The options are &quot;Normal&quot;, &quot;Bernoulli&quot;, &quot;Poisson&quot; and &quot;Exponential&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_n.t">n.t</code></td>
<td>
<p>Sample size of the treatment group for the simulated datasets.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_n.c">n.c</code></td>
<td>
<p>Sample size of the control group for the simulated datasets.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_historical">historical</code></td>
<td>
<p>(Optional) matrix of historical dataset(s). If <code>data.type</code> is &quot;Normal&quot;, <code>historical</code> is a matrix with four columns:
</p>

<ul>
<li><p> The first column contains the sum of responses for the control group.
</p>
</li>
<li><p> The second column contains the sample size of the control group.
</p>
</li>
<li><p> The third column contains the sample variance of responses for the control group.
</p>
</li>
<li><p> The fourth column contains the discounting parameter value <code class="reqn">a_0</code> (between 0 and 1).
</p>
</li></ul>

<p>For all other data types, <code>historical</code> is a matrix with three columns:
</p>

<ul>
<li><p> The first column contains the sum of responses for the control group.
</p>
</li>
<li><p> The second column contains the sample size of the control group.
</p>
</li>
<li><p> The third column contains the discounting parameter value <code class="reqn">a_0</code> (between 0 and 1).
</p>
</li></ul>

<p>Each row represents a historical dataset.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_nullspace.ineq">nullspace.ineq</code></td>
<td>
<p>Character string specifying the inequality of the null hypothesis. The options are &quot;&gt;&quot; and &quot;&lt;&quot;. If &quot;&gt;&quot; is specified, the null hypothesis (for non-exponential data) is <code class="reqn">H_0</code>: <code class="reqn">\mu_t</code> - <code class="reqn">\mu_c</code> <code class="reqn">\ge</code> <code class="reqn">\delta</code>. If &quot;&lt;&quot; is specified, the null hypothesis is <code class="reqn">H_0</code>: <code class="reqn">\mu_t</code> - <code class="reqn">\mu_c</code> <code class="reqn">\le</code> <code class="reqn">\delta</code>. The default choice is &quot;&gt;&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_samp.prior.mu.t">samp.prior.mu.t</code></td>
<td>
<p>Vector of possible values of <code class="reqn">\mu_t</code> to sample (with replacement) from. The vector contains realizations from the sampling prior (e.g. normal distribution) for <code class="reqn">\mu_t</code>.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_samp.prior.mu.c">samp.prior.mu.c</code></td>
<td>
<p>Vector of possible values of <code class="reqn">\mu_c</code> to sample (with replacement) from. The vector contains realizations from the sampling prior (e.g. normal distribution) for <code class="reqn">\mu_c</code>.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_samp.prior.var.t">samp.prior.var.t</code></td>
<td>
<p>Vector of possible values of <code class="reqn">\sigma^2_t</code> to sample (with replacement) from. Only applies if <code>data.type</code> is &quot;Normal&quot;. The vector contains realizations from the sampling prior (e.g. inverse-gamma distribution) for <code class="reqn">\sigma^2_t</code>.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_samp.prior.var.c">samp.prior.var.c</code></td>
<td>
<p>Vector of possible values of <code class="reqn">\sigma^2_c</code> to sample (with replacement) from. Only applies if <code>data.type</code> is &quot;Normal&quot;. The vector contains realizations from the sampling prior (e.g. inverse-gamma distribution) for <code class="reqn">\sigma^2_c</code></p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_prior.mu.t.shape1">prior.mu.t.shape1</code></td>
<td>
<p>First hyperparameter of the initial prior for <code class="reqn">\mu_t</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_prior.mu.t.shape2">prior.mu.t.shape2</code></td>
<td>
<p>Second hyperparameter of the initial prior for <code class="reqn">\mu_t</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_prior.mu.c.shape1">prior.mu.c.shape1</code></td>
<td>
<p>First hyperparameter of the initial prior for <code class="reqn">\mu_c</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_prior.mu.c.shape2">prior.mu.c.shape2</code></td>
<td>
<p>Second hyperparameter of the initial prior for <code class="reqn">\mu_c</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_delta">delta</code></td>
<td>
<p>Prespecified constant that defines the boundary of the null hypothesis. The default is zero.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_gamma">gamma</code></td>
<td>
<p>Posterior probability threshold for rejecting the null. The null hypothesis is rejected if posterior probability is greater <code>gamma</code>. The default is 0.95.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_nmc">nMC</code></td>
<td>
<p>Number of iterations (excluding burn-in samples) for the slice sampler or Gibbs sampler. The default is 10,000.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_nbi">nBI</code></td>
<td>
<p>Number of burn-in samples for the slice sampler or Gibbs sampler. The default is 250.</p>
</td></tr>
<tr><td><code id="power.two.grp.fixed.a0_+3A_n">N</code></td>
<td>
<p>Number of simulated datasets to generate. The default is 10,000.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>data.type</code> is &quot;Bernoulli&quot;, &quot;Poisson&quot; or &quot;Exponential&quot;, a single response from the treatment group is assumed to follow Bern(<code class="reqn">\mu_t</code>), Pois(<code class="reqn">\mu_t</code>) or Exp(rate=<code class="reqn">\mu_t</code>), respectively,
where <code class="reqn">\mu_t</code> is the mean of responses for the treatment group. If <code>data.type</code> is &quot;Normal&quot;, a single response from the treatment group is assumed to follow <code class="reqn">N(\mu_t, \tau^{-1})</code>
where <code class="reqn">\tau</code> is the precision parameter.
The distributional assumptions for the control group data are analogous.
</p>
<p><code>samp.prior.mu.t</code> and <code>samp.prior.mu.c</code> can be generated using the sampling priors (see example).
</p>
<p>If <code>data.type</code> is &quot;Bernoulli&quot;, the initial prior for <code class="reqn">\mu_t</code> is
beta(<code>prior.mu.t.shape1</code>, <code>prior.mu.t.shape2</code>).
If <code>data.type</code> is &quot;Poisson&quot;, the initial prior for <code class="reqn">\mu_t</code> is
Gamma(<code>prior.mu.t.shape1</code>, rate=<code>prior.mu.t.shape2</code>).
If <code>data.type</code> is &quot;Exponential&quot;, the initial prior for <code class="reqn">\mu_t</code> is
Gamma(<code>prior.mu.t.shape1</code>, rate=<code>prior.mu.t.shape2</code>).
The initial priors used for the control group data are analogous.
</p>
<p>If <code>data.type</code> is &quot;Normal&quot;, each historical dataset <code class="reqn">D_{0k}</code> is assumed to have a different precision parameter <code class="reqn">\tau_k</code>.
The initial prior for <code class="reqn">\tau</code> is the Jeffery's prior, <code class="reqn">\tau^{-1}</code>, and the initial prior for <code class="reqn">\tau_k</code> is <code class="reqn">\tau_k^{-1}</code>.
The initial prior for the <code class="reqn">\mu_c</code> is the uniform improper prior.
</p>
<p>If a sampling prior with support in the null space is used, the value returned is a Bayesian type I error rate.
If a sampling prior with support in the alternative space is used, the value returned is a Bayesian power.
</p>
<p>If <code>data.type</code> is &quot;Normal&quot;, Gibbs sampling is used for model fitting. For all other data types,
numerical integration is used for modeling fitting.
</p>


<h3>Value</h3>

<p>The function returns a S3 object with a <code>summary</code> method. Power or type I error is returned, depending on the sampling prior used.
The posterior probabilities of the alternative hypothesis are returned.
Average posterior means of <code class="reqn">\mu_t</code> and <code class="reqn">\mu_c</code> and their corresponding biases are returned.
If <code>data.type</code> is &quot;Normal&quot;, average posterior means of <code class="reqn">\tau</code> and <code class="reqn">\tau_k</code>'s (if historical data is given) are also returned.
</p>


<h3>References</h3>

<p>Yixuan Qiu, Sreekumar Balan, Matt Beall, Mark Sauder, Naoaki Okazaki and Thomas Hahn (2019). RcppNumerical: 'Rcpp' Integration for Numerical Computing Libraries. R package version 0.4-0. https://CRAN.R-project.org/package=RcppNumerical
</p>
<p>Chen, Ming-Hui, et al. &quot;Bayesian design of noninferiority trials for medical devices using historical data.&quot; Biometrics 67.3 (2011): 1163-1170.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+two.grp.fixed.a0">two.grp.fixed.a0</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.type &lt;- "Bernoulli"
n.t &lt;- 100
n.c &lt;- 100

# Simulate three historical datasets
historical &lt;- matrix(0, ncol=3, nrow=3)
historical[1,] &lt;- c(70, 100, 0.3)
historical[2,] &lt;- c(60, 100, 0.5)
historical[3,] &lt;- c(50, 100, 0.7)

# Generate sampling priors
set.seed(1)
b_st1 &lt;- b_st2 &lt;- 1
b_sc1 &lt;- b_sc2 &lt;- 1
samp.prior.mu.t &lt;- rbeta(50000, b_st1, b_st2)
samp.prior.mu.c &lt;- rbeta(50000, b_st1, b_st2)
# The null hypothesis here is H0: mu_t - mu_c &gt;= 0. To calculate power,
# we can provide samples of mu.t and mu.c such that the mass of mu_t - mu_c &lt; 0.
# To calculate type I error, we can provide samples of mu.t and mu.c such that
# the mass of mu_t - mu_c &gt;= 0.
sub_ind &lt;- which(samp.prior.mu.t &lt; samp.prior.mu.c)
# Here, mass is put on the alternative region, so power is calculated.
samp.prior.mu.t &lt;- samp.prior.mu.t[sub_ind]
samp.prior.mu.c &lt;- samp.prior.mu.c[sub_ind]

N &lt;- 1000 # N should be larger in practice
result &lt;- power.two.grp.fixed.a0(data.type=data.type, n.t=n.t, n.c=n.t, historical=historical,
                                 samp.prior.mu.t=samp.prior.mu.t, samp.prior.mu.c=samp.prior.mu.c,
                                 delta=0, N=N)
summary(result)
</code></pre>

<hr>
<h2 id='power.two.grp.random.a0'>Power/type I error calculation for two groups (treatment and control group, no covariates) with random a0</h2><span id='topic+power.two.grp.random.a0'></span>

<h3>Description</h3>

<p>Power/type I error calculation using normalized power priors for two groups (treatment and control group, no covariates) with random <code class="reqn">a_0</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.two.grp.random.a0(
  data.type,
  n.t,
  n.c,
  historical,
  nullspace.ineq = "&gt;",
  samp.prior.mu.t,
  samp.prior.mu.c,
  samp.prior.var.t = 0,
  samp.prior.var.c = 0,
  prior.mu.t.shape1 = 1,
  prior.mu.t.shape2 = 1,
  prior.mu.c.shape1 = 1,
  prior.mu.c.shape2 = 1,
  prior.a0.shape1 = rep(1, 10),
  prior.a0.shape2 = rep(1, 10),
  lower.limits = rep(0, 10),
  upper.limits = rep(1, 10),
  slice.widths = rep(0.1, 10),
  delta = 0,
  gamma = 0.95,
  nMC = 10000,
  nBI = 250,
  N = 10000
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="power.two.grp.random.a0_+3A_data.type">data.type</code></td>
<td>
<p>Character string specifying the type of response. The options are &quot;Normal&quot;, &quot;Bernoulli&quot;, &quot;Poisson&quot; and &quot;Exponential&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_n.t">n.t</code></td>
<td>
<p>Sample size of the treatment group for the simulated datasets.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_n.c">n.c</code></td>
<td>
<p>Sample size of the control group for the simulated datasets.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_historical">historical</code></td>
<td>
<p>Matrix of historical dataset(s). If <code>data.type</code> is &quot;Normal&quot;, <code>historical</code> is a matrix with three columns:
</p>

<ul>
<li><p> The first column contains the sum of responses for the control group.
</p>
</li>
<li><p> The second column contains the sample size of the control group.
</p>
</li>
<li><p> The third column contains the sample variance of responses for the control group.
</p>
</li></ul>

<p>For all other data types, <code>historical</code> is a matrix with two columns:
</p>

<ul>
<li><p> The first column contains the sum of responses for the control group.
</p>
</li>
<li><p> The second column contains the sample size of the control group.
</p>
</li></ul>

<p>Each row represents a historical dataset.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_nullspace.ineq">nullspace.ineq</code></td>
<td>
<p>Character string specifying the inequality of the null hypothesis. The options are &quot;&gt;&quot; and &quot;&lt;&quot;. If &quot;&gt;&quot; is specified, the null hypothesis (for non-exponential data) is <code class="reqn">H_0</code>: <code class="reqn">\mu_t</code> - <code class="reqn">\mu_c</code> <code class="reqn">\ge</code> <code class="reqn">\delta</code>. If &quot;&lt;&quot; is specified, the null hypothesis is <code class="reqn">H_0</code>: <code class="reqn">\mu_t</code> - <code class="reqn">\mu_c</code> <code class="reqn">\le</code> <code class="reqn">\delta</code>. The default choice is &quot;&gt;&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_samp.prior.mu.t">samp.prior.mu.t</code></td>
<td>
<p>Vector of possible values of <code class="reqn">\mu_t</code> to sample (with replacement) from. The vector contains realizations from the sampling prior (e.g. normal distribution) for <code class="reqn">\mu_t</code>.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_samp.prior.mu.c">samp.prior.mu.c</code></td>
<td>
<p>Vector of possible values of <code class="reqn">\mu_c</code> to sample (with replacement) from. The vector contains realizations from the sampling prior (e.g. normal distribution) for <code class="reqn">\mu_c</code>.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_samp.prior.var.t">samp.prior.var.t</code></td>
<td>
<p>Vector of possible values of <code class="reqn">\sigma^2_t</code> to sample (with replacement) from. Only applies if <code>data.type</code> is &quot;Normal&quot;. The vector contains realizations from the sampling prior (e.g. inverse-gamma distribution) for <code class="reqn">\sigma^2_t</code>.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_samp.prior.var.c">samp.prior.var.c</code></td>
<td>
<p>Vector of possible values of <code class="reqn">\sigma^2_c</code> to sample (with replacement) from. Only applies if <code>data.type</code> is &quot;Normal&quot;. The vector contains realizations from the sampling prior (e.g. inverse-gamma distribution) for <code class="reqn">\sigma^2_c</code></p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_prior.mu.t.shape1">prior.mu.t.shape1</code></td>
<td>
<p>First hyperparameter of the initial prior for <code class="reqn">\mu_t</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_prior.mu.t.shape2">prior.mu.t.shape2</code></td>
<td>
<p>Second hyperparameter of the initial prior for <code class="reqn">\mu_t</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_prior.mu.c.shape1">prior.mu.c.shape1</code></td>
<td>
<p>First hyperparameter of the initial prior for <code class="reqn">\mu_c</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_prior.mu.c.shape2">prior.mu.c.shape2</code></td>
<td>
<p>Second hyperparameter of the initial prior for <code class="reqn">\mu_c</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_prior.a0.shape1">prior.a0.shape1</code></td>
<td>
<p>Vector of the first shape parameters of the independent beta priors for <code class="reqn">a_0</code>. The length of the vector should be equal to the number of historical datasets. The default is a vector of one's.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_prior.a0.shape2">prior.a0.shape2</code></td>
<td>
<p>Vector of the second shape parameters of the independent beta priors for <code class="reqn">a_0</code>. The length of the vector should be equal to the number of historical datasets. The default is a vector of one's.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_lower.limits">lower.limits</code></td>
<td>
<p>Vector of lower limits for parameters to be used by the slice sampler. The length of the vector should be equal to the number of historical datasets. The default is 0 for all parameters (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_upper.limits">upper.limits</code></td>
<td>
<p>Vector of upper limits for parameters to be used by the slice sampler. The length of the vector should be equal to the number of historical datasets. The default is 1 for all parameters (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_slice.widths">slice.widths</code></td>
<td>
<p>Vector of initial slice widths used by the slice sampler. The length of the vector should be equal to the number of historical datasets. The default is 0.1 for all parameter (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_delta">delta</code></td>
<td>
<p>Prespecified constant that defines the boundary of the null hypothesis. The default is zero.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_gamma">gamma</code></td>
<td>
<p>Posterior probability threshold for rejecting the null. The null hypothesis is rejected if posterior probability is greater <code>gamma</code>. The default is 0.95.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_nmc">nMC</code></td>
<td>
<p>Number of iterations (excluding burn-in samples) for the slice sampler or Gibbs sampler. The default is 10,000.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_nbi">nBI</code></td>
<td>
<p>Number of burn-in samples for the slice sampler or Gibbs sampler. The default is 250.</p>
</td></tr>
<tr><td><code id="power.two.grp.random.a0_+3A_n">N</code></td>
<td>
<p>Number of simulated datasets to generate. The default is 10,000.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>data.type</code> is &quot;Bernoulli&quot;, &quot;Poisson&quot; or &quot;Exponential&quot;, a single response from the treatment group is assumed to follow Bern(<code class="reqn">\mu_t</code>), Pois(<code class="reqn">\mu_t</code>) or Exp(rate=<code class="reqn">\mu_t</code>), respectively,
where <code class="reqn">\mu_t</code> is the mean of responses for the treatment group. If <code>data.type</code> is &quot;Normal&quot;, a single response from the treatment group is assumed to follow <code class="reqn">N(\mu_t, \tau^{-1})</code>
where <code class="reqn">\tau</code> is the precision parameter.
The distributional assumptions for the control group data are analogous.
</p>
<p><code>samp.prior.mu.t</code> and <code>samp.prior.mu.c</code> can be generated using the sampling priors (see example).
</p>
<p>If <code>data.type</code> is &quot;Bernoulli&quot;, the initial prior for <code class="reqn">\mu_t</code> is beta(<code>prior.mu.t.shape1</code>, <code>prior.mu.t.shape2</code>).
If <code>data.type</code> is &quot;Poisson&quot;, the initial prior for <code class="reqn">\mu_t</code> is Gamma(<code>prior.mu.t.shape1</code>, rate=<code>prior.mu.t.shape2</code>).
If <code>data.type</code> is &quot;Exponential&quot;, the initial prior for <code class="reqn">\mu_t</code> is Gamma(<code>prior.mu.t.shape1</code>, rate=<code>prior.mu.t.shape2</code>).
The initial priors used for the control group data are analogous.
</p>
<p>If <code>data.type</code> is &quot;Normal&quot;, historical datasets are assumed to have the same precision parameter as the current dataset for computational simplicity.
The initial prior for <code class="reqn">\tau</code> is the Jeffery's prior, <code class="reqn">\tau^{-1}</code>. The initial prior for the <code class="reqn">\mu_c</code> is the uniform improper prior.
Posterior samples of <code class="reqn">\mu_c</code> and <code class="reqn">\tau</code> are obtained through Gibbs sampling.
</p>
<p>Independent beta(<code>prior.a0.shape1</code>,<code>prior.a0.shape1</code>) priors are used for <code class="reqn">a_0</code>. Posterior samples of <code class="reqn">a_0</code> are obtained through slice sampling. The default lower limits for the parameters are 0. The default upper limits
for the parameters are 1.  The default slice widths for the parameters are 0.1.
The defaults may not be appropriate for all situations, and the user can specify the appropriate limits
and slice width for each parameter.
</p>
<p>If a sampling prior with support in the null space is used, the value returned is a Bayesian type I error rate.
If a sampling prior with support in the alternative space is used, the value returned is a Bayesian power.
</p>


<h3>Value</h3>

<p>The function returns a S3 object with a <code>summary</code> method. Power or type I error is returned, depending on the sampling prior used.
The posterior probabilities of the alternative hypothesis are returned.
Average posterior means of <code class="reqn">\mu_t</code> and <code class="reqn">\mu_c</code> and their corresponding biases are returned.
The average posterior mean of <code class="reqn">a_0</code> is returned.
If <code>data.type</code> is &quot;Normal&quot;, the average posterior mean of <code class="reqn">\tau</code> is also returned.
</p>


<h3>References</h3>

<p>Chen, Ming-Hui, et al. &quot;Bayesian design of noninferiority trials for medical devices using historical data.&quot; Biometrics 67.3 (2011): 1163-1170.
</p>
<p>Neal, Radford M. Slice sampling. Ann. Statist. 31 (2003), no. 3, 705&ndash;767.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+two.grp.random.a0">two.grp.random.a0</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.type &lt;- "Bernoulli"
n.t &lt;- 100
n.c &lt;- 100

# Simulate three historical datasets
historical &lt;- matrix(0, ncol=2, nrow=3)
historical[1,] &lt;- c(70, 100)
historical[2,] &lt;- c(60, 100)
historical[3,] &lt;- c(50, 100)

# Generate sampling priors
set.seed(1)
b_st1 &lt;- b_st2 &lt;- 1
b_sc1 &lt;- b_sc2 &lt;- 1
samp.prior.mu.t &lt;- rbeta(50000, b_st1, b_st2)
samp.prior.mu.c &lt;- rbeta(50000, b_st1, b_st2)
# The null hypothesis here is H0: mu_t - mu_c &gt;= 0. To calculate power,
# we can provide samples of mu.t and mu.c such that the mass of mu_t - mu_c &lt; 0.
# To calculate type I error, we can provide samples of mu.t and mu.c such that
# the mass of mu_t - mu_c &gt;= 0.
sub_ind &lt;- which(samp.prior.mu.t &lt; samp.prior.mu.c)
# Here, mass is put on the alternative region, so power is calculated.
samp.prior.mu.t &lt;- samp.prior.mu.t[sub_ind]
samp.prior.mu.c &lt;- samp.prior.mu.c[sub_ind]

N &lt;- 10 # N should be larger in practice
result &lt;- power.two.grp.random.a0(data.type=data.type, n.t=n.t, n.c=n.c, historical=historical,
                                  samp.prior.mu.t=samp.prior.mu.t, samp.prior.mu.c=samp.prior.mu.c,
                                  delta=0, nMC=10000, nBI=250, N=N)
summary(result)
</code></pre>

<hr>
<h2 id='two.grp.fixed.a0'>Model fitting for two groups (treatment and control group, no covariates) with fixed a0</h2><span id='topic+two.grp.fixed.a0'></span>

<h3>Description</h3>

<p>Model fitting using power priors for two groups (treatment and control group, no covariates) with fixed <code class="reqn">a_0</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>two.grp.fixed.a0(
  data.type,
  y.c,
  n.c,
  v.c,
  historical = matrix(0, 1, 4),
  prior.mu.c.shape1 = 1,
  prior.mu.c.shape2 = 1,
  nMC = 10000,
  nBI = 250
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="two.grp.fixed.a0_+3A_data.type">data.type</code></td>
<td>
<p>Character string specifying the type of response. The options are &quot;Normal&quot;, &quot;Bernoulli&quot;, &quot;Poisson&quot; and &quot;Exponential&quot;.</p>
</td></tr>
<tr><td><code id="two.grp.fixed.a0_+3A_y.c">y.c</code></td>
<td>
<p>Sum of responses for the control group.</p>
</td></tr>
<tr><td><code id="two.grp.fixed.a0_+3A_n.c">n.c</code></td>
<td>
<p>Sample size of the control group.</p>
</td></tr>
<tr><td><code id="two.grp.fixed.a0_+3A_v.c">v.c</code></td>
<td>
<p>(For normal data only) sample variance of responses for the control group.</p>
</td></tr>
<tr><td><code id="two.grp.fixed.a0_+3A_historical">historical</code></td>
<td>
<p>(Optional) matrix of historical dataset(s). If <code>data.type</code> is &quot;Normal&quot;, <code>historical</code> is a matrix with four columns:
</p>

<ul>
<li><p> The first column contains the sum of responses for the control group.
</p>
</li>
<li><p> The second column contains the sample size of the control group.
</p>
</li>
<li><p> The third column contains the sample variance of responses for the control group.
</p>
</li>
<li><p> The fourth column contains the discounting parameter value <code class="reqn">a_0</code> (between 0 and 1).
</p>
</li></ul>

<p>For all other data types, <code>historical</code> is a matrix with three columns:
</p>

<ul>
<li><p> The first column contains the sum of responses for the control group.
</p>
</li>
<li><p> The second column contains the sample size of the control group.
</p>
</li>
<li><p> The third column contains the discounting parameter value <code class="reqn">a_0</code> (between 0 and 1).
</p>
</li></ul>

<p>Each row represents a historical dataset.</p>
</td></tr>
<tr><td><code id="two.grp.fixed.a0_+3A_prior.mu.c.shape1">prior.mu.c.shape1</code></td>
<td>
<p>First hyperparameter of the initial prior for <code class="reqn">\mu_c</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="two.grp.fixed.a0_+3A_prior.mu.c.shape2">prior.mu.c.shape2</code></td>
<td>
<p>Second hyperparameter of the initial prior for <code class="reqn">\mu_c</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="two.grp.fixed.a0_+3A_nmc">nMC</code></td>
<td>
<p>(For normal data only) number of iterations (excluding burn-in samples) for the Gibbs sampler. The default is 10,000.</p>
</td></tr>
<tr><td><code id="two.grp.fixed.a0_+3A_nbi">nBI</code></td>
<td>
<p>(For normal data only) number of burn-in samples for the Gibbs sampler. The default is 250.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The power prior is applied on the data of the control group only.
Therefore, only summaries of the responses of the control group need to be entered.
</p>
<p>If <code>data.type</code> is &quot;Bernoulli&quot;, &quot;Poisson&quot; or &quot;Exponential&quot;, a single response from the treatment group is assumed to follow Bern(<code class="reqn">\mu_t</code>), Pois(<code class="reqn">\mu_t</code>) or Exp(rate=<code class="reqn">\mu_t</code>), respectively,
where <code class="reqn">\mu_t</code> is the mean of responses for the treatment group. The distributional assumptions for the control group data are analogous.
</p>
<p>If <code>data.type</code> is &quot;Bernoulli&quot;, the initial prior for <code class="reqn">\mu_t</code> is beta(<code>prior.mu.t.shape1</code>, <code>prior.mu.t.shape2</code>).
If <code>data.type</code> is &quot;Poisson&quot;, the initial prior for <code class="reqn">\mu_t</code> is Gamma(<code>prior.mu.t.shape1</code>, rate=<code>prior.mu.t.shape2</code>).
If <code>data.type</code> is &quot;Exponential&quot;, the initial prior for <code class="reqn">\mu_t</code> is Gamma(<code>prior.mu.t.shape1</code>, rate=<code>prior.mu.t.shape2</code>).
The initial priors used for the control group data are analogous.
</p>
<p>If <code>data.type</code> is &quot;Normal&quot;, the responses are assumed to follow <code class="reqn">N(\mu_c, \tau^{-1})</code> where <code class="reqn">\mu_c</code> is the mean of responses for the control group
and <code class="reqn">\tau</code> is the precision parameter. Each historical dataset <code class="reqn">D_{0k}</code> is assumed to have a different precision parameter <code class="reqn">\tau_k</code>.
The initial prior for <code class="reqn">\tau</code> is the Jeffery's prior, <code class="reqn">\tau^{-1}</code>, and the initial prior for <code class="reqn">\tau_k</code> is <code class="reqn">\tau_k^{-1}</code>. The initial prior for the <code class="reqn">\mu_c</code> is the uniform improper prior.
Posterior samples are obtained through Gibbs sampling.
</p>


<h3>Value</h3>

<p>The function returns a S3 object with a <code>summary</code> method. If <code>data.type</code> is &quot;Normal&quot;, posterior samples of <code class="reqn">\mu_c</code>, <code class="reqn">\tau</code> and <code class="reqn">\tau_k</code>'s (if historical data is given) are returned
in the list item named <code>posterior.params</code>.
For all other data types, two scalars, <code class="reqn">c_1</code> and <code class="reqn">c_2</code>, are returned in the list item named <code>posterior.params</code>, representing the two parameters of the posterior distribution of <code class="reqn">\mu_c</code>.
For Bernoulli responses, the posterior distribution of <code class="reqn">\mu_c</code> is beta(<code class="reqn">c_1</code>, <code class="reqn">c_2</code>).
For Poisson responses, the posterior distribution of <code class="reqn">\mu_c</code> is Gamma(<code class="reqn">c_1</code>, <code class="reqn">c_2</code>) where <code class="reqn">c_2</code> is the rate parameter.
For exponential responses, the posterior distribution of <code class="reqn">\mu_c</code> is Gamma(<code class="reqn">c_1</code>, <code class="reqn">c_2</code>) where <code class="reqn">c_2</code> is the rate parameter.
</p>


<h3>References</h3>

<p>Chen, Ming-Hui, et al. &quot;Bayesian design of noninferiority trials for medical devices using historical data.&quot; Biometrics 67.3 (2011): 1163-1170.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+power.two.grp.fixed.a0">power.two.grp.fixed.a0</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.type &lt;- "Bernoulli"
y.c &lt;- 70
n.c &lt;- 100

# Simulate three historical datasets
historical &lt;- matrix(0, ncol=3, nrow=3)
historical[1,] &lt;- c(70, 100, 0.3)
historical[2,] &lt;- c(60, 100, 0.5)
historical[3,] &lt;- c(50, 100, 0.7)

set.seed(1)
result &lt;- two.grp.fixed.a0(data.type=data.type, y.c=y.c, n.c=n.c, historical=historical)
summary(result)
</code></pre>

<hr>
<h2 id='two.grp.random.a0'>Model fitting for two groups (treatment and control group, no covariates) with random a0</h2><span id='topic+two.grp.random.a0'></span>

<h3>Description</h3>

<p>Model fitting using normalized power priors for two groups (treatment and control group, no covariates) with random <code class="reqn">a_0</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>two.grp.random.a0(
  data.type,
  y.c,
  n.c,
  v.c,
  historical,
  prior.mu.c.shape1 = 1,
  prior.mu.c.shape2 = 1,
  prior.a0.shape1 = rep(1, 10),
  prior.a0.shape2 = rep(1, 10),
  lower.limits = rep(0, 10),
  upper.limits = rep(1, 10),
  slice.widths = rep(0.1, 10),
  nMC = 10000,
  nBI = 250
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="two.grp.random.a0_+3A_data.type">data.type</code></td>
<td>
<p>Character string specifying the type of response. The options are &quot;Normal&quot;, &quot;Bernoulli&quot;, &quot;Poisson&quot; and &quot;Exponential&quot;.</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_y.c">y.c</code></td>
<td>
<p>Sum of responses for the control group.</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_n.c">n.c</code></td>
<td>
<p>Sample size of the control group.</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_v.c">v.c</code></td>
<td>
<p>(For normal data only) sample variance of responses for the control group.</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_historical">historical</code></td>
<td>
<p>Matrix of historical dataset(s). If <code>data.type</code> is &quot;Normal&quot;, <code>historical</code> is a matrix with three columns:
</p>

<ul>
<li><p> The first column contains the sum of responses for the control group.
</p>
</li>
<li><p> The second column contains the sample size of the control group.
</p>
</li>
<li><p> The third column contains the sample variance of responses for the control group.
</p>
</li></ul>

<p>For all other data types, <code>historical</code> is a matrix with two columns:
</p>

<ul>
<li><p> The first column contains the sum of responses for the control group.
</p>
</li>
<li><p> The second column contains the sample size of the control group.
</p>
</li></ul>

<p>Each row represents a historical dataset.</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_prior.mu.c.shape1">prior.mu.c.shape1</code></td>
<td>
<p>First hyperparameter of the initial prior for <code class="reqn">\mu_c</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_prior.mu.c.shape2">prior.mu.c.shape2</code></td>
<td>
<p>Second hyperparameter of the initial prior for <code class="reqn">\mu_c</code>. The default is 1. Does not apply if <code>data.type</code> is &quot;Normal&quot;.</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_prior.a0.shape1">prior.a0.shape1</code></td>
<td>
<p>Vector of the first shape parameters of the independent beta priors for <code class="reqn">a_0</code>. The length of the vector should be equal to the number of historical datasets. The default is a vector of one's.</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_prior.a0.shape2">prior.a0.shape2</code></td>
<td>
<p>Vector of the second shape parameters of the independent beta priors for <code class="reqn">a_0</code>. The length of the vector should be equal to the number of historical datasets. The default is a vector of one's.</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_lower.limits">lower.limits</code></td>
<td>
<p>Vector of lower limits for parameters to be used by the slice sampler. The length of the vector should be equal to the number of historical datasets. The default is 0 for all parameters (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_upper.limits">upper.limits</code></td>
<td>
<p>Vector of upper limits for parameters to be used by the slice sampler. The length of the vector should be equal to the number of historical datasets. The default is 1 for all parameters (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_slice.widths">slice.widths</code></td>
<td>
<p>Vector of initial slice widths used by the slice sampler. The length of the vector should be equal to the number of historical datasets. The default is 0.1 for all parameter (may not be appropriate for all situations).</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_nmc">nMC</code></td>
<td>
<p>Number of iterations (excluding burn-in samples) for the slice sampler or Gibbs sampler. The default is 10,000.</p>
</td></tr>
<tr><td><code id="two.grp.random.a0_+3A_nbi">nBI</code></td>
<td>
<p>Number of burn-in samples for the slice sampler or Gibbs sampler. The default is 250.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>data.type</code> is &quot;Bernoulli&quot;, &quot;Poisson&quot; or &quot;Exponential&quot;, a single response from the treatment group is assumed to follow Bern(<code class="reqn">\mu_t</code>), Pois(<code class="reqn">\mu_t</code>) or Exp(rate=<code class="reqn">\mu_t</code>), respectively,
where <code class="reqn">\mu_t</code> is the mean of responses for the treatment group. If <code>data.type</code> is &quot;Normal&quot;, a single response from the treatment group is assumed to follow <code class="reqn">N(\mu_t, \tau^{-1})</code>
where <code class="reqn">\tau</code> is the precision parameter.
The distributional assumptions for the control group data are analogous.
</p>
<p>If <code>data.type</code> is &quot;Bernoulli&quot;, the initial prior for <code class="reqn">\mu_t</code> is beta(<code>prior.mu.t.shape1</code>, <code>prior.mu.t.shape2</code>).
If <code>data.type</code> is &quot;Poisson&quot;, the initial prior for <code class="reqn">\mu_t</code> is Gamma(<code>prior.mu.t.shape1</code>, rate=<code>prior.mu.t.shape2</code>).
If <code>data.type</code> is &quot;Exponential&quot;, the initial prior for <code class="reqn">\mu_t</code> is Gamma(<code>prior.mu.t.shape1</code>, rate=<code>prior.mu.t.shape2</code>).
The initial priors used for the control group data are analogous.
</p>
<p>If <code>data.type</code> is &quot;Normal&quot;, historical datasets are assumed to have the same precision parameter <code class="reqn">\tau</code> as the current dataset for computational simplicity.
The initial prior for <code class="reqn">\tau</code> is the Jeffery's prior, <code class="reqn">\tau^{-1}</code>. The initial prior for the <code class="reqn">\mu_c</code> is the uniform improper prior.
Posterior samples of <code class="reqn">\mu_c</code> and <code class="reqn">\tau</code> are obtained through Gibbs sampling.
</p>
<p>Independent beta(<code>prior.a0.shape1</code>,<code>prior.a0.shape1</code>) priors are used for <code class="reqn">a_0</code>. Posterior samples of <code class="reqn">a_0</code> are obtained through slice sampling. The default lower limits for the parameters are 0. The default upper limits
for the parameters are 1.  The default slice widths for the parameters are 0.1.
The defaults may not be appropriate for all situations, and the user can specify the appropriate limits
and slice width for each parameter.
</p>


<h3>Value</h3>

<p>The function returns a S3 object with a <code>summary</code> method. If <code>data.type</code> is &quot;Normal&quot;, posterior samples of <code class="reqn">\mu_c</code>, <code class="reqn">\tau</code> and <code class="reqn">a_0</code> are returned.
For all other data types, posterior samples of <code class="reqn">\mu_c</code> and <code class="reqn">a_0</code> are returned. If there are <code class="reqn">K</code> historical datasets,
then <code class="reqn">a_0 = (a_{01},\cdots,a_{0K})</code>.
</p>


<h3>References</h3>

<p>Neal, Radford M. Slice sampling. Ann. Statist. 31 (2003), no. 3, 705&ndash;767.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+power.two.grp.random.a0">power.two.grp.random.a0</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.type &lt;- "Bernoulli"
y.c &lt;- 70
n.c &lt;- 100

# Simulate three historical datasets
historical &lt;- matrix(0, ncol=2, nrow=3)
historical[1,] &lt;- c(70, 100)
historical[2,] &lt;- c(60, 100)
historical[3,] &lt;- c(50, 100)

# Set parameters of the slice sampler
lower.limits &lt;- rep(0, 3) # The dimension is the number of historical datasets
upper.limits &lt;- rep(1, 3)
slice.widths &lt;- rep(0.1, 3)

set.seed(1)
result &lt;- two.grp.random.a0(data.type=data.type, y.c=y.c, n.c=n.c, historical=historical,
                            lower.limits=lower.limits, upper.limits=upper.limits,
                            slice.widths=slice.widths, nMC=10000, nBI=250)
summary(result)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
