<!DOCTYPE html><html><head><title>Help for package PsychWordVec</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PsychWordVec}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as_embed'><p>Word vectors data class: <code>wordvec</code> and <code>embed</code>.</p></a></li>
<li><a href='#cosine_similarity'><p>Cosine similarity/distance between two vectors.</p></a></li>
<li><a href='#data_transform'><p>Transform plain text of word vectors into</p>
<code>wordvec</code> (data.table) or <code>embed</code> (matrix),
saved in a compressed &quot;.RData&quot; file.</a></li>
<li><a href='#data_wordvec_load'><p>Load word vectors data (<code>wordvec</code> or <code>embed</code>) from &quot;.RData&quot; file.</p></a></li>
<li><a href='#data_wordvec_subset'><p>Extract a subset of word vectors data (with S3 methods).</p></a></li>
<li><a href='#demodata'><p>Demo data (pre-trained using word2vec on Google News; 8000 vocab, 300 dims).</p></a></li>
<li><a href='#dict_expand'><p>Expand a dictionary from the most similar words.</p></a></li>
<li><a href='#dict_reliability'><p>Reliability analysis and PCA of a dictionary.</p></a></li>
<li><a href='#get_wordvec'><p>Extract word vector(s).</p></a></li>
<li><a href='#most_similar'><p>Find the Top-N most similar words.</p></a></li>
<li><a href='#normalize'><p>Normalize all word vectors to the unit length 1.</p></a></li>
<li><a href='#orth_procrustes'><p>Orthogonal Procrustes rotation for matrix alignment.</p></a></li>
<li><a href='#pair_similarity'><p>Compute a matrix of cosine similarity/distance of word pairs.</p></a></li>
<li><a href='#plot_network'><p>Visualize a (partial correlation) network graph of words.</p></a></li>
<li><a href='#plot_similarity'><p>Visualize cosine similarity of word pairs.</p></a></li>
<li><a href='#plot_wordvec'><p>Visualize word vectors.</p></a></li>
<li><a href='#plot_wordvec_tSNE'><p>Visualize word vectors with dimensionality reduced using t-SNE.</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#sum_wordvec'><p>Calculate the sum vector of multiple words.</p></a></li>
<li><a href='#tab_similarity'><p>Tabulate cosine similarity/distance of word pairs.</p></a></li>
<li><a href='#test_RND'><p>Relative Norm Distance (RND) analysis.</p></a></li>
<li><a href='#test_WEAT'><p>Word Embedding Association Test (WEAT) and Single-Category WEAT.</p></a></li>
<li><a href='#text_init'><p>Install required Python modules</p>
in a new conda environment
and initialize the environment,
necessary for all <code>text_*</code> functions
designed for contextualized word embeddings.</a></li>
<li><a href='#text_model_download'><p>Download pre-trained language models from HuggingFace.</p></a></li>
<li><a href='#text_model_remove'><p>Remove downloaded models from the local .cache folder.</p></a></li>
<li><a href='#text_to_vec'><p>Extract contextualized word embeddings from transformers (pre-trained language models).</p></a></li>
<li><a href='#text_unmask'><p>&lt;Deprecated&gt; Fill in the blank mask(s) in a query (sentence).</p></a></li>
<li><a href='#tokenize'><p>Tokenize raw text for training word embeddings.</p></a></li>
<li><a href='#train_wordvec'><p>Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Word Embedding Research Framework for Psychological Science</td>
</tr>
<tr>
<td>Version:</td>
<td>2023.9</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-09-27</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Han-Wu-Shuang Bao &lt;baohws@foxmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>
    An integrative toolbox of word embedding research that provides:
    (1) a collection of 'pre-trained' static word vectors in the '.RData'
    compressed format <a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>;
    (2) a series of functions to process, analyze, and visualize word vectors;
    (3) a range of tests to examine conceptual associations, including
    the Word Embedding Association Test &lt;<a href="https://doi.org/10.1126%2Fscience.aal4230">doi:10.1126/science.aal4230</a>&gt;
    and the Relative Norm Distance &lt;<a href="https://doi.org/10.1073%2Fpnas.1720347115">doi:10.1073/pnas.1720347115</a>&gt;,
    with permutation test of significance;
    (4) a set of training methods to locally train (static) word vectors
    from text corpora, including 'Word2Vec' &lt;<a href="https://doi.org/10.48550/arXiv.1301.3781">doi:10.48550/arXiv.1301.3781</a>&gt;,
    'GloVe' &lt;<a href="https://doi.org/10.3115%2Fv1%2FD14-1162">doi:10.3115/v1/D14-1162</a>&gt;, and 'FastText' &lt;<a href="https://doi.org/10.48550/arXiv.1607.04606">doi:10.48550/arXiv.1607.04606</a>&gt;;
    (5) a group of functions to download 'pre-trained' language models
    (e.g., 'GPT', 'BERT') and extract contextualized (dynamic) word vectors
    (based on the R package 'text').</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LazyDataCompression:</td>
<td>xz</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://psychbruce.github.io/PsychWordVec/">https://psychbruce.github.io/PsychWordVec/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/psychbruce/PsychWordVec/issues">https://github.com/psychbruce/PsychWordVec/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>bruceR, dplyr, stringr, data.table, purrr, vroom, cli,
ggplot2, ggrepel, corrplot, psych, Rtsne, rgl, qgraph, rsparse,
text2vec, word2vec, fastTextR, text, reticulate</td>
</tr>
<tr>
<td>Suggests:</td>
<td>wordsalad, sweater, glue</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-27 13:50:27 UTC; Bruce</td>
</tr>
<tr>
<td>Author:</td>
<td>Han-Wu-Shuang Bao <a href="https://orcid.org/0000-0003-3043-710X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-27 14:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as_embed'>Word vectors data class: <code>wordvec</code> and <code>embed</code>.</h2><span id='topic+as_embed'></span><span id='topic+as_wordvec'></span><span id='topic++5B.embed'></span><span id='topic+pattern'></span>

<h3>Description</h3>

<p><code>PsychWordVec</code> uses two types of word vectors data:
<code>wordvec</code> (data.table, with two variables <code>word</code> and <code>vec</code>)
and <code>embed</code> (matrix, with dimensions as columns and words as row names).
Note that matrix operation makes <code>embed</code> much faster than <code>wordvec</code>.
Users are suggested to reshape data to <code>embed</code> before using the other functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_embed(x, normalize = FALSE)

as_wordvec(x, normalize = FALSE)

## S3 method for class 'embed'
x[i, j]

pattern(pattern)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_embed_+3A_x">x</code></td>
<td>
<p>Object to be reshaped. See examples.</p>
</td></tr>
<tr><td><code id="as_embed_+3A_normalize">normalize</code></td>
<td>
<p>Normalize all word vectors to unit length?
Defaults to <code>FALSE</code>. See <code><a href="#topic+normalize">normalize</a></code>.</p>
</td></tr>
<tr><td><code id="as_embed_+3A_i">i</code>, <code id="as_embed_+3A_j">j</code></td>
<td>
<p>Row (<code>i</code>) and column (<code>j</code>) filter to be used in <code>embed[i, j]</code>.</p>
</td></tr>
<tr><td><code id="as_embed_+3A_pattern">pattern</code></td>
<td>
<p>Regular expression to be used in <code>embed[pattern("...")]</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>wordvec</code> (data.table) or <code>embed</code> (matrix).
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>as_embed()</code>: From <code>wordvec</code> (data.table) to <code>embed</code> (matrix).
</p>
</li>
<li> <p><code>as_wordvec()</code>: From <code>embed</code> (matrix) to <code>wordvec</code> (data.table).
</p>
</li></ul>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+load_wordvec">load_wordvec</a></code> / <code><a href="#topic+load_embed">load_embed</a></code>
</p>
<p><code><a href="#topic+normalize">normalize</a></code>
</p>
<p><code><a href="#topic+data_transform">data_transform</a></code>
</p>
<p><code><a href="#topic+data_wordvec_subset">data_wordvec_subset</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt = head(demodata, 10)
str(dt)

embed = as_embed(dt, normalize=TRUE)
embed
str(embed)

wordvec = as_wordvec(embed, normalize=TRUE)
wordvec
str(wordvec)

df = data.frame(token=LETTERS, D1=1:26/10000, D2=26:1/10000)
as_embed(df)
as_wordvec(df)

dd = rbind(dt[1:5], dt[1:5])
dd  # duplicate words
unique(dd)

dm = as_embed(dd)
dm  # duplicate words
unique(dm)

# more examples for extracting a subset using `x[i, j]`
# (3x faster than `wordvec`)
embed = as_embed(demodata)
embed[1]
embed[1:5]
embed["for"]
embed[pattern("^for.{0,2}$")]
embed[cc("for, in, on, xxx")]
embed[cc("for, in, on, xxx"), 5:10]
embed[1:5, 5:10]
embed[, 5:10]
embed[3, 4]
embed["that", 4]

</code></pre>

<hr>
<h2 id='cosine_similarity'>Cosine similarity/distance between two vectors.</h2><span id='topic+cosine_similarity'></span><span id='topic+cos_sim'></span><span id='topic+cos_dist'></span>

<h3>Description</h3>

<p>Cosine similarity/distance between two vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cosine_similarity(v1, v2, distance = FALSE)

cos_sim(v1, v2)

cos_dist(v1, v2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cosine_similarity_+3A_v1">v1</code>, <code id="cosine_similarity_+3A_v2">v2</code></td>
<td>
<p>Numeric vector (of the same length).</p>
</td></tr>
<tr><td><code id="cosine_similarity_+3A_distance">distance</code></td>
<td>
<p>Compute cosine distance instead?
Defaults to <code>FALSE</code> (cosine similarity).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cosine similarity =
</p>
<p><code>sum(v1 * v2) / ( sqrt(sum(v1^2)) * sqrt(sum(v2^2)) )</code>
</p>
<p>Cosine distance =
</p>
<p><code>1 - cosine_similarity(v1, v2)</code>
</p>


<h3>Value</h3>

<p>A value of cosine similarity/distance.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pair_similarity">pair_similarity</a></code>
</p>
<p><code><a href="#topic+tab_similarity">tab_similarity</a></code>
</p>
<p><code><a href="#topic+most_similar">most_similar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cos_sim(v1=c(1,1,1), v2=c(2,2,2))  # 1
cos_sim(v1=c(1,4,1), v2=c(4,1,1))  # 0.5
cos_sim(v1=c(1,1,0), v2=c(0,0,1))  # 0

cos_dist(v1=c(1,1,1), v2=c(2,2,2))  # 0
cos_dist(v1=c(1,4,1), v2=c(4,1,1))  # 0.5
cos_dist(v1=c(1,1,0), v2=c(0,0,1))  # 1

</code></pre>

<hr>
<h2 id='data_transform'>Transform plain text of word vectors into
<code>wordvec</code> (data.table) or <code>embed</code> (matrix),
saved in a compressed &quot;.RData&quot; file.</h2><span id='topic+data_transform'></span>

<h3>Description</h3>

<p>Transform plain text of word vectors into
<code>wordvec</code> (data.table) or <code>embed</code> (matrix),
saved in a compressed &quot;.RData&quot; file.
</p>
<p><em>Speed</em>: In total (preprocess + compress + save),
it can process about 30000 words/min
with the slowest settings (<code>compress="xz"</code>, <code>compress.level=9</code>)
on a modern computer (HP ProBook 450, Windows 11, Intel i7-1165G7 CPU, 32GB RAM).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_transform(
  file.load,
  file.save,
  as = c("wordvec", "embed"),
  sep = " ",
  header = "auto",
  encoding = "auto",
  compress = "bzip2",
  compress.level = 9,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data_transform_+3A_file.load">file.load</code></td>
<td>
<p>File name of raw text (must be plain text).
</p>
<p>Data must be in this format (values separated by <code>sep</code>):
</p>
<p>cat 0.001 0.002 0.003 0.004 0.005 ... 0.300
</p>
<p>dog 0.301 0.302 0.303 0.304 0.305 ... 0.600</p>
</td></tr>
<tr><td><code id="data_transform_+3A_file.save">file.save</code></td>
<td>
<p>File name of to-be-saved R data (must be .RData).</p>
</td></tr>
<tr><td><code id="data_transform_+3A_as">as</code></td>
<td>
<p>Transform the text to which R object?
<code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix).
Defaults to <code>wordvec</code>.</p>
</td></tr>
<tr><td><code id="data_transform_+3A_sep">sep</code></td>
<td>
<p>Column separator. Defaults to <code>" "</code>.</p>
</td></tr>
<tr><td><code id="data_transform_+3A_header">header</code></td>
<td>
<p>Is the 1st row a header (e.g., meta-information such as &quot;2000000 300&quot;)?
Defaults to <code>"auto"</code>, which automatically determines whether there is a header.
If <code>TRUE</code>, then the 1st row will be dropped.</p>
</td></tr>
<tr><td><code id="data_transform_+3A_encoding">encoding</code></td>
<td>
<p>File encoding. Defaults to <code>"auto"</code>
(using <code><a href="vroom.html#topic+vroom_lines">vroom::vroom_lines()</a></code> to fast read the file).
If specified to any other value (e.g., <code>"UTF-8"</code>),
then it uses <code><a href="base.html#topic+readLines">readLines()</a></code> to read the file,
which is much slower than <code>vroom</code>.</p>
</td></tr>
<tr><td><code id="data_transform_+3A_compress">compress</code></td>
<td>
<p>Compression method for the saved file. Defaults to <code>"bzip2"</code>.
</p>
<p>Options include:
</p>

<ul>
<li> <p><code>1</code> or <code>"gzip"</code>: modest file size (fastest)
</p>
</li>
<li> <p><code>2</code> or <code>"bzip2"</code>: small file size (fast)
</p>
</li>
<li> <p><code>3</code> or <code>"xz"</code>: minimized file size (slow)
</p>
</li></ul>
</td></tr>
<tr><td><code id="data_transform_+3A_compress.level">compress.level</code></td>
<td>
<p>Compression level from <code>0</code> (none) to <code>9</code>
(maximal compression for minimal file size). Defaults to <code>9</code>.</p>
</td></tr>
<tr><td><code id="data_transform_+3A_verbose">verbose</code></td>
<td>
<p>Print information to the console? Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>wordvec</code> (data.table) or <code>embed</code> (matrix).
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as_wordvec">as_wordvec</a></code> / <code><a href="#topic+as_embed">as_embed</a></code>
</p>
<p><code><a href="#topic+load_wordvec">load_wordvec</a></code> / <code><a href="#topic+load_embed">load_embed</a></code>
</p>
<p><code><a href="#topic+normalize">normalize</a></code>
</p>
<p><code><a href="#topic+data_wordvec_subset">data_wordvec_subset</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# please first manually download plain text data of word vectors
# e.g., from: https://fasttext.cc/docs/en/crawl-vectors.html

# the text file must be on your disk
# the following code cannot run unless you have the file
library(bruceR)
set.wd()
data_transform(file.load="cc.zh.300.vec",   # plain text file
               file.save="cc.zh.300.vec.RData",  # RData file
               header=TRUE, compress="xz")  # of minimal size

## End(Not run)

</code></pre>

<hr>
<h2 id='data_wordvec_load'>Load word vectors data (<code>wordvec</code> or <code>embed</code>) from &quot;.RData&quot; file.</h2><span id='topic+data_wordvec_load'></span><span id='topic+load_wordvec'></span><span id='topic+load_embed'></span>

<h3>Description</h3>

<p>Load word vectors data (<code>wordvec</code> or <code>embed</code>) from &quot;.RData&quot; file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_wordvec_load(
  file,
  as = c("wordvec", "embed"),
  normalize = FALSE,
  verbose = TRUE
)

load_wordvec(file, normalize = TRUE)

load_embed(file, normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data_wordvec_load_+3A_file">file</code></td>
<td>
<p>File name of .RData transformed by <code><a href="#topic+data_transform">data_transform</a></code>.
Can also be an .RData file containing an embedding matrix with words as row names.</p>
</td></tr>
<tr><td><code id="data_wordvec_load_+3A_as">as</code></td>
<td>
<p>Load as
<code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix).
Defaults to the original class of the R object in <code>file</code>.
The two wrapper functions <code>load_wordvec</code> and <code>load_embed</code>
automatically reshape the data to the corresponding class and
normalize all word vectors (for faster future use).</p>
</td></tr>
<tr><td><code id="data_wordvec_load_+3A_normalize">normalize</code></td>
<td>
<p>Normalize all word vectors to unit length?
Defaults to <code>FALSE</code>. See <code><a href="#topic+normalize">normalize</a></code>.</p>
</td></tr>
<tr><td><code id="data_wordvec_load_+3A_verbose">verbose</code></td>
<td>
<p>Print information to the console? Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>wordvec</code> (data.table) or <code>embed</code> (matrix).
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as_wordvec">as_wordvec</a></code> / <code><a href="#topic+as_embed">as_embed</a></code>
</p>
<p><code><a href="#topic+normalize">normalize</a></code>
</p>
<p><code><a href="#topic+data_transform">data_transform</a></code>
</p>
<p><code><a href="#topic+data_wordvec_subset">data_wordvec_subset</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = demodata[1:200]
save(d, file="demo.RData")
d = load_wordvec("demo.RData")
d
d = load_embed("demo.RData")
d
unlink("demo.RData")  # delete file for code check

## Not run: 
# please first manually download the .RData file
# (see https://psychbruce.github.io/WordVector_RData.pdf)
# or transform plain text data by using `data_transform()`

# the RData file must be on your disk
# the following code cannot run unless you have the file
library(bruceR)
set.wd()
d = load_embed("../data-raw/GloVe/glove_wiki_50d.RData")
d

## End(Not run)

</code></pre>

<hr>
<h2 id='data_wordvec_subset'>Extract a subset of word vectors data (with S3 methods).</h2><span id='topic+data_wordvec_subset'></span><span id='topic+subset.wordvec'></span><span id='topic+subset.embed'></span>

<h3>Description</h3>

<p>Extract a subset of word vectors data (with S3 methods).
You may specify either a <code>wordvec</code> or <code>embed</code> loaded by <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>)
or an .RData file transformed by <code><a href="#topic+data_transform">data_transform</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_wordvec_subset(
  x,
  words = NULL,
  pattern = NULL,
  as = c("wordvec", "embed"),
  file.save,
  compress = "bzip2",
  compress.level = 9,
  verbose = TRUE
)

## S3 method for class 'wordvec'
subset(x, ...)

## S3 method for class 'embed'
subset(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data_wordvec_subset_+3A_x">x</code></td>
<td>
<p>Can be:
</p>

<ul>
<li><p>a <code>wordvec</code> or <code>embed</code> loaded by <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>
</p>
</li>
<li><p>an .RData file transformed by <code><a href="#topic+data_transform">data_transform</a></code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="data_wordvec_subset_+3A_words">words</code></td>
<td>
<p>[Option 1] Character string(s).</p>
</td></tr>
<tr><td><code id="data_wordvec_subset_+3A_pattern">pattern</code></td>
<td>
<p>[Option 2] Regular expression (see <code><a href="stringr.html#topic+str_subset">str_subset</a></code>).
If neither <code>words</code> nor <code>pattern</code> are specified (i.e., both are <code>NULL</code>),
then all words in the data will be extracted.</p>
</td></tr>
<tr><td><code id="data_wordvec_subset_+3A_as">as</code></td>
<td>
<p>Reshape to
<code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix).
Defaults to the original class of <code>x</code>.</p>
</td></tr>
<tr><td><code id="data_wordvec_subset_+3A_file.save">file.save</code></td>
<td>
<p>File name of to-be-saved R data (must be .RData).</p>
</td></tr>
<tr><td><code id="data_wordvec_subset_+3A_compress">compress</code></td>
<td>
<p>Compression method for the saved file. Defaults to <code>"bzip2"</code>.
</p>
<p>Options include:
</p>

<ul>
<li> <p><code>1</code> or <code>"gzip"</code>: modest file size (fastest)
</p>
</li>
<li> <p><code>2</code> or <code>"bzip2"</code>: small file size (fast)
</p>
</li>
<li> <p><code>3</code> or <code>"xz"</code>: minimized file size (slow)
</p>
</li></ul>
</td></tr>
<tr><td><code id="data_wordvec_subset_+3A_compress.level">compress.level</code></td>
<td>
<p>Compression level from <code>0</code> (none) to <code>9</code>
(maximal compression for minimal file size). Defaults to <code>9</code>.</p>
</td></tr>
<tr><td><code id="data_wordvec_subset_+3A_verbose">verbose</code></td>
<td>
<p>Print information to the console? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="data_wordvec_subset_+3A_...">...</code></td>
<td>
<p>Parameters passed to <code>data_wordvec_subset</code>
when using the S3 method <code>subset</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A subset of <code>wordvec</code> or <code>embed</code> of valid (available) words.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as_wordvec">as_wordvec</a></code> / <code><a href="#topic+as_embed">as_embed</a></code>
</p>
<p><code><a href="#topic+load_wordvec">load_wordvec</a></code> / <code><a href="#topic+load_embed">load_embed</a></code>
</p>
<p><code><a href="#topic+get_wordvec">get_wordvec</a></code>
</p>
<p><code><a href="#topic+data_transform">data_transform</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## directly use `embed[i, j]` (3x faster than `wordvec`):
d = as_embed(demodata)
d[1:5]
d["people"]
d[c("China", "Japan", "Korea")]

## specify `x` as a `wordvec` or `embed` object:
subset(demodata, c("China", "Japan", "Korea"))
subset(d, pattern="^Chi")

## specify `x` and `pattern`, and save with `file.save`:
subset(demodata, pattern="Chin[ae]|Japan|Korea",
       file.save="subset.RData")

## load the subset:
d.subset = load_wordvec("subset.RData")
d.subset

## specify `x` as an .RData file and save with `file.save`:
data_wordvec_subset("subset.RData",
                    words=c("China", "Chinese"),
                    file.save="new.subset.RData")
d.new.subset = load_embed("new.subset.RData")
d.new.subset

unlink("subset.RData")  # delete file for code check
unlink("new.subset.RData")  # delete file for code check

</code></pre>

<hr>
<h2 id='demodata'>Demo data (pre-trained using word2vec on Google News; 8000 vocab, 300 dims).</h2><span id='topic+demodata'></span>

<h3>Description</h3>

<p>This demo data contains a sample of 8000 English words
with 300-dimension word vectors pre-trained
using the &quot;word2vec&quot; algorithm based on the Google News corpus.
Most of these words are from the Top 8000 frequent wordlist,
whereas a few are selected from less frequent words and appended.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(demodata)
</code></pre>


<h3>Format</h3>

<p>A <code>data.table</code> (of new class <code>wordvec</code>) with two variables <code>word</code> and <code>vec</code>,
transformed from the raw data (see the URL in Source) into <code>.RData</code>
using the <code><a href="#topic+data_transform">data_transform</a></code> function.
</p>


<h3>Source</h3>

<p>Google Code - word2vec (<a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>class(demodata)
demodata

embed = as_embed(demodata, normalize=TRUE)
class(embed)
embed

</code></pre>

<hr>
<h2 id='dict_expand'>Expand a dictionary from the most similar words.</h2><span id='topic+dict_expand'></span>

<h3>Description</h3>

<p>Expand a dictionary from the most similar words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dict_expand(data, words, threshold = 0.5, iteration = 5, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dict_expand_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="dict_expand_+3A_words">words</code></td>
<td>
<p>A single word or a list of words,
used to calculate the
<a href="#topic+sum_wordvec">sum vector</a>.</p>
</td></tr>
<tr><td><code id="dict_expand_+3A_threshold">threshold</code></td>
<td>
<p>Threshold of cosine similarity,
used to find all words with similarities higher than this value.
Defaults to <code>0.5</code>. A low threshold may lead to failure of convergence.</p>
</td></tr>
<tr><td><code id="dict_expand_+3A_iteration">iteration</code></td>
<td>
<p>Number of maximum iterations. Defaults to <code>5</code>.</p>
</td></tr>
<tr><td><code id="dict_expand_+3A_verbose">verbose</code></td>
<td>
<p>Print information to the console? Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An expanded list (character vector) of words.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sum_wordvec">sum_wordvec</a></code>
</p>
<p><code><a href="#topic+most_similar">most_similar</a></code>
</p>
<p><code><a href="#topic+dict_reliability">dict_reliability</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dict = dict_expand(demodata, "king")
dict

dict = dict_expand(demodata, cc("king, queen"))
dict

most_similar(demodata, dict)

dict.cn = dict_expand(demodata, "China")
dict.cn  # too inclusive if setting threshold = 0.5

dict.cn = dict_expand(demodata,
                      cc("China, Chinese"),
                      threshold=0.6)
dict.cn  # adequate to represent "China"

</code></pre>

<hr>
<h2 id='dict_reliability'>Reliability analysis and PCA of a dictionary.</h2><span id='topic+dict_reliability'></span>

<h3>Description</h3>

<p>Reliability analysis (Cronbach's <code class="reqn">\alpha</code> and average cosine similarity) and
Principal Component Analysis (PCA) of a dictionary,
with <a href="#topic+plot_similarity">visualization of cosine similarities</a>
between words (ordered by the first principal component loading).
Note that Cronbach's <code class="reqn">\alpha</code> can be misleading
when the number of items/words is large.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dict_reliability(
  data,
  words = NULL,
  pattern = NULL,
  alpha = TRUE,
  sort = TRUE,
  plot = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dict_reliability_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="dict_reliability_+3A_words">words</code></td>
<td>
<p>[Option 1] Character string(s).</p>
</td></tr>
<tr><td><code id="dict_reliability_+3A_pattern">pattern</code></td>
<td>
<p>[Option 2] Regular expression (see <code><a href="stringr.html#topic+str_subset">str_subset</a></code>).
If neither <code>words</code> nor <code>pattern</code> are specified (i.e., both are <code>NULL</code>),
then all words in the data will be extracted.</p>
</td></tr>
<tr><td><code id="dict_reliability_+3A_alpha">alpha</code></td>
<td>
<p>Estimate the Cronbach's <code class="reqn">\alpha</code>? Defaults to <code>TRUE</code>.
Note that this can be <em>misleading</em> and <em>time-consuming</em>
when the number of items/words is large.</p>
</td></tr>
<tr><td><code id="dict_reliability_+3A_sort">sort</code></td>
<td>
<p>Sort items by the first principal component loading (PC1)?
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="dict_reliability_+3A_plot">plot</code></td>
<td>
<p>Visualize the cosine similarities? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="dict_reliability_+3A_...">...</code></td>
<td>
<p>Other parameters passed to <code><a href="#topic+plot_similarity">plot_similarity</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> object of new class <code>reliability</code>:
</p>

<dl>
<dt><code>alpha</code></dt><dd>
<p>Cronbach's <code class="reqn">\alpha</code></p>
</dd>
<dt><code>eigen</code></dt><dd>
<p>Eigen values from PCA</p>
</dd>
<dt><code>pca</code></dt><dd>
<p>PCA (only 1 principal component)</p>
</dd>
<dt><code>pca.rotation</code></dt><dd>
<p>PCA with varimax rotation (if potential principal components &gt; 1)</p>
</dd>
<dt><code>items</code></dt><dd>
<p>Item statistics</p>
</dd>
<dt><code>cos.sim.mat</code></dt><dd>
<p>A matrix of cosine similarities of all word pairs</p>
</dd>
<dt><code>cos.sim</code></dt><dd>
<p>Lower triangular part of the matrix of cosine similarities</p>
</dd>
</dl>



<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>References</h3>

<p>Nicolas, G., Bai, X., &amp; Fiske, S. T. (2021).
Comprehensive stereotype content dictionaries using a semi-automated method.
<em>European Journal of Social Psychology, 51</em>(1), 178&ndash;196.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cosine_similarity">cosine_similarity</a></code>
</p>
<p><code><a href="#topic+pair_similarity">pair_similarity</a></code>
</p>
<p><code><a href="#topic+plot_similarity">plot_similarity</a></code>
</p>
<p><code><a href="#topic+tab_similarity">tab_similarity</a></code>
</p>
<p><code><a href="#topic+most_similar">most_similar</a></code>
</p>
<p><code><a href="#topic+dict_expand">dict_expand</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = as_embed(demodata, normalize=TRUE)

dict = dict_expand(d, "king")
dict_reliability(d, dict)

dict.cn = dict_expand(d, "China", threshold=0.65)
dict_reliability(d, dict.cn)

dict_reliability(d, c(dict, dict.cn))
# low-loading items should be removed

</code></pre>

<hr>
<h2 id='get_wordvec'>Extract word vector(s).</h2><span id='topic+get_wordvec'></span>

<h3>Description</h3>

<p>Extract word vector(s), using either a list of words or a regular expression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_wordvec(
  data,
  words = NULL,
  pattern = NULL,
  plot = FALSE,
  plot.dims = NULL,
  plot.step = 0.05,
  plot.border = "white"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_wordvec_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="get_wordvec_+3A_words">words</code></td>
<td>
<p>[Option 1] Character string(s).</p>
</td></tr>
<tr><td><code id="get_wordvec_+3A_pattern">pattern</code></td>
<td>
<p>[Option 2] Regular expression (see <code><a href="stringr.html#topic+str_subset">str_subset</a></code>).
If neither <code>words</code> nor <code>pattern</code> are specified (i.e., both are <code>NULL</code>),
then all words in the data will be extracted.</p>
</td></tr>
<tr><td><code id="get_wordvec_+3A_plot">plot</code></td>
<td>
<p>Generate a plot to illustrate the word vectors? Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="get_wordvec_+3A_plot.dims">plot.dims</code></td>
<td>
<p>Dimensions to be plotted (e.g., <code>1:100</code>).
Defaults to <code>NULL</code> (plot all dimensions).</p>
</td></tr>
<tr><td><code id="get_wordvec_+3A_plot.step">plot.step</code></td>
<td>
<p>Step for value breaks. Defaults to <code>0.05</code>.</p>
</td></tr>
<tr><td><code id="get_wordvec_+3A_plot.border">plot.border</code></td>
<td>
<p>Color of tile border. Defaults to <code>"white"</code>.
To remove the border color, set <code>plot.border=NA</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.table</code> with words as columns and dimensions as rows.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_wordvec_subset">data_wordvec_subset</a></code>
</p>
<p><code><a href="#topic+plot_wordvec">plot_wordvec</a></code>
</p>
<p><code><a href="#topic+plot_wordvec_tSNE">plot_wordvec_tSNE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = as_embed(demodata, normalize=TRUE)

get_wordvec(d, c("China", "Japan", "Korea"))
get_wordvec(d, cc(" China, Japan; Korea "))

## specify `pattern`:
get_wordvec(d, pattern="Chin[ae]|Japan|Korea")

## plot word vectors:
get_wordvec(d, cc("China, Japan, Korea,
                   Mac, Linux, Windows"),
            plot=TRUE, plot.dims=1:100)

## a more complex example:

words = cc("
China
Chinese
Japan
Japanese
good
bad
great
terrible
morning
evening
king
queen
man
woman
he
she
cat
dog
")

dt = get_wordvec(
  d, words,
  plot=TRUE,
  plot.dims=1:100,
  plot.step=0.06)

# if you want to change something:
attr(dt, "ggplot") +
  scale_fill_viridis_b(n.breaks=10, show.limits=TRUE) +
  theme(legend.key.height=unit(0.1, "npc"))

# or to save the plot:
ggsave(attr(dt, "ggplot"),
       filename="wordvecs.png",
       width=8, height=5, dpi=500)
unlink("wordvecs.png")  # delete file for code check

</code></pre>

<hr>
<h2 id='most_similar'>Find the Top-N most similar words.</h2><span id='topic+most_similar'></span>

<h3>Description</h3>

<p>Find the Top-N most similar words, which replicates the results produced
by the Python <code>gensim</code> module <code>most_similar()</code> function.
(Exact replication of <code>gensim</code> requires the same word vectors data,
not the <code>demodata</code> used here in examples.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>most_similar(
  data,
  x = NULL,
  topn = 10,
  above = NULL,
  keep = FALSE,
  row.id = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="most_similar_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="most_similar_+3A_x">x</code></td>
<td>
<p>Can be:
</p>

<ul>
<li><p><code>NULL</code>: use the sum of all word vectors in <code>data</code>
</p>
</li>
<li><p>a single word:
</p>
<p><code>"China"</code>
</p>
</li>
<li><p>a list of words:
</p>
<p><code>c("king", "queen")</code>
</p>
<p><code>cc(" king , queen ; man | woman")</code>
</p>
</li>
<li><p>an R formula (<code>~ xxx</code>) specifying
words that positively and negatively
contribute to the similarity (for word analogy):
</p>
<p><code>~ boy - he + she</code>
</p>
<p><code>~ king - man + woman</code>
</p>
<p><code>~ Beijing - China + Japan</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="most_similar_+3A_topn">topn</code></td>
<td>
<p>Top-N most similar words. Defaults to <code>10</code>.</p>
</td></tr>
<tr><td><code id="most_similar_+3A_above">above</code></td>
<td>
<p>Defaults to <code>NULL</code>. Can be:
</p>

<ul>
<li><p>a threshold value to find all words with cosine similarities
higher than this value
</p>
</li>
<li><p>a critical word to find all words with cosine similarities
higher than that with this critical word
</p>
</li></ul>

<p>If both <code>topn</code> and <code>above</code> are specified, <code>above</code> wins.</p>
</td></tr>
<tr><td><code id="most_similar_+3A_keep">keep</code></td>
<td>
<p>Keep words specified in <code>x</code> in results?
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="most_similar_+3A_row.id">row.id</code></td>
<td>
<p>Return the row number of each word? Defaults to <code>TRUE</code>,
which may help determine the relative word frequency in some cases.</p>
</td></tr>
<tr><td><code id="most_similar_+3A_verbose">verbose</code></td>
<td>
<p>Print information to the console? Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.table</code> with the most similar words and their cosine similarities.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sum_wordvec">sum_wordvec</a></code>
</p>
<p><code><a href="#topic+dict_expand">dict_expand</a></code>
</p>
<p><code><a href="#topic+dict_reliability">dict_reliability</a></code>
</p>
<p><code><a href="#topic+cosine_similarity">cosine_similarity</a></code>
</p>
<p><code><a href="#topic+pair_similarity">pair_similarity</a></code>
</p>
<p><code><a href="#topic+plot_similarity">plot_similarity</a></code>
</p>
<p><code><a href="#topic+tab_similarity">tab_similarity</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = as_embed(demodata, normalize=TRUE)

most_similar(d)
most_similar(d, "China")
most_similar(d, c("king", "queen"))
most_similar(d, cc(" king , queen ; man | woman "))

# the same as above:
most_similar(d, ~ China)
most_similar(d, ~ king + queen)
most_similar(d, ~ king + queen + man + woman)

most_similar(d, ~ boy - he + she)
most_similar(d, ~ Jack - he + she)
most_similar(d, ~ Rose - she + he)

most_similar(d, ~ king - man + woman)
most_similar(d, ~ Tokyo - Japan + China)
most_similar(d, ~ Beijing - China + Japan)

most_similar(d, "China", above=0.7)
most_similar(d, "China", above="Shanghai")

# automatically normalized for more accurate results
ms = most_similar(demodata, ~ king - man + woman)
ms
str(ms)

</code></pre>

<hr>
<h2 id='normalize'>Normalize all word vectors to the unit length 1.</h2><span id='topic+normalize'></span>

<h3>Description</h3>

<p>L2-normalization (scaling to unit euclidean length):
the <em>norm</em> of each vector in the vector space will be normalized to 1.
It is necessary for any linear operation of word vectors.
</p>
<p>R code:
</p>

<ul>
<li><p>Vector: <code>vec / sqrt(sum(vec^2))</code>
</p>
</li>
<li><p>Matrix: <code>mat / sqrt(rowSums(mat^2))</code>
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>normalize(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>wordvec</code> (data.table) or <code>embed</code> (matrix) with <strong>normalized</strong> word vectors.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as_wordvec">as_wordvec</a></code> / <code><a href="#topic+as_embed">as_embed</a></code>
</p>
<p><code><a href="#topic+load_wordvec">load_wordvec</a></code> / <code><a href="#topic+load_embed">load_embed</a></code>
</p>
<p><code><a href="#topic+data_transform">data_transform</a></code>
</p>
<p><code><a href="#topic+data_wordvec_subset">data_wordvec_subset</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = normalize(demodata)
# the same: d = as_wordvec(demodata, normalize=TRUE)

</code></pre>

<hr>
<h2 id='orth_procrustes'>Orthogonal Procrustes rotation for matrix alignment.</h2><span id='topic+orth_procrustes'></span>

<h3>Description</h3>

<p>In order to compare word embeddings from different time periods,
we must ensure that the embedding matrices are aligned to
the same semantic space (coordinate axes).
The Orthogonal Procrustes solution (Schönemann, 1966) is
commonly used to align historical embeddings over time
(Hamilton et al., 2016; Li et al., 2020).
</p>
<p>Note that this kind of rotation <em>does not</em> change the
relative relationships between vectors in the space,
and thus <em>does not</em> affect semantic similarities or distances
within each embedding matrix.
But it does influence the semantic relationships between
different embedding matrices, and thus would be necessary
for some purposes such as the &quot;semantic drift analysis&quot;
(e.g., Hamilton et al., 2016; Li et al., 2020).
</p>
<p>This function produces the same results as by
<code>cds::orthprocr()</code>,
<code>psych::Procrustes()</code>, and
<code>pracma::procrustes()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>orth_procrustes(M, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="orth_procrustes_+3A_m">M</code>, <code id="orth_procrustes_+3A_x">X</code></td>
<td>
<p>Two embedding matrices of the same size (rows and columns),
can be <code><a href="#topic+as_embed">embed</a></code>
or <code><a href="#topic+as_wordvec">wordvec</a></code> objects.
</p>

<ul>
<li><p><code>M</code> is the reference (anchor/baseline/target) matrix,
e.g., the embedding matrix learned at
the later year (<code class="reqn">t + 1</code>).
</p>
</li>
<li><p><code>X</code> is the matrix to be transformed/rotated.
</p>
</li></ul>

<p><em>Note</em>: The function automatically extracts only
the intersection (overlapped part) of words in <code>M</code> and <code>X</code>
and sorts them in the same order (according to <code>M</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>matrix</code> or <code>wordvec</code> object of
<code>X</code> after rotation, depending on the class of
<code>M</code> and <code>X</code>.
</p>


<h3>References</h3>

<p>Hamilton, W. L., Leskovec, J., &amp; Jurafsky, D. (2016).
Diachronic word embeddings reveal statistical laws of semantic change.
In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</em>
(Vol. 1, pp. 1489&ndash;1501). Association for Computational Linguistics.
</p>
<p>Li, Y., Hills, T., &amp; Hertwig, R. (2020).
A brief history of risk. <em>Cognition, 203</em>, 104344.
</p>
<p>Schönemann, P. H. (1966).
A generalized solution of the orthogonal Procrustes problem.
<em>Psychometrika, 31</em>(1), 1&ndash;10.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as_wordvec">as_wordvec</a></code> / <code><a href="#topic+as_embed">as_embed</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>M = matrix(c(0,0,  1,2,  2,0,  3,2,  4,0), ncol=2, byrow=TRUE)
X = matrix(c(0,0, -2,1,  0,2, -2,3,  0,4), ncol=2, byrow=TRUE)
rownames(M) = rownames(X) = cc("A, B, C, D, E")  # words
colnames(M) = colnames(X) = cc("dim1, dim2")  # dimensions

ggplot() +
  geom_path(data=as.data.frame(M), aes(x=dim1, y=dim2),
            color="red") +
  geom_path(data=as.data.frame(X), aes(x=dim1, y=dim2),
            color="blue") +
  coord_equal()

# Usage 1: input two matrices (can be `embed` objects)
XR = orth_procrustes(M, X)
XR  # aligned with M

ggplot() +
  geom_path(data=as.data.frame(XR), aes(x=dim1, y=dim2)) +
  coord_equal()

# Usage 2: input two `wordvec` objects
M.wv = as_wordvec(M)
X.wv = as_wordvec(X)
XR.wv = orth_procrustes(M.wv, X.wv)
XR.wv  # aligned with M.wv

# M and X must have the same set and order of words
# and the same number of word vector dimensions.
# The function extracts only the intersection of words
# and sorts them in the same order according to M.

Y = rbind(X, X[rev(rownames(X)),])
rownames(Y)[1:5] = cc("F, G, H, I, J")
M.wv = as_wordvec(M)
Y.wv = as_wordvec(Y)
M.wv  # words: A, B, C, D, E
Y.wv  # words: F, G, H, I, J, E, D, C, B, A
YR.wv = orth_procrustes(M.wv, Y.wv)
YR.wv  # aligned with M.wv, with the same order of words

</code></pre>

<hr>
<h2 id='pair_similarity'>Compute a matrix of cosine similarity/distance of word pairs.</h2><span id='topic+pair_similarity'></span>

<h3>Description</h3>

<p>Compute a matrix of cosine similarity/distance of word pairs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pair_similarity(
  data,
  words = NULL,
  pattern = NULL,
  words1 = NULL,
  words2 = NULL,
  distance = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pair_similarity_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="pair_similarity_+3A_words">words</code></td>
<td>
<p>[Option 1] Character string(s).</p>
</td></tr>
<tr><td><code id="pair_similarity_+3A_pattern">pattern</code></td>
<td>
<p>[Option 2] Regular expression (see <code><a href="stringr.html#topic+str_subset">str_subset</a></code>).
If neither <code>words</code> nor <code>pattern</code> are specified (i.e., both are <code>NULL</code>),
then all words in the data will be extracted.</p>
</td></tr>
<tr><td><code id="pair_similarity_+3A_words1">words1</code>, <code id="pair_similarity_+3A_words2">words2</code></td>
<td>
<p>[Option 3]
Two sets of words for only n1 * n2 word pairs. See examples.</p>
</td></tr>
<tr><td><code id="pair_similarity_+3A_distance">distance</code></td>
<td>
<p>Compute cosine distance instead?
Defaults to <code>FALSE</code> (cosine similarity).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of pairwise cosine similarity/distance.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cosine_similarity">cosine_similarity</a></code>
</p>
<p><code><a href="#topic+plot_similarity">plot_similarity</a></code>
</p>
<p><code><a href="#topic+tab_similarity">tab_similarity</a></code>
</p>
<p><code><a href="#topic+most_similar">most_similar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pair_similarity(demodata, c("China", "Chinese"))

pair_similarity(demodata, pattern="^Chi")

pair_similarity(demodata,
                words1=c("China", "Chinese"),
                words2=c("Japan", "Japanese"))

</code></pre>

<hr>
<h2 id='plot_network'>Visualize a (partial correlation) network graph of words.</h2><span id='topic+plot_network'></span>

<h3>Description</h3>

<p>Visualize a (partial correlation) network graph of words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_network(
  data,
  words = NULL,
  pattern = NULL,
  index = c("pcor", "cor", "glasso", "sim"),
  alpha = 0.05,
  bonf = FALSE,
  max = NULL,
  node.size = "auto",
  node.group = NULL,
  node.color = NULL,
  label.text = NULL,
  label.size = 1.2,
  label.size.equal = TRUE,
  label.color = "black",
  edge.color = c("#009900", "#BF0000"),
  edge.label = FALSE,
  edge.label.size = 1,
  edge.label.color = NULL,
  edge.label.bg = "white",
  file = NULL,
  width = 10,
  height = 6,
  dpi = 500,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_network_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_words">words</code></td>
<td>
<p>[Option 1] Character string(s).</p>
</td></tr>
<tr><td><code id="plot_network_+3A_pattern">pattern</code></td>
<td>
<p>[Option 2] Regular expression (see <code><a href="stringr.html#topic+str_subset">str_subset</a></code>).
If neither <code>words</code> nor <code>pattern</code> are specified (i.e., both are <code>NULL</code>),
then all words in the data will be extracted.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_index">index</code></td>
<td>
<p>Use which index to perform network analysis?
Can be <code>"pcor"</code> (partial correlation, default and suggested),
<code>"cor"</code> (raw correlation),
<code>"glasso"</code> (graphical lasso-estimation of partial correlation matrix
using the <code>glasso</code> package),
or <code>"sim"</code> (pairwise cosine similarity).</p>
</td></tr>
<tr><td><code id="plot_network_+3A_alpha">alpha</code></td>
<td>
<p>Significance level to be used for not showing edges. Defaults to <code>0.05</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_bonf">bonf</code></td>
<td>
<p>Bonferroni correction of <em>p</em> value. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_max">max</code></td>
<td>
<p>Maximum value for scaling edge widths and colors. Defaults to the highest value of the index.
Can be <code>1</code> if you want to compare several graphs.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_node.size">node.size</code></td>
<td>
<p>Node size. Defaults to 8*exp(-nNodes/80)+1.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_node.group">node.group</code></td>
<td>
<p>Node group(s). Can be a named list (see examples) in which each element
is a vector of integers identifying the numbers of the nodes that belong together, or a factor.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_node.color">node.color</code></td>
<td>
<p>Node color(s). Can be a character vector of colors corresponding to <code>node.group</code>.
Defaults to white (if <code>node.group</code> is not specified)
or the palette of ggplot2 (if <code>node.group</code> is specified).</p>
</td></tr>
<tr><td><code id="plot_network_+3A_label.text">label.text</code></td>
<td>
<p>Node label of text. Defaults to original words.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_label.size">label.size</code></td>
<td>
<p>Node label font size. Defaults to <code>1.2</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_label.size.equal">label.size.equal</code></td>
<td>
<p>Make the font size of all labels equal. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_label.color">label.color</code></td>
<td>
<p>Node label color. Defaults to <code>"black"</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_edge.color">edge.color</code></td>
<td>
<p>Edge colors for positive and negative values, respectively.
Defaults to <code>c("#009900", "#BF0000")</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_edge.label">edge.label</code></td>
<td>
<p>Edge label of values. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_edge.label.size">edge.label.size</code></td>
<td>
<p>Edge label font size. Defaults to <code>1</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_edge.label.color">edge.label.color</code></td>
<td>
<p>Edge label color. Defaults to <code>edge.color</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_edge.label.bg">edge.label.bg</code></td>
<td>
<p>Edge label background color. Defaults to <code>"white"</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_file">file</code></td>
<td>
<p>File name to be saved, should be png or pdf.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_width">width</code>, <code id="plot_network_+3A_height">height</code></td>
<td>
<p>Width and height (in inches) for the saved file.
Defaults to <code>10</code> and <code>6</code>.</p>
</td></tr>
<tr><td><code id="plot_network_+3A_dpi">dpi</code></td>
<td>
<p>Dots per inch. Defaults to <code>500</code> (i.e., file resolution: 4000 * 3000).</p>
</td></tr>
<tr><td><code id="plot_network_+3A_...">...</code></td>
<td>
<p>Other parameters passed to <code><a href="qgraph.html#topic+qgraph">qgraph</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly return a <code><a href="qgraph.html#topic+qgraph">qgraph</a></code> object,
which further can be plotted using <code>plot()</code>.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot_similarity">plot_similarity</a></code>
</p>
<p><code><a href="#topic+plot_wordvec_tSNE">plot_wordvec_tSNE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = as_embed(demodata, normalize=TRUE)

words = cc("
man, woman,
he, she,
boy, girl,
father, mother,
mom, dad,
China, Japan
")

plot_network(d, words)

p = plot_network(
  d, words,
  node.group=list(Gender=1:6, Family=7:10, Country=11:12),
  node.color=c("antiquewhite", "lightsalmon", "lightblue"),
  file="network.png")
plot(p)

unlink("network.png")  # delete file for code check

# network analysis with centrality plot (see `qgraph` package)
qgraph::centralityPlot(p, include="all", scale="raw",
                       orderBy="Strength")

# graphical lasso-estimation of partial correlation matrix
plot_network(
  d, words,
  index="glasso",
  # threshold=TRUE,
  node.group=list(Gender=1:6, Family=7:10, Country=11:12),
  node.color=c("antiquewhite", "lightsalmon", "lightblue"))

</code></pre>

<hr>
<h2 id='plot_similarity'>Visualize cosine similarity of word pairs.</h2><span id='topic+plot_similarity'></span>

<h3>Description</h3>

<p>Visualize cosine similarity of word pairs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_similarity(
  data,
  words = NULL,
  pattern = NULL,
  words1 = NULL,
  words2 = NULL,
  label = "auto",
  value.color = NULL,
  value.percent = FALSE,
  order = c("original", "AOE", "FPC", "hclust", "alphabet"),
  hclust.method = c("complete", "ward", "ward.D", "ward.D2", "single", "average",
    "mcquitty", "median", "centroid"),
  hclust.n = NULL,
  hclust.color = "black",
  hclust.line = 2,
  file = NULL,
  width = 10,
  height = 6,
  dpi = 500,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_similarity_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_words">words</code></td>
<td>
<p>[Option 1] Character string(s).</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_pattern">pattern</code></td>
<td>
<p>[Option 2] Regular expression (see <code><a href="stringr.html#topic+str_subset">str_subset</a></code>).
If neither <code>words</code> nor <code>pattern</code> are specified (i.e., both are <code>NULL</code>),
then all words in the data will be extracted.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_words1">words1</code>, <code id="plot_similarity_+3A_words2">words2</code></td>
<td>
<p>[Option 3]
Two sets of words for only n1 * n2 word pairs. See examples.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_label">label</code></td>
<td>
<p>Position of text labels.
Defaults to <code>"auto"</code> (add labels if less than 20 words).
Can be <code>TRUE</code> (left and top), <code>FALSE</code> (add no labels of words),
or a character string (see the usage of <code>tl.pos</code> in <code><a href="corrplot.html#topic+corrplot">corrplot</a></code>.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_value.color">value.color</code></td>
<td>
<p>Color of values added on the plot.
Defaults to <code>NULL</code> (add no values).</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_value.percent">value.percent</code></td>
<td>
<p>Whether to transform values into percentage style for space saving.
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_order">order</code></td>
<td>
<p>Character, the ordering method of the correlation matrix.
</p>

<ul>
<li><p><code>'original'</code> for original order (default).
</p>
</li>
<li><p><code>'AOE'</code> for the angular order of the eigenvectors.
</p>
</li>
<li><p><code>'FPC'</code> for the first principal component order.
</p>
</li>
<li><p><code>'hclust'</code> for the hierarchical clustering order.
</p>
</li>
<li><p><code>'alphabet'</code> for alphabetical order.
</p>
</li></ul>

<p>See function <code><a href="corrplot.html#topic+corrMatOrder">corrMatOrder</a></code> for details.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_hclust.method">hclust.method</code></td>
<td>
<p>Character, the agglomeration method to be used when
<code>order</code> is <code><a href="stats.html#topic+hclust">hclust</a></code>. This should be one of <code>'ward'</code>,
<code>'ward.D'</code>, <code>'ward.D2'</code>, <code>'single'</code>, <code>'complete'</code>,
<code>'average'</code>, <code>'mcquitty'</code>, <code>'median'</code> or <code>'centroid'</code>.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_hclust.n">hclust.n</code></td>
<td>
<p>Number of rectangles to be drawn on the plot according to
the hierarchical clusters, only valid when <code>order="hclust"</code>.
Defaults to <code>NULL</code> (add no rectangles).</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_hclust.color">hclust.color</code></td>
<td>
<p>Color of rectangle border, only valid when <code>hclust.n</code> &gt;= 1.
Defaults to <code>"black"</code>.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_hclust.line">hclust.line</code></td>
<td>
<p>Line width of rectangle border, only valid when <code>hclust.n</code> &gt;= 1.
Defaults to <code>2</code>.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_file">file</code></td>
<td>
<p>File name to be saved, should be png or pdf.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_width">width</code>, <code id="plot_similarity_+3A_height">height</code></td>
<td>
<p>Width and height (in inches) for the saved file.
Defaults to <code>10</code> and <code>6</code>.</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_dpi">dpi</code></td>
<td>
<p>Dots per inch. Defaults to <code>500</code> (i.e., file resolution: 4000 * 3000).</p>
</td></tr>
<tr><td><code id="plot_similarity_+3A_...">...</code></td>
<td>
<p>Other parameters passed to <code><a href="corrplot.html#topic+corrplot">corrplot</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly return a matrix of cosine similarity between each pair of words.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cosine_similarity">cosine_similarity</a></code>
</p>
<p><code><a href="#topic+pair_similarity">pair_similarity</a></code>
</p>
<p><code><a href="#topic+tab_similarity">tab_similarity</a></code>
</p>
<p><code><a href="#topic+most_similar">most_similar</a></code>
</p>
<p><code><a href="#topic+plot_network">plot_network</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>w1 = cc("king, queen, man, woman")
plot_similarity(demodata, w1)
plot_similarity(demodata, w1,
                value.color="grey",
                value.percent=TRUE)
plot_similarity(demodata, w1,
                value.color="grey",
                order="hclust",
                hclust.n=2)

plot_similarity(
  demodata,
  words1=cc("man, woman, king, queen"),
  words2=cc("he, she, boy, girl, father, mother"),
  value.color="grey20"
)

w2 = cc("China, Chinese,
         Japan, Japanese,
         Korea, Korean,
         man, woman, boy, girl,
         good, bad, positive, negative")
plot_similarity(demodata, w2,
                order="hclust",
                hclust.n=3)
plot_similarity(demodata, w2,
                order="hclust",
                hclust.n=7,
                file="plot.png")

unlink("plot.png")  # delete file for code check

</code></pre>

<hr>
<h2 id='plot_wordvec'>Visualize word vectors.</h2><span id='topic+plot_wordvec'></span>

<h3>Description</h3>

<p>Visualize word vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_wordvec(x, dims = NULL, step = 0.05, border = "white")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_wordvec_+3A_x">x</code></td>
<td>
<p>Can be:
</p>

<ul>
<li><p>a <code>data.table</code> returned by <code><a href="#topic+get_wordvec">get_wordvec</a></code>
</p>
</li>
<li><p>a <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table)
or <code><a href="#topic+as_embed">embed</a></code> (matrix)
loaded by <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_wordvec_+3A_dims">dims</code></td>
<td>
<p>Dimensions to be plotted (e.g., <code>1:100</code>).
Defaults to <code>NULL</code> (plot all dimensions).</p>
</td></tr>
<tr><td><code id="plot_wordvec_+3A_step">step</code></td>
<td>
<p>Step for value breaks. Defaults to <code>0.05</code>.</p>
</td></tr>
<tr><td><code id="plot_wordvec_+3A_border">border</code></td>
<td>
<p>Color of tile border. Defaults to <code>"white"</code>.
To remove the border color, set <code>border=NA</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot</code> object.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_wordvec">get_wordvec</a></code>
</p>
<p><code><a href="#topic+plot_similarity">plot_similarity</a></code>
</p>
<p><code><a href="#topic+plot_wordvec_tSNE">plot_wordvec_tSNE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = as_embed(demodata, normalize=TRUE)

plot_wordvec(d[1:10])

dt = get_wordvec(d, cc("king, queen, man, woman"))
dt[, QUEEN := king - man + woman]
dt[, QUEEN := QUEEN / sqrt(sum(QUEEN^2))]  # normalize
names(dt)[5] = "king - man + woman"
plot_wordvec(dt[, c(1,3,4,5,2)], dims=1:50)

dt = get_wordvec(d, cc("boy, girl, he, she"))
dt[, GIRL := boy - he + she]
dt[, GIRL := GIRL / sqrt(sum(GIRL^2))]  # normalize
names(dt)[5] = "boy - he + she"
plot_wordvec(dt[, c(1,3,4,5,2)], dims=1:50)

dt = get_wordvec(d, cc("
  male, man, boy, he, his,
  female, woman, girl, she, her"))

p = plot_wordvec(dt, dims=1:100)

# if you want to change something:
p + theme(legend.key.height=unit(0.1, "npc"))

# or to save the plot:
ggsave(p, filename="wordvecs.png",
       width=8, height=5, dpi=500)
unlink("wordvecs.png")  # delete file for code check

</code></pre>

<hr>
<h2 id='plot_wordvec_tSNE'>Visualize word vectors with dimensionality reduced using t-SNE.</h2><span id='topic+plot_wordvec_tSNE'></span>

<h3>Description</h3>

<p>Visualize word vectors with dimensionality reduced
using the t-Distributed Stochastic Neighbor Embedding (t-SNE) method
(i.e., projecting high-dimensional vectors into a low-dimensional vector space),
implemented by <code><a href="Rtsne.html#topic+Rtsne">Rtsne::Rtsne()</a></code>.
You should specify a random seed if you expect reproducible results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_wordvec_tSNE(
  x,
  dims = 2,
  perplexity,
  theta = 0.5,
  colors = NULL,
  seed = NULL,
  custom.Rtsne = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_wordvec_tSNE_+3A_x">x</code></td>
<td>
<p>Can be:
</p>

<ul>
<li><p>a <code>data.table</code> returned by <code><a href="#topic+get_wordvec">get_wordvec</a></code>
</p>
</li>
<li><p>a <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table)
or <code><a href="#topic+as_embed">embed</a></code> (matrix)
loaded by <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_wordvec_tSNE_+3A_dims">dims</code></td>
<td>
<p>Output dimensionality: <code>2</code> (default, the most common choice) or <code>3</code>.</p>
</td></tr>
<tr><td><code id="plot_wordvec_tSNE_+3A_perplexity">perplexity</code></td>
<td>
<p>Perplexity parameter, should not be larger than (number of words - 1) / 3.
Defaults to <code>floor((length(dt)-1)/3)</code> (where columns of <code>dt</code> are words).
See the <code><a href="Rtsne.html#topic+Rtsne">Rtsne</a></code> package for details.</p>
</td></tr>
<tr><td><code id="plot_wordvec_tSNE_+3A_theta">theta</code></td>
<td>
<p>Speed/accuracy trade-off (increase for less accuracy), set to 0 for exact t-SNE.
Defaults to 0.5.</p>
</td></tr>
<tr><td><code id="plot_wordvec_tSNE_+3A_colors">colors</code></td>
<td>
<p>A character vector specifying (1) the categories of words (for 2-D plot only)
or (2) the exact colors of words (for 2-D and 3-D plot). See examples for its usage.</p>
</td></tr>
<tr><td><code id="plot_wordvec_tSNE_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducible results. Defaults to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="plot_wordvec_tSNE_+3A_custom.rtsne">custom.Rtsne</code></td>
<td>
<p>User-defined <code><a href="Rtsne.html#topic+Rtsne">Rtsne</a></code> object using the same <code>dt</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>2-D: A <code>ggplot</code> object.
You may extract the data from this object using <code>$data</code>.
</p>
<p>3-D: Nothing but only the data was invisibly returned,
because <code><a href="rgl.html#topic+plot3d">rgl::plot3d()</a></code> is
&quot;called for the side effect of drawing the plot&quot;
and thus cannot return any 3-D plot object.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>References</h3>

<p>Hinton, G. E., &amp; Salakhutdinov, R. R. (2006).
Reducing the dimensionality of data with neural networks.
<em>Science, 313</em>(5786), 504&ndash;507.
</p>
<p>van der Maaten, L., &amp; Hinton, G. (2008).
Visualizing data using t-SNE.
<em>Journal of Machine Learning Research, 9</em>, 2579&ndash;2605.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot_wordvec">plot_wordvec</a></code>
</p>
<p><code><a href="#topic+plot_network">plot_network</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d = as_embed(demodata, normalize=TRUE)

dt = get_wordvec(d, cc("
  man, woman,
  king, queen,
  China, Beijing,
  Japan, Tokyo"))

## 2-D (default):
plot_wordvec_tSNE(dt, seed=1234)

plot_wordvec_tSNE(dt, seed=1234)$data

colors = c(rep("#2B579A", 4), rep("#B7472A", 4))
plot_wordvec_tSNE(dt, colors=colors, seed=1234)

category = c(rep("gender", 4), rep("country", 4))
plot_wordvec_tSNE(dt, colors=category, seed=1234) +
  scale_x_continuous(limits=c(-200, 200),
                     labels=function(x) x/100) +
  scale_y_continuous(limits=c(-200, 200),
                     labels=function(x) x/100) +
  scale_color_manual(values=c("#B7472A", "#2B579A"))

## 3-D:
colors = c(rep("#2B579A", 4), rep("#B7472A", 4))
plot_wordvec_tSNE(dt, dims=3, colors=colors, seed=1)

</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+cc'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>bruceR</dt><dd><p><code><a href="bruceR.html#topic+cc">cc</a></code></p>
</dd>
</dl>

<hr>
<h2 id='sum_wordvec'>Calculate the sum vector of multiple words.</h2><span id='topic+sum_wordvec'></span>

<h3>Description</h3>

<p>Calculate the sum vector of multiple words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sum_wordvec(data, x = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sum_wordvec_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="sum_wordvec_+3A_x">x</code></td>
<td>
<p>Can be:
</p>

<ul>
<li><p><code>NULL</code>: use the sum of all word vectors in <code>data</code>
</p>
</li>
<li><p>a single word:
</p>
<p><code>"China"</code>
</p>
</li>
<li><p>a list of words:
</p>
<p><code>c("king", "queen")</code>
</p>
<p><code>cc(" king , queen ; man | woman")</code>
</p>
</li>
<li><p>an R formula (<code>~ xxx</code>) specifying
words that positively and negatively
contribute to the similarity (for word analogy):
</p>
<p><code>~ boy - he + she</code>
</p>
<p><code>~ king - man + woman</code>
</p>
<p><code>~ Beijing - China + Japan</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="sum_wordvec_+3A_verbose">verbose</code></td>
<td>
<p>Print information to the console? Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Normalized sum vector.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normalize">normalize</a></code>
</p>
<p><code><a href="#topic+most_similar">most_similar</a></code>
</p>
<p><code><a href="#topic+dict_expand">dict_expand</a></code>
</p>
<p><code><a href="#topic+dict_reliability">dict_reliability</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sum_wordvec(normalize(demodata), ~ king - man + woman)

</code></pre>

<hr>
<h2 id='tab_similarity'>Tabulate cosine similarity/distance of word pairs.</h2><span id='topic+tab_similarity'></span>

<h3>Description</h3>

<p>Tabulate cosine similarity/distance of word pairs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tab_similarity(
  data,
  words = NULL,
  pattern = NULL,
  words1 = NULL,
  words2 = NULL,
  unique = FALSE,
  distance = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tab_similarity_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="tab_similarity_+3A_words">words</code></td>
<td>
<p>[Option 1] Character string(s).</p>
</td></tr>
<tr><td><code id="tab_similarity_+3A_pattern">pattern</code></td>
<td>
<p>[Option 2] Regular expression (see <code><a href="stringr.html#topic+str_subset">str_subset</a></code>).
If neither <code>words</code> nor <code>pattern</code> are specified (i.e., both are <code>NULL</code>),
then all words in the data will be extracted.</p>
</td></tr>
<tr><td><code id="tab_similarity_+3A_words1">words1</code>, <code id="tab_similarity_+3A_words2">words2</code></td>
<td>
<p>[Option 3]
Two sets of words for only n1 * n2 word pairs. See examples.</p>
</td></tr>
<tr><td><code id="tab_similarity_+3A_unique">unique</code></td>
<td>
<p>Return unique word pairs (<code>TRUE</code>)
or all pairs with duplicates (<code>FALSE</code>; default).</p>
</td></tr>
<tr><td><code id="tab_similarity_+3A_distance">distance</code></td>
<td>
<p>Compute cosine distance instead?
Defaults to <code>FALSE</code> (cosine similarity).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.table</code> of words, word pairs,
and their cosine similarity (<code>cos_sim</code>)
or cosine distance (<code>cos_dist</code>).
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cosine_similarity">cosine_similarity</a></code>
</p>
<p><code><a href="#topic+pair_similarity">pair_similarity</a></code>
</p>
<p><code><a href="#topic+plot_similarity">plot_similarity</a></code>
</p>
<p><code><a href="#topic+most_similar">most_similar</a></code>
</p>
<p><code><a href="#topic+test_WEAT">test_WEAT</a></code>
</p>
<p><code><a href="#topic+test_RND">test_RND</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tab_similarity(demodata, cc("king, queen, man, woman"))
tab_similarity(demodata, cc("king, queen, man, woman"),
               unique=TRUE)

tab_similarity(demodata, cc("Beijing, China, Tokyo, Japan"))
tab_similarity(demodata, cc("Beijing, China, Tokyo, Japan"),
               unique=TRUE)

## only n1 * n2 word pairs across two sets of words
tab_similarity(demodata,
               words1=cc("king, queen, King, Queen"),
               words2=cc("man, woman"))

</code></pre>

<hr>
<h2 id='test_RND'>Relative Norm Distance (RND) analysis.</h2><span id='topic+test_RND'></span>

<h3>Description</h3>

<p>Tabulate data and conduct the permutation test of significance
for the <em>Relative Norm Distance</em> (RND; also known as <em>Relative Euclidean Distance</em>).
This is an alternative method to <a href="#topic+test_WEAT">Single-Category WEAT</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_RND(
  data,
  T1,
  A1,
  A2,
  use.pattern = FALSE,
  labels = list(),
  p.perm = TRUE,
  p.nsim = 10000,
  p.side = 2,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_RND_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="test_RND_+3A_t1">T1</code></td>
<td>
<p>Target words of a single category (a vector of words or a pattern of regular expression).</p>
</td></tr>
<tr><td><code id="test_RND_+3A_a1">A1</code>, <code id="test_RND_+3A_a2">A2</code></td>
<td>
<p>Attribute words (a vector of words or a pattern of regular expression).
Both must be specified.</p>
</td></tr>
<tr><td><code id="test_RND_+3A_use.pattern">use.pattern</code></td>
<td>
<p>Defaults to <code>FALSE</code> (using a vector of words).
If you use regular expression in <code>T1</code>, <code>T2</code>, <code>A1</code>, and <code>A2</code>,
please specify this argument as <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="test_RND_+3A_labels">labels</code></td>
<td>
<p>Labels for target and attribute concepts (a named <code>list</code>),
such as (the default)
<code>list(T1="Target", A1="Attrib1", A2="Attrib2")</code>.</p>
</td></tr>
<tr><td><code id="test_RND_+3A_p.perm">p.perm</code></td>
<td>
<p>Permutation test to get exact or approximate <em>p</em> value of the overall effect.
Defaults to <code>TRUE</code>. See also the <code><a href="sweater.html#topic+weat_exact">sweater</a></code> package.</p>
</td></tr>
<tr><td><code id="test_RND_+3A_p.nsim">p.nsim</code></td>
<td>
<p>Number of samples for resampling in permutation test. Defaults to <code>10000</code>.
</p>
<p>If <code>p.nsim</code> is larger than the number of all possible permutations (rearrangements of data),
then it will be ignored and an exact permutation test will be conducted.
Otherwise (in most cases for real data and always for SC-WEAT), a resampling test is performed,
which takes much less computation time and produces the approximate <em>p</em> value
(comparable to the exact one).</p>
</td></tr>
<tr><td><code id="test_RND_+3A_p.side">p.side</code></td>
<td>
<p>One-sided (<code>1</code>) or two-sided (<code>2</code>) <em>p</em> value.
Defaults to <code>2</code>.
</p>
<p>In Caliskan et al.'s (2017) article, they reported one-sided <em>p</em> value for WEAT.
Here, I suggest reporting two-sided <em>p</em> value as a more conservative estimate.
The users take the full responsibility for the choice.
</p>

<ul>
<li><p>The one-sided <em>p</em> value is calculated as the proportion of sampled permutations
where the difference in means is greater than the test statistic.
</p>
</li>
<li><p>The two-sided <em>p</em> value is calculated as the proportion of sampled permutations
where the absolute difference is greater than the test statistic.
</p>
</li></ul>
</td></tr>
<tr><td><code id="test_RND_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducible results of permutation test. Defaults to <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> object of new class <code>rnd</code>:
</p>

<dl>
<dt><code>words.valid</code></dt><dd>
<p>Valid (actually matched) words</p>
</dd>
<dt><code>words.not.found</code></dt><dd>
<p>Words not found</p>
</dd>
<dt><code>data.raw</code></dt><dd>
<p>A <code>data.table</code> of (absolute and relative) norm distances</p>
</dd>
<dt><code>eff.label</code></dt><dd>
<p>Description for the difference between the two attribute concepts</p>
</dd>
<dt><code>eff.type</code></dt><dd>
<p>Effect type: RND</p>
</dd>
<dt><code>eff</code></dt><dd>
<p>Raw effect and p value (if <code>p.perm=TRUE</code>)</p>
</dd>
<dt><code>eff.interpretation</code></dt><dd>
<p>Interpretation of the RND score</p>
</dd>
</dl>



<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>References</h3>

<p>Garg, N., Schiebinger, L., Jurafsky, D., &amp; Zou, J. (2018).
Word embeddings quantify 100 years of gender and ethnic stereotypes.
<em>Proceedings of the National Academy of Sciences, 115</em>(16), E3635&ndash;E3644.
</p>
<p>Bhatia, N., &amp; Bhatia, S. (2021).
Changes in gender stereotypes over time: A computational analysis.
<em>Psychology of Women Quarterly, 45</em>(1), 106&ndash;125.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tab_similarity">tab_similarity</a></code>
</p>
<p><code><a href="#topic+dict_expand">dict_expand</a></code>
</p>
<p><code><a href="#topic+dict_reliability">dict_reliability</a></code>
</p>
<p><code><a href="#topic+test_WEAT">test_WEAT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rnd = test_RND(
  demodata,
  labels=list(T1="Occupation", A1="Male", A2="Female"),
  T1=cc("
    architect, boss, leader, engineer, CEO, officer, manager,
    lawyer, scientist, doctor, psychologist, investigator,
    consultant, programmer, teacher, clerk, counselor,
    salesperson, therapist, psychotherapist, nurse"),
  A1=cc("male, man, boy, brother, he, him, his, son"),
  A2=cc("female, woman, girl, sister, she, her, hers, daughter"),
  seed=1)
rnd

</code></pre>

<hr>
<h2 id='test_WEAT'>Word Embedding Association Test (WEAT) and Single-Category WEAT.</h2><span id='topic+test_WEAT'></span>

<h3>Description</h3>

<p>Tabulate data (cosine similarity and standardized effect size) and
conduct the permutation test of significance for the
<em>Word Embedding Association Test</em> (WEAT) and
<em>Single-Category Word Embedding Association Test</em> (SC-WEAT).
</p>

<ul>
<li><p>For WEAT, two-samples permutation test is conducted (i.e., rearrangements of data).
</p>
</li>
<li><p>For SC-WEAT, one-sample permutation test is conducted (i.e., rearrangements of +/- signs to data).
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>test_WEAT(
  data,
  T1,
  T2,
  A1,
  A2,
  use.pattern = FALSE,
  labels = list(),
  p.perm = TRUE,
  p.nsim = 10000,
  p.side = 2,
  seed = NULL,
  pooled.sd = "Caliskan"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_WEAT_+3A_data">data</code></td>
<td>
<p>A <code><a href="#topic+as_wordvec">wordvec</a></code> (data.table) or
<code><a href="#topic+as_embed">embed</a></code> (matrix),
see <code><a href="#topic+data_wordvec_load">data_wordvec_load</a></code>.</p>
</td></tr>
<tr><td><code id="test_WEAT_+3A_t1">T1</code>, <code id="test_WEAT_+3A_t2">T2</code></td>
<td>
<p>Target words (a vector of words or a pattern of regular expression).
If only <code>T1</code> is specified,
it will tabulate data for single-category WEAT (SC-WEAT).</p>
</td></tr>
<tr><td><code id="test_WEAT_+3A_a1">A1</code>, <code id="test_WEAT_+3A_a2">A2</code></td>
<td>
<p>Attribute words (a vector of words or a pattern of regular expression).
Both must be specified.</p>
</td></tr>
<tr><td><code id="test_WEAT_+3A_use.pattern">use.pattern</code></td>
<td>
<p>Defaults to <code>FALSE</code> (using a vector of words).
If you use regular expression in <code>T1</code>, <code>T2</code>, <code>A1</code>, and <code>A2</code>,
please specify this argument as <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="test_WEAT_+3A_labels">labels</code></td>
<td>
<p>Labels for target and attribute concepts (a named <code>list</code>),
such as (the default)
<code>list(T1="Target1", T2="Target2", A1="Attrib1", A2="Attrib2")</code>.</p>
</td></tr>
<tr><td><code id="test_WEAT_+3A_p.perm">p.perm</code></td>
<td>
<p>Permutation test to get exact or approximate <em>p</em> value of the overall effect.
Defaults to <code>TRUE</code>. See also the <code><a href="sweater.html#topic+weat_exact">sweater</a></code> package.</p>
</td></tr>
<tr><td><code id="test_WEAT_+3A_p.nsim">p.nsim</code></td>
<td>
<p>Number of samples for resampling in permutation test. Defaults to <code>10000</code>.
</p>
<p>If <code>p.nsim</code> is larger than the number of all possible permutations (rearrangements of data),
then it will be ignored and an exact permutation test will be conducted.
Otherwise (in most cases for real data and always for SC-WEAT), a resampling test is performed,
which takes much less computation time and produces the approximate <em>p</em> value
(comparable to the exact one).</p>
</td></tr>
<tr><td><code id="test_WEAT_+3A_p.side">p.side</code></td>
<td>
<p>One-sided (<code>1</code>) or two-sided (<code>2</code>) <em>p</em> value.
Defaults to <code>2</code>.
</p>
<p>In Caliskan et al.'s (2017) article, they reported one-sided <em>p</em> value for WEAT.
Here, I suggest reporting two-sided <em>p</em> value as a more conservative estimate.
The users take the full responsibility for the choice.
</p>

<ul>
<li><p>The one-sided <em>p</em> value is calculated as the proportion of sampled permutations
where the difference in means is greater than the test statistic.
</p>
</li>
<li><p>The two-sided <em>p</em> value is calculated as the proportion of sampled permutations
where the absolute difference is greater than the test statistic.
</p>
</li></ul>
</td></tr>
<tr><td><code id="test_WEAT_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducible results of permutation test. Defaults to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="test_WEAT_+3A_pooled.sd">pooled.sd</code></td>
<td>
<p>Method used to calculate the pooled <em>SD</em> for effect size estimate in WEAT.
</p>

<ul>
<li><p>Defaults to <code>"Caliskan"</code>: <code>sd(data.diff$cos_sim_diff)</code>, which is highly suggested
and identical to Caliskan et al.'s (2017) original approach.
</p>
</li>
<li><p>Otherwise specified, it will calculate the pooled <em>SD</em> as:
<code class="reqn">\sqrt{[(n_1 - 1) * \sigma_1^2 + (n_2 - 1) * \sigma_2^2] / (n_1 + n_2 - 2)}</code>.
</p>
<p>This is <strong>NOT suggested</strong> because it may <em>overestimate</em> the effect size,
especially when there are only a few T1 and T2 words that have small variances.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> object of new class <code>weat</code>:
</p>

<dl>
<dt><code>words.valid</code></dt><dd>
<p>Valid (actually matched) words</p>
</dd>
<dt><code>words.not.found</code></dt><dd>
<p>Words not found</p>
</dd>
<dt><code>data.raw</code></dt><dd>
<p>A <code>data.table</code> of cosine similarities between all word pairs</p>
</dd>
<dt><code>data.mean</code></dt><dd>
<p>A <code>data.table</code> of <em>mean</em> cosine similarities
<em>across</em> all attribute words</p>
</dd>
<dt><code>data.diff</code></dt><dd>
<p>A <code>data.table</code> of <em>differential</em> mean cosine similarities
<em>between</em> the two attribute concepts</p>
</dd>
<dt><code>eff.label</code></dt><dd>
<p>Description for the difference between the two attribute concepts</p>
</dd>
<dt><code>eff.type</code></dt><dd>
<p>Effect type: WEAT or SC-WEAT</p>
</dd>
<dt><code>eff</code></dt><dd>
<p>Raw effect, standardized effect size, and p value (if <code>p.perm=TRUE</code>)</p>
</dd>
</dl>



<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>References</h3>

<p>Caliskan, A., Bryson, J. J., &amp; Narayanan, A. (2017).
Semantics derived automatically from language corpora contain human-like biases.
<em>Science, 356</em>(6334), 183&ndash;186.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tab_similarity">tab_similarity</a></code>
</p>
<p><code><a href="#topic+dict_expand">dict_expand</a></code>
</p>
<p><code><a href="#topic+dict_reliability">dict_reliability</a></code>
</p>
<p><code><a href="#topic+test_RND">test_RND</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## cc() is more convenient than c()!

weat = test_WEAT(
  demodata,
  labels=list(T1="King", T2="Queen", A1="Male", A2="Female"),
  T1=cc("king, King"),
  T2=cc("queen, Queen"),
  A1=cc("male, man, boy, brother, he, him, his, son"),
  A2=cc("female, woman, girl, sister, she, her, hers, daughter"),
  seed=1)
weat

sc_weat = test_WEAT(
  demodata,
  labels=list(T1="Occupation", A1="Male", A2="Female"),
  T1=cc("
    architect, boss, leader, engineer, CEO, officer, manager,
    lawyer, scientist, doctor, psychologist, investigator,
    consultant, programmer, teacher, clerk, counselor,
    salesperson, therapist, psychotherapist, nurse"),
  A1=cc("male, man, boy, brother, he, him, his, son"),
  A2=cc("female, woman, girl, sister, she, her, hers, daughter"),
  seed=1)
sc_weat

## Not run: 

## the same as the first example, but using regular expression
weat = test_WEAT(
  demodata,
  labels=list(T1="King", T2="Queen", A1="Male", A2="Female"),
  use.pattern=TRUE,  # use regular expression below
  T1="^[kK]ing$",
  T2="^[qQ]ueen$",
  A1="^male$|^man$|^boy$|^brother$|^he$|^him$|^his$|^son$",
  A2="^female$|^woman$|^girl$|^sister$|^she$|^her$|^hers$|^daughter$",
  seed=1)
weat

## replicating Caliskan et al.'s (2017) results
## WEAT7 (Table 1): d = 1.06, p = .018
## (requiring installation of the `sweater` package)
Caliskan.WEAT7 = test_WEAT(
  as_wordvec(sweater::glove_math),
  labels=list(T1="Math", T2="Arts", A1="Male", A2="Female"),
  T1=cc("math, algebra, geometry, calculus, equations, computation, numbers, addition"),
  T2=cc("poetry, art, dance, literature, novel, symphony, drama, sculpture"),
  A1=cc("male, man, boy, brother, he, him, his, son"),
  A2=cc("female, woman, girl, sister, she, her, hers, daughter"),
  p.side=1, seed=1234)
Caliskan.WEAT7
# d = 1.055, p = .0173 (= 173 counts / 10000 permutation samples)

## replicating Caliskan et al.'s (2017) supplemental results
## WEAT7 (Table S1): d = 0.97, p = .027
Caliskan.WEAT7.supp = test_WEAT(
  demodata,
  labels=list(T1="Math", T2="Arts", A1="Male", A2="Female"),
  T1=cc("math, algebra, geometry, calculus, equations, computation, numbers, addition"),
  T2=cc("poetry, art, dance, literature, novel, symphony, drama, sculpture"),
  A1=cc("male, man, boy, brother, he, him, his, son"),
  A2=cc("female, woman, girl, sister, she, her, hers, daughter"),
  p.side=1, seed=1234)
Caliskan.WEAT7.supp
# d = 0.966, p = .0221 (= 221 counts / 10000 permutation samples)

## End(Not run)

</code></pre>

<hr>
<h2 id='text_init'>Install required Python modules
in a new conda environment
and initialize the environment,
necessary for all <code>text_*</code> functions
designed for contextualized word embeddings.</h2><span id='topic+text_init'></span>

<h3>Description</h3>

<p>Install required Python modules
in a new conda environment
and initialize the environment,
necessary for all <code>text_*</code> functions
designed for contextualized word embeddings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_init()
</code></pre>


<h3>Details</h3>

<p>Users may first need to manually install
<a href="https://www.anaconda.com/">Anaconda</a> or
<a href="https://docs.conda.io/en/main/miniconda.html">Miniconda</a>.
</p>
<p>The R package <code>text</code> (<a href="https://www.r-text.org/">https://www.r-text.org/</a>) enables users access to
<a href="https://huggingface.co/models">HuggingFace Transformers models</a> in R,
through the R package <code>reticulate</code> as an interface to Python
and the Python modules <code>torch</code> and <code>transformers</code>.
</p>
<p>For advanced usage, see
</p>

<ul>
<li><p><code><a href="text.html#topic+textrpp_install">text::textrpp_install()</a></code>
</p>
</li>
<li><p><code><a href="text.html#topic+textrpp_install">text::textrpp_install_virtualenv()</a></code>
</p>
</li>
<li><p><code><a href="text.html#topic+textrpp_uninstall">text::textrpp_uninstall()</a></code>
</p>
</li>
<li><p><code><a href="text.html#topic+textrpp_initialize">text::textrpp_initialize()</a></code>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+text_model_download">text_model_download</a></code>
</p>
<p><code><a href="#topic+text_model_remove">text_model_remove</a></code>
</p>
<p><code><a href="#topic+text_to_vec">text_to_vec</a></code>
</p>
<p><code><a href="#topic+text_unmask">text_unmask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
text_init()

# You may need to specify the version of Python:
# RStudio -&gt; Tools -&gt; Global/Project Options
# -&gt; Python -&gt; Select -&gt; Conda Environments
# -&gt; Choose ".../textrpp_condaenv/python.exe"

## End(Not run)

</code></pre>

<hr>
<h2 id='text_model_download'>Download pre-trained language models from HuggingFace.</h2><span id='topic+text_model_download'></span>

<h3>Description</h3>

<p>Download pre-trained language models (Transformers Models,
such as GPT, BERT, RoBERTa, DeBERTa, DistilBERT, etc.)
from <a href="https://huggingface.co/models">HuggingFace</a> to
your local &quot;.cache&quot; folder (&quot;C:/Users/[YourUserName]/.cache/&quot;).
The models will never be removed unless you run
<code><a href="#topic+text_model_remove">text_model_remove</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_model_download(model = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="text_model_download_+3A_model">model</code></td>
<td>
<p>Character string(s) specifying the
pre-trained language model(s) to be downloaded.
For a full list of options, see
<a href="https://huggingface.co/models">HuggingFace</a>.
Defaults to download nothing and check currently downloaded models.
</p>
<p>Example choices:
</p>

<ul>
<li><p><code>"gpt2"</code> (50257 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"openai-gpt"</code> (40478 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"bert-base-uncased"</code> (30522 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"bert-large-uncased"</code> (30522 vocab, 1024 dims, 24 layers)
</p>
</li>
<li><p><code>"bert-base-cased"</code> (28996 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"bert-large-cased"</code> (28996 vocab, 1024 dims, 24 layers)
</p>
</li>
<li><p><code>"bert-base-chinese"</code> (21128 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"bert-base-multilingual-cased"</code> (119547 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"distilbert-base-uncased"</code> (30522 vocab, 768 dims, 6 layers)
</p>
</li>
<li><p><code>"distilbert-base-cased"</code> (28996 vocab, 768 dims, 6 layers)
</p>
</li>
<li><p><code>"distilbert-base-multilingual-cased"</code> (119547 vocab, 768 dims, 6 layers)
</p>
</li>
<li><p><code>"albert-base-v2"</code> (30000 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"albert-large-v2"</code> (30000 vocab, 1024 dims, 24 layers)
</p>
</li>
<li><p><code>"roberta-base"</code> (50265 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"roberta-large"</code> (50265 vocab, 1024 dims, 24 layers)
</p>
</li>
<li><p><code>"xlm-roberta-base"</code> (250002 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"xlm-roberta-large"</code> (250002 vocab, 1024 dims, 24 layers)
</p>
</li>
<li><p><code>"xlnet-base-cased"</code> (32000 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"xlnet-large-cased"</code> (32000 vocab, 1024 dims, 24 layers)
</p>
</li>
<li><p><code>"microsoft/deberta-v3-base"</code> (128100 vocab, 768 dims, 12 layers)
</p>
</li>
<li><p><code>"microsoft/deberta-v3-large"</code> (128100 vocab, 1024 dims, 24 layers)
</p>
</li>
<li><p><code>...</code> (see <a href="https://huggingface.co/models">https://huggingface.co/models</a>)
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly return the names of all downloaded models.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+text_init">text_init</a></code>
</p>
<p><code><a href="#topic+text_model_remove">text_model_remove</a></code>
</p>
<p><code><a href="#topic+text_to_vec">text_to_vec</a></code>
</p>
<p><code><a href="#topic+text_unmask">text_unmask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# text_init()  # initialize the environment

text_model_download()  # check downloaded models
text_model_download(c(
  "bert-base-uncased",
  "bert-base-cased",
  "bert-base-multilingual-cased"
))

## End(Not run)

</code></pre>

<hr>
<h2 id='text_model_remove'>Remove downloaded models from the local .cache folder.</h2><span id='topic+text_model_remove'></span>

<h3>Description</h3>

<p>Remove downloaded models from the local .cache folder.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_model_remove(model = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="text_model_remove_+3A_model">model</code></td>
<td>
<p>Model name. See <code><a href="#topic+text_model_download">text_model_download</a></code>.
Defaults to automatically find all downloaded models in the .cache folder.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+text_init">text_init</a></code>
</p>
<p><code><a href="#topic+text_model_download">text_model_download</a></code>
</p>
<p><code><a href="#topic+text_to_vec">text_to_vec</a></code>
</p>
<p><code><a href="#topic+text_unmask">text_unmask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# text_init()  # initialize the environment

text_model_remove()

## End(Not run)

</code></pre>

<hr>
<h2 id='text_to_vec'>Extract contextualized word embeddings from transformers (pre-trained language models).</h2><span id='topic+text_to_vec'></span>

<h3>Description</h3>

<p>Extract hidden layers from a language model and aggregate them to
get token (roughly word) embeddings and text embeddings
(all reshaped to <code><a href="#topic+as_embed">embed</a></code> matrix).
It is a wrapper function of <code><a href="text.html#topic+textEmbed">text::textEmbed()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_to_vec(
  text,
  model,
  layers = "all",
  layer.to.token = "concatenate",
  token.to.word = TRUE,
  token.to.text = TRUE,
  encoding = "UTF-8",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="text_to_vec_+3A_text">text</code></td>
<td>
<p>Can be:
</p>

<ul>
<li><p>a character string or vector of text (usually sentences)
</p>
</li>
<li><p>a data frame with at least one character variable
(for text from all character variables in a given data frame)
</p>
</li>
<li><p>a file path on disk containing text
</p>
</li></ul>
</td></tr>
<tr><td><code id="text_to_vec_+3A_model">model</code></td>
<td>
<p>Model name at <a href="https://huggingface.co/models">HuggingFace</a>.
See <code><a href="#topic+text_model_download">text_model_download</a></code>.
If the model has not been downloaded, it would automatically download the model.</p>
</td></tr>
<tr><td><code id="text_to_vec_+3A_layers">layers</code></td>
<td>
<p>Layers to be extracted from the <code>model</code>,
which are then aggregated in the function
<code><a href="text.html#topic+textEmbedLayerAggregation">text::textEmbedLayerAggregation()</a></code>.
Defaults to <code>"all"</code> which extracts all layers.
You may extract only the layers you need (e.g., <code>11:12</code>).
Note that layer 0 is the <em>decontextualized</em> input layer
(i.e., not comprising hidden states).</p>
</td></tr>
<tr><td><code id="text_to_vec_+3A_layer.to.token">layer.to.token</code></td>
<td>
<p>Method to aggregate hidden layers to each token.
Defaults to <code>"concatenate"</code>,
which links together each word embedding layer to one long row.
Options include <code>"mean"</code>, <code>"min"</code>, <code>"max"</code>, and <code>"concatenate"</code>.</p>
</td></tr>
<tr><td><code id="text_to_vec_+3A_token.to.word">token.to.word</code></td>
<td>
<p>Aggregate subword token embeddings (if whole word is out of vocabulary)
to whole word embeddings. Defaults to <code>TRUE</code>, which sums up subword token embeddings.</p>
</td></tr>
<tr><td><code id="text_to_vec_+3A_token.to.text">token.to.text</code></td>
<td>
<p>Aggregate token embeddings to each text.
Defaults to <code>TRUE</code>, which averages all token embeddings.
If <code>FALSE</code>, the text embedding will be the token embedding of <code>[CLS]</code>
(the special token that is used to represent the beginning of a text sequence).</p>
</td></tr>
<tr><td><code id="text_to_vec_+3A_encoding">encoding</code></td>
<td>
<p>Text encoding (only used if <code>text</code> is a file).
Defaults to <code>"UTF-8"</code>.</p>
</td></tr>
<tr><td><code id="text_to_vec_+3A_...">...</code></td>
<td>
<p>Other parameters passed to
<code><a href="text.html#topic+textEmbed">text::textEmbed()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> of:
</p>

<dl>
<dt><code>token.embed</code></dt><dd>
<p>Token (roughly word) embeddings</p>
</dd>
<dt><code>text.embed</code></dt><dd>
<p>Text embeddings, aggregated from token embeddings</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+text_init">text_init</a></code>
</p>
<p><code><a href="#topic+text_model_download">text_model_download</a></code>
</p>
<p><code><a href="#topic+text_model_remove">text_model_remove</a></code>
</p>
<p><code><a href="#topic+text_unmask">text_unmask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# text_init()  # initialize the environment

text = c("Download models from HuggingFace",
         "Chinese are East Asian",
         "Beijing is the capital of China")
embed = text_to_vec(text, model="bert-base-cased", layers=c(0, 12))
embed

embed1 = embed$token.embed[[1]]
embed2 = embed$token.embed[[2]]
embed3 = embed$token.embed[[3]]

View(embed1)
View(embed2)
View(embed3)
View(embed$text.embed)

plot_similarity(embed1, value.color="grey")
plot_similarity(embed2, value.color="grey")
plot_similarity(embed3, value.color="grey")
plot_similarity(rbind(embed1, embed2, embed3))

## End(Not run)

</code></pre>

<hr>
<h2 id='text_unmask'>&lt;Deprecated&gt; Fill in the blank mask(s) in a query (sentence).</h2><span id='topic+text_unmask'></span>

<h3>Description</h3>

<p><em>Note</em>: This function has been deprecated and will not be updated
since I have developed new package
<a href="https://psychbruce.github.io/FMAT/">FMAT</a>
as the integrative toolbox of <em>Fill-Mask Association Test</em> (FMAT).
</p>
<p>Predict the probably correct masked token(s) in a sequence,
based on the Python module <code>transformers</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_unmask(query, model, targets = NULL, topn = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="text_unmask_+3A_query">query</code></td>
<td>
<p>A query (sentence/prompt) with masked token(s) <code>[MASK]</code>.
Multiple queries are also supported. See examples.</p>
</td></tr>
<tr><td><code id="text_unmask_+3A_model">model</code></td>
<td>
<p>Model name at <a href="https://huggingface.co/models">HuggingFace</a>.
See <code><a href="#topic+text_model_download">text_model_download</a></code>.
If the model has not been downloaded, it would automatically download the model.</p>
</td></tr>
<tr><td><code id="text_unmask_+3A_targets">targets</code></td>
<td>
<p>Specific target word(s) to be filled in the blank <code>[MASK]</code>.
Defaults to <code>NULL</code> (i.e., return <code>topn</code>).
If specified, then <code>topn</code> will be ignored (see examples).</p>
</td></tr>
<tr><td><code id="text_unmask_+3A_topn">topn</code></td>
<td>
<p>Number of the most likely predictions to return.
Defaults to <code>5</code>. If <code>targets</code> is specified,
then it will automatically change to the length of <code>targets</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Masked language modeling is the task of masking some of the words in a sentence
and predicting which words should replace those masks.
These models are useful when we want to get a statistical understanding of
the language in which the model is trained in.
See <a href="https://huggingface.co/tasks/fill-mask">https://huggingface.co/tasks/fill-mask</a> for details.
</p>


<h3>Value</h3>

<p>A <code>data.table</code> of query results:
</p>

<dl>
<dt><code>query_id</code> (if there are more than one <code>query</code>)</dt><dd>
<p><code>query</code> ID (indicating multiple queries)</p>
</dd>
<dt><code>mask_id</code> (if there are more than one <code>[MASK]</code> in <code>query</code>)</dt><dd>
<p><code>[MASK]</code> ID (position in sequence, indicating multiple masks)</p>
</dd>
<dt><code>prob</code></dt><dd>
<p>Probability of the predicted token in the sequence</p>
</dd>
<dt><code>token_id</code></dt><dd>
<p>Predicted token ID (to replace <code>[MASK]</code>)</p>
</dd>
<dt><code>token</code></dt><dd>
<p>Predicted token (to replace <code>[MASK]</code>)</p>
</dd>
<dt><code>sequence</code></dt><dd>
<p>Complete sentence with the predicted token</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+text_init">text_init</a></code>
</p>
<p><code><a href="#topic+text_model_download">text_model_download</a></code>
</p>
<p><code><a href="#topic+text_model_remove">text_model_remove</a></code>
</p>
<p><code><a href="#topic+text_to_vec">text_to_vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# text_init()  # initialize the environment

model = "distilbert-base-cased"

text_unmask("Beijing is the [MASK] of China.", model)

# multiple [MASK]s:
text_unmask("Beijing is the [MASK] [MASK] of China.", model)

# multiple queries:
text_unmask(c("The man worked as a [MASK].",
              "The woman worked as a [MASK]."),
            model)

# specific targets:
text_unmask("The [MASK] worked as a nurse.", model,
            targets=c("man", "woman"))

## End(Not run)

</code></pre>

<hr>
<h2 id='tokenize'>Tokenize raw text for training word embeddings.</h2><span id='topic+tokenize'></span>

<h3>Description</h3>

<p>Tokenize raw text for training word embeddings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize(
  text,
  tokenizer = text2vec::word_tokenizer,
  split = " ",
  remove = "_|'|&lt;br/&gt;|&lt;br /&gt;|e\\.g\\.|i\\.e\\.",
  encoding = "UTF-8",
  simplify = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_+3A_text">text</code></td>
<td>
<p>A character vector of text,
or a file path on disk containing text.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_tokenizer">tokenizer</code></td>
<td>
<p>Function used to tokenize the text.
Defaults to <code><a href="text2vec.html#topic+tokenizers">text2vec::word_tokenizer</a></code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_split">split</code></td>
<td>
<p>Separator between tokens, only used when <code>simplify=TRUE</code>.
Defaults to <code>" "</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_remove">remove</code></td>
<td>
<p>Strings (in regular expression) to be removed from the text.
Defaults to <code>"_|'|&lt;br/&gt;|&lt;br /&gt;|e\\.g\\.|i\\.e\\."</code>.
You may turn off this by specifying <code>remove=NULL</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_encoding">encoding</code></td>
<td>
<p>Text encoding (only used if <code>text</code> is a file).
Defaults to <code>"UTF-8"</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_simplify">simplify</code></td>
<td>
<p>Return a character vector (<code>TRUE</code>) or a list of character vectors (<code>FALSE</code>).
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_verbose">verbose</code></td>
<td>
<p>Print information to the console? Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p><code>simplify=TRUE</code>: A tokenized character vector,
with each element as a sentence.
</p>
</li>
<li><p><code>simplify=FALSE</code>: A list of tokenized character vectors,
with each element as a vector of tokens in a sentence.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+train_wordvec">train_wordvec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt1 = c(
  "I love natural language processing (NLP)!",
  "I've been in this city for 10 years. I really like here!",
  "However, my computer is not among the \"Top 10\" list."
)
tokenize(txt1, simplify=FALSE)
tokenize(txt1) %&gt;% cat(sep="\n----\n")

txt2 = text2vec::movie_review$review[1:5]
texts = tokenize(txt2)

txt2[1]
texts[1:20]  # all sentences in txt2[1]

</code></pre>

<hr>
<h2 id='train_wordvec'>Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm.</h2><span id='topic+train_wordvec'></span>

<h3>Description</h3>

<p>Train static word embeddings using the
<code><a href="word2vec.html#topic+word2vec">Word2Vec</a></code>,
<code><a href="rsparse.html#topic+GloVe">GloVe</a></code>, or
<code><a href="fastTextR.html#topic+ft_train">FastText</a></code> algorithm
with multi-threading.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_wordvec(
  text,
  method = c("word2vec", "glove", "fasttext"),
  dims = 300,
  window = 5,
  min.freq = 5,
  threads = 8,
  model = c("skip-gram", "cbow"),
  loss = c("ns", "hs"),
  negative = 5,
  subsample = 1e-04,
  learning = 0.05,
  ngrams = c(3, 6),
  x.max = 10,
  convergence = -1,
  stopwords = character(0),
  encoding = "UTF-8",
  tolower = FALSE,
  normalize = FALSE,
  iteration,
  tokenizer,
  remove,
  file.save,
  compress = "bzip2",
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_wordvec_+3A_text">text</code></td>
<td>
<p>A character vector of text,
or a file path on disk containing text.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_method">method</code></td>
<td>
<p>Training algorithm:
</p>

<ul>
<li><p><code>"word2vec"</code> (default):
using the <code><a href="word2vec.html#topic+word2vec">word2vec</a></code> package
</p>
</li>
<li><p><code>"glove"</code>:
using the <code><a href="rsparse.html#topic+GloVe">rsparse</a></code> and
<code><a href="text2vec.html#topic+text2vec">text2vec</a></code> packages
</p>
</li>
<li><p><code>"fasttext"</code>:
using the <code><a href="fastTextR.html#topic+ft_train">fastTextR</a></code> package
</p>
</li></ul>
</td></tr>
<tr><td><code id="train_wordvec_+3A_dims">dims</code></td>
<td>
<p>Number of dimensions of word vectors to be trained.
Common choices include 50, 100, 200, 300, and 500.
Defaults to <code>300</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_window">window</code></td>
<td>
<p>Window size (number of nearby words behind/ahead the current word).
It defines how many surrounding words to be included in training:
[window] words behind and [window] words ahead ([window]*2 in total).
Defaults to <code>5</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_min.freq">min.freq</code></td>
<td>
<p>Minimum frequency of words to be included in training.
Words that appear less than this value of times will be excluded from vocabulary.
Defaults to <code>5</code> (take words that appear at least five times).</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_threads">threads</code></td>
<td>
<p>Number of CPU threads used for training.
A modest value produces the fastest training.
Too many threads are not always helpful.
Defaults to <code>8</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_model">model</code></td>
<td>
<p><strong>&lt;Only for Word2Vec / FastText&gt;</strong>
</p>
<p>Learning model architecture:
</p>

<ul>
<li><p><code>"skip-gram"</code> (default): Skip-Gram,
which predicts surrounding words given the current word
</p>
</li>
<li><p><code>"cbow"</code>: Continuous Bag-of-Words,
which predicts the current word based on the context
</p>
</li></ul>
</td></tr>
<tr><td><code id="train_wordvec_+3A_loss">loss</code></td>
<td>
<p><strong>&lt;Only for Word2Vec / FastText&gt;</strong>
</p>
<p>Loss function (computationally efficient approximation):
</p>

<ul>
<li><p><code>"ns"</code> (default): Negative Sampling
</p>
</li>
<li><p><code>"hs"</code>: Hierarchical Softmax
</p>
</li></ul>
</td></tr>
<tr><td><code id="train_wordvec_+3A_negative">negative</code></td>
<td>
<p><strong>&lt;Only for Negative Sampling in Word2Vec / FastText&gt;</strong>
</p>
<p>Number of negative examples.
Values in the range 5~20 are useful for small training datasets,
while for large datasets the value can be as small as 2~5.
Defaults to <code>5</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_subsample">subsample</code></td>
<td>
<p><strong>&lt;Only for Word2Vec / FastText&gt;</strong>
</p>
<p>Subsampling of frequent words (threshold for occurrence of words).
Those that appear with higher frequency in the training data will be randomly down-sampled.
Defaults to <code>0.0001</code> (<code>1e-04</code>).</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_learning">learning</code></td>
<td>
<p><strong>&lt;Only for Word2Vec / FastText&gt;</strong>
</p>
<p>Initial (starting) learning rate, also known as alpha.
Defaults to <code>0.05</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_ngrams">ngrams</code></td>
<td>
<p><strong>&lt;Only for FastText&gt;</strong>
</p>
<p>Minimal and maximal ngram length.
Defaults to <code>c(3, 6)</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_x.max">x.max</code></td>
<td>
<p><strong>&lt;Only for GloVe&gt;</strong>
</p>
<p>Maximum number of co-occurrences to use in the weighting function.
Defaults to <code>10</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_convergence">convergence</code></td>
<td>
<p><strong>&lt;Only for GloVe&gt;</strong>
</p>
<p>Convergence tolerance for SGD iterations. Defaults to <code>-1</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_stopwords">stopwords</code></td>
<td>
<p><strong>&lt;Only for Word2Vec / GloVe&gt;</strong>
</p>
<p>A character vector of stopwords to be excluded from training.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_encoding">encoding</code></td>
<td>
<p>Text encoding. Defaults to <code>"UTF-8"</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_tolower">tolower</code></td>
<td>
<p>Convert all upper-case characters to lower-case?
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_normalize">normalize</code></td>
<td>
<p>Normalize all word vectors to unit length?
Defaults to <code>FALSE</code>. See <code><a href="#topic+normalize">normalize</a></code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_iteration">iteration</code></td>
<td>
<p>Number of training iterations.
More iterations makes a more precise model,
but computational cost is linearly proportional to iterations.
Defaults to <code>5</code> for Word2Vec and FastText
while <code>10</code> for GloVe.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_tokenizer">tokenizer</code></td>
<td>
<p>Function used to tokenize the text.
Defaults to <code><a href="text2vec.html#topic+tokenizers">text2vec::word_tokenizer</a></code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_remove">remove</code></td>
<td>
<p>Strings (in regular expression) to be removed from the text.
Defaults to <code>"_|'|&lt;br/&gt;|&lt;br /&gt;|e\\.g\\.|i\\.e\\."</code>.
You may turn off this by specifying <code>remove=NULL</code>.</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_file.save">file.save</code></td>
<td>
<p>File name of to-be-saved R data (must be .RData).</p>
</td></tr>
<tr><td><code id="train_wordvec_+3A_compress">compress</code></td>
<td>
<p>Compression method for the saved file. Defaults to <code>"bzip2"</code>.
</p>
<p>Options include:
</p>

<ul>
<li> <p><code>1</code> or <code>"gzip"</code>: modest file size (fastest)
</p>
</li>
<li> <p><code>2</code> or <code>"bzip2"</code>: small file size (fast)
</p>
</li>
<li> <p><code>3</code> or <code>"xz"</code>: minimized file size (slow)
</p>
</li></ul>
</td></tr>
<tr><td><code id="train_wordvec_+3A_verbose">verbose</code></td>
<td>
<p>Print information to the console? Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>wordvec</code> (data.table) with three variables:
<code>word</code>, <code>vec</code>, <code>freq</code>.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>References</h3>

<p>All-in-one package:
</p>

<ul>
<li><p><a href="https://CRAN.R-project.org/package=wordsalad">https://CRAN.R-project.org/package=wordsalad</a>
</p>
</li></ul>

<p>Word2Vec:
</p>

<ul>
<li><p><a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a>
</p>
</li>
<li><p><a href="https://CRAN.R-project.org/package=word2vec">https://CRAN.R-project.org/package=word2vec</a>
</p>
</li>
<li><p><a href="https://github.com/maxoodf/word2vec">https://github.com/maxoodf/word2vec</a>
</p>
</li></ul>

<p>GloVe:
</p>

<ul>
<li><p><a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>
</p>
</li>
<li><p><a href="https://text2vec.org/glove.html">https://text2vec.org/glove.html</a>
</p>
</li>
<li><p><a href="https://CRAN.R-project.org/package=text2vec">https://CRAN.R-project.org/package=text2vec</a>
</p>
</li>
<li><p><a href="https://CRAN.R-project.org/package=rsparse">https://CRAN.R-project.org/package=rsparse</a>
</p>
</li></ul>

<p>FastText:
</p>

<ul>
<li><p><a href="https://fasttext.cc/">https://fasttext.cc/</a>
</p>
</li>
<li><p><a href="https://CRAN.R-project.org/package=fastTextR">https://CRAN.R-project.org/package=fastTextR</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+tokenize">tokenize</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>review = text2vec::movie_review  # a data.frame'
text = review$review

## Note: All the examples train 50 dims for faster code check.

## Word2Vec (SGNS)
dt1 = train_wordvec(
  text,
  method="word2vec",
  model="skip-gram",
  dims=50, window=5,
  normalize=TRUE)

dt1
most_similar(dt1, "Ive")  # evaluate performance
most_similar(dt1, ~ man - he + she, topn=5)  # evaluate performance
most_similar(dt1, ~ boy - he + she, topn=5)  # evaluate performance

## GloVe
dt2 = train_wordvec(
  text,
  method="glove",
  dims=50, window=5,
  normalize=TRUE)

dt2
most_similar(dt2, "Ive")  # evaluate performance
most_similar(dt2, ~ man - he + she, topn=5)  # evaluate performance
most_similar(dt2, ~ boy - he + she, topn=5)  # evaluate performance

## FastText
dt3 = train_wordvec(
  text,
  method="fasttext",
  model="skip-gram",
  dims=50, window=5,
  normalize=TRUE)

dt3
most_similar(dt3, "Ive")  # evaluate performance
most_similar(dt3, ~ man - he + she, topn=5)  # evaluate performance
most_similar(dt3, ~ boy - he + she, topn=5)  # evaluate performance

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
