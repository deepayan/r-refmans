<!DOCTYPE html><html><head><title>Help for package greybox</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {greybox}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#accuracy.greybox'><p>Error measures for an estimated model</p></a></li>
<li><a href='#actuals'><p>Function extracts the actual values from the function</p></a></li>
<li><a href='#AICc'><p>Corrected Akaike's Information Criterion and Bayesian Information Criterion</p></a></li>
<li><a href='#alm'><p>Augmented Linear Model</p></a></li>
<li><a href='#association'><p>Measures of association</p></a></li>
<li><a href='#coef.greybox'><p>Coefficients of the model and their statistics</p></a></li>
<li><a href='#coefbootstrap'><p>Bootstrap for parameters of models</p></a></li>
<li><a href='#cramer'><p>Calculate Cramer's V for categorical variables</p></a></li>
<li><a href='#dalaplace'><p>Asymmetric Laplace Distribution</p></a></li>
<li><a href='#dbcnorm'><p>Box-Cox Normal Distribution</p></a></li>
<li><a href='#detectdst'><p>DST and Leap year detector functions</p></a></li>
<li><a href='#determination'><p>Coefficients of determination</p></a></li>
<li><a href='#dfnorm'><p>Folded Normal Distribution</p></a></li>
<li><a href='#dgnorm'><p>The generalized normal distribution</p></a></li>
<li><a href='#Distributions'><p>Distribution functions of the greybox package</p></a></li>
<li><a href='#dlaplace'><p>Laplace Distribution</p></a></li>
<li><a href='#dlogitnorm'><p>Logit Normal Distribution</p></a></li>
<li><a href='#drectnorm'><p>Rectified Normal Distribution</p></a></li>
<li><a href='#ds'><p>S Distribution</p></a></li>
<li><a href='#dtplnorm'><p>Three Parameter Log Normal Distribution</p></a></li>
<li><a href='#errorType'><p>Functions that extracts type of error from the model</p></a></li>
<li><a href='#extractScale'><p>Functions to extract scale and standard error from a model</p></a></li>
<li><a href='#graphmaker'><p>Linear graph construction function</p></a></li>
<li><a href='#greybox'><p>Grey box</p></a></li>
<li><a href='#hm'><p>Half moment of a distribution and its derivatives.</p></a></li>
<li><a href='#implant'><p>Implant the scale model in the location model</p></a></li>
<li><a href='#is.greybox'><p>Greybox classes checkers</p></a></li>
<li><a href='#lmCombine'><p>Combine regressions based on information criteria</p></a></li>
<li><a href='#lmDynamic'><p>Combine regressions based on point information criteria</p></a></li>
<li><a href='#mcor'><p>Multiple correlation</p></a></li>
<li><a href='#ME'><p>Error measures</p></a></li>
<li><a href='#measures'><p>Error measures for the provided forecasts</p></a></li>
<li><a href='#nparam'><p>Number of parameters and number of variates in the model</p></a></li>
<li><a href='#outlierdummy'><p>Outlier detection and matrix creation</p></a></li>
<li><a href='#pAIC'><p>Point AIC</p></a></li>
<li><a href='#pcor'><p>Partial correlations</p></a></li>
<li><a href='#pinball'><p>Pinball function</p></a></li>
<li><a href='#plot.greybox'><p>Plots of the fit and residuals</p></a></li>
<li><a href='#pointLik'><p>Point likelihood values</p></a></li>
<li><a href='#polyprod'><p>This function calculates parameters for the polynomials</p></a></li>
<li><a href='#predict.alm'><p>Forecasting using greybox functions</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#rmcb'><p>Regression for Multiple Comparison with the Best</p></a></li>
<li><a href='#ro'><p>Rolling Origin</p></a></li>
<li><a href='#sm'><p>Scale Model</p></a></li>
<li><a href='#spread'><p>Construct scatterplot / boxplots for the data</p></a></li>
<li><a href='#stepwise'><p>Stepwise selection of regressors</p></a></li>
<li><a href='#tableplot'><p>Construct a plot for categorical variable</p></a></li>
<li><a href='#temporaldummy'><p>Dummy variables for provided seasonality type</p></a></li>
<li><a href='#xregExpander'><p>Exogenous variables expander</p></a></li>
<li><a href='#xregMultiplier'><p>Exogenous variables cross-products</p></a></li>
<li><a href='#xregTransformer'><p>Exogenous variables transformer</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Toolbox for Model Building and Forecasting</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-09-15</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/config-i1/greybox">https://github.com/config-i1/greybox</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/config-i1/greybox/issues">https://github.com/config-i1/greybox/issues</a></td>
</tr>
<tr>
<td>Language:</td>
<td>en-GB</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements functions and instruments for regression model building and its
             application to forecasting. The main scope of the package is in variables selection
             and models specification for cases of time series data. This includes promotional
             modelling, selection between different dynamic regressions with non-standard
             distributions of errors, selection based on cross validation, solutions to the fat
             regression model problem and more. Models developed in the package are tailored
             specifically for forecasting purposes. So as a results there are several methods
             that allow producing forecasts from these models and visualising them.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, generics (&ge; 0.1.2), graphics, utils, pracma, nloptr,
statmod, zoo, texreg, xtable, methods</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>smooth (&ge; 3.1.0), doMC, doParallel, foreach, testthat,
rmarkdown, knitr</td>
</tr>
<tr>
<td>Enhances:</td>
<td>vars,</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-15 16:22:47 UTC; config</td>
</tr>
<tr>
<td>Author:</td>
<td>Ivan Svetunkov [aut, cre] (Lecturer at Centre for Marketing Analytics
    and Forecasting, Lancaster University, UK),
  Yves R. Sagaert [ctb] (Visiting Research at Centre for Marketing
    Analytics and Forecasting, Lancaster University, UK)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ivan Svetunkov &lt;ivan@svetunkov.ru&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-16 05:50:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='accuracy.greybox'>Error measures for an estimated model</h2><span id='topic+accuracy.greybox'></span><span id='topic+accuracy.predict.greybox'></span>

<h3>Description</h3>

<p>Function produces error measures for the provided object and the holdout values of the
response variable. Note that instead of parameters <code>x</code>, <code>test</code>, the function
accepts the vector of values in <code>holdout</code>. Also, the parameters <code>d</code> and <code>D</code>
are not supported - MASE is always calculated via division by first differences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'greybox'
accuracy(object, holdout = NULL, ...)

## S3 method for class 'predict.greybox'
accuracy(object, holdout = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="accuracy.greybox_+3A_object">object</code></td>
<td>
<p>The estimated model or a forecast from the estimated model generated via
either <code>predict()</code> or <code>forecast()</code> functions.</p>
</td></tr>
<tr><td><code id="accuracy.greybox_+3A_holdout">holdout</code></td>
<td>
<p>The vector of values of the response variable in the holdout (test) set.
If not provided, then the function will return the in-sample error measures.</p>
</td></tr>
<tr><td><code id="accuracy.greybox_+3A_...">...</code></td>
<td>
<p>Other variables passed to the <code>forecast()</code> function (e.g. <code>newdata</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is a wrapper for the <a href="#topic+measures">measures</a> function and is implemented
for convenience.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rlaplace(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rlaplace(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")

ourModel &lt;- alm(y~x1+x2+trend, xreg, subset=c(1:80), distribution="dlaplace")
predict(ourModel,xreg[-c(1:80),]) |&gt;
   accuracy(xreg[-c(1:80),"y"])
</code></pre>

<hr>
<h2 id='actuals'>Function extracts the actual values from the function</h2><span id='topic+actuals'></span><span id='topic+actuals.default'></span><span id='topic+actuals.lm'></span><span id='topic+actuals.alm'></span><span id='topic+actuals.predict.greybox'></span>

<h3>Description</h3>

<p>This is a simple method that returns the values of the response variable of the model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>actuals(object, all = TRUE, ...)

## Default S3 method:
actuals(object, all = TRUE, ...)

## S3 method for class 'lm'
actuals(object, all = TRUE, ...)

## S3 method for class 'alm'
actuals(object, all = TRUE, ...)

## S3 method for class 'predict.greybox'
actuals(object, all = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="actuals_+3A_object">object</code></td>
<td>
<p>Model estimated using one of the functions of smooth package.</p>
</td></tr>
<tr><td><code id="actuals_+3A_all">all</code></td>
<td>
<p>If <code>FALSE</code>, then in the case of the occurrence model, only demand
sizes will be returned.</p>
</td></tr>
<tr><td><code id="actuals_+3A_...">...</code></td>
<td>
<p>Other parameters to pass to the method. Currently nothing is supported here.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vector of the response variable.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")

ourModel &lt;- stepwise(xreg)

actuals(ourModel)

</code></pre>

<hr>
<h2 id='AICc'>Corrected Akaike's Information Criterion and Bayesian Information Criterion</h2><span id='topic+AICc'></span><span id='topic+BICc'></span>

<h3>Description</h3>

<p>This function extracts AICc / BICc from models. It can be applied to wide
variety of models that use logLik() and nobs() methods (including the
popular lm, forecast, smooth classes).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AICc(object, ...)

BICc(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AICc_+3A_object">object</code></td>
<td>
<p>Time series model.</p>
</td></tr>
<tr><td><code id="AICc_+3A_...">...</code></td>
<td>
<p>Some stuff.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>AICc was proposed by Nariaki Sugiura in 1978 and is used on small samples
for the models with normally distributed residuals. BICc was derived in
McQuarrie (1999) and is used in similar circumstances.
</p>
<p>IMPORTANT NOTE: both of the criteria can only be used for univariate models
(regression models, ARIMA, ETS etc) with normally distributed residuals!
In case of multivariate models, both criteria need to be modified. See
Bedrick &amp; Tsai (1994) for details.
</p>


<h3>Value</h3>

<p>This function returns numeric value.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Burnham Kenneth P. and Anderson David R. (2002). Model Selection
and Multimodel Inference. A Practical Information-Theoretic Approach.
Springer-Verlag New York. DOI: [10.1007/b97636](http://dx.doi.org/10.1007/b97636).
</p>
</li>
<li><p> McQuarrie, A. D. (1999). A small-sample correction for the Schwarz SIC model
selection criterion. Statistics &amp; Probability Letters, 44(1), 79–86.
[10.1016/S0167-7152(98)00294-6](https://doi.org/10.1016/S0167-7152(98)00294-6).
</p>
</li></ul>


<ul>
<li><p> McQuarrie A.D., A small-sample correction for the Schwarz SIC
model selection criterion, Statistics &amp; Probability Letters 44 (1999)
pp.79-86. <a href="https://doi.org/10.1016/S0167-7152%2898%2900294-6">doi:10.1016/S0167-7152(98)00294-6</a>
</p>
</li>
<li><p> Sugiura Nariaki (1978) Further analysts of the data by Akaike's
information criterion and the finite corrections, Communications in
Statistics - Theory and Methods, 7:1, 13-26,
<a href="https://doi.org/10.1080/03610927808827599">doi:10.1080/03610927808827599</a>
</p>
</li>
<li><p> Bedrick, E. J., &amp; Tsai, C.-L. (1994). Model Selection for
Multivariate Regression in Small Samples. Biometrics, 50(1), 226.
<a href="https://doi.org/10.2307/2533213">doi:10.2307/2533213</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="stats.html#topic+AIC">AIC</a>, <a href="stats.html#topic+BIC">BIC</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")

ourModel &lt;- stepwise(xreg)

AICc(ourModel)
BICc(ourModel)

</code></pre>

<hr>
<h2 id='alm'>Augmented Linear Model</h2><span id='topic+alm'></span>

<h3>Description</h3>

<p>Function estimates model based on the selected distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alm(formula, data, subset, na.action, distribution = c("dnorm", "dlaplace",
  "ds", "dgnorm", "dlogis", "dt", "dalaplace", "dlnorm", "dllaplace", "dls",
  "dlgnorm", "dbcnorm", "dinvgauss", "dgamma", "dexp", "dfnorm", "drectnorm",
  "dpois", "dnbinom", "dbeta", "dlogitnorm", "plogis", "pnorm"),
  loss = c("likelihood", "MSE", "MAE", "HAM", "LASSO", "RIDGE"),
  occurrence = c("none", "plogis", "pnorm"), scale = NULL, orders = c(0,
  0, 0), parameters = NULL, fast = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alm_+3A_formula">formula</code></td>
<td>
<p>an object of class &quot;formula&quot; (or one that can be coerced to
that class): a symbolic description of the model to be fitted. Can also include
<code>trend</code>, which would add the global trend.</p>
</td></tr>
<tr><td><code id="alm_+3A_data">data</code></td>
<td>
<p>a data frame or a matrix, containing the variables in the model.</p>
</td></tr>
<tr><td><code id="alm_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td></tr>
<tr><td><code id="alm_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the
data contain NAs. The default is set by the na.action setting of
<a href="base.html#topic+options">options</a>, and is <a href="stats.html#topic+na.fail">na.fail</a> if that is unset. The
factory-fresh default is <a href="stats.html#topic+na.omit">na.omit</a>. Another possible value
is NULL, no action. Value <a href="stats.html#topic+na.exclude">na.exclude</a> can be useful.</p>
</td></tr>
<tr><td><code id="alm_+3A_distribution">distribution</code></td>
<td>
<p>what density function to use in the process. The full
name of the distribution should be provided here. Values with &quot;d&quot; in the
beginning of the name refer to the density function, while &quot;p&quot; stands for
&quot;probability&quot; (cumulative distribution function). The names align with the
names of distribution functions in R. For example, see <a href="stats.html#topic+dnorm">dnorm</a>.</p>
</td></tr>
<tr><td><code id="alm_+3A_loss">loss</code></td>
<td>
<p>The type of Loss Function used in optimization. <code>loss</code> can
be:
</p>

<ul>
<li> <p><code>likelihood</code> - the model is estimated via the maximisation of the
likelihood of the function specified in <code>distribution</code>;
</p>
</li>
<li> <p><code>MSE</code> (Mean Squared Error),
</p>
</li>
<li> <p><code>MAE</code> (Mean Absolute Error),
</p>
</li>
<li> <p><code>HAM</code> (Half Absolute Moment),
</p>
</li>
<li> <p><code>LASSO</code> - use LASSO to shrink the parameters of the model;
</p>
</li>
<li> <p><code>RIDGE</code> - use RIDGE to shrink the parameters of the model;
</p>
</li></ul>

<p>In case of LASSO / RIDGE, the variables are not normalised prior to the estimation,
but the parameters are divided by the standard deviations of explanatory variables
inside the optimisation. As the result the parameters of the final model have the
same interpretation as in the case of classical linear regression. Note that the
user is expected to provide the parameter <code>lambda</code>.
</p>
<p>A user can also provide their own function here as well, making sure
that it accepts parameters <code>actual</code>, <code>fitted</code> and <code>B</code>. Here is an
example:
</p>
<p><code>lossFunction &lt;- function(actual, fitted, B, xreg) return(mean(abs(actual-fitted)))</code>
<code>loss=lossFunction</code>
</p>
<p>See <code>vignette("alm","greybox")</code> for some details on losses and distributions.</p>
</td></tr>
<tr><td><code id="alm_+3A_occurrence">occurrence</code></td>
<td>
<p>what distribution to use for occurrence variable. Can be
<code>"none"</code>, then nothing happens; <code>"plogis"</code> - then the logistic
regression using <code>alm()</code> is estimated for the occurrence part;
<code>"pnorm"</code> - then probit is constructed via <code>alm()</code> for the
occurrence part. In both of the latter cases, the formula used is the same
as the formula for the sizes. Alternatively, you can provide the formula here,
and <code>alm</code> will estimate logistic occurrence model with that formula.
Finally, an &quot;alm&quot; model can be provided and its estimates will be used in
the model construction.
</p>
<p>If this is not <code>"none"</code>, then the model is estimated
in two steps: 1. Occurrence part of the model; 2. Sizes part of the model
(excluding zeroes from the data).</p>
</td></tr>
<tr><td><code id="alm_+3A_scale">scale</code></td>
<td>
<p>formula for scale parameter of the model. If <code>NULL</code>, then it is
assumed that the scale is constant. This might be useful if you need a model with
changing variance (i.e. in case of heteroscedasticity). The log-link is used for
the scale (i.e. take exponent of obtained fitted value for the scale, so that
it is always positive).</p>
</td></tr>
<tr><td><code id="alm_+3A_orders">orders</code></td>
<td>
<p>the orders of ARIMA to include in the model. Only non-seasonal
orders are accepted.</p>
</td></tr>
<tr><td><code id="alm_+3A_parameters">parameters</code></td>
<td>
<p>vector of parameters of the linear model. When <code>NULL</code>, it
is estimated.</p>
</td></tr>
<tr><td><code id="alm_+3A_fast">fast</code></td>
<td>
<p>if <code>TRUE</code>, then the function won't check whether
the data has variability and whether the regressors are correlated. Might
cause trouble, especially in cases of multicollinearity.</p>
</td></tr>
<tr><td><code id="alm_+3A_...">...</code></td>
<td>
<p>additional parameters to pass to distribution functions. This
includes:
</p>

<ul>
<li> <p><code>alpha</code> - value for Asymmetric Laplace distribution;
</p>
</li>
<li> <p><code>size</code> - the size for the Negative Binomial distribution;
</p>
</li>
<li> <p><code>nu</code> - the number of degrees of freedom for Chi-Squared and Student's t;
</p>
</li>
<li> <p><code>shape</code> - the shape parameter for Generalised Normal distribution;
</p>
</li>
<li> <p><code>lambda</code> - the meta parameter for LASSO / RIDGE. Should be between 0 and 1,
regulating the strength of shrinkage, where 0 means don't shrink parameters (use MSE)
and 1 means shrink everything (ignore MSE);
</p>
</li>
<li> <p><code>lambdaBC</code> - lambda for Box-Cox transform parameter in case of Box-Cox
Normal Distribution.
</p>
</li>
<li> <p><code>FI=TRUE</code> will make the function also produce Fisher Information
matrix, which then can be used to calculated variances of smoothing parameters
and initial states of the model. This is used in the <a href="stats.html#topic+vcov">vcov</a> method;
</p>
</li></ul>

<p>You can also pass parameters to the optimiser:
</p>

<ol>
<li> <p><code>B</code> - the vector of starting values of parameters for the optimiser,
should correspond to the explanatory variables. If formula for scale was provided,
the parameters for that part should follow the parameters for location;
</p>
</li>
<li> <p><code>algorithm</code> - the algorithm to use in optimisation
(<code>"NLOPT_LN_SBPLX"</code> by default);
</p>
</li>
<li> <p><code>maxeval</code> - maximum number of evaluations to carry out. Default is 40 per
estimated parameter. In case of LASSO / RIDGE the default is 80 per estimated parameter;
</p>
</li>
<li> <p><code>maxtime</code> - stop, when the optimisation time (in seconds) exceeds this;
</p>
</li>
<li> <p><code>xtol_rel</code> - the precision of the optimiser (the default is 1E-6);
</p>
</li>
<li> <p><code>xtol_abs</code> - the absolute precision of the optimiser (the default is 1E-8);
</p>
</li>
<li> <p><code>ftol_rel</code> - the stopping criterion in case of the relative change in the loss
function (the default is 1E-4);
</p>
</li>
<li> <p><code>ftol_abs</code> - the stopping criterion in case of the absolute change in the loss
function (the default is 0 - not used);
</p>
</li>
<li> <p><code>print_level</code> - the level of output for the optimiser (0 by default).
If equal to 41, then the detailed results of the optimisation are returned.
</p>
</li></ol>

<p>You can read more about these parameters by running the function
<a href="nloptr.html#topic+nloptr.print.options">nloptr.print.options</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a function, similar to <a href="stats.html#topic+lm">lm</a>, but using likelihood for the cases
of several non-normal distributions. These include:
</p>

<ol>
<li> <p><a href="stats.html#topic+dnorm">dnorm</a> - Normal distribution,
</p>
</li>
<li> <p><a href="#topic+dlaplace">dlaplace</a> - Laplace distribution,
</p>
</li>
<li> <p><a href="#topic+ds">ds</a> - S-distribution,
</p>
</li>
<li> <p><a href="#topic+dgnorm">dgnorm</a> - Generalised Normal distribution,
</p>
</li>
<li> <p><a href="stats.html#topic+dlogis">dlogis</a> - Logistic Distribution,
</p>
</li>
<li> <p><a href="stats.html#topic+dt">dt</a> - T-distribution,
</p>
</li>
<li> <p><a href="#topic+dalaplace">dalaplace</a> - Asymmetric Laplace distribution,
</p>
</li>
<li> <p><a href="stats.html#topic+dlnorm">dlnorm</a> - Log-Normal distribution,
</p>
</li>
<li><p> dllaplace - Log-Laplace distribution,
</p>
</li>
<li><p> dls - Log-S distribution,
</p>
</li>
<li><p> dlgnorm - Log-Generalised Normal distribution,
</p>
</li>
<li> <p><a href="#topic+dfnorm">dfnorm</a> - Folded normal distribution,
</p>
</li>
<li> <p><a href="#topic+drectnorm">drectnorm</a> - Rectified normal distribution,
</p>
</li>
<li> <p><a href="#topic+dbcnorm">dbcnorm</a> - Box-Cox normal distribution,
</p>
</li>
<li> <p><a href="statmod.html#topic+dinvgauss">dinvgauss</a> - Inverse Gaussian distribution,
</p>
</li>
<li> <p><a href="stats.html#topic+dgamma">dgamma</a> - Gamma distribution,
</p>
</li>
<li> <p><a href="stats.html#topic+dexp">dexp</a> - Exponential distribution,
</p>
</li>
<li> <p><a href="#topic+dlogitnorm">dlogitnorm</a> - Logit-normal distribution,
</p>
</li>
<li> <p><a href="stats.html#topic+dbeta">dbeta</a> - Beta distribution,
</p>
</li>
<li> <p><a href="stats.html#topic+dpois">dpois</a> - Poisson Distribution,
</p>
</li>
<li> <p><a href="stats.html#topic+dnbinom">dnbinom</a> - Negative Binomial Distribution,
</p>
</li>
<li> <p><a href="stats.html#topic+plogis">plogis</a> - Cumulative Logistic Distribution,
</p>
</li>
<li> <p><a href="stats.html#topic+pnorm">pnorm</a> - Cumulative Normal distribution.
</p>
</li></ol>

<p>This function can be considered as an analogue of <a href="stats.html#topic+glm">glm</a>, but with the
focus on time series. This is why, for example, the function has <code>orders</code> parameter
for ARIMA and produces time series analysis plots with <code>plot(alm(...))</code>.
</p>
<p>This function is slower than <code>lm</code>, because it relies on likelihood estimation
of parameters, hessian calculation and matrix multiplication. So think twice when
using <code>distribution="dnorm"</code> here.
</p>
<p>The estimation is done via the maximisation of likelihood of a selected distribution,
so the number of estimated parameters always includes the scale. Thus the number of degrees
of freedom of the model in case of <code>alm</code> will typically be lower than in the case of
<code>lm</code>.
</p>
<p>See more details and examples in the vignette for &quot;ALM&quot;: <code>vignette("alm","greybox")</code>
</p>


<h3>Value</h3>

<p>Function returns <code>model</code> - the final model of the class
&quot;alm&quot;, which contains:
</p>

<ul>
<li><p> coefficients - estimated parameters of the model,
</p>
</li>
<li><p> FI - Fisher Information of parameters of the model. Returned only when <code>FI=TRUE</code>,
</p>
</li>
<li><p> fitted - fitted values,
</p>
</li>
<li><p> residuals - residuals of the model,
</p>
</li>
<li><p> mu - the estimated location parameter of the distribution,
</p>
</li>
<li><p> scale - the estimated scale parameter of the distribution. If a formula was provided for
scale, then an object of class &quot;scale&quot; will be returned.
</p>
</li>
<li><p> distribution - distribution used in the estimation,
</p>
</li>
<li><p> logLik - log-likelihood of the model. Only returned, when <code>loss="likelihood"</code>
and in several other special cases of distribution and loss combinations (e.g. <code>loss="MSE"</code>,
distribution=&quot;dnorm&quot;),
</p>
</li>
<li><p> loss - the type of the loss function used in the estimation,
</p>
</li>
<li><p> lossFunction - the loss function, if the custom is provided by the user,
</p>
</li>
<li><p> lossValue - the value of the loss function,
</p>
</li>
<li><p> df.residual - number of degrees of freedom of the residuals of the model,
</p>
</li>
<li><p> df - number of degrees of freedom of the model,
</p>
</li>
<li><p> call - how the model was called,
</p>
</li>
<li><p> rank - rank of the model,
</p>
</li>
<li><p> data - data used for the model construction,
</p>
</li>
<li><p> terms - terms of the data. Needed for some additional methods to work,
</p>
</li>
<li><p> occurrence - the occurrence model used in the estimation,
</p>
</li>
<li><p> B - the value of the optimised parameters. Typically, this is a duplicate of coefficients,
</p>
</li>
<li><p> other - the list of all the other parameters either passed to the
function or estimated in the process, but not included in the standard output
(e.g. <code>alpha</code> for Asymmetric Laplace),
</p>
</li>
<li><p> timeElapsed - the time elapsed for the estimation of the model.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stepwise">stepwise</a>, <a href="#topic+lmCombine">lmCombine</a>,
<a href="#topic+xregTransformer">xregTransformer</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### An example with mtcars data and factors
mtcars2 &lt;- within(mtcars, {
   vs &lt;- factor(vs, labels = c("V", "S"))
   am &lt;- factor(am, labels = c("automatic", "manual"))
   cyl  &lt;- factor(cyl)
   gear &lt;- factor(gear)
   carb &lt;- factor(carb)
})
# The standard model with Log-Normal distribution
ourModel &lt;- alm(mpg~., mtcars2[1:30,], distribution="dlnorm")
summary(ourModel)
plot(ourModel)
# Produce table based on the output for LaTeX
xtable(summary(ourModel))

# Produce predictions with the one sided interval (upper bound)
predict(ourModel, mtcars2[-c(1:30),], interval="p", side="u")

# Model with heteroscedasticity (scale changes with the change of qsec)
ourModel &lt;- alm(mpg~., mtcars2[1:30,], scale=~qsec)

### Artificial data for the other examples
xreg &lt;- cbind(rlaplace(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rlaplace(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")

# An example with Laplace distribution
ourModel &lt;- alm(y~x1+x2+trend, xreg, subset=c(1:80), distribution="dlaplace")
summary(ourModel)
plot(predict(ourModel,xreg[-c(1:80),]))

# And another one with Asymmetric Laplace distribution (quantile regression)
# with optimised alpha
ourModel &lt;- alm(y~x1+x2, xreg, subset=c(1:80), distribution="dalaplace")

# An example with AR(1) order
ourModel &lt;- alm(y~x1+x2, xreg, subset=c(1:80), distribution="dnorm", orders=c(1,0,0))
summary(ourModel)
plot(predict(ourModel,xreg[-c(1:80),]))

### Examples with the count data
xreg[,1] &lt;- round(exp(xreg[,1]-70),0)

# Negative Binomial distribution
ourModel &lt;- alm(y~x1+x2, xreg, subset=c(1:80), distribution="dnbinom")
summary(ourModel)
predict(ourModel,xreg[-c(1:80),],interval="p",side="u")

# Poisson distribution
ourModel &lt;- alm(y~x1+x2, xreg, subset=c(1:80), distribution="dpois")
summary(ourModel)
predict(ourModel,xreg[-c(1:80),],interval="p",side="u")


### Examples with binary response variable
xreg[,1] &lt;- round(xreg[,1] / (1 + xreg[,1]),0)

# Logistic distribution (logit regression)
ourModel &lt;- alm(y~x1+x2, xreg, subset=c(1:80), distribution="plogis")
summary(ourModel)
plot(predict(ourModel,xreg[-c(1:80),],interval="c"))

# Normal distribution (probit regression)
ourModel &lt;- alm(y~x1+x2, xreg, subset=c(1:80), distribution="pnorm")
summary(ourModel)
plot(predict(ourModel,xreg[-c(1:80),],interval="p"))

</code></pre>

<hr>
<h2 id='association'>Measures of association</h2><span id='topic+association'></span><span id='topic+assoc'></span>

<h3>Description</h3>

<p>Function returns the matrix of measures of association for different types of
variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>association(x, y = NULL, use = c("na.or.complete", "complete.obs",
  "everything", "all.obs"), method = c("auto", "pearson", "spearman",
  "kendall", "cramer"))

assoc(x, y = NULL, use = c("na.or.complete", "complete.obs", "everything",
  "all.obs"), method = c("auto", "pearson", "spearman", "kendall", "cramer"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="association_+3A_x">x</code></td>
<td>
<p>Either data.frame or a matrix</p>
</td></tr>
<tr><td><code id="association_+3A_y">y</code></td>
<td>
<p>The numerical variable.</p>
</td></tr>
<tr><td><code id="association_+3A_use">use</code></td>
<td>
<p>What observations to use. See <a href="stats.html#topic+cor">cor</a> function for details.
The only option that is not available here is <code>"pairwise.complete.obs"</code>.</p>
</td></tr>
<tr><td><code id="association_+3A_method">method</code></td>
<td>
<p>Which method to use for the calculation of measures of association.
By default this is <code>"auto"</code>, which means that the function will use:
<a href="stats.html#topic+cor">cor</a>, <a href="#topic+mcor">mcor</a> or <a href="#topic+cramer">cramer</a> - depending on
the scales of variables. The other options force the function to use one and
the same method for all the variables:
</p>

<ul>
<li> <p><code>"pearson"</code> - Pearson's correlation coefficient using <a href="stats.html#topic+cor">cor</a>;
</p>
</li>
<li> <p><code>"spearman"</code> - Spearman's correlation coefficient based on <a href="stats.html#topic+cor">cor</a>;
</p>
</li>
<li> <p><code>"kendall"</code> - Kendall's correlation coefficient via <a href="stats.html#topic+cor">cor</a>;
</p>
</li>
<li> <p><code>"cramer"</code> - Cramer's V using <a href="#topic+cramer">cramer</a>;
</p>
</li></ul>

<p>Be aware that the wrong usage of measures of association might give misleading results.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function looks at the types of the variables and calculates different
measures depending on the result:
</p>

<ul>
<li><p> If both variables are numeric, then Pearson's correlation is calculated;
</p>
</li>
<li><p> If both variables are categorical, then Cramer's V is calculated;
</p>
</li>
<li><p> Finally, if one of the variables is categorical, and the other is numeric,
then multiple correlation is returned.
</p>
</li></ul>

<p>After that the measures are wrapped up in a matrix.
</p>
<p>Function also calculates the p-values associated with the respective measures
(see the return).
</p>
<p>See details in the vignette &quot;Marketing analytics with greybox&quot;:
<code>vignette("maUsingGreybox","greybox")</code>
</p>
<p><code>assoc()</code> is just a short name for the <code>association{}</code>.
</p>


<h3>Value</h3>

<p>The following list of values is returned:
</p>

<ul>
<li><p>value - Matrix of the coefficients of association;
</p>
</li>
<li><p>p.value - The p-values for the parameters;
</p>
</li>
<li><p>type - The matrix of the types of measures of association.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+table">table</a>, <a href="#topic+tableplot">tableplot</a>, <a href="#topic+spread">spread</a>,
<a href="#topic+cramer">cramer</a>, <a href="#topic+mcor">mcor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
association(mtcars)

</code></pre>

<hr>
<h2 id='coef.greybox'>Coefficients of the model and their statistics</h2><span id='topic+coef.greybox'></span><span id='topic+coef.alm'></span><span id='topic+confint.alm'></span><span id='topic+confint.scale'></span><span id='topic+vcov.alm'></span><span id='topic+vcov.scale'></span><span id='topic+summary.alm'></span>

<h3>Description</h3>

<p>These are the basic methods for the alm and greybox models that extract coefficients,
their covariance matrix, confidence intervals or generating the summary of the model.
If the non-likelihood related loss was used in the process, then it is recommended to
use bootstrap (which is slow, but more reliable).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'greybox'
coef(object, bootstrap = FALSE, ...)

## S3 method for class 'alm'
confint(object, parm, level = 0.95, bootstrap = FALSE, ...)

## S3 method for class 'scale'
confint(object, parm, level = 0.95, bootstrap = FALSE, ...)

## S3 method for class 'alm'
vcov(object, bootstrap = FALSE, ...)

## S3 method for class 'scale'
vcov(object, bootstrap = FALSE, ...)

## S3 method for class 'alm'
summary(object, level = 0.95, bootstrap = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.greybox_+3A_object">object</code></td>
<td>
<p>The model estimated using alm or other greybox function.</p>
</td></tr>
<tr><td><code id="coef.greybox_+3A_bootstrap">bootstrap</code></td>
<td>
<p>The logical, which determines, whether to use bootstrap in the
process or not.</p>
</td></tr>
<tr><td><code id="coef.greybox_+3A_...">...</code></td>
<td>
<p>Parameters passed to <a href="#topic+coefbootstrap">coefbootstrap</a> function.</p>
</td></tr>
<tr><td><code id="coef.greybox_+3A_parm">parm</code></td>
<td>
<p>The parameters that need to be extracted.</p>
</td></tr>
<tr><td><code id="coef.greybox_+3A_level">level</code></td>
<td>
<p>The confidence level for the construction of the interval.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>coef()</code> method returns the vector of parameters of the model. If
<code>bootstrap=TRUE</code>, then the coefficients are calculated as the mean values of the
bootstrapped ones.
</p>
<p>The <code>vcov()</code> method returns the covariance matrix of parameters. If
<code>bootstrap=TRUE</code>, then the bootstrap is done using <a href="#topic+coefbootstrap">coefbootstrap</a>
function
</p>
<p>The <code>confint()</code> constructs the confidence intervals for parameters. Once again,
this can be done using <code>bootstrap=TRUE</code>.
</p>
<p>Finally, the <code>summary()</code> returns the table with parameters, their standard errors,
confidence intervals and general information about the model.
</p>


<h3>Value</h3>

<p>Depending on the used method, different values are returned.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+alm">alm</a>, <a href="#topic+coefbootstrap">coefbootstrap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An example with ALM
ourModel &lt;- alm(mpg~., mtcars, distribution="dlnorm")
coef(ourModel)
vcov(ourModel)
confint(ourModel)
summary(ourModel)

</code></pre>

<hr>
<h2 id='coefbootstrap'>Bootstrap for parameters of models</h2><span id='topic+coefbootstrap'></span><span id='topic+coefbootstrap.lm'></span><span id='topic+coefbootstrap.alm'></span>

<h3>Description</h3>

<p>The function does the bootstrap for parameters of models and returns covariance matrix
together with the original bootstrapped data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coefbootstrap(object, nsim = 1000, size = floor(0.75 * nobs(object)),
  replace = FALSE, prob = NULL, parallel = FALSE)

## S3 method for class 'lm'
coefbootstrap(object, nsim = 1000, size = floor(0.75 *
  nobs(object)), replace = FALSE, prob = NULL, parallel = FALSE)

## S3 method for class 'alm'
coefbootstrap(object, nsim = 1000, size = floor(0.75 *
  nobs(object)), replace = FALSE, prob = NULL, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coefbootstrap_+3A_object">object</code></td>
<td>
<p>The model estimated using either lm, or alm, or glm.</p>
</td></tr>
<tr><td><code id="coefbootstrap_+3A_nsim">nsim</code></td>
<td>
<p>Number of iterations (simulations) to run.</p>
</td></tr>
<tr><td><code id="coefbootstrap_+3A_size">size</code></td>
<td>
<p>A non-negative integer giving the number of items to choose (the sample size),
passed to <a href="base.html#topic+sample">sample</a> function in R. If not provided and model contains ARIMA
components, this value will be selected at random on each iteration.</p>
</td></tr>
<tr><td><code id="coefbootstrap_+3A_replace">replace</code></td>
<td>
<p>Should sampling be with replacement? Also, passed to <a href="base.html#topic+sample">sample</a>
function in R.</p>
</td></tr>
<tr><td><code id="coefbootstrap_+3A_prob">prob</code></td>
<td>
<p>A vector of probability weights for obtaining the elements of the vector
being sampled. This is passed to the <a href="base.html#topic+sample">sample</a> as well.</p>
</td></tr>
<tr><td><code id="coefbootstrap_+3A_parallel">parallel</code></td>
<td>
<p>Either a logical, specifying whether to do the calculations in parallel,
or the number, specifying the number of cores to use for the parallel calculation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function applies the same model as in the provided object on a smaller sample in
order to get the estimates of parameters and capture the uncertainty about them. This is
a simple implementation of the case resampling, which assumes that the observations are
independent.
</p>


<h3>Value</h3>

<p>Class &quot;bootstrap&quot; is returned, which contains:
</p>

<ul>
<li><p> vcov - the covariance matrix of parameters;
</p>
</li>
<li><p> coefficients - the matrix with the bootstrapped coefficients.
</p>
</li>
<li><p> nsim - number of runs done;
</p>
</li>
<li><p> size - the sample size used in the bootstrap;
</p>
</li>
<li><p> replace - whether the sampling was done with replacement;
</p>
</li>
<li><p> prob - a vector of probability weights used in the process;
</p>
</li>
<li><p> parallel - whether the calculations were done in parallel;
</p>
</li>
<li><p> model - the name of the model used (the name of the function);
</p>
</li>
<li><p> timeElapsed - the time that was spend on the calculations.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+alm">alm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An example with ALM
ourModel &lt;- alm(mpg~., mtcars, distribution="dlnorm", loss="HAM")
# A fast example with 10 iterations. Use at least 1000 to get better results
coefbootstrap(ourModel, nsim=10)

</code></pre>

<hr>
<h2 id='cramer'>Calculate Cramer's V for categorical variables</h2><span id='topic+cramer'></span>

<h3>Description</h3>

<p>Function calculates Cramer's V for two categorical variables based on the table
function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cramer(x, y, use = c("na.or.complete", "complete.obs", "everything",
  "all.obs"), unbiased = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cramer_+3A_x">x</code></td>
<td>
<p>First categorical variable.</p>
</td></tr>
<tr><td><code id="cramer_+3A_y">y</code></td>
<td>
<p>Second categorical variable.</p>
</td></tr>
<tr><td><code id="cramer_+3A_use">use</code></td>
<td>
<p>What observations to use. See <a href="stats.html#topic+cor">cor</a> function for details.
The only option that is not available here is <code>"pairwise.complete.obs"</code>.</p>
</td></tr>
<tr><td><code id="cramer_+3A_unbiased">unbiased</code></td>
<td>
<p>Determines whether to calculate the biased version of Cramer's V or
the one with the small sample correction.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates Cramer's V and also returns the associated statistics from
Chi-Squared test with the null hypothesis of independence of the two variables.
</p>
<p>See details in the vignette &quot;Marketing analytics with greybox&quot;:
<code>vignette("maUsingGreybox","greybox")</code>
</p>


<h3>Value</h3>

<p>The following list of values is returned:
</p>

<ul>
<li><p>valueThe value of Cramer's V;
</p>
</li>
<li><p>statisticThe value of Chi squared statistic associated with the Cramer's V;
</p>
</li>
<li><p>p.valueThe p-value of Chi squared test associated with the Cramer's V;
</p>
</li>
<li><p>dfThe number of degrees of freedom from the test.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>

<p>Wicher Bergsma (2013), A bias-correction for Cramér's V and Tschuprow's T. Journal
of the Korean Statistical Society, 42, pp. 323-328. <a href="https://doi.org/10.1016/j.jkss.2012.10.002">doi:10.1016/j.jkss.2012.10.002</a>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+table">table</a>, <a href="#topic+tableplot">tableplot</a>, <a href="#topic+spread">spread</a>,
<a href="#topic+mcor">mcor</a>, <a href="#topic+association">association</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cramer(mtcars$am, mtcars$gear)

</code></pre>

<hr>
<h2 id='dalaplace'>Asymmetric Laplace Distribution</h2><span id='topic+dalaplace'></span><span id='topic+ALaplace'></span><span id='topic+palaplace'></span><span id='topic+qalaplace'></span><span id='topic+ralaplace'></span>

<h3>Description</h3>

<p>Density, cumulative distribution, quantile functions and random number
generation for the Asymmetric Laplace distribution with the
location parameter mu, scale and the asymmetry parameter alpha.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dalaplace(q, mu = 0, scale = 1, alpha = 0.5, log = FALSE)

palaplace(q, mu = 0, scale = 1, alpha = 0.5)

qalaplace(p, mu = 0, scale = 1, alpha = 0.5)

ralaplace(n = 1, mu = 0, scale = 1, alpha = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dalaplace_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="dalaplace_+3A_mu">mu</code></td>
<td>
<p>vector of location parameters (means).</p>
</td></tr>
<tr><td><code id="dalaplace_+3A_scale">scale</code></td>
<td>
<p>vector of scale parameters.</p>
</td></tr>
<tr><td><code id="dalaplace_+3A_alpha">alpha</code></td>
<td>
<p>value of asymmetry parameter. Varies from 0 to 1.</p>
</td></tr>
<tr><td><code id="dalaplace_+3A_log">log</code></td>
<td>
<p>if <code>TRUE</code>, then probabilities are returned in
logarithms.</p>
</td></tr>
<tr><td><code id="dalaplace_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="dalaplace_+3A_n">n</code></td>
<td>
<p>number of observations. Should be a single number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When mu=0 and scale=1, the Laplace distribution becomes standardized.
The distribution has the following density function:
</p>
<p>f(x) = alpha (1-alpha) / scale exp(-(x-mu)/scale (alpha - I(x&lt;=mu))),
</p>
<p>where I(.) is the indicator function (equal to 1 if the condition is
satisfied and zero otherwise).
</p>
<p>When alpha=0.5, then the distribution becomes Symmetric Laplace, where
scale = 1/2 MAE.
</p>
<p>This distribution function aligns with the quantile estimates of
parameters (Geraci &amp; Bottai, 2007).
</p>
<p>Finally, both <code>palaplace</code> and <code>qalaplace</code> are returned for
the lower tail of the distribution.
</p>


<h3>Value</h3>

<p>Depending on the function, various things are returned
(usually either vector or scalar):
</p>

<ul>
<li> <p><code>dalaplace</code> returns the density function value for the
provided parameters.
</p>
</li>
<li> <p><code>palaplace</code> returns the value of the cumulative function
for the provided parameters.
</p>
</li>
<li> <p><code>qalaplace</code> returns quantiles of the distribution. Depending
on what was provided in <code>p</code>, <code>mu</code> and <code>scale</code>, this
can be either a vector or a matrix, or an array.
</p>
</li>
<li> <p><code>ralaplace</code> returns a vector of random variables
generated from the Laplace distribution. Depending on what was
provided in <code>mu</code> and <code>scale</code>, this can be either a vector
or a matrix or an array.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Geraci Marco, Bottai Matteo (2007). Quantile regression for
longitudinal data using the asymmetric Laplace distribution.
Biostatistics (2007), 8, 1, pp. 140-154
<a href="https://doi.org/10.1093/biostatistics/kxj039">doi:10.1093/biostatistics/kxj039</a>
</p>
</li>
<li><p> Yu, K., &amp; Zhang, J. (2005). A three-parameter asymmetric
laplace distribution and its extension. Communications in Statistics
- Theory and Methods, 34, 1867-1879.
<a href="https://doi.org/10.1080/03610920500199018">doi:10.1080/03610920500199018</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Distributions">Distributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- dalaplace(c(-100:100)/10, 0, 1, 0.2)
plot(x, type="l")

x &lt;- palaplace(c(-100:100)/10, 0, 1, 0.2)
plot(x, type="l")

qalaplace(c(0.025,0.975), 0, c(1,2), c(0.2,0.3))

x &lt;- ralaplace(1000, 0, 1, 0.2)
hist(x)

</code></pre>

<hr>
<h2 id='dbcnorm'>Box-Cox Normal Distribution</h2><span id='topic+dbcnorm'></span><span id='topic+BCNormal'></span><span id='topic+pbcnorm'></span><span id='topic+qbcnorm'></span><span id='topic+rbcnorm'></span>

<h3>Description</h3>

<p>Density, cumulative distribution, quantile functions and random number
generation for the distribution that becomes normal after the Box-Cox
transformation. Note that this is based on the original Box-Cox paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dbcnorm(q, mu = 0, sigma = 1, lambda = 0, log = FALSE)

pbcnorm(q, mu = 0, sigma = 1, lambda = 0)

qbcnorm(p, mu = 0, sigma = 1, lambda = 0)

rbcnorm(n = 1, mu = 0, sigma = 1, lambda = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dbcnorm_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="dbcnorm_+3A_mu">mu</code></td>
<td>
<p>vector of location parameters (means).</p>
</td></tr>
<tr><td><code id="dbcnorm_+3A_sigma">sigma</code></td>
<td>
<p>vector of scale parameters.</p>
</td></tr>
<tr><td><code id="dbcnorm_+3A_lambda">lambda</code></td>
<td>
<p>the value of the Box-Cox transform parameter.</p>
</td></tr>
<tr><td><code id="dbcnorm_+3A_log">log</code></td>
<td>
<p>if <code>TRUE</code>, then probabilities are returned in
logarithms.</p>
</td></tr>
<tr><td><code id="dbcnorm_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="dbcnorm_+3A_n">n</code></td>
<td>
<p>number of observations. Should be a single number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution has the following density function:
</p>
<p>f(y) = y^lambda-1 1/sqrt(2 pi) exp(-((y^lambda-1)/lambda -mu)^2 / (2 sigma^2))
</p>
<p>Both <code>pbcnorm</code> and <code>qbcnorm</code> are returned for the lower
tail of the distribution.
</p>
<p>In case of lambda=0, the values of the log normal distribution are returned.
In case of lambda=1, the values of the normal distribution are returned with mu=mu+1.
</p>
<p>All the functions are defined for non-negative values only.
</p>


<h3>Value</h3>

<p>Depending on the function, various things are returned
(usually either vector or scalar):
</p>

<ul>
<li> <p><code>dbcnorm</code> returns the density function value for the
provided parameters.
</p>
</li>
<li> <p><code>pbcnorm</code> returns the value of the cumulative function
for the provided parameters.
</p>
</li>
<li> <p><code>qbcnorm</code> returns quantiles of the distribution. Depending
on what was provided in <code>p</code>, <code>mu</code> and <code>sigma</code>, this
can be either a vector or a matrix, or an array.
</p>
</li>
<li> <p><code>rbcnorm</code> returns a vector of random variables
generated from the bcnorm distribution. Depending on what was
provided in <code>mu</code> and <code>sigma</code>, this can be either a vector
or a matrix or an array.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Box, G. E., &amp; Cox, D. R. (1964). An Analysis of Transformations.
Journal of the Royal Statistical Society. Series B (Methodological),
26(2), 211–252. Retrieved from https://www.jstor.org/stable/2984418
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Distributions">Distributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- dbcnorm(c(-1000:1000)/200, 0, 1, 1)
plot(c(-1000:1000)/200, x, type="l")

x &lt;- pbcnorm(c(-1000:1000)/200, 0, 1, 1)
plot(c(-1000:1000)/200, x, type="l")

qbcnorm(c(0.025,0.975), 0, c(1,2), 1)

x &lt;- rbcnorm(1000, 0, 1, 1)
hist(x)

</code></pre>

<hr>
<h2 id='detectdst'>DST and Leap year detector functions</h2><span id='topic+detectdst'></span><span id='topic+detectleap'></span>

<h3>Description</h3>

<p>Functions to detect, when Daylight Saving Time and leap year start and finish
</p>


<h3>Usage</h3>

<pre><code class='language-R'>detectdst(object)

detectleap(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="detectdst_+3A_object">object</code></td>
<td>
<p>Either a zoo / xts object or a vector of dates / times in POSIXt / Date
class. Note that in order for <code>detectdst()</code> to work correctly, your data should
not have missing observations. Otherwise it might not be possible to locate, when DST
happens.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>detectdst</code> function detects, when the change for the DST starts and ends. It
assumes that the frequency of data is not lower than hourly.
The <code>detectleap</code> function does similar for the leap year, but flagging the 29th
of February as a starting and to the 28th of February next year as the ending dates.
</p>
<p>In order for the methods to work, the object needs to be of either zoo / xts or POSIXt
class and should contain valid dates.
</p>


<h3>Value</h3>

<p>List containing:
</p>

<ul>
<li><p> start - data frame with id (number of observation) and the respective dates,
when the DST / leap year start;
</p>
</li>
<li><p> end - data frame with id and dates, when DST / leap year end.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xregExpander">xregExpander</a>, <a href="#topic+temporaldummy">temporaldummy</a>,
<a href="#topic+outlierdummy">outlierdummy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate matrix with monthly dummies for a zoo object
x &lt;- as.POSIXct("2004-01-01")+0:(365*24*8)*60*60
detectdst(x)
detectleap(x)

</code></pre>

<hr>
<h2 id='determination'>Coefficients of determination</h2><span id='topic+determination'></span><span id='topic+determ'></span>

<h3>Description</h3>

<p>Function produces coefficients of determination for the provided data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>determination(xreg, bruteforce = TRUE, ...)

determ(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="determination_+3A_xreg">xreg</code></td>
<td>
<p>Data frame or a matrix, containing the exogenous variables.</p>
</td></tr>
<tr><td><code id="determination_+3A_bruteforce">bruteforce</code></td>
<td>
<p>If <code>TRUE</code>, then all the variables will be used
for the regression construction (sink regression). If the number of
observations is smaller than the number of series, the function will
use <a href="#topic+stepwise">stepwise</a> function and select only meaningful
variables. So the reported values will be based on stepwise regressions
for each variable.</p>
</td></tr>
<tr><td><code id="determination_+3A_...">...</code></td>
<td>
<p>Other values passed to cor function.</p>
</td></tr>
<tr><td><code id="determination_+3A_object">object</code></td>
<td>
<p>The object, for which to calculate the coefficients of
determination.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates coefficients of determination (aka R^2)
between all the provided variables. The higher the coefficient for a
variable is, the higher the potential multicollinearity effect in the
model with the variable will be. Coefficients of determination are
connected directly to Variance Inflation Factor (VIF): VIF = 1 / (1 -
determination). Arguably it is easier to interpret, because it is
restricted with (0, 1) bounds. The multicollinearity can be
considered as serious, when determination &gt; 0.9 (which corresponds
to VIF &gt; 10).
</p>
<p>The method <code>determ</code> can be applied to wide variety of classes,
including <code>lm</code>, <code>glm</code> and <code>alm</code>.
</p>
<p>See details in the vignette &quot;Marketing analytics with greybox&quot;:
<code>vignette("maUsingGreybox","greybox")</code>
</p>


<h3>Value</h3>

<p>Function returns the vector of determination coefficients.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><a href="stats.html#topic+cor">cor</a>, <a href="#topic+mcor">mcor</a>, <a href="#topic+stepwise">stepwise</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Simple example
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("x1","x2","x3","Noise")
determination(xreg)

</code></pre>

<hr>
<h2 id='dfnorm'>Folded Normal Distribution</h2><span id='topic+dfnorm'></span><span id='topic+FNormal'></span><span id='topic+pfnorm'></span><span id='topic+qfnorm'></span><span id='topic+rfnorm'></span>

<h3>Description</h3>

<p>Density, cumulative distribution, quantile functions and random number
generation for the folded normal distribution with the location
parameter mu and the scale sigma (which corresponds to standard
deviation in normal distribution).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfnorm(q, mu = 0, sigma = 1, log = FALSE)

pfnorm(q, mu = 0, sigma = 1)

qfnorm(p, mu = 0, sigma = 1)

rfnorm(n = 1, mu = 0, sigma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfnorm_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="dfnorm_+3A_mu">mu</code></td>
<td>
<p>vector of location parameters (means).</p>
</td></tr>
<tr><td><code id="dfnorm_+3A_sigma">sigma</code></td>
<td>
<p>vector of scale parameters.</p>
</td></tr>
<tr><td><code id="dfnorm_+3A_log">log</code></td>
<td>
<p>if <code>TRUE</code>, then probabilities are returned in
logarithms.</p>
</td></tr>
<tr><td><code id="dfnorm_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="dfnorm_+3A_n">n</code></td>
<td>
<p>number of observations. Should be a single number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution has the following density function:
</p>
<p>f(x) = 1/sqrt(2 pi) (exp(-(x-mu)^2 / (2 sigma^2)) + exp(-(x+mu)^2 / (2 sigma^2)))
</p>
<p>Both <code>pfnorm</code> and <code>qfnorm</code> are returned for the lower
tail of the distribution.
</p>


<h3>Value</h3>

<p>Depending on the function, various things are returned
(usually either vector or scalar):
</p>

<ul>
<li> <p><code>dfnorm</code> returns the density function value for the
provided parameters.
</p>
</li>
<li> <p><code>pfnorm</code> returns the value of the cumulative function
for the provided parameters.
</p>
</li>
<li> <p><code>qfnorm</code> returns quantiles of the distribution. Depending
on what was provided in <code>p</code>, <code>mu</code> and <code>sigma</code>, this
can be either a vector or a matrix, or an array.
</p>
</li>
<li> <p><code>rfnorm</code> returns a vector of random variables
generated from the fnorm distribution. Depending on what was
provided in <code>mu</code> and <code>sigma</code>, this can be either a vector
or a matrix or an array.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Wikipedia page on folded normal distribution:
<a href="https://en.wikipedia.org/wiki/Folded_normal_distribution">https://en.wikipedia.org/wiki/Folded_normal_distribution</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Distributions">Distributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- dfnorm(c(-1000:1000)/200, 0, 1)
plot(x, type="l")

x &lt;- pfnorm(c(-1000:1000)/200, 0, 1)
plot(x, type="l")

qfnorm(c(0.025,0.975), 0, c(1,2))

x &lt;- rfnorm(1000, 0, 1)
hist(x)

</code></pre>

<hr>
<h2 id='dgnorm'>The generalized normal distribution</h2><span id='topic+dgnorm'></span><span id='topic+pgnorm'></span><span id='topic+qgnorm'></span><span id='topic+rgnorm'></span>

<h3>Description</h3>

<p>Density, cumulative distribution, quantile functions and random number
generation for the Generalised Normal distribution with the location
mu, a scale and a shape parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dgnorm(q, mu = 0, scale = 1, shape = 1, log = FALSE)

pgnorm(q, mu = 0, scale = 1, shape = 1, lower.tail = TRUE,
  log.p = FALSE)

qgnorm(p, mu = 0, scale = 1, shape = 1, lower.tail = TRUE,
  log.p = FALSE)

rgnorm(n, mu = 0, scale = 1, shape = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dgnorm_+3A_q">q</code></td>
<td>
<p>vector of quantiles</p>
</td></tr>
<tr><td><code id="dgnorm_+3A_mu">mu</code></td>
<td>
<p>location parameter</p>
</td></tr>
<tr><td><code id="dgnorm_+3A_scale">scale</code></td>
<td>
<p>scale parameter</p>
</td></tr>
<tr><td><code id="dgnorm_+3A_shape">shape</code></td>
<td>
<p>shape parameter</p>
</td></tr>
<tr><td><code id="dgnorm_+3A_log">log</code>, <code id="dgnorm_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="dgnorm_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are <code class="reqn">P[X\leq x]</code>,
otherwise <code class="reqn">P[X&gt; x]</code></p>
</td></tr>
<tr><td><code id="dgnorm_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="dgnorm_+3A_n">n</code></td>
<td>
<p>number of observations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A generalized normal random variable <code class="reqn">x</code> with parameters location <code class="reqn">\mu</code>,
scale <code class="reqn">s &gt; 0</code> and shape <code class="reqn">\beta &gt; 0</code> has density:
</p>
<p style="text-align: center;"><code class="reqn">p(x) = \beta exp{-(|x - \mu|/s)^\beta}/(2s \Gamma(1/\beta)).</code>
</p>
 <p><br />
The mean and variance of <code class="reqn">x</code> are <code class="reqn">\mu</code> and
<code class="reqn">s^2 \Gamma(3/\beta)/\Gamma(1/\beta)</code>, respectively.
</p>
<p>The function are based on the functions from gnorm package of Maryclare Griffin
(package has been abandoned since 2018).
</p>
<p>The quantile and cumulative functions use uniform approximation for cases
<code>shape&gt;100</code>. This is needed, because otherwise it is not possible to calculate
the values correctly due to <code>scale^(shape)=Inf</code> in R.
</p>


<h3>Author(s)</h3>

<p>Maryclare Griffin and Ivan Svetunkov
</p>


<h3>Source</h3>

<p><code>dgnorm</code>, <code>pgnorm</code>, <code>qgnorm</code> and<code>rgnorm</code> are all
parametrized as in Version 1 of the
<a href="https://en.wikipedia.org/wiki/Generalized_normal_distribution">Generalized
Normal Distribution Wikipedia page</a>,
which uses the parametrization given by in Nadarajah (2005).
The same distribution was described much earlier by Subbotin (1923) and named
the exponential power distribution by Box and Tiao (1973). <br /> <br />
</p>


<h3>References</h3>


<ul>
<li><p> Box, G. E. P. and G. C. Tiao. &quot;Bayesian inference in Statistical Analysis.&quot;
Addison-Wesley Pub. Co., Reading, Mass (1973).
</p>
</li>
<li><p> Nadarajah, Saralees. &quot;A generalized normal distribution.&quot; Journal of
Applied Statistics 32.7 (2005): 685-694.
</p>
</li>
<li><p> Subbotin, M. T. &quot;On the Law of Frequency of Error.&quot; Matematicheskii
Sbornik 31.2 (1923):  206-301.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Distributions">Distributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Density function values for standard normal distribution
x &lt;- dgnorm(seq(-1, 1, length.out = 100), 0, sqrt(2), 2)
plot(x, type="l")

#CDF of standard Laplace
x &lt;- pgnorm(c(-100:100), 0, 1, 1)
plot(x, type="l")

# Quantiles of S distribution
qgnorm(c(0.025,0.975), 0, 1, 0.5)

# Random numbers from a distribution with shape=10000 (approximately uniform)
x &lt;- rgnorm(1000, 0, 1, 1000)
hist(x)

</code></pre>

<hr>
<h2 id='Distributions'>Distribution functions of the greybox package</h2><span id='topic+Distributions'></span>

<h3>Description</h3>

<p>The greybox package implements several distribution functions. In this document,
I list the main functions and provide links to related resources.
</p>


<h3>Details</h3>


<ul>
<li><p> Generalised normal distribution (with a kurtosis parameter by Nadarajah, 2005):
<a href="#topic+qgnorm">qgnorm</a>, <a href="#topic+dgnorm">dgnorm</a>,
<a href="#topic+pgnorm">pgnorm</a>, <a href="#topic+rgnorm">rgnorm</a>.
</p>
</li>
<li><p> S distribution (a special case of Generalised Normal with shape=0.5):
<a href="#topic+qs">qs</a>, <a href="#topic+ds">ds</a>,
<a href="#topic+ps">ps</a>, <a href="#topic+rs">rs</a>.
</p>
</li>
<li><p> Laplace distribution (special case of Generalised Normal with shape=1):
<a href="#topic+qlaplace">qlaplace</a>, <a href="#topic+dlaplace">dlaplace</a>,
<a href="#topic+plaplace">plaplace</a>, <a href="#topic+rlaplace">rlaplace</a>.
</p>
</li>
<li><p> Asymmetric Laplace distribution (Yu &amp; Zhang, 2005):
<a href="#topic+qalaplace">qalaplace</a>, <a href="#topic+dalaplace">dalaplace</a>,
<a href="#topic+palaplace">palaplace</a>, <a href="#topic+ralaplace">ralaplace</a>.
</p>
</li>
<li><p> Logit Normal distribution (Mead, 1965):
<a href="#topic+qlogitnorm">qlogitnorm</a>, <a href="#topic+dlogitnorm">dlogitnorm</a>,
<a href="#topic+plogitnorm">plogitnorm</a>, <a href="#topic+rlogitnorm">rlogitnorm</a>.
</p>
</li>
<li><p> Box-Cox Normal distribution (Box &amp; Cox, 1964):
<a href="#topic+qbcnorm">qbcnorm</a>, <a href="#topic+dbcnorm">dbcnorm</a>,
<a href="#topic+pbcnorm">pbcnorm</a>, <a href="#topic+rbcnorm">rbcnorm</a>.
</p>
</li>
<li><p> Folded Normal distribution:
<a href="#topic+qfnorm">qfnorm</a>, <a href="#topic+dfnorm">dfnorm</a>,
<a href="#topic+pfnorm">pfnorm</a>, <a href="#topic+rfnorm">rfnorm</a>.
</p>
</li>
<li><p> Rectified Normal distribution:
<a href="#topic+qrectnorm">qrectnorm</a>, <a href="#topic+drectnorm">drectnorm</a>,
<a href="#topic+prectnorm">prectnorm</a>, <a href="#topic+rrectnorm">rrectnorm</a>.
</p>
</li>
<li><p> Three parameter Log Normal distribution (Sangal &amp; Biswas, 1970):
<a href="#topic+qtplnorm">qtplnorm</a>, <a href="#topic+dtplnorm">dtplnorm</a>,
<a href="#topic+ptplnorm">ptplnorm</a>, <a href="#topic+rtplnorm">rtplnorm</a>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Nadarajah, Saralees. &quot;A generalized normal distribution.&quot; Journal of
Applied Statistics 32.7 (2005): 685-694.
</p>
</li>
<li><p> Wikipedia page on Laplace distribution:
<a href="https://en.wikipedia.org/wiki/Laplace_distribution">https://en.wikipedia.org/wiki/Laplace_distribution</a>.
</p>
</li>
<li><p> Yu, K., &amp; Zhang, J. (2005). A three-parameter asymmetric
laplace distribution and its extension. Communications in Statistics
- Theory and Methods, 34, 1867-1879.
<a href="https://doi.org/10.1080/03610920500199018">doi:10.1080/03610920500199018</a>
</p>
</li>
<li><p> Mead, R. (1965). A Generalised Logit-Normal Distribution.
Biometrics, 21 (3), 721–732. doi: 10.2307/2528553
</p>
</li>
<li><p> Box, G. E., &amp; Cox, D. R. (1964). An Analysis of Transformations.
Journal of the Royal Statistical Society. Series B (Methodological),
26(2), 211–252. Retrieved from https://www.jstor.org/stable/2984418
</p>
</li>
<li><p> Sangal, B. P., &amp; Biswas, A. K. (1970). The 3-Parameter
Distribution Applications in Hydrology. Water Resources Research,
6(2), 505–515. <a href="https://doi.org/10.1029/WR006i002p00505">doi:10.1029/WR006i002p00505</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="stats.html#topic+Distributions">Distributions</a></code> from the stats package.
</p>

<hr>
<h2 id='dlaplace'>Laplace Distribution</h2><span id='topic+dlaplace'></span><span id='topic+Laplace'></span><span id='topic+plaplace'></span><span id='topic+qlaplace'></span><span id='topic+rlaplace'></span>

<h3>Description</h3>

<p>Density, cumulative distribution, quantile functions and random number
generation for the Laplace distribution with the location parameter mu
and the scale parameter (which is equal to Mean Absolute Error, aka
Mean Absolute Deviation).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlaplace(q, mu = 0, scale = 1, log = FALSE)

plaplace(q, mu = 0, scale = 1)

qlaplace(p, mu = 0, scale = 1)

rlaplace(n = 1, mu = 0, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dlaplace_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="dlaplace_+3A_mu">mu</code></td>
<td>
<p>vector of location parameters (means).</p>
</td></tr>
<tr><td><code id="dlaplace_+3A_scale">scale</code></td>
<td>
<p>vector of mean absolute errors.</p>
</td></tr>
<tr><td><code id="dlaplace_+3A_log">log</code></td>
<td>
<p>if <code>TRUE</code>, then probabilities are returned in
logarithms.</p>
</td></tr>
<tr><td><code id="dlaplace_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="dlaplace_+3A_n">n</code></td>
<td>
<p>number of observations. Should be a single number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When mu=0 and scale=1, the Laplace distribution becomes standardized.
The distribution has the following density function:
</p>
<p>f(x) = 1/(2 scale) exp(-abs(x-mu) / scale)
</p>
<p>Both <code>plaplace</code> and <code>qlaplace</code> are returned for the lower
tail of the distribution.
</p>


<h3>Value</h3>

<p>Depending on the function, various things are returned
(usually either vector or scalar):
</p>

<ul>
<li> <p><code>dlaplace</code> returns the density function value for the
provided parameters.
</p>
</li>
<li> <p><code>plaplace</code> returns the value of the cumulative function
for the provided parameters.
</p>
</li>
<li> <p><code>qlaplace</code> returns quantiles of the distribution. Depending
on what was provided in <code>p</code>, <code>mu</code> and <code>scale</code>, this
can be either a vector or a matrix, or an array.
</p>
</li>
<li> <p><code>rlaplace</code> returns a vector of random variables
generated from the Laplace distribution. Depending on what was
provided in <code>mu</code> and <code>scale</code>, this can be either a vector
or a matrix or an array.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Wikipedia page on Laplace distribution:
<a href="https://en.wikipedia.org/wiki/Laplace_distribution">https://en.wikipedia.org/wiki/Laplace_distribution</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Distributions">Distributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- dlaplace(c(-100:100)/10, 0, 1)
plot(x, type="l")

x &lt;- plaplace(c(-100:100)/10, 0, 1)
plot(x, type="l")

qlaplace(c(0.025,0.975), 0, c(1,2))

x &lt;- rlaplace(1000, 0, 1)
hist(x)

</code></pre>

<hr>
<h2 id='dlogitnorm'>Logit Normal Distribution</h2><span id='topic+dlogitnorm'></span><span id='topic+LogitNormal'></span><span id='topic+plogitnorm'></span><span id='topic+qlogitnorm'></span><span id='topic+rlogitnorm'></span>

<h3>Description</h3>

<p>Density, cumulative distribution, quantile functions and random number
generation for the distribution that becomes normal after the Logit
transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlogitnorm(q, mu = 0, sigma = 1, log = FALSE)

plogitnorm(q, mu = 0, sigma = 1)

qlogitnorm(p, mu = 0, sigma = 1)

rlogitnorm(n = 1, mu = 0, sigma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dlogitnorm_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="dlogitnorm_+3A_mu">mu</code></td>
<td>
<p>vector of location parameters (means).</p>
</td></tr>
<tr><td><code id="dlogitnorm_+3A_sigma">sigma</code></td>
<td>
<p>vector of scale parameters.</p>
</td></tr>
<tr><td><code id="dlogitnorm_+3A_log">log</code></td>
<td>
<p>if <code>TRUE</code>, then probabilities are returned in
logarithms.</p>
</td></tr>
<tr><td><code id="dlogitnorm_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="dlogitnorm_+3A_n">n</code></td>
<td>
<p>number of observations. Should be a single number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution has the following density function:
</p>
<p>f(y) = 1/(sqrt(2 pi) y (1-y)) exp(-(logit(y) -mu)^2 / (2 sigma^2))
</p>
<p>where y is in (0, 1) and logit(y) =log(y/(1-y)).
</p>
<p>Both <code>plogitnorm</code> and <code>qlogitnorm</code> are returned for the lower
tail of the distribution.
</p>
<p>All the functions are defined for the values between 0 and 1.
</p>


<h3>Value</h3>

<p>Depending on the function, various things are returned
(usually either vector or scalar):
</p>

<ul>
<li> <p><code>dlogitnorm</code> returns the density function value for the
provided parameters.
</p>
</li>
<li> <p><code>plogitnorm</code> returns the value of the cumulative function
for the provided parameters.
</p>
</li>
<li> <p><code>qlogitnorm</code> returns quantiles of the distribution. Depending
on what was provided in <code>p</code>, <code>mu</code> and <code>sigma</code>, this
can be either a vector or a matrix, or an array.
</p>
</li>
<li> <p><code>rlogitnorm</code> returns a vector of random variables
generated from the logitnorm distribution. Depending on what was
provided in <code>mu</code> and <code>sigma</code>, this can be either a vector
or a matrix or an array.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Mead, R. (1965). A Generalised Logit-Normal Distribution.
Biometrics, 21 (3), 721–732. doi: 10.2307/2528553
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Distributions">Distributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- dlogitnorm(c(-1000:1000)/200, 0, 1)
plot(c(-1000:1000)/200, x, type="l")

x &lt;- plogitnorm(c(-1000:1000)/200, 0, 1)
plot(c(-1000:1000)/200, x, type="l")

qlogitnorm(c(0.025,0.975), 0, c(1,2))

x &lt;- rlogitnorm(1000, 0, 1)
hist(x)

</code></pre>

<hr>
<h2 id='drectnorm'>Rectified Normal Distribution</h2><span id='topic+drectnorm'></span><span id='topic+rectNormal'></span><span id='topic+prectnorm'></span><span id='topic+qrectnorm'></span><span id='topic+rrectnorm'></span>

<h3>Description</h3>

<p>Density, cumulative distribution, quantile functions and random number
generation for the Rectified Normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>drectnorm(q, mu = 0, sigma = 1, log = FALSE)

prectnorm(q, mu = 0, sigma = 1)

qrectnorm(p, mu = 0, sigma = 1)

rrectnorm(n = 1, mu = 0, sigma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="drectnorm_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="drectnorm_+3A_mu">mu</code></td>
<td>
<p>vector of location parameters (means).</p>
</td></tr>
<tr><td><code id="drectnorm_+3A_sigma">sigma</code></td>
<td>
<p>vector of scale parameters.</p>
</td></tr>
<tr><td><code id="drectnorm_+3A_log">log</code></td>
<td>
<p>if <code>TRUE</code>, then probabilities are returned in
logarithms.</p>
</td></tr>
<tr><td><code id="drectnorm_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="drectnorm_+3A_n">n</code></td>
<td>
<p>number of observations. Should be a single number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If x~N(mu, sigma^2) then y = max(0, x) follows Rectified Normal distribution:
y~RectN(mu, sigma^2), which can be written as:
</p>
<p>f_y = 1(x&lt;=0) F_x(mu, sigma) + 1(x&gt;0) f_x(x, mu, sigma),
</p>
<p>where F_x is the cumulative distribution function and f_x is the probability
density function of normal distribution.
</p>
<p>Both <code>prectnorm</code> and <code>qrectnorm</code> are returned for the lower
tail of the distribution.
</p>
<p>All the functions are defined for non-negative values only.
</p>


<h3>Value</h3>

<p>Depending on the function, various things are returned
(usually either vector or scalar):
</p>

<ul>
<li> <p><code>drectnorm</code> returns the density function value for the
provided parameters.
</p>
</li>
<li> <p><code>prectnorm</code> returns the value of the cumulative function
for the provided parameters.
</p>
</li>
<li> <p><code>qrectnorm</code> returns quantiles of the distribution. Depending
on what was provided in <code>p</code>, <code>mu</code> and <code>sigma</code>, this
can be either a vector or a matrix, or an array.
</p>
</li>
<li> <p><code>rrectnorm</code> returns a vector of random variables
generated from the RectN distribution. Depending on what was
provided in <code>mu</code> and <code>sigma</code>, this can be either a vector
or a matrix or an array.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Distributions">Distributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- drectnorm(c(-1000:1000)/200, 0, 1)
plot(c(-1000:1000)/200, x, type="l")

x &lt;- prectnorm(c(-1000:1000)/200, 0, 1)
plot(c(-1000:1000)/200, x, type="l")

qrectnorm(c(0.025,0.975), 0, c(1,2))

x &lt;- rrectnorm(1000, 0, 1)
hist(x)

</code></pre>

<hr>
<h2 id='ds'>S Distribution</h2><span id='topic+ds'></span><span id='topic+SDistribution'></span><span id='topic+ps'></span><span id='topic+qs'></span><span id='topic+rs'></span>

<h3>Description</h3>

<p>Density, cumulative distribution, quantile functions and random number
generation for the S distribution with the location parameter mu
and a scaling parameter scale.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ds(q, mu = 0, scale = 1, log = FALSE)

ps(q, mu = 0, scale = 1)

qs(p, mu = 0, scale = 1)

rs(n = 1, mu = 0, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ds_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="ds_+3A_mu">mu</code></td>
<td>
<p>vector of location parameters (means).</p>
</td></tr>
<tr><td><code id="ds_+3A_scale">scale</code></td>
<td>
<p>vector of scaling parameter (which are equal to ham/2).</p>
</td></tr>
<tr><td><code id="ds_+3A_log">log</code></td>
<td>
<p>if <code>TRUE</code>, then probabilities are returned in
logarithms.</p>
</td></tr>
<tr><td><code id="ds_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="ds_+3A_n">n</code></td>
<td>
<p>number of observations. Should be a single number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When mu=0 and ham=2, the S distribution becomes standardized with
scale=1 (this is because scale=ham/2). The distribution has the following
density function:
</p>
<p>f(x) = 1/(4 scale^2) exp(-sqrt(abs(x-mu)) / scale)
</p>
<p>The S distribution has fat tails and large excess.
</p>
<p>Both <code>ps</code> and <code>qs</code> are returned for the lower tail of
the distribution.
</p>


<h3>Value</h3>

<p>Depending on the function, various things are returned
(usually either vector or scalar):
</p>

<ul>
<li> <p><code>ds</code> returns the density function value for the
provided parameters.
</p>
</li>
<li> <p><code>ps</code> returns the value of the cumulative function
for the provided parameters.
</p>
</li>
<li> <p><code>qs</code> returns quantiles of the distribution. Depending
on what was provided in <code>p</code>, <code>mu</code> and <code>scale</code>, this
can be either a vector or a matrix, or an array.
</p>
</li>
<li> <p><code>rs</code> returns a vector of random variables
generated from the S distribution. Depending on what was provided
in <code>mu</code> and <code>scale</code>, this can be either a vector or a matrix
or an array.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Distributions">Distributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- ds(c(-1000:1000)/10, 0, 1)
plot(x, type="l")

x &lt;- ps(c(-1000:1000)/10, 0, 1)
plot(x, type="l")

qs(c(0.025,0.975), 0, 1)

x &lt;- rs(1000, 0, 1)
hist(x)

</code></pre>

<hr>
<h2 id='dtplnorm'>Three Parameter Log Normal Distribution</h2><span id='topic+dtplnorm'></span><span id='topic+TPLNormal'></span><span id='topic+ptplnorm'></span><span id='topic+qtplnorm'></span><span id='topic+rtplnorm'></span>

<h3>Description</h3>

<p>Density, cumulative distribution, quantile functions and random number
generation for the 3 parameter log normal distribution with the location
parameter mu, scale sigma (which corresponds to standard deviation in normal
distribution) and shifting parameter shift.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtplnorm(q, mu = 0, sigma = 1, shift = 0, log = FALSE)

ptplnorm(q, mu = 0, sigma = 1, shift = 0)

qtplnorm(p, mu = 0, sigma = 1, shift = 0)

rtplnorm(n = 1, mu = 0, sigma = 1, shift = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dtplnorm_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="dtplnorm_+3A_mu">mu</code></td>
<td>
<p>vector of location parameters (means).</p>
</td></tr>
<tr><td><code id="dtplnorm_+3A_sigma">sigma</code></td>
<td>
<p>vector of scale parameters.</p>
</td></tr>
<tr><td><code id="dtplnorm_+3A_shift">shift</code></td>
<td>
<p>vector of shift parameters.</p>
</td></tr>
<tr><td><code id="dtplnorm_+3A_log">log</code></td>
<td>
<p>if <code>TRUE</code>, then probabilities are returned in
logarithms.</p>
</td></tr>
<tr><td><code id="dtplnorm_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="dtplnorm_+3A_n">n</code></td>
<td>
<p>number of observations. Should be a single number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution has the following density function:
</p>
<p>f(x) = 1/(x-a) 1/sqrt(2 pi) exp(-(log(x-a)-mu)^2 / (2 sigma^2))
</p>
<p>Both <code>ptplnorm</code> and <code>qtplnorm</code> are returned for the lower
tail of the distribution.
</p>
<p>The function is based on the lnorm functions from stats package, introducing
the shift parameter.
</p>


<h3>Value</h3>

<p>Depending on the function, various things are returned
(usually either vector or scalar):
</p>

<ul>
<li> <p><code>dtplnorm</code> returns the density function value for the
provided parameters.
</p>
</li>
<li> <p><code>ptplnorm</code> returns the value of the cumulative function
for the provided parameters.
</p>
</li>
<li> <p><code>qtplnorm</code> returns quantiles of the distribution. Depending
on what was provided in <code>p</code>, <code>mu</code> and <code>sigma</code>, this
can be either a vector or a matrix, or an array.
</p>
</li>
<li> <p><code>rtplnorm</code> returns a vector of random variables
generated from the tplnorm distribution. Depending on what was
provided in <code>mu</code> and <code>sigma</code>, this can be either a vector
or a matrix or an array.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Sangal, B. P., &amp; Biswas, A. K. (1970). The 3-Parameter
Distribution Applications in Hydrology. Water Resources Research,
6(2), 505–515. <a href="https://doi.org/10.1029/WR006i002p00505">doi:10.1029/WR006i002p00505</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Distributions">Distributions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- dtplnorm(c(-1000:1000)/200, 0, 1, 1)
plot(c(-1000:1000)/200, x, type="l")

x &lt;- ptplnorm(c(-1000:1000)/200, 0, 1, 1)
plot(c(-1000:1000)/200, x, type="l")

qtplnorm(c(0.025,0.975), 0, c(1,2), 1)

x &lt;- rtplnorm(1000, 0, 1, 1)
hist(x)

</code></pre>

<hr>
<h2 id='errorType'>Functions that extracts type of error from the model</h2><span id='topic+errorType'></span>

<h3>Description</h3>

<p>This function allows extracting error type from any model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>errorType(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="errorType_+3A_object">object</code></td>
<td>
<p>Model estimated using one of the functions of smooth package.</p>
</td></tr>
<tr><td><code id="errorType_+3A_...">...</code></td>
<td>
<p>Currently nothing is accepted via ellipsis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>errorType</code> extracts the type of error from the model
(either additive or multiplicative).
</p>


<h3>Value</h3>

<p>Either <code>"A"</code> for additive error or <code>"M"</code> for multiplicative.
All the other functions return strings of character.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")
ourModel &lt;- alm(y~x1+x2,as.data.frame(xreg))

errorType(ourModel)

</code></pre>

<hr>
<h2 id='extractScale'>Functions to extract scale and standard error from a model</h2><span id='topic+extractScale'></span><span id='topic+extractScale.default'></span><span id='topic+extractScale.greybox'></span><span id='topic+extractSigma'></span><span id='topic+extractSigma.default'></span><span id='topic+extractSigma.greybox'></span>

<h3>Description</h3>

<p>Functions extract scale and the standard error of the residuals. Mainly needed for
the work with the model estimated via <a href="#topic+sm">sm</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractScale(object, ...)

## Default S3 method:
extractScale(object, ...)

## S3 method for class 'greybox'
extractScale(object, ...)

extractSigma(object, ...)

## Default S3 method:
extractSigma(object, ...)

## S3 method for class 'greybox'
extractSigma(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extractScale_+3A_object">object</code></td>
<td>
<p>The model estimated using lm / alm / etc.</p>
</td></tr>
<tr><td><code id="extractScale_+3A_...">...</code></td>
<td>
<p>Other parameters (currently nothing).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In case of a simpler model, the functions will return the scalar using
<code>sigma()</code> method. If the scale model was estimated, <code>extractScale()</code> and
<code>extractSigma()</code> will return the conditional scale and the conditional
standard error of the residuals respectively.
</p>


<h3>Value</h3>

<p>One of the two is returned, depending on the type of estimated model:
</p>

<ul>
<li><p> Scalar from <code>sigma()</code> method if the variance is assumed to be constant.
</p>
</li>
<li><p> Vector of values if the scale model was estimated
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sm">sm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate the data
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+sqrt(exp(0.8+0.2*xreg[,1]))*rnorm(100,0,1),
              xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")

# Estimate the location and scale model
ourModel &lt;- alm(y~., xreg, scale=~x1+x2)

# Extract scale
extractScale(ourModel)
# Extract standard error
extractSigma(ourModel)

</code></pre>

<hr>
<h2 id='graphmaker'>Linear graph construction function</h2><span id='topic+graphmaker'></span>

<h3>Description</h3>

<p>The function makes a standard linear graph using the provided actuals and
forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graphmaker(actuals, forecast, fitted = NULL, lower = NULL, upper = NULL,
  level = NULL, legend = TRUE, cumulative = FALSE, vline = TRUE,
  parReset = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="graphmaker_+3A_actuals">actuals</code></td>
<td>
<p>The vector of actual values</p>
</td></tr>
<tr><td><code id="graphmaker_+3A_forecast">forecast</code></td>
<td>
<p>The vector of forecasts. Should be <code>ts</code> object that starts at
the end of <code>fitted</code> values.</p>
</td></tr>
<tr><td><code id="graphmaker_+3A_fitted">fitted</code></td>
<td>
<p>The vector of fitted values.</p>
</td></tr>
<tr><td><code id="graphmaker_+3A_lower">lower</code></td>
<td>
<p>The vector of lower bound values of a prediction interval.
Should be ts object that start at the end of <code>fitted</code> values.</p>
</td></tr>
<tr><td><code id="graphmaker_+3A_upper">upper</code></td>
<td>
<p>The vector of upper bound values of a prediction interval.
Should be ts object that start at the end of <code>fitted</code> values.</p>
</td></tr>
<tr><td><code id="graphmaker_+3A_level">level</code></td>
<td>
<p>The width of the prediction interval.</p>
</td></tr>
<tr><td><code id="graphmaker_+3A_legend">legend</code></td>
<td>
<p>If <code>TRUE</code>, the legend is drawn.</p>
</td></tr>
<tr><td><code id="graphmaker_+3A_cumulative">cumulative</code></td>
<td>
<p>If <code>TRUE</code>, then the forecast is treated as
cumulative and value per period is plotted.</p>
</td></tr>
<tr><td><code id="graphmaker_+3A_vline">vline</code></td>
<td>
<p>Whether to draw the vertical line, splitting the in-sample
and the holdout sample.</p>
</td></tr>
<tr><td><code id="graphmaker_+3A_parreset">parReset</code></td>
<td>
<p>Whether to reset par() after plotting things or not.
If <code>FALSE</code> then you can add elements to the plot (e.g. additional lines).</p>
</td></tr>
<tr><td><code id="graphmaker_+3A_...">...</code></td>
<td>
<p>Other parameters passed to <code>plot()</code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function uses the provided data to construct a linear graph. It is strongly
advised to use <code>ts</code> objects to define the start of each of the vectors.
Otherwise the data may be plotted incorrectly.
</p>


<h3>Value</h3>

<p>Function does not return anything.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ts">ts</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(y=rnorm(100,100,10),x=rnorm(100,10,10))
almModel &lt;- alm(y~x, xreg, subset=c(1:90))
values &lt;- predict(almModel, newdata=xreg[-c(1:90),], interval="prediction")

graphmaker(xreg[,1],values$mean,fitted(values))
graphmaker(xreg[,1],values$mean,fitted(values),legend=FALSE)
graphmaker(xreg[,1],values$mean,fitted(values),legend=FALSE,lower=values$lower,upper=values$upper)

# Produce the necessary ts objects from an arbitrary vectors
actuals &lt;- ts(c(1:10), start=c(2000,1), frequency=4)
forecast &lt;- ts(c(11:15),start=end(actuals)[1]+end(actuals)[2]*deltat(actuals),
               frequency=frequency(actuals))
graphmaker(actuals,forecast)

# This should work as well
graphmaker(c(1:10),c(11:15))

# This way you can add additional elements to the plot
graphmaker(c(1:10),c(11:15), parReset=FALSE)
points(c(1:15))
# But don't forget to do dev.off() in order to reset the plotting area afterwards

</code></pre>

<hr>
<h2 id='greybox'>Grey box</h2><span id='topic+greybox'></span><span id='topic+greybox-package'></span>

<h3>Description</h3>

<p>Toolbox for working with univariate models for purposes of analysis and forecasting
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;"> Package: </td><td style="text-align: left;"> greybox</td>
</tr>
<tr>
 <td style="text-align: left;"> Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;"> Date: </td><td style="text-align: left;">
2018-02-13 - Inf</td>
</tr>
<tr>
 <td style="text-align: left;"> License: </td><td style="text-align: left;"> GPL-2 </td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>
<p> The following functions are
included in the package:
</p>

<ul>
<li> <p><a href="#topic+AICc">AICc</a> and <a href="#topic+BICc">BICc</a> - AIC / BIC corrected for the
sample size.
</p>
</li>
<li> <p><a href="#topic+pointLik">pointLik</a> - point likelihood of the function.
</p>
</li>
<li> <p><a href="#topic+pAIC">pAIC</a>, <a href="#topic+pAICc">pAICc</a>, <a href="#topic+pBIC">pBIC</a>,
<a href="#topic+pBICc">pBICc</a> - point versions of respective information criteria.
</p>
</li>
<li> <p><a href="#topic+determination">determination</a> - Coefficients of determination between different
exogenous variables.
</p>
</li>
<li> <p><a href="#topic+temporaldummy">temporaldummy</a> - Matrix with seasonal dummy variables.
</p>
</li>
<li> <p><a href="#topic+outlierdummy">outlierdummy</a> - Matrix with dummies for outliers.
</p>
</li>
<li> <p><a href="#topic+alm">alm</a> - Advanced Linear Model - regression, estimated using
likelihood with specified distribution (e.g. Laplace or Chi-Squared).
</p>
</li>
<li> <p><a href="#topic+sm">sm</a> - Scale Model - Regression model for scale of distribution
(e.g. for Variance of Normal distribution). Requires an <code>lm()</code> or <code>alm()</code>
model.
</p>
</li>
<li> <p><a href="#topic+stepwise">stepwise</a> - Stepwise based on information criteria and partial
correlations. Efficient and fast.
</p>
</li>
<li> <p><a href="#topic+xregExpander">xregExpander</a> - Function that expands the provided data into
the data with lags and leads.
</p>
</li>
<li> <p><a href="#topic+xregTransformer">xregTransformer</a> - Function produces mathematical transformations
of the variables, such as taking logarithms, square roots etc.
</p>
</li>
<li> <p><a href="#topic+xregMultiplier">xregMultiplier</a> - Function produces cross-products of the
matrix of the provided variables.
</p>
</li>
<li> <p><a href="#topic+lmCombine">lmCombine</a> - Function combines lm models from the estimated
based on information criteria weights.
</p>
</li>
<li> <p><a href="#topic+lmDynamic">lmDynamic</a> - Dynamic regression based on point AIC.
</p>
</li>
<li> <p><a href="#topic+ro">ro</a> - Rolling origin evaluation.
</p>
</li>
<li> <p><a href="#topic+Distributions">Distributions</a> - document explaining the distribution functions
of the greybox package.
</p>
</li>
<li> <p><a href="#topic+spread">spread</a> - function that produces scatterplots / boxplots / tableplots,
depending on the types of variables.
</p>
</li>
<li> <p><a href="#topic+assoc">assoc</a> - function that calculates measures of association,
depending on the types of variables.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>
<p>Maintainer: Ivan Svetunkov
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stepwise">stepwise</a>, <a href="#topic+lmCombine">lmCombine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")

stepwise(xreg)


</code></pre>

<hr>
<h2 id='hm'>Half moment of a distribution and its derivatives.</h2><span id='topic+hm'></span><span id='topic+ham'></span><span id='topic+asymmetry'></span><span id='topic+extremity'></span><span id='topic+cextremity'></span>

<h3>Description</h3>

<p><code>hm()</code> function estimates half moment from some predefined constant
<code>C</code>. <code>ham()</code> estimates the Half Absolute Moment. <code>asymmetry()</code>
function returns Asymmetry coefficient, while <code>extremity()</code>
returns the coefficient of Extremity, both based on <code>hm()</code>. Finally,
<code>cextremity()</code> returns the Complex Extremity coefficient, based on <code>hm()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hm(x, C = mean(x, na.rm = TRUE), ...)

ham(x, C = mean(x, na.rm = TRUE), ...)

asymmetry(x, C = mean(x, na.rm = TRUE), ...)

extremity(x, C = mean(x, na.rm = TRUE), ...)

cextremity(x, C = mean(x, na.rm = TRUE), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hm_+3A_x">x</code></td>
<td>
<p>A variable based on which HM is estimated.</p>
</td></tr>
<tr><td><code id="hm_+3A_c">C</code></td>
<td>
<p>Centring parameter.</p>
</td></tr>
<tr><td><code id="hm_+3A_...">...</code></td>
<td>
<p>Other parameters passed to mean function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>NA</code> values of <code>x</code> are excluded on the first step of calculation.
</p>


<h3>Value</h3>

<p>A complex variable is returned for the <code>hm()</code> and <code>cextremity()</code>
functions, and real values are returned for <code>ham()</code>,
<code>asymmetry()</code> and <code>extremity()</code>.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Svetunkov I., Kourentzes N., Svetunkov S. &quot;Half Central Moment for Data Analysis&quot;.
Working Paper of Department of Management Science, Lancaster University, 2023:3, 1–21.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- rnorm(100,0,1)
hm(x)
ham(x)
asymmetry(x)
extremity(x)
cextremity(x)

</code></pre>

<hr>
<h2 id='implant'>Implant the scale model in the location model</h2><span id='topic+implant'></span>

<h3>Description</h3>

<p>The function implants the scale model into the location model. It currently works
with <a href="#topic+alm">alm</a> / <a href="smooth.html#topic+adam">adam</a> and <code>sm()</code> method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>implant(location, scale, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="implant_+3A_location">location</code></td>
<td>
<p>Model estimated using either <code>alm</code> or <code>adam</code>.</p>
</td></tr>
<tr><td><code id="implant_+3A_scale">scale</code></td>
<td>
<p>The scale model, estimate with <code>sm</code> method.</p>
</td></tr>
<tr><td><code id="implant_+3A_...">...</code></td>
<td>
<p>Currently nothing. Implemented for flexibility.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is needed in order to treat the scale of model correctly in the methods
like <code>forecast()</code>.
</p>


<h3>Value</h3>

<p>The model of the same class as the location model, but with scale
from the estimated model via <code>sm()</code>. This is needed to produce
appropriate forecasts in case of scale model and to take  into account
the correct number of estimated parameters.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><a href="#topic+alm">alm</a>, <a href="smooth.html#topic+adam">adam</a>, <a href="#topic+sm">sm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+sqrt(exp(0.8+0.2*xreg[,1]))*rnorm(100,0,1),
              xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")

# Estimate the location model
ourModel &lt;- alm(y~.,xreg)
# Estimate the scale model
ourScale &lt;- sm(ourModel,formula=~x1+x2)
# Implant scale into location
ourModel &lt;- implant(ourModel, ourScale)

</code></pre>

<hr>
<h2 id='is.greybox'>Greybox classes checkers</h2><span id='topic+is.greybox'></span><span id='topic+is.alm'></span><span id='topic+is.occurrence'></span><span id='topic+is.greyboxC'></span><span id='topic+is.greyboxD'></span><span id='topic+is.rollingOrigin'></span><span id='topic+is.rmc'></span><span id='topic+is.scale'></span>

<h3>Description</h3>

<p>Functions to check if an object is of the specified class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.greybox(x)

is.alm(x)

is.occurrence(x)

is.greyboxC(x)

is.greyboxD(x)

is.rollingOrigin(x)

is.rmc(x)

is.scale(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.greybox_+3A_x">x</code></td>
<td>
<p>The object to check.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The list of functions includes:
</p>

<ul>
<li> <p><code>is.greybox()</code> tests if the object was produced by a greybox function
(e.g. <a href="#topic+alm">alm</a> / <a href="#topic+stepwise">stepwise</a> / <a href="#topic+lmCombine">lmCombine</a>
/ <a href="#topic+lmDynamic">lmDynamic</a>);
</p>
</li>
<li> <p><code>is.alm()</code> tests if the object was produced by <code>alm()</code> function;
</p>
</li>
<li> <p><code>is.occurrence()</code> tests if an occurrence part of the model was produced;
</p>
</li>
<li> <p><code>is.greyboxC()</code> tests if the object was produced by <code>lmCombine()</code>
function;
</p>
</li>
<li> <p><code>is.greyboxD()</code> tests if the object was produced by <code>lmDynamic()</code>
function;
</p>
</li>
<li> <p><code>is.rmc()</code> tests if the object was produced by <code>rmc()</code> function;
</p>
</li>
<li> <p><code>is.rollingOrigin()</code> tests if the object was produced by <code>ro()</code>
function;
</p>
</li>
<li> <p><code>is.scale()</code> tests if the object is of the class &quot;scale&quot; (produced by
<a href="#topic+alm">alm</a> or <a href="#topic+sm">sm</a> in case of heteroscedastic model);
</p>
</li></ul>



<h3>Value</h3>

<p><code>TRUE</code> if this is the specified class and <code>FALSE</code> otherwise.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rlaplace(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rlaplace(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")

ourModel &lt;- alm(y~x1+x2, xreg, distribution="dnorm")

is.alm(ourModel)
is.greybox(ourModel)
is.greyboxC(ourModel)
is.greyboxD(ourModel)

</code></pre>

<hr>
<h2 id='lmCombine'>Combine regressions based on information criteria</h2><span id='topic+lmCombine'></span>

<h3>Description</h3>

<p>Function combines parameters of linear regressions of the first variable
on all the other provided data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmCombine(data, ic = c("AICc", "AIC", "BIC", "BICc"), bruteforce = FALSE,
  silent = TRUE, formula = NULL, subset = NULL,
  distribution = c("dnorm", "dlaplace", "ds", "dgnorm", "dlogis", "dt",
  "dalaplace", "dlnorm", "dllaplace", "dls", "dlgnorm", "dbcnorm", "dfnorm",
  "dinvgauss", "dgamma", "dpois", "dnbinom", "dlogitnorm", "plogis", "pnorm"),
  parallel = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmCombine_+3A_data">data</code></td>
<td>
<p>Data frame containing dependent variable in the first column and
the others in the rest.</p>
</td></tr>
<tr><td><code id="lmCombine_+3A_ic">ic</code></td>
<td>
<p>Information criterion to use.</p>
</td></tr>
<tr><td><code id="lmCombine_+3A_bruteforce">bruteforce</code></td>
<td>
<p>If <code>TRUE</code>, then all the possible models are generated
and combined. Otherwise the best model is found and then models around that
one are produced and then combined.</p>
</td></tr>
<tr><td><code id="lmCombine_+3A_silent">silent</code></td>
<td>
<p>If <code>FALSE</code>, then nothing is silent, everything is printed
out. <code>TRUE</code> means that nothing is produced.</p>
</td></tr>
<tr><td><code id="lmCombine_+3A_formula">formula</code></td>
<td>
<p>If provided, then the selection will be done from the listed
variables in the formula after all the necessary transformations.</p>
</td></tr>
<tr><td><code id="lmCombine_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td></tr>
<tr><td><code id="lmCombine_+3A_distribution">distribution</code></td>
<td>
<p>Distribution to pass to <code>alm()</code>. See <a href="#topic+alm">alm</a>
for details.</p>
</td></tr>
<tr><td><code id="lmCombine_+3A_parallel">parallel</code></td>
<td>
<p>If <code>TRUE</code>, then the model fitting is done in parallel.
WARNING! Packages <code>foreach</code> and either <code>doMC</code> (Linux and Mac only)
or <code>doParallel</code> are needed in order to run the function in parallel.</p>
</td></tr>
<tr><td><code id="lmCombine_+3A_...">...</code></td>
<td>
<p>Other parameters passed to <code>alm()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm uses alm() to fit different models and then combines the models
based on the selected IC. The parameters are combined so that if they are not
present in some of models, it is assumed that they are equal to zero. Thus,
there is a shrinkage effect in the combination.
</p>
<p>Some details and examples of application are also given in the vignette
&quot;Greybox&quot;: <code>vignette("greybox","greybox")</code>
</p>


<h3>Value</h3>

<p>Function returns <code>model</code> - the final model of the class
&quot;greyboxC&quot;. The list of variables:
</p>

<ul>
<li><p> coefficients - combined parameters of the model,
</p>
</li>
<li><p> vcov - combined covariance matrix of the model,
</p>
</li>
<li><p> fitted - the fitted values,
</p>
</li>
<li><p> residuals - residual of the model,
</p>
</li>
<li><p> distribution - distribution used in the estimation,
</p>
</li>
<li><p> logLik - combined log-likelihood of the model,
</p>
</li>
<li><p> IC - the values of the combined information criterion,
</p>
</li>
<li><p> ICType - the type of information criterion used,
</p>
</li>
<li><p> df.residual - number of degrees of freedom of the residuals of
the combined model,
</p>
</li>
<li><p> df - number of degrees of freedom of the combined model,
</p>
</li>
<li><p> importance - importance of the parameters,
</p>
</li>
<li><p> combination - the table, indicating which variables were used in every
model construction and what were the weights for each model,
</p>
</li>
<li><p> timeElapsed - the time elapsed for the estimation of the model.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Burnham Kenneth P. and Anderson David R. (2002). Model Selection
and Multimodel Inference. A Practical Information-Theoretic Approach.
Springer-Verlag New York. DOI: [10.1007/b97636](http://dx.doi.org/10.1007/b97636).
</p>
</li>
<li><p> McQuarrie, A. D. (1999). A small-sample correction for the Schwarz SIC model
selection criterion. Statistics &amp; Probability Letters, 44(1), 79–86.
[10.1016/S0167-7152(98)00294-6](https://doi.org/10.1016/S0167-7152(98)00294-6).
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="stats.html#topic+step">step</a>, <a href="#topic+xregExpander">xregExpander</a>,
<a href="#topic+stepwise">stepwise</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Simple example
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")
inSample &lt;- xreg[1:80,]
outSample &lt;- xreg[-c(1:80),]
# Combine all the possible models
ourModel &lt;- lmCombine(inSample,bruteforce=TRUE)
predict(ourModel,outSample)
plot(predict(ourModel,outSample))

### Fat regression example
xreg &lt;- matrix(rnorm(5000,10,3),50,100)
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(50,0,3),xreg,rnorm(50,300,10))
colnames(xreg) &lt;- c("y",paste0("x",c(1:100)),"Noise")
inSample &lt;- xreg[1:40,]
outSample &lt;- xreg[-c(1:40),]
# Combine only the models close to the optimal
ourModel &lt;- lmCombine(inSample, ic="BICc",bruteforce=FALSE)
summary(ourModel)
plot(predict(ourModel, outSample))

# Combine in parallel - should increase speed in case of big data
## Not run: ourModel &lt;- lmCombine(inSample, ic="BICc", bruteforce=TRUE, parallel=TRUE)
summary(ourModel)
plot(predict(ourModel, outSample))
## End(Not run)

</code></pre>

<hr>
<h2 id='lmDynamic'>Combine regressions based on point information criteria</h2><span id='topic+lmDynamic'></span>

<h3>Description</h3>

<p>Function combines parameters of linear regressions of the first variable
on all the other provided data using pAIC weights. This is an extension of the
<a href="#topic+lmCombine">lmCombine</a> function, which relies upon the idea that the combination
weights might change over time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmDynamic(data, ic = c("AICc", "AIC", "BIC", "BICc"), bruteforce = FALSE,
  silent = TRUE, formula = NULL, subset = NULL,
  distribution = c("dnorm", "dlaplace", "ds", "dgnorm", "dlogis", "dt",
  "dalaplace", "dlnorm", "dllaplace", "dls", "dlgnorm", "dbcnorm", "dfnorm",
  "dinvgauss", "dgamma", "dpois", "dnbinom", "dlogitnorm", "plogis", "pnorm"),
  parallel = FALSE, lowess = TRUE, f = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmDynamic_+3A_data">data</code></td>
<td>
<p>Data frame containing dependent variable in the first column and
the others in the rest.</p>
</td></tr>
<tr><td><code id="lmDynamic_+3A_ic">ic</code></td>
<td>
<p>Information criterion to use.</p>
</td></tr>
<tr><td><code id="lmDynamic_+3A_bruteforce">bruteforce</code></td>
<td>
<p>If <code>TRUE</code>, then all the possible models are generated
and combined. Otherwise the best model is found and then models around that
one are produced and then combined.</p>
</td></tr>
<tr><td><code id="lmDynamic_+3A_silent">silent</code></td>
<td>
<p>If <code>FALSE</code>, then nothing is silent, everything is printed
out. <code>TRUE</code> means that nothing is produced.</p>
</td></tr>
<tr><td><code id="lmDynamic_+3A_formula">formula</code></td>
<td>
<p>If provided, then the selection will be done from the listed
variables in the formula after all the necessary transformations.</p>
</td></tr>
<tr><td><code id="lmDynamic_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td></tr>
<tr><td><code id="lmDynamic_+3A_distribution">distribution</code></td>
<td>
<p>Distribution to pass to <code>alm()</code>. See <a href="#topic+alm">alm</a>
for details.</p>
</td></tr>
<tr><td><code id="lmDynamic_+3A_parallel">parallel</code></td>
<td>
<p>If <code>TRUE</code>, then the model fitting is done in parallel.
WARNING! Packages <code>foreach</code> and either <code>doMC</code> (Linux and Mac only)
or <code>doParallel</code> are needed in order to run the function in parallel.</p>
</td></tr>
<tr><td><code id="lmDynamic_+3A_lowess">lowess</code></td>
<td>
<p>Logical defining, whether LOWESS should be used to smooth the
dynamic weights. By default it is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="lmDynamic_+3A_f">f</code></td>
<td>
<p>the smoother span for LOWESS. This gives the proportion of points in
the plot which influence the smooth at each value. Larger values give more
smoothness. If <code>NULL</code> the parameter will be optimised by minimising
<code>ic</code>.</p>
</td></tr>
<tr><td><code id="lmDynamic_+3A_...">...</code></td>
<td>
<p>Other parameters passed to <code>alm()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm uses alm() to fit different models and then combines the models
based on the selected point IC. The combination weights are calculated for each
observation based on the point IC and then smoothed via LOWESS if the respective
parameter (<code>lowess</code>) is set to TRUE.
</p>
<p>Some details and examples of application are also given in the vignette
&quot;Greybox&quot;: <code>vignette("greybox","greybox")</code>
</p>


<h3>Value</h3>

<p>Function returns <code>model</code> - the final model of the class
&quot;greyboxD&quot;, which includes time varying parameters and dynamic importance
of each variable. The list of variables:
</p>

<ul>
<li><p> coefficients - the mean (over time) parameters of the model,
</p>
</li>
<li><p> vcov - the combined covariance matrix of the model,
</p>
</li>
<li><p> fitted - the fitted values,
</p>
</li>
<li><p> residuals - the residuals of the model,
</p>
</li>
<li><p> distribution - the distribution used in the estimation,
</p>
</li>
<li><p> logLik - the mean (over time) log-likelihood of the model,
</p>
</li>
<li><p> IC - dynamic values of the information criterion (pIC),
</p>
</li>
<li><p> ICType - the type of information criterion used,
</p>
</li>
<li><p> df.residual - mean number of degrees of freedom of the residuals of
the model,
</p>
</li>
<li><p> df - mean number of degrees of freedom of the model,
</p>
</li>
<li><p> importance - dynamic importance of the parameters,
</p>
</li>
<li><p> call - call used in the function,
</p>
</li>
<li><p> rank - rank of the combined model,
</p>
</li>
<li><p> data - the data used in the model,
</p>
</li>
<li><p> mu - the location value of the distribution,
</p>
</li>
<li><p> scale - the scale parameter if alm() was used,
</p>
</li>
<li><p> coefficientsDynamic - table with parameters of the model, varying over
the time,
</p>
</li>
<li><p> df.residualDynamic - dynamic df.residual,
</p>
</li>
<li><p> dfDynamic - dynamic df,
</p>
</li>
<li><p> weights - the dynamic weights for each model under consideration,
</p>
</li>
<li><p> timeElapsed - the time elapsed for the estimation of the model.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Burnham Kenneth P. and Anderson David R. (2002). Model Selection
and Multimodel Inference. A Practical Information-Theoretic Approach.
Springer-Verlag New York. DOI: [10.1007/b97636](http://dx.doi.org/10.1007/b97636).
</p>
</li>
<li><p> McQuarrie, A. D. (1999). A small-sample correction for the Schwarz SIC model
selection criterion. Statistics &amp; Probability Letters, 44(1), 79–86.
[10.1016/S0167-7152(98)00294-6](https://doi.org/10.1016/S0167-7152(98)00294-6).
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+stepwise">stepwise</a>, <a href="#topic+lmCombine">lmCombine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Simple example
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")
inSample &lt;- xreg[1:80,]
outSample &lt;- xreg[-c(1:80),]
# Combine all the possible models
ourModel &lt;- lmDynamic(inSample,bruteforce=TRUE)
predict(ourModel,outSample)
plot(predict(ourModel,outSample))

</code></pre>

<hr>
<h2 id='mcor'>Multiple correlation</h2><span id='topic+mcor'></span>

<h3>Description</h3>

<p>Function calculates multiple correlation between y and x, constructing a linear
regression model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcor(x, y, use = c("na.or.complete", "complete.obs", "everything",
  "all.obs"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcor_+3A_x">x</code></td>
<td>
<p>Either data.frame or a matrix</p>
</td></tr>
<tr><td><code id="mcor_+3A_y">y</code></td>
<td>
<p>The numerical variable.</p>
</td></tr>
<tr><td><code id="mcor_+3A_use">use</code></td>
<td>
<p>What observations to use. See <a href="stats.html#topic+cor">cor</a> function for details.
The only option that is not available here is <code>"pairwise.complete.obs"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is based on the linear regression model with the set of variables in x. The
returned value is just a coefficient of multiple correlation from regression,
the F-statistics of the model (thus testing the null hypothesis that all the
parameters are equal to zero), the associated p-value and the degrees of freedom.
</p>
<p>See details in the vignette &quot;Marketing analytics with greybox&quot;:
<code>vignette("maUsingGreybox","greybox")</code>
</p>


<h3>Value</h3>

<p>The following list of values is returned:
</p>

<ul>
<li><p>valueThe value of the coefficient;
</p>
</li>
<li><p>statisticThe value of F-statistics associated with the parameter;
</p>
</li>
<li><p>p.valueThe p-value of F-statistics associated with the parameter;
</p>
</li>
<li><p>df.residualThe number of degrees of freedom for the residuals;
</p>
</li>
<li><p>dfThe number of degrees of freedom for the data.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+table">table</a>, <a href="#topic+tableplot">tableplot</a>, <a href="#topic+spread">spread</a>,
<a href="#topic+cramer">cramer</a>, <a href="#topic+association">association</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mcor(mtcars$am, mtcars$mpg)

</code></pre>

<hr>
<h2 id='ME'>Error measures</h2><span id='topic+ME'></span><span id='topic+Errors'></span><span id='topic+MAE'></span><span id='topic+MSE'></span><span id='topic+MRE'></span><span id='topic+MIS'></span><span id='topic+MPE'></span><span id='topic+MAPE'></span><span id='topic+MASE'></span><span id='topic+RMSSE'></span><span id='topic+rMAE'></span><span id='topic+rRMSE'></span><span id='topic+rAME'></span><span id='topic+rMIS'></span><span id='topic+sMSE'></span><span id='topic+sPIS'></span><span id='topic+sCE'></span><span id='topic+sMIS'></span><span id='topic+GMRAE'></span>

<h3>Description</h3>

<p>Functions allow to calculate different types of errors for point and
interval predictions:
</p>

<ol>
<li><p> ME - Mean Error,
</p>
</li>
<li><p> MAE - Mean Absolute Error,
</p>
</li>
<li><p> MSE - Mean Squared Error,
</p>
</li>
<li><p> MRE - Mean Root Error (Kourentzes, 2014),
</p>
</li>
<li><p> MIS - Mean Interval Score (Gneiting &amp; Raftery, 2007),
</p>
</li>
<li><p> MPE - Mean Percentage Error,
</p>
</li>
<li><p> MAPE - Mean Absolute Percentage Error (See Svetunkov, 2017 for
the critique),
</p>
</li>
<li><p> MASE - Mean Absolute Scaled Error (Hyndman &amp; Koehler, 2006),
</p>
</li>
<li><p> RMSSE - Root Mean Squared Scaled Error (used in M5 Competition),
</p>
</li>
<li><p> rMAE - Relative Mean Absolute Error (Davydenko &amp; Fildes, 2013),
</p>
</li>
<li><p> rRMSE - Relative Root Mean Squared Error,
</p>
</li>
<li><p> rAME - Relative Absolute Mean Error,
</p>
</li>
<li><p> rMIS - Relative Mean Interval Score,
</p>
</li>
<li><p> sMSE - Scaled Mean Squared Error (Petropoulos &amp; Kourentzes, 2015),
</p>
</li>
<li><p> sPIS- Scaled Periods-In-Stock (Wallstrom &amp; Segerstedt, 2010),
</p>
</li>
<li><p> sCE - Scaled Cumulative Error,
</p>
</li>
<li><p> sMIS - Scaled Mean Interval Score,
</p>
</li>
<li><p> GMRAE - Geometric Mean Relative Absolute Error.
</p>
</li></ol>



<h3>Usage</h3>

<pre><code class='language-R'>ME(holdout, forecast, na.rm = TRUE)

MAE(holdout, forecast, na.rm = TRUE)

MSE(holdout, forecast, na.rm = TRUE)

MRE(holdout, forecast, na.rm = TRUE)

MIS(holdout, lower, upper, level = 0.95, na.rm = TRUE)

MPE(holdout, forecast, na.rm = TRUE)

MAPE(holdout, forecast, na.rm = TRUE)

MASE(holdout, forecast, scale, na.rm = TRUE)

RMSSE(holdout, forecast, scale, na.rm = TRUE)

rMAE(holdout, forecast, benchmark, na.rm = TRUE)

rRMSE(holdout, forecast, benchmark, na.rm = TRUE)

rAME(holdout, forecast, benchmark, na.rm = TRUE)

rMIS(holdout, lower, upper, benchmarkLower, benchmarkUpper, level = 0.95,
  na.rm = TRUE)

sMSE(holdout, forecast, scale, na.rm = TRUE)

sPIS(holdout, forecast, scale, na.rm = TRUE)

sCE(holdout, forecast, scale, na.rm = TRUE)

sMIS(holdout, lower, upper, scale, level = 0.95, na.rm = TRUE)

GMRAE(holdout, forecast, benchmark, na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ME_+3A_holdout">holdout</code></td>
<td>
<p>The vector or matrix of holdout values.</p>
</td></tr>
<tr><td><code id="ME_+3A_forecast">forecast</code></td>
<td>
<p>The vector or matrix of forecasts values.</p>
</td></tr>
<tr><td><code id="ME_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical, defining whether to remove the NAs from the provided data or not.</p>
</td></tr>
<tr><td><code id="ME_+3A_lower">lower</code></td>
<td>
<p>The lower bound of the prediction interval.</p>
</td></tr>
<tr><td><code id="ME_+3A_upper">upper</code></td>
<td>
<p>The upper bound of the prediction interval.</p>
</td></tr>
<tr><td><code id="ME_+3A_level">level</code></td>
<td>
<p>The confidence level of the constructed interval.</p>
</td></tr>
<tr><td><code id="ME_+3A_scale">scale</code></td>
<td>
<p>The value that should be used in the denominator of MASE. Can
be anything but advised values are: mean absolute deviation of in-sample one
step ahead Naive error or mean absolute value of the in-sample actuals.</p>
</td></tr>
<tr><td><code id="ME_+3A_benchmark">benchmark</code></td>
<td>
<p>The vector or matrix of the forecasts of the benchmark
model.</p>
</td></tr>
<tr><td><code id="ME_+3A_benchmarklower">benchmarkLower</code></td>
<td>
<p>The lower bound of the prediction interval of the
benchmark model.</p>
</td></tr>
<tr><td><code id="ME_+3A_benchmarkupper">benchmarkUpper</code></td>
<td>
<p>The upper bound of the prediction interval of the
benchmark model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In case of <code>sMSE</code>, <code>scale</code> needs to be a squared value. Typical
one &ndash; squared mean value of in-sample actuals.
</p>
<p>If all the measures are needed, then <a href="#topic+measures">measures</a> function
can help.
</p>
<p>There are several other measures, see details of <a href="#topic+pinball">pinball</a>
and <a href="#topic+hm">hm</a>.
</p>


<h3>Value</h3>

<p>All the functions return the scalar value.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Kourentzes N. (2014). The Bias Coefficient: a new metric for forecast bias
<a href="https://kourentzes.com/forecasting/2014/12/17/the-bias-coefficient-a-new-metric-for-forecast-bias/">https://kourentzes.com/forecasting/2014/12/17/the-bias-coefficient-a-new-metric-for-forecast-bias/</a>
</p>
</li>
<li><p> Svetunkov, I. (2017). Naughty APEs and the quest for the holy grail.
<a href="https://forecasting.svetunkov.ru/en/2017/07/29/naughty-apes-and-the-quest-for-the-holy-grail/">https://forecasting.svetunkov.ru/en/2017/07/29/naughty-apes-and-the-quest-for-the-holy-grail/</a>
</p>
</li>
<li><p> Fildes R. (1992). The evaluation of
extrapolative forecasting methods. International Journal of Forecasting, 8,
pp.81-98.
</p>
</li>
<li><p> Hyndman R.J., Koehler A.B. (2006). Another look at measures of
forecast accuracy. International Journal of Forecasting, 22, pp.679-688.
</p>
</li>
<li><p> Petropoulos F., Kourentzes N. (2015). Forecast combinations for
intermittent demand. Journal of the Operational Research Society, 66,
pp.914-924.
</p>
</li>
<li><p> Wallstrom P., Segerstedt A. (2010). Evaluation of forecasting error
measurements and techniques for intermittent demand. International Journal
of Production Economics, 128, pp.625-636.
</p>
</li>
<li><p> Davydenko, A., Fildes, R. (2013). Measuring Forecasting Accuracy:
The Case Of Judgmental Adjustments To Sku-Level Demand Forecasts.
International Journal of Forecasting, 29(3), 510-522.
<a href="https://doi.org/10.1016/j.ijforecast.2012.09.002">doi:10.1016/j.ijforecast.2012.09.002</a>
</p>
</li>
<li><p> Gneiting, T., &amp; Raftery, A. E. (2007). Strictly proper scoring rules,
prediction, and estimation. Journal of the American Statistical Association,
102(477), 359–378. <a href="https://doi.org/10.1198/016214506000001437">doi:10.1198/016214506000001437</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+pinball">pinball</a>, <a href="#topic+hm">hm</a>, <a href="#topic+measures">measures</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

y &lt;- rnorm(100,10,2)
testForecast &lt;- rep(mean(y[1:90]),10)

MAE(y[91:100],testForecast)
MSE(y[91:100],testForecast)

MPE(y[91:100],testForecast)
MAPE(y[91:100],testForecast)

# Measures from Petropoulos &amp; Kourentzes (2015)
MASE(y[91:100],testForecast,mean(abs(y[1:90])))
sMSE(y[91:100],testForecast,mean(abs(y[1:90]))^2)
sPIS(y[91:100],testForecast,mean(abs(y[1:90])))
sCE(y[91:100],testForecast,mean(abs(y[1:90])))

# Original MASE from Hyndman &amp; Koehler (2006)
MASE(y[91:100],testForecast,mean(abs(diff(y[1:90]))))

testForecast2 &lt;- rep(y[91],10)
# Relative measures, from and inspired by Davydenko &amp; Fildes (2013)
rMAE(y[91:100],testForecast2,testForecast)
rRMSE(y[91:100],testForecast2,testForecast)
rAME(y[91:100],testForecast2,testForecast)
GMRAE(y[91:100],testForecast2,testForecast)

#### Measures for the prediction intervals
# An example with mtcars data
ourModel &lt;- alm(mpg~., mtcars[1:30,], distribution="dnorm")
ourBenchmark &lt;- alm(mpg~1, mtcars[1:30,], distribution="dnorm")

# Produce predictions with the interval
ourForecast &lt;- predict(ourModel, mtcars[-c(1:30),], interval="p")
ourBenchmarkForecast &lt;- predict(ourBenchmark, mtcars[-c(1:30),], interval="p")

MIS(mtcars$mpg[-c(1:30)],ourForecast$lower,ourForecast$upper,0.95)
sMIS(mtcars$mpg[-c(1:30)],ourForecast$lower,ourForecast$upper,mean(mtcars$mpg[1:30]),0.95)
rMIS(mtcars$mpg[-c(1:30)],ourForecast$lower,ourForecast$upper,
       ourBenchmarkForecast$lower,ourBenchmarkForecast$upper,0.95)

### Also, see pinball function for other measures for the intervals

</code></pre>

<hr>
<h2 id='measures'>Error measures for the provided forecasts</h2><span id='topic+measures'></span>

<h3>Description</h3>

<p>Function calculates several error measures using the provided
forecasts and the data for the holdout sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measures(holdout, forecast, actual, digits = NULL, benchmark = c("naive",
  "mean"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="measures_+3A_holdout">holdout</code></td>
<td>
<p>The vector of the holdout values.</p>
</td></tr>
<tr><td><code id="measures_+3A_forecast">forecast</code></td>
<td>
<p>The vector of forecasts produced by a model.</p>
</td></tr>
<tr><td><code id="measures_+3A_actual">actual</code></td>
<td>
<p>The vector of actual in-sample values.</p>
</td></tr>
<tr><td><code id="measures_+3A_digits">digits</code></td>
<td>
<p>Number of digits of the output. If <code>NULL</code>
then no rounding is done.</p>
</td></tr>
<tr><td><code id="measures_+3A_benchmark">benchmark</code></td>
<td>
<p>The character variable, defining what to use as
benchmark for relative measures. Can be either <code>"naive"</code> or
<code>"mean"</code> (arithmetic mean of the whole series. The latter
can be useful when dealing with intermittent data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The functions returns the named vector of errors:
</p>

<ul>
<li><p> ME,
</p>
</li>
<li><p> MAE,
</p>
</li>
<li><p> MSE
</p>
</li>
<li><p> MPE,
</p>
</li>
<li><p> MAPE,
</p>
</li>
<li><p> MASE,
</p>
</li>
<li><p> sMAE,
</p>
</li>
<li><p> RMSSE,
</p>
</li>
<li><p> sMSE,
</p>
</li>
<li><p> sCE,
</p>
</li>
<li><p> rMAE,
</p>
</li>
<li><p> rRMSE,
</p>
</li>
<li><p> rAME,
</p>
</li>
<li><p> asymmetry,
</p>
</li>
<li><p> sPIS.
</p>
</li></ul>

<p>For the details on these errors, see <a href="#topic+Errors">Errors</a>.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Svetunkov, I. (2017). Naughty APEs and the quest for the holy grail.
<a href="https://forecasting.svetunkov.ru/en/2017/07/29/naughty-apes-and-the-quest-for-the-holy-grail/">https://forecasting.svetunkov.ru/en/2017/07/29/naughty-apes-and-the-quest-for-the-holy-grail/</a>
</p>
</li>
<li><p> Fildes R. (1992). The evaluation of
extrapolative forecasting methods. International Journal of Forecasting, 8,
pp.81-98.
</p>
</li>
<li><p> Hyndman R.J., Koehler A.B. (2006). Another look at measures of
forecast accuracy. International Journal of Forecasting, 22, pp.679-688.
</p>
</li>
<li><p> Petropoulos F., Kourentzes N. (2015). Forecast combinations for
intermittent demand. Journal of the Operational Research Society, 66,
pp.914-924.
</p>
</li>
<li><p> Wallstrom P., Segerstedt A. (2010). Evaluation of forecasting error
measurements and techniques for intermittent demand. International Journal
of Production Economics, 128, pp.625-636.
</p>
</li>
<li><p> Davydenko, A., Fildes, R. (2013). Measuring Forecasting Accuracy:
The Case Of Judgmental Adjustments To Sku-Level Demand Forecasts.
International Journal of Forecasting, 29(3), 510-522.
<a href="https://doi.org/10.1016/j.ijforecast.2012.09.002">doi:10.1016/j.ijforecast.2012.09.002</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>

y &lt;- rnorm(100,10,2)
ourForecast &lt;- rep(mean(y[1:90]),10)

measures(y[91:100],ourForecast,y[1:90],digits=5)

</code></pre>

<hr>
<h2 id='nparam'>Number of parameters and number of variates in the model</h2><span id='topic+nparam'></span><span id='topic+nvariate'></span>

<h3>Description</h3>

<p><code>nparam()</code> returns the number of estimated parameters in the model,
while <code>nvariate()</code> returns number of variates for the response
variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nparam(object, ...)

nvariate(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nparam_+3A_object">object</code></td>
<td>
<p>Time series model.</p>
</td></tr>
<tr><td><code id="nparam_+3A_...">...</code></td>
<td>
<p>Some other parameters passed to the method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nparam()</code> is a very basic and a simple function which does what it says:
extracts number of estimated parameters in the model. <code>nvariate()</code> returns
number of variates (dimensions, columns) for the response variable (1 for the
univariate regression).
</p>


<h3>Value</h3>

<p>Both functions return numeric values.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><a href="stats.html#topic+nobs">nobs</a>, <a href="stats.html#topic+logLik">logLik</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Simple example
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")
ourModel &lt;- lm(y~.,data=as.data.frame(xreg))

nparam(ourModel)
nvariate(ourModel)

</code></pre>

<hr>
<h2 id='outlierdummy'>Outlier detection and matrix creation</h2><span id='topic+outlierdummy'></span><span id='topic+outlierdummy.default'></span><span id='topic+outlierdummy.alm'></span>

<h3>Description</h3>

<p>Function detects outliers and creates a matrix with dummy variables. Only point
outliers are considered (no level shifts).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>outlierdummy(object, ...)

## Default S3 method:
outlierdummy(object, level = 0.999, type = c("rstandard",
  "rstudent"), ...)

## S3 method for class 'alm'
outlierdummy(object, level = 0.999, type = c("rstandard",
  "rstudent"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="outlierdummy_+3A_object">object</code></td>
<td>
<p>Model estimated using one of the functions of smooth package.</p>
</td></tr>
<tr><td><code id="outlierdummy_+3A_...">...</code></td>
<td>
<p>Other parameters. Not used yet.</p>
</td></tr>
<tr><td><code id="outlierdummy_+3A_level">level</code></td>
<td>
<p>Confidence level to use. Everything that is outside the constructed
bounds based on that is flagged as outliers.</p>
</td></tr>
<tr><td><code id="outlierdummy_+3A_type">type</code></td>
<td>
<p>Type of residuals to use: either standardised or studentised.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The detection is done based on the type of distribution used and confidence level
specified by user.
</p>


<h3>Value</h3>

<p>The class &quot;outlierdummy&quot;, which contains the list:
</p>

<ul>
<li><p> outliers - the matrix with the dummy variables, flagging outliers;
</p>
</li>
<li><p> statistic - the value of the statistic for the normalised variable;
</p>
</li>
<li><p> id - the ids of the outliers (which observations have them);
</p>
</li>
<li><p> level - the confidence level used in the process;
</p>
</li>
<li><p> type - the type of the residuals used.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><a href="stats.html#topic+influence.measures">influence.measures</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate the data with S distribution
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rs(100,0,3),xreg)
colnames(xreg) &lt;- c("y","x1","x2")

# Fit the normal distribution model
ourModel &lt;- alm(y~x1+x2, xreg, distribution="dnorm")

# Detect outliers
xregOutlierDummy &lt;- outlierdummy(ourModel)

</code></pre>

<hr>
<h2 id='pAIC'>Point AIC</h2><span id='topic+pAIC'></span><span id='topic+pAICc'></span><span id='topic+pBIC'></span><span id='topic+pBICc'></span>

<h3>Description</h3>

<p>This function returns a vector of AIC values for the in-sample observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pAIC(object, ...)

pAICc(object, ...)

pBIC(object, ...)

pBICc(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pAIC_+3A_object">object</code></td>
<td>
<p>Time series model.</p>
</td></tr>
<tr><td><code id="pAIC_+3A_...">...</code></td>
<td>
<p>Some stuff.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is based on <a href="#topic+pointLik">pointLik</a> function. The formula for this is:
pAIC_t = 2 * k - 2 * T * l_t ,
where k is the number of parameters, T is the number of observations and l_t is
the point likelihood. This way we preserve the property that AIC = mean(pAIC).
</p>


<h3>Value</h3>

<p>The function returns the vector of point AIC values.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><a href="#topic+pointLik">pointLik</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")
ourModel &lt;- alm(y~x1+x2,as.data.frame(xreg))

pAICValues &lt;- pAIC(ourModel)

mean(pAICValues)
AIC(ourModel)

</code></pre>

<hr>
<h2 id='pcor'>Partial correlations</h2><span id='topic+pcor'></span>

<h3>Description</h3>

<p>Function calculates partial correlations between the provided variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcor(x, y = NULL, use = c("na.or.complete", "complete.obs", "everything",
  "all.obs"), method = c("pearson", "spearman", "kendall"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcor_+3A_x">x</code></td>
<td>
<p>Either data.frame or a matrix with numeric values.</p>
</td></tr>
<tr><td><code id="pcor_+3A_y">y</code></td>
<td>
<p>The numerical variable.</p>
</td></tr>
<tr><td><code id="pcor_+3A_use">use</code></td>
<td>
<p>What observations to use. See <a href="stats.html#topic+cor">cor</a> function for details.
The only option that is not available here is <code>"pairwise.complete.obs"</code>.</p>
</td></tr>
<tr><td><code id="pcor_+3A_method">method</code></td>
<td>
<p>Which method to use for the calculation of the partial correlations.
This can be either Pearson's, Spearman's or Kendall's coefficient. See <a href="stats.html#topic+cor">cor</a>
for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The calculation is done based on multiple linear regressions. The function calculates
them for each pair of variables based on the residuals of linear models of those
variables from the other variables in the dataset.
</p>


<h3>Value</h3>

<p>The following list of values is returned:
</p>

<ul>
<li><p>value - Matrix of the coefficients of partial correlations;
</p>
</li>
<li><p>p.value - The p-values for the parameters;
</p>
</li>
<li><p>method - The method used in the calculations.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mcor">mcor</a>, <a href="#topic+cramer">cramer</a>, <a href="#topic+association">association</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pcor(mtcars)

</code></pre>

<hr>
<h2 id='pinball'>Pinball function</h2><span id='topic+pinball'></span>

<h3>Description</h3>

<p>The function returns the value from the pinball function for the specified level and
the type of loss
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pinball(holdout, forecast, level, loss = 1, na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pinball_+3A_holdout">holdout</code></td>
<td>
<p>The vector or matrix of the holdout values.</p>
</td></tr>
<tr><td><code id="pinball_+3A_forecast">forecast</code></td>
<td>
<p>The forecast of a distribution (e.g. quantile or expectile).
It should be the same length as the holdout.</p>
</td></tr>
<tr><td><code id="pinball_+3A_level">level</code></td>
<td>
<p>The level associated with the forecast (e.g. level of quantile).</p>
</td></tr>
<tr><td><code id="pinball_+3A_loss">loss</code></td>
<td>
<p>The type of loss to use. The number which corresponds to L1, L2 etc.
L1 implies the loss for quantiles, while L2 is for the expectile.</p>
</td></tr>
<tr><td><code id="pinball_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical, defining whether to remove the NAs from the provided data or not.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns the scalar value.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An example with mtcars data
ourModel &lt;- alm(mpg~., mtcars[1:30,], distribution="dnorm")

# Produce predictions with the interval
ourForecast &lt;- predict(ourModel, mtcars[-c(1:30),], interval="p")

# Pinball with the L1 (quantile value)
pinball(mtcars$mpg[-c(1:30)],ourForecast$upper,level=0.975,loss=1)
pinball(mtcars$mpg[-c(1:30)],ourForecast$lower,level=0.025,loss=1)

# Pinball with the L2 (expectile value)
pinball(mtcars$mpg[-c(1:30)],ourForecast$upper,level=0.975,loss=2)
pinball(mtcars$mpg[-c(1:30)],ourForecast$lower,level=0.025,loss=2)

</code></pre>

<hr>
<h2 id='plot.greybox'>Plots of the fit and residuals</h2><span id='topic+plot.greybox'></span><span id='topic+plot.alm'></span>

<h3>Description</h3>

<p>The function produces diagnostics plots for a <code>greybox</code> model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'greybox'
plot(x, which = c(1, 2, 4, 6), level = 0.95,
  legend = FALSE, ask = prod(par("mfcol")) &lt; length(which) &amp;&amp;
  dev.interactive(), lowess = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.greybox_+3A_x">x</code></td>
<td>
<p>Estimated greybox model.</p>
</td></tr>
<tr><td><code id="plot.greybox_+3A_which">which</code></td>
<td>
<p>Which of the plots to produce. The possible options (see details for explanations):
</p>

<ol>
<li><p> Actuals vs Fitted values;
</p>
</li>
<li><p> Standardised residuals vs Fitted;
</p>
</li>
<li><p> Studentised residuals vs Fitted;
</p>
</li>
<li><p> Absolute residuals vs Fitted;
</p>
</li>
<li><p> Squared residuals vs Fitted;
</p>
</li>
<li><p> Q-Q plot with the specified distribution;
</p>
</li>
<li><p> Fitted over time;
</p>
</li>
<li><p> Standardised residuals vs Time;
</p>
</li>
<li><p> Studentised residuals vs Time;
</p>
</li>
<li><p> ACF of the residuals;
</p>
</li>
<li><p> PACF of the residuals;
</p>
</li>
<li><p> Cook's distance over time with 0.5, 0.75 and 0.95 quantile lines from Fisher's distribution;
</p>
</li>
<li><p> Absolute standardised residuals vs Fitted;
</p>
</li>
<li><p> Squared standardised residuals vs Fitted;
</p>
</li>
<li><p> ACF of the squared residuals;
</p>
</li>
<li><p> PACF of the squared residuals.
</p>
</li></ol>
</td></tr>
<tr><td><code id="plot.greybox_+3A_level">level</code></td>
<td>
<p>Confidence level. Defines width of confidence interval. Used in plots (2), (3), (7),
(8), (9), (10) and (11).</p>
</td></tr>
<tr><td><code id="plot.greybox_+3A_legend">legend</code></td>
<td>
<p>If <code>TRUE</code>, then the legend is produced on plots (2), (3) and (7).</p>
</td></tr>
<tr><td><code id="plot.greybox_+3A_ask">ask</code></td>
<td>
<p>Logical; if <code>TRUE</code>, the user is asked to press Enter before each plot.</p>
</td></tr>
<tr><td><code id="plot.greybox_+3A_lowess">lowess</code></td>
<td>
<p>Logical; if <code>TRUE</code>, LOWESS lines are drawn on scatterplots, see <a href="stats.html#topic+lowess">lowess</a>.</p>
</td></tr>
<tr><td><code id="plot.greybox_+3A_...">...</code></td>
<td>
<p>The parameters passed to the plot functions. Recommended to use with separate plots.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The list of produced plots includes:
</p>

<ol>
<li><p> Actuals vs Fitted values. Allows analysing, whether there are any issues in the fit.
Does the variability of actuals increase with the increase of fitted values? Is the relation
well captured? They grey line on the plot corresponds to the perfect fit of the model.
</p>
</li>
<li><p> Standardised residuals vs Fitted. Plots the points and the confidence bounds
(red lines) for the specified confidence <code>level</code>. Useful for the analysis of outliers;
</p>
</li>
<li><p> Studentised residuals vs Fitted. This is similar to the previous plot, but with the
residuals divided by the scales with the leave-one-out approach. Should be more sensitive
to outliers;
</p>
</li>
<li><p> Absolute residuals vs Fitted. Useful for the analysis of heteroscedasticity;
</p>
</li>
<li><p> Squared residuals vs Fitted - similar to (3), but with squared values;
</p>
</li>
<li><p> Q-Q plot with the specified distribution. Can be used in order to see if the
residuals follow the assumed distribution. The type of distribution depends on the one used
in the estimation (see <code>distribution</code> parameter in <a href="#topic+alm">alm</a>);
</p>
</li>
<li><p> Fitted over time. Plots actuals (black line), fitted values (purple line) and
prediction interval (red lines) of width <code>level</code>, but only in the case, when there
are some values lying outside of it. Can be used in order to make sure that the model
did not miss any important events over time;
</p>
</li>
<li><p> Standardised residuals vs Time. Useful if you want to see, if there is autocorrelation or
if there is heteroscedasticity in time. This also shows, when the outliers happen;
</p>
</li>
<li><p> Studentised residuals vs Time. Similar to previous, but with studentised residuals;
</p>
</li>
<li><p> ACF of the residuals. Are the residuals autocorrelated? See <a href="stats.html#topic+acf">acf</a> for
details;
</p>
</li>
<li><p> PACF of the residuals. No, really, are they autocorrelated? See <a href="stats.html#topic+pacf">pacf</a>
for details;
</p>
</li>
<li><p> Cook's distance over time. Shows influential observations. 0.5, 0.75 and 0.95 quantile
lines from Fisher's distribution are also plotted. If the value is above them then the
observation is influencial. This does not work well for non-normal distributions;
</p>
</li>
<li><p> Absolute standardised residuals vs Fitted. Similar to the previous, but with absolute
values. This is more relevant to the models where scale is calculated as an absolute value of
something (e.g. Laplace);
</p>
</li>
<li><p> Squared standardised residuals vs Fitted. This is an additional plot needed to diagnose
heteroscedasticity in a model with varying scale. The variance on this plot will be constant if
the adequate model for <code>scale</code> was constructed. This is more appropriate for normal and
the related distributions.
</p>
</li></ol>

<p>Which of the plots to produce, is specified via the <code>which</code> parameter. The plots 2, 3, 7,
8 and 9 also use the parameters <code>level</code>, which specifies the confidence level for
the intervals.
</p>


<h3>Value</h3>

<p>The function produces the number of plots, specified in the parameter <code>which</code>.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><a href="stats.html#topic+plot.lm">plot.lm</a>, <a href="stats.html#topic+rstandard">rstandard</a>, <a href="stats.html#topic+rstudent">rstudent</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rlaplace(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rlaplace(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")

ourModel &lt;- alm(y~x1+x2, xreg, distribution="dnorm")

par(mfcol=c(4,4))
plot(ourModel, c(1:14))

</code></pre>

<hr>
<h2 id='pointLik'>Point likelihood values</h2><span id='topic+pointLik'></span>

<h3>Description</h3>

<p>This function returns a vector of logarithms of likelihoods for each observation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pointLik(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pointLik_+3A_object">object</code></td>
<td>
<p>Time series model.</p>
</td></tr>
<tr><td><code id="pointLik_+3A_...">...</code></td>
<td>
<p>Some stuff.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of taking the expected log-likelihood for the whole series, this function
calculates the individual value for each separate observation. Note that these
values are biased, so you would possibly need to take number of degrees of freedom
into account in order to have an unbiased estimator.
</p>
<p>This value is based on the general likelihood (not its concentrated version), so
the sum of these values may slightly differ from the output of logLik.
</p>


<h3>Value</h3>

<p>This function returns a vector.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><a href="stats.html#topic+AIC">AIC</a>, <a href="stats.html#topic+BIC">BIC</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")
ourModel &lt;- alm(y~x1+x2,as.data.frame(xreg))

pointLik(ourModel)

# Bias correction
pointLik(ourModel) - nparam(ourModel)

# Bias correction in AIC style
2*(nparam(ourModel)/nobs(ourModel) - pointLik(ourModel))

# BIC calculation based on pointLik
log(nobs(ourModel))*nparam(ourModel) - 2*sum(pointLik(ourModel))

</code></pre>

<hr>
<h2 id='polyprod'>This function calculates parameters for the polynomials</h2><span id='topic+polyprod'></span>

<h3>Description</h3>

<p>The function accepts two vectors with the parameters for the polynomials and returns
the vector of parameters after their multiplication. This can be especially useful,
when working with ARIMA models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polyprod(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="polyprod_+3A_x">x</code></td>
<td>
<p>The vector of parameters of the first polynomial.</p>
</td></tr>
<tr><td><code id="polyprod_+3A_y">y</code></td>
<td>
<p>The vector of parameters of the second polynomial.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a matrix with one column with the parameters for
the polynomial, starting from the 0-order.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><a href="stats.html#topic+convolve">convolve</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: polyprod(c(1,-2,-1),c(1,0.5,0.3))

</code></pre>

<hr>
<h2 id='predict.alm'>Forecasting using greybox functions</h2><span id='topic+predict.alm'></span><span id='topic+predict.greybox'></span><span id='topic+forecast.greybox'></span><span id='topic+forecast.alm'></span><span id='topic+predict.scale'></span>

<h3>Description</h3>

<p>The functions allow producing forecasts based on the provided model and newdata.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'alm'
predict(object, newdata = NULL, interval = c("none",
  "confidence", "prediction"), level = 0.95, side = c("both", "upper",
  "lower"), occurrence = NULL, ...)

## S3 method for class 'greybox'
predict(object, newdata = NULL, interval = c("none",
  "confidence", "prediction"), level = 0.95, side = c("both", "upper",
  "lower"), ...)

## S3 method for class 'scale'
predict(object, newdata = NULL, interval = c("none",
  "confidence", "prediction"), level = 0.95, side = c("both", "upper",
  "lower"), ...)

## S3 method for class 'greybox'
forecast(object, newdata = NULL, h = NULL, ...)

## S3 method for class 'alm'
forecast(object, newdata = NULL, h = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.alm_+3A_object">object</code></td>
<td>
<p>Time series model for which forecasts are required.</p>
</td></tr>
<tr><td><code id="predict.alm_+3A_newdata">newdata</code></td>
<td>
<p>The new data needed in order to produce forecasts.</p>
</td></tr>
<tr><td><code id="predict.alm_+3A_interval">interval</code></td>
<td>
<p>Type of intervals to construct: either &quot;confidence&quot; or
&quot;prediction&quot;. Can be abbreviated</p>
</td></tr>
<tr><td><code id="predict.alm_+3A_level">level</code></td>
<td>
<p>Confidence level. Defines width of prediction interval.</p>
</td></tr>
<tr><td><code id="predict.alm_+3A_side">side</code></td>
<td>
<p>What type of interval to produce: <code>"both"</code> - produces both
lower and upper bounds of the interval, <code>"upper"</code> - upper only, <code>"lower"</code>
- respectively lower only. In the <code>"both"</code> case the probability is split into
two parts: ((1-level)/2, (1+level)/2). When <code>"upper"</code> is specified, then
the intervals for (0, level) are constructed Finally, with <code>"lower"</code> the interval
for (1-level, 1) is returned.</p>
</td></tr>
<tr><td><code id="predict.alm_+3A_occurrence">occurrence</code></td>
<td>
<p>If occurrence was provided, then a user can provide a vector of future
values via this variable.</p>
</td></tr>
<tr><td><code id="predict.alm_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>vcov</code> function (see <a href="#topic+coef.alm">coef.alm</a>
for details).</p>
</td></tr>
<tr><td><code id="predict.alm_+3A_h">h</code></td>
<td>
<p>The forecast horizon.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predict</code> produces predictions for the provided model and <code>newdata</code>. If
<code>newdata</code> is not provided, then the data from the model is extracted and the
fitted values are reproduced. This might be useful when confidence / prediction
intervals are needed for the in-sample values.
</p>
<p><code>forecast</code> function produces forecasts for <code>h</code> steps ahead. There are four
scenarios in this function:
</p>

<ol>
<li><p> If the <code>newdata</code> is  not provided, then it will produce forecasts of the
explanatory variables to the horizon <code>h</code> (using <code>es</code> from smooth package
or using Naive if <code>smooth</code> is not installed) and use them as <code>newdata</code>.
</p>
</li>
<li><p> If <code>h</code> and <code>newdata</code> are provided, then the number of rows to use
will be regulated by <code>h</code>.
</p>
</li>
<li><p> If <code>h</code> is <code>NULL</code>, then it is set equal to the number of rows in
<code>newdata</code>.
</p>
</li>
<li><p> If both <code>h</code> and <code>newdata</code> are not provided, then it will use the
data from the model itself, reproducing the fitted values.
</p>
</li></ol>

<p>After forming the <code>newdata</code> the <code>forecast</code> function calls for
<code>predict</code>, so you can provide parameters <code>interval</code>, <code>level</code> and
<code>side</code> in the call for <code>forecast</code>.
</p>


<h3>Value</h3>

<p><code>predict.greybox()</code> returns object of class &quot;predict.greybox&quot;,
which contains:
</p>

<ul>
<li> <p><code>model</code> - the estimated model.
</p>
</li>
<li> <p><code>mean</code> - the expected values.
</p>
</li>
<li> <p><code>fitted</code> - fitted values of the model.
</p>
</li>
<li> <p><code>lower</code> - lower bound of prediction / confidence intervals.
</p>
</li>
<li> <p><code>upper</code> - upper bound of prediction / confidence intervals.
</p>
</li>
<li> <p><code>level</code> - confidence level.
</p>
</li>
<li> <p><code>newdata</code> - the data provided in the call to the function.
</p>
</li>
<li> <p><code>variances</code> - conditional variance for the holdout sample.
In case of <code>interval="prediction"</code> includes variance of the error.
</p>
</li></ul>

<p><code>predict.alm()</code> is based on <code>predict.greybox()</code> and returns
object of class &quot;predict.alm&quot;, which in addition contains:
</p>

<ul>
<li> <p><code>location</code> - the location parameter of the distribution.
</p>
</li>
<li> <p><code>scale</code> - the scale parameter of the distribution.
</p>
</li>
<li> <p><code>distribution</code> - name of the fitted distribution.
</p>
</li></ul>

<p><code>forecast()</code> functions return the same &quot;predict.alm&quot; and
&quot;predict.greybox&quot; classes, with the same set of output variables.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><a href="stats.html#topic+predict.lm">predict.lm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rlaplace(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rlaplace(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")
inSample &lt;- xreg[1:80,]
outSample &lt;- xreg[-c(1:80),]

ourModel &lt;- alm(y~x1+x2, inSample, distribution="dlaplace")

predict(ourModel,outSample)
predict(ourModel,outSample,interval="c")

plot(predict(ourModel,outSample,interval="p"))
plot(forecast(ourModel,h=10,interval="p"))

</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+xtable'></span><span id='topic+forecast'></span><span id='topic+accuracy'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+accuracy">accuracy</a></code>, <code><a href="generics.html#topic+forecast">forecast</a></code></p>
</dd>
<dt>xtable</dt><dd><p><code><a href="xtable.html#topic+xtable">xtable</a></code></p>
</dd>
</dl>

<hr>
<h2 id='rmcb'>Regression for Multiple Comparison with the Best</h2><span id='topic+rmcb'></span><span id='topic+plot.rmcb'></span>

<h3>Description</h3>

<p>RMCB stands for &quot;Regression for Multiple Comparison with the Best&quot;, referring to the
comparison of forecasting methods. This is a regression-based version of the
Nemenyi / MCB test relies on the ranks of variables.
This test is based on Nemenyi / MCB test (Demsar, 2006). It transforms the data into
ranks and then constructs a regression on them of the type:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmcb(data, level = 0.95, outplot = c("mcb", "lines", "none"),
  select = NULL, ...)

## S3 method for class 'rmcb'
plot(x, outplot = c("mcb", "lines"), select = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmcb_+3A_data">data</code></td>
<td>
<p>Matrix or data frame with observations in rows and variables in
columns.</p>
</td></tr>
<tr><td><code id="rmcb_+3A_level">level</code></td>
<td>
<p>The width of the confidence interval. Default is 0.95.</p>
</td></tr>
<tr><td><code id="rmcb_+3A_outplot">outplot</code></td>
<td>
<p>What type of plot to use after the calculations. This can be
either &quot;MCB&quot; (<code>"mcb"</code>), or &quot;Vertical lines&quot; (<code>"lines"</code>), or nothing
(<code>"none"</code>). You can also use plot method on the produced object in order
to get the same effect.</p>
</td></tr>
<tr><td><code id="rmcb_+3A_select">select</code></td>
<td>
<p>What column of data to highlight on the plot. If NULL, then
the method with the lowest value is selected.</p>
</td></tr>
<tr><td><code id="rmcb_+3A_...">...</code></td>
<td>
<p>Other parameters passed to <a href="base.html#topic+rank">rank</a> function.</p>
</td></tr>
<tr><td><code id="rmcb_+3A_x">x</code></td>
<td>
<p>The produced rmcb model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>y = b' X + e,
</p>
<p>where y is the vector of the ranks of provided data (as.vector(data)), X is the matrix
of dummy variables for each column of the data (forecasting method), b is the
vector of coefficients for the dummies and e is the error term of the model. Given
that the data is ranked, it test the differences in medians between the methods and
then produces plots based on that.
</p>
<p>There is also a <code>plot()</code> method that allows producing either &quot;mcb&quot; or &quot;lines&quot;
style of plot. This can be regulated via <code>plot(x, outplot="lines")</code>.
</p>


<h3>Value</h3>

<p>If <code>outplot!="none"</code>, then the function plots the results after all
the calculations using plot.rmcb() function.
</p>
<p>Function returns a list of a class &quot;rmcb&quot;, which contains the following
variables:
</p>

<ul>
<li><p>meanMean values for each method.
</p>
</li>
<li><p>intervalConfidence intervals for each method.
</p>
</li>
<li><p>vlinesCoordinates used for outplot=&quot;l&quot;, marking the groups of methods.
</p>
</li>
<li><p>groupsThe table containing the groups. <code>TRUE</code> - methods are in the
same group, <code>FALSE</code> - they are not.
</p>
</li>
<li><p>methodsSimilar to <code>group</code> parameter, but with a slightly different
presentation.
</p>
</li>
<li><p>p.valuep-value for the test of the significance of the model. This is the
value from the F test of the linear regression.
</p>
</li>
<li><p>levelConfidence level.
</p>
</li>
<li><p>modellm model produced for the calculation of the intervals.
</p>
</li>
<li><p>outplotStyle of the plot to produce.
</p>
</li>
<li><p>selectThe selected variable to highlight.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p>  Demsar, J. (2006). Statistical Comparisons of Classifiers over
Multiple Data Sets. Journal of Machine Learning Research, 7, 1-30.
<a href="https://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf">https://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 50
M &lt;- 4
ourData &lt;- matrix(rnorm(N*M,mean=0,sd=1), N, M)
ourData[,2] &lt;- ourData[,2]+4
ourData[,3] &lt;- ourData[,3]+3
ourData[,4] &lt;- ourData[,4]+2
colnames(ourData) &lt;- c("Method A","Method B","Method C - long name","Method D")
ourTest &lt;- rmcb(ourData, level=0.95)

# See the mean ranks:
ourTest$mean
# The same is for the intervals:
ourTest$interval

# You can also reproduce plots in different styles:
plot(ourTest, outplot="lines")

# Or you can use the default "mcb" style and set additional parameters for the plot():
par(mar=c(2,2,4,0)+0.1)
plot(ourTest, main="Four methods")

</code></pre>

<hr>
<h2 id='ro'>Rolling Origin</h2><span id='topic+ro'></span>

<h3>Description</h3>

<p>The function does rolling origin for any forecasting function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ro(data, h = 10, origins = 10, call, value = NULL, ci = FALSE,
  co = TRUE, silent = TRUE, parallel = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ro_+3A_data">data</code></td>
<td>
<p>Data vector or ts object with the response variable passed to the
function.</p>
</td></tr>
<tr><td><code id="ro_+3A_h">h</code></td>
<td>
<p>The forecasting horizon.</p>
</td></tr>
<tr><td><code id="ro_+3A_origins">origins</code></td>
<td>
<p>The number of rolling origins.</p>
</td></tr>
<tr><td><code id="ro_+3A_call">call</code></td>
<td>
<p>The call that is passed to the function. The call must be in quotes.
Example: <code>"forecast(ets(data),h)"</code>. Here <code>data</code> shows where the data is
and <code>h</code> defines where the horizon should be passed in the <code>call</code>. Some
hidden parameters can also be specified in the call. For example, parameters
<code>counti</code>, <code>counto</code> and <code>countf</code> are used in the inner loop
and can be used for the regulation of exogenous variables sizes. See examples
for the details.</p>
</td></tr>
<tr><td><code id="ro_+3A_value">value</code></td>
<td>
<p>The variable or set of variables returned by the <code>call</code>.
For example, <code>mean</code> for functions of <code>forecast</code> package. This can
also be a vector of variables. See examples for the details. If the parameter
is <code>NULL</code>, then all the values from the call are returned (could be really
messy!). Note that if your function returns a list with matrices, then ro will
return an array. If your function returns a list, then you will have a list of
lists in the end. So it makes sense to understand what you want to get before
running the function.</p>
</td></tr>
<tr><td><code id="ro_+3A_ci">ci</code></td>
<td>
<p>The parameter defines if the in-sample window size should be constant.
If <code>TRUE</code>, then with each origin one observation is added at the end of
series and another one is removed from the beginning.</p>
</td></tr>
<tr><td><code id="ro_+3A_co">co</code></td>
<td>
<p>The parameter defines whether the holdout sample window size should
be constant. If <code>TRUE</code>, the rolling origin will stop when less than
<code>h</code> observations are left in the holdout.</p>
</td></tr>
<tr><td><code id="ro_+3A_silent">silent</code></td>
<td>
<p>If <code>TRUE</code>, nothing is printed out in the console.</p>
</td></tr>
<tr><td><code id="ro_+3A_parallel">parallel</code></td>
<td>
<p>If <code>TRUE</code>, then the model fitting is done in parallel.
WARNING! Packages <code>foreach</code> and either <code>doMC</code> (Linux and Mac only)
or <code>doParallel</code> are needed in order to run the function in parallel.</p>
</td></tr>
<tr><td><code id="ro_+3A_...">...</code></td>
<td>
<p>This is temporary and is needed in order to capture &quot;silent&quot;
parameter if it is provided.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function produces rolling origin forecasts using the <code>data</code> and a
<code>call</code> passed as parameters. The function can do all of that either in
serial or in parallel, but it needs <code>foreach</code> and either <code>doMC</code>
(Linux only) or <code>doParallel</code> packages installed in order to do the latter.
</p>
<p>This is a dangerous function, so be careful with the call that you pass to
it, and make sure that it is well formulated before the execution. Also, do not
forget to provide the value that needs to be returned or you might end up with
very messy results.
</p>
<p>For more details and more examples of usage, please see vignette for the function.
In order to do that, just run the command: <code>vignette("ro","greybox")</code>
</p>


<h3>Value</h3>

<p>Function returns the following variables:
</p>

<ul>
<li><p><code>actuals</code> - the data provided to the function.
</p>
</li>
<li><p><code>holdout</code> - the matrix of actual values corresponding to the
produced forecasts from each origin.
</p>
</li>
<li><p><code>value</code> - the matrices / array / lists with the produced data
from each origin. Name of each object corresponds to the names in the
parameter <code>value</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Yves Sagaert
</p>
<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Tashman, (2000) Out-of-sample tests of forecasting accuracy:
an analysis and review International Journal of Forecasting, 16,
pp. 437-450. <a href="https://doi.org/10.1016/S0169-2070%2800%2900065-0">doi:10.1016/S0169-2070(00)00065-0</a>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rnorm(100,0,1)
ourCall &lt;- "predict(arima(x=data,order=c(0,1,1)),n.ahead=h)"
# NOTE that the "data" needs to be used in the call, not "y".
# This way we tell the function, where "y" should be used in the call of the function.

# The default call and values
ourValue &lt;- "pred"
ourRO &lt;- ro(y, h=5, origins=5, ourCall, ourValue)

# We can now plot the results of this evaluation:
plot(ourRO)

# You can also use dolar sign
ourValue &lt;- "$pred"
# And you can have constant in-sample size
ro(y, h=5, origins=5, ourCall, ourValue, ci=TRUE)

# You can ask for several values
ourValue &lt;- c("pred","se")
# And you can have constant holdout size
ro(y, h=5, origins=20, ourCall, ourValue, ci=TRUE, co=TRUE)

#### The following code will give exactly the same result as above,
#### but computed in parallel using all but 1 core of CPU:
## Not run: ro(y, h=5, origins=20, ourCall, ourValue, ci=TRUE, co=TRUE, parallel=TRUE)

#### If you want to use functions from forecast package, please note that you need to
#### set the values that need to be returned explicitly. There are two options for this.
# Example 1:
## Not run: ourCall &lt;- "forecast(ets(data), h=h, level=95)"
ourValue &lt;- c("mean", "lower", "upper")
ro(y,h=5,origins=5,ourCall,ourValue)
## End(Not run)

# Example 2:
## Not run: ourCall &lt;- "forecast(ets(data), h=h, level=c(80,95))"
ourValue &lt;- c("mean", "lower[,1]", "upper[,1]", "lower[,2]", "upper[,2]")
ro(y,h=5,origins=5,ourCall,ourValue)
## End(Not run)

#### A more complicated example using the for loop and
#### several time series
x &lt;- matrix(rnorm(120*3,0,1), 120, 3)

## Form an array for the forecasts we will produce
## We will have 4 origins with 6-steps ahead forecasts
ourForecasts &lt;- array(NA,c(6,4,3))

## Define models that need to be used for each series
ourModels &lt;- list(c(0,1,1), c(0,0,1), c(0,1,0))

## This call uses specific models for each time series
ourCall &lt;- "predict(arima(data, order=ourModels[[i]]), n.ahead=h)"
ourValue &lt;- "pred"

## Start the loop. The important thing here is to use the same variable 'i' as in ourCall.
for(i in 1:3){
    ourData &lt;- x[,i]
    ourForecasts[,,i] &lt;- ro(data=ourData,h=6,origins=4,call=ourCall,
                            value=ourValue,co=TRUE,silent=TRUE)$pred
}

## ourForecasts array now contains rolling origin forecasts from specific
## models.

##### An example with exogenous variables
x &lt;- rnorm(100,0,1)
xreg &lt;- matrix(rnorm(200,0,1),100,2,dimnames=list(NULL,c("x1","x2")))

## 'counti' is used to define in-sample size of xreg,
## 'counto' - the size of the holdout sample of xreg

ourCall &lt;- "predict(arima(x=data, order=c(0,1,1), xreg=xreg[counti,,drop=FALSE]),
            n.ahead=h, newxreg=xreg[counto,,drop=FALSE])"
ourValue &lt;- "pred"
ro(x,h=5,origins=5,ourCall,ourValue)

##### Poisson regression with alm
x &lt;- rpois(100,2)
xreg &lt;- cbind(x,matrix(rnorm(200,0,1),100,2,dimnames=list(NULL,c("x1","x2"))))
ourCall &lt;- "predict(alm(x~., data=xreg[counti,,drop=FALSE], distribution='dpois'),
                    newdata=xreg[counto,,drop=FALSE])"
ourValue &lt;- "mean"
testRO &lt;- ro(xreg[,1],h=5,origins=5,ourCall,ourValue,co=TRUE)
plot(testRO)

## 'countf' is used to take xreg of the size corresponding to the whole
## sample on each iteration
## This is useful when working with functions from smooth package.
## The following call will return the forecasts from es() function of smooth.
## Not run: ourCall &lt;- "es(data=data, h=h, xreg=xreg[countf,,drop=FALSE])"
ourValue &lt;- "forecast"
ro(x,h=5,origins=5,ourCall,ourValue)
## End(Not run)

</code></pre>

<hr>
<h2 id='sm'>Scale Model</h2><span id='topic+sm'></span><span id='topic+sm.default'></span><span id='topic+sm.lm'></span><span id='topic+sm.alm'></span>

<h3>Description</h3>

<p>This method produces a model for scale of distribution for the provided pre-estimated model.
The model can be estimated either via <code>lm</code> or <code>alm</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sm(object, ...)

## Default S3 method:
sm(object, formula = NULL, data = NULL,
  parameters = NULL, ...)

## S3 method for class 'lm'
sm(object, formula = NULL, data = NULL, parameters = NULL,
  ...)

## S3 method for class 'alm'
sm(object, formula = NULL, data = NULL, parameters = NULL,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sm_+3A_object">object</code></td>
<td>
<p>The pre-estimated <code>alm</code> or <code>lm</code> model.</p>
</td></tr>
<tr><td><code id="sm_+3A_...">...</code></td>
<td>
<p>Other parameters to pass to the method, including those explained in
<a href="#topic+alm">alm</a> (e.g. parameters for optimiser).</p>
</td></tr>
<tr><td><code id="sm_+3A_formula">formula</code></td>
<td>
<p>The formula for scale. It should start with ~ and contain all variables
that should impact the scale.</p>
</td></tr>
<tr><td><code id="sm_+3A_data">data</code></td>
<td>
<p>The data, on which the scale model needs to be estimated. If not provided,
then the one used in the <code>object</code> is used.</p>
</td></tr>
<tr><td><code id="sm_+3A_parameters">parameters</code></td>
<td>
<p>The parameters to use in the model. Only needed if you know the parameters
in advance or want to test yours.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is useful, when you suspect a heteroscedasticity in your model and want to
fit a model for the scale of the pre-specified distribution. This function is complementary
for <code>lm</code> or <code>alm</code>.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+sqrt(exp(0.8+0.2*xreg[,1]))*rnorm(100,0,1),
              xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")

# Estimate the location model
ourModel &lt;- alm(y~.,xreg)
# Estimate the scale model
ourScale &lt;- sm(ourModel,formula=~x1+x2)
# Summary of the scale model
summary(ourScale)

</code></pre>

<hr>
<h2 id='spread'>Construct scatterplot / boxplots for the data</h2><span id='topic+spread'></span>

<h3>Description</h3>

<p>Function constructs the plots depending on the types of variables in the provided
matrix / data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spread(data, histograms = FALSE, log = FALSE, lowess = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spread_+3A_data">data</code></td>
<td>
<p>Either matrix or data frame with the data.</p>
</td></tr>
<tr><td><code id="spread_+3A_histograms">histograms</code></td>
<td>
<p>If <code>TRUE</code>, then the histograms and barplots are produced on
the diagonal of the matrix. Otherwise the names of the variables are written there.</p>
</td></tr>
<tr><td><code id="spread_+3A_log">log</code></td>
<td>
<p>If <code>TRUE</code>, then the logarithms of all numerical variables are taken.</p>
</td></tr>
<tr><td><code id="spread_+3A_lowess">lowess</code></td>
<td>
<p>If <code>TRUE</code>, then LOWESS lines are added to scatterplots and means
are connected with lines on boxplots, see <a href="stats.html#topic+lowess">lowess</a> for details.</p>
</td></tr>
<tr><td><code id="spread_+3A_...">...</code></td>
<td>
<p>Other parameters passed to the plot function. Currently only &quot;main&quot;
parameter is accepted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If both variables are in metric scale, then the classical scatterplot is constructed.
If one of them is either integer (up to 10 values) or categorical (aka 'factor'),
then boxplots (with grey dots corresponding to mean values) are constructed. Finally,
for the two categorical variables the tableplot is returned (see
<a href="#topic+tableplot">tableplot</a> function for the details). All of this is packed in a matrix.
</p>
<p>See details in the vignette &quot;Marketing analytics with greybox&quot;:
<code>vignette("maUsingGreybox","greybox")</code>
</p>


<h3>Value</h3>

<p>Function does not return anything. It just plots things.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+plot">plot</a>, <a href="base.html#topic+table">table</a>, <a href="#topic+tableplot">tableplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Simple example
spread(mtcars)
spread(mtcars,log=TRUE)

</code></pre>

<hr>
<h2 id='stepwise'>Stepwise selection of regressors</h2><span id='topic+stepwise'></span>

<h3>Description</h3>

<p>Function selects variables that give linear regression with the lowest
information criteria. The selection is done stepwise (forward) based on
partial correlations. This should be a simpler and faster implementation
than step() function from &lsquo;stats&rsquo; package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stepwise(data, ic = c("AICc", "AIC", "BIC", "BICc"), silent = TRUE,
  df = NULL, formula = NULL, subset = NULL, method = c("pearson",
  "kendall", "spearman"), distribution = c("dnorm", "dlaplace", "ds",
  "dgnorm", "dlogis", "dt", "dalaplace", "dlnorm", "dllaplace", "dls",
  "dlgnorm", "dbcnorm", "dfnorm", "dinvgauss", "dgamma", "dpois", "dnbinom",
  "dlogitnorm", "plogis", "pnorm"), occurrence = c("none", "plogis",
  "pnorm"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stepwise_+3A_data">data</code></td>
<td>
<p>Data frame containing dependant variable in the first column and
the others in the rest.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_ic">ic</code></td>
<td>
<p>Information criterion to use.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_silent">silent</code></td>
<td>
<p>If <code>silent=FALSE</code>, then nothing is silent, everything is
printed out. <code>silent=TRUE</code> means that nothing is produced.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_df">df</code></td>
<td>
<p>Number of degrees of freedom to add (should be used if stepwise is
used on residuals).</p>
</td></tr>
<tr><td><code id="stepwise_+3A_formula">formula</code></td>
<td>
<p>If provided, then the selection will be done from the listed
variables in the formula after all the necessary transformations.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_method">method</code></td>
<td>
<p>Method of correlations calculation. The default is Pearson's
correlation, which should be applicable to a wide range of data in different scales.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_distribution">distribution</code></td>
<td>
<p>Distribution to pass to <code>alm()</code>. See <a href="#topic+alm">alm</a>
for details.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_occurrence">occurrence</code></td>
<td>
<p>what distribution to use for occurrence part. See
<a href="#topic+alm">alm</a> for details.</p>
</td></tr>
<tr><td><code id="stepwise_+3A_...">...</code></td>
<td>
<p>This is temporary and is needed in order to capture &quot;silent&quot;
parameter if it is provided.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm uses alm() to fit different models and cor() to select the next
regressor in the sequence.
</p>
<p>Some details and examples of application are also given in the vignette
&quot;Greybox&quot;: <code>vignette("greybox","greybox")</code>
</p>


<h3>Value</h3>

<p>Function returns <code>model</code> - the final model of the class &quot;alm&quot;.
See <a href="#topic+alm">alm</a> for details of the output.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li><p> Burnham Kenneth P. and Anderson David R. (2002). Model Selection
and Multimodel Inference. A Practical Information-Theoretic Approach.
Springer-Verlag New York. DOI: [10.1007/b97636](http://dx.doi.org/10.1007/b97636).
</p>
</li>
<li><p> McQuarrie, A. D. (1999). A small-sample correction for the Schwarz SIC model
selection criterion. Statistics &amp; Probability Letters, 44(1), 79–86.
[10.1016/S0167-7152(98)00294-6](https://doi.org/10.1016/S0167-7152(98)00294-6).
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="stats.html#topic+step">step</a>, <a href="#topic+xregExpander">xregExpander</a>,
<a href="#topic+lmCombine">lmCombine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Simple example
xreg &lt;- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y","x1","x2","Noise")
stepwise(xreg)

### Mixture distribution of Log Normal and Cumulative Logit
xreg[,1] &lt;- xreg[,1] * round(exp(xreg[,1]-70) / (1 + exp(xreg[,1]-70)),0)
colnames(xreg) &lt;- c("y","x1","x2","Noise")
ourModel &lt;- stepwise(xreg, distribution="dlnorm",
                     occurrence=stepwise(xreg, distribution="plogis"))
summary(ourModel)

### Fat regression example
xreg &lt;- matrix(rnorm(20000,10,3),100,200)
xreg &lt;- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) &lt;- c("y",paste0("x",c(1:200)),"Noise")
ourModel &lt;- stepwise(xreg,ic="AICc")
plot(ourModel$ICs,type="l",ylim=range(min(ourModel$ICs),max(ourModel$ICs)+5))
points(ourModel$ICs)
text(c(1:length(ourModel$ICs))+0.1,ourModel$ICs+5,names(ourModel$ICs))

</code></pre>

<hr>
<h2 id='tableplot'>Construct a plot for categorical variable</h2><span id='topic+tableplot'></span>

<h3>Description</h3>

<p>Function constructs a plot for two categorical variables based on table function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tableplot(x, y = NULL, labels = TRUE, legend = FALSE, points = TRUE,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tableplot_+3A_x">x</code></td>
<td>
<p>First categorical variable. Can be either vector, factor, matrix or a data
frame. If <code>y</code> is NULL and x is either matrix of a data frame, then the first two
variables of the data will be plotted against each other.</p>
</td></tr>
<tr><td><code id="tableplot_+3A_y">y</code></td>
<td>
<p>Second categorical variable. If not provided, then only <code>x</code> will be
plotted.</p>
</td></tr>
<tr><td><code id="tableplot_+3A_labels">labels</code></td>
<td>
<p>Whether to print table labels inside the plot or not.</p>
</td></tr>
<tr><td><code id="tableplot_+3A_legend">legend</code></td>
<td>
<p>If <code>TRUE</code>, then the legend for the tableplot is drawn. The plot is
then produced on a separate canvas (new <code>par()</code>).</p>
</td></tr>
<tr><td><code id="tableplot_+3A_points">points</code></td>
<td>
<p>Whether to plot points in the areas. They help in understanding how
many values lie in specific categories.</p>
</td></tr>
<tr><td><code id="tableplot_+3A_...">...</code></td>
<td>
<p>Other parameters passed to the plot function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function produces the plot of the <code>table()</code> function with colour densities
corresponding to the respective frequencies of appearance. If the value appears more
often than the other (e.g. 0.5 vs 0.15), then it will be darker. The frequency of 0
corresponds to the white colour, the frequency of 1 corresponds to the black.
</p>
<p>See details in the vignette &quot;Marketing analytics with greybox&quot;:
<code>vignette("maUsingGreybox","greybox")</code>
</p>


<h3>Value</h3>

<p>Function does not return anything. It just plots things.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+plot">plot</a>, <a href="base.html#topic+table">table</a>, <a href="#topic+spread">spread</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tableplot(mtcars$am, mtcars$gear)

</code></pre>

<hr>
<h2 id='temporaldummy'>Dummy variables for provided seasonality type</h2><span id='topic+temporaldummy'></span><span id='topic+temporaldummy.default'></span><span id='topic+temporaldummy.ts'></span><span id='topic+temporaldummy.Date'></span><span id='topic+temporaldummy.POSIXt'></span><span id='topic+temporaldummy.zoo'></span>

<h3>Description</h3>

<p>Function generates the matrix of dummy variables for the months / weeks / days /
hours / minutes / seconds of year / month / week / day / hour / minute.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>temporaldummy(object, ...)

## Default S3 method:
temporaldummy(object, type = c("month", "quarter", "week",
  "day", "hour", "halfhour", "minute", "second"), of = c("year", "quarter",
  "month", "week", "day", "hour", "minute"), factors = FALSE, h = 0, ...)

## S3 method for class 'ts'
temporaldummy(object, type = c("month", "quarter", "week",
  "day", "hour", "halfhour", "minute", "second"), of = c("year", "quarter",
  "month", "week", "day", "hour", "minute"), factors = FALSE, h = 0, ...)

## S3 method for class 'Date'
temporaldummy(object, type = c("month", "quarter", "week",
  "day", "hour", "halfhour", "minute", "second"), of = c("year", "quarter",
  "month", "week", "day", "hour", "minute"), factors = FALSE, h = 0, ...)

## S3 method for class 'POSIXt'
temporaldummy(object, type = c("month", "quarter", "week",
  "day", "hour", "halfhour", "minute", "second"), of = c("year", "quarter",
  "month", "week", "day", "hour", "minute"), factors = FALSE, h = 0, ...)

## S3 method for class 'zoo'
temporaldummy(object, type = c("month", "quarter", "week",
  "day", "hour", "halfhour", "minute", "second"), of = c("year", "quarter",
  "month", "week", "day", "hour", "minute"), factors = FALSE, h = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="temporaldummy_+3A_object">object</code></td>
<td>
<p>Either a ts / msts / zoo / xts / tsibble object or a vector
of dates.</p>
</td></tr>
<tr><td><code id="temporaldummy_+3A_...">...</code></td>
<td>
<p>Other parameters.</p>
</td></tr>
<tr><td><code id="temporaldummy_+3A_type">type</code></td>
<td>
<p>Specifies what type of frequency to produce. For example, if
<code>type="month"</code>, then the matrix with dummies for months of the year will
be created.</p>
</td></tr>
<tr><td><code id="temporaldummy_+3A_of">of</code></td>
<td>
<p>Specifies the frequency of what is needed. Used together with <code>type</code>
e.g. <code>type="day"</code> and <code>of="month"</code> will produce a matrix with dummies
for days of month (31 dummies).</p>
</td></tr>
<tr><td><code id="temporaldummy_+3A_factors">factors</code></td>
<td>
<p>If <code>TRUE</code>, the function will return the categorical variable
instead of the matrix with dummies.</p>
</td></tr>
<tr><td><code id="temporaldummy_+3A_h">h</code></td>
<td>
<p>If not <code>NULL</code>, then the function will produce dummies for this
set of observations ahead as well, binding them to the original matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function extracts dates from the provided object and returns a matrix with
dummy variables for the specified frequency type, with the number of rows equal
to the length of the object + the specified horizon. If a numeric vector is provided
then it will produce dummies based on typical values (e.g. 30 days in month). So it
is recommended to use proper classes with this method.
</p>
<p>Several notes on how the dummies are calculated in some special cases:
</p>

<ul>
<li><p> In case of weeks of years, the first week is defined according to ISO 8601.
</p>
</li></ul>

<p>Note that not all the combinations of <code>type</code> and <code>of</code> are supported. For
example, there is no such thing as dummies for months of week. Also note that some
combinations are not very useful and would take a lot of memory (e.g. minutes of year).
</p>
<p>The function will return all the dummy variables. If you want to avoid the dummy
variables trap, you will need to exclude one of them manually.
</p>
<p>If you want to have a different type of dummy variables, let me know, I will
implement it.
</p>


<h3>Value</h3>

<p>One of the two is returned, depending on the value of <code>factors</code> variable:
</p>

<ul>
<li> <p><code>factors=FALSE</code>: Class &quot;dgCMatrix&quot; with all the dummy variables is returned
in case of numeric variable. Feel free to drop one (making it a reference variable) or
convert the object into matrix (this will consume more memory than the returned class).
In other cases the object of the same class as the provided is returned.
</p>
</li>
<li> <p><code>factors=TRUE</code>: The categorical variable (factor) containing specific values
for each observation.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+xregExpander">xregExpander</a>, <a href="#topic+xregMultiplier">xregMultiplier</a>,
<a href="#topic+outlierdummy">outlierdummy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate matrix with dummies for a ts object
x &lt;- ts(rnorm(100,100,1),frequency=12)
temporaldummy(x)

# Generate matrix with monthly dummies for a zoo object
x &lt;- as.Date("2003-01-01")+0:99
temporaldummy(x, type="month", of="year", h=10)

</code></pre>

<hr>
<h2 id='xregExpander'>Exogenous variables expander</h2><span id='topic+xregExpander'></span>

<h3>Description</h3>

<p>Function expands the provided matrix or vector of variables, producing
values with lags and leads specified by <code>lags</code> variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xregExpander(xreg, lags = c(-frequency(xreg):frequency(xreg)),
  silent = TRUE, gaps = c("auto", "NAs", "zero", "naive", "extrapolate"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xregExpander_+3A_xreg">xreg</code></td>
<td>
<p>Vector / matrix / data.frame, containing variables that need
to be expanded. In case of vector / matrix it is recommended to provide
<code>ts</code> object, so the frequency of the data is taken into account.</p>
</td></tr>
<tr><td><code id="xregExpander_+3A_lags">lags</code></td>
<td>
<p>Vector of lags / leads that we need to have. Negative values
mean lags, positive ones mean leads.</p>
</td></tr>
<tr><td><code id="xregExpander_+3A_silent">silent</code></td>
<td>
<p>If <code>silent=FALSE</code>, then the progress is printed out.
Otherwise the function won't print anything in the console.</p>
</td></tr>
<tr><td><code id="xregExpander_+3A_gaps">gaps</code></td>
<td>
<p>Defines how to fill in the gaps in the data. <code>"NAs"</code> will
leave missing values, <code>"zero"</code> will substitute them by zeroes,
<code>"naive"</code> will use the last / the first actual value, while
<code>"extrapolate"</code> will use <a href="smooth.html#topic+es">es</a> function from smooth package
(if present, otherwise - naive) in order to fill in values. Finally,
<code>"auto"</code> will let the function select between <code>"extrapolate"</code> and
<code>"naive"</code> depending on the length of series.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function could be handy when you want to check if lags and leads
of a variable influence the dependent variable. Can be used together
with <code>xregDo="select"</code> in <a href="smooth.html#topic+adam">adam</a>, <a href="smooth.html#topic+es">es</a>,
<a href="smooth.html#topic+ces">ces</a> and <a href="smooth.html#topic+ssarima">ssarima</a>. All the missing values
in the beginning and at the end of lagged series are substituted by
mean forecasts produced using <a href="smooth.html#topic+adam">adam</a>.
</p>


<h3>Value</h3>

<p><code>ts</code> matrix with the expanded variables is returned.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="smooth.html#topic+es">es</a>, <a href="#topic+stepwise">stepwise</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create matrix of two variables, make it ts object and expand it
x &lt;- cbind(rnorm(100,100,1),rnorm(100,50,3))
x &lt;- ts(x,frequency=12)
xregExpander(x)

</code></pre>

<hr>
<h2 id='xregMultiplier'>Exogenous variables cross-products</h2><span id='topic+xregMultiplier'></span>

<h3>Description</h3>

<p>Function generates the cross-products of the provided exogenous variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xregMultiplier(xreg, silent = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xregMultiplier_+3A_xreg">xreg</code></td>
<td>
<p>matrix or data.frame, containing variables that need
to be expanded. This matrix needs to contain at least two columns.</p>
</td></tr>
<tr><td><code id="xregMultiplier_+3A_silent">silent</code></td>
<td>
<p>If <code>silent=FALSE</code>, then the progress is printed out.
Otherwise the function won't print anything in the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function might be useful if you have several variables and want to
introduce their cross-products. This might be useful when introducing the
interactions between dummy and continuous variables.
</p>


<h3>Value</h3>

<p><code>ts</code> matrix with the transformed and the original variables
is returned.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="smooth.html#topic+es">es</a>, <a href="#topic+stepwise">stepwise</a>,
<a href="#topic+xregExpander">xregExpander</a>, <a href="#topic+xregTransformer">xregTransformer</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create matrix of two variables and expand it
x &lt;- cbind(rnorm(100,100,1),rnorm(100,50,3))
xregMultiplier(x)

</code></pre>

<hr>
<h2 id='xregTransformer'>Exogenous variables transformer</h2><span id='topic+xregTransformer'></span>

<h3>Description</h3>

<p>Function transforms each variable in the provided matrix or vector,
producing non-linear values, depending on the selected pool of functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xregTransformer(xreg, functions = c("log", "exp", "inv", "sqrt", "square"),
  silent = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xregTransformer_+3A_xreg">xreg</code></td>
<td>
<p>Vector / matrix / data.frame, containing variables that need
to be expanded. In case of vector / matrix it is recommended to provide
<code>ts</code> object, so the frequency of the data is taken into account.</p>
</td></tr>
<tr><td><code id="xregTransformer_+3A_functions">functions</code></td>
<td>
<p>Vector of names for functions used.</p>
</td></tr>
<tr><td><code id="xregTransformer_+3A_silent">silent</code></td>
<td>
<p>If <code>silent=FALSE</code>, then the progress is printed out.
Otherwise the function won't print anything in the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function could be useful when you want to automatically select the
necessary transformations of the variables. This can be used together
with <code>xregDo="select"</code> in <a href="smooth.html#topic+es">es</a>, <a href="smooth.html#topic+ces">ces</a>,
<a href="smooth.html#topic+gum">gum</a> and <a href="smooth.html#topic+ssarima">ssarima</a>. However, this might be
dangerous, as it might lead to the overfitting the data. So be reasonable
when you produce the transformed variables.
</p>


<h3>Value</h3>

<p><code>ts</code> matrix with the transformed and the original variables
is returned.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>See Also</h3>

<p><code><a href="smooth.html#topic+es">es</a>, <a href="#topic+stepwise">stepwise</a>,
<a href="#topic+xregExpander">xregExpander</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create matrix of two variables and expand it
x &lt;- cbind(rnorm(100,100,1),rnorm(100,50,3))
xregTransformer(x)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
