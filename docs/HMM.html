<!DOCTYPE html><html><head><title>Help for package HMM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {HMM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#backward'><p>Computes the backward probabilities</p></a></li>
<li><a href='#baumWelch'><p>Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm</p></a></li>
<li><a href='#dishonestCasino'><p>Example application for Hidden Markov Models</p></a></li>
<li><a href='#forward'><p>Computes the forward probabilities</p></a></li>
<li><a href='#HMM'>
<p>HMM - Hidden Markov Models</p></a></li>
<li><a href='#initHMM'><p>Initialisation of HMMs</p></a></li>
<li><a href='#posterior'><p>Computes the posterior probabilities for the states</p></a></li>
<li><a href='#simHMM'><p>Simulate states and observations for a Hidden Markov Model</p></a></li>
<li><a href='#viterbi'><p>Computes the most probable path of states</p></a></li>
<li><a href='#viterbiTraining'><p>Inferring the parameters of a Hidden Markov Model via Viterbi-training</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Title:</td>
<td>Hidden Markov Models</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-03-20</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lin Himmelmann &lt;hmm@linhi.de&gt;</td>
</tr>
<tr>
<td>Author:</td>
<td>Scientific Software - Dr. Lin Himmelmann</td>
</tr>
<tr>
<td>URL:</td>
<td>www.linhi.de</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.0.0)</td>
</tr>
<tr>
<td>Description:</td>
<td>Easy to use library to setup, apply and make inference with discrete time and discrete space Hidden Markov Models.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-03-23 09:08:47 UTC; lh</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-03-23 13:30:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='backward'>Computes the backward probabilities</h2><span id='topic+backward'></span>

<h3>Description</h3>

<p>The <code>backward</code>-function computes the backward probabilities.
The backward probability for state X and observation at time k is defined as the probability
of observing the sequence of observations e_k+1, ... ,e_n under the condition that the
state at time k is X. That is:<br />
<code>b[X,k] := Prob(E_k+1 = e_k+1, ... , E_n = e_n | X_k = X)</code>.<br />
Where <code>E_1...E_n = e_1...e_n</code> is the sequence of observed emissions and
<code>X_k</code> is a random variable that represents the state at time <code>k</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backward(hmm, observation)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="backward_+3A_hmm">hmm</code></td>
<td>
<p> A Hidden Markov Model.</p>
</td></tr>
<tr><td><code id="backward_+3A_observation">observation</code></td>
<td>
<p> A sequence of observations.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>Dimension and Format of the Arguments.
</p>

<dl>
<dt>hmm         </dt><dd><p>A valid Hidden Markov Model, for example instantiated by <code><a href="#topic+initHMM">initHMM</a></code>.</p>
</dd>
<dt>observation </dt><dd><p>A vector of strings with the observations.</p>
</dd>
</dl>



<h3>Value</h3>

<p>Return Value:
</p>
<table>
<tr><td><code>backward</code></td>
<td>
<p>A matrix containing the backward probabilities.
The probabilities are given on a logarithmic scale (natural logarithm).
The first dimension refers to the state and the second dimension to time.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lin Himmelmann &lt;hmm@linhi.com&gt;, Scientific Software Development
</p>


<h3>References</h3>

<p>Lawrence R. Rabiner: A Tutorial on Hidden Markov Models and Selected Applications
in Speech Recognition. Proceedings of the IEEE 77(2) p.257-286, 1989.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+forward">forward</a></code> for computing the forward probabilities.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Initialise HMM
hmm = initHMM(c("A","B"), c("L","R"), transProbs=matrix(c(.8,.2,.2,.8),2),
	emissionProbs=matrix(c(.6,.4,.4,.6),2))
print(hmm)
# Sequence of observations
observations = c("L","L","R","R")
# Calculate backward probablities
logBackwardProbabilities = backward(hmm,observations)
print(exp(logBackwardProbabilities))
</code></pre>

<hr>
<h2 id='baumWelch'>Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm</h2><span id='topic+baumWelch'></span>

<h3>Description</h3>

<p>For an initial Hidden Markov Model (HMM) and a given sequence of observations, the
Baum-Welch algorithm infers optimal parameters to the HMM. Since the Baum-Welch algorithm
is a variant of the Expectation-Maximisation algorithm, the algorithm converges to
a local solution which might not be the global optimum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>baumWelch(hmm, observation, maxIterations=100, delta=1E-9, pseudoCount=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="baumWelch_+3A_hmm">hmm</code></td>
<td>
<p> A Hidden Markov Model.</p>
</td></tr>
<tr><td><code id="baumWelch_+3A_observation">observation</code></td>
<td>
<p> A sequence of observations.</p>
</td></tr>
<tr><td><code id="baumWelch_+3A_maxiterations">maxIterations</code></td>
<td>
<p> The maximum number of iterations in the Baum-Welch algorithm.</p>
</td></tr>
<tr><td><code id="baumWelch_+3A_delta">delta</code></td>
<td>
<p> Additional termination condition, if the transition
and emission matrices converge, before reaching the maximum
number of iterations (<code>maxIterations</code>). The difference
of transition and emission parameters in consecutive iterations
must be smaller than <code>delta</code> to terminate the algorithm.</p>
</td></tr>
<tr><td><code id="baumWelch_+3A_pseudocount">pseudoCount</code></td>
<td>
<p> Adding this amount of pseudo counts in the estimation-step
of the Baum-Welch algorithm.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>Dimension and Format of the Arguments.
</p>

<dl>
<dt>hmm         </dt><dd><p>A valid Hidden Markov Model, for example instantiated by <code><a href="#topic+initHMM">initHMM</a></code>.</p>
</dd>
<dt>observation </dt><dd><p>A vector of observations.</p>
</dd>
</dl>



<h3>Value</h3>

<p>Return Values:
</p>
<table>
<tr><td><code>hmm</code></td>
<td>
<p>The inferred HMM. The representation is equivalent to the
representation in <code><a href="#topic+initHMM">initHMM</a></code>.</p>
</td></tr>
<tr><td><code>difference</code></td>
<td>
<p>Vector of differences calculated from consecutive transition and emission
matrices in each iteration of the Baum-Welch procedure.
The difference is the sum of the distances between consecutive
transition and emission matrices in the L2-Norm.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lin Himmelmann &lt;hmm@linhi.com&gt;, Scientific Software Development
</p>


<h3>References</h3>

<p>For details see: Lawrence R. Rabiner: A Tutorial on Hidden Markov Models and Selected Applications
in Speech Recognition. Proceedings of the IEEE 77(2) p.257-286, 1989.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+viterbiTraining">viterbiTraining</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Initial HMM
hmm = initHMM(c("A","B"),c("L","R"),
	transProbs=matrix(c(.9,.1,.1,.9),2),
	emissionProbs=matrix(c(.5,.51,.5,.49),2))
print(hmm)
# Sequence of observation
a = sample(c(rep("L",100),rep("R",300)))
b = sample(c(rep("L",300),rep("R",100)))
observation = c(a,b)
# Baum-Welch
bw = baumWelch(hmm,observation,10)
print(bw$hmm)
</code></pre>

<hr>
<h2 id='dishonestCasino'>Example application for Hidden Markov Models</h2><span id='topic+dishonestCasino'></span>

<h3>Description</h3>

<p>The dishonest casino gives an example for the application of Hidden Markov Models.
This example is taken from Durbin et. al. 1999:
A dishonest casino uses two dice, one of them is fair the other is loaded.
The probabilities of the fair die are (1/6,...,1/6) for throwing (&quot;1&quot;,...,&quot;6&quot;).
The probabilities of the loaded die are (1/10,...,1/10,1/2) for throwing (&quot;1&quot;,...&quot;5&quot;,&quot;6&quot;).
The observer doesn't know which die is actually taken (the state is hidden),
but the sequence of throws (observations) can be used to infer which die (state) was used. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dishonestCasino()
</code></pre>


<h3>Format</h3>

<p>The function <code>dishonestCasino</code> has no arguments.
</p>


<h3>Value</h3>

<p>The function <code>dishonestCasino</code> returns nothing.
</p>


<h3>Author(s)</h3>

<p>Lin Himmelmann &lt;hmm@linhi.com&gt;, Scientific Software Development
</p>


<h3>References</h3>

<p>Richard Durbin, Sean R. Eddy, Anders Krogh, Graeme Mitchison (1999). Biological Sequence Analysis:
Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press. ISBN 0-521-62971-3.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Dishonest casino example
dishonestCasino()
</code></pre>

<hr>
<h2 id='forward'>Computes the forward probabilities</h2><span id='topic+forward'></span>

<h3>Description</h3>

<p>The <code>forward</code>-function computes the forward probabilities.
The forward probability for state X up to observation at time k is defined as the probability
of observing the sequence of observations e_1, ... ,e_k and that the state at time k is X.
That is:<br />
<code>f[X,k] := Prob(E_1 = e_1, ... , E_k = e_k , X_k = X)</code>.<br />
Where <code>E_1...E_n = e_1...e_n</code> is the sequence of observed emissions and
<code>X_k</code> is a random variable that represents the state at time <code>k</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forward(hmm, observation)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forward_+3A_hmm">hmm</code></td>
<td>
<p> A Hidden Markov Model.</p>
</td></tr>
<tr><td><code id="forward_+3A_observation">observation</code></td>
<td>
<p> A sequence of observations.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>Dimension and Format of the Arguments.
</p>

<dl>
<dt>hmm         </dt><dd><p>A valid Hidden Markov Model, for example instantiated by <code><a href="#topic+initHMM">initHMM</a></code>.</p>
</dd>
<dt>observation </dt><dd><p>A vector of strings with the observations.</p>
</dd>
</dl>



<h3>Value</h3>

<p>Return Value:
</p>
<table>
<tr><td><code>forward</code></td>
<td>
<p>A matrix containing the forward probabilities.
The probabilities are given on a logarithmic scale (natural logarithm).
The first dimension refers to the state and the second dimension to time.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lin Himmelmann &lt;hmm@linhi.com&gt;, Scientific Software Development
</p>


<h3>References</h3>

<p>Lawrence R. Rabiner: A Tutorial on Hidden Markov Models and Selected Applications
in Speech Recognition. Proceedings of the IEEE 77(2) p.257-286, 1989.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+backward">backward</a></code> for computing the backward probabilities.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Initialise HMM
hmm = initHMM(c("A","B"), c("L","R"), transProbs=matrix(c(.8,.2,.2,.8),2),
	emissionProbs=matrix(c(.6,.4,.4,.6),2))
print(hmm)
# Sequence of observations
observations = c("L","L","R","R")
# Calculate forward probablities
logForwardProbabilities = forward(hmm,observations)
print(exp(logForwardProbabilities))
</code></pre>

<hr>
<h2 id='HMM'>
HMM - Hidden Markov Models
</h2><span id='topic+HMM'></span>

<h3>Description</h3>

<p>Modelling, analysis and inference with discrete time and discrete space Hidden Markov Models.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> HMM - Rpackage</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2010-01-10</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL version 2 or later</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Scientific Software Development - Dr. Lin Himmelmann, www.linhi.com</td>
</tr>
<tr>
 <td style="text-align: left;">
URL: </td><td style="text-align: left;">  www.linhi.com</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<hr>
<h2 id='initHMM'>Initialisation of HMMs</h2><span id='topic+initHMM'></span>

<h3>Description</h3>

<p>This function initialises a general discrete time and discrete space Hidden Markov Model (HMM).
A HMM consists of an alphabet of states and emission symbols. A HMM assumes that
the states are hidden from the observer, while only the emissions of the states are observable.
The HMM is designed to make inference on the states through the observation of emissions.
The stochastics of the  HMM is fully described by the initial starting probabilities of the states,
the transition probabilities between states and the emission probabilities of the states.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initHMM(States, Symbols, startProbs=NULL, transProbs=NULL, emissionProbs=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="initHMM_+3A_states">States</code></td>
<td>
<p> Vector with the names of the states.</p>
</td></tr>
<tr><td><code id="initHMM_+3A_symbols">Symbols</code></td>
<td>
<p> Vector with the names of the symbols.</p>
</td></tr>
<tr><td><code id="initHMM_+3A_startprobs">startProbs</code></td>
<td>
<p> Vector with the starting probabilities of the states.</p>
</td></tr>
<tr><td><code id="initHMM_+3A_transprobs">transProbs</code></td>
<td>
<p> Stochastic matrix containing the transition probabilities between the states.</p>
</td></tr>
<tr><td><code id="initHMM_+3A_emissionprobs">emissionProbs</code></td>
<td>
<p> Stochastic matrix containing the emission probabilities of the states.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>Dimension and Format of the Arguments.
</p>

<dl>
<dt>States</dt><dd><p>     Vector of strings.</p>
</dd>
<dt>Symbols</dt><dd><p>    Vector of strings.</p>
</dd>
<dt>startProbs</dt><dd><p> Vector with the starting probabilities of the states.
The entries must sum to 1.</p>
</dd>
<dt>transProbs</dt><dd> <p><code>transProbs</code> is a (number of states)x(number of states)-sized
matrix, which contains the transition probabilities between states.
The entry <code>transProbs[X,Y]</code> gives the probability of a transition
from state <code>X</code> to state <code>Y</code>. The rows of the matrix must sum to 1.</p>
</dd>
<dt>emissionProbs</dt><dd> <p><code>emissionProbs</code> is a (number of states)x(number of states)-sized
matrix, which contains the emission probabilities of the states.
The entry <code>emissionProbs[X,e]</code> gives the probability of emission
<code>e</code> from state <code>X</code>. The rows of the matrix must sum to 1.</p>
</dd>
</dl>



<h3>Details</h3>

<p>In <code>transProbs</code> and <code>emissionProbs</code> NA's can be used in order to forbid
specific transitions and emissions. This might be useful for Viterbi training or
the Baum-Welch algorithm when using pseudocounts.
</p>


<h3>Value</h3>

<p>The function <code>initHMM</code> returns a HMM that consists of a list of 5 elements:
</p>
<table>
<tr><td><code>States</code></td>
<td>
<p> Vector with the names of the states.</p>
</td></tr>
<tr><td><code>Symbols</code></td>
<td>
<p> Vector with the names of the symbols.</p>
</td></tr>
<tr><td><code>startProbs</code></td>
<td>
<p>Annotated vector with the starting probabilities of the states.</p>
</td></tr>
<tr><td><code>transProbs</code></td>
<td>
<p> Annotated matrix containing the transition probabilities between the states.</p>
</td></tr>
<tr><td><code>emissionProbs</code></td>
<td>
<p> Annotated matrix containing the emission probabilities of the states.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lin Himmelmann &lt;hmm@linhi.com&gt;, Scientific Software Development
</p>


<h3>References</h3>

<p>For an introduction in the HMM-literature see for example:
</p>

<ul>
<li><p> Lawrence R. Rabiner: A Tutorial on Hidden Markov Models and Selected Applications
in Speech Recognition. Proceedings of the IEEE 77(2) p.257-286, 1989.
</p>
</li>
<li><p> Olivier Cappe, Eric Moulines, Tobias Ryden: Inference in Hidden Markov Models. Springer. ISBN 0-387-40264-0.
</p>
</li>
<li><p> Ephraim Y., Merhav N.: Hidden Markov processes. IEEE Trans. Inform. Theory 48 p.1518-1569, 2002.
</p>
</li></ul>


<h3>See Also</h3>

<p>See <code><a href="#topic+simHMM">simHMM</a></code> to simulate a path of states and observations from a Hidden Markov Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Initialise HMM nr.1
initHMM(c("X","Y"), c("a","b","c"))
# Initialise HMM nr.2
initHMM(c("X","Y"), c("a","b"), c(.3,.7), matrix(c(.9,.1,.1,.9),2),
        matrix(c(.3,.7,.7,.3),2))
</code></pre>

<hr>
<h2 id='posterior'>Computes the posterior probabilities for the states</h2><span id='topic+posterior'></span>

<h3>Description</h3>

<p>This function computes the posterior probabilities of being in state X at time k
for a given sequence of observations and a given Hidden Markov Model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posterior(hmm, observation)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="posterior_+3A_hmm">hmm</code></td>
<td>
<p> A Hidden Markov Model.</p>
</td></tr>
<tr><td><code id="posterior_+3A_observation">observation</code></td>
<td>
<p> A sequence of observations.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>Dimension and Format of the Arguments.
</p>

<dl>
<dt>hmm         </dt><dd><p>A valid Hidden Markov Model, for example instantiated by <code><a href="#topic+initHMM">initHMM</a></code>.</p>
</dd>
<dt>observation </dt><dd><p>A vector of observations.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The posterior probability of being in a state X at time k can be computed from the
<code><a href="#topic+forward">forward</a></code> and <code><a href="#topic+backward">backward</a></code> probabilities:<br />
<code> Ws(X_k = X | E_1 = e_1, ... , E_n = e_n) = f[X,k] * b[X,k] / Prob(E_1 = e_1, ... , E_n = e_n)</code><br />
Where <code>E_1...E_n = e_1...e_n</code> is the sequence of observed emissions and
<code>X_k</code> is a random variable that represents the state at time <code>k</code>.
</p>


<h3>Value</h3>

<p>Return Values:
</p>
<table>
<tr><td><code>posterior</code></td>
<td>
<p>A matrix containing the posterior probabilities.
The first dimension refers to the state and the second dimension to time.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lin Himmelmann &lt;hmm@linhi.com&gt;, Scientific Software Development
</p>


<h3>References</h3>

<p>Lawrence R. Rabiner: A Tutorial on Hidden Markov Models and Selected Applications
in Speech Recognition. Proceedings of the IEEE 77(2) p.257-286, 1989.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+forward">forward</a></code> for computing the forward probabilities and <code><a href="#topic+backward">backward</a></code>
for computing the backward probabilities.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Initialise HMM
hmm = initHMM(c("A","B"), c("L","R"), transProbs=matrix(c(.8,.2,.2,.8),2),
	emissionProbs=matrix(c(.6,.4,.4,.6),2))
print(hmm)
# Sequence of observations
observations = c("L","L","R","R")
# Calculate posterior probablities of the states
posterior = posterior(hmm,observations)
print(posterior)
</code></pre>

<hr>
<h2 id='simHMM'>Simulate states and observations for a Hidden Markov Model</h2><span id='topic+simHMM'></span>

<h3>Description</h3>

<p>Simulates a path of states and observations for a given Hidden Markov Model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simHMM(hmm, length)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simHMM_+3A_hmm">hmm</code></td>
<td>
<p> A Hidden Markov Model.</p>
</td></tr>
<tr><td><code id="simHMM_+3A_length">length</code></td>
<td>
<p> The length of the simulated sequence of observations and states.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>Dimension and Format of the Arguments.
</p>

<dl>
<dt>hmm </dt><dd><p>A valid Hidden Markov Model, for example instantiated by <code><a href="#topic+initHMM">initHMM</a></code>.</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function <code>simHMM</code> returns a path of states and associated observations:
</p>
<table>
<tr><td><code>states</code></td>
<td>
<p> The path of states.</p>
</td></tr>
<tr><td><code>observations</code></td>
<td>
<p> The sequence of observations.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lin Himmelmann &lt;hmm@linhi.com&gt;, Scientific Software Development
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+initHMM">initHMM</a></code> for instantiation of Hidden Markov Models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Initialise HMM
hmm = initHMM(c("X","Y"),c("a","b","c"))
# Simulate from the HMM
simHMM(hmm, 100)
</code></pre>

<hr>
<h2 id='viterbi'>Computes the most probable path of states</h2><span id='topic+viterbi'></span>

<h3>Description</h3>

<p>The Viterbi-algorithm computes the most probable path of states for a sequence
of observations for a given Hidden Markov Model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>viterbi(hmm, observation)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="viterbi_+3A_hmm">hmm</code></td>
<td>
<p> A Hidden Markov Model.</p>
</td></tr>
<tr><td><code id="viterbi_+3A_observation">observation</code></td>
<td>
<p> A sequence of observations.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>Dimension and Format of the Arguments.
</p>

<dl>
<dt>hmm         </dt><dd><p>A valid Hidden Markov Model, for example instantiated by <code><a href="#topic+initHMM">initHMM</a></code>.</p>
</dd>
<dt>observation </dt><dd><p>A vector of observations.</p>
</dd>
</dl>



<h3>Value</h3>

<p>Return Value:
</p>
<table>
<tr><td><code>viterbiPath</code></td>
<td>
<p>A vector of strings, containing the most probable path of states.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lin Himmelmann &lt;hmm@linhi.com&gt;, Scientific Software Development
</p>


<h3>References</h3>

<p>Lawrence R. Rabiner: A Tutorial on Hidden Markov Models and Selected Applications
in Speech Recognition. Proceedings of the IEEE 77(2) p.257-286, 1989.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Initialise HMM
hmm = initHMM(c("A","B"), c("L","R"), transProbs=matrix(c(.6,.4,.4,.6),2),
	emissionProbs=matrix(c(.6,.4,.4,.6),2))
print(hmm)
# Sequence of observations
observations = c("L","L","R","R")
# Calculate Viterbi path
viterbi = viterbi(hmm,observations)
print(viterbi)
</code></pre>

<hr>
<h2 id='viterbiTraining'>Inferring the parameters of a Hidden Markov Model via Viterbi-training</h2><span id='topic+viterbiTraining'></span>

<h3>Description</h3>

<p>For an initial Hidden Markov Model (HMM) and a given sequence of observations, the
Viterbi-training algorithm infers optimal parameters to the HMM. Viterbi-training
usually converges much faster than the Baum-Welch algorithm, but the underlying
algorithm is theoretically less justified. Be careful: The algorithm converges to
a local solution which might not be the optimum. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>viterbiTraining(hmm, observation, maxIterations=100, delta=1E-9, pseudoCount=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="viterbiTraining_+3A_hmm">hmm</code></td>
<td>
<p> A Hidden Markov Model.</p>
</td></tr>
<tr><td><code id="viterbiTraining_+3A_observation">observation</code></td>
<td>
<p> A sequence of observations.</p>
</td></tr>
<tr><td><code id="viterbiTraining_+3A_maxiterations">maxIterations</code></td>
<td>
<p> The maximum number of iterations in the Viterbi-training algorithm.</p>
</td></tr>
<tr><td><code id="viterbiTraining_+3A_delta">delta</code></td>
<td>
<p> Additional termination condition, if the transition
and emission matrices converge, before reaching the maximum
number of iterations (<code>maxIterations</code>). The difference
of transition and emission parameters in consecutive iterations
must be smaller than <code>delta</code> to terminate the algorithm.</p>
</td></tr>
<tr><td><code id="viterbiTraining_+3A_pseudocount">pseudoCount</code></td>
<td>
<p> Adding this amount of pseudo counts in the estimation-step
of the Viterbi-training algorithm.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>Dimension and Format of the Arguments.
</p>

<dl>
<dt>hmm         </dt><dd><p>A valid Hidden Markov Model, for example instantiated by <code><a href="#topic+initHMM">initHMM</a></code>.</p>
</dd>
<dt>observation </dt><dd><p>A vector of observations.</p>
</dd>
</dl>



<h3>Value</h3>

<p>Return Values:
</p>
<table>
<tr><td><code>hmm</code></td>
<td>
<p>The inferred HMM. The representation is equivalent to the
representation in <code><a href="#topic+initHMM">initHMM</a></code>.</p>
</td></tr>
<tr><td><code>difference</code></td>
<td>
<p>Vector of differences calculated from consecutive transition and emission
matrices in each iteration of the Viterbi-training.
The difference is the sum of the distances between consecutive
transition and emission matrices in the L2-Norm.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lin Himmelmann &lt;hmm@linhi.com&gt;, Scientific Software Development
</p>


<h3>References</h3>

<p>For details see: Lawrence R. Rabiner: A Tutorial on Hidden Markov Models and Selected Applications
in Speech Recognition. Proceedings of the IEEE 77(2) p.257-286, 1989.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+baumWelch">baumWelch</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Initial HMM
hmm = initHMM(c("A","B"),c("L","R"),
	transProbs=matrix(c(.9,.1,.1,.9),2),
	emissionProbs=matrix(c(.5,.51,.5,.49),2))
print(hmm)
# Sequence of observation
a = sample(c(rep("L",100),rep("R",300)))
b = sample(c(rep("L",300),rep("R",100)))
observation = c(a,b)
# Viterbi-training
vt = viterbiTraining(hmm,observation,10)
print(vt$hmm)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
