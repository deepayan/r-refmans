<!DOCTYPE html><html><head><title>Help for package L0Learn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {L0Learn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#coef.L0Learn'><p>Extract Solutions</p></a></li>
<li><a href='#GenSynthetic'><p>Generate Synthetic Data</p></a></li>
<li><a href='#GenSyntheticHighCorr'><p>Generate Exponential Correlated Synthetic Data</p></a></li>
<li><a href='#GenSyntheticLogistic'><p>Generate Logistic Synthetic Data</p></a></li>
<li><a href='#L0Learn-package'><p>A package for L0-regularized learning</p></a></li>
<li><a href='#L0Learn.cvfit'><p>Cross Validation</p></a></li>
<li><a href='#L0Learn.fit'><p>Fit an L0-regularized model</p></a></li>
<li><a href='#plot.L0Learn'><p>Plot Regularization Path</p></a></li>
<li><a href='#plot.L0LearnCV'><p>Plot Cross-validation Errors</p></a></li>
<li><a href='#predict.L0Learn'><p>Predict Response</p></a></li>
<li><a href='#print.L0Learn'><p>Print L0Learn.fit object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast Algorithms for Best Subset Selection</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-03-04</td>
</tr>
<tr>
<td>Description:</td>
<td>Highly optimized toolkit for approximately solving L0-regularized learning problems (a.k.a. best subset selection).
    The algorithms are based on coordinate descent and local combinatorial search.
    For more details, check the paper by Hazimeh and Mazumder (2020) &lt;<a href="https://doi.org/10.1287%2Fopre.2019.1919">doi:10.1287/opre.2019.1919</a>&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/hazimehh/L0Learn">https://github.com/hazimehh/L0Learn</a>
<a href="https://pubsonline.informs.org/doi/10.1287/opre.2019.1919">https://pubsonline.informs.org/doi/10.1287/opre.2019.1919</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/hazimehh/L0Learn/issues">https://github.com/hazimehh/L0Learn/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.13), Matrix, methods, ggplot2, reshape2, MASS</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat, pracma, raster, covr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-04 19:02:37 UTC; hh</td>
</tr>
<tr>
<td>Author:</td>
<td>Hussein Hazimeh [aut, cre],
  Rahul Mazumder [aut],
  Tim Nonet [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hussein Hazimeh &lt;husseinhaz@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-07 08:00:18 UTC</td>
</tr>
</table>
<hr>
<h2 id='coef.L0Learn'>Extract Solutions</h2><span id='topic+coef.L0Learn'></span><span id='topic+coef.L0LearnCV'></span>

<h3>Description</h3>

<p>Extracts a specific solution in the regularization path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'L0Learn'
coef(object, lambda = NULL, gamma = NULL, ...)

## S3 method for class 'L0LearnCV'
coef(object, lambda = NULL, gamma = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.L0Learn_+3A_object">object</code></td>
<td>
<p>The output of L0Learn.fit or L0Learn.cvfit</p>
</td></tr>
<tr><td><code id="coef.L0Learn_+3A_lambda">lambda</code></td>
<td>
<p>The value of lambda at which to extract the solution.</p>
</td></tr>
<tr><td><code id="coef.L0Learn_+3A_gamma">gamma</code></td>
<td>
<p>The value of gamma at which to extract the solution.</p>
</td></tr>
<tr><td><code id="coef.L0Learn_+3A_...">...</code></td>
<td>
<p>ignore</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sparse Matrix of class <code>dgCMatrix</code>, which contains the model
coefficients. If both lambda and gamma are not supplied, then a matrix of 
coefficients for all the solutions in the regularization path is returned. 
If lambda is supplied but gamma is not, the smallest value of gamma is used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate synthetic data for this example
data &lt;- GenSynthetic(n=100,p=20,k=10,seed=1)
X = data$X
y = data$y

# Fit an L0L2 Model with 3 values of Gamma ranging from 0.0001 to 10, using coordinate descent
fit &lt;- L0Learn.fit(X, y, penalty="L0L2", nGamma=3, gammaMin=0.0001, gammaMax = 10)
print(fit)
# Extract the coefficients of the solution at lambda = 2.45513e-02 and gamma = 0.0001
coef(fit, lambda=2.45513e-02, gamma=0.0001)
# Extract the coefficients of all the solutions in the path
coef(fit)

</code></pre>

<hr>
<h2 id='GenSynthetic'>Generate Synthetic Data</h2><span id='topic+GenSynthetic'></span>

<h3>Description</h3>

<p>Generates a synthetic dataset as follows: 1) Sample every element in data matrix X from N(0,1).
2) Generate a vector B with the first k entries set to 1 and the rest are zeros. 3) Sample every element in the noise
vector e from N(0,1). 4) Set y = XB + b0 + e.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GenSynthetic(n, p, k, seed, rho = 0, b0 = 0, snr = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GenSynthetic_+3A_n">n</code></td>
<td>
<p>Number of samples</p>
</td></tr>
<tr><td><code id="GenSynthetic_+3A_p">p</code></td>
<td>
<p>Number of features</p>
</td></tr>
<tr><td><code id="GenSynthetic_+3A_k">k</code></td>
<td>
<p>Number of non-zeros in true vector of coefficients</p>
</td></tr>
<tr><td><code id="GenSynthetic_+3A_seed">seed</code></td>
<td>
<p>The seed used for randomly generating the data</p>
</td></tr>
<tr><td><code id="GenSynthetic_+3A_rho">rho</code></td>
<td>
<p>The threshold for setting values to 0.  if |X(i, j)| &gt; rho =&gt; X(i, j) &lt;- 0</p>
</td></tr>
<tr><td><code id="GenSynthetic_+3A_b0">b0</code></td>
<td>
<p>intercept value to translate y by.</p>
</td></tr>
<tr><td><code id="GenSynthetic_+3A_snr">snr</code></td>
<td>
<p>desired Signal-to-Noise ratio. This sets the magnitude of the error term 'e'. 
SNR is defined as  SNR = Var(XB)/Var(e)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
the data matrix X,
the response vector y,
the coefficients B,
the error vector e,
the intercept term b0.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- GenSynthetic(n=100,p=20,k=10,seed=1)
X = data$X
y = data$y
</code></pre>

<hr>
<h2 id='GenSyntheticHighCorr'>Generate Exponential Correlated Synthetic Data</h2><span id='topic+GenSyntheticHighCorr'></span>

<h3>Description</h3>

<p>Generates a synthetic dataset as follows: 1) Generate a correlation matrix, SIG,  where item [i, j] = A^|i-j|.
2) Draw from a Multivariate Normal Distribution using (mu and SIG) to generate X. 3) Generate a vector B with every ~p/k entry set to 1 and the rest are zeros.
4) Sample every element in the noise vector e from N(0,1). 4) Set y = XB + b0 + e.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GenSyntheticHighCorr(
  n,
  p,
  k,
  seed,
  rho = 0,
  b0 = 0,
  snr = 1,
  mu = 0,
  base_cor = 0.9
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GenSyntheticHighCorr_+3A_n">n</code></td>
<td>
<p>Number of samples</p>
</td></tr>
<tr><td><code id="GenSyntheticHighCorr_+3A_p">p</code></td>
<td>
<p>Number of features</p>
</td></tr>
<tr><td><code id="GenSyntheticHighCorr_+3A_k">k</code></td>
<td>
<p>Number of non-zeros in true vector of coefficients</p>
</td></tr>
<tr><td><code id="GenSyntheticHighCorr_+3A_seed">seed</code></td>
<td>
<p>The seed used for randomly generating the data</p>
</td></tr>
<tr><td><code id="GenSyntheticHighCorr_+3A_rho">rho</code></td>
<td>
<p>The threshold for setting values to 0.  if |X(i, j)| &gt; rho =&gt; X(i, j) &lt;- 0</p>
</td></tr>
<tr><td><code id="GenSyntheticHighCorr_+3A_b0">b0</code></td>
<td>
<p>intercept value to scale y by.</p>
</td></tr>
<tr><td><code id="GenSyntheticHighCorr_+3A_snr">snr</code></td>
<td>
<p>desired Signal-to-Noise ratio. This sets the magnitude of the error term 'e'. 
SNR is defined as  SNR = Var(XB)/Var(e)</p>
</td></tr>
<tr><td><code id="GenSyntheticHighCorr_+3A_mu">mu</code></td>
<td>
<p>The mean for drawing from the Multivariate Normal Distribution. A scalar of vector of length p.</p>
</td></tr>
<tr><td><code id="GenSyntheticHighCorr_+3A_base_cor">base_cor</code></td>
<td>
<p>The base correlation, A in [i, j] = A^|i-j|.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
the data matrix X,
the response vector y,
the coefficients B,
the error vector e,
the intercept term b0.
</p>

<hr>
<h2 id='GenSyntheticLogistic'>Generate Logistic Synthetic Data</h2><span id='topic+GenSyntheticLogistic'></span>

<h3>Description</h3>

<p>Generates a synthetic dataset as follows: 1) Generate a data matrix, 
X, drawn from a multivariate Gaussian distribution with mean = 0, sigma = Sigma
2) Generate a vector B with k entries set to 1 and the rest are zeros.
3) Every coordinate yi of the outcome vector y exists in -1, 1^n is sampled 
independently from a Bernoulli distribution with success probability: 
P(yi = 1|xi) = 1/(1 + exp(-s&lt;xi, B&gt;))
Source https://arxiv.org/pdf/2001.06471.pdf Section 5.1 Data Generation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GenSyntheticLogistic(
  n,
  p,
  k,
  seed,
  rho = 0,
  s = 1,
  sigma = NULL,
  shuffle_B = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GenSyntheticLogistic_+3A_n">n</code></td>
<td>
<p>Number of samples</p>
</td></tr>
<tr><td><code id="GenSyntheticLogistic_+3A_p">p</code></td>
<td>
<p>Number of features</p>
</td></tr>
<tr><td><code id="GenSyntheticLogistic_+3A_k">k</code></td>
<td>
<p>Number of non-zeros in true vector of coefficients</p>
</td></tr>
<tr><td><code id="GenSyntheticLogistic_+3A_seed">seed</code></td>
<td>
<p>The seed used for randomly generating the data</p>
</td></tr>
<tr><td><code id="GenSyntheticLogistic_+3A_rho">rho</code></td>
<td>
<p>The threshold for setting values to 0.  if |X(i, j)| &gt; rho =&gt; X(i, j) &lt;- 0</p>
</td></tr>
<tr><td><code id="GenSyntheticLogistic_+3A_s">s</code></td>
<td>
<p>Signal-to-noise parameter. As s -&gt; +Inf, the data generated becomes linearly separable.</p>
</td></tr>
<tr><td><code id="GenSyntheticLogistic_+3A_sigma">sigma</code></td>
<td>
<p>Correlation matrix, defaults to I.</p>
</td></tr>
<tr><td><code id="GenSyntheticLogistic_+3A_shuffle_b">shuffle_B</code></td>
<td>
<p>A boolean flag for whether or not to randomly shuffle the Beta vector, B.
If FALSE, the first k entries in B are set to 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
the data matrix X,
the response vector y,
the coefficients B.
</p>

<hr>
<h2 id='L0Learn-package'>A package for L0-regularized learning</h2><span id='topic+L0Learn-package'></span>

<h3>Description</h3>

<p>L0Learn fits regularization paths for L0-regularized regression and classification problems. Specifically,
it can solve either one of the following problems over a grid of <code class="reqn">\lambda</code> and <code class="reqn">\gamma</code> values:
</p>
<p style="text-align: center;"><code class="reqn">\min_{\beta_0, \beta} \sum_{i=1}^{n} \ell(y_i, \beta_0+ \langle x_i, \beta \rangle) + \lambda ||\beta||_0 \quad \quad (L0)</code>
</p>

<p style="text-align: center;"><code class="reqn">\min_{\beta_0, \beta} \sum_{i=1}^{n} \ell(y_i, \beta_0+ \langle x_i, \beta \rangle) + \lambda ||\beta||_0 + \gamma||\beta||_1 \quad (L0L1)</code>
</p>

<p style="text-align: center;"><code class="reqn">\min_{\beta_0, \beta} \sum_{i=1}^{n} \ell(y_i, \beta_0+ \langle x_i, \beta \rangle) + \lambda ||\beta||_0 + \gamma||\beta||_2^2  \quad (L0L2)</code>
</p>

<p>where <code class="reqn">\ell</code> is the loss function. We currently support regression using squared error loss and classification using either logistic loss or squared hinge loss.
Pathwise optimization can be done using either cyclic coordinate descent (CD) or local combinatorial search. The core of the toolkit is implemented in C++ and employs
many computational tricks and heuristics, leading to competitive running times. CD runs very fast and typically
leads to relatively good solutions. Local combinatorial search can find higher-quality solutions (at the
expense of increased running times).
The toolkit has the following six main methods:
</p>

<ul>
<li><p><code><a href="#topic+L0Learn.fit">L0Learn.fit</a></code>: Fits an L0-regularized model.
</p>
</li>
<li><p><code><a href="#topic+L0Learn.cvfit">L0Learn.cvfit</a></code>: Performs k-fold cross-validation.
</p>
</li>
<li><p><code><a href="#topic+print.L0Learn">print</a></code>: Prints a summary of the path.
</p>
</li>
<li><p><code><a href="#topic+coef.L0Learn">coef</a></code>: Extracts solutions(s) from the path.
</p>
</li>
<li><p><code><a href="#topic+predict.L0Learn">predict</a></code>: Predicts response using a solution in the path.
</p>
</li>
<li><p><code><a href="#topic+plot.L0Learn">plot</a></code>: Plots the regularization path or cross-validation error.
</p>
</li></ul>



<h3>References</h3>

<p>Hazimeh and Mazumder. Fast Best Subset Selection: Coordinate Descent and Local Combinatorial
Optimization Algorithms. Operations Research (2020). <a href="https://pubsonline.informs.org/doi/10.1287/opre.2019.1919">https://pubsonline.informs.org/doi/10.1287/opre.2019.1919</a>.
</p>

<hr>
<h2 id='L0Learn.cvfit'>Cross Validation</h2><span id='topic+L0Learn.cvfit'></span>

<h3>Description</h3>

<p>Computes a regularization path and performs K-fold cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>L0Learn.cvfit(
  x,
  y,
  loss = "SquaredError",
  penalty = "L0",
  algorithm = "CD",
  maxSuppSize = 100,
  nLambda = 100,
  nGamma = 10,
  gammaMax = 10,
  gammaMin = 1e-04,
  partialSort = TRUE,
  maxIters = 200,
  rtol = 1e-06,
  atol = 1e-09,
  activeSet = TRUE,
  activeSetNum = 3,
  maxSwaps = 100,
  scaleDownFactor = 0.8,
  screenSize = 1000,
  autoLambda = NULL,
  lambdaGrid = list(),
  nFolds = 10,
  seed = 1,
  excludeFirstK = 0,
  intercept = TRUE,
  lows = -Inf,
  highs = Inf
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="L0Learn.cvfit_+3A_x">x</code></td>
<td>
<p>The data matrix.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_y">y</code></td>
<td>
<p>The response vector. For classification, we only support binary vectors.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_loss">loss</code></td>
<td>
<p>The loss function. Currently we support the choices &quot;SquaredError&quot; (for regression), &quot;Logistic&quot; (for logistic regression), and &quot;SquaredHinge&quot; (for smooth SVM).</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_penalty">penalty</code></td>
<td>
<p>The type of regularization. This can take either one of the following choices:
&quot;L0&quot;, &quot;L0L2&quot;, and &quot;L0L1&quot;.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_algorithm">algorithm</code></td>
<td>
<p>The type of algorithm used to minimize the objective function. Currently &quot;CD&quot; and &quot;CDPSI&quot; are
are supported. &quot;CD&quot; is a variant of cyclic coordinate descent and runs very fast. &quot;CDPSI&quot; performs
local combinatorial search on top of CD and typically achieves higher quality solutions (at the expense
of increased running time).</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_maxsuppsize">maxSuppSize</code></td>
<td>
<p>The maximum support size at which to terminate the regularization path. We recommend setting
this to a small fraction of min(n,p) (e.g. 0.05 * min(n,p)) as L0 regularization typically selects a small
portion of non-zeros.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_nlambda">nLambda</code></td>
<td>
<p>The number of Lambda values to select (recall that Lambda is the regularization parameter
corresponding to the L0 norm). This value is ignored if 'lambdaGrid' is supplied.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_ngamma">nGamma</code></td>
<td>
<p>The number of Gamma values to select (recall that Gamma is the regularization parameter
corresponding to L1 or L2, depending on the chosen penalty). This value is ignored if 'lambdaGrid' is supplied 
and will be set to length(lambdaGrid)</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_gammamax">gammaMax</code></td>
<td>
<p>The maximum value of Gamma when using the L0L2 penalty. For the L0L1 penalty this is
automatically selected.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_gammamin">gammaMin</code></td>
<td>
<p>The minimum value of Gamma when using the L0L2 penalty. For the L0L1 penalty, the minimum
value of gamma in the grid is set to gammaMin * gammaMax. Note that this should be a strictly positive quantity.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_partialsort">partialSort</code></td>
<td>
<p>If TRUE partial sorting will be used for sorting the coordinates to do greedy cycling (see our paper for
for details). Otherwise, full sorting is used.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_maxiters">maxIters</code></td>
<td>
<p>The maximum number of iterations (full cycles) for CD per grid point.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_rtol">rtol</code></td>
<td>
<p>The relative tolerance which decides when to terminate optimization (based on the relative change in the objective between iterations).</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_atol">atol</code></td>
<td>
<p>The absolute tolerance which decides when to terminate optimization (based on the absolute L2 norm of the residuals).</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_activeset">activeSet</code></td>
<td>
<p>If TRUE, performs active set updates.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_activesetnum">activeSetNum</code></td>
<td>
<p>The number of consecutive times a support should appear before declaring support stabilization.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_maxswaps">maxSwaps</code></td>
<td>
<p>The maximum number of swaps used by CDPSI for each grid point.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_scaledownfactor">scaleDownFactor</code></td>
<td>
<p>This parameter decides how close the selected Lambda values are. The choice should be
strictly between 0 and 1 (i.e., 0 and 1 are not allowed). Larger values lead to closer lambdas and typically to smaller
gaps between the support sizes. For details, see our paper - Section 5 on Adaptive Selection of Tuning Parameters).</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_screensize">screenSize</code></td>
<td>
<p>The number of coordinates to cycle over when performing initial correlation screening.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_autolambda">autoLambda</code></td>
<td>
<p>Ignored parameter. Kept for backwards compatibility.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_lambdagrid">lambdaGrid</code></td>
<td>
<p>A grid of Lambda values to use in computing the regularization path. This is by default an empty list and is ignored.
When specified, LambdaGrid should be a list of length 'nGamma', where the ith element (corresponding to the ith gamma) should be a decreasing sequence of lambda values
which are used by the algorithm when fitting for the ith value of gamma (see the vignette for details).</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_nfolds">nFolds</code></td>
<td>
<p>The number of folds for cross-validation.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_seed">seed</code></td>
<td>
<p>The seed used in randomly shuffling the data for cross-validation.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_excludefirstk">excludeFirstK</code></td>
<td>
<p>This parameter takes non-negative integers. The first excludeFirstK features in x will be excluded from variable selection,
i.e., the first excludeFirstK variables will not be included in the L0-norm penalty (they will still be included in the L1 or L2 norm penalties.).</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_intercept">intercept</code></td>
<td>
<p>If FALSE, no intercept term is included in the model.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_lows">lows</code></td>
<td>
<p>Lower bounds for coefficients. Either a scalar for all coefficients to have the same bound or a vector of size p (number of columns of X) where lows[i] is the lower bound for coefficient i.</p>
</td></tr>
<tr><td><code id="L0Learn.cvfit_+3A_highs">highs</code></td>
<td>
<p>Upper bounds for coefficients. Either a scalar for all coefficients to have the same bound or a vector of size p (number of columns of X) where highs[i] is the upper bound for coefficient i.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An S3 object of type &quot;L0LearnCV&quot; describing the regularization path. The object has the following members.
</p>
<table>
<tr><td><code>cvMeans</code></td>
<td>
<p>This is a list, where the ith element is the sequence of cross-validation errors corresponding to the ith gamma value, i.e., the sequence
cvMeans[[i]] corresponds to fit$gamma[i]</p>
</td></tr>
<tr><td><code>cvSDs</code></td>
<td>
<p>This a list, where the ith element is a sequence of standard deviations for the cross-validation errors: cvSDs[[i]] corresponds to cvMeans[[i]].</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>The fitted model with type &quot;L0Learn&quot;, i.e., this is the same object returned by <code><a href="#topic+L0Learn.fit">L0Learn.fit</a></code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Generate synthetic data for this example
data &lt;- GenSynthetic(n=100,p=20,k=10,seed=1)
X = data$X
y = data$y
#'
# Perform 3-fold cross-validation on an L0L2 regression model with 3 values of
# Gamma ranging from 0.0001 to 10
fit &lt;- L0Learn.cvfit(X, y, nFolds=3, seed=1, penalty="L0L2", maxSuppSize=20, nGamma=3,
gammaMin=0.0001, gammaMax = 10)
print(fit)
# Plot the graph of cross-validation error versus lambda for gamma = 0.0001
plot(fit, gamma=0.0001)
# Extract the coefficients at lambda = 0.0361829 and gamma = 0.0001
coef(fit, lambda=2.45513e-02, gamma=0.0001)
# Apply the fitted model on X to predict the response
predict(fit, newx = X, lambda=2.45513e-02, gamma=0.0001)

</code></pre>

<hr>
<h2 id='L0Learn.fit'>Fit an L0-regularized model</h2><span id='topic+L0Learn.fit'></span>

<h3>Description</h3>

<p>Computes the regularization path for the specified loss function and
penalty function (which can be a combination of the L0, L1, and L2 norms).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>L0Learn.fit(
  x,
  y,
  loss = "SquaredError",
  penalty = "L0",
  algorithm = "CD",
  maxSuppSize = 100,
  nLambda = 100,
  nGamma = 10,
  gammaMax = 10,
  gammaMin = 1e-04,
  partialSort = TRUE,
  maxIters = 200,
  rtol = 1e-06,
  atol = 1e-09,
  activeSet = TRUE,
  activeSetNum = 3,
  maxSwaps = 100,
  scaleDownFactor = 0.8,
  screenSize = 1000,
  autoLambda = NULL,
  lambdaGrid = list(),
  excludeFirstK = 0,
  intercept = TRUE,
  lows = -Inf,
  highs = Inf
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="L0Learn.fit_+3A_x">x</code></td>
<td>
<p>The data matrix.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_y">y</code></td>
<td>
<p>The response vector. For classification, we only support binary vectors.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_loss">loss</code></td>
<td>
<p>The loss function. Currently we support the choices &quot;SquaredError&quot; (for regression), &quot;Logistic&quot; (for logistic regression), and &quot;SquaredHinge&quot; (for smooth SVM).</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_penalty">penalty</code></td>
<td>
<p>The type of regularization. This can take either one of the following choices:
&quot;L0&quot;, &quot;L0L2&quot;, and &quot;L0L1&quot;.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_algorithm">algorithm</code></td>
<td>
<p>The type of algorithm used to minimize the objective function. Currently &quot;CD&quot; and &quot;CDPSI&quot; are
are supported. &quot;CD&quot; is a variant of cyclic coordinate descent and runs very fast. &quot;CDPSI&quot; performs
local combinatorial search on top of CD and typically achieves higher quality solutions (at the expense
of increased running time).</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_maxsuppsize">maxSuppSize</code></td>
<td>
<p>The maximum support size at which to terminate the regularization path. We recommend setting
this to a small fraction of min(n,p) (e.g. 0.05 * min(n,p)) as L0 regularization typically selects a small
portion of non-zeros.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_nlambda">nLambda</code></td>
<td>
<p>The number of Lambda values to select (recall that Lambda is the regularization parameter
corresponding to the L0 norm). This value is ignored if 'lambdaGrid' is supplied.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_ngamma">nGamma</code></td>
<td>
<p>The number of Gamma values to select (recall that Gamma is the regularization parameter
corresponding to L1 or L2, depending on the chosen penalty). This value is ignored if 'lambdaGrid' is supplied 
and will be set to length(lambdaGrid)</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_gammamax">gammaMax</code></td>
<td>
<p>The maximum value of Gamma when using the L0L2 penalty. For the L0L1 penalty this is
automatically selected.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_gammamin">gammaMin</code></td>
<td>
<p>The minimum value of Gamma when using the L0L2 penalty. For the L0L1 penalty, the minimum
value of gamma in the grid is set to gammaMin * gammaMax. Note that this should be a strictly positive quantity.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_partialsort">partialSort</code></td>
<td>
<p>If TRUE partial sorting will be used for sorting the coordinates to do greedy cycling (see our paper for
for details). Otherwise, full sorting is used.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_maxiters">maxIters</code></td>
<td>
<p>The maximum number of iterations (full cycles) for CD per grid point.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_rtol">rtol</code></td>
<td>
<p>The relative tolerance which decides when to terminate optimization (based on the relative change in the objective between iterations).</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_atol">atol</code></td>
<td>
<p>The absolute tolerance which decides when to terminate optimization (based on the absolute L2 norm of the residuals).</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_activeset">activeSet</code></td>
<td>
<p>If TRUE, performs active set updates.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_activesetnum">activeSetNum</code></td>
<td>
<p>The number of consecutive times a support should appear before declaring support stabilization.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_maxswaps">maxSwaps</code></td>
<td>
<p>The maximum number of swaps used by CDPSI for each grid point.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_scaledownfactor">scaleDownFactor</code></td>
<td>
<p>This parameter decides how close the selected Lambda values are. The choice should be
strictly between 0 and 1 (i.e., 0 and 1 are not allowed). Larger values lead to closer lambdas and typically to smaller
gaps between the support sizes. For details, see our paper - Section 5 on Adaptive Selection of Tuning Parameters).</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_screensize">screenSize</code></td>
<td>
<p>The number of coordinates to cycle over when performing initial correlation screening.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_autolambda">autoLambda</code></td>
<td>
<p>Ignored parameter. Kept for backwards compatibility.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_lambdagrid">lambdaGrid</code></td>
<td>
<p>A grid of Lambda values to use in computing the regularization path. This is by default an empty list and is ignored.
When specified, LambdaGrid should be a list of length 'nGamma', where the ith element (corresponding to the ith gamma) should be a decreasing sequence of lambda values
which are used by the algorithm when fitting for the ith value of gamma (see the vignette for details).</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_excludefirstk">excludeFirstK</code></td>
<td>
<p>This parameter takes non-negative integers. The first excludeFirstK features in x will be excluded from variable selection,
i.e., the first excludeFirstK variables will not be included in the L0-norm penalty (they will still be included in the L1 or L2 norm penalties.).</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_intercept">intercept</code></td>
<td>
<p>If FALSE, no intercept term is included in the model.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_lows">lows</code></td>
<td>
<p>Lower bounds for coefficients. Either a scalar for all coefficients to have the same bound or a vector of size p (number of columns of X) where lows[i] is the lower bound for coefficient i.</p>
</td></tr>
<tr><td><code id="L0Learn.fit_+3A_highs">highs</code></td>
<td>
<p>Upper bounds for coefficients. Either a scalar for all coefficients to have the same bound or a vector of size p (number of columns of X) where highs[i] is the upper bound for coefficient i.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An S3 object of type &quot;L0Learn&quot; describing the regularization path. The object has the following members.
</p>
<table>
<tr><td><code>a0</code></td>
<td>
<p>a0 is a list of intercept sequences. The ith element of the list (i.e., a0[[i]]) is the sequence of intercepts corresponding to the ith gamma value (i.e., gamma[i]).</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>This is a list of coefficient matrices. The ith element of the list is a p x <code>length(lambda)</code> matrix which
corresponds to the ith gamma value. The jth column in each coefficient matrix is the vector of coefficients for the jth lambda value.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>This is the list of lambda sequences used in fitting the model. The ith element of lambda (i.e., lambda[[i]]) is the sequence
of Lambda values corresponding to the ith gamma value.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>This is the sequence of gamma values used in fitting the model.</p>
</td></tr>
<tr><td><code>suppSize</code></td>
<td>
<p>This is a list of support size sequences. The ith element of the list is a sequence of support sizes (i.e., number of non-zero coefficients)
corresponding to the ith gamma value.</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>This is a list of sequences for checking whether the algorithm has converged at every grid point. The ith element of the list is a sequence
corresponding to the ith value of gamma, where the jth element in each sequence indicates whether the algorithm has converged at the jth value of lambda.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Generate synthetic data for this example
data &lt;- GenSynthetic(n=100,p=20,k=10,seed=1)
X = data$X
y = data$y

# Fit an L0 regression model with a maximum of 20 non-zeros using coordinate descent (CD)
fit1 &lt;- L0Learn.fit(X, y, penalty="L0", maxSuppSize=20)
print(fit1)
# Extract the coefficients at lambda = 2.28552e-02
coef(fit1, lambda=2.28552e-02)
# Apply the fitted model on X to predict the response
predict(fit1, newx = X, lambda=2.28552e-02)

# Fit an L0 regression model with a maximum of 20 non-zeros using CD and local search
fit2 &lt;- L0Learn.fit(X, y, penalty="L0", algorithm="CDPSI", maxSuppSize=20)
print(fit2)

# Fit an L0L2 regression model with 3 values of Gamma ranging from 0.0001 to 10, using CD
fit3 &lt;- L0Learn.fit(X, y, penalty="L0L2", maxSuppSize=20, nGamma=3, gammaMin=0.0001, gammaMax = 10)
print(fit3)
# Extract the coefficients at lambda = 2.45513e-02 and gamma = 0.0001
coef(fit3, lambda=2.45513e-02, gamma=0.0001)
# Apply the fitted model on X to predict the response
predict(fit3, newx = X, lambda=2.45513e-02, gamma=0.0001)

# Fit an L0 logistic regression model
# First, convert the response to binary
y = sign(y)
fit4 &lt;- L0Learn.fit(X, y, loss="Logistic", maxSuppSize=10)
print(fit4)

</code></pre>

<hr>
<h2 id='plot.L0Learn'>Plot Regularization Path</h2><span id='topic+plot.L0Learn'></span>

<h3>Description</h3>

<p>Plots the regularization path for a given gamma.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'L0Learn'
plot(x, gamma = 0, showLines = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.L0Learn_+3A_x">x</code></td>
<td>
<p>The output of L0Learn.fit</p>
</td></tr>
<tr><td><code id="plot.L0Learn_+3A_gamma">gamma</code></td>
<td>
<p>The value of gamma at which to plot.</p>
</td></tr>
<tr><td><code id="plot.L0Learn_+3A_showlines">showLines</code></td>
<td>
<p>If TRUE, the lines connecting the points in the plot are shown.</p>
</td></tr>
<tr><td><code id="plot.L0Learn_+3A_...">...</code></td>
<td>
<p>ignore</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate synthetic data for this example
data &lt;- GenSynthetic(n=100,p=20,k=10,seed=1)
X = data$X
y = data$y
# Fit an L0 Model
fit &lt;- L0Learn.fit(X, y, penalty="L0")
plot(fit, gamma=0)

</code></pre>

<hr>
<h2 id='plot.L0LearnCV'>Plot Cross-validation Errors</h2><span id='topic+plot.L0LearnCV'></span>

<h3>Description</h3>

<p>Plots cross-validation errors for a given gamma.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'L0LearnCV'
plot(x, gamma = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.L0LearnCV_+3A_x">x</code></td>
<td>
<p>The output of L0Learn.cvfit</p>
</td></tr>
<tr><td><code id="plot.L0LearnCV_+3A_gamma">gamma</code></td>
<td>
<p>The value of gamma at which to plot.</p>
</td></tr>
<tr><td><code id="plot.L0LearnCV_+3A_...">...</code></td>
<td>
<p>ignore</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate synthetic data for this example
data &lt;- GenSynthetic(n=100,p=20,k=10,seed=1)
X = data$X
y = data$y

# Perform 3-fold cross-validation on an L0L2 Model with 3 values of
# Gamma ranging from 0.0001 to 10
fit &lt;- L0Learn.cvfit(X, y, nFolds=3, seed=1, penalty="L0L2",
maxSuppSize=20, nGamma=3, gammaMin=0.0001, gammaMax = 10)
# Plot the graph of cross-validation error versus lambda for gamma = 0.0001
plot(fit, gamma=0.0001)

</code></pre>

<hr>
<h2 id='predict.L0Learn'>Predict Response</h2><span id='topic+predict.L0Learn'></span><span id='topic+predict.L0LearnCV'></span>

<h3>Description</h3>

<p>Predicts the response for a given sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'L0Learn'
predict(object, newx, lambda = NULL, gamma = NULL, ...)

## S3 method for class 'L0LearnCV'
predict(object, newx, lambda = NULL, gamma = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.L0Learn_+3A_object">object</code></td>
<td>
<p>The output of L0Learn.fit or L0Learn.cvfit</p>
</td></tr>
<tr><td><code id="predict.L0Learn_+3A_newx">newx</code></td>
<td>
<p>A matrix on which predictions are made. The matrix should have p columns.</p>
</td></tr>
<tr><td><code id="predict.L0Learn_+3A_lambda">lambda</code></td>
<td>
<p>The value of lambda to use for prediction. A summary of the lambdas in the regularization
path can be obtained using <code>print(fit)</code>.</p>
</td></tr>
<tr><td><code id="predict.L0Learn_+3A_gamma">gamma</code></td>
<td>
<p>The value of gamma to use for prediction. A summary of the gammas in the regularization
path can be obtained using <code>print(fit)</code>.</p>
</td></tr>
<tr><td><code id="predict.L0Learn_+3A_...">...</code></td>
<td>
<p>ignore</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A Matrix of class <code>dgeMatrix</code>, which contains the model
predictions. If both lambda and gamma are not supplied, then a matrix of
predictions for all the solutions in the regularization path is returned.
If lambda is supplied but gamma is not, the smallest value of gamma is used.
In case of logistic regression, probability values are returned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate synthetic data for this example
data &lt;- GenSynthetic(n=100,p=20,k=10,seed=1)
X = data$X
y = data$y

# Fit an L0L2 Model with 3 values of Gamma ranging from 0.0001 to 10, using coordinate descent
fit &lt;- L0Learn.fit(X,y, penalty="L0L2", nGamma=3, gammaMin=0.0001, gammaMax = 10)
print(fit)
# Apply the fitted model with lambda=2.45513e-02 and gamma=0.0001 on X to predict the response
predict(fit, newx = X, lambda=2.45513e-02, gamma=0.0001)
# Apply the fitted model on X to predict the response for all the solutions in the path
predict(fit, newx = X)

</code></pre>

<hr>
<h2 id='print.L0Learn'>Print L0Learn.fit object</h2><span id='topic+print.L0Learn'></span><span id='topic+print.L0LearnCV'></span>

<h3>Description</h3>

<p>Prints a summary of L0Learn.fit
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'L0Learn'
print(x, ...)

## S3 method for class 'L0LearnCV'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.L0Learn_+3A_x">x</code></td>
<td>
<p>The output of L0Learn.fit or L0Learn.cvfit</p>
</td></tr>
<tr><td><code id="print.L0Learn_+3A_...">...</code></td>
<td>
<p>ignore</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Prints a summary of the models to the console.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
