<!DOCTYPE html><html><head><title>Help for package hdme</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {hdme}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#coef.corrected_lasso'><p>Extract Coefficients of a Corrected Lasso object</p></a></li>
<li><a href='#coef.gds'><p>Extract Coefficients of a Generalized Dantzig Selector Object</p></a></li>
<li><a href='#coef.gmu_lasso'><p>Extract Coefficients of a GMU Lasso object</p></a></li>
<li><a href='#coef.gmus'><p>Extract Coefficients of a GMUS object</p></a></li>
<li><a href='#corrected_lasso'><p>Corrected Lasso</p></a></li>
<li><a href='#cv_corrected_lasso'><p>Cross-validated Corrected lasso</p></a></li>
<li><a href='#cv_gds'><p>Cross-Validated Generalized Dantzig Selector</p></a></li>
<li><a href='#gds'><p>Generalized Dantzig Selector</p></a></li>
<li><a href='#gmu_lasso'><p>Generalized Matrix Uncertainty Lasso</p></a></li>
<li><a href='#gmus'><p>Generalized Matrix Uncertainty Selector</p></a></li>
<li><a href='#mus'><p>Matrix Uncertainty Selector</p></a></li>
<li><a href='#mus_glm'><p>Generalized Matrix Uncertainty Selector for logistic regression</p></a></li>
<li><a href='#musalgorithm'><p>Algorithm for mus</p></a></li>
<li><a href='#plot.corrected_lasso'><p>plot.corrected_lasso</p></a></li>
<li><a href='#plot.cv_corrected_lasso'><p>plot.cv_corrected_lasso</p></a></li>
<li><a href='#plot.cv_gds'><p>plot.cv_gds</p></a></li>
<li><a href='#plot.gds'><p>Plot the estimates returned by gds</p></a></li>
<li><a href='#plot.gmu_lasso'><p>Plot the estimates returned by gmu_lasso</p></a></li>
<li><a href='#plot.gmus'><p>Plot the estimates returned by gmus and mus</p></a></li>
<li><a href='#print.corrected_lasso'><p>Print a Corrected Lasso object</p></a></li>
<li><a href='#print.cv_corrected_lasso'><p>Print a Cross-Validated Corrected Lasso object</p></a></li>
<li><a href='#print.cv_gds'><p>Print a Cross-Validated GDS Object</p></a></li>
<li><a href='#print.gds'><p>Print a Generalized Dantzig Selector Object</p></a></li>
<li><a href='#print.gmu_lasso'><p>Print a GMU Lasso object</p></a></li>
<li><a href='#print.gmus'><p>Print a GMUS object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>High-Dimensional Regression with Measurement Error</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.0</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Oystein Sorensen &lt;oystein.sorensen.1985@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Penalized regression for generalized linear models for
  measurement error problems (aka. errors-in-variables). The package
  contains a version of the lasso (L1-penalization) which corrects
  for measurement error (Sorensen et al. (2015) &lt;<a href="https://doi.org/10.5705%2Fss.2013.180">doi:10.5705/ss.2013.180</a>&gt;). 
  It also contains an implementation of the Generalized Matrix Uncertainty 
  Selector, which is a version the (Generalized) Dantzig Selector for the 
  case of measurement error (Sorensen et al. (2018) &lt;<a href="https://doi.org/10.1080%2F10618600.2018.1425626">doi:10.1080/10618600.2018.1425626</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet (&ge; 3.0.0), ggplot2 (&ge; 2.2.1), Rdpack, Rcpp (&ge;
0.12.15), Rglpk (&ge; 0.6-1), rlang (&ge; 1.0), stats</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/osorensen/hdme">https://github.com/osorensen/hdme</a></td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat, dplyr, tidyr, covr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-16 18:52:58 UTC; oyss</td>
</tr>
<tr>
<td>Author:</td>
<td>Oystein Sorensen <a href="https://orcid.org/0000-0003-0724-3542"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-16 19:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='coef.corrected_lasso'>Extract Coefficients of a Corrected Lasso object</h2><span id='topic+coef.corrected_lasso'></span>

<h3>Description</h3>

<p>Default coef method for a <code>corrected_lasso</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'corrected_lasso'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.corrected_lasso_+3A_object">object</code></td>
<td>
<p>Fitted model object returned by <code><a href="#topic+corrected_lasso">corrected_lasso</a></code>.</p>
</td></tr>
<tr><td><code id="coef.corrected_lasso_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='coef.gds'>Extract Coefficients of a Generalized Dantzig Selector Object</h2><span id='topic+coef.gds'></span>

<h3>Description</h3>

<p>Default coef method for a <code>gds</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gds'
coef(object, all = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.gds_+3A_object">object</code></td>
<td>
<p>Fitted model object returned by <code><a href="#topic+gds">gds</a></code>.</p>
</td></tr>
<tr><td><code id="coef.gds_+3A_all">all</code></td>
<td>
<p>Logical indicating whether to show all coefficient estimates, or only non-zeros.</p>
</td></tr>
<tr><td><code id="coef.gds_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='coef.gmu_lasso'>Extract Coefficients of a GMU Lasso object</h2><span id='topic+coef.gmu_lasso'></span>

<h3>Description</h3>

<p>Default coef method for a <code>gmu_lasso</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gmu_lasso'
coef(object, all = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.gmu_lasso_+3A_object">object</code></td>
<td>
<p>Fitted model object returned by <code><a href="#topic+gmu_lasso">gmu_lasso</a></code>.</p>
</td></tr>
<tr><td><code id="coef.gmu_lasso_+3A_all">all</code></td>
<td>
<p>Logical indicating whether to show all coefficient estimates, or
only non-zeros. Only used when delta is a single value.</p>
</td></tr>
<tr><td><code id="coef.gmu_lasso_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='coef.gmus'>Extract Coefficients of a GMUS object</h2><span id='topic+coef.gmus'></span>

<h3>Description</h3>

<p>Default coef method for a <code>gmus</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gmus'
coef(object, all = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.gmus_+3A_object">object</code></td>
<td>
<p>Fitted model object returned by <code><a href="#topic+gmus">gmus</a></code>.</p>
</td></tr>
<tr><td><code id="coef.gmus_+3A_all">all</code></td>
<td>
<p>Logical indicating whether to show all coefficient estimates, or
only non-zeros. Only used when delta is a single value.</p>
</td></tr>
<tr><td><code id="coef.gmus_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='corrected_lasso'>Corrected Lasso</h2><span id='topic+corrected_lasso'></span>

<h3>Description</h3>

<p>Lasso (L1-regularization) for generalized linear models with
measurement error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corrected_lasso(
  W,
  y,
  sigmaUU,
  family = c("gaussian", "binomial", "poisson"),
  radii = NULL,
  no_radii = NULL,
  alpha = 0.1,
  maxits = 5000,
  tol = 1e-12
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corrected_lasso_+3A_w">W</code></td>
<td>
<p>Design matrix, measured with error. Must be a numeric matrix.</p>
</td></tr>
<tr><td><code id="corrected_lasso_+3A_y">y</code></td>
<td>
<p>Vector of responses.</p>
</td></tr>
<tr><td><code id="corrected_lasso_+3A_sigmauu">sigmaUU</code></td>
<td>
<p>Covariance matrix of the measurement error.</p>
</td></tr>
<tr><td><code id="corrected_lasso_+3A_family">family</code></td>
<td>
<p>Response type. Character string of length 1. Possible values are
&quot;gaussian&quot;, &quot;binomial&quot; and &quot;poisson&quot;.</p>
</td></tr>
<tr><td><code id="corrected_lasso_+3A_radii">radii</code></td>
<td>
<p>Vector containing the set of radii of the l1-ball onto which the
solution is projected. If not provided, the algorithm will select an evenly
spaced vector of 20 radii.</p>
</td></tr>
<tr><td><code id="corrected_lasso_+3A_no_radii">no_radii</code></td>
<td>
<p>Length of vector radii, i.e., the number of regularization
parameters to fit the corrected lasso for.</p>
</td></tr>
<tr><td><code id="corrected_lasso_+3A_alpha">alpha</code></td>
<td>
<p>Step size of the projected gradient descent algorithm. Default is
0.1.</p>
</td></tr>
<tr><td><code id="corrected_lasso_+3A_maxits">maxits</code></td>
<td>
<p>Maximum number of iterations of the project gradient descent
algorithm for each radius. Default is 5000.</p>
</td></tr>
<tr><td><code id="corrected_lasso_+3A_tol">tol</code></td>
<td>
<p>Iteration tolerance for change in sum of squares of beta. Defaults
. to 1e-12.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Corrected version of the lasso for generalized linear models. The
method does require an estimate of the measurement error covariance matrix.
The Poisson regression option might sensitive to numerical overflow, please
file a GitHub issue in the source repository if you experience this.
</p>


<h3>Value</h3>

<p>An object of class &quot;corrected_lasso&quot;.
</p>


<h3>References</h3>

<p>Loh P, Wainwright MJ (2012).
&ldquo;High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity.&rdquo;
<em>Ann. Statist.</em>, <b>40</b>(3), 1637&ndash;1664.
</p>
<p>Sorensen O, Frigessi A, Thoresen M (2015).
&ldquo;Measurement error in lasso: Impact and likelihood bias correction.&rdquo;
<em>Statistica Sinica</em>, <b>25</b>(2), 809-829.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with linear regression
# Number of samples
n &lt;- 100
# Number of covariates
p &lt;- 50
# True (latent) variables
X &lt;- matrix(rnorm(n * p), nrow = n)
# Measurement error covariance matrix
# (typically estimated by replicate measurements)
sigmaUU &lt;- diag(x = 0.2, nrow = p, ncol = p)
# Measurement matrix (this is the one we observe)
W &lt;- X + rnorm(n, sd = sqrt(diag(sigmaUU)))
# Coefficient
beta &lt;- c(seq(from = 0.1, to = 1, length.out = 5), rep(0, p-5))
# Response
y &lt;- X %*% beta + rnorm(n, sd = 1)
# Run the corrected lasso
fit &lt;- corrected_lasso(W, y, sigmaUU, family = "gaussian")
coef(fit)
plot(fit)
plot(fit, type = "path")

# Binomial, logistic regression
# Number of samples
n &lt;- 1000
# Number of covariates
p &lt;- 50
# True (latent) variables
X &lt;- matrix(rnorm(n * p), nrow = n)
# Measurement error covariance matrix
sigmaUU &lt;- diag(x = 0.2, nrow = p, ncol = p)
# Measurement matrix (this is the one we observe)
W &lt;- X + rnorm(n, sd = sqrt(diag(sigmaUU)))
# Response
y &lt;- rbinom(n, size = 1, prob = plogis(X %*% c(rep(5, 5), rep(0, p-5))))
fit &lt;- corrected_lasso(W, y, sigmaUU, family = "binomial")
plot(fit)
coef(fit)

</code></pre>

<hr>
<h2 id='cv_corrected_lasso'>Cross-validated Corrected lasso</h2><span id='topic+cv_corrected_lasso'></span>

<h3>Description</h3>

<p>Cross-validated Corrected lasso
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_corrected_lasso(
  W,
  y,
  sigmaUU,
  n_folds = 10,
  family = "gaussian",
  radii = NULL,
  no_radii = 100,
  alpha = 0.1,
  maxits = 5000,
  tol = 1e-12
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_corrected_lasso_+3A_w">W</code></td>
<td>
<p>Design matrix, measured with error.</p>
</td></tr>
<tr><td><code id="cv_corrected_lasso_+3A_y">y</code></td>
<td>
<p>Vector of the continuous response value.</p>
</td></tr>
<tr><td><code id="cv_corrected_lasso_+3A_sigmauu">sigmaUU</code></td>
<td>
<p>Covariance matrix of the measurement error.</p>
</td></tr>
<tr><td><code id="cv_corrected_lasso_+3A_n_folds">n_folds</code></td>
<td>
<p>Number of folds to use in cross-validation. Default is 100.</p>
</td></tr>
<tr><td><code id="cv_corrected_lasso_+3A_family">family</code></td>
<td>
<p>Only &quot;gaussian&quot; is implemented at the moment.</p>
</td></tr>
<tr><td><code id="cv_corrected_lasso_+3A_radii">radii</code></td>
<td>
<p>Optional vector containing the set of radii of the l1-ball onto
which the solution is projected.</p>
</td></tr>
<tr><td><code id="cv_corrected_lasso_+3A_no_radii">no_radii</code></td>
<td>
<p>Length of vector radii, i.e., the number of regularization
parameters to fit the corrected lasso for.</p>
</td></tr>
<tr><td><code id="cv_corrected_lasso_+3A_alpha">alpha</code></td>
<td>
<p>Optional step size of the projected gradient descent algorithm.
Default is 0.1.</p>
</td></tr>
<tr><td><code id="cv_corrected_lasso_+3A_maxits">maxits</code></td>
<td>
<p>Optional maximum number of iterations of the project gradient
descent algorithm for each radius. Default is 5000.</p>
</td></tr>
<tr><td><code id="cv_corrected_lasso_+3A_tol">tol</code></td>
<td>
<p>Iteration tolerance for change in sum of squares of beta. Defaults
to 1e-12.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Corrected version of the lasso for the case of linear regression,
estimated using cross-validation. The method does require an estimate of
the measurement error covariance matrix.
</p>


<h3>Value</h3>

<p>An object of class &quot;cv_corrected_lasso&quot;.
</p>


<h3>References</h3>

<p>Loh P, Wainwright MJ (2012).
&ldquo;High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity.&rdquo;
<em>Ann. Statist.</em>, <b>40</b>(3), 1637&ndash;1664.
</p>
<p>Sorensen O, Frigessi A, Thoresen M (2015).
&ldquo;Measurement error in lasso: Impact and likelihood bias correction.&rdquo;
<em>Statistica Sinica</em>, <b>25</b>(2), 809-829.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Gaussian
set.seed(100)
n &lt;- 100; p &lt;- 50 # Problem dimensions
# True (latent) variables
X &lt;- matrix(rnorm(n * p), nrow = n)
# Measurement error covariance matrix
# (typically estimated by replicate measurements)
sigmaUU &lt;- diag(x = 0.2, nrow = p, ncol = p)
# Measurement matrix (this is the one we observe)
W &lt;- X + rnorm(n, sd = sqrt(diag(sigmaUU)))
# Coefficient
beta &lt;- c(seq(from = 0.1, to = 1, length.out = 5), rep(0, p-5))
# Response
y &lt;- X %*% beta + rnorm(n, sd = 1)
# Run the corrected lasso
cvfit &lt;- cv_corrected_lasso(W, y, sigmaUU, no_radii = 5, n_folds = 3)
plot(cvfit)
print(cvfit)
# Run the standard lasso using the radius found by cross-validation
fit &lt;- corrected_lasso(W, y, sigmaUU, family = "gaussian",
radii = cvfit$radius_min)
coef(fit)
plot(fit)
</code></pre>

<hr>
<h2 id='cv_gds'>Cross-Validated Generalized Dantzig Selector</h2><span id='topic+cv_gds'></span>

<h3>Description</h3>

<p>Generalized Dantzig Selector with cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_gds(
  X,
  y,
  family = "gaussian",
  no_lambda = 10,
  lambda = NULL,
  n_folds = 5,
  weights = rep(1, length(y))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_gds_+3A_x">X</code></td>
<td>
<p>Design matrix.</p>
</td></tr>
<tr><td><code id="cv_gds_+3A_y">y</code></td>
<td>
<p>Vector of the continuous response value.</p>
</td></tr>
<tr><td><code id="cv_gds_+3A_family">family</code></td>
<td>
<p>Use &quot;gaussian&quot; for linear regression, &quot;binomial&quot; for logistic
regression and &quot;poisson&quot; for Poisson regression.</p>
</td></tr>
<tr><td><code id="cv_gds_+3A_no_lambda">no_lambda</code></td>
<td>
<p>Length of the vector <code>lambda</code> of regularization
parameters. Note that if <code>lambda</code> is not provided, the actual number
of values might differ slightly, due to the algorithm used by
<code>glmnet::glmnet</code> in finding a grid of <code>lambda</code> values.</p>
</td></tr>
<tr><td><code id="cv_gds_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter. If not supplied and if
<code>no_lambda &gt; 1</code>, a sequence of <code>no_lambda</code> regularization
parameters is computed with <code>glmnet::glmnet</code>. If <code>no_lambda = 1</code>
then the cross-validated optimum for the lasso is computed using
<code>glmnet::cv.glmnet</code>.</p>
</td></tr>
<tr><td><code id="cv_gds_+3A_n_folds">n_folds</code></td>
<td>
<p>Number of cross-validation folds to use.</p>
</td></tr>
<tr><td><code id="cv_gds_+3A_weights">weights</code></td>
<td>
<p>A vector of weights for each row of <code>X</code>. Defaults to 1
per observation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cross-validation loss is calculated as the deviance of the model divided
by the number of observations.
For the Gaussian case, this is the mean squared error. Weights supplied
through the <code>weights</code> argument are used both in fitting the models
and when evaluating the test set deviance.
</p>


<h3>Value</h3>

<p>An object of class <code>cv_gds</code>.
</p>


<h3>References</h3>

<p>Candes E, Tao T (2007).
&ldquo;The Dantzig selector: Statistical estimation when p is much larger than n.&rdquo;
<em>Ann. Statist.</em>, <b>35</b>(6), 2313&ndash;2351.
</p>
<p>James GM, Radchenko P (2009).
&ldquo;A generalized Dantzig selector with shrinkage tuning.&rdquo;
<em>Biometrika</em>, <b>96</b>(2), 323-337.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Example with logistic regression
n &lt;- 1000  # Number of samples
p &lt;- 10 # Number of covariates
X &lt;- matrix(rnorm(n * p), nrow = n) # True (latent) variables # Design matrix
beta &lt;- c(seq(from = 0.1, to = 1, length.out = 5), rep(0, p-5)) # True regression coefficients
y &lt;- rbinom(n, 1, (1 + exp(-X %*% beta))^(-1)) # Binomially distributed response
cv_fit &lt;- cv_gds(X, y, family = "binomial", no_lambda = 50, n_folds = 10)
print(cv_fit)
plot(cv_fit)

# Now fit a single GDS at the optimum lambda value determined by cross-validation
fit &lt;- gds(X, y, lambda = cv_fit$lambda_min, family = "binomial")
plot(fit)

# Compare this to the fit for which lambda is selected by GDS
# This automatic selection is performed by glmnet::cv.glmnet, for
# the sake of speed
fit2 &lt;- gds(X, y, family = "binomial")

The following plot compares the two fits.
library(ggplot2)
library(tidyr)
df &lt;- data.frame(fit = fit$beta, fit2 = fit2$beta, index = seq(1, p, by = 1))
ggplot(gather(df, key = "Model", value = "Coefficient", -index),
       aes(x = index, y = Coefficient, color = Model)) +
       geom_point() +
       theme(legend.title = element_blank())


## End(Not run)

</code></pre>

<hr>
<h2 id='gds'>Generalized Dantzig Selector</h2><span id='topic+gds'></span>

<h3>Description</h3>

<p>Generalized Dantzig Selector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gds(X, y, lambda = NULL, family = "gaussian", weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gds_+3A_x">X</code></td>
<td>
<p>Design matrix.</p>
</td></tr>
<tr><td><code id="gds_+3A_y">y</code></td>
<td>
<p>Vector of the continuous response value.</p>
</td></tr>
<tr><td><code id="gds_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter. Only a single value is supported.</p>
</td></tr>
<tr><td><code id="gds_+3A_family">family</code></td>
<td>
<p>Use &quot;gaussian&quot; for linear regression, &quot;binomial&quot; for logistic regression and &quot;poisson&quot; for Poisson regression.</p>
</td></tr>
<tr><td><code id="gds_+3A_weights">weights</code></td>
<td>
<p>A vector of weights for each row of <code>X</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Intercept and coefficients at the values of lambda specified.
</p>


<h3>References</h3>

<p>Candes E, Tao T (2007).
&ldquo;The Dantzig selector: Statistical estimation when p is much larger than n.&rdquo;
<em>Ann. Statist.</em>, <b>35</b>(6), 2313&ndash;2351.
</p>
<p>James GM, Radchenko P (2009).
&ldquo;A generalized Dantzig selector with shrinkage tuning.&rdquo;
<em>Biometrika</em>, <b>96</b>(2), 323-337.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with logistic regression
n &lt;- 1000  # Number of samples
p &lt;- 10 # Number of covariates
X &lt;- matrix(rnorm(n * p), nrow = n) # True (latent) variables # Design matrix
beta &lt;- c(seq(from = 0.1, to = 1, length.out = 5), rep(0, p-5)) # True regression coefficients
y &lt;- rbinom(n, 1, (1 + exp(-X %*% beta))^(-1)) # Binomially distributed response
fit &lt;- gds(X, y, family = "binomial")
print(fit)
plot(fit)
coef(fit)

# Try with more penalization
fit &lt;- gds(X, y, family = "binomial", lambda = 0.1)
coef(fit)
coef(fit, all = TRUE)


# Case weighting
# Assume we wish to put more emphasis on predicting the positive cases correctly
# In this case we give the 1s three times the weight of the zeros.
weights &lt;- (y == 0) * 1 + (y == 1) * 3
fit_w &lt;- gds(X, y, family = "binomial", weights = weights, lambda = 0.1)

# Next we test this on a new dataset, generated with the same parameters
X_new &lt;- matrix(rnorm(n * p), nrow = n)
y_new &lt;- rbinom(n, 1, (1 + exp(-X_new %*% beta))^(-1))
# We use a 50 % threshold as classification rule
# Unweighted classifcation
classification &lt;- ((1 + exp(- fit$intercept - X_new %*% fit$beta))^(-1) &gt; 0.5) * 1
# Weighted classification
classification_w &lt;- ((1 + exp(- fit_w$intercept - X_new %*% fit_w$beta))^(-1) &gt; 0.5) * 1

# As expected, the weighted classification predicts many more 1s than 0s, since
# these are heavily up-weighted
table(classification, classification_w)

# Here we compare the performance of the weighted and unweighted models.
# The weighted model gets most of the 1s right, while the unweighted model
# gets the highest overall performance.
table(classification, y_new)
table(classification_w, y_new)

</code></pre>

<hr>
<h2 id='gmu_lasso'>Generalized Matrix Uncertainty Lasso</h2><span id='topic+gmu_lasso'></span>

<h3>Description</h3>

<p>Generalized Matrix Uncertainty Lasso
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmu_lasso(
  W,
  y,
  lambda = NULL,
  delta = NULL,
  family = "binomial",
  active_set = TRUE,
  maxit = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gmu_lasso_+3A_w">W</code></td>
<td>
<p>Design matrix, measured with error. Must be a numeric matrix.</p>
</td></tr>
<tr><td><code id="gmu_lasso_+3A_y">y</code></td>
<td>
<p>Vector of responses.</p>
</td></tr>
<tr><td><code id="gmu_lasso_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter. If not set, lambda.min from
glmnet::cv.glmnet is used.</p>
</td></tr>
<tr><td><code id="gmu_lasso_+3A_delta">delta</code></td>
<td>
<p>Additional regularization parameter, bounding the measurement
error.</p>
</td></tr>
<tr><td><code id="gmu_lasso_+3A_family">family</code></td>
<td>
<p>Character string. Currently &quot;binomial&quot; and &quot;poisson&quot; are
supported.</p>
</td></tr>
<tr><td><code id="gmu_lasso_+3A_active_set">active_set</code></td>
<td>
<p>Logical. Whether or not to use an active set strategy to
speed up coordinate descent algorithm.</p>
</td></tr>
<tr><td><code id="gmu_lasso_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations of iterative reweighing algorithm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;gmu_lasso&quot;.
</p>


<h3>References</h3>

<p>Rosenbaum M, Tsybakov AB (2010).
&ldquo;Sparse recovery under matrix uncertainty.&rdquo;
<em>Ann. Statist.</em>, <b>38</b>(5), 2620&ndash;2651.
</p>
<p>Sorensen O, Hellton KH, Frigessi A, Thoresen M (2018).
&ldquo;Covariate Selection in High-Dimensional Generalized Linear Models With Measurement Error.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>27</b>(4), 739-749.
<a href="https://doi.org/10.1080/10618600.2018.1425626">doi:10.1080/10618600.2018.1425626</a>, https://doi.org/10.1080/10618600.2018.1425626.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
# Number of samples
n &lt;- 200
# Number of covariates
p &lt;- 100
# Number of nonzero features
s &lt;- 10
# True coefficient vector
beta &lt;- c(rep(1,s),rep(0,p-s))
# Standard deviation of measurement error
sdU &lt;- 0.2
# True data, not observed
X &lt;- matrix(rnorm(n*p),nrow = n,ncol = p)
# Measured data, with error
W &lt;- X + sdU * matrix(rnorm(n * p), nrow = n, ncol = p)
# Binomial response
y &lt;- rbinom(n, 1, (1 + exp(-X%*%beta))**(-1))
# Run the GMU Lasso
fit &lt;- gmu_lasso(W, y, delta = NULL)
print(fit)
plot(fit)
coef(fit)
# Get an elbow plot, in order to choose delta.
plot(fit)


</code></pre>

<hr>
<h2 id='gmus'>Generalized Matrix Uncertainty Selector</h2><span id='topic+gmus'></span>

<h3>Description</h3>

<p>Generalized Matrix Uncertainty Selector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmus(W, y, lambda = NULL, delta = NULL, family = "gaussian", weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gmus_+3A_w">W</code></td>
<td>
<p>Design matrix, measured with error. Must be a numeric matrix.</p>
</td></tr>
<tr><td><code id="gmus_+3A_y">y</code></td>
<td>
<p>Vector of responses.</p>
</td></tr>
<tr><td><code id="gmus_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter.</p>
</td></tr>
<tr><td><code id="gmus_+3A_delta">delta</code></td>
<td>
<p>Additional regularization parameter, bounding the measurement
error.</p>
</td></tr>
<tr><td><code id="gmus_+3A_family">family</code></td>
<td>
<p>&quot;gaussian&quot; for linear regression, &quot;binomial&quot; for logistic
regression or &quot;poisson&quot; for Poisson regression. Defaults go &quot;gaussian&quot;.</p>
</td></tr>
<tr><td><code id="gmus_+3A_weights">weights</code></td>
<td>
<p>A vector of weights for each row of <code>X</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;gmus&quot;.
</p>


<h3>References</h3>

<p>Rosenbaum M, Tsybakov AB (2010).
&ldquo;Sparse recovery under matrix uncertainty.&rdquo;
<em>Ann. Statist.</em>, <b>38</b>(5), 2620&ndash;2651.
</p>
<p>Sorensen O, Hellton KH, Frigessi A, Thoresen M (2018).
&ldquo;Covariate Selection in High-Dimensional Generalized Linear Models With Measurement Error.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>27</b>(4), 739-749.
<a href="https://doi.org/10.1080/10618600.2018.1425626">doi:10.1080/10618600.2018.1425626</a>, https://doi.org/10.1080/10618600.2018.1425626.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with linear regression
set.seed(1)
n &lt;- 100 # Number of samples
p &lt;- 50 # Number of covariates
# True (latent) variables
X &lt;- matrix(rnorm(n * p), nrow = n)
# Measurement matrix (this is the one we observe)
W &lt;- X + matrix(rnorm(n*p, sd = 1), nrow = n, ncol = p)
# Coefficient vector
beta &lt;- c(seq(from = 0.1, to = 1, length.out = 5), rep(0, p-5))
# Response
y &lt;- X %*% beta + rnorm(n, sd = 1)
# Run the MU Selector
fit1 &lt;- gmus(W, y)
# Draw an elbow plot to select delta
plot(fit1)
coef(fit1)

# Now, according to the "elbow rule", choose
# the final delta where the curve has an "elbow".
# In this case, the elbow is at about delta = 0.08,
# so we use this to compute the final estimate:
fit2 &lt;- gmus(W, y, delta = 0.08)
# Plot the coefficients
plot(fit2)
coef(fit2)
coef(fit2, all = TRUE)

</code></pre>

<hr>
<h2 id='mus'>Matrix Uncertainty Selector</h2><span id='topic+mus'></span>

<h3>Description</h3>

<p>Matrix Uncertainty Selector for linear regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mus(W, y, lambda = NULL, delta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mus_+3A_w">W</code></td>
<td>
<p>Design matrix, measured with error. Must be a numeric matrix.</p>
</td></tr>
<tr><td><code id="mus_+3A_y">y</code></td>
<td>
<p>Vector of responses.</p>
</td></tr>
<tr><td><code id="mus_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter.</p>
</td></tr>
<tr><td><code id="mus_+3A_delta">delta</code></td>
<td>
<p>Additional regularization parameter, bounding the measurement error.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is just a
wrapper for <code>gmus(W, y, lambda, delta, family = "gaussian")</code>.
</p>


<h3>Value</h3>

<p>An object of class &quot;gmus&quot;.
</p>


<h3>References</h3>

<p>Rosenbaum M, Tsybakov AB (2010).
&ldquo;Sparse recovery under matrix uncertainty.&rdquo;
<em>Ann. Statist.</em>, <b>38</b>(5), 2620&ndash;2651.
</p>
<p>Sorensen O, Hellton KH, Frigessi A, Thoresen M (2018).
&ldquo;Covariate Selection in High-Dimensional Generalized Linear Models With Measurement Error.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>27</b>(4), 739-749.
<a href="https://doi.org/10.1080/10618600.2018.1425626">doi:10.1080/10618600.2018.1425626</a>, https://doi.org/10.1080/10618600.2018.1425626.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with Gaussian response
set.seed(1)
# Number of samples
n &lt;- 100
# Number of covariates
p &lt;- 50
# True (latent) variables
X &lt;- matrix(rnorm(n * p), nrow = n)
# Measurement matrix (this is the one we observe)
W &lt;- X + matrix(rnorm(n*p, sd = 1), nrow = n, ncol = p)
# Coefficient vector
beta &lt;- c(seq(from = 0.1, to = 1, length.out = 5), rep(0, p-5))
# Response
y &lt;- X %*% beta + rnorm(n, sd = 1)
# Run the MU Selector
fit1 &lt;- mus(W, y)
# Draw an elbow plot to select delta
plot(fit1)
coef(fit1)

# Now, according to the "elbow rule", choose the final delta where the curve has an "elbow".
# In this case, the elbow is at about delta = 0.08, so we use this to compute the final estimate:
fit2 &lt;- mus(W, y, delta = 0.08)
plot(fit2) # Plot the coefficients
coef(fit2)
coef(fit2, all = TRUE)

</code></pre>

<hr>
<h2 id='mus_glm'>Generalized Matrix Uncertainty Selector for logistic regression</h2><span id='topic+mus_glm'></span>

<h3>Description</h3>

<p>Internal function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mus_glm(W, y, lambda, delta, family = c("binomial", "poisson"), weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mus_glm_+3A_w">W</code></td>
<td>
<p>Design matrix, measured with error.</p>
</td></tr>
<tr><td><code id="mus_glm_+3A_y">y</code></td>
<td>
<p>Vector of the binomial response value.</p>
</td></tr>
<tr><td><code id="mus_glm_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter due to model error.</p>
</td></tr>
<tr><td><code id="mus_glm_+3A_delta">delta</code></td>
<td>
<p>Regularization parameter due to measurement error.</p>
</td></tr>
<tr><td><code id="mus_glm_+3A_family">family</code></td>
<td>
<p>&quot;binomial&quot; or &quot;poisson&quot;</p>
</td></tr>
<tr><td><code id="mus_glm_+3A_weights">weights</code></td>
<td>
<p>Case weights.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Intercept and coefficients at the values of lambda and delta specified.
</p>

<hr>
<h2 id='musalgorithm'>Algorithm for mus</h2><span id='topic+musalgorithm'></span>

<h3>Description</h3>

<p>Algorithm for mus
</p>


<h3>Usage</h3>

<pre><code class='language-R'>musalgorithm(W, y, lambda, delta, weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="musalgorithm_+3A_w">W</code></td>
<td>
<p>Matrix of measurements.</p>
</td></tr>
<tr><td><code id="musalgorithm_+3A_y">y</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code id="musalgorithm_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter due to residual.</p>
</td></tr>
<tr><td><code id="musalgorithm_+3A_delta">delta</code></td>
<td>
<p>Regularization parameter due to measurement error.</p>
</td></tr>
</table>

<hr>
<h2 id='plot.corrected_lasso'>plot.corrected_lasso</h2><span id='topic+plot.corrected_lasso'></span>

<h3>Description</h3>

<p>Plot the output of corrected_lasso
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'corrected_lasso'
plot(x, type = "nonzero", label = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.corrected_lasso_+3A_x">x</code></td>
<td>
<p>Object of class corrected_lasso, returned from calling
corrected_lasso()</p>
</td></tr>
<tr><td><code id="plot.corrected_lasso_+3A_type">type</code></td>
<td>
<p>Type of plot. Either &quot;nonzero&quot; or &quot;path&quot;. Ignored if
<code>length(x$radii) == 1</code>, in case of which all coefficient estimates are
plotted at the given regularization parameter.</p>
</td></tr>
<tr><td><code id="plot.corrected_lasso_+3A_label">label</code></td>
<td>
<p>Logical specifying whether to add labels to coefficient paths.
Only used when <code>type = "path"</code>.</p>
</td></tr>
<tr><td><code id="plot.corrected_lasso_+3A_...">...</code></td>
<td>
<p>Other arguments to plot (not used)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Example with linear regression
n &lt;- 100 # Number of samples
p &lt;- 50 # Number of covariates
# True (latent) variables
X &lt;- matrix(rnorm(n * p), nrow = n)
# Measurement error covariance matrix
# (typically estimated by replicate measurements)
sigmaUU &lt;- diag(x = 0.2, nrow = p, ncol = p)
# Measurement matrix (this is the one we observe)
W &lt;- X + rnorm(n, sd = sqrt(diag(sigmaUU)))
# Coefficient
beta &lt;- c(seq(from = 0.1, to = 1, length.out = 5), rep(0, p-5))
# Response
y &lt;- X %*% beta + rnorm(n, sd = 1)
# Run the corrected lasso
fit &lt;- corrected_lasso(W, y, sigmaUU, family = "gaussian")
plot(fit)

</code></pre>

<hr>
<h2 id='plot.cv_corrected_lasso'>plot.cv_corrected_lasso</h2><span id='topic+plot.cv_corrected_lasso'></span>

<h3>Description</h3>

<p>Plot the output of <code><a href="#topic+cv_corrected_lasso">cv_corrected_lasso</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv_corrected_lasso'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cv_corrected_lasso_+3A_x">x</code></td>
<td>
<p>The object to be plotted, returned from <code><a href="#topic+cv_corrected_lasso">cv_corrected_lasso</a></code>.</p>
</td></tr>
<tr><td><code id="plot.cv_corrected_lasso_+3A_...">...</code></td>
<td>
<p>Other arguments to plot (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='plot.cv_gds'>plot.cv_gds</h2><span id='topic+plot.cv_gds'></span>

<h3>Description</h3>

<p>Plot the output of <code><a href="#topic+cv_gds">cv_gds</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv_gds'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cv_gds_+3A_x">x</code></td>
<td>
<p>The object to be plotted, returned from <code><a href="#topic+cv_gds">cv_gds</a></code>.</p>
</td></tr>
<tr><td><code id="plot.cv_gds_+3A_...">...</code></td>
<td>
<p>Other arguments to plot (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='plot.gds'>Plot the estimates returned by gds</h2><span id='topic+plot.gds'></span>

<h3>Description</h3>

<p>Plot the number of nonzero coefficients at the given lambda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gds'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.gds_+3A_x">x</code></td>
<td>
<p>An object of class gds</p>
</td></tr>
<tr><td><code id="plot.gds_+3A_...">...</code></td>
<td>
<p>Other arguments to plot (not used).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
# Example with logistic regression
# Number of samples
n &lt;- 1000
# Number of covariates
p &lt;- 10
# True (latent) variables (Design matrix)
X &lt;- matrix(rnorm(n * p), nrow = n)
# True regression coefficients
beta &lt;- c(seq(from = 0.1, to = 1, length.out = 5), rep(0, p-5))
# Binomially distributed response
y &lt;- rbinom(n, 1, (1 + exp(-X %*% beta))^(-1))
# Fit the generalized Dantzig Selector
gds &lt;- gds(X, y, family = "binomial")
# Plot the estimated coefficients at the chosen lambda
plot(gds)

</code></pre>

<hr>
<h2 id='plot.gmu_lasso'>Plot the estimates returned by gmu_lasso</h2><span id='topic+plot.gmu_lasso'></span>

<h3>Description</h3>

<p>Plot the number of nonzero coefficients along a range of delta
values if delta has length larger than 1, or the estimated coefficients of
delta has length 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gmu_lasso'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.gmu_lasso_+3A_x">x</code></td>
<td>
<p>An object of class gmu_lasso</p>
</td></tr>
<tr><td><code id="plot.gmu_lasso_+3A_...">...</code></td>
<td>
<p>Other arguments to plot (not used).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 200
p &lt;- 50
s &lt;- 10
beta &lt;- c(rep(1,s),rep(0,p-s))
sdU &lt;- 0.2

X &lt;- matrix(rnorm(n*p),nrow = n,ncol = p)
W &lt;- X + sdU * matrix(rnorm(n * p), nrow = n, ncol = p)

y &lt;- rbinom(n, 1, (1 + exp(-X%*%beta))**(-1))
gmu_lasso &lt;- gmu_lasso(W, y)

plot(gmu_lasso)

</code></pre>

<hr>
<h2 id='plot.gmus'>Plot the estimates returned by gmus and mus</h2><span id='topic+plot.gmus'></span>

<h3>Description</h3>

<p>Plot the number of nonzero coefficients along a range of delta
values if delta has length larger than 1, or the estimated coefficients if
delta has length 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gmus'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.gmus_+3A_x">x</code></td>
<td>
<p>An object of class gmus</p>
</td></tr>
<tr><td><code id="plot.gmus_+3A_...">...</code></td>
<td>
<p>Other arguments to plot (not used).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Example with linear regression
set.seed(1)
# Number of samples
n &lt;- 100
# Number of covariates
p &lt;- 50
# True (latent) variables
X &lt;- matrix(rnorm(n * p), nrow = n)
# Measurement matrix (this is the one we observe)
W &lt;- X + matrix(rnorm(n*p, sd = 0.4), nrow = n, ncol = p)
# Coefficient vector
beta &lt;- c(seq(from = 0.1, to = 1, length.out = 5), rep(0, p-5))
# Response
y &lt;- X %*% beta + rnorm(n, sd = 1)
# Run the MU Selector
mus1 &lt;- mus(W, y)
# Draw an elbow plot to select delta
plot(mus1)

# Now, according to the "elbow rule", choose the final
# delta where the curve has an "elbow".
# In this case, the elbow is at about delta = 0.08, so
# we use this to compute the final estimate:
mus2 &lt;- mus(W, y, delta = 0.08)
# Plot the coefficients
plot(mus2)

</code></pre>

<hr>
<h2 id='print.corrected_lasso'>Print a Corrected Lasso object</h2><span id='topic+print.corrected_lasso'></span>

<h3>Description</h3>

<p>Default print method for a <code>corrected_lasso</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'corrected_lasso'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.corrected_lasso_+3A_x">x</code></td>
<td>
<p>Fitted model object returned by <code><a href="#topic+corrected_lasso">corrected_lasso</a></code>.</p>
</td></tr>
<tr><td><code id="print.corrected_lasso_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='print.cv_corrected_lasso'>Print a Cross-Validated Corrected Lasso object</h2><span id='topic+print.cv_corrected_lasso'></span>

<h3>Description</h3>

<p>Default print method for a <code>cv_corrected_lasso</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv_corrected_lasso'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.cv_corrected_lasso_+3A_x">x</code></td>
<td>
<p>Fitted model object returned by <code><a href="#topic+cv_corrected_lasso">cv_corrected_lasso</a></code>.</p>
</td></tr>
<tr><td><code id="print.cv_corrected_lasso_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='print.cv_gds'>Print a Cross-Validated GDS Object</h2><span id='topic+print.cv_gds'></span>

<h3>Description</h3>

<p>Default print method for a <code>cv_gds</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv_gds'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.cv_gds_+3A_x">x</code></td>
<td>
<p>Fitted model object returned by <code><a href="#topic+cv_gds">cv_gds</a></code>.</p>
</td></tr>
<tr><td><code id="print.cv_gds_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='print.gds'>Print a Generalized Dantzig Selector Object</h2><span id='topic+print.gds'></span>

<h3>Description</h3>

<p>Default print method for a <code>gds</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gds'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.gds_+3A_x">x</code></td>
<td>
<p>Fitted model object returned by <code><a href="#topic+gds">gds</a></code>.</p>
</td></tr>
<tr><td><code id="print.gds_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='print.gmu_lasso'>Print a GMU Lasso object</h2><span id='topic+print.gmu_lasso'></span>

<h3>Description</h3>

<p>Default print method for a <code>gmu_lasso</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gmu_lasso'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.gmu_lasso_+3A_x">x</code></td>
<td>
<p>Fitted model object returned by <code><a href="#topic+gmu_lasso">gmu_lasso</a></code>.</p>
</td></tr>
<tr><td><code id="print.gmu_lasso_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>

<hr>
<h2 id='print.gmus'>Print a GMUS object</h2><span id='topic+print.gmus'></span>

<h3>Description</h3>

<p>Default print method for a <code>gmus</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gmus'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.gmus_+3A_x">x</code></td>
<td>
<p>Fitted model object returned by <code><a href="#topic+gmus">gmus</a></code>.</p>
</td></tr>
<tr><td><code id="print.gmus_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
