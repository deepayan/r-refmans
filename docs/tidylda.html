<!DOCTYPE html><html lang="en-US"><head><title>Help for package tidylda</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tidylda}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#tidylda-package'><p>Latent Dirichlet Allocation Using 'tidyverse' Conventions</p></a></li>
<li><a href='#augment.tidylda'><p>Augment method for <code>tidylda</code> objects</p></a></li>
<li><a href='#calc_lambda'><p>Calculate a matrix whose rows represent P(topic_i|tokens)</p></a></li>
<li><a href='#calc_lda_r2'><p>Calculate R-squared for a tidylda Model</p></a></li>
<li><a href='#calc_prob_coherence'><p>Probabilistic coherence of topics</p></a></li>
<li><a href='#convert_dtm'><p>Convert various things to a <code>dgCMatrix</code> to work with various functions</p>
and methods</a></li>
<li><a href='#create_lexicon'><p>Make a lexicon for looping over in the gibbs sampler</p></a></li>
<li><a href='#fit_lda_c'><p>Main C++ Gibbs sampler for Latent Dirichlet Allocation</p></a></li>
<li><a href='#format_alpha'><p>Format <code>alpha</code> For Input into <code>fit_lda_c</code></p></a></li>
<li><a href='#format_eta'><p>Format <code>eta</code> For Input into <code>fit_lda_c</code></p></a></li>
<li><a href='#generate_sample'><p>Generate a sample of LDA posteriors</p></a></li>
<li><a href='#glance.tidylda'><p>Glance method for <code>tidylda</code> objects</p></a></li>
<li><a href='#initialize_topic_counts'><p>Initialize topic counts for gibbs sampling</p></a></li>
<li><a href='#new_tidylda'><p>Construct a new object of class <code>tidylda</code></p></a></li>
<li><a href='#nih'><p>Abstracts and metadata from NIH research grants awarded in 2014</p></a></li>
<li><a href='#posterior'><p>Draw from the marginal posteriors of a tidylda topic model</p></a></li>
<li><a href='#predict.tidylda'><p>Get predictions from a Latent Dirichlet Allocation model</p></a></li>
<li><a href='#print.tidylda'><p>Print Method for tidylda</p></a></li>
<li><a href='#recover_counts_from_probs'><p>Get Count Matrices from Beta or Theta (and Priors)</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#refit.tidylda'><p>Update a Latent Dirichlet Allocation topic model</p></a></li>
<li><a href='#summarize_topics'><p>Summarize a topic model consistently across methods/functions</p></a></li>
<li><a href='#tidy_dgcmatrix'><p>Create a tidy tibble for a dgCMatrix</p></a></li>
<li><a href='#tidy_triplet'><p>Utility function to tidy a simple triplet matrix</p></a></li>
<li><a href='#tidy.tidylda'><p>Tidy a matrix from a <code>tidylda</code> topic model</p></a></li>
<li><a href='#tidylda'><p>Fit a Latent Dirichlet Allocation topic model</p></a></li>
<li><a href='#tidylda_bridge'><p>Bridge function for fitting <code>tidylda</code> topic models</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Latent Dirichlet Allocation Using 'tidyverse' Conventions</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.5</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements an algorithm for Latent Dirichlet
    Allocation (LDA), Blei et at. (2003) <a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf</a>,
    using style conventions from the 'tidyverse',
    Wickham et al. (2019)&lt;<a href="https://doi.org/10.21105%2Fjoss.01686">doi:10.21105/joss.01686</a>&gt;,
    and 'tidymodels', Kuhn et al.<a href="https://tidymodels.github.io/model-implementation-principles/">https://tidymodels.github.io/model-implementation-principles/</a>.
    Fitting is done via collapsed Gibbs sampling.
    Also implements several novel features for LDA such as guided models and
    transfer learning based on ongoing and, as yet, unpublished research.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/TommyJones/tidylda/">https://github.com/TommyJones/tidylda/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/TommyJones/tidylda/issues">https://github.com/TommyJones/tidylda/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, generics, gtools, Matrix, methods, mvrsquared (&ge;
0.1.0), Rcpp (&ge; 1.0.2), rlang, stats, stringr, tibble, tidyr,
tidytext</td>
</tr>
<tr>
<td>Suggests:</td>
<td>ggplot2, knitr, parallel, quanteda, testthat, tm, slam,
spelling, covr, rmarkdown</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppProgress, RcppThread</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-20 21:24:04 UTC; tommy</td>
</tr>
<tr>
<td>Author:</td>
<td>Tommy Jones <a href="https://orcid.org/0000-0001-6457-2452"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Brendan Knapp <a href="https://orcid.org/0000-0003-3284-4972"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Barum Park [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tommy Jones &lt;jones.thos.w@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-22 18:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='tidylda-package'>Latent Dirichlet Allocation Using 'tidyverse' Conventions</h2><span id='topic+tidylda-package'></span>

<h3>Description</h3>

<p>Implements an algorithm for Latent Dirichlet Allocation (LDA)
using style conventions from the 'tidyverse' and specifically 'tidymodels'.
Also implements several novel features for LDA such as guided models and
transfer learning.
</p>

<hr>
<h2 id='augment.tidylda'>Augment method for <code>tidylda</code> objects</h2><span id='topic+augment.tidylda'></span>

<h3>Description</h3>

<p><code>augment</code> appends observation level model outputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tidylda'
augment(
  x,
  data,
  type = c("class", "prob"),
  document_col = "document",
  term_col = "term",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="augment.tidylda_+3A_x">x</code></td>
<td>
<p>an object of class <code>tidylda</code></p>
</td></tr>
<tr><td><code id="augment.tidylda_+3A_data">data</code></td>
<td>
<p>a tidy tibble containing one row per original document-token pair, 
such as is returned by <a href="tidytext.html#topic+tdm_tidiers">tdm_tidiers</a> with column names
c(&quot;document&quot;, &quot;term&quot;) at a minimum.</p>
</td></tr>
<tr><td><code id="augment.tidylda_+3A_type">type</code></td>
<td>
<p>one of either &quot;class&quot; or &quot;prob&quot;</p>
</td></tr>
<tr><td><code id="augment.tidylda_+3A_document_col">document_col</code></td>
<td>
<p>character specifying the name of the column that
corresponds to document IDs. Defaults to <code>"document"</code>.</p>
</td></tr>
<tr><td><code id="augment.tidylda_+3A_term_col">term_col</code></td>
<td>
<p>character specifying the name of the column that
corresponds to term/token IDs. Defaults to <code>"term"</code>.</p>
</td></tr>
<tr><td><code id="augment.tidylda_+3A_...">...</code></td>
<td>
<p>other arguments passed to methods,currently not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The key statistic for <code>augment</code> is P(topic | document, token) =
P(topic | token) * P(token | document). P(topic | token) are the entries
of the 'lambda' matrix in the <code><a href="#topic+tidylda">tidylda</a></code> object passed
with <code>x</code>. P(token | document) is taken to be the frequency of each
token normalized within each document.
</p>


<h3>Value</h3>

<p><code>augment</code> returns a tidy tibble containing one row per document-token
pair, with one or more columns appended, depending on the value of <code>type</code>.
</p>
<p>If <code>type = 'prob'</code>, then one column per topic is appended. Its value
is P(topic | document, token).
</p>
<p>If <code>type = 'class'</code>, then the most-probable topic for each document-token
pair is returned. If multiple topics are equally probable, then the topic
with the smallest index is returned by default.
</p>

<hr>
<h2 id='calc_lambda'>Calculate a matrix whose rows represent P(topic_i|tokens)</h2><span id='topic+calc_lambda'></span>

<h3>Description</h3>

<p>Use Bayes' rule to get P(topic|token) from the estimated parameters of a
probabilistic topic model.This resulting &quot;lambda&quot; matrix can be used for
classifying new documents in a frequentist context and supports
<code><a href="#topic+augment">augment</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_lambda(beta, theta, p_docs = NULL, correct = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_lambda_+3A_beta">beta</code></td>
<td>
<p>a beta matrix</p>
</td></tr>
<tr><td><code id="calc_lambda_+3A_theta">theta</code></td>
<td>
<p>a theta matrix</p>
</td></tr>
<tr><td><code id="calc_lambda_+3A_p_docs">p_docs</code></td>
<td>
<p>A numeric vector of length <code>nrow(theta)</code> that is
proportional to the number of terms in each document,  defaults to NULL.</p>
</td></tr>
<tr><td><code id="calc_lambda_+3A_correct">correct</code></td>
<td>
<p>Logical. Do you want to set NAs or NaNs in the final result to
zero? Useful when hitting computational underflow. Defaults to <code>TRUE</code>.
Set to <code>FALSE</code> for troubleshooting or diagnostics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>matrix</code> whose rows correspond to topics and whose columns
correspond to tokens. The i,j entry corresponds to P(topic_i|token_j)
</p>

<hr>
<h2 id='calc_lda_r2'>Calculate R-squared for a tidylda Model</h2><span id='topic+calc_lda_r2'></span>

<h3>Description</h3>

<p>Formats inputs and hands off to <a href="mvrsquared.html#topic+calc_rsquared">calc_rsquared</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_lda_r2(dtm, theta, beta, threads)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_lda_r2_+3A_dtm">dtm</code></td>
<td>
<p>must be of class dgCMatrix</p>
</td></tr>
<tr><td><code id="calc_lda_r2_+3A_theta">theta</code></td>
<td>
<p>a theta matrix</p>
</td></tr>
<tr><td><code id="calc_lda_r2_+3A_beta">beta</code></td>
<td>
<p>a beta matrix</p>
</td></tr>
<tr><td><code id="calc_lda_r2_+3A_threads">threads</code></td>
<td>
<p>number of parallel threads</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric scalar between negative infinity and 1
</p>

<hr>
<h2 id='calc_prob_coherence'>Probabilistic coherence of topics</h2><span id='topic+calc_prob_coherence'></span>

<h3>Description</h3>

<p>Calculates the probabilistic coherence of a topic or topics. 
This approximates semantic coherence or human understandability of a topic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_prob_coherence(beta, data, m = 5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_prob_coherence_+3A_beta">beta</code></td>
<td>
<p>A numeric matrix or a numeric vector. The vector, or rows of the 
matrix represent the numeric relationship between topic(s) and terms. For
example, this relationship may be p(word|topic) or p(topic|word).</p>
</td></tr>
<tr><td><code id="calc_prob_coherence_+3A_data">data</code></td>
<td>
<p>A document term matrix or term co-occurrence matrix. The preferred
class is a <code><a href="Matrix.html#topic+dgCMatrix-class">dgCMatrix-class</a></code>. However there is support
for any <code><a href="Matrix.html#topic+Matrix-class">Matrix-class</a></code> object as well as several other
commonly-used classes such as <code><a href="base.html#topic+matrix">matrix</a></code>,
<code><a href="quanteda.html#topic+dfm">dfm</a></code>, <code><a href="tm.html#topic+DocumentTermMatrix">DocumentTermMatrix</a></code>, and
<code><a href="slam.html#topic+simple_triplet_matrix">simple_triplet_matrix</a></code></p>
</td></tr>
<tr><td><code id="calc_prob_coherence_+3A_m">m</code></td>
<td>
<p>An integer for the number of words to be used in the calculation. 
Defaults to 5</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each pair of words {a, b} in the top M words in a topic, probabilistic
coherence calculates P(b|a) - P(b), where {a} is more probable than {b} in
the topic. For example, suppose the top 4 words in a topic are {a, b, c, d}.
Then, we calculate 1. P(a|b) - P(b), P(a|c) - P(c), P(a|d) - P(d)
2. P(b|c) - P(c), P(b|d) - P(d)
3. P(c|d) - P(d)
All 6 differences are averaged together.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>numeric</code> corresponding to the 
probabilistic coherence of the input topic(s).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm and topic model
data(nih_sample_dtm)

# fit a model
set.seed(12345)
model &lt;- tidylda(
  data = nih_sample_dtm[1:20, ], k = 5,
  iterations = 100, burnin = 50
)

calc_prob_coherence(beta = model$beta, data = nih_sample_dtm, m = 5)
</code></pre>

<hr>
<h2 id='convert_dtm'>Convert various things to a <code>dgCMatrix</code> to work with various functions
and methods</h2><span id='topic+convert_dtm'></span>

<h3>Description</h3>

<p>Presently, <code>tidylda</code> makes heavy usage of the <code>dgCMatrix</code> class.
However, a user may have created a DTM (or TCM) in one of several classes.
Since data could be in several formats, this function converts them to a
<code>dgCMatrix</code> before passing them along.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_dtm(dtm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="convert_dtm_+3A_dtm">dtm</code></td>
<td>
<p>the data you want to convert</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>dgCMatrix</code>
</p>

<hr>
<h2 id='create_lexicon'>Make a lexicon for looping over in the gibbs sampler</h2><span id='topic+create_lexicon'></span>

<h3>Description</h3>

<p>One run of the Gibbs sampler and other magic to initialize some objects.
Works in concert with <code><a href="#topic+initialize_topic_counts">initialize_topic_counts</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_lexicon(Cd_in, Beta_in, dtm_in, alpha, freeze_topics)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_lexicon_+3A_cd_in">Cd_in</code></td>
<td>
<p>IntegerMatrix denoting counts of topics in documents</p>
</td></tr>
<tr><td><code id="create_lexicon_+3A_beta_in">Beta_in</code></td>
<td>
<p>NumericMatrix denoting probability of words in topics</p>
</td></tr>
<tr><td><code id="create_lexicon_+3A_dtm_in">dtm_in</code></td>
<td>
<p>arma::sp_mat document term matrix</p>
</td></tr>
<tr><td><code id="create_lexicon_+3A_alpha">alpha</code></td>
<td>
<p>NumericVector prior for topics over documents</p>
</td></tr>
<tr><td><code id="create_lexicon_+3A_freeze_topics">freeze_topics</code></td>
<td>
<p>bool if making predictions, set to <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Arguments ending in <code>_in</code> are copied and their copies modified in
some way by this function. In the case of <code>Cd_in</code> and <code>Beta_in</code>,
the only modification is that they are converted from matrices to nested
<code>std::vector</code> for speed, reliability, and thread safety. <code>dtm_in</code>
is transposed for speed when looping over columns.
</p>


<h3>Value</h3>

<p>Returns a list with five entries.
</p>
<p><code>Docs</code> is a list of vectors. Each element is a document, and the contents
are indices for tokens. Used as an iterator for the Gibbs sampler.
</p>
<p><code>Zd</code> is a list of vectors, similar to Docs. However, its contents are topic
assignments of each document/token pair. Used as an iterator for Gibbs
sampling.
</p>
<p><code>Cd</code> is a matrix counting the number of times each topic is sampled per
document.
</p>
<p><code>Cv</code> is a matrix counting the number of times each topic is sampled per token.
</p>
<p><code>Ck</code> is a vector counting the total number of times each topic is sampled overall.
</p>
<p><code>Cd</code>, <code>Cv</code>, and <code>Ck</code> are derivatives of <code>Zd</code>.
</p>

<hr>
<h2 id='fit_lda_c'>Main C++ Gibbs sampler for Latent Dirichlet Allocation</h2><span id='topic+fit_lda_c'></span>

<h3>Description</h3>

<p>This is the C++ Gibbs sampler for LDA. &quot;Abandon all hope, ye who enter here.&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_lda_c(
  Docs,
  Zd_in,
  Cd_in,
  Cv_in,
  Ck_in,
  alpha_in,
  eta_in,
  iterations,
  burnin,
  optimize_alpha,
  calc_likelihood,
  Beta_in,
  freeze_topics,
  threads = 1L,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit_lda_c_+3A_docs">Docs</code></td>
<td>
<p>List with one element for each document and one entry for each token
as formatted by <code><a href="#topic+initialize_topic_counts">initialize_topic_counts</a></code></p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_zd_in">Zd_in</code></td>
<td>
<p>List with one element for each document and one entry for each token
as formatted by <code><a href="#topic+initialize_topic_counts">initialize_topic_counts</a></code></p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_cd_in">Cd_in</code></td>
<td>
<p>IntegerMatrix denoting counts of topics in documents</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_cv_in">Cv_in</code></td>
<td>
<p>IntegerMatrix denoting counts of tokens in topics</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_ck_in">Ck_in</code></td>
<td>
<p>IntegerVector denoting counts of topics across all tokens</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_alpha_in">alpha_in</code></td>
<td>
<p>NumericVector prior for topics over documents</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_eta_in">eta_in</code></td>
<td>
<p>NumericMatrix for prior of tokens over topics</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_iterations">iterations</code></td>
<td>
<p>int number of gibbs iterations to run in total</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_burnin">burnin</code></td>
<td>
<p>int number of burn in iterations</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_optimize_alpha">optimize_alpha</code></td>
<td>
<p>bool do you want to optimize alpha each iteration?</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_calc_likelihood">calc_likelihood</code></td>
<td>
<p>bool do you want to calculate the log likelihood each
iteration?</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_beta_in">Beta_in</code></td>
<td>
<p>NumericMatrix denoting probability of tokens in topics</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_freeze_topics">freeze_topics</code></td>
<td>
<p>bool if making predictions, set to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_threads">threads</code></td>
<td>
<p>unsigned integer, how many parallel threads?
For now, nothing is actually parallel</p>
</td></tr>
<tr><td><code id="fit_lda_c_+3A_verbose">verbose</code></td>
<td>
<p>bool do you want to print out a progress bar?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Arguments ending in <code>_in</code> are copied and their copies modified in
some way by this function. In the case of <code>eta_in</code> and <code>Beta_in</code>,
the only modification is that they are converted from matrices to nested
<code>std::vector</code> for speed, reliability, and thread safety. In the case
of all others, they may be explicitly modified during training.
</p>


<h3>Value</h3>

<p>Returns a list with the following entries.
</p>
<p><code>Cd</code> is a matrix counting the number of times each topic is sampled per
document.
</p>
<p><code>Cv</code> is a matrix counting the number of times each topic is sampled per token.
</p>
<p><code>Cd_mean</code> the same as <code>Cd</code> but values averaged across iterations
greater than <code>burnin</code> iterations.
</p>
<p><code>Cv_mean</code> the same as <code>Cv</code> but values averaged across iterations
greater than <code>burnin</code> iterations.
</p>
<p><code>Cd_sum</code> the same as <code>Cd</code> but values summed across iterations
greater than <code>burnin</code> iterations.
</p>
<p><code>Cv_sum</code> the same as <code>Cv</code> but values summed across iterations
greater than <code>burnin</code> iterations.
</p>
<p><code>log_likelihood</code> a matrix with one row indexing iterations and one
row of the log likelihood for each iteration.
</p>
<p><code>alpha</code> a vector of the document-topic prior
</p>
<p><code>_eta</code> a matrix of the topic-token prior
</p>

<hr>
<h2 id='format_alpha'>Format <code>alpha</code> For Input into <code>fit_lda_c</code></h2><span id='topic+format_alpha'></span>

<h3>Description</h3>

<p>There are a bunch of ways users could format <code>alpha</code> but the C++ Gibbs
sampler in <code><a href="#topic+fit_lda_c">fit_lda_c</a></code> only takes it one way. This function does the
appropriate formatting. It also returns errors if the user input a malformatted
<code>alpha</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_alpha(alpha, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="format_alpha_+3A_alpha">alpha</code></td>
<td>
<p>the prior for topics over documents. Can be a numeric scalar or
numeric vector.</p>
</td></tr>
<tr><td><code id="format_alpha_+3A_k">k</code></td>
<td>
<p>the number of topics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with two elements: <code>alpha</code> and <code>alpha_class</code>.
<code>alpha</code> is the post-formatted version of <code>alpha</code> in the form of a
<code>k</code>-length numeric vector. <code>alpha_class</code> is a character
denoting whether or not the user-supplied <code>alpha</code> was a &quot;scalar&quot; or
&quot;vector&quot;.
</p>

<hr>
<h2 id='format_eta'>Format <code>eta</code> For Input into <code>fit_lda_c</code></h2><span id='topic+format_eta'></span>

<h3>Description</h3>

<p>There are a bunch of ways users could format <code>eta</code> but the C++ Gibbs
sampler in <code><a href="#topic+fit_lda_c">fit_lda_c</a></code> only takes it one way. This function does the
appropriate formatting. It also returns errors if the user input a malformatted
<code>eta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_eta(eta, k, Nv)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="format_eta_+3A_eta">eta</code></td>
<td>
<p>the prior for words over topics. Can be a numeric scalar, numeric
vector, or numeric matrix.</p>
</td></tr>
<tr><td><code id="format_eta_+3A_k">k</code></td>
<td>
<p>the number of topics.</p>
</td></tr>
<tr><td><code id="format_eta_+3A_nv">Nv</code></td>
<td>
<p>the total size of the vocabulary as inherited from <code>ncol(dtm)</code>
in <code><a href="#topic+tidylda">tidylda</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with two elements: <code>eta</code> and <code>eta_class</code>.
<code>eta</code> is the post-formatted version of <code>eta</code> in the form of a
<code>k</code> by <code>Nv</code> numeric matrix. <code>eta_class</code> is a character
denoting whether or not the user-supplied <code>eta</code> was a &quot;scalar&quot;,
&quot;vector&quot;, or &quot;matrix&quot;.
</p>

<hr>
<h2 id='generate_sample'>Generate a sample of LDA posteriors</h2><span id='topic+generate_sample'></span>

<h3>Description</h3>

<p>Helper function called by both posterior.tidylda and predict.tidylda to
generate samples from the posterior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_sample(dir_par, matrix, times)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_sample_+3A_dir_par">dir_par</code></td>
<td>
<p>matrix of Dirichlet hyperparameters, one column per</p>
</td></tr>
<tr><td><code id="generate_sample_+3A_matrix">matrix</code></td>
<td>
<p>character of &quot;theta&quot; or &quot;beta&quot;, indicating which posterior
matrix <code>dir_par</code>'s columns are from.</p>
</td></tr>
<tr><td><code id="generate_sample_+3A_times">times</code></td>
<td>
<p>Integer, number of samples to draw.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a tibble with one row per parameter per sample.
</p>

<hr>
<h2 id='glance.tidylda'>Glance method for <code>tidylda</code> objects</h2><span id='topic+glance.tidylda'></span>

<h3>Description</h3>

<p><code>glance</code> constructs a single-row summary &quot;glance&quot; of a <code>tidylda</code>
topic model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tidylda'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="glance.tidylda_+3A_x">x</code></td>
<td>
<p>an object of class <code>tidylda</code></p>
</td></tr>
<tr><td><code id="glance.tidylda_+3A_...">...</code></td>
<td>
<p>other arguments passed to methods,currently not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>glance</code> returns a one-row <code><a href="tibble.html#topic+tibble">tibble</a></code> with the
following columns:
</p>
<p><code>num_topics</code>: the number of topics in the model
<code>num_documents</code>: the number of documents used for fitting
<code>num_tokens</code>: the number of tokens covered by the model
<code>iterations</code>: number of total Gibbs iterations run
<code>burnin</code>: number of burn-in Gibbs iterations run
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dtm &lt;- nih_sample_dtm

lda &lt;- tidylda(data = dtm, k = 10, iterations = 100, burnin = 75)

glance(lda)

</code></pre>

<hr>
<h2 id='initialize_topic_counts'>Initialize topic counts for gibbs sampling</h2><span id='topic+initialize_topic_counts'></span>

<h3>Description</h3>

<p>Implementing seeded (or guided) LDA models and transfer learning means that
we can't initialize topics with a uniform-random start. This function prepares
data and then calls a C++ function, <code><a href="#topic+create_lexicon">create_lexicon</a></code>, that runs a single
Gibbs iteration to populate topic counts (and other objects) used during the
main Gibbs sampling run of <code><a href="#topic+fit_lda_c">fit_lda_c</a></code>. In the event that
you aren't using fancy seeding or transfer learning, this makes a random
initialization by sampling from Dirichlet distributions parameterized by
priors <code>alpha</code> and <code>eta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initialize_topic_counts(
  dtm,
  k,
  alpha,
  eta,
  beta_initial = NULL,
  theta_initial = NULL,
  freeze_topics = FALSE,
  threads = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="initialize_topic_counts_+3A_dtm">dtm</code></td>
<td>
<p>a document term matrix or term co-occurrence matrix of class <code>dgCMatrix</code>.</p>
</td></tr>
<tr><td><code id="initialize_topic_counts_+3A_k">k</code></td>
<td>
<p>the number of topics</p>
</td></tr>
<tr><td><code id="initialize_topic_counts_+3A_alpha">alpha</code></td>
<td>
<p>the numeric vector prior for topics over documents as formatted
by <code><a href="#topic+format_alpha">format_alpha</a></code></p>
</td></tr>
<tr><td><code id="initialize_topic_counts_+3A_eta">eta</code></td>
<td>
<p>the numeric matrix prior for topics over documents as formatted
by <code><a href="#topic+format_eta">format_eta</a></code></p>
</td></tr>
<tr><td><code id="initialize_topic_counts_+3A_beta_initial">beta_initial</code></td>
<td>
<p>if specified, a numeric matrix for the probability of tokens
in topics. Must be specified for predictions or updates as called by
<code><a href="#topic+predict.tidylda">predict.tidylda</a></code> or <code><a href="#topic+refit.tidylda">refit.tidylda</a></code>
respectively.</p>
</td></tr>
<tr><td><code id="initialize_topic_counts_+3A_theta_initial">theta_initial</code></td>
<td>
<p>if specified, a numeric matrix for the probability of
topics in documents. Must be specified for updates as called by
<code><a href="#topic+refit.tidylda">refit.tidylda</a></code></p>
</td></tr>
<tr><td><code id="initialize_topic_counts_+3A_freeze_topics">freeze_topics</code></td>
<td>
<p>if <code>TRUE</code> does not update counts of tokens in topics.
This is <code>TRUE</code> for predictions.</p>
</td></tr>
<tr><td><code id="initialize_topic_counts_+3A_threads">threads</code></td>
<td>
<p>number of parallel threads, currently unused</p>
</td></tr>
<tr><td><code id="initialize_topic_counts_+3A_...">...</code></td>
<td>
<p>Additional arguments, currently unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with 5 elements: <code>docs</code>, <code>Zd</code>, <code>Cd</code>, <code>Cv</code>,
and <code>Ck</code>. All of these are used by <code><a href="#topic+fit_lda_c">fit_lda_c</a></code>.
</p>
<p><code>docs</code> is a list with one element per document. Each element is a vector
of integers of length <code>sum(dtm[j,])</code> for the j-th document. The integer
entries correspond to the zero-index column of the <code>dtm</code>.
</p>
<p><code>Zd</code> is a list of similar format as <code>docs</code>. The difference is that
the integer values correspond to the zero-index for topics.
</p>
<p><code>Cd</code> is a matrix of integers denoting how many times each topic has
been sampled in each document.
</p>
<p><code>Cv</code> is similar to <code>Cd</code> but it counts how many times each topic
has been sampled for each token.
</p>
<p><code>Ck</code> is an integer vector denoting how many times each topic has been
sampled overall.
</p>


<h3>Note</h3>

<p>All of <code>Cd</code>, <code>Cv</code>, and <code>Ck</code> should be derivable by summing
over Zd in various ways.
</p>

<hr>
<h2 id='new_tidylda'>Construct a new object of class <code>tidylda</code></h2><span id='topic+new_tidylda'></span>

<h3>Description</h3>

<p>Since all three of <code><a href="#topic+tidylda">tidylda</a></code>,
<code><a href="#topic+refit.tidylda">refit.tidylda</a></code>, and
<code><a href="#topic+predict.tidylda">predict.tidylda</a></code> call <code><a href="#topic+fit_lda_c">fit_lda_c</a></code>,
we need a way to format the resulting posteriors and other user-facing
objects consistently. This function does that.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_tidylda(
  lda,
  dtm,
  burnin,
  is_prediction = FALSE,
  alpha = NULL,
  eta = NULL,
  optimize_alpha = NULL,
  calc_r2 = NULL,
  calc_likelihood = NULL,
  call = NULL,
  threads
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="new_tidylda_+3A_lda">lda</code></td>
<td>
<p>list output of <code><a href="#topic+fit_lda_c">fit_lda_c</a></code></p>
</td></tr>
<tr><td><code id="new_tidylda_+3A_dtm">dtm</code></td>
<td>
<p>a document term matrix or term co-occurrence matrix of class <code>dgCMatrix</code></p>
</td></tr>
<tr><td><code id="new_tidylda_+3A_burnin">burnin</code></td>
<td>
<p>integer number of burnin iterations.</p>
</td></tr>
<tr><td><code id="new_tidylda_+3A_is_prediction">is_prediction</code></td>
<td>
<p>is this for a prediction (as opposed to initial fitting,
or update)? Defaults to <code>FALSE</code></p>
</td></tr>
<tr><td><code id="new_tidylda_+3A_alpha">alpha</code></td>
<td>
<p>output of <code><a href="#topic+format_alpha">format_alpha</a></code></p>
</td></tr>
<tr><td><code id="new_tidylda_+3A_eta">eta</code></td>
<td>
<p>output of <code><a href="#topic+format_eta">format_eta</a></code></p>
</td></tr>
<tr><td><code id="new_tidylda_+3A_optimize_alpha">optimize_alpha</code></td>
<td>
<p>did you optimize <code>alpha</code> when making a call to
<code><a href="#topic+fit_lda_c">fit_lda_c</a></code>?  If <code>is_prediction = TRUE</code>, this
argument is ignored.</p>
</td></tr>
<tr><td><code id="new_tidylda_+3A_calc_r2">calc_r2</code></td>
<td>
<p>did the user want to calculate R-squared when calculating the
the model? If <code>is_prediction = TRUE</code>, this argument is ignored.</p>
</td></tr>
<tr><td><code id="new_tidylda_+3A_calc_likelihood">calc_likelihood</code></td>
<td>
<p>did you calculate the log likelihood when making a call
to <code><a href="#topic+fit_lda_c">fit_lda_c</a></code>?  If <code>is_prediction = TRUE</code>, this
argument is ignored.</p>
</td></tr>
<tr><td><code id="new_tidylda_+3A_call">call</code></td>
<td>
<p>the result of calling <code><a href="base.html#topic+match.call">match.call</a></code> at the top of
<code><a href="#topic+tidylda">tidylda</a></code>.</p>
</td></tr>
<tr><td><code id="new_tidylda_+3A_threads">threads</code></td>
<td>
<p>number of parallel threads</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an S3 object of class <code>tidylda</code> with the following slots:
</p>
<p><code>beta</code> is a numeric matrix whose rows are the posterior estimates
of P(token|topic)
</p>
<p><code>theta</code> is a numeric matrix  whose rows are the posterior estimates of
P(topic|document)
</p>
<p><code>lambda</code> is a numeric matrix whose rows are the posterior estimates of
P(topic|token), calculated using Bayes's rule.
See <code><a href="#topic+calc_lambda">calc_lambda</a></code>.
</p>
<p><code>alpha</code> is the prior for topics over documents. If <code>optimize_alpha</code>
is <code>FALSE</code>, <code>alpha</code> is what the user passed when calling
<code><a href="#topic+tidylda">tidylda</a></code>. If <code>optimize_alpha</code> is <code>TRUE</code>,
<code>alpha</code> is a numeric vector returned in the <code>alpha</code> slot from a
call to <code><a href="#topic+fit_lda_c">fit_lda_c</a></code>.
</p>
<p><code>eta</code> is the prior for tokens over topics. This is what the user passed
when calling <code><a href="#topic+tidylda">tidylda</a></code>.
</p>
<p><code>summary</code> is the result of a call to <code><a href="#topic+summarize_topics">summarize_topics</a></code>
</p>
<p><code>call</code> is the result of <code><a href="base.html#topic+match.call">match.call</a></code> called at the top
of <code><a href="#topic+tidylda">tidylda</a></code>
</p>
<p><code>log_likelihood</code> is a <code><a href="tibble.html#topic+tibble">tibble</a></code> whose columns are
the iteration and log likelihood at that iteration. This slot is only populated
if <code>calc_likelihood = TRUE</code>
</p>
<p><code>r2</code> is a numeric scalar resulting from a call to
<code><a href="mvrsquared.html#topic+calc_rsquared">calc_rsquared</a></code>. This slot only populated if
<code>calc_r2 = TRUE</code>
</p>


<h3>Note</h3>

<p>In general, the arguments of this function should be what the user passed
when calling <code><a href="#topic+tidylda">tidylda</a></code>.
</p>
<p><code>burnin</code> is used only to determine whether or not burn in iterations
were used when fitting the model. If <code>burnin &gt; -1</code> then posteriors
are calculated using <code>lda$Cd_mean</code> and <code>lda$Cv_mean</code> respectively.
Otherwise, posteriors are calculated using <code>lda$Cd_mean</code> and
<code>lda$Cv_mean</code>.
</p>
<p>The class of <code>call</code> isn't checked. It's just passed through to the
object returned by this function. Might be useful if you are using this
function for troubleshooting or something.
</p>

<hr>
<h2 id='nih'>Abstracts and metadata from NIH research grants awarded in 2014</h2><span id='topic+nih'></span><span id='topic+nih_sample'></span><span id='topic+nih_sample_dtm'></span>

<h3>Description</h3>

<p>This dataset holds information on research grants awarded by the National
Institutes of Health (NIH) in 2014. The data set was downloaded in
approximately January of 2015. It includes both 'projects' and 'abstracts' 
files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("nih_sample")
</code></pre>


<h3>Format</h3>

<p>For <code>nih_sample</code>, a <code><a href="tibble.html#topic+tibble">tibble</a></code> of 100 randomly-sampled
grants' abstracts and metadata. For <code>nih_sample_dtm</code>, a
<code><a href="Matrix.html#topic+dgCMatrix-class">dgCMatrix-class</a></code> representing the document term matrix
of abstracts from 100 randomly-sampled grants.
</p>


<h3>Source</h3>

<p>National Institutes of Health ExPORTER
<a href="https://reporter.nih.gov/exporter">https://reporter.nih.gov/exporter</a>
</p>

<hr>
<h2 id='posterior'>Draw from the marginal posteriors of a tidylda topic model</h2><span id='topic+posterior'></span><span id='topic+posterior.tidylda'></span>

<h3>Description</h3>

<p>Sample from the marginal posteriors of a <code>tidylda</code> topic
model. This is useful for quantifying uncertainty around the parameters of
<code>beta</code> or <code>theta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posterior(x, ...)

## S3 method for class 'tidylda'
posterior(x, matrix, which, times, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="posterior_+3A_x">x</code></td>
<td>
<p>An object of class <code>tidylda</code>.</p>
</td></tr>
<tr><td><code id="posterior_+3A_...">...</code></td>
<td>
<p>Other arguments, currently not used.</p>
</td></tr>
<tr><td><code id="posterior_+3A_matrix">matrix</code></td>
<td>
<p>A character of either 'theta' or 'beta', indicating from which
matrix to draw posterior samples.</p>
</td></tr>
<tr><td><code id="posterior_+3A_which">which</code></td>
<td>
<p>Row index of <code>theta</code>, for document, or <code>beta</code>, for
topic, from which to draw samples. <code>which</code> may also be a vector of
indices to sample from multiple documents or topics simultaneously.</p>
</td></tr>
<tr><td><code id="posterior_+3A_times">times</code></td>
<td>
<p>Integer, number of samples to draw.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>posterior</code> returns a tibble with one row per parameter per sample.
</p>
<p>Returns a data frame where each row is a single sample from the posterior. 
Each column is the distribution over a single parameter. The variable <code>var</code>
is a facet for subsetting by document (for theta) or topic (for beta).
</p>


<h3>References</h3>

<p>Heinrich, G. (2005) Parameter estimation for text analysis. Technical report. 
<a href="http://www.arbylon.net/publications/text-est.pdf">http://www.arbylon.net/publications/text-est.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load some data
data(nih_sample_dtm)

# fit a model
set.seed(12345)

m &lt;- tidylda(
  data = nih_sample_dtm[1:20, ], k = 5,
  iterations = 200, burnin = 175
)

# sample from the marginal posterior corresponding to topic 1
t1 &lt;- posterior(
  x = m,
  matrix = "beta",
  which = 1,
  times = 100  
)

# sample from the marginal posterior corresponding to documents 5 and 6
d5 &lt;- posterior(
  x = m,
  matrix = "theta",
  which = c(5, 6),
  times = 100
)

</code></pre>

<hr>
<h2 id='predict.tidylda'>Get predictions from a Latent Dirichlet Allocation model</h2><span id='topic+predict.tidylda'></span>

<h3>Description</h3>

<p>Obtains predictions of topics for new documents from a fitted LDA model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tidylda'
predict(
  object,
  new_data,
  type = c("prob", "class", "distribution"),
  method = c("gibbs", "dot"),
  iterations = NULL,
  burnin = -1,
  no_common_tokens = c("default", "zero", "uniform"),
  times = 100,
  threads = 1,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.tidylda_+3A_object">object</code></td>
<td>
<p>a fitted object of class <code>tidylda</code></p>
</td></tr>
<tr><td><code id="predict.tidylda_+3A_new_data">new_data</code></td>
<td>
<p>a DTM or TCM of class <code>dgCMatrix</code> or a numeric vector</p>
</td></tr>
<tr><td><code id="predict.tidylda_+3A_type">type</code></td>
<td>
<p>one of &quot;prob&quot;, &quot;class&quot;, or &quot;distribution&quot;. Defaults to &quot;prob&quot;.</p>
</td></tr>
<tr><td><code id="predict.tidylda_+3A_method">method</code></td>
<td>
<p>one of either &quot;gibbs&quot; or &quot;dot&quot;. If &quot;gibbs&quot; Gibbs sampling is used
and <code>iterations</code> must be specified.</p>
</td></tr>
<tr><td><code id="predict.tidylda_+3A_iterations">iterations</code></td>
<td>
<p>If <code>method = "gibbs"</code>, an integer number of iterations
for the Gibbs sampler to run. A future version may include automatic stopping criteria.</p>
</td></tr>
<tr><td><code id="predict.tidylda_+3A_burnin">burnin</code></td>
<td>
<p>If <code>method = "gibbs"</code>, an integer number of burnin iterations.
If <code>burnin</code> is greater than -1, the entries of the resulting &quot;theta&quot; matrix
are an average over all iterations greater than <code>burnin</code>.
Behavior is the same as documented in <code><a href="#topic+tidylda">tidylda</a></code>.</p>
</td></tr>
<tr><td><code id="predict.tidylda_+3A_no_common_tokens">no_common_tokens</code></td>
<td>
<p>behavior when encountering documents that have no tokens
in common with the model. Options are &quot;<code>default</code>&quot;, &quot;<code>zero</code>&quot;,
or &quot;<code>uniform</code>&quot;. See 'details', below for explanation of behavior.</p>
</td></tr>
<tr><td><code id="predict.tidylda_+3A_times">times</code></td>
<td>
<p>Integer, number of samples to draw if <code>type = "distribution"</code>.
Ignored if <code>type</code> is &quot;class&quot; or &quot;prob&quot;. Defaults to 100.</p>
</td></tr>
<tr><td><code id="predict.tidylda_+3A_threads">threads</code></td>
<td>
<p>Number of parallel threads, defaults to 1. Note: currently
ignored; only single-threaded prediction is implemented.</p>
</td></tr>
<tr><td><code id="predict.tidylda_+3A_verbose">verbose</code></td>
<td>
<p>Logical. Do you want to print a progress bar out to the console?
Only active if <code>method = "gibbs"</code>. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="predict.tidylda_+3A_...">...</code></td>
<td>
<p>Additional arguments, currently unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>predict.tidylda</code> encounters documents that have no tokens in common
with the model in <code>object</code> it will engage in one of three behaviors based
on the setting of <code>no_common_tokens</code>.
</p>
<p><code>default</code> (the default) sets all topics to 0 for offending documents. This 
enables continued computations downstream in a way that <code>NA</code> would not.
However, if <code>no_common_tokens == "default"</code>, then <code>predict.tidylda</code>
will emit a warning for every such document it encounters.
</p>
<p><code>zero</code> has the same behavior as <code>default</code> but it emits a message
instead of a warning.
</p>
<p><code>uniform</code> sets all topics to 1/k for every topic for offending documents.
it does not emit a warning or message.
</p>


<h3>Value</h3>

<p><code>type</code> gives different outputs depending on whether the user selects
&quot;prob&quot;, &quot;class&quot;, or &quot;distribution&quot;. If &quot;prob&quot;, the default, returns a
a &quot;theta&quot; matrix with one row per document and one column per topic. If
&quot;class&quot;, returns a vector with the topic index of the most likely topic in
each document. If &quot;distribution&quot;, returns a tibble with one row per
parameter per sample. Number of samples is set by the <code>times</code> argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load some data
data(nih_sample_dtm)

# fit a model
set.seed(12345)

m &lt;- tidylda(
  data = nih_sample_dtm[1:20, ], k = 5,
  iterations = 200, burnin = 175
)

str(m)

# predict on held-out documents using gibbs sampling "fold in"
p1 &lt;- predict(m, nih_sample_dtm[21:100, ],
  method = "gibbs",
  iterations = 200, burnin = 175
)

# predict on held-out documents using the dot product
p2 &lt;- predict(m, nih_sample_dtm[21:100, ], method = "dot")

# compare the methods
barplot(rbind(p1[1, ], p2[1, ]), beside = TRUE, col = c("red", "blue"))

# predict classes on held out documents
p3 &lt;- predict(m, nih_sample_dtm[21:100, ],
  method = "gibbs",
  type = "class",
  iterations = 100, burnin = 75
)

# predict distribution on held out documents
p4 &lt;- predict(m, nih_sample_dtm[21:100, ],
  method = "gibbs",
  type = "distribution",
  iterations = 100, burnin = 75,
  times = 10
)

</code></pre>

<hr>
<h2 id='print.tidylda'>Print Method for tidylda</h2><span id='topic+print.tidylda'></span>

<h3>Description</h3>

<p>Print a summary for objects of class <code>tidylda</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tidylda'
print(x, digits = max(3L, getOption("digits") - 3L), n = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.tidylda_+3A_x">x</code></td>
<td>
<p>an object of class <code>tidylda</code></p>
</td></tr>
<tr><td><code id="print.tidylda_+3A_digits">digits</code></td>
<td>
<p>minimal number of significant digits</p>
</td></tr>
<tr><td><code id="print.tidylda_+3A_n">n</code></td>
<td>
<p>Number of rows to show in each displayed <code><a href="tibble.html#topic+tibble">tibble</a></code>.</p>
</td></tr>
<tr><td><code id="print.tidylda_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Silently returns <code>x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dtm &lt;- nih_sample_dtm

lda &lt;- tidylda(data = dtm, k = 10, iterations = 100)

print(lda)

lda

print(lda, digits = 2)

</code></pre>

<hr>
<h2 id='recover_counts_from_probs'>Get Count Matrices from Beta or Theta (and Priors)</h2><span id='topic+recover_counts_from_probs'></span>

<h3>Description</h3>

<p>This function is a core component of <code><a href="#topic+initialize_topic_counts">initialize_topic_counts</a></code>.
See details, below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recover_counts_from_probs(prob_matrix, prior_matrix, total_vector)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="recover_counts_from_probs_+3A_prob_matrix">prob_matrix</code></td>
<td>
<p>a numeric <code>beta</code> or <code>theta</code> matrix</p>
</td></tr>
<tr><td><code id="recover_counts_from_probs_+3A_prior_matrix">prior_matrix</code></td>
<td>
<p>a matrix of same dimension as <code>prob_matrix</code> whose 
entries represent the relevant prior (<code>alpha</code> or <code>eta</code>)</p>
</td></tr>
<tr><td><code id="recover_counts_from_probs_+3A_total_vector">total_vector</code></td>
<td>
<p>a vector of token counts of length <code>ncol(prob_matrix)</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses a probability matrix (theta or beta), its prior (alpha or
eta, respectively), and a vector of counts to simulate what the the Cd or
Cv matrix would be at the end of a Gibbs run that resulted in that probability
matrix.
</p>
<p>For example, theta is calculated from a matrix of counts, Cd, and a prior,
alpha. Specifically, the i,j entry of theta is given by
</p>
<p><code>(Cd[i, j] + alpha[i, j]) / sum(Cd[, j] + alpha[, j])</code>
</p>
<p>Similarly, beta comes from
</p>
<p><code>(Cv[i, j] + eta[i, j]) / sum(Cv[, j] + eta[, j])</code>
</p>
<p>(The above are written to be general with respect to alpha and eta being
matrices. They could also be vectors or scalars.)
</p>
<p>So, this function uses the above formulas to try and reconstruct Cd or Cv
from theta and alpha or beta and eta, respectively. As of this writing,
this method is experimental. In the future, there will be a paper with
more technical details cited here.
</p>
<p>The priors must be matrices for the purposes of the function. This is to
support topic seeding and model updates. The former requires eta to be a 
matrix. The latter may require eta to be a matrix. Here, alpha is also
required to be a matrix for compatibility.
</p>
<p>All that said, for now <code><a href="#topic+initialize_topic_counts">initialize_topic_counts</a></code> only
uses this function to calculate Cd.
</p>


<h3>Value</h3>

<p>Returns a matrix corresponding to the number of times each topic sampled
for each document (<code>Cd</code>) or for each token (<code>Cv</code>) depending on
whether or not <code>prob_matrix</code>/<code>prior_matrix</code> corresponds to
<code>theta</code>/<code>alpha</code> or <code>beta</code>/<code>eta</code> respectively.
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+augment'></span><span id='topic+glance'></span><span id='topic+tidy'></span><span id='topic+refit'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+augment">augment</a></code>, <code><a href="generics.html#topic+glance">glance</a></code>, <code><a href="generics.html#topic+refit">refit</a></code>, <code><a href="generics.html#topic+tidy">tidy</a></code></p>
</dd>
</dl>

<hr>
<h2 id='refit.tidylda'>Update a Latent Dirichlet Allocation topic model</h2><span id='topic+refit.tidylda'></span>

<h3>Description</h3>

<p>Update an LDA model using collapsed Gibbs sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tidylda'
refit(
  object,
  new_data,
  iterations = NULL,
  burnin = -1,
  prior_weight = 1,
  additional_k = 0,
  additional_eta_sum = 250,
  optimize_alpha = FALSE,
  calc_likelihood = FALSE,
  calc_r2 = FALSE,
  return_data = FALSE,
  threads = 1,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="refit.tidylda_+3A_object">object</code></td>
<td>
<p>a fitted object of class <code>tidylda</code>.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_new_data">new_data</code></td>
<td>
<p>A document term matrix or term co-occurrence matrix of class dgCMatrix.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_iterations">iterations</code></td>
<td>
<p>Integer number of iterations for the Gibbs sampler to run.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_burnin">burnin</code></td>
<td>
<p>Integer number of burnin iterations. If <code>burnin</code> is greater than -1,
the resulting &quot;beta&quot; and &quot;theta&quot; matrices are an average over all iterations
greater than <code>burnin</code>.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_prior_weight">prior_weight</code></td>
<td>
<p>Numeric, 0 or greater or <code>NA</code>. The weight of the 
<code>beta</code> as a prior from the base model. See Details, below.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_additional_k">additional_k</code></td>
<td>
<p>Integer number of topics to add, defaults to 0.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_additional_eta_sum">additional_eta_sum</code></td>
<td>
<p>Numeric magnitude of prior for additional topics.
Ignored if <code>additional_k</code> is 0. Defaults to 250.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_optimize_alpha">optimize_alpha</code></td>
<td>
<p>Logical. Experimental. Do you want to optimize alpha
every iteration? Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_calc_likelihood">calc_likelihood</code></td>
<td>
<p>Logical. Do you want to calculate the log likelihood every iteration?
Useful for assessing convergence. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_calc_r2">calc_r2</code></td>
<td>
<p>Logical. Do you want to calculate R-squared after the model is trained?
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_return_data">return_data</code></td>
<td>
<p>Logical. Do you want <code>new_data</code> returned as part of the model object?</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_threads">threads</code></td>
<td>
<p>Number of parallel threads, defaults to 1.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_verbose">verbose</code></td>
<td>
<p>Logical. Do you want to print a progress bar out to the console?
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="refit.tidylda_+3A_...">...</code></td>
<td>
<p>Additional arguments, currently unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>refit</code> allows you to (a) update the probabilities (i.e. weights) of
a previously-fit model with new data or additional iterations and (b) optionally
use <code>beta</code> of a previously-fit LDA topic model as the <code>eta</code> prior
for the new model. This is tuned by setting <code>beta_as_prior = FALSE</code> or
<code>beta_as_prior = TRUE</code> respectively.
</p>
<p><code>prior_weight</code> tunes how strong the base model is represented in the prior.
If <code>prior_weight = 1</code>, then the tokens from the base model's training data
have the same relative weight as tokens in <code>new_data</code>. In other words,
it is like just adding training data. If <code>prior_weight</code> is less than 1,
then tokens in <code>new_data</code> are given more weight. If <code>prior_weight</code>
is greater than 1, then the tokens from the base model's training data are
given more weight.
</p>
<p>If <code>prior_weight</code> is <code>NA</code>, then the new <code>eta</code> is equal to 
<code>eta</code> from the old model, with new tokens folded in. 
(For handling of new tokens, see below.) Effectively, this just controls
how the sampler initializes (described below), but does not give prior
weight to the base model.
</p>
<p>Instead of initializing token-topic assignments in the manner for new
models (see <code><a href="#topic+tidylda">tidylda</a></code>), the update initializes in 2
steps:
</p>
<p>First, topic-document probabilities (i.e. <code>theta</code>) are obtained by a
call to <code><a href="#topic+predict.tidylda">predict.tidylda</a></code> using <code>method = "dot"</code>
for the documents in <code>new_data</code>. Next, both <code>beta</code> and <code>theta</code> are
passed to an internal function, <code><a href="#topic+initialize_topic_counts">initialize_topic_counts</a></code>,
which assigns topics to tokens in a manner approximately proportional to 
the posteriors and executes a single Gibbs iteration.
</p>
<p><code>refit</code> handles the addition of new vocabulary by adding a flat prior
over new tokens. Specifically, each entry in the new prior is equal to the
10th percentile of <code>eta</code> from the old model. The resulting model will
have the total vocabulary of the old model plus any new vocabulary tokens.
In other words, after running <code>refit.tidylda</code> <code>ncol(beta) &gt;= ncol(new_data)</code>
where <code>beta</code> is from the new model and <code>new_data</code> is the additional data.
</p>
<p>You can add additional topics by setting the <code>additional_k</code> parameter
to an integer greater than zero. New entries to <code>alpha</code> have a flat
prior equal to the median value of <code>alpha</code> in the old model. (Note that
if <code>alpha</code> itself is a flat prior, i.e. scalar, then the new topics have
the same value for their prior.) New entries to <code>eta</code> have a shape 
from the average of all previous topics in <code>eta</code> and scaled by
<code>additional_eta_sum</code>.
</p>


<h3>Value</h3>

<p>Returns an S3 object of class c(&quot;tidylda&quot;).
</p>


<h3>Note</h3>

<p>Updates are, as of this writing, are almost-surely useful but their behaviors
have not been optimized or well-studied. Caveat emptor!
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load a document term matrix
data(nih_sample_dtm)

d1 &lt;- nih_sample_dtm[1:50, ]

d2 &lt;- nih_sample_dtm[51:100, ]

# fit a model
m &lt;- tidylda(d1,
  k = 10,
  iterations = 200, burnin = 175
)

# update an existing model by adding documents using old model as prior
m2 &lt;- refit(
  object = m,
  new_data = rbind(d1, d2),
  iterations = 200,
  burnin = 175,
  prior_weight = 1
)

# use an old model to initialize new model and not use old model as prior
m3 &lt;- refit(
  object = m,
  new_data = d2, # new documents only
  iterations = 200,
  burnin = 175,
  prior_weight = NA
)

# add topics while updating a model by adding documents
m4 &lt;- refit(
  object = m,
  new_data = rbind(d1, d2),
  additional_k = 3,
  iterations = 200,
  burnin = 175
)

</code></pre>

<hr>
<h2 id='summarize_topics'>Summarize a topic model consistently across methods/functions</h2><span id='topic+summarize_topics'></span>

<h3>Description</h3>

<p>Summarizes topics in a model. Called by <code><a href="#topic+tidylda">tidylda</a></code>
and <code><a href="#topic+refit.tidylda">refit.tidylda</a></code> and used to augment
<code><a href="#topic+print.tidylda">print.tidylda</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summarize_topics(theta, beta, dtm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summarize_topics_+3A_theta">theta</code></td>
<td>
<p>numeric matrix whose rows represent P(topic|document)</p>
</td></tr>
<tr><td><code id="summarize_topics_+3A_beta">beta</code></td>
<td>
<p>numeric matrix whose rows represent P(token|topic)</p>
</td></tr>
<tr><td><code id="summarize_topics_+3A_dtm">dtm</code></td>
<td>
<p>a document term matrix or term co-occurrence matrix of class <code>dgCMatrix</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code><a href="tibble.html#topic+tibble">tibble</a></code> with the following columns:
<code>topic</code> is the integer row number of <code>beta</code>.
<code>prevalence</code> is the frequency of each topic throughout the corpus it
was trained on normalized so that it sums to 100.
<code>coherence</code> makes a call to <code><a href="#topic+calc_prob_coherence">calc_prob_coherence</a></code>
using the default 5 most-probable terms in each topic.
<code>top_terms</code> displays the top 5 most-probable terms in each topic.
</p>


<h3>Note</h3>

<p><code>prevalence</code> should be proportional to P(topic). It is calculated by
weighting on document length. So, topics prevalent in longer documents get
more weight than topics prevalent in shorter documents. It is calculated
by
</p>
<p><code>prevalence &lt;- rowSums(dtm) * theta %&gt;% colSums()</code>
</p>
<p><code>prevalence &lt;- (prevalence * 100) %&gt;% round(3)</code>
</p>
<p>An alternative calculation (not implemented here) might have been
</p>
<p><code>prevalence &lt;- colSums(dtm) * t(beta) %&gt;% colSums()</code>
</p>
<p><code>prevalence &lt;- (prevalence * 100) %&gt;% round(3)</code>
</p>

<hr>
<h2 id='tidy_dgcmatrix'>Create a tidy tibble for a dgCMatrix</h2><span id='topic+tidy_dgcmatrix'></span>

<h3>Description</h3>

<p>Create a tidy tibble for a dgCMatrix. Will probably be a PR to
<a href="tidytext.html#topic+tidytext">tidytext</a> in the future
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_dgcmatrix(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidy_dgcmatrix_+3A_x">x</code></td>
<td>
<p>must be of class dgCMatrix</p>
</td></tr>
<tr><td><code id="tidy_dgcmatrix_+3A_...">...</code></td>
<td>
<p>Extra arguments, not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a triplet matrix with columns &quot;document&quot;, &quot;term&quot;, and &quot;count&quot;
</p>

<hr>
<h2 id='tidy_triplet'>Utility function to tidy a simple triplet matrix</h2><span id='topic+tidy_triplet'></span>

<h3>Description</h3>

<p>Utility function to tidy a simple triplet matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_triplet(x, triplets, row_names = NULL, col_names = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidy_triplet_+3A_x">x</code></td>
<td>
<p>Object with rownames and colnames</p>
</td></tr>
<tr><td><code id="tidy_triplet_+3A_triplets">triplets</code></td>
<td>
<p>A data frame or list of i, j, x</p>
</td></tr>
<tr><td><code id="tidy_triplet_+3A_row_names">row_names</code></td>
<td>
<p>rownames, if not gotten from rownames(x)</p>
</td></tr>
<tr><td><code id="tidy_triplet_+3A_col_names">col_names</code></td>
<td>
<p>colnames, if not gotten from colnames(x)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a triplet matrix in the form of a data frame. The first
column indexes rows. The second column indexes columns. The third column
contains the i,j values.
</p>


<h3>Note</h3>

<p>This function ported from <code><a href="tidytext.html#topic+tidytext">tidytext</a></code>, copyright
2017 David Robinson and Julia Silge. Moved the function here for stability
reasons, as it is internal to tidytext
</p>

<hr>
<h2 id='tidy.tidylda'>Tidy a matrix from a <code>tidylda</code> topic model</h2><span id='topic+tidy.tidylda'></span><span id='topic+tidy.matrix'></span>

<h3>Description</h3>

<p>Tidy the result of a <code>tidylda</code> topic model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tidylda'
tidy(x, matrix, log = FALSE, ...)

## S3 method for class 'matrix'
tidy(x, matrix, log = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidy.tidylda_+3A_x">x</code></td>
<td>
<p>an object of class <code>tidylda</code> or an individual <code>beta</code>, 
<code>theta</code>, or <code>lambda</code> matrix.</p>
</td></tr>
<tr><td><code id="tidy.tidylda_+3A_matrix">matrix</code></td>
<td>
<p>the matrix to tidy; one of <code>'beta'</code>, <code>'theta'</code>, or
<code>'lambda'</code></p>
</td></tr>
<tr><td><code id="tidy.tidylda_+3A_log">log</code></td>
<td>
<p>do you want to have the result on a log scale? Defaults to <code>FALSE</code></p>
</td></tr>
<tr><td><code id="tidy.tidylda_+3A_...">...</code></td>
<td>
<p>other arguments passed to methods,currently not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code><a href="tibble.html#topic+tibble">tibble</a></code>.
</p>
<p>If <code>matrix = "beta"</code> then the result is a table of one row per topic
and token with the following columns: <code>topic</code>, <code>token</code>, <code>beta</code>
</p>
<p>If <code>matrix = "theta"</code> then the result is a table of one row per document
and topic with the following columns: <code>document</code>, <code>topic</code>, <code>theta</code>
</p>
<p>If <code>matrix = "lambda"</code> then the result is a table of one row per topic
and token with the following columns: <code>topic</code>, <code>token</code>, <code>lambda</code>
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>tidy(matrix)</code>: Tidy an individual matrix.
Useful for predictions and called from tidy.tidylda
</p>
</li></ul>


<h3>Note</h3>

<p>If <code>log = TRUE</code> then &quot;log_&quot; will be appended to the name of the third
column of the resulting table. e.g &quot;<code>beta</code>&quot; becomes &quot;<code>log_beta</code>&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dtm &lt;- nih_sample_dtm

lda &lt;- tidylda(data = dtm, k = 10, iterations = 100, burnin = 75)

tidy_beta &lt;- tidy(lda, matrix = "beta")

tidy_theta &lt;- tidy(lda, matrix = "theta")

tidy_lambda &lt;- tidy(lda, matrix = "lambda")

</code></pre>

<hr>
<h2 id='tidylda'>Fit a Latent Dirichlet Allocation topic model</h2><span id='topic+tidylda'></span>

<h3>Description</h3>

<p>Fit a Latent Dirichlet Allocation topic model using collapsed Gibbs sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidylda(
  data,
  k,
  iterations = NULL,
  burnin = -1,
  alpha = 0.1,
  eta = 0.05,
  optimize_alpha = FALSE,
  calc_likelihood = TRUE,
  calc_r2 = FALSE,
  threads = 1,
  return_data = FALSE,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidylda_+3A_data">data</code></td>
<td>
<p>A document term matrix or term co-occurrence matrix. The preferred
class is a <code><a href="Matrix.html#topic+dgCMatrix-class">dgCMatrix-class</a></code>. However there is support
for any <code><a href="Matrix.html#topic+Matrix-class">Matrix-class</a></code> object as well as several other
commonly-used classes such as <code><a href="base.html#topic+matrix">matrix</a></code>,
<code><a href="quanteda.html#topic+dfm">dfm</a></code>, <code><a href="tm.html#topic+DocumentTermMatrix">DocumentTermMatrix</a></code>, and
<code><a href="slam.html#topic+simple_triplet_matrix">simple_triplet_matrix</a></code></p>
</td></tr>
<tr><td><code id="tidylda_+3A_k">k</code></td>
<td>
<p>Integer number of topics.</p>
</td></tr>
<tr><td><code id="tidylda_+3A_iterations">iterations</code></td>
<td>
<p>Integer number of iterations for the Gibbs sampler to run.</p>
</td></tr>
<tr><td><code id="tidylda_+3A_burnin">burnin</code></td>
<td>
<p>Integer number of burnin iterations. If <code>burnin</code> is greater than -1,
the resulting &quot;beta&quot; and &quot;theta&quot; matrices are an average over all iterations
greater than <code>burnin</code>.</p>
</td></tr>
<tr><td><code id="tidylda_+3A_alpha">alpha</code></td>
<td>
<p>Numeric scalar or vector of length <code>k</code>. This is the prior
for topics over documents.</p>
</td></tr>
<tr><td><code id="tidylda_+3A_eta">eta</code></td>
<td>
<p>Numeric scalar, numeric vector of length <code>ncol(data)</code>,
or numeric matrix with <code>k</code> rows and <code>ncol(data)</code> columns.
This is the prior for words over topics.</p>
</td></tr>
<tr><td><code id="tidylda_+3A_optimize_alpha">optimize_alpha</code></td>
<td>
<p>Logical. Do you want to optimize alpha every iteration?
Defaults to <code>FALSE</code>. See 'details' below for more information.</p>
</td></tr>
<tr><td><code id="tidylda_+3A_calc_likelihood">calc_likelihood</code></td>
<td>
<p>Logical. Do you want to calculate the log likelihood every iteration?
Useful for assessing convergence. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tidylda_+3A_calc_r2">calc_r2</code></td>
<td>
<p>Logical. Do you want to calculate R-squared after the model is trained?
Defaults to <code>FALSE</code>. See <code><a href="#topic+calc_lda_r2">calc_lda_r2</a></code>.</p>
</td></tr>
<tr><td><code id="tidylda_+3A_threads">threads</code></td>
<td>
<p>Number of parallel threads, defaults to 1. See Details, below.</p>
</td></tr>
<tr><td><code id="tidylda_+3A_return_data">return_data</code></td>
<td>
<p>Logical. Do you want <code>data</code> returned as part of the model object?</p>
</td></tr>
<tr><td><code id="tidylda_+3A_verbose">verbose</code></td>
<td>
<p>Logical. Do you want to print a progress bar out to the console?
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tidylda_+3A_...">...</code></td>
<td>
<p>Additional arguments, currently unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calls a collapsed Gibbs sampler for Latent Dirichlet Allocation
written using the excellent Rcpp package. Some implementation notes follow:
</p>
<p>Topic-token and topic-document assignments are not initialized based on a
uniform-random sampling, as is common. Instead, topic-token probabilities
(i.e. <code>beta</code>) are initialized by sampling from a Dirichlet distribution
with <code>eta</code> as its parameter. The same is done for topic-document
probabilities (i.e. <code>theta</code>) using <code>alpha</code>. Then an internal
function is called (<code><a href="#topic+initialize_topic_counts">initialize_topic_counts</a></code>) to run
a single Gibbs iteration to initialize assignments of tokens to topics and
topics to documents.
</p>
<p>When you use burn-in iterations (i.e. <code>burnin = TRUE</code>), the resulting
<code>beta</code> and <code>theta</code> matrices are calculated by averaging over every
iteration after the specified  number of burn-in iterations. If you do not
use burn-in iterations, then the matrices are calculated from the last run
only. Ideally, you'd burn in every iteration before convergence, then average
over the chain after its converged (and thus every observation is independent).
</p>
<p>If you set <code>optimize_alpha</code> to <code>TRUE</code>, then each element of <code>alpha</code>
is proportional to the number of times each topic has be sampled that iteration
averaged with the value of <code>alpha</code> from the previous iteration. This lets
you start with a symmetric <code>alpha</code> and drift into an asymmetric one.
However, (a) this probably means that convergence will take longer to happen
or convergence may not happen at all. And (b) I make no guarantees that doing this
will give you any benefit or that it won't hurt your model. Caveat emptor!
</p>
<p>The log likelihood calculation is the same that can be found on page 9 of
<a href="https://arxiv.org/pdf/1510.08628.pdf">https://arxiv.org/pdf/1510.08628.pdf</a>. The only difference is that the
version in <code><a href="#topic+tidylda">tidylda</a></code> allows <code>eta</code> to be a
vector or matrix. (Vector used in this function, matrix used for model
updates in <code><a href="#topic+refit.tidylda">refit.tidylda</a></code>. At present, the
log likelihood function appears to be ok for assessing convergence. i.e. It
has the right shape. However, it is, as of this writing, returning positive
numbers, rather than the expected negative numbers. Looking into that, but
in the meantime caveat emptor once again.
</p>
<p>Parallelism, is not currently implemented. The <code>threads</code> argument is a
placeholder for planned enhancements.
</p>


<h3>Value</h3>

<p>Returns an S3 object of class <code>tidylda</code>. See <code><a href="#topic+new_tidylda">new_tidylda</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load some data
data(nih_sample_dtm)

# fit a model
set.seed(12345)
m &lt;- tidylda(
  data = nih_sample_dtm[1:20, ], k = 5,
  iterations = 200, burnin = 175
)

str(m)

# predict on held-out documents using gibbs sampling "fold in"
p1 &lt;- predict(m, nih_sample_dtm[21:100, ],
  method = "gibbs",
  iterations = 200, burnin = 175
)

# predict on held-out documents using the dot product method
p2 &lt;- predict(m, nih_sample_dtm[21:100, ], method = "dot")

# compare the methods
barplot(rbind(p1[1, ], p2[1, ]), beside = TRUE, col = c("red", "blue"))
</code></pre>

<hr>
<h2 id='tidylda_bridge'>Bridge function for fitting <code>tidylda</code> topic models</h2><span id='topic+tidylda_bridge'></span>

<h3>Description</h3>

<p>Takes in arguments from various <code>tidylda</code> S3 methods and fits the
resulting topic model. The arguments to this function are documented in
<code><a href="#topic+tidylda">tidylda</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidylda_bridge(
  data,
  k,
  iterations,
  burnin,
  alpha,
  eta,
  optimize_alpha,
  calc_likelihood,
  calc_r2,
  threads,
  return_data,
  verbose,
  mc,
  ...
)
</code></pre>


<h3>Value</h3>

<p>Returns a <code>tidylda</code> S3 object as documented in <code><a href="#topic+new_tidylda">new_tidylda</a></code>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
