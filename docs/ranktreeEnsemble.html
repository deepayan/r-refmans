<!DOCTYPE html><html><head><title>Help for package ranktreeEnsemble</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ranktreeEnsemble}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#extract.rules'>
<p>Extract Interpretable Decision Rules from a Random Forest Model</p></a></li>
<li><a href='#importance'>
<p>Variable Importance Index for Each Predictor</p></a></li>
<li><a href='#pair'>
<p>Transform Continuous Variables into Ranked Binary Pairs</p></a></li>
<li><a href='#predict'>
<p>Prediction or Extract Predicted Values for Random Forest, Random Forest Rule or Boosting Models</p></a></li>
<li><a href='#ranktreeEnsemble'>
<p>Ensemble Models of Rank-Based Trees for Single Sample Classification with Interpretable Rules</p></a></li>
<li><a href='#rboost'><p>Generalized Boosted Modeling via Rank-Based Trees for Single Sample Classification with Gene Expression Profiles</p></a></li>
<li><a href='#rforest'>
<p>Random Forest via Rank-Based Trees for Single Sample Classification with Gene Expression Profiles</p></a></li>
<li><a href='#select.rules'>
<p>Select Decision Rules to Achieve Higher Prediction Accuracy</p></a></li>
<li><a href='#tnbc'><p>Gene expression profiles in triple-negative breast cancer cell</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Ensemble Models of Rank-Based Trees with Extracted Decision
Rules</td>
</tr>
<tr>
<td>Version:</td>
<td>0.22</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-08</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Min Lu &lt;luminwin@gmail.com&gt;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/TransBioInfoLab/ranktreeEnsemble/issues/">https://github.com/TransBioInfoLab/ranktreeEnsemble/issues/</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.10),randomForestSRC,gbm,methods,data.tree</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Description:</td>
<td>Fast computing an ensemble of rank-based trees via boosting or random forest on binary and multi-class problems. It converts continuous gene expression profiles into ranked gene pairs, for which the variable importance indices are computed and adopted for dimension reduction. Decision rules can be extracted from trees. </td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/TransBioInfoLab/ranktreeEnsemble/">https://github.com/TransBioInfoLab/ranktreeEnsemble/</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-18 20:04:54 UTC; min</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-18 20:20:05 UTC</td>
</tr>
<tr>
<td>Author:</td>
<td>Ruijie Yin [aut],
  Chen Ye [aut],
  Min Lu <a href="https://orcid.org/0000-0002-1386-1315"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
</table>
<hr>
<h2 id='extract.rules'>
Extract Interpretable Decision Rules from a Random Forest Model
</h2><span id='topic+extract.rules'></span>

<h3>Description</h3>

<p>Extract rules from a random forest <code>(rfsrc)</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract.rules(object, subtrees = 5,
              treedepth = 2,
              digit = 2,
              pairs = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract.rules_+3A_object">object</code></td>
<td>

<p>A random forest <code>(rfsrc)</code> object
</p>
</td></tr>
<tr><td><code id="extract.rules_+3A_subtrees">subtrees</code></td>
<td>

<p>Number of trees to extract rules
</p>
</td></tr>
<tr><td><code id="extract.rules_+3A_treedepth">treedepth</code></td>
<td>

<p>Tree depth. The larger the number, the longer the extracted rules are.
</p>
</td></tr>
<tr><td><code id="extract.rules_+3A_digit">digit</code></td>
<td>

<p>Digit to be displayed in the extracted rules.
</p>
</td></tr>
<tr><td><code id="extract.rules_+3A_pairs">pairs</code></td>
<td>

<p>Are varibles in <code>(object)</code> generated from the <code>pair</code> function? Set <code>pairs = FALSE</code> to extract rules from regular random forest <code>(rfsrc)</code> object with continuous predictors.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>rule</code></td>
<td>
<p>Interpretable extracted rules. Note that the performance score displayed is inaccurate based on few samples. </p>
</td></tr>
<tr><td><code>rule.raw</code></td>
<td>
<p>Rules directly extracted from trees for prediction purpose</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>Data used to grow trees from the argument <code>(object)</code>. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ruijie Yin (Maintainer,&lt;ruijieyin428@gmail.com&gt;), Chen Ye and Min Lu
</p>


<h3>References</h3>

<p>Lu M. Yin R. and Chen X.S. (2023). Ensemble Methods of Rank-Based Trees for Single Sample Classification with Gene Expression Profiles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(tnbc)
obj &lt;- rforest(subtype~., data = tnbc[1:100,c(1:5,337)])
objr &lt;- extract.rules(obj)
objr$rule[,1:3]

#### extract rules from a regular random forest
library(randomForestSRC)
obj2 &lt;- rfsrc(subtype~., data = tnbc[1:100,c(1:5,337)])
objr2 &lt;- extract.rules(obj2, pairs = FALSE)
objr2$rule[,1:3]

</code></pre>

<hr>
<h2 id='importance'>
Variable Importance Index for Each Predictor
</h2><span id='topic+importance'></span>

<h3>Description</h3>

<p>The function computes variable importance for each predictor from a rank-based random forests model or boosting model. A higher value indicates a more important predictor. The random forest implementation was performed via the function <code>vimp</code> directly imported from the <span class="pkg">randomForestSRC</span>
package.  Use the command <code>package?randomForestSRC</code> for more information. The boosting implementation was performed via the function <code>relative.influence</code> directly imported from the <span class="pkg">gbm</span>
package. For technical details, see the
vignette: <code>utils::browseVignettes("gbm")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>importance(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="importance_+3A_object">object</code></td>
<td>

<p>An object of class <code>rfsrc</code> generated from the function <code>rforest</code> or
<code>gbm</code> generated from the function <code>rboost</code>.
</p>
</td></tr>
<tr><td><code id="importance_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For the boosting model, a vector of variable importance values is given. For the random forest model, a matrix of variable importance values is given for the variable importance index for <code>all</code> the class labels, followed by the index for each class label.
</p>


<h3>Author(s)</h3>

<p>Ruijie Yin (Maintainer,&lt;ruijieyin428@gmail.com&gt;), Chen Ye and Min Lu
</p>


<h3>References</h3>

<p>Lu M. Yin R. and Chen X.S. (2023). Ensemble Methods of Rank-Based Trees for Single Sample Classification with Gene Expression Profiles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(tnbc)
######################################################
# Random Forest
######################################################
obj &lt;- rforest(subtype~., data = tnbc[,c(1:10,337)])
importance(obj)
######################################################
# Boosting
######################################################
obj &lt;- rboost(subtype~., data = tnbc[,c(1:10,337)])
importance(obj)

</code></pre>

<hr>
<h2 id='pair'>
Transform Continuous Variables into Ranked Binary Pairs
</h2><span id='topic+pair'></span>

<h3>Description</h3>

<p>The function transforms a dataset with <code class="reqn">p</code> continuous predictors into <code class="reqn">\frac{p*(p-1)}{2}</code> binary predictors of ranked pairs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pair(data, yvar.name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pair_+3A_data">data</code></td>
<td>

<p>A dataset with <code class="reqn">p</code> continuous variables or with <code class="reqn">p+1</code> variables including a dependent variable.
</p>
</td></tr>
<tr><td><code id="pair_+3A_yvar.name">yvar.name</code></td>
<td>

<p>The column name of the independent variable in <code>data</code>. By default, there is no dependent variable.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with the transformed data. The dependent variable is moved to the last column of the data.
</p>


<h3>Note</h3>

<p>The function is efficiently coded in C++.
</p>


<h3>Author(s)</h3>

<p>Ruijie Yin (Maintainer,&lt;ruijieyin428@gmail.com&gt;), Chen Ye and Min Lu
</p>


<h3>References</h3>

<p>Lu M. Yin R. and Chen X.S. (2023). Ensemble Methods of Rank-Based Trees for Single Sample Classification with Gene Expression Profiles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(tnbc)
datp &lt;- pair(tnbc[101:105,c(1:5,337)],"subtype")
datp
datp &lt;- pair(tnbc[105:110,1:5])
datp

</code></pre>

<hr>
<h2 id='predict'>
Prediction or Extract Predicted Values for Random Forest, Random Forest Rule or Boosting Models
</h2><span id='topic+predict'></span>

<h3>Description</h3>

<p>Obtain predicted values using a random forest <code>(rfsrc)</code>, random forest extracted rule <code>(rules)</code> or boosting <code>(gbm)</code> object. If no new data is provided, it extracts the out-of-bag predicted values of the outcome for the training data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict(object,
        newdata = NULL,
        newdata.pair = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>

<p>An object of class <code>rfsrc</code> generated from the function <code>rforest</code> or
<code>gbm</code> generated from the function <code>rboost</code>.
</p>
</td></tr>
<tr><td><code id="predict_+3A_newdata">newdata</code></td>
<td>

<p>Test data. If missing, the original training data is used for extracting the out-of-bag predicted values without running the model again.
</p>
</td></tr>
<tr><td><code id="predict_+3A_newdata.pair">newdata.pair</code></td>
<td>

<p>Is <code>newdata</code> already converted into binary ranked pairs from the <code>pair</code> function?
</p>
</td></tr>
<tr><td><code id="predict_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the boosting <code>(gbm)</code> object, the cross-validation predicted values are provided if <code>cv.folds&gt;=2</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>value</code></td>
<td>
<p>Predicted value of the outcome. For the random forest <code>(rfsrc)</code> object, it is the predicted probability. For the boosting <code>(gbm)</code> object, it is the fitted
values on the scale of regression function (e.g. log-odds scale). For the random forest extracted rule <code>(rules)</code> object, it is empty. </p>
</td></tr>
<tr><td><code>label</code></td>
<td>
<p>Predicted label of the outcome. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ruijie Yin (Maintainer,&lt;ruijieyin428@gmail.com&gt;), Chen Ye and Min Lu
</p>


<h3>References</h3>

<p>Lu M. Yin R. and Chen X.S. (2023). Ensemble Methods of Rank-Based Trees for Single Sample Classification with Gene Expression Profiles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(tnbc)
######################################################
# Random Forest
######################################################
obj &lt;- rforest(subtype~., data = tnbc[1:100,c(1:5,337)])
predict(obj)$label
predict(obj, tnbc[101:110,1:5])$label

datp &lt;- pair(tnbc[101:110,1:5])
predict(obj, datp, newdata.pair = TRUE)$label
######################################################
# Random Forest Extracted Rule
######################################################
 objr &lt;- extract.rules(obj)
 predict(objr)$label[1:5]
 predict(obj, tnbc[101:110,1:5])$label
######################################################
# Boosting
######################################################
obj &lt;- rboost(subtype~., data = tnbc[1:100,c(1:5,337)])
predict(obj)$label
predict(obj, tnbc[101:110,1:5])$label


</code></pre>

<hr>
<h2 id='ranktreeEnsemble'>
Ensemble Models of Rank-Based Trees for Single Sample Classification with Interpretable Rules
</h2><span id='topic+ranktreeEnsemble'></span><span id='topic+ranktreeEnsemble-package'></span>

<h3>Description</h3>

<p>The package <code>ranktreeEnsemble</code> implements an ensemble of rank-based trees in boosting with the LogitBoost cost  and random forests on both binary and multi-class problems. It converts continuous gene expression profiles into
ranked gene pairs, for which the variable importance indices are computed and adopted for dimension reduction. Interpretable rules can be extracted from trees.
</p>


<h3>Author(s)</h3>

<p>Ruijie Yin (Maintainer,&lt;ruijieyin428@gmail.com&gt;), Chen Ye and Min Lu
</p>


<h3>References</h3>

<p>Lu M. Yin R. and Chen X.S. (2023). Ensemble Methods of Rank-Based Trees for Single Sample Classification with Gene Expression Profiles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ranktreeEnsemble)
data(tnbc)
########### performance of Random Rank Forest
obj &lt;- rforest(subtype~., data = tnbc[,c(1:10,337)])
obj
# variable importance
importance(obj)
########### predict new data from Random Rank Forest
predict(obj, tnbc[101:110,1:10])$label
########### extract decision rules from rank-based trees
objr &lt;- extract.rules(obj)
objr$rule[1:5,]
predict(objr, tnbc[101:110,1:10])$label
########### filter decision rules with higher performance
objrs &lt;- select.rules(objr,tnbc[110:130,c(1:10,337)])
predict(objrs, tnbc[101:110,1:10])$label

</code></pre>

<hr>
<h2 id='rboost'>Generalized Boosted Modeling via Rank-Based Trees for Single Sample Classification with Gene Expression Profiles</h2><span id='topic+rboost'></span>

<h3>Description</h3>

<p>The function fits generalized boosted models via Rank-Based Trees on both binary and multi-class problems. It converts continuous gene expression profiles into
ranked gene pairs, for which the variable importance indices are computed and adopted for dimension reduction.   The boosting implementation was directly imported from the <span class="pkg">gbm</span>
package.  For technical details, see the
vignette: <code>utils::browseVignettes("gbm")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rboost(
  formula,
  data,
  dimreduce = TRUE,
  datrank = TRUE,
  distribution = "multinomial",
  weights,
  ntree = 100,
  nodedepth = 3,
  nodesize = 5,
  shrinkage = 0.05,
  bag.fraction = 0.5,
  train.fraction = 1,
  cv.folds = 5,
  keep.data = TRUE,
  verbose = TRUE,
  class.stratify.cv = TRUE,
  n.cores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rboost_+3A_formula">formula</code></td>
<td>
<p>Object of class 'formula' describing the model to fit.</p>
</td></tr>
<tr><td><code id="rboost_+3A_data">data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables.</p>
</td></tr>
<tr><td><code id="rboost_+3A_dimreduce">dimreduce</code></td>
<td>
<p>Dimension reduction via variable importance weighted forests. <code>FALSE</code>
means no dimension reduction; <code>TRUE</code> means reducing 75% variables before binary rank conversion and then fitting a weighted forest; a numeric value x% between 0 and 1 means reducing x% variables before binary rank conversion and then fitting a weighted forest.</p>
</td></tr>
<tr><td><code id="rboost_+3A_datrank">datrank</code></td>
<td>
<p> If using ranked raw data for fitting the dimension reduction model. </p>
</td></tr>
<tr><td><code id="rboost_+3A_distribution">distribution</code></td>
<td>
<p>Either a character string specifying the name of the
distribution to use: if the response has only 2 unique values,
<code>bernoulli</code> is assumed; otherwise, if the response is a factor, <code>multinomial</code> is
assumed.</p>
</td></tr>
<tr><td><code id="rboost_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process. It must be positive but does not need to be normalized.</p>
</td></tr>
<tr><td><code id="rboost_+3A_ntree">ntree</code></td>
<td>
<p>Integer specifying the total number of trees to fit. This is
equivalent to the number of iterations and the number of basis functions in
the additive expansion, which matches <code>n.tree</code> in the <code>gbm</code> package.</p>
</td></tr>
<tr><td><code id="rboost_+3A_nodedepth">nodedepth</code></td>
<td>
<p>Integer specifying the maximum depth of each tree. A value of 1
implies an additive model. This matches <code>interaction.depth</code> in the <code>gbm</code> package.</p>
</td></tr>
<tr><td><code id="rboost_+3A_nodesize">nodesize</code></td>
<td>
<p>Integer specifying the minimum number of observations
in the terminal nodes of the trees, which matches <code>n.minobsinnode</code> in the <code>gbm</code> package.. Note that this is the actual number of
observations, not the total weight.</p>
</td></tr>
<tr><td><code id="rboost_+3A_shrinkage">shrinkage</code></td>
<td>
<p>a shrinkage parameter applied to each tree in the
expansion. Also known as the learning rate or step-size reduction; 0.001 to
0.1 usually work, but a smaller learning rate typically requires more trees.
Default is 0.05.</p>
</td></tr>
<tr><td><code id="rboost_+3A_bag.fraction">bag.fraction</code></td>
<td>
<p>the fraction of the training set observations randomly
selected to propose the next tree in the expansion. This introduces
randomnesses into the model fit. If <code>bag.fraction</code> &lt; 1 then running the
same model twice will result in similar but different fits. <code>gbm</code> uses
the R random number generator so <code>set.seed</code> can ensure that the model
can be reconstructed. Preferably, the user can save the returned
<code>gbm.object</code> using <code><a href="base.html#topic+save">save</a></code>. Default is 0.5.</p>
</td></tr>
<tr><td><code id="rboost_+3A_train.fraction">train.fraction</code></td>
<td>
<p>The first <code>train.fraction * nrows(data)</code>
observations are used to fit the <code>gbm</code> and the remaining observations are used for
computing out-of-sample estimates of the loss function.</p>
</td></tr>
<tr><td><code id="rboost_+3A_cv.folds">cv.folds</code></td>
<td>
<p>Number of cross-validation folds to perform. If
<code>cv.folds</code>&gt;1 then <code>gbm</code>, in addition to the usual fit, will
perform cross-validation and calculate an estimate of generalization error
returned in <code>cv.error</code>.</p>
</td></tr>
<tr><td><code id="rboost_+3A_keep.data">keep.data</code></td>
<td>
<p>a logical variable indicating whether to keep the data and
an index of the data stored with the object. Keeping the data and index
makes subsequent calls to <code>gbm.more</code> faster at the cost of
storing an extra copy of the dataset.</p>
</td></tr>
<tr><td><code id="rboost_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating whether or not to print out progress and
performance indicators (<code>TRUE</code>). If this option is left unspecified for
<code>gbm.more</code>, then it uses <code>verbose</code> from <code>object</code>. Default is
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="rboost_+3A_class.stratify.cv">class.stratify.cv</code></td>
<td>
<p>Logical indicating whether or not the
cross-validation should be stratified by class. The purpose of stratifying the
cross-validation is to help avoid situations in which training sets do
not contain all classes.</p>
</td></tr>
<tr><td><code id="rboost_+3A_n.cores">n.cores</code></td>
<td>
<p>The number of CPU cores to use. The cross-validation loop
will attempt to send different CV folds off to different cores. If
<code>n.cores</code> is not specified by the user, it is guessed using the
<code>detectCores</code> function in the <code>parallel</code> package. Note that the
documentation for <code>detectCores</code> makes clear that it is not failsafe and
could return a spurious number of available cores.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>fit</code></td>
<td>
<p>A vector containing the fitted
values on the scale of regression function (e.g. log-odds scale for
bernoulli).</p>
</td></tr>
<tr><td><code>train.error</code></td>
<td>
<p>A vector of length
equal to the number of fitted trees containing the value of the loss
function for each boosting iteration evaluated on the training data.</p>
</td></tr>
<tr><td><code>valid.error</code></td>
<td>
<p>A vector of length equal to the number of fitted trees
containing the value of the loss function for each boosting iteration
evaluated on the validation data.</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>If <code>cv.folds</code> &lt; 2 this
component is <code>NULL</code>. Otherwise, this component is a vector of length equal to
the number of fitted trees containing a cross-validated estimate of the loss
function for each boosting iteration.</p>
</td></tr>
<tr><td><code>oobag.improve</code></td>
<td>
<p>A vector of
length equal to the number of fitted trees containing an out-of-bag estimate
of the marginal reduction in the expected value of the loss function. The
out-of-bag estimate uses only the training data and is useful for estimating
the optimal number of boosting iterations. See <code>gbm.perf</code>.</p>
</td></tr>
<tr><td><code>cv.fitted</code></td>
<td>
<p>If cross-validation was performed, the cross-validation
predicted values on the scale of the linear predictor. That is, the fitted
values from the i-th CV-fold, for the model having been trained on the data
in all other folds.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ruijie Yin (Maintainer,&lt;ruijieyin428@gmail.com&gt;), Chen Ye and Min Lu
</p>


<h3>References</h3>

<p>Lu M. Yin R. and Chen X.S. (2023). Ensemble Methods of Rank-Based Trees for Single Sample Classification with Gene Expression Profiles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(tnbc)
obj &lt;- rboost(subtype~., data = tnbc[,c(1:10,337)])
obj
</code></pre>

<hr>
<h2 id='rforest'>
Random Forest via Rank-Based Trees for Single Sample Classification with Gene Expression Profiles
</h2><span id='topic+rforest'></span><span id='topic+rforest.tree'></span>

<h3>Description</h3>

<p>The function implements the ensembled rank-based trees in random forests on both binary and multi-class problems. It converts continuous gene expression profiles into
ranked gene pairs, for which the variable importance indices are computed and adopted for dimension reduction.   The random forest implementation was directly imported from the <span class="pkg">randomForestSRC</span>
package.  Use the command <code>package?randomForestSRC</code> for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rforest(formula, data,
  dimreduce = TRUE,
  datrank = TRUE,
  ntree = 500, mtry = NULL,
  nodesize = NULL, nodedepth = NULL,
  splitrule = NULL, nsplit = NULL,
  importance = c(FALSE, TRUE, "none", "anti", "permute", "random"),
  bootstrap = c("by.root", "none"),
  membership = FALSE,
  na.action = c("na.omit", "na.impute"), nimpute = 1,
  perf.type = NULL,
  xvar.wt = NULL, yvar.wt = NULL, split.wt = NULL, case.wt  = NULL,
  forest = TRUE,
  var.used = c(FALSE, "all.trees", "by.tree"),
  split.depth = c(FALSE, "all.trees", "by.tree"),
  seed = NULL,
  statistics = FALSE,
  ...)

## convenient interface for growing a rank-based tree
rforest.tree(formula, data, dimreduce = FALSE,
             ntree = 1, mtry = ncol(data),
             bootstrap = "none", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rforest_+3A_formula">formula</code></td>
<td>
<p>Object of class 'formula' describing the model to fit.
Interaction terms are not supported. </p>
</td></tr>
<tr><td><code id="rforest_+3A_data">data</code></td>
<td>
<p>Data frame containing the y-outcome and x-variables.</p>
</td></tr>
<tr><td><code id="rforest_+3A_dimreduce">dimreduce</code></td>
<td>
<p>Dimension reduction via variable importance weighted forests. <code>FALSE</code>
means no dimension reduction; <code>TRUE</code> means reducing 75% variables before binary rank conversion and then fitting a weighted forest; a numeric value x% between 0 and 1 means reducing x% variables before binary rank conversion and then fitting a weighted forest.</p>
</td></tr>
<tr><td><code id="rforest_+3A_datrank">datrank</code></td>
<td>
<p> If using ranked raw data for fitting the dimension reduction model. </p>
</td></tr>
<tr><td><code id="rforest_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees.</p>
</td></tr>
<tr><td><code id="rforest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables to possibly split at each node.
Default is number of variables divided by 3 for regression.  For all
other families (including unsupervised settings), the square root
of number of variables.  Values are rounded up.</p>
</td></tr>
<tr><td><code id="rforest_+3A_nodesize">nodesize</code></td>
<td>
<p>Minumum size of terminal node.  The defaults are:
survival (15), competing risk (15), regression (5), classification
(1), mixed outcomes (3), unsupervised (3).  It is recommended to
experiment with different <code>nodesize</code> values.</p>
</td></tr>
<tr><td><code id="rforest_+3A_nodedepth">nodedepth</code></td>
<td>
<p>Maximum depth to which a tree should be grown.
Parameter is ignored by default.</p>
</td></tr>
<tr><td><code id="rforest_+3A_splitrule">splitrule</code></td>
<td>
<p>Splitting rule (see below).</p>
</td></tr>
<tr><td><code id="rforest_+3A_nsplit">nsplit</code></td>
<td>
<p>Non-negative integer specifying number of random splits
for splitting a variable.  When zero, all split values are
used (deterministic splitting), which can be slower.  By default
10 is used.</p>
</td></tr>
<tr><td><code id="rforest_+3A_importance">importance</code></td>
<td>
<p>Method for computing variable importance (VIMP); see
below.  Default action is <code>importance="none"</code> but VIMP can
be recovered later using <code>vimp</code> or <code>predict</code>.</p>
</td></tr>
<tr><td><code id="rforest_+3A_bootstrap">bootstrap</code></td>
<td>
<p>Bootstrap protocol.  Default is <code>by.root</code> which
bootstraps the data by sampling without replacement.
If <code>none</code>, the data is not bootstrapped (it is not possible to
return OOB ensembles or prediction error in this case).</p>
</td></tr>
<tr><td><code id="rforest_+3A_membership">membership</code></td>
<td>
<p>Should terminal node membership and inbag
information be returned?</p>
</td></tr>
<tr><td><code id="rforest_+3A_na.action">na.action</code></td>
<td>
<p>Action taken if the data contains <code>NA</code>'s.
Possible values are <code>na.omit</code> or <code>na.impute</code>.  The default
<code>na.omit</code> removes the entire record if any entry is
<code>NA</code>. Selecting <code>na.impute</code> imputes the data (see below
for details).  Also see the function <code>impute</code> for fast
imputation.</p>
</td></tr>
<tr><td><code id="rforest_+3A_nimpute">nimpute</code></td>
<td>
<p>Number of iterations of the missing data algorithm.
Performance measures such as out-of-bag (OOB) error rates are
optimistic if <code>nimpute</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="rforest_+3A_perf.type">perf.type</code></td>
<td>
<p>Optional character value specifying metric used
for predicted value, variable importance (VIMP), and error rate.
Reverts to the family default metric if not specified.
Values allowed for
univariate/multivariate classification are:
<code>perf.type="misclass"</code> (default), <code>perf.type="brier"</code> and
<code>perf.type="gmean"</code>.</p>
</td></tr>
<tr><td><code id="rforest_+3A_xvar.wt">xvar.wt</code></td>
<td>
<p>Vector of non-negative weights (does not have to sum
to 1) representing the probability of selecting a variable for
splitting.  Default is uniform weights.</p>
</td></tr>
<tr><td><code id="rforest_+3A_yvar.wt">yvar.wt</code></td>
<td>
<p>Used for sending in features with custom splitting.
For expert use only.</p>
</td></tr>
<tr><td><code id="rforest_+3A_split.wt">split.wt</code></td>
<td>
<p>Vector of non-negative weights used for multiplying
the split statistic for a variable. A large value encourages the
node to split on a specific variable. Default is uniform
weights.</p>
</td></tr>
<tr><td><code id="rforest_+3A_case.wt">case.wt</code></td>
<td>
<p>Vector of non-negative weights (does not have to sum to
1) for sampling cases.  Observations with larger weights will be
selected with higher probability in the bootstrap (or subsampled)
samples.  It is generally better to use real weights rather than
integers. See the breast data example below illustrating its use
for class imbalanced data.</p>
</td></tr>
<tr><td><code id="rforest_+3A_forest">forest</code></td>
<td>
<p>Save key forest values?  Used for prediction on new data
and required by many of the package functions. Turn this off if you
are only interested in training a forest.</p>
</td></tr>
<tr><td><code id="rforest_+3A_var.used">var.used</code></td>
<td>
<p>Return statistics on number of times a variable split?
Default is <code>FALSE</code>.  Possible values are <code>all.trees</code> which
returns total number of splits of each variable, and <code>by.tree</code>
which returns a matrix of number a splits for each variable for each
tree.</p>
</td></tr>
<tr><td><code id="rforest_+3A_split.depth">split.depth</code></td>
<td>
<p>Records the minimal depth for each variable.
Default is <code>FALSE</code>.  Possible values are <code>all.trees</code> which
returns a matrix of the average minimal depth for a variable
(columns) for a specific case (rows), and <code>by.tree</code> which
returns a three-dimensional array recording minimal depth for a
specific case (first dimension) for a variable (second dimension)
for a specific tree (third dimension).</p>
</td></tr>
<tr><td><code id="rforest_+3A_seed">seed</code></td>
<td>
<p>Negative integer specifying seed for the random number
generator.</p>
</td></tr>
<tr><td><code id="rforest_+3A_statistics">statistics</code></td>
<td>
<p>Should split statistics be returned?  Values can be
parsed using <code>stat.split</code>.</p>
</td></tr>
<tr><td><code id="rforest_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>Splitting</em>
</p>

<ol>
<li><p> Splitting rules are specified by the option <code>splitrule</code>.
</p>
</li>
<li><p> For all families, pure random splitting can be invoked by setting
<code>splitrule="random"</code>.
</p>
</li>
<li><p> For all families, computational speed can be increased using
randomized splitting invoked by the option <code>nsplit</code>.
See Improving Computational Speed.
</p>
</li></ol>

<p><em>Available splitting rules</em>
</p>

<ol>
<li> <p><code>splitrule="gini"</code> (default splitrule): Gini
index splitting (Breiman et al. 1984, Chapter 4.3).
</p>
</li>
<li> <p><code>splitrule="auc"</code>: AUC (area under the ROC curve) splitting
for both two-class and multiclass setttings.  AUC splitting is
appropriate for imbalanced data.  See <code>imbalanced</code> for
more information.
</p>
</li>
<li> <p><code>splitrule="entropy"</code>: entropy splitting (Breiman et
al. 1984, Chapter 2.5, 4.3).
</p>
</li></ol>



<h3>Value</h3>

<p>An object of class <code>(rfsrc, grow)</code> with the following
components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>The original call to <code>rfsrc</code> for growing the random forest object.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>The family used in the analysis.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Sample size of the data (depends upon <code>NA</code>'s, see <code>na.action</code>).</p>
</td></tr>
<tr><td><code>ntree</code></td>
<td>
<p>Number of trees grown.</p>
</td></tr>
<tr><td><code>mtry</code></td>
<td>
<p>Number of variables randomly selected for splitting at each node.</p>
</td></tr>
<tr><td><code>nodesize</code></td>
<td>
<p>Minimum size of terminal nodes.</p>
</td></tr>
<tr><td><code>nodedepth</code></td>
<td>
<p>Maximum depth allowed for a tree.</p>
</td></tr>
<tr><td><code>splitrule</code></td>
<td>
<p>Splitting rule used.</p>
</td></tr>
<tr><td><code>nsplit</code></td>
<td>
<p>Number of randomly selected split points.</p>
</td></tr>
<tr><td><code>yvar</code></td>
<td>
<p>y-outcome values.</p>
</td></tr>
<tr><td><code>yvar.names</code></td>
<td>
<p>A character vector of the y-outcome names.</p>
</td></tr>
<tr><td><code>xvar</code></td>
<td>
<p>Data frame of x-variables.</p>
</td></tr>
<tr><td><code>xvar.names</code></td>
<td>
<p>A character vector of the x-variable names.</p>
</td></tr>
<tr><td><code>xvar.wt</code></td>
<td>
<p>Vector of non-negative weights for dimension reduction
which specify the probability used to select a variable for splitting a node.</p>
</td></tr>
<tr><td><code>split.wt</code></td>
<td>
<p>Vector of non-negative weights specifying
multiplier by which the split statistic for a covariate is adjusted.</p>
</td></tr>
<tr><td><code>cause.wt</code></td>
<td>
<p>Vector of weights used for the composite competing
risk splitting rule.</p>
</td></tr>
<tr><td><code>leaf.count</code></td>
<td>
<p>Number of terminal nodes for each tree in the
forest. Vector of length <code>ntree</code>.  A value of zero indicates
a rejected tree (can occur when imputing missing data).
Values of one indicate tree stumps.</p>
</td></tr>
<tr><td><code>proximity</code></td>
<td>
<p>Proximity matrix recording the frequency of pairs of data points
occur within the same terminal node.</p>
</td></tr>
<tr><td><code>forest</code></td>
<td>
<p>If <code>forest=TRUE</code>, the forest object is returned.
This object is used for prediction with new test data
sets and is required for other R-wrappers.</p>
</td></tr>
<tr><td><code>membership</code></td>
<td>
<p>Matrix recording terminal node membership where
each column records node mebership for a case for a tree (rows).</p>
</td></tr>
<tr><td><code>splitrule</code></td>
<td>
<p>Splitting rule used.</p>
</td></tr>
<tr><td><code>inbag</code></td>
<td>
<p>Matrix recording inbag membership where each column
contains the number of times that a case appears in the bootstrap
sample for a tree (rows).</p>
</td></tr>
<tr><td><code>var.used</code></td>
<td>
<p>Count of the number of times a variable is used in
growing the forest.</p>
</td></tr>
<tr><td><code>imputed.indv</code></td>
<td>
<p>Vector of indices for cases with missing
values.</p>
</td></tr>
<tr><td><code>imputed.data</code></td>
<td>
<p>Data frame of the imputed data. The first
column(s) are reserved for the y-outcomes, after which the
x-variables are listed.</p>
</td></tr>
<tr><td><code>split.depth</code></td>
<td>
<p>Matrix (i,j) or array (i,j,k) recording the
minimal depth for variable j for case i, either averaged over
the forest, or by tree k.</p>
</td></tr>
<tr><td><code>node.stats</code></td>
<td>
<p>Split statistics returned when
<code>statistics=TRUE</code> which can be parsed using <code>stat.split</code>.</p>
</td></tr>
<tr><td><code>err.rate</code></td>
<td>
<p>Tree cumulative OOB error rate.</p>
</td></tr>
<tr><td><code>importance</code></td>
<td>
<p>Variable importance (VIMP) for each x-variable.</p>
</td></tr>
<tr><td><code>predicted</code></td>
<td>
<p>In-bag predicted value.</p>
</td></tr>
<tr><td><code>predicted.oob</code></td>
<td>
<p>OOB predicted value.</p>
</td></tr></table>
<p><br />
</p>
<table>
<tr><td><code>class</code></td>
<td>
<p>In-bag predicted class labels.</p>
</td></tr>
<tr><td><code>class.oob</code></td>
<td>
<p>OOB predicted class labels.</p>
</td></tr></table>
<p><br />
</p>


<h3>Author(s)</h3>

<p>Ruijie Yin (Maintainer,&lt;ruijieyin428@gmail.com&gt;), Chen Ye and Min Lu
</p>


<h3>References</h3>

<p>Lu M. Yin R. and Chen X.S. (2023). Ensemble Methods of Rank-Based Trees for Single Sample Classification with Gene Expression Profiles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(tnbc)
########### performance of Random Rank Forest
obj &lt;- rforest(subtype~., data = tnbc[,c(1:10,337)])
obj

</code></pre>

<hr>
<h2 id='select.rules'>
Select Decision Rules to Achieve Higher Prediction Accuracy
</h2><span id='topic+select.rules'></span>

<h3>Description</h3>

<p>Select rules from a extrat.rules <code>(rules)</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select.rules(object, data, data.pair = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select.rules_+3A_object">object</code></td>
<td>

<p>An extracted rule <code>(rules)</code> object generated from the <code>extract.rules</code> function.
</p>
</td></tr>
<tr><td><code id="select.rules_+3A_data">data</code></td>
<td>

<p>A validation dataset for selecting rules.
</p>
</td></tr>
<tr><td><code id="select.rules_+3A_data.pair">data.pair</code></td>
<td>

<p>Is data already converted into binary ranked pairs from the <code>pair</code> function?
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>rule</code></td>
<td>
<p>Interpretable selected rules. Note that the performance score displayed is inaccurate based on few samples from the original argument <code>object</code>. </p>
</td></tr>
<tr><td><code>rule.raw</code></td>
<td>
<p>Rules directly extracted from trees for prediction purpose</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>Data used to grow trees from the argument <code>(object)</code>. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ruijie Yin (Maintainer,&lt;ruijieyin428@gmail.com&gt;), Chen Ye and Min Lu
</p>


<h3>References</h3>

<p>Lu M. Yin R. and Chen X.S. (2023). Ensemble Methods of Rank-Based Trees for Single Sample Classification with Gene Expression Profiles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(tnbc)
obj &lt;- rforest(subtype~., data = tnbc[1:100,c(1:5,337)])
objr &lt;- extract.rules(obj)
predict(objr, tnbc[101:110,1:5])$label
objrs &lt;- select.rules(objr,tnbc[110:130,c(1:5,337)])
predict(objrs, tnbc[101:110,1:5])$label

</code></pre>

<hr>
<h2 id='tnbc'>Gene expression profiles in triple-negative breast cancer cell</h2><span id='topic+tnbc'></span>

<h3>Description</h3>

<p>Gene expression profiles in triple-negative breast cancer cells with 215 observations and 337 variables. Gene expression values were randomly chosen from the original dataset. The outcome variable is <em>subtype</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(tnbc)
</code></pre>


<h3>Source</h3>

<p>Chen, X., Li, J., Gray, W. H., Lehmann, B. D., Bauer, J. A., Shyr, Y., &amp; Pietenpol, J. A. (2012). TNBCtype: a subtyping tool for triple-negative breast cancer. <em>Cancer informatics</em>, 11, CIN-S9983.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(tnbc)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
