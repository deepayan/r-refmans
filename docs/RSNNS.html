<!DOCTYPE html><html><head><title>Help for package RSNNS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RSNNS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#RSNNS-package'><p>Getting started with the RSNNS package</p></a></li>
<li><a href='#analyzeClassification'><p>Converts continuous outputs to class labels</p></a></li>
<li><a href='#art1'><p>Create and train an art1 network</p></a></li>
<li><a href='#art2'><p>Create and train an art2 network</p></a></li>
<li><a href='#artmap'><p>Create and train an artmap network</p></a></li>
<li><a href='#assoz'><p>Create and train an (auto-)associative memory</p></a></li>
<li><a href='#confusionMatrix'><p>Computes a confusion matrix</p></a></li>
<li><a href='#decodeClassLabels'><p>Decode class labels to a binary matrix</p></a></li>
<li><a href='#denormalizeData'><p>Revert data normalization</p></a></li>
<li><a href='#dlvq'><p>Create and train a dlvq network</p></a></li>
<li><a href='#elman'><p>Create and train an Elman network</p></a></li>
<li><a href='#encodeClassLabels'><p>Encode a matrix of (decoded) class labels</p></a></li>
<li><a href='#exportToSnnsNetFile'><p>Export the net to a file in the original SNNS file format</p></a></li>
<li><a href='#extractNetInfo'><p>Extract information from a network</p></a></li>
<li><a href='#getNormParameters'><p>Get normalization parameters of the input data</p></a></li>
<li><a href='#getSnnsRDefine'><p>Get a define of the SNNS kernel</p></a></li>
<li><a href='#getSnnsRFunctionTable'><p>Get SnnsR function table</p></a></li>
<li><a href='#inputColumns'><p>Get the columns that are inputs</p></a></li>
<li><a href='#jordan'><p>Create and train a Jordan network</p></a></li>
<li><a href='#matrixToActMapList'><p>Convert matrix of activations to activation map list</p></a></li>
<li><a href='#mlp'><p>Create and train a multi-layer perceptron (MLP)</p></a></li>
<li><a href='#normalizeData'><p>Data normalization</p></a></li>
<li><a href='#normTrainingAndTestSet'><p>Function to normalize training and test set</p></a></li>
<li><a href='#outputColumns'><p>Get the columns that are targets</p></a></li>
<li><a href='#plotActMap'><p>Plot activation map</p></a></li>
<li><a href='#plotIterativeError'><p>Plot iterative errors of an rsnns object</p></a></li>
<li><a href='#plotRegressionError'><p>Plot a regression error plot</p></a></li>
<li><a href='#plotROC'><p>Plot a ROC curve</p></a></li>
<li><a href='#predict.rsnns'><p>Generic predict function for rsnns object</p></a></li>
<li><a href='#print.rsnns'><p>Generic print function for rsnns objects</p></a></li>
<li><a href='#rbf'><p>Create and train a radial basis function (RBF) network</p></a></li>
<li><a href='#rbfDDA'><p>Create and train an RBF network with the DDA algorithm</p></a></li>
<li><a href='#readPatFile'><p>Load data from a pat file</p></a></li>
<li><a href='#readResFile'><p>Rudimentary parser for res files.</p></a></li>
<li><a href='#resolveSnnsRDefine'><p>Resolve a define of the SNNS kernel</p></a></li>
<li><a href='#rsnnsObjectFactory'><p>Object factory for generating rsnns objects</p></a></li>
<li><a href='#savePatFile'><p>Save data to a pat file</p></a></li>
<li><a href='#setSnnsRSeedValue'><p>DEPRECATED, Set the SnnsR seed value</p></a></li>
<li><a href='#snnsData'><p>Example data of the package</p></a></li>
<li><a href='#SnnsR-class'><p>The main class of the package</p></a></li>
<li><a href='#SnnsRObject+24createNet'><p>Create a layered network</p></a></li>
<li><a href='#SnnsRObject+24createPatSet'><p>Create a pattern set</p></a></li>
<li><a href='#SnnsRObject+24extractNetInfo'><p>Get characteristics of the network.</p></a></li>
<li><a href='#SnnsRObject+24extractPatterns'><p>Extract the current pattern set to a matrix</p></a></li>
<li><a href='#SnnsRObject+24getAllHiddenUnits'><p>Get all hidden units of the net</p></a></li>
<li><a href='#SnnsRObject+24getAllInputUnits'><p>Get all input units of the net</p></a></li>
<li><a href='#SnnsRObject+24getAllOutputUnits'><p>Get all output units of the net.</p></a></li>
<li><a href='#SnnsRObject+24getAllUnits'><p>Get all units present in the net.</p></a></li>
<li><a href='#SnnsRObject+24getAllUnitsTType'><p>Get all units in the net of a certain <code>ttype</code>.</p></a></li>
<li><a href='#SnnsRObject+24getCompleteWeightMatrix'><p>Get the complete weight matrix.</p></a></li>
<li><a href='#SnnsRObject+24getInfoHeader'><p>Get an info header of the network.</p></a></li>
<li><a href='#SnnsRObject+24getSiteDefinitions'><p>Get the sites definitions of the network.</p></a></li>
<li><a href='#SnnsRObject+24getTypeDefinitions'><p>Get the FType definitions of the network.</p></a></li>
<li><a href='#SnnsRObject+24getUnitDefinitions'><p>Get the unit definitions of the network.</p></a></li>
<li><a href='#SnnsRObject+24getUnitsByName'><p>Find all units whose name begins with a given prefix.</p></a></li>
<li><a href='#SnnsRObject+24getWeightMatrix'><p>Get the weight matrix between two sets of units</p></a></li>
<li><a href='#SnnsRObject+24initializeNet'><p>Initialize the network</p></a></li>
<li><a href='#SnnsRObject+24predictCurrPatSet'><p>Predict values with a trained net</p></a></li>
<li><a href='#SnnsRObject+24resetRSNNS'><p>Reset the SnnsR object.</p></a></li>
<li><a href='#SnnsRObject+24setTTypeUnitsActFunc'><p>Set the activation function for all units of a certain ttype.</p></a></li>
<li><a href='#SnnsRObject+24setUnitDefaults'><p>Set the unit defaults</p></a></li>
<li><a href='#SnnsRObject+24somPredictComponentMaps'><p>Calculate the som component maps</p></a></li>
<li><a href='#SnnsRObject+24somPredictCurrPatSetWinners'><p>Get most of the relevant results from a som</p></a></li>
<li><a href='#SnnsRObject+24somPredictCurrPatSetWinnersSpanTree'><p>Get the spanning tree of the SOM</p></a></li>
<li><a href='#SnnsRObject+24train'><p>Train a network and test it in every training iteration</p></a></li>
<li><a href='#SnnsRObject+24whereAreResults'><p>Get a list of output units of a net</p></a></li>
<li><a href='#SnnsRObjectFactory'><p>SnnsR object factory</p></a></li>
<li><a href='#SnnsRObjectMethodCaller'><p>Method caller for SnnsR objects</p></a></li>
<li><a href='#som'><p>Create and train a self-organizing map (SOM)</p></a></li>
<li><a href='#splitForTrainingAndTest'><p>Function to split data into training and test set</p></a></li>
<li><a href='#summary.rsnns'><p>Generic summary function for rsnns objects</p></a></li>
<li><a href='#toNumericClassLabels'><p>Convert a vector (of class labels) to a numeric vector</p></a></li>
<li><a href='#train'><p>Internal generic train function for rsnns objects</p></a></li>
<li><a href='#vectorToActMap'><p>Convert a vector to an activation map</p></a></li>
<li><a href='#weightMatrix'><p>Function to extract the weight matrix of an rsnns object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Maintainer:</td>
<td>Christoph Bergmeir &lt;c.bergmeir@decsai.ugr.es&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> | file LICENSE [expanded from: LGPL (&ge; 2) | file LICENSE]</td>
</tr>
<tr>
<td>Title:</td>
<td>Neural Networks using the Stuttgart Neural Network Simulator
(SNNS)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Copyright:</td>
<td>Original SNNS software Copyright (C) 1990-1995 SNNS Group,
IPVR, Univ. Stuttgart, FRG; 1996-1998 SNNS Group, WSI, Univ.
Tuebingen, FRG. R interface Copyright (C) DiCITS Lab, Sci2s
group, DECSAI, University of Granada.</td>
</tr>
<tr>
<td>Description:</td>
<td>The Stuttgart Neural Network Simulator (SNNS) is a library
    containing many standard implementations of neural networks. This
    package wraps the SNNS functionality to make it available from
    within R. Using the 'RSNNS' low-level interface, all of the
    algorithmic functionality and flexibility of SNNS can be accessed.
    Furthermore, the package contains a convenient high-level
    interface, so that the most common neural network topologies and
    learning algorithms integrate seamlessly into R.</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4-17</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/cbergmeir/RSNNS">https://github.com/cbergmeir/RSNNS</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/cbergmeir/RSNNS/issues">https://github.com/cbergmeir/RSNNS/issues</a></td>
</tr>
<tr>
<td>MailingList:</td>
<td>rsnns@googlegroups.com</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-30</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.0), methods, Rcpp (&ge; 0.8.5)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>scatterplot3d,NeuralNetTools</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-30 04:22:55 UTC; bergmeir</td>
</tr>
<tr>
<td>Author:</td>
<td>Christoph Bergmeir [aut, cre, cph],
  José M. Benítez [ths],
  Andreas Zell [ctb] (Part of original SNNS development team),
  Niels Mache [ctb] (Part of original SNNS development team),
  Günter Mamier [ctb] (Part of original SNNS development team),
  Michael Vogt [ctb] (Part of original SNNS development team),
  Sven Döring [ctb] (Part of original SNNS development team),
  Ralf Hübner [ctb] (Part of original SNNS development team),
  Kai-Uwe Herrmann [ctb] (Part of original SNNS development team),
  Tobias Soyez [ctb] (Part of original SNNS development team),
  Michael Schmalzl [ctb] (Part of original SNNS development team),
  Tilman Sommer [ctb] (Part of original SNNS development team),
  Artemis Hatzigeorgiou [ctb] (Part of original SNNS development team),
  Dietmar Posselt [ctb] (Part of original SNNS development team),
  Tobias Schreiner [ctb] (Part of original SNNS development team),
  Bernward Kett [ctb] (Part of original SNNS development team),
  Martin Reczko [ctb] (Part of original SNNS external contributors),
  Martin Riedmiller [ctb] (Part of original SNNS external contributors),
  Mark Seemann [ctb] (Part of original SNNS external contributors),
  Marcus Ritt [ctb] (Part of original SNNS external contributors),
  Jamie DeCoster [ctb] (Part of original SNNS external contributors),
  Jochen Biedermann [ctb] (Part of original SNNS external contributors),
  Joachim Danz [ctb] (Part of original SNNS development team),
  Christian Wehrfritz [ctb] (Part of original SNNS development team),
  Patrick Kursawe [ctb] (Contributors to SNNS Version 4.3),
  Andre El-Ama [ctb] (Contributors to SNNS Version 4.3)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-30 05:50:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='RSNNS-package'>Getting started with the RSNNS package</h2><span id='topic+RSNNS-package'></span><span id='topic+RSNNS'></span>

<h3>Description</h3>

<p>The Stuttgart Neural Network Simulator (SNNS) is a library containing many 
standard implementations of neural networks. This package wraps the SNNS 
functionality to make it available from within R.
</p>


<h3>Details</h3>

<p>If you have problems using RSNNS, find a bug, or have suggestions, please
do not write to the general R lists or contact the authors of the original 
SNNS software. Instead, you should: File an issue on github (bugs/suggestions),
Ask your question on Stackoverflow under the tag RSNNS, or write to the mailing list
(rsnns@googlegroups.com). If all that fails, then you can also contact the maintainer 
directly by email. 
</p>
<p>If you use the package, please cite the following work in your publications:
</p>
<p>Bergmeir, C. and Benítez, J.M. (2012), Neural Networks in R Using the Stuttgart Neural Network Simulator: RSNNS. Journal of Statistical Software, 46(7), 1-26.
</p>
<p>The package has a hierarchical architecture with three levels:
</p>

<ul>
<li><p> RSNNS high-level api (rsnns)
</p>
</li>
<li><p> RSNNS low-level api (SnnsR)
</p>
</li>
<li><p> The api of our C++ port of SNNS (SnnsCLib)
</p>
</li></ul>

<p>Many demos for using both low-level and high-level api of the package are
available. To get a list of them, type:
</p>
<p><code>library(RSNNS)</code>
</p>
<p><code>demo()</code>
</p>
<p>It is a good idea to start with the demos of the high-level api (which is
much more convenient to use). E.g., to access the iris classification demo
type:
</p>
<p><code>demo(iris)</code>
</p>
<p>or for the laser regression demo type:
</p>
<p><code>demo(laser)</code> 
</p>
<p>As the high-level api is already quite powerful and flexible, you'll most
probably normally end up using one of the functions: <code><a href="#topic+mlp">mlp</a></code>,
<code><a href="#topic+dlvq">dlvq</a></code>, <code><a href="#topic+rbf">rbf</a></code>, <code><a href="#topic+rbfDDA">rbfDDA</a></code>,
<code><a href="#topic+elman">elman</a></code>,  <code><a href="#topic+jordan">jordan</a></code>, <code><a href="#topic+som">som</a></code>,
<code><a href="#topic+art1">art1</a></code>, <code><a href="#topic+art2">art2</a></code>, <code><a href="#topic+artmap">artmap</a></code>, or
<code><a href="#topic+assoz">assoz</a></code>, with some pre- and postprocessing. These S3 classes are
all subclasses of <code><a href="#topic+rsnns">rsnns</a></code>.
</p>
<p>You might also want to have a look at the original SNNS program and the SNNS
User Manual 4.2, especially pp 67-87 for explications on all the parameters
of the learning functions, and pp 145-215 for detailed (theoretical)
explications of the methods and advice on their use. And, there is also the 
javaNNS, the sucessor of SNNS from the original authors. It makes the C core 
functionality available from a Java GUI.
</p>
<p>Demos ending with &quot;SnnsR&quot; show the use of the low-level api. If you want to
do special things with neural networks that are currently not implemented in
the high-level api, you can see in this demos how to do it. Many demos are
present both as high-level and low-level versions.
</p>
<p>The low-level api consists mainly of the class <code><a href="#topic+SnnsR-class">SnnsR-class</a></code>,
which internally holds a pointer to a C++ object of the class
<code>SnnsCLib</code>, i.e., an instance of the SNNS kernel. The class furthermore
implements a calling mechanism for methods of the <code>SnnsCLib</code> object, so
that they can be called conveniently using the &quot;$&quot;-operator. This calling
mechanism also allows for transparent masking of methods or extending the
kernel with new methods from within R. See
<code><a href="#topic++24+2CSnnsR-method">$,SnnsR-method</a></code>. R-functions that are added by RSNNS to the
kernel are documented in this manual under topics beginning with
<code>SnnsRObject$</code>. Documentation of the original SNNS kernel user interface
functions can be found in the SNNS User Manual 4.2 pp 290-314.  A call to,
e.g., the SNNS kernel function <code>krui_getNoOfUnits(...)</code> can be done with
<code>SnnsRObject$getNoOfUnits(...)</code>. However, a few functions were excluded
from the wrapping for various reasons. Fur more details and other known
issues see the file /inst/doc/KnownIssues.
</p>
<p>Another nice tool is the <code>NeuralNetTools</code> package, that can be used to
visualize and analyse the networks generated with RSNNS.
</p>
<p>Most of the example data included in SNNS is also present in this package, see <code><a href="#topic+snnsData">snnsData</a></code>.
</p>
<p>A comprehensive report with many examples showing the usage of RSNNS, 
developed by Seymour Shlien, is available here:
</p>
<p><a href="https://ifdo.ca/~seymour/R/">https://ifdo.ca/~seymour/R/</a>
</p>


<h3>Author(s)</h3>

<p>Christoph Bergmeir <a href="mailto:c.bergmeir@decsai.ugr.es">c.bergmeir@decsai.ugr.es</a> 
</p>
<p>and José M. Benítez <a href="mailto:j.m.benitez@decsai.ugr.es">j.m.benitez@decsai.ugr.es</a>
</p>
<p>DiCITS Lab, Sci2s group, DECSAI, University of Granada.
</p>
<p><a href="http://dicits.ugr.es">http://dicits.ugr.es</a>, <a href="https://sci2s.ugr.es">https://sci2s.ugr.es</a>
</p>


<h3>References</h3>

<p>Bergmeir, C. and Benítez, J.M. (2012), 'Neural Networks in R Using the Stuttgart Neural Network Simulator: RSNNS', Journal of Statistical Software, 46(7), 1-26.
</p>
<p><em>General neural network literature:</em>
</p>
<p>Bishop, C. M. (2003), Neural networks for pattern recognition, University Press, Oxford.
</p>
<p>Haykin, S. S. (1999), Neural networks :a comprehensive foundation, Prentice Hall, Upper Saddle River, NJ.
</p>
<p>Kriesel, D. ( 2007 ), A Brief Introduction to Neural
Networks. http://www.dkriesel.com
</p>
<p>Ripley, B. D. (2007), Pattern recognition and neural networks, Cambridge
University Press, Cambridge.
</p>
<p>Rojas, R. (1996), Neural networks :a systematic introduction, Springer-Verlag, Berlin.
</p>
<p>Rumelhart, D. E.; Clelland, J. L. M. &amp; Group, P. R. (1986), Parallel
distributed processing :explorations in the microstructure of cognition, Mit,
Cambridge, MA etc..
</p>
<p><em>Literature on the original SNNS software:</em>
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen.
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>
<p>javaNNS, the sucessor of the original SNNS with a Java GUI: 
<a href="https://github.com/mwri/javanns">https://github.com/mwri/javanns</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley.
</p>
<p><em>Other resources:</em>
</p>
<p>A function to plot networks from the <code><a href="#topic+mlp">mlp</a></code> function:
<a href="https://beckmw.wordpress.com/2013/11/14/visualizing-neural-networks-in-r-update/">https://beckmw.wordpress.com/2013/11/14/visualizing-neural-networks-in-r-update/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mlp">mlp</a></code>, <code><a href="#topic+dlvq">dlvq</a></code>, <code><a href="#topic+rbf">rbf</a></code>, <code><a href="#topic+rbfDDA">rbfDDA</a></code>, <code><a href="#topic+elman">elman</a></code>, 
<code><a href="#topic+jordan">jordan</a></code>, <code><a href="#topic+som">som</a></code>, <code><a href="#topic+art1">art1</a></code>, <code><a href="#topic+art2">art2</a></code>, <code><a href="#topic+artmap">artmap</a></code>, <code><a href="#topic+assoz">assoz</a></code>
</p>

<hr>
<h2 id='analyzeClassification'>Converts continuous outputs to class labels</h2><span id='topic+analyzeClassification'></span>

<h3>Description</h3>

<p>This function converts the continuous outputs to binary outputs that can be
used for classification. The two methods 402040, and winner-takes-all (WTA),
are implemented as described in the SNNS User Manual 4.2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>analyzeClassification(y, method = "WTA", l = 0, h = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="analyzeClassification_+3A_y">y</code></td>
<td>
<p>inputs</p>
</td></tr>
<tr><td><code id="analyzeClassification_+3A_method">method</code></td>
<td>
<p>&quot;WTA&quot; or &quot;402040&quot;</p>
</td></tr>
<tr><td><code id="analyzeClassification_+3A_l">l</code></td>
<td>
<p>lower bound, e.g. in 402040: l=0.4</p>
</td></tr>
<tr><td><code id="analyzeClassification_+3A_h">h</code></td>
<td>
<p>upper bound, e.g. in 402040: h=0.6</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following text is an edited citation from the SNNS User Manual 4.2 (pp
269):
</p>

<dl>
<dt>402040</dt><dd> 
<p>A pattern is recognized as classified correctly, if (i) the output of exactly one output unit is &gt;= h 
(ii) the teaching output of this unit is the maximum teaching output (&gt; 0) of the pattern (iii) the output of all
other output units is &lt;= l.
</p>
<p>A pattern is recognized as classified incorrectly, if (i) and (iii) hold as above, but for (ii) holds that the teaching output is <em>not</em>
the maximum teaching output of the pattern or there is no teaching output &gt; 0.
</p>
<p>A pattern is recognized as unclassified in all other cases.
</p>
<p>The method derives its name from the commonly used default values l = 0.4, h = 0.6.
</p>
</dd>
<dt>WTA</dt><dd>
<p>A pattern is recognized as classified correctly, if (i) there is an output unit with the value greater than the output value of all other
output units (this output value is supposed to be a) (ii) a &gt; h (iii) the teaching output of this unit is the maximum teaching output 
of the pattern (&gt; 0) (iv) the output of all other units is &lt; a - l.
</p>
<p>A pattern is recognized as classified incorrectly, if (i), (ii), and (iv) hold as above, but for (iii) holds that the teaching output of this 
unit is <em>not</em> the maximum teaching output of the pattern or there is no teaching output &gt; 0.
</p>
<p>A pattern is recognized as unclassified in all other cases. 
</p>
<p>Commonly used default values for this method are: l = 0.0, h = 0.0.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>the position of the winning unit (i.e., the winning class), or zero, if no classification was done.
</p>


<h3>References</h3>

<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen.
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+encodeClassLabels">encodeClassLabels</a></code>
</p>

<hr>
<h2 id='art1'>Create and train an art1 network</h2><span id='topic+art1'></span><span id='topic+art1.default'></span>

<h3>Description</h3>

<p>Adaptive resonance theory (ART) networks perform clustering by finding prototypes. 
They are mainly designed to solve the stability/plasticity dilemma (which is one of the 
central problems in neural networks) in the following way: new input patterns 
may generate new prototypes (plasticity), but patterns already present in the net 
(represented by their prototypes) are only altered by similar new patterns, 
not by others (stability).
ART1 is for binary inputs only,
if you have real-valued input, use <code><a href="#topic+art2">art2</a></code> instead. 
</p>
<p>Learning in an ART network works as follows: 
A new input is intended to be classified according 
to the prototypes already present in the net. The similarity between the input and 
all prototypes is calculated. The most similar prototype is the <em>winner</em>. 
If the similarity between the input and the winner is high enough (defined by a
<em>vigilance parameter</em>), the winner is adapted to make it more similar to the input. 
If similarity is not high enough, a new prototype is created. So, at most the winner 
is adapted, all other prototypes remain unchanged.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>art1(x, ...)

## Default S3 method:
art1(
  x,
  dimX,
  dimY,
  f2Units = nrow(x),
  maxit = 100,
  initFunc = "ART1_Weights",
  initFuncParams = c(1, 1),
  learnFunc = "ART1",
  learnFuncParams = c(0.9, 0, 0),
  updateFunc = "ART1_Stable",
  updateFuncParams = c(0),
  shufflePatterns = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="art1_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr><td><code id="art1_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="art1_+3A_dimx">dimX</code></td>
<td>
<p>x dimension of inputs and outputs</p>
</td></tr>
<tr><td><code id="art1_+3A_dimy">dimY</code></td>
<td>
<p>y dimension of inputs and outputs</p>
</td></tr>
<tr><td><code id="art1_+3A_f2units">f2Units</code></td>
<td>
<p>controls the number of clusters assumed to be present</p>
</td></tr>
<tr><td><code id="art1_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="art1_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="art1_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="art1_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="art1_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="art1_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="art1_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="art1_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The architecture of an ART network is the following:
ART is based on the more general concept of <em>competitive learning</em>. The networks have 
two fully connected layers (in both directions), the input/comparison layer and the recognition layer. 
They propagate activation back and forth (resonance). The units in the recognition layer have lateral
inhibition, so that they show a winner-takes-all behaviour, i.e., the unit that has the highest activation
inhibits activation of other units, so that after a few cycles its activation will converge to one, whereas
the other units activations converge to zero. ART stabilizes this general learning mechanism by the presence
of some special units. For details refer to the referenced literature. 
</p>
<p>The default initialization function, <code>ART1_Weights</code>, is the only one suitable for ART1 networks. It has 
two parameters, which are explained in the SNNS User Manual pp.189. A default of 1.0 for both is usually fine.
The only learning function suitable for ART1 is <code>ART1</code>. Update functions are <code>ART1_Stable</code> and 
<code>ART1_Synchronous</code>. The difference between the two is that the first one updates until the network is in a 
stable state, and the latter one only performs one update step. Both the learning function and the update functions 
have one parameter, the vigilance parameter.
</p>
<p>In its current implementation, the network has two-dimensional input. The matrix <code>x</code> contains all 
(one dimensional) input patterns. Internally, every one of these patterns
is converted to a two-dimensional pattern using parameters <code>dimX</code> and <code>dimY</code>.
The parameter <code>f2Units</code> controls the number of units in the recognition layer, and therewith the maximal amount of clusters 
that are assumed to be present in the input patterns. 
</p>
<p>A detailed description of the theory and the parameters is available from the SNNS documentation and the other referenced literature.
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object. The <code>fitted.values</code> member of the object contains a 
list of two-dimensional activation patterns.
</p>


<h3>References</h3>

<p>Carpenter, G. A. &amp; Grossberg, S. (1987), 'A massively parallel architecture for a self-organizing neural pattern recognition machine', Comput. Vision Graph. Image Process. 37, 54&ndash;115.
</p>
<p>Grossberg, S. (1988), Adaptive pattern classification and universal recoding. I.: parallel development and coding of neural feature detectors, MIT Press, Cambridge, MA, USA, chapter I, pp. 243&ndash;258.
</p>
<p>Herrmann, K.-U. (1992), 'ART &ndash; Adaptive Resonance Theory &ndash; Architekturen, Implementierung und Anwendung', Master's thesis, IPVR, University of Stuttgart. (in German)
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen. 
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+art2">art2</a></code>, <code><a href="#topic+artmap">artmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(art1_letters)
## Not run: demo(art1_lettersSnnsR)


data(snnsData)
patterns &lt;- snnsData$art1_letters.pat

inputMaps &lt;- matrixToActMapList(patterns, nrow=7)
par(mfrow=c(3,3))
for (i in 1:9) plotActMap(inputMaps[[i]])

model &lt;- art1(patterns, dimX=7, dimY=5)
encodeClassLabels(model$fitted.values)
</code></pre>

<hr>
<h2 id='art2'>Create and train an art2 network</h2><span id='topic+art2'></span><span id='topic+art2.default'></span>

<h3>Description</h3>

<p>ART2 is very similar to ART1, but for real-valued input. See <code><a href="#topic+art1">art1</a></code>
for more information. Opposed to the ART1 implementation, the ART2 implementation 
does not assume two-dimensional input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>art2(x, ...)

## Default S3 method:
art2(
  x,
  f2Units = 5,
  maxit = 100,
  initFunc = "ART2_Weights",
  initFuncParams = c(0.9, 2),
  learnFunc = "ART2",
  learnFuncParams = c(0.98, 10, 10, 0.1, 0),
  updateFunc = "ART2_Stable",
  updateFuncParams = c(0.98, 10, 10, 0.1, 0),
  shufflePatterns = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="art2_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr><td><code id="art2_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="art2_+3A_f2units">f2Units</code></td>
<td>
<p>controls the number of clusters assumed to be present</p>
</td></tr>
<tr><td><code id="art2_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="art2_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="art2_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="art2_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="art2_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="art2_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="art2_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="art2_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As comparison of real-valued vectors is more difficult than comparison of binary vectors, 
the comparison layer is more complex in ART2, and actually consists of three layers. With a more
complex comparison layer, also other parts of the network enhance their complexity.
In SNNS, this enhanced complexity is reflected by the presence of more parameters in initialization-, learning-,
and update function.  
</p>
<p>In analogy to the implementation of ART1, there are one initialization function, one learning function and two 
update functions suitable for ART2.  The learning and update functions have five parameters, the initialization function has two 
parameters. For details see the SNNS User Manual, p. 67 and pp. 192.
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object. The <code>fitted.values</code> member contains the 
activation patterns for all inputs.
</p>


<h3>References</h3>

<p>Carpenter, G. A. &amp; Grossberg, S. (1987), 'ART 2: self-organization of stable category recognition codes for analog input patterns', Appl. Opt. 26(23), 4919&ndash;4930.
</p>
<p>Grossberg, S. (1988), Adaptive pattern classification and universal recoding. I.: parallel development and coding of neural feature detectors, MIT Press, Cambridge, MA, USA, chapter I, pp. 243&ndash;258.
</p>
<p>Herrmann, K.-U. (1992), 'ART &ndash; Adaptive Resonance Theory &ndash; Architekturen, Implementierung und Anwendung', Master's thesis, IPVR, University of Stuttgart. (in German)
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen. 
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+art1">art1</a></code>, <code><a href="#topic+artmap">artmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(art2_tetra)
## Not run: demo(art2_tetraSnnsR)


data(snnsData)
patterns &lt;- snnsData$art2_tetra_med.pat

model &lt;- art2(patterns, f2Units=5, learnFuncParams=c(0.99, 20, 20, 0.1, 0), 
                  updateFuncParams=c(0.99, 20, 20, 0.1, 0))
model

testPatterns &lt;- snnsData$art2_tetra_high.pat
predictions &lt;- predict(model, testPatterns)

## Not run: library(scatterplot3d)

## Not run: par(mfrow=c(2,2))
## Not run: scatterplot3d(patterns, pch=encodeClassLabels(model$fitted.values))
## Not run: scatterplot3d(testPatterns, pch=encodeClassLabels(predictions))
</code></pre>

<hr>
<h2 id='artmap'>Create and train an artmap network</h2><span id='topic+artmap'></span><span id='topic+artmap.default'></span>

<h3>Description</h3>

<p>An ARTMAP performs supervised learning. It consists of two coupled ART networks.
In theory, these could be ART1, ART2, or others. However, in SNNS ARTMAP is
implemented for ART1 only. So, this function is to be used with binary input. 
As explained in the description of <code><a href="#topic+art1">art1</a></code>, ART aims at solving the stability/plasticity
dilemma. So the advantage of ARTMAP is that it is a supervised learning mechanism
that guarantees stability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>artmap(x, ...)

## Default S3 method:
artmap(
  x,
  nInputsTrain,
  nInputsTargets,
  nUnitsRecLayerTrain,
  nUnitsRecLayerTargets,
  maxit = 1,
  nRowInputsTrain = 1,
  nRowInputsTargets = 1,
  nRowUnitsRecLayerTrain = 1,
  nRowUnitsRecLayerTargets = 1,
  initFunc = "ARTMAP_Weights",
  initFuncParams = c(1, 1, 1, 1, 0),
  learnFunc = "ARTMAP",
  learnFuncParams = c(0.8, 1, 1, 0, 0),
  updateFunc = "ARTMAP_Stable",
  updateFuncParams = c(0.8, 1, 1, 0, 0),
  shufflePatterns = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="artmap_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs and targets for the network</p>
</td></tr>
<tr><td><code id="artmap_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="artmap_+3A_ninputstrain">nInputsTrain</code></td>
<td>
<p>the number of columns of the matrix that are training input</p>
</td></tr>
<tr><td><code id="artmap_+3A_ninputstargets">nInputsTargets</code></td>
<td>
<p>the number of columns that are target values</p>
</td></tr>
<tr><td><code id="artmap_+3A_nunitsreclayertrain">nUnitsRecLayerTrain</code></td>
<td>
<p>number of units in the recognition layer of the training data ART network</p>
</td></tr>
<tr><td><code id="artmap_+3A_nunitsreclayertargets">nUnitsRecLayerTargets</code></td>
<td>
<p>number of units in the recognition layer of the target data ART network</p>
</td></tr>
<tr><td><code id="artmap_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to perform</p>
</td></tr>
<tr><td><code id="artmap_+3A_nrowinputstrain">nRowInputsTrain</code></td>
<td>
<p>number of rows the training input units are to be organized in (only for visualization purposes of the net in the original SNNS software)</p>
</td></tr>
<tr><td><code id="artmap_+3A_nrowinputstargets">nRowInputsTargets</code></td>
<td>
<p>same, but for the target value input units</p>
</td></tr>
<tr><td><code id="artmap_+3A_nrowunitsreclayertrain">nRowUnitsRecLayerTrain</code></td>
<td>
<p>same, but for the recognition layer of the training data ART network</p>
</td></tr>
<tr><td><code id="artmap_+3A_nrowunitsreclayertargets">nRowUnitsRecLayerTargets</code></td>
<td>
<p>same, but for the recognition layer of the target data ART network</p>
</td></tr>
<tr><td><code id="artmap_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="artmap_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="artmap_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="artmap_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="artmap_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="artmap_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="artmap_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See also the details section of <code><a href="#topic+art1">art1</a></code>. The two ART1 networks are connected by a <em>map field</em>.
The input of the first ART1 network is the training input, the input of the second network are the target values, 
the teacher signals. The two networks are often called ARTa and ARTb, we call them here training data network 
and target data network.  
</p>
<p>In analogy to the ART1 and ART2 implementations, there are one initialization function, one learning function, 
and two update functions present that are suitable for ARTMAP. The parameters are basically as in ART1, but for 
two networks. The learning function and the update functions have 3 parameters, the vigilance parameters of the 
two ART1 networks and an additional vigilance parameter for inter ART reset control. The initialization function
has four parameters, two for every ART1 network.
</p>
<p>A detailed description of the theory and the parameters is available from 
the SNNS documentation and the other referenced literature.
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object. The <code>fitted.values</code> member of the object contains a 
list of two-dimensional activation patterns.
</p>


<h3>References</h3>

<p>Carpenter, G. A.; Grossberg, S. &amp; Reynolds, J. H. (1991), 'ARTMAP: Supervised real-time learning and classification of nonstationary data by a self-organizing neural network', Neural Networks 4(5), 565&ndash;588.
</p>
<p>Grossberg, S. (1988), Adaptive pattern classification and universal recoding. I.: parallel development and coding of neural feature detectors, MIT Press, Cambridge, MA, USA, chapter I, pp. 243&ndash;258.
</p>
<p>Herrmann, K.-U. (1992), 'ART &ndash; Adaptive Resonance Theory &ndash; Architekturen, Implementierung und Anwendung', Master's thesis, IPVR, University of Stuttgart. (in German)
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen. 
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+art1">art1</a></code>, <code><a href="#topic+art2">art2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(artmap_letters)
## Not run: demo(artmap_lettersSnnsR)


data(snnsData)
trainData &lt;- snnsData$artmap_train.pat
testData &lt;- snnsData$artmap_test.pat

model &lt;- artmap(trainData, nInputsTrain=70, nInputsTargets=5, 
                  nUnitsRecLayerTrain=50, nUnitsRecLayerTargets=26)
model$fitted.values

predict(model, testData)
</code></pre>

<hr>
<h2 id='assoz'>Create and train an (auto-)associative memory</h2><span id='topic+assoz'></span><span id='topic+assoz.default'></span>

<h3>Description</h3>

<p>The autoassociative memory performs clustering by finding a prototype to the given input. 
The implementation assumes two-dimensional input and output (cf. <code><a href="#topic+art1">art1</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>assoz(x, ...)

## Default S3 method:
assoz(
  x,
  dimX,
  dimY,
  maxit = 100,
  initFunc = "RM_Random_Weights",
  initFuncParams = c(1, -1),
  learnFunc = "RM_delta",
  learnFuncParams = c(0.01, 100, 0, 0, 0),
  updateFunc = "Auto_Synchronous",
  updateFuncParams = c(50),
  shufflePatterns = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="assoz_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr><td><code id="assoz_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="assoz_+3A_dimx">dimX</code></td>
<td>
<p>x dimension of inputs and outputs</p>
</td></tr>
<tr><td><code id="assoz_+3A_dimy">dimY</code></td>
<td>
<p>y dimension of inputs and outputs</p>
</td></tr>
<tr><td><code id="assoz_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="assoz_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="assoz_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="assoz_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="assoz_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="assoz_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="assoz_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="assoz_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default initialization and update functions are the only ones suitable for this kind of 
network. The update function takes one parameter, which is the number of iterations that will 
be performed. The default of 50 usually does not have to be modified. For learning, <code>RM_delta</code> 
and <code>Hebbian</code> functions can be used, though the first one usually performs better.
</p>
<p>A more detailed description of the theory and the parameters is available from 
the SNNS documentation and the other referenced literature.
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object. The <code>fitted.values</code> member contains the 
activation patterns for all inputs.
</p>


<h3>References</h3>

<p>Palm, G. (1980), 'On associative memory', Biological Cybernetics 36, 19-31.
</p>
<p>Rojas, R. (1996), Neural networks :a systematic introduction, Springer-Verlag, Berlin.
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen. 
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+art1">art1</a></code>, <code><a href="#topic+art2">art2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(assoz_letters)
## Not run: demo(assoz_lettersSnnsR)


data(snnsData)
patterns &lt;- snnsData$art1_letters.pat

model &lt;- assoz(patterns, dimX=7, dimY=5)

actMaps &lt;- matrixToActMapList(model$fitted.values, nrow=7)

par(mfrow=c(3,3))
for (i in 1:9) plotActMap(actMaps[[i]])
</code></pre>

<hr>
<h2 id='confusionMatrix'>Computes a confusion matrix</h2><span id='topic+confusionMatrix'></span>

<h3>Description</h3>

<p>The confusion matrix shows how many times a pattern
with the real class x was classified as class y. A perfect method
should result in a diagonal matrix. All values not on the diagonal
are errors of the method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confusionMatrix(targets, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confusionMatrix_+3A_targets">targets</code></td>
<td>
<p>the known, correct target values</p>
</td></tr>
<tr><td><code id="confusionMatrix_+3A_predictions">predictions</code></td>
<td>
<p>the corresponding predictions of a method for the targets</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the class labels are not already encoded, they are encoded using <code><a href="#topic+encodeClassLabels">encodeClassLabels</a></code> 
(with default values).
</p>


<h3>Value</h3>

<p>the confusion matrix
</p>

<hr>
<h2 id='decodeClassLabels'>Decode class labels to a binary matrix</h2><span id='topic+decodeClassLabels'></span>

<h3>Description</h3>

<p>This method decodes class labels from a numerical or levels vector to a
binary matrix, i.e.,  it converts the input vector to a binary matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decodeClassLabels(x, valTrue = 1, valFalse = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decodeClassLabels_+3A_x">x</code></td>
<td>
<p>class label vector</p>
</td></tr>
<tr><td><code id="decodeClassLabels_+3A_valtrue">valTrue</code></td>
<td>
<p>see Details paragraph</p>
</td></tr>
<tr><td><code id="decodeClassLabels_+3A_valfalse">valFalse</code></td>
<td>
<p>see Details paragraph</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the matrix, the value <code>valTrue</code> (e.g. 1) is present exactly in the
column given by the value in the input vector, and the value <code>valFalse</code>
(e.g. 0) in the other columns. The number of columns of the resulting matrix
depends on the number of unique labels found in the vector. E.g. the input
c(1, 3, 2, 3) will result in an output matrix with rows: 100 001 010 001
</p>


<h3>Value</h3>

<p>a matrix containing the decoded class labels
</p>


<h3>Author(s)</h3>

<p>The implementation is a slightly modified version of the function
<code>class.ind</code> from the <code>nnet</code> package of Brian Ripley.
</p>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002), 'Modern Applied Statistics with S', Springer-Verlag.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>decodeClassLabels(c(1,3,2,3))
decodeClassLabels(c("r","b","b","r", "g", "g"))

data(iris)
decodeClassLabels(iris[,5])
</code></pre>

<hr>
<h2 id='denormalizeData'>Revert data normalization</h2><span id='topic+denormalizeData'></span>

<h3>Description</h3>

<p>Column-wise normalization of the input matrix is reverted, using the given parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>denormalizeData(x, normParams)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="denormalizeData_+3A_x">x</code></td>
<td>
<p>input data</p>
</td></tr>
<tr><td><code id="denormalizeData_+3A_normparams">normParams</code></td>
<td>
<p>the parameters generated by an earlier call to <code><a href="#topic+normalizeData">normalizeData</a></code> that will be used for reverting normalization</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The input matrix is column-wise denormalized using the parameters given by <code>normParams</code>. 
E.g., if <code>normParams</code> contains mean and sd for every column, the values are multiplied 
by sd and the mean is added
</p>


<h3>Value</h3>

<p>column-wise denormalized input
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normalizeData">normalizeData</a></code>, <code><a href="#topic+getNormParameters">getNormParameters</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
values &lt;- normalizeData(iris[,1:4])
denormalizeData(values, getNormParameters(values))
</code></pre>

<hr>
<h2 id='dlvq'>Create and train a dlvq network</h2><span id='topic+dlvq'></span><span id='topic+dlvq.default'></span>

<h3>Description</h3>

<p>Dynamic learning vector quantization (DLVQ) networks are similar to 
self-organizing maps (SOM, <code><a href="#topic+som">som</a></code>). But they perform supervised learning
and lack a neighborhood relationship between the prototypes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlvq(x, ...)

## Default S3 method:
dlvq(
  x,
  y,
  initFunc = "DLVQ_Weights",
  initFuncParams = c(1, -1),
  learnFunc = "Dynamic_LVQ",
  learnFuncParams = c(0.03, 0.03, 10),
  updateFunc = "Dynamic_LVQ",
  updateFuncParams = c(0),
  shufflePatterns = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dlvq_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr><td><code id="dlvq_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="dlvq_+3A_y">y</code></td>
<td>
<p>the corresponding target values</p>
</td></tr>
<tr><td><code id="dlvq_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="dlvq_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="dlvq_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="dlvq_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="dlvq_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="dlvq_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="dlvq_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The input data has to be normalized in order to use DLVQ.
</p>
<p>Learning in DLVQ: For each class, a mean vector (prototype) is calculated and stored 
in a (newly generated) hidden unit. Then, the net is used to classify every pattern 
by using the nearest prototype. If a pattern gets misclassified as class y instead of 
class x, the prototype of class y is moved away from the pattern, and the prototype 
of class x is moved towards the pattern. This procedure is repeated iteratively until no more changes 
in classification take place. Then, new prototypes are introduced in the net per class
as new hidden units, and initialized by the mean vector of misclassified patterns in that class.
</p>
<p>Network architecture: The network only has one hidden layer, containing one unit for each prototype.
The prototypes/hidden units are also called codebook vectors. Because SNNS generates the units 
automatically, and does not need their number to be specified in advance, the procedure is called
<em>dynamic</em> LVQ in SNNS.
</p>
<p>The default initialization, learning, and update functions are the only ones suitable for this kind of 
network. The three parameters of the learning function specify two learning rates (for the cases 
correctly/uncorrectly classified), and the number of cycles the net is trained before mean vectors are 
calculated.
</p>
<p>A detailed description of the theory and the parameters is available, as always, from the SNNS 
documentation and the other referenced literature.
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object. The <code>fitted.values</code> member contains the 
activation patterns for all inputs.
</p>


<h3>References</h3>

<p>Kohonen, T. (1988), Self-organization and associative memory, Vol. 8, Springer-Verlag.
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen. 
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(dlvq_ziff)
## Not run: demo(dlvq_ziffSnnsR)


data(snnsData)
dataset &lt;- snnsData$dlvq_ziff_100.pat

inputs &lt;- dataset[,inputColumns(dataset)]
outputs &lt;- dataset[,outputColumns(dataset)]

model &lt;- dlvq(inputs, outputs)

fitted(model) == outputs
mean(fitted(model) - outputs)
</code></pre>

<hr>
<h2 id='elman'>Create and train an Elman network</h2><span id='topic+elman'></span><span id='topic+elman.default'></span>

<h3>Description</h3>

<p>Elman networks are partially recurrent networks and similar to Jordan
networks (function <code><a href="#topic+jordan">jordan</a></code>). For details, see explanations
there.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elman(x, ...)

## Default S3 method:
elman(
  x,
  y,
  size = c(5),
  maxit = 100,
  initFunc = "JE_Weights",
  initFuncParams = c(1, -1, 0.3, 1, 0.5),
  learnFunc = "JE_BP",
  learnFuncParams = c(0.2),
  updateFunc = "JE_Order",
  updateFuncParams = c(0),
  shufflePatterns = FALSE,
  linOut = TRUE,
  outContext = FALSE,
  inputsTest = NULL,
  targetsTest = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="elman_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr><td><code id="elman_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="elman_+3A_y">y</code></td>
<td>
<p>the corresponding targets values</p>
</td></tr>
<tr><td><code id="elman_+3A_size">size</code></td>
<td>
<p>number of units in the hidden layer(s)</p>
</td></tr>
<tr><td><code id="elman_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="elman_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="elman_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="elman_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="elman_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="elman_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="elman_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="elman_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
<tr><td><code id="elman_+3A_linout">linOut</code></td>
<td>
<p>sets the activation function of the output units to linear or logistic</p>
</td></tr>
<tr><td><code id="elman_+3A_outcontext">outContext</code></td>
<td>
<p>if TRUE, the context units are also output units (untested)</p>
</td></tr>
<tr><td><code id="elman_+3A_inputstest">inputsTest</code></td>
<td>
<p>a matrix with inputs to test the network</p>
</td></tr>
<tr><td><code id="elman_+3A_targetstest">targetsTest</code></td>
<td>
<p>the corresponding targets for the test input</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Learning in Elman networks:
Same as in Jordan networks (see <code><a href="#topic+jordan">jordan</a></code>).
</p>
<p>Network architecture: The difference between Elman and Jordan networks is
that in an Elman network the context units get input not from the output
units, but from the hidden units. Furthermore, there is no direct feedback in
the context units. In an Elman net, the number of context units and hidden
units has to be the same. The main advantage of Elman nets is that the number
of context units is not directly determined by the output dimension (as in
Jordan nets), but by the number of hidden units, which is more flexible, as
it is easy to add/remove hidden units, but not output units.
</p>
<p>A detailed description of the theory and the parameters is available, as
always, from the SNNS documentation and the other referenced literature.
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object.
</p>


<h3>References</h3>

<p>Elman, J. L. (1990), 'Finding structure in time', Cognitive Science 14(2),
179&ndash;211.
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual,
Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen.
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+jordan">jordan</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(iris)
## Not run: demo(laser)
## Not run: demo(eight_elman)
## Not run: demo(eight_elmanSnnsR)


data(snnsData)
inputs &lt;- snnsData$eight_016.pat[,inputColumns(snnsData$eight_016.pat)]
outputs &lt;- snnsData$eight_016.pat[,outputColumns(snnsData$eight_016.pat)]

par(mfrow=c(1,2))

modelElman &lt;- elman(inputs, outputs, size=8, learnFuncParams=c(0.1), maxit=1000)
modelElman
modelJordan &lt;- jordan(inputs, outputs, size=8, learnFuncParams=c(0.1), maxit=1000)
modelJordan

plotIterativeError(modelElman)
plotIterativeError(modelJordan)

summary(modelElman)
summary(modelJordan)
</code></pre>

<hr>
<h2 id='encodeClassLabels'>Encode a matrix of (decoded) class labels</h2><span id='topic+encodeClassLabels'></span>

<h3>Description</h3>

<p>Applies <code>analyzeClassification</code> row-wise to a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>encodeClassLabels(x, method = "WTA", l = 0, h = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="encodeClassLabels_+3A_x">x</code></td>
<td>
<p>inputs</p>
</td></tr>
<tr><td><code id="encodeClassLabels_+3A_method">method</code></td>
<td>
<p>see <code>analyzeClassification</code></p>
</td></tr>
<tr><td><code id="encodeClassLabels_+3A_l">l</code></td>
<td>
<p>idem</p>
</td></tr>
<tr><td><code id="encodeClassLabels_+3A_h">h</code></td>
<td>
<p>idem</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector, each number represents a different class. A zero means
that no class was assigned to the pattern.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+analyzeClassification">analyzeClassification</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
labels &lt;- decodeClassLabels(iris[,5])
encodeClassLabels(labels)
</code></pre>

<hr>
<h2 id='exportToSnnsNetFile'>Export the net to a file in the original SNNS file format</h2><span id='topic+exportToSnnsNetFile'></span>

<h3>Description</h3>

<p>Export the net that is present in the <code><a href="#topic+rsnns">rsnns</a></code> object in the 
original (.net) SNNS file format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exportToSnnsNetFile(object, filename, netname = "RSNNS_untitled")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="exportToSnnsNetFile_+3A_object">object</code></td>
<td>
<p>the <code><a href="#topic+rsnns">rsnns</a></code> object</p>
</td></tr>
<tr><td><code id="exportToSnnsNetFile_+3A_filename">filename</code></td>
<td>
<p>path and filename to be written to</p>
</td></tr>
<tr><td><code id="exportToSnnsNetFile_+3A_netname">netname</code></td>
<td>
<p>name that is given to the network in the file</p>
</td></tr>
</table>

<hr>
<h2 id='extractNetInfo'>Extract information from a network</h2><span id='topic+extractNetInfo'></span>

<h3>Description</h3>

<p>This function generates a list of data.frames containing the most important information 
that defines a network, in a format that is easy to use. To get the full definition in 
the original SNNS format, use <code><a href="#topic+summary.rsnns">summary.rsnns</a></code> or <code><a href="#topic+exportToSnnsNetFile">exportToSnnsNetFile</a></code> 
instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractNetInfo(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extractNetInfo_+3A_object">object</code></td>
<td>
<p>the <code><a href="#topic+rsnns">rsnns</a></code> object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internally, a call to <code><a href="#topic+SnnsRObject+24extractNetInfo">SnnsRObject$extractNetInfo</a></code> is done, and the results of 
this call are returned.
</p>


<h3>Value</h3>

<p>a list containing information extracted from the network (see <code><a href="#topic+SnnsRObject+24extractNetInfo">SnnsRObject$extractNetInfo</a></code>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SnnsRObject+24extractNetInfo">SnnsRObject$extractNetInfo</a></code>
</p>

<hr>
<h2 id='getNormParameters'>Get normalization parameters of the input data</h2><span id='topic+getNormParameters'></span>

<h3>Description</h3>

<p>Get the normalization parameters that are appended by <code><a href="#topic+normalizeData">normalizeData</a></code>
as attributes to the input data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getNormParameters(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getNormParameters_+3A_x">x</code></td>
<td>
<p>input data</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is equivalent to calling <code>attr(x, "normParams")</code>.
</p>


<h3>Value</h3>

<p>the parameters generated by an earlier call to <code><a href="#topic+normalizeData">normalizeData</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normalizeData">normalizeData</a></code>, <code><a href="#topic+denormalizeData">denormalizeData</a></code>
</p>

<hr>
<h2 id='getSnnsRDefine'>Get a define of the SNNS kernel</h2><span id='topic+getSnnsRDefine'></span>

<h3>Description</h3>

<p>Get a define of the SNNS kernel from a defines-list.
All defines-lists present can be shown with <code>RSNNS:::SnnsDefines</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSnnsRDefine(defList, defValue)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getSnnsRDefine_+3A_deflist">defList</code></td>
<td>
<p>the defines-list from which to get the define from</p>
</td></tr>
<tr><td><code id="getSnnsRDefine_+3A_defvalue">defValue</code></td>
<td>
<p>the value in the list</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a string with the name of the define
</p>


<h3>See Also</h3>

<p><code><a href="#topic+resolveSnnsRDefine">resolveSnnsRDefine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>getSnnsRDefine("topologicalUnitTypes",3)
getSnnsRDefine("errorCodes",-50)
</code></pre>

<hr>
<h2 id='getSnnsRFunctionTable'>Get SnnsR function table</h2><span id='topic+getSnnsRFunctionTable'></span>

<h3>Description</h3>

<p>Get the function table of available SNNS functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSnnsRFunctionTable()
</code></pre>


<h3>Value</h3>

<p>a data.frame with columns:
</p>
<table>
<tr><td><code>name</code></td>
<td>
<p>name of the function</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>the type of the function (learning, init, update,...)</p>
</td></tr>
<tr><td><code>#inParams</code></td>
<td>
<p>the number of input parameters of the function</p>
</td></tr>
<tr><td><code>#outParams</code></td>
<td>
<p>the number of output parameters of the function</p>
</td></tr>
</table>

<hr>
<h2 id='inputColumns'>Get the columns that are inputs</h2><span id='topic+inputColumns'></span>

<h3>Description</h3>

<p>This function extracts all columns from a matrix whose column names begin with &quot;in&quot;.
The example data of this package follows this naming convention.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inputColumns(patterns)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inputColumns_+3A_patterns">patterns</code></td>
<td>
<p>matrix or data.frame containing the patterns</p>
</td></tr>
</table>

<hr>
<h2 id='jordan'>Create and train a Jordan network</h2><span id='topic+jordan'></span><span id='topic+jordan.default'></span>

<h3>Description</h3>

<p>Jordan networks are partially recurrent networks and similar to Elman
networks (see <code><a href="#topic+elman">elman</a></code>). Partially recurrent networks are useful
when working with time series data. I.e., when the output of the network not
only should depend on the current pattern, but also on the patterns presented
before.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jordan(x, ...)

## Default S3 method:
jordan(
  x,
  y,
  size = c(5),
  maxit = 100,
  initFunc = "JE_Weights",
  initFuncParams = c(1, -1, 0.3, 1, 0.5),
  learnFunc = "JE_BP",
  learnFuncParams = c(0.2),
  updateFunc = "JE_Order",
  updateFuncParams = c(0),
  shufflePatterns = FALSE,
  linOut = TRUE,
  inputsTest = NULL,
  targetsTest = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jordan_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr><td><code id="jordan_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="jordan_+3A_y">y</code></td>
<td>
<p>the corresponding targets values</p>
</td></tr>
<tr><td><code id="jordan_+3A_size">size</code></td>
<td>
<p>number of units in the hidden layer(s)</p>
</td></tr>
<tr><td><code id="jordan_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="jordan_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="jordan_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="jordan_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="jordan_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="jordan_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="jordan_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="jordan_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
<tr><td><code id="jordan_+3A_linout">linOut</code></td>
<td>
<p>sets the activation function of the output units to linear or logistic</p>
</td></tr>
<tr><td><code id="jordan_+3A_inputstest">inputsTest</code></td>
<td>
<p>a matrix with inputs to test the network</p>
</td></tr>
<tr><td><code id="jordan_+3A_targetstest">targetsTest</code></td>
<td>
<p>the corresponding targets for the test input</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Learning on Jordan networks: Backpropagation algorithms for feed-forward
networks can be adapted for their use with this type of networks. In SNNS,
there exist adapted versions of several backpropagation-type algorithms for
Jordan and Elman networks.
</p>
<p>Network architecture: A Jordan network can be seen as a feed-forward network
with additional context units in the input layer. These context units take
input from themselves (direct feedback), and from the output units. The
context units save the current state of the net. In a Jordan net, the number
of context units and output units has to be the same.
</p>
<p>Initialization of Jordan and Elman nets should be done with the default init
function <code>JE_Weights</code>, which has five parameters. The first two
parameters define an interval from which the forward connections are randomly
chosen. The third parameter gives the self-excitation weights of the context
units. The fourth parameter gives the weights of context units between them,
and the fifth parameter gives the initial activation of context units.
</p>
<p>Learning functions are <code>JE_BP</code>, <code>JE_BP_Momentum</code>,
<code>JE_Quickprop</code>, and <code>JE_Rprop</code>, which are all adapted versions of
their standard-procedure counterparts.  Update functions that can be used are
<code>JE_Order</code> and <code>JE_Special</code>.
</p>
<p>A detailed description of the theory and the parameters is available, as
always, from the SNNS documentation and the other referenced literature.
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object.
</p>


<h3>References</h3>

<p>Jordan, M. I. (1986), 'Serial Order: A Parallel, Distributed
Processing Approach', Advances in Connectionist Theory Speech 121(ICS-8604),
471-495.
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual,
Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen.
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+elman">elman</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(iris)
## Not run: demo(laser)
## Not run: demo(eight_elman)
## Not run: demo(eight_elmanSnnsR)


data(snnsData)
inputs &lt;- snnsData$laser_1000.pat[,inputColumns(snnsData$laser_1000.pat)]
outputs &lt;- snnsData$laser_1000.pat[,outputColumns(snnsData$laser_1000.pat)]

patterns &lt;- splitForTrainingAndTest(inputs, outputs, ratio=0.15)

modelJordan &lt;- jordan(patterns$inputsTrain, patterns$targetsTrain, 
                       size=c(8), learnFuncParams=c(0.1), maxit=100,
                       inputsTest=patterns$inputsTest, 
                       targetsTest=patterns$targetsTest, linOut=FALSE)

names(modelJordan)

par(mfrow=c(3,3))
plotIterativeError(modelJordan)

plotRegressionError(patterns$targetsTrain, modelJordan$fitted.values)
plotRegressionError(patterns$targetsTest, modelJordan$fittedTestValues)
hist(modelJordan$fitted.values - patterns$targetsTrain, col="lightblue")

plot(inputs, type="l")
plot(inputs[1:100], type="l")
lines(outputs[1:100], col="red")
lines(modelJordan$fitted.values[1:100], col="green")
</code></pre>

<hr>
<h2 id='matrixToActMapList'>Convert matrix of activations to activation map list</h2><span id='topic+matrixToActMapList'></span>

<h3>Description</h3>

<p>Organize a matrix containing 1d vectors of network activations as 2d maps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrixToActMapList(m, nrow = 0, ncol = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matrixToActMapList_+3A_m">m</code></td>
<td>
<p>the matrix containing one activation pattern in every row</p>
</td></tr>
<tr><td><code id="matrixToActMapList_+3A_nrow">nrow</code></td>
<td>
<p>number of rows the resulting matrices will have</p>
</td></tr>
<tr><td><code id="matrixToActMapList_+3A_ncol">ncol</code></td>
<td>
<p>number of columns the resulting matrices will have</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The input to this function is a matrix containing in each row an activation
pattern/output of a neural network. This function uses <a href="#topic+vectorToActMap">vectorToActMap</a> to 
reorganize the matrix to a list of matrices, whereby each row of the input matrix 
is converted to a matrix in the output list.
</p>


<h3>Value</h3>

<p>a list containing the activation map matrices
</p>


<h3>See Also</h3>

<p><a href="#topic+vectorToActMap">vectorToActMap</a> <a href="#topic+plotActMap">plotActMap</a>
</p>

<hr>
<h2 id='mlp'>Create and train a multi-layer perceptron (MLP)</h2><span id='topic+mlp'></span><span id='topic+mlp.default'></span>

<h3>Description</h3>

<p>This function creates a multilayer perceptron (MLP) and trains it. MLPs are
fully connected feedforward networks, and probably the most common network
architecture in use.  Training is usually performed by error backpropagation
or a related procedure.
</p>
<p>There are a lot of different learning functions present in SNNS that can be
used together with this function, e.g., <code>Std_Backpropagation</code>,
<code>BackpropBatch</code>, <code>BackpropChunk</code>, <code>BackpropMomentum</code>,
<code>BackpropWeightDecay</code>, <code>Rprop</code>, <code>Quickprop</code>, <code>SCG</code>
(scaled conjugate gradient), ...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mlp(x, ...)

## Default S3 method:
mlp(
  x,
  y,
  size = c(5),
  maxit = 100,
  initFunc = "Randomize_Weights",
  initFuncParams = c(-0.3, 0.3),
  learnFunc = "Std_Backpropagation",
  learnFuncParams = c(0.2, 0),
  updateFunc = "Topological_Order",
  updateFuncParams = c(0),
  hiddenActFunc = "Act_Logistic",
  shufflePatterns = TRUE,
  linOut = FALSE,
  outputActFunc = if (linOut) "Act_Identity" else "Act_Logistic",
  inputsTest = NULL,
  targetsTest = NULL,
  pruneFunc = NULL,
  pruneFuncParams = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mlp_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr><td><code id="mlp_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="mlp_+3A_y">y</code></td>
<td>
<p>the corresponding targets values</p>
</td></tr>
<tr><td><code id="mlp_+3A_size">size</code></td>
<td>
<p>number of units in the hidden layer(s)</p>
</td></tr>
<tr><td><code id="mlp_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="mlp_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="mlp_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="mlp_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="mlp_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="mlp_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="mlp_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="mlp_+3A_hiddenactfunc">hiddenActFunc</code></td>
<td>
<p>the activation function of all hidden units</p>
</td></tr>
<tr><td><code id="mlp_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
<tr><td><code id="mlp_+3A_linout">linOut</code></td>
<td>
<p>sets the activation function of the output units to linear or logistic (ignored if outputActFunc is given)</p>
</td></tr>
<tr><td><code id="mlp_+3A_outputactfunc">outputActFunc</code></td>
<td>
<p>the activation function of all output units</p>
</td></tr>
<tr><td><code id="mlp_+3A_inputstest">inputsTest</code></td>
<td>
<p>a matrix with inputs to test the network</p>
</td></tr>
<tr><td><code id="mlp_+3A_targetstest">targetsTest</code></td>
<td>
<p>the corresponding targets for the test input</p>
</td></tr>
<tr><td><code id="mlp_+3A_prunefunc">pruneFunc</code></td>
<td>
<p>the pruning function to use</p>
</td></tr>
<tr><td><code id="mlp_+3A_prunefuncparams">pruneFuncParams</code></td>
<td>
<p>the parameters for the pruning function. Unlike the other functions, 
these have to be given in a named list. See the pruning demos for further explanation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Std_Backpropagation</code>, <code>BackpropBatch</code>, e.g., have two parameters,
the learning rate and the maximum output difference. The learning rate is
usually a value between 0.1 and 1. It specifies the gradient descent step
width. The maximum difference defines, how much difference between output and
target value is treated as zero error, and not backpropagated. This parameter
is used to prevent overtraining. For a complete list of the parameters of all
the learning functions, see the SNNS User Manual, pp. 67.
</p>
<p>The defaults that are set for initialization and update functions usually don't have to be changed.
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object.
</p>


<h3>References</h3>

<p>Rosenblatt, F. (1958), 'The perceptron: A probabilistic model for
information storage and organization in the brain', Psychological Review
65(6), 386&ndash;408.
</p>
<p>Rumelhart, D. E.; Clelland, J. L. M. &amp; Group, P. R. (1986), Parallel distributed processing :explorations in the microstructure of cognition, Mit, Cambridge, MA etc.
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen. 
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(iris)
## Not run: demo(laser)
## Not run: demo(encoderSnnsCLib)


data(iris)

#shuffle the vector
iris &lt;- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]

irisValues &lt;- iris[,1:4]
irisTargets &lt;- decodeClassLabels(iris[,5])
#irisTargets &lt;- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)

iris &lt;- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)
iris &lt;- normTrainingAndTestSet(iris)

model &lt;- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1), 
              maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)

summary(model)
model
weightMatrix(model)
extractNetInfo(model)

par(mfrow=c(2,2))
plotIterativeError(model)

predictions &lt;- predict(model,iris$inputsTest)

plotRegressionError(predictions[,2], iris$targetsTest[,2])

confusionMatrix(iris$targetsTrain,fitted.values(model))
confusionMatrix(iris$targetsTest,predictions)

plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])
plotROC(predictions[,2], iris$targetsTest[,2])

#confusion matrix with 402040-method
confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),
                                                       method="402040", l=0.4, h=0.6))
</code></pre>

<hr>
<h2 id='normalizeData'>Data normalization</h2><span id='topic+normalizeData'></span>

<h3>Description</h3>

<p>The input matrix is column-wise normalized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalizeData(x, type = "norm")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalizeData_+3A_x">x</code></td>
<td>
<p>input data</p>
</td></tr>
<tr><td><code id="normalizeData_+3A_type">type</code></td>
<td>

<dl>
<dt>either</dt><dd><p>type string specifying the type of normalization. Implemented
are &quot;0_1&quot;, &quot;center&quot;, and &quot;norm&quot;</p>
</dd>
<dt>or</dt><dd><p>attribute list of a former call to this method to apply
e.g. normalization of the training data to the test data</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameter <code>type</code> specifies, how normalization takes place:
</p>

<dl>
<dt>0_1</dt><dd><p>values are normalized to the [0,1]-interval. The minimum in the data is mapped to zero, the maximum to one.</p>
</dd>
<dt>center</dt><dd><p>the data is centered, i.e. the mean is substracted</p>
</dd>
<dt>norm</dt><dd><p>the data is normalized to mean zero, variance one</p>
</dd>
</dl>



<h3>Value</h3>

<p>column-wise normalized input. The normalization parameters that were used for the normalization are present as attributes
of the output. They can be obtained with <code><a href="#topic+getNormParameters">getNormParameters</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+denormalizeData">denormalizeData</a></code>, <code><a href="#topic+getNormParameters">getNormParameters</a></code>
</p>

<hr>
<h2 id='normTrainingAndTestSet'>Function to normalize training and test set</h2><span id='topic+normTrainingAndTestSet'></span>

<h3>Description</h3>

<p>Normalize training and test set as obtained by <code><a href="#topic+splitForTrainingAndTest">splitForTrainingAndTest</a></code> in the following way:
The <code>inputsTrain</code> member is normalized using <code><a href="#topic+normalizeData">normalizeData</a></code> with the parameters given in <code>type</code>.
The normalization parameters obtained during this normalization are then used to normalize the <code>inputsTest</code> member.
if <code>dontNormTargets</code> is not set, then the targets are normalized in the same way. In classification problems,
normalizing the targets normally makes no sense. For regression, normalizing also the targets is usually a good idea.
The default is to not normalize targets values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normTrainingAndTestSet(x, dontNormTargets = TRUE, type = "norm")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normTrainingAndTestSet_+3A_x">x</code></td>
<td>
<p>a list containing training and test data. Usually the output of <code><a href="#topic+splitForTrainingAndTest">splitForTrainingAndTest</a></code>.</p>
</td></tr>
<tr><td><code id="normTrainingAndTestSet_+3A_dontnormtargets">dontNormTargets</code></td>
<td>
<p>should the target values also be normalized?</p>
</td></tr>
<tr><td><code id="normTrainingAndTestSet_+3A_type">type</code></td>
<td>
<p>type of the normalization. This parameter is passed to <code><a href="#topic+normalizeData">normalizeData</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with the same elements as <code><a href="#topic+splitForTrainingAndTest">splitForTrainingAndTest</a></code>, but with normalized values.
The normalization parameters are appended to each member of the list as attributes, as in <code><a href="#topic+normalizeData">normalizeData</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+splitForTrainingAndTest">splitForTrainingAndTest</a></code>, <code><a href="#topic+normalizeData">normalizeData</a></code>, <code><a href="#topic+denormalizeData">denormalizeData</a></code>, 
<code><a href="#topic+getNormParameters">getNormParameters</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
#shuffle the vector
iris &lt;- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]

irisValues &lt;- iris[,1:4]
irisTargets &lt;- decodeClassLabels(iris[,5])

iris &lt;- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)
normTrainingAndTestSet(iris)
</code></pre>

<hr>
<h2 id='outputColumns'>Get the columns that are targets</h2><span id='topic+outputColumns'></span>

<h3>Description</h3>

<p>This function extracts all columns from a matrix whose column names begin with &quot;out&quot;.
The example data of this package follows this naming convention.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>outputColumns(patterns)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="outputColumns_+3A_patterns">patterns</code></td>
<td>
<p>matrix or data.frame containing the patterns</p>
</td></tr>
</table>

<hr>
<h2 id='plotActMap'>Plot activation map</h2><span id='topic+plotActMap'></span>

<h3>Description</h3>

<p>Plot an activation map as a heatmap.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotActMap(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotActMap_+3A_x">x</code></td>
<td>
<p>the input data matrix</p>
</td></tr>
<tr><td><code id="plotActMap_+3A_...">...</code></td>
<td>
<p>parameters passed to <code>image</code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+vectorToActMap">vectorToActMap</a> <a href="#topic+matrixToActMapList">matrixToActMapList</a>
</p>

<hr>
<h2 id='plotIterativeError'>Plot iterative errors of an rsnns object</h2><span id='topic+plotIterativeError'></span><span id='topic+plotIterativeError.rsnns'></span>

<h3>Description</h3>

<p>Plot the iterative training and test error of the net of this <code><a href="#topic+rsnns">rsnns</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotIterativeError(object, ...)

## S3 method for class 'rsnns'
plotIterativeError(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotIterativeError_+3A_object">object</code></td>
<td>
<p>a rsnns object</p>
</td></tr>
<tr><td><code id="plotIterativeError_+3A_...">...</code></td>
<td>
<p>parameters passed to <code>plot</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots (if present) the class members <code>IterativeFitError</code> (as black line) and 
<code>IterativeTestError</code> (as red line).
</p>

<hr>
<h2 id='plotRegressionError'>Plot a regression error plot</h2><span id='topic+plotRegressionError'></span>

<h3>Description</h3>

<p>The plot shows target values on the x-axis and fitted/predicted values on the y-axis. 
The optimal fit would yield a line through zero with gradient one.
This optimal line is shown in black color. A linear fit to the actual data
is shown in red color.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotRegressionError(targets, fits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotRegressionError_+3A_targets">targets</code></td>
<td>
<p>the target values</p>
</td></tr>
<tr><td><code id="plotRegressionError_+3A_fits">fits</code></td>
<td>
<p>the values predicted/fitted by the model</p>
</td></tr>
<tr><td><code id="plotRegressionError_+3A_...">...</code></td>
<td>
<p>parameters passed to <code>plot</code></p>
</td></tr>
</table>

<hr>
<h2 id='plotROC'>Plot a ROC curve</h2><span id='topic+plotROC'></span>

<h3>Description</h3>

<p>This function plots a receiver operating characteristic (ROC) curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotROC(T, D, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotROC_+3A_t">T</code></td>
<td>
<p>predictions</p>
</td></tr>
<tr><td><code id="plotROC_+3A_d">D</code></td>
<td>
<p>targets</p>
</td></tr>
<tr><td><code id="plotROC_+3A_...">...</code></td>
<td>
<p>parameters passed to plot</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Code is taken from R news Volume 4/1, June 2004.
</p>


<h3>References</h3>

<p>R news Volume 4/1, June 2004
</p>

<hr>
<h2 id='predict.rsnns'>Generic predict function for rsnns object</h2><span id='topic+predict.rsnns'></span>

<h3>Description</h3>

<p>Predict values using the given network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rsnns'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.rsnns_+3A_object">object</code></td>
<td>
<p>the <code><a href="#topic+rsnns">rsnns</a></code> object</p>
</td></tr>
<tr><td><code id="predict.rsnns_+3A_newdata">newdata</code></td>
<td>
<p>the new input data which is used for prediction</p>
</td></tr>
<tr><td><code id="predict.rsnns_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the predicted values
</p>

<hr>
<h2 id='print.rsnns'>Generic print function for rsnns objects</h2><span id='topic+print.rsnns'></span>

<h3>Description</h3>

<p>Print out some characteristics of an <code><a href="#topic+rsnns">rsnns</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rsnns'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.rsnns_+3A_x">x</code></td>
<td>
<p>the <code><a href="#topic+rsnns">rsnns</a></code> object</p>
</td></tr>
<tr><td><code id="print.rsnns_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
</table>

<hr>
<h2 id='rbf'>Create and train a radial basis function (RBF) network</h2><span id='topic+rbf'></span><span id='topic+rbf.default'></span>

<h3>Description</h3>

<p>The use of an RBF network is similar to that of an <code><a href="#topic+mlp">mlp</a></code>. 
The idea of radial basis function networks comes from function 
interpolation theory. The RBF performs a linear combination of 
n basis functions that are radially symmetric around a center/prototype.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbf(x, ...)

## Default S3 method:
rbf(
  x,
  y,
  size = c(5),
  maxit = 100,
  initFunc = "RBF_Weights",
  initFuncParams = c(0, 1, 0, 0.02, 0.04),
  learnFunc = "RadialBasisLearning",
  learnFuncParams = c(1e-05, 0, 1e-05, 0.1, 0.8),
  updateFunc = "Topological_Order",
  updateFuncParams = c(0),
  shufflePatterns = TRUE,
  linOut = TRUE,
  inputsTest = NULL,
  targetsTest = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbf_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr><td><code id="rbf_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="rbf_+3A_y">y</code></td>
<td>
<p>the corresponding targets values</p>
</td></tr>
<tr><td><code id="rbf_+3A_size">size</code></td>
<td>
<p>number of units in the hidden layer(s)</p>
</td></tr>
<tr><td><code id="rbf_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="rbf_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="rbf_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="rbf_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="rbf_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="rbf_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="rbf_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="rbf_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
<tr><td><code id="rbf_+3A_linout">linOut</code></td>
<td>
<p>sets the activation function of the output units to linear or logistic</p>
</td></tr>
<tr><td><code id="rbf_+3A_inputstest">inputsTest</code></td>
<td>
<p>a matrix with inputs to test the network</p>
</td></tr>
<tr><td><code id="rbf_+3A_targetstest">targetsTest</code></td>
<td>
<p>the corresponding targets for the test input</p>
</td></tr>
</table>


<h3>Details</h3>

<p>RBF networks are feed-forward networks with one hidden layer. Their activation 
is not sigmoid (as in MLP), but radially symmetric (often gaussian). Thereby,
information is represented locally in the network (in contrast to MLP, where 
it is globally represented). Advantages of RBF networks in comparison to MLPs 
are mainly, that the networks are more interpretable, training ought to be easier
and faster, and the network only activates in areas of the feature space where it 
was actually trained, and has therewith the possibility to indicate that it &quot;just 
doesn't know&quot;.
</p>
<p>Initialization of an RBF network can be difficult and require prior knowledge. 
Before use of this function, you might want
to read pp 172-183 of the SNNS User Manual 4.2. The initialization is performed in
the current implementation by a call to <code>RBF_Weights_Kohonen(0,0,0,0,0)</code> 
and a successive call to the given <code>initFunc</code> (usually <code>RBF_Weights</code>).
If this initialization doesn't fit your needs, you should use the RSNNS low-level interface
to implement your own one. Have a look then at the demos/examples. 
Also, we note that depending on whether linear or logistic output is chosen, 
the initialization parameters have to be different (normally <code>c(0,1,...)</code>
for linear and <code>c(-4,4,...)</code> for logistic output).
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object.
</p>


<h3>References</h3>

<p>Poggio, T. &amp; Girosi, F. (1989), 'A Theory of Networks for Approximation and Learning'(A.I. Memo No.1140, C.B.I.P. Paper No. 31), Technical report, MIT ARTIFICIAL INTELLIGENCE LABORATORY.
</p>
<p>Vogt, M. (1992), 'Implementierung und Anwendung von Generalized Radial Basis Functions in einem Simulator neuronaler Netze', Master's thesis, IPVR, University of Stuttgart. (in German)
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen. 
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(rbf_irisSnnsR)
## Not run: demo(rbf_sin)
## Not run: demo(rbf_sinSnnsR)


inputs &lt;- as.matrix(seq(0,10,0.1))
outputs &lt;- as.matrix(sin(inputs) + runif(inputs*0.2))
outputs &lt;- normalizeData(outputs, "0_1")

model &lt;- rbf(inputs, outputs, size=40, maxit=1000, 
                     initFuncParams=c(0, 1, 0, 0.01, 0.01), 
                     learnFuncParams=c(1e-8, 0, 1e-8, 0.1, 0.8), linOut=TRUE)

par(mfrow=c(2,1))
plotIterativeError(model)
plot(inputs, outputs)
lines(inputs, fitted(model), col="green")
</code></pre>

<hr>
<h2 id='rbfDDA'>Create and train an RBF network with the DDA algorithm</h2><span id='topic+rbfDDA'></span><span id='topic+rbfDDA.default'></span>

<h3>Description</h3>

<p>Create and train an RBF network with the dynamic decay adjustment (DDA) algorithm. 
This type of network can only be used for classification. The training typically begins
with an empty network, i.e., a network only consisting of input and output units, and
adds new units successively. It is a lot easier to use than normal RBF, because it only 
requires two quite uncritical parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbfDDA(x, ...)

## Default S3 method:
rbfDDA(
  x,
  y,
  maxit = 1,
  initFunc = "Randomize_Weights",
  initFuncParams = c(-0.3, 0.3),
  learnFunc = "RBF-DDA",
  learnFuncParams = c(0.4, 0.2, 5),
  updateFunc = "Topological_Order",
  updateFuncParams = c(0),
  shufflePatterns = TRUE,
  linOut = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbfDDA_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_y">y</code></td>
<td>
<p>the corresponding targets values</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
<tr><td><code id="rbfDDA_+3A_linout">linOut</code></td>
<td>
<p>sets the activation function of the output units to linear or logistic</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default functions do not have to be altered. The learning function <code>RBF-DDA</code> has
three parameters: a positive threshold, and a negative threshold, that controls adding units to 
the network, and a parameter for display purposes in the original SNNS. This parameter has 
no effect in RSNNS. See p 74 of the original SNNS User Manual for details.
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object.
</p>


<h3>References</h3>

<p>Berthold, M. R. &amp; Diamond, J. (1995), Boosting the Performance of RBF Networks with Dynamic Decay Adjustment, in 'Advances in Neural Information Processing Systems', MIT Press, , pp. 521&ndash;528.
</p>
<p>Hudak, M. (1993), 'RCE classifiers: theory and practice', Cybernetics and Systems 23(5), 483&ndash;515.
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen. 
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(iris)
## Not run: demo(rbfDDA_spiralsSnnsR)


data(iris)
iris &lt;- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]
irisValues &lt;- iris[,1:4]
irisTargets &lt;- decodeClassLabels(iris[,5])
iris &lt;- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)
iris &lt;- normTrainingAndTestSet(iris)

model &lt;- rbfDDA(iris$inputsTrain, iris$targetsTrain)

summary(model)
plotIterativeError(model)
</code></pre>

<hr>
<h2 id='readPatFile'>Load data from a pat file</h2><span id='topic+readPatFile'></span>

<h3>Description</h3>

<p>This function generates an <a href="#topic+SnnsR-class">SnnsR-class</a> object, loads the given .pat file 
there as a pattern set and then extracts the patterns to a matrix, 
using <a href="#topic+SnnsRObject+24extractPatterns">SnnsRObject$extractPatterns</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readPatFile(filename)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readPatFile_+3A_filename">filename</code></td>
<td>
<p>the name of the .pat file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix containing the data loaded from the .pat file.
</p>

<hr>
<h2 id='readResFile'>Rudimentary parser for res files.</h2><span id='topic+readResFile'></span>

<h3>Description</h3>

<p>This function contains a rudimentary parser for SNNS .res files. It
is completely implemented in R and doesn't make use of SNNS functionality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readResFile(filename)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readResFile_+3A_filename">filename</code></td>
<td>
<p>the name of the .res file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix containing the predicted values that were found in the .res file
</p>

<hr>
<h2 id='resolveSnnsRDefine'>Resolve a define of the SNNS kernel</h2><span id='topic+resolveSnnsRDefine'></span>

<h3>Description</h3>

<p>Resolve a define of the SNNS kernel using a defines-list.
All defines-lists present can be shown with <code>RSNNS:::SnnsDefines</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resolveSnnsRDefine(defList, def)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="resolveSnnsRDefine_+3A_deflist">defList</code></td>
<td>
<p>the defines-list from which to resolve the define from</p>
</td></tr>
<tr><td><code id="resolveSnnsRDefine_+3A_def">def</code></td>
<td>
<p>the name of the define</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of the define
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getSnnsRDefine">getSnnsRDefine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>resolveSnnsRDefine("topologicalUnitTypes","UNIT_HIDDEN")
</code></pre>

<hr>
<h2 id='rsnnsObjectFactory'>Object factory for generating rsnns objects</h2><span id='topic+rsnnsObjectFactory'></span><span id='topic+rsnns'></span>

<h3>Description</h3>

<p>The object factory generates an <code>rsnns</code> object and initializes its
member variables with the values given as parameters. Furthermore, it
generates an object of <code><a href="#topic+SnnsR-class">SnnsR-class</a></code>. Later, this information is
to be used to train the network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rsnnsObjectFactory(
  subclass,
  nInputs,
  maxit,
  initFunc,
  initFuncParams,
  learnFunc,
  learnFuncParams,
  updateFunc,
  updateFuncParams,
  shufflePatterns = TRUE,
  computeIterativeError = TRUE,
  pruneFunc = NULL,
  pruneFuncParams = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rsnnsObjectFactory_+3A_subclass">subclass</code></td>
<td>
<p>the subclass of rsnns to generate (vector of strings)</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_ninputs">nInputs</code></td>
<td>
<p>the number of inputs the network will have</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_computeiterativeerror">computeIterativeError</code></td>
<td>
<p>should the error be computed in every iteration?</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_prunefunc">pruneFunc</code></td>
<td>
<p>the pruning function to use</p>
</td></tr>
<tr><td><code id="rsnnsObjectFactory_+3A_prunefuncparams">pruneFuncParams</code></td>
<td>
<p>the parameters for the pruning function. Unlike the other functions, 
these have to be given in a named list. See the pruning demos for further explanation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The typical procedure implemented in <code>rsnns</code> subclasses is the following: 
</p>

<ul>
<li><p> generate the <code>rsnns</code> object with this object factory
</p>
</li>
<li><p> generate the network according to the architecture needed
</p>
</li>
<li><p> train the network (with <code><a href="#topic+train">train</a></code>)
</p>
</li></ul>

<p>In every <code>rsnns</code> object, the iterative error is the summed squared error
(SSE) of all patterns. If the SSE is computed on the test set, then it is
weighted to take care of the different amount of patterns in the sets.
</p>


<h3>Value</h3>

<p>a partly initialized <code>rsnns</code> object
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mlp">mlp</a></code>, <code><a href="#topic+dlvq">dlvq</a></code>, <code><a href="#topic+rbf">rbf</a></code>, <code><a href="#topic+rbfDDA">rbfDDA</a></code>, <code><a href="#topic+elman">elman</a></code>, 
<code><a href="#topic+jordan">jordan</a></code>, <code><a href="#topic+som">som</a></code>, <code><a href="#topic+art1">art1</a></code>, <code><a href="#topic+art2">art2</a></code>, <code><a href="#topic+artmap">artmap</a></code>, <code><a href="#topic+assoz">assoz</a></code>
</p>

<hr>
<h2 id='savePatFile'>Save data to a pat file</h2><span id='topic+savePatFile'></span>

<h3>Description</h3>

<p>This function generates an <a href="#topic+SnnsR-class">SnnsR-class</a> object, loads the given data there 
as a pattern set and then uses the functionality of SNNS to save the data as a .pat file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>savePatFile(inputs, targets, filename)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="savePatFile_+3A_inputs">inputs</code></td>
<td>
<p>a matrix with input values</p>
</td></tr>
<tr><td><code id="savePatFile_+3A_targets">targets</code></td>
<td>
<p>a matrix with target values</p>
</td></tr>
<tr><td><code id="savePatFile_+3A_filename">filename</code></td>
<td>
<p>the name of the .pat file</p>
</td></tr>
</table>

<hr>
<h2 id='setSnnsRSeedValue'>DEPRECATED, Set the SnnsR seed value</h2><span id='topic+setSnnsRSeedValue'></span>

<h3>Description</h3>

<p>DEPRECATED, now just calls R's set.seed(), that should be used instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setSnnsRSeedValue(seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setSnnsRSeedValue_+3A_seed">seed</code></td>
<td>
<p>the seed to use. If 0, a seed based on the system time is generated.</p>
</td></tr>
</table>

<hr>
<h2 id='snnsData'>Example data of the package</h2><span id='topic+snnsData'></span>

<h3>Description</h3>

<p>This is data from the original SNNS examples directory ported to R and stored as one list.
The function <code><a href="#topic+readPatFile">readPatFile</a></code> was used to parse all pattern files (.pat) from the 
original SNNS examples directory. Due to limitations of that function, pattern files
containing patterns with variable size were omitted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(snnsData)
names(snnsData)
</code></pre>

<hr>
<h2 id='SnnsR-class'>The main class of the package</h2><span id='topic+SnnsR-class'></span>

<h3>Description</h3>

<p>An S4 class that is the main class of RSNNS. Each instance of this class 
contains a pointer to a C++ object of type SnnsCLib, i.e. an instance 
of the SNNS kernel.
</p>


<h3>Details</h3>

<p>The only slot <code>variables</code> holds an environment with all member variables. 
Currently, there are two members (constructed by the object factory):
</p>

<dl>
<dt>snnsCLibPointer</dt><dd><p>A pointer to the corresponding C++ object</p>
</dd>
<dt>serialization</dt><dd><p>a serialization of the C++ object, in SNNS .net format</p>
</dd>
</dl>

<p>The member variables are not directly present as slots but wrapped in an environment
to allow for changing the serialization (by call by reference). 
</p>
<p>An object of this class is used internally by all the models in the package. 
The object is always accessible by <code>model$snnsObject$...</code>
</p>
<p>To make full use of the SNNS functionalities, you might want to use this class directly.
Always use the object factory <code><a href="#topic+SnnsRObjectFactory">SnnsRObjectFactory</a></code> to construct an object, 
and the calling mechanism <code><a href="#topic++24">$</a></code> to call functions. Through the calling mechanism,
many functions of SnnsCLib are present that are not documented here, but in the SNNS User 
Manual. So, if you choose to use the low-level interface, it is highly recommended to have
a look at the demos and at the SNNS User Manual.
</p>


<h3>References</h3>

<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen. 
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic++24">$</a></code>, <code><a href="#topic+SnnsRObjectFactory">SnnsRObjectFactory</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(encoderSnnsCLib) 
## Not run: demo(art1_lettersSnnsR)
## Not run: demo(art2_tetraSnnsR) 
## Not run: demo(artmap_lettersSnnsR) 
## Not run: demo(eight_elmanSnnsR)
## Not run: demo(rbf_irisSnnsR)
## Not run: demo(rbf_sinSnnsR)
## Not run: demo(rbfDDA_spiralsSnnsR)
## Not run: demo(som_cubeSnnsR)


#This is the demo eight_elmanSnnsR
#Here, we train an Elman network
#and save a trained and an untrained version
#to disk, as well as the used training data

basePath &lt;- ("./")

data(snnsData)

inputs &lt;- snnsData$eight_016.pat[,inputColumns(snnsData$eight_016.pat)]
outputs &lt;- snnsData$eight_016.pat[,outputColumns(snnsData$eight_016.pat)]

snnsObject &lt;- SnnsRObjectFactory()

snnsObject$setLearnFunc('JE_BP')
snnsObject$setUpdateFunc('JE_Order')
snnsObject$setUnitDefaults(1,0,1,0,1,'Act_Logistic','Out_Identity')

snnsObject$elman_createNet(c(2,8,2),c(1,1,1),FALSE)


patset &lt;- snnsObject$createPatSet(inputs, outputs)
snnsObject$setCurrPatSet(patset$set_no)

snnsObject$initializeNet(c(1.0,  -1.0,  0.3,  1.0,  0.5) )
snnsObject$shufflePatterns(TRUE)
snnsObject$DefTrainSubPat()

## Not run: snnsObject$saveNet(paste(basePath,"eight_elmanSnnsR_untrained.net",sep=""),
                                          "eight_elmanSnnsR_untrained")
## End(Not run)

parameters &lt;- c(0.2, 0, 0, 0, 0)
maxit &lt;- 1000

error &lt;- vector()
for(i in 1:maxit) {
  res &lt;- snnsObject$learnAllPatterns(parameters)
  if(res[[1]] != 0) print(paste("Error at iteration ", i, " : ", res, sep=""))
  error[i] &lt;- res[[2]]
}

error[1:500]
plot(error, type="l")

## Not run: snnsObject$saveNet(paste(basePath,"eight_elmanSnnsR.net",sep=""),
                                             "eight_elmanSnnsR")
## End(Not run)
## Not run: snnsObject$saveNewPatterns(paste(basePath,"eight_elmanSnnsR.pat",sep=""), 
                                                         patset$set_no)
## End(Not run)
</code></pre>

<hr>
<h2 id='SnnsRObject+24createNet'>Create a layered network</h2><span id='topic+SnnsRObject+24createNet'></span><span id='topic+SnnsR__createNet'></span><span id='topic+createNet+2CSnnsR-method'></span>

<h3>Description</h3>

<p>This function creates a layered network in the given SnnsR object.
This is an SnnsR low-level function. You may want to have a look 
at <code><a href="#topic+SnnsR-class">SnnsR-class</a></code> to find out how to properly use it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
createNet(unitsPerLayer, fullyConnectedFeedForward = TRUE, iNames = NULL, oNames = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24createNet_+3A_unitsperlayer">unitsPerLayer</code></td>
<td>
<p>a vector of integers that represents the number of units in each layer, including input and output layer</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24createNet_+3A_fullyconnectedfeedforward">fullyConnectedFeedForward</code></td>
<td>
<p>if <code>TRUE</code>, the network is fully connected as a feed-forward network. If <code>FALSE</code>, 
no connections are created</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24createNet_+3A_inames">iNames</code></td>
<td>
<p>names of input units</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24createNet_+3A_onames">oNames</code></td>
<td>
<p>names of output units</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SnnsR-class">SnnsR-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>obj1 &lt;- SnnsRObjectFactory()
obj1$createNet(c(2,2), FALSE)
obj1$getUnitDefinitions()

obj2 &lt;- SnnsRObjectFactory()
obj2$createNet(c(8,5,5,2), TRUE)
obj2$getUnitDefinitions()
</code></pre>

<hr>
<h2 id='SnnsRObject+24createPatSet'>Create a pattern set</h2><span id='topic+SnnsRObject+24createPatSet'></span><span id='topic+SnnsR__createPatSet'></span><span id='topic+createPatSet+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to create a pattern set in the SNNS kernel from 
the values given, so that they are available in the SNNS kernel for use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
createPatSet(inputs, targets)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24createPatSet_+3A_inputs">inputs</code></td>
<td>
<p>the input values</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24createPatSet_+3A_targets">targets</code></td>
<td>
<p>the target values</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with elements <code>err</code> and <code>set_no</code>. The latter one identifies the pattern set within the <code><a href="#topic+SnnsR-class">SnnsR-class</a></code> object
</p>

<hr>
<h2 id='SnnsRObject+24extractNetInfo'>Get characteristics of the network.</h2><span id='topic+SnnsRObject+24extractNetInfo'></span><span id='topic+SnnsR__extractNetInfo'></span><span id='topic+extractNetInfo+2CSnnsR-method'></span>

<h3>Description</h3>

<p>The returned list has three members: 
</p>

<ul>
<li><p> infoHeader general information about the network
</p>
</li>
<li><p> unitDefinitions information about the units
</p>
</li>
<li><p> fullWeightMatrix weight matrix of the connections
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
extractNetInfo()
</code></pre>


<h3>Value</h3>

<p>a list of data frames containing information extracted from the network.
</p>

<hr>
<h2 id='SnnsRObject+24extractPatterns'>Extract the current pattern set to a matrix</h2><span id='topic+SnnsRObject+24extractPatterns'></span><span id='topic+SnnsR__extractPatterns'></span><span id='topic+extractPatterns+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function that extracts all patterns of the current pattern set and
returns them as a matrix. Columns are named with the prefix &quot;in&quot; or &quot;out&quot;, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
extractPatterns()
</code></pre>


<h3>Value</h3>

<p>a matrix containing the patterns of the currently loaded patern set.
</p>

<hr>
<h2 id='SnnsRObject+24getAllHiddenUnits'>Get all hidden units of the net</h2><span id='topic+SnnsRObject+24getAllHiddenUnits'></span><span id='topic+SnnsR__getAllHiddenUnits'></span><span id='topic+getAllHiddenUnits+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to get all units from the net with the ttype &quot;UNIT_HIDDEN&quot;.
This function calls <code><a href="#topic+SnnsRObject+24getAllUnitsTType">SnnsRObject$getAllUnitsTType</a></code> with the parameter &quot;UNIT_HIDDEN&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getAllHiddenUnits()
</code></pre>


<h3>Value</h3>

<p>a vector with integer numbers identifying the units.
</p>


<h3>See Also</h3>

<p><a href="#topic+SnnsRObject+24getAllUnitsTType">SnnsRObject$getAllUnitsTType</a>
</p>

<hr>
<h2 id='SnnsRObject+24getAllInputUnits'>Get all input units of the net</h2><span id='topic+SnnsRObject+24getAllInputUnits'></span><span id='topic+SnnsR__getAllInputUnits'></span><span id='topic+getAllInputUnits+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to get all units from the net with the ttype &quot;UNIT_INPUT&quot;.
This function calls <code><a href="#topic+SnnsRObject+24getAllUnitsTType">SnnsRObject$getAllUnitsTType</a></code> with the parameter &quot;UNIT_INPUT&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getAllInputUnits()
</code></pre>


<h3>Value</h3>

<p>a vector with integer numbers identifying the units.
</p>


<h3>See Also</h3>

<p><a href="#topic+SnnsRObject+24getAllUnitsTType">SnnsRObject$getAllUnitsTType</a>
</p>

<hr>
<h2 id='SnnsRObject+24getAllOutputUnits'>Get all output units of the net.</h2><span id='topic+SnnsRObject+24getAllOutputUnits'></span><span id='topic+SnnsR__getAllOutputUnits'></span><span id='topic+getAllOutputUnits+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to get all units from the net with the ttype &quot;UNIT_OUTPUT&quot;.
This function calls <code><a href="#topic+SnnsRObject+24getAllUnitsTType">SnnsRObject$getAllUnitsTType</a></code> with the parameter &quot;UNIT_OUTPUT&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getAllOutputUnits()
</code></pre>


<h3>Value</h3>

<p>a vector with integer numbers identifying the units.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SnnsRObject+24getAllUnitsTType">SnnsRObject$getAllUnitsTType</a></code>
</p>

<hr>
<h2 id='SnnsRObject+24getAllUnits'>Get all units present in the net.</h2><span id='topic+SnnsRObject+24getAllUnits'></span><span id='topic+SnnsR__getAllUnits'></span><span id='topic+getAllUnits+2CSnnsR-method'></span>

<h3>Description</h3>

<p>Get all units present in the net.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getAllUnits()
</code></pre>


<h3>Value</h3>

<p>a vector with integer numbers identifying the units.
</p>

<hr>
<h2 id='SnnsRObject+24getAllUnitsTType'>Get all units in the net of a certain <code>ttype</code>.</h2><span id='topic+SnnsRObject+24getAllUnitsTType'></span><span id='topic+SnnsR__getAllUnitsTType'></span><span id='topic+getAllUnitsTType+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to get all units in the net of a certain <code>ttype</code>. 
Possible <code>ttype</code> defined by SNNS are, among others:
&quot;UNIT_OUTPUT&quot;, &quot;UNIT_INPUT&quot;, and &quot;UNIT_HIDDEN&quot;. For a full list, 
call <code>RSNNS:::SnnsDefines$topologicalUnitTypes</code>
As this is an SnnsR low-level function, you may want to have a look 
at <code><a href="#topic+SnnsR-class">SnnsR-class</a></code> to find out how to properly use it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getAllUnitsTType(ttype)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24getAllUnitsTType_+3A_ttype">ttype</code></td>
<td>
<p>a string containing the <code>ttype</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with integer numbers identifying the units.
</p>


<h3>See Also</h3>

<p><a href="#topic+SnnsRObject+24getAllOutputUnits">SnnsRObject$getAllOutputUnits</a>, <a href="#topic+SnnsRObject+24getAllInputUnits">SnnsRObject$getAllInputUnits</a>, <a href="#topic+SnnsRObject+24getAllHiddenUnits">SnnsRObject$getAllHiddenUnits</a>
</p>

<hr>
<h2 id='SnnsRObject+24getCompleteWeightMatrix'>Get the complete weight matrix.</h2><span id='topic+SnnsRObject+24getCompleteWeightMatrix'></span><span id='topic+SnnsR__getCompleteWeightMatrix'></span><span id='topic+getCompleteWeightMatrix+2CSnnsR-method'></span>

<h3>Description</h3>

<p>Get a weight matrix containing all weights of all neurons present in the net.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getCompleteWeightMatrix(setDimNames)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24getCompleteWeightMatrix_+3A_setdimnames">setDimNames</code></td>
<td>
<p>indicates, whether names of units are extracted and set as row/col names in the weight matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the complete weight matrix
</p>

<hr>
<h2 id='SnnsRObject+24getInfoHeader'>Get an info header of the network.</h2><span id='topic+SnnsRObject+24getInfoHeader'></span><span id='topic+SnnsR__getInfoHeader'></span><span id='topic+getInfoHeader+2CSnnsR-method'></span>

<h3>Description</h3>

<p>Get an info header of the network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getInfoHeader()
</code></pre>


<h3>Value</h3>

<p>a data frame containing some general characteristics of the network.
</p>

<hr>
<h2 id='SnnsRObject+24getSiteDefinitions'>Get the sites definitions of the network.</h2><span id='topic+SnnsRObject+24getSiteDefinitions'></span><span id='topic+SnnsR__getSiteDefinitions'></span><span id='topic+getSiteDefinitions+2CSnnsR-method'></span>

<h3>Description</h3>

<p>Get the sites definitions of the network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getSiteDefinitions()
</code></pre>


<h3>Value</h3>

<p>a data frame containing information about all sites present in the network.
</p>

<hr>
<h2 id='SnnsRObject+24getTypeDefinitions'>Get the FType definitions of the network.</h2><span id='topic+SnnsRObject+24getTypeDefinitions'></span><span id='topic+SnnsR__getTypeDefinitions'></span><span id='topic+getTypeDefinitions+2CSnnsR-method'></span>

<h3>Description</h3>

<p>Get the FType definitions of the network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getTypeDefinitions()
</code></pre>


<h3>Value</h3>

<p>a data frame containing information about FType units present in the network.
</p>

<hr>
<h2 id='SnnsRObject+24getUnitDefinitions'>Get the unit definitions of the network.</h2><span id='topic+SnnsRObject+24getUnitDefinitions'></span><span id='topic+SnnsR__getUnitDefinitions'></span><span id='topic+getUnitDefinitions+2CSnnsR-method'></span>

<h3>Description</h3>

<p>Get the unit definitions of the network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getUnitDefinitions()
</code></pre>


<h3>Value</h3>

<p>a data frame containing information about all units present in the network.
</p>

<hr>
<h2 id='SnnsRObject+24getUnitsByName'>Find all units whose name begins with a given prefix.</h2><span id='topic+SnnsRObject+24getUnitsByName'></span><span id='topic+SnnsR__getUnitsByName'></span><span id='topic+getUnitsByName+2CSnnsR-method'></span>

<h3>Description</h3>

<p>Find all units whose name begins with a given prefix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getUnitsByName(prefix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24getUnitsByName_+3A_prefix">prefix</code></td>
<td>
<p>a prefix that the names of the units to find have.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with integer numbers identifying the units.
</p>

<hr>
<h2 id='SnnsRObject+24getWeightMatrix'>Get the weight matrix between two sets of units</h2><span id='topic+SnnsRObject+24getWeightMatrix'></span><span id='topic+SnnsR__getWeightMatrix'></span><span id='topic+getWeightMatrix+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to get the weight matrix between two sets of units.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
getWeightMatrix(unitsSource, unitsTarget, setDimNames)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24getWeightMatrix_+3A_unitssource">unitsSource</code></td>
<td>
<p>a vector with numbers identifying the source units</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24getWeightMatrix_+3A_unitstarget">unitsTarget</code></td>
<td>
<p>a vector with numbers identifying the target units</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24getWeightMatrix_+3A_setdimnames">setDimNames</code></td>
<td>
<p>indicates, whether names of units are extracted and set as row/col names in the weight matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the weight matrix between the two sets of neurons
</p>


<h3>See Also</h3>

<p><a href="#topic+SnnsRObject+24getAllUnitsTType">SnnsRObject$getAllUnitsTType</a>
</p>

<hr>
<h2 id='SnnsRObject+24initializeNet'>Initialize the network</h2><span id='topic+SnnsRObject+24initializeNet'></span><span id='topic+SnnsR__initializeNet'></span><span id='topic+initializeNet+2CSnnsR-method'></span>

<h3>Description</h3>

<p>This SnnsR low-level function masks the SNNS kernel function of the same name
to allow for both giving the initialization function
directly in the call or to use the one that is currently set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
initializeNet(parameterInArray, initFunc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24initializeNet_+3A_parameterinarray">parameterInArray</code></td>
<td>
<p>the parameters of the initialization function</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24initializeNet_+3A_initfunc">initFunc</code></td>
<td>
<p>the name of the initialization function</p>
</td></tr>
</table>

<hr>
<h2 id='SnnsRObject+24predictCurrPatSet'>Predict values with a trained net</h2><span id='topic+SnnsRObject+24predictCurrPatSet'></span><span id='topic+SnnsR__predictCurrPatSet'></span><span id='topic+predictCurrPatSet+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to predict values with a trained net.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
predictCurrPatSet(outputMethod="reg_class", updateFuncParams=c(0.0))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24predictCurrPatSet_+3A_outputmethod">outputMethod</code></td>
<td>
<p>is passed to <a href="#topic+SnnsRObject+24whereAreResults">SnnsRObject$whereAreResults</a></p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24predictCurrPatSet_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>parameters passed to the networks update function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function has to be used embedded in a step of loading and afterwards 
removing the patterns into the <code><a href="#topic+SnnsR-class">SnnsR-class</a></code> object. As SNNS only supports 2 pattern sets
in parallel, removing unneeded pattern sets is quite important.
</p>


<h3>Value</h3>

<p>the predicted values
</p>

<hr>
<h2 id='SnnsRObject+24resetRSNNS'>Reset the SnnsR object.</h2><span id='topic+SnnsRObject+24resetRSNNS'></span><span id='topic+SnnsR__resetRSNNS'></span><span id='topic+resetRSNNS+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to delete all pattern sets and 
delete the current net in the <code><a href="#topic+SnnsR-class">SnnsR-class</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
resetRSNNS()
</code></pre>

<hr>
<h2 id='SnnsRObject+24setTTypeUnitsActFunc'>Set the activation function for all units of a certain ttype.</h2><span id='topic+SnnsRObject+24setTTypeUnitsActFunc'></span><span id='topic+SnnsR__setTTypeUnitsActFunc'></span><span id='topic+setTTypeUnitsActFunc+2CSnnsR-method'></span>

<h3>Description</h3>

<p>The function uses the function <code><a href="#topic+SnnsRObject+24getAllUnitsTType">SnnsRObject$getAllUnitsTType</a></code> to find all units of a certain
<code>ttype</code>, and sets the activation function of all these units to the given activation function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
setTTypeUnitsActFunc(ttype, act_func)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24setTTypeUnitsActFunc_+3A_ttype">ttype</code></td>
<td>
<p>a string containing the <code>ttype</code>.</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24setTTypeUnitsActFunc_+3A_act_func">act_func</code></td>
<td>
<p>the name of the activation function to set.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SnnsRObject+24getAllUnitsTType">SnnsRObject$getAllUnitsTType</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: SnnsRObject$setTTypeUnitsActFunc("UNIT_HIDDEN", "Act_Logistic")
</code></pre>

<hr>
<h2 id='SnnsRObject+24setUnitDefaults'>Set the unit defaults</h2><span id='topic+SnnsRObject+24setUnitDefaults'></span><span id='topic+SnnsR__setUnitDefaults'></span><span id='topic+setUnitDefaults+2CSnnsR-method'></span>

<h3>Description</h3>

<p>This SnnsR low-level function masks the SNNS kernel function of the same name 
to allow both for giving the parameters directly or as a vector.
If the second parameter, <code>bias</code>, is missing, it is assumed 
that the first parameter should be interpreted as a vector containing
all parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
setUnitDefaults(act, bias, st, subnet_no, layer_no, act_func, out_func)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24setUnitDefaults_+3A_act">act</code></td>
<td>
<p>same as SNNS kernel function</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24setUnitDefaults_+3A_bias">bias</code></td>
<td>
<p>idem</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24setUnitDefaults_+3A_st">st</code></td>
<td>
<p>idem</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24setUnitDefaults_+3A_subnet_no">subnet_no</code></td>
<td>
<p>idem</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24setUnitDefaults_+3A_layer_no">layer_no</code></td>
<td>
<p>idem</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24setUnitDefaults_+3A_act_func">act_func</code></td>
<td>
<p>idem</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24setUnitDefaults_+3A_out_func">out_func</code></td>
<td>
<p>idem</p>
</td></tr>
</table>

<hr>
<h2 id='SnnsRObject+24somPredictComponentMaps'>Calculate the som component maps</h2><span id='topic+SnnsRObject+24somPredictComponentMaps'></span><span id='topic+SnnsR__somPredictComponentMaps'></span><span id='topic+somPredictComponentMaps+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to calculate the som component maps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
somPredictComponentMaps(updateFuncParams=c(0.0, 0.0, 1.0))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24somPredictComponentMaps_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>parameters passed to the networks update function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix containing all componant maps as 1d vectors
</p>


<h3>See Also</h3>

<p><code><a href="#topic+som">som</a></code>
</p>

<hr>
<h2 id='SnnsRObject+24somPredictCurrPatSetWinners'>Get most of the relevant results from a som</h2><span id='topic+SnnsRObject+24somPredictCurrPatSetWinners'></span><span id='topic+SnnsR__somPredictCurrPatSetWinners'></span><span id='topic+somPredictCurrPatSetWinners+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to get most of the relevant results from a SOM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
somPredictCurrPatSetWinners(updateFuncParams=c(0.0, 0.0, 1.0), 
saveWinnersPerPattern=TRUE, targets=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24somPredictCurrPatSetWinners_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>parameters passed to the networks update function</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24somPredictCurrPatSetWinners_+3A_savewinnersperpattern">saveWinnersPerPattern</code></td>
<td>
<p>should a list with the winners for every pattern be saved?</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24somPredictCurrPatSetWinners_+3A_targets">targets</code></td>
<td>
<p>optional target classes of the patterns</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with three elements:
</p>
<table>
<tr><td><code>nWinnersPerUnit</code></td>
<td>
<p>For each unit, the amount of patterns where this unit won is given. So, this is a 1d vector representing the normal version of the som.</p>
</td></tr>
<tr><td><code>winnersPerPattern</code></td>
<td>
<p>a vector where for each pattern the number of the winning unit is given. This is an intermediary result
that normally won't be saved.</p>
</td></tr>
<tr><td><code>labeledUnits</code></td>
<td>
<p>a matrix which &ndash; if the <code>targets</code> parameter is given &ndash; contains for each unit (rows) and each class 
present in the <code>targets</code> (columns), the amount of patterns of the class where the unit has won. From the <code>labeledUnits</code>, 
the <code>labeledMap</code> can be computed, e.g. by voting of the class labels for the final label of the unit.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+som">som</a></code>
</p>

<hr>
<h2 id='SnnsRObject+24somPredictCurrPatSetWinnersSpanTree'>Get the spanning tree of the SOM</h2><span id='topic+SnnsRObject+24somPredictCurrPatSetWinnersSpanTree'></span><span id='topic+SnnsR__somPredictCurrPatSetWinnersSpanTree'></span><span id='topic+somPredictCurrPatSetWinnersSpanTree+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to get the spanning tree of the SOM, This function 
calls directly the corresponding SNNS kernel function (the only one available for SOM).
Advantage are faster computation, disadvantage is somewhat limited information in
the output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
somPredictCurrPatSetWinnersSpanTree()
</code></pre>


<h3>Value</h3>

<p>the spanning tree, which is the som, showing for each unit a number identifying 
the last pattern for which this unit won. (We note that, also if there are more than 
one patterns, only the last one is saved)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+som">som</a></code>
</p>

<hr>
<h2 id='SnnsRObject+24train'>Train a network and test it in every training iteration</h2><span id='topic+SnnsRObject+24train'></span><span id='topic+SnnsR__train'></span><span id='topic+train+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to train a network and test it in every training iteration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
train(inputsTrain, targetsTrain=NULL, 
    initFunc="Randomize_Weights", initFuncParams=c(1.0, -1.0), 
    learnFunc="Std_Backpropagation", learnFuncParams=c(0.2, 0),
    updateFunc="Topological_Order", updateFuncParams=c(0.0), 
    outputMethod="reg_class", maxit=100, shufflePatterns=TRUE, 
    computeError=TRUE, inputsTest=NULL, targetsTest=NULL,
    pruneFunc=NULL, pruneFuncParams=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24train_+3A_inputstrain">inputsTrain</code></td>
<td>
<p>a matrix with inputs for the network</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_targetstrain">targetsTrain</code></td>
<td>
<p>the corresponding targets</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_initfunc">initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_learnfunc">learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_updatefunc">updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_outputmethod">outputMethod</code></td>
<td>
<p>the output method of the net</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_computeerror">computeError</code></td>
<td>
<p>should the error be computed in every iteration?</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_inputstest">inputsTest</code></td>
<td>
<p>a matrix with inputs to test the network</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_targetstest">targetsTest</code></td>
<td>
<p>the corresponding targets for the test input</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_prunefunc">pruneFunc</code></td>
<td>
<p>the pruning function to use</p>
</td></tr>
<tr><td><code id="SnnsRObject+2B24train_+3A_prunefuncparams">pruneFuncParams</code></td>
<td>
<p>the parameters for the pruning function. Unlike the other functions, 
these have to be given in a named list. See the pruning demos for further explanation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing:
</p>
<table>
<tr><td><code>fitValues</code></td>
<td>
<p>the fitted values, i.e. outputs of the training inputs</p>
</td></tr>
<tr><td><code>IterativeFitError</code></td>
<td>
<p>The SSE in every iteration/epoch on the training set</p>
</td></tr>
<tr><td><code>testValues</code></td>
<td>
<p>the predicted values, i.e. outputs of the test inputs</p>
</td></tr>
<tr><td><code>IterativeTestError</code></td>
<td>
<p>The SSE in every iteration/epoch on the test set</p>
</td></tr>
</table>

<hr>
<h2 id='SnnsRObject+24whereAreResults'>Get a list of output units of a net</h2><span id='topic+SnnsRObject+24whereAreResults'></span><span id='topic+SnnsR__whereAreResults'></span><span id='topic+whereAreResults+2CSnnsR-method'></span>

<h3>Description</h3>

<p>SnnsR low-level function to get a list of output units of a net.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
whereAreResults(outputMethod="output")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObject+2B24whereAreResults_+3A_outputmethod">outputMethod</code></td>
<td>
<p>a string defining the output method of the net. Possible values are: &quot;art1&quot;, &quot;art2&quot;, &quot;artmap&quot;, &quot;assoz&quot;, &quot;som&quot;, &quot;output&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Depending on the network architecture, output is present in hidden units, in
output units, etc. In some network types, the output units have a certain
name prefix in SNNS. This function finds the output units according to
certain network types. The type is specified by <code>outputMethod</code>. If the
given <code>outputMethod</code> is unknown, the function defaults to &quot;output&quot;.
</p>


<h3>Value</h3>

<p>a list of numbers identifying the units
</p>

<hr>
<h2 id='SnnsRObjectFactory'>SnnsR object factory</h2><span id='topic+SnnsRObjectFactory'></span>

<h3>Description</h3>

<p>Object factory to create a new object of type <code><a href="#topic+SnnsR-class">SnnsR-class</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SnnsRObjectFactory(x = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObjectFactory_+3A_x">x</code></td>
<td>
<p>(optional) object of class <a href="#topic+SnnsR-class">SnnsR-class</a>, to be deep-copied</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a new object of class <code><a href="#topic+SnnsR-class">SnnsR-class</a></code>, initializes its only slot <code>variables</code>
with a new environment, generates a new C++ object of class SnnsCLib, and an empty object serialization.
</p>


<h3>See Also</h3>

<p><code><a href="#topic++24">$</a></code>, <code><a href="#topic+SnnsR-class">SnnsR-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mySnnsObject &lt;- SnnsRObjectFactory()
mySnnsObject$setLearnFunc('Quickprop')
mySnnsObject$setUpdateFunc('Topological_Order') 
</code></pre>

<hr>
<h2 id='SnnsRObjectMethodCaller'>Method caller for SnnsR objects</h2><span id='topic+SnnsRObjectMethodCaller'></span><span id='topic++24+2CSnnsR-method'></span><span id='topic++24'></span>

<h3>Description</h3>

<p>Enable calling of C++ functions as methods of <code>SnnsR-class</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'SnnsR'
x$name
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SnnsRObjectMethodCaller_+3A_x">x</code></td>
<td>
<p>object of class <a href="#topic+SnnsR-class">SnnsR-class</a></p>
</td></tr>
<tr><td><code id="SnnsRObjectMethodCaller_+3A_name">name</code></td>
<td>
<p>function to call</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function makes methods of SnnsR__ and SnnsCLib__ accessible via &quot;$&quot;. If
no SnnsR__ method is present, then the according SnnsCLib__ method is
called. This enables a very flexible method handling. To mask a method from
SnnsCLib, e.g. to do some parameter checking or postprocessing, only a method
with the same name, but beginning with SnnsR__ has to be present in R.  See
e.g. <code><a href="#topic+SnnsRObject+24initializeNet">SnnsRObject$initializeNet</a></code> for such an implementation.
</p>
<p>Error handling is also done within the method caller. If the result of a
function is a list with a member <code>err</code>,  then <code>SnnsCLib__error</code> is
called to use the SNNS kernel function to get the corresponding error message
code and an R warning is thrown containing this message.
</p>
<p>Furthermore, a serialization mechanism is implemented which all models
present in the package use to be able to be saved and loaded by R's normal
save/load mechanism (as RData files).
</p>
<p>The completely trained object can be serialized with
</p>
<p><code>s &lt;- snnsObject$serializeNet("RSNNS_untitled")</code>
</p>
<p><code>snnsObject@variables$serialization &lt;- s$serialization</code>
</p>
<p>For the models implemented, this is done in <code><a href="#topic+SnnsRObject+24train">SnnsRObject$train</a></code>. If the S4 object is then saved and loaded, 
the calling mechanism will notice on the next use of a function that the pointer to the C++ SnnsCLib object is <code>nil</code>, 
and if a serialization is present, the object is restored from this serialization before the method is called.
</p>

<hr>
<h2 id='som'>Create and train a self-organizing map (SOM)</h2><span id='topic+som'></span><span id='topic+som.default'></span>

<h3>Description</h3>

<p>This function creates and trains a self-organizing map (SOM).
SOMs are neural networks with one hidden layer. 
The network structure is similar to LVQ, but the method is unsupervised 
and uses a notion of neighborhood between the units. 
The general idea is that the map develops by itself a notion of similarity among 
the input and represents this as spatial nearness on the map.
Every hidden unit represents a prototype. The goal of learning is to
distribute the prototypes in the feature space such that the (probability 
density of the) input is represented well.
SOMs are usually built with 1d, 2d quadratic, 2d hexagonal, or 3d 
neighborhood, so that they can be visualized straightforwardly.
The SOM implemented in SNNS has a 2d quadratic neighborhood.
</p>
<p>As the computation of this function might be slow if many patterns are involved,
much of its output is made switchable (see comments on return values).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>som(x, ...)

## Default S3 method:
som(
  x,
  mapX = 16,
  mapY = 16,
  maxit = 100,
  initFuncParams = c(1, -1),
  learnFuncParams = c(0.5, mapX/2, 0.8, 0.8, mapX),
  updateFuncParams = c(0, 0, 1),
  shufflePatterns = TRUE,
  calculateMap = TRUE,
  calculateActMaps = FALSE,
  calculateSpanningTree = FALSE,
  saveWinnersPerPattern = FALSE,
  targets = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="som_+3A_x">x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr><td><code id="som_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="som_+3A_mapx">mapX</code></td>
<td>
<p>the x dimension of the som</p>
</td></tr>
<tr><td><code id="som_+3A_mapy">mapY</code></td>
<td>
<p>the y dimension of the som</p>
</td></tr>
<tr><td><code id="som_+3A_maxit">maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr><td><code id="som_+3A_initfuncparams">initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr><td><code id="som_+3A_learnfuncparams">learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr><td><code id="som_+3A_updatefuncparams">updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr><td><code id="som_+3A_shufflepatterns">shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
<tr><td><code id="som_+3A_calculatemap">calculateMap</code></td>
<td>
<p>should the som be calculated?</p>
</td></tr>
<tr><td><code id="som_+3A_calculateactmaps">calculateActMaps</code></td>
<td>
<p>should the activation maps be calculated?</p>
</td></tr>
<tr><td><code id="som_+3A_calculatespanningtree">calculateSpanningTree</code></td>
<td>
<p>should the SNNS kernel algorithm for generating a spanning tree be applied?</p>
</td></tr>
<tr><td><code id="som_+3A_savewinnersperpattern">saveWinnersPerPattern</code></td>
<td>
<p>should a list with the winners for every pattern be saved?</p>
</td></tr>
<tr><td><code id="som_+3A_targets">targets</code></td>
<td>
<p>optional target classes of the patterns</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internally, this function uses the initialization function <code>Kohonen_Weights_v3.2</code>,
the learning function <code>Kohonen</code>, and the update function <code>Kohonen_Order</code> of
SNNS.
</p>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object. Depending on which calculation flags are 
switched on, the som generates some special members:
</p>
<table>
<tr><td><code>map</code></td>
<td>
<p>the som. For each unit, the amount of patterns where this unit won is given.</p>
</td></tr>
<tr><td><code>componentMaps</code></td>
<td>
<p>a map for every input component, showing where in the map this component leads to high activation.</p>
</td></tr>
<tr><td><code>actMaps</code></td>
<td>
<p>a list containing for each pattern its activation map, i.e. all unit activations.
The <code>actMaps</code> are an intermediary result, from which all other results can be computed. This list can be very long,
so normally it won't be saved.</p>
</td></tr>
<tr><td><code>winnersPerPattern</code></td>
<td>
<p>a vector where for each pattern the number of the winning unit is given. Also, an intermediary result
that normally won't be saved.</p>
</td></tr>
<tr><td><code>labeledUnits</code></td>
<td>
<p>a matrix which &ndash; if the <code>targets</code> parameter is given &ndash; contains for each unit (rows) and each class 
present in the <code>targets</code> (columns), the amount of patterns of the class where the unit has won. From the <code>labeledUnits</code>, 
the <code>labeledMap</code> can be computed, e.g. by voting of the class labels for the final label of the unit.</p>
</td></tr>  
<tr><td><code>labeledMap</code></td>
<td>
<p>a labeled som that is computed from <code>labeledUnits</code> using <code><a href="#topic+decodeClassLabels">decodeClassLabels</a></code>.</p>
</td></tr>
<tr><td><code>spanningTree</code></td>
<td>
<p>the result of the original SNNS function to calculate the map. For each unit, the last pattern where this unit won is present.
As the other results are more informative, the spanning tree is only interesting, if the other functions are too slow or if the original SNNS
implementation is needed.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kohonen, T. (1988), Self-organization and associative memory, Vol. 8, Springer-Verlag.
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen. 
<a href="https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html">https://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: demo(som_iris)
## Not run: demo(som_cubeSnnsR)


data(iris)
inputs &lt;- normalizeData(iris[,1:4], "norm")

model &lt;- som(inputs, mapX=16, mapY=16, maxit=500,  
                calculateActMaps=TRUE, targets=iris[,5])

par(mfrow=c(3,3))
for(i in 1:ncol(inputs)) plotActMap(model$componentMaps[[i]], 
                                       col=rev(topo.colors(12)))

plotActMap(model$map, col=rev(heat.colors(12)))
plotActMap(log(model$map+1), col=rev(heat.colors(12)))
persp(1:model$archParams$mapX, 1:model$archParams$mapY, log(model$map+1), 
     theta = 30, phi = 30, expand = 0.5, col = "lightblue")

plotActMap(model$labeledMap)

model$componentMaps
model$labeledUnits
model$map

names(model)
</code></pre>

<hr>
<h2 id='splitForTrainingAndTest'>Function to split data into training and test set</h2><span id='topic+splitForTrainingAndTest'></span>

<h3>Description</h3>

<p>Split the input and target values to a training and a test set. Test set is taken from the end of the
data. If the data is to be shuffled, this should be done before calling this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitForTrainingAndTest(x, y, ratio = 0.15)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="splitForTrainingAndTest_+3A_x">x</code></td>
<td>
<p>inputs</p>
</td></tr>
<tr><td><code id="splitForTrainingAndTest_+3A_y">y</code></td>
<td>
<p>targets</p>
</td></tr>
<tr><td><code id="splitForTrainingAndTest_+3A_ratio">ratio</code></td>
<td>
<p>ratio of training and test sets (default: 15% of the data is used for testing)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with the following elements:
</p>
<table>
<tr><td><code>inputsTrain</code></td>
<td>
<p>a matrix containing the training inputs</p>
</td></tr>
<tr><td><code>targetsTrain</code></td>
<td>
<p>a matrix containing the training targets</p>
</td></tr>
<tr><td><code>inputsTest</code></td>
<td>
<p>a matrix containing the test inputs</p>
</td></tr>
<tr><td><code>targetsTest</code></td>
<td>
<p>a matrix containing the test targets</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
#shuffle the vector
iris &lt;- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]

irisValues &lt;- iris[,1:4]
irisTargets &lt;- decodeClassLabels(iris[,5])

splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)
</code></pre>

<hr>
<h2 id='summary.rsnns'>Generic summary function for rsnns objects</h2><span id='topic+summary.rsnns'></span>

<h3>Description</h3>

<p>Prints out a summary of the network. The printed information can be either 
all information of the network in the original SNNS file format,
or the information given by <code><a href="#topic+extractNetInfo">extractNetInfo</a></code>.
This behaviour is controlled with the parameter <code>origSnnsFormat</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rsnns'
summary(object, origSnnsFormat = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.rsnns_+3A_object">object</code></td>
<td>
<p>the <code><a href="#topic+rsnns">rsnns</a></code> object</p>
</td></tr>
<tr><td><code id="summary.rsnns_+3A_origsnnsformat">origSnnsFormat</code></td>
<td>
<p>show data in SNNS's original format in which networks are saved, or show output of <code><a href="#topic+extractNetInfo">extractNetInfo</a></code></p>
</td></tr>
<tr><td><code id="summary.rsnns_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either the contents of the .net file that SNNS would generate from 
the object, as a string. Or the output of <code><a href="#topic+extractNetInfo">extractNetInfo</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+extractNetInfo">extractNetInfo</a></code>
</p>

<hr>
<h2 id='toNumericClassLabels'>Convert a vector (of class labels) to a numeric vector</h2><span id='topic+toNumericClassLabels'></span>

<h3>Description</h3>

<p>This function converts a vector (of class labels) to a numeric vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toNumericClassLabels(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="toNumericClassLabels_+3A_x">x</code></td>
<td>
<p>inputs</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the vector converted to a numeric vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
toNumericClassLabels(iris[,5])
</code></pre>

<hr>
<h2 id='train'>Internal generic train function for rsnns objects</h2><span id='topic+train'></span><span id='topic+train.rsnns'></span>

<h3>Description</h3>

<p>The function calls <code><a href="#topic+SnnsRObject+24train">SnnsRObject$train</a></code> and saves the result in the
current <code><a href="#topic+rsnns">rsnns</a></code> object. This function is used internally by the 
models (e.g. <code><a href="#topic+mlp">mlp</a></code>) for training. Unless you are not about to implement
a new model on the S3 layer you most probably don't want to use this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train(object, ...)

## S3 method for class 'rsnns'
train(
  object,
  inputsTrain,
  targetsTrain = NULL,
  inputsTest = NULL,
  targetsTest = NULL,
  serializeTrainedObject = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_+3A_object">object</code></td>
<td>
<p>the <code><a href="#topic+rsnns">rsnns</a></code> object</p>
</td></tr>
<tr><td><code id="train_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr><td><code id="train_+3A_inputstrain">inputsTrain</code></td>
<td>
<p>training input</p>
</td></tr>
<tr><td><code id="train_+3A_targetstrain">targetsTrain</code></td>
<td>
<p>training targets</p>
</td></tr>
<tr><td><code id="train_+3A_inputstest">inputsTest</code></td>
<td>
<p>test input</p>
</td></tr>
<tr><td><code id="train_+3A_targetstest">targetsTest</code></td>
<td>
<p>test targets</p>
</td></tr>
<tr><td><code id="train_+3A_serializetrainedobject">serializeTrainedObject</code></td>
<td>
<p>parameter passed to <code><a href="#topic+SnnsRObject+24train">SnnsRObject$train</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code><a href="#topic+rsnns">rsnns</a></code> object, to which the results of training have been added.
</p>

<hr>
<h2 id='vectorToActMap'>Convert a vector to an activation map</h2><span id='topic+vectorToActMap'></span>

<h3>Description</h3>

<p>Organize network activation as 2d map.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vectorToActMap(v, nrow = 0, ncol = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vectorToActMap_+3A_v">v</code></td>
<td>
<p>the vector containing the activation pattern</p>
</td></tr>
<tr><td><code id="vectorToActMap_+3A_nrow">nrow</code></td>
<td>
<p>number of rows the resulting matrices will have</p>
</td></tr>
<tr><td><code id="vectorToActMap_+3A_ncol">ncol</code></td>
<td>
<p>number of columns the resulting matrices will have</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The input to this function is a vector containing in each row an activation
pattern/output of a neural network. This function reorganizes the vector to
a matrix. Normally, only the number of rows <code>nrow</code> will be used.
</p>


<h3>Value</h3>

<p>a matrix containing the 2d reorganized input
</p>


<h3>See Also</h3>

<p><a href="#topic+matrixToActMapList">matrixToActMapList</a> <a href="#topic+plotActMap">plotActMap</a>
</p>

<hr>
<h2 id='weightMatrix'>Function to extract the weight matrix of an rsnns object</h2><span id='topic+weightMatrix'></span><span id='topic+weightMatrix.rsnns'></span>

<h3>Description</h3>

<p>The function calls <code><a href="#topic+SnnsRObject+24getCompleteWeightMatrix">SnnsRObject$getCompleteWeightMatrix</a></code> and returns its result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightMatrix(object, ...)

## S3 method for class 'rsnns'
weightMatrix(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightMatrix_+3A_object">object</code></td>
<td>
<p>the <code><a href="#topic+rsnns">rsnns</a></code> object</p>
</td></tr>
<tr><td><code id="weightMatrix_+3A_...">...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with all weights from all neurons present in the net.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
