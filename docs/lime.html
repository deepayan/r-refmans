<!DOCTYPE html><html><head><title>Help for package lime</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {lime}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.load_image_example'><p>Load an example image explanation</p></a></li>
<li><a href='#.load_text_example'><p>Load an example text explanation</p></a></li>
<li><a href='#as_classifier'><p>Indicate model type to lime</p></a></li>
<li><a href='#default_tokenize'><p>Default function to tokenize</p></a></li>
<li><a href='#explain'><p>Explain model predictions</p></a></li>
<li><a href='#interactive_text_explanations'><p>Interactive explanations</p></a></li>
<li><a href='#lime'><p>Create a model explanation function based on training data</p></a></li>
<li><a href='#lime-package'><p>lime: Local Interpretable Model-Agnostic Explanations</p></a></li>
<li><a href='#model_support'><p>Methods for extending limes model support</p></a></li>
<li><a href='#plot_explanations'><p>Plot a condensed overview of all explanations</p></a></li>
<li><a href='#plot_features'><p>Plot the features in an explanation</p></a></li>
<li><a href='#plot_image_explanation'><p>Display image explanations as superpixel areas</p></a></li>
<li><a href='#plot_superpixels'><p>Test super pixel segmentation</p></a></li>
<li><a href='#plot_text_explanations'><p>Plot text explanations</p></a></li>
<li><a href='#slic'><p>Segment image into superpixels</p></a></li>
<li><a href='#stop_words_sentences'><p>Stop words list</p></a></li>
<li><a href='#test_sentences'><p>Sentence corpus - test part</p></a></li>
<li><a href='#train_sentences'><p>Sentence corpus - train part</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Local Interpretable Model-Agnostic Explanations</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.3</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Emil Hvitfeldt &lt;emilhhvitfeldt@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>When building complex models, it is often difficult to explain why
    the model should be trusted. While global measures such as accuracy are
    useful, they cannot be used for explaining why a model made a specific
    prediction. 'lime' (a port of the 'lime' 'Python' package) is a method for
    explaining the outcome of black box models by fitting a local model around
    the point in question an perturbations of this point. The approach is
    described in more detail in the article by Ribeiro et al. (2016) 
    &lt;<a href="https://arxiv.org/abs/1602.04938">arXiv:1602.04938</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://lime.data-imaginist.com">https://lime.data-imaginist.com</a>, <a href="https://github.com/thomasp85/lime">https://github.com/thomasp85/lime</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/thomasp85/lime/issues">https://github.com/thomasp85/lime/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet, stats, ggplot2, tools, stringi, Matrix, Rcpp,
assertthat, methods, grDevices, gower</td>
</tr>
<tr>
<td>Suggests:</td>
<td>xgboost, testthat, mlr, h2o, text2vec, MASS, covr, knitr,
rmarkdown, sessioninfo, magick, keras, htmlwidgets, shiny,
shinythemes, ranger</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-08-18 12:46:04 UTC; thomas</td>
</tr>
<tr>
<td>Author:</td>
<td>Emil Hvitfeldt <a href="https://orcid.org/0000-0002-0679-1945"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Thomas Lin Pedersen
    <a href="https://orcid.org/0000-0002-5147-4711"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  MichaÃ«l Benesty [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-08-19 08:50:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='.load_image_example'>Load an example image explanation</h2><span id='topic+.load_image_example'></span>

<h3>Description</h3>

<p>This function is needed to cut down on package size. It reassembles the
explanation data.frame by attaching the image data to the saved data.frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.load_image_example()
</code></pre>


<h3>Value</h3>

<p>A data.frame containing an example of a image explanation
</p>

<hr>
<h2 id='.load_text_example'>Load an example text explanation</h2><span id='topic+.load_text_example'></span>

<h3>Description</h3>

<p>Load an example text explanation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.load_text_example()
</code></pre>


<h3>Value</h3>

<p>A data.frame containing an example of a text explanation
</p>

<hr>
<h2 id='as_classifier'>Indicate model type to lime</h2><span id='topic+as_classifier'></span><span id='topic+as_regressor'></span>

<h3>Description</h3>

<p><code>lime</code> requires knowledge about the type of model it is dealing with, more
specifically whether the model is a regressor or a classifier. If the model
class has a <code><a href="#topic+model_type">model_type()</a></code> method defined lime can figure it out on its own
but if not, you can wrap your model in either of these functions to indicate
what type of model lime is dealing with. This can also be used to overwrite
the output from <code><a href="#topic+model_type">model_type()</a></code> if the implementation uses some heuristic that
doesn't work for your particular model (e.g. keras models types are found by
checking if the activation in the last layer is linear or not - this is
rather crude). In addition <code>as_classifier</code> can be used to overwrite the
returned class labels - this is handy if the model does not store the labels
(again, keras springs to mind).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_classifier(x, labels = NULL)

as_regressor(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_classifier_+3A_x">x</code></td>
<td>
<p>The model object</p>
</td></tr>
<tr><td><code id="as_classifier_+3A_labels">labels</code></td>
<td>
<p>An optional character vector giving labels for each class</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A model augmented with information about the model type and
(potentially) the class labels.
</p>

<hr>
<h2 id='default_tokenize'>Default function to tokenize</h2><span id='topic+default_tokenize'></span>

<h3>Description</h3>

<p>This tokenizer uses <code><a href="stringi.html#topic+stri_split_boundaries">stringi::stri_split_boundaries()</a></code> to tokenize a
<code>character</code> vector. To be used with [explain.character()'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_tokenize(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="default_tokenize_+3A_text">text</code></td>
<td>
<p>text to tokenize as a <code>character</code> vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>character</code> vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('train_sentences')
default_tokenize(train_sentences$text[1])

</code></pre>

<hr>
<h2 id='explain'>Explain model predictions</h2><span id='topic+explain'></span><span id='topic+explain.data.frame'></span><span id='topic+explain.character'></span><span id='topic+explain.imagefile'></span>

<h3>Description</h3>

<p>Once an explainer has been created using the <code><a href="#topic+lime">lime()</a></code> function it can be used
to explain the result of the model on new observations. The <code>explain()</code>
function takes new observation along with the explainer and returns a
data.frame with prediction explanations, one observation per row. The
returned explanations can then be visualised in a number of ways, e.g. with
<code><a href="#topic+plot_features">plot_features()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'data.frame'
explain(
  x,
  explainer,
  labels = NULL,
  n_labels = NULL,
  n_features,
  n_permutations = 5000,
  feature_select = "auto",
  dist_fun = "gower",
  kernel_width = NULL,
  gower_pow = 1,
  ...
)

## S3 method for class 'character'
explain(
  x,
  explainer,
  labels = NULL,
  n_labels = NULL,
  n_features,
  n_permutations = 5000,
  feature_select = "auto",
  single_explanation = FALSE,
  ...
)

explain(
  x,
  explainer,
  labels,
  n_labels = NULL,
  n_features,
  n_permutations = 5000,
  feature_select = "auto",
  ...
)

## S3 method for class 'imagefile'
explain(
  x,
  explainer,
  labels = NULL,
  n_labels = NULL,
  n_features,
  n_permutations = 1000,
  feature_select = "auto",
  n_superpixels = 50,
  weight = 20,
  n_iter = 10,
  p_remove = 0.5,
  batch_size = 10,
  background = "grey",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="explain_+3A_x">x</code></td>
<td>
<p>New observations to explain, of the same format as used when
creating the explainer</p>
</td></tr>
<tr><td><code id="explain_+3A_explainer">explainer</code></td>
<td>
<p>An <code>explainer</code> object to use for explaining the observations</p>
</td></tr>
<tr><td><code id="explain_+3A_labels">labels</code></td>
<td>
<p>The specific labels (classes) to explain in case the model is
a classifier. For classifiers either this or <code>n_labels</code> must be given.</p>
</td></tr>
<tr><td><code id="explain_+3A_n_labels">n_labels</code></td>
<td>
<p>The number of labels to explain. If this is given for
classifiers the top <code>n_label</code> classes will be explained.</p>
</td></tr>
<tr><td><code id="explain_+3A_n_features">n_features</code></td>
<td>
<p>The number of features to use for each explanation.</p>
</td></tr>
<tr><td><code id="explain_+3A_n_permutations">n_permutations</code></td>
<td>
<p>The number of permutations to use for each explanation.</p>
</td></tr>
<tr><td><code id="explain_+3A_feature_select">feature_select</code></td>
<td>
<p>The algorithm to use for selecting features. One of:
</p>

<ul>
<li> <p><code>"auto"</code>: If <code>n_features &lt;= 6</code> use <code>"forward_selection"</code> else use <code>"highest_weights"</code>.
</p>
</li>
<li> <p><code>"none"</code>: Ignore <code>n_features</code> and use all features.
</p>
</li>
<li> <p><code>"forward_selection"</code>: Add one feature at a time until <code>n_features</code> is
reached, based on quality of a ridge regression model.
</p>
</li>
<li> <p><code>"highest_weights"</code>: Fit a ridge regression and select the <code>n_features</code> with
the highest absolute weight.
</p>
</li>
<li> <p><code>"lasso_path"</code>: Fit a lasso model and choose the <code>n_features</code> whose lars
path converge to zero the latest.
</p>
</li>
<li> <p><code>"tree"</code> : Fit a tree to select <code>n_features</code> (which needs to be a power of
2). It requires last version of <code>XGBoost</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="explain_+3A_dist_fun">dist_fun</code></td>
<td>
<p>The distance function to use for calculating the distance
from the observation to the permutations. If <code>dist_fun = 'gower'</code> (default)
it will use <code><a href="gower.html#topic+gower_dist">gower::gower_dist()</a></code>. Otherwise it will be forwarded to
<code><a href="stats.html#topic+dist">stats::dist()</a></code></p>
</td></tr>
<tr><td><code id="explain_+3A_kernel_width">kernel_width</code></td>
<td>
<p>The width of the exponential kernel that will be used to
convert the distance to a similarity in case <code>dist_fun != 'gower'</code>.</p>
</td></tr>
<tr><td><code id="explain_+3A_gower_pow">gower_pow</code></td>
<td>
<p>A modifier for gower distance. The calculated distance will
be raised to the power of this value.</p>
</td></tr>
<tr><td><code id="explain_+3A_...">...</code></td>
<td>
<p>Parameters passed on to the <code>predict_model()</code> method</p>
</td></tr>
<tr><td><code id="explain_+3A_single_explanation">single_explanation</code></td>
<td>
<p>A boolean indicating whether to pool all text in
<code>x</code> into a single explanation.</p>
</td></tr>
<tr><td><code id="explain_+3A_n_superpixels">n_superpixels</code></td>
<td>
<p>The number of segments an image should be split into</p>
</td></tr>
<tr><td><code id="explain_+3A_weight">weight</code></td>
<td>
<p>How high should locality be weighted compared to colour. High
values leads to more compact superpixels, while low values follow the image
structure more</p>
</td></tr>
<tr><td><code id="explain_+3A_n_iter">n_iter</code></td>
<td>
<p>How many iterations should the segmentation run for</p>
</td></tr>
<tr><td><code id="explain_+3A_p_remove">p_remove</code></td>
<td>
<p>The probability that a superpixel will be removed in each
permutation</p>
</td></tr>
<tr><td><code id="explain_+3A_batch_size">batch_size</code></td>
<td>
<p>The number of explanations to handle at a time</p>
</td></tr>
<tr><td><code id="explain_+3A_background">background</code></td>
<td>
<p>The colour to use for blocked out superpixels</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame encoding the explanations one row per explained
observation. The columns are:
</p>

<ul>
<li> <p><code>model_type</code>: The type of the model used for prediction.
</p>
</li>
<li> <p><code>case</code>: The case being explained (the rowname in <code>cases</code>).
</p>
</li>
<li> <p><code>model_r2</code>: The quality of the model used for the explanation
</p>
</li>
<li> <p><code>model_intercept</code>: The intercept of the model used for the explanation
</p>
</li>
<li> <p><code>model_prediction</code>: The prediction of the observation based on the model
used for the explanation.
</p>
</li>
<li> <p><code>feature</code>: The feature used for the explanation
</p>
</li>
<li> <p><code>feature_value</code>: The value of the feature used
</p>
</li>
<li> <p><code>feature_weight</code>: The weight of the feature in the explanation
</p>
</li>
<li> <p><code>feature_desc</code>: A human readable description of the feature importance.
</p>
</li>
<li> <p><code>data</code>: Original data being explained
</p>
</li>
<li> <p><code>prediction</code>: The original prediction from the model
</p>
</li></ul>

<p>Furthermore classification explanations will also contain:
</p>

<ul>
<li> <p><code>label</code>: The label being explained
</p>
</li>
<li> <p><code>label_prob</code>: The probability of <code>label</code> as predicted by <code>model</code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Explaining a model and an explainer for it
library(MASS)
iris_test &lt;- iris[1, 1:4]
iris_train &lt;- iris[-1, 1:4]
iris_lab &lt;- iris[[5]][-1]
model &lt;- lda(iris_train, iris_lab)
explanation &lt;- lime(iris_train, model)

# This can now be used together with the explain method
explain(iris_test, explanation, n_labels = 1, n_features = 2)

</code></pre>

<hr>
<h2 id='interactive_text_explanations'>Interactive explanations</h2><span id='topic+interactive_text_explanations'></span><span id='topic+text_explanations_output'></span><span id='topic+render_text_explanations'></span>

<h3>Description</h3>

<p>Display text explanation in an interactive way.
You can :
</p>
<p>Create an output to insert text explanation plot in Shiny application.
</p>
<p>Render the text explanations in Shiny application.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interactive_text_explanations(
  explainer,
  window_title = "Text model explainer",
  title = "Local Interpretable Model-agnostic Explanations",
  place_holder = "Put here the text to explain",
  minimum_lentgh = 3,
  minimum_lentgh_error = "Text provided is too short to be explained (&gt;= 3).",
  max_feature_to_select = 20
)

text_explanations_output(outputId, width = "100%", height = "400px")

render_text_explanations(expr, env = parent.frame(), quoted = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interactive_text_explanations_+3A_explainer">explainer</code></td>
<td>
<p>parameters</p>
</td></tr>
<tr><td><code id="interactive_text_explanations_+3A_window_title">window_title</code>, <code id="interactive_text_explanations_+3A_title">title</code>, <code id="interactive_text_explanations_+3A_place_holder">place_holder</code>, <code id="interactive_text_explanations_+3A_minimum_lentgh_error">minimum_lentgh_error</code></td>
<td>
<p>text to be displayed on the page</p>
</td></tr>
<tr><td><code id="interactive_text_explanations_+3A_minimum_lentgh">minimum_lentgh</code></td>
<td>
<p>don't update display if text is shorter than this parameter</p>
</td></tr>
<tr><td><code id="interactive_text_explanations_+3A_max_feature_to_select">max_feature_to_select</code></td>
<td>
<p>up limit to the number of words that can be selected</p>
</td></tr>
<tr><td><code id="interactive_text_explanations_+3A_outputid">outputId</code></td>
<td>
<p>output variable to read from</p>
</td></tr>
<tr><td><code id="interactive_text_explanations_+3A_width">width</code>, <code id="interactive_text_explanations_+3A_height">height</code></td>
<td>
<p>Must be a valid CSS unit or a number, which will be coerced to a string and have &quot;px&quot; appended.</p>
</td></tr>
<tr><td><code id="interactive_text_explanations_+3A_expr">expr</code></td>
<td>
<p>An expression that generates an HTML widget</p>
</td></tr>
<tr><td><code id="interactive_text_explanations_+3A_env">env</code></td>
<td>
<p>The environment in which to evaluate <code>expr</code>.</p>
</td></tr>
<tr><td><code id="interactive_text_explanations_+3A_quoted">quoted</code></td>
<td>
<p>Is <code>expr</code> a quoted expression (with <code><a href="base.html#topic+quote">quote()</a></code>)? This
is useful if you want to save an expression in a variable.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> send a new sentence
</p>
</li>
<li><p> update the parameters of the explainer
</p>
</li></ul>



<h3>Value</h3>

<p>An output function that enables the use of the widget within Shiny applications.
</p>
<p>A render function that enables the use of the widget within Shiny applications.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(text2vec)
library(xgboost)

data(train_sentences)
data(test_sentences)

get_matrix &lt;- function(text) {
  it &lt;- itoken(text, progressbar = FALSE)
  create_dtm(it, vectorizer = hash_vectorizer())
}

dtm_train = get_matrix(train_sentences$text)

xgb_model &lt;- xgb.train(list(max_depth = 7, eta = 0.1, objective = "binary:logistic",
                 eval_metric = "error", nthread = 1),
                 xgb.DMatrix(dtm_train, label = train_sentences$class.text == "OWNX"),
                 nrounds = 50)

sentences &lt;- head(test_sentences[test_sentences$class.text == "OWNX", "text"], 1)
explainer &lt;- lime(train_sentences$text, xgb_model, get_matrix)

# The explainer can now be queried interactively:
interactive_text_explanations(explainer)

## End(Not run)
</code></pre>

<hr>
<h2 id='lime'>Create a model explanation function based on training data</h2><span id='topic+lime'></span><span id='topic+lime.data.frame'></span><span id='topic+lime.character'></span><span id='topic+lime.imagefile'></span>

<h3>Description</h3>

<p>This is the main function of the <code>lime</code> package. It is a factory function
that returns a new function that can be used to explain the predictions made
by black box models. This is a generic with methods for the different data
types supported by lime.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'data.frame'
lime(
  x,
  model,
  preprocess = NULL,
  bin_continuous = TRUE,
  n_bins = 4,
  quantile_bins = TRUE,
  use_density = TRUE,
  ...
)

## S3 method for class 'character'
lime(
  x,
  model,
  preprocess = NULL,
  tokenization = default_tokenize,
  keep_word_position = FALSE,
  ...
)

## S3 method for class 'imagefile'
lime(x, model, preprocess = NULL, ...)

lime(x, model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lime_+3A_x">x</code></td>
<td>
<p>The training data used for training the model that should be
explained.</p>
</td></tr>
<tr><td><code id="lime_+3A_model">model</code></td>
<td>
<p>The model whose output should be explained</p>
</td></tr>
<tr><td><code id="lime_+3A_preprocess">preprocess</code></td>
<td>
<p>Function to transform a <code>character</code> vector to the format
expected from the model.</p>
</td></tr>
<tr><td><code id="lime_+3A_bin_continuous">bin_continuous</code></td>
<td>
<p>Should continuous variables be binned when making the explanation</p>
</td></tr>
<tr><td><code id="lime_+3A_n_bins">n_bins</code></td>
<td>
<p>The number of bins for continuous variables if <code>bin_continuous = TRUE</code></p>
</td></tr>
<tr><td><code id="lime_+3A_quantile_bins">quantile_bins</code></td>
<td>
<p>Should the bins be based on <code>n_bins</code> quantiles or spread evenly over the range of the training data</p>
</td></tr>
<tr><td><code id="lime_+3A_use_density">use_density</code></td>
<td>
<p>If <code>bin_continuous = FALSE</code> should continuous data be sampled using a kernel density estimation. If not, continuous features are expected to follow a normal distribution.</p>
</td></tr>
<tr><td><code id="lime_+3A_...">...</code></td>
<td>
<p>Arguments passed on to methods</p>
</td></tr>
<tr><td><code id="lime_+3A_tokenization">tokenization</code></td>
<td>
<p>function used to tokenize text for the permutations.</p>
</td></tr>
<tr><td><code id="lime_+3A_keep_word_position">keep_word_position</code></td>
<td>
<p>set to <code>TRUE</code> if to keep order of words. Warning:
each word will be replaced by <code>word_position</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Return an explainer which can be used together with <code><a href="#topic+explain">explain()</a></code> to
explain model predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Explaining a model based on tabular data
library(MASS)
iris_test &lt;- iris[1, 1:4]
iris_train &lt;- iris[-1, 1:4]
iris_lab &lt;- iris[[5]][-1]
# Create linear discriminant model on iris data
model &lt;- lda(iris_train, iris_lab)
# Create explanation object
explanation &lt;- lime(iris_train, model)

# This can now be used together with the explain method
explain(iris_test, explanation, n_labels = 1, n_features = 2)

## Not run: 
# Explaining a model based on text data

# Purpose is to classify sentences from scientific publications
# and find those where the team writes about their own work
# (category OWNX in the provided dataset).

library(text2vec)
library(xgboost)

data(train_sentences)
data(test_sentences)

get_matrix &lt;- function(text) {
  it &lt;- itoken(text, progressbar = FALSE)
  create_dtm(it, vectorizer = hash_vectorizer())
}

dtm_train = get_matrix(train_sentences$text)

xgb_model &lt;- xgb.train(list(max_depth = 7, eta = 0.1, objective = "binary:logistic",
                 eval_metric = "error", nthread = 1),
                 xgb.DMatrix(dtm_train, label = train_sentences$class.text == "OWNX"),
                 nrounds = 50)

sentences &lt;- head(test_sentences[test_sentences$class.text == "OWNX", "text"], 1)
explainer &lt;- lime(train_sentences$text, xgb_model, get_matrix)
explanations &lt;- explain(sentences, explainer, n_labels = 1, n_features = 2)

# We can see that many explanations are based
# on the presence of the word `we` in the sentences
# which makes sense regarding the task.
print(explanations)

## End(Not run)
## Not run: 
library(keras)
library(abind)
# get some image
img_path &lt;- system.file('extdata', 'produce.png', package = 'lime')
# load a predefined image classifier
model &lt;- application_vgg16(
  weights = "imagenet",
  include_top = TRUE
)

# create a function that prepare images for the model
img_preprocess &lt;- function(x) {
  arrays &lt;- lapply(x, function(path) {
    img &lt;- image_load(path, target_size = c(224,224))
    x &lt;- image_to_array(img)
    x &lt;- array_reshape(x, c(1, dim(x)))
    x &lt;- imagenet_preprocess_input(x)
  })
  do.call(abind, c(arrays, list(along = 1)))
}

# Create an explainer (lime recognise the path as an image)
explainer &lt;- lime(img_path, as_classifier(model, unlist(labels)), img_preprocess)

# Explain the model (can take a long time depending on your system)
explanation &lt;- explain(img_path, explainer, n_labels = 2, n_features = 10, n_superpixels = 70)

## End(Not run)
</code></pre>

<hr>
<h2 id='lime-package'>lime: Local Interpretable Model-Agnostic Explanations</h2><span id='topic+lime-package'></span><span id='topic+_PACKAGE'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>When building complex models, it is often difficult to explain why the model should be trusted. While global measures such as accuracy are useful, they cannot be used for explaining why a model made a specific prediction. 'lime' (a port of the 'lime' 'Python' package) is a method for explaining the outcome of black box models by fitting a local model around the point in question an perturbations of this point. The approach is described in more detail in the article by Ribeiro et al. (2016) <a href="https://arxiv.org/abs/1602.04938">arXiv:1602.04938</a>.
</p>


<h3>Details</h3>

<p>This package is a port of the original Python lime package implementing the
prediction explanation framework laid out Ribeiro <em>et al.</em> (2016). The
package supports models from <code>caret</code> and <code>mlr</code> natively, but see
<a href="#topic+model_support">the docs</a> for how to make it work for any model.
</p>
<p><strong>Main functions:</strong>
</p>
<p>Use of <code>lime</code> is mainly through two functions. First you create an
<code>explainer</code> object using the <code><a href="#topic+lime">lime()</a></code> function based on the training data and
the model, and then you can use the <code><a href="#topic+explain">explain()</a></code> function along with new data
and the explainer to create explanations for the model output.
</p>
<p>Along with these two functions, <code>lime</code> also provides the <code><a href="#topic+plot_features">plot_features()</a></code>
and <code><a href="#topic+plot_text_explanations">plot_text_explanations()</a></code> function to visualise the explanations
directly.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Emil Hvitfeldt <a href="mailto:emilhhvitfeldt@gmail.com">emilhhvitfeldt@gmail.com</a> (<a href="https://orcid.org/0000-0002-0679-1945">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Thomas Lin Pedersen <a href="mailto:thomasp85@gmail.com">thomasp85@gmail.com</a> (<a href="https://orcid.org/0000-0002-5147-4711">ORCID</a>)
</p>
</li>
<li><p> MichaÃ«l Benesty <a href="mailto:michael@benesty.fr">michael@benesty.fr</a>
</p>
</li></ul>



<h3>References</h3>

<p>Ribeiro, M.T., Singh, S., Guestrin, C. <em>&quot;Why Should I Trust You?&quot;: Explaining the Predictions of Any Classifier</em>. 2016, <a href="https://arxiv.org/abs/1602.04938">https://arxiv.org/abs/1602.04938</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://lime.data-imaginist.com">https://lime.data-imaginist.com</a>
</p>
</li>
<li> <p><a href="https://github.com/thomasp85/lime">https://github.com/thomasp85/lime</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/thomasp85/lime/issues">https://github.com/thomasp85/lime/issues</a>
</p>
</li></ul>


<hr>
<h2 id='model_support'>Methods for extending limes model support</h2><span id='topic+model_support'></span><span id='topic+predict_model'></span><span id='topic+model_type'></span>

<h3>Description</h3>

<p>In order to have <code>lime</code> support for your model of choice <code>lime</code> needs to be
able to get predictions from the model in a standardised way, and it needs to
be able to know whether it is a classification or regression model. For the
former it calls the <code>predict_model()</code> generic which the user is free to
supply methods for without overriding the standard <code>predict()</code> method. For
the latter the model must respond to the <code>model_type()</code> generic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_model(x, newdata, type, ...)

model_type(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_support_+3A_x">x</code></td>
<td>
<p>A model object</p>
</td></tr>
<tr><td><code id="model_support_+3A_newdata">newdata</code></td>
<td>
<p>The new observations to predict</p>
</td></tr>
<tr><td><code id="model_support_+3A_type">type</code></td>
<td>
<p>Either <code>'raw'</code> to indicate predicted values, or <code>'prob'</code> to
indicate class probabilities</p>
</td></tr>
<tr><td><code id="model_support_+3A_...">...</code></td>
<td>
<p>passed on to <code>predict</code> method</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame in the case of <code>predict_model()</code>. If <code>type = 'raw'</code> it
will contain one column named <code>'Response'</code> holding the predicted values. If
<code>type = 'prob'</code> it will contain a column for each of the possible classes
named after the class, each column holding the probability score for class
membership. For <code>model_type()</code> a character string. Either <code>'regression'</code> or
<code>'classification'</code> is currently supported.
</p>


<h3>Supported Models</h3>

<p>Out of the box, <code>lime</code> supports the following model objects:
</p>

<ul>
<li> <p><code>train</code> from caret
</p>
</li>
<li> <p><code>WrappedModel</code> from mlr
</p>
</li>
<li> <p><code>xgb.Booster</code> from xgboost
</p>
</li>
<li> <p><code>H2OModel</code> from h2o
</p>
</li>
<li> <p><code>keras.engine.training.Model</code> from keras
</p>
</li>
<li> <p><code>lda</code> from MASS (used for low-dependency examples)
</p>
</li></ul>

<p>If your model is not one of the above you'll need to implement support
yourself. If the model has a predict interface mimicking that of
<code>predict.train()</code> from <code>caret</code>, it will be enough to wrap your model in
<code><a href="#topic+as_classifier">as_classifier()</a></code>/<code><a href="#topic+as_regressor">as_regressor()</a></code> to gain support. Otherwise you'll need
need to implement a <code>predict_model()</code> method and potentially a <code>model_type()</code>
method (if the latter is omitted the model should be wrapped in
<code><a href="#topic+as_classifier">as_classifier()</a></code>/<code><a href="#topic+as_regressor">as_regressor()</a></code>, everytime it is used in <code><a href="#topic+lime">lime()</a></code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example of adding support for lda models (already available in lime)
predict_model.lda &lt;- function(x, newdata, type, ...) {
  res &lt;- predict(x, newdata = newdata, ...)
  switch(
    type,
    raw = data.frame(Response = res$class, stringsAsFactors = FALSE),
    prob = as.data.frame(res$posterior, check.names = FALSE)
  )
}

model_type.lda &lt;- function(x, ...) 'classification'

</code></pre>

<hr>
<h2 id='plot_explanations'>Plot a condensed overview of all explanations</h2><span id='topic+plot_explanations'></span>

<h3>Description</h3>

<p>This function produces a facetted heatmap visualisation of all
case/label/feature combinations. Compared to <code><a href="#topic+plot_features">plot_features()</a></code> it is much
more condensed, thus allowing for an overview of many explanations in one
plot. On the other hand it is less useful for getting exact numerical
statistics of the explanation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_explanations(explanation, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_explanations_+3A_explanation">explanation</code></td>
<td>
<p>A <code>data.frame</code> as returned by <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_explanations_+3A_...">...</code></td>
<td>
<p>Parameters passed on to <code><a href="ggplot2.html#topic+facet_wrap">ggplot2::facet_wrap()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot</code> object
</p>


<h3>See Also</h3>

<p>Other explanation plots: 
<code><a href="#topic+plot_features">plot_features</a>()</code>,
<code><a href="#topic+plot_text_explanations">plot_text_explanations</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create some explanations
library(MASS)
iris_test &lt;- iris[c(1, 51, 101), 1:4]
iris_train &lt;- iris[-c(1, 51, 101), 1:4]
iris_lab &lt;- iris[[5]][-c(1, 51, 101)]
model &lt;- lda(iris_train, iris_lab)
explanation &lt;- lime(iris_train, model)
explanations &lt;- explain(iris_test, explanation, n_labels = 1, n_features = 2)

# Get an overview with the standard plot
plot_explanations(explanations)
</code></pre>

<hr>
<h2 id='plot_features'>Plot the features in an explanation</h2><span id='topic+plot_features'></span>

<h3>Description</h3>

<p>This functions creates a compact visual representation of the explanations
for each case and label combination in an explanation. Each extracted feature
is shown with its weight, thus giving the importance of the feature in the
label prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_features(explanation, ncol = 2, cases = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_features_+3A_explanation">explanation</code></td>
<td>
<p>A <code>data.frame</code> as returned by <code><a href="#topic+explain">explain()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_features_+3A_ncol">ncol</code></td>
<td>
<p>The number of columns in the facetted plot</p>
</td></tr>
<tr><td><code id="plot_features_+3A_cases">cases</code></td>
<td>
<p>An optional vector with case names to plot. <code>explanation</code> will
be filtered to only include these cases prior to plotting</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot</code> object
</p>


<h3>See Also</h3>

<p>Other explanation plots: 
<code><a href="#topic+plot_explanations">plot_explanations</a>()</code>,
<code><a href="#topic+plot_text_explanations">plot_text_explanations</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create some explanations
library(MASS)
iris_test &lt;- iris[1, 1:4]
iris_train &lt;- iris[-1, 1:4]
iris_lab &lt;- iris[[5]][-1]
model &lt;- lda(iris_train, iris_lab)
explanation &lt;- lime(iris_train, model)
explanations &lt;- explain(iris_test, explanation, n_labels = 1, n_features = 2)

# Get an overview with the standard plot
plot_features(explanations)

</code></pre>

<hr>
<h2 id='plot_image_explanation'>Display image explanations as superpixel areas</h2><span id='topic+plot_image_explanation'></span>

<h3>Description</h3>

<p>When classifying images one is often interested in seeing the areas that
supports and/or contradicts a classification. <code>plot_image_explanation()</code> will
take the result of an image explanation and highlight the areas found
relevant to each label in the explanation. The highlighting can either be
done by blocking the parts of the image not related to the classification, or
by encircling and colouring the areas that influence the explanation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_image_explanation(
  explanation,
  which = 1,
  threshold = 0.02,
  show_negative = FALSE,
  display = "outline",
  fill_alpha = 0.3,
  outline_col = c("blue", "red"),
  block_col = "grey"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_image_explanation_+3A_explanation">explanation</code></td>
<td>
<p>The explanation created with an <code>image_explainer</code></p>
</td></tr>
<tr><td><code id="plot_image_explanation_+3A_which">which</code></td>
<td>
<p>The case in <code>explanation</code> to illustrate. <code>plot_image_explanation</code>
only supports showing one case at a time.</p>
</td></tr>
<tr><td><code id="plot_image_explanation_+3A_threshold">threshold</code></td>
<td>
<p>The lowest absolute weighted superpixels to include</p>
</td></tr>
<tr><td><code id="plot_image_explanation_+3A_show_negative">show_negative</code></td>
<td>
<p>Should areas that contradicts the prediction also be
shown</p>
</td></tr>
<tr><td><code id="plot_image_explanation_+3A_display">display</code></td>
<td>
<p>How should the areas be shown? Either <code>outline</code> or <code>block</code></p>
</td></tr>
<tr><td><code id="plot_image_explanation_+3A_fill_alpha">fill_alpha</code></td>
<td>
<p>In case of <code>display = 'outline'</code> how opaque should the area
colour be?</p>
</td></tr>
<tr><td><code id="plot_image_explanation_+3A_outline_col">outline_col</code></td>
<td>
<p>A vector of length 2 giving the colour for supporting and
contradicting areas respectively if <code>display = 'outline'</code></p>
</td></tr>
<tr><td><code id="plot_image_explanation_+3A_block_col">block_col</code></td>
<td>
<p>The colour to use for the unimportant areas if
<code>display = 'block'</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
# load precalculated explanation as it takes a long time to create
explanation &lt;- .load_image_example()

# Default
plot_image_explanation(explanation)

# Block out background instead
plot_image_explanation(explanation, display = 'block')

# Show negatively correlated areas as well
plot_image_explanation(explanation, show_negative = TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='plot_superpixels'>Test super pixel segmentation</h2><span id='topic+plot_superpixels'></span>

<h3>Description</h3>

<p>The segmentation of an image into superpixels are an important step in
generating explanations for image models. It is both important that the
segmentation is correct and follows meaningful patterns in the picture, but
also that the size/number of superpixels are appropriate. If the important
features in the image are chopped into too many segments the permutations
will probably damage the picture beyond recognition in almost all cases
leading to a poor or failing explanation model. As the size of the object of
interest is varying it is impossible to set up hard rules for the number of
superpixels to segment into - the larger the object is relative to the size
of the image, the fewer superpixels should be generated. Using
<code>plot_superpixels</code> it is possible to evaluate the superpixel parameters
before starting the time consuming explanation function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_superpixels(
  path,
  n_superpixels = 50,
  weight = 20,
  n_iter = 10,
  colour = "black"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_superpixels_+3A_path">path</code></td>
<td>
<p>The path to the image. Must be readable by <code><a href="magick.html#topic+editing">magick::image_read()</a></code></p>
</td></tr>
<tr><td><code id="plot_superpixels_+3A_n_superpixels">n_superpixels</code></td>
<td>
<p>The number of superpixels to segment into</p>
</td></tr>
<tr><td><code id="plot_superpixels_+3A_weight">weight</code></td>
<td>
<p>How high should locality be weighted compared to colour. High
values leads to more compact superpixels, while low values follow the image
structure more</p>
</td></tr>
<tr><td><code id="plot_superpixels_+3A_n_iter">n_iter</code></td>
<td>
<p>How many iterations should the segmentation run for</p>
</td></tr>
<tr><td><code id="plot_superpixels_+3A_colour">colour</code></td>
<td>
<p>What line colour should be used to show the segment boundaries</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>image &lt;- system.file('extdata', 'produce.png', package = 'lime')

# plot with default settings
plot_superpixels(image)

# Test different settings
plot_superpixels(image, n_superpixels = 100, colour = 'white')

</code></pre>

<hr>
<h2 id='plot_text_explanations'>Plot text explanations</h2><span id='topic+plot_text_explanations'></span>

<h3>Description</h3>

<p>Highlight words which explains a prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_text_explanations(explanations, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_text_explanations_+3A_explanations">explanations</code></td>
<td>
<p>object returned by the <a href="#topic+lime.character">lime.character</a> function.</p>
</td></tr>
<tr><td><code id="plot_text_explanations_+3A_...">...</code></td>
<td>
<p>parameters passed to <code>htmlwidgets::sizingPolicy()</code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other explanation plots: 
<code><a href="#topic+plot_explanations">plot_explanations</a>()</code>,
<code><a href="#topic+plot_features">plot_features</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># We load a precalculated explanation set based on the procedure in the ?lime
# examples
explanations &lt;- .load_text_example()

# We see that the explanations are in the expected format
print(explanations)

# We can now get the explanations in the context of the input text
plot_text_explanations(explanations)

</code></pre>

<hr>
<h2 id='slic'>Segment image into superpixels</h2><span id='topic+slic'></span>

<h3>Description</h3>

<p>This is an implementation of the SLIC superpixel algorithm for
segmenting images into connected similar patches. It is used by lime for
permuting image input but exported so that others might use it for other
things
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slic(L, a, b, n_sp, weight, n_iter)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slic_+3A_l">L</code>, <code id="slic_+3A_a">a</code>, <code id="slic_+3A_b">b</code></td>
<td>
<p>Raw matrices giving the L, a, and b component of each pixel in
the image to segment. The dimensions of all matrices must match.</p>
</td></tr>
<tr><td><code id="slic_+3A_n_sp">n_sp</code></td>
<td>
<p>The number of superpixels to segment the image into</p>
</td></tr>
<tr><td><code id="slic_+3A_weight">weight</code></td>
<td>
<p>A numeric giving the tradeoff between spatial and colour
distance. Higher values give more compact and heterogeneous superpixels,
while lower values will give superpixels of more irregular shape but with a
more homogeneous colour. Good values to start with is 10-20.</p>
</td></tr>
<tr><td><code id="slic_+3A_n_iter">n_iter</code></td>
<td>
<p>The number of iterations to run the algorithm for. The authors
suggest 10 and increasing it doesn't add much.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer matrix of the same dimensions as <code>L</code>, <code>a</code>, and <code>b</code>,
indexing each pixel into its corresponding superpixel
</p>

<hr>
<h2 id='stop_words_sentences'>Stop words list</h2><span id='topic+stop_words_sentences'></span>

<h3>Description</h3>

<p>List of words that can be safely removed from sentences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stop_words_sentences
</code></pre>


<h3>Format</h3>

<p>Character vector of stop words
</p>


<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/">https://archive.ics.uci.edu/ml/datasets/</a>
</p>

<hr>
<h2 id='test_sentences'>Sentence corpus - test part</h2><span id='topic+test_sentences'></span>

<h3>Description</h3>

<p>This corpus contains sentences from
the abstract and introduction of 30 scientific articles that have been
annotated (i.e. labeled or tagged) according to a modified version of the
Argumentative Zones annotation scheme.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_sentences
</code></pre>


<h3>Format</h3>

<p>2 data frame with 3117 rows and 2 variables:
</p>

<dl>
<dt>text</dt><dd><p>the sentences as a character vector</p>
</dd>
<dt>class.text</dt><dd><p>the category of the sentence</p>
</dd>
</dl>



<h3>Details</h3>

<p>These 30 scientific articles come
from three different domains:
</p>

<ol>
<li><p> PLoS Computational Biology (PLOS)
</p>
</li>
<li><p> The machine learning repository on arXiv (ARXIV)
</p>
</li>
<li><p> The psychology journal Judgment and Decision Making (JDM)
</p>
</li></ol>

<p>There are 10 articles from each domain. In addition to the labeled data, this
corpus also contains a corresponding set of unlabeled articles. These unlabeled
articles also come from PLOS, ARXIV, and JDM. There are 300 unlabeled articles
from each domain (again, only the sentences from the abstract and
introduction). These unlabeled articles can be used for unsupervised or
semi-supervised approaches to sentence classification which rely on a small set
of labeled data and a larger set of unlabeled data.
</p>
<p>===== References =====
</p>
<p>S. Teufel and M. Moens. Summarizing scientific articles: experiments with
relevance and rhetorical status. Computational Linguistics, 28(4):409-445,
2002.
</p>
<p>S. Teufel. Argumentative zoning: information extraction from scientific
text. PhD thesis, School of Informatics, University of Edinburgh, 1999.
</p>


<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/Sentence+Classification">https://archive.ics.uci.edu/ml/datasets/Sentence+Classification</a>
</p>

<hr>
<h2 id='train_sentences'>Sentence corpus - train part</h2><span id='topic+train_sentences'></span>

<h3>Description</h3>

<p>This corpus contains sentences from
the abstract and introduction of 30 scientific articles that have been
annotated (i.e. labeled or tagged) according to a modified version of the
Argumentative Zones annotation scheme.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_sentences
</code></pre>


<h3>Format</h3>

<p>2 data frame with 3117 rows and 2 variables:
</p>

<dl>
<dt>text</dt><dd><p>the sentences as a character vector</p>
</dd>
<dt>class.text</dt><dd><p>the category of the sentence</p>
</dd>
</dl>



<h3>Details</h3>

<p>These 30 scientific articles come
from three different domains:
</p>

<ol>
<li><p> PLoS Computational Biology (PLOS)
</p>
</li>
<li><p> The machine learning repository on arXiv (ARXIV)
</p>
</li>
<li><p> The psychology journal Judgment and Decision Making (JDM)
</p>
</li></ol>

<p>There are 10 articles from each domain. In addition to the labeled data, this
corpus also contains a corresponding set of unlabeled articles. These unlabeled
articles also come from PLOS, ARXIV, and JDM. There are 300 unlabeled articles
from each domain (again, only the sentences from the abstract and
introduction). These unlabeled articles can be used for unsupervised or
semi-supervised approaches to sentence classification which rely on a small set
of labeled data and a larger set of unlabeled data.
</p>
<p>===== References =====
</p>
<p>S. Teufel and M. Moens. Summarizing scientific articles: experiments with
relevance and rhetorical status. Computational Linguistics, 28(4):409-445,
2002.
</p>
<p>S. Teufel. Argumentative zoning: information extraction from scientific
text. PhD thesis, School of Informatics, University of Edinburgh, 1999.
</p>


<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/Sentence+Classification">https://archive.ics.uci.edu/ml/datasets/Sentence+Classification</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
