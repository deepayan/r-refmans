<!DOCTYPE html><html><head><title>Help for package tree.interpreter</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tree.interpreter}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#featureContribTree'><p>Feature Contribution</p></a></li>
<li><a href='#MDIoobTree'><p>Debiased Mean Decrease in Impurity</p></a></li>
<li><a href='#MDITree'><p>Mean Decrease in Impurity</p></a></li>
<li><a href='#tidyRF'><p>Tidy Random Forest</p></a></li>
<li><a href='#trainsetBiasTree'><p>Trainset Bias</p></a></li>
<li><a href='#tree.interpreter'><p>Random Forest Prediction Decomposition and Feature Importance Measure</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Random Forest Prediction Decomposition and Feature Importance
Measure</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-01-28</td>
</tr>
<tr>
<td>Author:</td>
<td>Qingyao Sun</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Qingyao Sun &lt;sunqingyao19970825@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An R re-implementation of the 'treeinterpreter' package on PyPI
        <a href="https://pypi.org/project/treeinterpreter/">https://pypi.org/project/treeinterpreter/</a>. Each prediction can be
        decomposed as 'prediction = bias + feature_1_contribution + ... +
        feature_n_contribution'. This decomposition is then used to calculate
        the Mean Decrease Impurity (MDI) and Mean Decrease Impurity using
        out-of-bag samples (MDI-oob) feature importance measures based on the
        work of Li et al. (2019) &lt;<a href="https://arxiv.org/abs/1906.10845">arXiv:1906.10845</a>&gt;.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.2)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, randomForest, ranger, testthat (&ge; 2.1.0), knitr,
rmarkdown, covr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/nalzok/tree.interpreter">https://github.com/nalzok/tree.interpreter</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/nalzok/tree.interpreter/issues">https://github.com/nalzok/tree.interpreter/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-01-28 21:58:01 UTC; nalzok</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-02-05 14:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='featureContribTree'>Feature Contribution</h2><span id='topic+featureContribTree'></span><span id='topic+featureContrib'></span>

<h3>Description</h3>

<p>Contribution of each feature to the prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>featureContribTree(tidy.RF, tree, X)

featureContrib(tidy.RF, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="featureContribTree_+3A_tidy.rf">tidy.RF</code></td>
<td>
<p>A tidy random forest. The random forest to make predictions
with.</p>
</td></tr>
<tr><td><code id="featureContribTree_+3A_tree">tree</code></td>
<td>
<p>An integer. The index of the tree to look at.</p>
</td></tr>
<tr><td><code id="featureContribTree_+3A_x">X</code></td>
<td>
<p>A data frame. Features of samples to be predicted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Recall that each node in a decision tree has a prediction associated with
it. For regression trees, it's the average response in that node, whereas
in classification trees, it's the frequency of each response class, or the
most frequent response class in that node.
</p>
<p>For a tree in the forest, the contribution of each feature to the prediction
of a sample is the sum of differences between the predictions of nodes which
split on the feature and those of their children, i.e. the sum of changes in
node prediction caused by spliting on the feature. This is the calculated by
<code>featureContribTree</code>.
</p>
<p>For a forest, the contribution of each feature to the prediction if a sample
is the average contribution across all trees in the forest. This is because
the prediction of a forest is the average of the predictions of its trees.
This is calculated by <code>featureContrib</code>.
</p>
<p>Together with <code>trainsetBias(Tree)</code>, they can decompose the prediction
by feature importance:
</p>
<p style="text-align: center;"><code class="reqn">prediction(MODEL, X) =
    trainsetBias(MODEL) +
    featureContrib_1(MODEL, X) + ... + featureContrib_p(MODEL, X),</code>
</p>

<p>where MODEL can be either a tree or a forest.
</p>


<h3>Value</h3>

<p>A cube (3D array). The content depends on the type of the response.
</p>

<ul>
<li><p> Regression: A P-by-1-by-N array, where P is the number of features
in <code>X</code>, and N the number of samples in <code>X</code>. The pth row of
the nth slice stands for the contribution of feature p to the
prediction for response n.
</p>
</li>
<li><p> Classification: A P-by-D-by-N array, where P is the number of
features in <code>X</code>, D is the number of response classes, and N is
the number of samples in <code>X</code>. The pth row of the nth slice stands
for the contribution of feature p to the prediction of each response
class for response n.
</p>
</li></ul>



<h3>Functions</h3>


<ul>
<li> <p><code>featureContribTree</code>: Feature contribution to prediction within a
single tree
</p>
</li>
<li> <p><code>featureContrib</code>: Feature contribution to prediction within the
whole forest
</p>
</li></ul>


<h3>References</h3>

<p>Interpreting random forests
<a href="http://blog.datadive.net/interpreting-random-forests/">http://blog.datadive.net/interpreting-random-forests/</a>
</p>
<p>Random forest interpretation with scikit-learn
<a href="http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/">http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trainsetBias">trainsetBias</a></code>
</p>
<p><code><a href="#topic+MDI">MDI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ranger)
test.id &lt;- 50 * seq(3)
rfobj &lt;- ranger(Species ~ ., iris[-test.id, ], keep.inbag=TRUE)
tidy.RF &lt;- tidyRF(rfobj, iris[-test.id, -5], iris[-test.id, 5])
featureContribTree(tidy.RF, 1, iris[test.id, -5])
featureContrib(tidy.RF, iris[test.id, -5])

</code></pre>

<hr>
<h2 id='MDIoobTree'>Debiased Mean Decrease in Impurity</h2><span id='topic+MDIoobTree'></span><span id='topic+MDIoob'></span>

<h3>Description</h3>

<p>Calculate the MDI-oob feature importance measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MDIoobTree(tidy.RF, tree, trainX, trainY)

MDIoob(tidy.RF, trainX, trainY)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MDIoobTree_+3A_tidy.rf">tidy.RF</code></td>
<td>
<p>A tidy random forest. The random forest to calculate MDI-oob
from.</p>
</td></tr>
<tr><td><code id="MDIoobTree_+3A_tree">tree</code></td>
<td>
<p>An integer. The index of the tree to look at.</p>
</td></tr>
<tr><td><code id="MDIoobTree_+3A_trainx">trainX</code></td>
<td>
<p>A data frame. Train set features, such that the <code>T</code>th
tree is trained with <code>X[tidy.RF$inbag.counts[[T]], ]</code>.</p>
</td></tr>
<tr><td><code id="MDIoobTree_+3A_trainy">trainY</code></td>
<td>
<p>A data frame. Train set responses, such that the <code>T</code>th
tree is trained with <code>Y[tidy.RF$inbag.counts[[T]], ]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It has long been known that MDI incorrectly assigns high importance to noisy
features, leading to systematic bias in feature selection. To address this
issue, Li et al. proposed a debiased MDI feature importance measure using
out-of-bag samples, called MDI-oob, which has achieved state-of-the-art
performance in feature selection for both simulated and real data.
</p>
<p>See <code>vignette('MDI', package='tree.interpreter')</code> for more context.
</p>


<h3>Value</h3>

<p>A matrix. The content depends on the type of the response.
</p>

<ul>
<li><p> Regression: A P-by-1 matrix, where P is the number of features in
<code>X</code>. The pth row contains the MDI-oob of feature p.
</p>
</li>
<li><p> Classification: A P-by-D matrix, where P is the number of features
in <code>X</code> and D is the number of response classes. The dth column of
the pth row contains the MDI-oob of feature p to class d. You can get
the MDI-oob of each feature by calling <code>rowSums</code> on the result.
</p>
</li></ul>



<h3>Functions</h3>


<ul>
<li> <p><code>MDIoobTree</code>: Debiased mean decrease in impurity within a single tree
</p>
</li>
<li> <p><code>MDIoob</code>: Debiased mean decrease in impurity within the whole
forest
</p>
</li></ul>


<h3>References</h3>

<p>A Debiased MDI Feature Importance Measure for Random Forests
<a href="https://arxiv.org/abs/1906.10845">https://arxiv.org/abs/1906.10845</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MDI">MDI</a></code>
</p>
<p><code>vignette('MDI', package='tree.interpreter')</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ranger)
rfobj &lt;- ranger(Species ~ ., iris, keep.inbag=TRUE)
tidy.RF &lt;- tidyRF(rfobj, iris[, -5], iris[, 5])
MDIoobTree(tidy.RF, 1, iris[, -5], iris[, 5])
MDIoob(tidy.RF, iris[, -5], iris[, 5])

</code></pre>

<hr>
<h2 id='MDITree'>Mean Decrease in Impurity</h2><span id='topic+MDITree'></span><span id='topic+MDI'></span>

<h3>Description</h3>

<p>Calculate the MDI feature importance measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MDITree(tidy.RF, tree, trainX, trainY)

MDI(tidy.RF, trainX, trainY)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MDITree_+3A_tidy.rf">tidy.RF</code></td>
<td>
<p>A tidy random forest. The random forest to calculate MDI
from.</p>
</td></tr>
<tr><td><code id="MDITree_+3A_tree">tree</code></td>
<td>
<p>An integer. The index of the tree to look at.</p>
</td></tr>
<tr><td><code id="MDITree_+3A_trainx">trainX</code></td>
<td>
<p>A data frame. Train set features, such that the <code>T</code>th
tree is trained with <code>X[tidy.RF$inbag.counts[[T]], ]</code>.</p>
</td></tr>
<tr><td><code id="MDITree_+3A_trainy">trainY</code></td>
<td>
<p>A data frame. Train set responses, such that the <code>T</code>th
tree is trained with <code>Y[tidy.RF$inbag.counts[[T]], ]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>MDI stands for Mean Decrease in Impurity. It is a widely adopted measure of
feature importance in random forests. In this package, we calculate MDI with
a new analytical expression derived by Li et al. (See references)
</p>
<p>See <code>vignette('MDI', package='tree.interpreter')</code> for more context.
</p>


<h3>Value</h3>

<p>A matrix. The content depends on the type of the response.
</p>

<ul>
<li><p> Regression: A P-by-1 matrix, where P is the number of features in
<code>X</code>. The pth row contains the MDI of feature p.
</p>
</li>
<li><p> Classification: A P-by-D matrix, where P is the number of features
in <code>X</code> and D is the number of response classes. The dth column of
the pth row contains the MDI of feature p to class d. You can get the
MDI of each feature by calling <code>rowSums</code> on the result.
</p>
</li></ul>



<h3>Functions</h3>


<ul>
<li> <p><code>MDITree</code>: Mean decrease in impurity within a single tree
</p>
</li>
<li> <p><code>MDI</code>: Mean decrease in impurity within the whole forest
</p>
</li></ul>


<h3>References</h3>

<p>A Debiased MDI Feature Importance Measure for Random Forests
<a href="https://arxiv.org/abs/1906.10845">https://arxiv.org/abs/1906.10845</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MDIoob">MDIoob</a></code>
</p>
<p><code>vignette('MDI', package='tree.interpreter')</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ranger)
rfobj &lt;- ranger(Species ~ ., iris, keep.inbag=TRUE)
tidy.RF &lt;- tidyRF(rfobj, iris[, -5], iris[, 5])
MDITree(tidy.RF, 1, iris[, -5], iris[, 5])
MDI(tidy.RF, iris[, -5], iris[, 5])

</code></pre>

<hr>
<h2 id='tidyRF'>Tidy Random Forest</h2><span id='topic+tidyRF'></span>

<h3>Description</h3>

<p>Converts random forest objects from various libraries into a common
structure, i.e. a &ldquo;tidy&rdquo; random forest, calculating absent auxiliary
information on demand, in order to provide a uniform interface for other
<code>tree.interpreter</code> functions. Note that the output is of a private
format, and is subject to change.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidyRF(rfobj, trainX, trainY)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidyRF_+3A_rfobj">rfobj</code></td>
<td>
<p>A random forest object. Currently, supported libraries include
<code>randomForest</code> and <code>ranger</code>.</p>
</td></tr>
<tr><td><code id="tidyRF_+3A_trainx">trainX</code></td>
<td>
<p>A data frame. Train set features.</p>
</td></tr>
<tr><td><code id="tidyRF_+3A_trainy">trainY</code></td>
<td>
<p>A data frame. Train set responses.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class <code>tidyRF</code>, with entries:
</p>

<dl>
<dt><code>num.trees</code></dt><dd><p>An integer. Number of trees.</p>
</dd>
<dt><code>feature.names</code></dt><dd><p>A vector. Names of features.</p>
</dd>
<dt><code>num.classes</code></dt><dd><p>An integer. For classification trees, number
of classes of the response. For regression trees, this field will
always be 1.</p>
</dd>
<dt><code>class.names</code></dt><dd><p>A vector. Names of response classes.</p>
</dd>
<dt><code>inbag.counts</code></dt><dd><p>A list. For each tree, a vector of the number
of times the observations are in-bag in the trees.</p>
</dd>
<dt><code>left.children</code></dt><dd><p>A list. For each tree, a vector of the left
children of its nodes.</p>
</dd>
<dt><code>right.children</code></dt><dd><p>A list. For each tree, a vector of the
right children of its nodes.</p>
</dd>
<dt><code>split.features</code></dt><dd><p>A list. For each tree, a vector of the
indices of features used at its nodes. Indices start from 0. A value
of 0 means the node is terminal. This does not cause ambiguity,
because the root node will never be a child of other nodes.</p>
</dd>
<dt><code>split.values</code></dt><dd><p>A list. For each tree, a vector of the
values of features used at its nodes.</p>
</dd>
<dt><code>node.sizes</code></dt><dd><p>A list. For each tree, a vector of the sizes
of its nodes.</p>
</dd>
<dt><code>node.resp</code></dt><dd><p>A list. For each tree, a vector of the responses
of its nodes.</p>
</dd>
<dt><code>delta.node.resp.left</code></dt><dd><p>A list. For each tree, a vector of
the difference between the responses of the left children of its nodes
and themselves.</p>
</dd>
<dt><code>delta.node.resp.right</code></dt><dd><p>A list. For each tree, a vector of
the difference between the responses of the right children of its
nodes and themselves.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(ranger)
rfobj &lt;- ranger(Species ~ ., iris, keep.inbag=TRUE)
tidy.RF &lt;- tidyRF(rfobj, iris[, -5], iris[, 5])
str(tidy.RF, max.level=1)

</code></pre>

<hr>
<h2 id='trainsetBiasTree'>Trainset Bias</h2><span id='topic+trainsetBiasTree'></span><span id='topic+trainsetBias'></span>

<h3>Description</h3>

<p>For a tree in the forest, trainset bias is the prediction of its root node,
or the unconditional prediction of the tree, or the average response of the
samples used to train the tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainsetBiasTree(tidy.RF, tree)

trainsetBias(tidy.RF)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trainsetBiasTree_+3A_tidy.rf">tidy.RF</code></td>
<td>
<p>A tidy random forest. The random forest to extract train
set bias from.</p>
</td></tr>
<tr><td><code id="trainsetBiasTree_+3A_tree">tree</code></td>
<td>
<p>An integer. The index of the tree to look at.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a forest, the trainset bias is simply the average trainset bias across
all trees. This is because the prediction of a forest is the average of the
predictions of its trees.
</p>
<p>Together with <code>featureContrib(Tree)</code>, they can decompose the prediction
by feature importance:
</p>
<p style="text-align: center;"><code class="reqn">prediction(MODEL, X) =
    trainsetBias(MODEL) +
    featureContrib_1(MODEL, X) + ... + featureContrib_p(MODEL, X),</code>
</p>

<p>where MODEL can be either a tree or a forest.
</p>


<h3>Value</h3>

<p>A matrix. The content depends the type of the response.
</p>

<ul>
<li><p> Regression: A 1-by-1 matrix. The trainset bias for the prediction
of the response.
</p>
</li>
<li><p> Classification: A 1-by-D matrix, where D is the number of response
classes. Each column of the matrix stands for the trainset bias for
the prediction of each response class.
</p>
</li></ul>



<h3>Functions</h3>


<ul>
<li> <p><code>trainsetBiasTree</code>: Trainset bias within a single tree
</p>
</li>
<li> <p><code>trainsetBias</code>: Trainset bias within the whole forest
</p>
</li></ul>


<h3>References</h3>

<p>Interpreting random forests
<a href="http://blog.datadive.net/interpreting-random-forests/">http://blog.datadive.net/interpreting-random-forests/</a>
</p>
<p>Random forest interpretation with scikit-learn
<a href="http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/">http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+featureContrib">featureContrib</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ranger)
rfobj &lt;- ranger(Species ~ ., iris, keep.inbag=TRUE)
tidy.RF &lt;- tidyRF(rfobj, iris[, -5], iris[, 5])
trainsetBiasTree(tidy.RF, 1)
trainsetBias(tidy.RF)

</code></pre>

<hr>
<h2 id='tree.interpreter'>Random Forest Prediction Decomposition and Feature Importance Measure</h2><span id='topic+tree.interpreter'></span>

<h3>Description</h3>

<p>An R re-implementation of the 'treeinterpreter' package on PyPI.
&lt;https://pypi.org/project/treeinterpreter/&gt;. Each prediction can be
decomposed as 'prediction = bias + feature_1_contribution + ... +
feature_n_contribution'. This decomposition is then used to calculate the
Mean Decrease Impurity (MDI) and Mean Decrease Impurity using out-of-bag
samples (MDI-oob) feature importance measures based on the work of Li et al.
(2019) &lt;arXiv:1906.10845&gt;.
</p>


<h3><code>tidyRF</code></h3>

<p>The function <code>tidyRF</code> can turn a <code>randomForest</code> or <code>ranger</code>
object into a package-agnostic random forest object. All other functions
in this package operate on such a <code>tidyRF</code> object.
</p>


<h3>The <code>featureContrib</code> and <code>trainsetBias</code> families</h3>

<p>The <code>featureContrib</code> and <code>trainsetBias</code> families can decompose the
prediction of regression/classification trees/forests into bias and feature
contribution components.
</p>


<h3>The <code>MDI</code> and <code>MDIoob</code> families</h3>

<p>The <code>MDI</code> family can calculate the good old MDI feature importance
measure, which unfortunately has some feature selection bias. MDI-oob is a
debiased MDI feature importance measure that has achieved state-of-the-art
performance in feature selection for both simulated and real data. It can be
calculated with functions from the <code>MDIoob</code> family.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ranger)
rfobj &lt;- ranger(mpg ~ ., mtcars, keep.inbag = TRUE)
tidy.RF &lt;- tidyRF(rfobj, mtcars[, -1], mtcars[, 1])
MDIoob(tidy.RF, mtcars[, -1], mtcars[, 1])

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
