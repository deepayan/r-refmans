<!DOCTYPE html><html lang="en"><head><title>Help for package sparseSEM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparseSEM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#sparseSEM-package'>
<p>sparseSEM: Elastic Net Penalized Maximum Likelihood for Structural Equation Models with Network GPT Framework</p></a></li>
<li><a href='#B'>
<p>True network edges</p></a></li>
<li><a href='#elasticNetSEM'>
<p>The Elastic Net penalized SEM with Network GPT Framework</p></a></li>
<li><a href='#elasticNetSEMcv'>
<p>The Elastic Net penalty for SEM with user supplied (alphas, lambdas) for grid search</p></a></li>
<li><a href='#elasticNetSEMpoint'>
<p>The Elastic Net penalty for SEM</p></a></li>
<li><a href='#enSEM_stability_selection'>
<p>Stability Selection for the Elastic Net penalized SEM</p></a></li>
<li><a href='#enSEM_stability_selection_parallel'>
<p>Parallel Stability Selection for the Elastic Net penalized SEM</p></a></li>
<li><a href='#lassoSEM'>
<p>The Lasso penalty for SEM</p></a></li>
<li><a href='#Missing'>
<p>Missing Network Node dependent variable data</p></a></li>
<li><a href='#sparseSEM-internal'><p>Internal sparseSEM function</p></a></li>
<li><a href='#X'>
<p>Genotype matrix</p></a></li>
<li><a href='#Y'>
<p>Gene expression matrix</p></a></li>
<li><a href='#yeast'>
<p>Yeast cis-QTL Gene Regulatory Network Dataset</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Elastic Net Penalized Maximum Likelihood for Structural Equation
Models with Network GPT Framework</td>
</tr>
<tr>
<td>Version:</td>
<td>4.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-10-25</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Anhui Huang &lt;anhuihuang@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides elastic net penalized maximum likelihood estimator for structural equation models (SEM). The package implements 'lasso' and 'elastic net' (l1/l2) penalized SEM and estimates the model parameters with an efficient block coordinate ascent algorithm that maximizes the penalized likelihood of the SEM.  Hyperparameters are inferred from cross-validation (CV).  A Stability Selection (STS) function is also available to provide accurate causal effect selection. The software achieves high accuracy performance through a 'Network Generative Pre-trained Transformer' (Network GPT) Framework with two steps: 1) pre-trains the model to generate a complete (fully connected) graph; and 2) uses the complete graph as the initial state to fit the 'elastic net' penalized SEM.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>parallel</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL]</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-10-27 15:12:26 UTC; anhui</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr,plot.matrix</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-10-27 15:30:02 UTC</td>
</tr>
<tr>
<td>Author:</td>
<td>Anhui Huang [aut, ctb, cre]</td>
</tr>
</table>
<hr>
<h2 id='sparseSEM-package'>
sparseSEM: Elastic Net Penalized Maximum Likelihood for Structural Equation Models with Network GPT Framework
</h2><span id='topic+sparseSEM-package'></span><span id='topic+sparseSEM'></span>

<h3>Description</h3>

<p>State-of-the-art Elastic Net Penalized Maximum Likelihood for Structural Equation Models implemented with a Network Generative Pre-training Transformer (Network GPT).
Two penalty functions including Lasso and Elastic-net are available.
</p>
<p>For users new to this package, function elasticNetSEM() provides the simplified entry point: 
Missing matrix can be all 0 (none or uknown), so as B matrix (unknow connections in the network), 
thus only Y and X are mandatory. Then model will fit SEM: Y = BY + fX + e. See the reference for model details. 
</p>
<p>The package also provides other functions with more flexibility to allow fine tuning the parameters: <br />
- elasticNetSEMcv(): user provides alphas (one or more) and lambdas; the function then computes the optimal parameters and network parameters; <br />
- elasticNetSEMpoint(): user provides one pair of (alpha, lambda), and the function computes the network parameters. <br />
- enSEM_stability_selection(): stability selection via bootstrapping. <br />
</p>
<p># Network Generative Pre-training Transformer (GPT) Framework: <br /> 
In all functions the &quot;Network GPT&quot; framework is deployed behind the scene. Specifically, a pre-trained network was built in the following steps:
</p>
<p>- Step 1. Pre-train the model with ridge (L2 penalty) SEM with k-fold CV: this step find the optimal ridge hyperparameter rho;
</p>
<p>- Step 2. Generate a complete graph by fitting the SEM ridge regression model with rho from Step 1, obtain the initial status of a (non-sparse, fully connected)
complete network structure (B_hat). 
</p>
<p>Note that the term &quot;Transformer&quot; does not carry the same meaning as the &quot;transformer architecture&quot; commonly used in Natural Language Processing (NLP). In Network GPT, the term means the creation and generation of the complete graph. <br />
</p>
<p># Regularization path: 
A lasso-strong rule is developed for SEM and applied to the selection path. In each step from lambda_max to lambda_min,
where lambda_max is the lambda that keeps only 1 non-zero edge, and lambda_min is the smallest lambda set arbitrarily (eg., from CV, or 0.001 * lambda_max), the elements in B are pre-set to 0 if they meet with the discarding rule. Those elements will not be computed again in the block coordinate ascent algorithm, resulting in reducing computational costs. See the Vignettes and references for more details. <br />
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> sparseSEM</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 4.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-10-25</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Anhui Huang
</p>
<p>Maintainer: Anhui Huang &lt;anhuihuang@gmail.com&gt;
</p>


<h3>References</h3>

<p>1. Cai, X., Bazerque, J.A., and Giannakis, G.B. (2013). Inference of Gene Regulatory Networks with Sparse Structural Equation Models Exploiting Genetic Perturbations. PLoS Comput Biol 9, e1003068. <br />
2. Huang, A. (2014). &quot;Sparse model learning for inferring genotype and phenotype associations.&quot; Ph.D Dissertation. University of Miami(1186).<br /> <br />	 <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sparseSEM)
</code></pre>

<hr>
<h2 id='B'>
True network edges
</h2><span id='topic+B'></span>

<h3>Description</h3>

<p>B is the M by M matrix defining network topology
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(B)</code></pre>


<h3>Format</h3>

<p>The format is: M by M, where M is the number of vertices 
num [1:30, 1:30] 0 0 0 0 0 ...
</p>


<h3>Details</h3>

<p>If B is not available (real data): the stat output that describes the true accuracy and FDR should be ignored.
</p>


<h3>References</h3>

<p>1. Cai, X., Bazerque, J.A., and Giannakis, G.B. (2013). Inference of Gene Regulatory Networks with Sparse Structural Equation Models Exploiting Genetic Perturbations. PLoS Comput Biol 9, e1003068. <br />
2. Huang, A. (2014). &quot;Sparse model learning for inferring genotype and phenotype associations.&quot; Ph.D Dissertation. University of Miami(1186).<br /> <br />	
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(B)
</code></pre>

<hr>
<h2 id='elasticNetSEM'>
The Elastic Net penalized SEM with Network GPT Framework
</h2><span id='topic+elasticNetSEM'></span>

<h3>Description</h3>

<p>Fit the elastic-net penalized structureal Equation Models (SEM) with input data (X, Y): Y = BY + fX + e.
</p>
<p>For users new to this package, elasticNetSEM provides the simplified entry point: 
Missing matrix can be all 0 (none or uknown), so as B matrix (unknow connections in the network), 
thus only Y and X are mandatory.
</p>
<p>Underlying the function, the program obtains the optimal hyperparameter (alpha, lambda) from 
k-fold cross validation (CV) with fixed k= 5.  Specifically, for each alpha from 0.95 to 0.05 at
a step of -0.05, the function perform 5 fold CV for lambda_max to lambda_min in 20 step 
to determine the optimal (alpha, lambda) for the data. 
</p>
<p>Generally, the software program performs the following Network GPT Framework to arrive at final network structure: <br />
Step 1. Generating a Complete Graph:
</p>
<p>- SEM-ridge regression (L2 penalty) with k-fold CV: this step find the optimal ridge hyperparameter rho; <br />
</p>
<p>- fit SEM ridge regression model (L2 penalty) with rho from Step 1, obtain the initial status (non-sparse)
of network structure (B_ridge); <br />
</p>
<p>Step 2. Elastic net penalized SEM regression with k-fold CV: this step finds the optimal hyperparameter (alpha, lambda); <br />
</p>
<p>Step 3. Fit elastic net SEM model with (alpha, lambda) from Step 2; This step applies a block cooridnate ascent algorithm, and the complete graph from Step-1 is used as the intial step;  <br />
</p>
<p>Step 4. Calculate results for PD, FDR, provide the function output.
</p>
<p>For large scale network inference, a standalone C/C++ software with openMPI for 
parallel computation is also available upon request.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elasticNetSEM(Y, X, Missing, B, verbose = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="elasticNetSEM_+3A_y">Y</code></td>
<td>

<p>The observed node response data with dimension of M (nodes) by N (samples). Y is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="elasticNetSEM_+3A_x">X</code></td>
<td>

<p>The network node attribute matrix with dimension of M by N. Theoretically, X can be L by N matrix, with L being the total
node attributes. In current implementation, each node only allows one and only one attribute. <br />
If you have more than one attributes for some nodes,  please consider selecting the top one by either
correlation or principal component methods.  <br />
If for some nodes there is no attribute available, fill in the rows with all zeros.  See the yeast data 'yeast.rda' for example. <br />
X is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="elasticNetSEM_+3A_missing">Missing</code></td>
<td>

<p>Optional M by N matrix corresponding to elements of Y. 0 denotes not missing, and 1 denotes missing.
If a node i in sample j has the label missing (Missing[i,j] = 1), then Y[i,j] is set to 0.
</p>
</td></tr>
<tr><td><code id="elasticNetSEM_+3A_b">B</code></td>
<td>

<p>Optional input. For a network with M nodes, B is the M by M adjacency matrix.
If data is simulated/with known true network topology (i.e., known adjacency matrix), the Power
of detection (PD) and False Discovery Rate (FDR) is computed in the output parameter 'statistics'.
</p>
<p>If the true network topology is unknown, B is optional, and the PD/FDR in output parameter
'statistics' should be ignored.
</p>
</td></tr>
<tr><td><code id="elasticNetSEM_+3A_verbose">verbose</code></td>
<td>

<p>describe the information output from -1 - 10, larger number means more output
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>the function perform CV and parameter inference, calculate power and FDR
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Bout</code></td>
<td>

<p>the computed weights for the network topology. B[i,j] = 0 means there is no edge between node i and j;
B[i,j]!=0 denotes an (undirected) edge between note i and j with B[i,j] being the weight of the edge.
</p>
</td></tr>
<tr><td><code>fout</code></td>
<td>

<p>f is 1 by M array keeping the weight for X (in SEM: Y = BY + FX + e). Theoretically, F can be M by L matrix,
with M being the number of nodes, and L being the total node attributes. However, in current implementation,
each node only allows one and only one attribute.
If you have more than one attributes for some nodes, please consider selecting the top one by either
correlation or principal component methods.
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>statistics is 1x6 array keeping record of: <br />
1. correct positive <br />
2. total positive   <br />
3. false positive   <br />
4. positive detected  <br />
5. Power of detection (PD) = correct positive/total positive  <br />
6. False Discovery Rate (FDR) = false positive/positive detected
</p>
</td></tr>
<tr><td><code>hyperparameters</code></td>
<td>
<p> Model hyperparameters obtained from cross validation.
</p>
</td></tr>
<tr><td><code>runTime</code></td>
<td>
<p>computational time</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object</p>
</td></tr>	
</table>


<h3>Note</h3>

<p>Difference in three functions:<br />
1) elasticNetSEM: Default alpha = 0.95: -0.05: 0.05; default 20 lambdas <br />
2) elasticNetSEMcv: user supplied alphas (one or more), lambdas; compute the optimal parameters and network parameters <br />
3) elasticNetSEMpoint: user supplied one alpha and one lambda, compute the network parameters 
</p>


<h3>Author(s)</h3>

<p>Anhui Huang; Dept of Electrical and Computer Engineering, Univ of Miami, Coral Gables, FL</p>


<h3>References</h3>

<p>1. Cai, X., Bazerque, J.A., and Giannakis, G.B. (2013). Inference of Gene Regulatory Networks with Sparse Structural Equation Models Exploiting Genetic Perturbations. PLoS Comput Biol 9, e1003068. <br />
2. Huang, A. (2014). &quot;Sparse model learning for inferring genotype and phenotype associations.&quot; Ph.D Dissertation Chapter 7. University of Miami(1186). <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	library(sparseSEM)
	data(B);
	data(Y);
	data(X);
	data(Missing);
	#Example
	
	  OUT &lt;- elasticNetSEM(Y, X, Missing, B, verbose = 1); 
  

</code></pre>

<hr>
<h2 id='elasticNetSEMcv'>
The Elastic Net penalty for SEM with user supplied (alphas, lambdas) for grid search
</h2><span id='topic+elasticNetSEMcv'></span>

<h3>Description</h3>

<p>Function elasticNetSEMcv allows users to set their own grid search through combination of a set of user provided
alphas an lambdas.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elasticNetSEMcv(Y, X, Missing, B, alpha_factors,lambda_factors,kFold, verbose)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="elasticNetSEMcv_+3A_y">Y</code></td>
<td>

<p>The observed node response data with dimension of M (nodes) by N (samples). Y is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="elasticNetSEMcv_+3A_x">X</code></td>
<td>

<p>The network node attribute matrix with dimension of M by N. Theoretically, X can be L by N matrix, with L being the total
node attributes. In current implementation, each node only allows one and only one attribute. <br />
If you have more than one attributes for some nodes,  please consider selecting the top one by either
correlation or principal component methods.  <br />
If for some nodes there is no attribute available, fill in the rows with all zeros.  See the yeast data 'yeast.rda' for example. <br />
X is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="elasticNetSEMcv_+3A_missing">Missing</code></td>
<td>

<p>Optional M by N matrix corresponding to elements of Y. 0 denotes not missing, and 1 denotes missing.
If a node i in sample j has the label missing (Missing[i,j] = 1), then Y[i,j] is set to 0.
</p>
</td></tr>
<tr><td><code id="elasticNetSEMcv_+3A_b">B</code></td>
<td>

<p>Optional input. For a network with M nodes, B is the M by M adjacency matrix.
If data is simulated/with known true network topology (i.e., known adjacency matrix), the Power
of detection (PD) and False Discovery Rate (FDR) is computed in the output parameter 'statistics'.
</p>
<p>If the true network topology is unknown, B is optional, and the PD/FDR in output parameter
'statistics' should be ignored.
</p>
</td></tr>
<tr><td><code id="elasticNetSEMcv_+3A_alpha_factors">alpha_factors</code></td>
<td>

<p>The set of candidate alpha values.  Default is seq(start = 0.95, to = 0.05, step = -0.05)
</p>
</td></tr>
<tr><td><code id="elasticNetSEMcv_+3A_lambda_factors">lambda_factors</code></td>
<td>

<p>The set of candidate lambda values. Default is 10^seq(start =1, to = 0.001, step = -0.2)
</p>
</td></tr>
<tr><td><code id="elasticNetSEMcv_+3A_kfold">kFold</code></td>
<td>

<p>k-fold cross validation, default k=5
</p>
</td></tr>
<tr><td><code id="elasticNetSEMcv_+3A_verbose">verbose</code></td>
<td>

<p>describe the information output from -1 - 10, larger number means more output
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>the function perform CV and parameter inference, calculate power and FDR
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>cv</code></td>
<td>

<p>dataframe stores the minimum Mean Square Error (MSE) for each alpha and the corresponding lambda from the selection path [lambda_max, ...., lambda_min]. <br />
col1: alpha <br />
col2: lambda (With the given alpha, this is the lambda having minimum MSE) <br />
col3: MSE <br />
col4: STE <br />
</p>
<p>The final (alpha, lambda) is set at the (alpha, lambda) that is within 1ste of the min(MSE) with higher level of penalty on the likehood function. 
</p>
</td></tr>
</table>

<ul>
<li><p> fit the model fit with optimal (alpha,lambda) from cv
</p>
 
<ul>
<li><p> Bout the computed weights for the network topology. B[i,j] = 0 means there is no edge between node i and j;
B[i,j]!=0 denotes an (undirected) edge between note i and j with B[i,j] being the weight of the edge.
</p>
</li>
<li><p> fout f is 1 by M array keeping the weight for X (in SEM: Y = BY + FX + e). Theoretically, F can be M by L matrix,
with M being the number of nodes, and L being the total node attributes. However, in current implementation,
each node only allows one and only one attribute.
If you have more than one attributes for some nodes, please consider selecting the top one by either
correlation or principal component methods.
</p>
</li>
<li><p> stat statistics is 1x6 array keeping record of: <br />
1. correct positive <br />
2. total positive   <br />
3. false positive   <br />
4. positive detected  <br />
5. Power of detection (PD) = correct positive/total positive  <br />
6. False Discovery Rate (FDR) = false positive/positive detected
</p>
</li>
<li><p> simTime computational time
</p>
</li>
<li><p> call the call that produced this object 
</p>
</li></ul>

</li></ul>



<h3>Note</h3>

<p>Difference in three functions:<br />
1) elasticNetSML: Default alpha = 0.95: -0.05: 0.05; default 20 lambdas <br />
2) elasticNetSEMcv: user supplied alphas (one or more), lambdas; compute the optimal parameters and network parameters <br />
3) elasticNetSMLpoint: user supplied one alpha and one lambda, compute the network parameters 
</p>
<p>User is responsible to set the random seed to guarantee repeatable results.
</p>


<h3>Author(s)</h3>

<p>Anhui Huang; Dept of Electrical and Computer Engineering, Univ of Miami, Coral Gables, FL</p>


<h3>References</h3>

<p>1. Cai, X., Bazerque, J.A., and Giannakis, G.B. (2013). Inference of Gene Regulatory Networks with Sparse Structural Equation Models Exploiting Genetic Perturbations. PLoS Comput Biol 9, e1003068. <br />
2. Huang, A. (2014). &quot;Sparse model learning for inferring genotype and phenotype associations.&quot; Ph.D Dissertation. University of Miami(1186).<br /> <br />	 <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	library(sparseSEM)
	data(B);
	data(Y);
	data(X);
	data(Missing);
	## Not run: OUT &lt;- elasticNetSEMcv(Y, X, Missing, B, alpha_factors = c(0.75, 0.5, 0.25),
	lambda_factors=c(0.1, 0.01, 0.001), kFold = 5, verbose  = 1);

## End(Not run)
</code></pre>

<hr>
<h2 id='elasticNetSEMpoint'>
The Elastic Net penalty for SEM
</h2><span id='topic+elasticNetSEMpoint'></span>

<h3>Description</h3>

<p>For user provided one alpha in range (0,1) and one lambda_factor in range (0,1), 
the function perform selection path from lambda_max to lambda 
to determine the optimal network topology. 
</p>
<p>In the case of the grid search in elasticNetSEMcv() function may not 
be granular enough and user would like to explore/twist (alpha, lambda)
a little bit, this function provides the solution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	elasticNetSEMpoint(Y, X, Missing, B, alpha_factor, lambda_factor, verbose)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="elasticNetSEMpoint_+3A_y">Y</code></td>
<td>

<p>The observed node response data with dimension of M (nodes) by N (samples). Y is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="elasticNetSEMpoint_+3A_x">X</code></td>
<td>

<p>The network node attribute matrix with dimension of M by N. Theoretically, X can be L by N matrix, with L being the total
node attributes. In current implementation, each node only allows one and only one attribute. <br />
If you have more than one attributes for some nodes,  please consider selecting the top one by either
correlation or principal component methods.  <br />
If for some nodes there is no attribute available, fill in the rows with all zeros.  See the yeast data 'yeast.rda' for example. <br />
X is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="elasticNetSEMpoint_+3A_missing">Missing</code></td>
<td>

<p>Optional M by N matrix corresponding to elements of Y. 0 denotes not missing, and 1 denotes missing.
If a node i in sample j has the label missing (Missing[i,j] = 1), then Y[i,j] is set to 0.
</p>
</td></tr>
<tr><td><code id="elasticNetSEMpoint_+3A_b">B</code></td>
<td>

<p>Optional input. For a network with M nodes, B is the M by M adjacency matrix.
If data is simulated/with known true network topology (i.e., known adjacency matrix), the Power
of detection (PD) and False Discovery Rate (FDR) is computed in the output parameter 'statistics'.
</p>
<p>If the true network topology is unknown, B is optional, and the PD/FDR in output parameter
'statistics' should be ignored.
</p>
</td></tr>
<tr><td><code id="elasticNetSEMpoint_+3A_alpha_factor">alpha_factor</code></td>
<td>

<p>alpha_factor: in range of (0, 1); must be scalar
</p>
</td></tr>
<tr><td><code id="elasticNetSEMpoint_+3A_lambda_factor">lambda_factor</code></td>
<td>

<p>penalty lambda_factor: in range of (0, 1); must be scalar
</p>
</td></tr>
<tr><td><code id="elasticNetSEMpoint_+3A_verbose">verbose</code></td>
<td>

<p>describe the information output from -1 - 10, larger number means more output
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>the function perform selection path from lambda_max to lambda, calculate power and FDR
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Bout</code></td>
<td>

<p>the computed weights for the network topology. B[i,j] = 0 means there is no edge between node i and j;
B[i,j]!=0 denotes an (undirected) edge between note i and j.
</p>
</td></tr>
<tr><td><code>fout</code></td>
<td>

<p>f is 1 by M array keeping the weight for X (in SEM: Y = BY + FX + e). Theoretically, F can be M by L matrix,
with M being the number of nodes, and L being the total node attributes. However, in current implementation,
each node only allows one and only one attribute.
If you have more than one attributes for some nodes, please consider selecting the top one by either
correlation or principal component methods.
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>statistics is 1x6 array keeping record of: <br />
1. correct positive <br />
2. total positive   <br />
3. false positive   <br />
4. positive detected  <br />
5. Power of detection (PD) = correct positive/total positive  <br />
6. False Discovery Rate (FDR) = false positive/positive detected
</p>
</td></tr>
<tr><td><code>simTime</code></td>
<td>
<p>computational time</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object</p>
</td></tr>	
</table>


<h3>Note</h3>

<p>Difference in three functions:<br />
1) elasticNetSEM: Default alpha = 0.95: -0.05: 0.05; default 20 lambdas <br />
2) elasticNetSEMcv: user supplied alphas (one or more), lambdas; compute the optimal parameters and network parameters <br />
3) elasticNetSEMpoint: user supplied one alpha and one lambda, compute the network parameters 
</p>


<h3>Author(s)</h3>

<p>Anhui Huang; Dept of Electrical and Computer Engineering, Univ of Miami, Coral Gables, FL</p>


<h3>References</h3>

<p>1. Cai, X., Bazerque, J.A., and Giannakis, G.B. (2013). Inference of Gene Regulatory Networks with Sparse Structural Equation Models Exploiting Genetic Perturbations. PLoS Comput Biol 9, e1003068. <br />
2. Huang, A. (2014). &quot;Sparse model learning for inferring genotype and phenotype associations.&quot; Ph.D Dissertation. University of Miami(1186).<br /> <br />	
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	library(sparseSEM)
	data(B);
	data(Y);
	data(X);
	data(Missing);
## Not run: 	OUT &lt;- elasticNetSEMpoint(Y, X, Missing, B,
		alpha_factor = 0.5, lambda_factor = 0.1, verbose = 1);

## End(Not run)
</code></pre>

<hr>
<h2 id='enSEM_stability_selection'>
Stability Selection for the Elastic Net penalized SEM
</h2><span id='topic+enSEM_stability_selection'></span>

<h3>Description</h3>

<p>Fit the elastic-net penalized structureal Equation Models (SEM) with input data (X, Y): Y = BY + fX + e.
Perform Stability Selection (STS) on the input dataset. This function implements STS described in
Meinshausen N. and Buhlmann P (2010) and Shah R. and Samworth R (2013). 
</p>
<p>Underlying the function, the program obtains the performs n rounds of boostraping each with half of the original 
sample size, and run the selection path of hyperparameter (alpha, lambda). The following stability selection scores are 
calculated: <br />
1. E(v): the upper bound of the expected number of falsely selected variables  <br />
2. pre-comparison error rate = E(v)/p where p is the total number of model parameters (in SEM, p = M*M -M) <br />
3. E(v)_ShaR the expected number of falsely selected variables described in Shah R. and Samworth R (2013) <br />
4. FDR: False discovery rate = E(v)/nSelected <br />
5. FDR_ShaR: FDR described in Shah R. and Samworth R (2013) <br />
</p>
<p>The final output is based on Scores described in described in Shah R. and Samworth R (2013), and original scores
described in Meinshausen N. and Buhlmann P (2010) are provided for reference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enSEM_stability_selection(Y,X, Missing,B,
                           alpha_factors, 
                           lambda_factors, 
                           kFold,
                           nBootstrap,
                           verbose)
                                      
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="enSEM_stability_selection_+3A_y">Y</code></td>
<td>

<p>The observed node response data with dimension of M (nodes) by N (samples). Y is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_+3A_x">X</code></td>
<td>

<p>The network node attribute matrix with dimension of M by N. Theoretically, X can be L by N matrix, with L being the total
node attributes. In current implementation, each node only allows one and only one attribute. <br />
If you have more than one attributes for some nodes,  please consider selecting the top one by either
correlation or principal component methods.  <br />
If for some nodes there is no attribute available, fill in the rows with all zeros.  See the yeast data 'yeast.rda' for example. <br />
X is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_+3A_missing">Missing</code></td>
<td>

<p>Optional M by N matrix corresponding to elements of Y. 0 denotes not missing, and 1 denotes missing.
If a node i in sample j has the label missing (Missing[i,j] = 1), then Y[i,j] is set to 0.
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_+3A_b">B</code></td>
<td>

<p>Optional input. For a network with M nodes, B is the M by M adjacency matrix.
If data is simulated/with known true network topology (i.e., known adjacency matrix), the Power
of detection (PD) and False Discovery Rate (FDR) is computed in the output parameter 'statistics'.
</p>
<p>If the true network topology is unknown, B is optional, and the PD/FDR in output parameter
'statistics' should be ignored.
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_+3A_alpha_factors">alpha_factors</code></td>
<td>

<p>The set of candidate alpha values.  Default is seq(start = 0.95, to = 0.05, step = -0.05)
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_+3A_lambda_factors">lambda_factors</code></td>
<td>

<p>The set of candidate lambda values. Default is 10^seq(start =1, to = 0.001, step = -0.2)
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_+3A_kfold">kFold</code></td>
<td>

<p>k-fold cross validation, default k=3.  Note STS result is not based on CV. However, fitting l1/l2 regularized SEM will
run the first step described in elasticNetSEM() function: 
Step 1. SEM-ridge regression (L2 penalty) with k-fold CV: this step find the optimal ridge hyperparameter rho to provide an initial values for l1/l2 regularized SEM. <br />
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_+3A_nbootstrap">nBootstrap</code></td>
<td>

<p>bootstrapping parameter. default nBootstrap = 100.
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_+3A_verbose">verbose</code></td>
<td>

<p>describe the information output from -1 - 10, larger number means more output
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>the function perform STS
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>STS</code></td>
<td>

<p>The stable effects are those effects selected by STS, i.e., the non-zero values in matrix B.
</p>
</td></tr>
<tr><td><code>statistics</code></td>
<td>

<p>the final STS scores with components of: <br />
1. threshold: denoted as pi in  Meinshausen N. and Buhlmann P (2010) <br />
2. pre-comparison error rate   <br />
3. E(v)   <br />
4. E(v)_ShahR <br />
5. nSTS: final number of stable effects with pi that leads to minimum FDR  <br />
6. FDR <br />
7. FDR_ShahR <br />
</p>
</td></tr>
<tr><td><code>STS data</code></td>
<td>
<p>Bootstrapping details.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object</p>
</td></tr>	
</table>


<h3>Author(s)</h3>

<p>Anhui Huang</p>


<h3>References</h3>

<p>[1]: Meinshausen, N. and Buhlmann, P., 2010. Stability selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(4), pp.417-473.
</p>
<p>[2] Shah, R.D. and Samworth, R.J., 2013. Variable selection with error control: another look at stability selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75(1), pp.55-80.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	library(sparseSEM)
	data(B);
	data(Y);
	data(X);
	data(Missing);
	#Example
	
output = enSEM_stability_selection(Y,X, Missing,B,
                                     alpha_factors = seq(1,0.05, -0.05), 
                                     lambda_factors =10^seq(-0.2,-4,-0.2), 
                                     kFold = 3,
                                     nBootstrap = 100,
                                     verbose = -1)
  

</code></pre>

<hr>
<h2 id='enSEM_stability_selection_parallel'>
Parallel Stability Selection for the Elastic Net penalized SEM  
</h2><span id='topic+enSEM_stability_selection_parallel'></span>

<h3>Description</h3>

<p>Fit the elastic-net penalized structureal Equation Models (SEM) with input data (X, Y): Y = BY + fX + e.
Perform Stability Selection (STS) on the input dataset. This function implements STS described in
Meinshausen N. and Buhlmann P (2010) and Shah R. and Samworth R (2013). 
</p>
<p>Underlying the function, the program obtains the performs n rounds of boostraping each with half of the original 
sample size, and run the selection path of hyperparameter (alpha, lambda). The following stability selection scores are 
calculated: <br />
1. E(v): the upper bound of the expected number of falsely selected variables  <br />
2. pre-comparison error rate = E(v)/p where p is the total number of model parameters (in SEM, p = M*M -M) <br />
3. E(v)_ShaR the expected number of falsely selected variables described in Shah R. and Samworth R (2013) <br />
4. FDR: False discovery rate = E(v)/nSelected <br />
5. FDR_ShaR: FDR described in Shah R. and Samworth R (2013) <br />
</p>
<p>The final output is based on Scores described in described in Shah R. and Samworth R (2013), and original scores
described in Meinshausen N. and Buhlmann P (2010) are provided for reference.
</p>
<p>This function 'enSEM_stability_selection_parallel' performs the same computation as that in function 'enSEM_stability_selection' 
with the only difference of setting up the bootstrapping in parallel leveraging the 'parallel' package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enSEM_stability_selection_parallel(Y,X, Missing,B,
                                    alpha_factors, 
                                    lambda_factors, 
                                    kFold,
                                    nBootstrap,
                                    verbose,
                                    clusters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="enSEM_stability_selection_parallel_+3A_y">Y</code></td>
<td>

<p>The observed node response data with dimension of M (nodes) by N (samples). Y is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_parallel_+3A_x">X</code></td>
<td>

<p>The network node attribute matrix with dimension of M by N. Theoretically, X can be L by N matrix, with L being the total
node attributes. In current implementation, each node only allows one and only one attribute. <br />
If you have more than one attributes for some nodes,  please consider selecting the top one by either
correlation or principal component methods.  <br />
If for some nodes there is no attribute available, fill in the rows with all zeros.  See the yeast data 'yeast.rda' for example. <br />
X is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_parallel_+3A_missing">Missing</code></td>
<td>

<p>Optional M by N matrix corresponding to elements of Y. 0 denotes not missing, and 1 denotes missing.
If a node i in sample j has the label missing (Missing[i,j] = 1), then Y[i,j] is set to 0.
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_parallel_+3A_b">B</code></td>
<td>

<p>Optional input. For a network with M nodes, B is the M by M adjacency matrix.
If data is simulated/with known true network topology (i.e., known adjacency matrix), the Power
of detection (PD) and False Discovery Rate (FDR) is computed in the output parameter 'statistics'.
</p>
<p>If the true network topology is unknown, B is optional, and the PD/FDR in output parameter
'statistics' should be ignored.
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_parallel_+3A_alpha_factors">alpha_factors</code></td>
<td>

<p>The set of candidate alpha values.  Default is seq(start = 0.95, to = 0.05, step = -0.05)
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_parallel_+3A_lambda_factors">lambda_factors</code></td>
<td>

<p>The set of candidate lambda values. Default is 10^seq(start =1, to = 0.001, step = -0.2)
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_parallel_+3A_kfold">kFold</code></td>
<td>

<p>k-fold cross validation, default k=3.  Note STS result is not based on CV. However, fitting l1/l2 regularized SEM will
run the first step described in elasticNetSEM() function: 
Step 1. SEM-ridge regression (L2 penalty) with k-fold CV: this step find the optimal ridge hyperparameter rho to provide an initial values for l1/l2 regularized SEM. <br />
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_parallel_+3A_nbootstrap">nBootstrap</code></td>
<td>

<p>bootstrapping parameter. default nBootstrap = 100.
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_parallel_+3A_verbose">verbose</code></td>
<td>

<p>describe the information output from -1 - 10, larger number means more output
</p>
</td></tr>
<tr><td><code id="enSEM_stability_selection_parallel_+3A_clusters">clusters</code></td>
<td>

<p>snow clusters
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>the function perform STS
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>STS</code></td>
<td>

<p>The stable effects are those effects selected by STS, i.e., the non-zero values in matrix B.
</p>
</td></tr>
<tr><td><code>statistics</code></td>
<td>

<p>the final STS scores with components of: <br />
1. threshold: denoted as pi in  Meinshausen N. and Buhlmann P (2010) <br />
2. pre-comparison error rate   <br />
3. E(v)   <br />
4. E(v)_ShahR <br />
5. nSTS: final number of stable effects with pi that leads to minimum FDR  <br />
6. FDR <br />
7. FDR_ShahR <br />
</p>
</td></tr>
<tr><td><code>STS data</code></td>
<td>
<p>Bootstrapping details.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object</p>
</td></tr>	
</table>


<h3>Author(s)</h3>

<p>Anhui Huang</p>


<h3>References</h3>

<p>[1]: Meinshausen, N. and Buhlmann, P., 2010. Stability selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(4), pp.417-473.
</p>
<p>[2] Shah, R.D. and Samworth, R.J., 2013. Variable selection with error control: another look at stability selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75(1), pp.55-80.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	library(sparseSEM)
	library(parallel)
	data(B);
	data(Y);
	data(X);
	data(Missing);
	#Example
	

  cl&lt;-makeCluster(2)
  clusterEvalQ(cl,{library(sparseSEM)})
  output = enSEM_stability_selection_parallel(Y,X, Missing,B,
                                            alpha_factors = seq(1,0.05, -0.05), 
                                            lambda_factors =10^seq(-0.2,-4,-0.2), 
                                            kFold = 3,
                                            nBootstrap = 100,
                                            verbose = -1,
                                            clusters = cl)
  stopCluster(cl)	
  
</code></pre>

<hr>
<h2 id='lassoSEM'>
The Lasso penalty for SEM
</h2><span id='topic+lassoSEM'></span>

<h3>Description</h3>

<p>Upon lambda_max to lambda_min in 20 step, the function compute 5 fold CV 
to determine the optimal lambda for the data. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	lassoSEM(Y, X, Missing, B, verbose = 5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lassoSEM_+3A_y">Y</code></td>
<td>

<p>gene expression M by N matrix
</p>
</td></tr>
<tr><td><code id="lassoSEM_+3A_x">X</code></td>
<td>

<p>The network node attribute matrix with dimension of M by N. Theoretically, X can be L by N matrix, with L being the total
node attributes. In current implementation, each node only allows one and only one attribute. <br />
If you have more than one attributes for some nodes,  please consider selecting the top one by either
correlation or principal component methods.  <br />
If for some nodes there is no attribute available, fill in the rows with all zeros.  See the yeast data 'yeast.rda' for example. <br />
X is normalized inside the function.
</p>
</td></tr>
<tr><td><code id="lassoSEM_+3A_missing">Missing</code></td>
<td>

<p>missing data in Y
</p>
</td></tr>
<tr><td><code id="lassoSEM_+3A_b">B</code></td>
<td>

<p>true network topology if available
</p>
</td></tr>
<tr><td><code id="lassoSEM_+3A_verbose">verbose</code></td>
<td>

<p>describe the information output from -1 - 10, larger number means more output
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>the function perform CV and parameter inference, calculate power and FDR
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Bout</code></td>
<td>
<p>the matrix B from SEM</p>
</td></tr>
<tr><td><code>fout</code></td>
<td>
<p>f: the weight for matrix X</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>
<p>compute the power and FDR statistics if the ture topology is provided</p>
</td></tr>
<tr><td><code>simTime</code></td>
<td>
<p>computational time</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Anhui Huang</p>


<h3>References</h3>

<p>1. Cai, X., Bazerque, J.A., and Giannakis, G.B. (2013). Inference of Gene Regulatory Networks with Sparse Structural Equation Models Exploiting Genetic Perturbations. PLoS Comput Biol 9, e1003068. <br />
2. Huang, A. (2014). &quot;Sparse model learning for inferring genotype and phenotype associations.&quot; Ph.D Dissertation. University of Miami(1186).<br /> <br />	
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	library(sparseSEM)
	data(B);
	data(Y);
	data(X);
	data(Missing);
	## Not run: OUT &lt;- lassoSEM(Y, X, Missing, B, verbose = 0); 

</code></pre>

<hr>
<h2 id='Missing'>
Missing Network Node dependent variable data
</h2><span id='topic+Missing'></span>

<h3>Description</h3>

<p>M by N matrix corresponding to elements of Y. 0 denotes no missing, while 1 denotes missing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Missing)</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:30, 1:200] 0 0 0 0 0 0 0 0 0 0 ...
</p>


<h3>References</h3>

<p>1. Cai, X., Bazerque, J.A., and Giannakis, G.B. (2013). Inference of Gene Regulatory Networks with Sparse Structural Equation Models Exploiting Genetic Perturbations. PLoS Comput Biol 9, e1003068. <br />
2. Huang, A. (2014). &quot;Sparse model learning for inferring genotype and phenotype associations.&quot; Ph.D Dissertation. University of Miami(1186).<br /> <br />	
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Missing)
</code></pre>

<hr>
<h2 id='sparseSEM-internal'>Internal sparseSEM function</h2><span id='topic+enSEM_STS'></span>

<h3>Description</h3>

<p>Internal sparseSEM function for Stability Selection</p>


<h3>Usage</h3>

<pre><code class='language-R'>	enSEM_STS(i,Y,X,Missing ,B,STS_para,kFold,verbose);
	
</code></pre>


<h3>Details</h3>

<p>These are not intended for use by users. 
<code>enSEM_STS</code> fits sparseSEM model by calling internal C function. 
</p>


<h3>Author(s)</h3>

<p>Anhui Huang</p>

<hr>
<h2 id='X'>
Genotype matrix
</h2><span id='topic+X'></span>

<h3>Description</h3>

<p>X is the M by N matrix corresponding to M network nodes from N samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(X)</code></pre>


<h3>Format</h3>

<p>The format is:
int [1:30, 1:200] 2 1 3 1 2 3 2 1 2 2 ...
</p>


<h3>Details</h3>

<p>current implementation only consider 1 inpepedent attribute per node. If users 
have more than one attributes for some nodes,  please consider selecting the 
top one by either correlation or principal component methods.
</p>


<h3>References</h3>

<p>1. Cai, X., Bazerque, J.A., and Giannakis, G.B. (2013). Inference of Gene Regulatory Networks with Sparse Structural Equation Models Exploiting Genetic Perturbations. PLoS Comput Biol 9, e1003068. <br />
2. Huang, A. (2014). &quot;Sparse model learning for inferring genotype and phenotype associations.&quot; Ph.D Dissertation. University of Miami(1186).<br /> <br />	
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(X)

</code></pre>

<hr>
<h2 id='Y'>
Gene expression matrix
</h2><span id='topic+Y'></span>

<h3>Description</h3>

<p>Y is the M by N matrix describes the dependent attribute of M nodes in N samples
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Y)</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:30, 1:200] 3.02 1.12 -2.24 3.58 2.18 ...
</p>


<h3>Details</h3>

<p>Gene expression data
</p>


<h3>References</h3>

<p>1. Cai, X., Bazerque, J.A., and Giannakis, G.B. (2013). Inference of Gene Regulatory Networks with Sparse Structural Equation Models Exploiting Genetic Perturbations. PLoS Comput Biol 9, e1003068. <br />
2. Huang, A. (2014). &quot;Sparse model learning for inferring genotype and phenotype associations.&quot; Ph.D Dissertation. University of Miami(1186).<br /> <br />	
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Y)
</code></pre>

<hr>
<h2 id='yeast'>
Yeast cis-QTL Gene Regulatory Network Dataset
</h2><span id='topic+yeast+20cisQTL+20GRN'></span>

<h3>Description</h3>

<p>The dataset (Y,X) are two matrices each with 3380 rows and 112 columns as described in the Vignettee &quot;Elastic Net Enabled Sparse-Aware Maximum Likelihood for
Structural Equation Models in Inferring Gene Regulatory Networks&quot;. 
</p>
<p>The Yeast expression trait data set was obatained from Brem R.B. and Kruglyak L (2005), and has been screened through:<br />
1. screen and keep the ORF names in Kellis's ORF list (Kellis M et al, 2003); <br />
2. screen out ORF with more than 5% of missing expression date; <br />
3. perform eQTL mapping by Wilcoxon test, adjust by qvalue; keep the top one cisQTL; <br />
4. fill in zeros for ORF without cisQTL in matrix X <br />
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(yeast)</code></pre>


<h3>Format</h3>

<p>The format is matrix.
</p>


<h3>Details</h3>

<p>Yeast cis-QTL Gene Regulatory Network Dataset 
</p>


<h3>References</h3>

<p>1. Brem RB, Kruglyak L: The landscape of genetic complexity across 5,700 gene expression traits in yeast. Proceedings of the National Academy of Sciences of the United States of America 2005, 102:1572-1577.
<br />
2. Kellis M, Patterson N, Endrizzi M, Birren B, Lander ES: Sequencing and comparison of yeast species to identify genes and regulatory elements. Nature 2003, 423:241-254<br /> <br />	
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(yeast)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
