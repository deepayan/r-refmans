<!DOCTYPE html><html><head><title>Help for package reverseR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {reverseR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Influence plots'><p>Several diagnostic plots for checking p-value influencers</p></a></li>
<li><a href='#lmExact'><p>Create random values that deliver linear regressions with exact parameters</p></a></li>
<li><a href='#lmInfl'><p>Checks and analyzes leave-one-out (LOO) p-values in linear regression</p></a></li>
<li><a href='#lmMult'><p>Checks and analyzes leave-multiple-out (LMO) p-values in linear regression</p></a></li>
<li><a href='#lmThresh'><p>Finds and analyzes significance reversal regions for each response value</p></a></li>
<li><a href='#shinyInfl'><p>Initializes a Shiny App with all implemented methods of this package</p></a></li>
<li><a href='#simInfl'><p>Simulates significance reversals and calculates their influence parameters</p></a></li>
<li><a href='#stability'><p>Calculates stability values for results of 'lmInfl', 'lmMult' and 'lmThresh'</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>no</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>Title:</td>
<td>Linear Regression Stability to Significance Reversal</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-04-18</td>
</tr>
<tr>
<td>Author:</td>
<td>Andrej-Nikolai Spiess &lt;a.spiess@uke.uni-hamburg.de&gt;
        Michal Burdukiewicz &lt;michalburdukiewicz@gmail.com&gt;  
        Stefan Roediger &lt;stefan.roediger@b-tu.de&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andrej-Nikolai Spiess &lt;a.spiess@uke.uni-hamburg.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Tests linear regressions for significance reversal through leave-one(multiple)-out and shifting/addition of response values. The paradigm of the package is loosely based on the somewhat forgotten "dfstat" criterion (Belsley, Kuh &amp; Welsch 1980 &lt;<a href="https://doi.org/10.1002%2F0471725153.ch2">doi:10.1002/0471725153.ch2</a>&gt;), which tests influential values in linear models from their effect on statistical inference, i.e. changes in p-value.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.13.0), shiny, markdown, knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>DT</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-04-18 16:21:57 UTC; anspiess</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-04-24 15:40:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='Influence+20plots'>Several diagnostic plots for checking p-value influencers</h2><span id='topic+lmPlot'></span><span id='topic+pvalPlot'></span><span id='topic+inflPlot'></span><span id='topic+slsePlot'></span><span id='topic+multPlot'></span><span id='topic+threshPlot'></span><span id='topic+stabPlot'></span>

<h3>Description</h3>

<p>Seven different plot types that visualize <em>p</em>-value influencers.<br />
</p>
<p>1. <code>lmPlot</code>: plots the linear regression, marks the influencer(s) in red and displays trend lines for the full and leave-one-out (LOO) data set (black and red, respectively).<br />
2. <code>pvalPlot</code>: plots the <em>p</em>-values for each LOO data point and displays the values as a full model/LOO model plot, together with the <code>alpha</code> border as defined in <code><a href="#topic+lmInfl">lmInfl</a></code>.<br />
3. <code>inflPlot</code>: plots <code><a href="stats.html#topic+dfbeta">dfbeta</a></code> for slope, <code><a href="stats.html#topic+dffits">dffits</a></code>, <code><a href="stats.html#topic+covratio">covratio</a></code>, <code><a href="stats.html#topic+cooks.distance">cooks.distance</a></code>, leverage (<code><a href="stats.html#topic+hatvalues">hatvalues</a></code>) and studentized residuals (<code><a href="stats.html#topic+rstudent">rstudent</a></code>) against the <code class="reqn">\Delta</code><em>p</em>-value. Herewith, changes in these six parameters can be compared to the effect on the corresponding drop/rise in <em>p</em>-value. The plots include vertical boundaries for threshold values as defined in the literature under 'References'.<br />
4. <code>slsePlot</code>: plots all LOO-slopes and their standard errors together with the corresponding original model values and a t-value border as calculated by <code class="reqn">\mathit{Q_t}(1 - \frac{\alpha}{2}, n-2)</code>. LOO of points on the right of this border result in a significant model, and <em>vice versa</em>.<br />
5. <code>threshPlot</code>: plots the output of <code><a href="#topic+lmThresh">lmThresh</a></code>, i.e. the regression plot including confidence/prediction intervals, as well as for each response value <code class="reqn">y_i</code> the region in which the model is significant (green). This is tested for either i) <code class="reqn">y_i</code> that are shifted into this region (<code>newobs = FALSE</code> in <code><a href="#topic+lmThresh">lmThresh</a></code>) or ii) when a new observation <code class="reqn">y2_i</code> is added (<code>newobs = TRUE</code> in <code><a href="#topic+lmThresh">lmThresh</a></code>). In the latter case, it is informative if this region resides within the prediction interval (dashed line), indicating that a future additional measurement at <code class="reqn">x_i</code> might reverse the significance statement.<br />
6. <code>multPlot</code>: plots the output of <code><a href="#topic+lmMult">lmMult</a></code> as a point cloud of <em>p</em>-values for each 1...<code>max</code> sample removals and <code>n</code> combinations. All combinations for which the sample removal resulted in a significance reversal are colored in red, the percentages of these are given on top of the plot.<br />
7. <code>stabPlot</code>: for single (to be selected) response values from the output of <code><a href="#topic+lmThresh">lmThresh</a></code>, this function displays the region of significance reversal within the surrounding prediction interval. The probability of a either shifting the response value (if <code>lmThresh(..., newobs = FALSE)</code>) or of including a future (measurement) point (if <code>lmThresh(..., newobs = TRUE)</code>) to reverse the significance is shown as the integral between the &quot;end of significance region&quot; (eosr) and the nearest prediction interval boundary.
</p>
<p><b>NOTE</b>: The visual display should always be supplemented with the corresponding <code><a href="#topic+stability">stability</a></code> analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmPlot(infl, ...) 
pvalPlot(infl, ...) 
inflPlot(infl, ...)
slsePlot(infl, ...)
threshPlot(thresh, bands = FALSE, ...)
multPlot(mult, log = FALSE, ...)
stabPlot(stab, which = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Influence+2B20plots_+3A_infl">infl</code></td>
<td>
<p>an object obtained from <code><a href="#topic+lmInfl">lmInfl</a></code>.</p>
</td></tr>
<tr><td><code id="Influence+2B20plots_+3A_thresh">thresh</code></td>
<td>
<p>an object obtained from <code><a href="#topic+lmThresh">lmThresh</a></code>.</p>
</td></tr>
<tr><td><code id="Influence+2B20plots_+3A_stab">stab</code></td>
<td>
<p>an object obtained from using <code><a href="#topic+stability">stability</a></code> on an <code><a href="#topic+lmThresh">lmThresh</a></code> output.</p>
</td></tr>
<tr><td><code id="Influence+2B20plots_+3A_bands">bands</code></td>
<td>
<p>logical. If <code>TRUE</code>, plots the confidence and prediction bands.</p>
</td></tr>
<tr><td><code id="Influence+2B20plots_+3A_mult">mult</code></td>
<td>
<p>an object obtained from <code><a href="#topic+lmMult">lmMult</a></code>.</p>
</td></tr>
<tr><td><code id="Influence+2B20plots_+3A_log">log</code></td>
<td>
<p>should the <em>p</em>-values be displayed on a logarithmic y-axis?</p>
</td></tr>
<tr><td><code id="Influence+2B20plots_+3A_which">which</code></td>
<td>
<p>which response value should be shown in <code>stabPlot</code>?</p>
</td></tr>
<tr><td><code id="Influence+2B20plots_+3A_...">...</code></td>
<td>
<p>other plotting parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The corresponding plot.
</p>


<h3>Note</h3>

<p>Cut-off values for the different influence measures are those defined in Belsley, Kuh E &amp; Welsch (1980):<br /><br />
<b>dfbeta slope</b>: <code class="reqn">| \Delta\beta1_i | &gt; 2/\sqrt{n}</code><br />
<b>dffits</b>: <code class="reqn">| \mathrm{dffits}_i | &gt; 2\sqrt{2/n}</code><br />
<b>covratio</b>: <code class="reqn">|\mathrm{covr}_i - 1| &gt; 3k/n</code> <br />
<b>Cook's D</b>: <code class="reqn">D_i &gt; Q_F(0.5, k, n - k)</code><br />
<b>leverage</b>: <code class="reqn">h_{ii} &gt; 2k/n</code><br />
<b>studentized residual</b>: <code class="reqn">t_i &gt; Q_t(0.975, n - k - 1)</code>
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p>Regression diagnostics: Identifying influential data and sources of collinearity.<br />
Belsley DA, Kuh E, Welsch RE.<br />
John Wiley, New York (1980).
</p>
<p>Applied Regression Analysis: A Research Tool.<br />
Rawlings JO, Pantula SG, Dickey DA.<br />
Springer; 2nd Corrected ed. 1998. Corr. 2nd printing 2001.
</p>
<p>Applied Regression Analysis and Generalized Linear Models.<br />
Fox J.<br />
SAGE Publishing, 3rd ed, 2016.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See Examples in 'lmInfl', 'lmThresh' and 'lmMult'.
</code></pre>

<hr>
<h2 id='lmExact'>Create random values that deliver linear regressions with exact parameters</h2><span id='topic+lmExact'></span>

<h3>Description</h3>

<p>Takes self-supplied x/y values or x/random values and transforms these as to deliver linear regressions <code class="reqn">y = \beta_0 + \beta_1x + \varepsilon</code> (with potential replicates) with either<br />
</p>
<p><b>1)</b> exact slope <code class="reqn">\beta_1</code> and intercept <code class="reqn">\beta_0</code>,<br />
<b>2)</b> exact <em>p</em>-value and intercept <code class="reqn">\beta_0</code>, or<br />
<b>3)</b> exact <code class="reqn">R^2</code> and intercept <code class="reqn">\beta_0</code>.<br />
</p>
<p>Intended for testing and education, not for cheating ! ;-)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmExact(x = 1:20, y = NULL, ny = 1, intercept = 0, slope = 0.1, error = 0.1, 
        seed = 123, pval = NULL, rsq = NULL, plot = TRUE, verbose = FALSE, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmExact_+3A_x">x</code></td>
<td>
<p>the predictor values.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_y">y</code></td>
<td>
<p><code>NULL</code>. A possible vector of <code class="reqn">y</code> values with <code>length(x)</code>.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_ny">ny</code></td>
<td>
<p>the number of replicate response values per predictor value.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_intercept">intercept</code></td>
<td>
<p>the desired intercept <code class="reqn">\beta_0</code>.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_slope">slope</code></td>
<td>
<p>the desired slope <code class="reqn">\beta_1</code>.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_error">error</code></td>
<td>
<p>if a single value, the standard deviation <code class="reqn">\sigma</code> for sampling from a normal distribution, or a user-supplied vector of length <code>x</code> with random deviates.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_seed">seed</code></td>
<td>
<p>the random generator seed for reproducibility.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_pval">pval</code></td>
<td>
<p>the desired <em>p</em>-value of the slope.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_rsq">rsq</code></td>
<td>
<p>the desired <code class="reqn">R^2</code>.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_plot">plot</code></td>
<td>
<p>logical. If <code>TRUE</code>, the linear regression is plotted.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_verbose">verbose</code></td>
<td>
<p>logical. If <code>TRUE</code>, a summary is printed to the console.</p>
</td></tr>
<tr><td><code id="lmExact_+3A_...">...</code></td>
<td>
<p>other arguments to <code><a href="stats.html#topic+lm">lm</a></code> or <code><a href="base.html#topic+plot">plot</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For case <b>1)</b>, the <code>error</code> values are added to the exact <code class="reqn">(x_i, \beta_0 + \beta_1 x_i)</code> values, the linear model <code class="reqn">y_i = \beta_0 + \beta_1 x_i + \varepsilon</code> is fit, and the residuals <code class="reqn">y_i - \hat{y_i}</code> are re-added to <code class="reqn">(x_i, \beta_0 + \beta_1 x_i)</code>.<br />
For case <b>2)</b>, the same as in <b>1)</b> is conducted, however the slope delivering the desired <em>p</em>-value is found by an optimizing algorithm.<br />
Finally, for case <b>3)</b>, a QR reconstruction, rescaling and refitting is conducted, using the code found under 'References'.<br />
</p>
<p>If <code>y</code> is supplied, changes in slope, intercept and <em>p</em>-value will deliver the sames residuals as the linear regression through <code>x</code> and <code>y</code>. A different <code class="reqn">R^2</code> will change the response value structure, however.
</p>


<h3>Value</h3>

<p>A list with the following items:<br />
</p>
<table>
<tr><td><code>lm</code></td>
<td>
<p>the linear model of class <code>lm</code>.</p>
</td></tr> 
<tr><td><code>x</code></td>
<td>
<p>the predictor values.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the (random) response values.</p>
</td></tr>
<tr><td><code>summary</code></td>
<td>
<p>the model summary for quick checking of obtained parameters.</p>
</td></tr>
</table>
<p>Using both <code>x</code> and <code>y</code> will give a linear regression with the desired parameter values when refitted.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p>For method <b>3</b>):<br />
http://stats.stackexchange.com/questions/15011/generate-a-random-variable-with-a-defined-correlation-to-an-existing-variable.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## No replicates, intercept = 3, slope = 0.2, sigma = 2, n = 20.
res1 &lt;- lmExact(x = 1:20, ny = 1, intercept = 3, slope = 2, error = 2)

## Same as above, but with 3 replicates, sigma = 1,  n = 20.
res2 &lt;- lmExact(x = 1:20, ny = 3, intercept = 3, slope = 2, error = 1)

## No replicates, intercept = 2 and p-value = 0.025, sigma = 3, n = 50.
## =&gt; slope = 0.063
res3 &lt;- lmExact(x = 1:50, ny = 1, intercept = 2, pval = 0.025, error = 3)

## 5 replicates, intercept = 1, R-square = 0.85, sigma = 2, n = 10.
## =&gt; slope = 0.117
res4 &lt;- lmExact(x = 1:10, ny = 5, intercept = 1, rsq = 0.85, error = 2)

## Heteroscedastic (magnitude-dependent) noise.
error &lt;- sapply(1:20, function(x) rnorm(3, 0, x/10))
res5 &lt;- lmExact(x = 1:20, ny = 3, intercept = 1, slope = 0.2,
                error = error)
                
## Supply own x/y values, residuals are similar to an
## initial linear regression.
X &lt;- c(1.05, 3, 5.2, 7.5, 10.2, 11.7)
set.seed(123)
Y &lt;- 0.5 + 2 * X + rnorm(6, 0, 2)
res6 &lt;- lmExact(x = X, y = Y, intercept = 1, slope = 0.2)
all.equal(residuals(lm(Y ~ X)), residuals(res6$lm))
</code></pre>

<hr>
<h2 id='lmInfl'>Checks and analyzes leave-one-out (LOO) p-values in linear regression</h2><span id='topic+lmInfl'></span>

<h3>Description</h3>

<p>This function calculates leave-one-out (LOO) <em>p</em>-values for all data points and identifies those resulting in &quot;significance reversal&quot;, i.e. in the <em>p</em>-value of the model's slope traversing the user-defined <code class="reqn">\alpha</code>-level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmInfl(model, alpha = 0.05, method = c("pearson", "spearman"), verbose = TRUE, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmInfl_+3A_model">model</code></td>
<td>
<p>the linear model of class <code><a href="stats.html#topic+lm">lm</a></code>.</p>
</td></tr>
<tr><td><code id="lmInfl_+3A_alpha">alpha</code></td>
<td>
<p>the <code class="reqn">\alpha</code>-level to use as the threshold border.</p>
</td></tr>
<tr><td><code id="lmInfl_+3A_method">method</code></td>
<td>
<p>select either parametric (<code>"pearson"</code>) or rank-based (<code>"spearman"</code>) statistics.</p>
</td></tr>
<tr><td><code id="lmInfl_+3A_verbose">verbose</code></td>
<td>
<p>logical. If <code>TRUE</code>, results are displayed on the console.</p>
</td></tr>
<tr><td><code id="lmInfl_+3A_...">...</code></td>
<td>
<p>other arguments to <code><a href="stats.html#topic+lm">lm</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm<br />
1) calculates the <em>p</em>-value of the full model (all points),<br />
2) calculates a LOO-<em>p</em>-value for each point removed,<br />
3) checks for significance reversal in all data points and<br />
4) returns all models as well as classical <code><a href="stats.html#topic+influence.measures">influence.measures</a></code> with LOO-<em>p</em>-values, <code class="reqn">\Delta</code><em>p</em>-values, slopes and standard errors attached.<br />
If <code>method = "spearman"</code>, <em>p</em>-values are based on Spearman Rank correlation, and the values given in the last column of the result matrix are Spearman's <code class="reqn">\rho</code>.
</p>
<p>The idea of <em>p</em>-value influencers was first introduced by Belsley, Kuh &amp; Welsch, and described as an influence measure pertaining directly to the change in <em>t</em>-statistics, that will &quot;show whether the conclusions of hypothesis testing would be affected&quot;, termed <b>dfstat</b> in [1, 2, 3] or <b>dfstud</b> in [4]:
</p>
<p style="text-align: center;"><code class="reqn">\rm{dfstat}_{ij} \equiv \frac{\hat{\beta}_j}{s\sqrt{(X'X)^{-1}_{jj}}}-\frac{\hat{\beta}_{j(i)}}{s_{(i)}\sqrt{(X'_{(i)}X_{(i)})^{-1}_{jj}}}</code>
</p>

<p>where <code class="reqn">\hat{\beta}_j</code> is the <em>j</em>-th estimate, <em>s</em> is the residual standard error, <em>X</em> is the design matrix and (<em>i</em>) denotes the <em>i</em>-th observation deleted.<br />
<b>dfstat</b>, which for the regression's slope <code class="reqn">\beta_1</code> is the difference of <em>t</em>-statistics 
</p>
<p style="text-align: center;"><code class="reqn">\Delta t = t_{\beta1} - t_{\beta1(i)} = \frac{\beta_1}{\rm{s.e.(\beta_1)}} - \frac{\beta_1(i)}{\rm{s.e.(\beta_1(i)})}</code>
</p>

<p>is inextricably linked to the changes in <em>p</em>-value <code class="reqn">\Delta p</code>, calculated from
</p>
<p style="text-align: center;"><code class="reqn">\Delta p = p_{\beta1} - p_{\beta1(i)} = 2\left(1-P_t(t_{\beta1}, \nu)\right) - 2\left(1-P_t(t_{\beta1(i)} , \nu-1)\right)</code>
</p>

<p>where <code class="reqn">P_t</code> is the Student's <em>t</em> cumulative distribution function with <code class="reqn">\nu</code> degrees of freedom, and where significance reversal is attained when <code class="reqn">\alpha \in [p_{\beta1}, p_{\beta1(i)}]</code>.
Interestingly, in linear regression the seemingly mandatory check of the influence of single data points on statistical inference is living in oblivion: apart from [1-4], there is, to the best of our knowledge, no reference to <b>dfstat</b> or <code class="reqn">\Delta p</code> in current literature on influence measures.
</p>
<p>The influence output also includes the more recent Hadi's measure (column &quot;hadi&quot;):
</p>
<p style="text-align: center;"><code class="reqn">H_i = \frac{p_{ii}}{1 - p_{ii}} + \frac{k}{1 - p_{ii}}\frac{d_i^2}{(1-d_i^2)}</code>
</p>

<p>where <code class="reqn">p_{ii}</code> are the diagonals of the hat matrix (leverages), <code class="reqn">k = 2</code> in univariate linear regression and <code class="reqn">d_i = e_i/\sqrt{\rm{SSE}}</code>.
</p>


<h3>Value</h3>

<p>A list with the following items:<br />
</p>
<table>
<tr><td><code>origModel</code></td>
<td>
<p>the original model with all data points.</p>
</td></tr> 
<tr><td><code>finalModels</code></td>
<td>
<p>a list of final models with the influencer(s) removed.</p>
</td></tr>
<tr><td><code>infl</code></td>
<td>
<p>a matrix with the original data, classical <code><a href="stats.html#topic+influence.measures">influence.measures</a></code>, studentized residuals, leverages, LOO-<em>p</em>-values, LOO-slopes/intercepts and their <code class="reqn">\Delta</code>'s, LOO-standard errors and <code class="reqn">R^2</code>s.</p>
</td></tr>
<tr><td><code>sel</code></td>
<td>
<p>a vector with the influencers' indices.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>the selected <code class="reqn">\alpha</code>-level.</p>
</td></tr>
<tr><td><code>origP</code></td>
<td>
<p>the original model's <em>p</em>-value.</p>
</td></tr>
<tr><td><code>stab</code></td>
<td>
<p>the stability measure, see <code><a href="#topic+stability">stability</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p><b>For dfstat / dfstud :</b><br />
1. Regression diagnostics: Identifying influential data and sources of collinearity.<br />
Belsley DA, Kuh E, Welsch RE.<br />
John Wiley, New York, USA (2004).<br />
</p>
<p>2. Econometrics, 5ed.<br />
Baltagi B.<br />
Springer-Verlag Berlin, Germany (2011).<br />
</p>
<p>3. Growth regressions and what the textbooks don't tell you.<br />
Temple J.<br />
<em>Bull Econom Res</em>, <b>52</b>, 2000, 181-205.<br />
</p>
<p>4. Robust Regression and Outlier Detection.<br />
Rousseeuw PJ &amp; Leroy AM.<br />
John Wiley &amp; Sons, New York, NY (1987).<br />
</p>
<p><b>Hadi's measure:</b><br />
A new measure of overall potential influence in linear regression.<br />
Hadi AS.<br />
<em>Comp Stat &amp; Data Anal</em>, <b>14</b>, 1992, 1-27.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example #1 with single influencers and insignificant model (p = 0.115).
## Removal of #18 results in p = 0.0227!
set.seed(123)
a &lt;- 1:20
b &lt;- 5 + 0.08 * a + rnorm(20, 0, 1)
LM1 &lt;- lm(b ~ a)
res1 &lt;- lmInfl(LM1) 
lmPlot(res1)
pvalPlot(res1)
inflPlot(res1)
slsePlot(res1)
stability(res1)

## Example #2 with multiple influencers and significant model (p = 0.0269).
## Removal of #2, #17, #18 or #20 result in crossing p = 0.05!
set.seed(125)
a &lt;- 1:20
b &lt;- 5 + 0.08 * a + rnorm(20, 0, 1)
LM2 &lt;- lm(b ~ a)
res2 &lt;- lmInfl(LM2) 
lmPlot(res2)
pvalPlot(res2)
inflPlot(res2)
slsePlot(res2)
stability(res2)

## Large Example #3 with top 10 influencers and significant model (p = 6.72E-8).
## Not possible to achieve a crossing of alpha with any point despite strong noise.
set.seed(123)
a &lt;- 1:100
b &lt;- 5 + 0.08 * a + rnorm(100, 0, 5)
LM3 &lt;- lm(b ~ a)
res3 &lt;- lmInfl(LM3) 
lmPlot(res3)
stability(res3)

## Example #4 with replicates and single influencer (p = 0.114).
## Removal of #58 results in p = 0.039.
set.seed(123)
a &lt;- rep(1:20, each = 3)
b &lt;- 5 + 0.08 * a + rnorm(20, 0, 2)
LM4 &lt;- lm(b ~ a)
res4 &lt;- lmInfl(LM4) 
lmPlot(res4)
pvalPlot(res4)
inflPlot(res4)
slsePlot(res4)
stability(res4)

## As Example #1, but with weights.
## Removal of #18 results in p = 0.04747.
set.seed(123)
a &lt;- 1:20
b &lt;- 5 + 0.08 * a + rnorm(20, 0, 1)
LM5 &lt;- lm(b ~ a, weights = 1:20)
res5 &lt;- lmInfl(LM5) 
lmPlot(res5)
stability(res5)
</code></pre>

<hr>
<h2 id='lmMult'>Checks and analyzes leave-multiple-out (LMO) p-values in linear regression</h2><span id='topic+lmMult'></span>

<h3>Description</h3>

<p>This function calculates leave-multiple-out (LMO) <em>p</em>-values for an increasing number of data points and identifies those resulting in &quot;significance reversal&quot; of the model, i.e. in the slope's <em>p</em>-value traversing the user-defined <code class="reqn">\alpha</code>-level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmMult(model, max = 5, n = 10000, alpha = 0.05, 
       method = c("pearson", "spearman"), verbose = TRUE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmMult_+3A_model">model</code></td>
<td>
<p>the linear model of class <code><a href="stats.html#topic+lm">lm</a></code>.</p>
</td></tr>
<tr><td><code id="lmMult_+3A_max">max</code></td>
<td>
<p>the maximum number of points to eliminate.</p>
</td></tr>
<tr><td><code id="lmMult_+3A_n">n</code></td>
<td>
<p>the number of samples to draw for each 1...<code>max</code>.</p>
</td></tr>
<tr><td><code id="lmMult_+3A_alpha">alpha</code></td>
<td>
<p>the <code class="reqn">\alpha</code>-level to use as the threshold border.</p>
</td></tr>
<tr><td><code id="lmMult_+3A_method">method</code></td>
<td>
<p>select either parametric (<code>"pearson"</code>) or rank-based (<code>"spearman"</code>) statistics.</p>
</td></tr>
<tr><td><code id="lmMult_+3A_verbose">verbose</code></td>
<td>
<p>logical. If <code>TRUE</code>, results for each 1...<code>max</code> will be printed to the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm<br />
1) calculates the <em>p</em>-value of the full model (all data points),<br />
2) calculates a LMO-<em>p</em>-value for all <code>n</code> sampled groups of 1...<code>max</code> points removed,<br />
3) checks for significance reversal in the resulting model and<br />
4) returns all <code>n</code> samples and the corresponding <em>p</em>-values.<br />
</p>


<h3>Value</h3>

<p>A list with the following items:<br />
</p>
<table>
<tr><td><code>sample</code></td>
<td>
<p>a matrix with all <code>max * n</code> iterations, where a 1 indicates the left-out sample(s), as well as the corresponding <em>p</em>-values and group.</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>
<p>for each 1...<code>max</code> LMO's, the percentage of model significance reversers.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example with single influencers and insignificant model (p = 0.115).
set.seed(123)
a &lt;- 1:20
b &lt;- 5 + 0.08 * a + rnorm(20, 0, 1)
LM1 &lt;- lm(b ~ a)
res1 &lt;- lmMult(LM1)
multPlot(res1)
stability(res1)

## Large example with 100 data points and highly significant model (p = 6.72E-8).
## No significance reversal up to the elimination of 20 points.
set.seed(123)
a &lt;- 1:100
b &lt;- 5 + 0.08 * a + rnorm(100, 0, 5)
LM2 &lt;- lm(b ~ a)
res2 &lt;- lmMult(LM2, max = 20)
multPlot(res2)
stability(res2)

</code></pre>

<hr>
<h2 id='lmThresh'>Finds and analyzes significance reversal regions for each response value</h2><span id='topic+lmThresh'></span>

<h3>Description</h3>

<p>This function finds (by iterating through a grid of values for each response) the approximate response value range(s) in which the regression is significant (when inside) or not (when outside), as defined by <code>alpha</code>. Here, two scenarios can be tested: i) if <code>newobs = FALSE</code> (default), the model's significance is tested by shifting <code class="reqn">y_i</code> along the search grid. If <code>newobs = TRUE</code>, <code class="reqn">y_i</code> is kept fixed and a <code>new</code> <code>obs</code>ervation <code class="reqn">y_{2i}</code> is added and shifted along the search grid. Hence, this function tests the regression for the sensitivity of being reversed in its significance through minor shifting of the original or added response values, as opposed to the effect of point removal (<code><a href="#topic+lmInfl">lmInfl</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmThresh(model, factor = 5, alpha = 0.05, 
         method = c("pearson", "spearman"),
         steps = 10000, newobs = FALSE, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmThresh_+3A_model">model</code></td>
<td>
<p>the linear model of class <code><a href="stats.html#topic+lm">lm</a></code>.</p>
</td></tr>
<tr><td><code id="lmThresh_+3A_factor">factor</code></td>
<td>
<p>a factor for the initial search grid. See 'Details'.</p>
</td></tr>
<tr><td><code id="lmThresh_+3A_alpha">alpha</code></td>
<td>
<p>the <code class="reqn">\alpha</code>-level to use as the threshold border.</p>
</td></tr>
<tr><td><code id="lmThresh_+3A_method">method</code></td>
<td>
<p>select either parametric (<code>"pearson"</code>) or rank-based (<code>"spearman"</code>) statistics.</p>
</td></tr>
<tr><td><code id="lmThresh_+3A_steps">steps</code></td>
<td>
<p>the number of steps within the search range. See 'Details'.</p>
</td></tr>
<tr><td><code id="lmThresh_+3A_newobs">newobs</code></td>
<td>
<p>logical. Should the significance region for each <code class="reqn">y_i</code> be calculated from shifting <code class="reqn">y_i</code> or from keeping <code class="reqn">y_i</code> fixed and adding a new observation <code class="reqn">y2_i</code>?</p>
</td></tr>
<tr><td><code id="lmThresh_+3A_...">...</code></td>
<td>
<p>other arguments to future methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In a first step, a grid is created with a range from <code class="reqn">y_i \pm \mathrm{factor} \cdot \mathrm{range}(y_{1...n})</code> with <code>steps</code> cuts. For each cut, the <em>p</em>-value is calculated for the model when <code class="reqn">y_i</code> is shifted to that value (<code>newobs = TRUE</code>) or a second observation <code class="reqn">y_{2i}</code> is added to the fixed <code class="reqn">y_i</code> (<code>newobs = TRUE</code>). When the original model <code class="reqn">y =  \beta_0 + \beta_1x + \varepsilon</code> is significant (<em>p</em> &lt; <code>alpha</code>), there are two boundaries that result in insignificance: one decreases the slope <code class="reqn">\beta_1</code> and the other inflates the standard error <code class="reqn">\mathrm{s.e.}(\beta_1)</code> in a way that <code class="reqn">P_t(\frac{\beta_1}{\mathrm{s.e.}(\beta_1)}, n-2) &gt; \alpha</code>. If the original model was insignificant, also two boundaries exists that either increase <code class="reqn">\beta_1</code> or reduce <code class="reqn">\mathrm{s.e.}(\beta_1)</code>. Often, no boundaries are found and increasing the <code>factor</code> grid range may alleviate this problem.
</p>
<p>This function is quite fast (~ 300ms/10 response values), as the slope's <em>p</em>-value is calculated from the <code>corr.test</code> function of the 'psych' package, which utilizes matrix multiplication and vectorized <code><a href="stats.html#topic+pt">pt</a></code> calculation. The vector of correlation coefficients <code class="reqn">r_i</code> from the <code><a href="stats.html#topic+cor">cor</a></code> function is transformed to t-values by </p>
<p style="text-align: center;"><code class="reqn">t_i = \frac{r_i\sqrt{n-2}}{\sqrt{1-r_i^2}}</code>
</p>
<p> which is equivalent to that employed in the linear regression's slope test. 
</p>


<h3>Value</h3>

<p>A list with the following items:<br />
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the predictor values.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response values.</p>
</td></tr>
<tr><td><code>pmat</code></td>
<td>
<p>the <em>p</em>-value matrix, with <code>length(x)</code> columns and <code>steps</code> rows.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>the selected <code class="reqn">\alpha</code>-level.</p>
</td></tr>
<tr><td><code>ySeq</code></td>
<td>
<p>the grid sequence for which the algorithm calculates <em>p</em>-values when <code class="reqn">y_i</code> is shifted within.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>the original <code><a href="stats.html#topic+lm">lm</a></code> model.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the original <code><a href="stats.html#topic+model.frame">model.frame</a></code>.</p>
</td></tr>
<tr><td><code>eosr</code></td>
<td>
<p>the y-values of the ends of the significance region.</p>
</td></tr>
<tr><td><code>diff</code></td>
<td>
<p>the <code class="reqn">\Delta</code> value between <code class="reqn">y_i</code> and the nearest border of significance reversal.</p>
</td></tr>
<tr><td><code>closest</code></td>
<td>
<p>the (approx.) value of the nearest border of significance reversal.</p>
</td></tr>
<tr><td><code>newobs</code></td>
<td>
<p>should a new observation be added?</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Significant model, no new observation.
set.seed(125)
a &lt;- 1:20
b &lt;- 5 + 0.08 * a + rnorm(length(a), 0, 1)
LM1 &lt;- lm(b ~ a)
res1 &lt;- lmThresh(LM1)
threshPlot(res1)
stability(res1)

## Insignificant model, no new observation.
set.seed(125)
a &lt;- 1:20
b &lt;- 5 + 0.08 * a + rnorm(length(a), 0, 2)
LM2 &lt;- lm(b ~ a)
res2 &lt;- lmThresh(LM2)
threshPlot(res2)
stability(res2)

## Significant model, new observation.
## Some significance reversal regions
## are within the prediction interval,
## e.g. 1 to 6 and 14 to 20.
set.seed(125)
a &lt;- 1:20
b &lt;- 5 + 0.08 * a + rnorm(length(a), 0, 1)
LM3 &lt;- lm(b ~ a)
res3 &lt;- lmThresh(LM3, newobs = TRUE)
threshPlot(res3)
stability(res3)

## More detailed example to the above:
## a (putative) new observation within the
## prediction interval may reverse significance.
set.seed(125)
a &lt;- 1:20
b &lt;- 5 + 0.08 * a + rnorm(length(a), 0, 1)
LM1 &lt;- lm(b ~ a)
summary(LM1) # =&gt; p-value = 0.02688
res1 &lt;- lmThresh(LM1, newobs = TRUE)
threshPlot(res1)
st &lt;- stability(res1, pval = TRUE)
st$stats # =&gt; upper prediction boundary = 7.48
         # and eosr = 6.49
stabPlot(st, 1)
## reverse significance if we add a new response y_1 = 7
a &lt;- c(1, a)
b &lt;- c(7, b)
LM2 &lt;- lm(b ~ a)
summary(LM2) # =&gt; p-value = 0.0767
</code></pre>

<hr>
<h2 id='shinyInfl'>Initializes a Shiny App with all implemented methods of this package</h2><span id='topic+shinyInfl'></span>

<h3>Description</h3>

<p>A comprehensive Shiny App that facilitates the import of user-supplied data and subsequent detailed testing of all implemented methods in this package. Analysis results can be exported as plots and text. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shinyInfl() 
</code></pre>


<h3>Arguments</h3>

<p>None.
</p>


<h3>Value</h3>

<p>The analysis including plots and result tables.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## shinyInfl() # &lt;= to initialize
</code></pre>

<hr>
<h2 id='simInfl'>Simulates significance reversals and calculates their influence parameters</h2><span id='topic+simInfl'></span>

<h3>Description</h3>

<p>This function simulates linear regressions and stores the parameters and influence measures of all simulations that resulted in LOO significance reversal, developed for research purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simInfl(x = 1:10, slope = 0.02, intercept = 1, error = 0.05, nrev = 1000, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simInfl_+3A_x">x</code></td>
<td>
<p>the <code class="reqn">x</code> values to be supplied to <code><a href="#topic+lmExact">lmExact</a></code>.</p>
</td></tr>
<tr><td><code id="simInfl_+3A_slope">slope</code></td>
<td>
<p>the slope <code class="reqn">\beta_1</code> to be supplied to <code><a href="#topic+lmExact">lmExact</a></code>.</p>
</td></tr>
<tr><td><code id="simInfl_+3A_intercept">intercept</code></td>
<td>
<p>the intercept <code class="reqn">\beta_0</code> to be supplied to <code><a href="#topic+lmExact">lmExact</a></code>.</p>
</td></tr>
<tr><td><code id="simInfl_+3A_error">error</code></td>
<td>
<p>the <code class="reqn">\varepsilon</code> value to be supplied to <code><a href="#topic+lmExact">lmExact</a></code>.</p>
</td></tr>
<tr><td><code id="simInfl_+3A_nrev">nrev</code></td>
<td>
<p>the number of desired significance reversals.</p>
</td></tr>
<tr><td><code id="simInfl_+3A_...">...</code></td>
<td>
<p>other parameters to <code><a href="#topic+lmExact">lmExact</a></code> and <code><a href="#topic+lmInfl">lmInfl</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Loops over an undefined number of EXACT regressions (<code><a href="#topic+lmExact">lmExact</a></code>) with incrementing random seeds, stores all models and in case of significance reversal, parameters and influence measures (<code><a href="#topic+lmInfl">lmInfl</a></code>). The simulation terminates when <code>nrev</code> reversals are counted.
</p>


<h3>Value</h3>

<p>A list with the following two items:
</p>
<table>
<tr><td><code>models</code></td>
<td>
<p>the linear models of all reversals.</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>
<p>the stored matrix with the resulting parameters and influence measures for all <code>nrev</code> reversals.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example with slight slope, intercept = 0.5 and 10 reversals.
res &lt;- simInfl(x = 1:10, intercept = 0.5, slope = 0.02, error = 0.05, nrev = 10)

## Plot Cook's D versus delta-P values
## and insert common cut-off.
plot(res$mat[, "cook.d"], res$mat[, "dP"], pch = 16, cex = 0.5,
     xlab = "Cook's D", ylab = "delta-P")
thresh &lt;- qf(0.5, 2, 8)  # threshold value for Qf(0.5, npar, df)
abline(v = thresh, col = "darkred", lwd = 2)  

## Plot dfbeta slope versus delta-P values
## and insert common cut-off.
plot(res$mat[, "dfb.Slope"], res$mat[, "dP"], pch = 16, cex = 0.5,
     xlab = "dfbeta Slope", ylab = "delta-P")
thresh &lt;- 2/sqrt(10)  # 2/sqrt(N)
abline(v = thresh, col = "darkred", lwd = 2)  

## Plot dffits versus delta-P values
## and insert common cut-off.
plot(abs(res$mat[, "dffit"]), res$mat[, "dP"], pch = 16, cex = 0.5,
     xlab = "dffits", ylab = "delta-P")
thresh &lt;- 2 * sqrt(2/10)  # 2 * sqrt(nPar/N)
abline(v = thresh, col = "darkred", lwd = 2)  


## More illustrative with more reverser samples!
## Example with slight slope, intercept = 0.5 and 10 reversals.
res &lt;- simInfl(x = 1:10, intercept = 0.5, slope = 0.02, error = 0.05, nrev = 200)
plot(res$mat[, "cook.d"], res$mat[, "dP"], pch = 16, cex = 0.5,
     xlab = "Cook's D", ylab = "delta-P")
thresh &lt;- qf(0.5, 2, 8)  # threshold value for Qf(0.5, npar, df)
abline(v = thresh, col = "darkred", lwd = 2)  

</code></pre>

<hr>
<h2 id='stability'>Calculates stability values for results of 'lmInfl', 'lmMult' and 'lmThresh'</h2><span id='topic+stability'></span>

<h3>Description</h3>

<p>This function calculates stability values for LOO (<code><a href="#topic+lmInfl">lmInfl</a></code>), LMO (<code><a href="#topic+lmMult">lmMult</a></code>) and response value shifting/addition (<code><a href="#topic+lmThresh">lmThresh</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stability(x, pval = FALSE, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stability_+3A_x">x</code></td>
<td>
<p>a result of either <code><a href="#topic+lmInfl">lmInfl</a></code>, <code><a href="#topic+lmMult">lmMult</a></code> or <code><a href="#topic+lmThresh">lmThresh</a></code>.</p>
</td></tr>
<tr><td><code id="stability_+3A_pval">pval</code></td>
<td>
<p>logical. If <code>TRUE</code>, for <code><a href="#topic+lmThresh">lmThresh</a></code>, objects an exact <em>p</em>-value is calculated for a future response to reverse significance.</p>
</td></tr>
<tr><td><code id="stability_+3A_...">...</code></td>
<td>
<p>other parameters, not yet implemented.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For results of <code><a href="#topic+lmInfl">lmInfl</a></code>:<br />
A [0, 1]-bounded stability measure <code class="reqn">S = 1-\frac{n}{N}</code>, with <code class="reqn">n</code> = number of influencers (significance reversers) and <code class="reqn">N</code> = total number of response values.<br /><br />
For results of <code><a href="#topic+lmMult">lmMult</a></code>:<br />
For each 1...<code>max</code>, the percentage of all resamples that did *NOT* result in significance reversal.<br /><br />
For results of <code><a href="#topic+lmThresh">lmThresh</a></code>:<br />
A [0, 1]-bounded stability measure <code class="reqn">S = 1-\frac{n}{N}</code>, with <code class="reqn">n</code> = number of response values where one of the ends of the significance region is within the prediction interval and <code class="reqn">N</code> = total number of response values.<br />
If <code>pval = TRUE</code>, the exact <em>p</em>-value is calculated in the following manner:<br />
</p>
<p>1) Mean square error (MSE) and prediction standard error (se) are calculated from the linear model:<br />
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{MSE} = \sum_{i=1}^n \frac{(y_i - \hat{y}_i)^2}{n-2} \quad\quad \mathrm{se}_i = \sqrt{\mathrm{MSE} \cdot \left(1 + \frac{1}{n} + \frac{(x_i - \bar{x}_i)^2}{\sum_{i=1}^n (x_i - \bar{x}_i)^2}\right)}</code>
</p>

<p>2) Upper and lower prediction intervals boundaries are calculated for each <code class="reqn">\hat{y}_i</code>:<br />
</p>
<p style="text-align: center;"><code class="reqn">\hat{y}_i \pm Q_t(\alpha/2, n-2) \cdot \rm{se}_i</code>
</p>

<p>The prediction interval around <code class="reqn">\hat{y}_i</code> is a scaled/shifted <code class="reqn">t</code>-distribution with density function </p>
<p style="text-align: center;"><code class="reqn">P_{tss}(y, n-2) = \frac{1}{\rm{se}_i} \cdot P_t\left(\frac{y - \hat{y}_i}{\rm{se}_i}, n-2\right)</code>
</p>
<p>, where <code class="reqn">P_t</code> is the density function of the central, unit-variance <code class="reqn">t</code>-distribution.<br />
3) The probability of either shifting the response value (if <code>lmThresh(..., newobs = FALSE)</code>) or including a future response value <code class="reqn">y_{2i}</code> (if <code>lmThresh(..., newobs = TRUE)</code>) to reverse the significance of the linear model is calculated as the integral between the end of the significance region (eosr) and the upper/lower <code class="reqn">\alpha/2, 1-\alpha/2</code> prediction interval:
</p>
<p style="text-align: center;"><code class="reqn">P(\mathrm{reverse}) = \int_{\mathrm{eosr}}^{1-\alpha/2} P_{tss}(y, n-2)dy \quad \mathrm{or} \quad \int_{\alpha/2}^{\mathrm{eosr}} P_{tss}(y, n-2)dy</code>
</p>



<h3>Value</h3>

<p>The stability value.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See examples in 'lmInfl' and 'lmThresh'.

## The implemented strategy of calculating the
## probability of significance reversal, as explained above
## and compared to 'stabPlot'.
set.seed(125)
a &lt;- 1:20
b &lt;- 5 + 0.08 * a + rnorm(length(a), 0, 1)
LM1 &lt;- lm(b ~ a)
res1 &lt;- lmThresh(LM1, newobs = TRUE)
st1 &lt;- stability(res1, pval = TRUE)

## Let's check that the prediction interval encompasses 95%:
dt.scaled &lt;- function(x, df, mu, s) 1/s * dt((x - mu)/s, df)
integrate(dt.scaled, lower = st1$stats[1, "lower"], st1$stats[1, "upper"], 
          df = 18, mu = st1$stats[1, "fitted"], s = st1$stats[1, "se"])
## =&gt; 0.95 with absolute error &lt; 8.4e-09

## This is the interval between "end of significance region" and upper 
## prediction boundary:
integrate(dt.scaled, lower = st1$stats[1, "eosr.2"], st1$stats[1, "upper"], 
          df = 18, mu = st1$stats[1, "fitted"], s = st1$stats[1, "se"])
## =&gt; 0.09264124 with absolute error &lt; 1e-15

## We can recheck this value by P(B) - P(A):
pt.scaled &lt;- function(x, df, mu, s) pt((x - mu)/s, df)
pA &lt;- pt.scaled(x = st1$stats[1, "eosr.2"], df =  18, mu = st1$stats[1, "fitted"], 
                s = st1$stats[1, "se"])
0.975 - pA 
##  =&gt; 0.09264124 as above
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
