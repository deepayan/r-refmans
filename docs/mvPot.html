<!DOCTYPE html><html><head><title>Help for package mvPot</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mvPot}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mvPot-package'>
<p>Multivariate Peaks-over-Threshold Modelling for Extreme Events Analysis</p></a></li>
<li><a href='#censoredLikelihoodBR'><p>Censored log-likelihood function for the Brown&ndash;Resnick model.</p></a></li>
<li><a href='#censoredLikelihoodXS'><p>Censored log-likelihood function of the extremal Student model</p></a></li>
<li><a href='#genVecQMC'><p>Generating vectors for lattice rules</p></a></li>
<li><a href='#mvtNormQuasiMonteCarlo'><p>Multivariate normal distribution function</p></a></li>
<li><a href='#mvTProbQuasiMonteCarlo'><p>Multivariate t distribution function</p></a></li>
<li><a href='#rExtremalStudentParetoProcess'><p>Simulation of extremal Student generalized Pareto vectors</p></a></li>
<li><a href='#scoreEstimation'><p>Gradient score function for the Brown&ndash;Resnick model.</p></a></li>
<li><a href='#simulBrownResnick'><p>Simulation of Brown&ndash;Resnick random vectors</p></a></li>
<li><a href='#simulPareto'><p>Simulate Pareto random vectors</p></a></li>
<li><a href='#spectralLikelihood'><p>Spectral log-likelihood function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Multivariate Peaks-over-Threshold Modelling for Spatial Extreme
Events</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-13</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools for high-dimensional peaks-over-threshold inference and simulation
  of spatial extremal processes. Key references include de Fondeville and Davison (2018) &lt;<a href="https://doi.org/10.1093%2Fbiomet%2Fasy026">doi:10.1093/biomet/asy026</a>&gt;, Thibaud and Opitz (2015) &lt;<a href="https://doi.org/10.1093%2Fbiomet%2Fasv045">doi:10.1093/biomet/asv045</a>&gt;, Wadsworth and Tawn &lt;<a href="https://doi.org/10.1093%2Fbiomet%2Fast042">doi:10.1093/biomet/ast042</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, evd, numbers, gmp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/r-fndv/mvPot">https://github.com/r-fndv/mvPot</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-13 07:56:59 UTC; dedeloye</td>
</tr>
<tr>
<td>Author:</td>
<td>Raphael de Fondeville [aut, cre],
  Leo Belzile <a href="https://orcid.org/0000-0002-9135-014X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Emeric Thibaud [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Raphael de Fondeville &lt;raphael.de-fondeville@epfl.ch&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-13 21:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='mvPot-package'>
Multivariate Peaks-over-Threshold Modelling for Extreme Events Analysis
</h2><span id='topic+mvPot-package'></span><span id='topic+mvPot'></span>

<h3>Description</h3>

<p>The mvPot package provides functions to perform high-dimensional peaks-over-threshold inference
of spatial processes such as the Brown&ndash;Resnick. Parallel implementation for censored likelihood
allows up to 500 locations, whereas the gradient score can handle thousands of locations. The package also
includes simulations algorithms for the Brown-Resnick max-stable process as well as its associated Pareto process.
A tutorial describing a complete case study of Red Sea temperature anomalies extremes can be found at <a href="https://github.com/r-fndv/mvPot_tutorial">https://github.com/r-fndv/mvPot_tutorial</a>.
</p>


<h3>Details</h3>

<p>The mvPot package provides functions to perform high-dimensional peaks-over-threshold inference of spatial processes such as the Brown&ndash;Resnick.
</p>
<p><code>spectralLikelihood</code> relies on the spectral likelihood as developed by Engelke et al. (2015). This methods is fast to compute, however it is not robust with regard to non-extreme components.
</p>
<p><code>censoredLikelihoodBR</code> (Wadsworth and Tawn, 2013) is a likelihood function for exceedances with at least one component exceeding a threshold and where low components, i.e., components under their threshold,. This approach is robust and performs best but requires heavy computations. The implementation in this package makes use of quasi-Monte Carlo estimation and thus can handle 100 locations in a reasonable time and up to 500 when parallelized. The analog function for extremal Student processes is <code>censoredLikelihoodXS</code>.
</p>
<p><code>scoreEstimation</code> is a faster alternative to the <code>censoredLikelihood</code>, which is more robust than <code>spectralLikelihood</code>. This method can also be used with any kind of differentiable risk functional (Fondeville and Davison, 2016). Here the algorithm is limited only by matrix inversion and thus thousands of locations can be used.
</p>
<p><code>simulBrownResnick</code> is an exact algorithm for simulation of Brown-Resnick max-stable processes as described in Dombry et al. (2015).
</p>
<p><code>simulPareto</code> allows for simulation of Pareto processes associated to log-Gaussian random functions.
</p>
<p><code>rExtremalStudentParetoProcess</code> allows for simulation of Pareto processes associated to Student random functions, using the accept-reject algorithm of Thibaud and Opitz (2015).
</p>
<p><code>mvtNormQuasiMonteCarlo</code> and <code>mvTProbQuasiMonteCarlo</code> are Cpp functions to evaluate the distribution function of Gaussian and t integrals, using a quasi-Monte Carlo algorithm based on randomly shifted lattice rules.
</p>


<h3>Author(s)</h3>

<p>Raphael de Fondeville
</p>
<p>Maintainer: Raphael de Fondeville &lt;raphael.de-fondeville@epfl.ch&gt;
</p>


<h3>References</h3>

<p>de Fondeville, R. and Davison A. (2018). High-dimensional peaks-over-threshold inference. Biometrika, 105(3), 575-592.
</p>
<p>Engelke, S. et al. (2015). Estimation of Huesler-Reiss Distributions and Brown-Resnick Processes. Journal of the Royal Statistical Society: Series B, 77(1), 239-265
</p>
<p>Wadsworth, J.L. and Tawn, J.A. (2013). Efficient inference for spatial extreme value processes associated to log-Gaussian random functions. Biometrika, 101(1), 1-15.
</p>
<p>Thibaud, E. and T. Opitz (2015). Efficient inference and simulation for elliptical Pareto processes. Biometrika, 102(4), 855-870.
</p>
<p>Dombry, C., Engelke, S. and Oesting, M. (2016). Exact simulation of max-stable processes. Biometrika, 103(2), 303-317.
</p>
<p>Genz, A. and Bretz, F. (2009). Computations of Multivariate Normal and t Probabilities, volume 105. Springer: Dordrecht.
</p>
<p>Genz, A. (2013). QSILATMVNV <a href="http://www.math.wsu.edu/faculty/genz/software/software.html">http://www.math.wsu.edu/faculty/genz/software/software.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Define semi-variogram function
vario &lt;- function(h, alpha = 1.5){
    norm(h,type = "2")^alpha
}

#Define locations
loc &lt;- expand.grid(1:4, 1:4)

#Simulate data
obs &lt;- simulPareto(1000, loc, vario)

#Evaluate risk functional
sums &lt;- sapply(obs, sum)

#Define weighting function
weigthFun &lt;- function(x, u){
 x * (1 - exp(-(sum(x) / u - 1)))
}

#Define partial derivative of weighting function
dWeigthFun &lt;- function(x, u){
 (1 - exp(-(sum(x) / u - 1))) + (x / u) * exp( - (sum(x) / u - 1))
}


#Select exceedances
threshold &lt;- quantile(sums, 0.9)
exceedances &lt;- obs[sums &gt; threshold]

#Define objective function
objectiveFunction = function(parameter, exceedances, loc, vario, weigthFun, dWeigthFun, threshold){

 #Define semi-variogram for the corresponding parameters
 varioModel &lt;- function(h){
  vario(h, parameter[1])
 }

 #Compute score
 scoreEstimation(exceedances, loc, varioModel, weigthFun, dWeigthFun, u = threshold)
}

#Estimate the parameter by optimization of the objective function
est &lt;- optim(par = c(1.5),
             fn = objectiveFunction,
             exceedances = exceedances,
             loc = loc,
             vario = vario,
             weigthFun = weigthFun,
             dWeigthFun = dWeigthFun,
             threshold = threshold,
             control = list(maxit = 100, trace = 1),
             lower = c(0.01),
             upper = c(1.99),
             method = "L-BFGS-B")
</code></pre>

<hr>
<h2 id='censoredLikelihoodBR'>Censored log-likelihood function for the Brown&ndash;Resnick model.</h2><span id='topic+censoredLikelihoodBR'></span><span id='topic+censoredLikelihood'></span>

<h3>Description</h3>

<p>Compute the peaks-over-threshold censored negative log-likelihood function for the Brown&ndash;Resnick model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>censoredLikelihoodBR(
  obs,
  loc,
  vario,
  u,
  p = 499L,
  vec = NULL,
  nCores = 1L,
  cl = NULL,
  likelihood = "mgp",
  ntot = NULL,
  ...
)

censoredLikelihood(
  obs,
  loc,
  vario,
  u,
  p = 499L,
  vec = NULL,
  nCores = 1L,
  cl = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="censoredLikelihoodBR_+3A_obs">obs</code></td>
<td>
<p>List of vectors for which at least one component exceeds a high threshold.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodBR_+3A_loc">loc</code></td>
<td>
<p>Matrix of coordinates as given by <code>expand.grid()</code>.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodBR_+3A_vario">vario</code></td>
<td>
<p>Semi-variogram function taking a vector of coordinates as input.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodBR_+3A_u">u</code></td>
<td>
<p>Vector of threshold under which to censor components.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodBR_+3A_p">p</code></td>
<td>
<p>Number of samples used for quasi-Monte Carlo estimation. Must be a prime number.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodBR_+3A_vec">vec</code></td>
<td>
<p>Generating vector for the quasi-Monte Carlo procedure. For a given prime <code>p</code> and dimension,
can be computed using <code>genVecQMC</code>.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodBR_+3A_ncores">nCores</code></td>
<td>
<p>Number of cores used for the computation</p>
</td></tr>
<tr><td><code id="censoredLikelihoodBR_+3A_cl">cl</code></td>
<td>
<p>Cluster instance as created by <code>makeCluster</code> of the <code>parallel</code> package.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodBR_+3A_likelihood">likelihood</code></td>
<td>
<p>vector of strings specifying the contribution. Either <code>"mgp"</code> for multivariate generalized Pareto,
<code>"poisson"</code> for a Poisson contribution for the observations falling below or <code>"binom"</code> for a binomial contribution.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodBR_+3A_ntot">ntot</code></td>
<td>
<p>integer number of observations below and above the threshold, to be used with Poisson or binomial likelihood</p>
</td></tr>
<tr><td><code id="censoredLikelihoodBR_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to Cpp routine.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the censored negative log-likelihood function based on the representation
developed by Wadsworth et al. (2014) and Engelke et al. (2015). Margins must have been
standardized first, for instance to the unit Frechet scale.
</p>


<h3>Value</h3>

<p>Negative censored log-likelihood for the set of observations <code>obs</code> and semi-variogram <code>vario</code> with <code>attributes</code>  <code>exponentMeasure</code> for all of the <code>likelihood</code> type selected, in the order <code>"mgp"</code>, <code>"poisson"</code>, <code>"binom"</code>.
</p>


<h3>Author(s)</h3>

<p>Raphael de Fondeville
</p>


<h3>References</h3>

<p>Wadsworth, J. L. and J. A. Tawn (2014). Efficient inference for spatial extreme value
processes associated to log-Gaussian random functions. Biometrika, 101(1), 1-15.
</p>
<p>Asadi, P., Davison A. C. and S. Engelke (2015). Extremes on River Networks.
Annals of Applied Statistics, 9(4), 2023-2050.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Define semi-variogram function
vario &lt;- function(h){
   0.5 * norm(h, type = "2")^1.5
}

#Define locations
loc &lt;- expand.grid(1:4, 1:4)

#Simulate data
obs &lt;- simulPareto(1000, loc, vario)

#Evaluate risk functional
maxima &lt;- sapply(obs, max)
thres &lt;- quantile(maxima, 0.9)

#Select exceedances
exceedances &lt;- obs[maxima &gt; thres]

#Compute generating vector
p &lt;- 499
latticeRule &lt;- genVecQMC(p, (nrow(loc) - 1))
primeP &lt;- latticeRule$primeP
vec &lt;- latticeRule$genVec


#Compute log-likelihood function
censoredLikelihoodBR(obs = exceedances, loc = loc, vario = vario,
 u = thres, p = primeP, vec = vec, ntot = 1000)
</code></pre>

<hr>
<h2 id='censoredLikelihoodXS'>Censored log-likelihood function of the extremal Student model</h2><span id='topic+censoredLikelihoodXS'></span>

<h3>Description</h3>

<p>Compute the peaks-over-threshold censored negative log-likelihood function for the extremal Student model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>censoredLikelihoodXS(
  obs,
  loc,
  corrFun,
  nu,
  u,
  p = 499L,
  vec = NULL,
  nCores = 1L,
  cl = NULL,
  likelihood = "mgp",
  ntot = NULL,
  std = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="censoredLikelihoodXS_+3A_obs">obs</code></td>
<td>
<p>List of vectors for which at least one component exceeds a high threshold.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_loc">loc</code></td>
<td>
<p>Matrix of coordinates as given by <code>expand.grid()</code>.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_corrfun">corrFun</code></td>
<td>
<p>correlation function taking a vector of coordinates as input.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_nu">nu</code></td>
<td>
<p>degrees of freedom of the Student process</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_u">u</code></td>
<td>
<p>Vector of thresholds under which to censor components.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_p">p</code></td>
<td>
<p>Number of samples used for quasi-Monte Carlo estimation. Must be a prime number.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_vec">vec</code></td>
<td>
<p>Generating vector for the quasi-Monte Carlo procedure. For a given <code>p</code> and dimensionality,
can be computed using <code>genVecQMC</code>.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_ncores">nCores</code></td>
<td>
<p>Number of cores used for the computation</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_cl">cl</code></td>
<td>
<p>Cluster instance as created by <code>makeCluster</code> of the <code>parallel</code> package.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_likelihood">likelihood</code></td>
<td>
<p>vector of string specifying the contribution. Either <code>"mgp"</code> for multivariate generalized Pareto, 
<code>"poisson"</code> for a Poisson contribution for the observations falling below or <code>"binom"</code> for a binomial contribution.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_ntot">ntot</code></td>
<td>
<p>integer number of observations below and above the threshold, to be used with Poisson or binomial likelihood</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_std">std</code></td>
<td>
<p>logical; if <code>std = TRUE</code>, consider <code>obs/u</code> for scalar u and exceedances over 1 rather than <code>obs</code> <code class="reqn">&gt;</code> <code>u</code> for potentially vector <code>u</code>. This affects the value of the log-likelihood function. Default to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="censoredLikelihoodXS_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to Cpp routine.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the censored log-likelihood function based on the representation
developed by Ribatet (2013); see also Thibaud and Opitz (2015). Margins must have been
standardized, for instance to unit Frechet.
</p>


<h3>Value</h3>

<p>Negative censored log-likelihood function for the set of observations <code>obs</code> and correlation function <code>corrFun</code>, with <code>attributes</code>  <code>exponentMeasure</code> for all of the <code>likelihood</code> type selected, in the order <code>"mgp"</code>, <code>"poisson"</code>, <code>"binom"</code>..
</p>


<h3>Author(s)</h3>

<p>Leo Belzile
</p>


<h3>References</h3>

<p>Thibaud, E. and T. Opitz (2015). Efficient inference and simulation for elliptical Pareto processes. Biometrika, 102(4), 855-870.
</p>
<p>Ribatet, M. (2013). Spatial extremes: max-stable processes at work. JSFS, 154(2), 156-177.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Define correlation function
corrFun &lt;- function(h, alpha = 1, lambda = 1){
   exp(-norm(h, type = "2")^alpha/lambda)
}

#Define locations
loc &lt;- expand.grid(1:4, 1:4)

#Compute generating vector
p &lt;- 499L
latticeRule &lt;- genVecQMC(p, (nrow(loc) - 1))
primeP &lt;- latticeRule$primeP
vec &lt;- latticeRule$genVec

#Simulate data
Sigma &lt;- exp(-as.matrix(dist(loc))^0.8)
obs &lt;- rExtremalStudentParetoProcess(n = 1000, nu = 5, Sigma = Sigma)
obs &lt;- split(obs, row(obs))

#Evaluate risk functional
maxima &lt;- sapply(obs, max)
thresh &lt;- quantile(maxima, 0.9)

#Select exceedances
exceedances &lt;- obs[maxima &gt; thresh]

#Compute log-likelihood function
eval &lt;- censoredLikelihoodXS(exceedances, loc, corrFun, nu = 5, u = thresh, primeP, vec)

</code></pre>

<hr>
<h2 id='genVecQMC'>Generating vectors for lattice rules</h2><span id='topic+genVecQMC'></span>

<h3>Description</h3>

<p>Compute an efficient generating vector for quasi-Monte Carlo estimation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genVecQMC(p, d, bt = rep(1, d), gm = c(1, (4/5)^(0:(d - 2))))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genVecQMC_+3A_p">p</code></td>
<td>
<p>number of samples to use in the quasi-Monte Carlo procedure.</p>
</td></tr>
<tr><td><code id="genVecQMC_+3A_d">d</code></td>
<td>
<p>Dimension of the multivariate integral to estimate.</p>
</td></tr>
<tr><td><code id="genVecQMC_+3A_bt">bt</code></td>
<td>
<p>Tuning parameter for finding the vector. See D. Nuyens and R. Cools (2004) for more details.</p>
</td></tr>
<tr><td><code id="genVecQMC_+3A_gm">gm</code></td>
<td>
<p>Tuning parameter for finding the vector. See D. Nuyens and R. Cools (2004) for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes a generating vector for efficient multivariate integral estimation
based on D. Nuyens and R. Cools (2004). If <code>p</code> is not a prime, the nearest smaller prime is used instead.
</p>


<h3>Value</h3>

<p><code>primeP</code>, the highest prime number smaller than <code>p</code> and <code>genVec</code>, a <code>d</code>-dimensional generating vector defining an efficient lattice rule for <code>primeP</code> samples.
</p>


<h3>References</h3>

<p>Nuyens, D. and R. Cools (2004). Fast component-by-component construction, a reprise for different kernels. In Monte Carlo and Quasi-Monte Carlo Methods 2004, H. Niederreiter and D. Talay, eds. Springer: Berlin, 373-87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Define the number of sample.
p &lt;- 500

#Choose a dimension
d &lt;- 300

#Compute the generating vector
latticeRule &lt;- genVecQMC(p,d)

print(latticeRule$primeP)
print(latticeRule$genVec)

</code></pre>

<hr>
<h2 id='mvtNormQuasiMonteCarlo'>Multivariate normal distribution function</h2><span id='topic+mvtNormQuasiMonteCarlo'></span>

<h3>Description</h3>

<p>Estimate the multivariate distribution function with quasi-Monte Carlo method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvtNormQuasiMonteCarlo(p, upperBound, cov, genVec, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvtNormQuasiMonteCarlo_+3A_p">p</code></td>
<td>
<p>Number of samples used for quasi-Monte Carlo estimation. Must be a prime number.</p>
</td></tr>
<tr><td><code id="mvtNormQuasiMonteCarlo_+3A_upperbound">upperBound</code></td>
<td>
<p>Vector of probabilities, i.e., the upper bound of the integral.</p>
</td></tr>
<tr><td><code id="mvtNormQuasiMonteCarlo_+3A_cov">cov</code></td>
<td>
<p>Covariance matrix of the multivariate normal distribution. Must be positive semi-definite.
WARNING: for performance in high-dimensions, no check is performed on the matrix. It is the user responsibility to ensure
that the matrix is positive semi-definite.</p>
</td></tr>
<tr><td><code id="mvtNormQuasiMonteCarlo_+3A_genvec">genVec</code></td>
<td>
<p>Generating vector for the quasi-Monte Carlo procedure. Can be computed using <code>genVecQMC</code>.</p>
</td></tr>
<tr><td><code id="mvtNormQuasiMonteCarlo_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to Cpp routine.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses a quasi-Monte Carlo procedure based on randomly shifted
lattice rules to estimate the distribution function a multivariate normal distribution
as described in Genz and Bretz (2009) on page 50.
</p>


<h3>Value</h3>

<p>A named vector with components estimate <code>estimate</code> of the distribution function 
along <code>error</code>, 3 times the empirical Monte Carlo standard error over the <code>nrep</code> replications.
</p>


<h3>References</h3>

<p>Genz, A. and Bretz, F. (2009). Computations of Multivariate Normal and t Probabilities, volume 105. Springer: Dordrecht.
</p>
<p>Genz, A. (2013). QSILATMVNV <a href="http://www.math.wsu.edu/faculty/genz/software/software.html">http://www.math.wsu.edu/faculty/genz/software/software.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Define locations
loc &lt;- expand.grid(1:4, 1:4)
ref &lt;- sample.int(16, 1)

#Compute variogram matrix
variogramMatrix &lt;- ((sqrt((outer(loc[,1],loc[,1],"-"))^2 +
(outer(loc[,2],loc[,2],"-"))^2)) / 2)^(1.5)

#Define an upper boud
upperBound &lt;- variogramMatrix[-ref,ref]

#Compute covariance matrix
cov &lt;-  (variogramMatrix[-ref,ref]%*%t(matrix(1, (nrow(loc) - 1), 1)) +
t(variogramMatrix[ref,-ref]%*%t(matrix(1, (nrow(loc) - 1), 1))) -
variogramMatrix[-ref,-ref])

#Compute generating vector
p &lt;- 499
latticeRule &lt;- genVecQMC(p, (nrow(loc) - 1))

#Estimate the multivariate distribution function
mvtNormQuasiMonteCarlo(latticeRule$primeP, upperBound, cov, latticeRule$genVec)
</code></pre>

<hr>
<h2 id='mvTProbQuasiMonteCarlo'>Multivariate t distribution function</h2><span id='topic+mvTProbQuasiMonteCarlo'></span>

<h3>Description</h3>

<p>Estimate the multivariate t distribution function with quasi-Monte Carlo method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvTProbQuasiMonteCarlo(p, upperBound, cov, nu, genVec, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvTProbQuasiMonteCarlo_+3A_p">p</code></td>
<td>
<p>Number of samples used for quasi-Monte Carlo estimation. Must be a prime number.</p>
</td></tr>
<tr><td><code id="mvTProbQuasiMonteCarlo_+3A_upperbound">upperBound</code></td>
<td>
<p>Vector of probabilities, i.e., the upper bound of the integral.</p>
</td></tr>
<tr><td><code id="mvTProbQuasiMonteCarlo_+3A_cov">cov</code></td>
<td>
<p>Covariance matrix of the multivariate normal distribution. Must be positive semi-definite.
WARNING: for performance in high-dimensions, no check is done to ensure positive-definiteness of the covariance matrix. It is the user responsibility to ensure that this property is verified.</p>
</td></tr>
<tr><td><code id="mvTProbQuasiMonteCarlo_+3A_nu">nu</code></td>
<td>
<p>Degrees of freedom of the t distribution.</p>
</td></tr>
<tr><td><code id="mvTProbQuasiMonteCarlo_+3A_genvec">genVec</code></td>
<td>
<p>Generating vector for the quasi-Monte Carlo procedure. Can be computed using <code>genVecQMC</code>.</p>
</td></tr>
<tr><td><code id="mvTProbQuasiMonteCarlo_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to Cpp routine.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses a quasi-Monte Carlo procedure based on randomly shifted
lattice rules to estimate the distribution function a multivariate normal distribution
as described in Genz and Bretz (2009) on page 50.
</p>
<p>For compatibility reasons, the function handles the univariate case, which is passed on to <code>pt</code>.
</p>


<h3>Value</h3>

<p>A named vector with components estimate <code>estimate</code> of the distribution function 
along <code>error</code>, 3 times the empirical Monte Carlo standard error over the <code>nrep</code> replications.
</p>


<h3>Author(s)</h3>

<p>Raphael de Fondeville
</p>


<h3>References</h3>

<p>Genz, A. and Bretz, F. (2009). Computations of Multivariate Normal and t Probabilities, volume 105. Springer: Dordrecht.
</p>
<p>Genz, A. (2013). QSILATMVTV <a href="http://www.math.wsu.edu/faculty/genz/software/software.html">http://www.math.wsu.edu/faculty/genz/software/software.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Define locations
loc &lt;- expand.grid(1:4, 1:4)
ref &lt;- sample.int(16, 1)

#Define degrees of freedom
nu &lt;- 3

#Compute variogram matrix
variogramMatrix &lt;- ((sqrt((outer(loc[,1],loc[,1],"-"))^2 +
(outer(loc[,2],loc[,2],"-"))^2)) / 2)^(1.5)

#Define an upper bound
upperBound &lt;- variogramMatrix[-ref,ref]

#Compute covariance matrix
cov &lt;-  (variogramMatrix[-ref,ref]%*%t(matrix(1, (nrow(loc) - 1), 1)) +
t(variogramMatrix[ref,-ref]%*%t(matrix(1, (nrow(loc) - 1), 1))) -
variogramMatrix[-ref,-ref])

#Compute generating vector
p &lt;- 499
latticeRule &lt;- genVecQMC(p, (nrow(loc) - 1))

#Estimate the multivariate distribution function
mvTProbQuasiMonteCarlo(latticeRule$primeP, upperBound, cov, nu, latticeRule$genVec)
</code></pre>

<hr>
<h2 id='rExtremalStudentParetoProcess'>Simulation of extremal Student generalized Pareto vectors</h2><span id='topic+rExtremalStudentParetoProcess'></span>

<h3>Description</h3>

<p>Simulation of Pareto processes associated to the max functional. The algorithm is described in section 4 of Thibaud and Opitz (2015). 
The Cholesky decomposition of the matrix <code>Sigma</code>
leads to samples on the unit sphere with respect to the Mahalanobis distance. 
An accept-reject algorithm is then used to simulate 
samples from the Pareto process. If <code>normalize = TRUE</code>, 
the vector is scaled by the exponent measure <code class="reqn">\kappa</code> so that the maximum of the sample is greater than <code class="reqn">\kappa</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rExtremalStudentParetoProcess(
  n,
  Sigma,
  nu,
  normalize = FALSE,
  matchol = NULL,
  trunc = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rExtremalStudentParetoProcess_+3A_n">n</code></td>
<td>
<p>sample size</p>
</td></tr>
<tr><td><code id="rExtremalStudentParetoProcess_+3A_sigma">Sigma</code></td>
<td>
<p>a <code>d</code> by <code>d</code> correlation matrix</p>
</td></tr>
<tr><td><code id="rExtremalStudentParetoProcess_+3A_nu">nu</code></td>
<td>
<p>degrees of freedom parameter</p>
</td></tr>
<tr><td><code id="rExtremalStudentParetoProcess_+3A_normalize">normalize</code></td>
<td>
<p>logical; should unit Pareto samples above <code class="reqn">\kappa</code> be returned?</p>
</td></tr>
<tr><td><code id="rExtremalStudentParetoProcess_+3A_matchol">matchol</code></td>
<td>
<p>Cholesky matrix <code class="reqn">\mathbf{A}</code> such that <code class="reqn">\mathbf{A}\mathbf{A}^\top = \boldsymbol{\Sigma}</code>. Corresponds to <code>t(chol(Sigma))</code>. Default to <code>NULL</code>, in which case the Cholesky root is computed within the function.</p>
</td></tr>
<tr><td><code id="rExtremalStudentParetoProcess_+3A_trunc">trunc</code></td>
<td>
<p>logical; should negative components be truncated at zero? Default to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code>n</code> by <code>d</code> matrix of samples, with <code>attributes</code> <code>"accept.rate"</code> indicating 
the fraction of samples accepted.
</p>


<h3>Note</h3>

<p>If <code class="reqn">\nu&gt;2</code>, an accept-reject algorithm using simulations from the angular measure on the
<code class="reqn">l_1</code> is at least twice as efficient. The relative efficiency of the latter is much larger for larger <code class="reqn">\nu</code>. 
This algorithm should therefore not be used in high dimensions as its acceptance rate
is several orders of magnitude smaller than that implemented in <a href="mev.html#topic+rparp">rparp</a>.
</p>


<h3>Author(s)</h3>

<p>Emeric Thibaud, Leo Belzile
</p>


<h3>References</h3>

<p>Thibaud, E. and T. Opitz (2015). Efficient inference and simulation for elliptical Pareto processes. Biometrika, 102(4), 855-870.
</p>


<h3>See Also</h3>

<p><a href="mev.html#topic+rparp">rparp</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>loc &lt;- expand.grid(1:4, 1:4)
Sigma &lt;- exp(-as.matrix(dist(loc))^1.5)
rExtremalStudentParetoProcess(100, Sigma, nu = 2)
</code></pre>

<hr>
<h2 id='scoreEstimation'>Gradient score function for the Brown&ndash;Resnick model.</h2><span id='topic+scoreEstimation'></span>

<h3>Description</h3>

<p>Compute the peaks-over-threshold gradient score function for the Brown&ndash;Resnick model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scoreEstimation(
  obs,
  loc,
  vario,
  weightFun = NULL,
  dWeightFun = NULL,
  nCores = 1L,
  cl = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scoreEstimation_+3A_obs">obs</code></td>
<td>
<p>List of vectors exceeding an R-threshold, see de Fondeville and Davison (2018) for more details.</p>
</td></tr>
<tr><td><code id="scoreEstimation_+3A_loc">loc</code></td>
<td>
<p>Matrix of coordinates as given by <code>expand.grid()</code>.</p>
</td></tr>
<tr><td><code id="scoreEstimation_+3A_vario">vario</code></td>
<td>
<p>Semi-variogram function taking a vector of coordinates as input.</p>
</td></tr>
<tr><td><code id="scoreEstimation_+3A_weightfun">weightFun</code></td>
<td>
<p>Function of weights.</p>
</td></tr>
<tr><td><code id="scoreEstimation_+3A_dweightfun">dWeightFun</code></td>
<td>
<p>Partial derivative function of <code>weightFun</code>.</p>
</td></tr>
<tr><td><code id="scoreEstimation_+3A_ncores">nCores</code></td>
<td>
<p>Number of cores used for the computation</p>
</td></tr>
<tr><td><code id="scoreEstimation_+3A_cl">cl</code></td>
<td>
<p>Cluster instance as created by <code>makeCluster</code> of the <code>parallel</code> package.</p>
</td></tr>
<tr><td><code id="scoreEstimation_+3A_...">...</code></td>
<td>
<p>Parameters for <code>weightFun</code> and <code>dWeightFun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the gradient score based on the representation developed by Wadsworth et al. (2014).
Margins must have been standardized. The weighting function must be differentiable and verify some properties
for consistency, see de Fondeville and Davison (2018) for more details.
</p>


<h3>Value</h3>

<p>Evaluation of the gradient score function for the set of observations <code>obs</code> and semi-variogram <code>vario</code>.
</p>


<h3>Author(s)</h3>

<p>Raphael de Fondeville
</p>


<h3>References</h3>

<p>de Fondeville, R. and Davison A. (2018). High-dimensional peaks-over-threshold inference. Biometrika, 105(3), 575-592.
</p>
<p>Wadsworth, J. L. and J. A. Tawn (2014). Efficient inference for spatial extreme value
processes associated to log-Gaussian random functions. Biometrika, 101(1), 1-15.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Define variogram function
vario &lt;- function(h){
   1 / 2 * norm(h,type = "2")^1.5
}

#Define locations
loc &lt;- expand.grid(1:4, 1:4)

#Simulate data
obs &lt;- simulPareto(1000, loc, vario)

#Evaluate risk functional
sums &lt;- sapply(obs, sum)

#Define weighting function
weightFun &lt;- function(x, u){
 x * (1 - exp(-(sum(x / u) - 1)))
}

#Define partial derivative of weighting function
dWeightFun &lt;- function(x, u){
(1 - exp(-(sum(x / u) - 1))) + (x / u) * exp( - (sum(x / u) - 1))
}

#Select exceedances
threshold &lt;- quantile(sums, 0.9)
exceedances &lt;- obs[sums &gt; threshold]

#Evaluate gradient score function
scoreEstimation(exceedances, loc, vario, weightFun = weightFun, dWeightFun, u = threshold)
</code></pre>

<hr>
<h2 id='simulBrownResnick'>Simulation of Brown&ndash;Resnick random vectors</h2><span id='topic+simulBrownResnick'></span>

<h3>Description</h3>

<p><code>simulBrownResnick</code> provides <code>n</code> replicates of a Brown&ndash;Resnick max-stable process with semi-variogram <code>vario</code>
at locations <code>loc</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulBrownResnick(n, loc, vario, nCores = 1, cl = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulBrownResnick_+3A_n">n</code></td>
<td>
<p>Number of replicates desired.</p>
</td></tr>
<tr><td><code id="simulBrownResnick_+3A_loc">loc</code></td>
<td>
<p>Matrix of coordinates as given by <code>expand.grid()</code>.</p>
</td></tr>
<tr><td><code id="simulBrownResnick_+3A_vario">vario</code></td>
<td>
<p>Semi-variogram function.</p>
</td></tr>
<tr><td><code id="simulBrownResnick_+3A_ncores">nCores</code></td>
<td>
<p>Number of cores needed for the computation</p>
</td></tr>
<tr><td><code id="simulBrownResnick_+3A_cl">cl</code></td>
<td>
<p>Cluster instance as created by <code>makeCluster</code> of the <code>parallel</code> package. Make sure
the random number generator has been properly initialized with
<code>clusterSetRNGStream()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm used here is based on the spectral representation of the Brown&ndash;Resnick
model as described in Dombry et al. (2015). It provides <code>n</code> exact simulations
on the unit Frechet scale and requires, in average, for each max-stable vector, the simulation of d Pareto processes,
where d is the number of locations.
</p>


<h3>Value</h3>

<p>List of <code>n</code> random vectors drawn from a max-stable Brown&ndash;Resnick process
with semi-variogram <code>vario</code> at location <code>loc</code>.
</p>


<h3>References</h3>

<p>Dombry, C., Engelke, S. and M. Oesting. Exact simulation of max-stable processes. Biometrika, 103(2), 303-317.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Define semi-variogram function
vario &lt;- function(h){
   1 / 2 * norm(h,type = "2")^1.5
}

#Define locations
loc &lt;- expand.grid(1:4, 1:4)

#Simulate data
obs &lt;- simulBrownResnick(10, loc, vario)
</code></pre>

<hr>
<h2 id='simulPareto'>Simulate Pareto random vectors</h2><span id='topic+simulPareto'></span>

<h3>Description</h3>

<p><code>simulPareto</code> provides <code>n</code> replicates of the multivariate Pareto distribution
associated to log-Gaussian random function with semi-variogram <code>vario</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulPareto(n, loc, vario, nCores = 1, cl = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulPareto_+3A_n">n</code></td>
<td>
<p>Number of replicates desired.</p>
</td></tr>
<tr><td><code id="simulPareto_+3A_loc">loc</code></td>
<td>
<p>Matrix of coordinates as given by <code>expand.grid()</code>.</p>
</td></tr>
<tr><td><code id="simulPareto_+3A_vario">vario</code></td>
<td>
<p>Semi-variogram function.</p>
</td></tr>
<tr><td><code id="simulPareto_+3A_ncores">nCores</code></td>
<td>
<p>Number of cores used for the computation</p>
</td></tr>
<tr><td><code id="simulPareto_+3A_cl">cl</code></td>
<td>
<p>Cluster instance as created by <code>makeCluster</code> of the <code>parallel</code> package. Make sure
the random number generator has been properly initialized with <code>clusterSetRNGStream()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm used here is based on the spectral representation of the Brown&ndash;Resnick
model as described in Dombry et al. (2015). It provides <code>n</code> replicates conditioned
that <code>mean(x) &gt; 1</code> on the unit Frechet scale.
</p>


<h3>Value</h3>

<p>List of <code>n</code> random vectors drawn from a multivariate Pareto distribution with semi-variogram <code>vario</code>.
</p>


<h3>References</h3>

<p>Dombry, C., Engelke, S. and M. Oesting. Exact simulation of max-stable processes. Biometrika, 103(2), 303-317.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Define variogram function
vario &lt;- function(h){
   1 / 2 * norm(h,type = "2")^1.5
}

#Define locations
loc &lt;- expand.grid(1:4, 1:4)

#Simulate data
obs &lt;- simulPareto(100, loc, vario)
</code></pre>

<hr>
<h2 id='spectralLikelihood'>Spectral log-likelihood function</h2><span id='topic+spectralLikelihood'></span>

<h3>Description</h3>

<p>Compute the negative spectral log-likelihood function for Brown&ndash;Resnick model with peaks-over-threshold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spectralLikelihood(obs, loc, vario, nCores = 1L, cl = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spectralLikelihood_+3A_obs">obs</code></td>
<td>
<p>List of observations vectors for which <code>sum(x)</code> exceeds a high threshold.</p>
</td></tr>
<tr><td><code id="spectralLikelihood_+3A_loc">loc</code></td>
<td>
<p>Matrix of coordinates as given by <code>expand.grid()</code>.</p>
</td></tr>
<tr><td><code id="spectralLikelihood_+3A_vario">vario</code></td>
<td>
<p>Semi-variogram function taking a vector of coordinates as input.</p>
</td></tr>
<tr><td><code id="spectralLikelihood_+3A_ncores">nCores</code></td>
<td>
<p>Number of cores used for the computation</p>
</td></tr>
<tr><td><code id="spectralLikelihood_+3A_cl">cl</code></td>
<td>
<p>Cluster instance as created by <code>makeCluster</code> of the <code>parallel</code> package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function compute the negative log-likelihood function based on the spectral representation developed
by Engelke et al. (2015). This simplified expression is obtained by conditioning on the event
'<code>sum(x)</code> exceeds a high threshold <code>u &gt; 1</code>'. Margins must have been standardized.
</p>


<h3>Value</h3>

<p>Negative spectral log-likelihood function evaluated at the set of observations <code>obs</code> with semi-variogram <code>vario</code>.
</p>


<h3>References</h3>

<p>Engelke, S. et al. (2015). Estimation of Huesler-Reiss distributions and Brown-Resnick processes. Journal of the Royal Statistical Society: Series B, 77(1), 239-265
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Define semi-variogram function
vario &lt;- function(h){
   1 / 2 * norm(h,type = "2")^1.5
}

#Define locations
loc &lt;- expand.grid(1:4, 1:4)

#Simulate data
obs &lt;- simulPareto(1000, loc, vario)

#Evaluate risk functional
sums &lt;- sapply(obs, sum)

#Select exceedances
exceedances &lt;- obs[sums &gt; quantile(sums, 0.9)]

#Evaluate the spectral function
spectralLikelihood(exceedances, loc, vario)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
