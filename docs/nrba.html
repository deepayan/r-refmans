<!DOCTYPE html><html><head><title>Help for package nrba</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nrba}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#nrba-package'><p>nrba: Methods for Conducting Nonresponse Bias Analysis (NRBA)</p></a></li>
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#assess_range_of_bias'><p>Assess the range of possible bias based on specified assumptions</p>
about how nonrespondents differ from respondents</a></li>
<li><a href='#calculate_response_rates'><p>Calculate Response Rates</p></a></li>
<li><a href='#chisq_test_ind_response'><p>Test the independence of survey response and auxiliary variables</p></a></li>
<li><a href='#chisq_test_vs_external_estimate'><p>Test of differences in survey percentages relative to external estimates</p></a></li>
<li><a href='#get_cumulative_estimates'><p>Calculate cumulative estimates of a mean/proportion</p></a></li>
<li><a href='#get_variance_method'><p>Summarize the variance estimation method for the survey design</p></a></li>
<li><a href='#involvement_survey_pop'><p>Parent involvement survey: population data</p></a></li>
<li><a href='#involvement_survey_srs'><p>Parent involvement survey: simple random sample</p></a></li>
<li><a href='#involvement_survey_str2s'><p>Parent involvement survey: stratified, two-stage sample</p></a></li>
<li><a href='#predict_outcome_via_glm'><p>Fit a regression model to predict survey outcomes</p></a></li>
<li><a href='#predict_response_status_via_glm'><p>Fit a logistic regression model to predict response to the survey.</p></a></li>
<li><a href='#rake_to_benchmarks'><p>Re-weight data to match population benchmarks, using raking or post-stratification</p></a></li>
<li><a href='#stepwise_model_selection'><p>Select and fit a model using stepwise regression</p></a></li>
<li><a href='#t_test_by_response_status'><p>t-test of differences in means/percentages between responding sample and full sample, or between responding sample and eligible sample</p></a></li>
<li><a href='#t_test_of_weight_adjustment'><p>t-test of differences in estimated means/percentages from two different sets of replicate weights.</p></a></li>
<li><a href='#t_test_overlap'><p>Test for differences in means/percentages between two potentially overlapping groups</p></a></li>
<li><a href='#t_test_vs_external_estimate'><p>t-test of differences in means/percentages relative to external estimates</p></a></li>
<li><a href='#wt_class_adjust'><p>Adjust weights in a replicate design for nonresponse or unknown eligibility status, using weighting classes</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Methods for Conducting Nonresponse Bias Analysis (NRBA)</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Facilitates nonresponse bias analysis (NRBA) 
    for survey data.  Such data may arise from a complex
    sampling design with features such as stratification, clustering, or
    unequal probabilities of selection. Multiple types of analyses may be
    conducted: comparisons of response rates across subgroups; comparisons
    of estimates before and after weighting adjustments; comparisons of
    sample-based estimates to external population totals; tests of
    systematic differences in covariate means between respondents
    and full samples; tests of independence between response status
    and covariates; and modeling of outcomes and response status as a 
    function of covariates. Extensive documentation and references are
    provided for each type of analysis. Krenzke, Van de Kerckhove,
    and Mohadjer (2005) <a href="http://www.asasrms.org/Proceedings/y2005/files/JSM2005-000572.pdf">http://www.asasrms.org/Proceedings/y2005/files/JSM2005-000572.pdf</a>
    and Lohr and Riddles (2016) <a href="https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2016002/article/14677-eng.pdf?st=q7PyNsGR">https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2016002/article/14677-eng.pdf?st=q7PyNsGR</a>
    provide an overview of the methods implemented in this package.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>broom, dplyr, magrittr, rlang, srvyr, stats, survey (&ge;
4.1-1), svrep, tidyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, stringr, testthat (&ge; 3.0.0), tibble</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-21 01:54:42 UTC; schneider_b</td>
</tr>
<tr>
<td>Author:</td>
<td>Ben Schneider <a href="https://orcid.org/0000-0002-0406-8470"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Jim Green [aut],
  Shelley Brock [aut] (Author of original SAS macro, WesNRBA),
  Tom Krenzke [aut] (Author of original SAS macro, WesNRBA),
  Michael Jones [aut] (Author of original SAS macro, WesNRBA),
  Wendy Van de Kerckhove [aut] (Author of original SAS macro, WesNRBA),
  David Ferraro [aut] (Author of original SAS macro, WesNRBA),
  Laura Alvarez-Rojas [aut] (Author of original SAS macro, WesNRBA),
  Katie Hubbell [aut] (Author of original SAS macro, WesNRBA),
  Westat [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ben Schneider &lt;BenjaminSchneider@westat.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-21 05:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='nrba-package'>nrba: Methods for Conducting Nonresponse Bias Analysis (NRBA)</h2><span id='topic+nrba'></span><span id='topic+nrba-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Facilitates nonresponse bias analysis (NRBA) for survey data. Such data may arise from a complex sampling design with features such as stratification, clustering, or unequal probabilities of selection. Multiple types of analyses may be conducted: comparisons of response rates across subgroups; comparisons of estimates before and after weighting adjustments; comparisons of sample-based estimates to external population totals; tests of systematic differences in covariate means between respondents and full samples; tests of independence between response status and covariates; and modeling of outcomes and response status as a function of covariates. Extensive documentation and references are provided for each type of analysis. Krenzke, Van de Kerckhove, and Mohadjer (2005) <a href="http://www.asasrms.org/Proceedings/y2005/files/JSM2005-000572.pdf">http://www.asasrms.org/Proceedings/y2005/files/JSM2005-000572.pdf</a> and Lohr and Riddles (2016) <a href="https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2016002/article/14677-eng.pdf?st=q7PyNsGR">https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2016002/article/14677-eng.pdf?st=q7PyNsGR</a> provide an overview of the methods implemented in this package.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Ben Schneider <a href="mailto:BenjaminSchneider@westat.com">BenjaminSchneider@westat.com</a> (<a href="https://orcid.org/0000-0002-0406-8470">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Jim Green <a href="mailto:JimGreen@westat.com">JimGreen@westat.com</a>
</p>
</li>
<li><p> Shelley Brock <a href="mailto:ShelleyBrock@westat.com">ShelleyBrock@westat.com</a> (Author of original SAS macro, WesNRBA)
</p>
</li>
<li><p> Tom Krenzke <a href="mailto:TomKrenzke@westat.com">TomKrenzke@westat.com</a> (Author of original SAS macro, WesNRBA)
</p>
</li>
<li><p> Michael Jones <a href="mailto:MichaelJones@westat.com">MichaelJones@westat.com</a> (Author of original SAS macro, WesNRBA)
</p>
</li>
<li><p> Wendy Van de Kerckhove (Author of original SAS macro, WesNRBA)
</p>
</li>
<li><p> David Ferraro (Author of original SAS macro, WesNRBA)
</p>
</li>
<li><p> Laura Alvarez-Rojas (Author of original SAS macro, WesNRBA)
</p>
</li>
<li><p> Katie Hubbell <a href="mailto:KatieHubbell@westat.com">KatieHubbell@westat.com</a> (Author of original SAS macro, WesNRBA)
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Westat [copyright holder]
</p>
</li></ul>


<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of calling <code>rhs(lhs)</code>.
</p>

<hr>
<h2 id='assess_range_of_bias'>Assess the range of possible bias based on specified assumptions
about how nonrespondents differ from respondents</h2><span id='topic+assess_range_of_bias'></span>

<h3>Description</h3>

<p>This range-of-bias analysis assesses the range of possible nonresponse bias
under varying assumptions about how nonrespondents differ from respondents.
The range of potential bias is calculated for both unadjusted estimates (i.e., from using base weights)
and nonresponse-adjusted estimates (i.e., based on nonresponse-adjusted weights).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>assess_range_of_bias(
  survey_design,
  y_var,
  comparison_cell,
  status,
  status_codes,
  assumed_multiple = c(0.5, 0.75, 0.9, 1.1, 1.25, 1.5),
  assumed_percentile = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="assess_range_of_bias_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the 'survey' package</p>
</td></tr>
<tr><td><code id="assess_range_of_bias_+3A_y_var">y_var</code></td>
<td>
<p>Name of a variable whose mean or proportion is to be estimated</p>
</td></tr>
<tr><td><code id="assess_range_of_bias_+3A_comparison_cell">comparison_cell</code></td>
<td>
<p>(Optional) The name of a variable in the data
dividing the sample into cells. If supplied, then the analysis is based on
assumptions about differences between respondents and nonrespondents
within the same cell. Typically, the variable used is a nonresponse adjustment cell
or post-stratification variable.</p>
</td></tr>
<tr><td><code id="assess_range_of_bias_+3A_status">status</code></td>
<td>
<p>A character string giving the name of the variable representing response/eligibility status.
The status variable should have at most four categories,
representing eligible respondents (ER),
eligible nonrespondents (EN),
known ineligible cases (IE),
and cases whose eligibility is unknown (UE).</p>
</td></tr>
<tr><td><code id="assess_range_of_bias_+3A_status_codes">status_codes</code></td>
<td>
<p>A named vector,
with four entries named 'ER', 'EN', 'IE', and 'UE'.
<code>status_codes</code> indicates how the values of the status variable are to be interpreted.</p>
</td></tr>
<tr><td><code id="assess_range_of_bias_+3A_assumed_multiple">assumed_multiple</code></td>
<td>
<p>One or more numeric values.
Within each nonresponse adjustment cell,
the mean for nonrespondents is assumed to be a specified multiple
of the mean for respondents. If <code>y_var</code> is a categorical variable,
then the assumed nonrespondent mean (i.e., the proportion) in each cell is capped at 1.</p>
</td></tr>
<tr><td><code id="assess_range_of_bias_+3A_assumed_percentile">assumed_percentile</code></td>
<td>
<p>One or more numeric values, ranging from 0 to 1.
Within each nonresponse adjustment cell,
the mean of a continuous variable among nonrespondents is
assumed to equal a specified percentile of the variable among respondents.
The <code>assumed_percentile</code> parameter should be used only when the <code>y_var</code>
variable is numeric. Quantiles are estimated with weights,
using the function
<code><a href="survey.html#topic+svyquantile">svyquantile</a>(..., qrule = "hf2")</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame summarizing the range of bias under each assumption.
For a numeric outcome variable, there is one row per value of
<code>assumed_multiple</code> or <code>assumed_percentile</code>. For a categorical
outcome variable, there is one row per combination of category
and <code>assumed_multiple</code> or <code>assumed_percentile</code>.
</p>
<p>The column <code>bias_of_unadj_estimate</code> is the nonresponse bias
of the estimate from respondents produced using the unadjusted weights.
The column <code>bias_of_adj_estimate</code> is the nonresponse bias
of the estimate from respondents produced
using nonresponse-adjusted weights, based on a weighting-class
adjustment with <code>comparison_cell</code> as the weighting class variable.
If no <code>comparison_cell</code> is specified, the two bias estimates
will be the same.
</p>


<h3>References</h3>

<p>See Petraglia et al. (2016) for an example of a range-of-bias analysis
using these methods.
</p>

<ul>
<li><p> Petraglia, E., Van de Kerckhove, W., and Krenzke, T. (2016).
<em>Review of the Potential for Nonresponse Bias in FoodAPS 2012</em>.
Prepared for the Economic Research Service,
U.S. Department of Agriculture. Washington, D.C.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load example data

suppressPackageStartupMessages(library(survey))
data(api)

base_weights_design &lt;- svydesign(
  data    = apiclus1,
  id      = ~dnum,
  weights = ~pw,
  fpc     = ~fpc
) |&gt; as.svrepdesign(type = "JK1")

base_weights_design$variables$response_status &lt;- sample(
  x = c("Respondent", "Nonrespondent"),
  prob = c(0.75, 0.25),
  size = nrow(base_weights_design),
  replace = TRUE
)

# Assess range of bias for mean of `api00`
# based on assuming nonrespondent means
# are equal to the 25th percentile or 75th percentile
# among respondents, within nonresponse adjustment cells

  assess_range_of_bias(
    survey_design = base_weights_design,
    y_var = "api00",
    comparison_cell = "stype",
    status = "response_status",
    status_codes = c("ER" = "Respondent",
                     "EN" = "Nonrespondent",
                     "IE" = "Ineligible",
                     "UE" = "Unknown"),
    assumed_percentile = c(0.25, 0.75)
  )

# Assess range of bias for proportions of `sch.wide`
# based on assuming nonrespondent proportions
# are equal to some multiple of respondent proportions,
# within nonresponse adjustment cells

  assess_range_of_bias(
    survey_design = base_weights_design,
    y_var = "sch.wide",
    comparison_cell = "stype",
    status = "response_status",
    status_codes = c("ER" = "Respondent",
                     "EN" = "Nonrespondent",
                     "IE" = "Ineligible",
                     "UE" = "Unknown"),
    assumed_multiple = c(0.25, 0.75)
  )
</code></pre>

<hr>
<h2 id='calculate_response_rates'>Calculate Response Rates</h2><span id='topic+calculate_response_rates'></span>

<h3>Description</h3>

<p>Calculates response rates using one of the response rate formulas
defined by AAPOR (American Association of Public Opinion Research).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_response_rates(
  data,
  status,
  status_codes = c("ER", "EN", "IE", "UE"),
  weights,
  rr_formula = "RR3",
  elig_method = "CASRO-subgroup",
  e = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calculate_response_rates_+3A_data">data</code></td>
<td>
<p>A data frame containing the selected sample, one row per case.</p>
</td></tr>
<tr><td><code id="calculate_response_rates_+3A_status">status</code></td>
<td>
<p>A character string giving the name of the variable representing response/eligibility status.
The <code>status</code> variable should have at most four categories,
representing eligible respondents (ER), eligible nonrespondents (EN),
known ineligible cases (IE), and cases whose eligibility is unknown (UE).</p>
</td></tr>
<tr><td><code id="calculate_response_rates_+3A_status_codes">status_codes</code></td>
<td>
<p>A named vector, with four entries named 'ER', 'EN', 'IE', and 'UE'.
<code>status_codes</code> indicates how the values of the <code>status</code> variable are to be interpreted.</p>
</td></tr>
<tr><td><code id="calculate_response_rates_+3A_weights">weights</code></td>
<td>
<p>(Optional) A character string giving the name of a variable representing weights in the data
to use for calculating weighted response rates</p>
</td></tr>
<tr><td><code id="calculate_response_rates_+3A_rr_formula">rr_formula</code></td>
<td>
<p>A character vector including any of the following: 'RR1', 'RR3', and 'RR5'. <br />
These are the names of formulas defined by AAPOR. See the <em>Formulas</em> section below for formulas.</p>
</td></tr>
<tr><td><code id="calculate_response_rates_+3A_elig_method">elig_method</code></td>
<td>
<p>If <code>rr_formula='RR3'</code>, this specifies how to estimate
an eligibility rate for cases with unknown eligibility. Must be one of the following: <br />
<br />
<code>'CASRO-overall'</code> <br />
Estimates an eligibility rate using the overall sample.
If response rates are calculated for subgroups, the single overall sample estimate
will be used as the estimated eligibility rate for subgroups as well. <br />
<br />
<code>'CASRO-subgroup'</code> <br />
Estimates eligibility rates separately for each subgroup. <br />
<br />
<code>'specified'</code> <br />
With this option, a numeric value is supplied by the user to the parameter <code>e</code>. <br />
<br />
For <code>elig_method='CASRO-overall'</code> or <code>elig_method='CASRO-subgroup'</code>,
the eligibility rate is estimated as <code class="reqn">(ER)/(ER + NR + IE)</code>.</p>
</td></tr>
<tr><td><code id="calculate_response_rates_+3A_e">e</code></td>
<td>
<p>(Required if <code>elig_method='specified'</code>). A numeric value between 0 and 1 specifying the estimated eligibility rate for cases with unknown eligibility.
A character string giving the name of a numeric variable may also be supplied; in that case, the eligibility rate must be constant for all cases in a subgroup.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Output consists of a data frame giving weighted and unweighted response rates. The following columns may be included, depending on the arguments supplied:
</p>

<ul>
<li><p><code>RR1_Unweighted</code>
</p>
</li>
<li><p><code>RR1_Weighted</code>
</p>
</li>
<li><p><code>RR3_Unweighted</code>
</p>
</li>
<li><p><code>RR3_Weighted</code>
</p>
</li>
<li><p><code>RR5_Unweighted</code>
</p>
</li>
<li><p><code>RR5_Weighted</code>
</p>
</li>
<li><p><code>n</code>: Total sample size
</p>
</li>
<li><p><code>Nhat</code>: Sum of weights for the total sample
</p>
</li>
<li><p><code>n_ER</code>: Number of eligible respondents
</p>
</li>
<li><p><code>Nhat_ER</code>: Sum of weights for eligible respondents
</p>
</li>
<li><p><code>n_EN</code>: Number of eligible nonrespondents
</p>
</li>
<li><p><code>Nhat_EN</code>: Sum of weights for eligible nonrespondents
</p>
</li>
<li><p><code>n_IE</code>: Number of ineligible cases
</p>
</li>
<li><p><code>Nhat_IE</code>: Sum of weights for ineligible cases
</p>
</li>
<li><p><code>n_UE</code>: Number of cases whose eligibility is unknown
</p>
</li>
<li><p><code>Nhat_UE</code>: Sum of weights for cases whose eligibility is unknown
</p>
</li>
<li><p><code>e_unwtd</code>: If <em>RR3</em> is calculated, the eligibility rate estimate <em>e</em> used for the unweighted response rate.
</p>
</li>
<li><p><code>e_wtd</code>: If <em>RR3</em> is calculated, the eligibility rate estimate <em>e</em> used for the weighted response rate.
</p>
</li></ul>

<p>If the data frame is grouped (i.e. by using <code>df %&gt;% group_by(Region)</code>),
then the output contains one row per subgroup.
</p>


<h3>Formulas</h3>

<p>Denote the sample totals as follows:
</p>

<ul>
<li><p><strong>ER</strong>: Total number of eligible respondents
</p>
</li>
<li><p><strong>EN</strong>: Total number of eligible non-respondents
</p>
</li>
<li><p><strong>IE</strong>: Total number of ineligible cases
</p>
</li>
<li><p><strong>UE</strong>: Total number of cases whose eligibility is unknown
</p>
</li></ul>

<p>For weighted response rates, these totals are calculated using weights.
</p>
<p>The response rate formulas are then as follows:
</p>
<p style="text-align: center;"><code class="reqn">RR1 = ER / ( ER + EN + UE )</code>
</p>

<p>RR1 essentially assumes that all cases with unknown eligibility are in fact eligible.
</p>
<p style="text-align: center;"><code class="reqn">RR3 = ER / ( ER + EN + (e * UE) )</code>
</p>

<p>RR3 uses an estimate, <em>e</em>, of the eligibility rate among cases with unknown eligibility.
</p>
<p style="text-align: center;"><code class="reqn">RR5 = ER / ( ER + EN )</code>
</p>

<p>RR5 essentially assumes that all cases with unknown eligibility are in fact ineligible. <br />
</p>
<p>For <em>RR3</em>, an estimate, <code>e</code>, of the eligibility rate among cases with unknown eligibility must be used.
AAPOR strongly recommends that the basis for the estimate should be explicitly stated and detailed.
</p>
<p>The CASRO methods, which might be appropriate for the design, use the formula <code class="reqn">e = 1 - ( IE / (ER + EN + IE) )</code>.
</p>

<ul>
<li><p> For <code>elig_method='CASRO-overall'</code>, an estimate is calculated for the overall sample
and this single estimate is used when calculating response rates for subgroups.
</p>
</li>
<li><p> For <code>elig_method='CASRO-subgroup'</code>, estimates are calculated separately for each subgroup.
</p>
</li></ul>

<p>Please consult AAPOR's current <em>Standard Definitions</em> for in-depth explanations.
</p>


<h3>References</h3>

<p>The American Association for Public Opinion Research. 2016. <em>Standard Definitions:
Final Dispositions of Case Codes and Outcome Rates for Surveys. 9th edition.</em> AAPOR.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load example data
data(involvement_survey_srs, package = "nrba")

involvement_survey_srs[["RESPONSE_STATUS"]] &lt;- sample(1:4, size = 5000, replace = TRUE)

# Calculate overall response rates

involvement_survey_srs %&gt;%
  calculate_response_rates(
    status = "RESPONSE_STATUS",
    status_codes = c("ER" = 1, "EN" = 2, "IE" = 3, "UE" = 4),
    weights = "BASE_WEIGHT",
    rr_formula = "RR3",
    elig_method = "CASRO-overall"
  )

# Calculate response rates by subgroup

library(dplyr)

involvement_survey_srs %&gt;%
  group_by(STUDENT_RACE, STUDENT_SEX) %&gt;%
  calculate_response_rates(
    status = "RESPONSE_STATUS",
    status_codes = c("ER" = 1, "EN" = 2, "IE" = 3, "UE" = 4),
    weights = "BASE_WEIGHT",
    rr_formula = "RR3",
    elig_method = "CASRO-overall"
  )

# Compare alternative approaches for handling of cases with unknown eligiblity

involvement_survey_srs %&gt;%
  group_by(STUDENT_RACE) %&gt;%
  calculate_response_rates(
    status = "RESPONSE_STATUS",
    status_codes = c("ER" = 1, "EN" = 2, "IE" = 3, "UE" = 4),
    rr_formula = "RR3",
    elig_method = "CASRO-overall"
  )

involvement_survey_srs %&gt;%
  group_by(STUDENT_RACE) %&gt;%
  calculate_response_rates(
    status = "RESPONSE_STATUS",
    status_codes = c("ER" = 1, "EN" = 2, "IE" = 3, "UE" = 4),
    rr_formula = "RR3",
    elig_method = "CASRO-subgroup"
  )

involvement_survey_srs %&gt;%
  group_by(STUDENT_RACE) %&gt;%
  calculate_response_rates(
    status = "RESPONSE_STATUS",
    status_codes = c("ER" = 1, "EN" = 2, "IE" = 3, "UE" = 4),
    rr_formula = "RR3",
    elig_method = "specified",
    e = 0.5
  )

involvement_survey_srs %&gt;%
  transform(e_by_email = ifelse(PARENT_HAS_EMAIL == "Has Email", 0.75, 0.25)) %&gt;%
  group_by(PARENT_HAS_EMAIL) %&gt;%
  calculate_response_rates(
    status = "RESPONSE_STATUS",
    status_codes = c("ER" = 1, "EN" = 2, "IE" = 3, "UE" = 4),
    rr_formula = "RR3",
    elig_method = "specified",
    e = "e_by_email"
  )

</code></pre>

<hr>
<h2 id='chisq_test_ind_response'>Test the independence of survey response and auxiliary variables</h2><span id='topic+chisq_test_ind_response'></span>

<h3>Description</h3>

<p>Tests whether response status among eligible sample cases is independent of categorical auxiliary variables,
using a Chi-Square test with Rao-Scott's second-order adjustment.
If the data include cases known to be ineligible or who have unknown eligibility status,
the data are subsetted to only include respondents and nonrespondents known to be eligible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chisq_test_ind_response(
  survey_design,
  status,
  status_codes = c("ER", "EN", "UE", "IE"),
  aux_vars
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chisq_test_ind_response_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="chisq_test_ind_response_+3A_status">status</code></td>
<td>
<p>A character string giving the name of the variable representing response/eligibility status.
The <code>status</code> variable should have at most four categories,
representing eligible respondents (ER), eligible nonrespondents (EN),
known ineligible cases (IE), and cases whose eligibility is unknown (UE).</p>
</td></tr>
<tr><td><code id="chisq_test_ind_response_+3A_status_codes">status_codes</code></td>
<td>
<p>A named vector, with four entries named 'ER', 'EN', 'IE', and 'UE'. <br />
<code>status_codes</code> indicates how the values of the <code>status</code> variable are to be interpreted.</p>
</td></tr>
<tr><td><code id="chisq_test_ind_response_+3A_aux_vars">aux_vars</code></td>
<td>
<p>A list of names of auxiliary variables.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Please see <a href="survey.html#topic+svychisq">svychisq</a> for details of how the Rao-Scott second-order adjusted test is conducted.
</p>


<h3>Value</h3>

<p>A data frame containing the results of the Chi-Square test(s) of independence between response status and each auxiliary variable.
If multiple auxiliary variables are specified, the output data contains one row per auxiliary variable.
<br />
<br />
The columns of the output dataset include: <br />
</p>

<ul>
<li> <p><code>auxiliary_variable</code>: The name of the auxiliary variable tested <br />
</p>
</li>
<li> <p><code>statistic</code>: The value of the test statistic <br />
</p>
</li>
<li> <p><code>ndf</code>: Numerator degrees of freedom for the reference distribution <br />
</p>
</li>
<li> <p><code>ddf</code>: Denominator degrees of freedom for the reference distribution <br />
</p>
</li>
<li> <p><code>p_value</code>: The p-value of the test of independence <br />
</p>
</li>
<li> <p><code>test_method</code>: Text giving the name of the statistical test
</p>
</li>
<li> <p><code>variance_method</code>: Text describing the method of variance estimation
</p>
</li></ul>



<h3>References</h3>


<ul>
<li><p> Rao, JNK, Scott, AJ (1984) &quot;On Chi-squared Tests For Multiway Contigency Tables with Proportions Estimated From Survey Data&quot; Annals of Statistics 12:46-60. <br />
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Create a survey design object ----
library(survey)
data(involvement_survey_srs, package = "nrba")

involvement_survey &lt;- svydesign(
  weights = ~BASE_WEIGHT,
  id = ~UNIQUE_ID,
  data = involvement_survey_srs
)


# Test whether response status varies by race or by sex ----

test_results &lt;- chisq_test_ind_response(
  survey_design = involvement_survey,
  status = "RESPONSE_STATUS",
  status_codes = c(
    "ER" = "Respondent",
    "EN" = "Nonrespondent",
    "UE" = "Unknown",
    "IE" = "Ineligible"
  ),
  aux_vars = c("STUDENT_RACE", "STUDENT_SEX")
)

print(test_results)
</code></pre>

<hr>
<h2 id='chisq_test_vs_external_estimate'>Test of differences in survey percentages relative to external estimates</h2><span id='topic+chisq_test_vs_external_estimate'></span>

<h3>Description</h3>

<p>Compare estimated percentages from the present survey to external estimates from a benchmark source.
A Chi-Square test with Rao-Scott's second-order adjustment is used to evaluate whether the survey's estimates differ from the external estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chisq_test_vs_external_estimate(survey_design, y_var, ext_ests, na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chisq_test_vs_external_estimate_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="chisq_test_vs_external_estimate_+3A_y_var">y_var</code></td>
<td>
<p>Name of dependent categorical variable.</p>
</td></tr>
<tr><td><code id="chisq_test_vs_external_estimate_+3A_ext_ests">ext_ests</code></td>
<td>
<p>A numeric vector containing the external estimate of the percentages for each category.
The vector must have names, each name corresponding to a given category.</p>
</td></tr>
<tr><td><code id="chisq_test_vs_external_estimate_+3A_na.rm">na.rm</code></td>
<td>
<p>Whether to drop cases with missing values</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Please see <a href="survey.html#topic+svygofchisq">svygofchisq</a> for details of how the Rao-Scott second-order adjusted test is conducted.
The test statistic, <code>statistic</code> is obtained by calculating the Pearson Chi-squared statistic for the estimated table of population totals. The reference distribution is a Satterthwaite approximation. The p-value is obtained by comparing <code>statistic</code>/<code>scale</code> to a Chi-squared distribution with <code>df</code> degrees of freedom.
</p>


<h3>Value</h3>

<p>A data frame containing the results of the Chi-Square test(s) of whether survey-based estimates systematically differ from external estimates.
<br />
<br />
The columns of the output dataset include: <br />
</p>

<ul>
<li> <p><code>statistic</code>: The value of the test statistic <br />
</p>
</li>
<li> <p><code>df</code>: Degrees of freedom for the reference Chi-Squared distribution <br />
</p>
</li>
<li> <p><code>scale</code>: Estimated scale parameter.
</p>
</li>
<li> <p><code>p_value</code>: The p-value of the test of independence <br />
</p>
</li>
<li> <p><code>test_method</code>: Text giving the name of the statistical test
</p>
</li>
<li> <p><code>variance_method</code>: Text describing the method of variance estimation
</p>
</li></ul>



<h3>References</h3>


<ul>
<li><p> Rao, JNK, Scott, AJ (1984) &quot;On Chi-squared Tests For Multiway Contigency Tables with Proportions Estimated From Survey Data&quot; Annals of Statistics 12:46-60. <br />
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
library(survey)

# Create a survey design ----
data("involvement_survey_pop", package = "nrba")
data("involvement_survey_str2s", package = "nrba")

involvement_survey_sample &lt;- svydesign(
  data = involvement_survey_str2s,
  weights = ~BASE_WEIGHT,
  strata = ~SCHOOL_DISTRICT,
  ids = ~ SCHOOL_ID + UNIQUE_ID,
  fpc = ~ N_SCHOOLS_IN_DISTRICT + N_STUDENTS_IN_SCHOOL
)

# Subset to only include survey respondents ----

involvement_survey_respondents &lt;- subset(
  involvement_survey_sample,
  RESPONSE_STATUS == "Respondent"
)

# Test whether percentages of categorical variable differ from benchmark ----

parent_email_benchmark &lt;- c(
  "Has Email" = 0.85,
  "No Email" = 0.15
)

chisq_test_vs_external_estimate(
  survey_design = involvement_survey_respondents,
  y_var = "PARENT_HAS_EMAIL",
  ext_ests = parent_email_benchmark
)

</code></pre>

<hr>
<h2 id='get_cumulative_estimates'>Calculate cumulative estimates of a mean/proportion</h2><span id='topic+get_cumulative_estimates'></span>

<h3>Description</h3>

<p>Calculates estimates of a mean/proportion which are cumulative with respect
to a predictor variable, such as week of data collection or number of contact attempts.
This can be useful for examining whether estimates are affected by decisions such as
whether to extend the data collection period or make additional contact attempts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_cumulative_estimates(
  survey_design,
  y_var,
  y_var_type = NULL,
  predictor_variable
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_cumulative_estimates_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="get_cumulative_estimates_+3A_y_var">y_var</code></td>
<td>
<p>Name of a variable whose mean or proportion is to be estimated.</p>
</td></tr>
<tr><td><code id="get_cumulative_estimates_+3A_y_var_type">y_var_type</code></td>
<td>
<p>Either <code>NULL</code>, <code>"categorical"</code> or <code>"numeric"</code>.
For <code>"categorical"</code>, proportions are estimated. For <code>"numeric"</code>, means are estimated.
For <code>NULL</code> (the default), then proportions are estimated if <code>y_var</code> is a factor or character variable.
Otherwise, means are estimated. The data will be subset to remove any missing values in this variable.</p>
</td></tr>
<tr><td><code id="get_cumulative_estimates_+3A_predictor_variable">predictor_variable</code></td>
<td>
<p>Name of a variable for which cumulative estimates of <code>y_var</code>
will be calculated. This variable should either be numeric or have categories which when sorted
by their label are arranged in ascending order. The data will be subset to remove
any missing values of the predictor variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe of cumulative estimates. The first column&ndash;whose name matches <code>predictor_variable</code>&ndash;gives
describes the values of <code>predictor_variable</code> for which a given estimate was computed.
The other columns of the result include the following:
</p>
<table>
<tr><td><code>outcome</code></td>
<td>
<p>The name of the variable for which estimates are computed</p>
</td></tr>
<tr><td><code>outcome_category</code></td>
<td>
<p>For a categorical variable, the category of that variable</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>The estimated mean or proportion.</p>
</td></tr>
<tr><td><code>std_error</code></td>
<td>
<p>The estimated standard error</p>
</td></tr>
<tr><td><code>respondent_sample_size</code></td>
<td>
<p>The number of cases used to produce the estimate (excluding missing values)</p>
</td></tr>
</table>


<h3>References</h3>

<p>See Maitland et al. (2017) for an example of a level-of-effort analysis
based on this method.
</p>

<ul>
<li><p> Maitland, A. et al. (2017). <em>A Nonresponse Bias Analysis
of the Health Information National Trends Survey (HINTS)</em>.
Journal of Health Communication 22, 545-553.
doi:10.1080/10810730.2017.1324539
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Create an example survey design
# with a variable representing number of contact attempts
library(survey)
data(involvement_survey_srs, package = "nrba")


survey_design &lt;- svydesign(
  weights = ~BASE_WEIGHT,
  id = ~UNIQUE_ID,
  fpc = ~N_STUDENTS,
  data = involvement_survey_srs
)

# Cumulative estimates from respondents for average student age ----
get_cumulative_estimates(
  survey_design = survey_design |&gt;
    subset(RESPONSE_STATUS == "Respondent"),
  y_var = "STUDENT_AGE",
  y_var_type = "numeric",
  predictor_variable = "CONTACT_ATTEMPTS"
)

# Cumulative estimates from respondents for proportions of categorical variable ----
get_cumulative_estimates(
  survey_design = survey_design |&gt;
    subset(RESPONSE_STATUS == "Respondent"),
  y_var = "WHETHER_PARENT_AGREES",
  y_var_type = "categorical",
  predictor_variable = "CONTACT_ATTEMPTS"
)

</code></pre>

<hr>
<h2 id='get_variance_method'>Summarize the variance estimation method for the survey design</h2><span id='topic+get_variance_method'></span>

<h3>Description</h3>

<p>Summarize the variance estimation method for the survey design
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_variance_method(survey_design)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_variance_method_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For replicate designs, the type of replicates will be determined
based on the <code>'type'</code> element of the survey design object.
If <code>type = 'bootstrap'</code>, this can correspond to any of various
types of bootstrap replication (Canty-Davison bootstrap, Rao-Wu's (n-1) bootstrap, etc.).
</p>
<p>For designs which use linearization-based variance estimation, the summary
only indicates that linearization is used for variance estimation and, if a special
method is used for PPS variance estimation (e.g. Overton's approximation),
that PPS variance estimation method will be described.
</p>


<h3>Value</h3>

<p>A text string describing the method used for variance estimation
</p>

<hr>
<h2 id='involvement_survey_pop'>Parent involvement survey: population data</h2><span id='topic+involvement_survey_pop'></span>

<h3>Description</h3>

<p>An example dataset describing a population of 20,000 students with disabilities
in 20 school districts. This population is the basis for selecting a sample of
students for a parent involvement survey.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>involvement_survey_pop
</code></pre>


<h3>Format</h3>

<p>A data frame with 20,000 rows and 9 variables
</p>


<h3>Fields</h3>


<dl>
<dt>UNIQUE_ID</dt><dd><p>A unique identifier for students</p>
</dd>
<dt>SCHOOL_DISTRICT</dt><dd><p>A unique identifier for school districts</p>
</dd>
<dt>SCHOOL_ID</dt><dd><p>A unique identifier for schools, nested within districts</p>
</dd>
<dt>STUDENT_GRADE</dt><dd><p>Student's grade level: 'PK', 'K', 1-12</p>
</dd>
<dt>STUDENT_AGE</dt><dd><p>Student's age, measured in years</p>
</dd>
<dt>STUDENT_DISABILITY_CODE</dt><dd><p>Code for student's disability category (e.g. 'VI' for 'Visual Impairments')</p>
</dd>
<dt>STUDENT_DISABILITY_CATEGORY</dt><dd><p>Student's disability category (e.g. 'Visual Impairments')</p>
</dd>
<dt>STUDENT_SEX</dt><dd><p>'Female' or 'Male'</p>
</dd>
<dt>STUDENT_RACE</dt><dd><p>Seven-level code with descriptive label (e.g. 'AS7 (Asian)')</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>involvement_survey_pop
</code></pre>

<hr>
<h2 id='involvement_survey_srs'>Parent involvement survey: simple random sample</h2><span id='topic+involvement_survey_srs'></span>

<h3>Description</h3>

<p>An example dataset describing a simple random sample of 5,000 parents
of students with disabilities, from a population of 20,000.
The parent involvement survey measures a single key outcome:
whether &quot;parents perceive that schools facilitate parent involvement
as a means of improving services and results for children with disabilities.&quot; <br /> <br />
The variable <code>BASE_WEIGHT</code> provides the base sampling weight.
The variable <code>N_STUDENTS_IN_SCHOOL</code> can be used to provide a finite population correction
for variance estimation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>involvement_survey_srs
</code></pre>


<h3>Format</h3>

<p>A data frame with 5,000 rows and 17 variables
</p>


<h3>Fields</h3>


<dl>
<dt>UNIQUE_ID</dt><dd><p>A unique identifier for students</p>
</dd>
<dt>RESPONSE_STATUS</dt><dd><p>Survey response/eligibility status: 'Respondent', 'Nonrespondent', 'Ineligble', 'Unknown'</p>
</dd>
<dt>WHETHER_PARENT_AGREES</dt><dd><p>Parent agreement ('AGREE' or 'DISAGREE') for whether they perceive that schools facilitate parent involvement</p>
</dd>
<dt>SCHOOL_DISTRICT</dt><dd><p>A unique identifier for school districts</p>
</dd>
<dt>SCHOOL_ID</dt><dd><p>A unique identifier for schools, nested within districts</p>
</dd>
<dt>STUDENT_GRADE</dt><dd><p>Student's grade level: 'PK', 'K', 1-12</p>
</dd>
<dt>STUDENT_AGE</dt><dd><p>Student's age, measured in years</p>
</dd>
<dt>STUDENT_DISABILITY_CODE</dt><dd><p>Code for student's disability category (e.g. 'VI' for 'Visual Impairments')</p>
</dd>
<dt>STUDENT_DISABILITY_CATEGORY</dt><dd><p>Student's disability category (e.g. 'Visual Impairments')</p>
</dd>
<dt>STUDENT_SEX</dt><dd><p>'Female' or 'Male'</p>
</dd>
<dt>STUDENT_RACE</dt><dd><p>Seven-level code with descriptive label (e.g. 'AS7 (Asian)')</p>
</dd>
<dt>PARENT_HAS_EMAIL</dt><dd><p>Whether parent has an e-mail address ('Has Email' vs 'No Email')</p>
</dd>
<dt>PARENT_HAS_EMAIL_BENCHMARK</dt><dd><p>Population benchmark for category of <code>PARENT_HAS_EMAIL</code></p>
</dd>
<dt>PARENT_HAS_EMAIL_BENCHMARK</dt><dd><p>Population benchmark for category of <code>STUDENT_RACE</code></p>
</dd>
<dt>BASE_WEIGHT</dt><dd><p>Sampling weight to use for weighted estimates</p>
</dd>
<dt>N_STUDENTS</dt><dd><p>Total number of students in the population</p>
</dd>
<dt>CONTACT_ATTEMPTS</dt><dd><p>The number of contact attempts made for each case (ranges between 1 and 6)</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>involvement_survey_srs
</code></pre>

<hr>
<h2 id='involvement_survey_str2s'>Parent involvement survey: stratified, two-stage sample</h2><span id='topic+involvement_survey_str2s'></span>

<h3>Description</h3>

<p>An example dataset describing a stratified, multistage sample of 1,000 parents
of students with disabilities, from a population of 20,000.
The parent involvement survey measures a single key outcome:
whether &quot;parents perceive that schools facilitate parent involvement
as a means of improving services and results for children with disabilities.&quot; <br /> <br />
The sample was selected by sampling 5 schools from each of 20 districts,
and then sampling parents of 10 children in each sampled school.
The variable <code>BASE_WEIGHT</code> provides the base sampling weight.
The variable <code>SCHOOL_DISTRICT</code> was used for stratification,
and the variables <code>SCHOOL_ID</code> and <code>UNIQUE_ID</code> uniquely identify
the first and second stage sampling units (schools and parents).
The variables <code>N_SCHOOLS_IN_DISTRICT</code> and <code>N_STUDENTS_IN_SCHOOL</code>
can be used to provide finite population corrections.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>involvement_survey_str2s
</code></pre>


<h3>Format</h3>

<p>A data frame with 5,000 rows and 18 variables
</p>


<h3>Fields</h3>


<dl>
<dt>UNIQUE_ID</dt><dd><p>A unique identifier for students</p>
</dd>
<dt>RESPONSE_STATUS</dt><dd><p>Survey response/eligibility status: 'Respondent', 'Nonrespondent', 'Ineligble', 'Unknown'</p>
</dd>
<dt>WHETHER_PARENT_AGREES</dt><dd><p>Parent agreement ('AGREE' or 'DISAGREE') for whether they perceive that schools facilitate parent involvement</p>
</dd>
<dt>SCHOOL_DISTRICT</dt><dd><p>A unique identifier for school districts</p>
</dd>
<dt>SCHOOL_ID</dt><dd><p>A unique identifier for schools, nested within districts</p>
</dd>
<dt>STUDENT_GRADE</dt><dd><p>Student's grade level: 'PK', 'K', 1-12</p>
</dd>
<dt>STUDENT_AGE</dt><dd><p>Student's age, measured in years</p>
</dd>
<dt>STUDENT_DISABILITY_CODE</dt><dd><p>Code for student's disability category (e.g. 'VI' for 'Visual Impairments')</p>
</dd>
<dt>STUDENT_DISABILITY_CATEGORY</dt><dd><p>Student's disability category (e.g. 'Visual Impairments')</p>
</dd>
<dt>STUDENT_SEX</dt><dd><p>'Female' or 'Male'</p>
</dd>
<dt>STUDENT_RACE</dt><dd><p>Seven-level code with descriptive label (e.g. 'AS7 (Asian)')</p>
</dd>
<dt>PARENT_HAS_EMAIL</dt><dd><p>Whether parent has an e-mail address ('Has Email' vs 'No Email')</p>
</dd>
<dt>PARENT_HAS_EMAIL_BENCHMARK</dt><dd><p>Population benchmark for category of <code>PARENT_HAS_EMAIL</code></p>
</dd>
<dt>STUDENT_RACE_BENCHMARK</dt><dd><p>Population benchmark for category of <code>STUDENT_RACE</code></p>
</dd>
<dt>N_SCHOOLS_IN_DISTRICT</dt><dd><p>Total number of schools in each district</p>
</dd>
<dt>N_STUDENTS_IN_SCHOOL</dt><dd><p>Total number of students in each school</p>
</dd>
<dt>BASE_WEIGHT</dt><dd><p>Sampling weight to use for weighted estimates</p>
</dd>
<dt>CONTACT_ATTEMPTS</dt><dd><p>The number of contact attempts made for each case (ranges between 1 and 6)</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'># Load the data
involvement_survey_str2s

# Prepare the data for analysis with the 'survey' package

  library(survey)

  involvement_survey &lt;- svydesign(
    data = involvement_survey_str2s,
    weights = ~ BASE_WEIGHT,
    strata =  ~ SCHOOL_DISTRICT,
    ids =     ~ SCHOOL_ID             + UNIQUE_ID,
    fpc =     ~ N_SCHOOLS_IN_DISTRICT + N_STUDENTS_IN_SCHOOL
  )

</code></pre>

<hr>
<h2 id='predict_outcome_via_glm'>Fit a regression model to predict survey outcomes</h2><span id='topic+predict_outcome_via_glm'></span>

<h3>Description</h3>

<p>A regression model is fit to the sample data to
predict outcomes measured by a survey.
This model can be used to identify auxiliary variables that are
predictive of survey outcomes and hence are potentially useful
for nonresponse bias analysis or weighting adjustments. <br />
</p>
<p>Only data from survey respondents will be used to fit the model,
since survey outcomes are only measured among respondents. <br />
</p>
<p>The function returns a summary of the model, including overall tests
for each variable of whether that variable improves the model's
ability to predict response status in the population of interest (not just in the random sample at hand). <br />
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_outcome_via_glm(
  survey_design,
  outcome_variable,
  outcome_type = "continuous",
  outcome_to_predict = NULL,
  numeric_predictors = NULL,
  categorical_predictors = NULL,
  model_selection = "main-effects",
  selection_controls = list(alpha_enter = 0.5, alpha_remain = 0.5, max_iterations = 100L)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_outcome_via_glm_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="predict_outcome_via_glm_+3A_outcome_variable">outcome_variable</code></td>
<td>
<p>Name of an outcome variable to use as the dependent variable in the model
The value of this variable is expected to be <code>NA</code> (i.e. missing)
for all cases other than eligible respondents.</p>
</td></tr>
<tr><td><code id="predict_outcome_via_glm_+3A_outcome_type">outcome_type</code></td>
<td>
<p>Either <code>"binary"</code> or <code>"continuous"</code>. For <code>"binary"</code>,
a logistic regression model is used. For <code>"continuous"</code>, a generalized linear model
is fit using using an identity link function.</p>
</td></tr>
<tr><td><code id="predict_outcome_via_glm_+3A_outcome_to_predict">outcome_to_predict</code></td>
<td>
<p>Only required if <code>outcome_type="binary"</code>.
Specify which category of <code>outcome_variable</code> is to be predicted.</p>
</td></tr>
<tr><td><code id="predict_outcome_via_glm_+3A_numeric_predictors">numeric_predictors</code></td>
<td>
<p>A list of names of numeric auxiliary variables to use for predicting response status.</p>
</td></tr>
<tr><td><code id="predict_outcome_via_glm_+3A_categorical_predictors">categorical_predictors</code></td>
<td>
<p>A list of names of categorical auxiliary variables to use for predicting response status.</p>
</td></tr>
<tr><td><code id="predict_outcome_via_glm_+3A_model_selection">model_selection</code></td>
<td>
<p>A character string specifying how to select a model.
The default and recommended method is 'main-effects', which simply includes main effects
for each of the predictor variables. <br />
The method <code>'stepwise'</code> can be used to perform stepwise selection of variables for the model.
However, stepwise selection invalidates p-values, standard errors, and confidence intervals,
which are generally calculated under the assumption that model specification is predetermined.</p>
</td></tr>
<tr><td><code id="predict_outcome_via_glm_+3A_selection_controls">selection_controls</code></td>
<td>
<p>Only required if <code>model-selection</code> isn't set to <code>"main-effects"</code>.
Otherwise, a list of parameters for model selection to pass on to <code><a href="#topic+stepwise_model_selection">stepwise_model_selection</a></code>,
with elements <code>alpha_enter</code>, <code>alpha_remain</code>, and <code>max_iterations</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Lumley and Scott (2017) for details of how regression models are fit to survey data.
For overall tests of variables, a Rao-Scott Likelihood Ratio Test is conducted
(see section 4 of Lumley and Scott (2017) for statistical details)
using the function <code>regTermTest(method = "LRT", lrt.approximation = "saddlepoint")</code>
from the 'survey' package. <br />
</p>
<p>If the user specifies <code>model_selection = "stepwise"</code>, a regression model
is selected by adding and removing variables based on the p-value from a
likelihood ratio rest. At each stage, a single variable is added to the model if
the p-value of the likelihood ratio test from adding the variable is below <code>alpha_enter</code>
and its p-value is less than that of all other variables not already in the model.
Next, of the variables already in the model, the variable with the largest p-value
is dropped if its p-value is greater than <code>alpha_remain</code>. This iterative process
continues until a maximum number of iterations is reached or until
either all variables have been added to the model or there are no unadded variables
for which the likelihood ratio test has a p-value below <code>alpha_enter</code>.
</p>


<h3>Value</h3>

<p>A data frame summarizing the fitted regression model. <br />
</p>
<p>Each row in the data frame represents a coefficient in the model.
The column <code>variable</code> describes the underlying variable
for the coefficient. For categorical variables, the column <code>variable_category</code>
indicates the particular category of that variable for which a coefficient is estimated. <br />
</p>
<p>The columns <code>estimated_coefficient</code>, <code>se_coefficient</code>,
<code>conf_intrvl_lower</code>, <code>conf_intrvl_upper</code>, and <code>p_value_coefficient</code>
are summary statistics for the estimated coefficient. Note that <code>p_value_coefficient</code>
is based on the Wald t-test for the coefficient. <br />
</p>
<p>The column <code>variable_level_p_value</code> gives the p-value of the
Rao-Scott Likelihood Ratio Test for including the variable in the model.
This likelihood ratio test has its test statistic given by the column
<code>LRT_chisq_statistic</code>, and the reference distribution
for this test is a linear combination of <code>p</code> F-distributions
with numerator degrees of freedom given by <code>LRT_df_numerator</code>
and denominator degrees of freedom given by <code>LRT_df_denominator</code>,
where <code>p</code> is the number of coefficients in the model corresponding to
the variable being tested.
</p>


<h3>References</h3>


<ul>
<li><p> Lumley, T., &amp; Scott A. (2017). Fitting Regression Models to Survey Data. Statistical Science 32 (2) 265 - 278. https://doi.org/10.1214/16-STS605
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(survey)

# Create a survey design ----
data(involvement_survey_str2s, package = "nrba")

survey_design &lt;- svydesign(
  weights = ~BASE_WEIGHT,
  strata = ~SCHOOL_DISTRICT,
  id = ~ SCHOOL_ID + UNIQUE_ID,
  fpc = ~ N_SCHOOLS_IN_DISTRICT + N_STUDENTS_IN_SCHOOL,
  data = involvement_survey_str2s
)

predict_outcome_via_glm(
  survey_design = survey_design,
  outcome_variable = "WHETHER_PARENT_AGREES",
  outcome_type = "binary",
  outcome_to_predict = "AGREE",
  model_selection = "main-effects",
  numeric_predictors = c("STUDENT_AGE"),
  categorical_predictors = c("STUDENT_DISABILITY_CATEGORY", "PARENT_HAS_EMAIL")
)

</code></pre>

<hr>
<h2 id='predict_response_status_via_glm'>Fit a logistic regression model to predict response to the survey.</h2><span id='topic+predict_response_status_via_glm'></span>

<h3>Description</h3>

<p>A logistic regression model is fit to the sample data to
predict whether an individual responds to the survey (i.e. is an eligible respondent)
rather than a nonrespondent. Ineligible cases and cases with unknown eligibility status
are not included in this model. <br />
</p>
<p>The function returns a summary of the model, including overall tests
for each variable of whether that variable improves the model's
ability to predict response status in the population of interest (not just in the random sample at hand). <br />
</p>
<p>This model can be used to identify auxiliary variables associated with response status
and compare multiple auxiliary variables in terms of their ability to predict response status.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_response_status_via_glm(
  survey_design,
  status,
  status_codes = c("ER", "EN", "IE", "UE"),
  numeric_predictors = NULL,
  categorical_predictors = NULL,
  model_selection = "main-effects",
  selection_controls = list(alpha_enter = 0.5, alpha_remain = 0.5, max_iterations = 100L)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_response_status_via_glm_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="predict_response_status_via_glm_+3A_status">status</code></td>
<td>
<p>A character string giving the name of the variable representing response/eligibility status.
The <code>status</code> variable should have at most four categories,
representing eligible respondents (ER), eligible nonrespondents (EN),
known ineligible cases (IE), and cases whose eligibility is unknown (UE).</p>
</td></tr>
<tr><td><code id="predict_response_status_via_glm_+3A_status_codes">status_codes</code></td>
<td>
<p>A named vector, with two entries named 'ER' and 'EN'
indicating which values of the <code>status</code> variable represent
eligible respondents (ER) and eligible nonrespondents (EN).</p>
</td></tr>
<tr><td><code id="predict_response_status_via_glm_+3A_numeric_predictors">numeric_predictors</code></td>
<td>
<p>A list of names of numeric auxiliary variables to use for predicting response status.</p>
</td></tr>
<tr><td><code id="predict_response_status_via_glm_+3A_categorical_predictors">categorical_predictors</code></td>
<td>
<p>A list of names of categorical auxiliary variables to use for predicting response status.</p>
</td></tr>
<tr><td><code id="predict_response_status_via_glm_+3A_model_selection">model_selection</code></td>
<td>
<p>A character string specifying how to select a model.
The default and recommended method is 'main-effects', which simply includes main effects
for each of the predictor variables. <br />
The method <code>'stepwise'</code> can be used to perform stepwise selection of variables for the model.
However, stepwise selection invalidates p-values, standard errors, and confidence intervals,
which are generally calculated under the assumption that model specification is predetermined.</p>
</td></tr>
<tr><td><code id="predict_response_status_via_glm_+3A_selection_controls">selection_controls</code></td>
<td>
<p>Only required if <code>model-selection</code> isn't set to <code>"main-effects"</code>.
Otherwise, a list of parameters for model selection to pass on to <code><a href="#topic+stepwise_model_selection">stepwise_model_selection</a></code>,
with elements <code>alpha_enter</code>, <code>alpha_remain</code>, and <code>max_iterations</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Lumley and Scott (2017) for details of how regression models are fit to survey data.
For overall tests of variables, a Rao-Scott Likelihood Ratio Test is conducted
(see section 4 of Lumley and Scott (2017) for statistical details)
using the function <code>regTermTest(method = "LRT", lrt.approximation = "saddlepoint")</code>
from the 'survey' package. <br />
</p>
<p>If the user specifies <code>model_selection = "stepwise"</code>, a regression model
is selected by adding and removing variables based on the p-value from a
likelihood ratio rest. At each stage, a single variable is added to the model if
the p-value of the likelihood ratio test from adding the variable is below <code>alpha_enter</code>
and its p-value is less than that of all other variables not already in the model.
Next, of the variables already in the model, the variable with the largest p-value
is dropped if its p-value is greater than <code>alpha_remain</code>. This iterative process
continues until a maximum number of iterations is reached or until
either all variables have been added to the model or there are no unadded variables
for which the likelihood ratio test has a p-value below <code>alpha_enter</code>.
</p>


<h3>Value</h3>

<p>A data frame summarizing the fitted logistic regression model. <br />
</p>
<p>Each row in the data frame represents a coefficient in the model.
The column <code>variable</code> describes the underlying variable
for the coefficient. For categorical variables, the column <code>variable_category</code> indicates
the particular category of that variable for which a coefficient is estimated. <br />
</p>
<p>The columns <code>estimated_coefficient</code>, <code>se_coefficient</code>, <code>conf_intrvl_lower</code>, <code>conf_intrvl_upper</code>,
and <code>p_value_coefficient</code> are summary statistics for
the estimated coefficient. Note that <code>p_value_coefficient</code> is based on the Wald t-test for the coefficient. <br />
</p>
<p>The column <code>variable_level_p_value</code> gives the p-value of the
Rao-Scott Likelihood Ratio Test for including the variable in the model.
This likelihood ratio test has its test statistic given by the column
<code>LRT_chisq_statistic</code>, and the reference distribution
for this test is a linear combination of <code>p</code> F-distributions
with numerator degrees of freedom given by <code>LRT_df_numerator</code> and
denominator degrees of freedom given by <code>LRT_df_denominator</code>,
where <code>p</code> is the number of coefficients in the model corresponding to
the variable being tested.
</p>


<h3>References</h3>


<ul>
<li><p> Lumley, T., &amp; Scott A. (2017). Fitting Regression Models to Survey Data. Statistical Science 32 (2) 265 - 278. https://doi.org/10.1214/16-STS605
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(survey)

# Create a survey design ----
data(involvement_survey_str2s, package = "nrba")

survey_design &lt;- survey_design &lt;- svydesign(
  data = involvement_survey_str2s,
  weights = ~BASE_WEIGHT,
  strata = ~SCHOOL_DISTRICT,
  ids = ~ SCHOOL_ID + UNIQUE_ID,
  fpc = ~ N_SCHOOLS_IN_DISTRICT + N_STUDENTS_IN_SCHOOL
)

predict_response_status_via_glm(
  survey_design = survey_design,
  status = "RESPONSE_STATUS",
  status_codes = c(
    "ER" = "Respondent",
    "EN" = "Nonrespondent",
    "IE" = "Ineligible",
    "UE" = "Unknown"
  ),
  model_selection = "main-effects",
  numeric_predictors = c("STUDENT_AGE"),
  categorical_predictors = c("PARENT_HAS_EMAIL", "STUDENT_GRADE")
)

</code></pre>

<hr>
<h2 id='rake_to_benchmarks'>Re-weight data to match population benchmarks, using raking or post-stratification</h2><span id='topic+rake_to_benchmarks'></span>

<h3>Description</h3>

<p>Adjusts weights in the data to ensure that estimated population totals for
grouping variables match known population benchmarks. If there is only one grouping variable,
simple post-stratification is used. If there are multiple grouping variables,
raking (also known as iterative post-stratification) is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rake_to_benchmarks(
  survey_design,
  group_vars,
  group_benchmark_vars,
  max_iterations = 100,
  epsilon = 5e-06
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rake_to_benchmarks_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="rake_to_benchmarks_+3A_group_vars">group_vars</code></td>
<td>
<p>Names of grouping variables in the data dividing the sample
into groups for which benchmark data are available. These variables cannot have any missing values</p>
</td></tr>
<tr><td><code id="rake_to_benchmarks_+3A_group_benchmark_vars">group_benchmark_vars</code></td>
<td>
<p>Names of group benchmark variables in the data corresponding to <code>group_vars</code>.
For each category of a grouping variable, the group benchmark variable gives the
population benchmark (i.e. population size) for that category.</p>
</td></tr>
<tr><td><code id="rake_to_benchmarks_+3A_max_iterations">max_iterations</code></td>
<td>
<p>If there are multiple grouping variables,
then raking is used rather than post-stratification.
The parameter <code>max_iterations</code> controls the maximum number of iterations to
use in raking.</p>
</td></tr>
<tr><td><code id="rake_to_benchmarks_+3A_epsilon">epsilon</code></td>
<td>
<p>If raking is used, convergence for a given margin is declared
if the maximum change in a re-weighted total is less than <code>epsilon</code> times
the total sum of the original weights in the design.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Raking adjusts the weight assigned to each sample member
so that, after reweighting, the weighted sample percentages for population subgroups
match their known population percentages. In a sense, raking causes
the sample to more closely resemble the population in terms of variables
for which population sizes are known.
<br />
<br />
Raking can be useful to reduce nonresponse bias caused by
having groups which are overrepresented in the responding sample
relative to their population size.
If the population subgroups systematically differ in terms of outcome variables of interest,
then raking can also be helpful in terms of reduce sampling variances. However,
when population subgroups do not differ in terms of outcome variables of interest,
then raking may increase sampling variances.
<br />
<br />
There are two basic requirements for raking. <br />
</p>

<ul>
<li><p> Basic Requirement 1 - Values of the grouping variable(s) must be known for all respondents.
</p>
</li>
<li><p> Basic Requirement 2 - The population size of each group must be known (or precisely estimated).
</p>
</li></ul>

<p>When there is effectively only one grouping variable
(though this variable can be defined as a combination of other variables),
raking amounts to simple post-stratification.
For example, simple post-stratification would be used if the grouping variable
is &quot;Age x Sex x Race&quot;, and the population size of each combination of
age, sex, and race is known.
The method of &quot;iterative poststratification&quot; (also known as &quot;iterative proportional fitting&quot;)
is used when there are multiple grouping variables,
and population sizes are known for each grouping variable
but not for combinations of grouping variables.
For example, iterative proportional fitting would be necessary
if population sizes are known for age groups and for gender categories
but not for combinations of age groups and gender categories.
</p>


<h3>Value</h3>

<p>A survey design object with raked or post-stratified weights
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load the survey data

data(involvement_survey_srs, package = "nrba")

# Calculate population benchmarks
population_benchmarks &lt;- list(
  "PARENT_HAS_EMAIL" = data.frame(
    PARENT_HAS_EMAIL = c("Has Email", "No Email"),
    PARENT_HAS_EMAIL_POP_BENCHMARK = c(17036, 2964)
  ),
  "STUDENT_RACE" = data.frame(
    STUDENT_RACE = c(
      "AM7 (American Indian or Alaska Native)", "AS7 (Asian)",
      "BL7 (Black or African American)",
      "HI7 (Hispanic or Latino Ethnicity)", "MU7 (Two or More Races)",
      "PI7 (Native Hawaiian or Other Pacific Islander)",
      "WH7 (White)"
    ),
    STUDENT_RACE_POP_BENCHMARK = c(206, 258, 3227, 1097, 595, 153, 14464)
  )
)

# Add the population benchmarks as variables in the data
involvement_survey_srs &lt;- merge(
  x = involvement_survey_srs,
  y = population_benchmarks$PARENT_HAS_EMAIL,
  by = "PARENT_HAS_EMAIL"
)
involvement_survey_srs &lt;- merge(
  x = involvement_survey_srs,
  y = population_benchmarks$STUDENT_RACE,
  by = "STUDENT_RACE"
)

# Create a survey design object
library(survey)

survey_design &lt;- svydesign(
  weights = ~BASE_WEIGHT,
  id = ~UNIQUE_ID,
  fpc = ~N_STUDENTS,
  data = involvement_survey_srs
)

# Subset data to only include respondents
survey_respondents &lt;- subset(
  survey_design,
  RESPONSE_STATUS == "Respondent"
)

# Rake to the benchmarks
raked_survey_design &lt;- rake_to_benchmarks(
  survey_design = survey_respondents,
  group_vars = c("PARENT_HAS_EMAIL", "STUDENT_RACE"),
  group_benchmark_vars = c(
    "PARENT_HAS_EMAIL_POP_BENCHMARK",
    "STUDENT_RACE_POP_BENCHMARK"
  ),
)

# Inspect estimates from respondents, before and after raking

svymean(
  x = ~PARENT_HAS_EMAIL,
  design = survey_respondents
)
svymean(
  x = ~PARENT_HAS_EMAIL,
  design = raked_survey_design
)

svymean(
  x = ~WHETHER_PARENT_AGREES,
  design = survey_respondents
)
svymean(
  x = ~WHETHER_PARENT_AGREES,
  design = raked_survey_design
)

</code></pre>

<hr>
<h2 id='stepwise_model_selection'>Select and fit a model using stepwise regression</h2><span id='topic+stepwise_model_selection'></span>

<h3>Description</h3>

<p>A regression model is selected by iteratively adding and removing variables based on the p-value from a
likelihood ratio rest. At each stage, a single variable is added to the model if
the p-value of the likelihood ratio test from adding the variable is below <code>alpha_enter</code>
and its p-value is less than that of all other variables not already in the model.
Next, of the variables already in the model, the variable with the largest p-value
is dropped if its p-value is greater than <code>alpha_remain</code>. This iterative process
continues until a maximum number of iterations is reached or until
either all variables have been added to the model or there are no variables
not yet in the model whose likelihood ratio test has a p-value below <code>alpha_enter</code>. <br />
</p>
<p>Stepwise model selection generally invalidates inferential statistics
such as p-values, standard errors, or confidence intervals and leads to
overestimation of the size of coefficients for variables included in the selected model.
This bias increases as the value of <code>alpha_enter</code> or <code>alpha_remain</code> decreases.
The use of stepwise model selection should be limited only to
reducing a large list of candidate variables for nonresponse adjustment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stepwise_model_selection(
  survey_design,
  outcome_variable,
  predictor_variables,
  model_type = "binary-logistic",
  max_iterations = 100L,
  alpha_enter = 0.5,
  alpha_remain = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stepwise_model_selection_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="stepwise_model_selection_+3A_outcome_variable">outcome_variable</code></td>
<td>
<p>The name of an outcome variable to use as the dependent variable.</p>
</td></tr>
<tr><td><code id="stepwise_model_selection_+3A_predictor_variables">predictor_variables</code></td>
<td>
<p>A list of names of variables to consider as predictors for the model.</p>
</td></tr>
<tr><td><code id="stepwise_model_selection_+3A_model_type">model_type</code></td>
<td>
<p>A character string describing the type of model to fit.
<code>'binary-logistic'</code> for a binary logistic regression,
<code>'ordinal-logistic'</code> for an ordinal logistic regression (cumulative proportional-odds),
<code>'normal'</code> for the typical model which assumes residuals follow a Normal distribution.</p>
</td></tr>
<tr><td><code id="stepwise_model_selection_+3A_max_iterations">max_iterations</code></td>
<td>
<p>Maximum number of iterations to try adding new variables to the model.</p>
</td></tr>
<tr><td><code id="stepwise_model_selection_+3A_alpha_enter">alpha_enter</code></td>
<td>
<p>The maximum p-value allowed for a variable to be added to the model.
Large values such as 0.5 or greater are recommended to reduce the bias
of estimates from the selected model.</p>
</td></tr>
<tr><td><code id="stepwise_model_selection_+3A_alpha_remain">alpha_remain</code></td>
<td>
<p>The maximum p-value allowed for a variable to remain in the model.
Large values such as 0.5 or greater are recommended to reduce the bias
of estimates from the selected model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Lumley and Scott (2017) for details of how regression models are fit to survey data.
For overall tests of variables, a Rao-Scott Likelihood Ratio Test is conducted
(see section 4 of Lumley and Scott (2017) for statistical details)
using the function <code>regTermTest(method = "LRT", lrt.approximation = "saddlepoint")</code>
from the 'survey' package.
</p>
<p>See Sauerbrei et al. (2020) for a discussion of statistical issues with using stepwise model selection.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="survey.html#topic+svyglm">svyglm</a></code> representing
a regression model fit using the 'survey' package.
</p>


<h3>References</h3>


<ul>
<li><p> Lumley, T., &amp; Scott A. (2017). Fitting Regression Models to Survey Data. Statistical Science 32 (2) 265 - 278. https://doi.org/10.1214/16-STS605
</p>
</li>
<li><p> Sauerbrei, W., Perperoglou, A., Schmid, M. et al. (2020). State of the art in selection of variables and functional forms in multivariable analysis - outstanding issues. Diagnostic and Prognostic Research 4, 3. https://doi.org/10.1186/s41512-020-00074-3
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(survey)

# Load example data and prepare it for analysis
data(involvement_survey_str2s, package = 'nrba')

involvement_survey &lt;- svydesign(
  data = involvement_survey_str2s,
  ids = ~ SCHOOL_ID + UNIQUE_ID,
  fpc = ~ N_SCHOOLS_IN_DISTRICT + N_STUDENTS_IN_SCHOOL,
  strata = ~ SCHOOL_DISTRICT,
  weights = ~ BASE_WEIGHT
)

involvement_survey &lt;- involvement_survey |&gt;
    transform(WHETHER_PARENT_AGREES = factor(WHETHER_PARENT_AGREES))

# Fit a regression model using stepwise selection
selected_model &lt;- stepwise_model_selection(
  survey_design = involvement_survey,
  outcome_variable = "WHETHER_PARENT_AGREES",
  predictor_variables = c("STUDENT_RACE", "STUDENT_DISABILITY_CATEGORY"),
  model_type = "binary-logistic",
  max_iterations = 100,
  alpha_enter = 0.5,
  alpha_remain = 0.5
)
</code></pre>

<hr>
<h2 id='t_test_by_response_status'>t-test of differences in means/percentages between responding sample and full sample, or between responding sample and eligible sample</h2><span id='topic+t_test_by_response_status'></span><span id='topic+t_test_resp_vs_full'></span><span id='topic+t_test_resp_vs_elig'></span>

<h3>Description</h3>

<p>The function <code>t_test_resp_vs_full</code> tests whether means of auxiliary variables differ between respondents and the full selected sample,
where the full sample consists of all cases regardless of response status or eligibility status. <br />
The function <code>t_test_resp_vs_elig</code> tests whether means differ between the responding sample and the eligible sample,
where the eligible sample consists of all cases known to be eligible, regardless of response status.
</p>
<p>See Lohr and Riddles (2016) for the statistical theory of this test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_test_resp_vs_full(
  survey_design,
  y_vars,
  na.rm = TRUE,
  status,
  status_codes = c("ER", "EN", "IE", "UE"),
  null_difference = 0,
  alternative = "unequal",
  degrees_of_freedom = survey::degf(survey_design) - 1
)

t_test_resp_vs_elig(
  survey_design,
  y_vars,
  na.rm = TRUE,
  status,
  status_codes = c("ER", "EN", "IE", "UE"),
  null_difference = 0,
  alternative = "unequal",
  degrees_of_freedom = survey::degf(survey_design) - 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_test_by_response_status_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="t_test_by_response_status_+3A_y_vars">y_vars</code></td>
<td>
<p>Names of dependent variables for tests. For categorical variables, percentages of each category are tested.</p>
</td></tr>
<tr><td><code id="t_test_by_response_status_+3A_na.rm">na.rm</code></td>
<td>
<p>Whether to drop cases with missing values for a given dependent variable.</p>
</td></tr>
<tr><td><code id="t_test_by_response_status_+3A_status">status</code></td>
<td>
<p>The name of the variable representing response/eligibility status. <br />
The <code>status</code> variable should have at most four categories,
representing eligible respondents (ER), eligible nonrespondents (EN),
known ineligible cases (IE), and cases whose eligibility is unknown (UE).</p>
</td></tr>
<tr><td><code id="t_test_by_response_status_+3A_status_codes">status_codes</code></td>
<td>
<p>A named vector, with four entries named 'ER', 'EN', 'IE', and 'UE'. <br />
<code>status_codes</code> indicates how the values of the <code>status</code> variable are to be interpreted.</p>
</td></tr>
<tr><td><code id="t_test_by_response_status_+3A_null_difference">null_difference</code></td>
<td>
<p>The difference between the two means under the null hypothesis. Default is <code>0</code>.</p>
</td></tr>
<tr><td><code id="t_test_by_response_status_+3A_alternative">alternative</code></td>
<td>
<p>Can be one of the following: <br />
</p>

<ul>
<li> <p><code>'unequal'</code> (the default): two-sided test of whether difference in means is equal to <code>null_difference</code> <br />
</p>
</li>
<li> <p><code>'less'</code>: one-sided test of whether difference is less than <code>null_difference</code> <br />
</p>
</li>
<li> <p><code>'greater'</code>: one-sided test of whether difference is greater than <code>null_difference</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="t_test_by_response_status_+3A_degrees_of_freedom">degrees_of_freedom</code></td>
<td>
<p>The degrees of freedom to use for the test's reference distribution.
Unless specified otherwise, the default is the design degrees of freedom minus one,
where the design degrees of freedom are estimated using the <code>survey</code> package's <code>degf</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame describing the results of the t-tests, one row per dependent variable.
</p>


<h3>Statistical Details</h3>

<p>The t-statistic used for the test has as its numerator the difference in means between the two samples, minus the <code>null_difference</code>.
The denominator for the t-statistic is the estimated standard error of the difference in means.
Because the two means are based on overlapping groups and thus have correlated sampling errors, special care is taken to estimate the covariance of the two estimates.
For designs which use sets of replicate weights for variance estimation, the two means and their difference are estimated using each set of replicate weights;
the estimated differences from the sets of replicate weights are then used to estimate sampling error with a formula appropriate to the replication method (JKn, BRR, etc.).
For designs which use linearization methods for variance estimation, the covariance between the two means is estimated using the method of linearization based on influence functions implemented in the <code>survey</code> package.
See Osier (2009) for an overview of the method of linearization based on influence functions.
Eckman et al. (2023) showed in a simulation study that linearization and replication
performed similarly in estimating the variance of a difference in means for overlapping samples. <br />
<br />
Unless specified otherwise using the <code>degrees_of_freedom</code> parameter, the degrees of freedom for the test are set to the design degrees of freedom minus one.
Design degrees of freedom are estimated using the <code>survey</code> package's <code>degf</code> method. <br />
<br />
See Lohr and Riddles (2016) for the statistical details of this test.
See Van de Kerckhove et al. (2009) and  Amaya and Presser (2017)
for examples of a nonresponse bias analysis which uses t-tests to compare responding samples to eligible samples.
</p>


<h3>References</h3>


<ul>
<li><p> Amaya, A., Presser, S. (2017). <em>Nonresponse Bias for Univariate and Multivariate Estimates of Social Activities and Roles</em>. Public Opinion Quarterly, Volume 81, Issue 1, 1 March 2017, Pages 136, https://doi.org/10.1093/poq/nfw037
</p>
</li>
<li><p> Eckman, S., Unangst, J., Dever, J., Antoun, A. (2023). <em>The Precision of Estimates of Nonresponse Bias in Means</em>. Journal of Survey Statistics and Methodology, 11(4), 758-783. https://doi.org/10.1093/jssam/smac019
</p>
</li>
<li><p> Lohr, S., Riddles, M. (2016). <em>Tests for Evaluating Nonresponse Bias in Surveys</em>. Survey Methodology 42(2): 195-218. https://www150.statcan.gc.ca/n1/pub/12-001-x/2016002/article/14677-eng.pdf
</p>
</li>
<li><p> Osier, G. (2009). <em>Variance estimation for complex indicators of poverty and inequality using linearization techniques</em>. Survey Research Methods, 3(3), 167-195. https://doi.org/10.18148/srm/2009.v3i3.369
</p>
</li>
<li><p> Van de Kerckhove, W., Krenzke, T., and Mohadjer, L. (2009). <em>Adult Literacy and Lifeskills Survey (ALL) 2003: U.S. Nonresponse Bias Analysis (NCES 2009-063)</em>. National Center for Education Statistics, Institute of Education Sciences, U.S. Department of Education. Washington, DC.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(survey)

# Create a survey design ----
data(involvement_survey_srs, package = 'nrba')

survey_design &lt;- svydesign(weights = ~ BASE_WEIGHT,
                           id = ~ UNIQUE_ID,
                           fpc = ~ N_STUDENTS,
                           data = involvement_survey_srs)

# Compare respondents' mean to the full sample mean ----

t_test_resp_vs_full(survey_design = survey_design,
                    y_vars = c("STUDENT_AGE", "WHETHER_PARENT_AGREES"),
                    status = 'RESPONSE_STATUS',
                    status_codes = c('ER' = "Respondent",
                                     'EN' = "Nonrespondent",
                                     'IE' = "Ineligible",
                                     'UE' = "Unknown"))

# Compare respondents' mean to the mean of all eligible cases ----

t_test_resp_vs_full(survey_design = survey_design,
                    y_vars = c("STUDENT_AGE", "WHETHER_PARENT_AGREES"),
                    status = 'RESPONSE_STATUS',
                    status_codes = c('ER' = "Respondent",
                                     'EN' = "Nonrespondent",
                                     'IE' = "Ineligible",
                                     'UE' = "Unknown"))
# One-sided tests ----

  ## Null Hypothesis: Y_bar_resp - Y_bar_full &lt;= 0.1
  ## Alt. Hypothesis: Y_bar_resp - Y_bar_full &gt;  0.1

t_test_resp_vs_full(survey_design = survey_design,
                    y_vars = c("STUDENT_AGE", "WHETHER_PARENT_AGREES"),
                    status = 'RESPONSE_STATUS',
                    status_codes = c('ER' = "Respondent",
                                     'EN' = "Nonrespondent",
                                     'IE' = "Ineligible",
                                     'UE' = "Unknown"),
                    null_difference = 0.1, alternative = 'greater')

  ## Null Hypothesis: Y_bar_resp - Y_bar_full &gt;= 0.1
  ## Alt. Hypothesis: Y_bar_resp - Y_bar_full &lt;  0.1

t_test_resp_vs_full(survey_design = survey_design,
                    y_vars = c("STUDENT_AGE", "WHETHER_PARENT_AGREES"),
                    status = 'RESPONSE_STATUS',
                    status_codes = c('ER' = "Respondent",
                                     'EN' = "Nonrespondent",
                                     'IE' = "Ineligible",
                                     'UE' = "Unknown"),
                    null_difference = 0.1, alternative = 'less')
</code></pre>

<hr>
<h2 id='t_test_of_weight_adjustment'>t-test of differences in estimated means/percentages from two different sets of replicate weights.</h2><span id='topic+t_test_of_weight_adjustment'></span>

<h3>Description</h3>

<p>Tests whether estimates of means/percentages differ systematically between two sets of replicate weights:
an original set of weights, and the weights after adjustment (e.g. post-stratification or nonresponse adjustments) and possibly subsetting (e.g. subsetting to only include respondents).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_test_of_weight_adjustment(
  orig_design,
  updated_design,
  y_vars,
  na.rm = TRUE,
  null_difference = 0,
  alternative = "unequal",
  degrees_of_freedom = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_test_of_weight_adjustment_+3A_orig_design">orig_design</code></td>
<td>
<p>A replicate design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="t_test_of_weight_adjustment_+3A_updated_design">updated_design</code></td>
<td>
<p>A potentially updated version of <code>orig_design</code>,
for example where weights have been adjusted for nonresponse or updated using post-stratification.
The type and number of sets of replicate weights must match that of <code>orig_design</code>.
The number of rows may differ (e.g. if <code>orig_design</code> includes the full sample but <code>updated_design</code> only includes respondents).</p>
</td></tr>
<tr><td><code id="t_test_of_weight_adjustment_+3A_y_vars">y_vars</code></td>
<td>
<p>Names of dependent variables for tests. For categorical variables, percentages of each category are tested.</p>
</td></tr>
<tr><td><code id="t_test_of_weight_adjustment_+3A_na.rm">na.rm</code></td>
<td>
<p>Whether to drop cases with missing values for a given dependent variable.</p>
</td></tr>
<tr><td><code id="t_test_of_weight_adjustment_+3A_null_difference">null_difference</code></td>
<td>
<p>The difference between the two means/percentages under the null hypothesis. Default is <code>0</code>.</p>
</td></tr>
<tr><td><code id="t_test_of_weight_adjustment_+3A_alternative">alternative</code></td>
<td>
<p>Can be one of the following: <br />
</p>

<ul>
<li> <p><code>'unequal'</code> (the default): two-sided test of whether difference in means is equal to <code>null_difference</code> <br />
</p>
</li>
<li> <p><code>'less'</code>: one-sided test of whether difference is less than <code>null_difference</code> <br />
</p>
</li>
<li> <p><code>'greater'</code>: one-sided test of whether difference is greater than <code>null_difference</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="t_test_of_weight_adjustment_+3A_degrees_of_freedom">degrees_of_freedom</code></td>
<td>
<p>The degrees of freedom to use for the test's reference distribution.
Unless specified otherwise, the default is the design degrees of freedom minus one,
where the design degrees of freedom are estimated using the <code>survey</code> package's <code>degf</code> method
applied to the 'stacked' design formed by combining <code>orig_design</code> and <code>updated_design</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame describing the results of the t-tests, one row per dependent variable.
</p>


<h3>Statistical Details</h3>

<p>The t-statistic used for the test has as its numerator the difference in means/percentages between the two samples, minus the <code>null_difference</code>.
The denominator for the t-statistic is the estimated standard error of the difference in means.
Because the two means are based on overlapping groups and thus have correlated sampling errors, special care is taken to estimate the covariance of the two estimates.
For designs which use sets of replicate weights for variance estimation, the two means and their difference are estimated using each set of replicate weights;
the estimated differences from the sets of replicate weights are then used to estimate sampling error with a formula appropriate to the replication method (JKn, BRR, etc.). <br />
<br />
This analysis is not implemented for designs which use linearization methods for variance estimation.
<br />
Unless specified otherwise using the <code>degrees_of_freedom</code> parameter, the degrees of freedom for the test are set to the design degrees of freedom minus one.
Design degrees of freedom are estimated using the <code>survey</code> package's <code>degf</code> method. <br />
<br />
See Van de Kerckhove et al. (2009) for an example of this type of nonresponse bias analysis (among others).
See Lohr and Riddles (2016) for the statistical details of this test.
</p>


<h3>References</h3>


<ul>
<li><p> Lohr, S., Riddles, M. (2016). <em>Tests for Evaluating Nonresponse Bias in Surveys</em>. Survey Methodology 42(2): 195-218. https://www150.statcan.gc.ca/n1/pub/12-001-x/2016002/article/14677-eng.pdf
</p>
</li>
<li><p> Van de Kerckhove, W., Krenzke, T., and Mohadjer, L. (2009). <em>Adult Literacy and Lifeskills Survey (ALL) 2003: U.S. Nonresponse Bias Analysis (NCES 2009-063)</em>. National Center for Education Statistics, Institute of Education Sciences, U.S. Department of Education. Washington, DC.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
library(survey)

# Create a survey design ----

data(involvement_survey_srs, package = 'nrba')

survey_design &lt;- svydesign(weights = ~ BASE_WEIGHT,
                           id = ~ UNIQUE_ID,
                           fpc = ~ N_STUDENTS,
                           data = involvement_survey_srs)

# Create replicate weights for the design ----
rep_svy_design &lt;- as.svrepdesign(survey_design, type = "subbootstrap",
                                 replicates = 500)

# Subset to only respondents (always subset *after* creating replicate weights)

rep_svy_respondents &lt;- subset(rep_svy_design,
                              RESPONSE_STATUS == "Respondent")

# Apply raking adjustment ----

raked_rep_svy_respondents &lt;- rake_to_benchmarks(
  survey_design = rep_svy_respondents,
  group_vars = c("PARENT_HAS_EMAIL", "STUDENT_RACE"),
  group_benchmark_vars = c("PARENT_HAS_EMAIL_BENCHMARK",
                           "STUDENT_RACE_BENCHMARK"),
)

# Compare estimates from respondents in original vs. adjusted design ----

t_test_of_weight_adjustment(orig_design = rep_svy_respondents,
                            updated_design = raked_rep_svy_respondents,
                            y_vars = c('STUDENT_AGE', 'STUDENT_SEX'))

t_test_of_weight_adjustment(orig_design = rep_svy_respondents,
                            updated_design = raked_rep_svy_respondents,
                            y_vars = c('WHETHER_PARENT_AGREES'))

# Compare estimates to true population values ----

data('involvement_survey_pop', package = 'nrba')

mean(involvement_survey_pop$STUDENT_AGE)

prop.table(table(involvement_survey_pop$STUDENT_SEX))

</code></pre>

<hr>
<h2 id='t_test_overlap'>Test for differences in means/percentages between two potentially overlapping groups</h2><span id='topic+t_test_overlap'></span>

<h3>Description</h3>

<p>Test for differences in means/percentages between two potentially overlapping groups
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_test_overlap(
  survey_design,
  y_vars,
  na.rm = TRUE,
  status,
  group_1,
  group_2,
  null_difference = 0,
  alternative = "unequal",
  degrees_of_freedom = survey::degf(survey_design) - 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_test_overlap_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="t_test_overlap_+3A_y_vars">y_vars</code></td>
<td>
<p>Names of dependent variables for tests. For categorical variables, percentages of each category are tested.</p>
</td></tr>
<tr><td><code id="t_test_overlap_+3A_na.rm">na.rm</code></td>
<td>
<p>Whether to drop cases with missing values for a given dependent variable.</p>
</td></tr>
<tr><td><code id="t_test_overlap_+3A_status">status</code></td>
<td>
<p>The name of the variable representing response/eligibility status.</p>
</td></tr>
<tr><td><code id="t_test_overlap_+3A_group_1">group_1</code></td>
<td>
<p>Vector of values of <code>status</code> variable representing the first group</p>
</td></tr>
<tr><td><code id="t_test_overlap_+3A_group_2">group_2</code></td>
<td>
<p>Vector of values of <code>status</code> variable representing the second group</p>
</td></tr>
<tr><td><code id="t_test_overlap_+3A_null_difference">null_difference</code></td>
<td>
<p>The hypothesized difference between the groups' means. Default is <code>0</code>.</p>
</td></tr>
<tr><td><code id="t_test_overlap_+3A_alternative">alternative</code></td>
<td>
<p>Can be one of the following: <br />
</p>

<ul>
<li> <p><code>'unequal'</code>: two-sided test of whether difference in means is equal to <code>null_difference</code> <br />
</p>
</li>
<li> <p><code>'less'</code>: one-sided test of whether difference is less than <code>null_difference</code> <br />
</p>
</li>
<li> <p><code>'greater'</code>: one-sided test of whether difference is greater than <code>null_difference</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="t_test_overlap_+3A_degrees_of_freedom">degrees_of_freedom</code></td>
<td>
<p>The degrees of freedom to use for the test's reference distribution.
Unless specified otherwise, the default is the design degrees of freedom minus one,
where the design degrees of freedom are estimated using the <code>survey</code> package's <code>degf</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame describing the difference in group means/percentages and the statistics from the t-test
</p>

<hr>
<h2 id='t_test_vs_external_estimate'>t-test of differences in means/percentages relative to external estimates</h2><span id='topic+t_test_vs_external_estimate'></span>

<h3>Description</h3>

<p>Compare estimated means/percentages from the present survey to external estimates from a benchmark source.
A t-test is used to evaluate whether the survey's estimates differ from the external estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_test_vs_external_estimate(
  survey_design,
  y_var,
  ext_ests,
  ext_std_errors = NULL,
  na.rm = TRUE,
  null_difference = 0,
  alternative = "unequal",
  degrees_of_freedom = survey::degf(survey_design) - 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_test_vs_external_estimate_+3A_survey_design">survey_design</code></td>
<td>
<p>A survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="t_test_vs_external_estimate_+3A_y_var">y_var</code></td>
<td>
<p>Name of dependent variable. For categorical variables, percentages of each category are tested.</p>
</td></tr>
<tr><td><code id="t_test_vs_external_estimate_+3A_ext_ests">ext_ests</code></td>
<td>
<p>A numeric vector containing the external estimate of the mean for the dependent variable.
If <code>variable</code> is a categorical variable, a named vector of means must be provided.</p>
</td></tr>
<tr><td><code id="t_test_vs_external_estimate_+3A_ext_std_errors">ext_std_errors</code></td>
<td>
<p>(Optional) The standard errors of the external estimates.
This is useful if the external data are estimated with an appreciable level of uncertainty,
for instance if the external data come from a survey with a small-to-moderate sample size.
If supplied, the variance of the difference between the survey and external estimates
is estimated by adding the variance of the external estimates to the estimated variance
of the survey's estimates.</p>
</td></tr>
<tr><td><code id="t_test_vs_external_estimate_+3A_na.rm">na.rm</code></td>
<td>
<p>Whether to drop cases with missing values for <code>y_var</code></p>
</td></tr>
<tr><td><code id="t_test_vs_external_estimate_+3A_null_difference">null_difference</code></td>
<td>
<p>The hypothesized difference between the estimate and the external mean. Default is <code>0</code>.</p>
</td></tr>
<tr><td><code id="t_test_vs_external_estimate_+3A_alternative">alternative</code></td>
<td>
<p>Can be one of the following: <br />
</p>

<ul>
<li> <p><code>'unequal'</code>: two-sided test of whether difference in means is equal to <code>null_difference</code> <br />
</p>
</li>
<li> <p><code>'less'</code>: one-sided test of whether difference is less than <code>null_difference</code> <br />
</p>
</li>
<li> <p><code>'greater'</code>: one-sided test of whether difference is greater than <code>null_difference</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="t_test_vs_external_estimate_+3A_degrees_of_freedom">degrees_of_freedom</code></td>
<td>
<p>The degrees of freedom to use for the test's reference distribution.
Unless specified otherwise, the default is the design degrees of freedom minus one,
where the design degrees of freedom are estimated using the survey package's <code>degf</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame describing the results of the t-tests, one row per mean being compared.
</p>


<h3>References</h3>

<p>See Brick and Bose (2001) for an example of this analysis method
and a discussion of its limitations.
</p>

<ul>
<li><p> Brick, M., and Bose, J. (2001). <em>Analysis of Potential Nonresponse Bias</em>. in
Proceedings of the Section on Survey Research Methods. Alexandria, VA: American Statistical Association.
http://www.asasrms.org/Proceedings/y2001/Proceed/00021.pdf
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>

library(survey)

# Create a survey design ----
data("involvement_survey_str2s", package = 'nrba')

involvement_survey_sample &lt;- svydesign(
  data = involvement_survey_str2s,
  weights = ~ BASE_WEIGHT,
  strata =  ~ SCHOOL_DISTRICT,
  ids =     ~ SCHOOL_ID             + UNIQUE_ID,
  fpc =     ~ N_SCHOOLS_IN_DISTRICT + N_STUDENTS_IN_SCHOOL
)

# Subset to only include survey respondents ----

involvement_survey_respondents &lt;- subset(involvement_survey_sample,
                                         RESPONSE_STATUS == "Respondent")

# Test whether percentages of categorical variable differ from benchmark ----

parent_email_benchmark &lt;- c(
  'Has Email' = 0.85,
  'No Email' = 0.15
)

t_test_vs_external_estimate(
  survey_design = involvement_survey_respondents,
  y_var = "PARENT_HAS_EMAIL",
  ext_ests = parent_email_benchmark
)

# Test whether the sample mean differs from the population benchmark ----

average_age_benchmark &lt;- 11

t_test_vs_external_estimate(
  survey_design = involvement_survey_respondents,
  y_var = "STUDENT_AGE",
  ext_ests = average_age_benchmark,
  null_difference = 0
)

</code></pre>

<hr>
<h2 id='wt_class_adjust'>Adjust weights in a replicate design for nonresponse or unknown eligibility status, using weighting classes</h2><span id='topic+wt_class_adjust'></span>

<h3>Description</h3>

<p>Updates weights in a survey design object to adjust for nonresponse and/or unknown eligibility
using the method of weighting class adjustment. For unknown eligibility adjustments, the weight in each class
is set to zero for cases with unknown eligibility, and the weight of all other cases in the class is
increased so that the total weight is unchanged. For nonresponse adjustments, the weight in each class
is set to zero for cases classified as eligible nonrespondents, and the weight of eligible respondent cases
in the class is increased so that the total weight is unchanged. <br />
<br />
This function currently only works for survey designs with replicate weights,
since the linearization-based estimators included in the <code>survey</code> package (or Stata or SAS for that matter)
are unable to fully reflect the impact of nonresponse adjustment.
Adjustments are made to both the full-sample weights and all of the sets of replicate weights. <br />
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wt_class_adjust(
  survey_design,
  status,
  status_codes,
  wt_class = NULL,
  type = c("UE", "NR")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wt_class_adjust_+3A_survey_design">survey_design</code></td>
<td>
<p>A replicate survey design object created with the <code>survey</code> package.</p>
</td></tr>
<tr><td><code id="wt_class_adjust_+3A_status">status</code></td>
<td>
<p>A character string giving the name of the variable representing response/eligibility status. <br />
The <code>status</code> variable should have at most four categories,
representing eligible respondents (ER), eligible nonrespondents (EN),
known ineligible cases (IE), and cases whose eligibility is unknown (UE).</p>
</td></tr>
<tr><td><code id="wt_class_adjust_+3A_status_codes">status_codes</code></td>
<td>
<p>A named vector, with four entries named 'ER', 'EN', 'IE', and 'UE'. <br />
<code>status_codes</code> indicates how the values of the <code>status</code> variable are to be interpreted.</p>
</td></tr>
<tr><td><code id="wt_class_adjust_+3A_wt_class">wt_class</code></td>
<td>
<p>(Optional) A character string giving the name of the variable which divides sample cases into weighting classes. <br />
If <code>wt_class=NULL</code> (the default), adjustment is done using the entire sample.</p>
</td></tr>
<tr><td><code id="wt_class_adjust_+3A_type">type</code></td>
<td>
<p>A character vector including one or more of the following options: <br />
</p>

<ul>
<li> <p><code>'UE'</code>: Adjust for unknown eligibility. <br />
</p>
</li>
<li> <p><code>'NR'</code>: Adjust for nonresponse.
<br />
To sequentially adjust for unknown eligibility and then nonresponse, set <code>type=c('UE', 'NR')</code>.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>See the vignette &quot;Nonresponse Adjustments&quot; from the svrep package for a step-by-step walkthrough of
nonresponse weighting adjustments in R: <br />
<code>vignette(topic = "nonresponse-adjustments", package = "svrep")</code>
</p>


<h3>Value</h3>

<p>A replicate survey design object, with adjusted full-sample and replicate weights
</p>


<h3>References</h3>

<p>See Chapter 2 of Heeringa, West, and Berglund (2017) or Chapter 13 of Valliant, Dever, and Kreuter (2018)
for an overview of nonresponse adjustment methods based on redistributing weights.
</p>

<ul>
<li><p> Heeringa, S., West, B., Berglund, P. (2017). Applied Survey Data Analysis, 2nd edition. Boca Raton, FL: CRC Press.
&quot;Applied Survey Data Analysis, 2nd edition.&quot; Boca Raton, FL: CRC Press.
</p>
</li>
<li><p> Valliant, R., Dever, J., Kreuter, F. (2018).
&quot;Practical Tools for Designing and Weighting Survey Samples, 2nd edition.&quot; New York: Springer.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="svrep.html#topic+redistribute_weights">svrep::redistribute_weights()</a></code>, <code>vignette(topic = "nonresponse-adjustments", package = "svrep")</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(survey)
# Load an example dataset
data("involvement_survey_str2s", package = "nrba")

# Create a survey design object

involvement_survey_sample &lt;- svydesign(
  data = involvement_survey_str2s,
  weights = ~BASE_WEIGHT,
  strata = ~SCHOOL_DISTRICT,
  ids = ~ SCHOOL_ID + UNIQUE_ID,
  fpc = ~ N_SCHOOLS_IN_DISTRICT + N_STUDENTS_IN_SCHOOL
)

rep_design &lt;- as.svrepdesign(involvement_survey_sample, type = "mrbbootstrap")

# Adjust weights for nonresponse within weighting classes
nr_adjusted_design &lt;- wt_class_adjust(
  survey_design = rep_design,
  status = "RESPONSE_STATUS",
  status_codes = c(
    "ER" = "Respondent",
    "EN" = "Nonrespondent",
    "IE" = "Ineligible",
    "UE" = "Unknown"
  ),
  wt_class = "PARENT_HAS_EMAIL",
  type = "NR"
)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
