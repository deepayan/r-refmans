<!DOCTYPE html><html lang="en"><head><title>Help for package softImpute</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {softImpute}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#biScale'>
<p>standardize a matrix to have optionally row means zero and variances one,</p>
and/or column means zero and variances one.</a></li>
<li><a href='#complete'>
<p>make predictions from an svd object</p></a></li>
<li><a href='#deBias'><p>Recompute the <code>$d</code> component of a <code>"softImpute"</code> object</p>
through regression.</a></li>
<li><a href='#Incomplete'>
<p>create a matrix of class <code>Incomplete</code></p></a></li>
<li><a href='#Incomplete-class'><p>Class <code>"Incomplete"</code></p></a></li>
<li><a href='#lambda0'>
<p>compute the smallest value for <code>lambda</code> such that</p>
<code>softImpute(x,lambda)</code> returns the zero solution.</a></li>
<li><a href='#softImpute'><p>impute missing values for a matrix via nuclear-norm regularization.</p></a></li>
<li><a href='#softImpute-internal'><p>Internal softImpute functions</p></a></li>
<li><a href='#SparseplusLowRank-class'><p>Class <code>"SparseplusLowRank"</code></p></a></li>
<li><a href='#splr'>
<p>create a <code>SparseplusLowRank</code> object</p></a></li>
<li><a href='#svd.als'><p>compute a low rank soft-thresholded svd by alternating orthogonal</p>
ridge regression</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Matrix Completion via Iterative Soft-Thresholded SVD</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4-1</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-5-08</td>
</tr>
<tr>
<td>Author:</td>
<td>Trevor Hastie &lt;hastie@stanford.edu&gt; and Rahul Mazumder &lt;rahul.mazumder@gmail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Trevor Hastie &lt;hastie@stanford.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Iterative methods for matrix completion that use nuclear-norm regularization. There are two main approaches.The one approach uses iterative soft-thresholded svds to impute the missing values. The second approach uses alternating least squares. Both have an 'EM' flavor, in that at each iteration the matrix is completed with the current estimate. For large matrices there is a special sparse-matrix class named "Incomplete" that efficiently handles all computations. The package includes procedures for centering and scaling rows, columns or both, and for computing low-rank SVDs on large sparse centered matrices (i.e. principal components).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>Matrix, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-05-09 00:14:29 UTC; hastie</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-05-09 05:10:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='biScale'>
standardize a matrix to have optionally row means zero and variances one,
and/or column means zero and variances one.
</h2><span id='topic+biScale'></span>

<h3>Description</h3>

<p>A function for standardizing a matrix in a symmetric
fashion. Generalizes the <code>scale</code> function in R. Works with
matrices with NAs, matrices of class &quot;Incomplete&quot;, and matrix in
&quot;sparseMatrix&quot; format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>biScale(x, maxit = 20, thresh = 1e-09, row.center = TRUE, row.scale =TRUE,
        col.center = TRUE, col.scale = TRUE, trace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="biScale_+3A_x">x</code></td>
<td>

<p>matrix, possibly with NAs, also of class &quot;Incomplete&quot; or &quot;sparseMatrix&quot; format.
</p>
</td></tr>
<tr><td><code id="biScale_+3A_maxit">maxit</code></td>
<td>

<p>When both row and column centering/scaling is requested, iteration is
may be necessary
</p>
</td></tr>
<tr><td><code id="biScale_+3A_thresh">thresh</code></td>
<td>

<p>Convergence threshold
</p>
</td></tr>
<tr><td><code id="biScale_+3A_row.center">row.center</code></td>
<td>

<p>if <code>row.center==TRUE</code> (the default), row centering will be performed resulting in a matrix
with row means zero. If <code>row.center</code> is a vector, it will be used
to center the rows. If <code>row.center=FALSE</code> nothing is done. See details for
more info.
</p>
</td></tr>
<tr><td><code id="biScale_+3A_row.scale">row.scale</code></td>
<td>

<p>if <code>row.scale==TRUE</code>, the rows are scaled (after possibly
centering, to have variance one. Alternatively, if a positive vector is
supplied, it is used for row centering. See details for
more info.
</p>
</td></tr>
<tr><td><code id="biScale_+3A_col.center">col.center</code></td>
<td>

<p>Similar to <code>row.center</code>
</p>
</td></tr>
<tr><td><code id="biScale_+3A_col.scale">col.scale</code></td>
<td>

<p>Similar to <code>row.scale</code>
</p>
</td></tr>
<tr><td><code id="biScale_+3A_trace">trace</code></td>
<td>

<p>with <code>trace=TRUE</code>, convergence progress is reported, when iteration
is needed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes a transformation
</p>
<p style="text-align: center;"><code class="reqn">\frac{X_{ij}-\alpha_i-\beta_j}{\gamma_i\tau_j}</code>
</p>

<p>to transform the matrix <code class="reqn">X</code>. It uses an iterative algorithm based
on &quot;method-of-moments&quot;. At each step, all but one of the parameter
vectors is fixed, and the remaining vector is computed to solve the
required condition. Although in genereal this is not guaranteed to
converge,
it mostly does, and quite rapidly. When there are convergence
problems, remove some of the required constraints. When any of the
row/column centers or scales are provided, they are used rather than
estimated in the above model.
</p>


<h3>Value</h3>

<p>A matrix like <code>x</code> is returned, with attributes:
</p>
<table role = "presentation">
<tr><td><code>biScale:row</code></td>
<td>
<p>a list with elements <code>"center"</code> and
<code>"scale"</code> (the <code class="reqn">alpha</code> and <code class="reqn">gamma</code> above. If no
centering was done, the center component will be a vector of
zeros. Likewise, of no row scaling was done, the scale component
will be a vector of ones.</p>
</td></tr>
<tr><td><code>biScale:column</code></td>
<td>
<p>Same details as <code>biScale:row</code></p>
</td></tr>
</table>
<p>For matrices with missing values, the constraints apply to the
non-missing entries. If <code>x</code> is of class <code>"sparseMatrix"</code>,
then the sparsity is maintained, and an object of class
<code>"SparseplusLowRank"</code> is returned, such that the low-rank part
does the centering.
</p>


<h3>Note</h3>

<p>This function will be described in detail in a forthcoming paper</p>


<h3>Author(s)</h3>

<p>Trevor Hastie, with help from Andreas Buja and Steven Boyd<br />,
Maintainer: Trevor Hastie  <a href="mailto:hastie@stanford.edu">hastie@stanford.edu</a>
</p>


<h3>See Also</h3>

<p><code>softImpute</code>,<code>Incomplete</code>,<code>lambda0</code>,<code>impute</code>,<code>complete</code>,
and class <code>"SparseplusLowRank"</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(101)
n=200
p=100
J=50
np=n*p
missfrac=0.3
x=matrix(rnorm(n*J),n,J)%*%matrix(rnorm(J*p),J,p)+matrix(rnorm(np),n,p)/5
xc=biScale(x)
ix=seq(np)
imiss=sample(ix,np*missfrac,replace=FALSE)
xna=x
xna[imiss]=NA
xnab=biScale(xna,row.scale=FALSE,trace=TRUE)
xnaC=as(xna,"Incomplete")
xnaCb=biScale(xnaC)
nnz=trunc(np*.3)
inz=sample(seq(np),nnz,replace=FALSE)
i=row(x)[inz]
j=col(x)[inz]
x=rnorm(nnz)
xS=sparseMatrix(x=x,i=i,j=j)
xSb=biScale(xS)
class(xSb)
</code></pre>

<hr>
<h2 id='complete'>
make predictions from an svd object
</h2><span id='topic+complete'></span><span id='topic+impute'></span><span id='topic+complete+2CIncomplete-method'></span><span id='topic+complete+2Cmatrix-method'></span>

<h3>Description</h3>

<p>These functions produce predictions from the low-rank solution of <code>softImpute</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>complete(x, object, unscale = TRUE)
impute(object, i, j, unscale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="complete_+3A_x">x</code></td>
<td>

<p>a matrix with NAs or a matrix of class <code>"Incomplete"</code>.
</p>
</td></tr>
<tr><td><code id="complete_+3A_object">object</code></td>
<td>

<p>an svd object with components u, d and v
</p>
</td></tr>
<tr><td><code id="complete_+3A_i">i</code></td>
<td>
<p>vector of row indices for the locations to be predicted</p>
</td></tr>
<tr><td><code id="complete_+3A_j">j</code></td>
<td>
<p>vector of column indices for the locations to be predicted</p>
</td></tr>
<tr><td><code id="complete_+3A_unscale">unscale</code></td>
<td>

<p>if <code>object</code> has <code>biScale</code> attributes, and <code>unscale=TRUE</code>,
the imputations reversed the centering and scaling on the predictions.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>impute</code> returns a vector of predictions, using the reconstructed
low-rank matrix representation represented by <code>object</code>. It is used by complete,
which returns a complete matrix with all the missing values imputed.
</p>


<h3>Value</h3>

<p>Either a vector of predictions or a complete matrix. WARNING: if
<code>x</code> has large dimensions, the matrix returned by <code>complete</code>
might be too large.</p>


<h3>Author(s)</h3>

<p>Trevor Hastie
</p>


<h3>See Also</h3>

<p><code>softImpute</code>, <code>biScale</code> and <code>Incomplete</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(101)
n=200
p=100
J=50
np=n*p
missfrac=0.3
x=matrix(rnorm(n*J),n,J)%*%matrix(rnorm(J*p),J,p)+matrix(rnorm(np),n,p)/5
ix=seq(np)
imiss=sample(ix,np*missfrac,replace=FALSE)
xna=x
xna[imiss]=NA
fit1=softImpute(xna,rank=50,lambda=30)
complete(xna,fit1)
</code></pre>

<hr>
<h2 id='deBias'>Recompute the <code>$d</code> component of a <code>"softImpute"</code> object
through regression.
</h2><span id='topic+deBias'></span>

<h3>Description</h3>

<p><code>softImpute</code> uses shrinkage when completing a matrix with
missing values. This function debiases the singular values using
ordinary least squares.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deBias(x, svdObject)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="deBias_+3A_x">x</code></td>
<td>
<p>matrix with missing entries, or a matrix of class <code>"Incomplete"</code>
</p>
</td></tr>
<tr><td><code id="deBias_+3A_svdobject">svdObject</code></td>
<td>
<p>an SVD object, the output of <code>softImpute</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Treating the <code>"d"</code> values as parameters, this function recomputes
them by linear regression.
</p>


<h3>Value</h3>

<p>An svd object is returned, with components &quot;u&quot;, &quot;d&quot;, and &quot;v&quot;.
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie<br />
Maintainer: Trevor Hastie  <a href="mailto:hastie@stanford.edu">hastie@stanford.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(101)
n=200
p=100
J=50
np=n*p
missfrac=0.3
x=matrix(rnorm(n*J),n,J)%*%matrix(rnorm(J*p),J,p)+matrix(rnorm(np),n,p)/5
ix=seq(np)
imiss=sample(ix,np*missfrac,replace=FALSE)
xna=x
xna[imiss]=NA
fit1=softImpute(xna,rank=50,lambda=30)
fit1d=deBias(xna,fit1)
</code></pre>

<hr>
<h2 id='Incomplete'>
create a matrix of class <code>Incomplete</code>
</h2><span id='topic+Incomplete'></span><span id='topic+coerce+2Cmatrix-method'></span>

<h3>Description</h3>

<p>creates an object of class <code>Incomplete</code>, which inherits from
class <code>dgCMatrix</code>, a specific instance of class <code>sparseMatrix</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Incomplete(i, j, x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Incomplete_+3A_i">i</code></td>
<td>
<p>row indices
</p>
</td></tr>
<tr><td><code id="Incomplete_+3A_j">j</code></td>
<td>
<p>column indices
</p>
</td></tr>
<tr><td><code id="Incomplete_+3A_x">x</code></td>
<td>

<p>a vector of values
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The matrix is represented in sparse-matrix format, except the &quot;zeros&quot;
represent missing values. Real zeros are represented explicitly as values.
</p>


<h3>Value</h3>

<p>a matrix of class <code>Incomplete</code> which inherits from
class  <code>dgCMatrix</code> </p>


<h3>Author(s)</h3>

<p>Trevor Hastie and Rahul Mazumder
</p>


<h3>See Also</h3>

<p><code>softImpute</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(101)
n=200
p=100
J=50
np=n*p
missfrac=0.3
x=matrix(rnorm(n*J),n,J)%*%matrix(rnorm(J*p),J,p)+matrix(rnorm(np),n,p)/5
ix=seq(np)
imiss=sample(ix,np*missfrac,replace=FALSE)
xna=x
xna[imiss]=NA
xnaC=as(xna,"Incomplete")
### here we do it a different way to demonstrate Incomplete
### In practise the observed values are stored in this market-matrix format.
i = row(xna)[-imiss]
j = col(xna)[-imiss]
xnaC=Incomplete(i,j,x=x[-imiss])
  </code></pre>

<hr>
<h2 id='Incomplete-class'>Class <code>"Incomplete"</code></h2><span id='topic+Incomplete-class'></span><span id='topic+as.matrix+2CIncomplete-method'></span><span id='topic+coerce+2Cmatrix+2CIncomplete-method'></span><span id='topic+coerce+2CsparseMatrix+2CIncomplete-method'></span>

<h3>Description</h3>

<p>a sparse matrix inheriting from class <code>dgCMatrix</code> with the NAs
represented as zeros
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("Incomplete", ...)</code>.
or by calling the function <code>Incomplete</code>
</p>


<h3>Slots</h3>


<dl>
<dt><code>i</code>:</dt><dd><p>Object of class <code>"integer"</code> ~~ </p>
</dd>
<dt><code>p</code>:</dt><dd><p>Object of class <code>"integer"</code> ~~ </p>
</dd>
<dt><code>Dim</code>:</dt><dd><p>Object of class <code>"integer"</code> ~~ </p>
</dd>
<dt><code>Dimnames</code>:</dt><dd><p>Object of class <code>"list"</code> ~~ </p>
</dd>
<dt><code>x</code>:</dt><dd><p>Object of class <code>"numeric"</code> ~~ </p>
</dd>
<dt><code>factors</code>:</dt><dd><p>Object of class <code>"list"</code> ~~ </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="Matrix.html#topic+dgCMatrix-class">dgCMatrix</a>"</code>, directly.
Class <code>"<a href="Matrix.html#topic+CsparseMatrix-class">CsparseMatrix</a>"</code>, by class &quot;dgCMatrix&quot;, distance 2.
Class <code>"<a href="Matrix.html#topic+dsparseMatrix-class">dsparseMatrix</a>"</code>, by class &quot;dgCMatrix&quot;, distance 2.
Class <code>"<a href="Matrix.html#topic+generalMatrix-class">generalMatrix</a>"</code>, by class &quot;dgCMatrix&quot;, distance 2.
Class <code>"<a href="Matrix.html#topic+dMatrix-class">dMatrix</a>"</code>, by class &quot;dgCMatrix&quot;, distance 3.
Class <code>"<a href="Matrix.html#topic+sparseMatrix-class">sparseMatrix</a>"</code>, by class &quot;dgCMatrix&quot;, distance 3.
Class <code>"<a href="Matrix.html#topic+compMatrix-class">compMatrix</a>"</code>, by class &quot;dgCMatrix&quot;, distance 3.
Class <code>"<a href="Matrix.html#topic+Matrix-class">Matrix</a>"</code>, by class &quot;dgCMatrix&quot;, distance 4.
</p>


<h3>Methods</h3>


<dl>
<dt>as.matrix</dt><dd><p><code>signature(x = "Incomplete")</code>: ... </p>
</dd>
<dt>coerce</dt><dd><p><code>signature(from = "matrix", to = "Incomplete")</code>: ... </p>
</dd>
<dt>complete</dt><dd><p><code>signature(x = "Incomplete")</code>: ... </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Trevor Hastie and Rahul Mazumder
</p>


<h3>See Also</h3>

<p><code>biScale</code>,<code>softImpute</code>,<code>Incomplete</code>,<code>impute</code>,<code>complete</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("Incomplete")
set.seed(101)
n=200
p=100
J=50
np=n*p
missfrac=0.3
x=matrix(rnorm(n*J),n,J)%*%matrix(rnorm(J*p),J,p)+matrix(rnorm(np),n,p)/5
ix=seq(np)
imiss=sample(ix,np*missfrac,replace=FALSE)
xna=x
xna[imiss]=NA
xnaC=as(xna,"Incomplete")
</code></pre>

<hr>
<h2 id='lambda0'>
compute the smallest value for <code>lambda</code> such that
<code>softImpute(x,lambda)</code> returns the zero solution.
</h2><span id='topic+lambda0'></span><span id='topic+lambda0+2CIncomplete-method'></span><span id='topic+lambda0+2CSparseplusLowRank-method'></span><span id='topic+lambda0+2CsparseMatrix-method'></span>

<h3>Description</h3>

<p>this determines the &quot;starting&quot; lambda for a sequence of values for
<code>softImpute</code>, and all nonzero solutions would require a smaller
value for <code>lambda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lambda0(x, lambda = 0, maxit = 100, trace.it = FALSE, thresh = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lambda0_+3A_x">x</code></td>
<td>

<p>An m by n matrix. Large matrices can be in &quot;sparseMatrix&quot; format, as
well as &quot;SparseplusLowRank&quot;. The latter arise after centering sparse
matrices, for example with <code>biScale</code>, as well as in applications
such as <code>softImpute</code>.
</p>
</td></tr>
</table>
<p>The remaining arguments only apply to matrices <code>x</code> in
<code>"sparseMatrix"</code>, <code>"Incomplete"</code>, or <code>"SparseplusLowRank"</code> format. 
</p>
<table role = "presentation">
<tr><td><code id="lambda0_+3A_lambda">lambda</code></td>
<td>

<p>As in <code>svd.als</code>, using a value for <code>lambda</code> can speed up
iterations. As long as the solution is not zero, the value returned
adds back this value.
</p>
</td></tr>
<tr><td><code id="lambda0_+3A_maxit">maxit</code></td>
<td>

<p>maximum number of iterations.
</p>
</td></tr>
<tr><td><code id="lambda0_+3A_trace.it">trace.it</code></td>
<td>

<p>with <code>trace.it=TRUE</code>, convergence progress is reported.
</p>
</td></tr>
<tr><td><code id="lambda0_+3A_thresh">thresh</code></td>
<td>

<p>convergence threshold, measured as the relative changed in the Frobenius
norm between two successive estimates.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is the largest singular value for the matrix,
with zeros replacing missing values. It uses <code>svd.als</code> with
<code>rank=2</code>.
</p>


<h3>Value</h3>

<p>a single number, the largest singular value
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie, Rahul Mazumder<br />
Maintainer: Trevor Hastie  <a href="mailto:hastie@stanford.edu">hastie@stanford.edu</a>
</p>


<h3>References</h3>

<p>Rahul Mazumder, Trevor Hastie and Rob Tibshirani (2010)
<em>Spectral Regularization Algorithms for Learning Large Incomplete
Matrices</em>,
<a href="https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf">https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf</a><br />
<em> Journal of Machine Learning Research 11 (2010) 2287-2322</em>
</p>


<h3>See Also</h3>

<p><code>softImpute</code>,<code>Incomplete</code>, and <code>svd.als</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(101)
n=200
p=100
J=50
np=n*p
missfrac=0.3
x=matrix(rnorm(n*J),n,J)%*%matrix(rnorm(J*p),J,p)+matrix(rnorm(np),n,p)/5
ix=seq(np)
imiss=sample(ix,np*missfrac,replace=FALSE)
xna=x
xna[imiss]=NA
lambda0(xna)
</code></pre>

<hr>
<h2 id='softImpute'>impute missing values for a matrix via nuclear-norm regularization.
</h2><span id='topic+softImpute'></span>

<h3>Description</h3>

<p>fit a low-rank matrix approximation to a matrix with
missing values via nuclear-norm regularization. The algorithm works
like EM, filling in the missing values with the current guess, and
then solving the optimization problem on the complete matrix using a
soft-thresholded SVD. Special sparse-matrix classes available for very
large matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softImpute(x, rank.max = 2, lambda = 0, type = c("als", "svd"), thresh = 1e-05,
           maxit = 100, trace.it = FALSE, warm.start = NULL, final.svd = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="softImpute_+3A_x">x</code></td>
<td>
<p>An m by n matrix with NAs. For large matrices can be of class
<code>"Incomplete"</code>, in which case the missing values are
represented as pseudo zeros leading to dramatic storage
reduction. <code>x</code> can have been centered and scaled via
<code>biScale</code>, and this information is carried along with the solution.
</p>
</td></tr>
<tr><td><code id="softImpute_+3A_rank.max">rank.max</code></td>
<td>

<p>This restricts the rank of the solution. If sufficiently large, and with
<code>type="svd"</code>, the solution solves the nuclear-norm convex
matrix-completion problem. In this case the number of nonzero singular
values returned will be less than or equal to <code>rank.max</code>. If smaller
ranks are used, the solution is not guaranteed to solve the problem,
although still results in good local minima. <code>rank.max</code> should be
no bigger than <code>min(dim(x)-1</code>.
</p>
</td></tr>
<tr><td><code id="softImpute_+3A_lambda">lambda</code></td>
<td>

<p>nuclear-norm regularization parameter. If <code>lambda=0</code>, the algorithm
reverts to &quot;hardImpute&quot;, for which convergence is typically slower, and
to local minimum. Ideally <code>lambda</code> should be chosen so that the solution
reached has rank slightly less than <code>rank.max</code>. See also
<code>lambda0()</code> for computing the smallest <code>lambda</code> with a zero solution.
</p>
</td></tr>
<tr><td><code id="softImpute_+3A_type">type</code></td>
<td>
<p>two algorithms are implements, <code>type="svd"</code> or
the default  <code>type="als"</code>. The &quot;svd&quot; algorithm repeatedly computes
the svd of the completed matrix, and soft thresholds its singular
values. Each new soft-thresholded svd is used to re-impute the missing
entries. For large matrices of class <code>"Incomplete"</code>, the svd is
achieved by an efficient form of alternating orthogonal ridge
regression. The &quot;als&quot; algorithm uses this same alternating ridge
regression, but updates the imputation at each step, leading to quite
substantial speedups in some cases. The &quot;als&quot; approach does not
currently have the same theoretical  convergence guarantees as the
&quot;svd&quot; approach.
</p>
</td></tr>
<tr><td><code id="softImpute_+3A_thresh">thresh</code></td>
<td>

<p>convergence threshold, measured as the relative change in the Frobenius
norm between two successive estimates.
</p>
</td></tr>
<tr><td><code id="softImpute_+3A_maxit">maxit</code></td>
<td>

<p>maximum number of iterations.
</p>
</td></tr>
<tr><td><code id="softImpute_+3A_trace.it">trace.it</code></td>
<td>

<p>with <code>trace.it=TRUE</code>, convergence progress is reported.
</p>
</td></tr>
<tr><td><code id="softImpute_+3A_warm.start">warm.start</code></td>
<td>

<p>an svd object can be supplied as a warm start. This is particularly
useful when constructing a path of solutions with decreasing values of
<code>lambda</code> and increasing <code>rank.max</code>. The previous solution can
be provided directly as a warm start for the next.
</p>
</td></tr>
<tr><td><code id="softImpute_+3A_final.svd">final.svd</code></td>
<td>

<p>only applicable to <code>type="als"</code>. The alternating ridge-regressions
do not lead to exact zeros. With the default <code>final.svd=TRUE</code>, at
the final iteration, a one step unregularized iteration is performed,
followed by soft-thresholding of the singular values, leading to hard zeros.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SoftImpute solves the following problem for a matrix <code class="reqn">X</code> with
missing entries:
</p>
<p style="text-align: center;"><code class="reqn">\min||X-M||_o^2 +\lambda||M||_*.</code>
</p>

<p>Here <code class="reqn">||\cdot||_o</code> is the Frobenius norm, restricted to the entries
corresponding to the
non-missing entries of <code class="reqn">X</code>, and  <code class="reqn">||M||_*</code> is the nuclear norm
of <code class="reqn">M</code> (sum of singular values).  
For full details of the &quot;svd&quot; algorithm are described in the reference
below.  The &quot;als&quot; algorithm will be described in a forthcoming
article. Both methods employ special sparse-matrix tricks for large
matrices with many missing values. This package creates a new
sparse-matrix class <code>"SparseplusLowRank"</code> for matrices of the form
</p>
<p style="text-align: center;"><code class="reqn">x+ab',</code>
</p>
<p> where <code class="reqn">x</code> is sparse and <code class="reqn">a</code> and <code class="reqn">b</code> are tall
skinny matrices, hence <code class="reqn">ab'</code> is low rank. Methods for efficient left
and right matrix multiplication are provided for this class. For large
matrices, the function <code>Incomplete()</code> can be used to build the
appropriate
sparse input matrix from market-format data.
</p>


<h3>Value</h3>

<p> An svd object is returned, with components &quot;u&quot;, &quot;d&quot;, and &quot;v&quot;.
If the solution has zeros in &quot;d&quot;, the solution is truncated to rank one
more than the number of zeros (so the zero is visible). If the input
matrix had been centered and scaled by <code>biScale</code>, the scaling
details are assigned as attributes inherited from the input matrix.
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie, Rahul Mazumder<br />
Maintainer: Trevor Hastie  <a href="mailto:hastie@stanford.edu">hastie@stanford.edu</a>
</p>


<h3>References</h3>

<p>Rahul Mazumder, Trevor Hastie and Rob Tibshirani (2010)
<em>Spectral Regularization Algorithms for Learning Large Incomplete
Matrices</em>,
<a href="https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf">https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf</a><br />
<em> Journal of Machine Learning Research 11 (2010) 2287-2322</em>
</p>


<h3>See Also</h3>

<p><code>biScale</code>, <code>svd.als</code>,<code>Incomplete</code>,
<code>lambda0</code>, <code>impute</code>, <code>complete</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(101)
n=200
p=100
J=50
np=n*p
missfrac=0.3
x=matrix(rnorm(n*J),n,J)%*%matrix(rnorm(J*p),J,p)+matrix(rnorm(np),n,p)/5
ix=seq(np)
imiss=sample(ix,np*missfrac,replace=FALSE)
xna=x
xna[imiss]=NA
###uses regular matrix method for matrices with NAs
fit1=softImpute(xna,rank=50,lambda=30)
###uses sparse matrix method for matrices of class "Incomplete"
xnaC=as(xna,"Incomplete")
fit2=softImpute(xnaC,rank=50,lambda=30)
###uses "svd" algorithm
fit3=softImpute(xnaC,rank=50,lambda=30,type="svd")
ximp=complete(xna,fit1)
### first scale xna
xnas=biScale(xna)
fit4=softImpute(xnas,rank=50,lambda=10)
ximp=complete(xna,fit4)
impute(fit4,i=c(1,3,7),j=c(2,5,10))
impute(fit4,i=c(1,3,7),j=c(2,5,10),unscale=FALSE)#ignore scaling and centering
  </code></pre>

<hr>
<h2 id='softImpute-internal'>Internal softImpute functions</h2><span id='topic+clean.warm.start'></span><span id='topic+simpute.als'></span><span id='topic+simpute.svd'></span><span id='topic+Ssimpute.als'></span><span id='topic+Ssimpute.svd'></span><span id='topic+Ssvd.als'></span><span id='topic+softImpute.x.matrix'></span><span id='topic+softImpute.x.Incomplete'></span>

<h3>Description</h3>

<p>Internal softImpute functions</p>


<h3>Details</h3>

<p>These functions are not intended to be called directly, but they can
be useful for understanding the structure of the models used.
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie</p>

<hr>
<h2 id='SparseplusLowRank-class'>Class <code>"SparseplusLowRank"</code></h2><span id='topic+SparseplusLowRank-class'></span><span id='topic++25+2A+25+2CANY+2CSparseplusLowRank-method'></span><span id='topic++25+2A+25+2CSparseplusLowRank+2CANY-method'></span><span id='topic++25+2A+25+2CMatrix+2CSparseplusLowRank-method'></span><span id='topic++25+2A+25+2CSparseplusLowRank+2CMatrix-method'></span><span id='topic+as.matrix+2CSparseplusLowRank-method'></span><span id='topic+colMeans+2CSparseplusLowRank-method'></span><span id='topic+colSums+2CSparseplusLowRank-method'></span><span id='topic+dim+2CSparseplusLowRank-method'></span><span id='topic+norm+2CSparseplusLowRank+2Ccharacter-method'></span><span id='topic+rowMeans+2CSparseplusLowRank-method'></span><span id='topic+rowSums+2CSparseplusLowRank-method'></span>

<h3>Description</h3>

<p>A structured matrix made up of a sparse part plus a low-rank part, all
which can be stored and operated on efficiently.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form
<code>new("SparseplusLowRank", ...)</code>
or by a call to <code>splr</code>
</p>


<h3>Slots</h3>


<dl>
<dt><code>x</code>:</dt><dd><p>Object of class <code>"sparseMatrix"</code></p>
</dd>
<dt><code>a</code>:</dt><dd><p>Object of class <code>"matrix"</code></p>
</dd>
<dt><code>b</code>:</dt><dd><p>Object of class <code>"matrix"</code></p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>%*%</dt><dd><p><code>signature(x = "ANY", y = "SparseplusLowRank")</code>: ... </p>
</dd>
<dt>%*%</dt><dd><p><code>signature(x = "SparseplusLowRank", y = "ANY")</code>: ... </p>
</dd>
<dt>%*%</dt><dd><p><code>signature(x = "Matrix", y = "SparseplusLowRank")</code>: ... </p>
</dd>
<dt>%*%</dt><dd><p><code>signature(x = "SparseplusLowRank", y = "Matrix")</code>: ... </p>
</dd>
<dt>as.matrix</dt><dd><p><code>signature(x = "SparseplusLowRank")</code>: ... </p>
</dd>
<dt>colMeans</dt><dd><p><code>signature(x = "SparseplusLowRank")</code>: ... </p>
</dd>
<dt>colSums</dt><dd><p><code>signature(x = "SparseplusLowRank")</code>: ... </p>
</dd>
<dt>dim</dt><dd><p><code>signature(x = "SparseplusLowRank")</code>: ... </p>
</dd>
<dt>norm</dt><dd><p><code>signature(x = "SparseplusLowRank", type = "character")</code>: ... </p>
</dd>
<dt>rowMeans</dt><dd><p><code>signature(x = "SparseplusLowRank")</code>: ... </p>
</dd>
<dt>rowSums</dt><dd><p><code>signature(x = "SparseplusLowRank")</code>: ... </p>
</dd>
<dt>svd.als</dt><dd><p><code>signature(x = "SparseplusLowRank")</code>: ... </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Trevor Hastie and Rahul Mazumder
</p>


<h3>See Also</h3>

<p><code>softImpute</code>,<code>splr</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("SparseplusLowRank")
x=matrix(sample(c(3,0),15,replace=TRUE),5,3)
x=as(x,"sparseMatrix")
a=matrix(rnorm(10),5,2)
b=matrix(rnorm(6),3,2)
new("SparseplusLowRank",x=x,a=a,b=b)
splr(x,a,b)
</code></pre>

<hr>
<h2 id='splr'>
create a <code>SparseplusLowRank</code> object
</h2><span id='topic+splr'></span>

<h3>Description</h3>

<p>create an object of class  <code>SparseplusLowRank</code> which can be
efficiently stored and for which efficient linear algebra operations are possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splr(x, a = NULL, b = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="splr_+3A_x">x</code></td>
<td>

<p>sparse matrix with dimension say m x n
</p>
</td></tr>
<tr><td><code id="splr_+3A_a">a</code></td>
<td>

<p>matrix with m rows and number of columns  r less than <code>min(dim(x))</code>
</p>
</td></tr>
<tr><td><code id="splr_+3A_b">b</code></td>
<td>

<p>matrix with n rows and number of columns  r less than <code>min(dim(x))</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of S4 class <code>SparseplusLowRank</code> is returned with slots
<code>x</code>, <code>a</code> and <code>b</code>
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie
</p>


<h3>See Also</h3>

<p><code>SparseplusLowRank-class</code>, <code>softImpute</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x=matrix(sample(c(3,0),15,replace=TRUE),5,3)
x=as(x,"sparseMatrix")
a=matrix(rnorm(10),5,2)
b=matrix(rnorm(6),3,2)
new("SparseplusLowRank",x=x,a=a,b=b)
splr(x,a,b)
</code></pre>

<hr>
<h2 id='svd.als'>compute a low rank soft-thresholded svd by alternating orthogonal
ridge regression
</h2><span id='topic+svd.als'></span><span id='topic+svd.als+2CsparseMatrix-method'></span><span id='topic+svd.als+2CSparseplusLowRank-method'></span>

<h3>Description</h3>

<p>fit a low-rank svd to a complete matrix by alternating
orthogonal ridge regression. Special sparse-matrix classes available for very
large matrices, including &quot;SparseplusLowRank&quot; versions for row and
column centered sparse matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svd.als(x, rank.max = 2, lambda = 0, thresh = 1e-05, maxit = 100,
        trace.it = FALSE, warm.start = NULL, final.svd = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="svd.als_+3A_x">x</code></td>
<td>

<p>An m by n matrix. Large matrices can be in &quot;sparseMatrix&quot; format, as
well as &quot;SparseplusLowRank&quot;. The latter arise after centering sparse
matrices, for example with <code>biScale</code>, as well as in applications
such as <code>softImpute</code>.
</p>
</td></tr>
<tr><td><code id="svd.als_+3A_rank.max">rank.max</code></td>
<td>

<p>The maximum rank for the solution. This is also the dimension of the
left and right matrices of orthogonal singular vectors. 'rank.max' should be no
bigger than 'min(dim(x)'.
</p>
</td></tr>
<tr><td><code id="svd.als_+3A_lambda">lambda</code></td>
<td>

<p>The regularization parameter. <code>lambda=0</code> corresponds to an
accelerated version of the orthogonal QR-algorithm. With <code>lambda&gt;0</code>
the algorithm amounts to alternating orthogonal ridge regression.
</p>
</td></tr>
<tr><td><code id="svd.als_+3A_thresh">thresh</code></td>
<td>

<p>convergence threshold, measured as the relative changed in the Frobenius
norm between two successive estimates.
</p>
</td></tr>
<tr><td><code id="svd.als_+3A_maxit">maxit</code></td>
<td>

<p>maximum number of iterations.
</p>
</td></tr>
<tr><td><code id="svd.als_+3A_trace.it">trace.it</code></td>
<td>

<p>with <code>trace.it=TRUE</code>, convergence progress is reported.
</p>
</td></tr>
<tr><td><code id="svd.als_+3A_warm.start">warm.start</code></td>
<td>

<p>an svd object can be supplied as a warm start. If the solution requested
has higher rank than the warm start, the additional subspace is
initialized with random Gaussians (and then orthogonalized wrt the rest).
</p>
</td></tr>
<tr><td><code id="svd.als_+3A_final.svd">final.svd</code></td>
<td>

<p>Although in theory, this algorithm converges to the solution to a
nuclear-norm regularized low-rank matrix approximation problem,
with potentially some singular values equal to zero, in practice only
near-zeros are achieved. This final step does one more iteration with
<code>lambda=0</code>,
followed by soft-thresholding.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This algorithm solves the problem
</p>
<p style="text-align: center;"><code class="reqn">\min ||X-M||_F^2 +\lambda ||M||_*</code>
</p>
<p> subject to <code class="reqn">rank(M)\leq
 r</code>, where <code class="reqn">||M||_*</code> is the nuclear norm of <code class="reqn">M</code> (sum of singular values).
It achieves this by solving the related problem
</p>
<p style="text-align: center;"><code class="reqn">\min ||X-AB'||_F^2 +\lambda/2 (||A||_F^2+||B||_F^2)</code>
</p>
<p> subject to
<code class="reqn">rank(A)=rank(B)\leq r</code>. The solution is a rank-restricted,
soft-thresholded SVD of <code class="reqn">X</code>.
</p>


<h3>Value</h3>

<p>An svd object is returned, with components &quot;u&quot;, &quot;d&quot;, and &quot;v&quot;.
</p>
<table role = "presentation">
<tr><td><code>u</code></td>
<td>
<p>an m by <code>rank.max</code> matrix with the left orthogonal singular
vectors</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a vector of length <code>rank.max</code> of soft-thresholded singular values</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>an n by <code>rank.max</code> matrix with the right orthogonal singular
vectors</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Trevor Hastie, Rahul Mazumder<br />
Maintainer: Trevor Hastie  <a href="mailto:hastie@stanford.edu">hastie@stanford.edu</a>
</p>


<h3>References</h3>

<p>Rahul Mazumder, Trevor Hastie and Rob Tibshirani (2010)
<em>Spectral Regularization Algorithms for Learning Large Incomplete
Matrices</em>,
<a href="https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf">https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf</a><br />
<em> Journal of Machine Learning Research 11 (2010) 2287-2322</em>
</p>


<h3>See Also</h3>

<p><code>biScale</code>, <code>softImpute</code>, <code>Incomplete</code>,
<code>lambda0</code>, <code>impute</code>, <code>complete</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#create a matrix and run the algorithm
set.seed(101)
n=100
p=50
J=25
np=n*p
x=matrix(rnorm(n*J),n,J)%*%matrix(rnorm(J*p),J,p)+matrix(rnorm(np),n,p)/5
fit=svd.als(x,rank=25,lambda=50)
fit$d
pmax(svd(x)$d-50,0)
# now create a sparse matrix and do the same
nnz=trunc(np*.3)
inz=sample(seq(np),nnz,replace=FALSE)
i=row(x)[inz]
j=col(x)[inz]
x=rnorm(nnz)
xS=sparseMatrix(x=x,i=i,j=j)
fit2=svd.als(xS,rank=20,lambda=7)
fit2$d
pmax(svd(as.matrix(xS))$d-7,0)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
