<!DOCTYPE html><html lang="en"><head><title>Help for package wskm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {wskm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ewkm'>
<p>Entropy Weighted K-Means</p>
</p></a></li>
<li><a href='#fgkm'>
<p>Feature Group Weighting K-Means for Subspace clustering</p></a></li>
<li><a href='#fgkm.sample'><p>Sample dataset to illustrate the fgkm algorithm.</p></a></li>
<li><a href='#plot.ewkm'>
<p>Plot Entropy Weighted K-Means Weights</p>
</p></a></li>
<li><a href='#predict.ewkm'><p>Predict method for <code>ewkm</code> model.</p></a></li>
<li><a href='#twkm'>
<p>Two-level variable weighting clustering</p></a></li>
<li><a href='#twkm.sample'><p>Sample dataset to test the twkm algorithm.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.4.40</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-04-04</td>
</tr>
<tr>
<td>Title:</td>
<td>Weighted k-Means Clustering</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>He Zhao &lt;Simon.Yansen.Zhao@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10), grDevices, stats, lattice, latticeExtra, fpc</td>
</tr>
<tr>
<td>Description:</td>
<td>Entropy weighted k-means (ewkm) by Liping Jing, Michael
        K. Ng and Joshua Zhexue Huang (2007)
        &lt;<a href="https://doi.org/10.1109%2FTKDE.2007.1048">doi:10.1109/TKDE.2007.1048</a>&gt; is a weighted subspace clustering
        algorithm that is well suited to very high dimensional data.
        Weights are calculated as the importance of a variable with
        regard to cluster membership.  The two-level variable
        weighting clustering algorithm tw-k-means (twkm) by Xiaojun
        Chen, Xiaofei Xu, Joshua Zhexue Huang and Yunming Ye (2013)
        &lt;<a href="https://doi.org/10.1109%2FTKDE.2011.262">doi:10.1109/TKDE.2011.262</a>&gt; introduces two types of weights,
        the weights on individual variables and the weights on
        variable groups, and they are calculated during the clustering
        process.  The feature group weighted k-means (fgkm) by Xiaojun
        Chen, Yunminng Ye, Xiaofei Xu and Joshua Zhexue Huang (2012)
        &lt;<a href="https://doi.org/10.1016%2Fj.patcog.2011.06.004">doi:10.1016/j.patcog.2011.06.004</a>&gt; extends this concept by
        grouping features and weighting the group in addition to
        weighting individual features.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Copyright:</td>
<td>2011-2014 Shenzhen Institutes of Advanced Technology Chinese
Academy of Sciences</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/SimonYansenZhao/wskm">https://github.com/SimonYansenZhao/wskm</a>,
<a href="http://english.siat.cas.cn/">http://english.siat.cas.cn/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/SimonYansenZhao/wskm/issues">https://github.com/SimonYansenZhao/wskm/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-04-04 01:19:56 UTC; simon</td>
</tr>
<tr>
<td>Author:</td>
<td>Graham Williams [aut],
  Joshua Z Huang [aut],
  Xiaojun Chen [aut],
  Qiang Wang [aut],
  Longfei Xiao [aut],
  He Zhao [cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-04-05 00:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='ewkm'>
Entropy Weighted K-Means
</h2><span id='topic+ewkm'></span>

<h3>Description</h3>

<p>Perform an entropy weighted subspace k-means.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
 ewkm(x, centers, lambda=1, maxiter=100, delta=0.00001, maxrestart=10)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ewkm_+3A_x">x</code></td>
<td>
<p>numeric matrix of observations and variables.</p>
</td></tr>
<tr><td><code id="ewkm_+3A_centers">centers</code></td>
<td>
<p>target number of clusters or the initial centers for
clustering.</p>
</td></tr>
<tr><td><code id="ewkm_+3A_lambda">lambda</code></td>
<td>
<p>parameter for variable weight distribution.</p>
</td></tr>
<tr><td><code id="ewkm_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="ewkm_+3A_delta">delta</code></td>
<td>
<p>maximum change allowed between iterations for convergence.</p>
</td></tr>
<tr><td><code id="ewkm_+3A_maxrestart">maxrestart</code></td>
<td>
<p>maximum number of restarts. Default is 10 so that we
stand a good chance of getting a full set of clusters. Normally, any
empty clusters that result are removed from the result, and so we may
obtain fewer than k clusters if we don't allow restarts (i.e.,
maxrestart=0). If &lt; 0 then there is no limit on the number of restarts
and we are much more likely to get a full set of k clusters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The entopy weighted k-means clustering algorithm is a subspace
clusterer ideal for high dimensional data. Along with each cluster we
also obtain variable weights that provide a relative measure of the
importance of each variable to that cluster.
</p>
<p>The algorithm is based on the k-means approach to clustering. An
initial set of k means are identified as the starting
centroids. Observartions are clustered to the nearest centroid
according to a distance measure. This defines the initial
clustrering. New centroids are then identified based on these
clusters.
</p>
<p>Weights are then calculated for each variable within each cluster,
based on the current clustering.  The weights are a measure of the
relative importance of each variable with regard to the membership of
the observations to that cluster. These weights are then incorporated
into the distance function, typically reducing the distance for the
more important variables.
</p>
<p>New centroids are then calculated, and using the weighted distance
measure each observation is once again clustered to its nearest
centroid.
</p>
<p>The process continues until convergence (using a measure of dispersion
and stopping when the change becomes less than delta) or until a
specified number of iterations has been reached (maxiter).
</p>
<p>Large lambda (e,g, &gt; 3) lead to a relatively even distribution of
weights across the variables. Small lambda (e.g., &lt; 1) lead to a more
uneven distribution of weights, giving more discrimintation between
features. Recommended values are between 1 and 3.
</p>
<p>Always check the number of iterations, the number of restarts, and the
total number of iterations as they give a good indication of whether
the algorithm converged.
</p>
<p>As with any distance based algorithm, be sure to rescale your numeric
data so that large values do not bias the clustering. A quick
rescaling method to use is <code><a href="base.html#topic+scale">scale</a></code>.
</p>


<h3>Value</h3>

<p>Returns an object of class &quot;kmeans&quot; and &quot;ewkm&quot;, compatible with other
functions that work with kmeans objects, such as the 'print'
method. The object is a list with the following components in addition
to the components of the kmeans object:
</p>
<p>weights: A matrix of weights recording the relative importance of each
variable for each cluster.
</p>
<p>iterations: This reports on the number of iterations before
termination. Check this to see whether the maxiters was reached. If so
then the algroithm may not be converging,and thus the resulting
clustering may not be particularly good.
</p>
<p>restarts: The number of times the clustering restarted because of a
disappearing cluster resulting from one or more k-means having no
observations associated with it. An number here greater than 0
indicates that the algorithm is not converging on a clustering for the
given k. It is recommended that k be reduced.
</p>
<p>total.iterations: The total number of iterations over all restarts.
</p>


<h3>Author(s)</h3>

<p>Qiang Wang, Xiaojun Chen, Graham J Williams, Joshua Z Huang
</p>


<h3>References</h3>

<p>Liping Jing, Michael K. Ng and Joshua Zhexue Huang (2007). An Entropy
Weighting k-Means Algorithm for Subspace Clustering of
High-Dimensional Sparse Data. IEEE Transactions on Knowledge and Data
Engineering, 19(8), 1026&ndash;1041.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.ewkm">plot.ewkm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
myewkm &lt;- ewkm(iris[1:4], 3, lambda=0.5, maxiter=100)

plot(iris[1:4], col=myewkm$cluster)

# For comparative testing

mykm &lt;- kmeans(iris[1:4], 3)

plot(iris[1:4], col=mykm$cluster)

</code></pre>

<hr>
<h2 id='fgkm'>
Feature Group Weighting K-Means for Subspace clustering
</h2><span id='topic+fgkm'></span>

<h3>Description</h3>

<p>Perform an feature group weighting subspace k-means.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fgkm(x, centers, groups, lambda, eta, maxiter=100, delta=0.000001,
       maxrestart=10,seed=-1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fgkm_+3A_x">x</code></td>
<td>
<p>numeric matrix of observations and features.</p>
</td></tr>
<tr><td><code id="fgkm_+3A_centers">centers</code></td>
<td>
<p>target number of clusters or the initial centers for
clustering.</p>
</td></tr>
<tr><td><code id="fgkm_+3A_groups">groups</code></td>
<td>
<p>a string give the group information, formatted as
<code>"0,1,2,4;3,5;6,7,8"</code> or <code>"0-2,4;3,5;6-8"</code>, where
<code>";"</code> defines a group; or a vector of length of features, each
element of the vector indicates the group of the feature. For
example, <code>c(1,1,1,2,1,2,3,3,3)</code> is the same as
<code>"0-2,4;3,5;6-8"</code>, or even<br />
<code>c("a","a","a","b","a","b","c","c","c")</code>.</p>
</td></tr>
<tr><td><code id="fgkm_+3A_lambda">lambda</code></td>
<td>
<p>parameter of feature weight distribution.</p>
</td></tr>
<tr><td><code id="fgkm_+3A_eta">eta</code></td>
<td>
<p>parameter of group weight distribution.</p>
</td></tr>
<tr><td><code id="fgkm_+3A_delta">delta</code></td>
<td>
<p>maximum change allowed between iterations for convergence.</p>
</td></tr>
<tr><td><code id="fgkm_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="fgkm_+3A_maxrestart">maxrestart</code></td>
<td>
<p>maximum number of restarts. Default is 10 so that we
stand a good chance of getting a full set of clusters. Normally, any
empty clusters that result are removed from the result, and so we may
obtain fewer than k clusters if we don't allow restarts(i.e.,
maxrestart=0). If &lt; 0, then there is no limit on the number of
restarts and we are much likely to get a full set of k clusters.</p>
</td></tr>
<tr><td><code id="fgkm_+3A_seed">seed</code></td>
<td>
<p>random seed. If it was set below 0, then a randomly
generated number will be assigned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The feature group weighting k-means clustering algorithm is a
extension to <a href="#topic+ewkm">ewkm</a>, which itself is a soft subspace clustering
method.
</p>
<p>The algorithm weights subspaces in both feature groups and individual
features.
</p>
<p>Always check the number of iterations, the number of restarts, and the
total number of iterations as they give a good indication of whether
the algorithm converged.
</p>
<p>As with any distance based algorithm, be sure to rescale your numeric
data so that large values do not bias the clustering. A quick
rescaling method to use is <code><a href="base.html#topic+scale">scale</a></code>.
</p>


<h3>Value</h3>

<p>Return an object of class &quot;kmeans&quot; and &quot;fgkm&quot;, compatible with other
function that work with kmeans objects, such as the 'print'
method. The object is a list with the following components in addition
to the components of the kmeans object:
</p>
<table role = "presentation">
<tr><td><code>cluster</code></td>
<td>
<p>A vector of integer (from 1:k) indicating the cluster
to which each point is allocated.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centers.</p>
</td></tr>
<tr><td><code>featureWeight</code></td>
<td>
<p>A matrix of weights recording the relative
importance of each feature for each cluster.</p>
</td></tr>
<tr><td><code>groupWeight</code></td>
<td>
<p>A matrix of group weights recording the relative
importance of each feature group for each cluster.</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>This report on the number of iterations before
termination. Check this to see whether the maxiters was reached. If so
then teh algorithm may not be converging, and thus the resulting
clustering may not be particularly good.</p>
</td></tr>
<tr><td><code>restarts</code></td>
<td>
<p>The number of times the clustering restarted because
of a disappearing cluster resulting from one or more k-means having no
observations associated with it. An number here greater than zero
indicates that the algorithm is not converging on a clustering for the
given k. It is recommended that k be reduced.</p>
</td></tr>
<tr><td><code>totalIterations</code></td>
<td>
<p>The total number of iterations over all
restarts.</p>
</td></tr>
<tr><td><code>totolCost</code></td>
<td>
<p>The total cost calculated in the cost function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Longfei Xiao <a href="mailto:lf.xiao@siat.ac.cn">lf.xiao@siat.ac.cn</a>
</p>


<h3>References</h3>

<p>Xiaojun Chen, Yunming Ye, Xiaofei Xu and Joshua Zhexue Huang (2012). A
Feature Group Weighting Method for Subspace Clustering of
High-Dimensional Data. Pattern Recognition, 45(1), 434&ndash;446.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+kmeans">kmeans</a></code>
<code><a href="#topic+ewkm">ewkm</a></code>
<code><a href="#topic+twkm">twkm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# The data fgkm.sample has 600 objects and 50 dimensions.
# Scale the data before clustering
x &lt;- scale(fgkm.sample)

# Group information is formated as below.
# Each group is separated by ';'.
strGroup &lt;- "0-9;10-19;20-49"
groups &lt;- c(rep(0, 10), rep(1, 10), rep(2, 30))

# Use the fgkm algorithm.
myfgkm &lt;- fgkm(x, 3, strGroup, 3, 1, seed=19)
myfgkm2 &lt;- fgkm(x, 3, groups, 3, 1, seed=19)
all.equal(myfgkm, myfgkm2)

# You can print the clustering result now.
myfgkm$cluster
myfgkm$featureWeight
myfgkm$groupWeight
myfgkm$iterations
myfgkm$restarts
myfgkm$totiters
myfgkm$totss

# Use a cluster validation method from package 'fpc'.

# real.cluster is the real class label of the data 'fgkm.sample'.
real.cluster &lt;- rep(1:3, each=200)

# cluster.stats() computes several distance based statistics.
kmstats &lt;- cluster.stats(d=dist(x), as.integer(myfgkm$cluster), real.cluster)

# corrected Rand index
kmstats$corrected.rand

# variation of information (VI) index
kmstats$vi

</code></pre>

<hr>
<h2 id='fgkm.sample'>Sample dataset to illustrate the fgkm algorithm.</h2><span id='topic+fgkm.sample'></span>

<h3>Description</h3>

<p>A sample dataset of 50 variables and 600 observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fgkm.sample</code></pre>


<h3>Format</h3>

<p>The <code>fgkm.sample</code> dataset is a data frame with 600 observations
and 50 variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fgkm">fgkm</a></code>.
</p>

<hr>
<h2 id='plot.ewkm'>
Plot Entropy Weighted K-Means Weights
</h2><span id='topic+plot.ewkm'></span><span id='topic+levelplot.ewkm'></span>

<h3>Description</h3>

<p>Plot a heatmap showing the variable weights from the subspace clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'ewkm'
plot(x, ...)
## S3 method for class 'ewkm'
levelplot(x, ...)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.ewkm_+3A_x">x</code></td>
<td>
<p>an object of class ewkm.</p>
</td></tr>
<tr><td><code id="plot.ewkm_+3A_...">...</code></td>
<td>
<p>arguments passed on through to heatmap.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The entopy weighted k-means clustering algorithm is a subspace
clusterer ideal for high dimensional data. Along with each cluster we
also obtain variable weights that provide a relative measure of the
importance of each variable to that cluster.
</p>
<p>This plot visualises these relative measures of variable importance
for each of the clusters using a heatmap. The top dendrogram
highlights the relationship between the clusters and the right side
dendrogram provides a visual clue to the correlation between the variables.
</p>
<p>The plot.ewkm() function uses heatmap() to display the weights. The
levelplot.ewkm() uses levelplot() with dendrogramGlobs from the
lattice package. Note that plot() will immediately draw the plot while
levelplot() does not draw immediately but returns a result object
which must be plot()ed.
</p>


<h3>Author(s)</h3>

<p>Graham J Williams
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
myewkm &lt;- ewkm(iris[1:4], 3, lambda=0.5, maxiter=100)

plot(myewkm)

</code></pre>

<hr>
<h2 id='predict.ewkm'>Predict method for <code>ewkm</code> model.</h2><span id='topic+predict'></span><span id='topic+predict.ewkm'></span>

<h3>Description</h3>

<p>Return the nearest cluster to each observation based on a
Euclidean distance with each variable weighted differently per cluster.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ewkm'
predict(object, data, ...)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.ewkm_+3A_object">object</code></td>
<td>
<p>object of class <code>ewkm</code>.</p>
</td></tr>
<tr><td><code id="predict.ewkm_+3A_data">data</code></td>
<td>
<p>the data that needs to be predicted.  Variables should
have the same names and order as used in building the
<code><a href="#topic+ewkm">ewkm</a></code> model.</p>
</td></tr>
<tr><td><code id="predict.ewkm_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of cluster numbers of length <code>nrow(data)</code>.</p>


<h3>Author(s)</h3>

<p>Graham Williams (Togaware)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ewkm">ewkm</a></code>
</p>

<hr>
<h2 id='twkm'>
Two-level variable weighting clustering
</h2><span id='topic+twkm'></span>

<h3>Description</h3>

<p>Two-level variable weighting clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  twkm(x, centers, groups, lambda, eta, maxiter=100, delta=0.000001,
       maxrestart=10,seed=-1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="twkm_+3A_x">x</code></td>
<td>
<p>numeric matrix of observations and features.</p>
</td></tr>
<tr><td><code id="twkm_+3A_centers">centers</code></td>
<td>
<p>target number of clusters or the initial centers for
clustering.</p>
</td></tr>
<tr><td><code id="twkm_+3A_groups">groups</code></td>
<td>
<p>a string give the group information, formatted as
<code>"0,1,2,4;3,5;6,7,8"</code> or <code>"0-2,4;3,5;6-8"</code>, where
<code>";"</code> defines a group; or a vector of length of features, each
element of the vector indicates the group of the feature. For
example, <code>c(1,1,1,2,1,2,3,3,3)</code> is the same as
<code>"0-2,4;3,5;6-8"</code>, or even<br />
<code>c("a","a","a","b","a","b","c","c","c")</code>.</p>
</td></tr>
<tr><td><code id="twkm_+3A_lambda">lambda</code></td>
<td>
<p>parameter of feature weight distribution.</p>
</td></tr>
<tr><td><code id="twkm_+3A_eta">eta</code></td>
<td>
<p>parameter of group weight distribution.</p>
</td></tr>
<tr><td><code id="twkm_+3A_delta">delta</code></td>
<td>
<p>maximum change allowed between iterations for convergence.</p>
</td></tr>
<tr><td><code id="twkm_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="twkm_+3A_maxrestart">maxrestart</code></td>
<td>
<p>maximum number of restarts. Default is 10 so that we
stand a good chance of getting a full set of clusters. Normally, any
empty clusters that result are removed from the result, and so we may
obtain fewer than k clusters if we don't allow restarts(i.e.,
maxrestart=0). If &lt; 0, then there is no limit on the number of
restarts and we are much likely to get a full set of k clusters.</p>
</td></tr>
<tr><td><code id="twkm_+3A_seed">seed</code></td>
<td>
<p>random seed. If it was set below 0, then a randomly
generated number will be assigned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The two-level variable weighting clustering algorithm is a
extension to <a href="#topic+ewkm">ewkm</a>, which itself is a soft subspace clustering
method.
</p>
<p>The algorithm weights subspaces in both feature groups and individual
features.
</p>
<p>Always check the number of iterations, the number of restarts, and the
total number of iterations as they give a good indication of whether
the algorithm converged.
</p>
<p>As with any distance based algorithm, be sure to rescale your numeric
data so that large values do not bias the clustering. A quick
rescaling method to use is <code><a href="base.html#topic+scale">scale</a></code>.
</p>


<h3>Value</h3>

<p>Return an object of class &quot;kmeans&quot; and &quot;twkm&quot;, compatible with other
function that work with kmeans objects, such as the 'print'
method. The object is a list with the following components in addition
to the components of the kmeans object:
</p>
<table role = "presentation">
<tr><td><code>cluster</code></td>
<td>
<p>A vector of integer (from 1:k) indicating the cluster
to which each point is allocated.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centers.</p>
</td></tr>
<tr><td><code>featureWeight</code></td>
<td>
<p>A vector of weights recording the relative
importance of each feature.</p>
</td></tr>
<tr><td><code>groupWeight</code></td>
<td>
<p>A vector of group weights recording the relative
importance of each feature group.</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>This report on the number of iterations before
termination. Check this to see whether the maxiters was reached. If so
then teh algorithm may not be converging, and thus the resulting
clustering may not be particularly good.</p>
</td></tr>
<tr><td><code>restarts</code></td>
<td>
<p>The number of times the clustering restarted because
of a disappearing cluster resulting from one or more k-means having no
observations associated with it. An number here greater than zero
indicates that the algorithm is not converging on a clustering for the
given k. It is recommended that k be reduced.</p>
</td></tr>
<tr><td><code>totalIterations</code></td>
<td>
<p>The total number of iterations over all
restarts.</p>
</td></tr>
<tr><td><code>totolCost</code></td>
<td>
<p>The total cost calculated in the cost function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Xiaojun Chen <a href="mailto:xjchen.hitsz@gmail.com">xjchen.hitsz@gmail.com</a>
</p>


<h3>References</h3>

<p>Xiaojun Chen, Xiaofei Xu, Joshua Zhexue Huang and Yunming Ye (2013).
TW-k-Means: Automated Two-level Variable Weighting Clustering
Algorithm for Multiview Data. IEEE Transactions on Knowledge and Data
Engineering, 25(4), 932&ndash;944.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+kmeans">kmeans</a></code>
<code><a href="#topic+ewkm">ewkm</a></code>
<code><a href="#topic+fgkm">fgkm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# The data twkm.sample has 2000 objects and 410 variables.
# Scale the data before clustering
x &lt;- scale(twkm.sample[,1:409])

# Group information is formated as below.
# Each group is separated by ';'.
strGroup &lt;- "0-75;76-291;292-355;356-402;403-408"
groups &lt;- c(rep(0, abs(0-75-1)), rep(1, abs(76-291-1)), rep(2, abs(292-355-1)),
            rep(3, abs(356-402-1)), rep(4, abs(403-408-1)))


# Use the twkm algorithm.
mytwkm &lt;- twkm(x, 10, strGroup, 3, 1, seed=19)
mytwkm2 &lt;- twkm(x, 10, groups, 3, 1, seed=19)
all.equal(mytwkm, mytwkm2)

# You can print the clustering result now.
mytwkm$cluster
mytwkm$featureWeight
mytwkm$groupWeight
mytwkm$iterations
mytwkm$restarts
mytwkm$totiters
mytwkm$totss

# Use a cluster validation method from package 'fpc'.

# real.cluster is the real class label of the data 'twkm.sample'.
real.cluster &lt;- twkm.sample[,410]

# cluster.stats() computes several distance based statistics.
kmstats &lt;- cluster.stats(d=dist(x), as.integer(mytwkm$cluster), real.cluster)

# corrected Rand index
kmstats$corrected.rand

# variation of information (VI) index
kmstats$vi


</code></pre>

<hr>
<h2 id='twkm.sample'>Sample dataset to test the twkm algorithm.</h2><span id='topic+twkm.sample'></span>

<h3>Description</h3>

<p>A sample dataset of 410 variables and 2000 observations. The last variable is the class variable, and should be removed before clustering.
The grouping of the variables are &quot;0-75;76-291;292-355;356-402;403-408&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>twkm.sample</code></pre>


<h3>Format</h3>

<p>The <code>twkm.sample</code> dataset is a data frame with 2000 observations
and 50 variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+twkm">twkm</a></code>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
