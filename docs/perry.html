<!DOCTYPE html><html><head><title>Help for package perry</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {perry}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#accessors'><p>Access or set information on resampling-based prediction error results</p></a></li>
<li><a href='#aggregate.perry'><p>Aggregate resampling-based prediction error results</p></a></li>
<li><a href='#bootControl'><p>Control object for bootstrap samples</p></a></li>
<li><a href='#bootSamples'><p>Bootstrap samples</p></a></li>
<li><a href='#cost'><p>Prediction loss</p></a></li>
<li><a href='#cvFolds'><p>Cross-validation folds</p></a></li>
<li><a href='#foldControl'><p>Control object for cross-validation folds</p></a></li>
<li><a href='#perry'><p>Resampling-based prediction error for fitted models</p></a></li>
<li><a href='#perry-deprecated'><p>Deprecated functions in package <span class="pkg">perry</span></p></a></li>
<li><a href='#perry-package'>
<p>Resampling-Based Prediction Error Estimation for Regression Models</p></a></li>
<li><a href='#perryFit'><p>Resampling-based prediction error for model evaluation</p></a></li>
<li><a href='#perryPlot'><p>Plot resampling-based prediction error results</p></a></li>
<li><a href='#perryReshape'><p>Reshape resampling-based prediction error results</p></a></li>
<li><a href='#perrySelect'><p>Model selection via resampling-based prediction error measures</p></a></li>
<li><a href='#perrySplits'><p>Data splits for resampling-based prediction error measures</p></a></li>
<li><a href='#perryTuning'><p>Resampling-based prediction error for tuning parameter selection</p></a></li>
<li><a href='#randomSplits'><p>Random data splits</p></a></li>
<li><a href='#reperry'><p>Recompute resampling-based prediction error measures</p></a></li>
<li><a href='#setupPerryPlot'><p>Set up a plot of resampling-based prediction error results</p></a></li>
<li><a href='#splitControl'><p>Control object for random data splits</p></a></li>
<li><a href='#subset.perry'><p>Subsetting resampling-based prediction error results</p></a></li>
<li><a href='#summary.perry'><p>Summarize resampling-based prediction error results</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Resampling-Based Prediction Error Estimation for Regression
Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-11-03</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14.1), ggplot2 (&ge; 0.9.2), parallel</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>perryExamples, robustbase</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools that allow developers to write functions for prediction
    error estimation with minimal programming effort and assist users with
    model selection in regression problems.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Author:</td>
<td>Andreas Alfons [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andreas Alfons &lt;alfons@ese.eur.nl&gt;</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-11-03 10:23:35 UTC; andreas</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-11-03 11:20:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='accessors'>Access or set information on resampling-based prediction error results</h2><span id='topic+accessors'></span><span id='topic+peNames'></span><span id='topic+peNames+3C-'></span><span id='topic+fits'></span><span id='topic+fits+3C-'></span><span id='topic+npe'></span><span id='topic+nfits'></span>

<h3>Description</h3>

<p>Retrieve or set the names of resampling-based prediction error results,
retrieve or set the identifiers of the models, or retrieve the number of
prediction error results or included models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>peNames(x)

peNames(x) &lt;- value

fits(x)

fits(x) &lt;- value

npe(x)

nfits(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="accessors_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>"perry"</code> or
<code>"perrySelect"</code> that contains prediction error results.</p>
</td></tr>
<tr><td><code id="accessors_+3A_value">value</code></td>
<td>
<p>a vector of replacement values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>peNames</code> returns the names of the prediction error results.  The
replacement function thereby returns them invisibly.
</p>
<p><code>fits</code> returns the identifiers of the models for objects inheriting
from class <code>"perrySelect"</code> and <code>NULL</code> for objects inheriting from
class <code>"perry"</code>.  The replacement function thereby returns those values
invisibly.
</p>
<p><code>npe</code> returns the number of prediction error results.
</p>
<p><code>nfits</code> returns the number of models included in objects inheriting
from class <code>"perrySelect"</code> and <code>NULL</code> for objects inheriting from
class <code>"perry"</code>.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perrySelect">perrySelect</a></code>,
<code><a href="#topic+perryTuning">perryTuning</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")
set.seed(1234)  # set seed for reproducibility

## set up folds for cross-validation
folds &lt;- cvFolds(nrow(coleman), K = 5, R = 10)

## compare raw and reweighted LTS estimators for
## 50% and 75% subsets

# 50% subsets
fit50 &lt;- ltsReg(Y ~ ., data = coleman, alpha = 0.5)
cv50 &lt;- perry(fit50, splits = folds, fit = "both",
              cost = rtmspe, trim = 0.1)

# 75% subsets
fit75 &lt;- ltsReg(Y ~ ., data = coleman, alpha = 0.75)
cv75 &lt;- perry(fit75, splits = folds, fit = "both",
              cost = rtmspe, trim = 0.1)

# combine results into one object
cv &lt;- perrySelect("0.5" = cv50, "0.75" = cv75)
cv

# "perry" object
npe(cv50)
nfits(cv50)
peNames(cv50)
peNames(cv50) &lt;- c("improved", "initial")
fits(cv50)
cv50

# "perrySelect" object
npe(cv)
nfits(cv)
peNames(cv)
peNames(cv) &lt;- c("improved", "initial")
fits(cv)
fits(cv) &lt;- 1:2
cv
</code></pre>

<hr>
<h2 id='aggregate.perry'>Aggregate resampling-based prediction error results</h2><span id='topic+aggregate.perry'></span><span id='topic+aggregate.perrySelect'></span><span id='topic+aggregate.perryTuning'></span>

<h3>Description</h3>

<p>Compute summary statistics of resampling-based prediction error results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'perry'
aggregate(x, FUN = mean, select = NULL, ...)

## S3 method for class 'perrySelect'
aggregate(x, FUN = mean, select = NULL, ...)

## S3 method for class 'perryTuning'
aggregate(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregate.perry_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>"perry"</code> or 
<code>"perrySelect"</code> that contains prediction error results (note that the 
latter includes objects of class <code>"perryTuning"</code>).</p>
</td></tr>
<tr><td><code id="aggregate.perry_+3A_fun">FUN</code></td>
<td>
<p>a function to compute the summary statistics.</p>
</td></tr>
<tr><td><code id="aggregate.perry_+3A_select">select</code></td>
<td>
<p>a character, integer or logical vector indicating the columns 
of prediction error results for which to compute the summary statistics.</p>
</td></tr>
<tr><td><code id="aggregate.perry_+3A_...">...</code></td>
<td>
<p>for the <code>"perryTuning"</code> method, additional arguments to 
be passed to the <code>"perrySelect"</code> method.  Otherwise additional 
arguments to be passed to <code>FUN</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>"perry"</code> method returns a vector or matrix of aggregated 
prediction error results, depending on whether <code>FUN</code> returns a single 
value or a vector.
</p>
<p>For the other methods, a data frame containing the aggregated 
prediction error results for each model is returned.  In the case of the 
<code>"perryTuning"</code> method, the data frame contains the combinations of 
tuning parameters rather than a column describing the models.
</p>


<h3>Note</h3>

<p>Duplicate indices in <code>subset</code> or <code>select</code> are removed such 
that all models and prediction error results are unique.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perrySelect">perrySelect</a></code>, 
<code><a href="#topic+perryTuning">perryTuning</a></code>, <code><a href="stats.html#topic+aggregate">aggregate</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")
set.seed(1234)  # set seed for reproducibility

## set up folds for cross-validation
folds &lt;- cvFolds(nrow(coleman), K = 5, R = 10)

## compare raw and reweighted LTS estimators for
## 50% and 75% subsets

# 50% subsets
fit50 &lt;- ltsReg(Y ~ ., data = coleman, alpha = 0.5)
cv50 &lt;- perry(fit50, splits = folds, fit = "both",
              cost = rtmspe, trim = 0.1)

# 75% subsets
fit75 &lt;- ltsReg(Y ~ ., data = coleman, alpha = 0.75)
cv75 &lt;- perry(fit75, splits = folds, fit = "both",
              cost = rtmspe, trim = 0.1)

# combine results into one object
cv &lt;- perrySelect("0.5" = cv50, "0.75" = cv75)
cv

# summary of the results with the 50% subsets
aggregate(cv50, summary)
# summary of the combined results
aggregate(cv, summary)
</code></pre>

<hr>
<h2 id='bootControl'>Control object for bootstrap samples</h2><span id='topic+bootControl'></span>

<h3>Description</h3>

<p>Generate an object that controls how to draw bootstrap samples and which
bootstrap estimator of prediction error to compute.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootControl(R = 1, type = c("0.632", "out-of-bag"), grouping = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootControl_+3A_r">R</code></td>
<td>
<p>an integer giving the number of bootstrap samples.</p>
</td></tr>
<tr><td><code id="bootControl_+3A_type">type</code></td>
<td>
<p>a character string specifying a bootstrap estimator.  Possible
values are <code>"0.632"</code> (the default), or <code>"out-of-bag"</code>.</p>
</td></tr>
<tr><td><code id="bootControl_+3A_grouping">grouping</code></td>
<td>
<p>a factor specifying groups of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"bootControl"</code> with the following
components:
</p>

<dl>
<dt><code>R</code></dt><dd><p>an integer giving the number of bootstrap samples.</p>
</dd>
<dt><code>type</code></dt><dd><p>a character string specifying the type of bootstrap
estimator.</p>
</dd>
<dt><code>grouping</code></dt><dd><p>if supplied, a factor specifying groups of
observations.  The groups will then be resampled rather than individual
observations such that all observations within a group belong either to the
bootstrap sample or the test data.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>References</h3>

<p>Efron, B. (1983) Estimating the error rate of a prediction rule: improvement
on cross-validation.  <em>Journal of the American Statistical
Association</em>, <b>78</b>(382), 316&ndash;331.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perrySplits">perrySplits</a></code>, <code><a href="#topic+bootSamples">bootSamples</a></code>,
<code><a href="#topic+foldControl">foldControl</a></code>, <code><a href="#topic+splitControl">splitControl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234)  # set seed for reproducibility
perrySplits(20, bootControl())
perrySplits(20, bootControl(R = 10))

</code></pre>

<hr>
<h2 id='bootSamples'>Bootstrap samples</h2><span id='topic+bootSamples'></span><span id='topic+print.bootSamples'></span>

<h3>Description</h3>

<p>Draw bootstrap samples of observations or groups of observations and specify
which bootstrap estimator of prediction error to compute.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootSamples(n, R = 1, type = c("0.632", "out-of-bag"), grouping = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootSamples_+3A_n">n</code></td>
<td>
<p>an integer giving the number of observations for which to draw
bootstrap samples.  This is ignored if <code>grouping</code> is supplied in
order to respect the group structure of the data in the bootstrap samples.</p>
</td></tr>
<tr><td><code id="bootSamples_+3A_r">R</code></td>
<td>
<p>an integer giving the number of bootstrap samples.</p>
</td></tr>
<tr><td><code id="bootSamples_+3A_type">type</code></td>
<td>
<p>a character string specifying a bootstrap estimator.  Possible
values are <code>"0.632"</code> (the default), or <code>"out-of-bag"</code>.</p>
</td></tr>
<tr><td><code id="bootSamples_+3A_grouping">grouping</code></td>
<td>
<p>a factor specifying groups of observations.  If supplied,
the groups are resampled rather than individual observations such that all
observations within a group belong either to the bootstrap sample or the
test data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"bootSamples"</code> with the following
components:
</p>

<dl>
<dt><code>n</code></dt><dd><p>an integer giving the number of observations or groups.</p>
</dd>
<dt><code>R</code></dt><dd><p>an integer giving the number of bootstrap samples.</p>
</dd>
<dt><code>subsets</code></dt><dd><p>an integer matrix in which each column contains the
indices of the observations or groups in the corresponding bootstrap
sample.</p>
</dd>
<dt><code>grouping</code></dt><dd><p>a list giving the indices of the observations
belonging to each group.  This is only returned if a grouping factor
has been supplied.</p>
</dd>
</dl>



<h3>Note</h3>

<p>This is a simple wrapper function for <code><a href="#topic+perrySplits">perrySplits</a></code> with a
control object generated by <code><a href="#topic+bootControl">bootControl</a></code>.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>References</h3>

<p>Efron, B. (1983) Estimating the error rate of a prediction rule: improvement
on cross-validation.  <em>Journal of the American Statistical
Association</em>, <b>78</b>(382), 316&ndash;331.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perrySplits">perrySplits</a></code>, <code><a href="#topic+bootControl">bootControl</a></code>,
<code><a href="#topic+cvFolds">cvFolds</a></code>, <code><a href="#topic+randomSplits">randomSplits</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234)  # set seed for reproducibility
bootSamples(20)
bootSamples(20, R = 10)

</code></pre>

<hr>
<h2 id='cost'>Prediction loss</h2><span id='topic+cost'></span><span id='topic+mspe'></span><span id='topic+rmspe'></span><span id='topic+mape'></span><span id='topic+tmspe'></span><span id='topic+rtmspe'></span>

<h3>Description</h3>

<p>Compute the prediction loss of a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspe(y, yHat, includeSE = FALSE)

rmspe(y, yHat, includeSE = FALSE)

mape(y, yHat, includeSE = FALSE)

tmspe(y, yHat, trim = 0.25, includeSE = FALSE)

rtmspe(y, yHat, trim = 0.25, includeSE = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cost_+3A_y">y</code></td>
<td>
<p>a numeric vector or matrix giving the observed values.</p>
</td></tr>
<tr><td><code id="cost_+3A_yhat">yHat</code></td>
<td>
<p>a numeric vector or matrix of the same dimensions as <code>y</code>
giving the fitted values.</p>
</td></tr>
<tr><td><code id="cost_+3A_includese">includeSE</code></td>
<td>
<p>a logical indicating whether standard errors should be computed
as well.</p>
</td></tr>
<tr><td><code id="cost_+3A_trim">trim</code></td>
<td>
<p>a numeric value giving the trimming proportion (the default is
0.25).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mspe</code> and <code>rmspe</code> compute the mean squared prediction error and
the root mean squared prediction error, respectively.  In addition,
<code>mape</code> returns the mean absolute prediction error, which is somewhat
more robust.
</p>
<p>Robust prediction loss based on trimming is implemented in <code>tmspe</code> and
<code>rtmspe</code>.  To be more precise, <code>tmspe</code> computes the trimmed mean
squared prediction error and <code>rtmspe</code> computes the root trimmed mean
squared prediction error.  A proportion of the largest squared differences
of the observed and fitted values are thereby trimmed.
</p>
<p>Standard errors can be requested via the <code>includeSE</code> argument.  Note that
standard errors for <code>tmspe</code> are based on a winsorized standard
deviation.  Furthermore, standard errors for <code>rmspe</code> and <code>rtmspe</code>
are computed from the respective standard errors of <code>mspe</code> and
<code>tmspe</code> via the delta method.
</p>


<h3>Value</h3>

<p>If standard errors are not requested, a numeric value giving the
prediction loss is returned.
</p>
<p>Otherwise a list is returned, with the first component containing the
prediction loss and the second component the corresponding standard error.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>References</h3>

<p>Tukey, J.W. and McLaughlin, D.H. (1963) Less vulnerable confidence and
significance procedures for location based on a single sample:
Trimming/winsorization.  <em>Sankhya: The Indian Journal of Statistics,
Series A</em>, <b>25</b>(3), 331&ndash;352
</p>
<p>Oehlert, G.W. (1992) A note on the delta method.  <em>The American
Statistician</em>, <b>46</b>(1), 27&ndash;29.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perryTuning">perryTuning</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># fit an MM-regression model
library("robustbase")
data("coleman")
fit &lt;- lmrob(Y~., data=coleman)

# compute the prediction loss from the fitted values
# (hence the prediction loss is underestimated in this simple
# example since all observations are used to fit the model)
mspe(coleman$Y, predict(fit))
rmspe(coleman$Y, predict(fit))
mape(coleman$Y, predict(fit))
tmspe(coleman$Y, predict(fit), trim = 0.1)
rtmspe(coleman$Y, predict(fit), trim = 0.1)

# include standard error
mspe(coleman$Y, predict(fit), includeSE = TRUE)
rmspe(coleman$Y, predict(fit), includeSE = TRUE)
mape(coleman$Y, predict(fit), includeSE = TRUE)
tmspe(coleman$Y, predict(fit), trim = 0.1, includeSE = TRUE)
rtmspe(coleman$Y, predict(fit), trim = 0.1, includeSE = TRUE)

</code></pre>

<hr>
<h2 id='cvFolds'>Cross-validation folds</h2><span id='topic+cvFolds'></span><span id='topic+print.cvFolds'></span>

<h3>Description</h3>

<p>Split observations or groups of observations into <code class="reqn">K</code> folds to be used
for (repeated) <code class="reqn">K</code>-fold cross-validation.  <code class="reqn">K</code> should thereby be
chosen such that all folds are of approximately equal size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvFolds(
  n,
  K = 5,
  R = 1,
  type = c("random", "consecutive", "interleaved"),
  grouping = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvFolds_+3A_n">n</code></td>
<td>
<p>an integer giving the number of observations to be split into
folds.  This is ignored if <code>grouping</code> is supplied in order to split
groups of observations into folds.</p>
</td></tr>
<tr><td><code id="cvFolds_+3A_k">K</code></td>
<td>
<p>an integer giving the number of folds into which the observations
should be split (the default is five).  Setting <code>K</code> equal to the number
of observations or groups yields leave-one-out cross-validation.</p>
</td></tr>
<tr><td><code id="cvFolds_+3A_r">R</code></td>
<td>
<p>an integer giving the number of replications for repeated
<code class="reqn">K</code>-fold cross-validation.  This is ignored for for leave-one-out
cross-validation and other non-random splits of the data.</p>
</td></tr>
<tr><td><code id="cvFolds_+3A_type">type</code></td>
<td>
<p>a character string specifying the type of folds to be
generated.  Possible values are <code>"random"</code> (the default),
<code>"consecutive"</code> or <code>"interleaved"</code>.</p>
</td></tr>
<tr><td><code id="cvFolds_+3A_grouping">grouping</code></td>
<td>
<p>a factor specifying groups of observations.  If supplied,
the data are split according to the groups rather than individual
observations such that all observations within a group belong to the same
fold.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"cvFolds"</code> with the following components:
</p>

<dl>
<dt><code>n</code></dt><dd><p>an integer giving the number of observations or groups.</p>
</dd>
<dt><code>K</code></dt><dd><p>an integer giving the number of folds.</p>
</dd>
<dt><code>R</code></dt><dd><p>an integer giving the number of replications.</p>
</dd>
<dt><code>subsets</code></dt><dd><p>an integer matrix in which each column contains a
permutation of the indices of the observations or groups.</p>
</dd>
<dt><code>which</code></dt><dd><p>an integer vector giving the fold for each permuted
observation or group.</p>
</dd>
<dt><code>grouping</code></dt><dd><p>a list giving the indices of the observations
belonging to each group.  This is only returned if a grouping factor
has been supplied.</p>
</dd>
</dl>



<h3>Note</h3>

<p>This is a simple wrapper function for <code><a href="#topic+perrySplits">perrySplits</a></code> with a
control object generated by <code><a href="#topic+foldControl">foldControl</a></code>.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perrySplits">perrySplits</a></code>, <code><a href="#topic+foldControl">foldControl</a></code>,
<code><a href="#topic+randomSplits">randomSplits</a></code>, <code><a href="#topic+bootSamples">bootSamples</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234)  # set seed for reproducibility
cvFolds(20, K = 5, type = "random")
cvFolds(20, K = 5, type = "consecutive")
cvFolds(20, K = 5, type = "interleaved")
cvFolds(20, K = 5, R = 10)

</code></pre>

<hr>
<h2 id='foldControl'>Control object for cross-validation folds</h2><span id='topic+foldControl'></span>

<h3>Description</h3>

<p>Generate an object that controls how to split <code class="reqn">n</code> observations or
groups of observations into <code class="reqn">K</code> folds to be used for (repeated)
<code class="reqn">K</code>-fold cross-validation.  <code class="reqn">K</code> should thereby be chosen such that
all folds are of approximately equal size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>foldControl(
  K = 5,
  R = 1,
  type = c("random", "consecutive", "interleaved"),
  grouping = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="foldControl_+3A_k">K</code></td>
<td>
<p>an integer giving the number of folds into which the observations
should be split (the default is five).</p>
</td></tr>
<tr><td><code id="foldControl_+3A_r">R</code></td>
<td>
<p>an integer giving the number of replications for repeated
<code class="reqn">K</code>-fold cross-validation.</p>
</td></tr>
<tr><td><code id="foldControl_+3A_type">type</code></td>
<td>
<p>a character string specifying the type of folds to be
generated.  Possible values are <code>"random"</code> (the default),
<code>"consecutive"</code> or <code>"interleaved"</code>.</p>
</td></tr>
<tr><td><code id="foldControl_+3A_grouping">grouping</code></td>
<td>
<p>a factor specifying groups of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"foldControl"</code> with the following
components:
</p>

<dl>
<dt><code>K</code></dt><dd><p>an integer giving the number of folds.  A value of
<code>K</code> equal to the number of observations or groups yields
eave-one-out cross-validation.</p>
</dd>
<dt><code>R</code></dt><dd><p>an integer giving the number of replications.  This will
be ignored for for leave-one-out cross-validation and other non-random
splits of the data.</p>
</dd>
<dt><code>type</code></dt><dd><p>a character string specifying the type of folds.</p>
</dd>
<dt><code>grouping</code></dt><dd><p>if supplied, a factor specifying groups of
observations.  The data will then be split according to the groups rather
than individual observations such that all observations within a group
belong to the same fold.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perrySplits">perrySplits</a></code>, <code><a href="#topic+cvFolds">cvFolds</a></code>,
<code><a href="#topic+splitControl">splitControl</a></code>, <code><a href="#topic+bootControl">bootControl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234)  # set seed for reproducibility
perrySplits(20, foldControl(K = 5))
perrySplits(20, foldControl(K = 5, R = 10))

</code></pre>

<hr>
<h2 id='perry'>Resampling-based prediction error for fitted models</h2><span id='topic+perry'></span>

<h3>Description</h3>

<p>Generic function to estimate the prediction error of a fitted model via 
(repeated) <code class="reqn">K</code>-fold cross-validation, (repeated) random splitting (also 
known as random subsampling or Monte Carlo cross-validation), or the 
bootstrap.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perry(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perry_+3A_object">object</code></td>
<td>
<p>the fitted model for which to estimate the prediction error.</p>
</td></tr>
<tr><td><code id="perry_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed down to methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea is that developers write easy-to-use methods for end users to 
leverage the prediction error estimation framework for their models.  A 
typical <code>perry</code> method consists of the following two parts: first the 
data are extracted from the model, then function <code><a href="#topic+perryFit">perryFit</a></code> is 
called to perform prediction error estimation.  The programming effort of 
implementing prediction error estimation for a certain model is thus greatly 
reduced.
</p>
<p>Examples for methods are available in package perryExamples (see 
<code><a href="perryExamples.html#topic+perry-methods">perry-methods</a></code>).
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="perryExamples.html#topic+perry-methods">perry-methods</a></code>
</p>

<hr>
<h2 id='perry-deprecated'>Deprecated functions in package <span class="pkg">perry</span></h2><span id='topic+perry-deprecated'></span><span id='topic+fortify.perry'></span><span id='topic+fortify.perrySelect'></span><span id='topic+fortify.perryTuning'></span><span id='topic+perryPlot.default'></span>

<h3>Description</h3>

<p>These functions are provided for compatibility with older versions only, and
may be defunct as soon as the next release.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'perry'
fortify(
  model,
  data,
  select = NULL,
  reps = model$splits$R &gt; 1,
  seFactor = NA,
  ...
)

## S3 method for class 'perrySelect'
fortify(
  model,
  data,
  subset = NULL,
  select = NULL,
  reps = model$splits$R &gt; 1,
  seFactor = model$seFactor,
  ...
)

## S3 method for class 'perryTuning'
fortify(model, data, ...)

## Default S3 method:
perryPlot(
  object,
  method = c("box", "density", "dot", "line"),
  mapping,
  facets = attr(object, "facets"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perry-deprecated_+3A_model">model</code></td>
<td>
<p>an object inheriting from class <code>"perry"</code> or
<code>"perrySelect"</code> that contains prediction error results.</p>
</td></tr>
<tr><td><code id="perry-deprecated_+3A_data">data</code></td>
<td>
<p>currently ignored.</p>
</td></tr>
<tr><td><code id="perry-deprecated_+3A_select">select</code></td>
<td>
<p>a character, integer or logical vector indicating the columns
of prediction error results to be converted.</p>
</td></tr>
<tr><td><code id="perry-deprecated_+3A_reps">reps</code></td>
<td>
<p>a logical indicating whether to convert the results from all
replications (<code>TRUE</code>) or the aggregated results (<code>FALSE</code>).  The
former is suitable for box plots or smooth density plots, while the latter
is suitable for dot plots or line plots (see <code><a href="#topic+perryPlot">perryPlot</a></code>).</p>
</td></tr>
<tr><td><code id="perry-deprecated_+3A_sefactor">seFactor</code></td>
<td>
<p>a numeric value giving the multiplication factor of the
standard error for displaying error bars in dot plots or line plots.  Error
bars in those plots can be suppressed by setting this to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="perry-deprecated_+3A_...">...</code></td>
<td>
<p>for the <code>"perryTuning"</code> method of <code>fortify</code>,
additional arguments to be passed down to its <code>"perrySelect"</code>
method.  For the other methods of <code>fortify</code>, additional arguments are
currently ignored.  For the default method of <code>perryPlot</code>, additional
arguments to be passed down to <code><a href="ggplot2.html#topic+geom_boxplot">geom_boxplot</a></code>,
<code><a href="ggplot2.html#topic+geom_density">geom_density</a></code>, <code><a href="ggplot2.html#topic+geom_pointrange">geom_pointrange</a></code>
or <code><a href="ggplot2.html#topic+geom_line">geom_line</a></code>.</p>
</td></tr>
<tr><td><code id="perry-deprecated_+3A_subset">subset</code></td>
<td>
<p>a character, integer or logical vector indicating the subset
of models to be converted.</p>
</td></tr>
<tr><td><code id="perry-deprecated_+3A_object">object</code></td>
<td>
<p>an object inheriting from class <code>"perry"</code> or
<code>"perrySelect"</code> that contains prediction error results, or a data frame
containing all necessary information for plotting (as generated by the
corresponding <code><a href="#topic+fortify.perry">fortify</a></code> method).</p>
</td></tr>
<tr><td><code id="perry-deprecated_+3A_method">method</code></td>
<td>
<p>a character string specifying the type of plot.  Possible
values are <code>"box"</code> to create a box plot, <code>"density"</code> to create a
smooth density plot, <code>"dot"</code> to create a dot plot, or <code>"line"</code> to
plot the (average) results for each model as a connected line (for objects
inheriting from class <code>"perrySelect"</code>).  Note that the first two plots
are only meaningful in case of repeated resampling.  The default is to use
<code>"box"</code> in case of repeated resampling and <code>"dot"</code> otherwise.  In
any case, partial string matching allows supply abbreviations of the
accepted values.</p>
</td></tr>
<tr><td><code id="perry-deprecated_+3A_mapping">mapping</code></td>
<td>
<p>an aesthetic mapping to override the default behavior (see
<code><a href="ggplot2.html#topic+aes">aes</a></code> or <code><a href="ggplot2.html#topic+aes_string">aes_string</a></code>).</p>
</td></tr>
<tr><td><code id="perry-deprecated_+3A_facets">facets</code></td>
<td>
<p>a faceting formula to override the default behavior.  If
supplied, <code><a href="ggplot2.html#topic+facet_wrap">facet_wrap</a></code> or
<code><a href="ggplot2.html#topic+facet_grid">facet_grid</a></code> is called depending on whether the formula
is one-sided or two-sided.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>fortify</code> methods extract all necessary information for plotting
from resampling-based prediction error results and store it in a data frame.
</p>
<p>The default method of <code>perryPlot</code> creates the corresponding plot from
the data frame returned by <code>fortify</code>.
</p>


<h3>Value</h3>

<p>The <code>fortify</code> methods return a data frame containing the
columns listed below, as well as additional information stored in the
attribute <code>"facets"</code> (default faceting formula for the plots).
</p>

<dl>
<dt><code>Fit</code></dt><dd><p>a vector or factor containing the identifiers of the
models.</p>
</dd>
<dt><code>Name</code></dt><dd><p>a factor containing the names of the predictor error
results (not returned in case of only one column of prediction error
results with the default name).</p>
</dd>
<dt><code>PE</code></dt><dd><p>the estimated prediction errors.</p>
</dd>
<dt><code>Lower</code></dt><dd><p>the lower end points of the error bars (only returned
if <code>reps</code> is <code>FALSE</code>).</p>
</dd>
<dt><code>Upper</code></dt><dd><p>the upper end points of the error bars (only returned
if <code>reps</code> is <code>FALSE</code>).</p>
</dd>
</dl>



<h3>Note</h3>

<p>Duplicate indices in <code>subset</code> or <code>select</code> are removed such
that all models and prediction error results are unique.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>

<hr>
<h2 id='perry-package'>
Resampling-Based Prediction Error Estimation for Regression Models
</h2><span id='topic+perry-package'></span>

<h3>Description</h3>

<p>Tools that allow developers to write functions for prediction
    error estimation with minimal programming effort and assist users with
    model selection in regression problems.
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> perry</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Resampling-Based Prediction Error Estimation for Regression Models</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.3.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2021-11-03</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 2.14.1),
ggplot2 (&gt;= 0.9.2),
parallel</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> stats,
utils</td>
</tr>
<tr>
 <td style="text-align: left;">
Suggests: </td><td style="text-align: left;"> perryExamples,
robustbase</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Tools that allow developers to write functions for prediction
    error estimation with minimal programming effort and assist users with
    model selection in regression problems.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> person("Andreas", "Alfons",
    email = "alfons@ese.eur.nl",
    role = c("aut", "cre"))</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Andreas Alfons [aut, cre]</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Andreas Alfons &lt;alfons@ese.eur.nl&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Encoding: </td><td style="text-align: left;"> UTF-8</td>
</tr>
<tr>
 <td style="text-align: left;">
RoxygenNote: </td><td style="text-align: left;"> 7.1.2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
accessors               Access or set information on resampling-based
                        prediction error results
aggregate.perry         Aggregate resampling-based prediction error
                        results
bootControl             Control object for bootstrap samples
bootSamples             Bootstrap samples
cost                    Prediction loss
cvFolds                 Cross-validation folds
foldControl             Control object for cross-validation folds
perry                   Resampling-based prediction error for fitted
                        models
perry-deprecated        Deprecated functions in package 'perry'
perry-package           Resampling-Based Prediction Error Estimation
                        for Regression Models
perryFit                Resampling-based prediction error for model
                        evaluation
perryPlot               Plot resampling-based prediction error results
perryReshape            Reshape resampling-based prediction error
                        results
perrySelect             Model selection via resampling-based prediction
                        error measures
perrySplits             Data splits for resampling-based prediction
                        error measures
perryTuning             Resampling-based prediction error for tuning
                        parameter selection
randomSplits            Random data splits
reperry                 Recompute resampling-based prediction error
                        measures
setupPerryPlot          Set up a plot of resampling-based prediction
                        error results
splitControl            Control object for random data splits
subset.perry            Subsetting resampling-based prediction error
                        results
summary.perry           Summarize resampling-based prediction error
                        results
</pre>


<h3>Author(s)</h3>

<p>Andreas Alfons [aut, cre]
</p>
<p>Maintainer: Andreas Alfons &lt;alfons@ese.eur.nl&gt;
</p>

<hr>
<h2 id='perryFit'>Resampling-based prediction error for model evaluation</h2><span id='topic+perryFit'></span><span id='topic+print.perry'></span><span id='topic+perryFit.default'></span><span id='topic+perryFit.function'></span><span id='topic+perryFit.call'></span>

<h3>Description</h3>

<p>Estimate the prediction error of a model via (repeated) <code class="reqn">K</code>-fold
cross-validation, (repeated) random splitting (also known as random
subsampling or Monte Carlo cross-validation), or the bootstrap.  It is
thereby possible to supply an object returned by a model fitting function,
a model fitting function itself, or an unevaluated function call to a model
fitting function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perryFit(object, ...)

## Default S3 method:
perryFit(
  object,
  data = NULL,
  x = NULL,
  y,
  splits = foldControl(),
  predictFun = predict,
  predictArgs = list(),
  cost = rmspe,
  costArgs = list(),
  names = NULL,
  envir = parent.frame(),
  ncores = 1,
  cl = NULL,
  seed = NULL,
  ...
)

## S3 method for class ''function''
perryFit(
  object,
  formula,
  data = NULL,
  x = NULL,
  y,
  args = list(),
  splits = foldControl(),
  predictFun = predict,
  predictArgs = list(),
  cost = rmspe,
  costArgs = list(),
  names = NULL,
  envir = parent.frame(),
  ncores = 1,
  cl = NULL,
  seed = NULL,
  ...
)

## S3 method for class 'call'
perryFit(
  object,
  data = NULL,
  x = NULL,
  y,
  splits = foldControl(),
  predictFun = predict,
  predictArgs = list(),
  cost = rmspe,
  costArgs = list(),
  names = NULL,
  envir = parent.frame(),
  ncores = 1,
  cl = NULL,
  seed = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perryFit_+3A_object">object</code></td>
<td>
<p>the fitted model for which to estimate the prediction error,
a function for fitting a model, or an unevaluated function call for fitting
a model (see <code><a href="base.html#topic+call">call</a></code> for the latter).  In the case of a fitted
model, the object is required to contain a component <code>call</code> that stores
the function call used to fit the model, which is typically the case for
objects returned by model fitting functions.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed down.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables required for fitting the
models.  This is typically used if the model in the function call is
described by a <code><a href="stats.html#topic+formula">formula</a></code>.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_x">x</code></td>
<td>
<p>a numeric matrix containing the predictor variables.  This is
typically used if the function call for fitting the models requires the
predictor matrix and the response to be supplied as separate arguments.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_y">y</code></td>
<td>
<p>a numeric vector or matrix containing the response.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_splits">splits</code></td>
<td>
<p>an object of class <code>"cvFolds"</code> (as returned by
<code><a href="#topic+cvFolds">cvFolds</a></code>) or a control object of class <code>"foldControl"</code>
(see <code><a href="#topic+foldControl">foldControl</a></code>) defining the folds of the data for
(repeated) <code class="reqn">K</code>-fold cross-validation, an object of class
<code>"randomSplits"</code> (as returned by <code><a href="#topic+randomSplits">randomSplits</a></code>) or a
control object of class <code>"splitControl"</code> (see
<code><a href="#topic+splitControl">splitControl</a></code>) defining random data splits, or an object of
class <code>"bootSamples"</code> (as returned by <code><a href="#topic+bootSamples">bootSamples</a></code>) or a
control object of class <code>"bootControl"</code> (see <code><a href="#topic+bootControl">bootControl</a></code>)
defining bootstrap samples.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_predictfun">predictFun</code></td>
<td>
<p>a function to compute predictions for the test data.  It
should expect the fitted model to be passed as the first argument and the test
data as the second argument, and must return either a vector or a matrix
containing the predicted values.  The default is to use the
<code><a href="stats.html#topic+predict">predict</a></code> method of the fitted model.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_predictargs">predictArgs</code></td>
<td>
<p>a list of additional arguments to be passed to
<code>predictFun</code>.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_cost">cost</code></td>
<td>
<p>a cost function measuring prediction loss.  It should expect
the observed values of the response to be passed as the first argument and
the predicted values as the second argument, and must return either a
non-negative scalar value, or a list with the first component containing
the prediction error and the second component containing the standard
error.  The default is to use the root mean squared prediction error
(see <code><a href="#topic+cost">cost</a></code>).</p>
</td></tr>
<tr><td><code id="perryFit_+3A_costargs">costArgs</code></td>
<td>
<p>a list of additional arguments to be passed to the
prediction loss function <code>cost</code>.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_names">names</code></td>
<td>
<p>an optional character vector giving names for the arguments
containing the data to be used in the function call (see &ldquo;Details&rdquo;).</p>
</td></tr>
<tr><td><code id="perryFit_+3A_envir">envir</code></td>
<td>
<p>the <code><a href="base.html#topic+environment">environment</a></code> in which to evaluate the
function call for fitting the models (see <code><a href="base.html#topic+eval">eval</a></code>).</p>
</td></tr>
<tr><td><code id="perryFit_+3A_ncores">ncores</code></td>
<td>
<p>a positive integer giving the number of processor cores to be
used for parallel computing (the default is 1 for no parallelization).  If
this is set to <code>NA</code>, all available processor cores are used.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_cl">cl</code></td>
<td>
<p>a <span class="pkg">parallel</span> cluster for parallel computing as generated by
<code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>.  If supplied, this is preferred over
<code>ncores</code>.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_seed">seed</code></td>
<td>
<p>optional initial seed for the random number generator (see
<code><a href="base.html#topic+.Random.seed">.Random.seed</a></code>).  Note that also in case of parallel computing,
resampling is performed on the manager process rather than the worker
processes. On the parallel worker processes, random number streams are
used and the seed is set via <code><a href="parallel.html#topic+clusterSetRNGStream">clusterSetRNGStream</a></code> for
reproducibility in case the model fitting function involves randomness.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_formula">formula</code></td>
<td>
<p>a <code><a href="stats.html#topic+formula">formula</a></code> describing the model.</p>
</td></tr>
<tr><td><code id="perryFit_+3A_args">args</code></td>
<td>
<p>a list of additional arguments to be passed to the model
fitting function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>(Repeated) <code class="reqn">K</code>-fold cross-validation is performed in the following
way.  The data are first split into <code class="reqn">K</code> previously obtained blocks of
approximately equal size (given by <code>folds</code>).  Each of the <code class="reqn">K</code> data
blocks is left out once to fit the model, and predictions are computed for
the observations in the left-out block with <code>predictFun</code>.  Thus a
prediction is obtained for each observation.  The response and the obtained
predictions for all observations are then passed to the prediction loss
function <code>cost</code> to estimate the prediction error.  For repeated
<code class="reqn">K</code>-fold cross-validation (as indicated by <code>splits</code>), this process
is replicated and the estimated prediction errors from all replications are
returned.
</p>
<p>(Repeated) random splitting is performed similarly.  In each replication,
the data are split into a training set and a test set at random.  Then the
training data are used to fit the model, and predictions are computed for
the test data.  Hence only the response values from the test data and the
corresponding predictions are passed to the prediction loss function
<code>cost</code>.
</p>
<p>For the bootstrap estimator, each bootstrap sample is used as training data
to fit the model.  The out-of-bag estimator uses the observations that do
not enter the bootstrap sample as test data and computes the prediction loss
function <code>cost</code> for those out-of-bag observations.  The 0.632 estimator
is computed as a linear combination of the out-of-bag estimator and the
prediction loss of the fitted values of the model computed from the full
sample.
</p>
<p>In any case, if the response is a vector but <code>predictFun</code> returns a
matrix, the prediction error is computed for each column.  A typical use
case for this behavior would be if <code>predictFun</code> returns predictions
from an initial model fit and stepwise improvements thereof.
</p>
<p>If <code>formula</code> or <code>data</code> are supplied, all variables required for
fitting the models are added as one argument to the function call, which is
the typical behavior of model fitting functions with a
<code><a href="stats.html#topic+formula">formula</a></code> interface.  In this case, the accepted values
for <code>names</code> depend on the method.  For the <code>function</code> method, a
character vector of length two should supplied, with the first element
specifying the argument name for the formula and the second element
specifying the argument name for the data (the default is to use
<code>c("formula", "data")</code>).  Note that names for both arguments should be
supplied even if only one is actually used.  For the other methods, which do
not have a <code>formula</code> argument, a character string specifying the
argument name for the data should be supplied (the default is to use
<code>"data"</code>).
</p>
<p>If <code>x</code> is supplied, on the other hand, the predictor matrix and the
response are added as separate arguments to the function call.  In this
case, <code>names</code> should be a character vector of length two, with the
first element specifying the argument name for the predictor matrix and the
second element specifying the argument name for the response (the default is
to use <code>c("x", "y")</code>).  It should be noted that the <code>formula</code> or
<code>data</code> arguments take precedence over <code>x</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>"perry"</code> with the following components:
</p>

<dl>
<dt><code>pe</code></dt><dd><p>a numeric vector containing the respective estimated
prediction errors.  In case of more than one replication, those are
average values over all replications.</p>
</dd>
<dt><code>se</code></dt><dd><p>a numeric vector containing the respective estimated
standard errors of the prediction loss.</p>
</dd>
<dt><code>reps</code></dt><dd><p>a numeric matrix in which each column contains the
respective estimated prediction errors from all replications.  This is
only returned in case of more than one replication.</p>
</dd>
<dt><code>splits</code></dt><dd><p>an object giving the data splits used to estimate the
prediction error.</p>
</dd>
<dt><code>y</code></dt><dd><p>the response.</p>
</dd>
<dt><code>yHat</code></dt><dd><p>a list containing the predicted values from all
replications.</p>
</dd>
<dt><code>call</code></dt><dd><p>the matched function call.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perrySelect">perrySelect</a></code>, <code><a href="#topic+perryTuning">perryTuning</a></code>,
<code><a href="#topic+cvFolds">cvFolds</a></code>, <code><a href="#topic+randomSplits">randomSplits</a></code>,
<code><a href="#topic+bootSamples">bootSamples</a></code>, <code><a href="#topic+cost">cost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")
set.seed(1234)  # set seed for reproducibility

## via model fit
# fit an MM regression model
fit &lt;- lmrob(Y ~ ., data=coleman)
# perform cross-validation
perryFit(fit, data = coleman, y = coleman$Y,
         splits = foldControl(K = 5, R = 10),
         cost = rtmspe, costArgs = list(trim = 0.1),
         seed = 1234)

## via model fitting function
# perform cross-validation
# note that the response is extracted from 'data' in
# this example and does not have to be supplied
perryFit(lmrob, formula = Y ~ ., data = coleman,
         splits = foldControl(K = 5, R = 10),
         cost = rtmspe, costArgs = list(trim = 0.1),
         seed = 1234)

## via function call
# set up function call
call &lt;- call("lmrob", formula = Y ~ .)
# perform cross-validation
perryFit(call, data = coleman, y = coleman$Y,
         splits = foldControl(K = 5, R = 10),
         cost = rtmspe, costArgs = list(trim = 0.1),
         seed = 1234)
</code></pre>

<hr>
<h2 id='perryPlot'>Plot resampling-based prediction error results</h2><span id='topic+perryPlot'></span><span id='topic+perryPlot.perry'></span><span id='topic+perryPlot.perrySelect'></span><span id='topic+perryPlot.setupPerryPlot'></span><span id='topic+autoplot.perry'></span><span id='topic+autoplot.perrySelect'></span><span id='topic+plot.perry'></span><span id='topic+plot.perrySelect'></span>

<h3>Description</h3>

<p>Plot results of resampling-based prediction error measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perryPlot(object, ...)

## S3 method for class 'perry'
perryPlot(
  object,
  which = c("box", "density", "dot"),
  select = NULL,
  seFactor = NA,
  ...
)

## S3 method for class 'perrySelect'
perryPlot(
  object,
  which = c("box", "density", "dot", "line"),
  subset = NULL,
  select = NULL,
  seFactor = object$seFactor,
  ...
)

## S3 method for class 'setupPerryPlot'
perryPlot(object, mapping = object$mapping, facets = object$facets, ...)

## S3 method for class 'perry'
autoplot(object, ...)

## S3 method for class 'perrySelect'
autoplot(object, ...)

## S3 method for class 'perry'
plot(x, ...)

## S3 method for class 'perrySelect'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perryPlot_+3A_object">object</code>, <code id="perryPlot_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>"perry"</code> or
<code>"perrySelect"</code> that contains prediction error results, or an object of
class <code>"setupPerryPlot"</code> containing all necessary information for
plotting (as generated by <code><a href="#topic+setupPerryPlot">setupPerryPlot</a></code>).</p>
</td></tr>
<tr><td><code id="perryPlot_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed down, eventually to
<code><a href="ggplot2.html#topic+geom_boxplot">geom_boxplot</a></code>, <code><a href="ggplot2.html#topic+geom_density">geom_density</a></code>,
<code><a href="ggplot2.html#topic+geom_pointrange">geom_pointrange</a></code>, or <code><a href="ggplot2.html#topic+geom_line">geom_line</a></code>.</p>
</td></tr>
<tr><td><code id="perryPlot_+3A_which">which</code></td>
<td>
<p>a character string specifying the type of plot.  Possible
values are <code>"box"</code> to create a box plot, <code>"density"</code> to create a
smooth density plot, <code>"dot"</code> to create a dot plot, or <code>"line"</code> to
plot the (average) results for each model as a connected line (for objects
inheriting from class <code>"perrySelect"</code>).  Note that the first two plots
are only meaningful in case of repeated resampling.  The default is to use
<code>"box"</code> in case of repeated resampling and <code>"dot"</code> otherwise.</p>
</td></tr>
<tr><td><code id="perryPlot_+3A_select">select</code></td>
<td>
<p>a character, integer or logical vector indicating the columns
of prediction error results to be plotted.</p>
</td></tr>
<tr><td><code id="perryPlot_+3A_sefactor">seFactor</code></td>
<td>
<p>a numeric value giving the multiplication factor of the
standard error for displaying error bars in dot plots or line plots.  Error
bars in those plots can be suppressed by setting this to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="perryPlot_+3A_subset">subset</code></td>
<td>
<p>a character, integer or logical vector indicating the subset
of models for which to plot the prediction error results.</p>
</td></tr>
<tr><td><code id="perryPlot_+3A_mapping">mapping</code></td>
<td>
<p>an aesthetic mapping to override the default behavior (see
<code><a href="ggplot2.html#topic+aes">aes</a></code> or <code><a href="ggplot2.html#topic+aes_string">aes_string</a></code>).</p>
</td></tr>
<tr><td><code id="perryPlot_+3A_facets">facets</code></td>
<td>
<p>a faceting formula to override the default behavior.  If
supplied, <code><a href="ggplot2.html#topic+facet_wrap">facet_wrap</a></code> or
<code><a href="ggplot2.html#topic+facet_grid">facet_grid</a></code> is called depending on whether the formula
is one-sided or two-sided.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For objects with multiple columns of prediction error results, conditional
plots are produced.
</p>


<h3>Value</h3>

<p>An object of class <code>"ggplot"</code> (see <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>).
</p>


<h3>Note</h3>

<p>Duplicate indices in <code>subset</code> or <code>select</code> are removed such
that all models and prediction error results are unique.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+setupPerryPlot">setupPerryPlot</a></code>,
</p>
<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perrySelect">perrySelect</a></code>,
<code><a href="#topic+perryTuning">perryTuning</a></code>,
</p>
<p><code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>, <code><a href="ggplot2.html#topic+autoplot">autoplot</a></code>,
<code><a href="graphics.html#topic+plot">plot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")
set.seed(1234)  # set seed for reproducibility

## set up folds for cross-validation
folds &lt;- cvFolds(nrow(coleman), K = 5, R = 10)

## compare LS, MM and LTS regression

# perform cross-validation for an LS regression model
fitLm &lt;- lm(Y ~ ., data = coleman)
cvLm &lt;- perry(fitLm, splits = folds,
              cost = rtmspe, trim = 0.1)

# perform cross-validation for an MM regression model
fitLmrob &lt;- lmrob(Y ~ ., data = coleman, maxit.scale = 500)
cvLmrob &lt;- perry(fitLmrob, splits = folds,
                 cost = rtmspe, trim = 0.1)

# perform cross-validation for an LTS regression model
fitLts &lt;- ltsReg(Y ~ ., data = coleman)
cvLts &lt;- perry(fitLts, splits = folds,
               cost = rtmspe, trim = 0.1)

# combine results into one object
cv &lt;- perrySelect(LS = cvLm, MM = cvLmrob, LTS = cvLts,
                  .selectBest = "min")
cv

# plot results for the MM regression model
plot(cvLmrob, which = "box")
plot(cvLmrob, which = "density")
plot(cvLmrob, which = "dot", seFactor = 2)

# plot combined results
plot(cv, which = "box")
plot(cv, which = "density")
plot(cv, which = "dot", seFactor = 2)
</code></pre>

<hr>
<h2 id='perryReshape'>Reshape resampling-based prediction error results</h2><span id='topic+perryReshape'></span>

<h3>Description</h3>

<p>Reshape resampling-based prediction error results into an object of class
<code>"perrySelect"</code> with only one column of results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perryReshape(
  x,
  selectBest = c("min", "hastie"),
  seFactor = 1,
  tuning = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perryReshape_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>"perry"</code> or
<code>"perrySelect"</code> that contains prediction error results.</p>
</td></tr>
<tr><td><code id="perryReshape_+3A_selectbest">selectBest</code></td>
<td>
<p>a character string specifying a criterion for selecting
the best model.  Possible values are <code>"min"</code> (the default) or
<code>"hastie"</code>.  The former selects the model with the smallest prediction
error.  The latter is useful for nested models or for models with a tuning
parameter controlling the complexity of the model (e.g., penalized
regression).  It selects the most parsimonious model whose prediction error
is no larger than <code>seFactor</code> standard errors above the prediction error
of the best overall model.  Note that the models are thereby assumed to be
ordered from the most parsimonious one to the most complex one.  In
particular a one-standard-error rule is frequently applied.</p>
</td></tr>
<tr><td><code id="perryReshape_+3A_sefactor">seFactor</code></td>
<td>
<p>a numeric value giving a multiplication factor of the
standard error for the selection of the best model.  This is ignored if
<code>selectBest</code> is <code>"min"</code>.</p>
</td></tr>
<tr><td><code id="perryReshape_+3A_tuning">tuning</code></td>
<td>
<p>a list of tuning parameter values that correspond to the
different prediction error results.  The names of the list components should
thereby correspond to the argument names of the tuning parameters.  For each
tuning parameter, a vector of values can be supplied.  A data frame
containing all possible combinations of tuning parameter values is then
added to the reshaped prediction error results.</p>
</td></tr>
<tr><td><code id="perryReshape_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed down.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"perrySelect"</code> (subclass
<code>"perryTuning"</code> if a list of tuning parameters is supplied) with the
following components:
</p>

<dl>
<dt><code>pe</code></dt><dd><p>a data frame containing the estimated prediction errors
for the models.  In case of more than one resampling replication, those
are average values over all replications.</p>
</dd>
<dt><code>se</code></dt><dd><p>a data frame containing the estimated standard errors of
the prediction loss for the models.</p>
</dd>
<dt><code>reps</code></dt><dd><p>a data frame containing the estimated prediction errors
for the models from all replications.  This is only returned in case of
more than one resampling replication.</p>
</dd>
<dt><code>splits</code></dt><dd><p>an object giving the data splits used to estimate the
prediction error.</p>
</dd>
<dt><code>y</code></dt><dd><p>the response.</p>
</dd>
<dt><code>yHat</code></dt><dd><p>a list containing the predicted values for the
models.  Each list component is again a list containing the corresponding
predicted values from all replications.</p>
</dd>
<dt><code>best</code></dt><dd><p>an integer giving the index of the model with the best
prediction performance.</p>
</dd>
<dt><code>selectBest</code></dt><dd><p>a character string specifying the criterion used for
selecting the best model.</p>
</dd>
<dt><code>seFactor</code></dt><dd><p>a numeric value giving the multiplication factor of
the standard error used for the selection of the best model.</p>
</dd>
<dt><code>tuning</code></dt><dd><p>a data frame containing the grid of tuning parameter
values that correspond to the different prediction error results (only
subclass <code>"perryTuning"</code>).</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>References</h3>

<p>Hastie, T., Tibshirani, R. and Friedman, J. (2009) <em>The Elements of
Statistical Learning: Data Mining, Inference, and Prediction</em>.  Springer,
2nd edition.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perrySelect">perrySelect</a></code>,
<code><a href="#topic+perryTuning">perryTuning</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")

# perform cross-validation for an LTS regression model
fit &lt;- ltsReg(Y ~ ., data = coleman)
folds &lt;- foldControl(K = 5, R = 10)
cv &lt;- perry(fit, splits = folds, fit = "both",
            cost = rtmspe, trim = 0.1, seed = 1234)

# compare original and reshaped object
cv
perryReshape(cv)
</code></pre>

<hr>
<h2 id='perrySelect'>Model selection via resampling-based prediction error measures</h2><span id='topic+perrySelect'></span><span id='topic+print.perrySelect'></span>

<h3>Description</h3>

<p>Combine resampling-based prediction error results for various models into
one object and select the model with the best prediction performance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perrySelect(
  ...,
  .list = list(...),
  .reshape = FALSE,
  .selectBest = c("min", "hastie"),
  .seFactor = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perrySelect_+3A_...">...</code></td>
<td>
<p>objects inheriting from class <code>"perry"</code> or
<code>"perrySelect"</code> that contain prediction error results.</p>
</td></tr>
<tr><td><code id="perrySelect_+3A_.list">.list</code></td>
<td>
<p>a list of objects  inheriting from class <code>"perry"</code> or
<code>"perrySelect"</code>.  If supplied, this is preferred over objects supplied
via the ... argument.</p>
</td></tr>
<tr><td><code id="perrySelect_+3A_.reshape">.reshape</code></td>
<td>
<p>a logical indicating whether objects with more than one
column of prediction error results should be reshaped to have only one
column (see &ldquo;Details&rdquo;).</p>
</td></tr>
<tr><td><code id="perrySelect_+3A_.selectbest">.selectBest</code></td>
<td>
<p>a character string specifying a criterion for selecting
the best model.  Possible values are <code>"min"</code> (the default) or
<code>"hastie"</code>.  The former selects the model with the smallest prediction
error.  The latter is useful for nested models or for models with a tuning
parameter controlling the complexity of the model (e.g., penalized
regression).  It selects the most parsimonious model whose prediction error
is no larger than <code>.seFactor</code> standard errors above the prediction error
of the best overall model.  Note that the models are thereby assumed to be
ordered from the most parsimonious one to the most complex one.  In
particular a one-standard-error rule is frequently applied.</p>
</td></tr>
<tr><td><code id="perrySelect_+3A_.sefactor">.seFactor</code></td>
<td>
<p>a numeric value giving a multiplication factor of the
standard error for the selection of the best model.  This is ignored if
<code>.selectBest</code> is <code>"min"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Keep in mind that objects inheriting from class <code>"perry"</code> or
<code>"perrySelect"</code> may contain multiple columns of prediction error
results.  This is the case if the response is univariate but the
function to compute predictions (usually the <code><a href="stats.html#topic+predict">predict</a></code>
method of the fitted model) returns a matrix.
</p>
<p>The <code>.reshape</code> argument determines how to handle such objects.  If
<code>.reshape</code> is <code>FALSE</code>, all objects are required to have the same
number of columns and the best model for each column is selected.  A typical
use case for this behavior would be if the investigated models contain
prediction error results for a raw and a reweighted fit.  It might then be
of interest to researchers to compare the best model for the raw estimators
with the best model for the reweighted estimators.
</p>
<p>If <code>.reshape</code> is <code>TRUE</code>, objects with more than one column of
results are first transformed with <code><a href="#topic+perryReshape">perryReshape</a></code> to have only
one column.  Then the best overall model is selected.
</p>
<p>It should also be noted that the argument names of <code>.list</code>,
<code>.reshape</code>, <code>.selectBest</code> and <code>.seFacor</code> start with a dot to
avoid conflicts with the argument names used for the objects containing
prediction error results.
</p>


<h3>Value</h3>

<p>An object of class <code>"perrySelect"</code> with the following
components:
</p>

<dl>
<dt><code>pe</code></dt><dd><p>a data frame containing the estimated prediction errors
for the models.  In case of more than one resampling replication, those
are average values over all replications.</p>
</dd>
<dt><code>se</code></dt><dd><p>a data frame containing the estimated standard errors of
the prediction loss for the models.</p>
</dd>
<dt><code>reps</code></dt><dd><p>a data frame containing the estimated prediction errors
for the models from all replications.  This is only returned in case of more
than one resampling replication.</p>
</dd>
<dt><code>splits</code></dt><dd><p>an object giving the data splits used to estimate the
prediction error of the models.</p>
</dd>
<dt><code>y</code></dt><dd><p>the response.</p>
</dd>
<dt><code>yHat</code></dt><dd><p>a list containing the predicted values for the
models.  Each list component is again a list containing the corresponding
predicted values from all replications.</p>
</dd>
<dt><code>best</code></dt><dd><p>an integer vector giving the indices of the models with
the best prediction performance.</p>
</dd>
<dt><code>selectBest</code></dt><dd><p>a character string specifying the criterion used
for selecting the best model.</p>
</dd>
<dt><code>seFactor</code></dt><dd><p>a numeric value giving the multiplication factor of
the standard error used for the selection of the best model.</p>
</dd>
</dl>



<h3>Note</h3>

<p>To ensure comparability, the prediction errors for all models are
required to be computed from the same data splits.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>References</h3>

<p>Hastie, T., Tibshirani, R. and Friedman, J. (2009) <em>The Elements of
Statistical Learning: Data Mining, Inference, and Prediction</em>.  Springer,
2nd edition.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perryTuning">perryTuning</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")
set.seed(1234)  # set seed for reproducibility

## set up folds for cross-validation
folds &lt;- cvFolds(nrow(coleman), K = 5, R = 10)

## compare LS, MM and LTS regression

# perform cross-validation for an LS regression model
fitLm &lt;- lm(Y ~ ., data = coleman)
cvLm &lt;- perry(fitLm, splits = folds,
              cost = rtmspe, trim = 0.1)

# perform cross-validation for an MM regression model
fitLmrob &lt;- lmrob(Y ~ ., data = coleman)
cvLmrob &lt;- perry(fitLmrob, splits = folds,
                 cost = rtmspe, trim = 0.1)

# perform cross-validation for an LTS regression model
fitLts &lt;- ltsReg(Y ~ ., data = coleman)
cvLts &lt;- perry(fitLts, splits = folds,
               cost = rtmspe, trim = 0.1)

# compare cross-validation results
perrySelect(LS = cvLm, MM = cvLmrob, LTS = cvLts)
</code></pre>

<hr>
<h2 id='perrySplits'>Data splits for resampling-based prediction error measures</h2><span id='topic+perrySplits'></span>

<h3>Description</h3>

<p>Split observations or groups of observations into segments to be used
for (repeated) <code class="reqn">K</code>-fold cross-validation, (repeated) random splitting
(also known as random subsampling or Monte Carlo cross-validation), or the
bootstrap.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perrySplits(n, control)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perrySplits_+3A_n">n</code></td>
<td>
<p>an integer giving the number of observations to be split.</p>
</td></tr>
<tr><td><code id="perrySplits_+3A_control">control</code></td>
<td>
<p>a control object of class <code>"foldControl"</code> (as generated
by <code><a href="#topic+foldControl">foldControl</a></code>), <code>"splitControl"</code> (as generated by
<code><a href="#topic+splitControl">splitControl</a></code>) or <code>"bootControl"</code> (as generated by
<code><a href="#topic+bootControl">bootControl</a></code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For the <code>foldControl</code> method, an object of class <code>"cvFolds"</code>
giving folds for (repeated) <code class="reqn">K</code>-fold cross-validation (see
<code><a href="#topic+cvFolds">cvFolds</a></code>).
</p>
<p>For the <code>splitControl</code> method, an object of class <code>"randomSplits"</code>
giving random data splits (see <code><a href="#topic+randomSplits">randomSplits</a></code>).
</p>
<p>For the <code>bootControl</code> method, an object of class <code>"bootSamples"</code>
giving bootstrap samples (see <code><a href="#topic+bootSamples">bootSamples</a></code>).
</p>


<h3>Note</h3>

<p>Users may prefer the wrapper functions <code><a href="#topic+cvFolds">cvFolds</a></code>,
<code><a href="#topic+randomSplits">randomSplits</a></code> and <code><a href="#topic+bootSamples">bootSamples</a></code>.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+foldControl">foldControl</a></code>, <code><a href="#topic+splitControl">splitControl</a></code>,
<code><a href="#topic+bootControl">bootControl</a></code>, <code><a href="#topic+cvFolds">cvFolds</a></code>,
<code><a href="#topic+randomSplits">randomSplits</a></code>, <code><a href="#topic+bootSamples">bootSamples</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234)  # set seed for reproducibility

## data folds for K-fold cross-validation
perrySplits(20, foldControl(K = 5))
perrySplits(20, foldControl(K = 5, R = 10))

## random data splits
perrySplits(20, splitControl(m = 5))
perrySplits(20, splitControl(m = 5, R = 10))

## bootstrap samples
perrySplits(20, bootControl())
perrySplits(20, bootControl(R = 10))

</code></pre>

<hr>
<h2 id='perryTuning'>Resampling-based prediction error for tuning parameter selection</h2><span id='topic+perryTuning'></span><span id='topic+coef.perryTuning'></span><span id='topic+fitted.perryTuning'></span><span id='topic+predict.perryTuning'></span><span id='topic+print.perryTuning'></span><span id='topic+residuals.perryTuning'></span><span id='topic+perryTuning.function'></span><span id='topic+perryTuning.call'></span>

<h3>Description</h3>

<p>Select tuning parameters of a model by estimating the respective prediction
errors via (repeated) <code class="reqn">K</code>-fold cross-validation, (repeated) random
splitting (also known as random subsampling or Monte Carlo
cross-validation), or the bootstrap.  It is thereby possible to supply a
model fitting function or an unevaluated function call to a model fitting
function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perryTuning(object, ...)

## S3 method for class ''function''
perryTuning(
  object,
  formula,
  data = NULL,
  x = NULL,
  y,
  tuning = list(),
  args = list(),
  splits = foldControl(),
  predictFun = predict,
  predictArgs = list(),
  cost = rmspe,
  costArgs = list(),
  selectBest = c("min", "hastie"),
  seFactor = 1,
  final = FALSE,
  names = NULL,
  envir = parent.frame(),
  ncores = 1,
  cl = NULL,
  seed = NULL,
  ...
)

## S3 method for class 'call'
perryTuning(
  object,
  data = NULL,
  x = NULL,
  y,
  tuning = list(),
  splits = foldControl(),
  predictFun = predict,
  predictArgs = list(),
  cost = rmspe,
  costArgs = list(),
  selectBest = c("min", "hastie"),
  seFactor = 1,
  final = FALSE,
  names = NULL,
  envir = parent.frame(),
  ncores = 1,
  cl = NULL,
  seed = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perryTuning_+3A_object">object</code></td>
<td>
<p>a function or an unevaluated function call for fitting
a model (see <code><a href="base.html#topic+call">call</a></code> for the latter).</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed down.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_formula">formula</code></td>
<td>
<p>a <code><a href="stats.html#topic+formula">formula</a></code> describing the model.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables required for fitting the
models.  This is typically used if the model in the function call is
described by a <code><a href="stats.html#topic+formula">formula</a></code>.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_x">x</code></td>
<td>
<p>a numeric matrix containing the predictor variables.  This is
typically used if the function call for fitting the models requires the
predictor matrix and the response to be supplied as separate arguments.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_y">y</code></td>
<td>
<p>a numeric vector or matrix containing the response.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_tuning">tuning</code></td>
<td>
<p>a list of arguments giving the tuning parameter values to be
evaluated.  The names of the list components should thereby correspond to
the argument names of the tuning parameters.  For each tuning parameter, a
vector of values can be supplied.  The prediction error is then estimated
for all possible combinations of tuning parameter values.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_args">args</code></td>
<td>
<p>a list of additional arguments to be passed to the model
fitting function.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_splits">splits</code></td>
<td>
<p>an object of class <code>"cvFolds"</code> (as returned by
<code><a href="#topic+cvFolds">cvFolds</a></code>) or a control object of class <code>"foldControl"</code>
(see <code><a href="#topic+foldControl">foldControl</a></code>) defining the folds of the data for
(repeated) <code class="reqn">K</code>-fold cross-validation, an object of class
<code>"randomSplits"</code> (as returned by <code><a href="#topic+randomSplits">randomSplits</a></code>) or a
control object of class <code>"splitControl"</code> (see
<code><a href="#topic+splitControl">splitControl</a></code>) defining random data splits, or an object of
class <code>"bootSamples"</code> (as returned by <code><a href="#topic+bootSamples">bootSamples</a></code>) or a
control object of class <code>"bootControl"</code> (see <code><a href="#topic+bootControl">bootControl</a></code>)
defining bootstrap samples.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_predictfun">predictFun</code></td>
<td>
<p>a function to compute predictions for the test data.  It
should expect the fitted model to be passed as the first argument and the test
data as the second argument, and must return either a vector or a matrix
containing the predicted values.  The default is to use the
<code><a href="stats.html#topic+predict">predict</a></code> method of the fitted model.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_predictargs">predictArgs</code></td>
<td>
<p>a list of additional arguments to be passed to
<code>predictFun</code>.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_cost">cost</code></td>
<td>
<p>a cost function measuring prediction loss.  It should expect
the observed values of the response to be passed as the first argument and
the predicted values as the second argument, and must return either a
non-negative scalar value, or a list with the first component containing
the prediction error and the second component containing the standard
error.  The default is to use the root mean squared prediction error
(see <code><a href="#topic+cost">cost</a></code>).</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_costargs">costArgs</code></td>
<td>
<p>a list of additional arguments to be passed to the
prediction loss function <code>cost</code>.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_selectbest">selectBest</code></td>
<td>
<p>a character string specifying a criterion for selecting
the best model.  Possible values are <code>"min"</code> (the default) or
<code>"hastie"</code>.  The former selects the model with the smallest prediction
error.  The latter is useful for models with a tuning parameter controlling
the complexity of the model (e.g., penalized regression).  It selects the
most parsimonious model whose prediction error is no larger than
<code>seFactor</code> standard errors above the prediction error of the best
overall model.  Note that the models are thereby assumed to be ordered
from the most parsimonious one to the most complex one.  In particular
a one-standard-error rule is frequently applied.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_sefactor">seFactor</code></td>
<td>
<p>a numeric value giving a multiplication factor of the
standard error for the selection of the best model.  This is ignored if
<code>selectBest</code> is <code>"min"</code>.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_final">final</code></td>
<td>
<p>a logical indicating whether to fit the final model with the
optimal combination of tuning parameters.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_names">names</code></td>
<td>
<p>an optional character vector giving names for the arguments
containing the data to be used in the function call (see &ldquo;Details&rdquo;).</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_envir">envir</code></td>
<td>
<p>the <code><a href="base.html#topic+environment">environment</a></code> in which to evaluate the
function call for fitting the models (see <code><a href="base.html#topic+eval">eval</a></code>).</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_ncores">ncores</code></td>
<td>
<p>a positive integer giving the number of processor cores to be
used for parallel computing (the default is 1 for no parallelization).  If
this is set to <code>NA</code>, all available processor cores are used.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_cl">cl</code></td>
<td>
<p>a <span class="pkg">parallel</span> cluster for parallel computing as generated by
<code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>.  If supplied, this is preferred over
<code>ncores</code>.</p>
</td></tr>
<tr><td><code id="perryTuning_+3A_seed">seed</code></td>
<td>
<p>optional initial seed for the random number generator (see
<code><a href="base.html#topic+.Random.seed">.Random.seed</a></code>).  Note that also in case of parallel computing,
resampling is performed on the manager process rather than the worker
processes. On the parallel worker processes, random number streams are
used and the seed is set via <code><a href="parallel.html#topic+clusterSetRNGStream">clusterSetRNGStream</a></code> for
reproducibility in case the model fitting function involves randomness.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>(Repeated) <code class="reqn">K</code>-fold cross-validation is performed in the following
way.  The data are first split into <code class="reqn">K</code> previously obtained blocks of
approximately equal size (given by <code>folds</code>).  Each of the <code class="reqn">K</code> data
blocks is left out once to fit the model, and predictions are computed for
the observations in the left-out block with <code>predictFun</code>.  Thus a
prediction is obtained for each observation.  The response and the obtained
predictions for all observations are then passed to the prediction loss
function <code>cost</code> to estimate the prediction error.  For repeated
<code class="reqn">K</code>-fold cross-validation (as indicated by <code>splits</code>), this process
is replicated and the estimated prediction errors from all replications are
returned.
</p>
<p>(Repeated) random splitting is performed similarly.  In each replication,
the data are split into a training set and a test set at random.  Then the
training data are used to fit the model, and predictions are computed for
the test data.  Hence only the response values from the test data and the
corresponding predictions are passed to the prediction loss function
<code>cost</code>.
</p>
<p>For the bootstrap estimator, each bootstrap sample is used as training data
to fit the model.  The out-of-bag estimator uses the observations that do
not enter the bootstrap sample as test data and computes the prediction loss
function <code>cost</code> for those out-of-bag observations.  The 0.632 estimator
is computed as a linear combination of the out-of-bag estimator and the
prediction loss of the fitted values of the model computed from the full
sample.
</p>
<p>In any case, if the response is a vector but <code>predictFun</code> returns a
matrix, the prediction error is computed for each column.  A typical use
case for this behavior would be if <code>predictFun</code> returns predictions
from an initial model fit and stepwise improvements thereof.
</p>
<p>If <code>formula</code> or <code>data</code> are supplied, all variables required for
fitting the models are added as one argument to the function call, which is
the typical behavior of model fitting functions with a
<code><a href="stats.html#topic+formula">formula</a></code> interface.  In this case, the accepted values
for <code>names</code> depend on the method.  For the <code>function</code> method, a
character vector of length two should supplied, with the first element
specifying the argument name for the formula and the second element
specifying the argument name for the data (the default is to use
<code>c("formula", "data")</code>).  Note that names for both arguments should be
supplied even if only one is actually used.  For the <code>call</code> method,
which does not have a <code>formula</code> argument, a character string specifying
the argument name for the data should be supplied (the default is to use
<code>"data"</code>).
</p>
<p>If <code>x</code> is supplied, on the other hand, the predictor matrix and the
response are added as separate arguments to the function call.  In this
case, <code>names</code> should be a character vector of length two, with the
first element specifying the argument name for the predictor matrix and the
second element specifying the argument name for the response (the default is
to use <code>c("x", "y")</code>).  It should be noted that the <code>formula</code> or
<code>data</code> arguments take precedence over <code>x</code>.
</p>


<h3>Value</h3>

<p>If <code>tuning</code> is an empty list, <code><a href="#topic+perryFit">perryFit</a></code> is called to
return an object of class <code>"perry"</code>.
</p>
<p>Otherwise an object of class <code>"perryTuning"</code> (which inherits from class
<code>"perrySelect"</code>) with the following components is returned:
</p>

<dl>
<dt><code>pe</code></dt><dd><p>a data frame containing the estimated prediction errors
for all combinations of tuning parameter values.  In case of more than one
replication, those are average values over all replications.</p>
</dd>
<dt><code>se</code></dt><dd><p>a data frame containing the estimated standard errors of
the prediction loss for all combinations of tuning parameter values.</p>
</dd>
<dt><code>reps</code></dt><dd><p>a data frame containing the estimated prediction
errors from all replications for all combinations of tuning parameter
values.  This is only returned in case of more than one replication.</p>
</dd>
<dt><code>splits</code></dt><dd><p>an object giving the data splits used to estimate
the prediction error.</p>
</dd>
<dt><code>y</code></dt><dd><p>the response.</p>
</dd>
<dt><code>yHat</code></dt><dd><p>a list containing the predicted values for all
combinations of tuning parameter values.  Each list component is again a
list containing the corresponding predicted values from all replications.</p>
</dd>
<dt><code>best</code></dt><dd><p>an integer vector giving the indices of the optimal
combinations of tuning parameters.</p>
</dd>
<dt><code>selectBest</code></dt><dd><p>a character string specifying the criterion used
for selecting the best model.</p>
</dd>
<dt><code>seFactor</code></dt><dd><p>a numeric value giving the multiplication factor of
the standard error used for the selection of the best model.</p>
</dd>
<dt><code>tuning</code></dt><dd><p>a data frame containing the grid of tuning parameter
values for which the prediction error was estimated.</p>
</dd>
<dt><code>finalModel</code></dt><dd><p>the final model fit with the optimal combination
of tuning parameters.  This is only returned if argument <code>final</code> is
<code>TRUE</code>.</p>
</dd>
<dt><code>call</code></dt><dd><p>the matched function call.</p>
</dd>
</dl>



<h3>Note</h3>

<p>The same data splits are used for all combinations of tuning parameter
values for maximum comparability.
</p>
<p>If a final model with the optimal combination of tuning parameters is
computed, class <code>"perryTuning"</code> inherits the <code>coef()</code>,
<code>fitted()</code>, <code>predict()</code> and <code>residuals()</code> methods from
its component <code>finalModel</code>.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>References</h3>

<p>Hastie, T., Tibshirani, R. and Friedman, J. (2009) <em>The Elements of
Statistical Learning: Data Mining, Inference, and Prediction</em>.  Springer,
2nd edition.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perrySelect">perrySelect</a></code>,
<code><a href="#topic+cvFolds">cvFolds</a></code>, <code><a href="#topic+randomSplits">randomSplits</a></code>,
<code><a href="#topic+bootSamples">bootSamples</a></code>, <code><a href="#topic+cost">cost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")

## evaluate MM regression models tuned for 85% and 95% efficiency
tuning &lt;- list(tuning.psi = c(3.443689, 4.685061))

## via model fitting function
# perform cross-validation
# note that the response is extracted from 'data' in
# this example and does not have to be supplied
perryTuning(lmrob, formula = Y ~ ., data = coleman,
            tuning = tuning, splits = foldControl(K = 5, R = 10),
            cost = rtmspe, costArgs = list(trim = 0.1), seed = 1234)

## via function call
# set up function call
call &lt;- call("lmrob", formula = Y ~ .)
# perform cross-validation
perryTuning(call, data = coleman, y = coleman$Y,
            tuning = tuning, splits = foldControl(K = 5, R = 10),
            cost = rtmspe, costArgs = list(trim = 0.1), seed = 1234)
</code></pre>

<hr>
<h2 id='randomSplits'>Random data splits</h2><span id='topic+randomSplits'></span><span id='topic+print.randomSplits'></span>

<h3>Description</h3>

<p>Split observations or groups of observations into training and test data to
be used for (repeated) random splitting (also known as random subsampling or
Monte Carlo cross-validation).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randomSplits(n, m, R = 1, grouping = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randomSplits_+3A_n">n</code></td>
<td>
<p>an integer giving the number of observations to be split into
training and test data.  This is ignored if <code>grouping</code> is supplied in
order to split groups of observations into folds.</p>
</td></tr>
<tr><td><code id="randomSplits_+3A_m">m</code></td>
<td>
<p>an integer giving the number of observations or groups of
observations to be used as test data.</p>
</td></tr>
<tr><td><code id="randomSplits_+3A_r">R</code></td>
<td>
<p>an integer giving the number of random data splits.</p>
</td></tr>
<tr><td><code id="randomSplits_+3A_grouping">grouping</code></td>
<td>
<p>a factor specifying groups of observations.  If supplied,
the data are split according to the groups rather than individual
observations such that all observations within a group belong either to the
training or test data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"randomSplits"</code> with the following
components:
</p>

<dl>
<dt><code>n</code></dt><dd><p>an integer giving the number of observations or groups.</p>
</dd>
<dt><code>m</code></dt><dd><p>an integer giving the number of observations or groups in
the test data.</p>
</dd>
<dt><code>R</code></dt><dd><p>an integer giving the number of random data splits.</p>
</dd>
<dt><code>subsets</code></dt><dd><p>an integer matrix in which each column contains
the indices of the observations or groups in the test data of the
corresponding random data split.</p>
</dd>
<dt><code>grouping</code></dt><dd><p>a list giving the indices of the observations
belonging to each group.  This is only returned if a grouping factor
has been supplied.</p>
</dd>
</dl>



<h3>Note</h3>

<p>This is a simple wrapper function for <code><a href="#topic+perrySplits">perrySplits</a></code> with a
control object generated by <code><a href="#topic+splitControl">splitControl</a></code>.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perrySplits">perrySplits</a></code>, <code><a href="#topic+splitControl">splitControl</a></code>,
<code><a href="#topic+cvFolds">cvFolds</a></code>, <code><a href="#topic+bootSamples">bootSamples</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234)  # set seed for reproducibility
randomSplits(20, m = 5)
randomSplits(20, m = 5, R = 10)

</code></pre>

<hr>
<h2 id='reperry'>Recompute resampling-based prediction error measures</h2><span id='topic+reperry'></span><span id='topic+reperry.perry'></span><span id='topic+reperry.perrySelect'></span>

<h3>Description</h3>

<p>Recompute prediction error measures for previously obtained objects that 
contain resampling-based prediction error results.  This is useful for 
computing a different measure of prediction loss.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reperry(object, ...)

## S3 method for class 'perry'
reperry(object, cost = rmspe, ...)

## S3 method for class 'perrySelect'
reperry(object, cost = rmspe, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reperry_+3A_object">object</code></td>
<td>
<p>an object inheriting from class <code>"perry"</code> or 
<code>"perrySelect"</code> that contains prediction error results.</p>
</td></tr>
<tr><td><code id="reperry_+3A_...">...</code></td>
<td>
<p>for the generic function, additional arguments to be passed 
down to methods.  For the methods,  additional arguments to be passed to the 
prediction loss function <code>cost</code>.</p>
</td></tr>
<tr><td><code id="reperry_+3A_cost">cost</code></td>
<td>
<p>a cost function measuring prediction loss.  It should expect 
the observed values of the response to be passed as the first argument and 
the predicted values as the second argument, and must return either a 
non-negative scalar value, or a list with the first component containing 
the prediction error and the second component containing the standard 
error.  The default is to use the root mean squared prediction error 
(see <code><a href="#topic+cost">cost</a></code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object similar to <code>object</code> containing the results for the 
new measure of prediction loss.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perryTuning">perryTuning</a></code>, 
<code><a href="#topic+perrySelect">perrySelect</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")
set.seed(1234)  # set seed for reproducibility

## set up folds for cross-validation
folds &lt;- cvFolds(nrow(coleman), K = 5, R = 10)

## compare raw and reweighted LTS estimators for 50% and 75%
## subsets based on their RTMSPE with 25% trimming

# 50% subsets
fit50 &lt;- ltsReg(Y ~ ., data = coleman, alpha = 0.5)
cv50 &lt;- perry(fit50, splits = folds, fit = "both",
              cost = rtmspe, trim = 0.25)

# 75% subsets
fit75 &lt;- ltsReg(Y ~ ., data = coleman, alpha = 0.75)
cv75 &lt;- perry(fit75, splits = folds, fit = "both",
              cost = rtmspe, trim = 0.25)

# combine results into one object
cv &lt;- perrySelect("0.5" = cv50, "0.75" = cv75)
cv

## recompute the RTMSPE with 10% trimming
reperry(cv50, cost = rtmspe, trim = 0.1)
reperry(cv, cost = rtmspe, trim = 0.1)
</code></pre>

<hr>
<h2 id='setupPerryPlot'>Set up a plot of resampling-based prediction error results</h2><span id='topic+setupPerryPlot'></span><span id='topic+setupPerryPlot.perry'></span><span id='topic+setupPerryPlot.perrySelect'></span><span id='topic+setupPerryPlot.perryTuning'></span>

<h3>Description</h3>

<p>Extract and prepare the relevant information for a plot of results of
resampling-based prediction error measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setupPerryPlot(object, ...)

## S3 method for class 'perry'
setupPerryPlot(
  object,
  which = c("box", "density", "dot"),
  select = NULL,
  seFactor = NA,
  ...
)

## S3 method for class 'perrySelect'
setupPerryPlot(
  object,
  which = c("box", "density", "dot", "line"),
  subset = NULL,
  select = NULL,
  seFactor = object$seFactor,
  ...
)

## S3 method for class 'perryTuning'
setupPerryPlot(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setupPerryPlot_+3A_object">object</code></td>
<td>
<p>an object inheriting from class <code>"perry"</code> or
<code>"perrySelect"</code> that contains prediction error results.</p>
</td></tr>
<tr><td><code id="setupPerryPlot_+3A_...">...</code></td>
<td>
<p>for the <code>"perryTuning"</code> method, additional arguments to
be passed down to the <code>"perrySelect"</code> method.  For the other methods,
additional arguments are currently ignored.</p>
</td></tr>
<tr><td><code id="setupPerryPlot_+3A_which">which</code></td>
<td>
<p>a character string specifying the type of plot to
prepare.  Possible values are <code>"box"</code> to prepare a box plot,
<code>"density"</code> to prepare a smooth density plot, <code>"dot"</code> to prepare
a dot plot, or <code>"line"</code> to prepare a plot of the (average) results for
each model as a connected line (for objects inheriting from class
<code>"perrySelect"</code>).  Note that the first two plots are only meaningful
in case of repeated resampling.  The default is to use <code>"box"</code> in case
of repeated resampling and <code>"dot"</code> otherwise.</p>
</td></tr>
<tr><td><code id="setupPerryPlot_+3A_select">select</code></td>
<td>
<p>a character, integer or logical vector indicating the columns
of prediction error results to be prepared for plotting.</p>
</td></tr>
<tr><td><code id="setupPerryPlot_+3A_sefactor">seFactor</code></td>
<td>
<p>a numeric value giving the multiplication factor of the
standard error for displaying error bars in dot plots or line plots.  Error
bars in those plots can be suppressed by setting this to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="setupPerryPlot_+3A_subset">subset</code></td>
<td>
<p>a character, integer or logical vector indicating the subset
of models to be prepared for plotting.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"setupPerryPlot"</code> with the following
components:
</p>

<dl>
<dt><code>data</code></dt><dd><p>a data frame containing the following columns:
</p>

<dl>
<dt><code>Fit</code></dt><dd><p>a vector or factor containing the identifiers of the
models.</p>
</dd>
<dt><code>Name</code></dt><dd><p>a factor containing the names of the predictor error
results (not returned in case of only one column of prediction error
results with the default name).</p>
</dd>
<dt><code>PE</code></dt><dd><p>the estimated prediction errors.</p>
</dd>
<dt><code>Lower</code></dt><dd><p>the lower end points of the error bars (only
returned if possible to compute).</p>
</dd>
<dt><code>Upper</code></dt><dd><p>the upper end points of the error bars (only
returned if possible to compute).</p>
</dd>
</dl>

</dd>
<dt><code>which</code></dt><dd><p>a character string specifying the type of plot.</p>
</dd>
<dt><code>grouped</code></dt><dd><p>a logical indicating whether density plots should
be grouped due to multiple model fits (only returned in case of density
plots for the <code>"perrySelect"</code> and <code>"perryTuning"</code> methods).</p>
</dd>
<dt><code>includeSE</code></dt><dd><p>a logical indicating whether error bars based on
standard errors are available (only returned in case of dot plots or line
plots).</p>
</dd>
<dt><code>mapping</code></dt><dd><p>default aesthetic mapping for the plots.</p>
</dd>
<dt><code>facets</code></dt><dd><p>default faceting formula for the plots (not returned
in case of only one column of prediction error results with the default
name).</p>
</dd>
<dt><code>tuning</code></dt><dd><p>a data frame containing the grid of tuning parameter
values for which the prediction error was estimated (only returned for the
<code>"perryTuning"</code> method).</p>
</dd>
</dl>



<h3>Note</h3>

<p>Duplicate indices in <code>subset</code> or <code>select</code> are removed such
that all models and prediction error results are unique.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryPlot">perryPlot</a></code>,
</p>
<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perrySelect">perrySelect</a></code>,
<code><a href="#topic+perryTuning">perryTuning</a></code>,
</p>
<p><code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>, <code><a href="ggplot2.html#topic+autoplot">autoplot</a></code>,
<code><a href="graphics.html#topic+plot">plot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")
set.seed(1234)  # set seed for reproducibility

## set up folds for cross-validation
folds &lt;- cvFolds(nrow(coleman), K = 5, R = 10)

## compare LS, MM and LTS regression

# perform cross-validation for an LS regression model
fitLm &lt;- lm(Y ~ ., data = coleman)
cvLm &lt;- perry(fitLm, splits = folds,
              cost = rtmspe, trim = 0.1)

# perform cross-validation for an MM regression model
fitLmrob &lt;- lmrob(Y ~ ., data = coleman, maxit.scale = 500)
cvLmrob &lt;- perry(fitLmrob, splits = folds,
                 cost = rtmspe, trim = 0.1)

# perform cross-validation for an LTS regression model
fitLts &lt;- ltsReg(Y ~ ., data = coleman)
cvLts &lt;- perry(fitLts, splits = folds,
               cost = rtmspe, trim = 0.1)

# combine results into one object
cv &lt;- perrySelect(LS = cvLm, MM = cvLmrob, LTS = cvLts,
                  .selectBest = "min")
cv

## convert MM regression results to data frame for plotting
# all replications for box plot
cvLmrobBox &lt;- setupPerryPlot(cvLmrob, which = "box")
perryPlot(cvLmrobBox)
# aggregated results for dot plot
cvLmrobDot &lt;- setupPerryPlot(cvLmrob, which = "dot", seFactor = 2)
perryPlot(cvLmrobDot)

## convert combined results to data frame for plotting
# all replications for box plot
cvBox &lt;- setupPerryPlot(cv, which = "box")
perryPlot(cvBox)
# aggregated results for dot plot
cvDot &lt;- setupPerryPlot(cv, which = "dot", seFactor = 2)
perryPlot(cvDot)
</code></pre>

<hr>
<h2 id='splitControl'>Control object for random data splits</h2><span id='topic+splitControl'></span>

<h3>Description</h3>

<p>Generate an object that controls how to split <code class="reqn">n</code> observations or
groups of observations into training and test data to be used for (repeated)
random splitting (also known as random subsampling or Monte Carlo
cross-validation).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitControl(m, R = 1, grouping = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="splitControl_+3A_m">m</code></td>
<td>
<p>an integer giving the number of observations or groups of
observations to be used as test data.</p>
</td></tr>
<tr><td><code id="splitControl_+3A_r">R</code></td>
<td>
<p>an integer giving the number of random data splits.</p>
</td></tr>
<tr><td><code id="splitControl_+3A_grouping">grouping</code></td>
<td>
<p>a factor specifying groups of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"splitControl"</code> with the following
components:
</p>

<dl>
<dt><code>m</code></dt><dd><p>an integer giving the number of observations or groups of
observations to be used as test data.</p>
</dd>
<dt><code>R</code></dt><dd><p>an integer giving the number of random data splits.</p>
</dd>
<dt><code>grouping</code></dt><dd><p>if supplied, a factor specifying groups of
observations.  The data will then be split according to the groups rather
than individual observations such that all observations within a group
belong either to the training or test data.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perrySplits">perrySplits</a></code>, <code><a href="#topic+randomSplits">randomSplits</a></code>,
<code><a href="#topic+foldControl">foldControl</a></code>, <code><a href="#topic+bootControl">bootControl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234)  # set seed for reproducibility
perrySplits(20, splitControl(m = 5))
perrySplits(20, splitControl(m = 5, R = 10))

</code></pre>

<hr>
<h2 id='subset.perry'>Subsetting resampling-based prediction error results</h2><span id='topic+subset.perry'></span><span id='topic+subset.perrySelect'></span>

<h3>Description</h3>

<p>Extract subsets of resampling-based prediction error results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'perry'
subset(x, select = NULL, ...)

## S3 method for class 'perrySelect'
subset(x, subset = NULL, select = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subset.perry_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>"perry"</code> or 
<code>"perrySelect"</code> that contains prediction error results.</p>
</td></tr>
<tr><td><code id="subset.perry_+3A_select">select</code></td>
<td>
<p>a character, integer or logical vector indicating the 
prediction error results to be extracted.</p>
</td></tr>
<tr><td><code id="subset.perry_+3A_...">...</code></td>
<td>
<p>currently ignored.</p>
</td></tr>
<tr><td><code id="subset.perry_+3A_subset">subset</code></td>
<td>
<p>a character, integer or logical vector indicating the subset 
of models for which to keep the prediction error results.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object similar to <code>x</code> containing just the selected results.
</p>


<h3>Note</h3>

<p>Duplicate indices in <code>subset</code> or <code>select</code> are removed such 
that all models and prediction error results are unique.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perrySelect">perrySelect</a></code>, 
<code><a href="#topic+perryTuning">perryTuning</a></code>, <code><a href="base.html#topic+subset">subset</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")
set.seed(1234)  # set seed for reproducibility

## set up folds for cross-validation
folds &lt;- cvFolds(nrow(coleman), K = 5, R = 10)

## compare raw and reweighted LTS estimators for
## 50% and 75% subsets

# 50% subsets
fit50 &lt;- ltsReg(Y ~ ., data = coleman, alpha = 0.5)
cv50 &lt;- perry(fit50, splits = folds, fit = "both",
              cost = rtmspe, trim = 0.1)

# 75% subsets
fit75 &lt;- ltsReg(Y ~ ., data = coleman, alpha = 0.75)
cv75 &lt;- perry(fit75, splits = folds, fit = "both",
              cost = rtmspe, trim = 0.1)

# combine results into one object
cv &lt;- perrySelect("0.5" = cv50, "0.75" = cv75)
cv

# extract reweighted LTS results with 50% subsets
subset(cv50, select = "reweighted")
subset(cv, subset = c(TRUE, FALSE), select = "reweighted")
</code></pre>

<hr>
<h2 id='summary.perry'>Summarize resampling-based prediction error results</h2><span id='topic+summary.perry'></span><span id='topic+summary.perrySelect'></span><span id='topic+summary.perryTuning'></span>

<h3>Description</h3>

<p>Produce a summary of resampling-based prediction error results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'perry'
summary(object, ...)

## S3 method for class 'perrySelect'
summary(object, ...)

## S3 method for class 'perryTuning'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.perry_+3A_object">object</code></td>
<td>
<p>an object inheriting from class <code>"perry"</code> or 
<code>"perrySelect"</code> that contains prediction error results (note that the 
latter includes objects of class <code>"perryTuning"</code>).</p>
</td></tr>
<tr><td><code id="summary.perry_+3A_...">...</code></td>
<td>
<p>currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"summary.perry"</code>, <code>"summary.perrySelect"</code> or 
<code>"summary.perryTuning"</code>, depending on the class of <code>object</code>.
</p>


<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+perryFit">perryFit</a></code>, <code><a href="#topic+perrySelect">perrySelect</a></code>, 
<code><a href="#topic+perryTuning">perryTuning</a></code>, <code><a href="base.html#topic+summary">summary</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("perryExamples")
data("coleman")
set.seed(1234)  # set seed for reproducibility

## set up folds for cross-validation
folds &lt;- cvFolds(nrow(coleman), K = 5, R = 10)

## compare raw and reweighted LTS estimators for
## 50% and 75% subsets

# 50% subsets
fit50 &lt;- ltsReg(Y ~ ., data = coleman, alpha = 0.5)
cv50 &lt;- perry(fit50, splits = folds, fit = "both",
              cost = rtmspe, trim = 0.1)

# 75% subsets
fit75 &lt;- ltsReg(Y ~ ., data = coleman, alpha = 0.75)
cv75 &lt;- perry(fit75, splits = folds, fit = "both",
              cost = rtmspe, trim = 0.1)

# combine results into one object
cv &lt;- perrySelect("0.5" = cv50, "0.75" = cv75)
cv

# summary of the results with the 50% subsets
summary(cv50)
# summary of the combined results
summary(cv)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
