<!DOCTYPE html><html><head><title>Help for package rsparse</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rsparse}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#detect_number_omp_threads'><p>Detects number of OpenMP threads in the system</p></a></li>
<li><a href='#FactorizationMachine'><p>Second order Factorization Machines</p></a></li>
<li><a href='#FTRL'><p>Logistic regression model with FTRL proximal SGD solver.</p></a></li>
<li><a href='#GloVe'><p>Global Vectors</p></a></li>
<li><a href='#LinearFlow'><p>Linear-FLow model for one-class collaborative filtering</p></a></li>
<li><a href='#MatrixFactorizationRecommender'><p>Base class for matrix factorization recommenders</p></a></li>
<li><a href='#metrics'><p>Ranking Metrics for Top-K Items</p></a></li>
<li><a href='#movielens100k'><p>MovieLens 100K Dataset</p></a></li>
<li><a href='#PureSVD'><p>PureSVD recommender model decompomposition</p></a></li>
<li><a href='#ScaleNormalize'><p>Re-scales input matrix proportinally to item popularity</p></a></li>
<li><a href='#soft_impute'><p>SoftImpute/SoftSVD matrix factorization</p></a></li>
<li><a href='#train_test_split'><p>Creates cross-validation set from user-item interactions</p></a></li>
<li><a href='#WRMF'><p>Weighted Regularized Matrix Factorization for collaborative filtering</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Statistical Learning on Sparse Matrices</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dmitriy Selivanov &lt;ds@rexy.ai&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements many algorithms for statistical learning on 
  sparse matrices - matrix factorizations, matrix completion, 
  elastic net regressions, factorization machines. 
  Also 'rsparse' enhances 'Matrix' package by providing methods for 
  multithreaded &lt;sparse, dense&gt; matrix products and native slicing of 
  the sparse matrices in Compressed Sparse Row (CSR) format.
  List of the algorithms for regression problems:
  1) Elastic Net regression via Follow The Proximally-Regularized Leader (FTRL) 
  Stochastic Gradient Descent (SGD), as per McMahan et al(, &lt;<a href="https://doi.org/10.1145%2F2487575.2488200">doi:10.1145/2487575.2488200</a>&gt;)
  2) Factorization Machines via SGD, as per Rendle (2010, &lt;<a href="https://doi.org/10.1109%2FICDM.2010.127">doi:10.1109/ICDM.2010.127</a>&gt;)
  List of algorithms for matrix factorization and matrix completion:
  1) Weighted Regularized Matrix Factorization (WRMF) via Alternating Least 
  Squares (ALS) - paper by Hu, Koren, Volinsky (2008, &lt;<a href="https://doi.org/10.1109%2FICDM.2008.22">doi:10.1109/ICDM.2008.22</a>&gt;)
  2) Maximum-Margin Matrix Factorization via ALS, paper by Rennie, Srebro 
  (2005, &lt;<a href="https://doi.org/10.1145%2F1102351.1102441">doi:10.1145/1102351.1102441</a>&gt;)
  3) Fast Truncated Singular Value Decomposition (SVD), Soft-Thresholded SVD, 
  Soft-Impute matrix completion via ALS - paper by Hastie, Mazumder 
  et al. (2014, &lt;<a href="https://arxiv.org/abs/1410.2596">arXiv:1410.2596</a>&gt;)
  4) Linear-Flow matrix factorization, from 'Practical linear models for 
  large-scale one-class collaborative filtering' by Sedhain, Bui, Kawale et al 
  (2016, ISBN:978-1-57735-770-4)
  5) GlobalVectors (GloVe) matrix factorization via SGD, paper by Pennington, 
  Socher, Manning (2014, <a href="https://aclanthology.org/D14-1162/">https://aclanthology.org/D14-1162/</a>)
  Package is reasonably fast and memory efficient - it allows to work with large
  datasets - millions of rows and millions of columns. This is particularly useful 
  for practitioners working on recommender systems.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0), methods, Matrix (&ge; 1.3)</td>
</tr>
<tr>
<td>Imports:</td>
<td>MatrixExtra (&ge; 0.1.7), Rcpp (&ge; 0.11), data.table (&ge;
1.10.0), float (&ge; 0.2-2), RhpcBLASctl, lgr (&ge; 0.2)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo (&ge; 0.9.100.5.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, covr</td>
</tr>
<tr>
<td>StagedInstall:</td>
<td>TRUE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/rexyai/rsparse">https://github.com/rexyai/rsparse</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/rexyai/rsparse/issues">https://github.com/rexyai/rsparse/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-09-11 03:05:04 UTC; dselivanov</td>
</tr>
<tr>
<td>Author:</td>
<td>Dmitriy Selivanov <a href="https://orcid.org/0000-0001-5413-1506"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph],
  David Cortes [ctb],
  Drew Schmidt [ctb] (configure script for BLAS, LAPACK detection),
  Wei-Chen Chen [ctb] (configure script and work on linking to float
    package)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-11 22:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='detect_number_omp_threads'>Detects number of OpenMP threads in the system</h2><span id='topic+detect_number_omp_threads'></span>

<h3>Description</h3>

<p>Detects number of OpenMP threads in the system respecting environment
variables such as <code>OMP_NUM_THREADS</code> and <code>OMP_THREAD_LIMIT</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>detect_number_omp_threads()
</code></pre>

<hr>
<h2 id='FactorizationMachine'>Second order Factorization Machines</h2><span id='topic+FactorizationMachine'></span>

<h3>Description</h3>

<p>Creates second order Factorization Machines model
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-FactorizationMachine-new"><code>FactorizationMachine$new()</code></a>
</p>
</li>
<li> <p><a href="#method-FactorizationMachine-partial_fit"><code>FactorizationMachine$partial_fit()</code></a>
</p>
</li>
<li> <p><a href="#method-FactorizationMachine-fit"><code>FactorizationMachine$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-FactorizationMachine-predict"><code>FactorizationMachine$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-FactorizationMachine-clone"><code>FactorizationMachine$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-FactorizationMachine-new"></a>



<h4>Method <code>new()</code></h4>

<p>creates Creates second order Factorization Machines model
</p>


<h5>Usage</h5>

<div class="r"><pre>FactorizationMachine$new(
  learning_rate_w = 0.2,
  rank = 4,
  lambda_w = 0,
  lambda_v = 0,
  family = c("binomial", "gaussian"),
  intercept = TRUE,
  learning_rate_v = learning_rate_w
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>learning_rate_w</code></dt><dd><p>learning rate for features intercations</p>
</dd>
<dt><code>rank</code></dt><dd><p>dimension of the latent dimensions which models features interactions</p>
</dd>
<dt><code>lambda_w</code></dt><dd><p>regularization for features interactions</p>
</dd>
<dt><code>lambda_v</code></dt><dd><p>regularization for features</p>
</dd>
<dt><code>family</code></dt><dd><p>one of <code>"binomial", "gaussian"</code></p>
</dd>
<dt><code>intercept</code></dt><dd><p>logical, indicates whether or not include intecept to the model</p>
</dd>
<dt><code>learning_rate_v</code></dt><dd><p>learning rate for features</p>
</dd>
</dl>

</div>


<hr>
<a id="method-FactorizationMachine-partial_fit"></a>



<h4>Method <code>partial_fit()</code></h4>

<p>fits/updates model
</p>


<h5>Usage</h5>

<div class="r"><pre>FactorizationMachine$partial_fit(x, y, weights = rep(1, length(y)), ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input sparse matrix. Native format is <code>Matrix::RsparseMatrix</code>.
If <code>x</code> is in different format, model will try to convert it to <code>RsparseMatrix</code>
with <code>as(x, "RsparseMatrix")</code>. Dimensions should be (n_samples, n_features)</p>
</dd>
<dt><code>y</code></dt><dd><p>vector of targets</p>
</dd>
<dt><code>weights</code></dt><dd><p>numeric vector of length 'n_samples'. Defines how to amplify SGD updates
for each sample. May be useful for highly unbalanced problems.</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-FactorizationMachine-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>shorthand for applying 'partial_fit' 'n_iter' times
</p>


<h5>Usage</h5>

<div class="r"><pre>FactorizationMachine$fit(x, y, weights = rep(1, length(y)), n_iter = 1L, ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input sparse matrix. Native format is <code>Matrix::RsparseMatrix</code>.
If <code>x</code> is in different format, model will try to convert it to <code>RsparseMatrix</code>
with <code>as(x, "RsparseMatrix")</code>. Dimensions should be (n_samples, n_features)</p>
</dd>
<dt><code>y</code></dt><dd><p>vector of targets</p>
</dd>
<dt><code>weights</code></dt><dd><p>numeric vector of length 'n_samples'. Defines how to amplify SGD updates
for each sample. May be useful for highly unbalanced problems.</p>
</dd>
<dt><code>n_iter</code></dt><dd><p>number of SGD epochs</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-FactorizationMachine-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>makes predictions based on fitted model
</p>


<h5>Usage</h5>

<div class="r"><pre>FactorizationMachine$predict(x, ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input sparse matrix of shape <em>(n_samples, n_featires)</em></p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-FactorizationMachine-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>FactorizationMachine$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'># Factorization Machines can fit XOR function!
x = rbind(
  c(0, 0),
  c(0, 1),
  c(1, 0),
  c(1, 1)
)
y = c(0, 1, 1, 0)

x = as(x, "RsparseMatrix")
fm = FactorizationMachine$new(learning_rate_w = 10, rank = 2, lambda_w = 0,
  lambda_v = 0, family = 'binomial', intercept = TRUE)
res = fm$fit(x, y, n_iter = 100)
preds = fm$predict(x)
all(preds[c(1, 4)] &lt; 0.01)
all(preds[c(2, 3)] &gt; 0.99)
</code></pre>

<hr>
<h2 id='FTRL'>Logistic regression model with FTRL proximal SGD solver.</h2><span id='topic+FTRL'></span>

<h3>Description</h3>

<p>Creates 'Follow the Regularized Leader' model.
Only logistic regression implemented at the moment.
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-FTRL-new"><code>FTRL$new()</code></a>
</p>
</li>
<li> <p><a href="#method-FTRL-partial_fit"><code>FTRL$partial_fit()</code></a>
</p>
</li>
<li> <p><a href="#method-FTRL-fit"><code>FTRL$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-FTRL-predict"><code>FTRL$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-FTRL-coef"><code>FTRL$coef()</code></a>
</p>
</li>
<li> <p><a href="#method-FTRL-clone"><code>FTRL$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-FTRL-new"></a>



<h4>Method <code>new()</code></h4>

<p>creates a model
</p>


<h5>Usage</h5>

<div class="r"><pre>FTRL$new(
  learning_rate = 0.1,
  learning_rate_decay = 0.5,
  lambda = 0,
  l1_ratio = 1,
  dropout = 0,
  family = c("binomial")
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>learning_rate</code></dt><dd><p>learning rate</p>
</dd>
<dt><code>learning_rate_decay</code></dt><dd><p>learning rate which controls decay. Please refer to FTRL proximal
paper for details. Usually convergense does not heavily depend on this parameter,
so default value 0.5 is safe.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>regularization parameter</p>
</dd>
<dt><code>l1_ratio</code></dt><dd><p>controls L1 vs L2 penalty mixing.
1 = Lasso regression, 0 = Ridge regression. Elastic net is in between</p>
</dd>
<dt><code>dropout</code></dt><dd><p>dropout - percentage of random features to
exclude from each sample. Acts as regularization.</p>
</dd>
<dt><code>family</code></dt><dd><p>a description of the error distribution and link function to be used in
the model. Only <code>binomial</code> (logistic regression) is implemented at the moment.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-FTRL-partial_fit"></a>



<h4>Method <code>partial_fit()</code></h4>

<p>fits model to the data
</p>


<h5>Usage</h5>

<div class="r"><pre>FTRL$partial_fit(x, y, weights = rep(1, length(y)), ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input sparse matrix. Native format is <code>Matrix::RsparseMatrix</code>.
If <code>x</code> is in different format, model will try to convert it to <code>RsparseMatrix</code>
with <code>as(x, "RsparseMatrix")</code>. Dimensions should be (n_samples, n_features)</p>
</dd>
<dt><code>y</code></dt><dd><p>vector of targets</p>
</dd>
<dt><code>weights</code></dt><dd><p>numeric vector of length 'n_samples'. Defines how to amplify SGD updates
for each sample. May be useful for highly unbalanced problems.</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-FTRL-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>shorthand for applying 'partial_fit' 'n_iter' times
</p>


<h5>Usage</h5>

<div class="r"><pre>FTRL$fit(x, y, weights = rep(1, length(y)), n_iter = 1L, ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input sparse matrix. Native format is <code>Matrix::RsparseMatrix</code>.
If <code>x</code> is in different format, model will try to convert it to <code>RsparseMatrix</code>
with <code>as(x, "RsparseMatrix")</code>. Dimensions should be (n_samples, n_features)</p>
</dd>
<dt><code>y</code></dt><dd><p>vector of targets</p>
</dd>
<dt><code>weights</code></dt><dd><p>numeric vector of length 'n_samples'. Defines how to amplify SGD updates
for each sample. May be useful for highly unbalanced problems.</p>
</dd>
<dt><code>n_iter</code></dt><dd><p>number of SGD epochs</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-FTRL-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>makes predictions based on fitted model
</p>


<h5>Usage</h5>

<div class="r"><pre>FTRL$predict(x, ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input matrix</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-FTRL-coef"></a>



<h4>Method <code>coef()</code></h4>

<p>returns coefficients of the regression model
</p>


<h5>Usage</h5>

<div class="r"><pre>FTRL$coef()</pre></div>


<hr>
<a id="method-FTRL-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>FTRL$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>library(rsparse)
library(Matrix)
i = sample(1000, 1000 * 100, TRUE)
j = sample(1000, 1000 * 100, TRUE)
y = sample(c(0, 1), 1000, TRUE)
x = sample(c(-1, 1), 1000 * 100, TRUE)
odd = seq(1, 99, 2)
x[i %in% which(y == 1) &amp; j %in% odd] = 1
x = sparseMatrix(i = i, j = j, x = x, dims = c(1000, 1000), repr="R")

ftrl = FTRL$new(learning_rate = 0.01, learning_rate_decay = 0.1,
lambda = 10, l1_ratio = 1, dropout = 0)
ftrl$partial_fit(x, y)

w = ftrl$coef()
head(w)
sum(w != 0)
p = ftrl$predict(x)
</code></pre>

<hr>
<h2 id='GloVe'>Global Vectors</h2><span id='topic+GloVe'></span>

<h3>Description</h3>

<p>Creates Global Vectors matrix factorization model
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>components</code></dt><dd><p>represents context embeddings</p>
</dd>
<dt><code>bias_i</code></dt><dd><p>bias term i as per paper</p>
</dd>
<dt><code>bias_j</code></dt><dd><p>bias term j as per paper</p>
</dd>
<dt><code>shuffle</code></dt><dd><p><code>logical = FALSE</code> by default. Whether to perform shuffling before
each SGD iteration. Generally shuffling is a good practice for SGD.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-GloVe-new"><code>GloVe$new()</code></a>
</p>
</li>
<li> <p><a href="#method-GloVe-fit_transform"><code>GloVe$fit_transform()</code></a>
</p>
</li>
<li> <p><a href="#method-GloVe-get_history"><code>GloVe$get_history()</code></a>
</p>
</li>
<li> <p><a href="#method-GloVe-clone"><code>GloVe$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-GloVe-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates GloVe model object
</p>


<h5>Usage</h5>

<div class="r"><pre>GloVe$new(
  rank,
  x_max,
  learning_rate = 0.15,
  alpha = 0.75,
  lambda = 0,
  shuffle = FALSE,
  init = list(w_i = NULL, b_i = NULL, w_j = NULL, b_j = NULL)
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>rank</code></dt><dd><p>desired dimension for the latent vectors</p>
</dd>
<dt><code>x_max</code></dt><dd><p><code>integer</code> maximum number of co-occurrences to use in the weighting function</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p><code>numeric</code> learning rate for SGD. I do not recommend that you
modify this parameter, since AdaGrad will quickly adjust it to optimal</p>
</dd>
<dt><code>alpha</code></dt><dd><p><code>numeric = 0.75</code> the alpha in weighting function formula :
<code class="reqn">f(x) = 1 if x &gt; x_max; else (x/x_max)^alpha</code></p>
</dd>
<dt><code>lambda</code></dt><dd><p><code>numeric = 0.0</code> regularization parameter</p>
</dd>
<dt><code>shuffle</code></dt><dd><p>see <code>shuffle</code> field</p>
</dd>
<dt><code>init</code></dt><dd><p><code>list(w_i = NULL, b_i = NULL, w_j = NULL, b_j = NULL)</code>
initialization for embeddings (w_i, w_j) and biases (b_i, b_j).
<code>w_i, w_j</code> - numeric matrices, should have #rows = rank, #columns =
expected number of rows (w_i) / columns(w_j) in the input matrix.
<code>b_i, b_j</code> = numeric vectors, should have length of
#expected number of rows(b_i) / columns(b_j) in input matrix</p>
</dd>
</dl>

</div>


<hr>
<a id="method-GloVe-fit_transform"></a>



<h4>Method <code>fit_transform()</code></h4>

<p>fits model and returns embeddings
</p>


<h5>Usage</h5>

<div class="r"><pre>GloVe$fit_transform(
  x,
  n_iter = 10L,
  convergence_tol = -1,
  n_threads = getOption("rsparse_omp_threads", 1L),
  ...
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>An input term co-occurence matrix. Preferably in <code>dgTMatrix</code> format</p>
</dd>
<dt><code>n_iter</code></dt><dd><p><code>integer</code> number of SGD iterations</p>
</dd>
<dt><code>convergence_tol</code></dt><dd><p><code>numeric = -1</code> defines early stopping strategy. Stop fitting
when one of two following conditions will be satisfied: (a) passed
all iterations (b) <code>cost_previous_iter / cost_current_iter - 1 &lt;
convergence_tol</code>.</p>
</dd>
<dt><code>n_threads</code></dt><dd><p>number of threads to use</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-GloVe-get_history"></a>



<h4>Method <code>get_history()</code></h4>

<p>returns value of the loss function for each epoch
</p>


<h5>Usage</h5>

<div class="r"><pre>GloVe$get_history()</pre></div>


<hr>
<a id="method-GloVe-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>GloVe$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p><a href="http://nlp.stanford.edu/projects/glove/">http://nlp.stanford.edu/projects/glove/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('movielens100k')
co_occurence = crossprod(movielens100k)
glove_model = GloVe$new(rank = 4, x_max = 10, learning_rate = .25)
embeddings = glove_model$fit_transform(co_occurence, n_iter = 2, n_threads = 1)
embeddings = embeddings + t(glove_model$components) # embeddings + context embedings
identical(dim(embeddings), c(ncol(movielens100k), 10L))
</code></pre>

<hr>
<h2 id='LinearFlow'>Linear-FLow model for one-class collaborative filtering</h2><span id='topic+LinearFlow'></span>

<h3>Description</h3>

<p>Creates <em>Linear-FLow</em> model described in
<a href="http://www.bkveton.com/docs/ijcai2016.pdf">Practical Linear Models for Large-Scale One-Class Collaborative Filtering</a>.
The goal is to find item-item (or user-user) similarity matrix which is <b>low-rank and has small Frobenius norm</b>. Such
double regularization allows to better control the generalization error of the model.
Idea of the method is somewhat similar to <b>Sparse Linear Methods(SLIM)</b> but scales to large datasets much better.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+MatrixFactorizationRecommender">rsparse::MatrixFactorizationRecommender</a></code> -&gt; <code>LinearFlow</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>v</code></dt><dd><p>right singular vector of the user-item matrix. Size is <code>n_items * rank</code>.
In the paper this matrix is called <b>v</b></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LinearFlow-new"><code>LinearFlow$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LinearFlow-fit_transform"><code>LinearFlow$fit_transform()</code></a>
</p>
</li>
<li> <p><a href="#method-LinearFlow-transform"><code>LinearFlow$transform()</code></a>
</p>
</li>
<li> <p><a href="#method-LinearFlow-cross_validate_lambda"><code>LinearFlow$cross_validate_lambda()</code></a>
</p>
</li>
<li> <p><a href="#method-LinearFlow-clone"><code>LinearFlow$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="rsparse" data-topic="MatrixFactorizationRecommender" data-id="predict"><a href='../../rsparse/html/MatrixFactorizationRecommender.html#method-MatrixFactorizationRecommender-predict'><code>rsparse::MatrixFactorizationRecommender$predict()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-LinearFlow-new"></a>



<h4>Method <code>new()</code></h4>

<p>creates Linear-FLow model with <code>rank</code> latent factors.
</p>


<h5>Usage</h5>

<div class="r"><pre>LinearFlow$new(
  rank = 8L,
  lambda = 0,
  init = NULL,
  preprocess = identity,
  solve_right_singular_vectors = c("soft_impute", "svd")
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>rank</code></dt><dd><p>size of the latent dimension</p>
</dd>
<dt><code>lambda</code></dt><dd><p>regularization parameter</p>
</dd>
<dt><code>init</code></dt><dd><p>initialization of the orthogonal basis.</p>
</dd>
<dt><code>preprocess</code></dt><dd><p><code>identity()</code> by default. User spectified function which will
be applied to user-item interaction matrix before running matrix factorization
(also applied during inference time before making predictions).
For example we may want to normalize each row of user-item matrix to have 1 norm.
Or apply <code>log1p()</code> to discount large counts.</p>
</dd>
<dt><code>solve_right_singular_vectors</code></dt><dd><p>type of the solver for initialization of the orthogonal
basis. Original paper uses SVD. See paper for details.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LinearFlow-fit_transform"></a>



<h4>Method <code>fit_transform()</code></h4>

<p>performs matrix factorization
</p>


<h5>Usage</h5>

<div class="r"><pre>LinearFlow$fit_transform(x, ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input matrix</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LinearFlow-transform"></a>



<h4>Method <code>transform()</code></h4>

<p>calculates user embeddings for the new input
</p>


<h5>Usage</h5>

<div class="r"><pre>LinearFlow$transform(x, ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input matrix</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LinearFlow-cross_validate_lambda"></a>



<h4>Method <code>cross_validate_lambda()</code></h4>

<p>performs fast tuning of the parameter 'lambda' with warm re-starts
</p>


<h5>Usage</h5>

<div class="r"><pre>LinearFlow$cross_validate_lambda(
  x,
  x_train,
  x_test,
  lambda = "auto@10",
  metric = "map@10",
  not_recommend = x_train,
  ...
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input user-item interactions matrix. Model performs matrix facrtorization based
only on this matrix</p>
</dd>
<dt><code>x_train</code></dt><dd><p>user-item interactions matrix. Model recommends items based on this matrix.
Usually should be different from 'x' to avoid overfitting</p>
</dd>
<dt><code>x_test</code></dt><dd><p>target user-item interactions. Model will evaluate predictions against this
matrix, 'x_test' should be treated as future interactions.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>numeric vector - sequaence of regularization parameters. Supports special
value like 'auto@10'. This will automatically fine a sequence of lambda of length 10. This
is recommended way to check for 'lambda'.</p>
</dd>
<dt><code>metric</code></dt><dd><p>a metric against which model will be evaluated for top-k recommendations.
Currently only <code>map@k</code> and <code>ndcg@k</code> are supported (<code>k</code> can be any integer)</p>
</dd>
<dt><code>not_recommend</code></dt><dd><p>matrix same shape as 'x_train'. Specifies which items to not recommend
for each user.</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LinearFlow-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LinearFlow$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>


<ul>
<li><p><a href="http://www.bkveton.com/docs/ijcai2016.pdf">http://www.bkveton.com/docs/ijcai2016.pdf</a>
</p>
</li>
<li><p><a href="https://www-users.cse.umn.edu/~ningx005/slides/ICDM2011_slides.pdf">https://www-users.cse.umn.edu/~ningx005/slides/ICDM2011_slides.pdf</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data("movielens100k")
train = movielens100k[1:900, ]
cv = movielens100k[901:nrow(movielens100k), ]
model = LinearFlow$new(
  rank = 10, lambda = 0,
  solve_right_singular_vectors = "svd"
)
user_emb = model$fit_transform(train)
preds = model$predict(cv, k = 10)
</code></pre>

<hr>
<h2 id='MatrixFactorizationRecommender'>Base class for matrix factorization recommenders</h2><span id='topic+MatrixFactorizationRecommender'></span>

<h3>Description</h3>

<p>All matrix factorization recommenders inherit from this class
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>components</code></dt><dd><p>item embeddings</p>
</dd>
<dt><code>global_bias</code></dt><dd><p>global mean (for centering values in explicit feedback)</p>
</dd>
<dt><code>global_bias_base</code></dt><dd><p>Pre-calculated '-(factors*global_bias)' (for centering values in
implicit feedback when not using user/item biases)</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-MatrixFactorizationRecommender-predict"><code>MatrixFactorizationRecommender$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-MatrixFactorizationRecommender-clone"><code>MatrixFactorizationRecommender$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-MatrixFactorizationRecommender-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>recommends items for users
</p>


<h5>Usage</h5>

<div class="r"><pre>MatrixFactorizationRecommender$predict(
  x,
  k,
  not_recommend = x,
  items_exclude = integer(0),
  ...
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>user-item interactions matrix (usually sparse - 'Matrix::sparseMatrix').Users are
rows and items are columns</p>
</dd>
<dt><code>k</code></dt><dd><p>number of items to recommend</p>
</dd>
<dt><code>not_recommend</code></dt><dd><p>user-item matrix (sparse) which describes which items method should NOT
recomment for each user. Usually this is same as &lsquo;x' as we don&rsquo;t want to recommend items user
already liked.</p>
</dd>
<dt><code>items_exclude</code></dt><dd><p>either integer indices or character identifiers of the items to not
recommend to any user.</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-MatrixFactorizationRecommender-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>MatrixFactorizationRecommender$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='metrics'>Ranking Metrics for Top-K Items</h2><span id='topic+metrics'></span><span id='topic+ap_k'></span><span id='topic+ndcg_k'></span>

<h3>Description</h3>

<p><code>ap_k</code> calculates <b>Average Precision at K (<code>ap@k</code>)</b>.
Please refer to <a href="https://en.wikipedia.org/wiki/Information_retrieval#Average_precision">Information retrieval wikipedia article</a>
</p>
<p><code>ndcg_k()</code> calculates <b>Normalized Discounted Cumulative Gain at K (<code>ndcg@k</code>)</b>.
Please refer to <a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG">Discounted cumulative gain</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ap_k(predictions, actual, ...)

ndcg_k(predictions, actual, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metrics_+3A_predictions">predictions</code></td>
<td>
<p>matrix of predictions. Predctions can be defined 2 ways:
</p>

<ol>
<li> <p><code>predictions</code> = <code>integer</code> matrix with item indices (correspond to column numbers in <code>actual</code>)
</p>
</li>
<li> <p><code>predictions</code> = <code>character</code> matrix with item identifiers (characters which correspond to <code>colnames(actual)</code>)
which has attribute &quot;indices&quot; (<code>integer</code> matrix with item indices which correspond to column numbers in <code>actual</code>).
</p>
</li></ol>
</td></tr>
<tr><td><code id="metrics_+3A_actual">actual</code></td>
<td>
<p>sparse Matrix of relevant items. Each non-zero entry considered as relevant item.
Value of the each non-zero entry considered as relevance for calculation of <code>ndcg@k</code>.
It should inherit from <code>Matrix::sparseMatrix</code>. Internally <code>Matrix::RsparseMatrix</code> is used.</p>
</td></tr>
<tr><td><code id="metrics_+3A_...">...</code></td>
<td>
<p>other arguments (not used at the moment)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>predictions = matrix(
  c(5L, 7L, 9L, 2L),
  nrow = 1
)
actual = matrix(
  c(0, 0, 0, 0, 1, 0, 1, 0, 1, 0),
  nrow = 1
)
actual = as(actual, "RsparseMatrix")
identical(rsparse::ap_k(predictions, actual), 1)
</code></pre>

<hr>
<h2 id='movielens100k'>MovieLens 100K Dataset</h2><span id='topic+movielens100k'></span>

<h3>Description</h3>

<p>This data set consists of:
</p>

<ol>
<li><p> 100,000 ratings (1-5) from 943 users on 1682 movies.
</p>
</li>
<li><p> Each user has rated at least 20 movies.
</p>
</li></ol>

<p>MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("movielens100k")
</code></pre>


<h3>Format</h3>

<p>A sparse column-compressed matrix (<code>Matrix::dgCMatrix</code>) with 943 rows and 1682 columns.
</p>

<ol>
<li><p> rows are users
</p>
</li>
<li><p> columns are movies
</p>
</li>
<li><p> values are ratings
</p>
</li></ol>



<h3>Source</h3>

<p><a href="https://en.wikipedia.org/wiki/MovieLens#Datasets">https://en.wikipedia.org/wiki/MovieLens#Datasets</a>
</p>

<hr>
<h2 id='PureSVD'>PureSVD recommender model decompomposition</h2><span id='topic+PureSVD'></span>

<h3>Description</h3>

<p>Creates PureSVD recommender model. Solver is based on Soft-SVD which is
very similar to truncated SVD but optionally adds regularization based on nuclear norm.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+MatrixFactorizationRecommender">rsparse::MatrixFactorizationRecommender</a></code> -&gt; <code>PureSVD</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-PureSVD-new"><code>PureSVD$new()</code></a>
</p>
</li>
<li> <p><a href="#method-PureSVD-fit_transform"><code>PureSVD$fit_transform()</code></a>
</p>
</li>
<li> <p><a href="#method-PureSVD-transform"><code>PureSVD$transform()</code></a>
</p>
</li>
<li> <p><a href="#method-PureSVD-clone"><code>PureSVD$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="rsparse" data-topic="MatrixFactorizationRecommender" data-id="predict"><a href='../../rsparse/html/MatrixFactorizationRecommender.html#method-MatrixFactorizationRecommender-predict'><code>rsparse::MatrixFactorizationRecommender$predict()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-PureSVD-new"></a>



<h4>Method <code>new()</code></h4>

<p>create PureSVD model
</p>


<h5>Usage</h5>

<div class="r"><pre>PureSVD$new(
  rank = 10L,
  lambda = 0,
  init = NULL,
  preprocess = identity,
  method = c("svd", "impute"),
  ...
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>rank</code></dt><dd><p>size of the latent dimension</p>
</dd>
<dt><code>lambda</code></dt><dd><p>regularization parameter</p>
</dd>
<dt><code>init</code></dt><dd><p>initialization of item embeddings</p>
</dd>
<dt><code>preprocess</code></dt><dd><p><code>identity()</code> by default. User spectified function which will
be applied to user-item interaction matrix before running matrix factorization
(also applied during inference time before making predictions).
For example we may want to normalize each row of user-item matrix to have 1 norm.
Or apply <code>log1p()</code> to discount large counts.</p>
</dd>
<dt><code>method</code></dt><dd><p>type of the solver for initialization of the orthogonal
basis. Original paper uses SVD. See paper for details.</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-PureSVD-fit_transform"></a>



<h4>Method <code>fit_transform()</code></h4>

<p>performs matrix factorization
</p>


<h5>Usage</h5>

<div class="r"><pre>PureSVD$fit_transform(x, n_iter = 100L, convergence_tol = 0.001, ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input sparse user-item matrix(of class <code>dgCMatrix</code>)</p>
</dd>
<dt><code>n_iter</code></dt><dd><p>maximum number of iterations</p>
</dd>
<dt><code>convergence_tol</code></dt><dd><p><code>numeric = -Inf</code> defines early stopping strategy.
Stops fitting when one of two following conditions will be satisfied: (a) passed
all iterations (b) relative change of Frobenious norm of the two consequent solution
is less then provided <code>convergence_tol</code>.</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-PureSVD-transform"></a>



<h4>Method <code>transform()</code></h4>

<p>calculates user embeddings for the new input
</p>


<h5>Usage</h5>

<div class="r"><pre>PureSVD$transform(x, ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input matrix</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-PureSVD-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>PureSVD$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>data('movielens100k')
i_train = sample(nrow(movielens100k), 900)
i_test = setdiff(seq_len(nrow(movielens100k)), i_train)
train = movielens100k[i_train, ]
test = movielens100k[i_test, ]
rank = 32
lambda = 0
model = PureSVD$new(rank = rank,  lambda = lambda)
user_emb = model$fit_transform(sign(test), n_iter = 100, convergence_tol = 0.00001)
item_emb = model$components
preds = model$predict(sign(test), k = 1500, not_recommend = NULL)
mean(ap_k(preds, actual = test))
</code></pre>

<hr>
<h2 id='ScaleNormalize'>Re-scales input matrix proportinally to item popularity</h2><span id='topic+ScaleNormalize'></span>

<h3>Description</h3>

<p>scales input user-item interaction matrix as per eq (16) from the paper.
Usage of such rescaled matrix with [PureSVD] model will be equal to running PureSVD
on the scaled cosine-based inter-item similarity matrix.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>norm</code></dt><dd><p>which norm model should make equal to one</p>
</dd>
<dt><code>scale</code></dt><dd><p>how to rescale norm vector</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-NA-new"><code>ScaleNormalize$new()</code></a>
</p>
</li>
<li> <p><a href="#method-NA-fit"><code>ScaleNormalize$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-NA-transform"><code>ScaleNormalize$transform()</code></a>
</p>
</li>
<li> <p><a href="#method-NA-fit_transform"><code>ScaleNormalize$fit_transform()</code></a>
</p>
</li>
<li> <p><a href="#method-unknown-clone"><code>ScaleNormalize$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-NA-new"></a>



<h4>Method <code>new()</code></h4>

<p>creates model
</p>


<h5>Usage</h5>

<div class="r"><pre>ScaleNormalize$new(scale = 0.5, norm = 2, target = c("rows", "columns"))</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>scale</code></dt><dd><p>numeric, how to rescale norm vector</p>
</dd>
<dt><code>norm</code></dt><dd><p>numeric, which norm model should make equal to one</p>
</dd>
<dt><code>target</code></dt><dd><p>character, defines whether rows or columns should be rescaled</p>
</dd>
</dl>

</div>


<hr>
<a id="method-NA-fit"></a>



<h4>Method <code>fit()</code></h4>

<p>fits the modes
</p>


<h5>Usage</h5>

<div class="r"><pre>ScaleNormalize$fit(x)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input sparse matrix</p>
</dd>
</dl>

</div>


<hr>
<a id="method-NA-transform"></a>



<h4>Method <code>transform()</code></h4>

<p>transforms new matrix
</p>


<h5>Usage</h5>

<div class="r"><pre>ScaleNormalize$transform(x)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input sparse matrix</p>
</dd>
</dl>

</div>


<hr>
<a id="method-NA-fit_transform"></a>



<h4>Method <code>fit_transform()</code></h4>

<p>fits the model and transforms input
</p>


<h5>Usage</h5>

<div class="r"><pre>ScaleNormalize$fit_transform(x)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input sparse matrix</p>
</dd>
</dl>

</div>


<hr>
<a id="method-unknown-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ScaleNormalize$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>See <a href="https://arxiv.org/pdf/1511.06033.pdf">EigenRec: Generalizing PureSVD for
Effective and Efficient Top-N Recommendations</a> for details.
</p>

<hr>
<h2 id='soft_impute'>SoftImpute/SoftSVD matrix factorization</h2><span id='topic+soft_impute'></span><span id='topic+soft_svd'></span>

<h3>Description</h3>

<p>Fit SoftImpute/SoftSVD via fast alternating least squares. Based on the
paper by Trevor Hastie, Rahul Mazumder, Jason D. Lee, Reza Zadeh
by &quot;Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares&quot; -
<a href="https://arxiv.org/pdf/1410.2596.pdf">https://arxiv.org/pdf/1410.2596.pdf</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>soft_impute(
  x,
  rank = 10L,
  lambda = 0,
  n_iter = 100L,
  convergence_tol = 0.001,
  init = NULL,
  final_svd = TRUE
)

soft_svd(
  x,
  rank = 10L,
  lambda = 0,
  n_iter = 100L,
  convergence_tol = 0.001,
  init = NULL,
  final_svd = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="soft_impute_+3A_x">x</code></td>
<td>
<p>sparse matrix. Both CSR <code>dgRMatrix</code> and CSC <code>dgCMatrix</code> are supported.
CSR matrix is preffered because in this case algorithm will benefit from multithreaded
CSR * dense matrix products (if OpenMP is supported on your platform).
On many-cores machines this reduces fitting time significantly.</p>
</td></tr>
<tr><td><code id="soft_impute_+3A_rank">rank</code></td>
<td>
<p>maximum rank of the low-rank solution.</p>
</td></tr>
<tr><td><code id="soft_impute_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter for the nuclear norm</p>
</td></tr>
<tr><td><code id="soft_impute_+3A_n_iter">n_iter</code></td>
<td>
<p>maximum number of iterations of the algorithms</p>
</td></tr>
<tr><td><code id="soft_impute_+3A_convergence_tol">convergence_tol</code></td>
<td>
<p>convergence tolerance.
Internally functions keeps track of the relative change of the Frobenious norm
of the two consequent iterations. If the change is less than <code>convergence_tol</code>
then the process is considered as converged and function returns result.</p>
</td></tr>
<tr><td><code id="soft_impute_+3A_init">init</code></td>
<td>
<p><a href="base.html#topic+svd">svd</a> like object with <code>u, v, d</code> components to initialize algorithm.
Algorithm benefit from warm starts. <code>init</code> could be rank up <code>rank</code> of the maximum allowed rank.
If <code>init</code> has rank less than max rank it will be padded automatically.</p>
</td></tr>
<tr><td><code id="soft_impute_+3A_final_svd">final_svd</code></td>
<td>
<p><code>logical</code> whether need to make final preprocessing with SVD.
This is not necessary but cleans up rank nicely - hithly recommnded to leave it <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="base.html#topic+svd">svd</a>-like object - <code>list(u, v, d)</code>. <code>u, v, d</code>
components represent left, right singular vectors and singular values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
data('movielens100k')
k = 10
seq_k = seq_len(k)
m = movielens100k[1:100, 1:200]
svd_ground_true = svd(m)
svd_soft_svd = soft_svd(m, rank = k, n_iter = 100, convergence_tol = 1e-6)
m_restored_svd = svd_ground_true$u[, seq_k]  %*%
   diag(x = svd_ground_true$d[seq_k]) %*%
   t(svd_ground_true$v[, seq_k])
m_restored_soft_svd = svd_soft_svd$u %*%
  diag(x = svd_soft_svd$d) %*%
  t(svd_soft_svd$v)
all.equal(m_restored_svd, m_restored_soft_svd, tolerance = 1e-1)
</code></pre>

<hr>
<h2 id='train_test_split'>Creates cross-validation set from user-item interactions</h2><span id='topic+train_test_split'></span>

<h3>Description</h3>

<p>Basic splitting of the user-item interaction matrix into train and testing part.
Useful for when data doesn't have time dimension.
Usually during model tuning it worth to keep some <code>x</code> matrix as hold-out data set.
Then this <code>x</code> could be splitted in 2 parts - <em>train</em> and <em>test</em>.
Model tries to predict <em>test</em> data using <em>train</em>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_test_split(x, test_proportion = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_test_split_+3A_x">x</code></td>
<td>
<p>sparse user-item interation matrix. Internally <code>Matrix::TsparseMatrix</code> is used.</p>
</td></tr>
<tr><td><code id="train_test_split_+3A_test_proportion">test_proportion</code></td>
<td>
<p>- proportion of the observations for each user to keep as &quot;test&quot; data.</p>
</td></tr>
</table>

<hr>
<h2 id='WRMF'>Weighted Regularized Matrix Factorization for collaborative filtering</h2><span id='topic+WRMF'></span>

<h3>Description</h3>

<p>Creates a matrix factorization model which is solved through Alternating Least Squares (Weighted ALS for implicit feedback).
For implicit feedback see &quot;Collaborative Filtering for Implicit Feedback Datasets&quot; (Hu, Koren, Volinsky).
For explicit feedback it corresponds to the classic model for rating matrix decomposition with MSE error.
These two algorithms are proven to work well in recommender systems.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+MatrixFactorizationRecommender">rsparse::MatrixFactorizationRecommender</a></code> -&gt; <code>WRMF</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-WRMF-new"><code>WRMF$new()</code></a>
</p>
</li>
<li> <p><a href="#method-WRMF-fit_transform"><code>WRMF$fit_transform()</code></a>
</p>
</li>
<li> <p><a href="#method-WRMF-transform"><code>WRMF$transform()</code></a>
</p>
</li>
<li> <p><a href="#method-WRMF-clone"><code>WRMF$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="rsparse" data-topic="MatrixFactorizationRecommender" data-id="predict"><a href='../../rsparse/html/MatrixFactorizationRecommender.html#method-MatrixFactorizationRecommender-predict'><code>rsparse::MatrixFactorizationRecommender$predict()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-WRMF-new"></a>



<h4>Method <code>new()</code></h4>

<p>creates WRMF model
</p>


<h5>Usage</h5>

<div class="r"><pre>WRMF$new(
  rank = 10L,
  lambda = 0,
  dynamic_lambda = TRUE,
  init = NULL,
  preprocess = identity,
  feedback = c("implicit", "explicit"),
  solver = c("conjugate_gradient", "cholesky", "nnls"),
  with_user_item_bias = FALSE,
  with_global_bias = FALSE,
  cg_steps = 3L,
  precision = c("double", "float"),
  ...
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>rank</code></dt><dd><p>size of the latent dimension</p>
</dd>
<dt><code>lambda</code></dt><dd><p>regularization parameter</p>
</dd>
<dt><code>dynamic_lambda</code></dt><dd><p>whether 'lambda' is to be scaled according to the number</p>
</dd>
<dt><code>init</code></dt><dd><p>initialization of item embeddings</p>
</dd>
<dt><code>preprocess</code></dt><dd><p><code>identity()</code> by default. User spectified function which will
be applied to user-item interaction matrix before running matrix factorization
(also applied during inference time before making predictions).
For example we may want to normalize each row of user-item matrix to have 1 norm.
Or apply <code>log1p()</code> to discount large counts.
This corresponds to the &quot;confidence&quot; function from
&quot;Collaborative Filtering for Implicit Feedback Datasets&quot; paper.
Note that it will not automatically add +1 to the weights of the positive entries.</p>
</dd>
<dt><code>feedback</code></dt><dd><p><code>character</code> - feedback type - one of <code>c("implicit", "explicit")</code></p>
</dd>
<dt><code>solver</code></dt><dd><p><code>character</code> - solver name.
One of <code>c("conjugate_gradient", "cholesky", "nnls")</code>.
Usually approximate <code>"conjugate_gradient"</code> is significantly faster and solution is
on par with <code>"cholesky"</code>.
<code>"nnls"</code> performs non-negative matrix factorization (NNMF) - restricts
user and item embeddings to be non-negative.</p>
</dd>
<dt><code>with_user_item_bias</code></dt><dd><p><code>bool</code> controls if  model should calculate user and item biases.
At the moment only implemented for <code>"explicit"</code> feedback.</p>
</dd>
<dt><code>with_global_bias</code></dt><dd><p><code>bool</code> controls if model should calculate global biases (mean).
At the moment only implemented for <code>"explicit"</code> feedback.</p>
</dd>
<dt><code>cg_steps</code></dt><dd><p><code>integer &gt; 0</code> - max number of internal steps in conjugate gradient
(if &quot;conjugate_gradient&quot; solver used). <code>cg_steps = 3</code> by default.
Controls precision of linear equation solution at the each ALS step. Usually no need to tune this parameter</p>
</dd>
<dt><code>precision</code></dt><dd><p>one of <code>c("double", "float")</code>. Should embedding matrices be
numeric or float (from <code>float</code> package). The latter is usually 2x faster and
consumes less RAM. BUT <code>float</code> matrices are not &quot;base&quot; objects. Use carefully.</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-WRMF-fit_transform"></a>



<h4>Method <code>fit_transform()</code></h4>

<p>fits the model
</p>


<h5>Usage</h5>

<div class="r"><pre>WRMF$fit_transform(
  x,
  n_iter = 10L,
  convergence_tol = ifelse(private$feedback == "implicit", 0.005, 0.001),
  ...
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>input matrix (preferably matrix  in CSC format -'CsparseMatrix'</p>
</dd>
<dt><code>n_iter</code></dt><dd><p>max number of ALS iterations</p>
</dd>
<dt><code>convergence_tol</code></dt><dd><p>convergence tolerance checked between iterations</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-WRMF-transform"></a>



<h4>Method <code>transform()</code></h4>

<p>create user embeddings for new input
</p>


<h5>Usage</h5>

<div class="r"><pre>WRMF$transform(x, ...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x</code></dt><dd><p>user-item iteraction matrix (preferrably as 'dgRMatrix')</p>
</dd>
<dt><code>...</code></dt><dd><p>not used at the moment</p>
</dd>
</dl>

</div>


<hr>
<a id="method-WRMF-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>WRMF$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>


<ul>
<li><p>Hu, Yifan, Yehuda Koren, and Chris Volinsky.
&quot;Collaborative filtering for implicit feedback datasets.&quot;
2008 Eighth IEEE International Conference on Data Mining. Ieee, 2008.
</p>
</li>
<li><p><a href="https://math.stackexchange.com/questions/1072451/analytic-solution-for-matrix-factorization-using-alternating-least-squares/1073170#1073170">https://math.stackexchange.com/questions/1072451/analytic-solution-for-matrix-factorization-using-alternating-least-squares/1073170#1073170</a>
</p>
</li>
<li><p><a href="http://activisiongamescience.github.io/2016/01/11/Implicit-Recommender-Systems-Biased-Matrix-Factorization/">http://activisiongamescience.github.io/2016/01/11/Implicit-Recommender-Systems-Biased-Matrix-Factorization/</a>
</p>
</li>
<li><p><a href="https://jessesw.com/Rec-System/">https://jessesw.com/Rec-System/</a>
</p>
</li>
<li><p><a href="http://www.benfrederickson.com/matrix-factorization/">http://www.benfrederickson.com/matrix-factorization/</a>
</p>
</li>
<li><p><a href="http://www.benfrederickson.com/fast-implicit-matrix-factorization/">http://www.benfrederickson.com/fast-implicit-matrix-factorization/</a>
</p>
</li>
<li><p>Franc, Vojtech, Vaclav Hlavac, and Mirko Navara.
&quot;Sequential coordinate-wise algorithm for the
non-negative least squares problem.&quot;
International Conference on Computer Analysis of Images
and Patterns. Springer, Berlin, Heidelberg, 2005.
</p>
</li>
<li><p>Zhou, Yunhong, et al.
&quot;Large-scale parallel collaborative filtering for the netflix prize.&quot;
International conference on algorithmic applications in management.
Springer, Berlin, Heidelberg, 2008.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data('movielens100k')
train = movielens100k[1:900, ]
cv = movielens100k[901:nrow(movielens100k), ]
model = WRMF$new(rank = 5,  lambda = 0, feedback = 'implicit')
user_emb = model$fit_transform(train, n_iter = 5, convergence_tol = -1)
item_emb = model$components
preds = model$predict(cv, k = 10, not_recommend = cv)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
