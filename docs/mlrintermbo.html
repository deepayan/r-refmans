<!DOCTYPE html><html lang="en"><head><title>Help for package mlrintermbo</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mlrintermbo}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mlrintermbo-package'><p>mlrintermbo: Model-Based Optimization for 'mlr3' Through 'mlrMBO'</p></a></li>
<li><a href='#captureSpecials'><p>Capture Warnings and Errors</p></a></li>
<li><a href='#makeMlr3Surrogate'><p>Create Surrogate Learner</p></a></li>
<li><a href='#OptimInstanceMultiCrit'><p>OptimInstanceMultiCrit Class</p></a></li>
<li><a href='#OptimInstanceSingleCrit'><p>OptimInstanceSingleCrit Class</p></a></li>
<li><a href='#Optimizer'><p>Optimizer Class</p></a></li>
<li><a href='#OptimizerInterMBO'><p>Tuner and Optimizer using mlrMBO</p></a></li>
<li><a href='#TuningInstanceMultiCrit'><p>TuningInstanceMultiCrit Class</p></a></li>
<li><a href='#TuningInstanceSingleCrit'><p>TuningInstanceSingleCrit Class</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Model-Based Optimization for 'mlr3' Through 'mlrMBO'</td>
</tr>
<tr>
<td>Description:</td>
<td>The 'mlrMBO' package can ordinarily not be used for optimization within 'mlr3', because of
  incompatibilities of their respective class systems. 'mlrintermbo' offers a compatibility
  interface that provides 'mlrMBO' as an 'mlr3tuning' 'Tuner' object, for tuning of machine
  learning algorithms within 'mlr3', as well as a 'bbotk' 'Optimizer' object for optimization
  of general objective functions using the 'bbotk' black box optimization framework. The
  control parameters of 'mlrMBO' are faithfully reproduced as a 'paradox' 'ParamSet'.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mb706/mlrintermbo">https://github.com/mb706/mlrintermbo</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mb706/mlrintermbo/issues">https://github.com/mb706/mlrintermbo/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>backports, checkmate, data.table, mlr3misc (&ge; 0.1.4),
paradox, R6, lhs, callr, bbotk, mlr3tuning</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mlr, ParamHelpers, testthat, rgenoud, DiceKriging, emoa,
cmaesr, randomForest, smoof, lgr, mlr3, mlr3learners,
mlr3pipelines, mlrMBO, ranger, rpart, mco</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.1-1</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Collate:</td>
<td>'utils.R' 'CapsuledMlr3Learner.R' 'ParamHelpersParamSet.R'
'optimize.R' 'paramset.R' 'TunerInterMBO.R' 'surrogates.R'
'zzz.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-07 22:15:10 UTC; user</td>
</tr>
<tr>
<td>Author:</td>
<td>Martin Binder [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Martin Binder &lt;developer.mb706@doublecaret.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-07 22:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='mlrintermbo-package'>mlrintermbo: Model-Based Optimization for 'mlr3' Through 'mlrMBO'</h2><span id='topic+mlrintermbo'></span><span id='topic+mlrintermbo-package'></span>

<h3>Description</h3>

<p>The 'mlrMBO' package can ordinarily not be used for optimization within 'mlr3', because of incompatibilities of their respective class systems. 'mlrintermbo' offers a compatibility interface that provides 'mlrMBO' as an 'mlr3tuning' 'Tuner' object, for tuning of machine learning algorithms within 'mlr3', as well as a 'bbotk' 'Optimizer' object for optimization of general objective functions using the 'bbotk' black box optimization framework. The control parameters of 'mlrMBO' are faithfully reproduced as a 'paradox' 'ParamSet'.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Martin Binder <a href="mailto:developer.mb706@doublecaret.com">developer.mb706@doublecaret.com</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/mb706/mlrintermbo">https://github.com/mb706/mlrintermbo</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/mb706/mlrintermbo/issues">https://github.com/mb706/mlrintermbo/issues</a>
</p>
</li></ul>


<hr>
<h2 id='captureSpecials'>Capture Warnings and Errors</h2><span id='topic+captureSpecials'></span>

<h3>Description</h3>

<p>Converts warnings and errors into in-band information.
This function is for internal use and is used within
the attached R session in the background.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>captureSpecials(expr)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="captureSpecials_+3A_expr">expr</code></td>
<td>
<p>expression to evaluate.</p>
</td></tr>
</table>

<hr>
<h2 id='makeMlr3Surrogate'>Create Surrogate Learner</h2><span id='topic+makeMlr3Surrogate'></span>

<h3>Description</h3>

<p>Creates the default mlrMBO surrogate learners as an <code><a href="mlr3.html#topic+Learner">mlr3::Learner</a></code>.
</p>
<p>This imitates the behaviour of mlrCPO when no <code>learner</code> argument is given to <code>mbo()</code> / <code>initSMBO()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeMlr3Surrogate(
  is.numeric = TRUE,
  is.noisy = TRUE,
  has.dependencies = !is.numeric
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="makeMlr3Surrogate_+3A_is.numeric">is.numeric</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Whether only numeric parameters are present. If so, a <code>LearnerRegrKM</code> (<span class="pkg">DiceKriging</span> package)
is constructed. Otherwise a <code>LearnerRegrRanger</code> (random forest from the <span class="pkg">ranger</span> package) is constructed.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="makeMlr3Surrogate_+3A_is.noisy">is.noisy</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Whether to use nugget estimation. Only considered when <code>is.numeric</code> is <code>TRUE</code>. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="makeMlr3Surrogate_+3A_has.dependencies">has.dependencies</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Whether to anticipate missing values in the surrogate model design. This adds out-of-range imputation to the model.
If more elaborate imputation is desired, it may be desirable to set this to <code>FALSE</code> and instead perform custom imputation
using <span class="pkg">mlr3pipelines</span>.
Default is <code>!numeric</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># DiceKriging Learner:
makeMlr3Surrogate()

# mlr3pipelines Graph: imputation %&gt;&gt;% 'ranger' (randomForest):
makeMlr3Surrogate(is.numeric = FALSE)

# just the 'ranger' Learner:
makeMlr3Surrogate(is.numeric = FALSE, has.dependencies = FALSE)

</code></pre>

<hr>
<h2 id='OptimInstanceMultiCrit'>OptimInstanceMultiCrit Class</h2><span id='topic+OptimInstanceMultiCrit'></span>

<h3>Description</h3>

<p><code>bbotk</code>'s <code>OptimInstanceMultiCrit</code> class.
Re-exported since <code>bbotk</code> will change the name.
</p>

<hr>
<h2 id='OptimInstanceSingleCrit'>OptimInstanceSingleCrit Class</h2><span id='topic+OptimInstanceSingleCrit'></span>

<h3>Description</h3>

<p><code>bbotk</code>'s <code>OptimInstanceSingleCrit</code> class.
Re-exported since <code>bbotk</code> will change the name.
</p>

<hr>
<h2 id='Optimizer'>Optimizer Class</h2><span id='topic+Optimizer'></span>

<h3>Description</h3>

<p><code>bbotk</code>'s <code>Optimizer</code> class.
Re-exported since <code>bbotk</code> will change the name.
</p>

<hr>
<h2 id='OptimizerInterMBO'>Tuner and Optimizer using mlrMBO</h2><span id='topic+OptimizerInterMBO'></span><span id='topic+mlr_optimizers_intermbo'></span><span id='topic+mlr_tuners_intermbo'></span><span id='topic+TunerInterMBO'></span>

<h3>Description</h3>

<p>mlrMBO tuning object.
</p>
<p>mlrMBO must not be loaded directly into R when using mlr3, for various reasons.
TunerInterMBO and OptimizerInterMBO take care that this does not happen.
</p>


<h3>Format</h3>

<p><a href="R6.html#topic+R6Class">R6::R6Class</a> object inheriting from <code>Tuner</code> (<code>mlr3tuning</code> package) or <code>Optimizer</code> (<code>bbotk</code> package).
</p>


<h3>Construction</h3>

<p>To optimize an objective (using the <code>bbotk</code> package), use the <code>OptimizerInterMBO</code> object,
ideally obtained through the <code><a href="bbotk.html#topic+opt">bbotk::opt()</a></code> function: <code>opt("intermbo")</code>.
</p>
<p>To tune a machine learning method represented by a <a href="mlr3.html#topic+Learner">mlr3::Learner</a> object,
use the <code>TunerInterMBO</code> obtained ideally through <code><a href="mlr3tuning.html#topic+tnr">mlr3tuning::tnr()</a></code>: <code>tnr("intermbo")</code>.
</p>
<p>Both have following optional arguments:
</p>

<ul>
<li> <p><code>n.objectives</code> :: <code>integer(1)</code><br />
Number of objectives to optimize. Default is 1 for ordinary (&quot;single objective&quot;) optimization,
but can be breater than 1 for multi-objective optimization. See <code><a href="mlrMBO.html#topic+setMBOControlMultiObj">mlrMBO::setMBOControlMultiObj()</a></code>
for details on multi-objective optimization in <code>mlrMBO</code>.
</p>
</li>
<li> <p><code>on.surrogate.error</code> :: <code>character(1)</code><br />
What to do when fitting or predicting the surrogate model fails. One of <code>"stop"</code> (throw error),
<code>"warn"</code>, and <code>"quiet"</code>(ignore and propose a random point).<br />
The surrogate model may fail sometimes, for example when the size
of the initial design is too small or when the objective function returns constant values. In practice
this is usually safe to ignore for single iterations (i.e. <code>"warn"</code> or <code>"quiet"</code>), but be aware
that MBO effectively degrades to random search when the surrogate model fails for all iterations.
</p>
</li></ul>



<h3>Configuration Parameters</h3>

<p>The <code><a href="paradox.html#topic+ParamSet">ParamSet</a></code> of the optimizer / tuner reflects the possible configuration
options of mlrMBO. The control parameters map directly to the arguments of
<code><a href="mlrMBO.html#topic+makeMBOControl">mlrMBO::makeMBOControl()</a></code>, <code><a href="mlrMBO.html#topic+setMBOControlInfill">mlrMBO::setMBOControlInfill()</a></code>, <code><a href="mlrMBO.html#topic+setMBOControlMultiObj">mlrMBO::setMBOControlMultiObj()</a></code>,
<code><a href="mlrMBO.html#topic+setMBOControlMultiPoint">mlrMBO::setMBOControlMultiPoint()</a></code>, and <code><a href="mlrMBO.html#topic+setMBOControlTermination">mlrMBO::setMBOControlTermination()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("paradox")
library("bbotk")

# silly example function: minimize x^2 for -1 &lt; x &lt; 1
domain &lt;- ps(x = p_dbl(lower = -1, upper = 1))
codomain &lt;- ps(y = p_dbl(tags = "minimize"))
objective &lt;- ObjectiveRFun$new(function(xs) list(y = xs$x^2), domain, codomain)

# initialize instance
instance &lt;- OptimInstanceSingleCrit$new(objective, domain, trm("evals", n_evals = 6))

# use intermbo optimizer
#
# Also warn on surrogate model errors
# (this is the default and can be omitted)
optser &lt;- opt("intermbo", on.surrogate.error = "warn")

# optimizer has hyperparameters from mlrMBO
optser$param_set$values$final.method &lt;- "best.predicted"

# optimization happens here.
optser$optimize(instance)

instance$result
</code></pre>

<hr>
<h2 id='TuningInstanceMultiCrit'>TuningInstanceMultiCrit Class</h2><span id='topic+TuningInstanceMultiCrit'></span>

<h3>Description</h3>

<p><code>mlr3tuning</code>'s <code>TuningInstanceMultiCrit</code> class.
Re-exported since <code>mlr3tuning</code> will change the name.
</p>

<hr>
<h2 id='TuningInstanceSingleCrit'>TuningInstanceSingleCrit Class</h2><span id='topic+TuningInstanceSingleCrit'></span>

<h3>Description</h3>

<p><code>mlr3tuning</code>'s <code>TuningInstanceSingleCrit</code> class.
Re-exported since <code>mlr3tuning</code> will change the name.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
