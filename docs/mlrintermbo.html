<!DOCTYPE html><html><head><title>Help for package mlrintermbo</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mlrintermbo}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mlrintermbo-package'><p>mlrintermbo: An 'mlrMBO' 'mlr3' Interface</p></a></li>
<li><a href='#captureSpecials'><p>Capture Warnings and Errors</p></a></li>
<li><a href='#makeMlr3Surrogate'><p>Create Surrogate Learner</p></a></li>
<li><a href='#OptimizerInterMBO'><p>Tuner and Optimizer using mlrMBO</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Model-Based Optimization for 'mlr3' Through 'mlrMBO'</td>
</tr>
<tr>
<td>Description:</td>
<td>The 'mlrMBO' package can ordinarily not be used for optimization within 'mlr3', because of
  incompatibilities of their respective class systems. 'mlrintermbo' offers a compatibility
  interface that provides 'mlrMBO' as an 'mlr3tuning' 'Tuner' object, for tuning of machine
  learning algorithms within 'mlr3', as well as a 'bbotk' 'Optimizer' object for optimization
  of general objective functions using the 'bbotk' black box optimization framework. The
  control parameters of 'mlrMBO' are faithfully reproduced as a 'paradox' 'ParamSet'.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mb706/mlrintermbo">https://github.com/mb706/mlrintermbo</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mb706/mlrintermbo/issues">https://github.com/mb706/mlrintermbo/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>backports, checkmate, data.table, mlr3misc (&ge; 0.1.4),
paradox, R6, lhs, callr, bbotk, mlr3tuning</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mlr, ParamHelpers, testthat, rgenoud, DiceKriging, emoa,
cmaesr, randomForest, smoof, lgr, mlr3, mlr3learners,
mlr3pipelines, mlrMBO, ranger, rpart</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.0</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Collate:</td>
<td>'utils.R' 'CapsuledMlr3Learner.R' 'ParamHelpersParamSet.R'
'optimize.R' 'paramset.R' 'TunerInterMBO.R' 'surrogates.R'
'zzz.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-02-24 23:08:46 UTC; user</td>
</tr>
<tr>
<td>Author:</td>
<td>Martin Binder [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Martin Binder &lt;developer.mb706@doublecaret.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-03-01 09:00:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='mlrintermbo-package'>mlrintermbo: An 'mlrMBO' 'mlr3' Interface</h2><span id='topic+mlrintermbo'></span><span id='topic+mlrintermbo-package'></span>

<h3>Description</h3>

<p>Model-based optimization for 'mlr3' through 'mlrMBO'.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Martin Binder <a href="mailto:developer.mb706@doublecaret.com">developer.mb706@doublecaret.com</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/mb706/mlrintermbo">https://github.com/mb706/mlrintermbo</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/mb706/mlrintermbo/issues">https://github.com/mb706/mlrintermbo/issues</a>
</p>
</li></ul>


<hr>
<h2 id='captureSpecials'>Capture Warnings and Errors</h2><span id='topic+captureSpecials'></span>

<h3>Description</h3>

<p>Converts warnings and errors into in-band information.
This function is for internal use and is used within
the attached R session in the background.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>captureSpecials(expr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="captureSpecials_+3A_expr">expr</code></td>
<td>
<p>expression to evaluate.</p>
</td></tr>
</table>

<hr>
<h2 id='makeMlr3Surrogate'>Create Surrogate Learner</h2><span id='topic+makeMlr3Surrogate'></span>

<h3>Description</h3>

<p>Creates the default mlrMBO surrogate learners as an <code><a href="mlr3.html#topic+Learner">mlr3::Learner</a></code>.
</p>
<p>This imitates the behaviour of mlrCPO when no <code>learner</code> argument is given to <code>mbo()</code> / <code>initSMBO()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeMlr3Surrogate(
  is.numeric = TRUE,
  is.noisy = TRUE,
  has.dependencies = !is.numeric
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeMlr3Surrogate_+3A_is.numeric">is.numeric</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Whether only numeric parameters are present. If so, a <code>LearnerRegrKM</code> (<span class="pkg">DiceKriging</span> package)
is constructed. Otherwise a <code>LearnerRegrRanger</code> (random forest from the <span class="pkg">ranger</span> package) is constructed.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="makeMlr3Surrogate_+3A_is.noisy">is.noisy</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Whether to use nugget estimation. Only considered when <code>is.numeric</code> is <code>TRUE</code>. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="makeMlr3Surrogate_+3A_has.dependencies">has.dependencies</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Whether to anticipate missing values in the surrogate model design. This adds out-of-range imputation to the model.
If more elaborate imputation is desired, it may be desirable to set this to <code>FALSE</code> and instead perform custom imputation
using <span class="pkg">mlr3pipelines</span>.
Default is <code>!numeric</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># DiceKriging Learner:
makeMlr3Surrogate()

# mlr3pipelines Graph: imputation %&gt;&gt;% 'ranger' (randomForest):
makeMlr3Surrogate(is.numeric = FALSE)

# just the 'ranger' Learner:
makeMlr3Surrogate(is.numeric = FALSE, has.dependencies = FALSE)

</code></pre>

<hr>
<h2 id='OptimizerInterMBO'>Tuner and Optimizer using mlrMBO</h2><span id='topic+OptimizerInterMBO'></span><span id='topic+mlr_optimizers_intermbo'></span><span id='topic+mlr_tuners_intermbo'></span><span id='topic+TunerInterMBO'></span>

<h3>Description</h3>

<p>mlrMBO tuning object.
</p>
<p>mlrMBO must not be loaded directly into R when using mlr3, for various reasons.
TunerInterMBO and OptimizerInterMBO take care that this does not happen.
</p>
<p>To optimize an objective (using the <code>bbotk</code> package), use the <code>OptimizerInterMBO</code> object,
ideally obtained through the <code><a href="bbotk.html#topic+opt">bbotk::opt()</a></code> function: <code>opt("intermbo")</code>.
</p>
<p>To tune a machine learning method represented by a <a href="mlr3.html#topic+Learner">mlr3::Learner</a> object,
use the <code>TunerInterMBO</code> obtained ideally through <code><a href="mlr3tuning.html#topic+tnr">mlr3tuning::tnr()</a></code>: <code>tnr("intermbo")</code>.
</p>
<p>The <code><a href="paradox.html#topic+ParamSet">ParamSet</a></code> of the optimizer / tuner reflects the possible configuration
options of mlrMBO. The control parameters map directly to the arguments of
<code><a href="mlrMBO.html#topic+makeMBOControl">mlrMBO::makeMBOControl()</a></code>, <code><a href="mlrMBO.html#topic+setMBOControlInfill">mlrMBO::setMBOControlInfill()</a></code>, <code><a href="mlrMBO.html#topic+setMBOControlMultiObj">mlrMBO::setMBOControlMultiObj()</a></code>,
<code><a href="mlrMBO.html#topic+setMBOControlMultiPoint">mlrMBO::setMBOControlMultiPoint()</a></code>, and <code><a href="mlrMBO.html#topic+setMBOControlTermination">mlrMBO::setMBOControlTermination()</a></code>.
</p>


<h3>Format</h3>

<p><a href="R6.html#topic+R6Class">R6::R6Class</a> object inheriting from <a href="mlr3tuning.html#topic+Tuner">mlr3tuning::Tuner</a> or <a href="bbotk.html#topic+Optimizer">bbotk::Optimizer</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("paradox")
library("bbotk")

# silly example function: minimize x^2 for -1 &lt; x &lt; 1
domain &lt;- ParamSet$new(list(ParamDbl$new("x", lower = -1, upper = 1)))
codomain &lt;- ParamSet$new(list(ParamDbl$new("y", tags = "minimize")))
objective &lt;- ObjectiveRFun$new(function(xs) list(y = xs$x^2), domain, codomain)

# initialize instance
instance &lt;- OptimInstanceSingleCrit$new(objective, domain, trm("evals", n_evals = 6))

# use intermbo optimizer
optser &lt;- opt("intermbo")

# optimizer has hyperparameters from mlrMBO
optser$param_set$values$final.method &lt;- "best.predicted"

# optimization happens here.
optser$optimize(instance)

instance$result
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
