<!DOCTYPE html><html><head><title>Help for package tokenizers.bpe</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tokenizers.bpe}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#belgium_parliament'><p>Dataset from 2017 with Questions asked in the Belgium Federal Parliament</p></a></li>
<li><a href='#bpe'><p>Construct a Byte Pair Encoding model</p></a></li>
<li><a href='#bpe_decode'><p>Decode Byte Pair Encoding sequences to text</p></a></li>
<li><a href='#bpe_encode'><p>Tokenise text alongside a Byte Pair Encoding model</p></a></li>
<li><a href='#bpe_load_model'><p>Load a Byte Pair Encoding model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Byte Pair Encoding Text Tokenization</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.3</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jan Wijffels &lt;jwijffels@bnosac.be&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Unsupervised text tokenizer focused on computational efficiency. Wraps the 'YouTokenToMe' library <a href="https://github.com/VKCOM/YouTokenToMe">https://github.com/VKCOM/YouTokenToMe</a> which is an implementation of fast Byte Pair Encoding (BPE) <a href="https://aclanthology.org/P16-1162/">https://aclanthology.org/P16-1162/</a>.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bnosac/tokenizers.bpe">https://github.com/bnosac/tokenizers.bpe</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.mozilla.org/en-US/MPL/2.0/">MPL-2.0</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.5)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-15 08:45:08 UTC; jwijf</td>
</tr>
<tr>
<td>Author:</td>
<td>Jan Wijffels [aut, cre, cph] (R wrapper),
  BNOSAC [cph] (R wrapper),
  VK.com [cph],
  Gregory Popovitch [ctb, cph] (Files at src/parallel_hashmap (Apache
    License, Version 2.0),
  The Abseil Authors [ctb, cph] (Files at src/parallel_hashmap (Apache
    License, Version 2.0),
  Ivan Belonogov [ctb, cph] (Files at src/youtokentome (MIT License))</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-15 22:12:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='belgium_parliament'>Dataset from 2017 with Questions asked in the Belgium Federal Parliament</h2><span id='topic+belgium_parliament'></span>

<h3>Description</h3>

<p>Dataset from 2017 with Questions asked by members of the Belgian Federal Parliament.<br />
The dataset was extracted from <a href="http://data.dekamer.be">http://data.dekamer.be</a> and contains questions asked by persons in the Belgium Federal parliament. <br />
The questions are translated in Dutch and French. <br />
</p>
<p>The dataset contains the following information:
</p>

<ul>
<li><p> doc_id: an identifier
</p>
</li>
<li><p> text: the question itself
</p>
</li>
<li><p> language: the language of the text
</p>
</li></ul>



<h3>Source</h3>

<p><a href="http://data.dekamer.be">http://data.dekamer.be</a>, data is provided by http://www.dekamer.be in the public domain (CC0).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(belgium_parliament)
str(belgium_parliament)
</code></pre>

<hr>
<h2 id='bpe'>Construct a Byte Pair Encoding model</h2><span id='topic+bpe'></span>

<h3>Description</h3>

<p>Construct a Byte Pair Encoding model on text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bpe(
  x,
  coverage = 0.9999,
  vocab_size = 5000,
  threads = -1L,
  pad_id = 0L,
  unk_id = 1L,
  bos_id = 2L,
  eos_id = 3L,
  model_path = file.path(getwd(), "youtokentome.bpe")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bpe_+3A_x">x</code></td>
<td>
<p>path to the text file containing training data or a character vector of text with training data</p>
</td></tr>
<tr><td><code id="bpe_+3A_coverage">coverage</code></td>
<td>
<p>fraction of characters covered by the model. Must be in the range [0, 1]. A good value to use is about 0.9999</p>
</td></tr>
<tr><td><code id="bpe_+3A_vocab_size">vocab_size</code></td>
<td>
<p>integer indicating the number of tokens in the final vocabulary</p>
</td></tr>
<tr><td><code id="bpe_+3A_threads">threads</code></td>
<td>
<p>integer with number of CPU threads to use for model processing. If equal to -1 then minimum of the number of available threads and 8 will be used</p>
</td></tr>
<tr><td><code id="bpe_+3A_pad_id">pad_id</code></td>
<td>
<p>integer, reserved id for padding</p>
</td></tr>
<tr><td><code id="bpe_+3A_unk_id">unk_id</code></td>
<td>
<p>integer, reserved id for unknown symbols</p>
</td></tr>
<tr><td><code id="bpe_+3A_bos_id">bos_id</code></td>
<td>
<p>integer, reserved id for begin of sentence token</p>
</td></tr>
<tr><td><code id="bpe_+3A_eos_id">eos_id</code></td>
<td>
<p>integer, reserved id for end of sentence token</p>
</td></tr>
<tr><td><code id="bpe_+3A_model_path">model_path</code></td>
<td>
<p>path to the file on disk where the model will be stored. Defaults to 'youtokentome.bpe' in the current working directory</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>youtokentome</code> which is defined at <code><a href="#topic+bpe_load_model">bpe_load_model</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bpe_load_model">bpe_load_model</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(belgium_parliament, package = "tokenizers.bpe")
x &lt;- subset(belgium_parliament, language == "french")
model &lt;- bpe(x$text, coverage = 0.999, vocab_size = 5000, threads = 1)
model
str(model$vocabulary)

text &lt;- c("L'appartement est grand &amp; vraiment bien situe en plein centre",
          "Proportion de femmes dans les situations de famille monoparentale.")
bpe_encode(model, x = text, type = "subwords")
bpe_encode(model, x = text, type = "ids")

encoded &lt;- bpe_encode(model, x = text, type = "ids")
decoded &lt;- bpe_decode(model, encoded)
decoded

## Remove the model file (Clean up for CRAN)
file.remove(model$model_path)
</code></pre>

<hr>
<h2 id='bpe_decode'>Decode Byte Pair Encoding sequences to text</h2><span id='topic+bpe_decode'></span>

<h3>Description</h3>

<p>Decode a sequence of Byte Pair Encoding ids into text again
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bpe_decode(model, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bpe_decode_+3A_model">model</code></td>
<td>
<p>an object of class <code>youtokentome</code> as returned by <code><a href="#topic+bpe_load_model">bpe_load_model</a></code></p>
</td></tr>
<tr><td><code id="bpe_decode_+3A_x">x</code></td>
<td>
<p>an integer vector of BPE id's</p>
</td></tr>
<tr><td><code id="bpe_decode_+3A_...">...</code></td>
<td>
<p>further arguments passed on to youtokentome_encode_as_ids</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(belgium_parliament, package = "tokenizers.bpe")
x &lt;- subset(belgium_parliament, language == "french")
model &lt;- bpe(x$text, coverage = 0.999, vocab_size = 5000, threads = 1)
model
str(model$vocabulary)

text &lt;- c("L'appartement est grand &amp; vraiment bien situe en plein centre",
          "Proportion de femmes dans les situations de famille monoparentale.")
bpe_encode(model, x = text, type = "subwords")
bpe_encode(model, x = text, type = "ids")

encoded &lt;- bpe_encode(model, x = text, type = "ids")
decoded &lt;- bpe_decode(model, encoded)
decoded

## Remove the model file (Clean up for CRAN)
file.remove(model$model_path)
</code></pre>

<hr>
<h2 id='bpe_encode'>Tokenise text alongside a Byte Pair Encoding model</h2><span id='topic+bpe_encode'></span>

<h3>Description</h3>

<p>Tokenise text alongside a Byte Pair Encoding model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bpe_encode(
  model,
  x,
  type = c("subwords", "ids"),
  bos = FALSE,
  eos = FALSE,
  reverse = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bpe_encode_+3A_model">model</code></td>
<td>
<p>an object of class <code>youtokentome</code> as returned by <code><a href="#topic+bpe_load_model">bpe_load_model</a></code></p>
</td></tr>
<tr><td><code id="bpe_encode_+3A_x">x</code></td>
<td>
<p>a character vector of text to tokenise</p>
</td></tr>
<tr><td><code id="bpe_encode_+3A_type">type</code></td>
<td>
<p>a character string, either 'subwords' or 'ids' to get the subwords or the corresponding ids of these subwords as defined in the vocabulary of the model. Defaults to 'subwords'.</p>
</td></tr>
<tr><td><code id="bpe_encode_+3A_bos">bos</code></td>
<td>
<p>logical if set to TRUE then token 'beginning of sentence' will be added</p>
</td></tr>
<tr><td><code id="bpe_encode_+3A_eos">eos</code></td>
<td>
<p>logical if set to TRUE then token 'end of sentence' will be added</p>
</td></tr>
<tr><td><code id="bpe_encode_+3A_reverse">reverse</code></td>
<td>
<p>logical if set to TRUE the output sequence of tokens will be reversed</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(belgium_parliament, package = "tokenizers.bpe")
x &lt;- subset(belgium_parliament, language == "french")
model &lt;- bpe(x$text, coverage = 0.999, vocab_size = 5000, threads = 1)
model
str(model$vocabulary)

text &lt;- c("L'appartement est grand &amp; vraiment bien situe en plein centre",
          "Proportion de femmes dans les situations de famille monoparentale.")
bpe_encode(model, x = text, type = "subwords")
bpe_encode(model, x = text, type = "ids")

encoded &lt;- bpe_encode(model, x = text, type = "ids")
decoded &lt;- bpe_decode(model, encoded)
decoded

## Remove the model file (Clean up for CRAN)
file.remove(model$model_path)
</code></pre>

<hr>
<h2 id='bpe_load_model'>Load a Byte Pair Encoding model</h2><span id='topic+bpe_load_model'></span>

<h3>Description</h3>

<p>Load a Byte Pair Encoding model trained with <code><a href="#topic+bpe">bpe</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bpe_load_model(file, threads = -1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bpe_load_model_+3A_file">file</code></td>
<td>
<p>path to the model</p>
</td></tr>
<tr><td><code id="bpe_load_model_+3A_threads">threads</code></td>
<td>
<p>integer with number of CPU threads to use for model processing. If equal to -1 then minimum of the number of available threads and 8 will be used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>youtokentome</code> which is a list with elements
</p>

<ol>
<li><p>model: an Rcpp pointer to the model
</p>
</li>
<li><p>model_path: the path to the model
</p>
</li>
<li><p>threads: the threads argument
</p>
</li>
<li><p>vocab_size: the size of the BPE vocabulary
</p>
</li>
<li><p>vocabulary: the BPE vocabulary with is a data.frame with columns id and subword
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'>## Reload a model
path  &lt;- system.file(package = "tokenizers.bpe", "extdata", "youtokentome.bpe")
model &lt;- bpe_load_model(path)

## Build a model and load it again

data(belgium_parliament, package = "tokenizers.bpe")
x &lt;- subset(belgium_parliament, language == "french")
model &lt;- bpe(x$text, coverage = 0.999, vocab_size = 5000, threads = 1)
model &lt;- bpe_load_model(model$model_path, threads = 1)

## Remove the model file (Clean up for CRAN)
file.remove(model$model_path)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
