<!DOCTYPE html><html lang="en"><head><title>Help for package parallelly</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {parallelly}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as.cluster'><p>Coerce an Object to a Cluster Object</p></a></li>
<li><a href='#autoStopCluster'><p>Automatically Stop a Cluster when Garbage Collected</p></a></li>
<li><a href='#availableConnections'><p>Number of Available and Free Connections</p></a></li>
<li><a href='#availableCores'><p>Get Number of Available Cores on The Current Machine</p></a></li>
<li><a href='#availableWorkers'><p>Get Set of Available Workers</p></a></li>
<li><a href='#cloneNode'><p>Clone one or more nodes</p></a></li>
<li><a href='#cpuLoad'><p>Get the Recent CPU Load</p></a></li>
<li><a href='#find_rshcmd'><p>Search for SSH clients on the current system</p></a></li>
<li><a href='#freeCores'><p>Get the Average Number of Free CPU Cores</p></a></li>
<li><a href='#freePort'><p>Find a TCP port that can be opened</p></a></li>
<li><a href='#isConnectionValid'><p>Checks if a Connection is Valid</p></a></li>
<li><a href='#isForkedChild'><p>Checks whether or not we are running in a forked child process</p></a></li>
<li><a href='#isForkedNode'><p>Checks whether or not a Cluster Node Runs in a Forked Process</p></a></li>
<li><a href='#isLocalhostNode'><p>Checks whether or not a Cluster Node Runs on Localhost</p></a></li>
<li><a href='#isNodeAlive'><p>Check whether or not the cluster nodes are alive</p></a></li>
<li><a href='#killNode'><p>Terminate one or more cluster nodes using process signaling</p></a></li>
<li><a href='#makeClusterMPI'><p>Create a Message Passing Interface (MPI) Cluster of R Workers for Parallel Processing</p></a></li>
<li><a href='#makeClusterPSOCK'><p>Create a PSOCK Cluster of R Workers for Parallel Processing</p></a></li>
<li><a href='#makeClusterSequential'><p>Create a &quot;parallel&quot; cluster running sequentially in the current session</p></a></li>
<li><a href='#parallelly.options'><p>Options Used by the 'parallelly' Package</p></a></li>
<li><a href='#pid_exists'><p>Check whether a process PID exists or not</p></a></li>
<li><a href='#serializedSize'><p>Calculate the size of an R object when it is serialized</p></a></li>
<li><a href='#supportsMulticore'><p>Check If Forked Processing (&quot;multicore&quot;) is Supported</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.42.0</td>
</tr>
<tr>
<td>Title:</td>
<td>Enhancing the 'parallel' Package</td>
</tr>
<tr>
<td>Imports:</td>
<td>parallel, tools, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>commonmark, base64enc</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>parallelly</td>
</tr>
<tr>
<td>Description:</td>
<td>Utility functions that enhance the 'parallel' package and support the built-in parallel backends of the 'future' package.  For example, availableCores() gives the number of CPU cores available to your R process as given by the operating system, 'cgroups' and Linux containers, R options, and environment variables, including those set by job schedulers on high-performance compute clusters. If none is set, it will fall back to parallel::detectCores(). Another example is makeClusterPSOCK(), which is backward compatible with parallel::makePSOCKcluster() while doing a better job in setting up remote cluster workers without the need for configuring the firewall to do port-forwarding to your local computer.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL (&ge; 2.1)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>TRUE</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>TRUE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://parallelly.futureverse.org">https://parallelly.futureverse.org</a>,
<a href="https://github.com/futureverse/parallelly">https://github.com/futureverse/parallelly</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/futureverse/parallelly/issues">https://github.com/futureverse/parallelly/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-30 15:20:15 UTC; henrik</td>
</tr>
<tr>
<td>Author:</td>
<td>Henrik Bengtsson <a href="https://orcid.org/0000-0002-7579-5165"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph],
  Mike Cheng [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Henrik Bengtsson &lt;henrikb@braju.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-30 16:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as.cluster'>Coerce an Object to a Cluster Object</h2><span id='topic+as.cluster'></span><span id='topic+as.cluster.cluster'></span><span id='topic+as.cluster.list'></span><span id='topic+as.cluster.SOCKnode'></span><span id='topic+as.cluster.SOCK0node'></span><span id='topic+as.cluster.RichSOCKnode'></span><span id='topic+c.cluster'></span>

<h3>Description</h3>

<p>Coerce an Object to a Cluster Object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.cluster(x, ...)

## S3 method for class 'cluster'
as.cluster(x, ...)

## S3 method for class 'list'
as.cluster(x, ...)

## S3 method for class 'SOCKnode'
as.cluster(x, ...)

## S3 method for class 'SOCK0node'
as.cluster(x, ...)

## S3 method for class 'RichSOCKnode'
as.cluster(x, ...)

## S3 method for class 'cluster'
c(..., recursive = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.cluster_+3A_x">x</code></td>
<td>
<p>An object to be coerced.</p>
</td></tr>
<tr><td><code id="as.cluster_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the underlying coercion method.
For <code>c(...)</code>, the clusters and cluster nodes to be combined.</p>
</td></tr>
<tr><td><code id="as.cluster_+3A_recursive">recursive</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>cluster</code>.
</p>
<p><code>c(...)</code> combine multiple clusters and / or cluster nodes into one
cluster returned as an of class <code>cluster</code>.  A warning will be produced if
there are duplicated nodes in the resulting cluster.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cl1 &lt;- makeClusterPSOCK(2, dryrun = TRUE)
cl2 &lt;- makeClusterPSOCK(c("n1", "server.remote.org"), dryrun = TRUE)
cl &lt;- c(cl1, cl2)
print(cl)
</code></pre>

<hr>
<h2 id='autoStopCluster'>Automatically Stop a Cluster when Garbage Collected</h2><span id='topic+autoStopCluster'></span>

<h3>Description</h3>

<p>Registers a finalizer to a cluster such that the cluster will
be stopped when garbage collected
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoStopCluster(cl, debug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="autoStopCluster_+3A_cl">cl</code></td>
<td>
<p>A cluster object created by for instance <code><a href="#topic+makeClusterPSOCK">makeClusterPSOCK()</a></code>
or <code><a href="parallel.html#topic+makeCluster">parallel::makeCluster()</a></code>.</p>
</td></tr>
<tr><td><code id="autoStopCluster_+3A_debug">debug</code></td>
<td>
<p>If TRUE, then debug messages are produced when
the cluster is garbage collected.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cluster is stopped using
<code><a href="parallel.html#topic+makeCluster">stopCluster</a>(cl)</code>.
An alternative to explicitly call this function on an existing
<code>cluster</code> object, is to create the <code>cluster</code> object using
<code>makeClusterPSOCK()</code> with argument <code>autoStop = TRUE</code>.
</p>


<h3>Value</h3>

<p>The cluster object with attribute <code>gcMe</code> set.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cl &lt;- makeClusterPSOCK(2, dryrun = TRUE)
cl &lt;- autoStopCluster(cl)
print(cl)
rm(list = "cl")
gc()
</code></pre>

<hr>
<h2 id='availableConnections'>Number of Available and Free Connections</h2><span id='topic+availableConnections'></span><span id='topic+freeConnections'></span>

<h3>Description</h3>

<p>The number of <a href="base.html#topic+connections">connections</a> that can be open at the same time in <span class="rlang"><b>R</b></span> is
<em>typically</em> 128, where the first three are occupied by the always open
<code><a href="base.html#topic+stdin">stdin()</a></code>, <code><a href="base.html#topic+stdout">stdout()</a></code>, and <code><a href="base.html#topic+stderr">stderr()</a></code> connections, which leaves 125 slots
available for other types of connections.  Connections are used in many
places, e.g. reading and writing to file, downloading URLs, communicating
with parallel <span class="rlang"><b>R</b></span> processes over a socket connections (e.g.
<code><a href="parallel.html#topic+makeCluster">parallel::makeCluster()</a></code> and <code><a href="#topic+makeClusterPSOCK">makeClusterPSOCK()</a></code>), and capturing
standard output via text connections (e.g. <code><a href="utils.html#topic+capture.output">utils::capture.output()</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>availableConnections()

freeConnections()
</code></pre>


<h3>Value</h3>

<p>A non-negative integer, or <code>+Inf</code> if the available number of connections
is greater than 16384, which is a limit be set via option
<code><a href="#topic+parallelly.options">parallelly.availableConnections.tries</a></code>.
</p>


<h3>How to increase the limit</h3>

<p>In R (&gt;= 4.4.0), it is possible to <em>increase</em> the limit of 128 connections
to a greater number via command-line option <code>--max-connections=N</code>, e.g.
</p>
<div class="sourceCode r"><pre>$ Rscript -e "parallelly::availableConnections()"
[1] 128

$ Rscript --max-connections=512 -e "parallelly::availableConnections()"
[1] 512
</pre></div>
<p>For R (&lt; 4.4.0), the limit can only be changed by rebuilding <span class="rlang"><b>R</b></span> from
source, because the limited is hardcoded as a
</p>
<div class="sourceCode c"><pre>#define NCONNECTIONS 128
</pre></div>
<p>in &lsquo;<span class="file">src/main/connections.c</span>&rsquo;.
</p>


<h3>How the limit is identified</h3>

<p>Since the limit <em>might</em> changed, for instance in custom <span class="rlang"><b>R</b></span> builds or in
future releases of <span class="rlang"><b>R</b></span>, we do not want to assume that the limit is 128 for
all <span class="rlang"><b>R</b></span> installation.  Unfortunately, it is not possible to query <span class="rlang"><b>R</b></span> for what
the limit is.
Instead, <code>availableConnections()</code> infers it from trial-and-error.
until it fails.  For efficiency, the result is memoized throughout the
current <span class="rlang"><b>R</b></span> session.
</p>


<h3>References</h3>


<ol>
<li><p> 'WISH: Increase limit of maximum number of open connections (currently 125+3)', 2016-07-09,
<a href="https://github.com/HenrikBengtsson/Wishlist-for-R/issues/28">https://github.com/HenrikBengtsson/Wishlist-for-R/issues/28</a>
</p>
</li></ol>



<h3>See Also</h3>

<p><code><a href="base.html#topic+showConnections">base::showConnections()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>total &lt;- availableConnections()
message("You can have ", total, " connections open in this R installation")
free &lt;- freeConnections()
message("There are ", free, " connections remaining")

</code></pre>

<hr>
<h2 id='availableCores'>Get Number of Available Cores on The Current Machine</h2><span id='topic+availableCores'></span>

<h3>Description</h3>

<p>The current/main <span class="rlang"><b>R</b></span> session counts as one, meaning the minimum
number of cores available is always at least one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>availableCores(
  constraints = NULL,
  methods = getOption2("parallelly.availableCores.methods", c("system",
    "/proc/self/status", "cgroups.cpuset", "cgroups.cpuquota", "cgroups2.cpu.max",
    "nproc", "mc.cores", "BiocParallel", "_R_CHECK_LIMIT_CORES_", "Bioconductor", "LSF",
    "PJM", "PBS", "SGE", "Slurm", "fallback", "custom")),
  na.rm = TRUE,
  logical = getOption2("parallelly.availableCores.logical", TRUE),
  default = c(current = 1L),
  which = c("min", "max", "all"),
  omit = getOption2("parallelly.availableCores.omit", 0L)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="availableCores_+3A_constraints">constraints</code></td>
<td>
<p>An optional character specifying under what
constraints (&quot;purposes&quot;) we are requesting the values.
For instance, on systems where multicore processing is not supported
(i.e. Windows), using <code>constraints = "multicore"</code> will force a
single core to be reported.
Using <code>constraints = "connections"</code>, will append <code>"connections"</code> to
the <code>methods</code> argument.
It is possible to specify multiple constraints, e.g.
<code>constraints = c("connections", "multicore")</code>.</p>
</td></tr>
<tr><td><code id="availableCores_+3A_methods">methods</code></td>
<td>
<p>A character vector specifying how to infer the number
of available cores.</p>
</td></tr>
<tr><td><code id="availableCores_+3A_na.rm">na.rm</code></td>
<td>
<p>If TRUE, only non-missing settings are considered/returned.</p>
</td></tr>
<tr><td><code id="availableCores_+3A_logical">logical</code></td>
<td>
<p>Passed to
<code><a href="parallel.html#topic+detectCores">detectCores</a>(logical = logical)</code>, which,
<em>if supported</em>, returns the number of logical CPUs (TRUE) or physical
CPUs/cores (FALSE).
At least as of R 4.2.2, <code>detectCores()</code> this argument on Linux.
This argument is only if argument <code>methods</code> includes <code>"system"</code>.</p>
</td></tr>
<tr><td><code id="availableCores_+3A_default">default</code></td>
<td>
<p>The default number of cores to return if no non-missing
settings are available.</p>
</td></tr>
<tr><td><code id="availableCores_+3A_which">which</code></td>
<td>
<p>A character specifying which settings to return.
If <code>"min"</code> (default), the minimum value is returned.
If <code>"max"</code>, the maximum value is returned (be careful!)
If <code>"all"</code>, all values are returned.</p>
</td></tr>
<tr><td><code id="availableCores_+3A_omit">omit</code></td>
<td>
<p>(integer; non-negative) Number of cores to not include.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following settings (&quot;methods&quot;) for inferring the number of cores
are supported:
</p>

<ul>
<li> <p><code>"system"</code> -
Query <code><a href="parallel.html#topic+detectCores">detectCores</a>(logical = logical)</code>.
</p>
</li>
<li> <p><code>"/proc/self/status"</code> -
Query <code>Cpus_allowed_list</code> of <code style="white-space: pre;">&#8288;/proc/self/status&#8288;</code>.
</p>
</li>
<li> <p><code>"cgroups.cpuset"</code> -
On Unix, query control group (cgroup v1) value <code>cpuset.set</code>.
</p>
</li>
<li> <p><code>"cgroups.cpuquota"</code> -
On Unix, query control group (cgroup v1) value
<code>cpu.cfs_quota_us</code> / <code>cpu.cfs_period_us</code>.
</p>
</li>
<li> <p><code>"cgroups2.cpu.max"</code> -
On Unix, query control group (cgroup v2) values <code>cpu.max</code>.
</p>
</li>
<li> <p><code>"nproc"</code> -
On Unix, query system command <code>nproc</code>.
</p>
</li>
<li> <p><code>"mc.cores"</code> -
If available, returns the value of option
<code><a href="base.html#topic+options">mc.cores</a></code>.
Note that <code>mc.cores</code> is defined as the number of
<em>additional</em> <span class="rlang"><b>R</b></span> processes that can be used in addition to the
main <span class="rlang"><b>R</b></span> process.  This means that with <code>mc.cores = 0</code> all
calculations should be done in the main <span class="rlang"><b>R</b></span> process, i.e. we have
exactly one core available for our calculations.
The <code>mc.cores</code> option defaults to environment variable
<span class="env">MC_CORES</span> (and is set accordingly when the <span class="pkg">parallel</span>
package is loaded).  The <code>mc.cores</code> option is used by for
instance <code><a href="parallel.html#topic+mclapply">mclapply</a>()</code> of the <span class="pkg">parallel</span>
package.
</p>
</li>
<li> <p><code>"connections"</code> -
Query the current number of available R connections per
<code><a href="#topic+freeConnections">freeConnections()</a></code>.  This is the maximum number of socket-based
<strong>parallel</strong> cluster nodes that are possible launch, because each
one needs its own R connection.
The exception is when <code>freeConnections()</code> is zero, then <code>1L</code> is
still returned, because <code>availableCores()</code> should always return a
positive integer.
</p>
</li>
<li> <p><code>"BiocParallel"</code> -
Query environment variable <span class="env">BIOCPARALLEL_WORKER_NUMBER</span> (integer),
which is defined and used by <strong>BiocParallel</strong> (&gt;= 1.27.2).
If the former is set, this is the number of cores considered.
</p>
</li>
<li> <p><code>"_R_CHECK_LIMIT_CORES_"</code> -
Query environment variable <span class="env">_R_CHECK_LIMIT_CORES_</span> (logical or
<code>"warn"</code>) used by <code style="white-space: pre;">&#8288;R CMD check&#8288;</code> and set to true by
<code style="white-space: pre;">&#8288;R CMD check --as-cran&#8288;</code>. If set to a non-false value, then a maximum
of 2 cores is considered.
</p>
</li>
<li> <p><code>"Bioconductor"</code> -
Query environment variable <span class="env">IS_BIOC_BUILD_MACHINE</span> (logical)
used by the Bioconductor (&gt;= 3.16) build and check system. If set to
true, then a maximum of 4 cores is considered.
</p>
</li>
<li> <p><code>"LSF"</code> -
Query Platform Load Sharing Facility (LSF) environment variable
<span class="env">LSB_DJOB_NUMPROC</span>.
Jobs with multiple (CPU) slots can be submitted on LSF using
<code style="white-space: pre;">&#8288;bsub -n 2 -R "span[hosts=1]" &lt; hello.sh&#8288;</code>.
</p>
</li>
<li> <p><code>"PJM"</code> -
Query Fujitsu Technical Computing Suite (that we choose to shorten
as &quot;PJM&quot;) environment variables <span class="env">PJM_VNODE_CORE</span> and
<span class="env">PJM_PROC_BY_NODE</span>.
The first is set when submitted with <code style="white-space: pre;">&#8288;pjsub -L vnode-core=8 hello.sh&#8288;</code>.
</p>
</li>
<li> <p><code>"PBS"</code> -
Query TORQUE/PBS environment variables <span class="env">PBS_NUM_PPN</span> and <span class="env">NCPUS</span>.
Depending on PBS system configuration, these <em>resource</em>
parameters may or may not default to one.
An example of a job submission that results in this is
<code style="white-space: pre;">&#8288;qsub -l nodes=1:ppn=2&#8288;</code>, which requests one node with two cores.
</p>
</li>
<li> <p><code>"SGE"</code> -
Query Sun Grid Engine/Oracle Grid Engine/Son of Grid Engine (SGE)
and Univa Grid Engine (UGE)/Altair Grid Engine (AGE) environment
variable <span class="env">NSLOTS</span>.
An example of a job submission that results in this is
<code style="white-space: pre;">&#8288;qsub -pe smp 2&#8288;</code> (or <code style="white-space: pre;">&#8288;qsub -pe by_node 2&#8288;</code>), which
requests two cores on a single machine.
</p>
</li>
<li> <p><code>"Slurm"</code> -
Query Simple Linux Utility for Resource Management (Slurm)
environment variable <span class="env">SLURM_CPUS_PER_TASK</span>.
This may or may not be set.  It can be set when submitting a job,
e.g. <code style="white-space: pre;">&#8288;sbatch --cpus-per-task=2 hello.sh&#8288;</code> or by adding
<code style="white-space: pre;">&#8288;#SBATCH --cpus-per-task=2&#8288;</code> to the &lsquo;<span class="file">hello.sh</span>&rsquo; script.
If <span class="env">SLURM_CPUS_PER_TASK</span> is not set, then it will fall back to
use <span class="env">SLURM_CPUS_ON_NODE</span> if the job is a single-node job
(<span class="env">SLURM_JOB_NUM_NODES</span> is 1), e.g. <code style="white-space: pre;">&#8288;sbatch --ntasks=2 hello.sh&#8288;</code>.
To make sure all tasks are assign to a single node, specify
<code>--nodes=1</code>, e.g. <code style="white-space: pre;">&#8288;sbatch --nodes=1 --ntasks=16 hello.sh&#8288;</code>.
</p>
</li>
<li> <p><code>"custom"</code> -
If option
<code><a href="#topic+parallelly.options">parallelly.availableCores.custom</a></code>
is set and a function,
then this function will be called (without arguments) and it's value
will be coerced to an integer, which will be interpreted as a number
of available cores.  If the value is NA, then it will be ignored.
It is safe for this custom function to call <code>availableCores()</code>; if
done, the custom function will <em>not</em> be recursively called.
</p>
</li></ul>

<p>For any other value of a <code>methods</code> element, the <span class="rlang"><b>R</b></span> option with the
same name is queried.  If that is not set, the system environment
variable is queried.  If neither is set, a missing value is returned.
</p>


<h3>Value</h3>

<p>Return a positive (&gt;= 1) integer.
If <code>which = "all"</code>, then more than one value may be returned.
Together with <code>na.rm = FALSE</code> missing values may also be returned.
</p>


<h3>Avoid ending up with zero cores</h3>

<p>Note that some machines might have a limited number of cores, or the R
process runs in a container or a cgroup that only provides a small number
of cores.  In such cases:
</p>
<div class="sourceCode r"><pre>ncores &lt;- availableCores() - 1
</pre></div>
<p>may return zero, which is often not intended and is likely to give an
error downstream.  Instead, use:
</p>
<div class="sourceCode r"><pre>ncores &lt;- availableCores(omit = 1)
</pre></div>
<p>to put aside one of the cores from being used.  Regardless how many cores
you put aside, this function is guaranteed to return at least one core.
</p>


<h3>Advanced usage</h3>

<p>It is possible to override the maximum number of cores on the machine
as reported by <code>availableCores(methods = "system")</code>.  This can be
done by first specifying
<code>options(parallelly.availableCores.methods = "mc.cores")</code> and
then the number of cores to use, e.g. <code>options(mc.cores = 8)</code>.
</p>


<h3>See Also</h3>

<p>To get the set of available workers regardless of machine,
see <code><a href="#topic+availableWorkers">availableWorkers()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>message(paste("Number of cores available:", availableCores()))

## Not run: 
options(mc.cores = 2L)
message(paste("Number of cores available:", availableCores()))

## End(Not run)

## Not run: 
## IMPORTANT: availableCores() may return 1L
options(mc.cores = 1L)
ncores &lt;- availableCores() - 1      ## ncores = 0
ncores &lt;- availableCores(omit = 1)  ## ncores = 1
message(paste("Number of cores to use:", ncores))

## End(Not run)

## Not run: 
## Use 75% of the cores on the system but never more than four
options(parallelly.availableCores.custom = function() {
  ncores &lt;- max(parallel::detectCores(), 1L, na.rm = TRUE)
  ncores &lt;- min(as.integer(0.75 * ncores), 4L)
  max(1L, ncores)
})
message(paste("Number of cores available:", availableCores()))

## Use 50% of the cores according to availableCores(), e.g.
## allocated by a job scheduler or cgroups.
## Note that it is safe to call availableCores() here.
options(parallelly.availableCores.custom = function() {
  0.50 * parallelly::availableCores()
})
message(paste("Number of cores available:", availableCores()))

## End(Not run)

</code></pre>

<hr>
<h2 id='availableWorkers'>Get Set of Available Workers</h2><span id='topic+availableWorkers'></span>

<h3>Description</h3>

<p>Get Set of Available Workers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>availableWorkers(
  constraints = NULL,
  methods = getOption2("parallelly.availableWorkers.methods", c("mc.cores",
    "BiocParallel", "_R_CHECK_LIMIT_CORES_", "Bioconductor", "LSF", "PJM", "PBS", "SGE",
    "Slurm", "custom", "cgroups.cpuset", "cgroups.cpuquota", "cgroups2.cpu.max", "nproc",
    "system", "fallback")),
  na.rm = TRUE,
  logical = getOption2("parallelly.availableCores.logical", TRUE),
  default = getOption2("parallelly.localhost.hostname", "localhost"),
  which = c("auto", "min", "max", "all")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="availableWorkers_+3A_constraints">constraints</code></td>
<td>
<p>An optional character specifying under what
constraints (&quot;purposes&quot;) we are requesting the values.
Using <code>constraints = "connections"</code>, will append <code>"connections"</code> to
the <code>methods</code> argument.</p>
</td></tr>
<tr><td><code id="availableWorkers_+3A_methods">methods</code></td>
<td>
<p>A character vector specifying how to infer the number
of available cores.</p>
</td></tr>
<tr><td><code id="availableWorkers_+3A_na.rm">na.rm</code></td>
<td>
<p>If TRUE, only non-missing settings are considered/returned.</p>
</td></tr>
<tr><td><code id="availableWorkers_+3A_logical">logical</code></td>
<td>
<p>Passed as-is to <code><a href="#topic+availableCores">availableCores()</a></code>.</p>
</td></tr>
<tr><td><code id="availableWorkers_+3A_default">default</code></td>
<td>
<p>The default set of workers.</p>
</td></tr>
<tr><td><code id="availableWorkers_+3A_which">which</code></td>
<td>
<p>A character specifying which set / sets to return.
If <code>"auto"</code> (default), the first non-empty set found.
If <code>"min"</code>, the minimum value is returned.
If <code>"max"</code>, the maximum value is returned (be careful!)
If <code>"all"</code>, all values are returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default set of workers for each method is
<code>rep("localhost", times = availableCores(methods = method, logical = logical))</code>,
which means that each will at least use as many parallel workers on the
current machine that <code><a href="#topic+availableCores">availableCores()</a></code> allows for that method.
</p>
<p>In addition, the following settings (&quot;methods&quot;) are also acknowledged:
</p>

<ul>
<li> <p><code>"LSF"</code> -
Query LSF/OpenLava environment variable <span class="env">LSB_HOSTS</span>.
</p>
</li>
<li> <p><code>"PJM"</code> -
Query Fujitsu Technical Computing Suite (that we choose to shorten
as &quot;PJM&quot;) the hostname file given by environment variable
<span class="env">PJM_O_NODEINF</span>.
The <span class="env">PJM_O_NODEINF</span> file lists the hostnames of the nodes allotted.
This function returns those hostnames each repeated <code>availableCores()</code>
times, where <code>availableCores()</code> reflects <span class="env">PJM_VNODE_CORE</span>.
For example, for <code style="white-space: pre;">&#8288;pjsub -L vnode=2 -L vnode-core=8 hello.sh&#8288;</code>, the
<span class="env">PJM_O_NODEINF</span> file gives two hostnames, and <span class="env">PJM_VNODE_CORE</span>
gives eight cores per host, resulting in a character vector of 16
hostnames (for two unique hostnames).
</p>
</li>
<li> <p><code>"PBS"</code> -
Query TORQUE/PBS environment variable <span class="env">PBS_NODEFILE</span>.
If this is set and specifies an existing file, then the set
of workers is read from that file, where one worker (node)
is given per line.
An example of a job submission that results in this is
<code style="white-space: pre;">&#8288;qsub -l nodes=4:ppn=2&#8288;</code>, which requests four nodes each
with two cores.
</p>
</li>
<li> <p><code>"SGE"</code> -
Query Sun Grid Engine/Oracle Grid Engine/Son of Grid Engine (SGE)
and Univa Grid Engine (UGE)/Altair Grid Engine (AGE) environment
variable <span class="env">PE_HOSTFILE</span>.
An example of a job submission that results in this is
<code style="white-space: pre;">&#8288;qsub -pe mpi 8&#8288;</code> (or <code style="white-space: pre;">&#8288;qsub -pe ompi 8&#8288;</code>), which
requests eight cores on a any number of machines.
</p>
</li>
<li> <p><code>"Slurm"</code> -
Query Slurm environment variable <span class="env">SLURM_JOB_NODELIST</span> (fallback
to legacy <span class="env">SLURM_NODELIST</span>) and parse set of nodes.
Then query Slurm environment variable <span class="env">SLURM_JOB_CPUS_PER_NODE</span>
(fallback <span class="env">SLURM_TASKS_PER_NODE</span>) to infer how many CPU cores
Slurm have allotted to each of the nodes.  If <span class="env">SLURM_CPUS_PER_TASK</span>
is set, which is always a scalar, then that is respected too, i.e.
if it is smaller, then that is used for all nodes.
For example, if <code>SLURM_NODELIST="n1,n[03-05]"</code> (expands to
<code>c("n1", "n03", "n04", "n05")</code>) and <code>SLURM_JOB_CPUS_PER_NODE="2(x2),3,2"</code>
(expands to <code>c(2, 2, 3, 2)</code>), then
<code>c("n1", "n1", "n03", "n03", "n04", "n04", "n04", "n05", "n05")</code> is
returned.  If in addition, <code>SLURM_CPUS_PER_TASK=1</code>, which can happen
depending on hyperthreading configurations on the Slurm cluster, then
<code>c("n1", "n03", "n04", "n05")</code> is returned.
</p>
</li>
<li> <p><code>"custom"</code> -
If option
<code><a href="#topic+parallelly.options">parallelly.availableWorkers.custom</a></code>
is set and a function,
then this function will be called (without arguments) and it's value
will be coerced to a character vector, which will be interpreted as
hostnames of available workers.
It is safe for this custom function to call <code>availableWorkers()</code>; if
done, the custom function will <em>not</em> be recursively called.
</p>
</li></ul>



<h3>Value</h3>

<p>Return a character vector of workers, which typically consists
of names of machines / compute nodes, but may also be IP numbers.
</p>


<h3>Known limitations</h3>

<p><code>availableWorkers(methods = "Slurm")</code> will expand <span class="env">SLURM_JOB_NODELIST</span>
using <code>scontrol show hostnames "$SLURM_JOB_NODELIST"</code>, if available.
If not available, then it attempts to parse the compressed nodelist based
on a best-guess understanding on what the possible syntax may be.
One known limitation is that &quot;multi-dimensional&quot; ranges are not supported,
e.g. <code>"a[1-2]b[3-4]"</code> is expanded by <code>scontrol</code> to
<code>c("a1b3", "a1b4", "a2b3", "a2b4")</code>.  If <code>scontrol</code> is not
available, then any components that failed to be parsed are dropped with
an informative warning message.  If no components could be parsed, then
the result of <code>methods = "Slurm"</code> will be empty.
</p>


<h3>See Also</h3>

<p>To get the number of available workers on the current machine,
see <code><a href="#topic+availableCores">availableCores()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>message(paste("Available workers:",
        paste(sQuote(availableWorkers()), collapse = ", ")))

## Not run: 
options(mc.cores = 2L)
message(paste("Available workers:",
        paste(sQuote(availableWorkers()), collapse = ", ")))

## End(Not run)

## Not run: 
## Always use two workers on host 'n1' and one on host 'n2'
options(parallelly.availableWorkers.custom = function() {
  c("n1", "n1", "n2")
})
message(paste("Available workers:",
        paste(sQuote(availableWorkers()), collapse = ", ")))

## End(Not run)

## Not run: 
## A 50% random subset of the available workers.
## Note that it is safe to call availableWorkers() here.
options(parallelly.availableWorkers.custom = function() {
  workers &lt;- parallelly::availableWorkers()
  sample(workers, size = 0.50 * length(workers))
})
message(paste("Available workers:",
        paste(sQuote(availableWorkers()), collapse = ", ")))

## End(Not run)

</code></pre>

<hr>
<h2 id='cloneNode'>Clone one or more nodes</h2><span id='topic+cloneNode'></span>

<h3>Description</h3>

<p>Clone one or more nodes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cloneNode(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cloneNode_+3A_x">x</code></td>
<td>
<p>A cluster node or a cluster.</p>
</td></tr>
<tr><td><code id="cloneNode_+3A_...">...</code></td>
<td>
<p>Optional arguments overriding the recorded ones.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>class(x)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cl &lt;- makeClusterPSOCK(2)
print(cl)

## Terminate the second cluster node
parallel::stopCluster(cl[2])

## Show that cluster node #2 is no longer alive (wait a bit first)
Sys.sleep(1.0)
print(isNodeAlive(cl))
print(cl)

## "Restart" it
cl[2] &lt;- cloneNode(cl[2])
print(cl)

## Check all nodes
print(isNodeAlive(cl))


</code></pre>

<hr>
<h2 id='cpuLoad'>Get the Recent CPU Load</h2><span id='topic+cpuLoad'></span>

<h3>Description</h3>

<p>Get the Recent CPU Load
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpuLoad()
</code></pre>


<h3>Details</h3>

<p>This function works only Unix-like system with &lsquo;<span class="file">/proc/loadavg</span>&rsquo;.
</p>


<h3>Value</h3>

<p>A named numeric vector with three elements <code style="white-space: pre;">&#8288;1min&#8288;</code>, <code style="white-space: pre;">&#8288;5min&#8288;</code>, and
<code style="white-space: pre;">&#8288;15min&#8288;</code> with non-negative values.
These values represent estimates of the CPU load during the last minute,
the last five minutes, and the last fifteen minutes [1].
An idle system have values close to zero, and a heavily loaded system
have values near <code>parallel::detectCores()</code>.
If they are unknown, missing values are returned.
</p>


<h3>References</h3>


<ol>
<li><p> Linux Load Averages: Solving the Mystery,
Brendan Gregg's Blog, 2017-08-08,
<a href="http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html">http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html</a>
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'>loadavg &lt;- cpuLoad()
print(loadavg)
</code></pre>

<hr>
<h2 id='find_rshcmd'>Search for SSH clients on the current system</h2><span id='topic+find_rshcmd'></span>

<h3>Description</h3>

<p>Search for SSH clients on the current system
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_rshcmd(which = NULL, first = FALSE, must_work = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="find_rshcmd_+3A_which">which</code></td>
<td>
<p>A character vector specifying which types of SSH clients
to search for.  If NULL, a default set of clients supported by the
current platform is searched for.</p>
</td></tr>
<tr><td><code id="find_rshcmd_+3A_first">first</code></td>
<td>
<p>If TRUE, the first client found is returned, otherwise
all located clients are returned.</p>
</td></tr>
<tr><td><code id="find_rshcmd_+3A_must_work">must_work</code></td>
<td>
<p>If TRUE and no clients was found, then an error
is produced, otherwise only a warning.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of pathnames to all located SSH clients.
The pathnames may be followed by zero or more command-line options,
i.e. the elements of the returned list are character vectors of length
one or more.
If <code>first = TRUE</code>, only the first one is returned.
Attribute <code>version</code> contains the output from querying the
executable for its version (via command-line option <code>-V</code>).
</p>

<hr>
<h2 id='freeCores'>Get the Average Number of Free CPU Cores</h2><span id='topic+freeCores'></span>

<h3>Description</h3>

<p>Get the Average Number of Free CPU Cores
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freeCores(
  memory = c("5min", "15min", "1min"),
  fraction = 0.9,
  logical = getOption2("parallelly.availableCores.logical", TRUE),
  default = parallelly::availableCores()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="freeCores_+3A_memory">memory</code></td>
<td>
<p>(character) The time period used to infer the system load,
with alternatives being 5 minutes (default), 15 minutes, or 1 minute.</p>
</td></tr>
<tr><td><code id="freeCores_+3A_fraction">fraction</code></td>
<td>
<p>(non-negative numeric) A scale factor.</p>
</td></tr>
<tr><td><code id="freeCores_+3A_logical">logical</code></td>
<td>
<p>Passed as-is to <code><a href="#topic+availableCores">availableCores()</a></code>.</p>
</td></tr>
<tr><td><code id="freeCores_+3A_default">default</code></td>
<td>
<p>(integer) The value to be returned if the system load is
unknown, i.e. <code><a href="#topic+cpuLoad">cpuLoad()</a></code> return missing values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An positive integer with attributes <code>loadavg</code> (named numeric),
<code>maxCores</code> (named integer), argument <code>memory</code> (character), and
argument <code>fraction</code> (numeric).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>free &lt;- freeCores()
print(free)

## Not run: 
## Make availableCores() agile to the system load
options(parallelly.availableCores.custom = function() freeCores())

## End(Not run)
</code></pre>

<hr>
<h2 id='freePort'>Find a TCP port that can be opened</h2><span id='topic+freePort'></span>

<h3>Description</h3>

<p>Find a TCP port that can be opened
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freePort(ports = 1024:65535, default = "random", randomize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="freePort_+3A_ports">ports</code></td>
<td>
<p>(integer vector, or character string)
Zero or more TCP ports in [0, 65535] to scan.
If <code>"random"</code>, then a random set of ports is considered.
If <code>"auto"</code>, then the port given by environment variable
<span class="env">R_PARALLEL_PORT</span> is used, which may also specify <code>random</code>.</p>
</td></tr>
<tr><td><code id="freePort_+3A_default">default</code></td>
<td>
<p>(integer) <code>NA_integer_</code> or a port to returned if
an available port could not be found.
If <code>"first"</code>, then <code>ports[1]</code>.  If <code>"random"</code>, then a random port
among <code>ports</code> is used. If <code>length(ports) == 0</code>, then <code>NA_integer_</code>.</p>
</td></tr>
<tr><td><code id="freePort_+3A_randomize">randomize</code></td>
<td>
<p>(logical) If TRUE, <code>ports</code> is randomly shuffled
before searched.  This shuffle does <em>not</em> forward the RNG seed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an integer representing the first port among <code>ports</code> that
can be opened.  If none can be opened, then <code>default</code> is returned.
</p>

<hr>
<h2 id='isConnectionValid'>Checks if a Connection is Valid</h2><span id='topic+isConnectionValid'></span><span id='topic+connectionId'></span>

<h3>Description</h3>

<p>Get a unique identifier for an R <a href="base.html#topic+connections">connection</a>
and check whether or not the connection is still valid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isConnectionValid(con)

connectionId(con)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isConnectionValid_+3A_con">con</code></td>
<td>
<p>A <a href="base.html#topic+connections">connection</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>isConnectionValid()</code> returns TRUE if the connection is still valid,
otherwise FALSE.  If FALSE, then character attribute <code>reason</code> provides
an explanation why the connection is not valid.
</p>
<p><code>connectionId()</code> returns an non-negative integer, -1, or <code>NA_integer_</code>.
For connections stdin, stdout, and stderr, 0, 1, and 2, are returned,
respectively.  For all other connections, an integer greater or equal to
3 based on the connection's internal pointer is returned.
A connection that has been serialized, which is no longer valid, has
identifier -1.
Attribute <code>raw_id</code> returns the pointer string from which the above is
inferred.
</p>


<h3>Connection Index versus Connection Identifier</h3>

<p>R represents <a href="base.html#topic+connections">connections</a> as indices using plain
integers, e.g. <code>idx &lt;- as.integer(con)</code>.
The three connections standard input (&quot;stdin&quot;), standard output (&quot;stdout&quot;),
and standard error (&quot;stderr&quot;) always exists and have indices 0, 1, and 2.
Any connection opened beyond these will get index three or greater,
depending on availability as given by <code><a href="base.html#topic+showConnections">base::showConnections()</a></code>.
To get the connection with a given index, use <code><a href="base.html#topic+showConnections">base::getConnection()</a></code>.
<strong>Unfortunately, this index representation of connections is non-robust</strong>,
e.g. there are cases where two or more 'connection' objects can end up with
the same index and if used, the written output may end up at the wrong
destination and files and database might get corrupted.  This can for
instance happen if <code><a href="base.html#topic+showConnections">base::closeAllConnections()</a></code> is used (*).
<strong>In contrast, <code>id &lt;- connectionId(con)</code> gives an identifier that is unique
to that 'connection' object.</strong>  This identifier is based on the internal
pointer address of the object.  The risk for two connections in the same
<span class="rlang"><b>R</b></span> session to end up with the same pointer address is very small.
Thus, in case we ended up in a situation where two connections <code>con1</code> and
<code>con2</code> share the same index&mdash;<code>as.integer(con1) == as.integer(con2)</code>&mdash;
they will never share the same identifier&mdash;
<code>connectionId(con1) != connectionId(con2)</code>.
Here, <code>isConnectionValid()</code> can be used to check which one of these
connections, if any, are valid.
</p>
<p>(*) Note that there is no good reason for calling <code>closeAllConnections()</code>
If called, there is a great risk that the files get corrupted etc.
See (1) for examples and details on this problem.
If you think there is a need to use it, it is much safer to restart <span class="rlang"><b>R</b></span>
because that is guaranteed to give you a working <span class="rlang"><b>R</b></span> session with
non-clashing connections.
It might also be that <code>closeAllConnections()</code> is used because
<code><a href="base.html#topic+base-internal">base::sys.save.image()</a></code> is called, which might happen if <span class="rlang"><b>R</b></span> is being
forced to terminate.
</p>


<h3>Connections Cannot be Serialized Or Saved</h3>

<p>A 'connection' cannot be serialized, e.g. it cannot be saved to file to
be read and used in another <span class="rlang"><b>R</b></span> session.  If attempted, the connection will
not be valid.  This is a problem that may occur in parallel processing
when passing an <span class="rlang"><b>R</b></span> object to parallel worker for further processing, e.g.
the exported object may hold an internal database connection which will
no longer be valid on the worker.
When a connection is serialized, its internal pointer address will be
invalidated (set to nil). In such cases, <code>connectionId(con)</code> returns -1
and <code>isConnectionValid(con)</code> returns FALSE.
</p>


<h3>References</h3>


<ol>
<li> <p><a href="https://github.com/HenrikBengtsson/Wishlist-for-R/issues/81">'BUG: A <code>connection</code> object may become corrupt and re-referenced to another connection (PATCH)'</a>, 2018-10-30.
</p>
</li>
<li><p> R-devel thread <a href="https://stat.ethz.ch/pipermail/r-devel/2018-October/077004.html">PATCH: Asserting that 'connection' used has not changed + R_GetConnection2()</a>, 2018-10-31.
</p>
</li></ol>



<h3>See Also</h3>

<p>See <code><a href="base.html#topic+showConnections">base::showConnections()</a></code> for currently open connections and their
indices. To get a connection by its index, use <code><a href="base.html#topic+showConnections">base::getConnection()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## R represents connections as plain indices
as.integer(stdin())          ## int 0
as.integer(stdout())         ## int 1
as.integer(stderr())         ## int 2

## The first three connections always exist and are always valid
isConnectionValid(stdin())   ## TRUE
connectionId(stdin())        ## 0L
isConnectionValid(stdout())  ## TRUE
connectionId(stdout())       ## 1L
isConnectionValid(stderr())  ## TRUE
connectionId(stderr())       ## 2L

## Connections cannot be serialized
con &lt;- file(tempfile(), open = "w")
x &lt;- list(value = 42, stderr = stderr(), con = con)
y &lt;- unserialize(serialize(x, connection = NULL))
isConnectionValid(y$stderr)  ## TRUE
connectionId(y$stderr)       ##  2L
isConnectionValid(y$con)     ## FALSE with attribute 'reason'
connectionId(y$con)          ## -1L
close(con)

</code></pre>

<hr>
<h2 id='isForkedChild'>Checks whether or not we are running in a forked child process</h2><span id='topic+isForkedChild'></span>

<h3>Description</h3>

<p>Checks whether or not we are running in a forked child process
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isForkedChild()
</code></pre>


<h3>Details</h3>

<p>Examples of setups and functions that rely on <em>forked</em> parallelization
are <code>parallel::makeCluster(n, type = "FORK")</code>, <code>parallel::mclapply()</code>,
and <code>future::plan("multicore")</code>.
</p>


<h3>Value</h3>

<p>(logical) Returns TRUE if the running in a forked child
process, otherwise FALSE.
</p>

<hr>
<h2 id='isForkedNode'>Checks whether or not a Cluster Node Runs in a Forked Process</h2><span id='topic+isForkedNode'></span>

<h3>Description</h3>

<p>Checks whether or not a Cluster Node Runs in a Forked Process
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isForkedNode(node, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isForkedNode_+3A_node">node</code></td>
<td>
<p>A cluster node of class <code>SOCKnode</code> or <code>SOCK0node</code>.</p>
</td></tr>
<tr><td><code id="isForkedNode_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(logical) Returns TRUE if the cluster node is running in a
forked child process and FALSE if it does not.
If it cannot be inferred, NA is returned.
</p>

<hr>
<h2 id='isLocalhostNode'>Checks whether or not a Cluster Node Runs on Localhost</h2><span id='topic+isLocalhostNode'></span>

<h3>Description</h3>

<p>Checks whether or not a Cluster Node Runs on Localhost
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isLocalhostNode(node, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isLocalhostNode_+3A_node">node</code></td>
<td>
<p>A cluster node of class <code>SOCKnode</code> or <code>SOCK0node</code>.</p>
</td></tr>
<tr><td><code id="isLocalhostNode_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(logical) Returns TRUE if the cluster node is running on the
current machine and FALSE if it runs on another machine.
If it cannot be inferred, NA is returned.
</p>

<hr>
<h2 id='isNodeAlive'>Check whether or not the cluster nodes are alive</h2><span id='topic+isNodeAlive'></span>

<h3>Description</h3>

<p>Check whether or not the cluster nodes are alive
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isNodeAlive(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isNodeAlive_+3A_x">x</code></td>
<td>
<p>A cluster or a cluster node (&quot;worker&quot;).</p>
</td></tr>
<tr><td><code id="isNodeAlive_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function works by checking whether the cluster node process is
running or not.  This is done by querying the system for its process
ID (PID), which is registered by <code><a href="#topic+makeClusterPSOCK">makeClusterPSOCK()</a></code> when the node
starts. If the PID is not known, the NA is returned.
On Unix and macOS, the PID is queried using <code><a href="tools.html#topic+pskill">tools::pskill()</a></code> with
fallback to <code>system("ps")</code>. On MS Windows, <code>system2("tasklist")</code> is used,
which may take a long time if there are a lot of processes running.
For details, see the <em>internal</em> <code><a href="#topic+pid_exists">pid_exists()</a></code> function.
</p>


<h3>Value</h3>

<p>A logical vector of length <code>length(x)</code> with values
FALSE, TRUE, and NA.  If it can be established that the
process for a cluster node is running, then TRUE is returned.
If it does not run, then FALSE is returned.
If neither can be inferred, or it times out, then NA is returned.
</p>


<h3>See Also</h3>

<p>Use <code><a href="parallel.html#topic+makeCluster">parallel::stopCluster()</a></code> to shut down cluster nodes.
If that's not sufficient, <code><a href="#topic+killNode">killNode()</a></code> may be attempted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cl &lt;- makeClusterPSOCK(2)

## Check if cluster node #2 is alive
print(isNodeAlive(cl[[2]]))

## Check all nodes
print(isNodeAlive(cl))


</code></pre>

<hr>
<h2 id='killNode'>Terminate one or more cluster nodes using process signaling</h2><span id='topic+killNode'></span>

<h3>Description</h3>

<p>Terminate one or more cluster nodes using process signaling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>killNode(x, signal = tools::SIGTERM, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="killNode_+3A_x">x</code></td>
<td>
<p>cluster or cluster node to terminate.</p>
</td></tr>
<tr><td><code id="killNode_+3A_signal">signal</code></td>
<td>
<p>An integer that specifies the signal level to be sent
to the parallel R process.
It's only <code>tools::SIGINT</code> (2) and <code>tools::SIGTERM</code> (15) that are
supported on all operating systems (i.e. Unix, macOS, and MS Windows).
All other signals are platform specific, cf. <code><a href="tools.html#topic+pskill">tools::pskill()</a></code>.</p>
</td></tr>
<tr><td><code id="killNode_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the preferred way to terminate a cluster is via
<code><a href="parallel.html#topic+makeCluster">parallel::stopCluster()</a></code>, because it terminates the cluster nodes
by kindly asking each of them to nicely shut themselves down.
Using <code>killNode()</code> is a much more sever approach. It abruptly
terminates the underlying R process, possibly without giving the
parallel worker a chance to terminate gracefully.  For example,
it might get terminated in the middle of writing to file.
</p>
<p><code><a href="tools.html#topic+pskill">tools::pskill()</a></code> is used to send the signal to the R process hosting
the parallel worker.
</p>


<h3>Value</h3>

<p>TRUE if the signal was successfully applied, FALSE if not, and NA if
signaling is not supported on the specific cluster or node.
<em>Warning</em>: With R (&lt; 3.5.0), NA is always returned. This is due to a
bug in R (&lt; 3.5.0), where the signaling result cannot be trusted.
</p>


<h3>Known limitations</h3>

<p>This function works only with cluster nodes of class <code>RichSOCKnode</code>,
which were created by <code><a href="#topic+makeClusterPSOCK">makeClusterPSOCK()</a></code>.  It does not work when
using <code><a href="parallel.html#topic+makeCluster">parallel::makeCluster()</a></code> and friends.
</p>
<p>Currently, it's only possible to send signals to parallel workers, that
is, cluster nodes, that run on the local machine.
If attempted to use <code>killNode()</code> on a remote parallel workers, <code>NA</code>
is returned and an informative warning is produced.
</p>


<h3>See Also</h3>

<p>Use <code><a href="#topic+isNodeAlive">isNodeAlive()</a></code> to check whether one or more cluster nodes are alive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cl &lt;- makeClusterPSOCK(2)
print(isNodeAlive(cl))  ## [1] TRUE TRUE

res &lt;- killNode(cl)
print(res)

## It might take a moment before the background
## workers are shutdown after having been signaled
Sys.sleep(1.0)

print(isNodeAlive(cl))  ## [1] FALSE FALSE


</code></pre>

<hr>
<h2 id='makeClusterMPI'>Create a Message Passing Interface (MPI) Cluster of R Workers for Parallel Processing</h2><span id='topic+makeClusterMPI'></span><span id='topic+MPI'></span>

<h3>Description</h3>

<p>The <code>makeClusterMPI()</code> function creates an MPI cluster of <span class="rlang"><b>R</b></span> workers
for parallel processing.  This function utilizes
<code>makeCluster(..., type = "MPI")</code> of the <span class="pkg">parallel</span> package and
tweaks the cluster in an attempt to avoid
<code><a href="parallel.html#topic+makeCluster">stopCluster()</a></code> from hanging (1).
<em>WARNING: This function is very much in a beta version and should
only be used if <code>parallel::makeCluster(..., type = "MPI")</code> fails.</em>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeClusterMPI(
  workers,
  ...,
  autoStop = FALSE,
  verbose = getOption2("parallelly.debug", FALSE)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="makeClusterMPI_+3A_workers">workers</code></td>
<td>
<p>The number workers (as a positive integer).</p>
</td></tr>
<tr><td><code id="makeClusterMPI_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to
<code><a href="parallel.html#topic+makeCluster">makeCluster</a>(workers, type = "MPI", ...)</code>.</p>
</td></tr>
<tr><td><code id="makeClusterMPI_+3A_autostop">autoStop</code></td>
<td>
<p>If TRUE, the cluster will be automatically stopped
using <code><a href="parallel.html#topic+makeCluster">stopCluster</a>()</code> when it is
garbage collected, unless already stopped.  See also <code><a href="#topic+autoStopCluster">autoStopCluster()</a></code>.</p>
</td></tr>
<tr><td><code id="makeClusterMPI_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, informative messages are outputted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>Creating MPI clusters requires that the <span class="pkg">Rmpi</span> and <span class="pkg">snow</span>
packages are installed.</em>
</p>


<h3>Value</h3>

<p>An object of class <code>c("RichMPIcluster", "MPIcluster", "cluster")</code> consisting
of a list of <code>"MPInode"</code> workers.
</p>


<h3>Alternative usage</h3>

<p>In R (&gt;= 4.4.0), an alternatively to using
<code>cl &lt;- parallelly::makeClusterMPI(workers)</code> is:
</p>
<div class="sourceCode"><pre>cl &lt;- parallel::makeCluster(workers, type = parallelly::MPI)
</pre></div>


<h3>References</h3>


<ol>
<li><p> R-sig-hpc thread <a href="https://stat.ethz.ch/pipermail/r-sig-hpc/2017-September/002065.html">Rmpi: mpi.close.Rslaves() 'hangs'</a> on 2017-09-28.
</p>
</li></ol>



<h3>See Also</h3>

<p><code><a href="#topic+makeClusterPSOCK">makeClusterPSOCK()</a></code> and <code><a href="parallel.html#topic+makeCluster">parallel::makeCluster()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
if (requireNamespace("Rmpi") &amp;&amp; requireNamespace("snow")) {
  cl &lt;- makeClusterMPI(2, autoStop = TRUE)
  print(cl)
  y &lt;- parLapply(cl, X = 1:3, fun = sqrt)
  print(y)
  rm(list = "cl")
}

## End(Not run)

</code></pre>

<hr>
<h2 id='makeClusterPSOCK'>Create a PSOCK Cluster of R Workers for Parallel Processing</h2><span id='topic+makeClusterPSOCK'></span><span id='topic+PSOCK'></span><span id='topic+makeNodePSOCK'></span>

<h3>Description</h3>

<p>The <code>makeClusterPSOCK()</code> function creates a cluster of <span class="rlang"><b>R</b></span> workers
for parallel processing.  These <span class="rlang"><b>R</b></span> workers may be background <span class="rlang"><b>R</b></span> sessions
on the current machine, <span class="rlang"><b>R</b></span> sessions on external machines (local or remote),
or a mix of such. For external workers, the default is to use SSH to
connect to those external machines.  This function works similarly to
<code><a href="parallel.html#topic+makeCluster">makePSOCKcluster</a>()</code> of the
<span class="pkg">parallel</span> package, but provides additional and more flexibility
options for controlling the setup of the system calls that launch the
background <span class="rlang"><b>R</b></span> workers, and how to connect to external machines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeClusterPSOCK(
  workers,
  makeNode = makeNodePSOCK,
  port = c("auto", "random"),
  user = NULL,
  ...,
  autoStop = FALSE,
  tries = getOption2("parallelly.makeNodePSOCK.tries", 3L),
  delay = getOption2("parallelly.makeNodePSOCK.tries.delay", 15),
  validate = getOption2("parallelly.makeNodePSOCK.validate", TRUE),
  verbose = getOption2("parallelly.debug", FALSE)
)

makeNodePSOCK(
  worker = getOption2("parallelly.localhost.hostname", "localhost"),
  master = NULL,
  port,
  connectTimeout = getOption2("parallelly.makeNodePSOCK.connectTimeout", 2 * 60),
  timeout = getOption2("parallelly.makeNodePSOCK.timeout", 30 * 24 * 60 * 60),
  rscript = NULL,
  homogeneous = NULL,
  rscript_args = NULL,
  rscript_envs = NULL,
  rscript_libs = NULL,
  rscript_startup = NULL,
  rscript_sh = c("auto", "cmd", "sh", "none"),
  default_packages = c("datasets", "utils", "grDevices", "graphics", "stats", if
    (methods) "methods"),
  methods = TRUE,
  socketOptions = getOption2("parallelly.makeNodePSOCK.socketOptions", "no-delay"),
  useXDR = getOption2("parallelly.makeNodePSOCK.useXDR", FALSE),
  outfile = "/dev/null",
  renice = NA_integer_,
  rshcmd = getOption2("parallelly.makeNodePSOCK.rshcmd", NULL),
  user = NULL,
  revtunnel = NA,
  rshlogfile = NULL,
  rshopts = getOption2("parallelly.makeNodePSOCK.rshopts", NULL),
  rank = 1L,
  manual = FALSE,
  dryrun = FALSE,
  quiet = FALSE,
  setup_strategy = getOption2("parallelly.makeNodePSOCK.setup_strategy", "parallel"),
  action = c("launch", "options"),
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="makeClusterPSOCK_+3A_workers">workers</code></td>
<td>
<p>The hostnames of workers (as a character vector) or the
number of localhost workers (as a positive integer).</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_makenode">makeNode</code></td>
<td>
<p>A function that creates a <code>"SOCKnode"</code> or
<code>"SOCK0node"</code> object, which represents a connection to a worker.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_port">port</code></td>
<td>
<p>The port number of the master used for communicating with all
the workers (via socket connections).  If an integer vector of ports, then
a random one among those is chosen.  If <code>"random"</code>, then a random port in
is chosen from <code>11000:11999</code>, or from the range specified by
environment variable <span class="env">R_PARALLELLY_RANDOM_PORTS</span>.
If <code>"auto"</code> (default), then the default (single) port is taken from
environment variable <span class="env">R_PARALLEL_PORT</span>, otherwise <code>"random"</code> is
used.
<em>Note, do not use this argument to specify the port number used by
<code>rshcmd</code>, which typically is an SSH client.  Instead, if the SSH daemon
runs on a different port than the default 22, specify the SSH port by
appending it to the hostname, e.g. <code>"remote.server.org:2200"</code> or via
SSH options <span class="option">-p</span>, e.g. <code>rshopts = c("-p", "2200")</code>.</em></p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_user">user</code></td>
<td>
<p>(optional) The user name to be used when communicating with
another host.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to
<code>makeNode(workers[i], ..., rank = i)</code> where <code>i = seq_along(workers)</code>.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_autostop">autoStop</code></td>
<td>
<p>If TRUE, the cluster will be automatically stopped
using <code><a href="parallel.html#topic+makeCluster">stopCluster</a>()</code> when it is
garbage collected, unless already stopped.  See also <code><a href="#topic+autoStopCluster">autoStopCluster()</a></code>.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_tries">tries</code>, <code id="makeClusterPSOCK_+3A_delay">delay</code></td>
<td>
<p>Maximum number of attempts done to launch each node
with <code>makeNode()</code> and the delay (in seconds) in-between attempts.
If argument <code>port</code> specifies more than one port, e.g. <code>port = "random"</code>
then a random port will be drawn and validated at most <code>tries</code> times.
Arguments <code>tries</code> and <code>delay</code> are used only when
<code>setup_strategy == "sequential"</code>.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_validate">validate</code></td>
<td>
<p>If TRUE (default), after the nodes have been created,
they are all validated that they work by inquiring about their session
information, which is saved in attribute <code>session_info</code> of each node.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, informative messages are outputted.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_worker">worker</code></td>
<td>
<p>The hostname or IP number of the machine where the worker
should run.
Attribute <code>localhost</code> can be set to TRUE or FALSE to manually indicate
whether <code>worker</code> is the same as the local host.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_master">master</code></td>
<td>
<p>The hostname or IP number of the master / calling machine, as
known to the workers.  If NULL (default), then the default is
<code>Sys.info()[["nodename"]]</code> unless <code>worker</code> is <em>localhost</em> or
<code>revtunnel = TRUE</code> in case it is <code>"localhost"</code>.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_connecttimeout">connectTimeout</code></td>
<td>
<p>The maximum time (in seconds) allowed for each socket
connection between the master and a worker to be established (defaults to
2 minutes). <em>See note below on current lack of support on Linux and
macOS systems.</em></p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_timeout">timeout</code></td>
<td>
<p>The maximum time (in seconds) allowed to pass without the
master and a worker communicate with each other (defaults to 30 days).</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_rscript">rscript</code>, <code id="makeClusterPSOCK_+3A_homogeneous">homogeneous</code></td>
<td>
<p>The system command for launching <code>Rscript</code>
on the worker and whether it is installed in the same path as the calling
machine or not.  For more details, see below.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_rscript_args">rscript_args</code></td>
<td>
<p>Additional arguments to <code>Rscript</code> (as a character
vector).  This argument can be used to customize the <span class="rlang"><b>R</b></span> environment of the
workers before they launches.
For instance, use <code>rscript_args = c("-e", shQuote('setwd("/path/to")'))</code>
to set the working directory to &lsquo;<span class="file">/path/to</span>&rsquo; on <em>all</em> workers.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_rscript_envs">rscript_envs</code></td>
<td>
<p>A named character vector environment variables to
set or unset on worker at startup, e.g.
<code>rscript_envs = c(FOO = "3.14", "HOME", "UNKNOWN", UNSETME = NA_character_)</code>.
If an element is not named, then the value of that variable will be used as
the name and the value will be the value of <code>Sys.getenv()</code> for that
variable.  Non-existing environment variables will be dropped.
These variables are set using <code>Sys.setenv()</code>.
An named element with value <code>NA_character_</code> will cause that variable to be
unset, which is done via <code>Sys.unsetenv()</code>.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_rscript_libs">rscript_libs</code></td>
<td>
<p>A character vector of <span class="rlang"><b>R</b></span> library paths that will be
used for the library search path of the <span class="rlang"><b>R</b></span> workers.  An asterisk
(<code>"*"</code>) will be resolved to the default <code>.libPaths()</code> <em>on the
worker</em>. That is, to <code>prepend</code> a folder, instead of replacing the
existing ones, use <code>rscript_libs = c("new_folder", "*")</code>.
To pass down a non-default library path currently set <em>on the main <span class="rlang"><b>R</b></span>
session</em> to the workers, use <code>rscript_libs = .libPaths()</code>.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_rscript_startup">rscript_startup</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> expression or a character vector of <span class="rlang"><b>R</b></span> code,
or a list with a mix of these, that will be evaluated on the <span class="rlang"><b>R</b></span> worker
prior to launching the worker's event loop.
For instance, use <code>rscript_startup = 'setwd("/path/to")'</code>
to set the working directory to &lsquo;<span class="file">/path/to</span>&rsquo; on <em>all</em> workers.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_rscript_sh">rscript_sh</code></td>
<td>
<p>The type of shell used where <code>rscript</code> is launched,
which should be <code>"sh"</code> is launched via a POSIX shell and <code>"cmd"</code> if
launched via an MS Windows shell.  This controls how shell command-line
options are quoted, but also how R string expression are quoted when
passed to <code>Rscript</code>.
If <code>"none"</code>, then no quoting is done.
If <code>"auto"</code> (default), and the cluster node is launched locally, then it
is set to <code>"sh"</code> or <code>"cmd"</code> according to the current platform.
<em>If launched remotely</em>, then it is set to <code>"sh"</code> based on the assumption
remote machines typically launch commands via SSH in a POSIX shell.
If the remote machines run MS Windows, use <code>rscript_sh = "cmd"</code>.
If <code>length(rscript_sh)</code> is two, then <code>rscript_sh[1]</code> is for the inner and
<code>rscript_sh[2]</code> is for the outer shell quoting of the Rscript call.
More precisely, <code>rscript_sh[1]</code> is for Rscript arguments that need shell
quoting (e.g. <code style="white-space: pre;">&#8288;Rscript -e "&lt;expr&gt;"&#8288;</code>), and <code>rscript_sh[2]</code> is for the whole
<code style="white-space: pre;">&#8288;Rscript ...&#8288;</code> call.
If <code>length(rscript_sh)</code> is one, then it is used for both the inner and the
outer shell quoting.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_default_packages">default_packages</code></td>
<td>
<p>A character vector or NULL that controls which R
packages are attached on each cluster node during startup.  An asterisk
(<code>"*"</code>) resolves to <code>getOption("defaultPackages")</code> <em>on the current machine</em>.
If NULL, then the default set of packages R are attached.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_methods">methods</code></td>
<td>
<p>If TRUE (default), then the <span class="pkg">methods</span> package is also
loaded. This is argument exists for legacy reasons due to how
<code>Rscript</code> worked in R (&lt; 3.5.0).</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_socketoptions">socketOptions</code></td>
<td>
<p>A character string that sets <span class="rlang"><b>R</b></span> option
<code>socketOptions</code> on the worker.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_usexdr">useXDR</code></td>
<td>
<p>If FALSE (default), the communication between master and workers, which is binary, will use small-endian (faster), otherwise big-endian (&quot;XDR&quot;; slower).</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_outfile">outfile</code></td>
<td>
<p>Where to direct the <a href="base.html#topic+showConnections">stdout</a> and
<a href="base.html#topic+showConnections">stderr</a> connection output from the workers.
If NULL, then no redirection of output is done, which means that the
output is relayed in the terminal on the local computer.  On Windows, the
output is only relayed when running <span class="rlang"><b>R</b></span> from a terminal but not from a GUI.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_renice">renice</code></td>
<td>
<p>A numerical 'niceness' (priority) to set for the worker
processes.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_rshcmd">rshcmd</code>, <code id="makeClusterPSOCK_+3A_rshopts">rshopts</code></td>
<td>
<p>The command (character vector) to be run on the master
to launch a process on another host and any additional arguments (character
vector).  These arguments are only applied if <code>machine</code> is not
<em>localhost</em>.  For more details, see below.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_revtunnel">revtunnel</code></td>
<td>
<p>If TRUE, a reverse SSH tunnel is set up for each worker such
that the worker <span class="rlang"><b>R</b></span> process sets up a socket connection to its local port
<code>(port + rank - 1)</code> which then reaches the master on port <code>port</code>.
If FALSE, then the worker will try to connect directly to port <code>port</code> on
<code>master</code>.
If NA, then TRUE or FALSE is inferred from inspection of <code>rshcmd[1]</code>.
For more details, see below.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_rshlogfile">rshlogfile</code></td>
<td>
<p>(optional) If a filename, the output produced by the
<code>rshcmd</code> call is logged to this file, of if TRUE, then it is logged
to a temporary file.  The log file name is available as an attribute
as part of the return node object.
<em>Warning: This only works with SSH clients that support command-line
option <span class="option">-E out.log`</span></em>.  For example, PuTTY's <code>plink</code> does
<em>not</em> support this option, and any attempts to specify <code>rshlogfile</code> will
cause the SSH connection to fail.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_rank">rank</code></td>
<td>
<p>A unique one-based index for each worker (automatically set).</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_manual">manual</code></td>
<td>
<p>If TRUE the workers will need to be run manually. The command
to run will be displayed.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_dryrun">dryrun</code></td>
<td>
<p>If TRUE, nothing is set up, but a message suggesting how to
launch the worker from the terminal is outputted.  This is useful for
troubleshooting.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_quiet">quiet</code></td>
<td>
<p>If TRUE, then no output will be produced other than that from
using <code>verbose = TRUE</code>.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_setup_strategy">setup_strategy</code></td>
<td>
<p>If <code>"parallel"</code> (default), the workers are set up
concurrently, one after the other.  If <code>"sequential"</code>, they are set up
sequentially.</p>
</td></tr>
<tr><td><code id="makeClusterPSOCK_+3A_action">action</code></td>
<td>
<p>This is an internal argument.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>c("RichSOCKcluster", "SOCKcluster", "cluster")</code>
consisting of a list of <code>"SOCKnode"</code> or <code>"SOCK0node"</code> workers (that also
inherit from <code>RichSOCKnode</code>).
</p>
<p><code>makeNodePSOCK()</code> returns a <code>"SOCKnode"</code> or
<code>"SOCK0node"</code> object representing an established connection to a worker.
</p>


<h3>Alternative usage</h3>

<p>In R (&gt;= 4.5.0), an alternatively to using
<code>cl &lt;- parallelly::makeClusterPSOCK(workers)</code> is:
</p>
<div class="sourceCode"><pre>cl &lt;- parallel::makeCluster(workers, type = parallelly::PSOCK)
</pre></div>


<h3>Protection against CPU overuse</h3>

<p>Using too many parallel workers on the same machine may result in
overusing the CPU.  For example, if an R script hard codes the
number of parallel workers to 32, as in
</p>
<div class="sourceCode r"><pre>cl &lt;- makeClusterPSOCK(32)
</pre></div>
<p>it will use more than 100% of the CPU cores when running on machine with
fewer than 32 CPU cores.  For example, on a eight-core machine, this
may run the CPU at 400% of its capacity, which has a significant
negative effect on the current R process, but also on all other processes
running on the same machine.  This also a problem on systems where R
gets allotted a specific number of CPU cores, which is the case on
high-performance compute (HPC) clusters, but also on other shared systems
that limits user processes via Linux Control Groups (cgroups).
For example, a free account on Posit Cloud is limited to a single
CPU core. Parallelizing with 32 workers when only having access to
a single core, will result in 3200% overuse and 32 concurrent R
processes competing for this single CPU core.
</p>
<p>To protect against CPU overuse by mistake, <code>makeClusterPSOCK()</code> will
warn when parallelizing above 100%;
</p>
<div class="sourceCode r"><pre>cl &lt;- parallelly::makeClusterPSOCK(12, dryrun = TRUE)
Warning message:
In checkNumberOfLocalWorkers(workers) :
  Careful, you are setting up 12 localhost parallel workers with
only 8 CPU cores available for this R process, which could result
in a 150% load. The maximum is set to 100%. Overusing the CPUs has
negative impact on the current R process, but also on all other
processes of yours and others running on the same machine. See
help("parallelly.options", package = "parallelly") for how to
override this threshold
</pre></div>
<p>Any attempts resulting in more than 300% overuse will be refused;
</p>
<div class="sourceCode r"><pre>&gt; cl &lt;- parallelly::makeClusterPSOCK(25, dryrun = TRUE)
Error in checkNumberOfLocalWorkers(workers) : 
  Attempting to set up 25 localhost parallel workers with only
8 CPU cores available for this R process, which could result in
a 312% load. The maximum is set to 300%. Overusing the CPUs has
negative impact on the current R process, but also on all other
processes of yours and others running on the same machine. See
help("parallelly.options", package = "parallelly") for how to
override this threshold
</pre></div>
<p>See <a href="#topic+parallelly.options">parallelly.options</a> for how to change the default thresholds.
</p>


<h3>Definition of <em>localhost</em></h3>

<p>A hostname is considered to be <em>localhost</em> if it equals:
</p>

<ul>
<li> <p><code>"localhost"</code>,
</p>
</li>
<li> <p><code>"127.0.0.1"</code>, or
</p>
</li>
<li> <p><code>Sys.info()[["nodename"]]</code>.
</p>
</li></ul>

<p>It is also considered <em>localhost</em> if it appears on the same line
as the value of <code>Sys.info()[["nodename"]]</code> in file &lsquo;<span class="file">/etc/hosts</span>&rsquo;.
</p>


<h3>Default SSH client and options (arguments <code>rshcmd</code> and <code>rshopts</code>)</h3>

<p>Arguments <code>rshcmd</code> and <code>rshopts</code> are only used when connecting
to an external host.
</p>
<p>The default method for connecting to an external host is via SSH and the
system executable for this is given by argument <code>rshcmd</code>.  The default
is given by option
<code><a href="#topic+parallelly.options">parallelly.makeNodePSOCK.rshcmd</a></code>.
If that is not
set, then the default is to use <code>ssh</code> on Unix-like systems,
including macOS as well as Windows 10.  On older MS Windows versions, which
does not have a built-in <code>ssh</code> client, the default is to use
(i) <code>plink</code> from the <a href="https://www.putty.org/"><code>PuTTY</code></a>
project, and then (ii) the <code>ssh</code> client that is distributed with
RStudio.
</p>
<p>PuTTY puts itself on Windows' system <span class="env">PATH</span> when installed, meaning this
function will find PuTTY automatically if installed.  If not, to manually
set specify PuTTY as the SSH client, specify the absolute pathname of
&lsquo;<span class="file">plink.exe</span>&rsquo; in the first element and option <code>-ssh</code> in the
second as in <code>rshcmd = c("C:/Path/PuTTY/plink.exe", "-ssh")</code>.
This is because all elements of <code>rshcmd</code> are individually &quot;shell&quot;
quoted and element <code>rshcmd[1]</code> must be on the system <span class="env">PATH</span>.
</p>
<p>Furthermore, when running <span class="rlang"><b>R</b></span> from RStudio on Windows, the <code>ssh</code>
client that is distributed with RStudio will also be considered.
This client, which is from <a href="https://en.wikipedia.org/wiki/MinGW">MinGW</a>
MSYS, is searched for in the folder given by the <span class="env">RSTUDIO_MSYS_SSH</span>
environment variable&mdash;a variable that is (only) set when running RStudio.
To use this SSH client outside of RStudio, set <span class="env">RSTUDIO_MSYS_SSH</span>
accordingly.
</p>
<p>You can override the default set of SSH clients that are searched for
by specifying them in argument <code>rshcmd</code> or via option
<code><a href="#topic+parallelly.options">parallelly.makeNodePSOCK.rshcmd</a></code>
using the format <code style="white-space: pre;">&#8288;&lt;...&gt;&#8288;</code>, e.g.
<code>rshcmd = c("&lt;rstudio-ssh&gt;", "&lt;putty-plink&gt;", "&lt;ssh&gt;")</code>.  See
below for examples.
</p>
<p>If no SSH-client is found, an informative error message is produced.
</p>
<p>Additional SSH command-line options may be specified via argument <code>rshopts</code>,
which defaults to option <code>parallelly.makeNodePSOCK.rshopts</code>. For
instance, a private SSH key can be provided as
<code>rshopts = c("-i", "~/.ssh/my_private_key")</code>.  PuTTY users should
specify a PuTTY PPK file, e.g.
<code>rshopts = c("-i", "C:/Users/joe/.ssh/my_keys.ppk")</code>.
Contrary to <code>rshcmd</code>, elements of <code>rshopts</code> are not quoted.
</p>


<h3>Accessing external machines that prompts for a password</h3>

<p><em>IMPORTANT: With one exception, it is not possible to for these
functions to log in and launch <span class="rlang"><b>R</b></span> workers on external machines that requires
a password to be entered manually for authentication.</em>
The only known exception is the PuTTY client on Windows for which one can
pass the password via command-line option <span class="option">-pw</span>, e.g.
<code>rshopts = c("-pw", "MySecretPassword")</code>.
</p>
<p>Note, depending on whether you run <span class="rlang"><b>R</b></span> in a terminal or via a GUI, you might
not even see the password prompt.  It is also likely that you cannot enter
a password, because the connection is set up via a background system call.
</p>
<p>The poor man's workaround for setup that requires a password is to manually
log into the each of the external machines and launch the <span class="rlang"><b>R</b></span> workers by hand.
For this approach, use <code>manual = TRUE</code> and follow the instructions
which include cut'n'pasteable commands on how to launch the worker from the
external machine.
</p>
<p>However, a much more convenient and less tedious method is to set up
key-based SSH authentication between your local machine and the external
machine(s), as explain below.
</p>


<h3>Accessing external machines via key-based SSH authentication</h3>

<p>The best approach to automatically launch <span class="rlang"><b>R</b></span> workers on external machines
over SSH is to set up key-based SSH authentication.  This will allow you
to log into the external machine without have to enter a password.
</p>
<p>Key-based SSH authentication is taken care of by the SSH client and not <span class="rlang"><b>R</b></span>.
To configure this, see the manuals of your SSH client or search the web
for &quot;ssh key authentication&quot;.
</p>


<h3>Reverse SSH tunneling</h3>

<p>If SSH is used, which is inferred from <code>rshcmd[1]</code>, then the default is
to use reverse SSH tunneling (<code>revtunnel = TRUE</code>), otherwise not
(<code>revtunnel = FALSE</code>). Using reverse SSH tunneling, avoids complications
from otherwise having to configure port forwarding in firewalls, which
often requires static IP address as well as privileges to edit the
firewall on your outgoing router, something most users don't have.
It also has the advantage of not having to know the internal and / or the
public IP address / hostname of the master.
Yet another advantage is that there will be no need for a DNS lookup by the
worker machines to the master, which may not be configured or is disabled
on some systems, e.g. compute clusters.
</p>


<h3>Argument <code>rscript</code></h3>

<p>If <code>homogeneous</code> is FALSE, the <code>rscript</code> defaults to <code>"Rscript"</code>, i.e. it
is assumed that the <code>Rscript</code> executable is available on the
<span class="env">PATH</span> of the worker.
If <code>homogeneous</code> is TRUE, the <code>rscript</code> defaults to
<code>file.path(R.home("bin"), "Rscript")</code>, i.e. it is basically assumed that
the worker and the caller share the same file system and <span class="rlang"><b>R</b></span> installation.
</p>
<p>When specified, argument <code>rscript</code> should be a character vector with one or
more elements.  Any asterisk (<code>"*"</code>) will be resolved to the above default
<code>homogeneous</code>-dependent <code>Rscript</code> path.
All elements are automatically shell quoted using <code><a href="base.html#topic+shQuote">base::shQuote()</a></code>, except
those that are of format <code style="white-space: pre;">&#8288;&lt;ENVVAR&gt;=&lt;VALUE&gt;&#8288;</code>, that is, the ones matching the
regular expression '&lsquo;<span class="samp">&#8288;^[[:alpha:]_][[:alnum:]_]*=.*&#8288;</span>&rsquo;'.
Another exception is when <code>rscript</code> inherits from 'AsIs'.
</p>


<h3>Default value of argument <code>homogeneous</code></h3>

<p>The default value of <code>homogeneous</code> is TRUE if and only if either
of the following is fulfilled:
</p>

<ul>
<li> <p><code>worker</code> is <em>localhost</em>
</p>
</li>
<li> <p><code>revtunnel</code> is FALSE and <code>master</code> is <em>localhost</em>
</p>
</li>
<li> <p><code>worker</code> is neither an IP number nor a fully qualified domain
name (FQDN).  A hostname is considered to be a FQDN if it contains
one or more periods
</p>
</li></ul>

<p>In all other cases, <code>homogeneous</code> defaults to FALSE.
</p>


<h3>Connection timeout</h3>

<p>Argument <code>connectTimeout</code> does <em>not</em> work properly on Unix and
macOS due to limitation in <span class="rlang"><b>R</b></span> itself.  For more details on this, please see
R-devel thread 'BUG?: On Linux setTimeLimit() fails to propagate timeout
error when it occurs (works on Windows)' on 2016-10-26
(<a href="https://stat.ethz.ch/pipermail/r-devel/2016-October/073309.html">https://stat.ethz.ch/pipermail/r-devel/2016-October/073309.html</a>).
When used, the timeout will eventually trigger an error, but it won't happen
until the socket connection timeout <code>timeout</code> itself happens.
</p>


<h3>Communication timeout</h3>

<p>If there is no communication between the master and a worker within the
<code>timeout</code> limit, then the corresponding socket connection will be
closed automatically.  This will eventually result in an error in code
trying to access the connection.
This timeout is also what terminates a stray-running parallel cluster-node
process.
</p>


<h3>Failing to set up local workers</h3>

<p>When setting up a cluster of localhost workers, that is, workers running
on the same machine as the master <span class="rlang"><b>R</b></span> process, occasionally a connection
to a worker (&quot;cluster node&quot;) may fail to be set up.
When this occurs, an informative error message with troubleshooting
suggestions will be produced.
The most common reason for such localhost failures is due to port
clashes.  Retrying will often resolve the problem.
</p>
<p>If R stalls when setting up a cluster of local workers, then it might
be that you have a virtual private network (VPN) enabled that is
configured to prevent you from connecting to <code>localhost</code>.  To verify that
this is the case, call the following from the terminal:
</p>
<div class="sourceCode sh"><pre>{local}$ ssh localhost "date"
</pre></div>
<p>This also freezed if the VPN intercepts connections to <code>localhost</code>.
If this happens, try also:
</p>
<div class="sourceCode sh"><pre>{local}$ ssh 127.0.0.1 "date"
</pre></div>
<p>In rare cases, <code style="white-space: pre;">&#8288;127.0.0.1&#8288;</code> might work when <code>localhost</code> does not.
If the latter works, setting R option:
</p>
<div class="sourceCode r"><pre>options(parallelly.localhost.hostname = "127.0.0.1")
</pre></div>
<p>should solve it (the default is <code>"localhost"</code>).  You can set this
automatically when R starts by adding it to your <code style="white-space: pre;">&#8288;~/.Rprofile&#8288;</code> startup
file. Alternatively, set environment variable
<code style="white-space: pre;">&#8288;R_PARALLELLY_LOCALHOST_HOSTNAME=127.0.0.1&#8288;</code> in your <code style="white-space: pre;">&#8288;~/.Renviron&#8288;</code> file.
</p>
<p>If using <code style="white-space: pre;">&#8288;127.0.0.1&#8288;</code> did not work around the problem, check your VPN
settings and make sure it allows connections to <code>localhost</code> or <code style="white-space: pre;">&#8288;127.0.0.1&#8288;</code>.
</p>


<h3>Failing to set up remote workers</h3>

<p>A cluster of remote workers runs <span class="rlang"><b>R</b></span> processes on external machines. These
external <span class="rlang"><b>R</b></span> processes are launched over, typically, SSH to the remote
machine.  For this to work, each of the remote machines needs to have
<span class="rlang"><b>R</b></span> installed, which preferably is of the same version as what is on the
main machine.  For this to work, it is required that one can SSH to the
remote machines.  Ideally, the SSH connections use authentication based
on public-private SSH keys such that the set up of the remote workers can
be fully automated (see above).  If <code>makeClusterPSOCK()</code> fails to set
up one or more remote <span class="rlang"><b>R</b></span> workers, then an informative error message is
produced.
There are a few reasons for failing to set up remote workers.  If this
happens, start by asserting that you can SSH to the remote machine and
launch &lsquo;<span class="file">Rscript</span>&rsquo; by calling something like:
</p>
<pre>
{local}$ ssh -l alice remote.server.org
{remote}$ Rscript --version
R scripting front-end version 4.2.2 (2022-10-31)
{remote}$ logout
{local}$
</pre>
<p>When you have confirmed the above to work, then confirm that you can achieve
the same in a single command-line call;
</p>
<pre>
{local}$ ssh -l alice remote.server.org Rscript --version
R scripting front-end version 4.2.2 (2022-10-31)
{local}$
</pre>
<p>The latter will assert that you have proper startup configuration also for
<em>non-interactive</em> shell sessions on the remote machine.
</p>
<p>If the remote machines are running on MS Windows, make sure to add argument
<code>rscript_sh = "cmd"</code> when calling <code>makeClusterPSOCK()</code>, because the default
is <code>rscript_sh = "sh"</code>, which assumes that that the remote machines are
Unix-like machines.
</p>
<p>Another reason for failing to setup remote workers could be that they are
running an <span class="rlang"><b>R</b></span> version that is not compatible with the version that your main
<span class="rlang"><b>R</b></span> session is running.  For instance, if we run R (&gt;= 3.6.0) locally and the
workers run R (&lt; 3.5.0), we will get:
<code style="white-space: pre;">&#8288;Error in unserialize(node$con) : error reading from connection&#8288;</code>.
This is because R (&gt;= 3.6.0) uses serialization format version 3 by default
whereas R (&lt; 3.5.0) only supports version 2.  We can see the version of the
<span class="rlang"><b>R</b></span> workers by adding <code>rscript_args = c("-e", shQuote("getRversion()"))</code> when
calling <code>makeClusterPSOCK()</code>.
</p>


<h3>For package developers</h3>

<p>When creating a <code>cluster</code> object, for instance via <code>parallel::makeCluster()</code>
or <code>parallelly::makeClusterPSOCK()</code>, in a package help example, in a package
vignette, or in a package test, we must <em>remember to stop the cluster at
the end of all examples(*), vignettes, and unit tests</em>. This is required in
order to not leave behind stray parallel <code>cluster</code> workers after our main R
session terminates. On Linux and macOS, the operating system often takes
care of terminating the worker processes if we forget, but on MS Windows
such processes will keep running in the background until they time out
themselves, which takes 30 days (sic!).
</p>
<p><code style="white-space: pre;">&#8288;R CMD check --as-cran&#8288;</code> will indirectly detect these stray worker processes
on MS Windows when running R (&gt;= 4.3.0). They are detected, because they
result in placeholder <code style="white-space: pre;">&#8288;Rscript&lt;hexcode&gt;&#8288;</code> files being left behind in
the temporary directory.  The check NOTE to look out for
(only in R (&gt;= 4.3.0)) is:
</p>
<div class="sourceCode"><pre>* checking for detritus in the temp directory ... NOTE
Found the following files/directories:
  'Rscript1058267d0c10' 'Rscriptbd4267d0c10'
</pre></div>
<p>Those <code style="white-space: pre;">&#8288;Rscript&lt;hexcode&gt;&#8288;</code> files are from background R worker processes,
which almost always are parallel <code>cluster</code>:s that we forgot to stop
at the end.  To stop all <code>cluster</code> workers, use <code><a href="parallel.html#topic+makeCluster">parallel::stopCluster()</a></code>
at the end of your examples(*), vignettes, and package tests for every
<code>cluster</code> object that is created.
</p>
<p>(*) Currently, examples are excluded from the detritus checks.
This was validated with R-devel revision 82991 (2022-10-02).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## NOTE: Drop 'dryrun = TRUE' below in order to actually connect.  Add
## 'verbose = TRUE' if you run into problems and need to troubleshoot.

## ---------------------------------------------------------------
## Section 1. Setting up parallel workers on the local machine
## ---------------------------------------------------------------
## EXAMPLE: Two workers on the local machine
workers &lt;- c("localhost", "localhost")
cl &lt;- makeClusterPSOCK(workers, dryrun = TRUE, quiet = TRUE)


## EXAMPLE: Launch 124 workers on MS Windows 10, where half are
## running on CPU Group #0 and half on CPU Group #1.  
## (https://lovickconsulting.com/2021/11/18/
##  running-r-clusters-on-an-amd-threadripper-3990x-in-windows-10-2/)
## The parallel workers are launched as:
## "%COMSPEC%" /c start /B /NODE 1 /AFFINITY 0xFFFFFFFFFFFFFFFE ...
## ...
## "%COMSPEC%" /c start /B /NODE 1 /AFFINITY 0xFFFFFFFFFFFFFFFE ...

## Temporarily disable CPU load protection for this example
oopts &lt;- options(parallelly.maxWorkers.localhost = Inf)

ncores &lt;- 124
cpu_groups &lt;- c(0, 1)
cl &lt;- lapply(cpu_groups, FUN = function(cpu_group) {
    parallelly::makeClusterPSOCK(ncores %/% length(cpu_groups),
      rscript = I(c(
        Sys.getenv("COMSPEC"), "/c", "start", "/B",
        "/NODE", cpu_group, "/AFFINITY", "0xFFFFFFFFFFFFFFFE",
        "*"
      )),
      dryrun = TRUE, quiet = TRUE
    )
})
## merge the two 62-node clusters into one with 124 nodes
cl &lt;- do.call(c, cl)

## Re-enable CPU load protection
options(oopts)


## ---------------------------------------------------------------
## Section 2. Setting up parallel workers on remote machines
## ---------------------------------------------------------------
## EXAMPLE: Three remote workers
## Setup of three R workers on two remote machines are set up
## The parallel workers are launched as:
## '/usr/bin/ssh' -R 11058:localhost:11058 n1.remote.org ...
## '/usr/bin/ssh' -R 11059:localhost:11058 n2.remote.org ...
## '/usr/bin/ssh' -R 11060:localhost:11058 n1.remote.org ...
workers &lt;- c("n1.remote.org", "n2.remote.org", "n1.remote.org")
cl &lt;- makeClusterPSOCK(workers, dryrun = TRUE, quiet = TRUE)


## EXAMPLE: Two remote workers running on MS Windows.  Because the
## remote workers are MS Windows machines, we need to use
## rscript_sh = "cmd".
## The parallel workers are launched as:
## '/usr/bin/ssh' -R 11912:localhost:11912 mswin1.remote.org ...
## '/usr/bin/ssh' -R 11913:localhost:11912 mswin2.remote.org ...
workers &lt;- c("mswin1.remote.org", "mswin2.remote.org")
cl &lt;- makeClusterPSOCK(workers, rscript_sh = "cmd", dryrun = TRUE, quiet = TRUE)


## EXAMPLE: Local and remote workers
## Same setup when the two machines are on the local network and
## have identical software setups
cl &lt;- makeClusterPSOCK(
  workers,
  revtunnel = FALSE, homogeneous = TRUE,
  dryrun = TRUE, quiet = TRUE
)


## EXAMPLE: Three remote workers 'n1', 'n2', and 'n3' that can only be
## accessed via jumphost 'login.remote.org'
## The parallel workers are launched as:
## '/usr/bin/ssh' -R 11226:localhost:11226 -J login.remote.org n1 ...
## '/usr/bin/ssh' -R 11227:localhost:11226 -J login.remote.org n2 ...
## '/usr/bin/ssh' -R 11228:localhost:11226 -J login.remote.org n1 ...
workers &lt;- c("n1", "n2", "n1")
cl &lt;- makeClusterPSOCK(
  workers,
  rshopts = c("-J", "login.remote.org"),
  homogeneous = FALSE,
  dryrun = TRUE, quiet = TRUE
)


## EXAMPLE: Remote worker running on Linux from MS Windows machine
## Connect to remote Unix machine 'remote.server.org' on port 2200
## as user 'bob' from a MS Windows machine with PuTTY installed.
## Using the explicit special rshcmd = "&lt;putty-plink&gt;", will force
## makeClusterPSOCK() to search for and use the PuTTY plink software,
## preventing it from using other SSH clients on the system search PATH.
## The parallel worker is launched as:
## 'plink' -l bob -P 2200 -i C:/Users/bobby/.ssh/putty.ppk remote.server.org ...
cl &lt;- makeClusterPSOCK(
  "remote.server.org", user = "bob",
  rshcmd = "&lt;putty-plink&gt;",
  rshopts = c("-P", 2200, "-i", "C:/Users/bobby/.ssh/putty.ppk"),
  dryrun = TRUE, quiet = TRUE
)


## EXAMPLE: Remote workers with specific setup
## Setup of remote worker with more detailed control on
## authentication and reverse SSH tunneling
## The parallel worker is launched as:
## '/usr/bin/ssh' -l johnny -v -R 11000:gateway:11942 remote.server.org ...
## "R_DEFAULT_PACKAGES=... 'nice' '/path/to/Rscript' --no-init-file ...
cl &lt;- makeClusterPSOCK(
  "remote.server.org", user = "johnny",
  ## Manual configuration of reverse SSH tunneling
  revtunnel = FALSE,
  rshopts = c("-v", "-R 11000:gateway:11942"),
  master = "gateway", port = 11942,
  ## Run Rscript nicely and skip any startup scripts
  rscript = c("nice", "/path/to/Rscript"),
  rscript_args = c("--no-init-file"),
  dryrun = TRUE, quiet = TRUE
)


## EXAMPLE: Remote worker running on Linux from RStudio on MS Windows
## Connect to remote Unix machine 'remote.server.org' on port 2200
## as user 'bob' from a MS Windows machine via RStudio's SSH client.
## Using the explicit special rshcmd = "&lt;rstudio-ssh&gt;", will force
## makeClusterPSOCK() to use the SSH client that comes with RStudio,
## preventing it from using other SSH clients on the system search PATH.
## The parallel worker is launched as:
## 'ssh' -l bob remote.server.org:2200 ...
cl &lt;- makeClusterPSOCK(
  "remote.server.org:2200", user = "bob", rshcmd = "&lt;rstudio-ssh&gt;",
  dryrun = TRUE, quiet = TRUE
)


## ---------------------------------------------------------------
## Section 3. Setting up parallel workers on HPC cluster
## ---------------------------------------------------------------
## EXAMPLE: 'Grid Engine' is a high-performance compute (HPC) job
## scheduler where one can request compute resources on multiple nodes,
## each running multiple cores. Examples of Grid Engine schedulers are
## Oracle Grid Engine (formerly Sun Grid Engine), Univa Grid Engine,
## and Son of Grid Engine - all commonly referred to as SGE schedulers.
## Each SGE cluster may have its own configuration with their own way
## of requesting parallel slots. Here are a few examples:
##
##   ## Request 18 slots on a single host
##   qsub -pe smp 18 script.sh
##
##   ## Request 18 slots on one or more hosts
##   qsub -pe mpi 18 script.sh
##
## This will launch the job script 'script.sh' on one host, while have
## reserved in total 18 slots (CPU cores) on this host and possible
## other hosts.
##
## This example shows how to use the SGE command 'qrsh' to launch
## 18 parallel workers from R, which is assumed to have been launched
## by 'script.sh'.
##
## The parallel workers are launched as:
## 'qrsh' -inherit -nostdin -V comphost01 ...
## 'qrsh' -inherit -nostdin -V comphost01 ...
## ...
## 'qrsh' -inherit -nostdin -V comphost06 ...
cl &lt;- makeClusterPSOCK(
  availableWorkers(),
  rshcmd = "qrsh", rshopts = c("-inherit", "-nostdin", "-V"),
  dryrun = TRUE, quiet = TRUE
)


## EXAMPLE: The 'Fujitsu Technical Computing Suite' is a high-performance
## compute (HPC) job scheduler where one can request compute resources on
## multiple nodes, each running multiple cores.  For example,
##
##   pjsub -L vnode=3 -L vnode-core=18 script.sh
##
## reserves 18 cores on three nodes. The job script runs on the first
## with enviroment variables set to infer the other nodes, resulting in
## availableWorkers() to return 3 * 18 workers. When the HPC environment
## does not support SSH between compute nodes, one can use the 'pjrsh'
## command to launch the parallel workers.
##
## The parallel workers are launched as:
## 'pjrsh' comphost01 ...
## 'pjrsh' comphost01 ...
## ...
## 'pjrsh' comphost06 ...
cl &lt;- makeClusterPSOCK(
  availableWorkers(),
  rshcmd = "pjrsh",
  dryrun = TRUE, quiet = TRUE
)



## ---------------------------------------------------------------
## Section 4. Setting up remote parallel workers in the cloud
## ---------------------------------------------------------------
## EXAMPLE: Remote worker running on AWS
## Launching worker on Amazon AWS EC2 running one of the
## Amazon Machine Images (AMI) provided by RStudio
## (https://www.louisaslett.com/RStudio_AMI/)
##
## The parallel worker is launched as:
## '/usr/bin/ssh' -R 11153:localhost:11153 -l ubuntu ...
## -o StrictHostKeyChecking=no -o IdentitiesOnly=yes ...
## -i ~/.ssh/my-private-aws-key.pem 1.2.3.4 ...
public_ip &lt;- "1.2.3.4"
ssh_private_key_file &lt;- "~/.ssh/my-private-aws-key.pem"
cl &lt;- makeClusterPSOCK(
  ## Public IP number of EC2 instance
  public_ip,
  ## User name (always 'ubuntu')
  user = "ubuntu",
  ## Use private SSH key registered with AWS
  rshopts = c(
    "-o", "StrictHostKeyChecking=no",
    "-o", "IdentitiesOnly=yes",
    "-i", ssh_private_key_file
  ),
  ## Set up .libPaths() for the 'ubuntu' user
  ## and then install the future package
  rscript_startup = quote(local({
    p &lt;- Sys.getenv("R_LIBS_USER")
    dir.create(p, recursive = TRUE, showWarnings = FALSE)
    .libPaths(p)
    install.packages("future")
  })),
  dryrun = TRUE, quiet = TRUE
)


## EXAMPLE: Remote worker running on GCE
## Launching worker on Google Cloud Engine (GCE) running a
## container based VM (with a #cloud-config specification)
public_ip &lt;- "1.2.3.4"
user &lt;- "johnny"
ssh_private_key_file &lt;- "~/.ssh/google_compute_engine"
cl &lt;- makeClusterPSOCK(
  ## Public IP number of GCE instance
  public_ip,
  ## User name (== SSH key label (sic!))
  user = user,
  ## Use private SSH key registered with GCE
  rshopts = c(
    "-o", "StrictHostKeyChecking=no",
    "-o", "IdentitiesOnly=yes",
    "-i", ssh_private_key_file
  ),
  ## Launch Rscript inside Docker container
  rscript = c(
    "docker", "run", "--net=host", "rocker/r-parallel",
    "Rscript"
  ),
  dryrun = TRUE, quiet = TRUE
)



## ---------------------------------------------------------------
## Section 5. Parallel workers running locally inside virtual
## machines, Linux containers, etc.
## ---------------------------------------------------------------
## EXAMPLE: Two workers limited to 100% CPU process and 50 MiB of
## memory using Linux CGroups management. The 100% CPU quota limit
## constrain each worker to use at most one CPU worth of
## processing preventing them from overusing the machine, e.g.
## through unintended nested parallelization. The 50 MiB memory
## limit is strict - if a worker use more than this, the operating
## system will terminate the worker instantly.
## See 'man systemd.resource-control' for more details.
cl &lt;- makeClusterPSOCK(2L,
  rscript = c("systemd-run", "--user", "--scope",
    "-p", "CPUQuota=100%",
    "-p", "MemoryMax=50M", "-p", "MemorySwapMax=50M",
    "*"
  ),
  dryrun = TRUE, quiet = TRUE
)


## EXAMPLE: Two workers running in Docker on the local machine
## Setup of 2 Docker workers running rocker/r-parallel
##
## The parallel workers are launched as:
## R_DEFAULT_PACKAGES=... '/usr/bin/docker' 'run' '--net=host' 'rocker/r-parallel' ...
## R_DEFAULT_PACKAGES=... '/usr/bin/docker' 'run' '--net=host' 'rocker/r-parallel' ...
cl &lt;- makeClusterPSOCK(
  rep("localhost", times = 2L),
  ## Launch Rscript inside Docker container
  rscript = c(
    "docker", "run", "--net=host", "rocker/r-parallel",
    "Rscript"
  ),
  ## IMPORTANT: Because Docker runs inside a virtual machine (VM) on macOS
  ## and MS Windows (not Linux), when the R worker tries to connect back to
  ## the default 'localhost' it will fail, because the main R session is
  ## not running in the VM, but outside on the host.  To reach the host on
  ## macOS and MS Windows, make sure to use master = "host.docker.internal"
  master = if (.Platform$OS.type == "unix") NULL else "host.docker.internal",
  dryrun = TRUE, quiet = TRUE
)


## EXAMPLE: Two workers running via Linux container 'rocker/r-parallel' from
## DockerHub on the local machine using Apptainer (formerly Singularity)
##
## The parallel workers are launched as:
## R_DEFAULT_PACKAGES=... '/usr/bin/apptainer' 'exec' 'docker://rocker/r-parallel' ...
## R_DEFAULT_PACKAGES=... '/usr/bin/apptainer' 'exec' 'docker://rocker/r-parallel' ...
cl &lt;- makeClusterPSOCK(
  rep("localhost", times = 2L),
  ## Launch Rscript inside Linux container
  rscript = c(
    "apptainer", "exec", "docker://rocker/r-parallel",
    "Rscript"
  ),
  dryrun = TRUE, quiet = TRUE
)


## EXAMPLE: One worker running in udocker on the local machine
## Setup of a single udocker.py worker running rocker/r-parallel
##
## The parallel worker is launched as:
## R_DEFAULT_PACKAGES=... 'udocker.py' 'run' 'rocker/r-parallel' ...
cl &lt;- makeClusterPSOCK(
  "localhost",
  ## Launch Rscript inside Docker container (using udocker)
  rscript = c(
    "udocker.py", "run", "rocker/r-parallel",
    "Rscript"
  ), 
  ## Manually launch parallel workers
  ## (need double shQuote():s because udocker.py drops one level)
  rscript_args = c(
    "-e", shQuote(shQuote("parallel:::.workRSOCK()"))
  ),
  dryrun = TRUE, quiet = TRUE
)


## EXAMPLE: One worker running in Wine for Linux on the local machine
## To install R for MS Windows in Wine, do something like:
##   winecfg  # In GUI, set 'Windows version' to 'Windows 10'
##   wget https://cran.r-project.org/bin/windows/base/R-4.4.2-win.exe
##   wine R-4.4.2-win.exe /SILENT
## Prevent packages from being installed to R's system library:
##   chmod ugo-w "$HOME/.wine/drive_c/Program Files/R/R-4.4.2/library/"
## Verify it works:
##   wine "C:/Program Files/R/R-4.4.2/bin/x64/Rscript.exe" --version
##
## The parallel worker is launched as:
## R_DEFAULT_PACKAGES=... WINEDEBUG=fixme-all R_LIBS_SITE= R_LIBS_USER= 'wine' ...
cl &lt;- makeClusterPSOCK(1L,
  rscript = c(
    ## Silence Wine warnings
    "WINEDEBUG=fixme-all",
    ## Don't pass LC_* and R_LIBS* environments from host to Wine
    sprintf("%s=", grep("^(LC_|R_LIBS)", names(Sys.getenv()), value = TRUE)),
    "wine",
    "C:/Program Files/R/R-4.4.2/bin/x64/Rscript.exe"
  ),
  dryrun = TRUE, quiet = TRUE
)
</code></pre>

<hr>
<h2 id='makeClusterSequential'>Create a &quot;parallel&quot; cluster running sequentially in the current session</h2><span id='topic+makeClusterSequential'></span><span id='topic+SEQ'></span>

<h3>Description</h3>

<p>The created cluster has only one node.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeClusterSequential()
</code></pre>


<h3>Details</h3>

<p>Expression and function calls are evaluated in a local environment,
inheriting the global environment.
</p>


<h3>Requirements</h3>

<p>This function is only defined for R (&gt;= 4.4.0).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(parallel)

cl &lt;- makeClusterSequential()
print(cl)

y &lt;- parLapply(cl, X = 1:3, fun = sqrt)
str(y)

pid &lt;- Sys.getpid()
print(pid)
y &lt;- clusterEvalQ(cl, Sys.getpid())
str(y)

abc &lt;- 3.14
y &lt;- clusterEvalQ(cl, { abc &lt;- 42; abc })
str(y)
stopifnot(abc == 3.14)



</code></pre>

<hr>
<h2 id='parallelly.options'>Options Used by the 'parallelly' Package</h2><span id='topic+parallelly.options'></span><span id='topic+parallelly.debug'></span><span id='topic+parallelly.availableCores.custom'></span><span id='topic+parallelly.availableCores.methods'></span><span id='topic+parallelly.availableCores.min'></span><span id='topic+parallelly.availableCores.fallback'></span><span id='topic+parallelly.availableCores.omit'></span><span id='topic+parallelly.availableCores.system'></span><span id='topic+parallelly.availableWorkers.methods'></span><span id='topic+parallelly.availableWorkers.custom'></span><span id='topic+parallelly.fork.enable'></span><span id='topic+parallelly.maxWorkers.localhost'></span><span id='topic+parallelly.supportsMulticore.disableOn'></span><span id='topic+parallelly.supportsMulticore.unstable'></span><span id='topic+R_PARALLELLY_AVAILABLECORES_FALLBACK'></span><span id='topic+R_PARALLELLY_AVAILABLECORES_OMIT'></span><span id='topic+R_PARALLELLY_AVAILABLECORES_SYSTEM'></span><span id='topic+R_PARALLELLY_AVAILABLECORES_MIN'></span><span id='topic+R_PARALLELLY_FORK_ENABLE'></span><span id='topic+R_PARALLELLY_SUPPORTSMULTICORE_DISABLEON'></span><span id='topic+R_PARALLELLY_SUPPORTSMULTICORE_UNSTABLE'></span><span id='topic+future.availableCores.custom'></span><span id='topic+future.availableCores.methods'></span><span id='topic+future.availableCores.fallback'></span><span id='topic+future.availableCores.system'></span><span id='topic+future.availableWorkers.methods'></span><span id='topic+future.availableWorkers.custom'></span><span id='topic+future.fork.enable'></span><span id='topic+future.supportsMulticore.unstable'></span><span id='topic+R_FUTURE_AVAILABLECORES_FALLBACK'></span><span id='topic+R_FUTURE_AVAILABLECORES_SYSTEM'></span><span id='topic+R_FUTURE_FORK_ENABLE'></span><span id='topic+R_FUTURE_SUPPORTSMULTICORE_UNSTABLE'></span><span id='topic+parallelly.makeNodePSOCK.setup_strategy'></span><span id='topic+parallelly.makeNodePSOCK.validate'></span><span id='topic+parallelly.makeNodePSOCK.connectTimeout'></span><span id='topic+parallelly.makeNodePSOCK.timeout'></span><span id='topic+parallelly.makeNodePSOCK.useXDR'></span><span id='topic+parallelly.makeNodePSOCK.socketOptions'></span><span id='topic+parallelly.makeNodePSOCK.rshcmd'></span><span id='topic+parallelly.makeNodePSOCK.rshopts'></span><span id='topic+parallelly.makeNodePSOCK.tries'></span><span id='topic+parallelly.makeNodePSOCK.tries.delay'></span><span id='topic+R_PARALLELLY_MAKENODEPSOCK_SETUP_STRATEGY'></span><span id='topic+R_PARALLELLY_MAKENODEPSOCK_VALIDATE'></span><span id='topic+R_PARALLELLY_MAKENODEPSOCK_CONNECTTIMEOUT'></span><span id='topic+R_PARALLELLY_MAKENODEPSOCK_TIMEOUT'></span><span id='topic+R_PARALLELLY_MAKENODEPSOCK_USEXDR'></span><span id='topic+R_PARALLELLY_MAKENODEPSOCK_SOCKETOPTIONS'></span><span id='topic+R_PARALLELLY_MAKENODEPSOCK_RSHCMD'></span><span id='topic+R_PARALLELLY_MAKENODEPSOCK_RSHOPTS'></span><span id='topic+R_PARALLELLY_MAKENODEPSOCK_TRIES'></span><span id='topic+R_PARALLELLY_MAKENODEPSOCK_TRIES_DELAY'></span>

<h3>Description</h3>

<p>Below are the <span class="rlang"><b>R</b></span> options and environment variables that are used by the
<span class="pkg">parallelly</span> package and packages enhancing it.<br />
<br />
<em>WARNING: Note that the names and the default values of these options may
change in future versions of the package.  Please use with care until
further notice.</em>
</p>


<h3>Backward compatibility with the <span class="pkg">future</span> package</h3>

<p>The functions in the <span class="pkg">parallelly</span> package originates from the
<span class="pkg">future</span> package.  Because they are widely used within the future
ecosystem, we need to keep them backward compatible for quite a long time,
in order for all existing packages and R scripts to have time to adjust.
This also goes for the <span class="rlang"><b>R</b></span> options and the environment variables used to
configure these functions.
All options and environment variables used here have prefixes <code>parallelly.</code>
and <code>R_PARALLELLY_</code>, respectively.  Because of the backward compatibility
with the <span class="pkg">future</span> package, the same settings can also be controlled
by options and environment variables with prefixes <code>future.</code> and
<code>R_FUTURE_</code> until further notice, e.g. setting option
<code>future.availableCores.fallback=1</code> is the same as setting option
<code>parallelly.availableCores.fallback=1</code>, and setting environment
variable <span class="env">R_FUTURE_AVAILABLECORES_FALLBACK=1</span> is the same as setting
<span class="env">R_PARALLELLY_AVAILABLECORES_FALLBACK=1</span>.
</p>


<h3>Configuring number of parallel workers</h3>

<p>The below <span class="rlang"><b>R</b></span> options and environment variables control the default results of <code><a href="#topic+availableCores">availableCores()</a></code> and <code><a href="#topic+availableWorkers">availableWorkers()</a></code>.
</p>

<dl>
<dt><code>parallelly.availableCores.logical</code>:</dt><dd><p>(logical) The default value of argument <code>logical</code> as used by <code>availableCores()</code>, <code>availableWorkers()</code>, and <code>availableCores()</code> for querying <code>parallel::detectCores(logical = logical)</code>.  The default is <code>TRUE</code> just like it is for <code><a href="parallel.html#topic+detectCores">parallel::detectCores()</a></code>.</p>
</dd>
<dt><code>parallelly.availableCores.methods</code>:</dt><dd><p>(character vector) Default lookup methods for <code><a href="#topic+availableCores">availableCores()</a></code>. (Default: <code>c("system", "cgroups.cpuset", "cgroups.cpuquota", "cgroups2.cpu.max", "nproc", "mc.cores", "BiocParallel", "_R_CHECK_LIMIT_CORES_", "Bioconductor", "LSF", "PJM", "PBS", "SGE", "Slurm", "fallback", "custom")</code>)</p>
</dd>
<dt><code>parallelly.availableCores.custom</code>:</dt><dd><p>(function) If set and a function, then this function will be called (without arguments) by <code><a href="#topic+availableCores">availableCores()</a></code> where its value, coerced to an integer, is interpreted as a number of cores.</p>
</dd>
<dt><code>parallelly.availableCores.fallback</code>:</dt><dd><p>(integer) Number of cores to use when no core-specifying settings are detected other than <code>"system"</code> and <code>"nproc"</code>.  This options makes it possible to set the default number of cores returned by <code>availableCores()</code> / <code>availableWorkers()</code> yet allow users and schedulers to override it.  In multi-tenant environment, such as HPC clusters, it is useful to set environment variable <span class="env">R_PARALLELLY_AVAILABLECORES_FALLBACK</span> to <code>1</code>, which will set this option when the package is loaded.</p>
</dd>
<dt><code>parallelly.availableCores.system</code>:</dt><dd><p>(integer) Number of &quot;system&quot; cores used instead of what is reported by <code><a href="#topic+availableCores">availableCores</a>(which = "system")</code>. This option allows you to effectively override what <code>parallel::detectCores()</code> reports the system has.</p>
</dd>
<dt><code>parallelly.availableCores.min</code>:</dt><dd><p>(integer) The minimum number of cores <code><a href="#topic+availableCores">availableCores()</a></code> is allowed to return. This can be used to force multiple cores on a single-core environment. If this is limit is applied, the names of the returned value are appended with an asterisk (<code>*</code>).  (Default: <code>1L</code>)</p>
</dd>
<dt><code>parallelly.availableCores.omit</code>:</dt><dd><p>(integer) Number of cores to set aside, i.e. not to include.</p>
</dd>
<dt><code>parallelly.availableWorkers.methods</code>:</dt><dd><p>(character vector) Default lookup methods for <code><a href="#topic+availableWorkers">availableWorkers()</a></code>. (Default: <code>c("mc.cores", "BiocParallel", "_R_CHECK_LIMIT_CORES_", "Bioconductor", "LSF", "PJM", "PBS", "SGE", "Slurm", "custom", "cgroups.cpuset", "cgroups.cpuquota", "cgroups2.cpu.max", "nproc", "system", "fallback")</code>)</p>
</dd>
<dt><code>parallelly.availableWorkers.custom</code>:</dt><dd><p>(function) If set and a function, then this function will be called (without arguments) by <code><a href="#topic+availableWorkers">availableWorkers()</a></code> where its value, coerced to a character vector, is interpreted as hostnames of available workers.</p>
</dd>
</dl>



<h3>Configuring forked parallel processing</h3>

<p>The below <span class="rlang"><b>R</b></span> options and environment variables control the default result of <code><a href="#topic+supportsMulticore">supportsMulticore()</a></code>.
</p>

<dl>
<dt><code>parallelly.fork.enable</code>:</dt><dd><p>(logical) Enable or disable <em>forked</em> processing.  If <code>FALSE</code>, multicore futures becomes sequential futures.  If <code>NA</code>, or not set (the default), the a set of best-practices rules decide whether should be supported or not.</p>
</dd>
<dt><code>parallelly.supportsMulticore.disableOn</code>:</dt><dd><p>(character vector)
because the environment in which R runs is considered unstable for
forked processing.
If this vector contains <code>"rstudio_console"</code>, it is disabled when
running R in the RStudio Console.
If this vector contains <code>"rstudio_terminal"</code>, it is disabled when
running R in the RStudio Terminal.
(Default: <code>c("rstudio_console", "rstudio_terminal")</code>)
</p>
</dd>
<dt><code>parallelly.supportsMulticore.unstable</code>:</dt><dd><p>(character) Controls whether a warning should be produced or not whenever multicore processing is automatically disabled per settings in option <code>parallelly.supportsMulticore.disableOn</code>.  If <code>"warn"</code> (default), then an informative warning is produces the first time 'multicore' futures are used.  If <code>"quiet"</code>, no warning is produced.</p>
</dd>
</dl>



<h3>Configuring setup of parallel PSOCK clusters</h3>

<p>The below <span class="rlang"><b>R</b></span> options and environment variables control the default results of <code><a href="#topic+makeClusterPSOCK">makeClusterPSOCK()</a></code> and its helper function <code><a href="#topic+makeNodePSOCK">makeNodePSOCK()</a></code> that creates the individual cluster nodes.
</p>

<dl>
<dt><code>parallelly.maxWorkers.localhost</code>:</dt><dd><p>(two numerics) Maximum number of localhost workers, relative to <code>availableCores()</code>, accepted and allowed. The first element corresponds to the threshold where a warning is produced, the second where an error is produced. Thresholds may be <code>+Inf</code>. If only the first exist, no error is produced (defaults to <code>c(1.0, 3.0)</code> corresponding to a maximum 100% and 300% use).</p>
</dd>
<dt><code>parallelly.makeNodePSOCK.setup_strategy</code>:</dt><dd><p>(character) If <code>"parallel"</code> (default), the PSOCK cluster nodes are set up concurrently.  If <code>"sequential"</code>, they are set up sequentially.</p>
</dd>
<dt><code>parallelly.makeNodePSOCK.validate</code>:</dt><dd><p>(logical) If TRUE (default), after the nodes have been created, they are all validated that they work by inquiring about their session information, which is saved in attribute <code>session_info</code> of each node.</p>
</dd>
<dt><code>parallelly.makeNodePSOCK.connectTimeout</code>:</dt><dd><p>(numeric) The maximum time (in seconds) allowed for each socket connection between the master and a worker to be established (defaults to 2*60 seconds = 2 minutes).</p>
</dd>
<dt><code>parallelly.makeNodePSOCK.timeout</code>:</dt><dd><p>(numeric) The maximum time (in seconds) allowed to pass without the master and a worker communicate with each other (defaults to 30<em>24</em>60*60 seconds = 30 days).</p>
</dd>
<dt><code>parallelly.makeNodePSOCK.useXDR</code>:</dt><dd><p>(logical) If FALSE (default), the communication between master and workers, which is binary, will use small-endian (faster), otherwise big-endian (&quot;XDR&quot;; slower).</p>
</dd>
<dt><code>parallelly.makeNodePSOCK.socketOptions</code>:</dt><dd><p>(character string) If set to another value than <code>"NULL"</code>, then option <code>socketOptions</code> is set to this value on the workers during startup. See <code><a href="base.html#topic+connections">base::socketConnection()</a></code> for details. (defaults to <code>"no-delay"</code>)</p>
</dd>
<dt><code>parallelly.makeNodePSOCK.rshcmd</code>:</dt><dd><p>(character vector) The command to be run on the master to launch a process on another host.</p>
</dd>
<dt><code>parallelly.makeNodePSOCK.rshopts</code>:</dt><dd><p>(character vector) Addition command-line options appended to <code>rshcmd</code>.  These arguments are only applied when connecting to non-localhost machines.</p>
</dd>
<dt><code>parallelly.makeNodePSOCK.tries</code>:</dt><dd><p>(integer) The maximum number of attempts done to launch each node.  Only used when setting up cluster nodes using the sequential strategy.</p>
</dd>
<dt><code>parallelly.makeNodePSOCK.tries.delay</code>:</dt><dd><p>(numeric) The number of seconds to wait before trying to launch a cluster node that failed to launch previously.  Only used when setting up cluster nodes using the sequential strategy.</p>
</dd>
</dl>



<h3>Options for debugging</h3>


<dl>
<dt><code>parallelly.debug</code>:</dt><dd><p>(logical) If <code>TRUE</code>, extensive debug messages are generated. (Default: <code>FALSE</code>)</p>
</dd>
</dl>



<h3>Environment variables that set R options</h3>

<p>All of the above <span class="rlang"><b>R</b></span> <code style="white-space: pre;">&#8288;parallelly.*&#8288;</code> options can be set by
corresponding environment variables <span class="env">R_PARALLELLY_*</span> <em>when the
<span class="pkg">parallelly</span> package is loaded</em>.
For example, if <code>R_PARALLELLY_MAKENODEPSOCK_SETUP_STRATEGY="sequential"</code>,
then option <code>parallelly.makeNodePSOCK.setup_strategy</code> is set to
<code>"sequential"</code> (character).
Similarly, if <code>R_PARALLELLY_AVAILABLECORES_FALLBACK="1"</code>, then option
<code>parallelly.availableCores.fallback</code> is set to <code>1</code> (integer).
</p>


<h3>See Also</h3>

<p>To set <span class="rlang"><b>R</b></span> options when <span class="rlang"><b>R</b></span> starts (even before the <span class="pkg">parallelly</span> package is loaded), see the <a href="base.html#topic+Startup">Startup</a> help page.  The <a href="https://cran.r-project.org/package=startup"><span class="pkg">startup</span></a> package provides a friendly mechanism for configuring <span class="rlang"><b>R</b></span>'s startup process.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Set an R option:
options(parallelly.availableCores.fallback = 1L)


</code></pre>

<hr>
<h2 id='pid_exists'>Check whether a process PID exists or not</h2><span id='topic+pid_exists'></span>

<h3>Description</h3>

<p>Check whether a process PID exists or not
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pid_exists(pid, debug = getOption2("parallelly.debug", FALSE))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pid_exists_+3A_pid">pid</code></td>
<td>
<p>A positive integer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is no single go-to function in <span class="rlang"><b>R</b></span> for testing whether a PID exists
or not.  Instead, this function tries to identify a working one among
multiple possible alternatives.  A method is considered working if the
PID of the current process is successfully identified as being existing
such that <code>pid_exists(Sys.getpid())</code> is <code>TRUE</code>.  If no working
approach is found, <code>pid_exists()</code> will always return <code>NA</code>
regardless of PID tested.
On Unix, including macOS, alternatives <code>tools::pskill(pid, signal = 0L)</code>
and <code>system2("ps", args = pid)</code> are used.
On MS Windows, various alternatives of <code>system2("tasklist", ...)</code> are used.
Note, some MS Windows machines are configures to not allow using
<code>tasklist</code> on other process IDs than the current one.
</p>


<h3>Value</h3>

<p>Returns <code>TRUE</code> if a process with the given PID exists,
<code>FALSE</code> if a process with the given PID does not exists, and
<code>NA</code> if it is not possible to check PIDs on the current system.
</p>


<h3>References</h3>


<ol>
<li><p> The Open Group Base Specifications Issue 7, 2018 edition,
IEEE Std 1003.1-2017 (Revision of IEEE Std 1003.1-2008)
<a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/kill.html">https://pubs.opengroup.org/onlinepubs/9699919799/functions/kill.html</a>
</p>
</li>
<li><p> Microsoft, tasklist, 2021-03-03,
<a href="https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/tasklist">https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/tasklist</a>
</p>
</li>
<li><p> R-devel thread 'Detecting whether a process exists or not by its PID?',
2018-08-30.
<a href="https://stat.ethz.ch/pipermail/r-devel/2018-August/076702.html">https://stat.ethz.ch/pipermail/r-devel/2018-August/076702.html</a>
</p>
</li></ol>



<h3>See Also</h3>

<p><code><a href="tools.html#topic+pskill">pskill</a>()</code> and <code><a href="base.html#topic+system2">system2</a>()</code>.
</p>

<hr>
<h2 id='serializedSize'>Calculate the size of an R object when it is serialized</h2><span id='topic+serializedSize'></span>

<h3>Description</h3>

<p>This function goes through all the motions of serializing an object, but
does nothing with the bytes other than to tally the total length.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>serializedSize(obj)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="serializedSize_+3A_obj">obj</code></td>
<td>
<p>An R object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(double) Number of bytes needed to serialize this object.
</p>


<h3>Author(s)</h3>

<p>Mike FC
</p>


<h3>Examples</h3>

<pre><code class='language-R'>object.size(mtcars)
serializedSize(mtcars)

</code></pre>

<hr>
<h2 id='supportsMulticore'>Check If Forked Processing (&quot;multicore&quot;) is Supported</h2><span id='topic+supportsMulticore'></span>

<h3>Description</h3>

<p>Certain parallelization methods in R rely on <em>forked</em> processing, e.g.
<code>parallel::mclapply()</code>, <code>parallel::makeCluster(n, type = "FORK")</code>,
<code>doMC::registerDoMC()</code>, and <code>future::plan("multicore")</code>.
Process forking is done by the operating system and support for it in
<span class="rlang"><b>R</b></span> is restricted to Unix-like operating systems such as Linux, Solaris,
and macOS.  R running on Microsoft Windows does not support forked
processing.
In R, forked processing is often referred to as &quot;multicore&quot; processing,
which stems from the 'mc' of the <code>mclapply()</code> family of functions, which
originally was in a package named <span class="pkg">multicore</span> which later was
incorporated into the <span class="pkg">parallel</span> package.
This function checks whether or not forked (aka &quot;multicore&quot;) processing
is supported in the current <span class="rlang"><b>R</b></span> session.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>supportsMulticore(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="supportsMulticore_+3A_...">...</code></td>
<td>
<p>Internal usage only.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if forked processing is supported and not disabled,
otherwise FALSE.
</p>


<h3>Support for process forking</h3>

<p>While R supports forked processing on Unix-like operating system such as
Linux and macOS, it does not on the Microsoft Windows operating system.
</p>
<p>For some R environments it is considered unstable to perform parallel
processing based on <em>forking</em>.
This is for example the case when using RStudio, cf.
<a href="https://github.com/rstudio/rstudio/issues/2597#issuecomment-482187011">RStudio Inc. recommends against using forked processing when running R from within the RStudio software</a>.
This function detects when running in such an environment and returns
<code>FALSE</code>, despite the underlying operating system supports forked processing.
A warning will also be produced informing the user about this the first
time time this function is called in an <span class="rlang"><b>R</b></span> session.
This warning can be disabled by setting R option
<code>parallelly.supportsMulticore.unstable</code>, or environment variable
<span class="env">R_PARALLELLY_SUPPORTSMULTICORE_UNSTABLE</span> to <code>"quiet"</code>.
</p>


<h3>Enable or disable forked processing</h3>

<p>It is possible to disable forked processing for futures by setting <span class="rlang"><b>R</b></span>
option <code>parallelly.fork.enable</code> to <code>FALSE</code>.  Alternatively, one can
set environment variable <span class="env">R_PARALLELLY_FORK_ENABLE</span> to <code>false</code>.
Analogously, it is possible to override disabled forking by setting one
of these to <code>TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Check whether or not forked processing is supported
supportsMulticore()

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
