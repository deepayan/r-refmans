<!DOCTYPE html><html><head><title>Help for package ddml</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ddml}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#AE98'><p>Random subsample from the data of Angrist &amp; Evans (1991).</p></a></li>
<li><a href='#crosspred'><p>Cross-Predictions using Stacking.</p></a></li>
<li><a href='#crossval'><p>Estimator of the Mean Squared Prediction Error using Cross-Validation.</p></a></li>
<li><a href='#ddml'><p>ddml: Double/Debiased Machine Learning in R</p></a></li>
<li><a href='#ddml_ate'><p>Estimators of Average Treatment Effects.</p></a></li>
<li><a href='#ddml_fpliv'><p>Estimator for the Flexible Partially Linear IV Model.</p></a></li>
<li><a href='#ddml_late'><p>Estimator of the Local Average Treatment Effect.</p></a></li>
<li><a href='#ddml_pliv'><p>Estimator for the Partially Linear IV Model.</p></a></li>
<li><a href='#ddml_plm'><p>Estimator for the Partially Linear Model.</p></a></li>
<li><a href='#mdl_glm'><p>Wrapper for <code>stats::glm()</code>.</p></a></li>
<li><a href='#mdl_glmnet'><p>Wrapper for <code>glmnet::glmnet()</code>.</p></a></li>
<li><a href='#mdl_ranger'><p>Wrapper for <code>ranger::ranger()</code>.</p></a></li>
<li><a href='#mdl_xgboost'><p>Wrapper for <code>xgboost::xgboost()</code>.</p></a></li>
<li><a href='#ols'><p>Ordinary least squares.</p></a></li>
<li><a href='#print.summary.ddml_ate'><p>Print Methods for Treatment Effect Estimators.</p></a></li>
<li><a href='#print.summary.ddml_fpliv'><p>Print Methods for Treatment Effect Estimators.</p></a></li>
<li><a href='#shortstacking'><p>Predictions using Short-Stacking.</p></a></li>
<li><a href='#summary.ddml_ate'><p>Inference Methods for Treatment Effect Estimators.</p></a></li>
<li><a href='#summary.ddml_fpliv'><p>Inference Methods for Partially Linear Estimators.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Double/Debiased Machine Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-06-26</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimate common causal parameters using double/debiased machine 
    learning as proposed by Chernozhukov et al. (2018) &lt;<a href="https://doi.org/10.1111%2Fectj.12097">doi:10.1111/ectj.12097</a>&gt;. 
    'ddml' simplifies estimation based on (short-)stacking as discussed in 
    Ahrens et al. (2024) &lt;<a href="https://doi.org/10.1177%2F1536867X241233641">doi:10.1177/1536867X241233641</a>&gt;, which leverages multiple base 
    learners to increase robustness to the underlying data generating process.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/thomaswiemann/ddml">https://github.com/thomaswiemann/ddml</a>,
<a href="https://thomaswiemann.com/ddml/">https://thomaswiemann.com/ddml/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/thomaswiemann/ddml/issues">https://github.com/thomaswiemann/ddml/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, stats, AER, MASS, Matrix, nnls, quadprog, glmnet,
ranger, xgboost</td>
</tr>
<tr>
<td>Suggests:</td>
<td>sandwich, covr, testthat (&ge; 3.0.0), knitr, rmarkdown</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-26 23:53:58 UTC; thomas</td>
</tr>
<tr>
<td>Author:</td>
<td>Achim Ahrens [aut],
  Christian B Hansen [aut],
  Mark E Schaffer [aut],
  Thomas Wiemann [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Thomas Wiemann &lt;wiemann@uchicago.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-27 00:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='AE98'>Random subsample from the data of Angrist &amp; Evans (1991).</h2><span id='topic+AE98'></span>

<h3>Description</h3>

<p>Random subsample from the data of Angrist &amp; Evans (1991).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AE98
</code></pre>


<h3>Format</h3>

<p>A data frame with 5,000 rows and 13 variables.
</p>

<dl>
<dt>worked</dt><dd><p>Indicator equal to 1 if the mother is employed.</p>
</dd>
<dt>weeksw</dt><dd><p>Number of weeks of employment.</p>
</dd>
<dt>hoursw</dt><dd><p>Hours worked per week.</p>
</dd>
<dt>morekids</dt><dd><p>Indicator equal to 1 if the mother has more than 2 kids.</p>
</dd>
<dt>samesex</dt><dd><p>Indicator equal to 1 if the first two children are of the
same sex.</p>
</dd>
<dt>age</dt><dd><p>Age in years.</p>
</dd>
<dt>agefst</dt><dd><p>Age in years at birth of the first child.</p>
</dd>
<dt>black</dt><dd><p>Indicator equal to 1 if the mother is black.</p>
</dd>
<dt>hisp</dt><dd><p>Indicator equal to 1 if the mother is Hispanic.</p>
</dd>
<dt>othrace</dt><dd><p>Indicator equal to 1 if the mother is neither black nor
Hispanic.</p>
</dd>
<dt>educ</dt><dd><p>Years of education.</p>
</dd>
<dt>boy1st</dt><dd><p>Indicator equal to 1 if the first child is male.</p>
</dd>
<dt>boy2nd</dt><dd><p>Indicator equal to 1 if the second child is male.</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/11288">https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/11288</a>
</p>


<h3>References</h3>

<p>Angrist J, Evans W (1998). &quot;Children and Their Parents' Labor Supply:
Evidence from Exogenous Variation in Family Size.&quot; American Economic
Review, 88(3), 450-477.
</p>

<hr>
<h2 id='crosspred'>Cross-Predictions using Stacking.</h2><span id='topic+crosspred'></span>

<h3>Description</h3>

<p>Cross-predictions using stacking.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crosspred(
  y,
  X,
  Z = NULL,
  learners,
  sample_folds = 2,
  ensemble_type = "average",
  cv_folds = 5,
  custom_ensemble_weights = NULL,
  compute_insample_predictions = FALSE,
  compute_predictions_bylearner = FALSE,
  subsamples = NULL,
  cv_subsamples_list = NULL,
  silent = FALSE,
  progress = NULL,
  auxilliary_X = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crosspred_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_x">X</code></td>
<td>
<p>A (sparse) matrix of predictive variables.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_z">Z</code></td>
<td>
<p>Optional additional (sparse) matrix of predictive variables.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_learners">learners</code></td>
<td>
<p>May take one of two forms, depending on whether a single
learner or stacking with multiple learners is used for estimation of the
predictor.
If a single learner is used, <code>learners</code> is a list with two named
elements:
</p>

<ul>
<li><p><code>what</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>what</code>.
</p>
</li></ul>

<p>If stacking with multiple learners is used, <code>learners</code> is a list of
lists, each containing four named elements:
</p>

<ul>
<li><p><code>fun</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>fun</code>.
</p>
</li>
<li><p><code>assign_X</code> An optional vector of column indices
corresponding to predictive variables in <code>X</code> that are passed to
the base learner.
</p>
</li>
<li><p><code>assign_Z</code> An optional vector of column indices
corresponding to predictive in <code>Z</code> that are passed to the
base learner.
</p>
</li></ul>

<p>Omission of the <code>args</code> element results in default arguments being
used in <code>fun</code>. Omission of <code>assign_X</code> (and/or <code>assign_Z</code>)
results in inclusion of all variables in <code>X</code> (and/or <code>Z</code>).</p>
</td></tr>
<tr><td><code id="crosspred_+3A_sample_folds">sample_folds</code></td>
<td>
<p>Number of cross-fitting folds.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_ensemble_type">ensemble_type</code></td>
<td>
<p>Ensemble method to combine base learners into final
estimate of the conditional expectation functions. Possible values are:
</p>

<ul>
<li><p><code>"nnls"</code> Non-negative least squares.
</p>
</li>
<li><p><code>"nnls1"</code> Non-negative least squares with the constraint
that all weights sum to one.
</p>
</li>
<li><p><code>"singlebest"</code> Select base learner with minimum MSPE.
</p>
</li>
<li><p><code>"ols"</code> Ordinary least squares.
</p>
</li>
<li><p><code>"average"</code> Simple average over base learners.
</p>
</li></ul>

<p>Multiple ensemble types may be passed as a vector of strings.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_cv_folds">cv_folds</code></td>
<td>
<p>Number of folds used for cross-validation in ensemble
construction.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_custom_ensemble_weights">custom_ensemble_weights</code></td>
<td>
<p>A numerical matrix with user-specified
ensemble weights. Each column corresponds to a custom ensemble
specification, each row corresponds to a base learner in <code>learners</code>
(in chronological order). Optional column names are used to name the
estimation results corresponding the custom ensemble specification.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_compute_insample_predictions">compute_insample_predictions</code></td>
<td>
<p>Indicator equal to 1 if in-sample
predictions should also be computed.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_compute_predictions_bylearner">compute_predictions_bylearner</code></td>
<td>
<p>Indicator equal to 1 if in-sample
predictions should also be computed for each learner (rather than the
entire ensemble).</p>
</td></tr>
<tr><td><code id="crosspred_+3A_subsamples">subsamples</code></td>
<td>
<p>List of vectors with sample indices for cross-fitting.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_cv_subsamples_list">cv_subsamples_list</code></td>
<td>
<p>List of lists, each corresponding to a subsample
containing vectors with subsample indices for cross-validation.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_silent">silent</code></td>
<td>
<p>Boolean to silence estimation updates.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_progress">progress</code></td>
<td>
<p>String to print before learner and cv fold progress.</p>
</td></tr>
<tr><td><code id="crosspred_+3A_auxilliary_x">auxilliary_X</code></td>
<td>
<p>An optional list of matrices of length
<code>sample_folds</code>, each containing additional observations to calculate
predictions for.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>crosspred</code> returns a list containing the following components:
</p>

<dl>
<dt><code>oos_fitted</code></dt><dd><p>A matrix of out-of-sample predictions,
each column corresponding to an ensemble type (in chronological
order).</p>
</dd>
<dt><code>weights</code></dt><dd><p>An array, providing the weight
assigned to each base learner (in chronological order) by the
ensemble procedures.</p>
</dd>
<dt><code>is_fitted</code></dt><dd><p>When <code>compute_insample_predictions = T</code>.
a list of matrices with in-sample predictions by sample fold.</p>
</dd>
<dt><code>auxilliary_fitted</code></dt><dd><p>When <code>auxilliary_X</code> is not
<code>NULL</code>, a list of matrices with additional predictions.</p>
</dd>
<dt><code>oos_fitted_bylearner</code></dt><dd><p>When
<code>compute_predictions_bylearner = T</code>, a matrix of
out-of-sample predictions, each column corresponding to a base
learner (in chronological order).</p>
</dd>
<dt><code>is_fitted_bylearner</code></dt><dd><p>When
<code>compute_insample_predictions = T</code> and
<code>compute_predictions_bylearner = T</code>, a list of matrices with
in-sample predictions by sample fold.</p>
</dd>
<dt><code>auxilliary_fitted_bylearner</code></dt><dd><p>When <code>auxilliary_X</code> is
not <code>NULL</code> and <code>compute_predictions_bylearner = T</code>, a
list of matrices with additional predictions for each learner.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ahrens A, Hansen C B, Schaffer M E, Wiemann T (2023). &quot;ddml: Double/debiased
machine learning in Stata.&quot; <a href="https://arxiv.org/abs/2301.09397">https://arxiv.org/abs/2301.09397</a>
</p>
<p>Wolpert D H (1992). &quot;Stacked generalization.&quot; Neural Networks, 5(2), 241-259.
</p>


<h3>See Also</h3>

<p>Other utilities: 
<code><a href="#topic+crossval">crossval</a>()</code>,
<code><a href="#topic+shortstacking">shortstacking</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
X = AE98[, c("morekids", "age","agefst","black","hisp","othrace","educ")]

# Compute cross-predictions using stacking with base learners ols and lasso.
#     Two stacking approaches are simultaneously computed: Equally
#     weighted (ensemble_type = "average") and MSPE-minimizing with weights
#     in the unit simplex (ensemble_type = "nnls1"). Predictions for each
#     learner are also calculated.
crosspred_res &lt;- crosspred(y, X,
                           learners = list(list(fun = ols),
                                           list(fun = mdl_glmnet)),
                           ensemble_type = c("average",
                                             "nnls1",
                                             "singlebest"),
                           compute_predictions_bylearner = TRUE,
                           sample_folds = 2,
                           cv_folds = 2,
                           silent = TRUE)
dim(crosspred_res$oos_fitted) # = length(y) by length(ensemble_type)
dim(crosspred_res$oos_fitted_bylearner) # = length(y) by length(learners)
</code></pre>

<hr>
<h2 id='crossval'>Estimator of the Mean Squared Prediction Error using Cross-Validation.</h2><span id='topic+crossval'></span>

<h3>Description</h3>

<p>Estimator of the mean squared prediction error of
different learners using cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crossval(
  y,
  X,
  Z = NULL,
  learners,
  cv_folds = 5,
  cv_subsamples = NULL,
  silent = FALSE,
  progress = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crossval_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="crossval_+3A_x">X</code></td>
<td>
<p>A (sparse) matrix of predictive variables.</p>
</td></tr>
<tr><td><code id="crossval_+3A_z">Z</code></td>
<td>
<p>Optional additional (sparse) matrix of predictive variables.</p>
</td></tr>
<tr><td><code id="crossval_+3A_learners">learners</code></td>
<td>
<p><code>learners</code> is a list of lists, each containing four
named elements:
</p>

<ul>
<li><p><code>fun</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>fun</code>.
</p>
</li>
<li><p><code>assign_X</code> An optional vector of column indices
corresponding to variables in <code>X</code> that are passed to
the base learner.
</p>
</li>
<li><p><code>assign_Z</code> An optional vector of column indices
corresponding to variables in <code>Z</code> that are passed to the
base learner.
</p>
</li></ul>

<p>Omission of the <code>args</code> element results in default arguments being
used in <code>fun</code>. Omission of <code>assign_X</code> (and/or <code>assign_Z</code>)
results in inclusion of all predictive variables in <code>X</code> (and/or
<code>Z</code>).</p>
</td></tr>
<tr><td><code id="crossval_+3A_cv_folds">cv_folds</code></td>
<td>
<p>Number of folds used for cross-validation.</p>
</td></tr>
<tr><td><code id="crossval_+3A_cv_subsamples">cv_subsamples</code></td>
<td>
<p>List of vectors with sample indices for
cross-validation.</p>
</td></tr>
<tr><td><code id="crossval_+3A_silent">silent</code></td>
<td>
<p>Boolean to silence estimation updates.</p>
</td></tr>
<tr><td><code id="crossval_+3A_progress">progress</code></td>
<td>
<p>String to print before learner and cv fold progress.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>crossval</code> returns a list containing the following components:
</p>

<dl>
<dt><code>mspe</code></dt><dd><p>A vector of MSPE estimates,
each corresponding to a base learners (in chronological order).</p>
</dd>
<dt><code>oos_resid</code></dt><dd><p>A matrix of out-of-sample prediction errors,
each column corresponding to a base learners (in chronological
order).</p>
</dd>
<dt><code>cv_subsamples</code></dt><dd><p>Pass-through of <code>cv_subsamples</code>.
See above.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Other utilities: 
<code><a href="#topic+crosspred">crosspred</a>()</code>,
<code><a href="#topic+shortstacking">shortstacking</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
X = AE98[, c("morekids", "age","agefst","black","hisp","othrace","educ")]

# Compare ols, lasso, and ridge using 4-fold cross-validation
cv_res &lt;- crossval(y, X,
                   learners = list(list(fun = ols),
                                   list(fun = mdl_glmnet),
                                   list(fun = mdl_glmnet,
                                        args = list(alpha = 0))),
                   cv_folds = 4,
                   silent = TRUE)
cv_res$mspe
</code></pre>

<hr>
<h2 id='ddml'>ddml: Double/Debiased Machine Learning in R</h2><span id='topic+ddml'></span>

<h3>Description</h3>

<p>Estimate common causal parameters using double/debiased machine
learning as proposed by Chernozhukov et al. (2018).
'ddml' simplifies estimation based on (short-)stacking, which leverages
multiple base learners to increase robustness to the underlying
data generating process.
</p>


<h3>References</h3>

<p>Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W,
Robins J (2018). &quot;Double/debiased machine learning for treatment and
structural parameters.&quot; The Econometrics Journal, 21(1), C1-C68.
</p>

<hr>
<h2 id='ddml_ate'>Estimators of Average Treatment Effects.</h2><span id='topic+ddml_ate'></span><span id='topic+ddml_att'></span>

<h3>Description</h3>

<p>Estimators of the average treatment effect and the average
treatment effect on the treated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddml_ate(
  y,
  D,
  X,
  learners,
  learners_DX = learners,
  sample_folds = 2,
  ensemble_type = "nnls",
  shortstack = FALSE,
  cv_folds = 5,
  custom_ensemble_weights = NULL,
  custom_ensemble_weights_DX = custom_ensemble_weights,
  subsamples_D0 = NULL,
  subsamples_D1 = NULL,
  cv_subsamples_list_D0 = NULL,
  cv_subsamples_list_D1 = NULL,
  trim = 0.01,
  silent = FALSE
)

ddml_att(
  y,
  D,
  X,
  learners,
  learners_DX = learners,
  sample_folds = 2,
  ensemble_type = "nnls",
  shortstack = FALSE,
  cv_folds = 5,
  custom_ensemble_weights = NULL,
  custom_ensemble_weights_DX = custom_ensemble_weights,
  subsamples_D0 = NULL,
  subsamples_D1 = NULL,
  cv_subsamples_list_D0 = NULL,
  cv_subsamples_list_D1 = NULL,
  trim = 0.01,
  silent = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddml_ate_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_d">D</code></td>
<td>
<p>The binary endogenous variable of interest.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_x">X</code></td>
<td>
<p>A (sparse) matrix of control variables.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_learners">learners</code></td>
<td>
<p>May take one of two forms, depending on whether a single
learner or stacking with multiple learners is used for estimation of the
conditional expectation functions.
If a single learner is used, <code>learners</code> is a list with two named
elements:
</p>

<ul>
<li><p><code>what</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>what</code>.
</p>
</li></ul>

<p>If stacking with multiple learners is used, <code>learners</code> is a list of
lists, each containing four named elements:
</p>

<ul>
<li><p><code>fun</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>fun</code>.
</p>
</li>
<li><p><code>assign_X</code> An optional vector of column indices
corresponding to control variables in <code>X</code> that are passed to
the base learner.
</p>
</li></ul>

<p>Omission of the <code>args</code> element results in default arguments being
used in <code>fun</code>. Omission of <code>assign_X</code> results in inclusion of
all variables in <code>X</code>.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_learners_dx">learners_DX</code></td>
<td>
<p>Optional argument to allow for different estimators of
<code class="reqn">E[D|X]</code>. Setup is identical to <code>learners</code>.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_sample_folds">sample_folds</code></td>
<td>
<p>Number of cross-fitting folds.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_ensemble_type">ensemble_type</code></td>
<td>
<p>Ensemble method to combine base learners into final
estimate of the conditional expectation functions. Possible values are:
</p>

<ul>
<li><p><code>"nnls"</code> Non-negative least squares.
</p>
</li>
<li><p><code>"nnls1"</code> Non-negative least squares with the constraint
that all weights sum to one.
</p>
</li>
<li><p><code>"singlebest"</code> Select base learner with minimum MSPE.
</p>
</li>
<li><p><code>"ols"</code> Ordinary least squares.
</p>
</li>
<li><p><code>"average"</code> Simple average over base learners.
</p>
</li></ul>

<p>Multiple ensemble types may be passed as a vector of strings.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_shortstack">shortstack</code></td>
<td>
<p>Boolean to use short-stacking.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_cv_folds">cv_folds</code></td>
<td>
<p>Number of folds used for cross-validation in ensemble
construction.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_custom_ensemble_weights">custom_ensemble_weights</code></td>
<td>
<p>A numerical matrix with user-specified
ensemble weights. Each column corresponds to a custom ensemble
specification, each row corresponds to a base learner in <code>learners</code>
(in chronological order). Optional column names are used to name the
estimation results corresponding the custom ensemble specification.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_custom_ensemble_weights_dx">custom_ensemble_weights_DX</code></td>
<td>
<p>Optional argument to allow for different
custom ensemble weights for <code>learners_DX</code>. Setup is identical to
<code>custom_ensemble_weights</code>. Note: <code>custom_ensemble_weights</code> and
<code>custom_ensemble_weights_DX</code> must have the same number of columns.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_subsamples_d0">subsamples_D0</code>, <code id="ddml_ate_+3A_subsamples_d1">subsamples_D1</code></td>
<td>
<p>List of vectors with sample indices for
cross-fitting, corresponding to untreated and treated observations,
respectively.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_cv_subsamples_list_d0">cv_subsamples_list_D0</code>, <code id="ddml_ate_+3A_cv_subsamples_list_d1">cv_subsamples_list_D1</code></td>
<td>
<p>List of lists, each
corresponding to a subsample containing vectors with subsample indices
for cross-validation. Arguments are separated for untreated and treated
observations, respectively.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_trim">trim</code></td>
<td>
<p>Number in (0, 1) for trimming the estimated propensity scores at
<code>trim</code> and <code>1-trim</code>.</p>
</td></tr>
<tr><td><code id="ddml_ate_+3A_silent">silent</code></td>
<td>
<p>Boolean to silence estimation updates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ddml_ate</code> and <code>ddml_att</code> provide double/debiased machine
learning  estimators for the average treatment effect and the average
treatment effect on the treated, respectively, in the interactive model
given by
</p>
<p><code class="reqn">Y = g_0(D, X) + U,</code>
</p>
<p>where <code class="reqn">(Y, D, X, U)</code> is a random vector such that
<code class="reqn">\operatorname{supp} D = \{0,1\}</code>, <code class="reqn">E[U\vert D, X] = 0</code>, and
<code class="reqn">\Pr(D=1\vert X) \in (0, 1)</code> with probability 1,
and <code class="reqn">g_0</code> is an unknown nuisance function.
</p>
<p>In this model, the average treatment effect is defined as
</p>
<p><code class="reqn">\theta_0^{\textrm{ATE}} \equiv E[g_0(1, X) - g_0(0, X)]</code>.
</p>
<p>and the average treatment effect on the treated is defined as
</p>
<p><code class="reqn">\theta_0^{\textrm{ATT}} \equiv E[g_0(1, X) - g_0(0, X)\vert D = 1]</code>.
</p>


<h3>Value</h3>

<p><code>ddml_ate</code> and <code>ddml_att</code> return an object of S3 class
<code>ddml_ate</code> and <code>ddml_att</code>, respectively. An object of class
<code>ddml_ate</code> or <code>ddml_att</code> is a list containing
the following components:
</p>

<dl>
<dt><code>ate</code> / <code>att</code></dt><dd><p>A vector with the average treatment
effect / average treatment effect on the treated estimates.</p>
</dd>
<dt><code>weights</code></dt><dd><p>A list of matrices, providing the weight
assigned to each base learner (in chronological order) by the
ensemble procedure.</p>
</dd>
<dt><code>mspe</code></dt><dd><p>A list of matrices, providing the MSPE of each
base learner (in chronological order) computed by the
cross-validation step in the ensemble construction.</p>
</dd>
<dt><code>psi_a</code>, <code>psi_b</code></dt><dd><p>Matrices needed for the computation
of scores. Used in <code><a href="#topic+summary.ddml_ate">summary.ddml_ate()</a></code> or
<code><a href="#topic+summary.ddml_att">summary.ddml_att()</a></code>.</p>
</dd>
<dt><code>oos_pred</code></dt><dd><p>List of matrices, providing the reduced form
predicted values.</p>
</dd>
<dt><code>learners</code>,<code>learners_DX</code>,
<code>subsamples_D0</code>,<code>subsamples_D1</code>,
<code>cv_subsamples_list_D0</code>,<code>cv_subsamples_list_D1</code>,
<code>ensemble_type</code></dt><dd><p>Pass-through of
selected user-provided arguments. See above.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ahrens A, Hansen C B, Schaffer M E, Wiemann T (2023). &quot;ddml: Double/debiased
machine learning in Stata.&quot; <a href="https://arxiv.org/abs/2301.09397">https://arxiv.org/abs/2301.09397</a>
</p>
<p>Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W,
Robins J (2018). &quot;Double/debiased machine learning for treatment and
structural parameters.&quot; The Econometrics Journal, 21(1), C1-C68.
</p>
<p>Wolpert D H (1992). &quot;Stacked generalization.&quot; Neural Networks, 5(2), 241-259.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.ddml_ate">summary.ddml_ate()</a></code>, <code><a href="#topic+summary.ddml_att">summary.ddml_att()</a></code>
</p>
<p>Other ddml: 
<code><a href="#topic+ddml_fpliv">ddml_fpliv</a>()</code>,
<code><a href="#topic+ddml_late">ddml_late</a>()</code>,
<code><a href="#topic+ddml_pliv">ddml_pliv</a>()</code>,
<code><a href="#topic+ddml_plm">ddml_plm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
D = AE98[, "morekids"]
X = AE98[, c("age","agefst","black","hisp","othrace","educ")]

# Estimate the average treatment effect using a single base learner, ridge.
ate_fit &lt;- ddml_ate(y, D, X,
                    learners = list(what = mdl_glmnet,
                                    args = list(alpha = 0)),
                    sample_folds = 2,
                    silent = TRUE)
summary(ate_fit)

# Estimate the average treatment effect using short-stacking with base
#     learners ols, lasso, and ridge. We can also use custom_ensemble_weights
#     to estimate the ATE using every individual base learner.
weights_everylearner &lt;- diag(1, 3)
colnames(weights_everylearner) &lt;- c("mdl:ols", "mdl:lasso", "mdl:ridge")
ate_fit &lt;- ddml_ate(y, D, X,
                    learners = list(list(fun = ols),
                                    list(fun = mdl_glmnet),
                                    list(fun = mdl_glmnet,
                                         args = list(alpha = 0))),
                    ensemble_type = 'nnls',
                    custom_ensemble_weights = weights_everylearner,
                    shortstack = TRUE,
                    sample_folds = 2,
                    silent = TRUE)
summary(ate_fit)
</code></pre>

<hr>
<h2 id='ddml_fpliv'>Estimator for the Flexible Partially Linear IV Model.</h2><span id='topic+ddml_fpliv'></span>

<h3>Description</h3>

<p>Estimator for the flexible partially linear IV model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddml_fpliv(
  y,
  D,
  Z,
  X,
  learners,
  learners_DXZ = learners,
  learners_DX = learners,
  sample_folds = 2,
  ensemble_type = "nnls",
  shortstack = FALSE,
  cv_folds = 5,
  enforce_LIE = TRUE,
  custom_ensemble_weights = NULL,
  custom_ensemble_weights_DXZ = custom_ensemble_weights,
  custom_ensemble_weights_DX = custom_ensemble_weights,
  subsamples = NULL,
  cv_subsamples_list = NULL,
  silent = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddml_fpliv_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_d">D</code></td>
<td>
<p>A matrix of endogenous variables.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_z">Z</code></td>
<td>
<p>A (sparse) matrix of instruments.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_x">X</code></td>
<td>
<p>A (sparse) matrix of control variables.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_learners">learners</code></td>
<td>
<p>May take one of two forms, depending on whether a single
learner or stacking with multiple learners is used for estimation of the
conditional expectation functions.
If a single learner is used, <code>learners</code> is a list with two named
elements:
</p>

<ul>
<li><p><code>what</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>what</code>.
</p>
</li></ul>

<p>If stacking with multiple learners is used, <code>learners</code> is a list of
lists, each containing four named elements:
</p>

<ul>
<li><p><code>fun</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>fun</code>.
</p>
</li>
<li><p><code>assign_X</code> An optional vector of column indices
corresponding to control variables in <code>X</code> that are passed to
the base learner.
</p>
</li>
<li><p><code>assign_Z</code> An optional vector of column indices
corresponding to instruments in <code>Z</code> that are passed to the
base learner.
</p>
</li></ul>

<p>Omission of the <code>args</code> element results in default arguments being
used in <code>fun</code>. Omission of <code>assign_X</code> (and/or <code>assign_Z</code>)
results in inclusion of all variables in <code>X</code> (and/or <code>Z</code>).</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_learners_dxz">learners_DXZ</code>, <code id="ddml_fpliv_+3A_learners_dx">learners_DX</code></td>
<td>
<p>Optional arguments to allow for different
estimators of <code class="reqn">E[D \vert X, Z]</code>, <code class="reqn">E[D \vert X]</code>. Setup is
identical to <code>learners</code>.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_sample_folds">sample_folds</code></td>
<td>
<p>Number of cross-fitting folds.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_ensemble_type">ensemble_type</code></td>
<td>
<p>Ensemble method to combine base learners into final
estimate of the conditional expectation functions. Possible values are:
</p>

<ul>
<li><p><code>"nnls"</code> Non-negative least squares.
</p>
</li>
<li><p><code>"nnls1"</code> Non-negative least squares with the constraint
that all weights sum to one.
</p>
</li>
<li><p><code>"singlebest"</code> Select base learner with minimum MSPE.
</p>
</li>
<li><p><code>"ols"</code> Ordinary least squares.
</p>
</li>
<li><p><code>"average"</code> Simple average over base learners.
</p>
</li></ul>

<p>Multiple ensemble types may be passed as a vector of strings.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_shortstack">shortstack</code></td>
<td>
<p>Boolean to use short-stacking.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_cv_folds">cv_folds</code></td>
<td>
<p>Number of folds used for cross-validation in ensemble
construction.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_enforce_lie">enforce_LIE</code></td>
<td>
<p>Indicator equal to 1 if the law of iterated expectations
is enforced in the first stage.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_custom_ensemble_weights">custom_ensemble_weights</code></td>
<td>
<p>A numerical matrix with user-specified
ensemble weights. Each column corresponds to a custom ensemble
specification, each row corresponds to a base learner in <code>learners</code>
(in chronological order). Optional column names are used to name the
estimation results corresponding the custom ensemble specification.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_custom_ensemble_weights_dxz">custom_ensemble_weights_DXZ</code>, <code id="ddml_fpliv_+3A_custom_ensemble_weights_dx">custom_ensemble_weights_DX</code></td>
<td>
<p>Optional
arguments to allow for different
custom ensemble weights for <code>learners_DXZ</code>,<code>learners_DX</code>. Setup
is identical to <code>custom_ensemble_weights</code>. Note:
<code>custom_ensemble_weights</code> and
<code>custom_ensemble_weights_DXZ</code>,<code>custom_ensemble_weights_DX</code> must
have the same number of columns.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_subsamples">subsamples</code></td>
<td>
<p>List of vectors with sample indices for cross-fitting.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_cv_subsamples_list">cv_subsamples_list</code></td>
<td>
<p>List of lists, each corresponding to a subsample
containing vectors with subsample indices for cross-validation.</p>
</td></tr>
<tr><td><code id="ddml_fpliv_+3A_silent">silent</code></td>
<td>
<p>Boolean to silence estimation updates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ddml_fpliv</code> provides a double/debiased machine learning
estimator for the parameter of interest <code class="reqn">\theta_0</code> in the partially
linear IV model given by
</p>
<p><code class="reqn">Y = \theta_0D + g_0(X) + U,</code>
</p>
<p>where <code class="reqn">(Y, D, X, Z, U)</code> is a random vector such that
<code class="reqn">E[U\vert X, Z] = 0</code> and <code class="reqn">E[Var(E[D\vert X, Z]\vert X)] \neq 0</code>,
and <code class="reqn">g_0</code> is an unknown nuisance function.
</p>


<h3>Value</h3>

<p><code>ddml_fpliv</code> returns an object of S3 class
<code>ddml_fpliv</code>. An object of class <code>ddml_fpliv</code> is a list
containing the following components:
</p>

<dl>
<dt><code>coef</code></dt><dd><p>A vector with the <code class="reqn">\theta_0</code> estimates.</p>
</dd>
<dt><code>weights</code></dt><dd><p>A list of matrices, providing the weight
assigned to each base learner (in chronological order) by the
ensemble procedure.</p>
</dd>
<dt><code>mspe</code></dt><dd><p>A list of matrices, providing the MSPE of each
base learner (in chronological order) computed by the
cross-validation step in the ensemble construction.</p>
</dd>
<dt><code>iv_fit</code></dt><dd><p>Object of class <code>ivreg</code> from the IV
regression of <code class="reqn">Y - \hat{E}[Y\vert X]</code> on
<code class="reqn">D - \hat{E}[D\vert X]</code> using
<code class="reqn">\hat{E}[D\vert X,Z] - \hat{E}[D\vert X]</code> as the instrument.</p>
</dd>
<dt><code>learners</code>,<code>learners_DX</code>,<code>learners_DXZ</code>,
<code>subsamples</code>,<code>cv_subsamples_list</code>,<code>ensemble_type</code>
</dt><dd><p>Pass-through of selected user-provided arguments. See above.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ahrens A, Hansen C B, Schaffer M E, Wiemann T (2023). &quot;ddml: Double/debiased
machine learning in Stata.&quot; <a href="https://arxiv.org/abs/2301.09397">https://arxiv.org/abs/2301.09397</a>
</p>
<p>Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W,
Robins J (2018). &quot;Double/debiased machine learning for treatment and
structural parameters.&quot; The Econometrics Journal, 21(1), C1-C68.
</p>
<p>Wolpert D H (1992). &quot;Stacked generalization.&quot; Neural Networks, 5(2), 241-259.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.ddml_fpliv">summary.ddml_fpliv()</a></code>, <code><a href="AER.html#topic+ivreg">AER::ivreg()</a></code>
</p>
<p>Other ddml: 
<code><a href="#topic+ddml_ate">ddml_ate</a>()</code>,
<code><a href="#topic+ddml_late">ddml_late</a>()</code>,
<code><a href="#topic+ddml_pliv">ddml_pliv</a>()</code>,
<code><a href="#topic+ddml_plm">ddml_plm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
D = AE98[, "morekids"]
Z = AE98[, "samesex", drop = FALSE]
X = AE98[, c("age","agefst","black","hisp","othrace","educ")]

# Estimate the partially linear IV model using a single base learner: Ridge.
fpliv_fit &lt;- ddml_fpliv(y, D, Z, X,
                        learners = list(what = mdl_glmnet,
                                        args = list(alpha = 0)),
                        sample_folds = 2,
                        silent = TRUE)
summary(fpliv_fit)
</code></pre>

<hr>
<h2 id='ddml_late'>Estimator of the Local Average Treatment Effect.</h2><span id='topic+ddml_late'></span>

<h3>Description</h3>

<p>Estimator of the local average treatment effect.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddml_late(
  y,
  D,
  Z,
  X,
  learners,
  learners_DXZ = learners,
  learners_ZX = learners,
  sample_folds = 2,
  ensemble_type = "nnls",
  shortstack = FALSE,
  cv_folds = 5,
  custom_ensemble_weights = NULL,
  custom_ensemble_weights_DXZ = custom_ensemble_weights,
  custom_ensemble_weights_ZX = custom_ensemble_weights,
  subsamples_Z0 = NULL,
  subsamples_Z1 = NULL,
  cv_subsamples_list_Z0 = NULL,
  cv_subsamples_list_Z1 = NULL,
  trim = 0.01,
  silent = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddml_late_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_d">D</code></td>
<td>
<p>The binary endogenous variable of interest.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_z">Z</code></td>
<td>
<p>Binary instrumental variable.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_x">X</code></td>
<td>
<p>A (sparse) matrix of control variables.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_learners">learners</code></td>
<td>
<p>May take one of two forms, depending on whether a single
learner or stacking with multiple learners is used for estimation of the
conditional expectation functions.
If a single learner is used, <code>learners</code> is a list with two named
elements:
</p>

<ul>
<li><p><code>what</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>what</code>.
</p>
</li></ul>

<p>If stacking with multiple learners is used, <code>learners</code> is a list of
lists, each containing four named elements:
</p>

<ul>
<li><p><code>fun</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>fun</code>.
</p>
</li>
<li><p><code>assign_X</code> An optional vector of column indices
corresponding to control variables in <code>X</code> that are passed to
the base learner.
</p>
</li>
<li><p><code>assign_Z</code> An optional vector of column indices
corresponding to instruments in <code>Z</code> that are passed to the
base learner.
</p>
</li></ul>

<p>Omission of the <code>args</code> element results in default arguments being
used in <code>fun</code>. Omission of <code>assign_X</code> (and/or <code>assign_Z</code>)
results in inclusion of all variables in <code>X</code> (and/or <code>Z</code>).</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_learners_dxz">learners_DXZ</code>, <code id="ddml_late_+3A_learners_zx">learners_ZX</code></td>
<td>
<p>Optional arguments to allow for different
estimators of <code class="reqn">E[D \vert X, Z]</code>, <code class="reqn">E[Z \vert X]</code>. Setup is
identical to <code>learners</code>.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_sample_folds">sample_folds</code></td>
<td>
<p>Number of cross-fitting folds.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_ensemble_type">ensemble_type</code></td>
<td>
<p>Ensemble method to combine base learners into final
estimate of the conditional expectation functions. Possible values are:
</p>

<ul>
<li><p><code>"nnls"</code> Non-negative least squares.
</p>
</li>
<li><p><code>"nnls1"</code> Non-negative least squares with the constraint
that all weights sum to one.
</p>
</li>
<li><p><code>"singlebest"</code> Select base learner with minimum MSPE.
</p>
</li>
<li><p><code>"ols"</code> Ordinary least squares.
</p>
</li>
<li><p><code>"average"</code> Simple average over base learners.
</p>
</li></ul>

<p>Multiple ensemble types may be passed as a vector of strings.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_shortstack">shortstack</code></td>
<td>
<p>Boolean to use short-stacking.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_cv_folds">cv_folds</code></td>
<td>
<p>Number of folds used for cross-validation in ensemble
construction.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_custom_ensemble_weights">custom_ensemble_weights</code></td>
<td>
<p>A numerical matrix with user-specified
ensemble weights. Each column corresponds to a custom ensemble
specification, each row corresponds to a base learner in <code>learners</code>
(in chronological order). Optional column names are used to name the
estimation results corresponding the custom ensemble specification.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_custom_ensemble_weights_dxz">custom_ensemble_weights_DXZ</code>, <code id="ddml_late_+3A_custom_ensemble_weights_zx">custom_ensemble_weights_ZX</code></td>
<td>
<p>Optional
arguments to allow for different
custom ensemble weights for <code>learners_DXZ</code>,<code>learners_ZX</code>. Setup
is identical to <code>custom_ensemble_weights</code>. Note:
<code>custom_ensemble_weights</code> and
<code>custom_ensemble_weights_DXZ</code>,<code>custom_ensemble_weights_ZX</code> must
have the same number of columns.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_subsamples_z0">subsamples_Z0</code>, <code id="ddml_late_+3A_subsamples_z1">subsamples_Z1</code></td>
<td>
<p>List of vectors with sample indices for
cross-fitting, corresponding to observations with <code class="reqn">Z=0</code> and
<code class="reqn">Z=1</code>, respectively.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_cv_subsamples_list_z0">cv_subsamples_list_Z0</code>, <code id="ddml_late_+3A_cv_subsamples_list_z1">cv_subsamples_list_Z1</code></td>
<td>
<p>List of lists, each
corresponding to a subsample containing vectors with subsample indices
for cross-validation. Arguments are separated for observations with
<code class="reqn">Z=0</code> and <code class="reqn">Z=1</code>, respectively.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_trim">trim</code></td>
<td>
<p>Number in (0, 1) for trimming the estimated propensity scores at
<code>trim</code> and <code>1-trim</code>.</p>
</td></tr>
<tr><td><code id="ddml_late_+3A_silent">silent</code></td>
<td>
<p>Boolean to silence estimation updates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ddml_late</code> provides a double/debiased machine learning
estimator for the local average treatment effect in the interactive model
given by
</p>
<p><code class="reqn">Y = g_0(D, X) + U,</code>
</p>
<p>where <code class="reqn">(Y, D, X, Z, U)</code> is a random vector such that
<code class="reqn">\operatorname{supp} D = \operatorname{supp} Z = \{0,1\}</code>,
<code class="reqn">E[U\vert X, Z] = 0</code>, <code class="reqn">E[Var(E[D\vert X, Z]\vert X)] \neq 0</code>,
<code class="reqn">\Pr(Z=1\vert X) \in (0, 1)</code> with probability 1,
<code class="reqn">p_0(1, X) \geq p_0(0, X)</code> with probability 1 where
<code class="reqn">p_0(Z, X) \equiv \Pr(D=1\vert Z, X)</code>, and
<code class="reqn">g_0</code> is an unknown nuisance function.
</p>
<p>In this model, the local average treatment effect is defined as
</p>
<p><code class="reqn">\theta_0^{\textrm{LATE}} \equiv
    E[g_0(1, X) - g_0(0, X)\vert p_0(1, X) &gt; p(0, X)]</code>.
</p>


<h3>Value</h3>

<p><code>ddml_late</code> returns an object of S3 class
<code>ddml_late</code>. An object of class <code>ddml_late</code> is a list
containing the following components:
</p>

<dl>
<dt><code>late</code></dt><dd><p>A vector with the average treatment effect
estimates.</p>
</dd>
<dt><code>weights</code></dt><dd><p>A list of matrices, providing the weight
assigned to each base learner (in chronological order) by the
ensemble procedure.</p>
</dd>
<dt><code>mspe</code></dt><dd><p>A list of matrices, providing the MSPE of each
base learner (in chronological order) computed by the
cross-validation step in the ensemble construction.</p>
</dd>
<dt><code>psi_a</code>, <code>psi_b</code></dt><dd><p>Matrices needed for the computation
of scores. Used in <code><a href="#topic+summary.ddml_late">summary.ddml_late()</a></code>.</p>
</dd>
<dt><code>oos_pred</code></dt><dd><p>List of matrices, providing the reduced form
predicted values.</p>
</dd>
<dt><code>learners</code>,<code>learners_DXZ</code>,<code>learners_ZX</code>,
<code>subsamples_Z0</code>,<code>subsamples_Z1</code>,
<code>cv_subsamples_list_Z0</code>,<code>cv_subsamples_list_Z1</code>,
<code>ensemble_type</code></dt><dd><p>Pass-through of
selected user-provided arguments. See above.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ahrens A, Hansen C B, Schaffer M E, Wiemann T (2023). &quot;ddml: Double/debiased
machine learning in Stata.&quot; <a href="https://arxiv.org/abs/2301.09397">https://arxiv.org/abs/2301.09397</a>
</p>
<p>Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W,
Robins J (2018). &quot;Double/debiased machine learning for treatment and
structural parameters.&quot; The Econometrics Journal, 21(1), C1-C68.
</p>
<p>Imbens G, Angrist J (1004). &quot;Identification and Estimation of Local Average
Treatment Effects.&quot; Econometrica, 62(2), 467-475.
</p>
<p>Wolpert D H (1992). &quot;Stacked generalization.&quot; Neural Networks, 5(2), 241-259.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.ddml_late">summary.ddml_late()</a></code>
</p>
<p>Other ddml: 
<code><a href="#topic+ddml_ate">ddml_ate</a>()</code>,
<code><a href="#topic+ddml_fpliv">ddml_fpliv</a>()</code>,
<code><a href="#topic+ddml_pliv">ddml_pliv</a>()</code>,
<code><a href="#topic+ddml_plm">ddml_plm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
D = AE98[, "morekids"]
Z = AE98[, "samesex"]
X = AE98[, c("age","agefst","black","hisp","othrace","educ")]

# Estimate the local average treatment effect using a single base learner,
#     ridge.
late_fit &lt;- ddml_late(y, D, Z, X,
                      learners = list(what = mdl_glmnet,
                                      args = list(alpha = 0)),
                      sample_folds = 2,
                      silent = TRUE)
summary(late_fit)

# Estimate the local average treatment effect using short-stacking with base
#     learners ols, lasso, and ridge. We can also use custom_ensemble_weights
#     to estimate the ATE using every individual base learner.
weights_everylearner &lt;- diag(1, 3)
colnames(weights_everylearner) &lt;- c("mdl:ols", "mdl:lasso", "mdl:ridge")
late_fit &lt;- ddml_late(y, D, Z, X,
                      learners = list(list(fun = ols),
                                      list(fun = mdl_glmnet),
                                      list(fun = mdl_glmnet,
                                           args = list(alpha = 0))),
                      ensemble_type = 'nnls',
                      custom_ensemble_weights = weights_everylearner,
                      shortstack = TRUE,
                      sample_folds = 2,
                      silent = TRUE)
summary(late_fit)
</code></pre>

<hr>
<h2 id='ddml_pliv'>Estimator for the Partially Linear IV Model.</h2><span id='topic+ddml_pliv'></span>

<h3>Description</h3>

<p>Estimator for the partially linear IV model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddml_pliv(
  y,
  D,
  Z,
  X,
  learners,
  learners_DX = learners,
  learners_ZX = learners,
  sample_folds = 2,
  ensemble_type = "nnls",
  shortstack = FALSE,
  cv_folds = 5,
  custom_ensemble_weights = NULL,
  custom_ensemble_weights_DX = custom_ensemble_weights,
  custom_ensemble_weights_ZX = custom_ensemble_weights,
  subsamples = NULL,
  cv_subsamples_list = NULL,
  silent = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddml_pliv_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_d">D</code></td>
<td>
<p>A matrix of endogenous variables.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_z">Z</code></td>
<td>
<p>A matrix of instruments.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_x">X</code></td>
<td>
<p>A (sparse) matrix of control variables.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_learners">learners</code></td>
<td>
<p>May take one of two forms, depending on whether a single
learner or stacking with multiple learners is used for estimation of the
conditional expectation functions.
If a single learner is used, <code>learners</code> is a list with two named
elements:
</p>

<ul>
<li><p><code>what</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>what</code>.
</p>
</li></ul>

<p>If stacking with multiple learners is used, <code>learners</code> is a list of
lists, each containing four named elements:
</p>

<ul>
<li><p><code>fun</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>fun</code>.
</p>
</li>
<li><p><code>assign_X</code> An optional vector of column indices
corresponding to control variables in <code>X</code> that are passed to
the base learner.
</p>
</li>
<li><p><code>assign_Z</code> An optional vector of column indices
corresponding to instruments in <code>Z</code> that are passed to the
base learner.
</p>
</li></ul>

<p>Omission of the <code>args</code> element results in default arguments being
used in <code>fun</code>. Omission of <code>assign_X</code> (and/or <code>assign_Z</code>)
results in inclusion of all variables in <code>X</code> (and/or <code>Z</code>).</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_learners_dx">learners_DX</code>, <code id="ddml_pliv_+3A_learners_zx">learners_ZX</code></td>
<td>
<p>Optional arguments to allow for different
base learners for estimation of <code class="reqn">E[D|X]</code>, <code class="reqn">E[Z|X]</code>. Setup is
identical to <code>learners</code>.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_sample_folds">sample_folds</code></td>
<td>
<p>Number of cross-fitting folds.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_ensemble_type">ensemble_type</code></td>
<td>
<p>Ensemble method to combine base learners into final
estimate of the conditional expectation functions. Possible values are:
</p>

<ul>
<li><p><code>"nnls"</code> Non-negative least squares.
</p>
</li>
<li><p><code>"nnls1"</code> Non-negative least squares with the constraint
that all weights sum to one.
</p>
</li>
<li><p><code>"singlebest"</code> Select base learner with minimum MSPE.
</p>
</li>
<li><p><code>"ols"</code> Ordinary least squares.
</p>
</li>
<li><p><code>"average"</code> Simple average over base learners.
</p>
</li></ul>

<p>Multiple ensemble types may be passed as a vector of strings.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_shortstack">shortstack</code></td>
<td>
<p>Boolean to use short-stacking.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_cv_folds">cv_folds</code></td>
<td>
<p>Number of folds used for cross-validation in ensemble
construction.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_custom_ensemble_weights">custom_ensemble_weights</code></td>
<td>
<p>A numerical matrix with user-specified
ensemble weights. Each column corresponds to a custom ensemble
specification, each row corresponds to a base learner in <code>learners</code>
(in chronological order). Optional column names are used to name the
estimation results corresponding the custom ensemble specification.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_custom_ensemble_weights_dx">custom_ensemble_weights_DX</code>, <code id="ddml_pliv_+3A_custom_ensemble_weights_zx">custom_ensemble_weights_ZX</code></td>
<td>
<p>Optional
arguments to allow for different
custom ensemble weights for <code>learners_DX</code>,<code>learners_ZX</code>. Setup
is identical to <code>custom_ensemble_weights</code>. Note:
<code>custom_ensemble_weights</code> and
<code>custom_ensemble_weights_DX</code>,<code>custom_ensemble_weights_ZX</code> must
have the same number of columns.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_subsamples">subsamples</code></td>
<td>
<p>List of vectors with sample indices for cross-fitting.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_cv_subsamples_list">cv_subsamples_list</code></td>
<td>
<p>List of lists, each corresponding to a subsample
containing vectors with subsample indices for cross-validation.</p>
</td></tr>
<tr><td><code id="ddml_pliv_+3A_silent">silent</code></td>
<td>
<p>Boolean to silence estimation updates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ddml_pliv</code> provides a double/debiased machine learning
estimator for the parameter of interest <code class="reqn">\theta_0</code> in the partially
linear IV model given by
</p>
<p><code class="reqn">Y = \theta_0D + g_0(X) + U,</code>
</p>
<p>where <code class="reqn">(Y, D, X, Z, U)</code> is a random vector such that
<code class="reqn">E[Cov(U, Z\vert X)] = 0</code> and <code class="reqn">E[Cov(D, Z\vert X)] \neq 0</code>, and
<code class="reqn">g_0</code> is an unknown nuisance function.
</p>


<h3>Value</h3>

<p><code>ddml_pliv</code> returns an object of S3 class
<code>ddml_pliv</code>. An object of class <code>ddml_pliv</code> is a list
containing the following components:
</p>

<dl>
<dt><code>coef</code></dt><dd><p>A vector with the <code class="reqn">\theta_0</code> estimates.</p>
</dd>
<dt><code>weights</code></dt><dd><p>A list of matrices, providing the weight
assigned to each base learner (in chronological order) by the
ensemble procedure.</p>
</dd>
<dt><code>mspe</code></dt><dd><p>A list of matrices, providing the MSPE of each
base learner (in chronological order) computed by the
cross-validation step in the ensemble construction.</p>
</dd>
<dt><code>iv_fit</code></dt><dd><p>Object of class <code>ivreg</code> from the IV
regression of <code class="reqn">Y - \hat{E}[Y\vert X]</code> on
<code class="reqn">D - \hat{E}[D\vert X]</code> using <code class="reqn">Z - \hat{E}[Z\vert X]</code> as
the instrument. See also <code><a href="AER.html#topic+ivreg">AER::ivreg()</a></code> for details.</p>
</dd>
<dt><code>learners</code>,<code>learners_DX</code>,<code>learners_ZX</code>,
<code>subsamples</code>,<code>cv_subsamples_list</code>,<code>ensemble_type</code>
</dt><dd><p>Pass-through of selected user-provided arguments. See above.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ahrens A, Hansen C B, Schaffer M E, Wiemann T (2023). &quot;ddml: Double/debiased
machine learning in Stata.&quot; <a href="https://arxiv.org/abs/2301.09397">https://arxiv.org/abs/2301.09397</a>
</p>
<p>Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W,
Robins J (2018). &quot;Double/debiased machine learning for treatment and
structural parameters.&quot; The Econometrics Journal, 21(1), C1-C68.
</p>
<p>Kleiber C, Zeileis A (2008). Applied Econometrics with R. Springer-Verlag,
New York.
</p>
<p>Wolpert D H (1992). &quot;Stacked generalization.&quot; Neural Networks, 5(2), 241-259.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.ddml_pliv">summary.ddml_pliv()</a></code>, <code><a href="AER.html#topic+ivreg">AER::ivreg()</a></code>
</p>
<p>Other ddml: 
<code><a href="#topic+ddml_ate">ddml_ate</a>()</code>,
<code><a href="#topic+ddml_fpliv">ddml_fpliv</a>()</code>,
<code><a href="#topic+ddml_late">ddml_late</a>()</code>,
<code><a href="#topic+ddml_plm">ddml_plm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
D = AE98[, "morekids"]
Z = AE98[, "samesex"]
X = AE98[, c("age","agefst","black","hisp","othrace","educ")]

# Estimate the partially linear IV model using a single base learner, ridge.
pliv_fit &lt;- ddml_pliv(y, D, Z, X,
                      learners = list(what = mdl_glmnet,
                                      args = list(alpha = 0)),
                      sample_folds = 2,
                      silent = TRUE)
summary(pliv_fit)
</code></pre>

<hr>
<h2 id='ddml_plm'>Estimator for the Partially Linear Model.</h2><span id='topic+ddml_plm'></span>

<h3>Description</h3>

<p>Estimator for the partially linear model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddml_plm(
  y,
  D,
  X,
  learners,
  learners_DX = learners,
  sample_folds = 2,
  ensemble_type = "nnls",
  shortstack = FALSE,
  cv_folds = 5,
  custom_ensemble_weights = NULL,
  custom_ensemble_weights_DX = custom_ensemble_weights,
  subsamples = NULL,
  cv_subsamples_list = NULL,
  silent = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddml_plm_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_d">D</code></td>
<td>
<p>A matrix of endogenous variables.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_x">X</code></td>
<td>
<p>A (sparse) matrix of control variables.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_learners">learners</code></td>
<td>
<p>May take one of two forms, depending on whether a single
learner or stacking with multiple learners is used for estimation of the
conditional expectation functions.
If a single learner is used, <code>learners</code> is a list with two named
elements:
</p>

<ul>
<li><p><code>what</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>what</code>.
</p>
</li></ul>

<p>If stacking with multiple learners is used, <code>learners</code> is a list of
lists, each containing four named elements:
</p>

<ul>
<li><p><code>fun</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>fun</code>.
</p>
</li>
<li><p><code>assign_X</code> An optional vector of column indices
corresponding to control variables in <code>X</code> that are passed to
the base learner.
</p>
</li></ul>

<p>Omission of the <code>args</code> element results in default arguments being
used in <code>fun</code>. Omission of <code>assign_X</code> results in inclusion of
all variables in <code>X</code>.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_learners_dx">learners_DX</code></td>
<td>
<p>Optional argument to allow for different estimators of
<code class="reqn">E[D|X]</code>. Setup is identical to <code>learners</code>.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_sample_folds">sample_folds</code></td>
<td>
<p>Number of cross-fitting folds.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_ensemble_type">ensemble_type</code></td>
<td>
<p>Ensemble method to combine base learners into final
estimate of the conditional expectation functions. Possible values are:
</p>

<ul>
<li><p><code>"nnls"</code> Non-negative least squares.
</p>
</li>
<li><p><code>"nnls1"</code> Non-negative least squares with the constraint
that all weights sum to one.
</p>
</li>
<li><p><code>"singlebest"</code> Select base learner with minimum MSPE.
</p>
</li>
<li><p><code>"ols"</code> Ordinary least squares.
</p>
</li>
<li><p><code>"average"</code> Simple average over base learners.
</p>
</li></ul>

<p>Multiple ensemble types may be passed as a vector of strings.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_shortstack">shortstack</code></td>
<td>
<p>Boolean to use short-stacking.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_cv_folds">cv_folds</code></td>
<td>
<p>Number of folds used for cross-validation in ensemble
construction.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_custom_ensemble_weights">custom_ensemble_weights</code></td>
<td>
<p>A numerical matrix with user-specified
ensemble weights. Each column corresponds to a custom ensemble
specification, each row corresponds to a base learner in <code>learners</code>
(in chronological order). Optional column names are used to name the
estimation results corresponding the custom ensemble specification.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_custom_ensemble_weights_dx">custom_ensemble_weights_DX</code></td>
<td>
<p>Optional argument to allow for different
custom ensemble weights for <code>learners_DX</code>. Setup is identical to
<code>custom_ensemble_weights</code>. Note: <code>custom_ensemble_weights</code> and
<code>custom_ensemble_weights_DX</code> must have the same number of columns.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_subsamples">subsamples</code></td>
<td>
<p>List of vectors with sample indices for cross-fitting.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_cv_subsamples_list">cv_subsamples_list</code></td>
<td>
<p>List of lists, each corresponding to a subsample
containing vectors with subsample indices for cross-validation.</p>
</td></tr>
<tr><td><code id="ddml_plm_+3A_silent">silent</code></td>
<td>
<p>Boolean to silence estimation updates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ddml_plm</code> provides a double/debiased machine learning
estimator for the parameter of interest <code class="reqn">\theta_0</code> in the partially
linear model given by
</p>
<p><code class="reqn">Y = \theta_0D + g_0(X) + U,</code>
</p>
<p>where <code class="reqn">(Y, D, X, U)</code> is a random vector such that
<code class="reqn">E[Cov(U, D\vert X)] = 0</code> and <code class="reqn">E[Var(D\vert X)] \neq 0</code>, and
<code class="reqn">g_0</code> is an unknown nuisance function.
</p>


<h3>Value</h3>

<p><code>ddml_plm</code> returns an object of S3 class
<code>ddml_plm</code>. An object of class <code>ddml_plm</code> is a list containing
the following components:
</p>

<dl>
<dt><code>coef</code></dt><dd><p>A vector with the <code class="reqn">\theta_0</code> estimates.</p>
</dd>
<dt><code>weights</code></dt><dd><p>A list of matrices, providing the weight
assigned to each base learner (in chronological order) by the
ensemble procedure.</p>
</dd>
<dt><code>mspe</code></dt><dd><p>A list of matrices, providing the MSPE of each
base learner (in chronological order) computed by the
cross-validation step in the ensemble construction.</p>
</dd>
<dt><code>ols_fit</code></dt><dd><p>Object of class <code>lm</code> from the second
stage regression of <code class="reqn">Y - \hat{E}[Y|X]</code> on
<code class="reqn">D - \hat{E}[D|X]</code>.</p>
</dd>
<dt><code>learners</code>,<code>learners_DX</code>,<code>subsamples</code>,
<code>cv_subsamples_list</code>,<code>ensemble_type</code></dt><dd><p>Pass-through of
selected user-provided arguments. See above.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ahrens A, Hansen C B, Schaffer M E, Wiemann T (2023). &quot;ddml: Double/debiased
machine learning in Stata.&quot; <a href="https://arxiv.org/abs/2301.09397">https://arxiv.org/abs/2301.09397</a>
</p>
<p>Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C B, Newey W,
Robins J (2018). &quot;Double/debiased machine learning for treatment and
structural parameters.&quot; The Econometrics Journal, 21(1), C1-C68.
</p>
<p>Wolpert D H (1992). &quot;Stacked generalization.&quot; Neural Networks, 5(2), 241-259.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.ddml_plm">summary.ddml_plm()</a></code>
</p>
<p>Other ddml: 
<code><a href="#topic+ddml_ate">ddml_ate</a>()</code>,
<code><a href="#topic+ddml_fpliv">ddml_fpliv</a>()</code>,
<code><a href="#topic+ddml_late">ddml_late</a>()</code>,
<code><a href="#topic+ddml_pliv">ddml_pliv</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
D = AE98[, "morekids"]
X = AE98[, c("age","agefst","black","hisp","othrace","educ")]

# Estimate the partially linear model using a single base learner, ridge.
plm_fit &lt;- ddml_plm(y, D, X,
                    learners = list(what = mdl_glmnet,
                                    args = list(alpha = 0)),
                    sample_folds = 2,
                    silent = TRUE)
summary(plm_fit)

# Estimate the partially linear model using short-stacking with base learners
#     ols, lasso, and ridge. We can also use custom_ensemble_weights
#     to estimate the ATE using every individual base learner.
weights_everylearner &lt;- diag(1, 3)
colnames(weights_everylearner) &lt;- c("mdl:ols", "mdl:lasso", "mdl:ridge")
plm_fit &lt;- ddml_plm(y, D, X,
                    learners = list(list(fun = ols),
                                    list(fun = mdl_glmnet),
                                    list(fun = mdl_glmnet,
                                         args = list(alpha = 0))),
                    ensemble_type = 'nnls',
                    custom_ensemble_weights = weights_everylearner,
                    shortstack = TRUE,
                    sample_folds = 2,
                    silent = TRUE)
summary(plm_fit)
</code></pre>

<hr>
<h2 id='mdl_glm'>Wrapper for <code><a href="stats.html#topic+glm">stats::glm()</a></code>.</h2><span id='topic+mdl_glm'></span>

<h3>Description</h3>

<p>Simple wrapper for <code><a href="stats.html#topic+glm">stats::glm()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdl_glm(y, X, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mdl_glm_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="mdl_glm_+3A_x">X</code></td>
<td>
<p>The feature matrix.</p>
</td></tr>
<tr><td><code id="mdl_glm_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>glm</code>. See
<code><a href="stats.html#topic+glm">stats::glm()</a></code> for a complete list of arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mdl_glm</code> returns an object of S3 class <code>mdl_glm</code> as a
simple mask of the return object of <code><a href="stats.html#topic+glm">stats::glm()</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+glm">stats::glm()</a></code>
</p>
<p>Other ml_wrapper: 
<code><a href="#topic+mdl_glmnet">mdl_glmnet</a>()</code>,
<code><a href="#topic+mdl_ranger">mdl_ranger</a>()</code>,
<code><a href="#topic+mdl_xgboost">mdl_xgboost</a>()</code>,
<code><a href="#topic+ols">ols</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>glm_fit &lt;- mdl_glm(sample(0:1, 100, replace = TRUE),
                   matrix(rnorm(1000), 100, 10))
class(glm_fit)
</code></pre>

<hr>
<h2 id='mdl_glmnet'>Wrapper for <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code>.</h2><span id='topic+mdl_glmnet'></span>

<h3>Description</h3>

<p>Simple wrapper for <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> and <code><a href="glmnet.html#topic+cv.glmnet">glmnet::cv.glmnet()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdl_glmnet(y, X, cv = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mdl_glmnet_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="mdl_glmnet_+3A_x">X</code></td>
<td>
<p>The (sparse) feature matrix.</p>
</td></tr>
<tr><td><code id="mdl_glmnet_+3A_cv">cv</code></td>
<td>
<p>Boolean to indicate use of lasso with cross-validated penalty.</p>
</td></tr>
<tr><td><code id="mdl_glmnet_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>glmnet</code>. See
<code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> and <code><a href="glmnet.html#topic+cv.glmnet">glmnet::cv.glmnet()</a></code> for a complete list of
arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mdl_glmnet</code> returns an object of S3 class <code>mdl_glmnet</code> as
a simple mask of the return object of <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> or
<code><a href="glmnet.html#topic+cv.glmnet">glmnet::cv.glmnet()</a></code>.
</p>


<h3>References</h3>

<p>Friedman J, Hastie T, Tibshirani R (2010). &quot;Regularization Paths for
Generalized Linear Models via Coordinate Descent.&quot; Journal of Statistical
Software, 33(1), 122.
</p>
<p>Simon N, Friedman J, Hastie T, Tibshirani R (2011). &quot;Regularization Paths for
Cox's Proportional Hazards Model via Coordinate Descent.&quot; Journal of
Statistical Software, 39(5), 113.
</p>


<h3>See Also</h3>

<p><code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code>,<code><a href="glmnet.html#topic+cv.glmnet">glmnet::cv.glmnet()</a></code>
</p>
<p>Other ml_wrapper: 
<code><a href="#topic+mdl_glm">mdl_glm</a>()</code>,
<code><a href="#topic+mdl_ranger">mdl_ranger</a>()</code>,
<code><a href="#topic+mdl_xgboost">mdl_xgboost</a>()</code>,
<code><a href="#topic+ols">ols</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>glmnet_fit &lt;- mdl_glmnet(rnorm(100), matrix(rnorm(1000), 100, 10))
class(glmnet_fit)
</code></pre>

<hr>
<h2 id='mdl_ranger'>Wrapper for <code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code>.</h2><span id='topic+mdl_ranger'></span>

<h3>Description</h3>

<p>Simple wrapper for <code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code>. Supports regression
(default) and probability forests (set <code>probability = TRUE</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdl_ranger(y, X, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mdl_ranger_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="mdl_ranger_+3A_x">X</code></td>
<td>
<p>The feature matrix.</p>
</td></tr>
<tr><td><code id="mdl_ranger_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>ranger</code>. See
<code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code> for a complete list of arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mdl_ranger</code> returns an object of S3 class <code>ranger</code> as a
simple mask of the return object of <code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code>.
</p>


<h3>References</h3>

<p>Wright M N, Ziegler A (2017). &quot;ranger: A fast implementation of random
forests for high dimensional data in C++ and R.&quot; Journal of Statistical
Software 77(1), 1-17.
</p>


<h3>See Also</h3>

<p><code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code>
</p>
<p>Other ml_wrapper: 
<code><a href="#topic+mdl_glmnet">mdl_glmnet</a>()</code>,
<code><a href="#topic+mdl_glm">mdl_glm</a>()</code>,
<code><a href="#topic+mdl_xgboost">mdl_xgboost</a>()</code>,
<code><a href="#topic+ols">ols</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ranger_fit &lt;- mdl_ranger(rnorm(100), matrix(rnorm(1000), 100, 10))
class(ranger_fit)
</code></pre>

<hr>
<h2 id='mdl_xgboost'>Wrapper for <code><a href="xgboost.html#topic+xgb.train">xgboost::xgboost()</a></code>.</h2><span id='topic+mdl_xgboost'></span>

<h3>Description</h3>

<p>Simple wrapper for <code><a href="xgboost.html#topic+xgb.train">xgboost::xgboost()</a></code> with some changes to the
default arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdl_xgboost(y, X, nrounds = 500, verbose = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mdl_xgboost_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="mdl_xgboost_+3A_x">X</code></td>
<td>
<p>The (sparse) feature matrix.</p>
</td></tr>
<tr><td><code id="mdl_xgboost_+3A_nrounds">nrounds</code></td>
<td>
<p>max number of boosting iterations.</p>
</td></tr>
<tr><td><code id="mdl_xgboost_+3A_verbose">verbose</code></td>
<td>
<p>If 0, xgboost will stay silent. If 1, it will print information about performance.
If 2, some additional information will be printed out.
Note that setting <code>verbose &gt; 0</code> automatically engages the
<code>cb.print.evaluation(period=1)</code> callback function.</p>
</td></tr>
<tr><td><code id="mdl_xgboost_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>xgboost</code>. See
<code><a href="xgboost.html#topic+xgb.train">xgboost::xgboost()</a></code> for a complete list of arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mdl_xgboost</code> returns an object of S3 class <code>mdl_xgboost</code>
as a simple mask to the return object of <code><a href="xgboost.html#topic+xgb.train">xgboost::xgboost()</a></code>.
</p>


<h3>References</h3>

<p>Chen T, Guestrin C (2011). &quot;Xgboost: A Scalable Tree Boosting System.&quot;
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, 785794.
</p>


<h3>See Also</h3>

<p><code><a href="xgboost.html#topic+xgb.train">xgboost::xgboost()</a></code>
</p>
<p>Other ml_wrapper: 
<code><a href="#topic+mdl_glmnet">mdl_glmnet</a>()</code>,
<code><a href="#topic+mdl_glm">mdl_glm</a>()</code>,
<code><a href="#topic+mdl_ranger">mdl_ranger</a>()</code>,
<code><a href="#topic+ols">ols</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>xgboost_fit &lt;- mdl_xgboost(rnorm(50), matrix(rnorm(150), 50, 3),
                           nrounds = 1)
class(xgboost_fit)
</code></pre>

<hr>
<h2 id='ols'>Ordinary least squares.</h2><span id='topic+ols'></span>

<h3>Description</h3>

<p>Simple implementation of ordinary least squares that computes
with sparse feature matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols(y, X, const = TRUE, w = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ols_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="ols_+3A_x">X</code></td>
<td>
<p>The feature matrix.</p>
</td></tr>
<tr><td><code id="ols_+3A_const">const</code></td>
<td>
<p>Boolean equal to <code>TRUE</code> if a constant should be included.
The default is <code>FALSE</code></p>
</td></tr>
<tr><td><code id="ols_+3A_w">w</code></td>
<td>
<p>A vector of weights for weighted least squares.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ols</code> returns an object of S3 class
<code>ols</code>. An object of class <code>ols</code> is a list containing
the following components:
</p>

<dl>
<dt><code>coef</code></dt><dd><p>A vector with the regression coefficents.</p>
</dd>
<dt><code>y</code>, <code>X</code>, <code>const</code>, <code>w</code></dt><dd><p>Pass-through of the
user-provided arguments. See above.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Other ml_wrapper: 
<code><a href="#topic+mdl_glmnet">mdl_glmnet</a>()</code>,
<code><a href="#topic+mdl_glm">mdl_glm</a>()</code>,
<code><a href="#topic+mdl_ranger">mdl_ranger</a>()</code>,
<code><a href="#topic+mdl_xgboost">mdl_xgboost</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ols_fit &lt;- ols(rnorm(100), cbind(rnorm(100), rnorm(100)), const = TRUE)
ols_fit$coef
</code></pre>

<hr>
<h2 id='print.summary.ddml_ate'>Print Methods for Treatment Effect Estimators.</h2><span id='topic+print.summary.ddml_ate'></span><span id='topic+print.summary.ddml_att'></span><span id='topic+print.summary.ddml_late'></span>

<h3>Description</h3>

<p>Inference methods for treatment effect estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.ddml_ate'
print(x, digits = 3, ...)

## S3 method for class 'summary.ddml_att'
print(x, digits = 3, ...)

## S3 method for class 'summary.ddml_late'
print(x, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.ddml_ate_+3A_x">x</code></td>
<td>
<p>An object of class <code>summary.ddml_ate</code>,
<code>summary.ddml_att</code>, and <code>ddml_late</code>, as returned by
<code><a href="#topic+summary.ddml_ate">summary.ddml_ate()</a></code>, <code><a href="#topic+summary.ddml_att">summary.ddml_att()</a></code>, and
<code><a href="#topic+summary.ddml_late">summary.ddml_late()</a></code>, respectively.</p>
</td></tr>
<tr><td><code id="print.summary.ddml_ate_+3A_digits">digits</code></td>
<td>
<p>The number of significant digits used for printing.</p>
</td></tr>
<tr><td><code id="print.summary.ddml_ate_+3A_...">...</code></td>
<td>
<p>Currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
D = AE98[, "morekids"]
X = AE98[, c("age","agefst","black","hisp","othrace","educ")]

# Estimate the average treatment effect using a single base learner, ridge.
ate_fit &lt;- ddml_ate(y, D, X,
                    learners = list(what = mdl_glmnet,
                                    args = list(alpha = 0)),
                    sample_folds = 2,
                    silent = TRUE)
summary(ate_fit)
</code></pre>

<hr>
<h2 id='print.summary.ddml_fpliv'>Print Methods for Treatment Effect Estimators.</h2><span id='topic+print.summary.ddml_fpliv'></span><span id='topic+print.summary.ddml_pliv'></span><span id='topic+print.summary.ddml_plm'></span>

<h3>Description</h3>

<p>Inference methods for treatment effect estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.ddml_fpliv'
print(x, digits = 3, ...)

## S3 method for class 'summary.ddml_pliv'
print(x, digits = 3, ...)

## S3 method for class 'summary.ddml_plm'
print(x, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.ddml_fpliv_+3A_x">x</code></td>
<td>
<p>An object of class <code>summary.ddml_plm</code>,
<code>summary.ddml_pliv</code>, and <code>summary.ddml_fpliv</code>, as
returned by <code><a href="#topic+summary.ddml_plm">summary.ddml_plm()</a></code>, <code><a href="#topic+summary.ddml_pliv">summary.ddml_pliv()</a></code>,
and <code><a href="#topic+summary.ddml_fpliv">summary.ddml_fpliv()</a></code>, respectively.</p>
</td></tr>
<tr><td><code id="print.summary.ddml_fpliv_+3A_digits">digits</code></td>
<td>
<p>Number of significant digits used for priniting.</p>
</td></tr>
<tr><td><code id="print.summary.ddml_fpliv_+3A_...">...</code></td>
<td>
<p>Currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
D = AE98[, "morekids"]
X = AE98[, c("age","agefst","black","hisp","othrace","educ")]

# Estimate the partially linear model using a single base learner, ridge.
plm_fit &lt;- ddml_plm(y, D, X,
                    learners = list(what = mdl_glmnet,
                                    args = list(alpha = 0)),
                    sample_folds = 2,
                    silent = TRUE)
summary(plm_fit)
</code></pre>

<hr>
<h2 id='shortstacking'>Predictions using Short-Stacking.</h2><span id='topic+shortstacking'></span>

<h3>Description</h3>

<p>Predictions using short-stacking.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shortstacking(
  y,
  X,
  Z = NULL,
  learners,
  sample_folds = 2,
  ensemble_type = "average",
  custom_ensemble_weights = NULL,
  compute_insample_predictions = FALSE,
  subsamples = NULL,
  silent = FALSE,
  progress = NULL,
  auxilliary_X = NULL,
  shortstack_y = y
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shortstacking_+3A_y">y</code></td>
<td>
<p>The outcome variable.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_x">X</code></td>
<td>
<p>A (sparse) matrix of predictive variables.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_z">Z</code></td>
<td>
<p>Optional additional (sparse) matrix of predictive variables.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_learners">learners</code></td>
<td>
<p>May take one of two forms, depending on whether a single
learner or stacking with multiple learners is used for estimation of the
predictor.
If a single learner is used, <code>learners</code> is a list with two named
elements:
</p>

<ul>
<li><p><code>what</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>what</code>.
</p>
</li></ul>

<p>If stacking with multiple learners is used, <code>learners</code> is a list of
lists, each containing four named elements:
</p>

<ul>
<li><p><code>fun</code> The base learner function. The function must be
such that it predicts a named input <code>y</code> using a named input
<code>X</code>.
</p>
</li>
<li><p><code>args</code> Optional arguments to be passed to <code>fun</code>.
</p>
</li>
<li><p><code>assign_X</code> An optional vector of column indices
corresponding to predictive variables in <code>X</code> that are passed to
the base learner.
</p>
</li>
<li><p><code>assign_Z</code> An optional vector of column indices
corresponding to predictive in <code>Z</code> that are passed to the
base learner.
</p>
</li></ul>

<p>Omission of the <code>args</code> element results in default arguments being
used in <code>fun</code>. Omission of <code>assign_X</code> (and/or <code>assign_Z</code>)
results in inclusion of all variables in <code>X</code> (and/or <code>Z</code>).</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_sample_folds">sample_folds</code></td>
<td>
<p>Number of cross-fitting folds.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_ensemble_type">ensemble_type</code></td>
<td>
<p>Ensemble method to combine base learners into final
estimate of the conditional expectation functions. Possible values are:
</p>

<ul>
<li><p><code>"nnls"</code> Non-negative least squares.
</p>
</li>
<li><p><code>"nnls1"</code> Non-negative least squares with the constraint
that all weights sum to one.
</p>
</li>
<li><p><code>"singlebest"</code> Select base learner with minimum MSPE.
</p>
</li>
<li><p><code>"ols"</code> Ordinary least squares.
</p>
</li>
<li><p><code>"average"</code> Simple average over base learners.
</p>
</li></ul>

<p>Multiple ensemble types may be passed as a vector of strings.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_custom_ensemble_weights">custom_ensemble_weights</code></td>
<td>
<p>A numerical matrix with user-specified
ensemble weights. Each column corresponds to a custom ensemble
specification, each row corresponds to a base learner in <code>learners</code>
(in chronological order). Optional column names are used to name the
estimation results corresponding the custom ensemble specification.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_compute_insample_predictions">compute_insample_predictions</code></td>
<td>
<p>Indicator equal to 1 if in-sample
predictions should also be computed.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_subsamples">subsamples</code></td>
<td>
<p>List of vectors with sample indices for cross-fitting.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_silent">silent</code></td>
<td>
<p>Boolean to silence estimation updates.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_progress">progress</code></td>
<td>
<p>String to print before learner and cv fold progress.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_auxilliary_x">auxilliary_X</code></td>
<td>
<p>An optional list of matrices of length
<code>sample_folds</code>, each containing additional observations to calculate
predictions for.</p>
</td></tr>
<tr><td><code id="shortstacking_+3A_shortstack_y">shortstack_y</code></td>
<td>
<p>Optional vector of the outcome variable to form
short-stacking predictions for. Base learners are always trained on
<code>y</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>shortstack</code> returns a list containing the following components:
</p>

<dl>
<dt><code>oos_fitted</code></dt><dd><p>A matrix of out-of-sample predictions,
each column corresponding to an ensemble type (in chronological
order).</p>
</dd>
<dt><code>weights</code></dt><dd><p>An array, providing the weight
assigned to each base learner (in chronological order) by the
ensemble procedures.</p>
</dd>
<dt><code>is_fitted</code></dt><dd><p>When <code>compute_insample_predictions = T</code>.
a list of matrices with in-sample predictions by sample fold.</p>
</dd>
<dt><code>auxilliary_fitted</code></dt><dd><p>When <code>auxilliary_X</code> is not
<code>NULL</code>, a list of matrices with additional predictions.</p>
</dd>
<dt><code>oos_fitted_bylearner</code></dt><dd><p>A matrix of
out-of-sample predictions, each column corresponding to a base
learner (in chronological order).</p>
</dd>
<dt><code>is_fitted_bylearner</code></dt><dd><p>When
<code>compute_insample_predictions = T</code>, a list of matrices with
in-sample predictions by sample fold.</p>
</dd>
<dt><code>auxilliary_fitted_bylearner</code></dt><dd><p>When <code>auxilliary_X</code> is
not <code>NULL</code>, a
list of matrices with additional predictions for each learner.</p>
</dd>
</dl>

<p>Note that unlike <code>crosspred</code>, <code>shortstack</code> always computes
out-of-sample predictions for each base learner (at no additional
computational cost).
</p>


<h3>References</h3>

<p>Ahrens A, Hansen C B, Schaffer M E, Wiemann T (2023). &quot;ddml: Double/debiased
machine learning in Stata.&quot; <a href="https://arxiv.org/abs/2301.09397">https://arxiv.org/abs/2301.09397</a>
</p>
<p>Wolpert D H (1992). &quot;Stacked generalization.&quot; Neural Networks, 5(2), 241-259.
</p>


<h3>See Also</h3>

<p>Other utilities: 
<code><a href="#topic+crosspred">crosspred</a>()</code>,
<code><a href="#topic+crossval">crossval</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
X = AE98[, c("morekids", "age","agefst","black","hisp","othrace","educ")]

# Compute predictions using shortstacking with base learners ols and lasso.
#     Two stacking approaches are simultaneously computed: Equally
#     weighted (ensemble_type = "average") and MSPE-minimizing with weights
#     in the unit simplex (ensemble_type = "nnls1"). Predictions for each
#     learner are also calculated.
shortstack_res &lt;- shortstacking(y, X,
                                learners = list(list(fun = ols),
                                                list(fun = mdl_glmnet)),
                                ensemble_type = c("average",
                                                  "nnls1",
                                                  "singlebest"),
                                sample_folds = 2,
                                silent = TRUE)
dim(shortstack_res$oos_fitted) # = length(y) by length(ensemble_type)
dim(shortstack_res$oos_fitted_bylearner) # = length(y) by length(learners)
</code></pre>

<hr>
<h2 id='summary.ddml_ate'>Inference Methods for Treatment Effect Estimators.</h2><span id='topic+summary.ddml_ate'></span><span id='topic+summary.ddml_att'></span><span id='topic+summary.ddml_late'></span>

<h3>Description</h3>

<p>Inference methods for treatment effect estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ddml_ate'
summary(object, ...)

## S3 method for class 'ddml_att'
summary(object, ...)

## S3 method for class 'ddml_late'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.ddml_ate_+3A_object">object</code></td>
<td>
<p>An object of class <code>ddml_ate</code>, <code>ddml_att</code>, and
<code>ddml_late</code>, as fitted by <code><a href="#topic+ddml_ate">ddml_ate()</a></code>, <code><a href="#topic+ddml_att">ddml_att()</a></code>,
and <code><a href="#topic+ddml_late">ddml_late()</a></code>, respectively.</p>
</td></tr>
<tr><td><code id="summary.ddml_ate_+3A_...">...</code></td>
<td>
<p>Currently unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with inference results.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
D = AE98[, "morekids"]
X = AE98[, c("age","agefst","black","hisp","othrace","educ")]

# Estimate the average treatment effect using a single base learner, ridge.
ate_fit &lt;- ddml_ate(y, D, X,
                    learners = list(what = mdl_glmnet,
                                    args = list(alpha = 0)),
                    sample_folds = 2,
                    silent = TRUE)
summary(ate_fit)
</code></pre>

<hr>
<h2 id='summary.ddml_fpliv'>Inference Methods for Partially Linear Estimators.</h2><span id='topic+summary.ddml_fpliv'></span><span id='topic+summary.ddml_pliv'></span><span id='topic+summary.ddml_plm'></span>

<h3>Description</h3>

<p>Inference methods for partially linear estimators. Simple
wrapper for <code><a href="sandwich.html#topic+vcovHC">sandwich::vcovHC()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ddml_fpliv'
summary(object, ...)

## S3 method for class 'ddml_pliv'
summary(object, ...)

## S3 method for class 'ddml_plm'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.ddml_fpliv_+3A_object">object</code></td>
<td>
<p>An object of class <code>ddml_plm</code>, <code>ddml_pliv</code>, or
<code>ddml_fpliv</code> as fitted by <code><a href="#topic+ddml_plm">ddml_plm()</a></code>, <code><a href="#topic+ddml_pliv">ddml_pliv()</a></code>,
and <code><a href="#topic+ddml_fpliv">ddml_fpliv()</a></code>, respectively.</p>
</td></tr>
<tr><td><code id="summary.ddml_fpliv_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>vcovHC</code>. See
<code><a href="sandwich.html#topic+vcovHC">sandwich::vcovHC()</a></code> for a complete list of arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An array with inference results for each <code>ensemble_type</code>.
</p>


<h3>References</h3>

<p>Zeileis A (2004). &quot;Econometric Computing with HC and HAC Covariance Matrix
Estimators. Journal of Statistical Software, 11(10), 1-17.
</p>
<p>Zeileis A (2006). Object-Oriented Computation of Sandwich Estimators.
Journal of Statistical Software, 16(9), 1-16.
</p>
<p>Zeileis A, Kll S, Graham N (2020). Various Versatile Variances: An
Object-Oriented Implementation of Clustered Covariances in R. Journal of
Statistical Software, 95(1), 1-36.
</p>


<h3>See Also</h3>

<p><code><a href="sandwich.html#topic+vcovHC">sandwich::vcovHC()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct variables from the included Angrist &amp; Evans (1998) data
y = AE98[, "worked"]
D = AE98[, "morekids"]
X = AE98[, c("age","agefst","black","hisp","othrace","educ")]

# Estimate the partially linear model using a single base learner, ridge.
plm_fit &lt;- ddml_plm(y, D, X,
                    learners = list(what = mdl_glmnet,
                                    args = list(alpha = 0)),
                    sample_folds = 2,
                    silent = TRUE)
summary(plm_fit)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
