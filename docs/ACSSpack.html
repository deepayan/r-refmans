<!DOCTYPE html><html lang="en"><head><title>Help for package ACSSpack</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ACSSpack}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ACSS_gs'><p>ACSS algorithm</p></a></li>
<li><a href='#Econ_data'><p>Economic data from the GLP paper</p></a></li>
<li><a href='#GLP_gs'><p>GLP algorithm</p></a></li>
<li><a href='#INSS_gs'><p>INSS algorithm</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>ACSS, Corresponding ACSS, and GLP Algorithm</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.1.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-06-30</td>
</tr>
<tr>
<td>Description:</td>
<td>Allow user to run the Adaptive Correlated Spike and Slab (ACSS) algorithm, corresponding INdependent Spike and Slab (INSS) algorithm, and Giannone, Lenza and Primiceri (GLP) algorithm with adaptive burn-in. 
            All of the three algorithms are used to fit high dimensional data set with either sparse structure, or dense structure with smaller contributions from all predictors. 
            The state-of-the-art GLP algorithm is in Giannone, D., Lenza, M., &amp; Primiceri, G. E. (2021, ISBN:978-92-899-4542-4) 
            "Economic predictions with big data: The illusion of sparsity".
            The two new algorithms, ACSS algorithm and INSS algorithm, and the discussion on their performance can be seen in 
            Yang, Z., Khare, K., &amp; Michailidis, G. (2024, preprint) "Bayesian methodology for adaptive sparsity and shrinkage in regression".</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, HDCI (&ge; 1.0-2), MASS (&ge; 7.3-60), extraDistr (&ge;
1.4-4)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp (&ge; 1.0.11), RcppArmadillo (&ge; 0.12.6.3.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-07-03 23:35:18 UTC; yangt</td>
</tr>
<tr>
<td>Author:</td>
<td>Ziqian Yang [cre, aut],
  Kshitij Khare [aut],
  George Michailidis [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ziqian Yang &lt;zi.yang@ufl.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-07-04 16:40:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='ACSS_gs'>ACSS algorithm</h2><span id='topic+ACSS_gs'></span>

<h3>Description</h3>

<p>Adaptive Correlated  Spike and Slab (ACSS) algorithm with/without adaptive burn-in Gibbs sampler. See paper of Yang, Z., Khare, K., &amp; Michailidis, G. (2024) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ACSS_gs(
  Y,
  X,
  a = 1,
  b = 1,
  c = 1,
  s,
  Max_burnin = 10,
  nmc = 5000,
  adaptive_burn_in = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ACSS_gs_+3A_y">Y</code></td>
<td>
<p>A vector.</p>
</td></tr>
<tr><td><code id="ACSS_gs_+3A_x">X</code></td>
<td>
<p>A matrix.</p>
</td></tr>
<tr><td><code id="ACSS_gs_+3A_a">a</code></td>
<td>
<p>shape parameter for marginal of q; default=1.</p>
</td></tr>
<tr><td><code id="ACSS_gs_+3A_b">b</code></td>
<td>
<p>shape parameter for marginal of q; default=1.</p>
</td></tr>
<tr><td><code id="ACSS_gs_+3A_c">c</code></td>
<td>
<p>shape parameter for marginal of lambda^2; larger c introduce more shrinkage and stronger correlation. default=1.</p>
</td></tr>
<tr><td><code id="ACSS_gs_+3A_s">s</code></td>
<td>
<p>scale (inversed) parameter for marginal of lambda^2; larger s introduce more shrinkage; default=sqrt(p).</p>
</td></tr>
<tr><td><code id="ACSS_gs_+3A_max_burnin">Max_burnin</code></td>
<td>
<p>Maximum burn-in (in 100 steps) for adaptive burn-in Gibbs sampler. Minimum value is 10, corresponding to 1000 hard burn-insteps. Default=10.</p>
</td></tr>
<tr><td><code id="ACSS_gs_+3A_nmc">nmc</code></td>
<td>
<p>Number of MCMC samples. Default=5000.</p>
</td></tr>
<tr><td><code id="ACSS_gs_+3A_adaptive_burn_in">adaptive_burn_in</code></td>
<td>
<p>Logical. If TRUE, use adaptive burn-in Gibbs sampler; If false, use fixed burn-in with burn-in = Max_burnin. Default=TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with betahat: predicted beta hat from majority voting, and Gibbs_res: 5000 samples of beta, q and lambda^2 from Gibbs sampler.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A toy example is given below to save time. The full example can be run to get better results
## by using X instead of X[, 1:30] and let nmc=5000 (default).

n = 30;
p = 2 * n;

beta1 = rep(0.1, p);
beta2 = c(rep(0.2, p / 2), rep(0, p / 2));
beta3 = c(rep(0.15, 3 * p / 4), rep(0, ceiling(p / 4)));
beta4 = c(rep(1, p / 4), rep(0, ceiling(3 * p / 4)));
beta5 = c(rep(3, ceiling(p / 20)), rep(0 , 19 * p / 20));
betas = list(beta1, beta3, beta2, beta4, beta5);

set.seed(123);
X = matrix(rnorm(n * p), n, p);
Y = c(X %*% betas[[1]] + rnorm(n));

## A toy example with p=30, total Gibbs steps=1100, takes ~0.6s
system.time({mod = ACSS_gs(Y, X[, 1:30], 1, 1, 1, sqrt(p), nmc = 100);})

mod$beta; ## estimated beta after the Majority voting
hist(mod$Gibbs_res$betamat[1,]); ## histogram of the beta_1
hist(mod$Gibbs_res$q); ## histogram of the q
hist(log(mod$Gibbs_res$lambdasq)); ## histogram of the log(lambda^2)
plot(mod$Gibbs_res$q); ## trace plot of the q
## joint posterior of model density and shrinkage
plot(log(mod$Gibbs_res$q / (1 - mod$Gibbs_res$q)), -log(mod$Gibbs_res$lambdasq),
    xlab = "logit(q)", ylab = "-log(lambda^2)",
    main = "Joint Posterior of Model Density and Shrinkage"); 
</code></pre>

<hr>
<h2 id='Econ_data'>Economic data from the GLP paper</h2><span id='topic+Econ_data'></span>

<h3>Description</h3>

<p>A list contains the five data set used in the paper of Giannone, Lenza, and Primiceri (2021).
Contains the following data sets: Macro1, Macro2, Micro1, Micro2, and Finance1
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Econ_data
</code></pre>


<h3>Format</h3>

<p>## 'Econ_data'
A list contains the five lists, as the 5 data sets
</p>

<dl>
<dt>Macro1</dt><dd><p>A list with a vector and a data frame, 'Y' and 'X'. 'Y' is the response vector, with 659 observations in it. 'X' is the data frame contains all predictors, with n=659, p=130. 
It have the structure of time series data.</p>
</dd>
<dt>Macro2</dt><dd><p>A list with a vector and a data frame, 'Y' and 'X'. 'Y' is the response vector, with 90 observations in it. 'X' is the data frame contains all predictors, with n=90, p=69.
It have the structure of sectional data.</p>
</dd>
<dt>Micro1</dt><dd><p>A list with a vector and a data frame, 'Y' and 'X'. 'Y' is the response vector, with 576 observations in it. 'X' is the data frame contains all predictors, with n=576, p=285.
It have the structure of panel data with 48 units on 12 time points.</p>
</dd>
<dt>Micro2</dt><dd><p>A list with a vector and a data frame, 'Y' and 'X'. 'Y' is the response vector, with 312 observations in it. 'X' is the data frame contains all predictors, with n=312, p=138.
It have the structure of panel data with 12 units on 26 time points.</p>
</dd>
<dt>Finance1</dt><dd><p>A list with a vector and a data frame, 'Y' and 'X'. 'Y' is the response vector, with 68 observations in it. 'X' is the data frame contains all predictors, with n=68, p=16.
It have the structure of time series data.</p>
</dd>
</dl>



<h3>Source</h3>

<p>&lt;https://www.econometricsociety.org/publications/econometrica/2021/09/01/economic-predictions-big-data-illusion-sparsity/supp/17842_Data_and_Programs.zip&gt;
</p>
<p>&lt;https://research.stlouisfed.org/econ/mccracken/fred-databases/&gt;
</p>

<hr>
<h2 id='GLP_gs'>GLP algorithm</h2><span id='topic+GLP_gs'></span>

<h3>Description</h3>

<p>Giannone, Lenza and Primiceri (GLP) algorithm with/without adaptive burn-in Gibbs sampler. See paper Giannone, D., Lenza, M., &amp; Primiceri, G. E. (2021) and Yang, Z., Khare, K., &amp; Michailidis, G. (2024) for details.
</p>
<p>Most of the codes are from https://github.com/bfava/IllusionOfIllusion with our modification to make it have adaptive burn-in Gibbs sampler, and some debugs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GLP_gs(
  Y,
  X,
  a = 1,
  b = 1,
  A = 1,
  B = 1,
  Max_burnin = 10,
  nmc = 5000,
  adaptive_burn_in = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GLP_gs_+3A_y">Y</code></td>
<td>
<p>A vector.</p>
</td></tr>
<tr><td><code id="GLP_gs_+3A_x">X</code></td>
<td>
<p>A matrix.</p>
</td></tr>
<tr><td><code id="GLP_gs_+3A_a">a</code></td>
<td>
<p>shape parameter for marginal of q; default=1.</p>
</td></tr>
<tr><td><code id="GLP_gs_+3A_b">b</code></td>
<td>
<p>shape parameter for marginal of q; default=1.</p>
</td></tr>
<tr><td><code id="GLP_gs_+3A_a">A</code></td>
<td>
<p>shape parameter for marginal of R^2; default=1.</p>
</td></tr>
<tr><td><code id="GLP_gs_+3A_b">B</code></td>
<td>
<p>shape parameter for marginal of R^2; default=1.</p>
</td></tr>
<tr><td><code id="GLP_gs_+3A_max_burnin">Max_burnin</code></td>
<td>
<p>Maximum burn-in (in 100 steps) for adaptive burn-in Gibbs sampler. Minimum value is 10, corresponding to 1000 hard burn-insteps. Default=10.</p>
</td></tr>
<tr><td><code id="GLP_gs_+3A_nmc">nmc</code></td>
<td>
<p>Number of MCMC samples. Default=5000.</p>
</td></tr>
<tr><td><code id="GLP_gs_+3A_adaptive_burn_in">adaptive_burn_in</code></td>
<td>
<p>Logical. If TRUE, use adaptive burn-in Gibbs sampler; If false, use fixed burn-in with burn-in = Max_burnin. Default=TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with betahat: predicted beta hat from majority voting, and Gibbs_res: 5000 samples of beta, q and lambda^2 from Gibbs sampler.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A toy example is given below to save your time, which will still take ~10s. 
## The full example can be run to get BETTER results, which will take more than 80s, 
## by using X instead of X[, 1:30] and let nmc=5000 (default).

n = 30;
p = 2 * n;

beta1 = rep(0.1, p);
beta2 = c(rep(0.2, p / 2), rep(0, p / 2));
beta3 = c(rep(0.15, 3 * p / 4), rep(0, ceiling(p / 4)));
beta4 = c(rep(1, p / 4), rep(0, ceiling(3 * p / 4)));
beta5 = c(rep(3, ceiling(p / 20)), rep(0 , 19 * p / 20));
betas = list(beta1, beta3, beta2, beta4, beta5);

set.seed(123);
X = matrix(rnorm(n * p), n, p);
Y = c(X %*% betas[[1]] + rnorm(n));

## A toy example with p=30, total Gibbs steps=1100
system.time({mod = GLP_gs(Y, X[, 1:30], 1, 1, 1, 1, nmc = 100);})

mod$beta; ## estimated beta after the Majority voting
hist(mod$Gibbs_res$betamat[1,]); ## histogram of the beta_1
hist(mod$Gibbs_res$q); ## histogram of the q
hist(log(mod$Gibbs_res$lambdasq)); ## histogram of the log(lambda^2)
plot(mod$Gibbs_res$q); ## trace plot of the q
## joint posterior of model density and shrinkage
plot(log(mod$Gibbs_res$q / (1 - mod$Gibbs_res$q)), -log(mod$Gibbs_res$lambdasq),
    xlab = "logit(q)", ylab = "-log(lambda^2)",
    main = "Joint Posterior of Model Density and Shrinkage"); 
</code></pre>

<hr>
<h2 id='INSS_gs'>INSS algorithm</h2><span id='topic+INSS_gs'></span>

<h3>Description</h3>

<p>INdependent Spike and Slab (INSS) algorithm with/without adaptive burn-in Gibbs sampler. See paper of Yang, Z., Khare, K., &amp; Michailidis, G. (2024) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>INSS_gs(
  Y,
  X,
  a = 1,
  b = 1,
  c = 1,
  s,
  Max_burnin = 10,
  nmc = 5000,
  adaptive_burn_in = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="INSS_gs_+3A_y">Y</code></td>
<td>
<p>A vector.</p>
</td></tr>
<tr><td><code id="INSS_gs_+3A_x">X</code></td>
<td>
<p>A matrix.</p>
</td></tr>
<tr><td><code id="INSS_gs_+3A_a">a</code></td>
<td>
<p>shape parameter for marginal of q; default=1.</p>
</td></tr>
<tr><td><code id="INSS_gs_+3A_b">b</code></td>
<td>
<p>shape parameter for marginal of q; default=1.</p>
</td></tr>
<tr><td><code id="INSS_gs_+3A_c">c</code></td>
<td>
<p>shape parameter for marginal of lambda^2; larger c introduce more shrinkage and stronger correlation. default=1.</p>
</td></tr>
<tr><td><code id="INSS_gs_+3A_s">s</code></td>
<td>
<p>scale (inversed) parameter for marginal of lambda^2; larger s introduce more shrinkage; default=sqrt(p).</p>
</td></tr>
<tr><td><code id="INSS_gs_+3A_max_burnin">Max_burnin</code></td>
<td>
<p>Maximum burn-in (in 100 steps) for adaptive burn-in Gibbs sampler. Minimum value is 10, corresponding to 1000 hard burn-insteps. Default=10.</p>
</td></tr>
<tr><td><code id="INSS_gs_+3A_nmc">nmc</code></td>
<td>
<p>Number of MCMC samples. Default=5000.</p>
</td></tr>
<tr><td><code id="INSS_gs_+3A_adaptive_burn_in">adaptive_burn_in</code></td>
<td>
<p>Logical. If TRUE, use adaptive burn-in Gibbs sampler; If false, use fixed burn-in with burn-in = Max_burnin. Default=TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with betahat: predicted beta hat from majority voting, and Gibbs_res: 5000 samples of beta, q and lambda^2 from Gibbs sampler.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A toy example is given below to save time. The full example can be run to get better results
## by using X instead of X[, 1:30] and let nmc=5000 (default).

n = 30;
p = 2 * n;

beta1 = rep(0.1, p);
beta2 = c(rep(0.2, p / 2), rep(0, p / 2));
beta3 = c(rep(0.15, 3 * p / 4), rep(0, ceiling(p / 4)));
beta4 = c(rep(1, p / 4), rep(0, ceiling(3 * p / 4)));
beta5 = c(rep(3, ceiling(p / 20)), rep(0 , 19 * p / 20));
betas = list(beta1, beta3, beta2, beta4, beta5);

set.seed(123);
X = matrix(rnorm(n * p), n, p);
Y = c(X %*% betas[[1]] + rnorm(n));

## A toy example with p=30, total Gibbs steps=1100, takes ~0.6s
system.time({mod = INSS_gs(Y, X[, 1:30], 1, 1, 1, sqrt(p), nmc = 100);})

mod$beta; ## estimated beta after the Majority voting
hist(mod$Gibbs_res$betamat[1,]); ## histogram of the beta_1
hist(mod$Gibbs_res$q); ## histogram of the q
hist(log(mod$Gibbs_res$lambdasq)); ## histogram of the log(lambda^2)
plot(mod$Gibbs_res$q); ## trace plot of the q
## joint posterior of model density and shrinkage
plot(log(mod$Gibbs_res$q / (1 - mod$Gibbs_res$q)), -log(mod$Gibbs_res$lambdasq),
    xlab = "logit(q)", ylab = "-log(lambda^2)",
    main = "Joint Posterior of Model Density and Shrinkage"); 
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
