<!DOCTYPE html><html lang="en"><head><title>Help for package ellmer</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ellmer}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ellmer-package'><p>ellmer: Chat with Large Language Models</p></a></li>
<li><a href='#Chat'><p>A chat</p></a></li>
<li><a href='#chat_azure'><p>Chat with a model hosted on Azure OpenAI</p></a></li>
<li><a href='#chat_bedrock'><p>Chat with an AWS bedrock model</p></a></li>
<li><a href='#chat_claude'><p>Chat with an Anthropic Claude model</p></a></li>
<li><a href='#chat_cortex'><p>Create a chatbot that speaks to the Snowflake Cortex Analyst</p></a></li>
<li><a href='#chat_cortex_analyst'><p>Create a chatbot that speaks to the Snowflake Cortex Analyst</p></a></li>
<li><a href='#chat_databricks'><p>Chat with a model hosted on Databricks</p></a></li>
<li><a href='#chat_deepseek'><p>Chat with a model hosted on DeepSeek</p></a></li>
<li><a href='#chat_gemini'><p>Chat with a Google Gemini model</p></a></li>
<li><a href='#chat_github'><p>Chat with a model hosted on the GitHub model marketplace</p></a></li>
<li><a href='#chat_groq'><p>Chat with a model hosted on Groq</p></a></li>
<li><a href='#chat_ollama'><p>Chat with a local Ollama model</p></a></li>
<li><a href='#chat_openai'><p>Chat with an OpenAI model</p></a></li>
<li><a href='#chat_openrouter'><p>Chat with one of the many models hosted on OpenRouter</p></a></li>
<li><a href='#chat_perplexity'><p>Chat with a model hosted on perplexity.ai</p></a></li>
<li><a href='#chat_snowflake'><p>Chat with a model hosted on Snowflake</p></a></li>
<li><a href='#chat_vllm'><p>Chat with a model hosted by vLLM</p></a></li>
<li><a href='#Content'><p>Content types received from and sent to a chatbot</p></a></li>
<li><a href='#content_image_url'><p>Encode images for chat input</p></a></li>
<li><a href='#content_pdf_file'><p>Encode PDFs content for chat input</p></a></li>
<li><a href='#contents_text'><p>Format contents into a textual representation</p></a></li>
<li><a href='#create_tool_def'><p>Create metadata for a tool</p></a></li>
<li><a href='#has_credentials'><p>Are credentials avaiable?</p></a></li>
<li><a href='#interpolate'><p>Helpers for interpolating data into prompts</p></a></li>
<li><a href='#live_console'><p>Open a live chat application</p></a></li>
<li><a href='#Provider'><p>A chatbot provider</p></a></li>
<li><a href='#token_usage'><p>Report on token usage in the current session</p></a></li>
<li><a href='#tool'><p>Define a tool</p></a></li>
<li><a href='#Turn'><p>A user or assistant turn</p></a></li>
<li><a href='#Type'><p>Type definitions for function calling and structured data extraction.</p></a></li>
<li><a href='#type_boolean'><p>Type specifications</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Chat with Large Language Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Chat with large language models from a range of providers
    including 'Claude' <a href="https://claude.ai">https://claude.ai</a>, 'OpenAI'
    <a href="https://chatgpt.com">https://chatgpt.com</a>, and more. Supports streaming, asynchronous
    calls, tool calling, and structured data extraction.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://ellmer.tidyverse.org">https://ellmer.tidyverse.org</a>, <a href="https://github.com/tidyverse/ellmer">https://github.com/tidyverse/ellmer</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tidyverse/ellmer/issues">https://github.com/tidyverse/ellmer/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, coro (&ge; 1.1.0), glue, httr2 (&ge; 1.1.0), jsonlite, later
(&ge; 1.4.0), lifecycle, promises (&ge; 1.3.1), R6, rlang (&ge;
1.1.0), S7 (&ge; 0.2.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>base64enc, bslib, connectcreds, curl (&ge; 6.0.1), gitcreds,
knitr, magick, openssl, paws.common, rmarkdown, shiny,
shinychat (&ge; 0.1.1), testthat (&ge; 3.0.0), withr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>tidyverse/tidytemplate, rmarkdown</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>true</td>
</tr>
<tr>
<td>Config/testthat/start-first:</td>
<td>test-provider-*</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Collate:</td>
<td>'utils-S7.R' 'types.R' 'content.R' 'provider.R' 'as-json.R'
'utils-coro.R' 'chat.R' 'content-image.R' 'content-pdf.R'
'content-tools.R' 'ellmer-package.R' 'httr2.R'
'import-standalone-obj-type.R' 'import-standalone-purrr.R'
'import-standalone-types-check.R' 'interpolate.R' 'tools-def.R'
'turns.R' 'provider-openai.R' 'provider-azure.R'
'provider-bedrock.R' 'provider-claude.R' 'provider-cortex.R'
'provider-databricks.R' 'provider-deepseek.R'
'provider-gemini.R' 'provider-github.R' 'provider-groq.R'
'provider-ollama.R' 'provider-openrouter.R'
'provider-perplexity.R' 'provider-snowflake.R'
'provider-vllm.R' 'shiny.R' 'tokens.R' 'tools-def-auto.R'
'utils-cat.R' 'utils-merge.R' 'utils.R' 'zzz.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-06 20:17:04 UTC; hadleywickham</td>
</tr>
<tr>
<td>Author:</td>
<td>Hadley Wickham <a href="https://orcid.org/0000-0003-4757-117X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Joe Cheng [aut],
  Aaron Jacobs [aut],
  Posit Software, PBC <a href="https://ror.org/03wc8by49"><img alt="ROR ID"  src="https://cloud.R-project.org/web/ror.svg" style="width:20px; height:20px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [cph,
    fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hadley Wickham &lt;hadley@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-07 00:40:15 UTC</td>
</tr>
</table>
<hr>
<h2 id='ellmer-package'>ellmer: Chat with Large Language Models</h2><span id='topic+ellmer'></span><span id='topic+ellmer-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Chat with large language models from a range of providers including 'Claude' <a href="https://claude.ai">https://claude.ai</a>, 'OpenAI' <a href="https://chatgpt.com">https://chatgpt.com</a>, and more. Supports streaming, asynchronous calls, tool calling, and structured data extraction.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Hadley Wickham <a href="mailto:hadley@posit.co">hadley@posit.co</a> (<a href="https://orcid.org/0000-0003-4757-117X">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Joe Cheng
</p>
</li>
<li><p> Aaron Jacobs
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Posit Software, PBC (03wc8by49) [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://ellmer.tidyverse.org">https://ellmer.tidyverse.org</a>
</p>
</li>
<li> <p><a href="https://github.com/tidyverse/ellmer">https://github.com/tidyverse/ellmer</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tidyverse/ellmer/issues">https://github.com/tidyverse/ellmer/issues</a>
</p>
</li></ul>


<hr>
<h2 id='Chat'>A chat</h2><span id='topic+Chat'></span>

<h3>Description</h3>

<p>A <code>Chat</code> is an sequence of sequence of user and assistant <a href="#topic+Turn">Turn</a>s sent
to a specific <a href="#topic+Provider">Provider</a>. A <code>Chat</code> is a mutable R6 object that takes care of
managing the state associated with the chat; i.e. it records the messages
that you send to the server, and the messages that you receive back.
If you register a tool (i.e. an R function that the assistant can call on
your behalf), it also takes care of the tool loop.
</p>
<p>You should generally not create this object yourself,
but instead call <code><a href="#topic+chat_openai">chat_openai()</a></code> or friends instead.
</p>


<h3>Value</h3>

<p>A Chat object
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-Chat-new"><code>Chat$new()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-get_turns"><code>Chat$get_turns()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-set_turns"><code>Chat$set_turns()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-get_system_prompt"><code>Chat$get_system_prompt()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-get_model"><code>Chat$get_model()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-set_system_prompt"><code>Chat$set_system_prompt()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-tokens"><code>Chat$tokens()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-last_turn"><code>Chat$last_turn()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-chat"><code>Chat$chat()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-extract_data"><code>Chat$extract_data()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-extract_data_async"><code>Chat$extract_data_async()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-chat_async"><code>Chat$chat_async()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-stream"><code>Chat$stream()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-stream_async"><code>Chat$stream_async()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-register_tool"><code>Chat$register_tool()</code></a>
</p>
</li>
<li> <p><a href="#method-Chat-clone"><code>Chat$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-Chat-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Chat$new(provider, turns, seed = NULL, echo = "none")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>provider</code></dt><dd><p>A provider object.</p>
</dd>
<dt><code>turns</code></dt><dd><p>An unnamed list of turns to start the chat with (i.e.,
continuing a previous conversation). If <code>NULL</code> or zero-length list, the
conversation begins from scratch.</p>
</dd>
<dt><code>seed</code></dt><dd><p>Optional integer seed that ChatGPT uses to try and make output
more reproducible.</p>
</dd>
<dt><code>echo</code></dt><dd><p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-get_turns"></a>



<h4>Method <code>get_turns()</code></h4>

<p>Retrieve the turns that have been sent and received so far
(optionally starting with the system prompt, if any).
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$get_turns(include_system_prompt = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>include_system_prompt</code></dt><dd><p>Whether to include the system prompt in the
turns (if any exists).</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-set_turns"></a>



<h4>Method <code>set_turns()</code></h4>

<p>Replace existing turns with a new list.
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$set_turns(value)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>value</code></dt><dd><p>A list of <a href="#topic+Turn">Turn</a>s.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-get_system_prompt"></a>



<h4>Method <code>get_system_prompt()</code></h4>

<p>If set, the system prompt, it not, <code>NULL</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$get_system_prompt()</pre></div>


<hr>
<a id="method-Chat-get_model"></a>



<h4>Method <code>get_model()</code></h4>

<p>Retrieve the model name
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$get_model()</pre></div>


<hr>
<a id="method-Chat-set_system_prompt"></a>



<h4>Method <code>set_system_prompt()</code></h4>

<p>Update the system prompt
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$set_system_prompt(value)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>value</code></dt><dd><p>A string giving the new system prompt</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-tokens"></a>



<h4>Method <code>tokens()</code></h4>

<p>List the number of tokens consumed by each assistant turn.
Currently tokens are recorded for assistant turns only; so user
turns will have zeros.
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$tokens()</pre></div>


<hr>
<a id="method-Chat-last_turn"></a>



<h4>Method <code>last_turn()</code></h4>

<p>The last turn returned by the assistant.
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$last_turn(role = c("assistant", "user", "system"))</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>role</code></dt><dd><p>Optionally, specify a role to find the last turn with
for the role.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Either a <code>Turn</code> or <code>NULL</code>, if no turns with the specified
role have occurred.
</p>


<hr>
<a id="method-Chat-chat"></a>



<h4>Method <code>chat()</code></h4>

<p>Submit input to the chatbot, and return the response as a
simple string (probably Markdown).
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$chat(..., echo = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>The input to send to the chatbot. Can be strings or images
(see <code><a href="#topic+content_image_file">content_image_file()</a></code> and <code><a href="#topic+content_image_url">content_image_url()</a></code>.</p>
</dd>
<dt><code>echo</code></dt><dd><p>Whether to emit the response to stdout as it is received. If
<code>NULL</code>, then the value of <code>echo</code> set when the chat object was created
will be used.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-extract_data"></a>



<h4>Method <code>extract_data()</code></h4>

<p>Extract structured data
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$extract_data(..., type, echo = "none", convert = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>The input to send to the chatbot. Will typically include
the phrase &quot;extract structured data&quot;.</p>
</dd>
<dt><code>type</code></dt><dd><p>A type specification for the extracted data. Should be
created with a <code><a href="#topic+type_boolean">type_()</a></code> function.</p>
</dd>
<dt><code>echo</code></dt><dd><p>Whether to emit the response to stdout as it is received.
Set to &quot;text&quot; to stream JSON data as it's generated (not supported by
all providers).</p>
</dd>
<dt><code>convert</code></dt><dd><p>Automatically convert from JSON lists to R data types
using the schema. For example, this will turn arrays of objects into
data frames and arrays of strings into a character vector.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-extract_data_async"></a>



<h4>Method <code>extract_data_async()</code></h4>

<p>Extract structured data, asynchronously. Returns a promise
that resolves to an object matching the type specification.
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$extract_data_async(..., type, echo = "none")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>The input to send to the chatbot. Will typically include
the phrase &quot;extract structured data&quot;.</p>
</dd>
<dt><code>type</code></dt><dd><p>A type specification for the extracted data. Should be
created with a <code><a href="#topic+type_boolean">type_()</a></code> function.</p>
</dd>
<dt><code>echo</code></dt><dd><p>Whether to emit the response to stdout as it is received.
Set to &quot;text&quot; to stream JSON data as it's generated (not supported by
all providers).</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-chat_async"></a>



<h4>Method <code>chat_async()</code></h4>

<p>Submit input to the chatbot, and receive a promise that
resolves with the response all at once. Returns a promise that resolves
to a string (probably Markdown).
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$chat_async(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>The input to send to the chatbot. Can be strings or images.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-stream"></a>



<h4>Method <code>stream()</code></h4>

<p>Submit input to the chatbot, returning streaming results.
Returns A <a href="https://coro.r-lib.org/articles/generator.html#iterating">coro generator</a>
that yields strings. While iterating, the generator will block while
waiting for more content from the chatbot.
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$stream(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>The input to send to the chatbot. Can be strings or images.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-stream_async"></a>



<h4>Method <code>stream_async()</code></h4>

<p>Submit input to the chatbot, returning asynchronously
streaming results. Returns a <a href="https://coro.r-lib.org/reference/async_generator.html">coro async generator</a> that
yields string promises.
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$stream_async(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>The input to send to the chatbot. Can be strings or images.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-register_tool"></a>



<h4>Method <code>register_tool()</code></h4>

<p>Register a tool (an R function) that the chatbot can use.
If the chatbot decides to use the function,  ellmer will automatically
call it and submit the results back.
</p>
<p>The return value of the function. Generally, this should either be a
string, or a JSON-serializable value. If you must have more direct
control of the structure of the JSON that's returned, you can return a
JSON-serializable value wrapped in <code><a href="base.html#topic+AsIs">base::I()</a></code>, which ellmer will leave
alone until the entire request is JSON-serialized.
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$register_tool(tool_def)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>tool_def</code></dt><dd><p>Tool definition created by <code><a href="#topic+tool">tool()</a></code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Chat-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Chat$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
chat &lt;- chat_openai(echo = TRUE)
chat$chat("Tell me a funny joke")

</code></pre>

<hr>
<h2 id='chat_azure'>Chat with a model hosted on Azure OpenAI</h2><span id='topic+chat_azure'></span>

<h3>Description</h3>

<p>The <a href="https://azure.microsoft.com/en-us/products/ai-services/openai-service">Azure OpenAI server</a>
hosts a number of open source models as well as proprietary models
from OpenAI.
</p>


<h4>Authentication</h4>

<p><code>chat_azure()</code> supports API keys and the <code>credentials</code> parameter, but it also
makes use of:
</p>

<ul>
<li><p> Azure service principals (when the <code>AZURE_TENANT_ID</code>, <code>AZURE_CLIENT_ID</code>,
and <code>AZURE_CLIENT_SECRET</code> environment variables are set).
</p>
</li>
<li><p> Interactive Entra ID authentication, like the Azure CLI.
</p>
</li>
<li><p> Viewer-based credentials on Posit Connect. Requires the <span class="pkg">connectcreds</span>
package.
</p>
</li></ul>




<h3>Usage</h3>

<pre><code class='language-R'>chat_azure(
  endpoint = azure_endpoint(),
  deployment_id,
  api_version = NULL,
  system_prompt = NULL,
  turns = NULL,
  api_key = NULL,
  token = deprecated(),
  credentials = NULL,
  api_args = list(),
  echo = c("none", "text", "all")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_azure_+3A_endpoint">endpoint</code></td>
<td>
<p>Azure OpenAI endpoint url with protocol and hostname, i.e.
<code style="white-space: pre;">&#8288;https://{your-resource-name}.openai.azure.com&#8288;</code>. Defaults to using the
value of the <code>AZURE_OPENAI_ENDPOINT</code> envinronment variable.</p>
</td></tr>
<tr><td><code id="chat_azure_+3A_deployment_id">deployment_id</code></td>
<td>
<p>Deployment id for the model you want to use.</p>
</td></tr>
<tr><td><code id="chat_azure_+3A_api_version">api_version</code></td>
<td>
<p>The API version to use.</p>
</td></tr>
<tr><td><code id="chat_azure_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_azure_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_azure_+3A_api_key">api_key</code></td>
<td>
<p>An API key to use for authentication. You generally should not
supply this directly, but instead set the <code>AZURE_OPENAI_API_KEY</code>
environment variable.</p>
</td></tr>
<tr><td><code id="chat_azure_+3A_token">token</code></td>
<td>
<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a> A literal Azure token to use
for authentication. Deprecated in favour of ambient Azure credentials or
an explicit <code>credentials</code> argument.</p>
</td></tr>
<tr><td><code id="chat_azure_+3A_credentials">credentials</code></td>
<td>
<p>A list of authentication headers to pass into
<code><a href="httr2.html#topic+req_headers">httr2::req_headers()</a></code>, a function that returns them, or <code>NULL</code> to use
<code>token</code> or <code>api_key</code> to generate these headers instead. This is an escape
hatch that allows users to incorporate Azure credentials generated by other
packages into <span class="pkg">ellmer</span>, or to manage the lifetime of credentials that
need to be refreshed.</p>
</td></tr>
<tr><td><code id="chat_azure_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_azure_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_azure(deployment_id = "gpt-4o-mini")
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_bedrock'>Chat with an AWS bedrock model</h2><span id='topic+chat_bedrock'></span>

<h3>Description</h3>

<p><a href="https://aws.amazon.com/bedrock/">AWS Bedrock</a> provides a number of
language models, including those from Anthropic's
<a href="https://aws.amazon.com/bedrock/claude/">Claude</a>, using the Bedrock
<a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html">Converse API</a>.
</p>


<h4>Authentication</h4>

<p>Authentication is handled through {paws.common}, so if authentication
does not work for you automatically, you'll need to follow the advice
at <a href="https://www.paws-r-sdk.com/#credentials">https://www.paws-r-sdk.com/#credentials</a>. In particular, if your
org uses AWS SSO, you'll need to run <code style="white-space: pre;">&#8288;aws sso login&#8288;</code> at the terminal.
</p>



<h3>Usage</h3>

<pre><code class='language-R'>chat_bedrock(
  system_prompt = NULL,
  turns = NULL,
  model = NULL,
  profile = NULL,
  api_args = list(),
  echo = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_bedrock_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_bedrock_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_bedrock_+3A_model">model</code></td>
<td>
<p>ellmer provides a default model, but you'll typically need to
you'll specify a model that you actually have access to.
</p>
<p>If you're using <a href="https://aws.amazon.com/blogs/machine-learning/getting-started-with-cross-region-inference-in-amazon-bedrock/">cross-region inference</a>,
you'll need to use the inference profile ID, e.g.
<code>model="us.anthropic.claude-3-5-sonnet-20240620-v1:0"</code>.</p>
</td></tr>
<tr><td><code id="chat_bedrock_+3A_profile">profile</code></td>
<td>
<p>AWS profile to use.</p>
</td></tr>
<tr><td><code id="chat_bedrock_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call. Some useful arguments include:
</p>
<div class="sourceCode R"><pre>api_args = list(
  inferenceConfig = list(
    maxTokens = 100,
    temperature = 0.7,
    topP = 0.9,
    topK = 20
  )
)
</pre></div></td></tr>
<tr><td><code id="chat_bedrock_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Basic usage
chat &lt;- chat_bedrock()
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_claude'>Chat with an Anthropic Claude model</h2><span id='topic+chat_claude'></span>

<h3>Description</h3>

<p><a href="https://www.anthropic.com">Anthropic</a> provides a number of chat based
models under the <a href="https://www.anthropic.com/claude">Claude</a> moniker.
Note that a Claude Pro membership does not give you the ability to call
models via the API; instead, you will need to sign up (and pay for) a
<a href="https://console.anthropic.com/">developer account</a>
</p>


<h4>Authentication</h4>

<p>To authenticate, we recommend saving your
<a href="https://console.anthropic.com/account/keys">API key</a> to
the <code>ANTHROPIC_API_KEY</code> env var in your <code>.Renviron</code>
(which you can easily edit by calling <code>usethis::edit_r_environ()</code>).
</p>



<h3>Usage</h3>

<pre><code class='language-R'>chat_claude(
  system_prompt = NULL,
  turns = NULL,
  max_tokens = 4096,
  model = NULL,
  api_args = list(),
  base_url = "https://api.anthropic.com/v1",
  api_key = anthropic_key(),
  echo = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_claude_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_claude_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_claude_+3A_max_tokens">max_tokens</code></td>
<td>
<p>Maximum number of tokens to generate before stopping.</p>
</td></tr>
<tr><td><code id="chat_claude_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_claude_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_claude_+3A_base_url">base_url</code></td>
<td>
<p>The base URL to the endpoint; the default uses OpenAI.</p>
</td></tr>
<tr><td><code id="chat_claude_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication. You generally should
not supply this directly, but instead set the <code>ANTHROPIC_API_KEY</code> environment
variable.</p>
</td></tr>
<tr><td><code id="chat_claude_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
chat &lt;- chat_claude()
chat$chat("Tell me three jokes about statisticians")

</code></pre>

<hr>
<h2 id='chat_cortex'>Create a chatbot that speaks to the Snowflake Cortex Analyst</h2><span id='topic+chat_cortex'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p><code><a href="#topic+chat_cortex">chat_cortex()</a></code> was renamed to <code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst()</a></code> to distinguish it from
the more general-purpose Snowflake Cortex chat function, <code><a href="#topic+chat_snowflake">chat_snowflake()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_cortex(
  account = snowflake_account(),
  credentials = NULL,
  model_spec = NULL,
  model_file = NULL,
  api_args = list(),
  echo = c("none", "text", "all")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_cortex_+3A_account">account</code></td>
<td>
<p>A Snowflake <a href="https://docs.snowflake.com/en/user-guide/admin-account-identifier">account identifier</a>,
e.g. <code>"testorg-test_account"</code>. Defaults to the value of the
<code>SNOWFLAKE_ACCOUNT</code> environment variable.</p>
</td></tr>
<tr><td><code id="chat_cortex_+3A_credentials">credentials</code></td>
<td>
<p>A list of authentication headers to pass into
<code><a href="httr2.html#topic+req_headers">httr2::req_headers()</a></code>, a function that returns them when called, or
<code>NULL</code>, the default, to use ambient credentials.</p>
</td></tr>
<tr><td><code id="chat_cortex_+3A_model_spec">model_spec</code></td>
<td>
<p>A semantic model specification, or <code>NULL</code> when
using <code>model_file</code> instead.</p>
</td></tr>
<tr><td><code id="chat_cortex_+3A_model_file">model_file</code></td>
<td>
<p>Path to a semantic model file stored in a Snowflake Stage,
or <code>NULL</code> when using <code>model_spec</code> instead.</p>
</td></tr>
<tr><td><code id="chat_cortex_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_cortex_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>

<hr>
<h2 id='chat_cortex_analyst'>Create a chatbot that speaks to the Snowflake Cortex Analyst</h2><span id='topic+chat_cortex_analyst'></span>

<h3>Description</h3>

<p>Chat with the LLM-powered <a href="https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst">Snowflake Cortex Analyst</a>.
</p>


<h4>Authentication</h4>

<p><code>chat_cortex()</code> picks up the following ambient Snowflake credentials:
</p>

<ul>
<li><p> A static OAuth token defined via the <code>SNOWFLAKE_TOKEN</code> environment
variable.
</p>
</li>
<li><p> Key-pair authentication credentials defined via the <code>SNOWFLAKE_USER</code> and
<code>SNOWFLAKE_PRIVATE_KEY</code> (which can be a PEM-encoded private key or a path
to one) environment variables.
</p>
</li>
<li><p> Posit Workbench-managed Snowflake credentials for the corresponding
<code>account</code>.
</p>
</li>
<li><p> Viewer-based credentials on Posit Connect. Requires the <span class="pkg">connectcreds</span>
package.
</p>
</li></ul>




<h4>Known limitations</h4>

<p>Unlike most comparable model APIs, Cortex does not take a system prompt.
Instead, the caller must provide a &quot;semantic model&quot; describing available
tables, their meaning, and verified queries that can be run against them as a
starting point. The semantic model can be passed as a YAML string or via
reference to an existing file in a Snowflake Stage.
</p>
<p>Note that Cortex does not support multi-turn, so it will not remember
previous messages. Nor does it support registering tools, and attempting to
do so will result in an error.
</p>
<p>See <code><a href="#topic+chat_snowflake">chat_snowflake()</a></code> to chat with more general-purpose models hosted on
Snowflake.
</p>



<h3>Usage</h3>

<pre><code class='language-R'>chat_cortex_analyst(
  account = snowflake_account(),
  credentials = NULL,
  model_spec = NULL,
  model_file = NULL,
  api_args = list(),
  echo = c("none", "text", "all")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_cortex_analyst_+3A_account">account</code></td>
<td>
<p>A Snowflake <a href="https://docs.snowflake.com/en/user-guide/admin-account-identifier">account identifier</a>,
e.g. <code>"testorg-test_account"</code>. Defaults to the value of the
<code>SNOWFLAKE_ACCOUNT</code> environment variable.</p>
</td></tr>
<tr><td><code id="chat_cortex_analyst_+3A_credentials">credentials</code></td>
<td>
<p>A list of authentication headers to pass into
<code><a href="httr2.html#topic+req_headers">httr2::req_headers()</a></code>, a function that returns them when called, or
<code>NULL</code>, the default, to use ambient credentials.</p>
</td></tr>
<tr><td><code id="chat_cortex_analyst_+3A_model_spec">model_spec</code></td>
<td>
<p>A semantic model specification, or <code>NULL</code> when
using <code>model_file</code> instead.</p>
</td></tr>
<tr><td><code id="chat_cortex_analyst_+3A_model_file">model_file</code></td>
<td>
<p>Path to a semantic model file stored in a Snowflake Stage,
or <code>NULL</code> when using <code>model_spec</code> instead.</p>
</td></tr>
<tr><td><code id="chat_cortex_analyst_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_cortex_analyst_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
chat &lt;- chat_cortex_analyst(
  model_file = "@my_db.my_schema.my_stage/model.yaml"
)
chat$chat("What questions can I ask?")

</code></pre>

<hr>
<h2 id='chat_databricks'>Chat with a model hosted on Databricks</h2><span id='topic+chat_databricks'></span>

<h3>Description</h3>

<p>Databricks provides out-of-the-box access to a number of <a href="https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html">foundation models</a>
and can also serve as a gateway for external models hosted by a third party.
</p>


<h4>Authentication</h4>

<p><code>chat_databricks()</code> picks up on ambient Databricks credentials for a subset
of the <a href="https://docs.databricks.com/en/dev-tools/auth/unified-auth.html">Databricks client unified authentication</a>
model. Specifically, it supports:
</p>

<ul>
<li><p> Personal access tokens
</p>
</li>
<li><p> Service principals via OAuth (OAuth M2M)
</p>
</li>
<li><p> User account via OAuth (OAuth U2M)
</p>
</li>
<li><p> Authentication via the Databricks CLI
</p>
</li>
<li><p> Posit Workbench-managed credentials
</p>
</li>
<li><p> Viewer-based credentials on Posit Connect. Requires the <span class="pkg">connectcreds</span>
package.
</p>
</li></ul>




<h4>Known limitations</h4>

<p>Databricks models do not support images, but they do support structured
outputs. Tool calling support is also very limited at present and is
currently not supported by ellmer.
</p>



<h3>Usage</h3>

<pre><code class='language-R'>chat_databricks(
  workspace = databricks_workspace(),
  system_prompt = NULL,
  turns = NULL,
  model = NULL,
  token = NULL,
  api_args = list(),
  echo = c("none", "text", "all")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_databricks_+3A_workspace">workspace</code></td>
<td>
<p>The URL of a Databricks workspace, e.g.
<code>"https://example.cloud.databricks.com"</code>. Will use the value of the
environment variable <code>DATABRICKS_HOST</code>, if set.</p>
</td></tr>
<tr><td><code id="chat_databricks_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_databricks_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_databricks_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick a
reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use. Available foundational
models include:
</p>

<ul>
<li> <p><code>databricks-dbrx-instruct</code> (the default)
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;databricks-mixtral-8x7b-instruct&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;databricks-meta-llama-3-1-70b-instruct&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;databricks-meta-llama-3-1-405b-instruct&#8288;</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="chat_databricks_+3A_token">token</code></td>
<td>
<p>An authentication token for the Databricks workspace, or
<code>NULL</code> to use ambient credentials.</p>
</td></tr>
<tr><td><code id="chat_databricks_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_databricks_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_databricks()
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_deepseek'>Chat with a model hosted on DeepSeek</h2><span id='topic+chat_deepseek'></span>

<h3>Description</h3>

<p>Sign up at <a href="https://platform.deepseek.com">https://platform.deepseek.com</a>.
</p>


<h4>Known limitations</h4>


<ul>
<li><p> Structured data extraction is not supported..
</p>
</li>
<li><p> Function calling is currently <a href="https://api-docs.deepseek.com/guides/function_calling">unstable</a>.
</p>
</li>
<li><p> Images are not supported.
</p>
</li></ul>




<h3>Usage</h3>

<pre><code class='language-R'>chat_deepseek(
  system_prompt = NULL,
  turns = NULL,
  base_url = "https://api.deepseek.com",
  api_key = deepseek_key(),
  model = NULL,
  seed = NULL,
  api_args = list(),
  echo = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_deepseek_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_deepseek_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_deepseek_+3A_base_url">base_url</code></td>
<td>
<p>The base URL to the endpoint; the default uses DeepSeek.</p>
</td></tr>
<tr><td><code id="chat_deepseek_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication. You generally should
not supply this directly, but instead set the <code>OPENAI_API_KEY</code> environment
variable.</p>
</td></tr>
<tr><td><code id="chat_deepseek_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_deepseek_+3A_seed">seed</code></td>
<td>
<p>Optional integer seed that ChatGPT uses to try and make output
more reproducible.</p>
</td></tr>
<tr><td><code id="chat_deepseek_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_deepseek_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_deepseek()
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_gemini'>Chat with a Google Gemini model</h2><span id='topic+chat_gemini'></span>

<h3>Description</h3>



<h4>Authentication</h4>

<p>To authenticate, we recommend saving your
<a href="https://aistudio.google.com/app/apikey">API key</a> to
the <code>GOOGLE_API_KEY</code> env var in your <code>.Renviron</code>
(which you can easily edit by calling <code>usethis::edit_r_environ()</code>).
</p>



<h3>Usage</h3>

<pre><code class='language-R'>chat_gemini(
  system_prompt = NULL,
  turns = NULL,
  base_url = "https://generativelanguage.googleapis.com/v1beta/",
  api_key = gemini_key(),
  model = NULL,
  api_args = list(),
  echo = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_gemini_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_gemini_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_gemini_+3A_base_url">base_url</code></td>
<td>
<p>The base URL to the endpoint; the default uses OpenAI.</p>
</td></tr>
<tr><td><code id="chat_gemini_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication. You generally should
not supply this directly, but instead set the <code>GOOGLE_API_KEY</code> environment
variable.</p>
</td></tr>
<tr><td><code id="chat_gemini_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_gemini_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_gemini_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_gemini()
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_github'>Chat with a model hosted on the GitHub model marketplace</h2><span id='topic+chat_github'></span>

<h3>Description</h3>

<p>GitHub (via Azure) hosts a number of open source and OpenAI models.
To access the GitHub model marketplace, you will need to apply for and
be accepted into the beta access program. See
<a href="https://github.com/marketplace/models">https://github.com/marketplace/models</a> for details.
</p>
<p>This function is a lightweight wrapper around <code><a href="#topic+chat_openai">chat_openai()</a></code> with
the defaults tweaked for the GitHub model marketplace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_github(
  system_prompt = NULL,
  turns = NULL,
  base_url = "https://models.inference.ai.azure.com/",
  api_key = github_key(),
  model = NULL,
  seed = NULL,
  api_args = list(),
  echo = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_github_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_github_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_github_+3A_base_url">base_url</code></td>
<td>
<p>The base URL to the endpoint; the default uses OpenAI.</p>
</td></tr>
<tr><td><code id="chat_github_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication. You generally should
not supply this directly, but instead manage your GitHub credentials
as described in <a href="https://usethis.r-lib.org/articles/git-credentials.html">https://usethis.r-lib.org/articles/git-credentials.html</a>.
For headless environments, this will also look in the <code>GITHUB_PAT</code>
env var.</p>
</td></tr>
<tr><td><code id="chat_github_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_github_+3A_seed">seed</code></td>
<td>
<p>Optional integer seed that ChatGPT uses to try and make output
more reproducible.</p>
</td></tr>
<tr><td><code id="chat_github_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_github_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_github()
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_groq'>Chat with a model hosted on Groq</h2><span id='topic+chat_groq'></span>

<h3>Description</h3>

<p>Sign up at <a href="https://groq.com">https://groq.com</a>.
</p>
<p>This function is a lightweight wrapper around <code><a href="#topic+chat_openai">chat_openai()</a></code> with
the defaults tweaked for groq.
</p>


<h4>Known limitations</h4>

<p>groq does not currently support structured data extraction.
</p>



<h3>Usage</h3>

<pre><code class='language-R'>chat_groq(
  system_prompt = NULL,
  turns = NULL,
  base_url = "https://api.groq.com/openai/v1",
  api_key = groq_key(),
  model = NULL,
  seed = NULL,
  api_args = list(),
  echo = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_groq_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_groq_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_groq_+3A_base_url">base_url</code></td>
<td>
<p>The base URL to the endpoint; the default uses OpenAI.</p>
</td></tr>
<tr><td><code id="chat_groq_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication. You generally should
not supply this directly, but instead set the <code>OPENAI_API_KEY</code> environment
variable.</p>
</td></tr>
<tr><td><code id="chat_groq_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_groq_+3A_seed">seed</code></td>
<td>
<p>Optional integer seed that ChatGPT uses to try and make output
more reproducible.</p>
</td></tr>
<tr><td><code id="chat_groq_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_groq_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_groq()
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_ollama'>Chat with a local Ollama model</h2><span id='topic+chat_ollama'></span>

<h3>Description</h3>

<p>To use <code>chat_ollama()</code> first download and install
<a href="https://ollama.com">Ollama</a>. Then install some models either from the
command line (e.g. with <code style="white-space: pre;">&#8288;ollama pull llama3.1&#8288;</code>) or within R using
<a href="https://hauselin.github.io/ollama-r/">ollamar</a> (e.g.
<code>ollamar::pull("llama3.1")</code>).
</p>
<p>This function is a lightweight wrapper around <code><a href="#topic+chat_openai">chat_openai()</a></code> with
the defaults tweaked for ollama.
</p>


<h4>Known limitations</h4>


<ul>
<li><p> Tool calling is not supported with streaming (i.e. when <code>echo</code> is
<code>"text"</code> or <code>"all"</code>)
</p>
</li>
<li><p> Models can only use 2048 input tokens, and there's no way
to get them to use more, except by creating a custom model with a
different default.
</p>
</li>
<li><p> Tool calling generally seems quite weak, at least with the models I have
tried it with.
</p>
</li></ul>




<h3>Usage</h3>

<pre><code class='language-R'>chat_ollama(
  system_prompt = NULL,
  turns = NULL,
  base_url = "http://localhost:11434",
  model,
  seed = NULL,
  api_args = list(),
  echo = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_ollama_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_ollama_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_ollama_+3A_base_url">base_url</code></td>
<td>
<p>The base URL to the endpoint; the default uses OpenAI.</p>
</td></tr>
<tr><td><code id="chat_ollama_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_ollama_+3A_seed">seed</code></td>
<td>
<p>Optional integer seed that ChatGPT uses to try and make output
more reproducible.</p>
</td></tr>
<tr><td><code id="chat_ollama_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_ollama_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_ollama(model = "llama3.2")
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_openai'>Chat with an OpenAI model</h2><span id='topic+chat_openai'></span>

<h3>Description</h3>

<p><a href="https://openai.com/">OpenAI</a> provides a number of chat-based models,
mostly under the <a href="https://chat.openai.com/">ChatGPT</a> brand.
Note that a ChatGPT Plus membership does not grant access to the API.
You will need to sign up for a developer account (and pay for it) at the
<a href="https://platform.openai.com">developer platform</a>.
</p>
<p>For authentication, we recommend saving your
<a href="https://platform.openai.com/account/api-keys">API key</a> to
the <code>OPENAI_API_KEY</code> environment variable in your <code>.Renviron</code> file.
You can easily edit this file by calling <code>usethis::edit_r_environ()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_openai(
  system_prompt = NULL,
  turns = NULL,
  base_url = "https://api.openai.com/v1",
  api_key = openai_key(),
  model = NULL,
  seed = NULL,
  api_args = list(),
  echo = c("none", "text", "all")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_openai_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_openai_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_openai_+3A_base_url">base_url</code></td>
<td>
<p>The base URL to the endpoint; the default uses OpenAI.</p>
</td></tr>
<tr><td><code id="chat_openai_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication. You generally should
not supply this directly, but instead set the <code>OPENAI_API_KEY</code> environment
variable.</p>
</td></tr>
<tr><td><code id="chat_openai_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_openai_+3A_seed">seed</code></td>
<td>
<p>Optional integer seed that ChatGPT uses to try and make output
more reproducible.</p>
</td></tr>
<tr><td><code id="chat_openai_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_openai_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
chat &lt;- chat_openai()
chat$chat("
  What is the difference between a tibble and a data frame?
  Answer with a bulleted list
")

chat$chat("Tell me three funny jokes about statistcians")

</code></pre>

<hr>
<h2 id='chat_openrouter'>Chat with one of the many models hosted on OpenRouter</h2><span id='topic+chat_openrouter'></span>

<h3>Description</h3>

<p>Sign up at <a href="https://openrouter.ai">https://openrouter.ai</a>.
</p>
<p>Support for features depends on the underlying model that you use; see
<a href="https://openrouter.ai/models">https://openrouter.ai/models</a> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_openrouter(
  system_prompt = NULL,
  turns = NULL,
  api_key = openrouter_key(),
  model = NULL,
  seed = NULL,
  api_args = list(),
  echo = c("none", "text", "all")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_openrouter_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_openrouter_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_openrouter_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication. You generally should
not supply this directly, but instead set the <code>OPENAI_API_KEY</code> environment
variable.</p>
</td></tr>
<tr><td><code id="chat_openrouter_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_openrouter_+3A_seed">seed</code></td>
<td>
<p>Optional integer seed that ChatGPT uses to try and make output
more reproducible.</p>
</td></tr>
<tr><td><code id="chat_openrouter_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_openrouter_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_perplexity">chat_perplexity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_openrouter()
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_perplexity'>Chat with a model hosted on perplexity.ai</h2><span id='topic+chat_perplexity'></span>

<h3>Description</h3>

<p>Sign up at <a href="https://www.perplexity.ai">https://www.perplexity.ai</a>.
</p>
<p>Perplexity AI is a platform for running LLMs that are capable of
searching the web in real-time to help them answer questions with
information that may not have been available when the model was
trained.
</p>
<p>This function is a lightweight wrapper around <code><a href="#topic+chat_openai">chat_openai()</a></code> with
the defaults tweaked for Perplexity AI.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_perplexity(
  system_prompt = NULL,
  turns = NULL,
  base_url = "https://api.perplexity.ai/",
  api_key = perplexity_key(),
  model = NULL,
  seed = NULL,
  api_args = list(),
  echo = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_perplexity_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_perplexity_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_perplexity_+3A_base_url">base_url</code></td>
<td>
<p>The base URL to the endpoint; the default uses OpenAI.</p>
</td></tr>
<tr><td><code id="chat_perplexity_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication. You generally should
not supply this directly, but instead set the <code>PERPLEXITY_API_KEY</code> environment
variable.</p>
</td></tr>
<tr><td><code id="chat_perplexity_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_perplexity_+3A_seed">seed</code></td>
<td>
<p>Optional integer seed that ChatGPT uses to try and make output
more reproducible.</p>
</td></tr>
<tr><td><code id="chat_perplexity_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_perplexity_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>See Also</h3>

<p>Other chatbots: 
<code><a href="#topic+chat_bedrock">chat_bedrock</a>()</code>,
<code><a href="#topic+chat_claude">chat_claude</a>()</code>,
<code><a href="#topic+chat_cortex_analyst">chat_cortex_analyst</a>()</code>,
<code><a href="#topic+chat_databricks">chat_databricks</a>()</code>,
<code><a href="#topic+chat_deepseek">chat_deepseek</a>()</code>,
<code><a href="#topic+chat_gemini">chat_gemini</a>()</code>,
<code><a href="#topic+chat_github">chat_github</a>()</code>,
<code><a href="#topic+chat_groq">chat_groq</a>()</code>,
<code><a href="#topic+chat_ollama">chat_ollama</a>()</code>,
<code><a href="#topic+chat_openai">chat_openai</a>()</code>,
<code><a href="#topic+chat_openrouter">chat_openrouter</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_perplexity()
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_snowflake'>Chat with a model hosted on Snowflake</h2><span id='topic+chat_snowflake'></span>

<h3>Description</h3>

<p>The Snowflake provider allows you to interact with LLM models available
through the <a href="https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-llm-rest-api">Cortex LLM REST API</a>.
</p>


<h4>Authentication</h4>

<p><code>chat_snowflake()</code> picks up the following ambient Snowflake credentials:
</p>

<ul>
<li><p> A static OAuth token defined via the <code>SNOWFLAKE_TOKEN</code> environment
variable.
</p>
</li>
<li><p> Key-pair authentication credentials defined via the <code>SNOWFLAKE_USER</code> and
<code>SNOWFLAKE_PRIVATE_KEY</code> (which can be a PEM-encoded private key or a path
to one) environment variables.
</p>
</li>
<li><p> Posit Workbench-managed Snowflake credentials for the corresponding
<code>account</code>.
</p>
</li>
<li><p> Viewer-based credentials on Posit Connect. Requires the <span class="pkg">connectcreds</span>
package.
</p>
</li></ul>




<h4>Known limitations</h4>

<p>Note that Snowflake-hosted models do not support images, tool calling, or
structured outputs.
</p>
<p>See <code><a href="#topic+chat_cortex">chat_cortex()</a></code> to chat with the Snowflake Cortex Analyst rather than a
general-purpose model.
</p>



<h3>Usage</h3>

<pre><code class='language-R'>chat_snowflake(
  system_prompt = NULL,
  turns = NULL,
  account = snowflake_account(),
  credentials = NULL,
  model = NULL,
  api_args = list(),
  echo = c("none", "text", "all")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_snowflake_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_snowflake_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_snowflake_+3A_account">account</code></td>
<td>
<p>A Snowflake <a href="https://docs.snowflake.com/en/user-guide/admin-account-identifier">account identifier</a>,
e.g. <code>"testorg-test_account"</code>. Defaults to the value of the
<code>SNOWFLAKE_ACCOUNT</code> environment variable.</p>
</td></tr>
<tr><td><code id="chat_snowflake_+3A_credentials">credentials</code></td>
<td>
<p>A list of authentication headers to pass into
<code><a href="httr2.html#topic+req_headers">httr2::req_headers()</a></code>, a function that returns them when called, or
<code>NULL</code>, the default, to use ambient credentials.</p>
</td></tr>
<tr><td><code id="chat_snowflake_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_snowflake_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_snowflake_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
chat &lt;- chat_snowflake()
chat$chat("Tell me a joke in the form of a SQL query.")

</code></pre>

<hr>
<h2 id='chat_vllm'>Chat with a model hosted by vLLM</h2><span id='topic+chat_vllm'></span>

<h3>Description</h3>

<p><a href="https://docs.vllm.ai/en/latest/">vLLM</a> is an open source library that
provides an efficient and convenient LLMs model server. You can use
<code>chat_vllm()</code> to connect to endpoints powered by vLLM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_vllm(
  base_url,
  system_prompt = NULL,
  turns = NULL,
  model,
  seed = NULL,
  api_args = list(),
  api_key = vllm_key(),
  echo = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_vllm_+3A_base_url">base_url</code></td>
<td>
<p>The base URL to the endpoint; the default uses OpenAI.</p>
</td></tr>
<tr><td><code id="chat_vllm_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A system prompt to set the behavior of the assistant.</p>
</td></tr>
<tr><td><code id="chat_vllm_+3A_turns">turns</code></td>
<td>
<p>A list of <a href="#topic+Turn">Turn</a>s to start the chat with (i.e., continuing a
previous conversation). If not provided, the conversation begins from
scratch.</p>
</td></tr>
<tr><td><code id="chat_vllm_+3A_model">model</code></td>
<td>
<p>The model to use for the chat. The default, <code>NULL</code>, will pick
a reasonable default, and tell you about. We strongly recommend explicitly
choosing a model for all but the most casual use.</p>
</td></tr>
<tr><td><code id="chat_vllm_+3A_seed">seed</code></td>
<td>
<p>Optional integer seed that ChatGPT uses to try and make output
more reproducible.</p>
</td></tr>
<tr><td><code id="chat_vllm_+3A_api_args">api_args</code></td>
<td>
<p>Named list of arbitrary extra arguments appended to the body
of every chat API call.</p>
</td></tr>
<tr><td><code id="chat_vllm_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication. You generally should
not supply this directly, but instead set the <code>VLLM_API_KEY</code> environment
variable.</p>
</td></tr>
<tr><td><code id="chat_vllm_+3A_echo">echo</code></td>
<td>
<p>One of the following options:
</p>

<ul>
<li> <p><code>none</code>: don't emit any output (default when running in a function).
</p>
</li>
<li> <p><code>text</code>: echo text output as it streams in (default when running at
the console).
</p>
</li>
<li> <p><code>all</code>: echo all input and output.
</p>
</li></ul>

<p>Note this only affects the <code>chat()</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Chat">Chat</a> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_vllm("http://my-vllm.com")
chat$chat("Tell me three jokes about statisticians")

## End(Not run)
</code></pre>

<hr>
<h2 id='Content'>Content types received from and sent to a chatbot</h2><span id='topic+Content'></span><span id='topic+ContentText'></span><span id='topic+ContentImage'></span><span id='topic+ContentImageRemote'></span><span id='topic+ContentImageInline'></span><span id='topic+ContentToolRequest'></span><span id='topic+ContentToolResult'></span><span id='topic+ContentPDF'></span>

<h3>Description</h3>

<p>Use these functions if you're writing a package that extends ellmer and need
to customise methods for various types of content. For normal use, see
<code><a href="#topic+content_image_url">content_image_url()</a></code> and friends.
</p>
<p>ellmer abstracts away differences in the way that different <a href="#topic+Provider">Provider</a>s
represent various types of content, allowing you to more easily write
code that works with any chatbot. This set of classes represents types of
content that can be either sent to and received from a provider:
</p>

<ul>
<li> <p><code>ContentText</code>: simple text (often in markdown format). This is the only
type of content that can be streamed live as it's received.
</p>
</li>
<li> <p><code>ContentImageRemote</code> and <code>ContentImageInline</code>: images, either as a pointer
to a remote URL or included inline in the object. See
<code><a href="#topic+content_image_file">content_image_file()</a></code> and friends for convenient ways to construct these
objects.
</p>
</li>
<li> <p><code>ContentToolRequest</code>: a request to perform a tool call (sent by the
assistant).
</p>
</li>
<li> <p><code>ContentToolResult</code>: the result of calling the tool (sent by the user).
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>Content()

ContentText(text = stop("Required"))

ContentImage()

ContentImageRemote(url = stop("Required"), detail = "")

ContentImageInline(type = stop("Required"), data = NULL)

ContentToolRequest(
  id = stop("Required"),
  name = stop("Required"),
  arguments = list()
)

ContentToolResult(id = stop("Required"), value = NULL, error = NULL)

ContentPDF(type = stop("Required"), data = stop("Required"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Content_+3A_text">text</code></td>
<td>
<p>A single string.</p>
</td></tr>
<tr><td><code id="Content_+3A_url">url</code></td>
<td>
<p>URL to a remote image.</p>
</td></tr>
<tr><td><code id="Content_+3A_detail">detail</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="Content_+3A_type">type</code></td>
<td>
<p>MIME type of the image.</p>
</td></tr>
<tr><td><code id="Content_+3A_data">data</code></td>
<td>
<p>Base64 encoded image data.</p>
</td></tr>
<tr><td><code id="Content_+3A_id">id</code></td>
<td>
<p>Tool call id (used to associate a request and a result)</p>
</td></tr>
<tr><td><code id="Content_+3A_name">name</code></td>
<td>
<p>Function name</p>
</td></tr>
<tr><td><code id="Content_+3A_arguments">arguments</code></td>
<td>
<p>Named list of arguments to call the function with.</p>
</td></tr>
<tr><td><code id="Content_+3A_value">value</code>, <code id="Content_+3A_error">error</code></td>
<td>
<p>Either the results of calling the function if
it succeeded, otherwise the error message, as a string. One of
<code>value</code> and <code>error</code> will always be <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S7 objects that all inherit from <code>Content</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Content()
ContentText("Tell me a joke")
ContentImageRemote("https://www.r-project.org/Rlogo.png")
ContentToolRequest(id = "abc", name = "mean", arguments = list(x = 1:5))
</code></pre>

<hr>
<h2 id='content_image_url'>Encode images for chat input</h2><span id='topic+content_image_url'></span><span id='topic+content_image_file'></span><span id='topic+content_image_plot'></span>

<h3>Description</h3>

<p>These functions are used to prepare image URLs and files for input to the
chatbot. The <code>content_image_url()</code> function is used to provide a URL to an
image, while <code>content_image_file()</code> is used to provide the image data itself.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>content_image_url(url, detail = c("auto", "low", "high"))

content_image_file(path, content_type = "auto", resize = "low")

content_image_plot(width = 768, height = 768)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="content_image_url_+3A_url">url</code></td>
<td>
<p>The URL of the image to include in the chat input. Can be a
<code style="white-space: pre;">&#8288;data:&#8288;</code> URL or a regular URL. Valid image types are PNG, JPEG, WebP, and
non-animated GIF.</p>
</td></tr>
<tr><td><code id="content_image_url_+3A_detail">detail</code></td>
<td>
<p>The <a href="https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding">detail setting</a>
for this image. Can be <code>"auto"</code>, <code>"low"</code>, or <code>"high"</code>.</p>
</td></tr>
<tr><td><code id="content_image_url_+3A_path">path</code></td>
<td>
<p>The path to the image file to include in the chat input. Valid
file extensions are <code>.png</code>, <code>.jpeg</code>, <code>.jpg</code>, <code>.webp</code>, and (non-animated)
<code>.gif</code>.</p>
</td></tr>
<tr><td><code id="content_image_url_+3A_content_type">content_type</code></td>
<td>
<p>The content type of the image (e.g. <code>image/png</code>). If
<code>"auto"</code>, the content type is inferred from the file extension.</p>
</td></tr>
<tr><td><code id="content_image_url_+3A_resize">resize</code></td>
<td>
<p>If <code>"low"</code>, resize images to fit within 512x512. If <code>"high"</code>,
resize to fit within 2000x768 or 768x2000. (See the <a href="https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding">OpenAI docs</a>
for more on why these specific sizes are used.) If <code>"none"</code>, do not resize.
</p>
<p>You can also pass a custom string to resize the image to a specific size,
e.g. <code>"200x200"</code> to resize to 200x200 pixels while preserving aspect ratio.
Append <code>&gt;</code> to resize only if the image is larger than the specified size,
and <code>!</code> to ignore aspect ratio (e.g. <code>"300x200&gt;!"</code>).
</p>
<p>All values other than <code>none</code> require the <code>magick</code> package.</p>
</td></tr>
<tr><td><code id="content_image_url_+3A_width">width</code>, <code id="content_image_url_+3A_height">height</code></td>
<td>
<p>Width and height in pixels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An input object suitable for including in the <code>...</code> parameter of
the <code>chat()</code>, <code>stream()</code>, <code>chat_async()</code>, or <code>stream_async()</code> methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
chat &lt;- chat_openai(echo = TRUE)
chat$chat(
  "What do you see in these images?",
  content_image_url("https://www.r-project.org/Rlogo.png"),
  content_image_file(system.file("httr2.png", package = "ellmer"))
)


plot(waiting ~ eruptions, data = faithful)
chat &lt;- chat_openai(echo = TRUE)
chat$chat(
  "Describe this plot in one paragraph, as suitable for inclusion in
   alt-text. You should briefly describe the plot type, the axes, and
   2-5 major visual patterns.",
   content_image_plot()
)

</code></pre>

<hr>
<h2 id='content_pdf_file'>Encode PDFs content for chat input</h2><span id='topic+content_pdf_file'></span><span id='topic+content_pdf_url'></span>

<h3>Description</h3>

<p>These functions are used to prepare PDFs as input to the chatbot. The
<code>content_pdf_url()</code> function is used to provide a URL to an PDF file,
while <code>content_pdf_file()</code> is used to for local PDF files.
</p>
<p>Not all providers support PDF input, so check the documentation for the
provider you are using.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>content_pdf_file(path)

content_pdf_url(url)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="content_pdf_file_+3A_path">path</code>, <code id="content_pdf_file_+3A_url">url</code></td>
<td>
<p>Path or URL to a PDF file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ContentPDF</code> object
</p>

<hr>
<h2 id='contents_text'>Format contents into a textual representation</h2><span id='topic+contents_text'></span><span id='topic+contents_html'></span><span id='topic+contents_markdown'></span>

<h3>Description</h3>

<p>These generic functions can be use to convert <a href="#topic+Turn">Turn</a> contents or <a href="#topic+Content">Content</a>
objects into textual representations.
</p>

<ul>
<li> <p><code>contents_text()</code> is the most minimal and only includes <a href="#topic+ContentText">ContentText</a>
objects in the output.
</p>
</li>
<li> <p><code>contents_markdown()</code> returns the text content (which it assumes to be
markdown and does not convert it) plus markdown representations of images
and other content types.
</p>
</li>
<li> <p><code>contents_html()</code> returns the text content, converted from markdown to
HTML with <code><a href="commonmark.html#topic+commonmark">commonmark::markdown_html()</a></code>, plus HTML representations of
images and other content types.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>contents_text(content, ...)

contents_html(content, ...)

contents_markdown(content, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="contents_text_+3A_content">content</code></td>
<td>
<p>The <a href="#topic+Turn">Turn</a> or <a href="#topic+Content">Content</a> object to be converted into text.
<code>contents_markdown()</code> also accepts <a href="#topic+Chat">Chat</a> instances to turn the entire
conversation history into markdown text.</p>
</td></tr>
<tr><td><code id="contents_text_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A string of text, markdown or HTML.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>turns &lt;- list(
  Turn("user", contents = list(
    ContentText("What's this image?"),
    content_image_url("https://placehold.co/200x200")
  )),
  Turn("assistant", "It's a placeholder image.")
)

lapply(turns, contents_text)
lapply(turns, contents_markdown)
if (rlang::is_installed("commonmark")) {
  contents_html(turns[[1]])
}

</code></pre>

<hr>
<h2 id='create_tool_def'>Create metadata for a tool</h2><span id='topic+create_tool_def'></span>

<h3>Description</h3>

<p>In order to use a function as a tool in a chat, you need to craft the right
call to <code><a href="#topic+tool">tool()</a></code>. This function helps you do that for documented functions by
extracting the function's R documentation and creating a <code>tool()</code> call for
you, using an LLM. It's meant to be used interactively while writing your
code, not as part of your final code.
</p>
<p>If the function has package documentation, that will be used. Otherwise, if
the source code of the function can be automatically detected, then the
comments immediately preceding the function are used (especially helpful if
those are Roxygen comments). If neither are available, then just the function
signature is used.
</p>
<p>Note that this function is inherently imperfect. It can't handle all possible
R functions, because not all parameters are suitable for use in a tool call
(for example, because they're not serializable to simple JSON objects). The
documentation might not specify the expected shape of arguments to the level
of detail that would allow an exact JSON schema to be generated. Please be
sure to review the generated code before using it!
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_tool_def(topic, model = "gpt-4o", echo = interactive(), verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_tool_def_+3A_topic">topic</code></td>
<td>
<p>A symbol or string literal naming the function to create
metadata for. Can also be an expression of the form <code>pkg::fun</code>.</p>
</td></tr>
<tr><td><code id="create_tool_def_+3A_model">model</code></td>
<td>
<p>The OpenAI model to use for generating the metadata. Defaults to
&quot;gpt-4o&quot;.</p>
</td></tr>
<tr><td><code id="create_tool_def_+3A_echo">echo</code></td>
<td>
<p>Emit the registration code to the console. Defaults to <code>TRUE</code> in
interactive sessions.</p>
</td></tr>
<tr><td><code id="create_tool_def_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, print the input we send to the LLM, which may be
useful for debugging unexpectedly poor results.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>register_tool</code> call that you can copy and paste into your code.
Returned invisibly if <code>echo</code> is <code>TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # These are all equivalent
  create_tool_def(rnorm)
  create_tool_def(stats::rnorm)
  create_tool_def("rnorm")

## End(Not run)

</code></pre>

<hr>
<h2 id='has_credentials'>Are credentials avaiable?</h2><span id='topic+has_credentials'></span>

<h3>Description</h3>

<p>Used for examples/testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>has_credentials(provider)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="has_credentials_+3A_provider">provider</code></td>
<td>
<p>Provider name.</p>
</td></tr>
</table>

<hr>
<h2 id='interpolate'>Helpers for interpolating data into prompts</h2><span id='topic+interpolate'></span><span id='topic+interpolate_file'></span>

<h3>Description</h3>

<p>These functions are lightweight wrappers around
<a href="https://glue.tidyverse.org/">glue</a> that make it easier to interpolate
dynamic data into a static prompt. Compared to glue, these functions
expect you to wrap dynamic values in <code>{{ }}</code>, making it easier to include
R code and JSON in your prompt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interpolate(prompt, ..., .envir = parent.frame())

interpolate_file(path, ..., .envir = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="interpolate_+3A_prompt">prompt</code></td>
<td>
<p>A prompt string. You should not generally expose this
to the end user, since glue interpolation makes it easy to run arbitrary
code.</p>
</td></tr>
<tr><td><code id="interpolate_+3A_...">...</code></td>
<td>
<p>Define additional temporary variables for substitution.</p>
</td></tr>
<tr><td><code id="interpolate_+3A_.envir">.envir</code></td>
<td>
<p>Environment to evaluate <code>...</code> expressions in. Used when
wrapping in another function. See <code>vignette("wrappers", package = "glue")</code>
for more details.</p>
</td></tr>
<tr><td><code id="interpolate_+3A_path">path</code></td>
<td>
<p>A path to a prompt file (often a <code>.md</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A {glue} string.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>joke &lt;- "You're a cool dude who loves to make jokes. Tell me a joke about {{topic}}."

# You can supply valuese directly:
interpolate(joke, topic = "bananas")

# Or allow interpolate to find them in the current environment:
topic &lt;- "applies"
interpolate(joke)
</code></pre>

<hr>
<h2 id='live_console'>Open a live chat application</h2><span id='topic+live_console'></span><span id='topic+live_browser'></span>

<h3>Description</h3>


<ul>
<li> <p><code>live_console()</code> lets you chat interactively in the console.
</p>
</li>
<li> <p><code>live_browser()</code> lets you chat interactively in a browser.
</p>
</li></ul>

<p>Note that these functions will mutate the input <code>chat</code> object as
you chat because your turns will be appended to the history.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>live_console(chat, quiet = FALSE)

live_browser(chat, quiet = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="live_console_+3A_chat">chat</code></td>
<td>
<p>A chat object created by <code><a href="#topic+chat_openai">chat_openai()</a></code> or friends.</p>
</td></tr>
<tr><td><code id="live_console_+3A_quiet">quiet</code></td>
<td>
<p>If <code>TRUE</code>, suppresses the initial message that explains how
to use the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(Invisibly) The input <code>chat</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat &lt;- chat_claude()
live_console(chat)
live_browser(chat)

## End(Not run)
</code></pre>

<hr>
<h2 id='Provider'>A chatbot provider</h2><span id='topic+Provider'></span>

<h3>Description</h3>

<p>A Provider captures the details of one chatbot service/API. This captures
how the API works, not the details of the underlying large language model.
Different providers might offer the same (open source) model behind a
different API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Provider(base_url = stop("Required"), extra_args = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Provider_+3A_base_url">base_url</code></td>
<td>
<p>The base URL for the API.</p>
</td></tr>
<tr><td><code id="Provider_+3A_extra_args">extra_args</code></td>
<td>
<p>Arbitrary extra arguments to be included in the request body.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To add support for a new backend, you will need to subclass <code>Provider</code>
(adding any additional fields that your provider needs) and then implement
the various generics that control the behavior of each provider.
</p>


<h3>Value</h3>

<p>An S7 Provider object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Provider(base_url = "https://cool-models.com")
</code></pre>

<hr>
<h2 id='token_usage'>Report on token usage in the current session</h2><span id='topic+token_usage'></span>

<h3>Description</h3>

<p>Call this function to find out the cumulative number of tokens that you
have sent and recieved in the current session.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>token_usage()
</code></pre>


<h3>Value</h3>

<p>A data frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>token_usage()
</code></pre>

<hr>
<h2 id='tool'>Define a tool</h2><span id='topic+tool'></span>

<h3>Description</h3>

<p>Define an R function for use by a chatbot. The function will always be
run in the current R instance.
</p>
<p>Learn more in <code>vignette("tool-calling")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tool(.fun, .description, ..., .name = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tool_+3A_.fun">.fun</code></td>
<td>
<p>The function to be invoked when the tool is called.</p>
</td></tr>
<tr><td><code id="tool_+3A_.description">.description</code></td>
<td>
<p>A detailed description of what the function does.
Generally, the more information that you can provide here, the better.</p>
</td></tr>
<tr><td><code id="tool_+3A_...">...</code></td>
<td>
<p>Name-type pairs that define the arguments accepted by the
function. Each element should be created by a <code><a href="#topic+type_boolean">type_*()</a></code>
function.</p>
</td></tr>
<tr><td><code id="tool_+3A_.name">.name</code></td>
<td>
<p>The name of the function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An S7 <code>ToolDef</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# First define the metadata that the model uses to figure out when to
# call the tool
tool_rnorm &lt;- tool(
  rnorm,
  "Drawn numbers from a random normal distribution",
  n = type_integer("The number of observations. Must be a positive integer."),
  mean = type_number("The mean value of the distribution."),
  sd = type_number("The standard deviation of the distribution. Must be a non-negative number.")
)
chat &lt;- chat_openai()
# Then register it
chat$register_tool(tool_rnorm)

# Then ask a question that needs it.
chat$chat("
  Give me five numbers from a random normal distribution.
")

# Look at the chat history to see how tool calling works:
# Assistant sends a tool request which is evaluated locally and
# results are send back in a tool result.

</code></pre>

<hr>
<h2 id='Turn'>A user or assistant turn</h2><span id='topic+Turn'></span>

<h3>Description</h3>

<p>Every conversation with a chatbot consists of pairs of user and assistant
turns, corresponding to an HTTP request and response. These turns are
represented by the <code>Turn</code> object, which contains a list of <a href="#topic+Content">Content</a>s representing
the individual messages within the turn. These might be text, images, tool
requests (assistant only), or tool responses (user only).
</p>
<p>Note that a call to <code style="white-space: pre;">&#8288;$chat()&#8288;</code> and related functions may result in multiple
user-assistant turn cycles. For example, if you have registered tools,
ellmer will automatically handle the tool calling loop, which may result in
any number of additional cycles. Learn more about tool calling in
<code>vignette("tool-calling")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Turn(role, contents = list(), json = list(), tokens = c(0, 0))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Turn_+3A_role">role</code></td>
<td>
<p>Either &quot;user&quot;, &quot;assistant&quot;, or &quot;system&quot;.</p>
</td></tr>
<tr><td><code id="Turn_+3A_contents">contents</code></td>
<td>
<p>A list of <a href="#topic+Content">Content</a> objects.</p>
</td></tr>
<tr><td><code id="Turn_+3A_json">json</code></td>
<td>
<p>The serialized JSON corresponding to the underlying data of
the turns. Currently only provided for assistant.
</p>
<p>This is useful if there's information returned by the provider that ellmer
doesn't otherwise expose.</p>
</td></tr>
<tr><td><code id="Turn_+3A_tokens">tokens</code></td>
<td>
<p>A numeric vector of length 2 representing the number of
input and output tokens (respectively) used in this turn. Currently
only recorded for assistant turns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An S7 <code>Turn</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Turn(role = "user", contents = list(ContentText("Hello, world!")))
</code></pre>

<hr>
<h2 id='Type'>Type definitions for function calling and structured data extraction.</h2><span id='topic+Type'></span><span id='topic+TypeBasic'></span><span id='topic+TypeEnum'></span><span id='topic+TypeArray'></span><span id='topic+TypeObject'></span>

<h3>Description</h3>

<p>These S7 classes are provided for use by package devlopers who are
extending ellmer. In every day use, use <code><a href="#topic+type_boolean">type_boolean()</a></code> and friends.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TypeBasic(description = NULL, required = TRUE, type = stop("Required"))

TypeEnum(description = NULL, required = TRUE, values = character(0))

TypeArray(description = NULL, required = TRUE, items = Type())

TypeObject(
  description = NULL,
  required = TRUE,
  properties = list(),
  additional_properties = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Type_+3A_description">description</code></td>
<td>
<p>The purpose of the component. This is
used by the LLM to determine what values to pass to the tool or what
values to extract in the structured data, so the more detail that you can
provide here, the better.</p>
</td></tr>
<tr><td><code id="Type_+3A_required">required</code></td>
<td>
<p>Is the component required? If <code>FALSE</code>, and the
component does not exist in the data, the LLM may hallucinate a value.
Only applies when the element is nested inside of a <code>type_object()</code>.</p>
</td></tr>
<tr><td><code id="Type_+3A_type">type</code></td>
<td>
<p>Basic type name. Must be one of <code>boolean</code>, <code>integer</code>,
<code>number</code>, or <code>string</code>.</p>
</td></tr>
<tr><td><code id="Type_+3A_values">values</code></td>
<td>
<p>Character vector of permitted values.</p>
</td></tr>
<tr><td><code id="Type_+3A_items">items</code></td>
<td>
<p>The type of the array items. Can be created by any of the
<code>type_</code> function.</p>
</td></tr>
<tr><td><code id="Type_+3A_properties">properties</code></td>
<td>
<p>Named list of properties stored inside the object.
Each element should be an S7 <code>Type</code> object.'</p>
</td></tr>
<tr><td><code id="Type_+3A_additional_properties">additional_properties</code></td>
<td>
<p>Can the object have arbitrary additional
properties that are not explicitly listed? Only supported by Claude.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S7 objects inheriting from <code>Type</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>TypeBasic(type = "boolean")
TypeArray(items = TypeBasic(type = "boolean"))
</code></pre>

<hr>
<h2 id='type_boolean'>Type specifications</h2><span id='topic+type_boolean'></span><span id='topic+type_integer'></span><span id='topic+type_number'></span><span id='topic+type_string'></span><span id='topic+type_enum'></span><span id='topic+type_array'></span><span id='topic+type_object'></span>

<h3>Description</h3>

<p>These functions specify object types in a way that chatbots understand and
are used for tool calling and structured data extraction. Their names are
based on the <a href="https://json-schema.org">JSON schema</a>, which is what the APIs
expect behind the scenes. The translation from R concepts to these types is
fairly straightforward.
</p>

<ul>
<li> <p><code>type_boolean()</code>, <code>type_integer()</code>, <code>type_number()</code>, and <code>type_string()</code>
each represent scalars. These are equivalent to length-1 logical,
integer, double, and character vectors (respectively).
</p>
</li>
<li> <p><code>type_enum()</code> is equivalent to a length-1 factor; it is a string that can
only take the specified values.
</p>
</li>
<li> <p><code>type_array()</code> is equivalent to a vector in R. You can use it to represent
an atomic vector: e.g. <code>type_array(items = type_boolean())</code> is equivalent
to a logical vector and <code>type_array(items = type_string())</code> is equivalent
to a character vector). You can also use it to represent a list of more
complicated types where every element is the same type (R has no base
equivalent to this), e.g. <code>type_array(items = type_array(items = type_string()))</code>
represents a list of character vectors.
</p>
</li>
<li> <p><code>type_object()</code> is equivalent to a named list in R, but where every element
must have the specified type. For example,
<code>type_object(a = type_string(), b = type_array(type_integer()))</code> is
equivalent to a list with an element called <code>a</code> that is a string and
an element called <code>b</code> that is an integer vector.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>type_boolean(description = NULL, required = TRUE)

type_integer(description = NULL, required = TRUE)

type_number(description = NULL, required = TRUE)

type_string(description = NULL, required = TRUE)

type_enum(description = NULL, values, required = TRUE)

type_array(description = NULL, items, required = TRUE)

type_object(
  .description = NULL,
  ...,
  .required = TRUE,
  .additional_properties = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="type_boolean_+3A_description">description</code>, <code id="type_boolean_+3A_.description">.description</code></td>
<td>
<p>The purpose of the component. This is
used by the LLM to determine what values to pass to the tool or what
values to extract in the structured data, so the more detail that you can
provide here, the better.</p>
</td></tr>
<tr><td><code id="type_boolean_+3A_required">required</code>, <code id="type_boolean_+3A_.required">.required</code></td>
<td>
<p>Is the component required? If <code>FALSE</code>, and the
component does not exist in the data, the LLM may hallucinate a value.
Only applies when the element is nested inside of a <code>type_object()</code>.</p>
</td></tr>
<tr><td><code id="type_boolean_+3A_values">values</code></td>
<td>
<p>Character vector of permitted values.</p>
</td></tr>
<tr><td><code id="type_boolean_+3A_items">items</code></td>
<td>
<p>The type of the array items. Can be created by any of the
<code>type_</code> function.</p>
</td></tr>
<tr><td><code id="type_boolean_+3A_...">...</code></td>
<td>
<p>Name-type pairs defineing the components that the object must
possess.</p>
</td></tr>
<tr><td><code id="type_boolean_+3A_.additional_properties">.additional_properties</code></td>
<td>
<p>Can the object have arbitrary additional
properties that are not explicitly listed? Only supported by Claude.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># An integer vector
type_array(items = type_integer())

# The closest equivalent to a data frame is an array of objects
type_array(items = type_object(
   x = type_boolean(),
   y = type_string(),
   z = type_number()
))

# There's no specific type for dates, but you use a string with the
# requested format in the description (it's not gauranteed that you'll
# get this format back, but you should most of the time)
type_string("The creation date, in YYYY-MM-DD format.")
type_string("The update date, in dd/mm/yyyy format.")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
