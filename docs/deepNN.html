<!DOCTYPE html><html lang="en"><head><title>Help for package deepNN</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {deepNN}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#deepNN-package'><p>deepNN</p></a></li>
<li><a href='#addGrad'><p>addGrad function</p></a></li>
<li><a href='#addList'><p>addList function</p></a></li>
<li><a href='#backprop_evaluate'><p>backprop_evaluate function</p></a></li>
<li><a href='#backpropagation_MLP'><p>backpropagation_MLP function</p></a></li>
<li><a href='#bias2list'><p>bias2list function</p></a></li>
<li><a href='#biasInit'><p>biasInit function</p></a></li>
<li><a href='#download_mnist'><p>download_mnist function</p></a></li>
<li><a href='#dropoutProbs'><p>dropoutProbs function</p></a></li>
<li><a href='#gradInit'><p>gradInit function</p></a></li>
<li><a href='#hyptan'><p>hyptan function</p></a></li>
<li><a href='#ident'><p>ident function</p></a></li>
<li><a href='#L1_regularisation'><p>L1_regularisation function</p></a></li>
<li><a href='#L2_regularisation'><p>L2_regularisation function</p></a></li>
<li><a href='#logistic'><p>logistic function</p></a></li>
<li><a href='#memInit'><p>memInit function</p></a></li>
<li><a href='#MLP_net'><p>MLP_net function</p></a></li>
<li><a href='#multinomial'><p>multinomial function</p></a></li>
<li><a href='#nbiaspar'><p>nbiaspar function</p></a></li>
<li><a href='#network'><p>network function</p></a></li>
<li><a href='#nnetpar'><p>nnetpar function</p></a></li>
<li><a href='#NNgrad_test'><p>NNgrad_test function</p></a></li>
<li><a href='#NNpredict'><p>NNpredict function</p></a></li>
<li><a href='#NNpredict.regression'><p>NNpredict.regression function</p></a></li>
<li><a href='#no_regularisation'><p>no_regularisation function</p></a></li>
<li><a href='#Qloss'><p>Qloss function</p></a></li>
<li><a href='#ReLU'><p>ReLU function</p></a></li>
<li><a href='#smoothReLU'><p>smoothReLU function</p></a></li>
<li><a href='#softmax'><p>softmax function</p></a></li>
<li><a href='#stopping'><p>stopping function</p></a></li>
<li><a href='#stopping.default'><p>stopping.default function</p></a></li>
<li><a href='#stopping.maxit'><p>stopping.maxit function</p></a></li>
<li><a href='#stopping.revdir'><p>stopping.revdir function</p></a></li>
<li><a href='#train'><p>train function</p></a></li>
<li><a href='#updateStopping'><p>updateStopping function</p></a></li>
<li><a href='#updateStopping.classification'><p>updateStopping.classification function</p></a></li>
<li><a href='#updateStopping.regression'><p>updateStopping.regression function</p></a></li>
<li><a href='#weights2list'><p>weights2list function</p></a></li>
<li><a href='#wmultinomial'><p>wmultinomial function</p></a></li>
<li><a href='#wQloss'><p>wQloss function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Deep Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of some Deep Learning methods. Includes multilayer perceptron, different activation functions, regularisation strategies, stochastic gradient descent and dropout. Thanks go to the following references for helping to inspire and develop the package: Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach (2016, ISBN:978-0262035613) Deep Learning. Terrence J. Sejnowski (2018, ISBN:978-0262038034) The Deep Learning Revolution. Grant Sanderson (3brown1blue) <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a> Neural Networks YouTube playlist. Michael A. Nielsen <a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a> Neural Networks and Deep Learning.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, graphics, utils, Matrix, methods</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-25 12:54:23 UTC; ben</td>
</tr>
<tr>
<td>Author:</td>
<td>Benjamin Taylor [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Benjamin Taylor &lt;benjamin.taylor.software@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-25 13:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='deepNN-package'>deepNN</h2><span id='topic+deepNN-package'></span><span id='topic+deepNN'></span>

<h3>Description</h3>

<p>Teaching resources (yet to be added) and implementation of some Deep Learning methods. Includes multilayer perceptron, different activation functions, regularisation strategies, stochastic gradient descent and dropout.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepNN
</code></pre>


<h3>Format</h3>

<p>An object of class <code>logical</code> of length 1.
</p>


<h3>Details</h3>

<p>sectionDependencies
The package <code>deepNN</code> depends upon some other important contributions to CRAN in order to operate; their uses here are indicated:<br /><br />
stats, graphics.

</p>
<p>sectionCitation
deepNN: Deep Learning. Benjamin M. Taylor

</p>
<p>references
Thanks go to the following references for helping to inspire and develop the package: Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach
(2016, ISBN:978-0262035613) Deep Learning. Terrence J. Sejnowski (2018, ISBN:978-0262038034) The Deep Learning Revolution. Grant Sanderson
(3brown1blue) &lt;https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&gt; Neural Networks YouTube playlist. Michael A. Nielsen
&lt;http://neuralnetworksanddeeplearning.com/&gt; Neural Networks and Deep Learning
</p>

<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>




<h3>Author(s)</h3>

<p>Benjamin Taylor, Department of Medicine, Lancaster University
</p>

<hr>
<h2 id='addGrad'>addGrad function</h2><span id='topic+addGrad'></span>

<h3>Description</h3>

<p>A function to add two gradients together, gradients expressed as nested lists.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addGrad(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="addGrad_+3A_x">x</code></td>
<td>
<p>a gradient list object, as used in network training via backpropagation</p>
</td></tr>
<tr><td><code id="addGrad_+3A_y">y</code></td>
<td>
<p>a gradient list object, as used in network training via backpropagation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>another gradient object
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='addList'>addList function</h2><span id='topic+addList'></span>

<h3>Description</h3>

<p>A function to add two lists together
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addList(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="addList_+3A_x">x</code></td>
<td>
<p>a list</p>
</td></tr>
<tr><td><code id="addList_+3A_y">y</code></td>
<td>
<p>a list</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list, the elements of which are the sums of the elements of the arguments x and y.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='backprop_evaluate'>backprop_evaluate function</h2><span id='topic+backprop_evaluate'></span>

<h3>Description</h3>

<p>A function used by the train function in order to conduct backpropagation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backprop_evaluate(parameters, dat, truth, net, loss, batchsize, dropout)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="backprop_evaluate_+3A_parameters">parameters</code></td>
<td>
<p>network weights and bias parameters as a vector</p>
</td></tr>
<tr><td><code id="backprop_evaluate_+3A_dat">dat</code></td>
<td>
<p>the input data, a list of vectors</p>
</td></tr>
<tr><td><code id="backprop_evaluate_+3A_truth">truth</code></td>
<td>
<p>the truth, a list of vectors to compare with output from the feed-forward network</p>
</td></tr>
<tr><td><code id="backprop_evaluate_+3A_net">net</code></td>
<td>
<p>an object of class network, see ?network</p>
</td></tr>
<tr><td><code id="backprop_evaluate_+3A_loss">loss</code></td>
<td>
<p>the loss function, see ?Qloss and ?multinomial</p>
</td></tr>
<tr><td><code id="backprop_evaluate_+3A_batchsize">batchsize</code></td>
<td>
<p>optional batchsize argument for use with stochastic gradient descent</p>
</td></tr>
<tr><td><code id="backprop_evaluate_+3A_dropout">dropout</code></td>
<td>
<p>optional list of dropout probabilities ?dropoutProbs</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the derivative of the cost function with respect to each of the parameters
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='backpropagation_MLP'>backpropagation_MLP function</h2><span id='topic+backpropagation_MLP'></span>

<h3>Description</h3>

<p>A function to perform backpropagation for a multilayer perceptron.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backpropagation_MLP(MLPNet, loss, truth)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="backpropagation_MLP_+3A_mlpnet">MLPNet</code></td>
<td>
<p>output from the function MLP_net, as applied to some data with given parameters</p>
</td></tr>
<tr><td><code id="backpropagation_MLP_+3A_loss">loss</code></td>
<td>
<p>the loss function, see ?Qloss and ?multinomial</p>
</td></tr>
<tr><td><code id="backpropagation_MLP_+3A_truth">truth</code></td>
<td>
<p>the truth, a list of vectors to compare with output from the feed-forward network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object containing the cost and the gradient with respect to each of the model parameters
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='bias2list'>bias2list function</h2><span id='topic+bias2list'></span>

<h3>Description</h3>

<p>A function to convert a vector of biases into a ragged array (coded here a list of vectors)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bias2list(bias, dims)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bias2list_+3A_bias">bias</code></td>
<td>
<p>a vector of biases</p>
</td></tr>
<tr><td><code id="bias2list_+3A_dims">dims</code></td>
<td>
<p>the dimensions of the network as stored from a call to the function network, see ?network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object with appropriate structures for compatibility with the functions network, train, MLP_net and backpropagation_MLP
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='biasInit'>biasInit function</h2><span id='topic+biasInit'></span>

<h3>Description</h3>

<p>A function to inialise memory space for bias parameters. Now redundant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>biasInit(dims)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="biasInit_+3A_dims">dims</code></td>
<td>
<p>the dimensions of the network as stored from a call to the function network, see ?network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>memory space for biases
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='download_mnist'>download_mnist function</h2><span id='topic+download_mnist'></span>

<h3>Description</h3>

<p>A function to download mnist data in .RData format. File includes objects train_set, truth, test_set and test_truth
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_mnist(fn)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="download_mnist_+3A_fn">fn</code></td>
<td>
<p>the name of the file to save as</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list, the elements of which are the sums of the elements of the arguments x and y.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li>
<li><p>Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. &quot;Gradient-based learning applied to document recognition.&quot; Proceedings of the IEEE, 86(11):2278-2324, November 1998
</p>
</li>
<li><p>http://yann.lecun.com/exdb/mnist/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Don't run at R check because the file is large (23Mb)
# download_mnist("mnist.RData")


</code></pre>

<hr>
<h2 id='dropoutProbs'>dropoutProbs function</h2><span id='topic+dropoutProbs'></span>

<h3>Description</h3>

<p>A function to specify dropout for a neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dropoutProbs(input = 1, hidden = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dropoutProbs_+3A_input">input</code></td>
<td>
<p>inclusion rate for input parameters</p>
</td></tr>
<tr><td><code id="dropoutProbs_+3A_hidden">hidden</code></td>
<td>
<p>inclusion rate for hidden parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns these probabilities in an appropriate format for interaction with the network and train functions, see ?network and ?train
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='gradInit'>gradInit function</h2><span id='topic+gradInit'></span>

<h3>Description</h3>

<p>A function to initialise memory for the gradient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gradInit(dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gradInit_+3A_dim">dim</code></td>
<td>
<p>the dimensions of the network as stored from a call to the function network, see ?network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>memory space and structure for the gradient, initialised as zeros
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='hyptan'>hyptan function</h2><span id='topic+hyptan'></span>

<h3>Description</h3>

<p>A function to evaluate the hyperbolic tanget activation function, the derivative and cost derivative to be used in defining a neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hyptan()
</code></pre>


<h3>Value</h3>

<p>a list of functions used to compute the activation function, the derivative and cost derivative.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example in context

net &lt;- network( dims = c(100,50,20,2),
                activ=list(hyptan(),ReLU(),softmax()))

</code></pre>

<hr>
<h2 id='ident'>ident function</h2><span id='topic+ident'></span>

<h3>Description</h3>

<p>A function to evaluate the identity (linear) activation function, the derivative and cost derivative to be used in defining a neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ident()
</code></pre>


<h3>Value</h3>

<p>a list of functions used to compute the activation function, the derivative and cost derivative.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+softmax">softmax</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example in context

net &lt;- network( dims = c(100,50,20,2),
                activ=list(ident(),ReLU(),softmax()))

</code></pre>

<hr>
<h2 id='L1_regularisation'>L1_regularisation function</h2><span id='topic+L1_regularisation'></span>

<h3>Description</h3>

<p>A function to return the L1 regularisation strategy for a network object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>L1_regularisation(alpha)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="L1_regularisation_+3A_alpha">alpha</code></td>
<td>
<p>parameter to weight the relative contribution of the regulariser</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list containing functions to evaluate the cost modifier and grandient modifier
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+L2_regularisation">L2_regularisation</a>, <a href="#topic+no_regularisation">no_regularisation</a>
</p>

<hr>
<h2 id='L2_regularisation'>L2_regularisation function</h2><span id='topic+L2_regularisation'></span>

<h3>Description</h3>

<p>A function to return the L2 regularisation strategy for a network object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>L2_regularisation(alpha)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="L2_regularisation_+3A_alpha">alpha</code></td>
<td>
<p>parameter to weight the relative contribution of the regulariser</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list containing functions to evaluate the cost modifier and grandient modifier
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>, <a href="#topic+no_regularisation">no_regularisation</a>
</p>

<hr>
<h2 id='logistic'>logistic function</h2><span id='topic+logistic'></span>

<h3>Description</h3>

<p>A function to evaluate the logistic activation function, the derivative and cost derivative to be used in defining a neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logistic()
</code></pre>


<h3>Value</h3>

<p>a list of functions used to compute the activation function, the derivative and cost derivative.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example in context

net &lt;- network( dims = c(100,50,20,2),
                activ=list(logistic(),ReLU(),softmax()))

</code></pre>

<hr>
<h2 id='memInit'>memInit function</h2><span id='topic+memInit'></span>

<h3>Description</h3>

<p>A function to initialise memory space. Likely this will become deprecated in future versions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>memInit(dim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="memInit_+3A_dim">dim</code></td>
<td>
<p>the dimensions of the network as stored from a call to the function network, see ?network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>memory space, only really of internal use
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='MLP_net'>MLP_net function</h2><span id='topic+MLP_net'></span>

<h3>Description</h3>

<p>A function to define a multilayer perceptron and compute quantities for backpropagation, if needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MLP_net(input, weights, bias, dims, nlayers, activ, back = TRUE, regulariser)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MLP_net_+3A_input">input</code></td>
<td>
<p>input data, a list of vectors (i.e. ragged array)</p>
</td></tr>
<tr><td><code id="MLP_net_+3A_weights">weights</code></td>
<td>
<p>a list object containing weights for the forward pass, see ?weights2list</p>
</td></tr>
<tr><td><code id="MLP_net_+3A_bias">bias</code></td>
<td>
<p>a list object containing biases for the forward pass, see ?bias2list</p>
</td></tr>
<tr><td><code id="MLP_net_+3A_dims">dims</code></td>
<td>
<p>the dimensions of the network as stored from a call to the function network, see ?network</p>
</td></tr>
<tr><td><code id="MLP_net_+3A_nlayers">nlayers</code></td>
<td>
<p>number of layers as stored from a call to the function network, see ?network</p>
</td></tr>
<tr><td><code id="MLP_net_+3A_activ">activ</code></td>
<td>
<p>list of activation functions as stored from a call to the function network, see ?network</p>
</td></tr>
<tr><td><code id="MLP_net_+3A_back">back</code></td>
<td>
<p>logical, whether to compute quantities for backpropagation (set to FALSE for feed-forward use only)</p>
</td></tr>
<tr><td><code id="MLP_net_+3A_regulariser">regulariser</code></td>
<td>
<p>type of regularisation strategy to, see ?train, ?no_regularisation ?L1_regularisation, ?L2_regularisation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object containing the evaluated forward pass and also, if selected, quantities for backpropagation.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='multinomial'>multinomial function</h2><span id='topic+multinomial'></span>

<h3>Description</h3>

<p>A function to evaluate the multinomial loss function and the derivative of this function to be used when training a neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinomial()
</code></pre>


<h3>Value</h3>

<p>a list object with elements that are functions, evaluating the loss and the derivative
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+Qloss">Qloss</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>, <a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='nbiaspar'>nbiaspar function</h2><span id='topic+nbiaspar'></span>

<h3>Description</h3>

<p>A function to calculate the number of bias parameters in a neural network, see ?network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nbiaspar(net)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nbiaspar_+3A_net">net</code></td>
<td>
<p>an object of class network, see ?network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an integer, the number of bias parameters in a neural network
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
net &lt;- network( dims = c(5,10,2),
                activ=list(ReLU(),softmax()))
nbiaspar(net)

</code></pre>

<hr>
<h2 id='network'>network function</h2><span id='topic+network'></span>

<h3>Description</h3>

<p>A function to set up a neural network structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>network(dims, activ = logistic(), regulariser = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="network_+3A_dims">dims</code></td>
<td>
<p>a vector giving the dimensions of the network. The first and last elements are respectively the input and output lengths and the intermediate elements are the dimensions of the hidden layers</p>
</td></tr>
<tr><td><code id="network_+3A_activ">activ</code></td>
<td>
<p>either a single function or a list of activation functions, one each for the hidden layers and one for the output layer. See for example ?ReLU, ?softmax etc.</p>
</td></tr>
<tr><td><code id="network_+3A_regulariser">regulariser</code></td>
<td>
<p>optional regularisation strategy, see for example ?no_regularisation (the default) ?L1_regularisation, ?L2_regularisation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object with all information to train the network
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
net &lt;- network( dims = c(5,10,2),
                activ=list(ReLU(),softmax()))

net &lt;- network( dims = c(100,50,50,20),
                activ=list(ReLU(),ReLU(),softmax()),
                regulariser=L1_regularisation())

</code></pre>

<hr>
<h2 id='nnetpar'>nnetpar function</h2><span id='topic+nnetpar'></span>

<h3>Description</h3>

<p>A function to calculate the number of weight parameters in a neural network, see ?network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnetpar(net)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nnetpar_+3A_net">net</code></td>
<td>
<p>an object of class network, see ?network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an integer, the number of weight parameters in a neural network
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
net &lt;- network( dims = c(5,10,2),
                activ=list(ReLU(),softmax()))
nnetpar(net)

</code></pre>

<hr>
<h2 id='NNgrad_test'>NNgrad_test function</h2><span id='topic+NNgrad_test'></span>

<h3>Description</h3>

<p>A function to test gradient evaluation of a neural network by comparing it with central finite differencing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NNgrad_test(net, loss = Qloss(), eps = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NNgrad_test_+3A_net">net</code></td>
<td>
<p>an object of class network, see ?network</p>
</td></tr>
<tr><td><code id="NNgrad_test_+3A_loss">loss</code></td>
<td>
<p>a loss function to compute, see ?Qloss, ?multinomial</p>
</td></tr>
<tr><td><code id="NNgrad_test_+3A_eps">eps</code></td>
<td>
<p>small value used in the computation of the finite differencing. Default value is 0.00001</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the exact (computed via backpropagation) and approximate (via central finite differencing) gradients and also a plot of one
against the other.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
net &lt;- network( dims = c(5,10,2),
                activ=list(ReLU(),softmax()))
NNgrad_test(net)

</code></pre>

<hr>
<h2 id='NNpredict'>NNpredict function</h2><span id='topic+NNpredict'></span>

<h3>Description</h3>

<p>A function to produce predictions from a trained network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NNpredict(
  net,
  param,
  newdata,
  newtruth = NULL,
  freq = 1000,
  record = FALSE,
  plot = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NNpredict_+3A_net">net</code></td>
<td>
<p>an object of class network, see ?network</p>
</td></tr>
<tr><td><code id="NNpredict_+3A_param">param</code></td>
<td>
<p>vector of trained parameters from the network, see ?train</p>
</td></tr>
<tr><td><code id="NNpredict_+3A_newdata">newdata</code></td>
<td>
<p>input data to be predicted, a list of vectors (i.e. ragged array)</p>
</td></tr>
<tr><td><code id="NNpredict_+3A_newtruth">newtruth</code></td>
<td>
<p>the truth, a list of vectors to compare with output from the feed-forward network</p>
</td></tr>
<tr><td><code id="NNpredict_+3A_freq">freq</code></td>
<td>
<p>frequency to print progress updates to the console, default is every 1000th training point</p>
</td></tr>
<tr><td><code id="NNpredict_+3A_record">record</code></td>
<td>
<p>logical, whether to record details of the prediction. Default is FALSE</p>
</td></tr>
<tr><td><code id="NNpredict_+3A_plot">plot</code></td>
<td>
<p>locical, whether to produce diagnostic plots. Default is FALSE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>if record is FALSE, the output of the neural network is returned. Otherwise a list of objects is returned including: rec, the predicted probabilities; err, the L1 error between truth and prediction; pred, the predicted categories based on maximum probability; pred_MC, the predicted categories based on maximum probability; truth, the object newtruth, turned into an integer class number
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+NNpredict.regression">NNpredict.regression</a>, <a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example 1 - mnist data

# See example at mnist repository under user bentaylor1 on githib

# Example 2

N &lt;- 1000
d &lt;- matrix(rnorm(5*N),ncol=5)

fun &lt;- function(x){
    lp &lt;- 2*x[2]
    pr &lt;- exp(lp) / (1 + exp(lp))
    ret &lt;- c(0,0)
    ret[1+rbinom(1,1,pr)] &lt;- 1
    return(ret)
}

d &lt;- lapply(1:N,function(i){return(d[i,])})

truth &lt;- lapply(d,fun)

net &lt;- network( dims = c(5,10,2),
                activ=list(ReLU(),softmax()))

netwts &lt;- train( dat=d,
                 truth=truth,
                 net=net,
                 eps=0.01,
                 tol=100,            # run for 100 iterations
                 batchsize=10,       # note this is not enough
                 loss=multinomial(), # for convergence
                 stopping="maxit")

pred &lt;- NNpredict(  net=net,
                    param=netwts$opt,
                    newdata=d,
                    newtruth=truth,
                    record=TRUE,
                    plot=TRUE)

</code></pre>

<hr>
<h2 id='NNpredict.regression'>NNpredict.regression function</h2><span id='topic+NNpredict.regression'></span>

<h3>Description</h3>

<p>A function to produce predictions from a trained network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NNpredict.regression(
  net,
  param,
  newdata,
  newtruth = NULL,
  freq = 1000,
  record = FALSE,
  plot = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NNpredict.regression_+3A_net">net</code></td>
<td>
<p>an object of class network, see ?network</p>
</td></tr>
<tr><td><code id="NNpredict.regression_+3A_param">param</code></td>
<td>
<p>vector of trained parameters from the network, see ?train</p>
</td></tr>
<tr><td><code id="NNpredict.regression_+3A_newdata">newdata</code></td>
<td>
<p>input data to be predicted, a list of vectors (i.e. ragged array)</p>
</td></tr>
<tr><td><code id="NNpredict.regression_+3A_newtruth">newtruth</code></td>
<td>
<p>the truth, a list of vectors to compare with output from the feed-forward network</p>
</td></tr>
<tr><td><code id="NNpredict.regression_+3A_freq">freq</code></td>
<td>
<p>frequency to print progress updates to the console, default is every 1000th training point</p>
</td></tr>
<tr><td><code id="NNpredict.regression_+3A_record">record</code></td>
<td>
<p>logical, whether to record details of the prediction. Default is FALSE</p>
</td></tr>
<tr><td><code id="NNpredict.regression_+3A_plot">plot</code></td>
<td>
<p>locical, whether to produce diagnostic plots. Default is FALSE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>if record is FALSE, the output of the neural network is returned. Otherwise a list of objects is returned including: rec, the predicted probabilities; err, the L1 error between truth and prediction; pred, the predicted categories based on maximum probability; pred_MC, the predicted categories based on maximum probability; truth, the object newtruth, turned into an integer class number
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+NNpredict">NNpredict</a>, <a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='no_regularisation'>no_regularisation function</h2><span id='topic+no_regularisation'></span>

<h3>Description</h3>

<p>A function to return the no regularisation strategy for a network object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>no_regularisation()
</code></pre>


<h3>Value</h3>

<p>list containing functions to evaluate the cost modifier and grandient modifier
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>, <a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='Qloss'>Qloss function</h2><span id='topic+Qloss'></span>

<h3>Description</h3>

<p>A function to evaluate the quadratic loss function and the derivative of this function to be used when training a neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Qloss()
</code></pre>


<h3>Value</h3>

<p>a list object with elements that are functions, evaluating the loss and the derivative
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+multinomial">multinomial</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>, <a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='ReLU'>ReLU function</h2><span id='topic+ReLU'></span>

<h3>Description</h3>

<p>A function to evaluate the ReLU activation function, the derivative and cost derivative to be used in defining a neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReLU()
</code></pre>


<h3>Value</h3>

<p>a list of functions used to compute the activation function, the derivative and cost derivative.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example in context

net &lt;- network( dims = c(100,50,20,2),
                activ=list(ReLU(),ReLU(),softmax()))

</code></pre>

<hr>
<h2 id='smoothReLU'>smoothReLU function</h2><span id='topic+smoothReLU'></span>

<h3>Description</h3>

<p>A function to evaluate the smooth ReLU (AKA softplus) activation function, the derivative and cost derivative to be used in defining a neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothReLU()
</code></pre>


<h3>Value</h3>

<p>a list of functions used to compute the activation function, the derivative and cost derivative.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example in context

net &lt;- network( dims = c(100,50,20,2),
                activ=list(smoothReLU(),ReLU(),softmax()))

</code></pre>

<hr>
<h2 id='softmax'>softmax function</h2><span id='topic+softmax'></span>

<h3>Description</h3>

<p>A function to evaluate the softmax activation function, the derivative and cost derivative to be used in defining a neural network. Note that at present, this unit can only be used as an output unit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softmax()
</code></pre>


<h3>Value</h3>

<p>a list of functions used to compute the activation function, the derivative and cost derivative.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example in context

net &lt;- network( dims = c(100,50,20,2),
                activ=list(logistic(),ReLU(),softmax()))

</code></pre>

<hr>
<h2 id='stopping'>stopping function</h2><span id='topic+stopping'></span>

<h3>Description</h3>

<p>Generic function for implementing stopping methods
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stopping(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stopping_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>method stopping
</p>


<h3>See Also</h3>

<p><a href="#topic+stopping.default">stopping.default</a>, <a href="#topic+stopping.maxit">stopping.maxit</a>
</p>

<hr>
<h2 id='stopping.default'>stopping.default function</h2><span id='topic+stopping.default'></span>

<h3>Description</h3>

<p>A function to halt computation when curcost &lt; tol
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
stopping(cost, curcost, count, tol, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stopping.default_+3A_cost">cost</code></td>
<td>
<p>the value of the loss function passed in</p>
</td></tr>
<tr><td><code id="stopping.default_+3A_curcost">curcost</code></td>
<td>
<p>current measure of cost, can be different to the parameter 'cost' above e.g. may consider smoothed cost over the last k iterations</p>
</td></tr>
<tr><td><code id="stopping.default_+3A_count">count</code></td>
<td>
<p>iteration count</p>
</td></tr>
<tr><td><code id="stopping.default_+3A_tol">tol</code></td>
<td>
<p>tolerance, or limit</p>
</td></tr>
<tr><td><code id="stopping.default_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>...
</p>


<h3>See Also</h3>

<p><a href="#topic+stopping.maxit">stopping.maxit</a>
</p>

<hr>
<h2 id='stopping.maxit'>stopping.maxit function</h2><span id='topic+stopping.maxit'></span>

<h3>Description</h3>

<p>A function to halt computation when the number of iterations reaches a given threshold, tol
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxit'
stopping(cost, curcost, count, tol, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stopping.maxit_+3A_cost">cost</code></td>
<td>
<p>the value of the loss function passed in</p>
</td></tr>
<tr><td><code id="stopping.maxit_+3A_curcost">curcost</code></td>
<td>
<p>current measure of cost, can be different to the parameter 'cost' above e.g. may consider smoothed cost over the last k iterations</p>
</td></tr>
<tr><td><code id="stopping.maxit_+3A_count">count</code></td>
<td>
<p>iteration count</p>
</td></tr>
<tr><td><code id="stopping.maxit_+3A_tol">tol</code></td>
<td>
<p>tolerance, or limit</p>
</td></tr>
<tr><td><code id="stopping.maxit_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>...
</p>

<hr>
<h2 id='stopping.revdir'>stopping.revdir function</h2><span id='topic+stopping.revdir'></span>

<h3>Description</h3>

<p>A function to halt computation when curcost &gt; tol
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'revdir'
stopping(cost, curcost, count, tol, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stopping.revdir_+3A_cost">cost</code></td>
<td>
<p>the value of the loss function passed in</p>
</td></tr>
<tr><td><code id="stopping.revdir_+3A_curcost">curcost</code></td>
<td>
<p>current measure of cost, can be different to the parameter 'cost' above e.g. may consider smoothed cost over the last k iterations</p>
</td></tr>
<tr><td><code id="stopping.revdir_+3A_count">count</code></td>
<td>
<p>iteration count</p>
</td></tr>
<tr><td><code id="stopping.revdir_+3A_tol">tol</code></td>
<td>
<p>tolerance, or limit</p>
</td></tr>
<tr><td><code id="stopping.revdir_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>...
</p>


<h3>See Also</h3>

<p><a href="#topic+stopping.maxit">stopping.maxit</a>
</p>

<hr>
<h2 id='train'>train function</h2><span id='topic+train'></span>

<h3>Description</h3>

<p>A function to train a neural network defined using the network function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train(
  dat,
  truth,
  net,
  loss = Qloss(),
  tol = 0.95,
  eps = 0.001,
  batchsize = NULL,
  dropout = dropoutProbs(),
  parinit = function(n) {
     return(runif(n, -0.01, 0.01))
 },
  monitor = TRUE,
  stopping = "default",
  update = "classification"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="train_+3A_dat">dat</code></td>
<td>
<p>the input data, a list of vectors</p>
</td></tr>
<tr><td><code id="train_+3A_truth">truth</code></td>
<td>
<p>the truth, a list of vectors to compare with output from the feed-forward network</p>
</td></tr>
<tr><td><code id="train_+3A_net">net</code></td>
<td>
<p>an object of class network, see ?network</p>
</td></tr>
<tr><td><code id="train_+3A_loss">loss</code></td>
<td>
<p>the loss function, see ?Qloss and ?multinomial</p>
</td></tr>
<tr><td><code id="train_+3A_tol">tol</code></td>
<td>
<p>stopping criteria for training. Current method monitors the quality of randomly chosen predictions from the data,
terminates when the mean predictive probabilities of the last 20 randomly chosen points exceeds tol, default is 0.95</p>
</td></tr>
<tr><td><code id="train_+3A_eps">eps</code></td>
<td>
<p>stepsize scaling constant in gradient descent, or stochastic gradient descent</p>
</td></tr>
<tr><td><code id="train_+3A_batchsize">batchsize</code></td>
<td>
<p>size of minibatches to be used with stochastic gradient descent</p>
</td></tr>
<tr><td><code id="train_+3A_dropout">dropout</code></td>
<td>
<p>optional list of dropout probabilities ?dropoutProbs</p>
</td></tr>
<tr><td><code id="train_+3A_parinit">parinit</code></td>
<td>
<p>a function of a single parameter returning the initial distribution of the weights, default is uniform on (-0.01,0.01)</p>
</td></tr>
<tr><td><code id="train_+3A_monitor">monitor</code></td>
<td>
<p>logical, whether to produce learning/convergence diagnostic plots</p>
</td></tr>
<tr><td><code id="train_+3A_stopping">stopping</code></td>
<td>
<p>method for stopping computation default, 'default', calls the function stopping.default</p>
</td></tr>
<tr><td><code id="train_+3A_update">update</code></td>
<td>
<p>and default for meth is 'classification', which calls updateStopping.classification</p>
</td></tr>
</table>


<h3>Value</h3>

<p>optimal cost and parameters from the trained network; at present, diagnostic plots are produced illustrating the parameters
of the model, the gradient and stopping criteria trace.
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example 1 - mnist data

# See example at mnist repository under user bentaylor1 on githib

# Example 2

N &lt;- 1000
d &lt;- matrix(rnorm(5*N),ncol=5)

fun &lt;- function(x){
    lp &lt;- 2*x[2]
    pr &lt;- exp(lp) / (1 + exp(lp))
    ret &lt;- c(0,0)
    ret[1+rbinom(1,1,pr)] &lt;- 1
    return(ret)
}

d &lt;- lapply(1:N,function(i){return(d[i,])})

truth &lt;- lapply(d,fun)

net &lt;- network( dims = c(5,10,2),
                activ=list(ReLU(),softmax()))

netwts &lt;- train( dat=d,
                 truth=truth,
                 net=net,
                 eps=0.01,
                 tol=100,            # run for 100 iterations
                 batchsize=10,       # note this is not enough
                 loss=multinomial(), # for convergence
                 stopping="maxit")

pred &lt;- NNpredict(  net=net,
                    param=netwts$opt,
                    newdata=d,
                    newtruth=truth,
                    record=TRUE,
                    plot=TRUE)

</code></pre>

<hr>
<h2 id='updateStopping'>updateStopping function</h2><span id='topic+updateStopping'></span>

<h3>Description</h3>

<p>Generic function for updating stopping criteria
</p>


<h3>Usage</h3>

<pre><code class='language-R'>updateStopping(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="updateStopping_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>method updateStopping
</p>


<h3>See Also</h3>

<p><a href="#topic+updateStopping.classification">updateStopping.classification</a>, <a href="#topic+updateStopping.regression">updateStopping.regression</a>
</p>

<hr>
<h2 id='updateStopping.classification'>updateStopping.classification function</h2><span id='topic+updateStopping.classification'></span>

<h3>Description</h3>

<p>A function to update the stopping criteria for a classification problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'classification'
updateStopping(
  dat,
  parms,
  net,
  truth,
  testoutput,
  count,
  monitor,
  mx,
  curcost,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="updateStopping.classification_+3A_dat">dat</code></td>
<td>
<p>data object</p>
</td></tr>
<tr><td><code id="updateStopping.classification_+3A_parms">parms</code></td>
<td>
<p>model parameters</p>
</td></tr>
<tr><td><code id="updateStopping.classification_+3A_net">net</code></td>
<td>
<p>an object of class network</p>
</td></tr>
<tr><td><code id="updateStopping.classification_+3A_truth">truth</code></td>
<td>
<p>the truth, to be compared with network outputs</p>
</td></tr>
<tr><td><code id="updateStopping.classification_+3A_testoutput">testoutput</code></td>
<td>
<p>a vector, the history of the stopping criteria</p>
</td></tr>
<tr><td><code id="updateStopping.classification_+3A_count">count</code></td>
<td>
<p>iteration number</p>
</td></tr>
<tr><td><code id="updateStopping.classification_+3A_monitor">monitor</code></td>
<td>
<p>logical, whether to produce a diagnostic plot</p>
</td></tr>
<tr><td><code id="updateStopping.classification_+3A_mx">mx</code></td>
<td>
<p>a number to be monitored e.g. the cost of the best performing paramerer configuration to date</p>
</td></tr>
<tr><td><code id="updateStopping.classification_+3A_curcost">curcost</code></td>
<td>
<p>current measure of cost, can be different to the value of the loss function e.g. may consider smoothed cost (i.e. loss) over the last k iterations</p>
</td></tr>
<tr><td><code id="updateStopping.classification_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>curcost, testoutput and mx, used for iterating the maximisation process
</p>

<hr>
<h2 id='updateStopping.regression'>updateStopping.regression function</h2><span id='topic+updateStopping.regression'></span>

<h3>Description</h3>

<p>A function to update the stopping criteria for a classification problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'regression'
updateStopping(
  dat,
  parms,
  net,
  truth,
  testoutput,
  count,
  monitor,
  mx,
  curcost,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="updateStopping.regression_+3A_dat">dat</code></td>
<td>
<p>data object</p>
</td></tr>
<tr><td><code id="updateStopping.regression_+3A_parms">parms</code></td>
<td>
<p>model parameters</p>
</td></tr>
<tr><td><code id="updateStopping.regression_+3A_net">net</code></td>
<td>
<p>an object of class network</p>
</td></tr>
<tr><td><code id="updateStopping.regression_+3A_truth">truth</code></td>
<td>
<p>the truth, to be compared with network outputs</p>
</td></tr>
<tr><td><code id="updateStopping.regression_+3A_testoutput">testoutput</code></td>
<td>
<p>a vector, the history of the stopping criteria</p>
</td></tr>
<tr><td><code id="updateStopping.regression_+3A_count">count</code></td>
<td>
<p>iteration number</p>
</td></tr>
<tr><td><code id="updateStopping.regression_+3A_monitor">monitor</code></td>
<td>
<p>logical, whether to produce a diagnostic plot</p>
</td></tr>
<tr><td><code id="updateStopping.regression_+3A_mx">mx</code></td>
<td>
<p>a number to be monitored e.g. the cost of the best performing paramerer configuration to date</p>
</td></tr>
<tr><td><code id="updateStopping.regression_+3A_curcost">curcost</code></td>
<td>
<p>current measure of cost, can be different to the value of the loss function e.g. may consider smoothed cost (i.e. loss) over the last k iterations</p>
</td></tr>
<tr><td><code id="updateStopping.regression_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>curcost, testoutput and mx, used for iterating the maximisation process
</p>

<hr>
<h2 id='weights2list'>weights2list function</h2><span id='topic+weights2list'></span>

<h3>Description</h3>

<p>A function to convert a vector of weights into a ragged array (coded here a list of vectors)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weights2list(weights, dims)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weights2list_+3A_weights">weights</code></td>
<td>
<p>a vector of weights</p>
</td></tr>
<tr><td><code id="weights2list_+3A_dims">dims</code></td>
<td>
<p>the dimensions of the network as stored from a call to the function network, see ?network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object with appropriate structures for compatibility with the functions network, train, MLP_net and backpropagation_MLP
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+logistic">logistic</a>, <a href="#topic+ReLU">ReLU</a>, <a href="#topic+smoothReLU">smoothReLU</a>, <a href="#topic+ident">ident</a>, <a href="#topic+softmax">softmax</a>, <a href="#topic+Qloss">Qloss</a>, <a href="#topic+multinomial">multinomial</a>,
<a href="#topic+NNgrad_test">NNgrad_test</a>, <a href="#topic+weights2list">weights2list</a>, <a href="#topic+bias2list">bias2list</a>, <a href="#topic+biasInit">biasInit</a>, <a href="#topic+memInit">memInit</a>, <a href="#topic+gradInit">gradInit</a>,
<a href="#topic+addGrad">addGrad</a>, <a href="#topic+nnetpar">nnetpar</a>, <a href="#topic+nbiaspar">nbiaspar</a>, <a href="#topic+addList">addList</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>,
<a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='wmultinomial'>wmultinomial function</h2><span id='topic+wmultinomial'></span>

<h3>Description</h3>

<p>A function to evaluate the weighted multinomial loss function and the derivative of this function
to be used when training a neural network. This is eqivalent to a multinomial cost function
employing a Dirichlet prior on the probabilities. Its effect is to regularise the estimation so
that in the case where we apriori expect more of one particular category compared to another
then this can be included in the objective.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wmultinomial(w, batchsize)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wmultinomial_+3A_w">w</code></td>
<td>
<p>a vector of weights, adding up whose length is equal to the output length of the net</p>
</td></tr>
<tr><td><code id="wmultinomial_+3A_batchsize">batchsize</code></td>
<td>
<p>of batch used in inference WARNING: ensure this matches with actual batchsize used!</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object with elements that are functions, evaluating the loss and the derivative
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+Qloss">Qloss</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>, <a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

<hr>
<h2 id='wQloss'>wQloss function</h2><span id='topic+wQloss'></span>

<h3>Description</h3>

<p>A function to evaluate the weighted quadratic loss function and the derivative of this function to be used when training a neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wQloss(w)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wQloss_+3A_w">w</code></td>
<td>
<p>a vector of weights, adding up to 1, whose length is equalt to the output length of the net</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object with elements that are functions, evaluating the loss and the derivative
</p>


<h3>References</h3>


<ol>
<li><p> Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach. Deep Learning. (2016)
</p>
</li>
<li><p> Terrence J. Sejnowski. The Deep Learning Revolution (The MIT Press). (2018)
</p>
</li>
<li><p> Neural Networks YouTube playlist by 3brown1blue: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>
</p>
</li>
<li><p>http://neuralnetworksanddeeplearning.com/
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+network">network</a>, <a href="#topic+train">train</a>, <a href="#topic+backprop_evaluate">backprop_evaluate</a>, <a href="#topic+MLP_net">MLP_net</a>, <a href="#topic+backpropagation_MLP">backpropagation_MLP</a>,
<a href="#topic+multinomial">multinomial</a>, <a href="#topic+no_regularisation">no_regularisation</a>, <a href="#topic+L1_regularisation">L1_regularisation</a>, <a href="#topic+L2_regularisation">L2_regularisation</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
