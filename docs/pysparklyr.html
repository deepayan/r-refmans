<!DOCTYPE html><html lang="en"><head><title>Help for package pysparklyr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pysparklyr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#connection_databricks_shinyapp'><p>A Shiny app that can be used to construct a <code>spark_connect</code> statement</p></a></li>
<li><a href='#deploy_databricks'><p>Deploys Databricks backed content to publishing server</p></a></li>
<li><a href='#install_pyspark'><p>Installs PySpark and Python dependencies</p></a></li>
<li><a href='#installed_components'><p>Lists installed Python libraries</p></a></li>
<li><a href='#ml_prepare_dataset'><p>Creates the 'label' and 'features' columns</p></a></li>
<li><a href='#requirements_write'><p>Writes the 'requirements.txt' file, containing the needed Python libraries</p></a></li>
<li><a href='#spark_connect_service_start'><p>Starts and stops Spark Connect locally</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Provides a 'PySpark' Back-End for the 'sparklyr' Package</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.6</td>
</tr>
<tr>
<td>Description:</td>
<td>It enables 'sparklyr' to integrate with 'Spark Connect', and
    'Databricks Connect' by providing a wrapper over the 'PySpark' 'python'
    library.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>arrow, cli, DBI, dplyr, dbplyr, glue, purrr, reticulate (&ge;
1.38), methods, rlang, sparklyr (&ge; 1.8.5), tidyselect, fs,
magrittr, tidyr, vctrs, processx, httr2, rstudioapi, rsconnect</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mlverse/pysparklyr">https://github.com/mlverse/pysparklyr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlverse/pysparklyr/issues">https://github.com/mlverse/pysparklyr/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>crayon, R6, testthat (&ge; 3.0.0), tibble, withr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-14 14:34:40 UTC; edgar</td>
</tr>
<tr>
<td>Author:</td>
<td>Edgar Ruiz [aut, cre],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Edgar Ruiz &lt;edgar@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-14 14:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of calling <code>rhs(lhs)</code>.
</p>

<hr>
<h2 id='connection_databricks_shinyapp'>A Shiny app that can be used to construct a <code>spark_connect</code> statement</h2><span id='topic+connection_databricks_shinyapp'></span>

<h3>Description</h3>

<p>A Shiny app that can be used to construct a <code>spark_connect</code> statement
</p>


<h3>Usage</h3>

<pre><code class='language-R'>connection_databricks_shinyapp()
</code></pre>


<h3>Value</h3>

<p>A Shiny app
</p>

<hr>
<h2 id='deploy_databricks'>Deploys Databricks backed content to publishing server</h2><span id='topic+deploy_databricks'></span>

<h3>Description</h3>

<p>This is a convenience function that is meant to make it easier for
you to publish your Databricks backed content to a publishing server. It is
meant to be primarily used with Posit Connect.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deploy_databricks(
  appDir = NULL,
  python = NULL,
  account = NULL,
  server = NULL,
  lint = FALSE,
  forceGeneratePythonEnvironment = TRUE,
  version = NULL,
  cluster_id = NULL,
  host = NULL,
  token = NULL,
  confirm = interactive(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="deploy_databricks_+3A_appdir">appDir</code></td>
<td>
<p>A directory containing an application (e.g. a Shiny app or plumber API)
Defaults to NULL. If left NULL, and if called within RStudio, it will attempt
to use the folder of the currently opened document within the IDE. If there are
no opened documents, or not working in the RStudio IDE, then it will use
<code>getwd()</code> as the default value.</p>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_python">python</code></td>
<td>
<p>Full path to a python binary for use by <code>reticulate.</code> It defaults to NULL.
If left NULL, this function will attempt to find a viable local Python
environment to replicate using the following hierarchy:
</p>

<ol>
<li> <p><code>version</code> - Cluster's DBR version
</p>
</li>
<li> <p><code>cluster_id</code> - Query the cluster to obtain its DBR version
</p>
</li>
<li><p> If one is loaded in the current R session, it will verify that the Python
environment is suited to be used as the one to use
</p>
</li></ol>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_account">account</code></td>
<td>
<p>The name of the account to use to publish</p>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_server">server</code></td>
<td>
<p>The name of the target server to publish</p>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_lint">lint</code></td>
<td>
<p>Lint the project before initiating the project? Default to FALSE.
It has been causing issues for this type of content.</p>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_forcegeneratepythonenvironment">forceGeneratePythonEnvironment</code></td>
<td>
<p>If an existing requirements.txt file is found,
it will be overwritten when this argument is TRUE.</p>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_version">version</code></td>
<td>
<p>The Databricks Runtime (DBR) version. Use if <code>python</code> is NULL.</p>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_cluster_id">cluster_id</code></td>
<td>
<p>The Databricks cluster ID. Use if <code>python</code>, and <code>version</code> are
NULL</p>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_host">host</code></td>
<td>
<p>The Databricks host URL. Defaults to NULL. If left NULL, it will
use the environment variable <code>DATABRICKS_HOST</code></p>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_token">token</code></td>
<td>
<p>The Databricks authentication token. Defaults to NULL. If left NULL, it will
use the environment variable <code>DATABRICKS_TOKEN</code></p>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_confirm">confirm</code></td>
<td>
<p>Should the user be prompted to confirm that the correct
information is being used for deployment? Defaults to <code>interactive()</code></p>
</td></tr>
<tr><td><code id="deploy_databricks_+3A_...">...</code></td>
<td>
<p>Additional named arguments passed to <code>rsconnect::deployApp()</code> function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No value is returned to R. Only output to the console.
</p>

<hr>
<h2 id='install_pyspark'>Installs PySpark and Python dependencies</h2><span id='topic+install_pyspark'></span><span id='topic+install_databricks'></span>

<h3>Description</h3>

<p>Installs PySpark and Python dependencies
</p>
<p>Installs Databricks Connect and Python dependencies
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_pyspark(
  version = NULL,
  envname = NULL,
  python_version = NULL,
  new_env = TRUE,
  method = c("auto", "virtualenv", "conda"),
  as_job = TRUE,
  install_ml = FALSE,
  ...
)

install_databricks(
  version = NULL,
  cluster_id = NULL,
  envname = NULL,
  python_version = NULL,
  new_env = TRUE,
  method = c("auto", "virtualenv", "conda"),
  as_job = TRUE,
  install_ml = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="install_pyspark_+3A_version">version</code></td>
<td>
<p>Version of 'databricks.connect' to install. Defaults to <code>NULL</code>.
If <code>NULL</code>, it will check against PyPi to get the current library version.</p>
</td></tr>
<tr><td><code id="install_pyspark_+3A_envname">envname</code></td>
<td>
<p>The name of the Python Environment to use to install the
Python libraries. Defaults to <code>NULL.</code> If <code>NULL</code>, a name will automatically
be assigned based on the version that will be installed</p>
</td></tr>
<tr><td><code id="install_pyspark_+3A_python_version">python_version</code></td>
<td>
<p>The minimum required version of Python to use to create
the Python environment. Defaults to <code>NULL</code>. If <code>NULL</code>, it will check against
PyPi to get the minimum required Python version.</p>
</td></tr>
<tr><td><code id="install_pyspark_+3A_new_env">new_env</code></td>
<td>
<p>If <code>TRUE</code>, any existing Python virtual environment and/or
Conda environment specified by <code>envname</code> is deleted first.</p>
</td></tr>
<tr><td><code id="install_pyspark_+3A_method">method</code></td>
<td>
<p>The installation method to use. If creating a new environment,
<code>"auto"</code> (the default) is equivalent to <code>"virtualenv"</code>. Otherwise <code>"auto"</code>
infers the installation method based on the type of Python environment
specified by <code>envname</code>.</p>
</td></tr>
<tr><td><code id="install_pyspark_+3A_as_job">as_job</code></td>
<td>
<p>Runs the installation if using this function within the
RStudio IDE.</p>
</td></tr>
<tr><td><code id="install_pyspark_+3A_install_ml">install_ml</code></td>
<td>
<p>Installs ML related Python libraries. Defaults to TRUE. This
is mainly for machines with limited storage to avoid installing the rather
large 'torch' library if the ML features are not going to be used. This will
apply to any environment backed by 'Spark' version 3.5 or above.</p>
</td></tr>
<tr><td><code id="install_pyspark_+3A_...">...</code></td>
<td>
<p>Passed on to <code><a href="reticulate.html#topic+py_install">reticulate::py_install()</a></code></p>
</td></tr>
<tr><td><code id="install_pyspark_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Target of the cluster ID that will be used with.
If provided, this value will be used to extract the cluster's
version</p>
</td></tr>
</table>


<h3>Value</h3>

<p>It returns no value to the R session. This function purpose is to
create the 'Python' environment, and install the appropriate set of 'Python'
libraries inside the new environment. During runtime, this function will send
messages to the console describing the steps that the function is
taking. For example, it will let the user know if it is getting the latest
version of the Python library from 'PyPi.org', and the result of such
query.
</p>

<hr>
<h2 id='installed_components'>Lists installed Python libraries</h2><span id='topic+installed_components'></span>

<h3>Description</h3>

<p>Lists installed Python libraries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>installed_components(list_all = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="installed_components_+3A_list_all">list_all</code></td>
<td>
<p>Flag that indicates to display all of the installed packages
or only the top two, namely, <code>pyspark</code> and <code>databricks.connect</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns no value, only sends information to the console. The
information includes the current versions of 'sparklyr', and 'pysparklyr',
as well as the 'Python' environment currently loaded.
</p>

<hr>
<h2 id='ml_prepare_dataset'>Creates the 'label' and 'features' columns</h2><span id='topic+ml_prepare_dataset'></span>

<h3>Description</h3>

<p>Creates the 'label' and 'features' columns
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ml_prepare_dataset(
  x,
  formula = NULL,
  label = NULL,
  features = NULL,
  label_col = "label",
  features_col = "features",
  keep_original = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ml_prepare_dataset_+3A_x">x</code></td>
<td>
<p>A <code>tbl_pyspark</code> object</p>
</td></tr>
<tr><td><code id="ml_prepare_dataset_+3A_formula">formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula.</p>
</td></tr>
<tr><td><code id="ml_prepare_dataset_+3A_label">label</code></td>
<td>
<p>The name of the label column.</p>
</td></tr>
<tr><td><code id="ml_prepare_dataset_+3A_features">features</code></td>
<td>
<p>The name(s) of the feature columns as a character vector.</p>
</td></tr>
<tr><td><code id="ml_prepare_dataset_+3A_label_col">label_col</code></td>
<td>
<p>Label column name, as a length-one character vector.</p>
</td></tr>
<tr><td><code id="ml_prepare_dataset_+3A_features_col">features_col</code></td>
<td>
<p>Features column name, as a length-one character vector.</p>
</td></tr>
<tr><td><code id="ml_prepare_dataset_+3A_keep_original">keep_original</code></td>
<td>
<p>Boolean flag that indicates if the output will contain,
or not, the original columns from <code>x</code>. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="ml_prepare_dataset_+3A_...">...</code></td>
<td>
<p>Added for backwards compatibility. Not in use today.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>At this time, 'Spark ML Connect', does not include a Vector Assembler
transformer. The main thing that this function does, is create a 'Pyspark'
array column. Pipelines require a 'label' and 'features' columns. Even though
it is is single column in the dataset, the 'features' column will contain all
of the predictors insde an array. This function also creates a new 'label'
column that copies the outcome variable. This makes it a lot easier to remove
the 'label', and 'outcome' columns.
</p>


<h3>Value</h3>

<p>A <code>tbl_pyspark</code>, with either the original columns from <code>x</code>, plus the
'label' and 'features' column, or, the 'label' and 'features' columns only.
</p>

<hr>
<h2 id='requirements_write'>Writes the 'requirements.txt' file, containing the needed Python libraries</h2><span id='topic+requirements_write'></span>

<h3>Description</h3>

<p>This is a helper function that it is meant to be used for deployments
of the document or application. By default, <code>deploy_databricks()</code> will run this
function the first time you use that function to deploy content to Posit Connect.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>requirements_write(
  envname = NULL,
  destfile = "requirements.txt",
  overwrite = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="requirements_write_+3A_envname">envname</code></td>
<td>
<p>The name of, or path to, a Python virtual environment.</p>
</td></tr>
<tr><td><code id="requirements_write_+3A_destfile">destfile</code></td>
<td>
<p>Target path for the requirements file. Defaults to 'requirements.txt'.</p>
</td></tr>
<tr><td><code id="requirements_write_+3A_overwrite">overwrite</code></td>
<td>
<p>Replace the contents of the file if it already exists?</p>
</td></tr>
<tr><td><code id="requirements_write_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>reticulate::py_list_packages()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>No value is returned to R. The output is a text file with the
list of Python libraries.
</p>

<hr>
<h2 id='spark_connect_service_start'>Starts and stops Spark Connect locally</h2><span id='topic+spark_connect_service_start'></span><span id='topic+spark_connect_service_stop'></span>

<h3>Description</h3>

<p>Starts and stops Spark Connect locally
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_connect_service_start(
  version = "3.5",
  scala_version = "2.12",
  include_args = TRUE,
  ...
)

spark_connect_service_stop(version = "3.5", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_connect_service_start_+3A_version">version</code></td>
<td>
<p>Spark version to use (3.4 or above)</p>
</td></tr>
<tr><td><code id="spark_connect_service_start_+3A_scala_version">scala_version</code></td>
<td>
<p>Acceptable Scala version of packages to be loaded</p>
</td></tr>
<tr><td><code id="spark_connect_service_start_+3A_include_args">include_args</code></td>
<td>
<p>Flag that indicates whether to add the additional arguments
to the command that starts the service. At this time, only the 'packages'
argument is submitted.</p>
</td></tr>
<tr><td><code id="spark_connect_service_start_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>It returns messages to the console with the status of starting, and
stopping the local Spark Connect service.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
