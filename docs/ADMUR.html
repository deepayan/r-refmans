<!DOCTYPE html><html><head><title>Help for package ADMUR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<script type="text/javascript" src="mathjax-config.js"></script>
<script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ADMUR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ADMUR'><p>ADMUR R package</p></a></li>
<li><a href='#bluhm2421'><p>Radiocarbon dataset from Bluhm and Surovell 2018</p></a></li>
<li><a href='#bryson1848'><p>Radiocarbon dataset from Bryson et al. 2006</p></a></li>
<li><a href='#checkData'><p>Checks a dataset for obvious clangers</p></a></li>
<li><a href='#convertPars'><p>Converts parameters to x,y coordinates (date and pdf) that describe a model</p></a></li>
<li><a href='#CPLparsToHinges'><p>Converts CPL parameters (0 to 1) into hinge (x,y) coordinates (date and pdf) that describe a model</p></a></li>
<li><a href='#data1'><p>Toy radiocarbon dataset</p></a></li>
<li><a href='#data2'><p>Toy radiocarbon dataset</p></a></li>
<li><a href='#data3'><p>Toy radiocarbon dataset</p></a></li>
<li><a href='#estimateDataDomain'><p>Estimates the calendar date domain of a 14C dataset.</p></a></li>
<li><a href='#intcal13'><p>Northern hemisphere 2013 calibration curve</p></a></li>
<li><a href='#intcal20'><p>Northern hemisphere 2020 calibration curve</p></a></li>
<li><a href='#loglik'><p>Calculates the log likelihood of a population model, given a calibrated date PD matrix</p></a></li>
<li><a href='#makeCalArray'><p>Makes a calibration curve probability array</p></a></li>
<li><a href='#mcmc'><p>Makes a MCMC chain using the Metropolis-Hastings algorithm.</p></a></li>
<li><a href='#objectiveFunction'><p>Objective function to be minimised in a search. Returns the negative log likelihood.</p></a></li>
<li><a href='#phaseCalibrator'><p>Generates an SPD for each phase in a dataset</p></a></li>
<li><a href='#plotCalArray'><p>Plots a calibration curve probability array</p></a></li>
<li><a href='#plotPD'><p>Plots a calibrated PD of a single date, or SPD of multiple dates, or multiple SPDs</p></a></li>
<li><a href='#plotSimulationSummary'><p>Plots a summary of the SPD simulation test</p></a></li>
<li><a href='#relativeRate'><p>Calculates the relative growth (or decline) rate per generation.</p></a></li>
<li><a href='#SAAD'><p>Radiocarbon dataset for South American Arid Diagonal (SAAD)</p></a></li>
<li><a href='#shcal13'><p>Southern hemisphere 2013 calibration curve</p></a></li>
<li><a href='#shcal20'><p>Southern hemisphere 2020 calibration curve</p></a></li>
<li><a href='#simulateCalendarDates'><p>Converts calendar dates to 14C dates</p></a></li>
<li><a href='#sinewavePDF'><p>PDF of a truncated sinusoidal curve</p></a></li>
<li><a href='#SPDsimulationTest'><p>Goodness of Fit test, using SPD simulation</p></a></li>
<li><a href='#summedCalibrator'><p>Generates a summed probability distribution (SPD) of calibrated dates</p></a></li>
<li><a href='#summedCalibratorWrapper'><p>Quick calibration of dates, without the need to choose a date range or generate a CalArray.</p></a></li>
<li><a href='#summedPhaseCalibrator'><p>Generates a summed probability distribution (SPD) after phasing dates</p></a></li>
<li><a href='#toy'><p>Toy population model</p></a></li>
<li><a href='#uncalibrateCalendarDates'><p>Converts calendar dates to 14C dates</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.0.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-03-19</td>
</tr>
<tr>
<td>Title:</td>
<td>Ancient Demographic Modelling Using Radiocarbon</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Adrian Timpson &lt;a.timpson@ucl.ac.uk&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, grDevices, mathjaxr, stats, scales, zoo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>DEoptimR, knitr, rmarkdown</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides tools to directly model underlying population dynamics using date datasets (radiocarbon and other) with a Continuous Piecewise Linear (CPL) model framework. Various other model types included. Taphonomic loss included optionally as a power function. Model comparison framework using BIC. Package also calibrates 14C samples, generates Summed Probability Distributions (SPD), and performs SPD simulation analysis to generate a Goodness-of-fit test for the best selected model. Details about the method can be found in Timpson A., Barberena R., Thomas M. G., Mendez C., Manning K. (2020) &lt;<a href="https://doi.org/10.1098%2Frstb.2019.0723">doi:10.1098/rstb.2019.0723</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/UCL/ADMUR">https://github.com/UCL/ADMUR</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>mathjaxr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-03-19 19:26:12 UTC; adrian</td>
</tr>
<tr>
<td>Author:</td>
<td>Adrian Timpson <a href="https://orcid.org/0000-0003-0292-8729"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-03-23 11:40:11 UTC</td>
</tr>
</table>
<hr>
<h2 id='ADMUR'>ADMUR R package</h2><span id='topic+ADMUR'></span>

<h3>Description</h3>

<p>ADMUR: Ancient Demographic Modelling Using Radiocarbon
</p>
<p>Tools to directly model underlying dynamics of radiocarbon date datasets using a Continuous Piecewise Linear (CPL) model framework, or Summed Probability Distributions (SPD) within a simulation framework. 
</p>


<h3>Details</h3>

<p>Package to model population dynamics from anthropogenic datasets of dates, including radiocarbon dates and other date such as thermoluminescence. Provides tools to perform two modelling approaches:
</p>
<p>1. Summed Probability Distribution generation, and simulation testing. Permits the rejection of a null hypothesis.
</p>
<p>2. Directly models the underlying population dynamics using a Continuous Piecewise Linear (CPL) model framework. 
</p>
<p>The CPL model framework includes tools to: Estimate the dates and magnitude of demographic events (hinge points), both the Maximum likelihood (using DEoptimR search algorithm) and Credible Intervals (using MCMC Metropolis Hastings Algorithm); Perform Goodness of Fit tests; perform model comparison.
</p>


<h3>References</h3>

<p>'Directly modelling population dynamics in the South American Arid Diagonal using 14C dates' by Adrian Timpson, Ramiro Barberena, Mark G. Thomas, César Méndez and Katie Manning, published in Philosophical Transactions of the Royal Society B, 2020. https://doi.org/10.1098/rstb.2019.0723
</p>

<hr>
<h2 id='bluhm2421'>Radiocarbon dataset from Bluhm and Surovell 2018</h2><span id='topic+bluhm2421'></span>

<h3>Description</h3>

<p>The 14C dataset used by Bluhm and Surovell in 'Validation of a global model of taphonomic bias using geologic radiocarbon', Quaternary Research 2018.
Data provided from <a href="https://www.cambridge.org/core/journals/quaternary-research/article/abs/validation-of-a-global-model-of-taphonomic-bias-using-geologic-radiocarbon-ages/42E38E070B2B813D55BCEE8D9DC0DF93#supplementary-materials">Supplementary materials</a> originally comprised 2457 rows, but 36 rows (dates( have no mean or SD (only a minimum age) and have been excluded from this data frame. Furthermore, 25 rows (dates) have to SDs (+ve and -ve), which have been changed to a single SD for this dataset by taking the mean average.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	bluhm2421
	</code></pre>


<h3>Format</h3>

<p>A data frame comprising 2421 rows and 4 columns. 
</p>

<hr>
<h2 id='bryson1848'>Radiocarbon dataset from Bryson et al. 2006</h2><span id='topic+bryson1848'></span>

<h3>Description</h3>

<p>The 14C dataset used by Surovell et al. in 'Correcting temporal frequency distributions for taphonomic bias', Journal of Archaeological Science 2009.
Data was originally published by Bryson et al. in 'A calibrated radiocarbon database of late Quaternary volcanic eruptions', Earth Discussions, 2006.
Bryson et al. <a href="http://www.electronic-earth-discuss.net/1/123/2006/eed-1-123-2006-supplement.zip">source data</a> contains two data tables. The second comprises 173 rows of dates missing their standard deviations. Instead this data frame corresponds to Brysons first table comprising 1848 rows. However, these  are often an amalgamation of several individual radiocarbon dates which cannot be reverse-engineered to obtain the individual dates. Therefore we assume each row represents a unique phase.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	bryson1848
	</code></pre>


<h3>Format</h3>

<p>A data frame comprising 1848 rows and 8 columns. 
</p>

<hr>
<h2 id='checkData'>Checks a dataset for obvious clangers</h2><span id='topic+checkData'></span>

<h3>Description</h3>

<p>Performs some rudimentary sanity checks on a radiocarbon (or other) dataset</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkData(data)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkData_+3A_data">data</code></td>
<td>

<p>A dataframe of 14C dates. Requires 'age' and 'sd', and at least one of 'site' and 'phase'. Optional 'datingType' to include '14C' and anything else.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Performs some rudimentary checks on the radiocarbon dataset, ensuring structure is as required, ages and sds look vaguely sensible etc. 
This is no substitute for poor data hygiene, and the analyst should of course have a toolkit of many other checks, e.g., to avoid duplicate labcodes.
</p>


<h3>Value</h3>

<p>NULL
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	checkData(SAAD)
	</code></pre>

<hr>
<h2 id='convertPars'>Converts parameters to x,y coordinates (date and pdf) that describe a model</h2><span id='topic+convertPars'></span>

<h3>Description</h3>

<p>Converts either a vector of parameters, or a matrix of many parameter sets to model x,y coordinates (date and pdf)
<script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script> </p>


<h3>Usage</h3>

<pre><code class='language-R'>convertPars(pars, years, type, taphonomy=FALSE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convertPars_+3A_pars">pars</code></td>
<td>

<p>Either a vector of one parameter set, or a matrix of several parameter sets (one set per row). 
</p>
</td></tr>
<tr><td><code id="convertPars_+3A_years">years</code></td>
<td>

<p>A vector of years.
</p>
</td></tr>
<tr><td><code id="convertPars_+3A_type">type</code></td>
<td>

<p>Choose from 'CPL', 'exp', 'norm', 'cauchy', 'sine', 'uniform', 'logistic', 'power'.
</p>
</td></tr>
<tr><td><code id="convertPars_+3A_taphonomy">taphonomy</code></td>
<td>

<p>If TRUE, the last two parameters determine the taphonomic loss rate.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parameter searches will yield either a single set of parameters, or a matrix with one parameter set per row (such as the 'res' value from <code><a href="#topic+mcmc">mcmc</a></code>) . Either can be handed directly to this function.
</p>
<p>The structure of the output differs depending on if converting a vector or matrix.
</p>
<p>All models are truncated, such that the total area between \(x_{min}\) and \(x_{max}\) (the date range of 'years') equals 1.  
</p>
<p>'CPL' is a Continuous Piecewise Linear model. Pars must be of odd length, each between 0 and 1. A n-CPL model has \(2n-1\) parameters (\(n-1\) x-parameters and \(n\) y-parameters) that are mapped to \(n+1\) PD coordinates (x,y pairs) using a modified Stick Breaking Dirichlet Process. The first and last x-coordinate are set as \(x_{min}\) and \(x_{max}\), and the remaining internal x-coordinates are converted from their respective x-parameters using the Beta distribution CDF (where \(\alpha = 1\) and \(\beta =\) the number of pieces still to be broken). The y-parameters (between 0 and 1) are converted to y-coordinates in two steps. Firstly, they are mapped from the parameter range (0,1) to the coordinate range (0, \(\infty\)) using the formula \(\frac{1}{(1 - y)^2} - 1\), and the single remaining y-coordinate is set as \(\frac{1}{(1 - 0.5)^2} - 1\). Secondly, they are normalised by the total area under the curve, calculated as the sum of the areas under all \(n\) pieces: \[Area = \sum_{i=1}^{n} (\frac{y_i + y_{i+1}}{2})(x_{i+1} - x_i) \]
</p>
<p>'exp' is a truncated exponential model of the form \(f(x)=ae^{rx}\) where \(x =\) years. The single parameter is used as the rate exponent \(r\) which gives growth through time if \(r > 0\), decline if \(r < 0\), and constant if \(r=0\). The PDF is as follows. Note the \(a\) parameter cancels out: \[\frac{-re^{-r x}}{e^{-rx_{max}} - e^{-rx_{min}}}\]  
</p>
<p>'logistic' is a truncated logistic model. The two parameters are used as the rate \(k\) and centre \(x_0\) where the PDF is: \[\frac{k}{(e^{-k(x_0 - x)} + 1)\ln{\Bigl(\frac{e^{-k(x_0 - x_{min})} + 1}{e^{-k(x_0 - x_{max})} + 1}}\Bigr)}\]
</p>
<p>'norm' is a truncated Gaussian model. The two parameters are used as \(\mu\) and \(\sigma\) in the formula for a truncated Normal distribution, the PDF of which is calculated in two steps. Firstly, the PDF of an ordinary Normal distribution is calculated. Secondly, it is normalised by the area within the date range.
</p>
<p>'cauchy' is a truncated Cauchy model. The two parameters are used as \(x_0\) (location) and \(\gamma\) (scale) in the formula for a truncated Cauchy distribution. The PDF is as follows where \(x =\) years: \[\frac{1}{\gamma[1+(\frac{x-x_0}{\gamma})^2][\arctan(\frac{x_0-x_{min}}{\gamma})-\arctan(\frac{x_0-x_{max}}{\gamma})]}\]  
</p>
<p>'power' is a truncated Power function model of the form \(f(x)=a(b+x)^c\) where \(x =\) years.  The PDF is as follows. Note the \(a\) parameter cancels out: \[\frac{(c+1)(b+x)^c}{(b+x_{max})^{(c+1)} - (b+x_{min})^{(c+1)}}\]
</p>
<p>'sine' is a truncated sinewave model. The three parameters are used as specified in <code><a href="#topic+sinewavePDF">sinewavePDF</a></code>
</p>
<p>'uniform' is a uniform model requiring no parameters. I.e. the argument pars must be NULL or c(), and trivially the PDF is: \[\frac{1}{x_{max} - x_{min}}\]
</p>
<p>If taphonomy is TRUE, the model PDF additionally includes taphonomic loss. The last two parameters are used to generate a taphonomic curve using the formula \((b+x)^c\) and the remaining parameters are used in the appropriate model type above. Then the model PDF is multiplied by the taphonomic curve and normalised across the date range, to give the final model PDF that includes taphonomy.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	# convert a single random 6-CPL parameter set
	pars &lt;- runif(11)
	x &lt;- convertPars( pars=pars, years=5500:7500, type='CPL')

	# single random 6-CPL parameter set with taphonomy parameters (b,c)
	pars &lt;- runif(13, c(rep(0,11),0,-3), c(rep(1,11),20000,0))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='CPL', taphonomy=TRUE)

	# convert a matrix of 5 random 6-CPL parameter sets
	pars &lt;- matrix( runif(11*5), 5, 11 )
	x &lt;- convertPars( pars=pars, years=5500:7500, type='CPL')

	# 5 random 6-CPL parameter sets with taphonomy parameters (b,c)
	pars &lt;- t(matrix(runif(13*5, c(rep(0,11),0,-3), c(rep(1,11),20000,0)),13,5))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='CPL', taphonomy=TRUE)

	# convert a single random exponential parameter
	pars &lt;- runif(1, -0.01, 0.01)
	x &lt;- convertPars( pars=pars, years=5500:7500, type='exp')

	# single random exponential parameter with taphonomy parameters (b,c)
	pars &lt;- runif(3, c(-0.01,0,-3), c(0.01,20000,0))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='exp', taphonomy=TRUE)
	
	# convert a matrix of 5 random exponential parameter sets
	pars &lt;- matrix( runif(5, -0.01, 0.01), 5, 1 )
	x &lt;- convertPars( pars=pars, years=5500:7500, type='exp')

	# 5 random exponential parameter sets with taphonomy parameters (b,c)
	pars &lt;- t(matrix(runif(3*5, c(-0.01,0,-3), c(0.01,20000,0)),3,5))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='exp', taphonomy=TRUE)

	# convert a single random Gaussian parameter pair (mean, sd)
	pars &lt;- runif(2, c(6000,200), c(7000,1000))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='norm')

	# single random Gaussian parameter pair (mean, sd) with taphonomy parameters (b,c)
	pars &lt;- runif(4, c(6000,200,0,-3), c(7000,1000,20000,0))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='norm', taphonomy=TRUE)

	# convert a matrix of 5 random Gaussian parameter pairs (mean, sd)
	pars &lt;- t(matrix(runif(2*5, c(6000,200), c(7000,1000)),2,5))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='norm')

	# 5 random Gaussian parameter pairs (mean, sd) with taphonomy parameters (b,c)
	pars &lt;- t(matrix(runif(4*5, c(6000,200,0,-3), c(7000,1000,20000,0)),4,5))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='norm', taphonomy=TRUE)

	# convert a single random Cauchy parameter pair (location, scale)
	pars &lt;- runif(2, c(6000,200), c(7000,1000))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='cauchy')

	# single random Cauchy parameter pair (location, scale) with taphonomy parameters (b,c)
	pars &lt;- runif(4, c(6000,200,0,-3), c(7000,1000,20000,0))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='cauchy', taphonomy=TRUE)

	# convert a matrix of 5 random Cauchy parameter pairs (location, scale)
	pars &lt;- t(matrix(runif(2*5, c(6000,200), c(7000,1000)),2,5))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='cauchy')

	# 5 random Cauchy parameter pairs (location, scale) with taphonomy parameters (b,c)
	pars &lt;- t(matrix(runif(4*5, c(6000,200,0,-3), c(7000,1000,20000,0)),4,5))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='cauchy', taphonomy=TRUE)

	# convert a single random logistic parameter pair (k, x0)
	pars &lt;- runif(2, c(0,6000), c(0.01,6500))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='logistic')

	# single random logistic parameter pair (k, x0) with taphonomy parameters (b,c)
	pars &lt;- runif(4, c(0,6000,0,-3), c(0.01,6500,20000,0))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='logistic', taphonomy=TRUE)

	# convert a matrix of 5 random logistic parameter pairs(k, x0)
	pars &lt;- t(matrix(runif(2*5, c(0,6000), c(0.01,6500)),2,5))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='logistic')

	#  5 random logistic parameter pairs(k, x0) with taphonomy parameters (b,c)
	pars &lt;- t(matrix(runif(4*5, c(0,6000,0,-3), c(0.01,6500,20000,0)),4,5))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='logistic', taphonomy=TRUE)

	# convert a single random power function parameter pair (b, c)
	pars &lt;- runif(2, c(2000,-1.7), c(4000,-1.2))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='power')

	# single random power function parameter pair (b, c) with taphonomy parameters 
	pars &lt;- runif(4, c(2000,-1.7,0,-3), c(4000,-1.2,20000,0))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='power', taphonomy=TRUE)

	# convert a matrix of 5 random power function parameter pairs(b, c)
	pars &lt;- t(matrix(runif(2*5, c(2000,-1.7), c(4000,-1.2)),2,5))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='power')

	#  5 random power function parameter pairs(b, c) with taphonomy parameters 
	pars &lt;- t(matrix(runif(4*5, c(2000,-1.7,0,-3), c(4000,-1.2,20000,0)),4,5))
	x &lt;- convertPars( pars=pars, years=5500:7500, type='power', taphonomy=TRUE)

	# convert a single random sinewave parameter set (f,p,r)
	f &lt;- 1/runif(1,200,1000)
	p &lt;- runif(1,0,2*pi)
	r &lt;- runif(1,0,1)
	x &lt;- convertPars( pars=c(f,p,r), years=5500:7500, type='sine')

	# single random sinewave parameter set (f,p,r) with taphonomy parameters (b,c)
	f &lt;- 1/runif(1,200,1000)
	p &lt;- runif(1,0,2*pi)
	r &lt;- runif(1,0,1)
	b &lt;- runif(1,0,20000)
	c &lt;- runif(1,-3,0)
	x &lt;- convertPars( pars=c(f,p,r,b,c), years=5500:7500, type='sine', taphonomy=TRUE)

	# convert a matrix of 5 random sinewave parameter sets (f,p,r)
	f &lt;- 1/runif(5,200,1000)
	p &lt;- runif(5,0,2*pi)
	r &lt;- runif(5,0,1)
	x &lt;- convertPars( pars=cbind(f,p,r), years=5500:7500, type='sine')

	# 5 random sinewave parameter sets (f,p,r) with taphonomy parameters (b,c)
	f &lt;- 1/runif(5,200,1000)
	p &lt;- runif(5,0,2*pi)
	r &lt;- runif(5,0,1)
	b &lt;- runif(5,0,20000)
	c &lt;- runif(5,-3,0)
	x &lt;- convertPars( pars=cbind(f,p,r,b,c), years=5500:7500, type='sine', taphonomy=TRUE)
	
	# although a uniform distribution has no parameters, a pdf can still be generated:
	x &lt;- convertPars(pars=NULL, years=5500:7500, type='uniform')

	# and if taphonomy is included, it does have the taphonomy parameters (b,c)
	pars &lt;- runif(2, c(0,-3), c(20000,0))
	x &lt;- convertPars(pars=pars, years=5500:7500, type='uniform', taphonomy=TRUE)

	# likewise for a matrix of 5 random taphonomy parameters (b,c)
	pars &lt;- t(matrix(runif(2*5, c(0,-3), c(20000,0)),2,5))
	x &lt;- convertPars(pars=pars, years=5500:7500, type='uniform', taphonomy=TRUE)
	</code></pre>

<hr>
<h2 id='CPLparsToHinges'>Converts CPL parameters (0 to 1) into hinge (x,y) coordinates (date and pdf) that describe a model</h2><span id='topic+CPLparsToHinges'></span>

<h3>Description</h3>

<p>Converts either a vector of parameters, or a matrix of many parameter sets to CPL-model hinges (date and pdf coordinates).
<script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script> </p>


<h3>Usage</h3>

<pre><code class='language-R'>CPLparsToHinges(pars, years)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CPLparsToHinges_+3A_pars">pars</code></td>
<td>

<p>Either a vector of one parameter set, or a matrix of several parameter sets (one set per row). 
</p>
</td></tr>
<tr><td><code id="CPLparsToHinges_+3A_years">years</code></td>
<td>

<p>A vector of years.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The CPL model requires pars to be of odd length, each between 0 and 1. A n-CPL model has \(2n-1\) parameters (\(n-1\) x-parameters and \(n\) y-parameters) that are mapped to \(n+1\) PD coordinates (x,y pairs) using a modified Stick Breaking Dirichlet Process. The first and last x-coordinate are set as \(x_{min}\) and \(x_{max}\), and the remaining internal x-coordinates are converted from their respective x-parameters using the Beta distribution CDF (where \(\alpha = 1\) and \(\beta =\) the number of pieces still to be broken). The y-parameters (between 0 and 1) are converted to y-coordinates in two steps. Firstly, they are mapped from the parameter range (0,1) to the coordinate range (0, \(\infty\)) using the formula \(\frac{1}{(1 - y)^2} - 1\), and the single remaining y-coordinate is set as \(\frac{1}{(1 - 0.5)^2} - 1\). Secondly, they are normalised by the total area under the curve, calculated as the sum of the areas under all \(n\) pieces: \[Area = \sum_{i=1}^{n} (\frac{y_i + y_{i+1}}{2})(x_{i+1} - x_i) \]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	# convert a single random 6-CPL parameter set
	x &lt;- CPLparsToHinges(pars=runif(11), years=5500:7500)
	</code></pre>

<hr>
<h2 id='data1'>Toy radiocarbon dataset</h2><span id='topic+data1'></span>

<h3>Description</h3>

<p>Data frame comprising a randomly generated 14C dataset. The data generation process is deliberately left to the imagination.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	data1
	</code></pre>


<h3>Format</h3>

<p>A data frame comprising 100 rows and 4 columns. 
</p>

<hr>
<h2 id='data2'>Toy radiocarbon dataset</h2><span id='topic+data2'></span>

<h3>Description</h3>

<p>Data frame comprising a randomly generated 14C dataset. The data generation process is deliberately left to the imagination.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	data2
	</code></pre>


<h3>Format</h3>

<p>A data frame comprising 100 rows and 4 columns. 
</p>

<hr>
<h2 id='data3'>Toy radiocarbon dataset</h2><span id='topic+data3'></span>

<h3>Description</h3>

<p>Data frame comprising a randomly generated 14C dataset. The data generation process is deliberately left to the imagination.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	data3
	</code></pre>


<h3>Format</h3>

<p>A data frame comprising 100 rows and 4 columns. 
</p>

<hr>
<h2 id='estimateDataDomain'>Estimates the calendar date domain of a 14C dataset.</h2><span id='topic+estimateDataDomain'></span>

<h3>Description</h3>

<p>Estimates the approximate date range of a 14C dataset, in calendar time.</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimateDataDomain(data, calcurve)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimateDataDomain_+3A_data">data</code></td>
<td>

<p>A dataframe of 14C dates. Requires 'age' and 'sd', and at least one of 'site' and 'phase'. Optional 'datingType' to include '14C' and anything else.
</p>
</td></tr>
<tr><td><code id="estimateDataDomain_+3A_calcurve">calcurve</code></td>
<td>

<p>A calibration curve object. Choose from intcal20 (default), shcal20, intcal13 or shcal13.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Since dates are derived from normal (or log normal) distributions, they have no absolute cut off point. However, in practice the tail of a PDF rapidly becomes vanishingly small, so a date range can be estimated using an arbitrarily large interval (0.000001 to 0.999999) of the cumulative distribution.
</p>
<p>In practice however, the date range chosen to model should not be selected using this function, but carefully chosen to ensure the date range is fairly represented by the data set.
</p>
<p>Therefore this function should only be used to select a date range to model if simulating a tiny sample size, to ensure the modelled range does not exceed the date domain of the data.
</p>


<h3>Value</h3>

<p>Returns a vector of two calendar dates BP.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# a single date within the 14C range 5000 to 10000
data &lt;- data.frame(
		age = round(runif(1,5000,10000)), 
		sd = 3,
		datingType = '14C'
		)
estimateDataDomain(data, calcurve=intcal20)

# 50 dates within the 14C range 5000 to 10000
data &lt;- data.frame(
		age = round(runif(50,5000,10000)), 
		sd = rep(50,50),
		datingType = rep('14C',50)
		)
estimateDataDomain(data, calcurve=intcal20)

</code></pre>

<hr>
<h2 id='intcal13'>Northern hemisphere 2013 calibration curve</h2><span id='topic+intcal13'></span>

<h3>Description</h3>

<p>Northern hemisphere 2013 calibration curve
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	intcal13
	</code></pre>


<h3>Format</h3>

<p>A data frame comprising 5141 rows and 3 columns:
cal BP, 14C BP, +/- Error.
</p>


<h3>Source</h3>

<p>Atmospheric data from Reimer et al (2013). Obtained from the raw intcal13.14c file downloaded from http://radiocarbon.webhost.uits.arizona.edu/node/19
</p>


<h3>References</h3>

<p>Reimer PJ, Bard E, Bayliss A, Beck JW, Blackwell PG, Bronk Ramsey C, Buck CE, Cheng H, Edwards RL, Friedrich M, Grootes PM, Guilderson TP, Haflidason H, Hajdas I, Hatte C, Heaton TJ, Hogg AG, Hughen KA, Kaiser KF, Kromer B, Manning SW, Niu M, Reimer RW, Richards DA, Scott EM, Southon JR, Turney CSM, van der Plicht J. Radiocarbon 55(4). DOI: 10.2458/azu_js_rc.55.16947</p>

<hr>
<h2 id='intcal20'>Northern hemisphere 2020 calibration curve</h2><span id='topic+intcal20'></span>

<h3>Description</h3>

<p>Northern hemisphere 2020 calibration curve
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	intcal20
	</code></pre>


<h3>Format</h3>

<p>A data frame comprising 9501 rows and 3 columns:
cal BP, 14C BP, +/- Error.
</p>


<h3>Source</h3>

<p>Atmospheric data from Reimer et al (2020). Obtained from the raw intcal20.14c file downloaded from http://intcal.org/curves/intcal20.14c
</p>


<h3>References</h3>

<p>Reimer P, Austin WEN, Bard E, Bayliss A, Blackwell PG, Bronk Ramsey C, Butzin M, Cheng H, Edwards RL, Friedrich M, Grootes PM, Guilderson TP, Hajdas I, Heaton TJ, Hogg AG, Hughen KA, Kromer B, Manning SW, Muscheler R, Palmer JG, Pearson C, van der Plicht J, Reimer RW, Richards DA, Scott EM, Southon JR, Turney CSM, Wacker L, Adolphi F, Büntgen U, Capano M, Fahrni S, Fogtmann-Schulz A, Friedrich R, Köhler P, Kudsk S, Miyake F, Olsen J, Reinig F, Sakamoto M, Sookdeo A, Talamo S. 2020. The IntCal20 Northern Hemisphere radiocarbon age calibration curve (0-55 cal kBP). Radiocarbon 62. doi: 10.1017/RDC.2020.41.</p>

<hr>
<h2 id='loglik'>Calculates the log likelihood of a population model, given a calibrated date PD matrix</h2><span id='topic+loglik'></span>

<h3>Description</h3>

<p>Calculates the log likelihood of a population model, given a calibrated date PD matrix</p>


<h3>Usage</h3>

<pre><code class='language-R'>loglik(PD, model)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loglik_+3A_pd">PD</code></td>
<td>

<p>A data frame of Probability Distributions (PDs). Each column representing the PD of a calibrated observation or phase. Row names correspond to the calendar years BP. This data frame can be generated by <code><a href="#topic+phaseCalibrator">phaseCalibrator</a></code> 
</p>
</td></tr>
<tr><td><code id="loglik_+3A_model">model</code></td>
<td>

<p>A data frame containing the columns 'pdf' representing a hypothesised population Probability Density Function; and 'year' corresponding to the calendar years BP. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Row names of both PD and model arguments must exactly match, since the probability of each phase given the model is calculated numerically.
</p>


<h3>Value</h3>

<p>Returns a single numeric log likelihood.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	# calibrate a dataset comprising just two phases
	data &lt;- data.frame(age=c(5800, 5100),sd=c(40, 35),phase=c('p1', 'p2'), datingType='14C')
	CalArray &lt;- makeCalArray(shcal20, calrange = range(toy$year))
	PD &lt;- phaseCalibrator(data, CalArray)	

	# calculate toy model log likelihood
	loglik(PD, toy)
</code></pre>

<hr>
<h2 id='makeCalArray'>Makes a calibration curve probability array</h2><span id='topic+makeCalArray'></span>

<h3>Description</h3>

<p>Generates CalArray containing a 2D probability array of the calibration curve. </p>


<h3>Usage</h3>

<pre><code class='language-R'>makeCalArray(calcurve, calrange, inc=5)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeCalArray_+3A_calcurve">calcurve</code></td>
<td>

<p>A calibration curve object. Choose from intcal20 (default), shcal20, intcal13 or shcal13.
</p>
</td></tr>
<tr><td><code id="makeCalArray_+3A_calrange">calrange</code></td>
<td>

<p>A vector of two calendar dates BP, giving the calendar range of CalArray. Can be in either order.
</p>
</td></tr>
<tr><td><code id="makeCalArray_+3A_inc">inc</code></td>
<td>

<p>Increments to interpolate calendar years. Default = 5
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generates an array of probabilities mapping the calibration curve and its error ribbon.
</p>
<p>Each column represents a Gaussian PDF constructed from the C14 date and error of the calibration curve (typically a column every 5 cal years).
</p>
<p>Row names of CalArray are 14C dates, column names are cal dates.
</p>
<p>This function is memory and time costly if used to construct the entire 50,000 year range of the calibration curve at a resolution of 1 cal years, therefore typically used only for a constrained date range, by specifying calrange.
</p>
<p>This array only needs constructing once to generate a Summed Probability Distribution of any number of calibrated dates, allowing efficient downstream calibration.
</p>


<h3>Value</h3>

<p>Returns a list including:
</p>
<table>
<tr><td><code>probs</code></td>
<td>
<p>a 2D probability array of the calibration curve</p>
</td></tr>
<tr><td><code>calcurve</code></td>
<td>
<p>the calibration curve as provided as an argument</p>
</td></tr>
<tr><td><code>cal</code></td>
<td>
<p>a numeric vector of calendar years</p>
</td></tr>
<tr><td><code>inc</code></td>
<td>
<p>the resolution of the array in calendar years</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>	# generate a CalArray of the intcal20 curve covering 5500 calBP to 7000 calBP
	x &lt;- makeCalArray(intcal13, c(5500,7000), inc=1 )
	</code></pre>

<hr>
<h2 id='mcmc'>Makes a MCMC chain using the Metropolis-Hastings algorithm.</h2><span id='topic+mcmc'></span>

<h3>Description</h3>

<p>Generates a single Markov Chain Monte Carlo using the Metropolis-Hastings algorithm.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc(PDarray, 
						startPars, 
						type, 
						taphonomy=FALSE, 
						taph.min=c(0,-3), 
						taph.max=c(20000,0), 
						N=30000, 
						burn=2000, 
						thin=5, 
						jumps=0.02)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_+3A_pdarray">PDarray</code></td>
<td>

<p>A Probability Density Array of calibrated dates, generated by <code><a href="#topic+phaseCalibrator">phaseCalibrator</a></code> .
</p>
</td></tr>
<tr><td><code id="mcmc_+3A_startpars">startPars</code></td>
<td>

<p>A vector of parameter values for the algorithm to start at. Suggest using the parameters found from a Maximum Likelihood search. Must have an odd length with values between 0 and 1 for an n-CPL model, or length 1 values between -0.1 and 0.1 for an exponential model.
</p>
</td></tr>
<tr><td><code id="mcmc_+3A_type">type</code></td>
<td>

<p>Choose from 'CPL', 'exp', 'norm', 'cauchy', 'sine', 'uniform', 'logistic', 'power'.
</p>
</td></tr>
<tr><td><code id="mcmc_+3A_taphonomy">taphonomy</code></td>
<td>

<p>If TRUE, the last two parameters determine the taphonomic loss rate.
</p>
</td></tr>
<tr><td><code id="mcmc_+3A_taph.min">taph.min</code></td>
<td>

<p>Minimum values for the prior range of taphonomic parameters (b,c).  
</p>
</td></tr>
<tr><td><code id="mcmc_+3A_taph.max">taph.max</code></td>
<td>

<p>Maximum values for the prior range of taphonomic parameters (b,c). 
</p>
</td></tr>
<tr><td><code id="mcmc_+3A_n">N</code></td>
<td>

<p>The total number of proposals made, before the chain is reduced for burn-in and thinning.
</p>
</td></tr>
<tr><td><code id="mcmc_+3A_burn">burn</code></td>
<td>

<p>The number of proposals made at the beginning of the chain to be disregarded as burn-in.
</p>
</td></tr>
<tr><td><code id="mcmc_+3A_thin">thin</code></td>
<td>

<p>Specifies the proportion of proposals to be disregarded (after burn-in), such that the default 5 will only keep every 5th proposal.
</p>
</td></tr>
<tr><td><code id="mcmc_+3A_jumps">jumps</code></td>
<td>

<p>Vector that determines the size of the random jump between the last parameters and the proposed parameters. A smaller value gives a smaller jump. Different jump values can be provided for each parameter. This can be tuned by observing how well mixed each parameter is in the chain.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generates a single MCMC chain using the Metropolis-Hastings algorithm, printing to screen progress every 1000 proposals. An acceptance ratio of around 0.4 to 0.5 should be sought by adapting the arguments 'burn' and 'jumps'. If the acceptance ratio is too low try reducing jump. A larger dataset will typically require smaller jumps.
</p>


<h3>Value</h3>

<p>Returns a list including:
</p>
<table>
<tr><td><code>res</code></td>
<td>
<p>A 2D matrix of free parameter values (between 0 and 1) in the chain, after burn-in and thinning.</p>
</td></tr>
<tr><td><code>all.pars</code></td>
<td>
<p>A 2D matrix of free parameter values (between 0 and 1) in the chain of all N proposals.</p>
</td></tr>
<tr><td><code>acceptance.ratio</code></td>
<td>
<p>The proportion of proposals (after burn-in) that are accepted.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># randomly sample calendar dates from the toy model
set.seed(12345) 
N &lt;- 350
cal &lt;- simulateCalendarDates(toy, N)

# Convert to 14C dates. 
age &lt;- uncalibrateCalendarDates(cal, shcal20)
data &lt;- data.frame(age = age, sd = 50, phase = 1:N, datingType = '14C')


# Calibrate each phase, taking care to restrict to the modelled date range
CalArray &lt;- makeCalArray(shcal20, calrange = range(toy$year), inc = 5)
PD &lt;- phaseCalibrator(data, CalArray, remove.external = TRUE)

# Run MCMC for a 3-CPL model (5 parameters)
chain &lt;- mcmc(PDarray = PD, 
	startPars = rep(0.5,5), 
	type='CPL', 
	jumps = 0.02)	

# Run MCMC for a 3-CPL model with taphonomy (5 + 2 parameters)
chain &lt;- mcmc(PDarray = PD, 
	startPars = c(rep(0.5,5),10000,-1.5), 
	type='CPL', 
	taphonomy=TRUE, 
	jumps = 0.02)	

</code></pre>

<hr>
<h2 id='objectiveFunction'>Objective function to be minimised in a search. Returns the negative log likelihood.</h2><span id='topic+objectiveFunction'></span>

<h3>Description</h3>

<p>Calculates the negative log likelihood of a model given a calibrated date matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>objectiveFunction(pars, PDarray, type, taphonomy=FALSE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="objectiveFunction_+3A_pars">pars</code></td>
<td>

<p>A numeric vector of parameters. 
</p>
</td></tr>
<tr><td><code id="objectiveFunction_+3A_pdarray">PDarray</code></td>
<td>

<p>A data frame typically generated by <code><a href="#topic+phaseCalibrator">phaseCalibrator</a></code>, such that each column represents the PD of each calibrated date (or phase), and row names are the corresponding years.
</p>
</td></tr>
<tr><td><code id="objectiveFunction_+3A_type">type</code></td>
<td>

<p>Choose from 'CPL', 'exponential', or 'uniform'. 
</p>
</td></tr>
<tr><td><code id="objectiveFunction_+3A_taphonomy">taphonomy</code></td>
<td>

<p>If TRUE, the last two parameters determine the taphonomic loss rate.
</p>
</td></tr>	
</table>


<h3>Details</h3>

<p>If type is 'CPL', pars must be an odd length each between 0 and 1 since parameters correspond to: (y,x,...y). 
If type is 'exp', pars must be a single positive or negative numeric (the exponential rate can be growth or decay). Typically this parameter should be close to zero (-0.1 to 0.1) to avoid numbers beyond floating point limits) 
If type is 'norm', pars must have length 2 (mean and sd)
If type is 'uniform', then no parameters are required, and pars must be NULL or c(). 
</p>


<h3>Value</h3>

<p>Returns a single value, the negative log likelihood.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	# generate a PD array from a handful of dates
	data &lt;- subset(SAAD, site %in% c('Carrizal','Pacopampa'))
	CalArray &lt;- makeCalArray(shcal13, calrange = c(2000,6000))
	PD &lt;- phaseCalibrator(data, CalArray)

	# the negative log likelihood given some random parameters for a 3-CPL model
	pars &lt;- runif(5)
	objectiveFunction(pars, PD, type='CPL')	

	# the negative log likelihood given a random exponential model
	pars &lt;- runif(1, -0.01, 0.01)
	objectiveFunction(pars, PD, type='exp')	

	# the negative log likelihood given a random Gaussian model
	pars &lt;- c(runif(1, 2000, 6000), runif(1, 100, 1000))
	objectiveFunction(pars, PD, type='norm')	

	# the negative log likelihood given a uniform model
	objectiveFunction(pars=NULL, PD, type='uniform')	

	# the negative log likelihood given a uniform model with taphonomy
	pars &lt;- c(runif(1, 0, 20000), runif(1, -3, 0))
	objectiveFunction(pars=pars, PD, type='uniform', taphonomy=TRUE)	
</code></pre>

<hr>
<h2 id='phaseCalibrator'>Generates an SPD for each phase in a dataset</h2><span id='topic+phaseCalibrator'></span>

<h3>Description</h3>

<p>Generates a data frame of SPDs, one for each phase. </p>


<h3>Usage</h3>

<pre><code class='language-R'>phaseCalibrator(data, CalArray, width = 200, remove.external = FALSE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="phaseCalibrator_+3A_data">data</code></td>
<td>

<p>A dataframe of 14C dates. Requires 'age' and 'sd', and at least one of 'site' and 'phase'. Optional 'datingType' comprising '14C' and/or anything else.
</p>
</td></tr>
<tr><td><code id="phaseCalibrator_+3A_calarray">CalArray</code></td>
<td>

<p>A 2D probability array of the calibration curve generated by <code><a href="#topic+makeCalArray">makeCalArray</a></code> containing row names and column names. 
</p>
</td></tr>
<tr><td><code id="phaseCalibrator_+3A_width">width</code></td>
<td>

<p>A timespan in C14 years used to automatically bin dates if they have not been phased, i.e., 'phase' is missing from the data. Default = 200.
</p>
</td></tr>
<tr><td><code id="phaseCalibrator_+3A_remove.external">remove.external</code></td>
<td>

<p>Default FALSE retains the SPDs of all phases, even if some have little or no probability mass inside the date range. If TRUE, those phases with the majority of their probability mass outside the date range are removed. 
</p>
</td></tr>	
</table>


<h3>Details</h3>

<p>Generates an SPD for each phase using <code><a href="#topic+summedCalibrator">summedCalibrator</a></code> with the default normalisation = 'standard'. Returns a data frame of probabilities, with phase names assigned to column names, and calendar years assigned to row names. 
</p>
<p>As a minimum requirement the data must include either 'phase' or 'site'. If 'phase' is unavailable the function will automatically use 'site' to bin the dates into site-phases. This binning is achieved with a crude algorithm that assigns a date to a bin if it is within 200 years of any other date in that bin, based on the uncalibrated C14 mean date (non-14C dates are also mapped to 14C time for the purpose of this binning). The need to bin dates into phases is an important step in modelling population dynamics to adjust for the data ascertainment bias of some archaeological finds having more dates by virtue of a larger research interest/budget. This binning algorithm provides a simple and useful solution to handling large datasets that have not been phased, but is not an alternative to an OxCal phase model if the objective is to directly estimate phase boundary dates at a specific site.
</p>
<p>Optionally 'datingType' can be provided in the data. Only '14C' will be calibrated in the usual way, anything else is assumed to be provided in calendar time. If 'datingType' is not provided, all dates are assumed to be 14C.
</p>
<p>Each column of the output data frame is a vector of probabilities representing the SPD of a phase. However, if the date range used to generate CalArray does not encompass the entire dataset, some phases will have PD outside the date range, giving a SPD area &lt; 1. This avoids deleterious edge effects.
</p>
<p>If using the output data frame to search for a population model, it is crucial to exclude dates outside (or mostly outside) the date range. This is achieved with remove.external = TRUE. 
</p>


<h3>Value</h3>

<p>Returns a data frame of probabilities. Each column provides the SPD of a phase. Column names are the phase names, row names are the calendar years.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	CalArray &lt;- makeCalArray(intcal20, calrange = c(9000,11000), inc = 5 )

	# minimum data requirement includes 'mean' and 'sd' and either 'site' or 'phase'   
	data &lt;- data.frame(age = c(8350,8500,8900,9200), 
			sd = c(50,50,50,50), 
			site = c('field','field','field','garden'))
	x &lt;- phaseCalibrator(data, CalArray)

	# notice three phases were automatically generated, each with a total SPD area = 1
	colSums(x)*5

	# in contrast, three dates are specified as coming from the same phase, 
	# and the 'garden.1' phase is partly outside the date range
	data &lt;- data.frame(age = c(8350,8500,8900,9480), 
			sd = c(50,50,50,50), 
			phase = c('field.1','field.1','field.1','garden.1'))
	x &lt;- phaseCalibrator(data, CalArray)
	colSums(x)*5
	</code></pre>

<hr>
<h2 id='plotCalArray'>Plots a calibration curve probability array</h2><span id='topic+plotCalArray'></span>

<h3>Description</h3>

<p>Generates a basic image plot of the calibration curve.</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotCalArray(CalArray)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotCalArray_+3A_calarray">CalArray</code></td>
<td>

<p>A 2D probability array of the calibration curve generated by <code><a href="#topic+makeCalArray">makeCalArray</a></code>, containing row names and column names. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots CalArray, a 2D probability array of the calibration curve. 
</p>
<p>Time costly if CalArray comprises the entire 50,000 year range of the calibration curve.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	# generate a CalArray of the intcal20 curve covering 5500 calBP to 6000 calBP
	x &lt;- makeCalArray( calcurve = intcal20, calrange = c(5500,6000), inc = 1 )
	plotCalArray(x)
	
	</code></pre>

<hr>
<h2 id='plotPD'>Plots a calibrated PD of a single date, or SPD of multiple dates, or multiple SPDs</h2><span id='topic+plotPD'></span>

<h3>Description</h3>

<p>Generates a basic plot of the Probability Distribution of calibrated date, or Summed Probability Distribution of multiple calibrated dates, or multiple SPDs such as a data frame of phases.</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotPD(x)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotPD_+3A_x">x</code></td>
<td>

<p>A data frame comprising row names of calendar years and optional column names. For example a one-column dataframe generated by <code><a href="#topic+summedCalibrator">summedCalibrator</a></code> or a multi-column dataframe generated by <code><a href="#topic+phaseCalibrator">phaseCalibrator</a></code> 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Presents the probability density as a grey polygon.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	data &lt;- data.frame(age=c(9144),sd=c(151))
	CalArray &lt;- makeCalArray(intcal20,calrange=c(8000,13000))
	cal &lt;- summedCalibrator(data, CalArray)
	plotPD(cal)
	</code></pre>

<hr>
<h2 id='plotSimulationSummary'>Plots a summary of the SPD simulation test</h2><span id='topic+plotSimulationSummary'></span>

<h3>Description</h3>

<p>Plots the SPD and confidence intervals of simulated SPDs, including regions outside the CI, the model, and 200 yr rolling mean.</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSimulationSummary(summary, title=NULL, legend.x=NULL, legend.y=NULL)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotSimulationSummary_+3A_summary">summary</code></td>
<td>

<p>A list of various objects generated by <code><a href="#topic+SPDsimulationTest">SPDsimulationTest</a></code> 
</p>
</td></tr>
<tr><td><code id="plotSimulationSummary_+3A_title">title</code></td>
<td>

<p>A string title for the plot. If NULL a summary is automatically generated. If no title is preferred, use title = &rdquo;. 
</p>
</td></tr>
<tr><td><code id="plotSimulationSummary_+3A_legend.x">legend.x</code></td>
<td>

<p>The x coordinate for the figure legend.
</p>
</td></tr>
<tr><td><code id="plotSimulationSummary_+3A_legend.y">legend.y</code></td>
<td>

<p>The y coordinate for the figure legend.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default NULL for legend.x and legend.y will automatically add a legend, which may not be ideally placed to avoid overlapping other components of the plot. To remove the legend, simply place well outside the boundary.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	
	summary &lt;- SPDsimulationTest(data=SAAD, 
		calcurve=shcal20, 
		calrange=c(2500,14000), 
		pars=-0.0001674152, 
		type='exp')
	
	plotSimulationSummary(summary)
	
	</code></pre>

<hr>
<h2 id='relativeRate'>Calculates the relative growth (or decline) rate per generation.</h2><span id='topic+relativeRate'></span>

<h3>Description</h3>

<p>Calculates the generational growth/decline rate for a linear piece of a CPL model, or between any two x,y coordinate pairs.</p>


<h3>Usage</h3>

<pre><code class='language-R'>relativeRate(x, y, generation = 25, N = 1000)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relativeRate_+3A_x">x</code></td>
<td>

<p>A numeric vector of length 2, giving the start and end date of linear piece, or a 2 column matrix such that each row is a start and end pair.
</p>
</td></tr>
<tr><td><code id="relativeRate_+3A_y">y</code></td>
<td>

<p>The corresponding y values such as PDs, or population size (numeric vector of length 2), or a matrix such that each row is a start and end pair. 
</p>
</td></tr>
<tr><td><code id="relativeRate_+3A_generation">generation</code></td>
<td>

<p>Years per generation. Default = 25.
</p>
</td></tr>
<tr><td><code id="relativeRate_+3A_n">N</code></td>
<td>

<p>Number of sections to average the growth rate across.  
</p>
</td></tr>	
</table>


<h3>Details</h3>

<p>The 'relative rate' (growth or decline) of a straight line between two x,y coordinate pairs is the expected generational growth rate across this line. It is calculated always relative to the larger y-value, providing a symmetric measure. E.g., the absolute percentage changes from 80 to 100 to 80 are calculated as +20
The expected rate is the mean average of the conventional rates for N equal sections of the line, as N approaches infinity. 
</p>


<h3>Value</h3>

<p>Returns a numeric vector of values between 0 and 100 representing a 'relative percentage rate per generation'. Negative values indicate a decline through time, positive indicate growth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
	x &lt;- c(5600,5500)
	y &lt;- c(75,80)

	# conventional growth/decline rate per 25 yr generation
	100 * exp(log(y[2]/y[1])/((x[1]-x[2])/25)) - 100

	# relative growth/decline rate per 25 yr generation
	relativeRate(x,y)

	x &lt;- c(5600,5500)
	y &lt;- c(480,75)

	# conventional growth/decline rate per 25 yr generation
	100 * exp(log(y[2]/y[1])/((x[1]-x[2])/25)) - 100

	# relative growth/decline rate per 25 yr generation
	relativeRate(x,y)

	x &lt;- c(5600,5500)
	y &lt;- c(480,0)

	# conventional growth/decline rate per 25 yr generation
	100 * exp(log(y[2]/y[1])/((x[1]-x[2])/25)) - 100

	# relative growth/decline rate per 25 yr generation
	relativeRate(x,y)

	# various random rates between 6000 and 5500 BP
	x &lt;- t(matrix(c(6000,5500),2,1000))
	y &lt;- matrix(runif(2000),1000,2)
	conventional &lt;- 100 * exp(log(y[,2]/y[,1])/((x[,1]-x[,2])/25)) - 100
	relative &lt;- relativeRate(x,y)
	plot(relative, conventional)

	</code></pre>

<hr>
<h2 id='SAAD'>Radiocarbon dataset for South American Arid Diagonal (SAAD)</h2><span id='topic+SAAD'></span>

<h3>Description</h3>

<p>Dataset of terrestrial 14C dates for SAAD. Specified by using data from sites falling within the geographically contiguous 'Arid' climatic categories of the SAAD as described in the World Köppen climate classification (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	SAAD
	</code></pre>


<h3>Format</h3>

<p>A data frame comprising 1527 rows and 10 columns
</p>


<h3>Source</h3>

<p>Dataset used in Timpson et al (2020). This is a subset of the larger midholo.csv dataset compiled and published by Riris and Arroyo-Kalin (2019).
</p>


<h3>References</h3>

<p>Timpson, A., Barberena, R., Thomas, M.G., Méndez, C., Manning, K. 2020 Directly modelling population dynamics in the South American Arid Diagonal using 14C dates. Philosophical Transactions of the Royal Society B.
</p>
<p>Kottek, M., Grieser, J., Beck, C., Rudolf, B. &amp; Rubel, F. 2006 World map of the Köppen-Geiger climate classification updated. Meteorologische Zeitschrift 15, 259-263.
</p>
<p>Riris, P. &amp; Arroyo-Kalin, M. 2019 Widespread population decline in South America correlates with mid-Holocene climate change. Scientific reports 9, 6850.
</p>

<hr>
<h2 id='shcal13'>Southern hemisphere 2013 calibration curve</h2><span id='topic+shcal13'></span>

<h3>Description</h3>

<p>Southern hemisphere 2013 calibration curve</p>


<h3>Usage</h3>

<pre><code class='language-R'>shcal13</code></pre>


<h3>Format</h3>

<p>A data frame comprising 5141 rows and 3 columns:
cal BP, 14C BP, +/- Error.
</p>


<h3>Source</h3>

<p>Atmospheric data from Hogg et al. (2013). Obtained from the raw shcal13.14c file downloaded from http://radiocarbon.webhost.uits.arizona.edu/node/19
</p>


<h3>References</h3>

<p>Hogg, A.G., Hua, Q., Blackwell, P.G., Niu, M., Buck, C.E., Guilderson, T.P., Heaton, T.J., Palmer, J.G., Reimer, P.J., Reimer, R.W., 2013. SHCal13 Southern Hemisphere calibration, 0–50,000 years cal BP, Radiocarbon 55, 1889-1903.</p>

<hr>
<h2 id='shcal20'>Southern hemisphere 2020 calibration curve</h2><span id='topic+shcal20'></span>

<h3>Description</h3>

<p>Southern hemisphere 2020 calibration curve</p>


<h3>Usage</h3>

<pre><code class='language-R'>shcal20</code></pre>


<h3>Format</h3>

<p>A data frame comprising 9501 rows and 3 columns:
cal BP, 14C BP, +/- Error.
</p>


<h3>Source</h3>

<p>Atmospheric data from Hogg et al. (2020). Obtained from the raw shcal20.14c file downloaded from http://intcal.org/curves/shcal20.14c
</p>


<h3>References</h3>

<p>Hogg AG, Heaton TJ, Hua Q, Palmer JG, Turney CSM, Southon J, Bayliss A, Blackwell PG, Boswijk G, Bronk Ramsey C, Pearson C, Petchey F, Reimer P, Reimer R, Wacker L. 2020. SHCal20 Southern Hemisphere calibration, 0-55,000 years cal BP. Radiocarbon 62. doi: 10.1017/RDC.2020.59</p>

<hr>
<h2 id='simulateCalendarDates'>Converts calendar dates to 14C dates</h2><span id='topic+simulateCalendarDates'></span>

<h3>Description</h3>

<p>Randomly samples calendar dates from a model, including dates slightly outside the model date range to avoid edge effects.</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulateCalendarDates(model, n)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulateCalendarDates_+3A_model">model</code></td>
<td>

<p>A data frame including columns 'year' and 'pdf' 
</p>
</td></tr>
<tr><td><code id="simulateCalendarDates_+3A_n">n</code></td>
<td>

<p>The number of dates to sample.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Samples n random calendar dates from a model pdf. This model must be defined in terms of a PDF vector and the corresponding calendar years. This can be provided at any preferred temporal resolution. For example, an exponential model can be provided with the PDF in annual intervals, whilst CPL model needs only the hinge points. <code><a href="#topic+convertPars">convertPars</a></code> will convert parameters into the required model format.
</p>


<h3>Value</h3>

<p>Returns a vector of calendar dates
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	# under a uniform model
	model &lt;- convertPars(pars=NULL, years=5000:6000,type='uniform')
	sims &lt;- simulateCalendarDates(model, 1000)
	range(sims)
	
	# simulate under an exponential model
	model &lt;- convertPars(pars=0.001, years=5000:6000,type='exp')
	sims &lt;- simulateCalendarDates(model, 1000)
	range(sims)
	
	# under a CPL model
	model &lt;- convertPars(pars=runif(5), years=5000:6000,type='CPL')
	sims &lt;- simulateCalendarDates(model, 1000)
	range(sims)
	</code></pre>

<hr>
<h2 id='sinewavePDF'>PDF of a truncated sinusoidal curve</h2><span id='topic+sinewavePDF'></span>

<h3>Description</h3>

<p>Probability density function for a truncated sinusoidal curve.
<script id="MathJax-script" async src="../../mathjaxr/doc/mathjax/es5/tex-chtml-full.js"></script> </p>


<h3>Usage</h3>

<pre><code class='language-R'>sinewavePDF(x, min, max, f, p, r)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sinewavePDF_+3A_x">x</code></td>
<td>

<p>Numeric vector of years
</p>
</td></tr>
<tr><td><code id="sinewavePDF_+3A_min">min</code>, <code id="sinewavePDF_+3A_max">max</code></td>
<td>

<p>Lower and upper \(x\) limits of the distribution
</p>
</td></tr>
<tr><td><code id="sinewavePDF_+3A_f">f</code></td>
<td>

<p>Numeric frequency (cycles per unit \(x\)).
</p>
</td></tr>
<tr><td><code id="sinewavePDF_+3A_p">p</code></td>
<td>

<p>Numeric between \(0\) and \(2\pi\), giving the cycle position (in radians) at \(x = 0\).
</p>
</td></tr>
<tr><td><code id="sinewavePDF_+3A_r">r</code></td>
<td>

<p>Numeric between 0 and 1, determining how flat the distribution is.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The usual function to describe a sine wave is \(f(x) = A\sin(2\pi f x + p)\), where \(A\) is the amplitude, \(f\) is the frequency (cycles per year), and \(p\) is the cycle position (in radians) at \(x = 0\), and therefore oscillates above and below the x-axis.
</p>
<p>However, a sinusoidal PDF must by definition always be non-negative, which can conceptually be considered as a sine wave stacked on top of a uniform distribution with a height \(A + k\), where \(k >= 0\).
Since the PDF is \(f(x)\) divided by the area below the curve, A and k simplify to a single parameter \(r\) that determines the relative proportions of the uniform and sinusoidal components, such that:
</p>
<p>when \(r = 0\) the amplitude of the sine wave component is zero, and the overall PDF is just a uniform distribution between min and max.
</p>
<p>when \(r = 1\) the uniform component is zero, and the minima of the sine wave touches zero. This does not necessarily mean the PDF minimum equals zero, since a minimum point of the sine wave may not occur with PDF domain (truncated between min and max).
</p>
<p>Therefore the formula for the PDF is:
</p>
\[\frac{1 + \sin(2\pi f x + p) - \ln(r)}{(x_{max} - x_{min})(1 - \ln(r)) + (\frac{1}{2\pi f})[\cos(2\pi f x_{min} - p) - \cos(2\pi f x_{max} - p)]}\] 
<p>where \(x =\) years, and \(x_{min}\) and \(x_{max}\) determine the truncated date range.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	# A sinewave with a period of 700 years
	x &lt;- seq(1500,4500, length.out=1000)
	y &lt;- sinewavePDF(x, min=2000, max=4000, f=1/700, p=0, r=0.2)
	plot(x,y,type='l')
	</code></pre>

<hr>
<h2 id='SPDsimulationTest'>Goodness of Fit test, using SPD simulation</h2><span id='topic+SPDsimulationTest'></span>

<h3>Description</h3>

<p>Calculates the data p-value given a model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SPDsimulationTest(data, calcurve, calrange, pars, type, inc=5, N=20000)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SPDsimulationTest_+3A_data">data</code></td>
<td>

<p>A dataframe of 14C dates. Requires 'age' and 'sd', and at least one of 'site' and 'phase'. Optional 'datingType' comprising '14C' and/or anything else.
</p>
</td></tr>
<tr><td><code id="SPDsimulationTest_+3A_calcurve">calcurve</code></td>
<td>

<p>A calibration curve object. Choose from intcal20 (default), shcal20, intcal13 or shcal13.
</p>
</td></tr>
<tr><td><code id="SPDsimulationTest_+3A_calrange">calrange</code></td>
<td>

<p>A vector of two calendar dates BP, giving the calendar range of CalArray. Can be in either order.
</p>
</td></tr>
<tr><td><code id="SPDsimulationTest_+3A_pars">pars</code></td>
<td>

<p>A single vector of one parameter combination. 
</p>
</td></tr>
<tr><td><code id="SPDsimulationTest_+3A_type">type</code></td>
<td>

<p>Choose 'CPL' for a Continuous Piecewise Linear model, 'exp' for an exponential model, or 'uniform' for a uniform model.
</p>
</td></tr>
<tr><td><code id="SPDsimulationTest_+3A_inc">inc</code></td>
<td>

<p>Increments to interpolate calendar years. Default = 5
</p>
</td></tr>
<tr><td><code id="SPDsimulationTest_+3A_n">N</code></td>
<td>

<p>The number of simulations to generate.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The returned list provides various summary statistics and timeseries of the observed and simulated data:
</p>
<p>timeseries: a data frame containing various CIs and: 
</p>
<p>calBP: a vector of calendar years BP. 
</p>
<p>expected.sim: a vector of the expected simulation (mean average of all N simulations). 
</p>
<p>local.sd: a vector of the local (for each year) standard deviation of all N simulations.
</p>
<p>model: a vector of the model PDF.
</p>
<p>SPD: a vector of the observed SPD PDF, generated from data.
</p>
<p>index: a vector of -1,0,+1 corresponding to the SPD points that are above, within or below the 95% CI of all N simulations.
</p>
<p>pvalue: the proportion of N simulated SPDs that have more points outside the 95% CI than the observed SPD has.
</p>
<p>observed.stat: the summary statistic for the observed data (number of points outside the 95% CI).
</p>
<p>simulated.stat: a vector of summary statistics (number of points outside the 95% CI), one for each simulated SPD.
</p>
<p>n.dates.all: the total number of dates in the whole data set. Trivially, the number of rows in data.
</p>
<p>n.dates.effective: the effective number of dates within the date range. Will be non-integer since a proportion of some dates will be outside the date range.
</p>
<p>n.phases.all: the total number of phases in the whole data set. 
</p>
<p>n.phases.effective: the effective number of phases within the date range. Will be non-integer since a proportion of some phases will be outside the date range.
</p>
<p>n.phases.internal: an integer subset of n.phases.all that have more than 50% of their total probability mass within the date range.
</p>
<p>The default N = 20000 can be increased if greater precision is required, however this can be very time costly.
</p>


<h3>Value</h3>

<p>Returns a list. See details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# trivial example showing a single date can never be rejected under a uniform model:
data &lt;- data.frame(age=6500, sd=50, phase=1, datingType='14C')
x &lt;- SPDsimulationTest(data, 
	calcurve=intcal20, 
	calrange=c(6000,9000), 
	pars=NULL, 
	type='uniform')
print(x$pvalue)

</code></pre>

<hr>
<h2 id='summedCalibrator'>Generates a summed probability distribution (SPD) of calibrated dates</h2><span id='topic+summedCalibrator'></span>

<h3>Description</h3>

<p>Generates a Summed Probability Distribution (SPD). Handles both C14 and other date types e.g., thermoluminescence.</p>


<h3>Usage</h3>

<pre><code class='language-R'>summedCalibrator(data, CalArray, normalise = 'standard', checks = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summedCalibrator_+3A_data">data</code></td>
<td>

<p>A dataframe of 14C dates. Requires 'age' and 'sd', and at least one of 'site' and 'phase'. Optional 'datingType' to include '14C' and anything else.
</p>
</td></tr>
<tr><td><code id="summedCalibrator_+3A_calarray">CalArray</code></td>
<td>

<p>A 2D probability array of the calibration curve generated by <code><a href="#topic+makeCalArray">makeCalArray</a></code> containing row names and column names. 
</p>
</td></tr>
<tr><td><code id="summedCalibrator_+3A_normalise">normalise</code></td>
<td>

<p>One of 'none', 'standard', or 'full'. 
</p>
</td></tr>
<tr><td><code id="summedCalibrator_+3A_checks">checks</code></td>
<td>

<p>Logical, performs various data checks if TRUE. Can be useful to set FALSE to avoid repetitive warnings if run in a loop. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses CalArray once to simultaneously calibrate and sum all 14C dates in data. The result is equivalent to calibrating each date, then summing. 
</p>
<p>Optionally 'datingType' can be provided in the data. Only '14C' will be calibrated in the usual way, anything else is assumed to be provided in calendar time. If 'datingType' is not provided, all dates are assumed to be 14C.
</p>
<p>If normalise is 'none', the output PD has an area equal to the total number of samples within the date range. This option is rarely required, but can be useful for example when plotting several SPDs and would also like to illustrate the relative magnitude of different datasets. However, if the date range used to generate CalArray does not encompass the entire dataset, some dates will have some probability mass outside the date range. Therefore the total probability can be non-integer and less than the sample size. 
</p>
<p>If normalise is 'standard', the output PD is normalised by the number of samples, giving an area equal to 1 provided all samples are within the date range. However, if the date range does not encompass all samples, some will have some probability mass outside the date range, resulting in the SPD area being less than 1.
</p>
<p>If normalise is 'full', the output PD has an area equal to 1. An appropriate use includes SPD simulation testing, where it is important to ensure each simulation has the same area. In contrast, it would be absurd to apply this full normalisation to the tiny tail of a single date that is otherwise mostly outside the date range.
</p>


<h3>Value</h3>

<p>Returns a single-column data frame of SPD probabilities. Row names are the calendar years.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	# SPD of three 14C dates
	CalArray &lt;- makeCalArray(intcal20, calrange=c(9000,10650), inc=1 )
	data &lt;- data.frame(
		age = c(8350,8900,9350), 
		sd = rep(50,3),
		datingType = rep('14C',3)
		)


	# with the default normalisation the SPD area is a little under 1
	# since one date is slighly outside the date range
	SPD &lt;- summedCalibrator(data, CalArray)
	plotPD(SPD)
	sum(SPD) 

	# without normalisation the total area is a little under 3
	SPD &lt;- summedCalibrator(data, CalArray, normalise='none')
	plotPD(SPD)
	sum(SPD)

	# with full normalisation the total area is exactly 1
	SPD &lt;- summedCalibrator(data, CalArray, normalise='full')
	plotPD(SPD)
	sum(SPD)
	</code></pre>

<hr>
<h2 id='summedCalibratorWrapper'>Quick calibration of dates, without the need to choose a date range or generate a CalArray.</h2><span id='topic+summedCalibratorWrapper'></span>

<h3>Description</h3>

<p>Wrapper function that easily generates and plots an SPD, at the cost of some user control.</p>


<h3>Usage</h3>

<pre><code class='language-R'>summedCalibratorWrapper(data, calcurve = intcal20, plot = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summedCalibratorWrapper_+3A_data">data</code></td>
<td>

<p>A dataframe of 14C dates. Requires 'age' and 'sd'.
</p>
</td></tr>
<tr><td><code id="summedCalibratorWrapper_+3A_calcurve">calcurve</code></td>
<td>

<p>A calibration curve object. Choose from intcal20 (default), shcal20, intcal13 or shcal13. 
</p>
</td></tr>
<tr><td><code id="summedCalibratorWrapper_+3A_plot">plot</code></td>
<td>

<p>By default (TRUE) will plot the SPD.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function to easily plot a calibrated Summed Probability Distribution from 14C dates. Automatically chooses a sensible date range and interpolation increments. Uses these to generate CalArray internally.
</p>


<h3>Value</h3>

<p>Returns a single-column data frame of SPD probabilities. Row names are the calendar years.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	# SPD of two 14C dates, calibrated through intcal20 (default)
	data &lt;- data.frame(
		age=c(6562,7144),
		sd=c(44,51)
		)
	x &lt;- summedCalibratorWrapper(data)

	# one date is not 14C
	data &lt;- data.frame(
		age = c(6562,7144),
		sd = c(44,51),
		datingType = c('14C','TL')
		)
	x &lt;- summedCalibratorWrapper(data)	
	</code></pre>

<hr>
<h2 id='summedPhaseCalibrator'>Generates a summed probability distribution (SPD) after phasing dates</h2><span id='topic+summedPhaseCalibrator'></span>

<h3>Description</h3>

<p>Generates a Summed Probability Distribution (SPD) after phasing dates.</p>


<h3>Usage</h3>

<pre><code class='language-R'>summedPhaseCalibrator(data, calcurve, calrange, inc=5, width=200)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summedPhaseCalibrator_+3A_data">data</code></td>
<td>

<p>A dataframe of 14C dates. Requires 'age' and 'sd', and at least one of 'site' and 'phase'. Optional 'datingType' comprising '14C' and/or anything else.
</p>
</td></tr>
<tr><td><code id="summedPhaseCalibrator_+3A_calcurve">calcurve</code></td>
<td>

<p>A calibration curve object. Choose from intcal20 (default), shcal20, intcal13 or shcal13.
</p>
</td></tr>
<tr><td><code id="summedPhaseCalibrator_+3A_calrange">calrange</code></td>
<td>

<p>A vector of two cal dates, giving the calendar range of CalArray. Can be in either order.
</p>
</td></tr>
<tr><td><code id="summedPhaseCalibrator_+3A_inc">inc</code></td>
<td>

<p>Increments to interpolate calendar years. Default = 5.
</p>
</td></tr>
<tr><td><code id="summedPhaseCalibrator_+3A_width">width</code></td>
<td>

<p>A timespan in 14C years used to automatically bin dates if they have not been phased, i.e., 'phase' is missing from the data. Default = 200.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Wrapper function to generate an overall SPD for phased dates. Internally this first generates an SPD for each phase. Data may be phased already, alternatively if 'phase' is not provided, this function will automatically bin dates into phases, see <code><a href="#topic+phaseCalibrator">phaseCalibrator</a></code>. Each phase's distribution is then summed, and the final SPD is normalised to unity.
</p>
<p>Optionally 'datingType' can be provided in the data. Only '14C' will be calibrated in the usual way, anything else is assumed to be provided in calendar time. If 'datingType' is not provided, all dates are assumed to be 14C.
</p>


<h3>Value</h3>

<p>Returns a single-column data frame of SPD probabilities. Row names are the calendar years.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	data &lt;- subset(SAAD, site %in% c('Carrizal','Pacopampa'))
	SPD &lt;- summedPhaseCalibrator(data, shcal20, c(2000,6000))
	plotPD(SPD)
	</code></pre>

<hr>
<h2 id='toy'>Toy population model</h2><span id='topic+toy'></span>

<h3>Description</h3>

<p>Data frame of a toy population model. Provides a discretised PDF across the time range of 7.5kyr to 5.5kyr BP. The model PDF outside this range is zero. Comprises two columns: year, pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	toy
	</code></pre>


<h3>Format</h3>

<p>A data frame comprising 4 rows and 2 columns
</p>


<h3>Source</h3>

<p>Toy used in Timpson et al (2020).
</p>


<h3>References</h3>

<p>Timpson, A., Barberena, R., Thomas, M.G., Méndez, C., Manning, K. 2020 Directly modelling population dynamics in the South American Arid Diagonal using 14C dates. Philosophical Transactions of the Royal Society B.
</p>

<hr>
<h2 id='uncalibrateCalendarDates'>Converts calendar dates to 14C dates</h2><span id='topic+uncalibrateCalendarDates'></span>

<h3>Description</h3>

<p>Randomly samples a 14C date from the calibration curve error ribbon, at the corresponding calendar date</p>


<h3>Usage</h3>

<pre><code class='language-R'>uncalibrateCalendarDates(dates, calcurve)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="uncalibrateCalendarDates_+3A_dates">dates</code></td>
<td>

<p>A vector of calendar dates. 
</p>
</td></tr>
<tr><td><code id="uncalibrateCalendarDates_+3A_calcurve">calcurve</code></td>
<td>

<p>A calibration curve object. Choose from intcal20 (default), shcal20, intcal13 or shcal13.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Conceptually this can be thought of as the reverse process of calibrating a 14C date into calendar time, however 'uncalibrating' is a misnomer as the full calibrated PD is not used. Instead, it uses a vector of calendar point estimates, and randomly samples 14C dates from the calibration curve error ribbon, at the corresponding calendar dates. Therefore values will differ each time. 
</p>


<h3>Value</h3>

<p>Returns a vector of 14C dates
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	uncalibrateCalendarDates(c(4500,5000), shcal20)

	# note the date outside the calcurve range has a 1 to 1 mapping between cal and c14 time
	uncalibrateCalendarDates(c(4500,70000), intcal20)

	# however, a soft fade is performed between the end of the calcurve and 60000
	uncalibrateCalendarDates(c(4500,58000), intcal20)
	</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
