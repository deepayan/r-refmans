<!DOCTYPE html><html><head><title>Help for package revtools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {revtools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#add_line_breaks'><p>Add line breaks to one or more strings</p></a></li>
<li><a href='#aggregate_tasks'><p>Combine (potentially overlapping) article sets generated by screening among a team of reviewers.</p></a></li>
<li><a href='#allocate_effort'><p>Determine optimal way to divide articles among 2 or more reviewers</p></a></li>
<li><a href='#avian_ecology_bibliography'><p>Bibliographic data from 20 papers on avian ecology</p></a></li>
<li><a href='#bibliography-class'><p>Description of class 'bibliography'</p></a></li>
<li><a href='#bibliography-methods'><p>Methods for class 'bibliography'</p></a></li>
<li><a href='#distribute_tasks'><p>Divide a set of articles among two or more reviewers</p></a></li>
<li><a href='#extract_unique_references'><p>Create a de-duplicated data.frame</p></a></li>
<li><a href='#find_duplicates'><p>Locate duplicated information within a data.frame</p></a></li>
<li><a href='#format_citation'><p>Format a citation</p></a></li>
<li><a href='#fuzz_functions'><p>Functions for fuzzy string matching</p></a></li>
<li><a href='#make_dtm'><p>Construct a document-term matrix (DTM)</p></a></li>
<li><a href='#merge_columns'><p>rbind two or more data frames with different columns</p></a></li>
<li><a href='#read_bibliography'><p>Import bibliographic data</p></a></li>
<li><a href='#revtools-package'><p>revtools: Tools to support reviews and evidence synthesis</p></a></li>
<li><a href='#revwords'><p>Load a set of stopwords</p></a></li>
<li><a href='#run_topic_model'><p>Calculate a topic model</p></a></li>
<li><a href='#screen_abstracts'><p>Shiny app for screening articles by their abstracts</p></a></li>
<li><a href='#screen_duplicates'><p>Shiny app for locating and excluding duplicated entries in a dataset</p></a></li>
<li><a href='#screen_titles'><p>Shiny app for screening articles by their titles</p></a></li>
<li><a href='#screen_topics'><p>Shiny app for screening bibliographies using topic models</p></a></li>
<li><a href='#screen_topics_progress-class'><p>Description of class 'screen_topics_progress'</p></a></li>
<li><a href='#screen_topics_progress-methods'><p>Methods for class 'screen_topics_progress'</p></a></li>
<li><a href='#tag_lookup'><p>Lookup table for ris tags</p></a></li>
<li><a href='#write_bibliography'><p>Export imported bibliographic data as .bib or .ris formats</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.4.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-12-17</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools to Support Evidence Synthesis</td>
</tr>
<tr>
<td>Description:</td>
<td>Researchers commonly need to summarize scientific information, a process known as 'evidence synthesis'. The first stage of a synthesis process (such as a systematic review or meta-analysis) is to download a list of references from academic search engines such as 'Web of Knowledge' or 'Scopus'. The traditional approach to systematic review is then to sort these data manually, first by locating and removing duplicated entries, and then screening to remove irrelevant content by viewing titles and abstracts (in that order). 'revtools' provides interfaces for each of these tasks. An alternative approach, however, is to draw on tools from machine learning to visualise patterns in the corpus. In this case, you can use 'revtools' to render ordinations of text drawn from article titles, keywords and abstracts, and interactively select or exclude individual references, words or topics.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>ade4, modeltools, ngram, plotly, slam, shiny, shinydashboard,
SnowballC, stringdist, tm, topicmodels, viridisLite</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://revtools.net">https://revtools.net</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mjwestgate/revtools/issues">https://github.com/mjwestgate/revtools/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-12-17 04:28:06 UTC; martin_westgate</td>
</tr>
<tr>
<td>Author:</td>
<td>Martin J. Westgate [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Martin J. Westgate &lt;martinjwestgate@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-12-17 07:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='add_line_breaks'>Add line breaks to one or more strings</h2><span id='topic+add_line_breaks'></span>

<h3>Description</h3>

<p>This function takes a vector of strings and adds line breaks every <code>n</code> characters. Primarily built to be called internally by <code>format_citation</code>, this function has been made available as it can be useful in other contexts.</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_line_breaks(x, n = 50, html = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_line_breaks_+3A_x">x</code></td>
<td>
<p>Either a string or a vector; if the vector is not of class <code>character</code> if will be coerced to one using <code>as.character</code>.</p>
</td></tr>
<tr><td><code id="add_line_breaks_+3A_n">n</code></td>
<td>
<p>Numeric: The number of characters that should separate consecutive line breaks.</p>
</td></tr>
<tr><td><code id="add_line_breaks_+3A_html">html</code></td>
<td>
<p>Logical: Should the function add HTML line breaks (<code>&lt;br&gt;</code>)? Defaults to FALSE, in which case the newline sequence is used instead (escape-n).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Line breaks are only added between words, so the value of <code>n</code> is acutally a threshold value rather than being matched exactly.
</p>


<h3>Value</h3>

<p>Returns the input vector unaltered except for the addition of line breaks.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)
add_line_breaks(x$title)
</code></pre>

<hr>
<h2 id='aggregate_tasks'>Combine (potentially overlapping) article sets generated by screening among a team of reviewers.</h2><span id='topic+aggregate_tasks'></span>

<h3>Description</h3>

<p>A common task in systematic review is to divide a dataset of articles located by a search (typically involving &gt;1 databases) and distributing them amongst a team of reviewers for screening. This function takes a dataset divided using <code>link{distribute_tasks}</code> and recombines them into a single <code>data.frame</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregate_tasks(file_names, match_column, selection_column, reviewer_names)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregate_tasks_+3A_file_names">file_names</code></td>
<td>
<p>a vector or list of file paths used to locate screened files. Must be in .csv format.</p>
</td></tr>
<tr><td><code id="aggregate_tasks_+3A_match_column">match_column</code></td>
<td>
<p>The name of the column used to match identical references. In revtools this is 'label', which is the default here.</p>
</td></tr>
<tr><td><code id="aggregate_tasks_+3A_selection_column">selection_column</code></td>
<td>
<p>The name of the column used to store 'selection' data; i.e. which entries have been retained and which excluded. In revtools this is 'selected', which is the default here.</p>
</td></tr>
<tr><td><code id="aggregate_tasks_+3A_reviewer_names">reviewer_names</code></td>
<td>
<p>Optional vector of names used to label the 'results' columns in the resulting <code>data.frame</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a data.frame with one row per unique value of <code>match_column</code>, showing the content of <code>selection_column</code> for each reviewer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+distribute_tasks">distribute_tasks</a></code> for the inverse problem of dividing a single dataset amongst many reviewers.
</p>

<hr>
<h2 id='allocate_effort'>Determine optimal way to divide articles among 2 or more reviewers</h2><span id='topic+allocate_effort'></span>

<h3>Description</h3>

<p>This function takes information on the number (and optionally, identity) of reviewers and combines it with data on how it should be allocated to return a data.frame showing the proportion of articles to be assessed by each person.</p>


<h3>Usage</h3>

<pre><code class='language-R'>allocate_effort(reviewers, effort, proportion_checked,
    max_reviewers = 3, precision = 2, quiet = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="allocate_effort_+3A_reviewers">reviewers</code></td>
<td>
<p>Either an integer giving the number of reviewers, or a vector of strings giving reviewer names.</p>
</td></tr>
<tr><td><code id="allocate_effort_+3A_effort">effort</code></td>
<td>
<p>Either a single number giving the proportion of articles to be reviewed by all reviewers, or a numeric vector giving a unique proportion for each reviewer. If the latter must be consistent with number given by 'reviewers' above.</p>
</td></tr>
<tr><td><code id="allocate_effort_+3A_proportion_checked">proportion_checked</code></td>
<td>
<p>Numeric value giving the proportion of entries that should be screened by two or more reviewers.</p>
</td></tr>
<tr><td><code id="allocate_effort_+3A_max_reviewers">max_reviewers</code></td>
<td>
<p>the maximum number of reviewers that should screen any single article. Useful for avoiding redundancy when working with large teams.</p>
</td></tr>
<tr><td><code id="allocate_effort_+3A_precision">precision</code></td>
<td>
<p>Number of decimal places with which to report results. Defaults to 2.</p>
</td></tr>
<tr><td><code id="allocate_effort_+3A_quiet">quiet</code></td>
<td>
<p>Logical - should the function return a summary of the proportion of articles allocated to each reviewer? Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function makes an attempt to return a sensible distribution of effort among a number of reviewers. If effort is given as a single value (or not provided at all), then the result is calculated exactly such that all proportions sum to 1. Conversely, if effort is given as a numeric vector of length &gt;1 and contains a range of values, then the function tries to optimize the proportion of articles allocated to each person, while also matching constrains given by <code>proportion_checked</code> and <code>max_reviewers</code>. In this case, and depending on the inputs, it is possible that no perfect solution will exist, meaning that some reviewers may be allocated a higher proportion of articles than anticipated. For this reason it is often worth setting <code>quiet = FALSE</code> when running <code>allocate_effort</code> with variation in reviewer effort, to allow assessment of the results.
</p>


<h3>Value</h3>

<p>Invisibly returns a data.frame giving one column per reviewer plus an extra column of proportions. The reviewer columns are binary and show which proportions apply to each person or combination of people.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+distribute_tasks">distribute_tasks</a></code> for how to use the output of <code>allocate_effort</code> to split a dataset.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># import some data
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)
# simple case - split evenly among 4 reviewers
result &lt;- allocate_effort(4, quiet = TRUE)
# more complex - specify names and amount of overlap
result &lt;- allocate_effort(
  reviewers = c("john", "paul", "george", "ringo"),
  proportion_checked = 0.2,
  max_reviewers = 3,
  quiet = TRUE
  )
# most complex - specify uneven effort among reviewers (experimental)
result &lt;- allocate_effort(
  reviewers = 4,
  effort = c(0.9, 0.7, 0.5, 0.3),
  max_reviewers = 3,
  quiet = TRUE
  )
</code></pre>

<hr>
<h2 id='avian_ecology_bibliography'>Bibliographic data from 20 papers on avian ecology</h2><span id='topic+avian_ecology_bibliography'></span>

<h3>Description</h3>

<p>This dataset lists basic information (title, authors, keywords etc.) for 20 scientific articles on avian ecology, stored in .ris format.</p>


<h3>Format</h3>

<p>A list of length 20, containing lists of named attributes for each article.</p>


<h3>Source</h3>

<p>Originally downloaded from Scopus.</p>


<h3>Examples</h3>

<pre><code class='language-R'>file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)
summary(x)
</code></pre>

<hr>
<h2 id='bibliography-class'>Description of class 'bibliography'</h2><span id='topic+bibliography-class'></span>

<h3>Description</h3>

<p>An S3 class designed to store data from common bibliographic formats in a standard way</p>


<h3>Details</h3>

<p>Class 'bibliography' is a nested list format; each object is a list containing multiple references, where each reference is a list with information on author, journal etc. Each entry has a unique name that is preserved in the 'label' column if the user subsequently calls <code>as.data.frame</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+read_bibliography">read_bibliography</a></code>, <code><a href="#topic+write_bibliography">write_bibliography</a></code>
</p>

<hr>
<h2 id='bibliography-methods'>Methods for class 'bibliography'</h2><span id='topic+as.bibliography'></span><span id='topic+c.bibliography'></span><span id='topic+as.data.frame.bibliography'></span><span id='topic+print.bibliography'></span><span id='topic+summary.bibliography'></span><span id='topic++5B.bibliography'></span><span id='topic+bibliography-methods'></span>

<h3>Description</h3>

<p>This is a small number of standard methods for interacting with class 'bibliography'. More may be added later.</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.bibliography(x, ...)
	## S3 method for class 'bibliography'
as.data.frame(x, ...)
	## S3 method for class 'bibliography'
x[n]
	## S3 method for class 'bibliography'
c(...)
	## S3 method for class 'bibliography'
print(x, n, ...)
	## S3 method for class 'bibliography'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bibliography-methods_+3A_x">x</code></td>
<td>
<p>An object of class 'bibliography'</p>
</td></tr>
<tr><td><code id="bibliography-methods_+3A_object">object</code></td>
<td>
<p>An object of class 'bibliography'</p>
</td></tr>
<tr><td><code id="bibliography-methods_+3A_n">n</code></td>
<td>
<p>Number of items to select/print</p>
</td></tr>
<tr><td><code id="bibliography-methods_+3A_...">...</code></td>
<td>
<p>Any further information</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># import some data
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location, return_df = FALSE)

# basic descriptions
summary(x)
print(x)
x[1]

# append two bibliography objects
y &lt;- c(x[1:5], x[2:4])

# conversion to and from data.frame
y &lt;- as.data.frame(x)
x_new &lt;- as.bibliography(y)
</code></pre>

<hr>
<h2 id='distribute_tasks'>Divide a set of articles among two or more reviewers</h2><span id='topic+distribute_tasks'></span>

<h3>Description</h3>

<p>A common task in systematic review is to divide a dataset of articles located by a search (typically involving &gt;1 databases) and distributing them amongst a team of reviewers for screening. This function takes a dataset divides it among the specified number of reviewers, returning the resulting data.frames either in a list to the workspace, or (by default) as a set of .csv files in the specified directory. The resulting files can be passed to any of the screening functions provided by revtools, i.e. <code><a href="#topic+screen_titles">screen_titles</a></code>, <code><a href="#topic+screen_abstracts">screen_abstracts</a></code>, or <code><a href="#topic+screen_topics">screen_topics</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>distribute_tasks(data, reviewers, write_csv = TRUE,
    file_name = "reviewer.csv", return_data = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distribute_tasks_+3A_data">data</code></td>
<td>
<p>a vector of strings</p>
</td></tr>
<tr><td><code id="distribute_tasks_+3A_reviewers">reviewers</code></td>
<td>
<p>Either a <code>data.frame</code> as returned by <code>allocate_effort</code>, an integer giving the number of reviewers, or a vector of strings giving reviewer names.</p>
</td></tr>
<tr><td><code id="distribute_tasks_+3A_write_csv">write_csv</code></td>
<td>
<p>Logical - should the function write a set of csv files (1 per reviewer)? Defaults to TRUE</p>
</td></tr>
<tr><td><code id="distribute_tasks_+3A_file_name">file_name</code></td>
<td>
<p>a file path &amp; name showing where .csv files should be saved. Ignored if <code>write_csv</code> is FALSE. Defaults to 'reviewer_[name].csv'.</p>
</td></tr>
<tr><td><code id="distribute_tasks_+3A_return_data">return_data</code></td>
<td>
<p>Logical - should a list be (invisibly) returned, in which each entry is the data sent to a single reviewer? Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="distribute_tasks_+3A_...">...</code></td>
<td>
<p>Further arguments passed to <code>allocate_effort</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dataset is allocated each author in the proportion of articles specified by <code><a href="#topic+allocate_effort">allocate_effort</a></code>, with the identity of articles passed to reviewer being chosen by <code>rnorm</code>. As a result, this function is very sensitive to the inputs provided to <code><a href="#topic+allocate_effort">allocate_effort</a></code>, so it is often worth running that function first and checking the results to be certain that effort is being distributed in a way that you are happy with.
</p>


<h3>Value</h3>

<p>Invisibly returns a list of data.frames, each with same columns as <code>data</code> but containing only a subset of rows.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+allocate_effort">allocate_effort</a></code> for a detailed description of how the division among reviewers is accomplished.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># import some data
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)
result &lt;- distribute_tasks(x, 4, write_csv = FALSE) # split evenly among 4 reviewers
</code></pre>

<hr>
<h2 id='extract_unique_references'>Create a de-duplicated data.frame</h2><span id='topic+extract_unique_references'></span>

<h3>Description</h3>

<p>Take a data.frame of bibliographic information showing potential duplicates (as returned by <code>find_duplicates</code>), and return a data.frame of unique references.</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_unique_references(x, matches)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_unique_references_+3A_x">x</code></td>
<td>
<p>a <code>data.frame</code> to be subsetted</p>
</td></tr>
<tr><td><code id="extract_unique_references_+3A_matches">matches</code></td>
<td>
<p>either a vector of matches, e.g. as returned from <code><a href="#topic+find_duplicates">find_duplicates</a></code>, or a column name (specified as a number or a string) from x showing where matches are stored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a subsetted <code>data.frame</code> containing one entry for each group identified in <code>matches</code>.
</p>


<h3>Note</h3>

<p>This function creates a simplified version of <code>x</code>, by extracting the reference from each group of 'identical' references that contains the most text. It is assumed that this is the most 'complete' record of those available in the dataset. This function does not merge data from multiple 'identical' records due to the potential for mis-matching that this approach would create.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find_duplicates">find_duplicates</a></code> for duplicate identification; <code><a href="#topic+screen_duplicates">screen_duplicates</a></code> for an interactive alternative to duplicate removal.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># import data
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools"
)
x &lt;- read_bibliography(file_location)

# generate duplicated references (for example purposes)
x_duplicated &lt;- rbind(x, x[1:5,])

# locate and extract unique references
x_check &lt;- find_duplicates(x_duplicated)
x_unique &lt;- extract_unique_references(x_duplicated, matches = x_check)
</code></pre>

<hr>
<h2 id='find_duplicates'>Locate duplicated information within a data.frame</h2><span id='topic+find_duplicates'></span>

<h3>Description</h3>

<p>Identify potential duplicates within a <code>data.frame</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_duplicates(data, match_variable, group_variables,
  match_function, method, threshold,
  to_lower = FALSE, remove_punctuation = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_duplicates_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> containing data to be matched</p>
</td></tr>
<tr><td><code id="find_duplicates_+3A_match_variable">match_variable</code></td>
<td>
<p>a length-1 integer or string listing the column in which duplicates should be sought. Defaults to doi where available, followed by title. If neither are found the function will fail.</p>
</td></tr>
<tr><td><code id="find_duplicates_+3A_group_variables">group_variables</code></td>
<td>
<p>an optional vector listing the columns to use as grouping variables; that is, categories withing which duplicates should be sought (see 'note'). Optionally NULL to compare all entries against one another.</p>
</td></tr>
<tr><td><code id="find_duplicates_+3A_match_function">match_function</code></td>
<td>
<p>a function to calculate dissimilarity between strings. Defaults to &quot;exact&quot; if doi's are available or &quot;stringdist&quot; otherwise.</p>
</td></tr>
<tr><td><code id="find_duplicates_+3A_method">method</code></td>
<td>
<p>the required 'method' option that corresponds with <code>match_function</code>. Defaults to NULL if <code>match_function</code> is &quot;exact&quot;, &quot;osa&quot; for match_function == &quot;stringdist&quot;, or &quot;fuzz_m_ratio&quot; for match_function == &quot;fuzzdist&quot;.</p>
</td></tr>
<tr><td><code id="find_duplicates_+3A_threshold">threshold</code></td>
<td>
<p>an upper limit above which similar articles are not recognized as duplicates. Defaults to 5 for stringdist and 0.1 for fuzzdist. Ignored if <code>match_function</code> == &quot;exact&quot;.</p>
</td></tr>
<tr><td><code id="find_duplicates_+3A_to_lower">to_lower</code></td>
<td>
<p>logical: should text be made lower case prior to searching? Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="find_duplicates_+3A_remove_punctuation">remove_punctuation</code></td>
<td>
<p>logical: should punctuation be removed prior to searching? Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an integer vector, in which entries with the same integer have been selected as duplicates by the selected algorithm.</p>


<h3>Note</h3>

<p><code>find_duplicates</code> runs a <code>while</code> loop. It starts by checking the first entry of <code>data</code> against every other entry for potential duplicates. If any matches are found, those entries are excluded from consideration. The loop then continues until all entries have been checked. In order to work, this function requires the <code>data</code> and <code>match_variable</code> arguments be specified. The remaining arguments affects how duplicates are identified, and can also strongly influence the speed of the outcome.
</p>
<p>The argument <code>group_variables</code> specifies variables that contain supplementary information that can reduce the number of entries that need to be searched. For example, you might want to only match article titles if they occur within the same journal, or in the same year. The more variables you specify, the fewer pairs of matches that have to be tested to locate duplicates, greatly increasing the speed of the algorithm. Conversely, if no variables are specified, then each entry is checked against every other entry that has yet to be excluded from the dataset. This is fine for small datasets, but massively increases computation time for large datasets.
</p>
<p>Missing values are handled differently. Entries that are <code>NA</code> for <code>match_variable</code> are always labelled as unique values, and are not checked for duplicates against the rest of the dataset. However, entries of <code>group_variables</code> that are <code>NA</code> are included in every comparison.
</p>
<p><code>find_duplicates</code> contains three 'built-in' methods for string matching. &quot;stringdist&quot; calls the function of the same name from the package <code>stringdist</code>; ditto for &quot;fuzzdist&quot; which is in <code>revtools</code>, but based on the Python library <code>fuzzywuzzy</code>. &quot;exact&quot; simply searches for exact matches. In principle you could call any function for string matching, so long as it accepts the arguments <code>a</code>, <code>b</code> and <code>method</code> (see documentation on stringdist for details), and returns a measure of distance (i.e. not similarity).
</p>
<p>Finally, <code>to_lower</code> and <code>remove_punctuation</code> specify whether to transform the text prior to searching for duplicates.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+screen_duplicates">screen_duplicates</a></code> and <code><a href="#topic+extract_unique_references">extract_unique_references</a></code> for manual and automated screening (respectively) of results from this function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># import data
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)

# generate then locate some 'fake' duplicates
x_duplicated &lt;- rbind(x, x[1:5,])
x_check &lt;- find_duplicates(x_duplicated)
# returns a vector of potential matches
</code></pre>

<hr>
<h2 id='format_citation'>Format a citation</h2><span id='topic+format_citation'></span><span id='topic+format_citation.bibliography'></span><span id='topic+format_citation.list'></span><span id='topic+format_citation.data.frame'></span>

<h3>Description</h3>

<p>takes an object of class <code>data.frame</code> or <code>bibliography</code> and returns a formatted citation.</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_citation(data, details = TRUE, abstract = FALSE,
  add_html = FALSE, line_breaks = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="format_citation_+3A_data">data</code></td>
<td>
<p>An object of class <code>data.frame</code> or <code>bibliography</code>.</p>
</td></tr>
<tr><td><code id="format_citation_+3A_details">details</code></td>
<td>
<p>Logical: Should identifying information such as author names &amp; journal titles be displayed? Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="format_citation_+3A_abstract">abstract</code></td>
<td>
<p>Logical: Should the abstract be shown (if available)? Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="format_citation_+3A_add_html">add_html</code></td>
<td>
<p>Logical: Should the journal title be italicized using html codes? Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="format_citation_+3A_line_breaks">line_breaks</code></td>
<td>
<p>Either logical, stating whether line breaks should be added, or numeric stating how many characters should separate consecutive line breaks. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="format_citation_+3A_...">...</code></td>
<td>
<p>any other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a string of length == length(x), containing formatted citations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location, return_df = FALSE)
format_citation(x[[1]])
format_citation(as.data.frame(x)[1, ]) # same result
</code></pre>

<hr>
<h2 id='fuzz_functions'>Functions for fuzzy string matching</h2><span id='topic+fuzz_functions'></span><span id='topic+fuzzdist'></span><span id='topic+fuzz_m_ratio'></span><span id='topic+fuzz_partial_ratio'></span><span id='topic+fuzz_token_sort_ratio'></span><span id='topic+fuzz_token_set_ratio'></span>

<h3>Description</h3>

<p>Duplicate of functions from the Python library 'fuzzywuzzy' (<a href="https://github.com/seatgeek/fuzzywuzzy">https://github.com/seatgeek/fuzzywuzzy</a>). These functions have been recoded from scratch based on the description given <a href="http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python">here</a>. For consistency with <code>stringdist</code>, however, these functions are computed as distances rather than similarities; i.e. low values signify similar strings.</p>


<h3>Usage</h3>

<pre><code class='language-R'>fuzzdist(a, b, method)
fuzz_m_ratio(a, b)
fuzz_partial_ratio(a, b)
fuzz_token_sort_ratio(a, b)
fuzz_token_set_ratio(a, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fuzz_functions_+3A_a">a</code></td>
<td>
<p>a string</p>
</td></tr>
<tr><td><code id="fuzz_functions_+3A_b">b</code></td>
<td>
<p>a vector containing one or more strings</p>
</td></tr>
<tr><td><code id="fuzz_functions_+3A_method">method</code></td>
<td>
<p>a function to be called by 'fuzzdist'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A score of same length as y, giving the proportional dissimilarity between x and y.
</p>


<h3>Note</h3>

<p><code>fuzz_m_ratio</code> is a measure of the number of letters that match between two strings. It is calculated as one minus two times the number of matched characters, divided by the number of characters in both strings.
</p>
<p><code>fuzz_partial_ratio</code> calculates the extent to which one string is a subset of the other. If one string is a perfect subset, then this will be zero.
</p>
<p><code>fuzz_token_sort_ratio</code> sorts the words in both strings into alphabetical order, and checks their similarity using <code>fuzz_m_ratio</code>.
</p>
<p><code>fuzz_token_set_ratio</code> is similar to <code>fuzz_token_sort_ratio</code>, but compares both sorted strings to
each other, and to a third group made of words common to both strings. It then returns the maximum value of <code>fuzz_m_ratio</code> from these comparisons.
</p>
<p><code>fuzzdist</code> is a wrapper function, for compatability with stringdist.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fuzz_m_ratio("NEW YORK METS", "NEW YORK MEATS")
fuzz_partial_ratio(
  "YANKEES",
  c("NEW YORK YANKEES", "something else", "YNAKEES")
)
fuzz_token_sort_ratio("New York Mets vs Atlanta Braves", "Atlanta Braves vs New York Melts")
fuzz_token_set_ratio(
  a = "mariners vs angels other words",
  b = c("los angeles angels of anaheim at seattle mariners", "angeles angels of anaheim ")
)
</code></pre>

<hr>
<h2 id='make_dtm'>Construct a document-term matrix (DTM)</h2><span id='topic+make_dtm'></span>

<h3>Description</h3>

<p>Takes bibliographic data and converts it to a DTM for passing to topic models.</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_dtm(x, stop_words, min_freq = 0.01, max_freq = 0.85,
  bigram_check = TRUE, bigram_quantile = 0.8,
  retain_empty_rows = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_dtm_+3A_x">x</code></td>
<td>
<p>a vector or <code>data.frame</code> containing text</p>
</td></tr>
<tr><td><code id="make_dtm_+3A_stop_words">stop_words</code></td>
<td>
<p>optional vector of strings, listing terms to be removed from the DTM prior to analysis. Defaults to <code>revwords()</code>.</p>
</td></tr>
<tr><td><code id="make_dtm_+3A_min_freq">min_freq</code></td>
<td>
<p>minimum proportion of entries that a term must be found in to be retained in the analysis. Defaults to 0.01.</p>
</td></tr>
<tr><td><code id="make_dtm_+3A_max_freq">max_freq</code></td>
<td>
<p>maximum proportion of entries that a term must be found in to be retained in the analysis. Defaults to 0.85.</p>
</td></tr>
<tr><td><code id="make_dtm_+3A_bigram_check">bigram_check</code></td>
<td>
<p>logical: should ngrams be searched for?</p>
</td></tr>
<tr><td><code id="make_dtm_+3A_bigram_quantile">bigram_quantile</code></td>
<td>
<p>what quantile of ngrams should be retained. Defaults to 0.8; i.e. the 80th percentile of bigram frequencies after removing all bigrams with frequencies &lt;=2.</p>
</td></tr>
<tr><td><code id="make_dtm_+3A_retain_empty_rows">retain_empty_rows</code></td>
<td>
<p>logical: should the function return an object with the same length as the input string (TRUE), or remove rows that contain no text after filtering rare &amp; common words etc? (FALSE, the default). The latter is the default because this is required by <code>run_topic_model</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is primarily intended to be called internally by <code>screen_topics</code>, but is made available for users to generate their own topic models with the same properties as those in revtools.
</p>
<p>This function uses some standard tools like stemming, converting words to lower case, and removal of numbers or punctuation. It also replaces stemmed words with the shortest version of all terms that share the same stem, which doesn't affect the calculations, but makes the resulting analyses easier to interpret. It doesn't use part-of-speech tagging.
</p>
<p>Words that occur in 2 entries or fewer are always removed by <code>make_dtm</code>, so values of <code>min_freq</code> that result in a threshold below this will not affect the result. Arguments to <code>max_freq</code> are passed as is. Similarly words consisting of three letters or fewer are removed.
</p>
<p>If <code>retain_empty_rows</code> is FALSE (the default) and the object returned is named <code>z</code>, then <code>as.numeric(z$dimnames$Docs)</code> provides an index to which entries have been retained from the input vector (<code>x</code>).
</p>


<h3>Value</h3>

<p>An object of class <code>simple_triplet_matrix</code>, listing the terms (columns) present in each row or string.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+run_topic_model">run_topic_model</a></code>, <code><a href="#topic+screen_topics">screen_topics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># import some data
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)

# construct a document-term matrix
# note: this can take a long time to run for large datasets
x_dtm &lt;- make_dtm(x$title)

# to return a matrix where length(output) == nrow(x)
x_dtm &lt;- make_dtm(x$title, retain_empty_rows = TRUE)
x_dtm &lt;- as.matrix(x_dtm) # to convert to class 'matrix'
dim(x_dtm) # 20 articles, 10 words
</code></pre>

<hr>
<h2 id='merge_columns'>rbind two or more data frames with different columns</h2><span id='topic+merge_columns'></span>

<h3>Description</h3>

<p>Take two data.frames with arbitraily different numbers and names of columns, and rbind them together.</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_columns(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge_columns_+3A_x">x</code></td>
<td>
<p>A data.frame, or a list containing multiple data.frames.</p>
</td></tr>
<tr><td><code id="merge_columns_+3A_y">y</code></td>
<td>
<p>An optional second data.frame. Only used if x is not a list.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class 'data.frame', containing data from all input data frames. That is, all unique columns are preserved, while rows that are missing data for a given column are assigned NA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># import some data
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)
y &lt;- x[, 1:3]
z &lt;- merge_columns(x, y)
# or equivalently
z &lt;- merge_columns(list(x, y))
</code></pre>

<hr>
<h2 id='read_bibliography'>Import bibliographic data</h2><span id='topic+read_bibliography'></span>

<h3>Description</h3>

<p>Import standard formats from academic search engines and referencing software.</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_bibliography(filename, return_df = TRUE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_bibliography_+3A_filename">filename</code></td>
<td>
<p>A vector or list containing paths to one or more bibliographic files.</p>
</td></tr>
<tr><td><code id="read_bibliography_+3A_return_df">return_df</code></td>
<td>
<p>logical; should the object returned be a data.frame? Defaults to TRUE. If FALSE, returns an object of class <code>bibliography</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function aims to import bibliographic data from a range of formats in a consistent manner.
</p>
<p>If the user provides a number of paths in a vector or list, then files are imported in sequence and joined into a single <code>data.frame</code> (or a <code>list</code> if <code>return_df = FALSE</code>) using <code><a href="#topic+merge_columns">merge_columns</a></code>. If <code>return_df = TRUE</code> (the default) then an extra column called 'file_name' is appended to the resulting <code>data.frame</code> to show the file in which each entry originated.
</p>
<p>If the file is in .csv format, then <code>read_bibliography</code> will import the file using <code>read.csv</code>, with three changes. First, it ensures that the first column contains an index (i.e. a unique value for each row), and creates one if it is absent. Second, it converts column names to lower case and switches all delimiters to underscores. Third, it ensures that author names are delimited by 'and' for consistency with <code>format_citation</code>.
</p>
<p>If the file is of any type other than .csv, <code>read_bibliography</code> auto-detects document formatting, first by detecting whether the document is ris-like or bib-like, and then running an appropriate import function depending on the result. In the case of ris-like files (including files from 'Endnote' &amp; 'Web of Science'), this involves attempting to detect both the delimiter between successive entries, and the means of separating tag labels (e.g. 'AU', 'TI') from their information content. Attempts have been made to ensure consistency with .ris, .bib, medline (.nbib) or web of science (.ciw) formats. Except for .csv, file extensions are not used to determine file type, and are ignored except to locate the file.
</p>
<p>If the imported file is in a ris-like format, then the object returned by <code>read_bibliography</code> will have different headings from the source document. This feature attempts to ensure consistency across file types. Tag substitutions are made using a lookup table, which can be viewed by calling <code><a href="#topic+tag_lookup">tag_lookup</a></code>. Unrecognized tags are grouped in the resulting <code>bibliography</code> object under the heading 'further_info'.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>data.frame</code> if <code>return_df</code> is <code>TRUE</code>; otherwise an object of class <code>bibliography</code>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+bibliography-class">bibliography-class</a></code>, <code><a href="#topic+tag_lookup">tag_lookup</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)
class(x) # = data.frame
x &lt;- read_bibliography(file_location, return_df = FALSE)
class(x) # = bibliography
summary(x)
</code></pre>

<hr>
<h2 id='revtools-package'>revtools: Tools to support reviews and evidence synthesis</h2><span id='topic+revtools'></span>

<h3>Description</h3>

<p>Researchers commonly need to summarize scientific information, a process known as 'evidence synthesis'. The first stage of a synthesis process (such as a systematic review or meta-analysis) is to download a list of references from academic search engines such as 'Web of Knowledge' or 'Scopus'. The traditional approach to systematic review is then to sort these data manually, first by locating and removing duplicated entries, and then screening to remove irrelevant content by viewing titles and abstracts (in that order). 'revtools' provides interfaces for each of these tasks. An alternative approach, however, is to draw on tools from machine learning to visualise patterns in the corpus. In this case, you can use 'revtools' to render ordinations of text drawn from article titles, keywords and abstracts, and interactively select or exclude individual references, words or topics.</p>


<h3>Functions</h3>

<p><strong>Article screening</strong>
</p>

<ul>
<li> <p><code><a href="#topic+screen_duplicates">screen_duplicates</a></code> Screen for duplicates
</p>
</li>
<li> <p><code><a href="#topic+screen_titles">screen_titles</a></code> Screen articles by title
</p>
</li>
<li> <p><code><a href="#topic+screen_abstracts">screen_abstracts</a></code> Screen articles by abstract
</p>
</li>
<li> <p><code><a href="#topic+screen_topics">screen_topics</a></code> Screen data by topic
</p>
</li></ul>

<p><strong>Bibliographic methods</strong>
</p>

<ul>
<li> <p><code><a href="#topic+read_bibliography">read_bibliography</a></code> Import bibliographic data
</p>
</li>
<li> <p><code><a href="#topic+write_bibliography">write_bibliography</a></code> Export bibliographic data
</p>
</li>
<li> <p><code><a href="#topic+bibliography-class">bibliography-class</a></code> Format for storing bibliographic data
</p>
</li>
<li> <p><code><a href="#topic+bibliography-methods">bibliography-methods</a></code> Print, summary, as.bibliography, as.data.frame and [ methods for class 'bibliography'
</p>
</li>
<li> <p><code><a href="#topic+tag_lookup">tag_lookup</a></code> Lookup table for ris tag transformations
</p>
</li>
<li> <p><code><a href="#topic+merge_columns">merge_columns</a></code> rbind two data.frames with different numbers of columns
</p>
</li>
<li> <p><code><a href="#topic+format_citation">format_citation</a></code> Return a clean citation from a bibliography or data.frame
</p>
</li>
<li> <p><code><a href="#topic+add_line_breaks">add_line_breaks</a></code> Set a maximum character width for strings in a vector
</p>
</li></ul>

<p><strong>Distributing tasks among a team</strong>
</p>

<ul>
<li> <p><code><a href="#topic+allocate_effort">allocate_effort</a></code> Specify how to distribute screening effort among a team of reviewers
</p>
</li>
<li> <p><code><a href="#topic+distribute_tasks">distribute_tasks</a></code> Split a dataset among a team of reviewers
</p>
</li>
<li> <p><code><a href="#topic+aggregate_tasks">aggregate_tasks</a></code> Combine screening results from a team of reviewers
</p>
</li></ul>

<p><strong>Duplicate detection</strong>
</p>

<ul>
<li> <p><code><a href="#topic+fuzz_functions">fuzz_functions</a></code> Fuzzy string matching
</p>
</li>
<li> <p><code><a href="#topic+find_duplicates">find_duplicates</a></code> Locate potentially duplicated references
</p>
</li>
<li> <p><code><a href="#topic+extract_unique_references">extract_unique_references</a></code> Return a data.frame with only 'unique' references
</p>
</li>
<li> <p><code><a href="#topic+screen_duplicates">screen_duplicates</a></code> Screen for duplicates
</p>
</li></ul>

<p><strong>Text mining</strong>
</p>

<ul>
<li> <p><code><a href="#topic+revwords">revwords</a></code> Stopwords used in revtools functions
</p>
</li>
<li> <p><code><a href="#topic+make_dtm">make_dtm</a></code> Construct a Document-Term Matrix from bibliographic data
</p>
</li>
<li> <p><code><a href="#topic+run_topic_model">run_topic_model</a></code> Wrapper function for topic models
</p>
</li>
<li> <p><code><a href="#topic+screen_topics">screen_topics</a></code> Screen data by topic
</p>
</li></ul>


<hr>
<h2 id='revwords'>Load a set of stopwords</h2><span id='topic+revwords'></span>

<h3>Description</h3>

<p>Generates a list of stopwords, consisting of all the terms given by <code>tm::stopwords</code>, plus some extra terms (mainly words that designate numbers).</p>


<h3>Usage</h3>

<pre><code class='language-R'>revwords()
</code></pre>


<h3>Value</h3>

<p>A vector of stopwords in English.
</p>


<h3>Note</h3>

<p>This is primarily an internal function, but may be useful in other contexts.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># import some data
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)

# construct a document-term matrix
x_dtm &lt;- make_dtm(x$title,
  stop_words = revwords())
# Note that make_dtm calls revwords by default, so this is technically redundant
</code></pre>

<hr>
<h2 id='run_topic_model'>Calculate a topic model</h2><span id='topic+run_topic_model'></span>

<h3>Description</h3>

<p>Calculate a topic model using Latent Dirichlet Allocation (<code>LDA</code>) or Correlated Topic Models (<code>CTM</code>), using the <code>topicmodels</code> package.</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_topic_model(dtm, type = "lda", n_topics = 5, iterations = 2000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_topic_model_+3A_dtm">dtm</code></td>
<td>
<p>a Document Term Matrix (DTM)</p>
</td></tr>
<tr><td><code id="run_topic_model_+3A_type">type</code></td>
<td>
<p>string specififying the type of model to run. Either 'lda' (the default) or 'ctm'.</p>
</td></tr>
<tr><td><code id="run_topic_model_+3A_n_topics">n_topics</code></td>
<td>
<p>Number of topics to calculate</p>
</td></tr>
<tr><td><code id="run_topic_model_+3A_iterations">iterations</code></td>
<td>
<p>The number of iterations. Only relevant for LDA.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A topic model with the specified parameters.
</p>


<h3>Note</h3>

<p>This is a basic wrapper function designed to allow consistent specification of model parameters within <code>shiny</code> apps.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+make_dtm">make_dtm</a></code> for constructing data to pass to this function; <code><a href="#topic+screen_topics">screen_topics</a></code> for interactive visualisation of topic model results.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># import data
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools"
)
x &lt;- read_bibliography(file_location)

# run a topic model based on these data
# note: the following lines can take a very long time to run, especially for large datasets!
x_dtm &lt;- make_dtm(x$title)
## Not run: x_lda &lt;- run_topic_model(x_dtm, "lda", 5, 5000)
</code></pre>

<hr>
<h2 id='screen_abstracts'>Shiny app for screening articles by their abstracts</h2><span id='topic+screen_abstracts'></span>

<h3>Description</h3>

<p>This is a simple app for displaying bibliographic data one entry at a time, and manually selecting or excluding them. Articles can be ordered by a user-specified column, or or in one of three automated ways: as in the input dataset, alphabetically by title, or in random order (the default).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_abstracts(x, max_file_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_abstracts_+3A_x">x</code></td>
<td>
<p>An (optional) object of class <code>data.frame</code> or <code>bibliography</code> to open in the browser. If empty, the app will launch with no data. Data can be added within the app via the 'import' button.</p>
</td></tr>
<tr><td><code id="screen_abstracts_+3A_max_file_size">max_file_size</code></td>
<td>
<p>Optional argument to set the maximum file size (in MB) that the app will accept.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function launches a Shiny app in the users' default browser, allowing the user to select or exclude individual articles.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+screen_titles">screen_titles</a></code> for screening articles in groups rather than individually; <code><a href="#topic+screen_topics">screen_topics</a></code> to view articles as a point cloud.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># to run the app and upload data interactively
## Not run: screen_abstracts()
# or to specify data from the workspace
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)
# to run the app using these data:
## Not run: screen_abstracts(x)
# or to run the app &amp; save results to the workspace:
## Not run: result &lt;- screen_abstracts(x)
</code></pre>

<hr>
<h2 id='screen_duplicates'>Shiny app for locating and excluding duplicated entries in a dataset</h2><span id='topic+screen_duplicates'></span>

<h3>Description</h3>

<p>This is a simple app for calculating, displaying and screening potential duplicates in bibliographic data</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_duplicates(x, max_file_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_duplicates_+3A_x">x</code></td>
<td>
<p>An (optional) object of class <code>data.frame</code> or <code>bibliography</code> to open in the browser. If empty, the app will launch with no data. Data can be added within the app via the 'import' button.</p>
</td></tr>
<tr><td><code id="screen_duplicates_+3A_max_file_size">max_file_size</code></td>
<td>
<p>Optional argument to set the maximum file size (in MB) that the app will accept.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This app is effectively a wrapper for <code><a href="#topic+find_duplicates">find_duplicates</a></code>, with the added option to manually screen pairs of duplicates to check the results. Consequently, this is a more reliable method than <code><a href="#topic+extract_unique_references">extract_unique_references</a></code> of dealing with the duplicates identified by <code><a href="#topic+find_duplicates">find_duplicates</a></code>, and for testing whether that function has returned sensible results for a given dataset.
</p>


<h3>Value</h3>

<p>This function launches a Shiny app in the users' default browser, allowing the user to customize their parameters for duplicate detection, and visualise the results.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+screen_titles">screen_titles</a></code> or <code><a href="#topic+screen_abstracts">screen_abstracts</a></code> for manual screening of individual articles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># to run the app and upload data interactively
## Not run: screen_duplicates()
# or to specify data from the workspace
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)
# to run the app using these data:
## Not run: screen_duplicates(x)
# or to run the app &amp; save results to the workspace:
## Not run: result &lt;- screen_duplicates(x)
</code></pre>

<hr>
<h2 id='screen_titles'>Shiny app for screening articles by their titles</h2><span id='topic+screen_titles'></span>

<h3>Description</h3>

<p>This is a simple app for displaying the titles in a bibliographic dataset in small groups, and manually selecting or excluding them. Articles can be ordered as in the input dataset, alphabetically by title, or in random order (the default).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_titles(x, max_file_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_titles_+3A_x">x</code></td>
<td>
<p>An (optional) object of class <code>data.frame</code> or <code>bibliography</code> to open in the browser. If empty, the app will launch with no data. Data can be added within the app via the 'import' button.</p>
</td></tr>
<tr><td><code id="screen_titles_+3A_max_file_size">max_file_size</code></td>
<td>
<p>Optional argument to set the maximum file size (in MB) that the app will accept.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function launches a Shiny app in the users' default browser, allowing the user to select or exclude articles.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+screen_abstracts">screen_abstracts</a></code> for screening articles one at a time rather than in groups; <code><a href="#topic+screen_topics">screen_topics</a></code> to view articles as a point cloud.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># to run the app and upload data interactively
## Not run: screen_titles()
# or to specify data from the workspace
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)
# to run the app using these data:
## Not run: screen_titles(x)
# or to run the app &amp; save results to the workspace:
## Not run: result &lt;- screen_titles(x)
</code></pre>

<hr>
<h2 id='screen_topics'>Shiny app for screening bibliographies using topic models</h2><span id='topic+screen_topics'></span>

<h3>Description</h3>

<p>Screening is usually achieved by manually sorting titles or abstracts one at a time. <code>screen_topics</code> offers an alternative by allowing the user to group data by any column in the input dataset, and running a topic model on the resulting data. This allows a great deal of flexibility to locate patterns in journals, years, or authors, rather than just articles. Data points can be selected or excluded individually, or by topic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_topics(x, remove_words, max_file_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_topics_+3A_x">x</code></td>
<td>
<p>An (optional) object of class <code>data.frame</code> or <code>bibliography</code> to open in the browser. If empty, the app will launch with no data. Data can be added within the app via the 'import' button.</p>
</td></tr>
<tr><td><code id="screen_topics_+3A_remove_words">remove_words</code></td>
<td>
<p>Optional vector of words to be removed from consideration by the Topic Model. If none are given, <code>screen_topics</code> will use <code><a href="#topic+revwords">revwords</a></code>. Note that this vector will be converted to lower case before processing, so the algorithm is not case sensitive.</p>
</td></tr>
<tr><td><code id="screen_topics_+3A_max_file_size">max_file_size</code></td>
<td>
<p>Optional argument to set the maximum file size (in MB) that the app will accept.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The display space is divided into three parts. From left to right, these are the sidebar; the plot window; and the selection panel.
</p>
<p>The sidebar shows a series of drop-down menus that can be used to customize or recalculate the central plot. It can be hidden when not in use. Note that the default settings for LDA (5 topics, 10,000 iterations) prioritize speed over reliability - higher numbers of iterations will give more reliable results.
</p>
<p>The plot window shows an ordination of article weights calculated using LDA, with articles colored by their highest-weighted topic. Hovering over a point shows the title and abstract below the plot; clicking allows selection or deselection of that article (and optionally displays co-authorship data). Selecting a region of the plot and clicking zooms on the selected region; double-clicking without selecting a region returns the plot to its full extent.
</p>
<p>The selection panel gives information on progress in selecting/deselecting articles. It also contains windows for displaying topic-level infromation and article abstracts. All boxes in this panel can be minimized when not required.
</p>
<p>Ordinations are calculated using LDA (library <code>"topicmodels"</code>) and are displayed using <code>shiny</code> and <code>plotly</code>.
</p>
<p>When you have finished viewing/screening, you can export information to a .csv or .rda file (saved to the working directory) using the 'Save' tab.
</p>
<p>Note that &quot;<code>start_review_window</code>&quot; is the earlier form of this function; this has been deprecated and will be removed from future versions of <code>revtools</code>.
</p>


<h3>Value</h3>

<p>This function launches a Shiny app in the users' default browser.</p>


<h3>See Also</h3>

<p><code><a href="#topic+screen_titles">screen_titles</a></code> or <code><a href="#topic+screen_abstracts">screen_abstracts</a></code> for manual screening; <code><a href="#topic+screen_topics_progress-class">screen_topics_progress-class</a></code> for saving and restoring progress in <code>screen_topics</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># to run the app and upload data interactively
## Not run: screen_topics()
# or to specify data from the workspace
file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location)
# to run the app using these data:
## Not run: screen_topics(x)
# or to run the app &amp; save results to the workspace:
## Not run: result &lt;- screen_topics(x)
</code></pre>

<hr>
<h2 id='screen_topics_progress-class'>Description of class 'screen_topics_progress'</h2><span id='topic+screen_topics_progress-class'></span>

<h3>Description</h3>

<p><code>screen_topics_progress</code> is an S3 class designed to store data from <code>screen_topics</code>, allowing the user to re-load a previously calculated topic model. If you just want to save your decisions on article inclusion/exclusion, along with your notes, then this is probably overkill as that information can simply be exported as a .csv file.</p>


<h3>slots</h3>

<p>Class 'screen_topics_progress' has seven slots containing the following information:
</p>

<ul>
<li> <p><strong>raw</strong> duplicate of data passed to <code>screen_topics</code>
</p>
</li>
<li> <p><strong>stopwords</strong> vector of words excluded from the dtm
</p>
</li>
<li> <p><strong>columns</strong> vector of column names in the original dataset
</p>
</li>
<li> <p><strong>grouped</strong> a data.frame showing grouped data as specified by the user
</p>
</li>
<li> <p><strong>dtm</strong> document-term matrix, created by <code>make_dtm</code>
</p>
</li>
<li> <p><strong>model</strong> most recent topic model, created by <code>run_topic_model</code>
</p>
</li>
<li> <p><strong>plot_ready</strong> data needed for the main plot (coordinates etc.)
</p>
</li></ul>


<hr>
<h2 id='screen_topics_progress-methods'>Methods for class 'screen_topics_progress'</h2><span id='topic+screen_topics_progress-methods'></span><span id='topic+summary.screen_topics_progress'></span>

<h3>Description</h3>

<p>Tools to display useful information on class <code>screen_topics_progress</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'screen_topics_progress'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_topics_progress-methods_+3A_object">object</code></td>
<td>
<p>An object of class 'screen_topics_progress'</p>
</td></tr>
<tr><td><code id="screen_topics_progress-methods_+3A_...">...</code></td>
<td>
<p>Any further information</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Prints useful information to the workspace.
</p>


<h3>Note</h3>

<p>Class <code>screen_topics_progress</code> is a format for exporting large quantities of data during reviews. It is typically stored within a .rds file in the working directory. When re-imported to R using <code>readRDS</code>, this file will contain an object of class <code>screen_topics_progress</code>.
</p>

<hr>
<h2 id='tag_lookup'>Lookup table for ris tags</h2><span id='topic+tag_lookup'></span>

<h3>Description</h3>

<p>ris-like bibliographic data files contain codes that describe their contents, such as 'AU' in place of 'author'. This function provides lookup tables for 'ris' tags of different kinds</p>


<h3>Usage</h3>

<pre><code class='language-R'>tag_lookup(type = "ris")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tag_lookup_+3A_type">type</code></td>
<td>
<p>Which lookup table should be returned? Accepted values are 'ris', 'ris_write' or 'medline'</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Primarily an internal function to support <code>read_bibliography</code> and <code>write_bibliography</code>. Tag substitutions for PubMed/Medline fields are taken directly from the NIH (<a href="https://www.nlm.nih.gov/bsd/mms/medlineelements.html">available here</a>). Substitutions for other ris-like formats are based on common examples, but are much less consistently documented.
</p>


<h3>Value</h3>

<p>a <code>data.frame</code> containing the original tag (column 'ris'), and the full-word substitution for that tag (column 'bib'). For type 'ris', there is also an added 'order' column showing the order those tags should be displayed in. 'ris_write' is a version of 'ris' with only one ris tag per bib tag.</p>


<h3>See Also</h3>

<p><code><a href="#topic+bibliography-class">bibliography-class</a></code>, <code><a href="#topic+read_bibliography">read_bibliography</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tag_lookup("ris") # standard ris format
tag_lookup("medline") # PubMed files
</code></pre>

<hr>
<h2 id='write_bibliography'>Export imported bibliographic data as .bib or .ris formats</h2><span id='topic+write_bibliography'></span>

<h3>Description</h3>

<p>Basic function to export bibliographic information for use in other programs. Work in progress. Very little error checking or advanced formatting in this version</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_bibliography(x, filename, format = "ris")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_bibliography_+3A_x">x</code></td>
<td>
<p>An object of class 'bibliography', such as imported using read_bibliography</p>
</td></tr>
<tr><td><code id="write_bibliography_+3A_filename">filename</code></td>
<td>
<p>Name of the exported file. Should ideally match 'format', but this is not enforced</p>
</td></tr>
<tr><td><code id="write_bibliography_+3A_format">format</code></td>
<td>
<p>Format of the exported file. Should be either &quot;ris&quot; (default) or &quot;bib&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>exports results as a .ris or .bib file.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>file_location &lt;- system.file(
  "extdata",
  "avian_ecology_bibliography.ris",
  package = "revtools")
x &lt;- read_bibliography(file_location, return_df = FALSE)

# export a subset of entries as a new file
write_bibliography(x[1:5],
  filename = paste0(tempdir(), "/x_out.ris"),
  format = "ris")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
