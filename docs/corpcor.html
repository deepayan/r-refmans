<!DOCTYPE html><html><head><title>Help for package corpcor</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {corpcor}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cor2pcor'><p>Compute Partial Correlation from Correlation Matrix &ndash; and Vice Versa</p></a></li>
<li><a href='#corpcor-internal'><p>Internal corpcor Functions</p></a></li>
<li><a href='#corpcor-package'><p>The corpcor Package</p></a></li>
<li><a href='#cov.shrink'><p>Shrinkage Estimates of Covariance and Correlation</p></a></li>
<li><a href='#fast.svd'><p>Fast Singular Value Decomposition</p></a></li>
<li><a href='#invcov.shrink'><p>Fast Computation of the Inverse of the Covariance and Correlation Matrix</p></a></li>
<li><a href='#mpower'><p>Compute the Power of a Real Symmetric Matrix</p></a></li>
<li><a href='#pcor.shrink'><p>Shrinkage Estimates of Partial Correlation and Partial Variance</p></a></li>
<li><a href='#powcor.shrink'><p>Fast Computation of the Power of the Shrinkage Correlation Matrix</p></a></li>
<li><a href='#pseudoinverse'><p>Pseudoinverse of a Matrix</p></a></li>
<li><a href='#rank.condition'><p>Positive Definiteness of a Matrix, Rank and Condition Number</p></a></li>
<li><a href='#rebuild.cov'><p>Rebuild and Decompose the (Inverse) Covariance Matrix</p></a></li>
<li><a href='#shrink.intensity'><p>Estimation of Shrinkage Intensities</p></a></li>
<li><a href='#smtools'><p>Some Tools for Handling Symmetric Matrices</p></a></li>
<li><a href='#wt.scale'><p>Weighted Expectations and Variances</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.6.10</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-09-16</td>
</tr>
<tr>
<td>Title:</td>
<td>Efficient Estimation of Covariance and (Partial) Correlation</td>
</tr>
<tr>
<td>Author:</td>
<td>Juliane Schafer, Rainer Opgen-Rhein, Verena Zuber, Miika Ahdesmaki, 
 A. Pedro Duarte Silva, and Korbinian Strimmer.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Korbinian Strimmer &lt;strimmerlab@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements a James-Stein-type shrinkage estimator for 
  the covariance matrix, with separate shrinkage for variances and correlations.  
  The details of the method are explained in Schafer and Strimmer (2005) 
  &lt;<a href="https://doi.org/10.2202%2F1544-6115.1175">doi:10.2202/1544-6115.1175</a>&gt; and Opgen-Rhein and Strimmer (2007) 
  &lt;<a href="https://doi.org/10.2202%2F1544-6115.1252">doi:10.2202/1544-6115.1252</a>&gt;.  The approach is both computationally as well
  as statistically very efficient, it is applicable to "small n, large p" data, 
  and always returns a positive definite and well-conditioned covariance matrix.  
  In addition to inferring the covariance matrix the package also provides 
  shrinkage estimators for partial correlations and partial variances.  
  The inverse of the covariance and correlation matrix 
  can be efficiently computed, as well as any arbitrary power of the 
  shrinkage correlation matrix.  Furthermore, functions are available for fast 
  singular value decomposition, for computing the pseudoinverse, and for 
  checking the rank and positive definiteness of a matrix.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://strimmerlab.github.io/software/corpcor/">https://strimmerlab.github.io/software/corpcor/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-09-16 14:05:52 UTC; strimmer</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-09-16 15:30:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='cor2pcor'>Compute Partial Correlation from Correlation Matrix &ndash; and Vice Versa</h2><span id='topic+cor2pcor'></span><span id='topic+pcor2cor'></span>

<h3>Description</h3>

<p><code>cor2pcor</code> computes the pairwise 
<em>partial</em> correlation coefficients from either a correlation 
or a covariance matrix. 
</p>
<p><code>pcor2cor</code> takes either a partial correlation matrix or 
a partial covariance matrix as input,
and computes from it the corresponding correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor2pcor(m, tol)
pcor2cor(m, tol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor2pcor_+3A_m">m</code></td>
<td>
<p>covariance matrix or (partial) correlation matrix</p>
</td></tr>   
<tr><td><code id="cor2pcor_+3A_tol">tol</code></td>
<td>
<p>tolerance - singular values larger than
tol are considered non-zero (default value:
<code>tol = max(dim(m))*max(D)*.Machine$double.eps</code>).
This parameter is needed for  the singular
value decomposition on which <code><a href="#topic+pseudoinverse">pseudoinverse</a></code> is based.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The partial
correlations are the negative standardized concentrations (which in 
turn are the off-diagonal elements of the inverse correlation or 
covariance matrix).  In graphical Gaussian models the partial 
correlations represent the 
direct interactions between two variables, conditioned on all
remaining variables.
</p>
<p>In the above functions the <code><a href="#topic+pseudoinverse">pseudoinverse</a></code> is employed
for inversion - hence even singular covariances (with some
zero eigenvalues) may be used.  However, a better option may be to
estimate a positive definite covariance matrix using 
<code><a href="#topic+cov.shrink">cov.shrink</a></code>.
</p>
<p>Note that for efficient computation of partial correlation coefficients from 
data  <code>x</code> it is advised to use  <code>pcor.shrink(x)</code>  and <em>not</em> 
<code>cor2pcor(cor.shrink(x))</code>.
</p>


<h3>Value</h3>

<p>A matrix with the pairwise partial correlation coefficients
(<code>cor2pcor</code>) or with pairwise
correlations (<code>pcor2cor</code>).
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Whittaker J. 1990.  Graphical Models in Applied Multivariate Statistics.
John Wiley, Chichester.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+decompose.invcov">decompose.invcov</a></code>, <code><a href="#topic+pcor.shrink">pcor.shrink</a></code>, <code><a href="#topic+pseudoinverse">pseudoinverse</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# covariance matrix
m.cov = rbind(
 c(3,1,1,0),
 c(1,3,0,1),
 c(1,0,2,0),
 c(0,1,0,2)
)
m.cov


# corresponding correlation matrix
m.cor.1 = cov2cor(m.cov)
m.cor.1

# compute partial correlations (from covariance matrix)
m.pcor.1 = cor2pcor(m.cov)
m.pcor.1

# compute partial correlations (from correlation matrix)
m.pcor.2 = cor2pcor(m.cor.1)
m.pcor.2



zapsmall( m.pcor.1 ) == zapsmall( m.pcor.2 )

# backtransformation
m.cor.2 = pcor2cor(m.pcor.1)
m.cor.2
zapsmall( m.cor.1 ) == zapsmall( m.cor.2 )

</code></pre>

<hr>
<h2 id='corpcor-internal'>Internal corpcor Functions</h2><span id='topic+positive.svd'></span><span id='topic+nsmall.svd'></span><span id='topic+psmall.svd'></span><span id='topic+pvt.powscor'></span><span id='topic+pvt.cppowscor'></span><span id='topic+pvt.svar'></span><span id='topic+pvt.check.w'></span><span id='topic+print.shrinkage'></span>

<h3>Description</h3>

<p>Internal corpcor functions.
</p>


<h3>Note</h3>

<p>These are not to be called by the user (or in some cases are just
waiting for proper documentation to be written).
</p>

<hr>
<h2 id='corpcor-package'>The corpcor Package</h2><span id='topic+corpcor-package'></span>

<h3>Description</h3>

<p>This package implements a James-Stein-type shrinkage estimator for 
the covariance matrix, with separate shrinkage for variances and correlations.  
The details of the method are explained in Sch\&quot;afer and Strimmer (2005) 
&lt;DOI:10.2202/1544-6115.1175&gt; and Opgen-Rhein and Strimmer (2007) 
&lt;DOI:10.2202/1544-6115.1252&gt;.  The approach is both computationally as well
as statistically very efficient, it is applicable to &ldquo;small n, large p&rdquo; data, 
and always returns a positive definite and well-conditioned covariance matrix.  
In addition to inferring the covariance matrix the package also provides 
shrinkage estimators for partial correlations, partial variances, and 
regression coefficients.  The inverse of the covariance and correlation matrix 
can be efficiently computed, and as well as any arbitrary power of the 
shrinkage correlation matrix.  Furthermore, functions are available for fast 
singular value decomposition, for computing the pseudoinverse, and for 
checking the rank and positive definiteness of a matrix.
</p>
<p>The name of the package refers to <b>cor</b>relations and 
<b>p</b>artial <b>cor</b>relations.
</p>


<h3>Author(s)</h3>

<p>Juliane Sch\&quot;afer, Rainer Opgen-Rhein, Verena Zuber, Miika Ahdesm\&quot;aki, 
A. Pedro Duarte Silva, and Korbinian Strimmer (<a href="https://strimmerlab.github.io/">https://strimmerlab.github.io/</a>)</p>


<h3>References</h3>

<p>See website: <a href="https://strimmerlab.github.io/software/corpcor/">https://strimmerlab.github.io/software/corpcor/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cov.shrink">cov.shrink</a>, <a href="#topic+invcov.shrink">invcov.shrink</a>, <a href="#topic+powcor.shrink">powcor.shrink</a>, <a href="#topic+pcor.shrink">pcor.shrink</a>,
 <a href="#topic+fast.svd">fast.svd</a>.</code>
</p>

<hr>
<h2 id='cov.shrink'>Shrinkage Estimates of Covariance and Correlation</h2><span id='topic+cov.shrink'></span><span id='topic+cor.shrink'></span><span id='topic+var.shrink'></span>

<h3>Description</h3>

<p>The functions <code>var.shrink</code>, <code>cor.shrink</code>, and <code>cov.shrink</code> compute 
shrinkage estimates of variance, correlation, and covariance, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var.shrink(x, lambda.var, w, verbose=TRUE)
cor.shrink(x, lambda, w, verbose=TRUE)
cov.shrink(x, lambda, lambda.var, w, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov.shrink_+3A_x">x</code></td>
<td>
<p>a data matrix</p>
</td></tr>
<tr><td><code id="cov.shrink_+3A_lambda">lambda</code></td>
<td>
<p>the correlation shrinkage intensity (range 0-1).
If <code>lambda</code> is not specified (the default) it is estimated
using an analytic formula from Sch\&quot;afer and Strimmer (2005) 
- see details below.  
For <code>lambda=0</code> the empirical correlations are recovered.</p>
</td></tr>
<tr><td><code id="cov.shrink_+3A_lambda.var">lambda.var</code></td>
<td>
<p>the variance shrinkage intensity (range 0-1). 
If <code>lambda.var</code> is not specified (the default) it is estimated
using an analytic formula from Opgen-Rhein and Strimmer (2007)
- see details below.  
For <code>lambda.var=0</code> the empirical variances are recovered.</p>
</td></tr>	 
<tr><td><code id="cov.shrink_+3A_w">w</code></td>
<td>
<p>optional: weights for each data point - if not specified uniform weights are assumed
(<code>w = rep(1/n, n)</code> with <code>n = nrow(x)</code>).</p>
</td></tr>
<tr><td><code id="cov.shrink_+3A_verbose">verbose</code></td>
<td>
<p>output some status messages while computing (default: TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>var.shrink</code> computes the empirical variance of each considered random variable, 
and shrinks them towards their median.  The shrinkage intensity is estimated using 
<code><a href="#topic+estimate.lambda.var">estimate.lambda.var</a></code> (Opgen-Rhein and Strimmer 2007).
</p>
<p>Similarly   <code>cor.shrink</code> computes a shrinkage estimate of the correlation matrix by
shrinking the empirical correlations towards  the identity matrix.  
In this case the shrinkage intensity is computed using  <code><a href="#topic+estimate.lambda">estimate.lambda</a></code>
(Sch\&quot;afer and Strimmer 2005).
</p>
<p>In comparison with the standard empirical estimates 
(<code><a href="stats.html#topic+var">var</a></code>, <code><a href="stats.html#topic+cov">cov</a></code>, and <code><a href="stats.html#topic+cor">cor</a></code>) the shrinkage estimates exhibit
a number of favorable properties.  For instance, 
</p>

<ol>
<li><p> they are typically much more efficient, i.e. they show (sometimes dramatically) better 
mean squared error,
</p>
</li>
<li><p> the estimated covariance and correlation matrices are always  positive definite
and well conditioned (so that there are no numerical problems when computing their inverse),
</p>
</li>
<li><p>  they are inexpensive to compute, and
</p>
</li>
<li><p> they are fully automatic and  do not require any
tuning parameters (as the shrinkage intensity is analytically estimated from the data), and
</p>
</li>
<li><p> they assume nothing about the underlying distributions, except for the existence of
the first two moments.</p>
</li></ol>

<p>These properties also carry over to derived quantities, such as partial variances and
partial correlations (<code><a href="#topic+pvar.shrink">pvar.shrink</a></code> and  <code><a href="#topic+pcor.shrink">pcor.shrink</a></code>).
</p>
<p>As an extra benefit, the shrinkage estimators have a form that can be <em>very</em> efficiently inverted, 
especially if the number of variables is large and the sample size is small.   Thus, instead of 
inverting the matrix output by <code>cov.shrink</code> and <code>cor.shrink</code> please use the functions
<code><a href="#topic+invcov.shrink">invcov.shrink</a></code> and <code><a href="#topic+invcor.shrink">invcor.shrink</a></code>, respectively.
</p>


<h3>Value</h3>

<p><code>var.shrink</code> returns a vector with estimated variances.
</p>
<p><code>cov.shrink</code> returns a covariance matrix.
</p>
<p><code>cor.shrink</code> returns the corresponding correlation matrix. 
</p>


<h3>Author(s)</h3>

<p>Juliane Sch\&quot;afer,
Rainer Opgen-Rhein,
and Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Opgen-Rhein, R., and K. Strimmer. 2007. Accurate ranking of 
differentially expressed genes by a distribution-free shrinkage 
approach.    Statist. Appl. Genet. Mol. Biol. <b>6</b>:9.
&lt;DOI:10.2202/1544-6115.1252&gt;
</p>
<p>Sch\&quot;afer, J., and K. Strimmer. 2005.  A shrinkage approach to large-scale
covariance estimation and implications for functional genomics. 
Statist. Appl. Genet. Mol. Biol. <b>4</b>:32.
&lt;DOI:10.2202/1544-6115.1175&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+invcov.shrink">invcov.shrink</a></code>, <code><a href="#topic+pcor.shrink">pcor.shrink</a></code>, <code><a href="#topic+cor2pcor">cor2pcor</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# small n, large p
p = 100
n = 20

# generate random pxp covariance matrix
sigma = matrix(rnorm(p*p),ncol=p)
sigma = crossprod(sigma)+ diag(rep(0.1, p))

# simulate multinormal data of sample size n  
sigsvd = svd(sigma)
Y = t(sigsvd$v %*% (t(sigsvd$u) * sqrt(sigsvd$d)))
X = matrix(rnorm(n * ncol(sigma)), nrow = n) %*% Y


# estimate covariance matrix
s1 = cov(X)
s2 = cov.shrink(X)


# squared error
sum((s1-sigma)^2)
sum((s2-sigma)^2)


# compare positive definiteness
is.positive.definite(sigma)
is.positive.definite(s1)
is.positive.definite(s2)


# compare ranks and condition
rank.condition(sigma)
rank.condition(s1)
rank.condition(s2)

# compare eigenvalues
e0 = eigen(sigma, symmetric=TRUE)$values
e1 = eigen(s1, symmetric=TRUE)$values
e2 = eigen(s2, symmetric=TRUE)$values
m = max(e0, e1, e2)
yl = c(0, m)

par(mfrow=c(1,3))
plot(e1,  main="empirical")
plot(e2,  ylim=yl, main="full shrinkage")
plot(e0,  ylim=yl, main="true")
par(mfrow=c(1,1))

</code></pre>

<hr>
<h2 id='fast.svd'>Fast Singular Value Decomposition</h2><span id='topic+fast.svd'></span>

<h3>Description</h3>

<p><code>fast.svd</code> returns the singular value decomposition of
a rectangular real matrix 
</p>
<p style="text-align: center;"><code class="reqn">M = U D V^{'},</code>
</p>

<p>where <code class="reqn">U</code> and <code class="reqn">V</code> are orthogonal matrices with <code class="reqn">U' U = I</code>
and <code class="reqn">V' V = I</code>, and <code class="reqn">D</code> is a diagonal matrix containing the 
singular values (see <code><a href="base.html#topic+svd">svd</a></code>).
</p>
<p>The main difference to the native version <code><a href="base.html#topic+svd">svd</a></code> is that 
<code>fast.svd</code> is substantially faster for  &quot;fat&quot; (small n, large p)
and &quot;thin&quot; (large n, small p) matrices.
In this case the decomposition of <code class="reqn">M</code> can be
greatly sped up by first computing the SVD of either <code class="reqn">M M'</code> (fat matrices) or
<code class="reqn">M' M</code> (thin matrices), rather than that of <code class="reqn">M</code>.
</p>
<p>A second difference to <code><a href="base.html#topic+svd">svd</a></code> is that <code>fast.svd</code> only
returns the <em>positive</em> singular values (thus the dimension of <code class="reqn">D</code>
always equals the rank of <code class="reqn">M</code>).  Note that the singular
vectors computed by <code>fast.svd</code> may differ in sign from those computed 
by <code><a href="base.html#topic+svd">svd</a></code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast.svd(m, tol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fast.svd_+3A_m">m</code></td>
<td>
<p>matrix</p>
</td></tr>
<tr><td><code id="fast.svd_+3A_tol">tol</code></td>
<td>
<p>tolerance - singular values larger than
tol are considered non-zero (default value:
<code>tol = max(dim(m))*max(D)*.Machine$double.eps</code>)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For &quot;fat&quot; <code class="reqn">M</code> (small n, large p) the SVD decomposition of <code class="reqn">M M'</code> yields
</p>
<p style="text-align: center;"><code class="reqn">M M^{'} = U D^2 U</code>
</p>

<p>As the matrix <code class="reqn">M M'</code> has dimension n x n only, this is faster to compute
than SVD of <code class="reqn">M</code>.  The <code class="reqn">V</code> matrix is subsequently obtained by
</p>
<p style="text-align: center;"><code class="reqn">V = M^{'} U D^{-1}</code>
</p>
  
<p>Similarly, for &quot;thin&quot; <code class="reqn">M</code> (large n, small p), the decomposition of <code class="reqn">M' M</code>
yields
</p>
<p style="text-align: center;"><code class="reqn">M^{'} M = V D^2 V^{'}</code>
</p>

<p>which is also quick to compute as <code class="reqn">M' M</code> has only dimension p x p.  The 
<code class="reqn">U</code> matrix is then computed via
</p>
<p style="text-align: center;"><code class="reqn">U = M V D^{-1}</code>
</p>



<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>d</code></td>
<td>
<p>a vector containing the <em>positive</em> singular values</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a matrix with the corresponding left singular vectors</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a matrix with the corresponding right singular vectors</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+svd">svd</a></code>, <code><a href="Matrix.html#topic+solve">solve</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")


# generate a "fat" data matrix
n = 50
p = 5000
X = matrix(rnorm(n*p), n, p)

# compute SVD
system.time( (s1 = svd(X)) ) 
system.time( (s2 = fast.svd(X)) )


eps = 1e-10
sum(abs(s1$d-s2$d) &gt; eps)
sum(abs(abs(s1$u)-abs(s2$u)) &gt; eps)
sum(abs(abs(s1$v)-abs(s2$v)) &gt; eps)
</code></pre>

<hr>
<h2 id='invcov.shrink'>Fast Computation of the Inverse of the Covariance and Correlation Matrix</h2><span id='topic+invcov.shrink'></span><span id='topic+invcor.shrink'></span>

<h3>Description</h3>

<p>The functions <code>invcov.shrink</code> and <code>invcor.shrink</code> implement an
algorithm to <em>efficiently</em> compute 
the inverses of shrinkage estimates of covariance (<code><a href="#topic+cov.shrink">cov.shrink</a></code>) 
and correlation (<code><a href="#topic+cor.shrink">cor.shrink</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>invcov.shrink(x, lambda, lambda.var, w, verbose=TRUE)
invcor.shrink(x, lambda, w, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="invcov.shrink_+3A_x">x</code></td>
<td>
<p>a data matrix</p>
</td></tr>
<tr><td><code id="invcov.shrink_+3A_lambda">lambda</code></td>
<td>
<p>the correlation shrinkage intensity (range 0-1).
If <code>lambda</code> is not specified (the default) it is estimated
using an analytic formula from Sch\&quot;afer and Strimmer (2005) 
- see <code><a href="#topic+cor.shrink">cor.shrink</a></code>.  
For <code>lambda=0</code> the empirical correlations are recovered.</p>
</td></tr>
<tr><td><code id="invcov.shrink_+3A_lambda.var">lambda.var</code></td>
<td>
<p>the variance shrinkage intensity (range 0-1). 
If <code>lambda.var</code> is not specified (the default) it is estimated
using an analytic formula from Sch\&quot;afer and Strimmer (2005) 
- see <code><a href="#topic+var.shrink">var.shrink</a></code>.  
For <code>lambda.var=0</code> the empirical variances are recovered.</p>
</td></tr>	 
<tr><td><code id="invcov.shrink_+3A_w">w</code></td>
<td>
<p>optional: weights for each data point - if not specified uniform weights are assumed
(<code>w = rep(1/n, n)</code> with <code>n = nrow(x)</code>).</p>
</td></tr>	
<tr><td><code id="invcov.shrink_+3A_verbose">verbose</code></td>
<td>
<p>output status while computing (default: TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Both <code>invcov.shrink</code> and <code>invcor.shrink</code> rely on 
<code><a href="#topic+powcor.shrink">powcor.shrink</a></code>.  This allows to compute the inverses in
a very efficient fashion (much more efficient than directly inverting
the matrices - see the example).
</p>


<h3>Value</h3>

<p><code>invcov.shrink</code> returns the inverse of the output from <code><a href="#topic+cov.shrink">cov.shrink</a></code>. 
</p>
<p><code>invcor.shrink</code> returns the inverse of the output from <code><a href="#topic+cor.shrink">cor.shrink</a></code>. 
</p>


<h3>Author(s)</h3>

<p>Juliane Sch\&quot;afer  
and Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Sch\&quot;afer, J., and K. Strimmer. 2005.  A shrinkage approach to large-scale
covariance estimation and implications for functional genomics. 
Statist. Appl. Genet. Mol. Biol. <b>4</b>:32.
&lt;DOI:10.2202/1544-6115.1175&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+powcor.shrink">powcor.shrink</a></code>, <code><a href="#topic+cov.shrink">cov.shrink</a></code>, <code><a href="#topic+pcor.shrink">pcor.shrink</a></code>, <code><a href="#topic+cor2pcor">cor2pcor</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# generate data matrix
p = 500
n = 10
X = matrix(rnorm(n*p), nrow = n, ncol = p)

lambda = 0.23  # some arbitrary lambda

# slow
system.time(
  (W1 =  solve(cov.shrink(X, lambda)))
)

# very fast
system.time(
  (W2 = invcov.shrink(X, lambda))
)

# no difference
sum((W1-W2)^2)
</code></pre>

<hr>
<h2 id='mpower'>Compute the Power of a Real Symmetric Matrix</h2><span id='topic+mpower'></span>

<h3>Description</h3>

<p><code>mpower</code> computes <code class="reqn">m^alpha</code>, i.e.
the <code>alpha</code>-th power of the real symmetric
matrix <code>m</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpower(m, alpha, pseudo=FALSE, tol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mpower_+3A_m">m</code></td>
<td>
<p>a real-valued symmetric matrix.</p>
</td></tr>
<tr><td><code id="mpower_+3A_alpha">alpha</code></td>
<td>
<p>exponent.</p>
</td></tr>
<tr><td><code id="mpower_+3A_pseudo">pseudo</code></td>
<td>
<p>if <code>pseudo=TRUE</code> then all zero eigenvalues are dropped 
(e.g. for computing the pseudoinverse).  The default is
to use all eigenvalues.</p>
</td></tr>
<tr><td><code id="mpower_+3A_tol">tol</code></td>
<td>
<p>tolerance - eigenvalues with absolute value smaller or equal
to <code>tol</code> are considered identically zero (default:
<code>tol = max(dim(m))*max(abs(eval))*.Machine$double.eps</code>).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The matrix power of <code>m</code> is obtained by first computing the spectral
decomposition of <code>m</code>, and subsequent modification of the resulting eigenvalues.
</p>
<p>Note that <code>m</code> is assumed to by symmetric, and only 
its lower triangle (diagonal  included) is used in <code><a href="base.html#topic+eigen">eigen</a></code>.
</p>
<p>For computing the matrix power of <code><a href="#topic+cor.shrink">cor.shrink</a></code>  use
the vastly more efficient function  <code><a href="#topic+powcor.shrink">powcor.shrink</a></code>.
</p>


<h3>Value</h3>

<p><code>mpower</code> returns
a matrix of the same dimensions as <code>m</code>.
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+powcor.shrink">powcor.shrink</a></code>, <code><a href="base.html#topic+eigen">eigen</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# generate symmetric matrix
p = 10
n = 20
X = matrix(rnorm(n*p), nrow = n, ncol = p)
m = cor(X)

m %*% m
mpower(m, 2)

solve(m)
mpower(m, -1)

msq = mpower(m, 0.5)
msq %*% msq
m

mpower(m, 1.234)
</code></pre>

<hr>
<h2 id='pcor.shrink'>Shrinkage Estimates of Partial Correlation and Partial Variance</h2><span id='topic+pcor.shrink'></span><span id='topic+pvar.shrink'></span>

<h3>Description</h3>

<p>The functions <code>pcor.shrink</code> and <code>pvar.shrink</code> compute shrinkage estimates
of partial correlation and partial variance, respectively.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcor.shrink(x, lambda, w, verbose=TRUE)
pvar.shrink(x, lambda, lambda.var, w, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcor.shrink_+3A_x">x</code></td>
<td>
<p>a data matrix</p>
</td></tr>
<tr><td><code id="pcor.shrink_+3A_lambda">lambda</code></td>
<td>
<p>the correlation shrinkage intensity (range 0-1).
If <code>lambda</code> is not specified (the default) it is estimated
using an analytic formula from Sch\&quot;afer and Strimmer (2005) 
- see <code><a href="#topic+cor.shrink">cor.shrink</a></code>.  
For <code>lambda=0</code> the empirical correlations are recovered.</p>
</td></tr>
<tr><td><code id="pcor.shrink_+3A_lambda.var">lambda.var</code></td>
<td>
<p>the variance shrinkage intensity (range 0-1). 
If <code>lambda.var</code> is not specified (the default) it is estimated
using an analytic formula from Opgen-Rhein and Strimmer (2007)
- see details below.  
For <code>lambda.var=0</code> the empirical variances are recovered.</p>
</td></tr>	 
<tr><td><code id="pcor.shrink_+3A_w">w</code></td>
<td>
<p>optional: weights for each data point - if not specified uniform weights
are assumed (<code>w = rep(1/n, n)</code> with <code>n = nrow(x)</code>).</p>
</td></tr>	 
<tr><td><code id="pcor.shrink_+3A_verbose">verbose</code></td>
<td>
<p>report progress while computing (default: TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The partial variance <code class="reqn">var(X_k | rest)</code> is the variance of <code class="reqn">X_k</code> conditioned on the
remaining variables.  It equals the inverse of the corresponding diagonal entry
of the precision matrix (see Whittaker 1990).
</p>
<p>The partial correlations <code class="reqn">corr(X_k, X_l | rest)</code> is the correlation between
<code class="reqn">X_k</code> and <code class="reqn">X_l</code> conditioned on the remaining variables. It equals the sign-reversed
entries of the off-diagonal entries of the precision matrix, standardized
by the the squared root of the associated inverse partial variances.
</p>
<p>Note that using <code>pcor.shrink(x)</code> <em>much</em> faster than 
<code>cor2pcor(cor.shrink(x))</code>.
</p>
<p>For details about the shrinkage procedure consult Sch\&quot;afer and Strimmer (2005),
Opgen-Rhein and Strimmer (2007), and the help page of <code><a href="#topic+cov.shrink">cov.shrink</a></code>.
</p>


<h3>Value</h3>

<p><code>pcor.shrink</code> returns the partial correlation matrix.  Attached to this
matrix are the standardized partial variances (i.e. PVAR/VAR) that 
can be retrieved using <code><a href="base.html#topic+attr">attr</a></code> under the attribute &quot;spv&quot;.
</p>
<p><code>pvar.shrink</code> returns the partial variances. 
</p>


<h3>Author(s)</h3>

<p>Juliane Sch\&quot;afer  
and Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Opgen-Rhein, R., and K. Strimmer. 2007. Accurate ranking of 
differentially expressed genes by a distribution-free shrinkage 
approach.    Statist. Appl. Genet. Mol. Biol. <b>6</b>:9.
&lt;DOI:10.2202/1544-6115.1252&gt;
</p>
<p>Sch\&quot;afer, J., and K. Strimmer. 2005.  A shrinkage approach to large-scale
covariance estimation and implications for functional genomics. 
Statist. Appl. Genet. Mol. Biol. <b>4</b>:32.
&lt;DOI:10.2202/1544-6115.1175&gt;
</p>
<p>Whittaker J. 1990.  Graphical Models in Applied Multivariate Statistics.
John Wiley, Chichester.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+invcov.shrink">invcov.shrink</a></code>, <code><a href="#topic+cov.shrink">cov.shrink</a></code>, <code><a href="#topic+cor2pcor">cor2pcor</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# generate data matrix
p = 50
n = 10
X = matrix(rnorm(n*p), nrow = n, ncol = p)


# partial variance
pv = pvar.shrink(X)
pv

# partial correlations (fast and recommend way)
pcr1 = pcor.shrink(X)

# other possibilities to estimate partial correlations
pcr2 = cor2pcor( cor.shrink(X) )


# all the same
sum((pcr1 - pcr2)^2)

</code></pre>

<hr>
<h2 id='powcor.shrink'>Fast Computation of the Power of the Shrinkage Correlation Matrix</h2><span id='topic+powcor.shrink'></span><span id='topic+crossprod.powcor.shrink'></span>

<h3>Description</h3>

<p>The function <code>powcor.shrink</code> efficiently computes the <code>alpha</code>-th power
of the shrinkage correlation matrix produced by <code><a href="#topic+cor.shrink">cor.shrink</a></code>. 
</p>
<p>For instance, this function may be used for fast computation of the (inverse)
square root of the  shrinkage correlation matrix (needed, e.g., for decorrelation).
</p>
<p><code>crossprod.powcor.shrink</code> efficiently computes <code class="reqn">R^{\alpha} y</code> without
actually computing the full matrix <code class="reqn">R^{\alpha}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>powcor.shrink(x, alpha, lambda, w, verbose=TRUE)
crossprod.powcor.shrink(x, y, alpha, lambda, w, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="powcor.shrink_+3A_x">x</code></td>
<td>
<p>a data matrix</p>
</td></tr>
<tr><td><code id="powcor.shrink_+3A_y">y</code></td>
<td>
<p>a matrix, the number of rows of y must be the same as the number of columns of x</p>
</td></tr>
<tr><td><code id="powcor.shrink_+3A_alpha">alpha</code></td>
<td>
<p>exponent</p>
</td></tr>
<tr><td><code id="powcor.shrink_+3A_lambda">lambda</code></td>
<td>
<p>the correlation shrinkage intensity (range 0-1).
If <code>lambda</code> is not specified (the default) it is estimated
using an analytic formula from Sch\&quot;afer and Strimmer (2005) 
- see <code><a href="#topic+cor.shrink">cor.shrink</a></code>.  
For <code>lambda=0</code> the empirical correlations are recovered.</p>
</td></tr>
<tr><td><code id="powcor.shrink_+3A_w">w</code></td>
<td>
<p>optional: weights for each data point - if not specified uniform weights are assumed
(<code>w = rep(1/n, n)</code> with <code>n = nrow(x)</code>).</p>
</td></tr>	
<tr><td><code id="powcor.shrink_+3A_verbose">verbose</code></td>
<td>
<p>output status while computing (default: TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function employs a special matrix identity to speed up the computation of
the matrix power of the shrinkage correlation matrix (see Zuber and Strimmer 2009 for details).
</p>
<p>Apart from a scaling factor the shrinkage correlation matrix computed 
by <code><a href="#topic+cor.shrink">cor.shrink</a></code>  takes
on the form
</p>
<p style="text-align: center;"><code class="reqn">Z = I_p + V M V^T ,</code>
</p>
   
<p>where <code>V M V^T</code> is a multiple of the empirical correlation matrix.
Crucially, <code>Z</code> is a matrix of size <code>p</code> times <code>p</code>
whereas  <code>M</code> is a potentially much smaller matrix of size <code>m</code> times <code>m</code>,
where <code>m</code> is the rank of the empirical correlation matrix.  
</p>
<p>In order to calculate the <code>alpha</code>-th power of <code>Z</code>
the function uses the identity
</p>
<p style="text-align: center;"><code class="reqn">Z^\alpha = I_p - V (I_m -(I_m + M)^\alpha) V^T</code>
</p>

<p>requiring only the computation of the <code>alpha</code>-th power of the <code>m</code> by <code>m</code> matrix
<code class="reqn">I_m + M</code>.  This trick enables substantial computational savings especially when the number
of observations is much smaller than the number of variables.
</p>
<p>Note that the above identity is related but not identical to the Woodbury matrix
identity for inversion of a matrix.
For <code class="reqn">\alpha=-1</code> the above identity reduces to
</p>
<p style="text-align: center;"><code class="reqn">Z^{-1} = I_p - V (I_m -(I_m + M)^{-1}) V^T ,</code>
</p>

<p>whereas the Woodbury matrix identity equals
</p>
<p style="text-align: center;"><code class="reqn">Z^{-1} = I_p - V (I_m + M^{-1})^{-1} V^T .</code>
</p>



<h3>Value</h3>

<p><code>powcor.shrink</code> returns a matrix of the same size as the correlation matrix <code>R</code>
</p>
<p><code>crossprod.powcor.shrink</code> returns a matrix of the same size as <code>R</code> <code>y</code>.
</p>


<h3>Author(s)</h3>

<p>Verena Zuber, A. Pedro Duarte Silva, and Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Zuber, V., and K. Strimmer. 2009. Gene ranking and biomarker
discovery under correlation.  Bioinformatics <b>25</b>:2700-2707.
&lt;DOI:10.1093/bioinformatics/btp460&gt;
</p>
<p>Zuber, V.,  A. P. Duarte Silva, and K. Strimmer. 2012. A novel algorithm for 
simultaneous SNP selection in high-dimensional genome-wide association studies.
BMC Bioinformatics 13: 284
&lt;DOI:10.1186/1471-2105-13-284&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+invcor.shrink">invcor.shrink</a></code>, <code><a href="#topic+cor.shrink">cor.shrink</a></code>, <code><a href="#topic+mpower">mpower</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# generate data matrix
p = 500
n = 10
X = matrix(rnorm(n*p), nrow = n, ncol = p)

lambda = 0.23  # some arbitrary lambda

### computing the inverse ###
# slow
system.time(
  (W1 = solve(cor.shrink(X, lambda=lambda)))
)

# very fast
system.time(
  (W2 = powcor.shrink(X, alpha=-1, lambda=lambda))
)

# no difference
sum((W1-W2)^2)

### computing the square root ###

system.time(
  (W1 = mpower(cor.shrink(X, lambda=lambda), alpha=0.5))
)

# very fast
system.time(
  (W2 = powcor.shrink(X, alpha=0.5, lambda=lambda))
)

# no difference
sum((W1-W2)^2)


### computing an arbitrary power (alpha=1.23) ###

system.time(
  (W1 = mpower(cor.shrink(X, lambda=lambda), alpha=1.23))
)

# very fast
system.time(
  (W2 = powcor.shrink(X, alpha=1.23, lambda=lambda))
)

# no difference
sum((W1-W2)^2)


### fast computation of cross product

y = rnorm(p)

system.time(
  (CP1 = crossprod(powcor.shrink(X, alpha=1.23, lambda=lambda), y))
)

system.time(
  (CP2 = crossprod.powcor.shrink(X, y, alpha=1.23, lambda=lambda))
)

# no difference
sum((CP1-CP2)^2)

</code></pre>

<hr>
<h2 id='pseudoinverse'>Pseudoinverse of a Matrix</h2><span id='topic+pseudoinverse'></span>

<h3>Description</h3>

<p>The standard definition for the inverse of a matrix fails 
if the matrix is not square or singular. However, one can
generalize the inverse using singular value decomposition.
Any rectangular real matrix M can be decomposed as
</p>
<p style="text-align: center;"><code class="reqn">M = U D V^{'},</code>
</p>

<p>where U and V are orthogonal, V' means V transposed, and 
D is a diagonal matrix containing only the positive singular values
(as determined by <code>tol</code>, see also <code><a href="#topic+fast.svd">fast.svd</a></code>). 
</p>
<p>The pseudoinverse, also known as Moore-Penrose or generalized inverse
is then obtained as
</p>
<p style="text-align: center;"><code class="reqn">iM = V D^{-1} U^{'}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>pseudoinverse(m, tol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pseudoinverse_+3A_m">m</code></td>
<td>
<p>matrix</p>
</td></tr>
<tr><td><code id="pseudoinverse_+3A_tol">tol</code></td>
<td>
<p>tolerance - singular values larger than
tol are considered non-zero (default value:
<code>tol = max(dim(m))*max(D)*.Machine$double.eps</code>)
</p>
</td></tr>
</table>


<h3>Details</h3>

  
<p>The pseudoinverse has the property that the sum of the squares of all
the entries in <code>iM %*% M - I</code>, where I is an appropriate
identity matrix, is minimized. For non-singular matrices the
pseudoinverse is equivalent to the standard inverse.
</p>


<h3>Value</h3>

<p>A matrix (the pseudoinverse of m).
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+solve">solve</a></code>, <code><a href="#topic+fast.svd">fast.svd</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# a singular matrix
m = rbind(
c(1,2),
c(1,2)
)

# not possible to invert exactly
try(solve(m))

# pseudoinverse
p = pseudoinverse(m)
p

# characteristics of the pseudoinverse
zapsmall( m %*% p %*% m )  ==  zapsmall( m )
zapsmall( p %*% m %*% p )  ==  zapsmall( p )
zapsmall( p %*% m )  ==  zapsmall( t(p %*% m ) )
zapsmall( m %*% p )  ==  zapsmall( t(m %*% p ) )


# example with an invertable matrix
m2 = rbind(
c(1,1),
c(1,0)
)
zapsmall( solve(m2) ) == zapsmall( pseudoinverse(m2) )
</code></pre>

<hr>
<h2 id='rank.condition'>Positive Definiteness of a Matrix, Rank and Condition Number</h2><span id='topic+is.positive.definite'></span><span id='topic+make.positive.definite'></span><span id='topic+rank.condition'></span>

<h3>Description</h3>

<p><code>is.positive.definite</code> tests whether all eigenvalues of a symmetric matrix
are positive.  
</p>
<p><code>make.positive.definite</code> computes the nearest positive definite of a
real symmetric matrix, using the algorithm of NJ Higham (1988) &lt;DOI:10.1016/0024-3795(88)90223-6&gt;.
</p>
<p><code>rank.condition</code> estimates the rank and the condition
of a matrix by 
computing its singular values D[i] (using  <code><a href="base.html#topic+svd">svd</a></code>).
The rank of the matrix is the number of singular values <code>D[i] &gt; tol</code>)
and the condition is the ratio  of the largest and the smallest
singular value.   
</p>
<p>The definition <code>tol= max(dim(m))*max(D)*.Machine$double.eps</code> 
is exactly compatible with the conventions used in &quot;Octave&quot; or &quot;Matlab&quot;.
</p>
<p>Also note that it is not checked whether the input matrix m is real and symmetric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.positive.definite(m, tol, method=c("eigen", "chol"))
make.positive.definite(m, tol)
rank.condition(m, tol)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rank.condition_+3A_m">m</code></td>
<td>
<p>a matrix (assumed to be real and symmetric)</p>
</td></tr>
<tr><td><code id="rank.condition_+3A_tol">tol</code></td>
<td>
<p>tolerance for singular values and for absolute eigenvalues
-  only those with values larger than
tol are considered non-zero (default:
<code>tol = max(dim(m))*max(D)*.Machine$double.eps</code>)
</p>
</td></tr>
<tr><td><code id="rank.condition_+3A_method">method</code></td>
<td>
<p>Determines the method to check for positive definiteness:
eigenvalue computation (<code>eigen</code>, default) or Cholesky decomposition
(<code>chol</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>is.positive.definite</code> returns
a logical value (<code>TRUE</code> or <code>FALSE</code>).
</p>
<p><code>rank.condition</code> returns a list object with the following components:
</p>
<table>
<tr><td><code>rank</code></td>
<td>
<p>Rank of the matrix.</p>
</td></tr>
<tr><td><code>condition</code></td>
<td>
<p>Condition number.</p>
</td></tr>
<tr><td><code>tol</code></td>
<td>
<p>Tolerance.</p>
</td></tr>
</table>
<p><code>make.positive.definite</code> returns a symmetric positive definite matrix.
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+svd">svd</a></code>, <code><a href="#topic+pseudoinverse">pseudoinverse</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# Hilbert matrix
hilbert = function(n) { i = 1:n; 1 / outer(i - 1, i, "+") }

# positive definite ?
m = hilbert(8)
is.positive.definite(m)

# numerically ill-conditioned
m = hilbert(15)
rank.condition(m)

# make positive definite
m2 = make.positive.definite(m)
is.positive.definite(m2)
rank.condition(m2)
m2 - m
</code></pre>

<hr>
<h2 id='rebuild.cov'>Rebuild and Decompose the (Inverse) Covariance Matrix</h2><span id='topic+rebuild.cov'></span><span id='topic+rebuild.invcov'></span><span id='topic+decompose.cov'></span><span id='topic+decompose.invcov'></span>

<h3>Description</h3>

<p><code>rebuild.cov</code> takes a correlation matrix and a vector with variances
and reconstructs the corresponding covariance matrix.
</p>
<p>Conversely, <code>decompose.cov</code> decomposes 
a covariance matrix into correlations and variances.
</p>
<p><code>decompose.invcov</code> decomposes a concentration matrix (=inverse covariance
matrix) into partial correlations and partial variances.  
</p>
<p><code>rebuild.invcov</code> takes a partial correlation matrix and a vector 
with partial variances and reconstructs the corresponding concentration matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
rebuild.cov(r, v)
rebuild.invcov(pr, pv)
decompose.cov(m)
decompose.invcov(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rebuild.cov_+3A_r">r</code></td>
<td>
<p>correlation matrix</p>
</td></tr>
<tr><td><code id="rebuild.cov_+3A_v">v</code></td>
<td>
<p>variance vector</p>
</td></tr>
<tr><td><code id="rebuild.cov_+3A_pr">pr</code></td>
<td>
<p>partial correlation matrix</p>
</td></tr>
<tr><td><code id="rebuild.cov_+3A_pv">pv</code></td>
<td>
<p>partial variance vector</p>
</td></tr>
<tr><td><code id="rebuild.cov_+3A_m">m</code></td>
<td>
<p>a covariance or a concentration matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The diagonal elements of the concentration matrix (=inverse covariance matrix)
are the precisions, and the off-diagonal elements
are the concentrations. Thus, the partial variances
correspond to the inverse precisions, and the partial correlations to the
negative standardized concentrations.
</p>


<h3>Value</h3>

<p><code>rebuild.cov</code> and <code>rebuild.invcov</code> return a matrix.
</p>
<p><code>decompose.cov</code> and <code>decompose.invcov</code> return a list containing
a matrix and a vector.
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cor">cor</a></code>, <code><a href="stats.html#topic+cov">cov</a></code>, <code><a href="#topic+pcor.shrink">pcor.shrink</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# a correlation matrix and some variances
r = matrix(c(1, 1/2, 1/2, 1),  nrow = 2, ncol=2)
r
v = c(2, 3)

# construct the associated covariance matrix
c = rebuild.cov(r, v)
c

# decompose into correlations and variances
decompose.cov(c)


# the corresponding concentration matrix
conc = pseudoinverse(c) 
conc

# decompose into partial correlation matrix and partial variances
tmp = decompose.invcov(conc)
tmp
# note: because this is an example with two variables,
# the partial and standard correlations are identical!


# reconstruct the concentration matrix from partial correlations and
# partial variances 
rebuild.invcov(tmp$pr, tmp$pv)

</code></pre>

<hr>
<h2 id='shrink.intensity'>Estimation of Shrinkage Intensities</h2><span id='topic+estimate.lambda'></span><span id='topic+estimate.lambda.var'></span>

<h3>Description</h3>

<p>The functions <code>estimate.lambda</code> and <code>estimate.lambda.var</code>  
shrinkage intensities used for correlations and variances used
in <code><a href="#topic+cor.shrink">cor.shrink</a></code> and <code><a href="#topic+var.shrink">var.shrink</a></code>, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate.lambda(x, w, verbose=TRUE)
estimate.lambda.var(x, w, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shrink.intensity_+3A_x">x</code></td>
<td>
<p>a data matrix</p>
</td></tr>
<tr><td><code id="shrink.intensity_+3A_w">w</code></td>
<td>
<p>optional: weights for each data point - if not specified uniform weights are assumed
(<code>w = rep(1/n, n)</code> with <code>n = nrow(x)</code>).</p>
</td></tr>
<tr><td><code id="shrink.intensity_+3A_verbose">verbose</code></td>
<td>
<p>report shrinkage intensities (default: TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>var.shrink</code> computes the empirical variance of each considered random variable, 
and shrinks them towards their median.  The corresponding 
shrinkage intensity <code>lambda.var</code> is estimated using
</p>
<p style="text-align: center;"><code class="reqn">\lambda_{var}^{*} = (  \sum_{k=1}^p Var(s_{kk}) )/ \sum_{k=1}^p (s_{kk} - median(s))^2  </code>
</p>

<p>where <code class="reqn">median(s)</code> denotes the median of the empirical variances (see Opgen-Rhein and Strimmer 2007).   
</p>
<p>Similarly,   <code>cor.shrink</code> computes a shrinkage estimate of the correlation matrix by
shrinking the empirical correlations towards  the identity matrix.  
In this case the shrinkage intensity <code>lambda</code> equals
</p>
<p style="text-align: center;"><code class="reqn">\lambda^{*} = \sum_{k \neq l} Var(r_{kl}) / \sum_{k \neq l} r_{kl}^2  </code>
</p>

<p>(Sch\&quot;afer and Strimmer 2005).
</p>
<p>Ahdesm\&quot;aki suggested (2012) a computationally highly efficient algorithm to compute 
the shrinkage intensity estimate for the correlation matrix (see the R code for the implementation).
</p>


<h3>Value</h3>

<p><code>estimate.lambda</code> and <code>estimate.lambda.var</code> returns a number between 0 and 1. 
</p>


<h3>Author(s)</h3>

<p>Juliane Sch\&quot;afer,
Rainer Opgen-Rhein, Miika Ahdesm\&quot;aki
and Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Opgen-Rhein, R., and K. Strimmer. 2007. Accurate ranking of 
differentially expressed genes by a distribution-free shrinkage 
approach.    Statist. Appl. Genet. Mol. Biol. <b>6</b>:9.
&lt;DOI:10.2202/1544-6115.1252&gt;
</p>
<p>Sch\&quot;afer, J., and K. Strimmer. 2005.  A shrinkage approach to large-scale
covariance estimation and implications for functional genomics. 
Statist. Appl. Genet. Mol. Biol. <b>4</b>:32.
&lt;DOI:10.2202/1544-6115.1175&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cor.shrink">cor.shrink</a></code>, <code><a href="#topic+var.shrink">var.shrink</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# small n, large p
p = 100
n = 20

# generate random pxp covariance matrix
sigma = matrix(rnorm(p*p),ncol=p)
sigma = crossprod(sigma)+ diag(rep(0.1, p))

# simulate multinormal data of sample size n  
sigsvd = svd(sigma)
Y = t(sigsvd$v %*% (t(sigsvd$u) * sqrt(sigsvd$d)))
X = matrix(rnorm(n * ncol(sigma)), nrow = n) %*% Y


# correlation shrinkage intensity
estimate.lambda(X) 
c = cor.shrink(X)
attr(c, "lambda")

# variance shrinkage intensity
estimate.lambda.var(X) 
v = var.shrink(X)
attr(v, "lambda.var")

</code></pre>

<hr>
<h2 id='smtools'>Some Tools for Handling Symmetric Matrices</h2><span id='topic+sm2vec'></span><span id='topic+sm.index'></span><span id='topic+vec2sm'></span>

<h3>Description</h3>

<p><code>sm2vec</code> takes a symmetric matrix and puts
the lower triagonal entries into a vector (cf. <code><a href="base.html#topic+lower.tri">lower.tri</a></code>).
</p>
<p><code>sm.index</code> lists the corresponding x-y-indices for each entry
in the vector produced by <code>sm2vec</code>.
</p>
<p><code>vec2sm</code> reverses the operation by  <code>sm2vec</code> and converts the
vector back to a symmetric matrix. If <code>diag=FALSE</code> the
diagonal of the resulting matrix will consist of NAs.  If <code>order</code>
is supplied then the input vector <code>vec</code> will first be rearranged accordingly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sm2vec(m, diag = FALSE)
sm.index(m, diag = FALSE)
vec2sm(vec, diag = FALSE, order = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smtools_+3A_m">m</code></td>
<td>
<p>symmetric matrix</p>
</td></tr>
<tr><td><code id="smtools_+3A_diag">diag</code></td>
<td>
<p>logical. Should the diagonal be included in the conversion to and from a vector?</p>
</td></tr>
<tr><td><code id="smtools_+3A_vec">vec</code></td>
<td>
<p>vector of unique elements from a symmetric matrix</p>
</td></tr>
<tr><td><code id="smtools_+3A_order">order</code></td>
<td>
<p>order of the entries in <code>vec</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector (<code>sm2vec</code>), a two-column matrix with indices (<code>sm.index</code>),
or a symmetric matrix (<code>vec2sm</code>).
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io/">https://strimmerlab.github.io/</a>).
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+lower.tri">lower.tri</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# a symmetric matrix
m = rbind(
 c(3,1,1,0),
 c(1,3,0,1),
 c(1,0,2,0),
 c(0,1,0,2)
)
m

# convert into vector (including the diagonals)
v = sm2vec(m, diag=TRUE)
v.idx = sm.index(m, diag=TRUE)
v
v.idx

# put back to symmetric matrix
vec2sm(v, diag=TRUE)

# convert from vector with specified order of the elements
sv = sort(v)
sv
ov = order(v)
ov
vec2sm(sv, diag=TRUE, order=ov)
</code></pre>

<hr>
<h2 id='wt.scale'>Weighted Expectations and Variances</h2><span id='topic+wt.scale'></span><span id='topic+wt.var'></span><span id='topic+wt.moments'></span>

<h3>Description</h3>

<p><code>wt.var</code> estimate the unbiased variance taking into account 
data weights.
</p>
<p><code>wt.moments</code> produces the weighted mean and weighted variance
for each column of a matrix.
</p>
<p><code>wt.scale</code> centers and standardized a matrix using 
the weighted means and variances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  wt.var(xvec, w) 
  wt.moments(x, w)
  wt.scale(x, w, center=TRUE, scale=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wt.scale_+3A_xvec">xvec</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="wt.scale_+3A_x">x</code></td>
<td>
<p>a matrix</p>
</td></tr>
<tr><td><code id="wt.scale_+3A_w">w</code></td>
<td>
<p>data weights</p>
</td></tr>
<tr><td><code id="wt.scale_+3A_center">center</code></td>
<td>
<p>logical value</p>
</td></tr>
<tr><td><code id="wt.scale_+3A_scale">scale</code></td>
<td>
<p>logical value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A rescaled matrix (<code>wt.scale</code>), a list containing the column means and 
variances (<code>wt.moments</code>), or single number (<code>wt.var</code>)
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+weighted.mean">weighted.mean</a></code>, <code><a href="stats.html#topic+cov.wt">cov.wt</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load corpcor library
library("corpcor")

# generate some data
p = 5
n = 5
X = matrix(rnorm(n*p), nrow = n, ncol = p)
w = c(1,1,1,3,3)/9


# standardize matrix
scale(X)
wt.scale(X)
wt.scale(X, w) # take into account data weights

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
