<!DOCTYPE html><html><head><title>Help for package corpora</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {corpora}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#corpora-package'>
<p>corpora: Statistical Inference from Corpus Frequency Data</p>
</p></a></li>
<li><a href='#binom.pval'><p>P-values of the binomial test for frequency counts (corpora)</p></a></li>
<li><a href='#BNCbiber'>
<p>Biber's (1988) register features for the British National Corpus</p>
</p></a></li>
<li><a href='#BNCcomparison'>
<p>Comparison of written and spoken noun frequencies in the British National Corpus</p>
</p></a></li>
<li><a href='#BNCdomains'>
<p>Distribution of domains in the British National Corpus (BNC)</p>
</p></a></li>
<li><a href='#BNCInChargeOf'>
<p>Collocations of the phrase &quot;in charge of&quot; (BNC)</p>
</p></a></li>
<li><a href='#BNCmeta'>
<p>Metadata for the British National Corpus (XML edition)</p>
</p></a></li>
<li><a href='#BNCqueries'>
<p>Per-text frequency counts for a selection of BNCweb corpus queries</p>
</p></a></li>
<li><a href='#BrownBigrams'>
<p>Bigrams of adjacent words from the Brown corpus</p>
</p></a></li>
<li><a href='#BrownLOBPassives'>
<p>Frequency counts of passive verb phrases in the Brown and LOB corpora</p>
</p></a></li>
<li><a href='#BrownPassives'>
<p>Frequency counts of passive verb phrases in the Brown corpus</p>
</p></a></li>
<li><a href='#BrownStats'>
<p>Basic statistics of texts in the Brown corpus</p>
</p></a></li>
<li><a href='#chisq'><p>Pearson's chi-squared statistic for frequency comparisons (corpora)</p></a></li>
<li><a href='#chisq.pval'><p>P-values of Pearson's chi-squared test for frequency comparisons (corpora)</p></a></li>
<li><a href='#cont.table'><p>Build contingency tables for frequency comparison (corpora)</p></a></li>
<li><a href='#corpora.palette'>
<p>Colour palettes for linguistic visualization (corpora)</p></a></li>
<li><a href='#DistFeatBrownFam'>
<p>Latent dimension scores from a distributional analysis of the Brown Family corpora</p>
</p></a></li>
<li><a href='#fisher.pval'><p>P-values of Fisher's exact test for frequency comparisons (corpora)</p></a></li>
<li><a href='#keyness'><p>Compute best-practice keyness measures (corpora)</p></a></li>
<li><a href='#KrennPPV'>
<p>German PP-Verb collocation candidates annotated by Brigitte Krenn (2000)</p>
</p></a></li>
<li><a href='#LOBPassives'>
<p>Frequency counts of passive verb phrases in the LOB corpus</p>
</p></a></li>
<li><a href='#LOBStats'>
<p>Basic statistics of texts in the LOB corpus</p>
</p></a></li>
<li><a href='#PassiveBrownFam'>
<p>By-text frequencies of passive verb phrases in the Brown Family corpora.</p>
</p></a></li>
<li><a href='#prop.cint'><p>Confidence interval for proportion based on frequency counts (corpora)</p></a></li>
<li><a href='#qw'>
<p>Split string into words, similar to qw() in Perl (corpora)</p></a></li>
<li><a href='#rowColVector'>
<p>Propagate vector to single-row or single-column matrix (corpora)</p></a></li>
<li><a href='#sample.df'><p>Random samples from data frames (corpora)</p></a></li>
<li><a href='#simulated.census'><p>Simulated census data for examples and illustrations (corpora)</p></a></li>
<li><a href='#simulated.language.course'><p>Simulated study on effectiveness of language course (corpora)</p></a></li>
<li><a href='#simulated.wikipedia'><p>Simulated type and token counts for Wikipedia articles (corpora)</p></a></li>
<li><a href='#stars.pval'>
<p>Show p-values as significance stars (corpora)</p></a></li>
<li><a href='#VSS'>
<p>A small corpus of very short stories with linguistic annotations</p>
</p></a></li>
<li><a href='#z.score'><p>The z-score statistic for frequency counts (corpora)</p></a></li>
<li><a href='#z.score.pval'><p>P-values of the z-score test for frequency counts (corpora)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Statistics and Data Sets for Corpus Frequency Data</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, stats, utils, grDevices</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-20</td>
</tr>
<tr>
<td>Description:</td>
<td>Utility functions for the statistical analysis of corpus frequency data.
        This package is a companion to the open-source course "Statistical Inference: 
        A Gentle Introduction for Computational Linguists and Similar Creatures" ('SIGIL').</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://SIGIL.R-Forge.R-Project.org/">http://SIGIL.R-Forge.R-Project.org/</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-20 15:53:03 UTC; ex47emin</td>
</tr>
<tr>
<td>Author:</td>
<td>Stephanie Evert <a href="https://orcid.org/0000-0002-4192-2437"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre, aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Stephanie Evert &lt;stephanie.evert@fau.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-20 22:52:41 UTC</td>
</tr>
</table>
<hr>
<h2 id='corpora-package'>
corpora: Statistical Inference from Corpus Frequency Data
</h2><span id='topic+corpora-package'></span><span id='topic+corpora'></span>

<h3>Description</h3>

<p>The <code>corpora</code> package provides a collection of functions for statistical inference
from corpus frequency data, as well as some convenience functions and example data sets.
</p>
<p>It is a companion package to the open-source course <em>Statistical Inference: a Gentle Introduction for Linguists and similar creatures</em> originally developed by Marco Baroni and Stephanie Evert. Statistical methods implemented in the package are described and illustrated in the units of this course.
</p>
<p>Starting with version 0.6 the package also includes best-practice implementations of various corpus-linguistic analysis techniques.
</p>


<h3>Details</h3>

<p>An overview of some important functions and data sets included in the <code>corpora</code> package.
See the package index for a complete listing.
</p>


<h4>Analysis functions</h4>


<ul>
<li> <p><code><a href="#topic+keyness">keyness</a>()</code> provides reference implementations for best-practice keyness measures, including the recommended LRC measure (Evert 2022)
</p>
</li>
<li> <p><code><a href="#topic+binom.pval">binom.pval</a>()</code> is a vectorised function that computes p-values of the binomial test more efficiently than <code><a href="stats.html#topic+binom.test">binom.test</a></code> (using central p-values in the two-sided case)
</p>
</li>
<li> <p><code><a href="#topic+fisher.pval">fisher.pval</a>()</code> is a vectorised function that efficiently computes p-values of Fisher's exact test on <code class="reqn">2\times 2</code> contingency tables for large samples (using central p-values in the two-sided case)
</p>
</li>
<li> <p><code><a href="#topic+prop.cint">prop.cint</a>()</code> is a vectorised function that computes multiple binomial confidence intervals much more efficiently than <code><a href="stats.html#topic+binom.test">binom.test</a></code>
</p>
</li>
<li> <p><code><a href="#topic+z.score">z.score</a>()</code> and <code><a href="#topic+z.score.pval">z.score.pval</a>()</code> can be used to carry out a z-test for a single proportion (as an approximation to <code><a href="stats.html#topic+binom.test">binom.test</a></code>)
</p>
</li>
<li> <p><code><a href="#topic+chisq">chisq</a>()</code> and <code><a href="#topic+chisq.pval">chisq.pval</a>()</code> are vectorised functions that compute the test statistic and p-value of a chi-squared test for <code class="reqn">2\times 2</code> contingency tables more efficiently than <code><a href="stats.html#topic+chisq.test">chisq.test</a></code>
</p>
</li></ul>




<h4>Utility functions</h4>


<ul>
<li> <p><code><a href="#topic+cont.table">cont.table</a>()</code> creates <code class="reqn">2\times 2</code> contingency tables for frequency comparison test that can be passed to <code><a href="stats.html#topic+chisq.test">chisq.test</a></code> and <code><a href="stats.html#topic+fisher.test">fisher.test</a></code>
</p>
</li>
<li> <p><code><a href="#topic+sample.df">sample.df</a>()</code> extracts random samples of rows from a data frame
</p>
</li>
<li> <p><code><a href="#topic+qw">qw</a>()</code> splits a string on whitespace or a user-specified regular expression (similar to Perl's <code>qw//</code> construct) 
</p>
</li>
<li> <p><code><a href="#topic+corpora.palette">corpora.palette</a>()</code> provides some nice colour palettes (better than R's default colours)
</p>
</li>
<li> <p><code><a href="#topic+rowVector">rowVector</a>()</code> and <code><a href="#topic+colVector">colVector</a>()</code> convert a vector into a single-row or single-column matrix
</p>
</li></ul>




<h4>Data sets</h4>


<ul>
<li><p> Several data sets based on the <a href="http://www.natcorp.ox.ac.uk">British National Corpus</a>, including complete metadata for all 4048 text files (<code><a href="#topic+BNCmeta">BNCmeta</a></code>), per-text frequency counts for a number of linguistic corpus queries (<code><a href="#topic+BNCqueries">BNCqueries</a></code>), and relative frequencies of 65 lexico-grammatical features for each text (<code><a href="#topic+BNCbiber">BNCbiber</a></code>)
</p>
</li>
<li><p> Frequency counts of passive constructions in all texts of the Brown and LOB corpora (<code><a href="#topic+BrownLOBPassives">BrownLOBPassives</a></code>) for frequency comparison with regression models, complemented by distributional features (<code><a href="#topic+DistFeatBrownFam">DistFeatBrownFam</a></code>) as additional predictors
</p>
</li>
<li><p> A small text corpus of <em>Very Short Stories</em> in the form of a data frame <code><a href="#topic+VSS">VSS</a></code>, with one row for each token in the corpus.
</p>
</li>
<li><p> Small example tables to illustrate frequency comparison of lexical items (<code><a href="#topic+BNCcomparison">BNCcomparison</a></code>) and collocation analysis (<code><a href="#topic+BNCInChargeOf">BNCInChargeOf</a></code>)
</p>
</li>
<li> <p><code><a href="#topic+KrennPPV">KrennPPV</a></code> is a data set of German verb-preposition-noun collocation candidates with manual annotation of true positives and pre-computed association scores
</p>
</li>
<li><p> Three functions for generating large synthetic data sets used in the SIGIL course: <code><a href="#topic+simulated.census">simulated.census</a>()</code>, <code><a href="#topic+simulated.language.course">simulated.language.course</a>()</code> and <code><a href="#topic+simulated.wikipedia">simulated.wikipedia</a>()</code>
</p>
</li></ul>




<h3>Author(s)</h3>

 
<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)
</p>


<h3>References</h3>

<p>The official homepage of the <code>corpora</code> package and the SIGIL course is <a href="http://SIGIL.R-Forge.R-Project.org/">http://SIGIL.R-Forge.R-Project.org/</a>.
</p>

<hr>
<h2 id='binom.pval'>P-values of the binomial test for frequency counts (corpora)</h2><span id='topic+binom.pval'></span>

<h3>Description</h3>

<p>This function computes the p-value of a binomial test for frequency
counts.  In the two-sided case, a &ldquo;central&rdquo; p-value (Fay 2010)
provides better numerical efficiency than the likelihood-based approach
of <code>binom.test</code> and is always consistent with confidence intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
binom.pval(k, n, p = 0.5,
           alternative = c("two.sided", "less", "greater"))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binom.pval_+3A_k">k</code></td>
<td>
<p>frequency of a type in the corpus (or an integer vector of
frequencies)</p>
</td></tr>
<tr><td><code id="binom.pval_+3A_n">n</code></td>
<td>
<p>number of tokens in the corpus, i.e. sample size (or an
integer vector specifying the sizes of different samples)</p>
</td></tr>
<tr><td><code id="binom.pval_+3A_p">p</code></td>
<td>
<p>null hypothesis, giving the assumed proportion of this
type in the population (or a vector of proportions for different
types and/or different populations)</p>
</td></tr>
<tr><td><code id="binom.pval_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis; must be one of <code>two.sided</code> (default), <code>less</code>
or <code>greater</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>alternative="two.sided"</code> (the default), a &ldquo;central&rdquo; p-value
is computed (Fay 2010: 53f), which differs from the likelihood-based two-sided
p-value determined by <code>binom.test</code> (the &ldquo;minlike&rdquo; method in Fay's
terminology).  This approach has two advantages: (i) it is numerically robust
and efficient, even for very large samples and frequency counts; (ii) it is 
always consistent with Clopper-Pearson confidence intervals (see examples below).
</p>


<h3>Value</h3>

<p>The p-value of a binomial test applied to the given data (or a vector
of p-values).
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Fay, Michael P. (2010). Two-sided exact tests and matching confidence intervals for discrete data. <em>The R Journal</em>, <b>2</b>(1), 53-58.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+z.score.pval">z.score.pval</a></code>, <code><a href="#topic+prop.cint">prop.cint</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># inconsistency btw likelihood-based two-sided binomial test and confidence interval
binom.test(2, 10, p=0.555)

# central two-sided test as implemented by binom.pval is always consistent
binom.pval(2, 10, p=0.555)
prop.cint(2, 10, method="binomial")
</code></pre>

<hr>
<h2 id='BNCbiber'>
Biber's (1988) register features for the British National Corpus
</h2><span id='topic+BNCbiber'></span>

<h3>Description</h3>

<p>This data set contains a table of the relative frequencies (per 1000 words)
of 65 linguistic features (Biber 1988, 1995) for each text document in the
British National Corpus (Aston &amp; Burnard 1998).
</p>
<p>Biber (1988) introduced these features for the purpose of a multidimensional
register analysis. Variables in the data set are numbered according to Biber's
list (see e.g. Biber 1995, 95f).
</p>
<p>Feature frequencies were automatically extracted from the British National Corpus
using query patterns based on part-of-speech tags (Gasthaus 2007).  Note that
features 60 and 65 had to be omitted because they cannot be identified
with sufficient accuracy by the automatic methods.  For further information on
the extraction methodology, see Gasthaus (2007, 20-21).  The original data set
and the Python scripts used for feature extraction are available from
<a href="https://portal.ikw.uni-osnabrueck.de/~CL/download/BSc_Gasthaus2007/">https://portal.ikw.uni-osnabrueck.de/~CL/download/BSc_Gasthaus2007/</a>; the
version included here contains some bug fixes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BNCbiber

</code></pre>


<h3>Format</h3>

<p>A numeric matrix with 4048 rows and 65 columns, specifying the relative frequencies
(per 1000 words) of 65 linguistic features.  Documents are listed in the same order
as the metadata in <code><a href="#topic+BNCmeta">BNCmeta</a></code> and rows are labelled with text IDs, so it
is straightforward to combine the two data sets.
</p>

<table>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>A. Tense and aspect markers</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_01_past_tense</code> </td><td style="text-align: left;"> Past tense </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_02_perfect_aspect</code> </td><td style="text-align: left;"> Perfect aspect </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_03_present_tense</code> </td><td style="text-align: left;"> Present tense </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>B. Place and time adverbials</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_04_place_adverbials</code> </td><td style="text-align: left;"> Place adverbials (e.g., <em>above, beside, outdoors</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_05_time_adverbials</code> </td><td style="text-align: left;"> Time adverbials (e.g., <em>early, instantly, soon</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>C. Pronouns and pro-verbs</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_06_first_person_pronouns</code> </td><td style="text-align: left;"> First-person pronouns </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_07_second_person_pronouns</code> </td><td style="text-align: left;"> Second-person pronouns </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_08_third_person_pronouns</code> </td><td style="text-align: left;"> Third-person personal pronouns (excluding <em>it</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_09_pronoun_it</code> </td><td style="text-align: left;"> Pronoun <em>it</em> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_10_demonstrative_pronoun</code> </td><td style="text-align: left;"> Demonstrative pronouns (<em>that, this, these, those</em> as pronouns) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_11_indefinite_pronoun</code> </td><td style="text-align: left;"> Indefinite pronounes (e.g., <em>anybody, nothing, someone</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_12_proverb_do</code> </td><td style="text-align: left;"> Pro-verb <em>do</em> </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>D. Questions</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_13_wh_question</code> </td><td style="text-align: left;"> Direct <em>wh</em>-questions </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>E. Nominal forms</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_14_nominalization</code> </td><td style="text-align: left;"> Nominalizations (ending in <em>-tion, -ment, -ness, -ity</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_15_gerunds</code> </td><td style="text-align: left;"> Gerunds (participial forms functioning as nouns) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_16_other_nouns</code> </td><td style="text-align: left;"> Total other nouns </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>F. Passives</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_17_agentless_passives</code> </td><td style="text-align: left;"> Agentless passives </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_18_by_passives</code> </td><td style="text-align: left;"> <em>by</em>-passives </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>G. Stative forms</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_19_be_main_verb</code> </td><td style="text-align: left;"> <em>be</em> as main verb </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_20_existential_there</code> </td><td style="text-align: left;"> Existential <em>there</em> </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>H. Subordination features</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_21_that_verb_comp</code> </td><td style="text-align: left;"> <em>that</em> verb complements (e.g., <em>I said that he went.</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_22_that_adj_comp</code> </td><td style="text-align: left;"> <em>that</em> adjective complements (e.g., <em>I'm glad that you like it.</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_23_wh_clause</code> </td><td style="text-align: left;"> <em>wh</em>-clauses (e.g., <em>I believed what he told me.</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_24_infinitives</code> </td><td style="text-align: left;"> Infinitives </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_25_present_participle</code> </td><td style="text-align: left;"> Present participial adverbial clauses (e.g., <em>Stuffing his mouth with cookies, Joe ran out the door.</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_26_past_participle</code> </td><td style="text-align: left;"> Past participial adverbial clauses (e.g., <em>Built in a single week, the house would stand for fifty years.</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_27_past_participle_whiz</code> </td><td style="text-align: left;"> Past participial postnominal (reduced relative) clauses (e.g., <em>the solution produced by this process</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_28_present_participle_whiz</code> </td><td style="text-align: left;"> Present participial postnominal (reduced relative) clauses (e.g., <em>the event causing this decline</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_29_that_subj</code> </td><td style="text-align: left;"> <em>that</em> relative clauses on subject position (e.g., <em>the dog that bit me</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_30_that_obj</code> </td><td style="text-align: left;"> <em>that</em> relative clauses on object position (e.g., <em>the dog that I saw</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_31_wh_subj</code> </td><td style="text-align: left;"> <em>wh</em> relatives on subject position (e.g., <em>the man who likes popcorn</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_32_wh_obj</code> </td><td style="text-align: left;"> <em>wh</em> relatives on object position (e.g., <em>the man who Sally likes</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_33_pied_piping</code> </td><td style="text-align: left;"> Pied-piping relative clauses (e.g., <em>the manner in which he was told</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_34_sentence_relatives</code> </td><td style="text-align: left;"> Sentence relatives (e.g., <em>Bob likes fried mangoes, which is the most disgusting thing I've ever heard of.</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_35_because</code> </td><td style="text-align: left;"> Causative adverbial subordinator (<em>because</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_36_though</code> </td><td style="text-align: left;"> Concessive adverbial subordinators (<em>although, though</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_37_if</code> </td><td style="text-align: left;"> Conditional adverbial subordinators (<em>if, unless</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_38_other_adv_sub</code> </td><td style="text-align: left;"> Other adverbial subordinators (e.g., <em>since, while, whereas</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>I. Prepositional phrases, adjectives and adverbs</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_39_prepositions</code> </td><td style="text-align: left;"> Total prepositional phrases </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_40_adj_attr</code> </td><td style="text-align: left;"> Attributive adjectives (e.g., <em>the big horse</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_41_adj_pred</code> </td><td style="text-align: left;"> Predicative adjectives (e.g., <em>The horse is big.</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_42_adverbs</code> </td><td style="text-align: left;"> Total adverbs </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>J. Lexical specificity</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_43_type_token</code> </td><td style="text-align: left;"> Type-token ratio (including punctuation)</td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_44_mean_word_length</code> </td><td style="text-align: left;"> Average word length (across tokens, excluding punctuation) </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>K. Lexical classes</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_45_conjuncts</code> </td><td style="text-align: left;"> Conjuncts (e.g., <em>consequently, furthermore, however</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_46_downtoners</code> </td><td style="text-align: left;"> Downtoners (e.g., <em>barely, nearly, slightly</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_47_hedges</code> </td><td style="text-align: left;"> Hedges (e.g., <em>at about, something like, almost</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_48_amplifiers</code> </td><td style="text-align: left;"> Amplifiers (e.g., <em>absolutely, extremely, perfectly</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_49_emphatics</code> </td><td style="text-align: left;"> Emphatics (e.g., <em>a lot, for sure, really</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_50_discourse_particles</code> </td><td style="text-align: left;"> Discourse particles (e.g., sentence-initial <em>well, now, anyway</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_51_demonstratives</code> </td><td style="text-align: left;"> Demonstratives </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>L. Modals</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_52_modal_possibility</code> </td><td style="text-align: left;"> Possibility modals (<em>can, may, might, could</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_53_modal_necessity</code> </td><td style="text-align: left;"> Necessity modals (<em>ought, should, must</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_54_modal_predictive</code> </td><td style="text-align: left;"> Predictive modals (<em>will, would, shall</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>M. Specialized verb classes</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_55_verb_public</code> </td><td style="text-align: left;"> Public verbs (e.g., <em>assert, declare, mention</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_56_verb_private</code> </td><td style="text-align: left;"> Private verbs (e.g., <em>assume, believe, doubt, know</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_57_verb_suasive</code> </td><td style="text-align: left;"> Suasive verbs (e.g., <em>command, insist, propose</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_58_verb_seem</code> </td><td style="text-align: left;"> <em>seem</em> and <em>appear</em> </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>N. Reduced forms and dispreferred structures</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_59_contractions</code> </td><td style="text-align: left;"> Contractions </td>
</tr>
<tr>
 <td style="text-align: left;">               
     <em>n/a</em> </td><td style="text-align: left;"> Subordinator <em>that</em> deletion (e.g., <em>I think [that] he went.</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_61_stranded_preposition</code> </td><td style="text-align: left;"> Stranded prepositions (e.g., <em>the candidate that I was thinking of</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_62_split_infinitve</code> </td><td style="text-align: left;"> Split infinitives (e.g., <em>He wants to convincingly prove that ...</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_63_split_auxiliary</code> </td><td style="text-align: left;"> Split auxiliaries (e.g., <em>They were apparently shown to ...</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>O. Co-ordination</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_64_phrasal_coordination</code> </td><td style="text-align: left;"> Phrasal co-ordination (N <em>and</em> N; Adj <em>and</em> Adj; V <em>and</em> V; Adv <em>and</em> Adv) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <em>n/a</em> </td><td style="text-align: left;"> Independent clause co-ordination (clause-initial <em>and</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     </td><td style="text-align: left;"> <b>P. Negation</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_66_neg_synthetic</code> </td><td style="text-align: left;"> Synthetic negation (e.g., <em>No answer is good enough for Jones.</em>) </td>
</tr>
<tr>
 <td style="text-align: left;">
     <code>f_67_neg_analytic</code> </td><td style="text-align: left;"> Analytic negation (e.g., <em>That's not likely.</em>)     
  </td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>); feature extractor by Jan Gasthaus (2007).</p>


<h3>References</h3>

<p>Aston, Guy and Burnard, Lou (1998). <em>The BNC Handbook.</em> Edinburgh
University Press, Edinburgh. See also the BNC homepage at
<a href="http://www.natcorp.ox.ac.uk/">http://www.natcorp.ox.ac.uk/</a>.
</p>
<p>Biber, Douglas (1988). <em>Variations Across Speech and Writing.</em>
Cambridge University Press, Cambridge.
</p>
<p>Biber, Douglas (1995). <em>Dimensions of Register Variation: A cross-linguistic comparison.</em>
Cambridge University Press, Cambridge.
</p>
<p>Gasthaus, Jan (2007). <em>Prototype-Based Relevance Learning for Genre Classification.</em>
B.Sc.\ thesis, Institute of Cognitive Science, University of Osnabrück.
Data sets and software available from <a href="https://portal.ikw.uni-osnabrueck.de/~CL/download/BSc_Gasthaus2007/">https://portal.ikw.uni-osnabrueck.de/~CL/download/BSc_Gasthaus2007/</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BNCmeta">BNCmeta</a></code></p>

<hr>
<h2 id='BNCcomparison'>
Comparison of written and spoken noun frequencies in the British National Corpus
</h2><span id='topic+BNCcomparison'></span>

<h3>Description</h3>

<p>This data set compares the frequencies of 60 selected nouns in the
written and spoken parts of the British National Corpus, World Edition
(BNC).  Nouns were chosen from three frequency bands, namely the 20
most frequent nouns in the corpus, 20 nouns with approximately 1000
occurrences, and 20 nouns with approximately 100 occurrences.
</p>
<p>See Aston &amp; Burnard (1998) for more information about the BNC, or go
to <a href="http://www.natcorp.ox.ac.uk/">http://www.natcorp.ox.ac.uk/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BNCcomparison

</code></pre>


<h3>Format</h3>

<p>A data frame with 61 rows and the following columns:
</p>

<dl>
<dt><code>noun</code>:</dt><dd><p>lemmatised noun (aka stem form)</p>
</dd>
<dt><code>written</code>:</dt><dd><p>frequency in the written part of the BNC</p>
</dd>
<dt><code>spoken</code>:</dt><dd><p>frequency in the spoken part of the BNC</p>
</dd>
</dl>



<h3>Details</h3>

<p>In addition to the 60 nouns, the data set contains a row labelled
<code>OTHER</code>, which represents the total frequency of all other nouns
in the BNC.  This value is needed in order to calculate the sample
sizes of the written and spoken part for frequency comparison tests.
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Aston, Guy and Burnard, Lou (1998). <em>The BNC Handbook.</em> Edinburgh
University Press, Edinburgh. See also the BNC homepage at
<a href="http://www.natcorp.ox.ac.uk/">http://www.natcorp.ox.ac.uk/</a>.
</p>

<hr>
<h2 id='BNCdomains'>
Distribution of domains in the British National Corpus (BNC)
</h2><span id='topic+BNCdomains'></span>

<h3>Description</h3>

<p>This data set gives the number of documents and tokens in each of the
18 domains represented in the British National Corpus, World Edition
(BNC).  See Aston &amp; Burnard (1998) for more information about the BNC
and the domain classification, or go to
<a href="http://www.natcorp.ox.ac.uk/">http://www.natcorp.ox.ac.uk/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BNCdomains

</code></pre>


<h3>Format</h3>

<p>A data frame with 19 rows and the following columns:
</p>

<dl>
<dt><code>domain</code>:</dt><dd><p>name of the respective domain in the BNC</p>
</dd>
<dt><code>documents</code>:</dt><dd><p>number of documents from this domain</p>
</dd>
<dt><code>tokens</code>:</dt><dd><p>total number of tokens in all documents from
this domain</p>
</dd>
</dl>



<h3>Details</h3>

<p>For one document in the BNC, the domain classification is missing.
This document is represented by the code <code>Unlabeled</code> in the data
set.
</p>


<h3>Author(s)</h3>

<p>Marco Baroni &lt;<a href="mailto:baroni@sslmit.unibo.it">baroni@sslmit.unibo.it</a>&gt;</p>


<h3>References</h3>

<p>Aston, Guy and Burnard, Lou (1998). <em>The BNC Handbook.</em> Edinburgh
University Press, Edinburgh. See also the BNC homepage at
<a href="http://www.natcorp.ox.ac.uk/">http://www.natcorp.ox.ac.uk/</a>.
</p>

<hr>
<h2 id='BNCInChargeOf'>
Collocations of the phrase &quot;in charge of&quot; (BNC)
</h2><span id='topic+BNCInChargeOf'></span>

<h3>Description</h3>

<p>This data set lists collocations (in the sense of Sinclair 1991) of
the phrase <em>in charge of</em> found in the British National Corpus,
World Edition (BNC).  A span size of 3 and a frequency threshold of 5
were used, i.e. all words that occur at least five times within a
distance of three tokens from the key phrase <em>in charge of</em> are
listed as collocates.  Note that collocations were not allowed to
cross sentence boundaries.
</p>
<p>See Aston &amp; Burnard (1998) for more information about the BNC, or go
to <a href="http://www.natcorp.ox.ac.uk/">http://www.natcorp.ox.ac.uk/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BNCInChargeOf

</code></pre>


<h3>Format</h3>

<p>A data frame with 250 rows and the following columns:
</p>

<dl>
<dt><code>collocate</code>:</dt><dd><p>a collocate of the key phrase <em>in charge of</em> (word form)</p>
</dd>
<dt><code>f.in</code>:</dt><dd><p>occurrences of the collocate within a distance of
3 tokens from the key phrase, i.e. <em>inside</em> the span</p>
</dd>
<dt><code>N.in</code>:</dt><dd><p>total number of tokens inside the span</p>
</dd>
<dt><code>f.out</code>:</dt><dd><p>occurrences of the collocate <em>outside</em> the span</p>
</dd>
<dt><code>N.out</code>:</dt><dd><p>total number of tokens outside the span</p>
</dd>
</dl>



<h3>Details</h3>

<p>Punctuation, numbers and any words containing non-alphabetic
characters (except for <code>-</code>) were not considered as potential
collocates.  Likewise, the number of tokens inside / outside the span
given in the columns <code>N.in</code> and <code>N.out</code> only includes simple
alphabetic word forms.
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Aston, Guy and Burnard, Lou (1998). <em>The BNC Handbook.</em> Edinburgh
University Press, Edinburgh. See also the BNC homepage at
<a href="http://www.natcorp.ox.ac.uk/">http://www.natcorp.ox.ac.uk/</a>.
</p>
<p>Sinclair, John (1991). <em>Corpus, Concordance, Collocation.</em> Oxford
University Press, Oxford.
</p>

<hr>
<h2 id='BNCmeta'>
Metadata for the British National Corpus (XML edition)
</h2><span id='topic+BNCmeta'></span>

<h3>Description</h3>

<p>This data set provides complete metadata for all 4048 texts of the
British National Corpus (XML edition).
See Aston &amp; Burnard (1998) for more information about the BNC, or go
to <a href="http://www.natcorp.ox.ac.uk/">http://www.natcorp.ox.ac.uk/</a>.
</p>
<p>The data have automatically been extracted from the original BNC source
files.  Some transformations were applied so that all attribute names and 
their values are given in a human-readable form.  The Perl scripts used
in the extraction procedure are available from
<a href="https://cwb.sourceforge.io/install.php#other">https://cwb.sourceforge.io/install.php#other</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BNCmeta

</code></pre>


<h3>Format</h3>

<p>A data frame with 4048 rows and the columns listed below.
Unless specified otherwise, columns are coded as factors.
</p>

<dl>
<dt><code>id</code>:</dt><dd><p>BNC document ID; character vector</p>
</dd>
<dt><code>title</code>:</dt><dd><p>Title of the document; character vector</p>
</dd>
<dt><code>n_words</code>:</dt><dd><p>Number of words in the document; integer vector</p>
</dd>
<dt><code>n_tokens</code>:</dt><dd><p>Total number of tokens (including punctuation and deleted material); integer vector</p>
</dd>
<dt><code>n_w</code>:</dt><dd><p>Number of w-units (words); integer vector</p>
</dd>
<dt><code>n_c</code>:</dt><dd><p>Number of c-units (punctuation); integer vector</p>
</dd>
<dt><code>n_s</code>:</dt><dd><p>Number of s-units (sentences); integer vector</p>
</dd>
<dt><code>publication_date</code>:</dt><dd><p>Publication date</p>
</dd>
<dt><code>text_type</code>:</dt><dd><p>Text type</p>
</dd>
<dt><code>context</code>:</dt><dd><p>Spoken context</p>
</dd>
<dt><code>respondent_age</code>:</dt><dd><p>Age-group of respondent</p>
</dd>
<dt><code>respondent_class</code>:</dt><dd><p>Social class of respondent (NRS social grades)</p>
</dd>
<dt><code>respondent_sex</code>:</dt><dd><p>Sex of respondent</p>
</dd>
<dt><code>interaction_type</code>:</dt><dd><p>Interaction type</p>
</dd>
<dt><code>region</code>:</dt><dd><p>Region</p>
</dd>
<dt><code>author_age</code>:</dt><dd><p>Author age-group</p>
</dd>
<dt><code>author_domicile</code>:</dt><dd><p>Domicile of author</p>
</dd>
<dt><code>author_sex</code>:</dt><dd><p>Sex of author</p>
</dd>
<dt><code>author_type</code>:</dt><dd><p>Author type</p>
</dd>
<dt><code>audience_age</code>:</dt><dd><p>Audience age</p>
</dd>
<dt><code>domain</code>:</dt><dd><p>Written domain</p>
</dd>
<dt><code>difficulty</code>:</dt><dd><p>Written difficulty</p>
</dd>
<dt><code>medium</code>:</dt><dd><p>Written medium</p>
</dd>
<dt><code>publication_place</code>:</dt><dd><p>Publication place</p>
</dd>
<dt><code>sampling_type</code>:</dt><dd><p>Sampling type</p>
</dd>
<dt><code>circulation</code>:</dt><dd><p>Estimated circulation size</p>
</dd>
<dt><code>audience_sex</code>:</dt><dd><p>Audience sex</p>
</dd>
<dt><code>availability</code>:</dt><dd><p>Availability</p>
</dd>
<dt><code>mode</code>:</dt><dd><p>Text mode (written/spoken)</p>
</dd>
<dt><code>derived_type</code>:</dt><dd><p>Text class</p>
</dd>
<dt><code>genre</code>:</dt><dd><p>David Lee's genre classification</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Aston, Guy and Burnard, Lou (1998). <em>The BNC Handbook.</em> Edinburgh
University Press, Edinburgh. See also the BNC homepage at
<a href="http://www.natcorp.ox.ac.uk/">http://www.natcorp.ox.ac.uk/</a>.
</p>

<hr>
<h2 id='BNCqueries'>
Per-text frequency counts for a selection of BNCweb corpus queries
</h2><span id='topic+BNCqueries'></span>

<h3>Description</h3>

<p>This data set contains a table of frequency counts obtained with a selection of BNCweb (Hoffmann et al. 2008)
queries for each text document in the British National Corpus (Aston &amp; Burnard 1998).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BNCqueries

</code></pre>


<h3>Format</h3>

<p>A data frame with 4048 rows and 12 columns.  The first column (<code>id</code>) contains a character vector of
text IDs, the remaining columns contain integer vector of the corresponding per-text frequency counts for
various BNCweb queries.  Column names ending in <code>.S</code> indicate sentence counts rather than token counts.
</p>
<p>The list below shows the BNCweb query used for each feature in CEQL syntax (Hoffmann et al. 2008, Ch. 6).
</p>

<dl>
<dt><code>id</code>:</dt><dd><p>text ID</p>
</dd>
<dt><code>split.inf.S</code>:</dt><dd><p>number of sentences containing a split infinitive with <em>-ly</em> adverb; query: <code>_TO0 +ly_AV0 _V?I</code></p>
</dd>
<dt><code>adv.inf.S</code>:</dt><dd><p>number of sentences containing a non-split infinitive with <em>-ly</em> adverb; query: <code>+ly_AV0 _TO0 _V?I</code></p>
</dd>
<dt><code>superlative.S</code>:</dt><dd><p>number of sentences containing a superlative adjective; query: <code>the (_AJS | most _AJ0)</code></p>
</dd>
<dt><code>past.S</code>:</dt><dd><p>number of sentences containing a paste tense verb; query: <code>_V?D</code></p>
</dd>
<dt><code>wh.question.S</code>:</dt><dd><p>number of wh-questions; query: <code>&lt;s&gt; _[PNQ,AVQ] _{V}</code></p>
</dd>
<dt><code>stop.to</code>:</dt><dd><p>frequency of the expression <em>stop to</em> + verb; query: <code>{stop/V} to _{V}</code></p>
</dd>
<dt><code>time</code>:</dt><dd><p>frequency of the noun <em>time</em>; query: <code>{time/N}</code></p>
</dd>
<dt><code>click</code>:</dt><dd><p>frequency of the verb <em>to click</em>; query: <code>{click/V}</code></p>
</dd>
<dt><code>noun</code>:</dt><dd><p>frequency of common nouns; query: <code>_NN?</code></p>
</dd>
<dt><code>nominalization</code>:</dt><dd><p>frequency of nominalizations; query: <code>+[tion,tions,ment,ments,ity,ities]_NN?</code></p>
</dd>
<dt><code>downtoner</code>:</dt><dd><p>frequency of downtoners; query: <code>[almost,barely,hardly,merely,mildly,nearly,only,partially,partly,practically,scarcely,slightly,somewhat]</code></p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Aston, Guy and Burnard, Lou (1998). <em>The BNC Handbook.</em> Edinburgh
University Press, Edinburgh. See also the BNC homepage at
<a href="http://www.natcorp.ox.ac.uk/">http://www.natcorp.ox.ac.uk/</a>.
</p>
<p>Hoffmann, Sebastian; Evert, Stefan; Smith, Nicholas; Lee, David; Berglund Prytz, Ylva (2008).
<em>Corpus Linguistics with BNCweb &ndash; a Practical Guide</em>, volume 6 of English Corpus Linguistics. Peter Lang, Frankfurt am Main.
See also <a href="http://corpora.lancs.ac.uk/BNCweb/">http://corpora.lancs.ac.uk/BNCweb/</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BNCmeta">BNCmeta</a></code></p>

<hr>
<h2 id='BrownBigrams'>
Bigrams of adjacent words from the Brown corpus
</h2><span id='topic+BrownBigrams'></span>

<h3>Description</h3>

<p>This data set contains bigrams of adjacent word forms from the Brown
corpus of written American English (Francis &amp; Kucera 1964).
Co-occurrence frequencies are specified in the form of an observed
contingency table, using the notation suggested by Evert (2008).
</p>
<p>Only bigrams that occur at least 5 times in the corpus are included.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BrownBigrams

</code></pre>


<h3>Format</h3>

<p>A data frame with 24167 rows and the following columns:
</p>

<dl>
<dt><code>id</code>:</dt><dd><p>unique ID of the bigram entry</p>
</dd>
<dt><code>word1</code>:</dt><dd><p>the first word form in the bigram (character)</p>
</dd>
<dt><code>pos1</code>:</dt><dd><p>part-of-speech category of the first word (factor)</p>
</dd>
<dt><code>word2</code>:</dt><dd><p>the second word form in the bigram (character)</p>
</dd>
<dt><code>pos2</code>:</dt><dd><p>part-of-speech category of the second word (factor)</p>
</dd>
<dt><code>O11</code>:</dt><dd><p>co-occurrence frequency of the bigram (numeric)</p>
</dd>
<dt><code>O12</code>:</dt><dd><p>occurrences of the first word without the second (numeric)</p>
</dd> 
<dt><code>O21</code>:</dt><dd><p>occurrences of the second word without the first (numeric)</p>
</dd>
<dt><code>O22</code>:</dt><dd><p>number of bigram tokens containing neither the first nor the second word (numeric)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Part-of-speech categories are identified by single-letter codes, corresponding
of the first character of the Penn tagset.
</p>
<p>Some important POS codes are
<code>N</code> (noun), <code>V</code> (verb), <code>J</code> (adjective), <code>R</code> (adverb or particle),
<code>I</code> (preposition), <code>D</code> (determiner), <code>W</code> (wh-word) and <code>M</code> (modal).
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Evert, Stefan (2008).
Corpora and collocations.
In A. Lüdeling and M. Kytö (eds.), <em>Corpus Linguistics. An International Handbook</em>, chapter 58, pages 1212&ndash;1248. Mouton de Gruyter, Berlin, New York.
</p>
<p>Francis, W.~N. and Kucera, H. (1964).
Manual of information to accompany a standard sample of present-day edited American English, for use with digital computers.
Technical report, Department of Linguistics, Brown University, Providence, RI.
</p>

<hr>
<h2 id='BrownLOBPassives'>
Frequency counts of passive verb phrases in the Brown and LOB corpora
</h2><span id='topic+BrownLOBPassives'></span>

<h3>Description</h3>

<p>This data set contains frequency counts of passive verb phrases for selected texts
from the Brown corpus of written American English (Francis &amp; Kucera 1964)
and the comparable LOB corpus of written British English (Johansson <em>et al.</em> 1978).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BrownLOBPassives

</code></pre>


<h3>Format</h3>

<p>A data frame with 622 rows and the following columns:
</p>

<dl>
<dt><code>id</code>:</dt><dd><p>a unique ID for each text (character)</p>
</dd>
<dt><code>passive</code>:</dt><dd><p>number of passive verb phrases</p>
</dd>
<dt><code>n_w</code>:</dt><dd><p>total number of words in the genre category</p>
</dd>
<dt><code>n_s</code>:</dt><dd><p>total number of sentences in the genre category</p>
</dd>
<dt><code>cat</code>:</dt><dd><p>genre category code (<code>A</code> ... <code>R</code>; factor)</p>
</dd>
<dt><code>genre</code>:</dt><dd><p>descriptive label for the genre category (factor)</p>
</dd>
<dt><code>lang</code>:</dt><dd><p>descriptive label for the genre category</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Francis, W.~N. and Kucera, H. (1964).
Manual of information to accompany a standard sample of present-day edited American English, for use with digital computers.
Technical report, Department of Linguistics, Brown University, Providence, RI.
</p>
<p>Johansson, Stig; Leech, Geoffrey; Goodluck, Helen (1978).
Manual of information to accompany the Lancaster-Oslo/Bergen corpus of British English, for use with digital computers.
Technical report, Department of English, University of Oslo, Oslo.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BrownPassives">BrownPassives</a></code>, <code><a href="#topic+LOBPassives">LOBPassives</a></code>
</p>

<hr>
<h2 id='BrownPassives'>
Frequency counts of passive verb phrases in the Brown corpus
</h2><span id='topic+BrownPassives'></span>

<h3>Description</h3>

<p>This data set contains frequency counts of passive verb phrases
in the Brown corpus of written American English (Francis &amp; Kucera 1964),
aggregated by genre category.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BrownPassives

</code></pre>


<h3>Format</h3>

<p>A data frame with 15 rows and the following columns:
</p>

<dl>
<dt><code>cat</code>:</dt><dd><p>genre category code (<code>A</code> ... <code>R</code>)</p>
</dd>
<dt><code>passive</code>:</dt><dd><p>number of passive verb phrases</p>
</dd>
<dt><code>n_w</code>:</dt><dd><p>total number of words in the genre category</p>
</dd>
<dt><code>n_s</code>:</dt><dd><p>total number of sentences in the genre category</p>
</dd>
<dt><code>name</code>:</dt><dd><p>descriptive label for the genre category</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Francis, W.~N. and Kucera, H. (1964).
Manual of information to accompany a standard sample of present-day edited American English, for use with digital computers.
Technical report, Department of Linguistics, Brown University, Providence, RI.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LOBPassives">LOBPassives</a></code>, <code><a href="#topic+BrownLOBPassives">BrownLOBPassives</a></code>
</p>

<hr>
<h2 id='BrownStats'>
Basic statistics of texts in the Brown corpus
</h2><span id='topic+BrownStats'></span>

<h3>Description</h3>

<p>This data set provides some basic quantiative measures for all texts
in the Brown corpus of written American English (Francis &amp; Kucera 1964),
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BrownStats

</code></pre>


<h3>Format</h3>

<p>A data frame with 500 rows and the following columns:
</p>

<dl>
<dt><code>ty</code>:</dt><dd><p>number of distinct types</p>
</dd>
<dt><code>to</code>:</dt><dd><p>number of tokens (including punctuation)</p>
</dd>
<dt><code>se</code>:</dt><dd><p>number of sentences</p>
</dd>
<dt><code>towl</code>:</dt><dd><p>mean word length in characters, averaged over tokens</p>
</dd>
<dt><code>tywl</code>:</dt><dd><p>mean word length in characters, averaged over types</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marco Baroni &lt;<a href="mailto:baroni@sslmit.unibo.it">baroni@sslmit.unibo.it</a>&gt;</p>


<h3>References</h3>

<p>Francis, W.~N. and Kucera, H. (1964).
Manual of information to accompany a standard sample of present-day edited American English, for use with digital computers.
Technical report, Department of Linguistics, Brown University, Providence, RI.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LOBStats">LOBStats</a></code>
</p>

<hr>
<h2 id='chisq'>Pearson's chi-squared statistic for frequency comparisons (corpora)</h2><span id='topic+chisq'></span>

<h3>Description</h3>

<p>This function computes Pearson's chi-squared statistic (often written
as <code class="reqn">X^2</code>) for frequency comparison data, with or without Yates'
continuity correction.  The implementation is based on the formula
given by Evert (2004, 82).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
chisq(k1, n1, k2, n2, correct = TRUE, one.sided=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chisq_+3A_k1">k1</code></td>
<td>
<p>frequency of a type in the first corpus (or an integer
vector of type frequencies)</p>
</td></tr>
<tr><td><code id="chisq_+3A_n1">n1</code></td>
<td>
<p>the sample size of the first corpus (or an integer vector
specifying the sizes of different samples)</p>
</td></tr>
<tr><td><code id="chisq_+3A_k2">k2</code></td>
<td>
<p>frequency of the type in the second corpus (or an integer
vector of type frequencies, in parallel to <code>k1</code>)</p>
</td></tr>
<tr><td><code id="chisq_+3A_n2">n2</code></td>
<td>
<p>the sample size of the second corpus (or an integer vector
specifying the sizes of different samples, in parallel to
<code>n1</code>)</p>
</td></tr>
<tr><td><code id="chisq_+3A_correct">correct</code></td>
<td>
<p>if <code>TRUE</code>, apply Yates' continuity correction
(default)</p>
</td></tr>
<tr><td><code id="chisq_+3A_one.sided">one.sided</code></td>
<td>
<p>if <code>TRUE</code>, compute the <em>signed square root</em>
of <code class="reqn">X^2</code> as a statistic for a one-sided test (see details below;
the default value is <code>FALSE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">X^2</code> values returned by this function are identical to those
computed by <code><a href="stats.html#topic+chisq.test">chisq.test</a></code>.  Unlike the latter, <code>chisq</code>
accepts vector arguments so that a large number of frequency
comparisons can be carried out with a single function call.
</p>
<p>The one-sided test statistic (for <code>one.sided=TRUE</code>) is the signed
square root of <code class="reqn">X^2</code>.  It is positive for <code class="reqn">k_1/n_1 &gt; k_2/n_2</code>
and negative for <code class="reqn">k_1/n_1 &lt; k_2/n_2</code>.  Note that this statistic
has a <em>standard normal distribution</em> rather than a chi-squared
distribution under the null hypothesis of equal proportions.
</p>


<h3>Value</h3>

<p>The chi-squared statistic <code class="reqn">X^2</code> corresponding to the specified
data (or a vector of <code class="reqn">X^2</code> values).  This statistic has a
<em>chi-squared distribution</em> with <code class="reqn">df=1</code> under the null
hypothesis of equal proportions.
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Evert, Stefan (2004). <em>The Statistics of Word Cooccurrences: Word
Pairs and Collocations.</em>  Ph.D. thesis, Institut f?r maschinelle
Sprachverarbeitung, University of Stuttgart.  Published in 2005, URN
urn:nbn:de:bsz:93-opus-23714. Available from
<a href="http://www.collocations.de/phd.html">http://www.collocations.de/phd.html</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chisq.pval">chisq.pval</a></code>, <code><a href="stats.html#topic+chisq.test">chisq.test</a></code>,
<code><a href="#topic+cont.table">cont.table</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>chisq.test(cont.table(99, 1000, 36, 1000))
chisq(99, 1000, 36, 1000)
</code></pre>

<hr>
<h2 id='chisq.pval'>P-values of Pearson's chi-squared test for frequency comparisons (corpora)</h2><span id='topic+chisq.pval'></span>

<h3>Description</h3>

<p>This function computes the p-value of Pearsons's chi-squared test for
the comparison of corpus frequency counts (under the null hypothesis
of equal population proportions). It is based on the chi-squared
statistic <code class="reqn">X^2</code> implemented by the <code><a href="#topic+chisq">chisq</a></code> function.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
chisq.pval(k1, n1, k2, n2, correct = TRUE,
           alternative = c("two.sided", "less", "greater"))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chisq.pval_+3A_k1">k1</code></td>
<td>
<p>frequency of a type in the first corpus (or an integer
vector of type frequencies)</p>
</td></tr>
<tr><td><code id="chisq.pval_+3A_n1">n1</code></td>
<td>
<p>the sample size of the first corpus (or an integer vector
specifying the sizes of different samples)</p>
</td></tr>
<tr><td><code id="chisq.pval_+3A_k2">k2</code></td>
<td>
<p>frequency of the type in the second corpus (or an integer
vector of type frequencies, in parallel to <code>k1</code>)</p>
</td></tr>
<tr><td><code id="chisq.pval_+3A_n2">n2</code></td>
<td>
<p>the sample size of the second corpus (or an integer vector
specifying the sizes of different samples, in parallel to
<code>n1</code>)</p>
</td></tr>
<tr><td><code id="chisq.pval_+3A_correct">correct</code></td>
<td>
<p>if <code>TRUE</code>, apply Yates' continuity correction
(default)</p>
</td></tr>
<tr><td><code id="chisq.pval_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis; must be one of <code>two.sided</code> (default), <code>less</code>
or <code>greater</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The p-values returned by this functions are identical to those
computed by <code><a href="stats.html#topic+chisq.test">chisq.test</a></code> (two-sided only) and
<code><a href="stats.html#topic+prop.test">prop.test</a></code> (one-sided and two-sided) for two-by-two
contingency tables.
</p>


<h3>Value</h3>

<p>The p-value of Pearson's chi-squared test applied to the given data
(or a vector of p-values).
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>See Also</h3>

<p><code><a href="#topic+chisq">chisq</a></code>, <code><a href="#topic+fisher.pval">fisher.pval</a></code>,
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code>, <code><a href="stats.html#topic+prop.test">prop.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>chisq.test(cont.table(99, 1000, 36, 1000))
chisq.pval(99, 1000, 36, 1000)
</code></pre>

<hr>
<h2 id='cont.table'>Build contingency tables for frequency comparison (corpora)</h2><span id='topic+cont.table'></span>

<h3>Description</h3>

<p>This is a convenience function which constructs 2x2 contingency tables
needed for frequency comparisons with <code><a href="stats.html#topic+chisq.test">chisq.test</a></code>, <code><a href="stats.html#topic+fisher.test">fisher.test</a></code>
and similar functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
cont.table(k1, n1, k2, n2, as.list=NA)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cont.table_+3A_k1">k1</code></td>
<td>
<p>frequency of a type in the first corpus, a numeric scalar or vector</p>
</td></tr>
<tr><td><code id="cont.table_+3A_n1">n1</code></td>
<td>
<p>the size of the first corpus (sample size), a numeric scalar or vector</p>
</td></tr>
<tr><td><code id="cont.table_+3A_k2">k2</code></td>
<td>
<p>frequency of the type in the second corpus, a numeric scalar or vector</p>
</td></tr>
<tr><td><code id="cont.table_+3A_n2">n2</code></td>
<td>
<p>the size of the second corpus (sample size), a numeric scalar or vector</p>
</td></tr>
<tr><td><code id="cont.table_+3A_as.list">as.list</code></td>
<td>
<p>whether multiple contingency tables can be constructed and are returned as a list (see &quot;Details&quot; below)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If all four arguments <code>k1 n1 k2 n2</code> are scalars (vectors of length 1),
<code>cont.table</code> constructs a single contingency table, i.e. a 2x2 matrix.
If at least one argument has length &gt; 1, shorter vectors are replicated as 
necessary, and a list of 2x2 contingency tables is constructed.
</p>
<p>With <code>as.list=TRUE</code>, the return value is always a list, even if it contains
just a single contingency table.  With <code>as.list=FALSE</code>, only scalar arguments
are accepted and the return value is guaranteed to be a 2x2 matrix.
</p>


<h3>Value</h3>

<p>A numeric matrix containing a two-by-two contingency table for the
specified frequency comparison, or a list of such matrices (see &quot;Details&quot;).
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+chisq.test">chisq.test</a></code>, <code><a href="stats.html#topic+fisher.test">fisher.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>ct &lt;- cont.table(42, 100, 66, 200)
ct

chisq.test(ct)
</code></pre>

<hr>
<h2 id='corpora.palette'>
Colour palettes for linguistic visualization (corpora)
</h2><span id='topic+corpora.palette'></span><span id='topic+alpha.col'></span>

<h3>Description</h3>

<p>Several useful colour palettes for plots and other visualizations.
</p>
<p>The function <code>alpha.col</code> can be used to turn colours (partially) translucent for used in crowded scatterplots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corpora.palette(name=c("seaborn", "muted", "bright", "simple"), 
                n=NULL, alpha=1)

alpha.col(col, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corpora.palette_+3A_name">name</code></td>
<td>

<p>name of the desired colour palette (see Details below)
</p>
</td></tr>
<tr><td><code id="corpora.palette_+3A_n">n</code></td>
<td>

<p>optional: number of colours to return. The palette will be shortened or recycled as necessary.
</p>
</td></tr>
<tr><td><code id="corpora.palette_+3A_col">col</code></td>
<td>

<p>a vector of R colour specifications (as accepted by <code><a href="grDevices.html#topic+col2rgb">col2rgb</a></code>)
</p>
</td></tr>
<tr><td><code id="corpora.palette_+3A_alpha">alpha</code></td>
<td>

<p>alpha value between 0 and 1; values below 1 make the colours translucent
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Every colour palette starts with the colours black, red, green and blue in this order.
</p>
<p><code>seaborn</code>, <code>muted</code> and <code>bright</code> are 7-colour palettes inspired by the <a href="https://seaborn.pydata.org/">seaborn</a> data visualization library, but add a shade of dark grey as first colour.  
</p>
<p><code>simple</code> is a 10-colour palette based on R's default palette.
</p>


<h3>Value</h3>

<p>A character vector with colour names or hexadecimal RGB specifications.
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>See Also</h3>

<p><code><a href="grDevices.html#topic+rgb">rgb</a></code> for R colour specification formats, <code><a href="grDevices.html#topic+palette">palette</a></code> for setting the default colour palette
</p>


<h3>Examples</h3>

<pre><code class='language-R'>par.save &lt;- par(mfrow=c(2, 2))
for (name in qw("seaborn muted bright simple")) {
  barplot(rep(1, 10), col=corpora.palette(name, 10), main=name)
}
par(par.save)
</code></pre>

<hr>
<h2 id='DistFeatBrownFam'>
Latent dimension scores from a distributional analysis of the Brown Family corpora
</h2><span id='topic+DistFeatBrownFam'></span>

<h3>Description</h3>

<p>This data frame provides unsupervised distributional features for each text in
the extended Brown Family of corpora (Brown, LOB, Frown, FLOB, BLOB), covering
edited written American and British English from 1930s, 1960s and 1990s (see Xiao 2008, 395&ndash;397).
</p>
<p>Latent topic dimensions were obtained by a method similar to Latent Semantic Indexing (Deerwester et al. 1990),
applying singular value decomposition to bag-of-words vectors for the 2500 texts in the extended Brown Family.
Register dimensions were obtained with the same methodology, using vectors of part-of-speech frequencies
(separately for all verb-related tags and all other tags).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
DistFeatBrownFam

</code></pre>


<h3>Format</h3>

<p>A data frame with 2500 rows and the following 23 columns:
</p>

<dl>
<dt><code>id</code>:</dt><dd><p>A unique ID for each text (also used as row name)</p>
</dd>
<dt><code>top1</code>, <code>top2</code>, <code>top3</code>, <code>top4</code>, <code>top5</code>, <code>top6</code>, <code>top7</code>, <code>top8</code>, <code>top9</code>:</dt><dd><p>latent dimension scores for the first 9 topic dimensions</p>
</dd>
<dt><code>reg1</code>, <code>reg2</code>, <code>reg3</code>, <code>reg4</code>, <code>reg5</code>, <code>reg6</code>, <code>reg7</code>, <code>reg8</code>, <code>reg9</code>:</dt><dd><p>latent dimension scores for the first 9 register dimensions (excluding verb-related tags)</p>
</dd>
<dt><code>vreg1</code>, <code>vreg2</code>, <code>vreg3</code>, <code>vreg4</code>:</dt><dd><p>latent dimension scores for the first 4 register dimensions based only on verb-related tags</p>
</dd>
</dl>



<h3>Details</h3>

<p><b>TODO</b>
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Deerwester, Scott; Dumais, Susan T.; Furnas, George W.; Landauer, Thomas K.; Harshman, Richard (1990). Indexing by latent semantic analysis. <em>Journal of the American Society For Information Science</em>, <b>41</b>(6), 391&ndash;407.
</p>
<p>Xiao, Richard (2008). Well-known and influential corpora. In A. Lüdeling and M. Kytö (eds.), <em>Corpus Linguistics. An International Handbook</em>, chapter 20, pages 383&ndash;457. Mouton de Gruyter, Berlin.
</p>

<hr>
<h2 id='fisher.pval'>P-values of Fisher's exact test for frequency comparisons (corpora)</h2><span id='topic+fisher.pval'></span>

<h3>Description</h3>

<p>This function computes the p-value of Fisher's exact test (Fisher
1934) for the comparison of corpus frequency counts (under the null
hypothesis of equal population proportions).  In the two-sided case,
a &ldquo;central&rdquo; p-value (Fay 2010) provides better numerical efficiency
than the likelihood-based approach of <code>fisher.test</code> and is always
consistent with confidence intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
fisher.pval(k1, n1, k2, n2, 
            alternative = c("two.sided", "less", "greater"),
            log.p = FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fisher.pval_+3A_k1">k1</code></td>
<td>
<p>frequency of a type in the first corpus (or an integer
vector of type frequencies)</p>
</td></tr>
<tr><td><code id="fisher.pval_+3A_n1">n1</code></td>
<td>
<p>the sample size of the first corpus (or an integer vector
specifying the sizes of different samples)</p>
</td></tr>
<tr><td><code id="fisher.pval_+3A_k2">k2</code></td>
<td>
<p>frequency of the type in the second corpus (or an integer
vector of type frequencies, in parallel to <code>k1</code>)</p>
</td></tr>
<tr><td><code id="fisher.pval_+3A_n2">n2</code></td>
<td>
<p>the sample size of the second corpus (or an integer vector
specifying the sizes of different samples, in parallel to
<code>n1</code>)</p>
</td></tr>
<tr><td><code id="fisher.pval_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis; must be one of <code>two.sided</code> (default), <code>less</code>
or <code>greater</code></p>
</td></tr>
<tr><td><code id="fisher.pval_+3A_log.p">log.p</code></td>
<td>
<p>if TRUE, the natural logarithm of the p-value is returned</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>alternative="two.sided"</code> (the default), the p-value of the
&ldquo;central&rdquo; Fisher's exact test (Fay 2010) is computed, which 
differs from the more common likelihood-based method implemented by
<code>fisher.test</code> (and referred to as the &ldquo;two-sided Fisher's
exact test&rdquo; by Fay).  This approach has two advantages:
(i) it is numerically robust and efficient, even for very large samples and frequency counts;
(ii) it is consistent with Clopper-Pearson type confidence intervals (see examples below).
</p>
<p>For one-sided tests, the p-values returned by this function are identical
to those computed by <code><a href="stats.html#topic+fisher.test">fisher.test</a></code> on two-by-two contingency tables.
</p>


<h3>Value</h3>

<p>The p-value of Fisher's exact test applied to the given data
(or a vector of p-values).
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Fay, Michael P. (2010). Confidence intervals that match Fisher's exact or Blaker's exact tests. <em>Biostatistics</em>, <b>11</b>(2), 373-374.
</p>
<p>Fisher, R. A. (1934). <em>Statistical Methods for Research Workers</em>.
Oliver &amp; Boyd, Edinburgh, 2nd edition (1st edition 1925, 14th
edition 1970).
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+fisher.test">fisher.test</a></code>, <code><a href="#topic+chisq.pval">chisq.pval</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Fisher's Tea Drinker (see ?fisher.test)
TeaTasting &lt;-
matrix(c(3, 1, 1, 3),
       nrow = 2,
       dimnames = list(Guess = c("Milk", "Tea"),
                       Truth = c("Milk", "Tea")))
print(TeaTasting)
##  - the "corpora" consist of 4 cups of tea each (n1 = n2 = 4)
##     =&gt; columns of TeaTasting
##  - frequency counts are the number of cups selected by drinker (k1 = 3, k2 = 1)
##     =&gt; first row of TeaTasting
##  - null hypothesis of equal type probability = drinker makes random guesses
fisher.pval(3, 4, 1, 4, alternative="greater")
fisher.test(TeaTasting, alternative="greater")$p.value # should be the same

fisher.pval(3, 4, 1, 4)         # central Fisher's exact test is equal to
fisher.test(TeaTasting)$p.value # standard two-sided Fisher's test for symmetric distribution

# inconsistency btw likelihood-based two-sided Fisher's test and confidence interval
# for 4/15 vs. 50/619 successes
fisher.test(cbind(c(4, 11), c(50, 619)))

# central Fisher's exact test is always consistent
fisher.pval(4, 15, 50, 619)
</code></pre>

<hr>
<h2 id='keyness'>Compute best-practice keyness measures (corpora)</h2><span id='topic+keyness'></span>

<h3>Description</h3>

<p>Compute best-practice keyness measures (according to Evert 2022) for the 
frequency comparison of lexical items in two corpora.
The function is fully vectorised and should be applied to a complete
data set of candidate items (so statistical analysis can be adjusted to
control the family-wise error rate).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
keyness(f1, n1, f2, n2, measure=c("LRC", "PositiveLRC", "G2", "LogRatio", "SimpleMaths"),
        conf.level=.95, alpha=NULL, p.adjust=TRUE, lambda=1)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keyness_+3A_f1">f1</code></td>
<td>
<p>a numeric vector specifying the frequencies of candidate items in corpus A (target corpus)</p>
</td></tr>
<tr><td><code id="keyness_+3A_n1">n1</code></td>
<td>
<p>sample size of target corpus, i.e. the total number of tokens in corpus A (usually a scalar, but can also be a vector parallel to <code>f1</code>)</p>
</td></tr>
<tr><td><code id="keyness_+3A_f2">f2</code></td>
<td>
<p>a numeric vector parallel to <code>f1</code>, specifying the frequencies of candidate items in corpus B (reference corpus)</p>
</td></tr>
<tr><td><code id="keyness_+3A_n2">n2</code></td>
<td>
<p>sample size of reference corpus, i.e. the total number of tokens in corpus B (usually a scalar, but can also be a vector parallel to <code>f2</code>)</p>
</td></tr>
<tr><td><code id="keyness_+3A_measure">measure</code></td>
<td>
<p>the keyness measure to be computed (see &ldquo;Details&rdquo; below)</p>
</td></tr>
<tr><td><code id="keyness_+3A_conf.level">conf.level</code></td>
<td>
<p>the desired confidence level for the <code>LRC</code> and <code>PositiveLRC</code> measures (defaults to 95%)</p>
</td></tr>
<tr><td><code id="keyness_+3A_alpha">alpha</code></td>
<td>
<p>if specified, filter out candidate items whose frequency difference between <code class="reqn">f_1</code> and <code class="reqn">f_2</code> is not significant at level <code class="reqn">\alpha</code>.
This is achieved by setting the score of such candidates to 0.</p>
</td></tr>
<tr><td><code id="keyness_+3A_p.adjust">p.adjust</code></td>
<td>
<p>if <code>TRUE</code>, apply a Bonferroni correction in order to control the family-wise error rate across all tests carried out
in a single function call (i.e. the common length of <code>f1</code> and <code>f2</code>).
Alternatively, the desired family size can be specified instead of <code>TRUE</code> (useful if a larger data set is processed in batches).
The adjustment applied both the the significance filter (<code>alpha</code>) and the confidence intervals (<code>conf.level</code>) underlying <code>LRC</code> and <code>PositiveLRC</code> measures.</p>
</td></tr>
<tr><td><code id="keyness_+3A_lambda">lambda</code></td>
<td>
<p>parameter <code class="reqn">\lambda</code> of the <code>SimpleMaths</code> measure.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes a range of best-practice keyness measures comparing the relative frequencies
<code class="reqn">\pi_1</code> and <code class="reqn">\pi_2</code> of lexical items in populations (i.e. sublanguages) A and B,
based on the observed sample frequencies <code class="reqn">f_1, f_2</code> and the corresponding sample sizes <code class="reqn">n_1, n_2</code>.
The function is fully vectorised with respect to arguments <code>f1</code>, <code>f2</code>, <code>n1</code> and <code>n2</code>,
but only a single keyness measure can be selected for each function call.
All implemented measures are robust for the corner cases <code class="reqn">f_1 = 0</code> and <code class="reqn">f_2 = 0</code>, but <code class="reqn">f_1 = f_2 = 0</code> is not allowed.
</p>
<p>Most of the keyness measures are <b>directional</b>, 
i.e. positive scores indicate positive keyness in A (<code class="reqn">\pi_1 &gt; \pi_2</code>)
and negative scores indicate negative keyness in A (<code class="reqn">\pi_1 &lt; \pi_2</code>).
By contrast, the <b>one-sided</b> measures <code>PositiveLRC</code> and <code>SimpleMaths</code> only detect positive keyness in A,
returning small (and possibly negative) scores otherwise, i.e. in case of insufficient evidence for <code class="reqn">\pi_1 &gt; \pi_2</code>
and in case of strong evidence for <code class="reqn">\pi_1 &lt; \pi_2</code>.
One-sided measures can be useful for a ranking of the entire data set as positive keyword candidates.
</p>
<p>Hardie (2014) and other authors recommend to combine effect-size measures (in particular <code>LogRatio</code>) with
a <b>significance filter</b> in order to weed out candidate items for which there is no significant evidence
against the null hypothesis <code class="reqn">H_0: \pi_1 = \pi_2</code>.  Such a filter is activated by specifying the desired
significance level <code>alpha</code>, and can be combined with all keyness measures.
In this case, the scores of all non-significant candidate items are set to 0.
The decision is based in the likelihood-ratio test implemented by the <code>G2</code> measure
and its asymptotic <code class="reqn">\chi^2_1</code> distribution under <code class="reqn">H_0</code>.
</p>
<p>Note that the significance filter can also be applied to the <code>G2</code> measure itself, setting all scores
below the critical value for the significance test to 0.
For one-sided measures (<code>PositiveLRC</code> and <code>SimpleMaths</code>), candidates with significant evidence 
for negative keyness are also filtered out (i.e. their scores are set to 0) in order to ensure a consistent ranking.
</p>
<p>By default, statistical inference corrects for multiple testing in order to control <b>family-wise error rates</b>.
This applies to the significance filter as well as to the confidence intervals underlying <code>LRC</code> and <code>PositiveLRC</code>.
Note that the <code>G2</code> scores themselves are never adjusted (only the critical value for the significance filter is modified).
</p>
<p>Family size <code class="reqn">m</code> is automatically determined from the number of candidate items processed in a single function call.
Alternatively, the family size can be specified explicitly in the <code>p.adjust</code> argument, e.g. if a large data set
is processed in multiple batches, or <code>p.adjust=FALSE</code> can be used to disable the correction.
</p>
<p>For the adjustment, a highly conservative Bonferroni correction <code class="reqn">\alpha' = \alpha / m</code> is applied to significance levels.
Since the large candidate sets and sample sizes often found in corpus linguistics tend to produce large numbers of false positives,
this conservative approach is considered to be useful.
</p>
<p>See Evert (2022) and its supplementary materials for a more detailed discussion of the implemented best-practice measures and some alternatives.
</p>


<h4>Keyness Measures</h4>


<dl>
<dt><code>G2</code></dt><dd>
<p>The <b>log-likelihood</b> measure (Rayson &amp; Garside 2003: 3) computes the score <code class="reqn">G^2</code>
of a likelihood-ratio test for <code class="reqn">H_0: \pi_1 = \pi_2</code>.  This test is two-sided and
always returns positive values, so the sign of its score is inverted for <code class="reqn">f_1 / n_1 &lt; f_2 / n_2</code> 
in order to obtain a directional keyness measure.
As a pure significance measure, it tends to prefer high-frequency candidates with large <code class="reqn">f_1</code>.
</p>
</dd>
<dt><code>LogRatio</code></dt><dd>
<p>A point estimate of the log <b>relative risk</b> <code class="reqn">\log_2 (\pi_1 / \pi_2)</code>, which has been suggested
as an intuitive keyness measure under the name <b>LogRatio</b> by Hardie (2014: 45).
The implementation uses Walter's (1975) adjusted estimator </p>
<p style="text-align: center;"><code class="reqn">%
          \log_2 \dfrac{f_1 + \frac12}{n_1 + \frac12} - \log_2 \dfrac{f_2 + \frac12}{n_2 + \frac12} 
          </code>
</p>
 
<p>which is less biased and robust against <code class="reqn">f_i = 0</code>.
As a pure effect-size measure, LogRatio tends to assign spuriously high scores to low-frequency candidates
with small <code class="reqn">f_1</code> and <code class="reqn">f_2</code> (due to sampling variation).
Combination with a significance filter is strongly recommended.
</p>
</dd>
<dt><code>LRC</code> (default)</dt><dd>
<p>A <b>conservative</b> estimate for <b>LogRatio</b> recommended by Evert (2022) in order to combine
and balance the advantages of effect-size and significance measures.
A confidence interval (according to the specified <code>conf.level</code>) for relative risk <code class="reqn">r = \pi_1 / \pi_2</code>
is obtained from an exact conditional Poisson test (Fay 2010: 55), adjusted for multiple testing by default.
If a candidate is not significant (i.e. the confidence interval includes <code class="reqn">H_0: r = 1</code>) its score is set to 0.
Otherwise the boundary of the confidence interval closer to 1 is taken as a conservative directional estimate
of <code class="reqn">r</code> and its <code class="reqn">\log_2</code> is returned.
</p>
</dd>
<dt><code>PositiveLRC</code></dt><dd>
<p>A <b>one-sided</b> variant of <b>LRC</b>, which returns the lower boundary of a one-sided confidence interval
for <code class="reqn">\log_2 r</code>. Scores <code class="reqn">\leq 0</code> indicate that there is no significant evidence for positive keyness.
The directional version of LRC is recommended for general use, but PositiveLRC may be preferred if the
hermeneutic interpretation should also consider non-significant candidates (especially with small data sets).
</p>
</dd>
<dt><code>SimpleMaths</code></dt><dd>
<p>The <b>simple maths</b> keyness measure (Kilgarriff 2009) used by the commercial corpus analysis
platform <b>Sketch Engine</b>: </p>
<p style="text-align: center;"><code class="reqn">
          \dfrac{10^6 \cdot \frac{f_1}{n_1} + \lambda}{10^6 \cdot \frac{f_2}{n_2} + \lambda} 
          </code>
</p>

<p>Its frequency bias can be adjusted with the user parameter <code class="reqn">\lambda &gt; 0</code>. The scaling
factor <code class="reqn">10^6</code> was chosen so that <code class="reqn">\lambda = 1</code> is a practical default value.
</p>
<p>There does not appear to be a convincing mathematical justification behind this measure. It is 
included here only because of the popularity of the Sketch Engine platform.
</p>
</dd>
</dl>




<h3>Value</h3>

<p>A numeric vector of the same length as <code>f1</code> and <code>f2</code>, containing keyness scores for all candidate lexical items.
For most measures, positive scores indicate positive keywords (i.e. higher frequency in the population underlying corpus A)
and negative scores indicate negative keywords (i.e. higher frequency in the population underlying corpus B).
If <code>alpha</code> is specified, non-significant candidates always have a score of 0.
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Evert, S. (2022). Measuring keyness. In <em>Digital Humanities 2022: Conference Abstracts</em>, pages 202-205, Tokyo, Japan / online. 
<a href="https://osf.io/cy6mw/">https://osf.io/cy6mw/</a>
</p>
<p>Fay, Michael P. (2010). Two-sided exact tests and matching confidence intervals for discrete data. <em>The R Journal</em>, <b>2</b>(1), 53-58.
</p>
<p>Hardie, A. (2014). A single statistical technique for keywords, lockwords, and collocations. Internal CASS working paper no. 1, unpublished.
</p>
<p>Kilgarriff, A. (2009). Simple maths for keywords. In <em>Proceedings of the Corpus Linguistics 2009 Conference</em>, Liverpool, UK.
</p>
<p>Rayson, P. and Garside, R. (2000). Comparing corpora using frequency profiling. In <em>Proceedings of the ACL Workshop on Comparing Corpora</em>, pages 1-6, Hong Kong.  
</p>
<p>Walter, S. D. (1975). The distribution of Levin’s measure of attributable risk. <em>Biometrika</em>, <b>62</b>(2): 371-374.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prop.cint">prop.cint</a></code>, which is used by the exact conditional Poisson test of the LRC measure</p>


<h3>Examples</h3>

<pre><code class='language-R'># compute all keyness measures for a single candidate item with f1=7, f2=2 and n1=n2=1000
keyness(7, 1000, 2, 1000, measure="G2") # log-likelihood
keyness(7, 1000, 2, 1000, measure="LogRatio")
keyness(7, 1000, 2, 1000, measure="LogRatio", alpha=0.05) # with significance filter
keyness(7, 1000, 2, 1000, measure="LRC") # the default measure
keyness(7, 1000, 2, 1000, measure="PositiveLRC")
keyness(7, 1000, 2, 1000, measure="SimpleMaths")

# a practical example: keywords of spoken British English (from BNC corpus)
n1 &lt;- sum(BNCcomparison$spoken) # sample sizes
n2 &lt;- sum(BNCcomparison$written)
kw &lt;- transform(BNCcomparison,
  G2 = keyness(spoken, n1, written, n2, measure="G2"),
  LogRatio = keyness(spoken, n1, written, n2, measure="LogRatio"),
  LRC = keyness(spoken, n1, written, n2))
kw &lt;- kw[order(-kw$LogRatio), ]
head(kw, 20)

# collocations of "in charge of" with LRC as an association measure
colloc &lt;- transform(BNCInChargeOf, 
  PosLRC = keyness(f.in, N.in, f.out, N.out, measure="PositiveLRC"))
colloc &lt;- colloc[order(-colloc$PosLRC), ]
head(colloc, 30)
</code></pre>

<hr>
<h2 id='KrennPPV'>
German PP-Verb collocation candidates annotated by Brigitte Krenn (2000)
</h2><span id='topic+KrennPPV'></span>

<h3>Description</h3>

<p>This data set lists 5102 frequent combinations of verbs and prepositional phrases (PP)
extracted from a German newspaper corpus.  The collocational status of each
PP-verb combination was manually annotated by Brigitte Krenn (2000).  In addition,
pre-computed scores of several standard association measures are provided.
</p>
<p>The <code>KrennPPV</code> candidate set forms part of the data used in the evaluation study
of Evert &amp; Krenn (2005). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
KrennPPV

</code></pre>


<h3>Format</h3>

<p>A data frame with 5102 rows and the following columns:
</p>

<dl>
<dt><code>PP</code>:</dt><dd><p>the prepositional phrase, represented by preposition and lemma of the nominal head (character).
Preposition-article fusion is indicated by a <code>+</code> sign. For example, the prepositional phrase
<em>im letzten Jahr</em> would appear as <code>in:Jahr</code> in the data set.
</p>
</dd>
<dt><code>verb</code>:</dt><dd><p>the verb lemma (character).
Separated particle verbs have been recombined.
</p>
</dd>
<dt><code>is.colloc</code>:</dt><dd><p>whether the PP-verb combination is a lexical collocation (logical)</p>
</dd>
<dt><code>is.SVC</code>:</dt><dd><p>whether a PP-verb collocation is a support verb construction (logical)</p>
</dd>
<dt><code>is.figur</code>:</dt><dd><p>whether a PP-verb-collocation is a figurative expression (logical)</p>
</dd>
<dt><code>freq</code>:</dt><dd><p>co-occurrence frequency of the PP-verb combination within clauses (integer)</p>
</dd>
<dt><code>MI</code>:</dt><dd><p>Mutual Information association measure</p>
</dd>
<dt><code>Dice</code>:</dt><dd><p>Dice coefficient association measure</p>
</dd>
<dt><code>z.score</code>:</dt><dd><p>z-score association measure</p>
</dd>
<dt><code>t.score</code>:</dt><dd><p>t-score association measure</p>
</dd> 
<dt><code>chisq</code>:</dt><dd><p>chi-squared association measure (without Yates' continuity correction)</p>
</dd>
<dt><code>chisq.corr</code>:</dt><dd><p>chi-squared association measure (with Yates' continuity correction)</p>
</dd>
<dt><code>log.like</code>:</dt><dd><p>log-likelihood association measure</p>
</dd>
<dt><code>Fisher</code>:</dt><dd><p>Fisher's exact test as an association measure (negative logarithm of one-sided p-value)</p>
</dd>
</dl>

<p>See Evert (2008) and <a href="http://www.collocations.de/AM/">http://www.collocations.de/AM/</a> for details on these association measures.
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Evert, Stefan (2008).
Corpora and collocations.
In A. Lüdeling and M. Kytö (eds.), <em>Corpus Linguistics. An International Handbook</em>, chapter 58, pages 1212&ndash;1248. Mouton de Gruyter, Berlin, New York.
</p>
<p>Evert, Stefan and Krenn, Brigitte (2005).
Using small random samples for the manual evaluation of statistical association measures.
<em>Computer Speech and Language</em>, <b>19</b>(4), 450&ndash;466.
</p>
<p>Krenn, Brigitte (2000).
<em>The Usual Suspects: Data-Oriented Models for the Identification and Representation of Lexical Collocations</em>,
volume~7 of <em>Saarbrücken Dissertations in Computational Linguistics and Language Technology</em>.
DFKI &amp; Universität des Saarlandes, Saarbrücken, Germany.
</p>

<hr>
<h2 id='LOBPassives'>
Frequency counts of passive verb phrases in the LOB corpus
</h2><span id='topic+LOBPassives'></span>

<h3>Description</h3>

<p>This data set contains frequency counts of passive verb phrases
in the LOB corpus of written British English (Johansson <em>et al.</em> 1978),
aggregated by genre category.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
BrownPassives

</code></pre>


<h3>Format</h3>

<p>A data frame with 15 rows and the following columns:
</p>

<dl>
<dt><code>cat</code>:</dt><dd><p>genre category code (<code>A</code> ... <code>R</code>)</p>
</dd>
<dt><code>passive</code>:</dt><dd><p>number of passive verb phrases</p>
</dd>
<dt><code>n_w</code>:</dt><dd><p>total number of words in the genre category</p>
</dd>
<dt><code>n_s</code>:</dt><dd><p>total number of sentences in the genre category</p>
</dd>
<dt><code>name</code>:</dt><dd><p>descriptive label for the genre category</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Johansson, Stig; Leech, Geoffrey; Goodluck, Helen (1978).
Manual of information to accompany the Lancaster-Oslo/Bergen corpus of British English, for use with digital computers.
Technical report, Department of English, University of Oslo, Oslo.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BrownPassives">BrownPassives</a></code>, <code><a href="#topic+BrownLOBPassives">BrownLOBPassives</a></code>
</p>

<hr>
<h2 id='LOBStats'>
Basic statistics of texts in the LOB corpus
</h2><span id='topic+LOBStats'></span>

<h3>Description</h3>

<p>This data set provides some basic quantiative measures for all texts
in the LOB corpus of written British English (Johansson <em>et al.</em> 1978).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
LOBStats

</code></pre>


<h3>Format</h3>

<p>A data frame with 500 rows and the following columns:
</p>

<dl>
<dt><code>ty</code>:</dt><dd><p>number of distinct types</p>
</dd>
<dt><code>to</code>:</dt><dd><p>number of tokens (including punctuation)</p>
</dd>
<dt><code>se</code>:</dt><dd><p>number of sentences</p>
</dd>
<dt><code>towl</code>:</dt><dd><p>mean word length in characters, averaged over tokens</p>
</dd>
<dt><code>tywl</code>:</dt><dd><p>mean word length in characters, averaged over types</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Marco Baroni &lt;<a href="mailto:baroni@sslmit.unibo.it">baroni@sslmit.unibo.it</a>&gt;</p>


<h3>References</h3>

<p>Johansson, Stig; Leech, Geoffrey; Goodluck, Helen (1978).
Manual of information to accompany the Lancaster-Oslo/Bergen corpus of British English, for use with digital computers.
Technical report, Department of English, University of Oslo, Oslo.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BrownStats">BrownStats</a></code>
</p>

<hr>
<h2 id='PassiveBrownFam'>
By-text frequencies of passive verb phrases in the Brown Family corpora.
</h2><span id='topic+PassiveBrownFam'></span>

<h3>Description</h3>

<p>This data set specifies the number of passive and active verb phrases for each text in
the extended Brown Family of corpora (Brown, LOB, Frown, FLOB, BLOB), covering
edited written American and British English from 1930s, 1960s and 1990s (see Xiao 2008, 395&ndash;397).
</p>
<p>Verb phrase and passive/active aspect counts are based on a fully
automatic analysis of the texts, using the Pro3Gres parser (Schneider et al. 2004).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
PassiveBrownFam

</code></pre>


<h3>Format</h3>

<p>A data frame with 2499 rows and the following 11 columns:
</p>

<dl>
<dt><code>id</code>:</dt><dd><p>A unique ID for each text (also used as row name)</p>
</dd>
<dt><code>corpus</code>:</dt><dd><p>Corpus, a factor with five levels <code>BLOB</code>, <code>Brown</code>, <code>LOB</code>, <code>Frown</code>, <code>FLOB</code></p>
</dd>
<dt><code>section</code>:</dt><dd><p>Genre, a factor with fifteen levels <code>A</code>, ..., <code>R</code> (Brown section codes)</p>
</dd>
<dt><code>genre</code>:</dt><dd><p>Genre labels, a factor with fifteen levels (e.g. <code>press reportage</code>)</p>
</dd>
<dt><code>period</code>:</dt><dd><p>Date of publication, a factor with three levels (<code>1930</code>, <code>1960</code>, <code>1990</code>)</p>
</dd>
<dt><code>lang</code>:</dt><dd><p>Language variety / region, a factor with levels <code>AmE</code> (U.S.) and <code>BrE</code> (UK)</p>
</dd>
<dt><code>n.words</code>:</dt><dd><p>Number of word tokens, an integer vector</p>
</dd>
<dt><code>act</code>:</dt><dd><p>Number of active verb phrases, an integer vector</p>
</dd>
<dt><code>pass</code>:</dt><dd><p>Number of passive verb phrases, an integer vector</p>
</dd>
<dt><code>verbs</code>:</dt><dd><p>Total number of verb phrases, an integer vector</p>
</dd>
<dt><code>p.pass</code>:</dt><dd><p>Percentage of passive verb phrases in the text, a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p>No frequency data could be obtained for text <code>N02</code> in the Frown corpus.  This entry has been omitted from the table.
</p>


<h3>Acknowledgements</h3>

<p>Frequency information for this data set was kindly provided by Gerold Schneider, University of Zurich (<a href="http://www.cl.uzh.ch/de/people/team/compling/gschneid.html">http://www.cl.uzh.ch/de/people/team/compling/gschneid.html</a>).
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Schneider, Gerold; Rinaldi, Fabio; Dowdall, James (2004). Fast, deep-linguistic statistical dependency parsing. In G.-J. M. Kruijff and D. Duchier (eds.), <em>Proceedings of the COLING 2004 Workshop on Recent Advances in Dependency Grammar</em>, pages 33-40, Geneva, Switzerland. <a href="https://files.ifi.uzh.ch/cl/gschneid/parser/">https://files.ifi.uzh.ch/cl/gschneid/parser/</a>
</p>
<p>Xiao, Richard (2008). Well-known and influential corpora. In A. Lüdeling and M. Kytö (eds.), <em>Corpus Linguistics. An International Handbook</em>, chapter 20, pages 383&ndash;457. Mouton de Gruyter, Berlin.
</p>

<hr>
<h2 id='prop.cint'>Confidence interval for proportion based on frequency counts (corpora)</h2><span id='topic+prop.cint'></span>

<h3>Description</h3>

<p>This function computes a confidence interval for a population proportion from
the corresponding frequency count in a sample.  It either uses the Clopper-Pearson
method (inverted exact binomial test) or the Wilson score method (inversion of a
z-score test, with or without continuity correction).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
prop.cint(k, n, method = c("binomial", "z.score"), correct = TRUE, p.adjust=FALSE,
          conf.level = 0.95, alternative = c("two.sided", "less", "greater"))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop.cint_+3A_k">k</code></td>
<td>
<p>frequency of a type in the corpus (or an integer vector of
frequencies)</p>
</td></tr>
<tr><td><code id="prop.cint_+3A_n">n</code></td>
<td>
<p>number of tokens in the corpus, i.e. sample size (or an
integer vector specifying the sizes of different samples)</p>
</td></tr>
<tr><td><code id="prop.cint_+3A_method">method</code></td>
<td>
<p>a character string specifying whether to compute
a Clopper-Pearson confidence interval (<code>binomial</code>) or
a Wilson score interval (<code>z.score</code>)</p>
</td></tr>
<tr><td><code id="prop.cint_+3A_correct">correct</code></td>
<td>
<p>if <code>TRUE</code>, apply Yates' continuity correction for
the z-score test (default)</p>
</td></tr>
<tr><td><code id="prop.cint_+3A_p.adjust">p.adjust</code></td>
<td>
<p>if <code>TRUE</code>, apply a Bonferroni correction to ensure
a family-wise confidence level over all tests carried out in a single
function call (i.e. the length of <code>k</code>). Alternatively, the desired 
family size can be specified instead of <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="prop.cint_+3A_conf.level">conf.level</code></td>
<td>
<p>the desired confidence level (defaults to 95%)</p>
</td></tr>
<tr><td><code id="prop.cint_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis, yielding a two-sided (<code>two.sided</code>, default), lower
one-sided (<code>less</code>) or upper one-sided (<code>greater</code>)
confidence interval</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The confidence intervals computed by this function correspond to those
returned by <code><a href="stats.html#topic+binom.test">binom.test</a></code> and <code><a href="stats.html#topic+prop.test">prop.test</a></code>,
respectively.  However, <code>prop.cint</code> accepts vector arguments,
allowing many confidence intervals to be computed with a single
function call in a computationally efficient manner.
</p>
<p>The <b>Clopper-Pearson</b> confidence interval (<code>binomial</code>) is
obtained by inverting the exact binomial test at significance level
<code class="reqn">\alpha</code> = 1 - <code>confidence.level</code>.
In the two-sided case, the p-value of the test is computed using the
&ldquo;central&rdquo; method Fay (2010: 53), i.e. as twice the tail probability
of the matching tail. This corresponds to the algorithm originally proposed
by Clopper &amp; Pearson (1934).
</p>
<p>The limits of the confidence interval are computed in an efficient and
numerically robust manner via (the inverse of) the incomplete Beta function.
</p>
<p>The <b>Wilscon score</b> confidence interval (<code>z.score</code>) is computed
by solving the equation of the z-score test </p>
<p style="text-align: center;"><code class="reqn">%
    \frac{k - np}{\sqrt{n p (1-p)}} = A </code>
</p>

<p>for <code class="reqn">p</code>, where <code class="reqn">A</code> is the <code class="reqn">z</code>-value corresponding
to the chosen confidence level (e.g. <code class="reqn">\pm 1.96</code> for a
two-sided test with 95% confidence).  This leads to the quadratic
equation </p>
<p style="text-align: center;"><code class="reqn">%
    p^2 (n + A^2) + p (-2k - A^2) + \frac{k^2}{n} = 0 </code>
</p>

<p>whose two solutions correspond to the lower and upper boundary of
the confidence interval.
</p>
<p>When Yates' continuity correction is applied, the value <code class="reqn">k</code> in the
numerator of the <code class="reqn">z</code>-score equation has to be replaced by
<code class="reqn">k^*</code>, with <code class="reqn">k^* = k - 1/2</code> for the
<em>lower</em> boundary of the confidence interval (where <code class="reqn">k &gt; np</code>)
and <code class="reqn">k^* = k + 1/2</code> for the <em>upper</em> boundary of
the confidence interval (where <code class="reqn">k &lt; np</code>).  In each case, the
corresponding solution of the quadratic equation has to be chosen
(i.e., the solution with <code class="reqn">k &gt; np</code> for the lower boundary and vice
versa).
</p>
<p>If a <b>Bonferroni</b> correction is applied, the significance level <code class="reqn">\alpha</code>
of the underlying test is divided by the number <code class="reqn">m</code> of tests carried out
(specified explicitly by the user or given implicitly by <code>length(k)</code>):
<code class="reqn">\alpha' = \alpha / m</code>.
</p>


<h3>Value</h3>

<p>A data frame with two columns, labelled <code>lower</code> for the lower
boundary and <code>upper</code> for the upper boundary of the confidence
interval.  The number of rows is determined by the length of the
longest input vector (<code>k</code>, <code>n</code> and <code>conf.level</code>).
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Clopper, C. J. &amp; Pearson, E. S. (1934). The use of confidence or fiducial limits illustrated in the case of the binomial. <em>Biometrika</em>, <b>26</b>(4), 404-413.
</p>
<p>Fay, Michael P. (2010). Two-sided exact tests and matching confidence intervals for discrete data. <em>The R Journal</em>, <b>2</b>(1), 53-58.
</p>
<p><a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval">https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+z.score.pval">z.score.pval</a></code>, <code><a href="stats.html#topic+prop.test">prop.test</a></code>,
<code><a href="#topic+binom.pval">binom.pval</a></code>, <code><a href="stats.html#topic+binom.test">binom.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Clopper-Pearson confidence interval
binom.test(19, 100)
prop.cint(19, 100, method="binomial")

# Wilson score confidence interval
prop.test(19, 100)
prop.cint(19, 100, method="z.score")
</code></pre>

<hr>
<h2 id='qw'>
Split string into words, similar to qw() in Perl (corpora)
</h2><span id='topic+qw'></span>

<h3>Description</h3>

<p>This function splits one or more character strings into words. By default, 
the strings are split on whitespace in order to emulate Perl's <code>qw()</code> (quote words) functionality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qw(s, sep="\\s+", names=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qw_+3A_s">s</code></td>
<td>

<p>one or more strings to be split (a character vector)
</p>
</td></tr>
<tr><td><code id="qw_+3A_sep">sep</code></td>
<td>

<p>PCRE regular expression on which to split (defaults to whitespace)
</p>
</td></tr>
<tr><td><code id="qw_+3A_names">names</code></td>
<td>

<p>if TRUE, the resulting character vector is labelled with itself, which is convenient for <code><a href="base.html#topic+lapply">lapply</a></code> and similar functions
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector of the resulting words.  Multiple strings in <code>s</code> are flattened into a single vector.
</p>
<p>If <code>names=TRUE</code>, the words are used both as values and as labels of the character vectors, which is convenient when iterating over it with <code><a href="base.html#topic+lapply">lapply</a></code> or <code><a href="base.html#topic+sapply">sapply</a></code>.
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>Examples</h3>

<pre><code class='language-R'>qw(c("alpha beta gamma", "42 111" ))
qw("alpha beta gamma", names=TRUE)
qw("words with blanks,  sep by commas", sep="\\s*,\\s*")
</code></pre>

<hr>
<h2 id='rowColVector'>
Propagate vector to single-row or single-column matrix (corpora)
</h2><span id='topic+rowVector'></span><span id='topic+colVector'></span>

<h3>Description</h3>

<p>This utility function converts a plain vector into a row or column vector,
i.e. a single-row or single-column matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rowVector(x, label=NULL)
colVector(x, label=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rowColVector_+3A_x">x</code></td>
<td>

<p>a (typically numeric) vector
</p>
</td></tr>
<tr><td><code id="rowColVector_+3A_label">label</code></td>
<td>

<p>an optional character string specifying a label for the single row or column returned
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single-row or single-column matrix of the same data type as <code>x</code>.
Labels of <code>x</code> are preserved as column/row names of the matrix.
</p>
<p>See <code><a href="base.html#topic+matrix">matrix</a></code> for details on how non-atomic objects are handled.
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>Examples</h3>

<pre><code class='language-R'>rowVector(1:5, "myvec")
colVector(c(A=1, B=2, C=3), label="myvec")
</code></pre>

<hr>
<h2 id='sample.df'>Random samples from data frames (corpora)</h2><span id='topic+sample.df'></span>

<h3>Description</h3>

<p>This function takes a random sample of rows from a data frame,
in analogy to the built-in function <code>sample</code> (which sadly
does not accept a data frame).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
sample.df(df, size, replace=FALSE, sort=FALSE, prob=NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.df_+3A_df">df</code></td>
<td>
<p>a data frame to be sampled from</p>
</td></tr>
<tr><td><code id="sample.df_+3A_size">size</code></td>
<td>
<p>positive integer giving the number of rows to choose</p>
</td></tr>
<tr><td><code id="sample.df_+3A_replace">replace</code></td>
<td>
<p>Should sampling be with replacement?</p>
</td></tr>
<tr><td><code id="sample.df_+3A_sort">sort</code></td>
<td>
<p>Should rows in sample be sorted in original order?</p>
</td></tr>
<tr><td><code id="sample.df_+3A_prob">prob</code></td>
<td>
<p>a vector of probability weights for obtaining the elements of the vector being sampled</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internally, rows are selected with the function <code><a href="base.html#topic+sample.int">sample.int</a></code>.  See its manual page
for details on the arguments (except for <code>sort</code>) and implementation.
</p>


<h3>Value</h3>

<p>A data frame containing the sampled rows of <code>df</code>, either their original order (<code>sort=TRUE</code>)
or shuffled randomly (<code>sort=FALSE</code>).
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>Examples</h3>

<pre><code class='language-R'>sample.df(BrownLOBPassives, 20, sort=TRUE)
</code></pre>

<hr>
<h2 id='simulated.census'>Simulated census data for examples and illustrations (corpora)</h2><span id='topic+simulated.census'></span><span id='topic+FakeCensus'></span>

<h3>Description</h3>

<p>This function generates a large simulated census data frame with body
measurements (height, weight, shoe size) for male and female inhabitants
of a highly fictitious country.
</p>
<p>The generated data set is usually named <code>FakeCensus</code> (see code examples below)
and is used for various exercises and illustrations in the SIGIL course.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
simulated.census(N=502202, p.male=0.55, seed.rng=42)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulated.census_+3A_n">N</code></td>
<td>
<p>population size, i.e. number of inhabitants of the fictitious country</p>
</td></tr>
<tr><td><code id="simulated.census_+3A_p.male">p.male</code></td>
<td>
<p>proportion of males in the country</p>
</td></tr>
<tr><td><code id="simulated.census_+3A_seed.rng">seed.rng</code></td>
<td>
<p>seed for the random number generator, so data sets with the same parameters (<code>N</code>, <code>p.male</code>, etc.) are reproducible</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default population size corresponds to the estimated populace of Luxembourg
on 1 January 2010 (according to <a href="https://en.wikipedia.org/wiki/Luxembourg">https://en.wikipedia.org/wiki/Luxembourg</a>).
</p>
<p>Further parameters of the simulation (standard deviation, correlations, non-linearity)
will be exposed as function arguments in future releases.
</p>


<h3>Value</h3>

<p>A data frame with <code>N</code> rows corresponding to inhabitants and the following columns:
</p>

<dl>
<dt><code>height</code>:</dt><dd><p>body height in cm</p>
</dd>
<dt><code>height</code>:</dt><dd><p>body weight in kg</p>
</dd>
<dt><code>shoe.size</code>:</dt><dd><p>shoe size in Paris points (Continental European scale)</p>
</dd>
<dt><code>sex</code>:</dt><dd><p>sex, either <code>m</code> or <code>f</code></p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>Examples</h3>

<pre><code class='language-R'>
FakeCensus &lt;- simulated.census()
summary(FakeCensus)



</code></pre>

<hr>
<h2 id='simulated.language.course'>Simulated study on effectiveness of language course (corpora)</h2><span id='topic+simulated.language.course'></span><span id='topic+LanguageCourse'></span>

<h3>Description</h3>

<p>This function generates simulated results of a study measuring the effectiveness
of a new corpus-driven foreign language teaching course.
</p>
<p>The generated data set is usually named <code>LanguageCourse</code> (see code examples below)
and is used for various exercises and illustrations in the SIGIL course.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
simulated.language.course(n=c(15,20,10,10,14,18,15), mean=c(60,50,30,70,55,50,60),
                          effect=c(5,8,12,-4,2,6,-5), sd.subject=15, sd.effect=5,
                          seed.rng=42)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulated.language.course_+3A_n">n</code></td>
<td>
<p>number of participants in each class</p>
</td></tr>
<tr><td><code id="simulated.language.course_+3A_mean">mean</code></td>
<td>
<p>average score of each class before the course</p>
</td></tr>
<tr><td><code id="simulated.language.course_+3A_effect">effect</code></td>
<td>
<p>improvement of each class during the course</p>
</td></tr>
<tr><td><code id="simulated.language.course_+3A_sd.subject">sd.subject</code></td>
<td>
<p>inter-subject variability, may be different in each class</p>
</td></tr>
<tr><td><code id="simulated.language.course_+3A_sd.effect">sd.effect</code></td>
<td>
<p>inter-subject variability of effect size, may also be different in each class</p>
</td></tr>
<tr><td><code id="simulated.language.course_+3A_seed.rng">seed.rng</code></td>
<td>
<p>seed for the random number generator, so data sets with the same parameters are reproducible</p>
</td></tr>
</table>


<h3>Details</h3>

<p>TODO
</p>


<h3>Value</h3>

<p>A data frame with <code>sum(n)</code> rows corresponding to individual subjects participating in the study and the following columns
</p>

<dl>
<dt><code>id</code>:</dt><dd><p>unique ID code of subject</p>
</dd>
<dt><code>class</code>:</dt><dd><p>name of the teaching class</p>
</dd>
<dt><code>pre</code>:</dt><dd><p>score in standardized language test before the course (<em>pre-test</em>)</p>
</dd>
<dt><code>post</code>:</dt><dd><p>score in standardized language test after the course (<em>post-test</em>)</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>Examples</h3>

<pre><code class='language-R'>
LanguageCourse &lt;- simulated.language.course()
head(LanguageCourse, 20)
summary(LanguageCourse)

</code></pre>

<hr>
<h2 id='simulated.wikipedia'>Simulated type and token counts for Wikipedia articles (corpora)</h2><span id='topic+simulated.wikipedia'></span><span id='topic+WackypediaStats'></span>

<h3>Description</h3>

<p>This function generates type and token counts, token-type ratios (TTR) and
average word length for simulated articles from the English Wikipedia.
Simulation paramters are based on data from the Wackypedia corpus.
</p>
<p>The generated data set is usually named <code>WackypediaStats</code> (see code examples below)
and is used for various exercises and illustrations in the SIGIL course.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
simulated.wikipedia(N=1429649, length=c(100,1000), seed.rng=42)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulated.wikipedia_+3A_n">N</code></td>
<td>
<p>population size, i.e. total number of Wikipedia articles</p>
</td></tr>
<tr><td><code id="simulated.wikipedia_+3A_length">length</code></td>
<td>
<p>a numeric vector of length 2, specifying the typical range of Wikipedia article lengths</p>
</td></tr>
<tr><td><code id="simulated.wikipedia_+3A_seed.rng">seed.rng</code></td>
<td>
<p>seed for the random number generator, so data sets with the same parameters (<code>N</code> and <code>lenght</code>) are reproducible</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default population size corresponds to the subset of the Wackypedia corpus from which
the simulation parameters were obtained.  This excludes all articles with extreme type-token
statistics (very short, very long, extremely long words, etc.).
</p>
<p>Article lengths are sampled from a lognormal distribution which is scaled so that the
central 95% of the values fall into the range specified by the <code>length</code> argument.
</p>
<p>The simulated data are surprising close to the original Wackypedia statistics.
</p>


<h3>Value</h3>

<p>A data frame with <code>N</code> rows corresponding to Wikipedia articles and the following columns:
</p>

<dl>
<dt><code>tokens</code>:</dt><dd><p>number of word tokens in the article</p>
</dd>
<dt><code>types</code>:</dt><dd><p>number of distinct word types in the article</p>
</dd>
<dt><code>ttr</code>:</dt><dd><p>token-type ratio (TTR) for the article</p>
</dd>
<dt><code>avglen</code>:</dt><dd><p>average word length in characters (averaged across tokens)</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>The Wackypedia corpus can be obtained from <a href="https://wacky.sslmit.unibo.it/doku.php?id=corpora">https://wacky.sslmit.unibo.it/doku.php?id=corpora</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
WackypediaStats &lt;- simulated.wikipedia()
summary(WackypediaStats)



</code></pre>

<hr>
<h2 id='stars.pval'>
Show p-values as significance stars (corpora)
</h2><span id='topic+stars.pval'></span>

<h3>Description</h3>

<p>A simple utility function that converts p-values into the customary significance stars.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stars.pval(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stars.pval_+3A_x">x</code></td>
<td>

<p>a numeric vector of non-negative p-values
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector with significance stars corresponding to the p-values.
</p>
<p>Significance levels are <code>***</code> (<code class="reqn">p &lt; .001</code>), <code>**</code> (<code class="reqn">p &lt; .01</code>), <code>*</code> (<code class="reqn">p &lt; .05</code>) and <code>.</code> (<code class="reqn">p &lt; .1</code>). For non-significant p-values (<code class="reqn">p \ge .1</code>), an empty string is returned.
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>Examples</h3>

<pre><code class='language-R'>stars.pval(c(0, .007, .01, .04, .1))
</code></pre>

<hr>
<h2 id='VSS'>
A small corpus of very short stories with linguistic annotations
</h2><span id='topic+VSS'></span>

<h3>Description</h3>

<p>This data set contains a small corpus (8043 tokens) of short stories
from the collection <em>Very Short Stories</em> (VSS, see
<a href="http://www.schtepf.de/History/pages/stories.html">http://www.schtepf.de/History/pages/stories.html</a>).  The text was
automatically segmented (tokenised) and annotated with part-of-speech
tags (from the Penn tagset) and lemmas (base forms), using the IMS
TreeTagger (Schmid 1994) and a custom lemmatizer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VSS
</code></pre>


<h3>Format</h3>

<p>A data set with 8043 rows corresponding to tokens and the following columns:
</p>

<dl>
<dt><code>word</code>:</dt><dd><p>the word form (or surface form) of the token</p>
</dd>
<dt><code>pos</code>:</dt><dd><p>the part-of-speech tag of the token (Penn tagset)</p>
</dd>
<dt><code>lemma</code>:</dt><dd><p>the lemma (or base form) of the token</p>
</dd>
<dt><code>sentence</code>:</dt><dd><p>number of the sentence in which the token occurs (integer)</p>
</dd>
<dt><code>story</code>:</dt><dd><p>title of the story to which the token belongs (factor)</p>
</dd>
</dl>



<h3>Details</h3>

<p>The Penn tagset defines the following part-of-speech tags:
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>CC</code>  </td><td style="text-align: left;">  Coordinating conjunction                   </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>CD</code>  </td><td style="text-align: left;">  Cardinal number				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>DT</code>  </td><td style="text-align: left;">  Determiner				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>EX</code>  </td><td style="text-align: left;">  Existential <em>there</em>		 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>FW</code>  </td><td style="text-align: left;">  Foreign word				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>IN</code>  </td><td style="text-align: left;">  Preposition or subordinating conjunction	 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>JJ</code>  </td><td style="text-align: left;">  Adjective					 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>JJR</code> </td><td style="text-align: left;">  Adjective, comparative			 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>JJS</code> </td><td style="text-align: left;">  Adjective, superlative			 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>LS</code>  </td><td style="text-align: left;">  List item marker				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>MD</code>  </td><td style="text-align: left;">  Modal					 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>NN</code>  </td><td style="text-align: left;">  Noun, singular or mass			 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>NNS</code> </td><td style="text-align: left;">  Noun, plural				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>NP</code>  </td><td style="text-align: left;">  Proper noun, singular			 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>NPS</code> </td><td style="text-align: left;">  Proper noun, plural			 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>PDT</code> </td><td style="text-align: left;">  Predeterminer				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>POS</code> </td><td style="text-align: left;">  Possessive ending				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>PP</code>  </td><td style="text-align: left;">  Personal pronoun				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>PP$</code> </td><td style="text-align: left;">  Possessive pronoun			 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>RB</code>  </td><td style="text-align: left;">  Adverb					 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>RBR</code> </td><td style="text-align: left;">  Adverb, comparative			 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>RBS</code> </td><td style="text-align: left;">  Adverb, superlative			 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>RP</code>  </td><td style="text-align: left;">  Particle					 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>SYM</code> </td><td style="text-align: left;">  Symbol					 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>TO</code>  </td><td style="text-align: left;">  <em>to</em>				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>UH</code>  </td><td style="text-align: left;">  Interjection				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>VB</code>  </td><td style="text-align: left;">  Verb, base form				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>VBD</code> </td><td style="text-align: left;">  Verb, past tense				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>VBG</code> </td><td style="text-align: left;">  Verb, gerund or present participle	 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>VBN</code> </td><td style="text-align: left;">  Verb, past participle			 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>VBP</code> </td><td style="text-align: left;">  Verb, non-3rd person singular present	 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>VBZ</code> </td><td style="text-align: left;">  Verb, 3rd person singular present		 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>WDT</code> </td><td style="text-align: left;">  Wh-determiner				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>WP</code>  </td><td style="text-align: left;">  Wh-pronoun				 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>WP$</code> </td><td style="text-align: left;">  Possessive wh-pronoun			 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>WRB</code> </td><td style="text-align: left;">  Wh-adverb
 </td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Schmid, Helmut (1994). Probabilistic part-of-speech tagging using
decision trees. In: <em>Proceedings of the International Conference
on New Methods in Language Processing (NeMLaP)</em>, pages 44-49.
</p>

<hr>
<h2 id='z.score'>The z-score statistic for frequency counts (corpora)</h2><span id='topic+z.score'></span>

<h3>Description</h3>

<p>This function computes a z-score statistic for frequency counts, based
on a normal approximation to the correct binomial distribution under
the random sampling model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
z.score(k, n, p = 0.5, correct = TRUE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="z.score_+3A_k">k</code></td>
<td>
<p>frequency of a type in the corpus (or an integer vector of
frequencies)</p>
</td></tr>
<tr><td><code id="z.score_+3A_n">n</code></td>
<td>
<p>number of tokens in the corpus, i.e. sample size (or an
integer vector specifying the sizes of different samples)</p>
</td></tr>
<tr><td><code id="z.score_+3A_p">p</code></td>
<td>
<p>null hypothesis, giving the assumed proportion of this
type in the population (or a vector of proportions for different
types and/or different populations)</p>
</td></tr>
<tr><td><code id="z.score_+3A_correct">correct</code></td>
<td>
<p>if <code>TRUE</code>, apply Yates' continuity correction
(default)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">z</code> statistic is given by </p>
<p style="text-align: center;"><code class="reqn">%
    z := \dfrac{k - np}{\sqrt{n p (1-p)}} </code>
</p>
 
<p>When Yates' continuity correction is enabled, the <em>absolute
value</em> of the numerator <code class="reqn">d := k - np</code> is reduced by <code class="reqn">1/2</code>,
but clamped to a non-negative value.
</p>


<h3>Value</h3>

<p>The <code class="reqn">z</code>-score corresponding to the specified data (or a vector of
<code class="reqn">z</code>-scores).
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>See Also</h3>

<p><code><a href="#topic+z.score.pval">z.score.pval</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># z-test for H0: pi = 0.15 with observed counts 10..30 in a sample of n=100 tokens
k &lt;- c(10:30)
z &lt;- z.score(k, 100, p=.15)
names(z) &lt;- k
round(z, 3)

abs(z) &gt;= 1.96  # significant results at p &lt; .05
</code></pre>

<hr>
<h2 id='z.score.pval'>P-values of the z-score test for frequency counts (corpora)</h2><span id='topic+z.score.pval'></span>

<h3>Description</h3>

<p>This function computes the p-value of a z-score test for frequency
counts, based on the z-score statistic implemented by
<code><a href="#topic+z.score">z.score</a></code>.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
z.score.pval(k, n, p = 0.5, correct = TRUE,
             alternative = c("two.sided", "less", "greater"))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="z.score.pval_+3A_k">k</code></td>
<td>
<p>frequency of a type in the corpus (or an integer vector of
frequencies)</p>
</td></tr>
<tr><td><code id="z.score.pval_+3A_n">n</code></td>
<td>
<p>number of tokens in the corpus, i.e. sample size (or an
integer vector specifying the sizes of different samples)</p>
</td></tr>
<tr><td><code id="z.score.pval_+3A_p">p</code></td>
<td>
<p>null hypothesis, giving the assumed proportion of this
type in the population (or a vector of proportions for different
types and/or different populations)</p>
</td></tr>
<tr><td><code id="z.score.pval_+3A_correct">correct</code></td>
<td>
<p>if <code>TRUE</code>, apply Yates' continuity correction
(default)</p>
</td></tr>
<tr><td><code id="z.score.pval_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis; must be one of <code>two.sided</code> (default), <code>less</code>
or <code>greater</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The p-value of a <code class="reqn">z</code>-score test applied to the given data (or a vector
of p-values).
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>See Also</h3>

<p><code><a href="#topic+z.score">z.score</a></code>, <code><a href="#topic+binom.pval">binom.pval</a></code>, <code><a href="#topic+prop.cint">prop.cint</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># compare z-test for H0: pi = 0.15 against binomial test
# with observed counts 10..30 in a sample of n=100 tokens
k &lt;- c(10:30)
p.compare &lt;- rbind(
  z.score = z.score.pval(k, 100, p=.15),
  binomial = binom.pval(k, 100, p=.15))
colnames(p.compare) &lt;- k
round(p.compare, 4)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
