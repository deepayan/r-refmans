<!DOCTYPE html><html lang="en"><head><title>Help for package maxent.ot</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {maxent.ot}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#compare_models'><p>Compare Maxent OT models using a variety of methods</p></a></li>
<li><a href='#cross_validate'><p>Cross-validate bias parameters for constraint weights.</p></a></li>
<li><a href='#monte_carlo_weights'><p>Create simulated data and learn weights for these data</p></a></li>
<li><a href='#optimize_weights'><p>Optimize MaxEnt OT constraint weights</p></a></li>
<li><a href='#otsoft_bias_to_df'><p>Converts an OTSoft bias file to a data frame</p></a></li>
<li><a href='#otsoft_tableaux_to_df'><p>Converts an OTSoft tableaux file to a data frame</p></a></li>
<li><a href='#predict_probabilities'><p>Predict probabilities of OT candidates</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Perform Phonological Analyses using Maximum Entropy Optimality
Theory</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Fit Maximum Entropy Optimality Theory models to data sets,
  generate the predictions made by such models for novel data, and compare the 
  fit of different models using a variety of metrics. The package is described
  in Mayer, C., Tan, A., Zuraw, K. (in press) <a href="https://sites.socsci.uci.edu/~cjmayer/papers/cmayer_et_al_maxent_ot_accepted.pdf">https://sites.socsci.uci.edu/~cjmayer/papers/cmayer_et_al_maxent_ot_accepted.pdf</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/connormayer/maxent.ot">https://github.com/connormayer/maxent.ot</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/connormayer/maxent.ot/issues">https://github.com/connormayer/maxent.ot/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-20 16:15:22 UTC; conno</td>
</tr>
<tr>
<td>Author:</td>
<td>Connor Mayer [aut, cre],
  Kie Zuraw [aut],
  Adeline Tan [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Connor Mayer &lt;cjmayer@uci.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-23 13:10:04 UTC</td>
</tr>
</table>
<hr>
<h2 id='compare_models'>Compare Maxent OT models using a variety of methods</h2><span id='topic+compare_models'></span>

<h3>Description</h3>

<p>Compares two or more model fit to the same data set to determine which
provides the best fit, using a variety of methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_models(..., method = "lrt")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compare_models_+3A_...">...</code></td>
<td>
<p>Two or more models objects to be compared. These objects should
be in the same format as the objects returned by the <code>optimize_weights</code>
function. Note that the likelihood ratio test applies to exactly two
models, while the other comparison methods can be applied to arbitrarily
many models.</p>
</td></tr>
<tr><td><code id="compare_models_+3A_method">method</code></td>
<td>
<p>The method of comparison to use. This currently includes <code>lrt</code>
(likelihood ratio test), <code>aic</code> (Akaike Information Criterion), <code>aic_c</code>
(Akaike Information Criterion adjusted for small sample sizes), and <code>bic</code>
(Bayesian Information Criterion).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The available comparison methods are
</p>

<ul>
<li> <p><strong>lrt</strong>: The likelihood ratio test. This method can be applied to a
maximum of two models, and the parameters of these models (i.e., their
constraints) <em>must be in a strict subset/superset relationship.</em> If your
models do not meet these requirements, you should use a different method.
</p>
<p>The likelihood ratio is calculated as follows:
</p>
<p style="text-align: center;"><code class="reqn">LR = 2(LL_2 - LL_1)</code>
</p>

<p>where <code class="reqn">LL_2</code> is log likelihood of the model with more parameters. A
p-value is calculated by conducting a chi-squared test with <code class="reqn">X^2 = LR</code>
and the degrees of freedom set to the difference in number of parameters
between the two models. This p-value tells us whether the difference in
likelihood between the two models is significant (i.e., whether the extra
parameters in the full model are justified by the increase in model fit).
</p>
</li>
<li> <p><strong>aic</strong>: The Akaike Information Criterion. This is calculated as
follows for each model:
</p>
<p style="text-align: center;"><code class="reqn">AIC = 2k - 2LL</code>
</p>

<p>where <code class="reqn">k</code> is the number of model parameters (i.e., constraints) and LL
is the model's log likelihood.
</p>
</li>
<li> <p><strong>aic_c</strong>: The Akaike Information Criterion corrected for small sample
sizes. This is calculated as follows:
</p>
<p style="text-align: center;"><code class="reqn">AIC_c = 2k - 2LL + \frac{2k^2 + 2k}{n - k - 1}</code>
</p>

<p>where <code class="reqn">n</code> is the number of samples and the other parameters are
identical to those used in the AIC calculation. As <code class="reqn">n</code> approaches
infinity, the final term converges to 0, and so this equation becomes
equivalent to AIC. Please see the note below for information about sample
sizes.
</p>
</li>
<li> <p><strong>bic</strong>: The Bayesian Information Criterion. This is calculated as
follows:
</p>
<p style="text-align: center;"><code class="reqn">BIC = k\ln(n) - 2LL</code>
</p>

<p>As with <code>aic_c</code>, this calculation relies on the number of samples. Please
see the discussion on sample sizes below before using this method.
</p>
</li></ul>

<p>A few caveats for several of the comparison methods:
</p>

<ul>
<li><p> The likelihood ratio test (<code>lrt</code>) method applies to exactly two
models, and assumes that the parameters of these models are <em>nested</em>:
that is, the constraints in the smaller model are a strict subset of the
constraints in the larger model. This function will verify this to some
extent based on the number and names of constraints.
</p>
</li>
<li><p> The Akaike Information Criterion adjusted for small sample sizes
(<code>aic_c</code>) and the Bayesian Information Criterion (<code>bic</code>) rely on sample
sizes in their calculations. The sample size for a data set is defined as
the sum of the column of surface form frequencies. If you want to apply
these methods, it is important that the values in the column are token
counts, not relative frequencies. Applying these methods to relative
frequencies, which effectively ignore sample size, will produce invalid
results.
</p>
</li></ul>

<p>The <code>aic</code>, <code>aic_c</code>, and <code>bic</code> comparison methods return raw AIC/AICc/BIC
values as well as weights corresponding to these values. These weights
are calculated similarly for each model:
</p>
<p style="text-align: center;"><code class="reqn">W_i = \frac{\exp(-0.5 \delta_i)}{\sum_{j=1}^{m}{\exp(-0.5 \delta_j)}}</code>
</p>

<p>where <code class="reqn">\delta_i</code> is the difference in score (AIC, AICc, BIC) between
model <code class="reqn">i</code> and the model with the best score, and <code class="reqn">m</code> is the number of
models being compared. These weights provide the relative likelihood or
conditional probability of this model being the best model (by whatever
definition of &quot;best&quot; is assumed by the measurement type) given the data and
the selection of models it is being compared to.
</p>


<h3>Value</h3>

<p>A data frame containing information about the comparison. The
contents and size of this data frame vary depending on the method used.
</p>

<ul>
<li> <p><code>lrt</code>: A data frame with a single row and the following columns:
</p>

<ul>
<li> <p><code>description</code>: the names of the two models being compared.
The name of the model with more parameters will be first.
</p>
</li>
<li> <p><code>chi_sq</code>: the chi-squared value calculated during the test.
</p>
</li>
<li> <p><code>k_delta</code>: the difference in parameters between the two
models used as degrees of freedom in the chi-squared test.
</p>
</li>
<li> <p><code>p_value</code>: the p-value calculated by the test
</p>
</li></ul>

</li>
<li> <p><code>aic</code>: A data frame with as many rows as there were models
passed in. The models are sorted in ascending order of AIC (i.e., best
first). This data frame has the following columns:
</p>

<ul>
<li> <p><code>model</code>: The name of the model.
</p>
</li>
<li> <p><code>k</code>: The number of parameters.
</p>
</li>
<li> <p><code>aic</code>: The model's AIC value.
</p>
</li>
<li> <p><code>aic.delta</code>: The difference between this model's AIC value
and the AIC value of the model with the smallest AIC value.
</p>
</li>
<li> <p><code>aic.wt</code>: The model's AIC weight: this reflects the relative
likelihood (or conditional probability) that this model is the
&quot;best&quot; model in the set.
</p>
</li>
<li> <p><code>cum.wt</code>: The cumulative sum of AIC weights up to and
including this model.
</p>
</li>
<li> <p><code>ll</code>: The log likelihood of this model.
</p>
</li></ul>

</li>
<li> <p><code>aicc</code>: The data frame returned here is analogous to the
structure of the AIC data frame, with AICc values replacing AICs and
accordingly modified column names. There is one additional column:
</p>

<ul>
<li> <p><code>n</code>: The number of samples in the data the model is fit to.
</p>
</li></ul>

</li>
<li> <p><code>bic</code>: The data frame returned here is analogous to the
structure of the AIC and AICc data frames. Like the AICc data frame,
it contains the <code>n</code> column.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>  # Get paths to toy data files
  # This file has two constraints
  data_file_small &lt;- system.file(
      "extdata", "sample_data_frame.csv", package = "maxent.ot"
  )
  # This file has three constraints
  data_file_large &lt;- system.file(
      "extdata", "sample_data_frame_large.csv", package = "maxent.ot"
  )

  # Fit weights to both data sets with no biases
  tableaux_small &lt;- read.csv(data_file_small)
  small_model &lt;- optimize_weights(tableaux_small)

  tableaux_large &lt;- read.csv(data_file_large)
  large_model &lt;- optimize_weights(tableaux_large)

  # Compare models using likelihood ratio test. This is appropriate here
  # because the constraints are nested.
  compare_models(small_model, large_model, method='lrt')

  # Compare models using AIC
  compare_models(small_model, large_model, method='aic')

  # Compare models using AICc
  compare_models(small_model, large_model, method='aic_c')

  # Compare models using BIC
  compare_models(small_model, large_model, method='bic')

</code></pre>

<hr>
<h2 id='cross_validate'>Cross-validate bias parameters for constraint weights.</h2><span id='topic+cross_validate'></span>

<h3>Description</h3>

<p>Performs k-fold cross-validation of a data set and a set of input bias
parameters. Cross-validation allows the space of bias parameters to be
searched to find the settings that best support generalization to unseen data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cross_validate(
  input,
  k,
  mu_values,
  sigma_values,
  grid_search = FALSE,
  output_path = NA,
  out_sep = ",",
  control_params = NA,
  upper_bound = DEFAULT_UPPER_BOUND,
  encoding = "unknown",
  model_name = NA,
  allow_negative_weights = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cross_validate_+3A_input">input</code></td>
<td>
<p>The input data frame/data table/tibble. This should contain one
or more OT tableaux consisting of mappings between underlying and surface
forms with observed frequency and violation profiles. Constraint violations
must be numeric.
</p>
<p>For an example of the data frame format, see inst/extdata/sample_data_frame.csv.
You can read this file into a data frame using read.csv or into a tibble
using dplyr::read_csv.
</p>
<p>This function also supports the legacy OTSoft file format. You can use this
format by passing in a file path string to the OTSoft file rather than a
data frame.
</p>
<p>For examples of OTSoft format, see inst/extdata/sample_data_file.txt.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_k">k</code></td>
<td>
<p>The number of folds to use in cross-validation.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_mu_values">mu_values</code></td>
<td>
<p>A vector or list of mu bias parameters to use in
cross-validation. Parameters may either be scalars, in which case the
same mu parameter will be applied to every constraint, or vectors/lists
containing a separate mu bias parameter for each constraint.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_sigma_values">sigma_values</code></td>
<td>
<p>A vector or list of sigma bias parameters to use in
cross-validation. Parameters may either be scalars, in which case the
same sigma parameter will be applied to every constraint, or vectors/lists
containing a separate sigma bias parameter for each constraint.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_grid_search">grid_search</code></td>
<td>
<p>(optional) If TRUE, the Cartesian product of the values
in <code>mu_values</code> and <code>sigma_values</code> will be validated. For example, if
<code>mu_values = c(0, 1)</code> and <code>sigma_values = c(0.1, 1)</code>, cross-validation will
be done on the mu/sigma pairs <code style="white-space: pre;">&#8288;(0, 0.1), (0, 1), (1, 0.1), (1, 1)&#8288;</code>. If
FALSE (default), cross-validation will be done on each pair of values at
the same indices in <code>mu_values</code> and <code>sigma_values</code>. For example, if
<code>mu_values = c(0, 1)</code> and <code>sigma_values = c(0.1, 1)</code>, cross-validation will
be done on the mu/sigma pairs <code style="white-space: pre;">&#8288;(0, 0.1), (1, 1)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_output_path">output_path</code></td>
<td>
<p>(optional) A string specifying the path to a file to
which the cross-validation results will be saved. If the file exists it
will be overwritten. If this argument isn't provided, the output will not
be written to a file.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_out_sep">out_sep</code></td>
<td>
<p>(optional) The delimiter used in the output files.
Defaults to tabs.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_control_params">control_params</code></td>
<td>
<p>(optional) A named list of control parameters that
will be passed to the <a href="stats.html#topic+optim">optim</a> function. See the documentation
of that function for details. Note that some parameter settings may
interfere with optimization. The parameter <code>fnscale</code> will be overwritten
with <code>-1</code> if specified, since this must be treated as a maximization
problem.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_upper_bound">upper_bound</code></td>
<td>
<p>(optional) The maximum value for constraint weights.
Defaults to 100.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_encoding">encoding</code></td>
<td>
<p>(optional) The character encoding of the input file. Defaults
to &quot;unknown&quot;.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_model_name">model_name</code></td>
<td>
<p>(optional) A name for the model. If not provided, the file
name will be used if the input is a file path. If the input is a data frame
the name of the variable will be used.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_allow_negative_weights">allow_negative_weights</code></td>
<td>
<p>(optional) Whether the optimizer should allow
negative weights. Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cross-validation procedure is as follows:
</p>

<ol>
<li><p> Randomly divide the data into k partitions.
</p>
</li>
<li><p> Iterate through every combination of mu and sigma specified in the
input arguments (see the documentation for the <code>grid_search</code> argument
for details on how this is done).
</p>
</li>
<li><p> For each combination, for each of the k partitions, train a model
on the other (k-1) partitions using <code>optimize_weights</code> and then run
<code>predict_probabilities</code> on the remaining partition.
</p>
</li>
<li><p> Record the mean log likelihood the models apply to the held-out
partitions.
</p>
</li></ol>



<h3>Value</h3>

<p>A data frame with the following columns:
</p>

<ul>
<li> <p><code>model_name</code>: the name of the model
</p>
</li>
<li> <p><code>mu</code>: the value(s) of mu tested
</p>
</li>
<li> <p><code>sigma</code>: the value(s) of sigma tested
</p>
</li>
<li> <p><code>folds</code>: the number of folds
</p>
</li>
<li> <p><code>mean_ll</code>: the mean log likelihood of k-fold cross-validation
using these bias parameters
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>  # Get paths to OTSoft file. Note that you can also pass dataframes into
  # this function, as described in the documentation for `optimize`.
  data_file &lt;- system.file(
      "extdata", "amp_demo_grammar.csv", package = "maxent.ot"
  )
  tableaux_df &lt;- read.csv(data_file)

  # Define mu and sigma parameters to try
  mus &lt;- c(0, 1)
  sigmas &lt;- c(0.01, 0.1)

  # Do 2-fold cross-validation
  cross_validate(tableaux_df, 2, mus, sigmas)

  # Do 2-fold cross-validation with grid search of parameters
  cross_validate(tableaux_df, 2, mus, sigmas, grid_search=TRUE)

  # You can also use vectors/lists for some/all of the bias parameters to set
  # separate biases for each constraint
  mus_v &lt;- list(
    c(0, 1),
    c(1, 0)
  )
  sigmas_v &lt;- list(
    c(0.01, 0.1),
    c(0.1, 0.01)
  )

  cross_validate(tableaux_df, 2, mus_v, sigmas_v)

  # Save cross-validation results to a file
  tmp_output &lt;- tempfile()
  cross_validate(tableaux_df, 2, mus, sigmas, output_path=tmp_output)
</code></pre>

<hr>
<h2 id='monte_carlo_weights'>Create simulated data and learn weights for these data</h2><span id='topic+monte_carlo_weights'></span>

<h3>Description</h3>

<p>Creates a simulated data set by picking an output for each instance of an
input.
The probability of picking a particular output is guided by its conditional
probability given the input.
Learns constraint weights for each simulated data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>monte_carlo_weights(
  pred_prob,
  num_simul,
  bias_file = NA,
  mu = NA,
  sigma = NA,
  output_path = NA,
  out_sep = ",",
  control_params = NA,
  upper_bound = DEFAULT_UPPER_BOUND,
  allow_negative_weights = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="monte_carlo_weights_+3A_pred_prob">pred_prob</code></td>
<td>
<p>A data frame with a column for predicted probabilities.
This object should be in the same format as the <code>predictions</code> attribute of
the object returned by the <code>predict_probabilities</code> function.</p>
</td></tr>
<tr><td><code id="monte_carlo_weights_+3A_num_simul">num_simul</code></td>
<td>
<p>The number of simulations to run.</p>
</td></tr>
<tr><td><code id="monte_carlo_weights_+3A_bias_file">bias_file</code></td>
<td>
<p>(optional) The path to the file containing mus and sigma
for constraint biases. If this argument is provided, the scalar and vector
mu and sigma arguments will be ignored. Each row in this file should be
the name of the constraint, followed by the mu, followed by the sigma
(separated by whatever the relevant separator is; default is commas).</p>
</td></tr>
<tr><td><code id="monte_carlo_weights_+3A_mu">mu</code></td>
<td>
<p>(optional) A scalar or vector that will serve as the mu for each
constraint in the bias term. Constraint weights will also be initialized to
this value. If a vector, its length must equal the number of constraints in
the input file. This value will not be used if <code>bias_file</code> is provided.</p>
</td></tr>
<tr><td><code id="monte_carlo_weights_+3A_sigma">sigma</code></td>
<td>
<p>(optional) A scalar or vector that will serve as the sigma for
each constraint in the bias term. If a vector, its length must equal the
number of constraints in the input file. This value will not be used if
<code>bias_file</code> is provided.</p>
</td></tr>
<tr><td><code id="monte_carlo_weights_+3A_output_path">output_path</code></td>
<td>
<p>(optional) A string specifying the path to a file to
which the output will be saved. If the file exists it will be overwritten.
If this argument isn't provided, the output will not be written to a file.</p>
</td></tr>
<tr><td><code id="monte_carlo_weights_+3A_out_sep">out_sep</code></td>
<td>
<p>(optional) The delimiter used in the output files.
Defaults to commas.</p>
</td></tr>
<tr><td><code id="monte_carlo_weights_+3A_control_params">control_params</code></td>
<td>
<p>(optional) A named list of control parameters that
will be passed to the <a href="stats.html#topic+optim">optim</a> function. See the documentation
of that function for details. Note that some parameter settings may
interfere with optimization. The parameter <code>fnscale</code> will be overwritten
with <code>-1</code> if specified, since this must be treated as a maximization
problem.</p>
</td></tr>
<tr><td><code id="monte_carlo_weights_+3A_upper_bound">upper_bound</code></td>
<td>
<p>(optional) The maximum value for constraint weights.
Defaults to 100.</p>
</td></tr>
<tr><td><code id="monte_carlo_weights_+3A_allow_negative_weights">allow_negative_weights</code></td>
<td>
<p>(optional) Whether the optimizer should allow
negative weights. Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates multiple simulated data sets, and learns a set of
weights that maximizes the likelihood of data for each simulated data set.
</p>
<p>To create a simulated data set, one output is randomly chosen for each
instance of an input.
The probability of picking a particular output, <code class="reqn">O_i</code>, which arises from
input <code class="reqn">I_j</code> depends on <code class="reqn">Pr(O_i|I_j)</code>.
</p>
<p>The function <code>optimize_weights()</code> is called to find a set of weights that
maximize the likelihood of the simulated data.
All optional arguments of <code>optimize_weights()</code> that were available for the
user to specify biases and bounds are likewise available in this function,
<code>monte_carlo_weights()</code>.
</p>
<p>The process of simulating a data set and learning weights that optimize the
likelihood of the simulated data is repeated as per the number of specified
simulations.
</p>


<h3>Value</h3>

<p>A data frame with the following structure:
</p>

<ul>
<li><p> rows: As many rows as the number of simulations
</p>
</li>
<li><p> columns: As many columns as the number of constraints
</p>
</li></ul>



<h3>Why use this function?</h3>

<p>This function gives us a way to estimate constraint weights via a Monte
Carlo process.
For example we might be interested in the effect of temperature on
polarizing predicted probabilities, and the resulting constraint weights.
This function can produce a distribution of constraint weights for the
simulated polarized data, as well as a distribution of constraint weights
for the simulated non-polarized ones, thereby allowing a comparison of the
two.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # Get paths to toy data file
  data_file &lt;- system.file(
      "extdata", "sample_data_frame.csv", package = "maxent.ot"
  )

  tableaux_df &lt;- read.csv(data_file)

  # Fit weights to data with no biases
  fit_model &lt;- optimize_weights(tableaux_df)

  # Predict probabilities for the same input with temperature = 2
  pred_obj &lt;- predict_probabilities(
      tableaux_df, fit_model$weights, temperature = 2
  )

 # Run 5 monte carlo simulations
 # based on predicted probabilities when temperature = 2,
 # and learn weights for these 5 simulated data sets
 monte_carlo_weights(pred_obj$predictions, 5)

 # Save learned weights to a file
 tmp_output &lt;- tempfile()
 monte_carlo_weights(pred_obj$predictions, 5, output_path=tmp_output)
</code></pre>

<hr>
<h2 id='optimize_weights'>Optimize MaxEnt OT constraint weights</h2><span id='topic+optimize_weights'></span>

<h3>Description</h3>

<p>Optimizes constraint weights given a data set and optional biases. If no
bias arguments are provided, the bias term(s) will not be included in the
optimization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimize_weights(
  input,
  bias_input = NA,
  mu = NA,
  sigma = NA,
  control_params = NA,
  upper_bound = DEFAULT_UPPER_BOUND,
  encoding = "unknown",
  model_name = NA,
  allow_negative_weights = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optimize_weights_+3A_input">input</code></td>
<td>
<p>The input data frame/data table/tibble. This should contain one
or more OT tableaux consisting of mappings between underlying and surface
forms with observed frequency and violation profiles. Constraint violations
must be numeric.
</p>
<p>For an example of the data frame format, see inst/extdata/sample_data_frame.csv.
You can read this file into a data frame using read.csv or into a tibble
using dplyr::read_csv.
</p>
<p>This function also supports the legacy OTSoft file format. You can use this
format by passing in a file path string to the OTSoft file rather than a
data frame.
</p>
<p>For examples of OTSoft format, see inst/extdata/sample_data_file.txt.</p>
</td></tr>
<tr><td><code id="optimize_weights_+3A_bias_input">bias_input</code></td>
<td>
<p>(optional)
A data frame/data table/tibble containing the bias mus and sigmas. Each row
corresponds to an individual constraint, and consists of three columns:
<code>Constraint</code>, which contains the constraint name, <code>Mu</code>, which contains the
mu, and <code>Sigma</code>, which contains the sigma. If this argument is provided,
the mu and sigma arguments will be ignored.
Like the <code>input</code> argument, this function also supports the legacy OTSoft
file format for this argument. In this case, <code>bias_input</code> should be a path
to the bias parameters in OTSoft format.
</p>
<p>For examples of OTSoft bias format, see inst/extdata/sample_bias_file_otsoft.txt.
Each row in this file should be the name of the constraint, followed by the
mu, followed by the sigma (separated by tabs).</p>
</td></tr>
<tr><td><code id="optimize_weights_+3A_mu">mu</code></td>
<td>
<p>(optional) A scalar or vector that will serve as the mu for each
constraint in the bias term. Constraint weights will also be initialized to
this value. If a vector, its length must equal the number of constraints in
the input file. This value will not be used if <code>bias_file</code> is provided.</p>
</td></tr>
<tr><td><code id="optimize_weights_+3A_sigma">sigma</code></td>
<td>
<p>(optional) A scalar or vector that will serve as the sigma for
each constraint in the bias term. If a vector, its length must equal the
number of constraints in the input file. This value will not be used if
<code>bias_file</code> is provided.</p>
</td></tr>
<tr><td><code id="optimize_weights_+3A_control_params">control_params</code></td>
<td>
<p>(optional) A named list of control parameters that
will be passed to the <a href="stats.html#topic+optim">optim</a> function. See the documentation
of that function for details. Note that some parameter settings may
interfere with optimization. The parameter <code>fnscale</code> will be overwritten
with <code>-1</code> if specified, since this must be treated as a maximization
problem.</p>
</td></tr>
<tr><td><code id="optimize_weights_+3A_upper_bound">upper_bound</code></td>
<td>
<p>(optional) The maximum value for constraint weights.
Defaults to 100.</p>
</td></tr>
<tr><td><code id="optimize_weights_+3A_encoding">encoding</code></td>
<td>
<p>(optional) The character encoding of the input file. Defaults
to &quot;unknown&quot;.</p>
</td></tr>
<tr><td><code id="optimize_weights_+3A_model_name">model_name</code></td>
<td>
<p>(optional) A name for the model. If not provided, the name
of the variable will be used if the input is a data frame. If the input
is a path to an OTSoft file, the filename will be used.</p>
</td></tr>
<tr><td><code id="optimize_weights_+3A_allow_negative_weights">allow_negative_weights</code></td>
<td>
<p>(optional) Whether the optimizer should allow
negative weights. Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The objective function <code class="reqn">J(w)</code> that is optimized is defined as
</p>
<p style="text-align: center;"><code class="reqn">J(w) = \sum_{i=1}^{n}{\ln P(y_i|x_i; w)}
- \sum_{k=1}^{m}{\frac{(w_k - \mu_k)^2}{2\sigma_k^2}}</code>
</p>

<p>The first term in this equation calculates the natural logarithm of the
conditional likelihood of the training data under the weights <code class="reqn">w</code>. <code class="reqn">n</code>
is the number of data points (i.e., the sample size or the sum of the frequency
column in the input),<code class="reqn">x_i</code> is the input form of the <code class="reqn">i</code>th data
point, and <code class="reqn">y_i</code> is the observed surface form corresponding to
<code class="reqn">x_i</code>.<code class="reqn">P(y_i|x_i; w)</code> represents the probability of realizing
underlying <code class="reqn">x_i</code> as surface <code class="reqn">y_i</code> given weights <code class="reqn">w</code>. This
probability is defined as
</p>
<p style="text-align: center;"><code class="reqn">P(y_i|x_i; w) = \frac{1}{Z_w(x_i)}\exp(-\sum_{k=1}^{m}{w_k f_k(y_i, x_i)})</code>
</p>

<p>where <code class="reqn">f_k(y_i, x_i)</code> is the number of violations of constraint <code class="reqn">k</code>
incurred by mapping underlying <code class="reqn">x_i</code> to surface <code class="reqn">y_i</code>. <code class="reqn">Z_w(x_i)</code>
is a normalization term defined as
</p>
<p style="text-align: center;"><code class="reqn">Z(x_i) = \sum_{y\in\mathcal{Y}(x_i)}{\exp(-\sum_{k=1}^{m}{w_k f_k(y, x_i)})}</code>
</p>

<p>where <code class="reqn">\mathcal{Y}(x_i)</code> is the set of observed surface realizations of
input <code class="reqn">x_i</code>.
</p>
<p>The second term of the equation for calculating the objective function is
the optional bias term, where <code class="reqn">w_k</code> is the weight of constraint <code class="reqn">k</code>, and
<code class="reqn">\mu_k</code> and <code class="reqn">\sigma_k</code> parameterize a normal distribution that
serves as a prior for the value of <code class="reqn">w_k</code>. <code class="reqn">\mu_k</code> specifies the mean
of this distribution (the expected weight of constraint <code class="reqn">k</code> before
seeing any data) and <code class="reqn">sigma_k</code> reflects certainty in this value: lower
values of <code class="reqn">\sigma_k</code> penalize deviations from <code class="reqn">\mu_k</code> more severely,
and thus require greater amounts of data to move <code class="reqn">w_k</code> away from
<code class="reqn">mu_k</code>. While increasing <code class="reqn">\sigma_k</code> will improve the fit to the
training data, it may result in overfitting, particularly for small data
sets.
</p>
<p>A general bias with <code class="reqn">\mu_k = 0</code> for all <code class="reqn">k</code> is commonly used as a
form of simple regularization to prevent overfitting (see, e.g., Goldwater
and Johnson 2003). Bias terms have also been used to model proposed
phonological learning biases; see for example Wilson (2006), White (2013),
and Mayer (2021, Ch. 4). The choice of <code class="reqn">\sigma</code> depends on the sample
size. As the number of data points increases, <code class="reqn">\sigma</code> must decrease in
order for the effect of the bias to remain constant: specifically,
<code class="reqn">n\sigma^2</code> must be held constant, where <code class="reqn">n</code> is the number of tokens.
</p>
<p>Optimization is done using the <a href="stats.html#topic+optim">optim</a> function from the R-core
statistics library. By default it uses <code>L-BFGS-B</code> optimization, which is a
quasi-Newtonian method that allows upper and lower bounds on variables.
Constraint weights are restricted to finite, non-negative values.
</p>
<p>If no bias parameters are specified (either the <code>bias_file</code> argument or the
mu and sigma parameters), optimization will be done without the bias term.
</p>


<h3>Value</h3>

<p>An object with the following named attributes:
</p>

<ul>
<li> <p><code>weights</code>: A named list of the optimal constraint weights
</p>
</li>
<li> <p><code>log_lik</code>: the log likelihood of the data under the discovered
weights
</p>
</li>
<li> <p><code>k</code>: the number of constraints
</p>
</li>
<li> <p><code>n</code>: the number of data points in the training set
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>  # Get paths to toy data and bias files.
  df_file &lt;- system.file(
      "extdata", "sample_data_frame.csv", package = "maxent.ot"
  )
  bias_file &lt;- system.file(
       "extdata", "sample_bias_data_frame.csv", package = "maxent.ot"
  )
  # Fit weights to data with no biases
  tableaux_df &lt;- read.csv(df_file)
  optimize_weights(tableaux_df)

  # Fit weights with biases specified in file
  bias_df &lt;- read.csv(bias_file)
  optimize_weights(tableaux_df, bias_df)

  # Fit weights with biases specified in vector form
  optimize_weights(
      tableaux_df, mu = c(1, 2), sigma = c(100, 200)
  )

  # Fit weights with biases specified as scalars
  optimize_weights(tableaux_df, mu = 0, sigma = 1000)

  # Fit weights with mix of scalar and vector biases
  optimize_weights(tableaux_df, mu = c(1, 2), sigma = 1000)

  # Pass additional arguments to optim function
  optimize_weights(tableaux_df, control_params = list(maxit = 500))

</code></pre>

<hr>
<h2 id='otsoft_bias_to_df'>Converts an OTSoft bias file to a data frame</h2><span id='topic+otsoft_bias_to_df'></span>

<h3>Description</h3>

<p>Loads an OTSoft bias file and converts it to the data frame format used by
the maxent.ot functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otsoft_bias_to_df(input, output_path = NA)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="otsoft_bias_to_df_+3A_input">input</code></td>
<td>
<p>The path to the input bias file. This should contain more
OT tableaux consisting of mappings between underlying and surface forms with
observed frequency and violation profiles. Constraint violations must be
numeric.
</p>
<p>The file should be in OTSoft format. For examples of OTSoft format, see
inst/extdata/sample_bias_file_otsoft.txt.</p>
</td></tr>
<tr><td><code id="otsoft_bias_to_df_+3A_output_path">output_path</code></td>
<td>
<p>(optional) A string specifying the path to a file to
which the data frame will be saved in CSV format. If the file exists it
will be overwritten. If this argument isn't provided, the output will not
be written to a file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame corresponding to the input OTSoft bias file, containing
the columns
</p>

<ul>
<li> <p><code>Constraint</code>: The constraint name.
</p>
</li>
<li> <p><code>Mu</code>: The mu value for the regularization term.
</p>
</li>
<li> <p><code>Sigma</code>: The sigma value for the regularization term.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>  # Convert OTSoft bias file to data frame format
  otsoft_file &lt;- system.file(
      "extdata", "sample_bias_file_otsoft.txt", package = "maxent.ot"
  )
  df_output &lt;- otsoft_bias_to_df(otsoft_file)

  # Save data frame to a file
  tmp_output &lt;- tempfile()
  otsoft_bias_to_df(otsoft_file, tmp_output)
</code></pre>

<hr>
<h2 id='otsoft_tableaux_to_df'>Converts an OTSoft tableaux file to a data frame</h2><span id='topic+otsoft_tableaux_to_df'></span>

<h3>Description</h3>

<p>Loads an OTSoft tableaux file and converts it to the data frame format used by
the maxent.ot functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otsoft_tableaux_to_df(input, output_path = NA, encoding = "unknown")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="otsoft_tableaux_to_df_+3A_input">input</code></td>
<td>
<p>The path to the input data file.
This should contain more OT tableaux consisting of
mappings between underlying and surface forms with observed frequency and
violation profiles. Constraint violations must be numeric.
</p>
<p>The file should be in OTSoft format.
For examples of OTSoft format, see inst/extdata/sample_data_file.txt.</p>
</td></tr>
<tr><td><code id="otsoft_tableaux_to_df_+3A_output_path">output_path</code></td>
<td>
<p>(optional) A string specifying the path to a file to
which the data frame will be saved in CSV format. If the file exists it
will be overwritten. If this argument isn't provided, the output will not
be written to a file.</p>
</td></tr>
<tr><td><code id="otsoft_tableaux_to_df_+3A_encoding">encoding</code></td>
<td>
<p>(optional) The character encoding of the input file. Defaults
to &quot;unknown&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame corresponding to the input OTSoft tableau, containing
the columns
</p>

<ul>
<li> <p><code>Input</code>: The input form.
</p>
</li>
<li> <p><code>Output</code>: The output form.
</p>
</li>
<li> <p><code>Frequency</code>: The frequency of the input/output mapping.
</p>
</li>
<li><p> One column for each constraint containing its violation counts.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>  # Convert OTSoft file to data frame format
  otsoft_file &lt;- system.file(
      "extdata", "sample_data_file_otsoft.txt", package = "maxent.ot"
  )
  df_output &lt;- otsoft_tableaux_to_df(otsoft_file)

  # Save data frame to a file
  tmp_output &lt;- tempfile()
  otsoft_tableaux_to_df(otsoft_file, tmp_output)
</code></pre>

<hr>
<h2 id='predict_probabilities'>Predict probabilities of OT candidates</h2><span id='topic+predict_probabilities'></span>

<h3>Description</h3>

<p>Predict probabilities of candidates based on their violation profiles and
constraint weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_probabilities(
  test_input,
  constraint_weights,
  output_path = NA,
  out_sep = ",",
  encoding = "unknown",
  temperature = DEFAULT_TEMPERATURE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_probabilities_+3A_test_input">test_input</code></td>
<td>
<p>The input data frame/data table/tibble. This should contain one
or more OT tableaux consisting of mappings between underlying and surface
forms with observed frequency and violation profiles. Constraint violations
must be numeric.
</p>
<p>For an example of the data frame format, see inst/extdata/sample_data_frame.csv.
You can read this file into a data frame using read.csv or into a tibble
using dplyr::read_csv.
</p>
<p>This function also supports the legacy OTSoft file format. You can use this
format by passing in a file path string to the OTSoft file rather than a
data frame.
</p>
<p>For examples of OTSoft format, see inst/extdata/sample_data_file.txt.</p>
</td></tr>
<tr><td><code id="predict_probabilities_+3A_constraint_weights">constraint_weights</code></td>
<td>
<p>A vector of constraint weights to use. These are typically
generated by the <code><a href="#topic+optimize_weights">optimize_weights</a></code> function.</p>
</td></tr>
<tr><td><code id="predict_probabilities_+3A_output_path">output_path</code></td>
<td>
<p>(optional) A string specifying the path to a file to
which the predictions will be saved. If the file exists it will be overwritten.
If this argument isn't provided, the output will not be written to a file.</p>
</td></tr>
<tr><td><code id="predict_probabilities_+3A_out_sep">out_sep</code></td>
<td>
<p>(optional) The delimiter used in the output files.
Defaults to commas.</p>
</td></tr>
<tr><td><code id="predict_probabilities_+3A_encoding">encoding</code></td>
<td>
<p>(optional) The character encoding of the input file. Defaults
to &quot;unknown&quot;.</p>
</td></tr>
<tr><td><code id="predict_probabilities_+3A_temperature">temperature</code></td>
<td>
<p>(optional) The temperature parameter, which should be a
real number <code class="reqn">&gt;= 1</code>. Defaults to 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each input/output pair in the provided file this function
will calculate the probability of that output given the input form and the
provided weights. This probability is defined as
</p>
<p style="text-align: center;"><code class="reqn">P(y|x; w) = \frac{1}{Z_w(x)}\exp(-\sum_{k=1}^{m}{w_k f_k(y, x)})</code>
</p>

<p>where <code class="reqn">f_k(y, x)</code> is the number of violations of constraint <code class="reqn">k</code>
incurred by mapping underlying <code class="reqn">x</code> to surface <code class="reqn">y</code>, <code class="reqn">w_k</code> is the
weight associated with constraint <code class="reqn">k</code>, and  <code class="reqn">Z_w(x)</code> is a
normalization term defined as
</p>
<p style="text-align: center;"><code class="reqn">Z_w(x) = \sum_{y\in\mathcal{Y}(x)}{\exp(-\sum_{k=1}^{m}{w_k f_k(y, x)})}</code>
</p>

<p>where <code class="reqn">\mathcal{Y}(x)</code> is the set of all output candidates for input
<code class="reqn">x</code>.
</p>
<p>The resulting probabilities will be appended to a data frame object
representing the input tableaux. This data frame can also be saved to a file
if the <code>output_path</code> argument is provided.
</p>


<h3>Value</h3>

<p>An object with the following named attributes:
</p>

<ul>
<li> <p><code>log_lik</code>: the log likelihood of the data under the provided
weights
</p>
</li>
<li> <p><code>predictions</code>: A data table containing all the tableaux, with
probabilities assigned to each candidate and errors.
</p>
</li></ul>



<h3>Using temperature</h3>

<p>If the temperature parameter <code class="reqn">T</code> is specified, <code class="reqn">P(y|x; w)</code> is
calculated as
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{Z_w(x)}\exp(-\sum_{k=1}^{m}{(w_k f_k(y, x)})/T)</code>
</p>
<p> and
<code class="reqn">Z_w(x)</code> is similarly calculated as
</p>
<p style="text-align: center;"><code class="reqn">\sum_{y\in \mathcal{Y}(x)}{\exp(-\sum_{k=1}^{m}{(w_k f_k(y, x))/T})}</code>
</p>

<p>Larger values of <code class="reqn">T</code> move the predicted probabilities of output
candidates for a particular input towards equality with one another. For
example, if a particular input has two candidate outputs, higher values of
<code class="reqn">T</code> will move the probability of each towards <code>0.5</code>.
</p>
<p>The temperature parameter can be used to generate less categorical
predictions in a way that is independent of the constraint weights. See
Ackley, Hinton, and Sejnowski (1985, p. 150-152) for more detail, and Hayes
et al. (2009) and Mayer (2021, Ch. 4) for examples of temperature used in
practice. By default this parameter is set to <code>1</code>, which renders the
equations in this section equivalent to the standard calculations of
probability.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # Get paths to toy data file
  df_file &lt;- system.file(
      "extdata", "sample_data_frame.csv", package = "maxent.ot"
  )
  # Fit weights to dataframe with no biases
  tableaux_df &lt;- read.csv(df_file)
  fit_model &lt;- optimize_weights(tableaux_df)
  predict_probabilities(tableaux_df, fit_model$weights)

  # Do so with a temperature parameter
  predict_probabilities(tableaux_df, fit_model$weights, temperature = 2)

  # Save predictions to a file
  tmp_output &lt;- tempfile()
  predict_probabilities(tableaux_df, fit_model$weights, output_path=tmp_output)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
