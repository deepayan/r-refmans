<!DOCTYPE html><html><head><title>Help for package sparsediscrim</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparsediscrim}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#center_data'><p>Centers the observations in a matrix by their respective class sample means</p></a></li>
<li><a href='#cov_autocorrelation'><p>Generates a <code class="reqn">p \times p</code> autocorrelated covariance matrix</p></a></li>
<li><a href='#cov_block_autocorrelation'><p>Generates a <code class="reqn">p \times p</code> block-diagonal covariance matrix with</p>
autocorrelated blocks.</a></li>
<li><a href='#cov_eigen'><p>Computes the eigenvalue decomposition of the maximum likelihood estimators</p>
(MLE) of the covariance matrices for the given data matrix</a></li>
<li><a href='#cov_intraclass'><p>Generates a <code class="reqn">p \times p</code> intraclass covariance matrix</p></a></li>
<li><a href='#cov_list'><p>Computes the covariance-matrix maximum likelihood estimators for each class</p>
and returns a list.</a></li>
<li><a href='#cov_mle'><p>Computes the maximum likelihood estimator for the sample covariance matrix</p>
under the assumption of multivariate normality.</a></li>
<li><a href='#cov_pool'><p>Computes the pooled maximum likelihood estimator (MLE) for the common</p>
covariance matrix</a></li>
<li><a href='#cov_shrink_diag'><p>Computes a shrunken version of the maximum likelihood estimator for the</p>
sample covariance matrix under the assumption of multivariate normality.</a></li>
<li><a href='#cv_partition'><p>Randomly partitions data for cross-validation.</p></a></li>
<li><a href='#diag_estimates'><p>Computes estimates and ancillary information for diagonal classifiers</p></a></li>
<li><a href='#dmvnorm_diag'><p>Computes multivariate normal density with a diagonal covariance matrix</p></a></li>
<li><a href='#generate_blockdiag'><p>Generates data from <code>K</code> multivariate normal data populations, where each</p>
population (class) has a covariance matrix consisting of block-diagonal
autocorrelation matrices.</a></li>
<li><a href='#generate_intraclass'><p>Generates data from <code>K</code> multivariate normal data populations, where each</p>
population (class) has an intraclass covariance matrix.</a></li>
<li><a href='#h'><p>Bias correction function from Pang et al. (2009).</p></a></li>
<li><a href='#lda_diag'><p>Diagonal Linear Discriminant Analysis (DLDA)</p></a></li>
<li><a href='#lda_eigen'><p>The Minimum Distance Rule using Moore-Penrose Inverse (MDMP) classifier</p></a></li>
<li><a href='#lda_emp_bayes'><p>The Minimum Distance Empirical Bayesian Estimator (MDEB) classifier</p></a></li>
<li><a href='#lda_emp_bayes_eigen'><p>The Minimum Distance Rule using Modified Empirical Bayes (MDMEB) classifier</p></a></li>
<li><a href='#lda_pseudo'><p>Linear Discriminant Analysis (LDA) with the Moore-Penrose Pseudo-Inverse</p></a></li>
<li><a href='#lda_schafer'><p>Linear Discriminant Analysis using the Schafer-Strimmer Covariance Matrix</p>
Estimator</a></li>
<li><a href='#lda_shrink_cov'><p>Shrinkage-based Diagonal Linear Discriminant Analysis (SDLDA)</p></a></li>
<li><a href='#lda_shrink_mean'><p>Shrinkage-mean-based Diagonal Linear Discriminant Analysis (SmDLDA) from</p>
Tong, Chen, and Zhao (2012)</a></li>
<li><a href='#lda_thomaz'><p>Linear Discriminant Analysis using the Thomaz-Kitani-Gillies Covariance</p>
Matrix Estimator</a></li>
<li><a href='#log_determinant'><p>Computes the log determinant of a matrix.</p></a></li>
<li><a href='#no_intercept'><p>Removes the intercept term from a formula if it is included</p></a></li>
<li><a href='#plot.rda_high_dim_cv'><p>Plots a heatmap of cross-validation error grid for a HDRDA classifier object.</p></a></li>
<li><a href='#posterior_probs'><p>Computes posterior probabilities via Bayes Theorem under normality</p></a></li>
<li><a href='#print.lda_diag'><p>Outputs the summary for a DLDA classifier object.</p></a></li>
<li><a href='#print.lda_eigen'><p>Outputs the summary for a MDMP classifier object.</p></a></li>
<li><a href='#print.lda_emp_bayes'><p>Outputs the summary for a MDEB classifier object.</p></a></li>
<li><a href='#print.lda_emp_bayes_eigen'><p>Outputs the summary for a MDMEB classifier object.</p></a></li>
<li><a href='#print.lda_pseudo'><p>Outputs the summary for a lda_pseudo classifier object.</p></a></li>
<li><a href='#print.lda_schafer'><p>Outputs the summary for a lda_schafer classifier object.</p></a></li>
<li><a href='#print.lda_shrink_cov'><p>Outputs the summary for a SDLDA classifier object.</p></a></li>
<li><a href='#print.lda_shrink_mean'><p>Outputs the summary for a SmDLDA classifier object.</p></a></li>
<li><a href='#print.lda_thomaz'><p>Outputs the summary for a lda_thomaz classifier object.</p></a></li>
<li><a href='#print.qda_diag'><p>Outputs the summary for a DQDA classifier object.</p></a></li>
<li><a href='#print.qda_shrink_cov'><p>Outputs the summary for a SDQDA classifier object.</p></a></li>
<li><a href='#print.qda_shrink_mean'><p>Outputs the summary for a SmDQDA classifier object.</p></a></li>
<li><a href='#print.rda_high_dim'><p>Outputs the summary for a HDRDA classifier object.</p></a></li>
<li><a href='#qda_diag'><p>Diagonal Quadratic Discriminant Analysis (DQDA)</p></a></li>
<li><a href='#qda_shrink_cov'><p>Shrinkage-based Diagonal Quadratic Discriminant Analysis (SDQDA)</p></a></li>
<li><a href='#qda_shrink_mean'><p>Shrinkage-mean-based Diagonal Quadratic Discriminant Analysis (SmDQDA) from</p>
Tong, Chen, and Zhao (2012)</a></li>
<li><a href='#quadform'><p>Quadratic form of a matrix and a vector</p></a></li>
<li><a href='#quadform_inv'><p>Quadratic Form of the inverse of a matrix and a vector</p></a></li>
<li><a href='#rda_cov'><p>Calculates the RDA covariance-matrix estimators for each class</p></a></li>
<li><a href='#rda_high_dim'><p>High-Dimensional Regularized Discriminant Analysis (HDRDA)</p></a></li>
<li><a href='#rda_high_dim_cv'><p>Helper function to optimize the HDRDA classifier via cross-validation</p></a></li>
<li><a href='#rda_weights'><p>Computes the observation weights for each class for the HDRDA classifier</p></a></li>
<li><a href='#regdiscrim_estimates'><p>Computes estimates and ancillary information for regularized discriminant</p>
classifiers</a></li>
<li><a href='#risk_stein'><p>Stein Risk function from Pang et al. (2009).</p></a></li>
<li><a href='#solve_chol'><p>Computes the inverse of a symmetric, positive-definite matrix using the</p>
Cholesky decomposition</a></li>
<li><a href='#tong_mean_shrinkage'><p>Tong et al. (2012)'s Lindley-type Shrunken Mean Estimator</p></a></li>
<li><a href='#two_class_sim_data'><p>Example bivariate classification data from caret</p></a></li>
<li><a href='#update_rda_high_dim'><p>Helper function to update tuning parameters for the HDRDA classifier</p></a></li>
<li><a href='#var_shrinkage'><p>Shrinkage-based estimator of variances for each feature from Pang et al.</p>
(2009).</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Sparse and Regularized Discriminant Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.0</td>
</tr>
<tr>
<td>Description:</td>
<td>A collection of sparse and regularized discriminant analysis
    methods intended for small-sample, high-dimensional data sets. The package
    features the High-Dimensional Regularized Discriminant Analysis classifier
    from Ramey et al. (2017) &lt;<a href="https://arxiv.org/abs/1602.01182">arXiv:1602.01182</a>&gt;. Other classifiers include
    those from Dudoit et al. (2002) &lt;<a href="https://doi.org/10.1198%2F016214502753479248">doi:10.1198/016214502753479248</a>&gt;, Pang et
    al. (2009) &lt;<a href="https://doi.org/10.1111%2Fj.1541-0420.2009.01200.x">doi:10.1111/j.1541-0420.2009.01200.x</a>&gt;, and Tong et al. (2012)
    &lt;<a href="https://doi.org/10.1093%2Fbioinformatics%2Fbtr690">doi:10.1093/bioinformatics/btr690</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>bdsmatrix, corpcor, dplyr, ggplot2, mvtnorm, rlang</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, MASS, covr, modeldata, spelling</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/topepo/sparsediscrim">https://github.com/topepo/sparsediscrim</a>,
<a href="https://topepo.github.io/sparsediscrim/">https://topepo.github.io/sparsediscrim/</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1.9001</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-06-30 16:01:38 UTC; max</td>
</tr>
<tr>
<td>Author:</td>
<td>Max Kuhn <a href="https://orcid.org/0000-0003-2402-136X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  John Ramey [aut] (original author)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Max Kuhn &lt;mxkuhn@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-07-01 07:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='center_data'>Centers the observations in a matrix by their respective class sample means</h2><span id='topic+center_data'></span>

<h3>Description</h3>

<p>Centers the observations in a matrix by their respective class sample means
</p>


<h3>Usage</h3>

<pre><code class='language-R'>center_data(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="center_data_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="center_data_+3A_y">y</code></td>
<td>
<p>vector of class labels for each training observation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix with observations centered by its corresponding class sample
mean
</p>

<hr>
<h2 id='cov_autocorrelation'>Generates a <code class="reqn">p \times p</code> autocorrelated covariance matrix</h2><span id='topic+cov_autocorrelation'></span>

<h3>Description</h3>

<p>This function generates a <code class="reqn">p \times p</code> autocorrelated covariance matrix
with autocorrelation parameter <code>rho</code>. The variance <code>sigma2</code> is
constant for each feature and defaulted to 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_autocorrelation(p, rho, sigma2 = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_autocorrelation_+3A_p">p</code></td>
<td>
<p>the size of the covariance matrix</p>
</td></tr>
<tr><td><code id="cov_autocorrelation_+3A_rho">rho</code></td>
<td>
<p>the autocorrelation parameter. Must be less than 1 in absolute
value.</p>
</td></tr>
<tr><td><code id="cov_autocorrelation_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance of each feature</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The autocorrelated covariance matrix is defined as:
The <code class="reqn">(i,j)</code>th entry of the autocorrelated covariance matrix is defined as:
<code class="reqn">\rho^{|i - j|}</code>.
</p>
<p>The value of <code>rho</code> must be such that <code class="reqn">|\rho| &lt; 1</code> to ensure that
the covariance matrix is positive definite.
</p>


<h3>Value</h3>

<p>autocorrelated covariance matrix
</p>

<hr>
<h2 id='cov_block_autocorrelation'>Generates a <code class="reqn">p \times p</code> block-diagonal covariance matrix with
autocorrelated blocks.</h2><span id='topic+cov_block_autocorrelation'></span>

<h3>Description</h3>

<p>This function generates a <code class="reqn">p \times p</code> covariance matrix with
autocorrelated blocks. The autocorrelation parameter is <code>rho</code>.
There are <code>num_blocks</code> blocks each with size, <code>block_size</code>.
The variance, <code>sigma2</code>, is constant for each feature and defaulted to 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_block_autocorrelation(num_blocks, block_size, rho, sigma2 = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_block_autocorrelation_+3A_num_blocks">num_blocks</code></td>
<td>
<p>the number of blocks in the covariance matrix</p>
</td></tr>
<tr><td><code id="cov_block_autocorrelation_+3A_block_size">block_size</code></td>
<td>
<p>the size of each square block within the covariance matrix</p>
</td></tr>
<tr><td><code id="cov_block_autocorrelation_+3A_rho">rho</code></td>
<td>
<p>the autocorrelation parameter. Must be less than 1 in absolute
value.</p>
</td></tr>
<tr><td><code id="cov_block_autocorrelation_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance of each feature</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The autocorrelated covariance matrix is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\Sigma = \Sigma^{(\rho)} \oplus \Sigma^{(-\rho)} \oplus \ldots \oplus
\Sigma^{(\rho)},</code>
</p>
<p> where <code class="reqn">\oplus</code> denotes the direct sum and the
<code class="reqn">(i,j)</code>th entry of <code class="reqn">\Sigma^{(\rho)}</code> is </p>
<p style="text-align: center;"><code class="reqn">\Sigma_{ij}^{(\rho)} =
\{ \rho^{|i - j|} \}.</code>
</p>

<p>The matrix <code class="reqn">\Sigma^{(\rho)}</code> is the autocorrelated block discussed above.
</p>
<p>The value of <code>rho</code> must be such that <code class="reqn">|\rho| &lt; 1</code> to ensure that
the covariance matrix is positive definite.
</p>
<p>The size of the resulting matrix is <code class="reqn">p \times p</code>, where
<code>p = num_blocks * block_size</code>.
</p>


<h3>Value</h3>

<p>autocorrelated covariance matrix
</p>

<hr>
<h2 id='cov_eigen'>Computes the eigenvalue decomposition of the maximum likelihood estimators
(MLE) of the covariance matrices for the given data matrix</h2><span id='topic+cov_eigen'></span>

<h3>Description</h3>

<p>For the classes given in the vector <code>y</code>, we compute the eigenvalue
(spectral) decomposition of the class sample covariance matrices (MLEs) using
the data matrix <code>x</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_eigen(x, y, pool = FALSE, fast = FALSE, tol = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_eigen_+3A_x">x</code></td>
<td>
<p>data matrix with <code>n</code> observations and <code>p</code> feature vectors</p>
</td></tr>
<tr><td><code id="cov_eigen_+3A_y">y</code></td>
<td>
<p>class labels for observations (rows) in <code>x</code></p>
</td></tr>
<tr><td><code id="cov_eigen_+3A_pool">pool</code></td>
<td>
<p>logical. Should the sample covariance matrices be pooled?</p>
</td></tr>
<tr><td><code id="cov_eigen_+3A_fast">fast</code></td>
<td>
<p>logical. Should the Fast SVD be used? See details.</p>
</td></tr>
<tr><td><code id="cov_eigen_+3A_tol">tol</code></td>
<td>
<p>tolerance value below which the singular values of <code>x</code> are
considered zero.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the <code>fast</code> argument is selected, we utilize the so-called Fast
Singular Value Decomposition (SVD) to quickly compute the eigenvalue
decomposition. To compute the Fast SVD, we use the <code><a href="corpcor.html#topic+fast.svd">corpcor::fast.svd()</a></code>
function, which employs a well-known trick for tall data (large <code>n</code>,
small <code>p</code>) and wide data (large <code>p</code>, small <code>n</code>) to compute the
SVD corresponding to the nonzero singular values. For more information about
the Fast SVD, see <code><a href="corpcor.html#topic+fast.svd">corpcor::fast.svd()</a></code>.
</p>


<h3>Value</h3>

<p>a list containing the eigendecomposition for each class. If
<code>pool = TRUE</code>, then a single list is returned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cov_eigen(x = iris[, -5], y = iris[, 5])
cov_eigen(x = iris[, -5], y = iris[, 5], pool = TRUE)
cov_eigen(x = iris[, -5], y = iris[, 5], pool = TRUE, fast = TRUE)

# Generates a data set having fewer observations than features.
# We apply the Fast SVD to compute the eigendecomposition corresponding to the
# nonzero eigenvalues of the covariance matrices.
set.seed(42)
n &lt;- 5
p &lt;- 20
num_classes &lt;- 3
x &lt;- lapply(seq_len(num_classes), function(k) {
  replicate(p, rnorm(n, mean = k))
})
x &lt;- do.call(rbind, x)
colnames(x) &lt;- paste0("x", 1:ncol(x))
y &lt;- gl(num_classes, n)
cov_eigen(x = x, y = y, fast = TRUE)
cov_eigen(x = x, y = y, pool = TRUE, fast = TRUE)
</code></pre>

<hr>
<h2 id='cov_intraclass'>Generates a <code class="reqn">p \times p</code> intraclass covariance matrix</h2><span id='topic+cov_intraclass'></span>

<h3>Description</h3>

<p>This function generates a <code class="reqn">p \times p</code> intraclass covariance matrix with
correlation <code>rho</code>. The variance <code>sigma2</code> is constant for each
feature and defaulted to 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_intraclass(p, rho, sigma2 = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_intraclass_+3A_p">p</code></td>
<td>
<p>the size of the covariance matrix</p>
</td></tr>
<tr><td><code id="cov_intraclass_+3A_rho">rho</code></td>
<td>
<p>the value of the off-diagonal elements</p>
</td></tr>
<tr><td><code id="cov_intraclass_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance of each feature</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The intraclass covariance matrix is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\sigma^2 * (\rho * J_p + (1 - \rho) * I_p),</code>
</p>

<p>where <code class="reqn">J_p</code> is the <code class="reqn">p \times p</code> matrix of ones and <code class="reqn">I_p</code> is the
<code class="reqn">p \times p</code> identity matrix.
</p>
<p>By default, with <code>sigma2 = 1</code>, the diagonal elements of the intraclass
covariance matrix are all 1, while the off-diagonal elements of the matrix
are all <code>rho</code>.
</p>
<p>The value of <code>rho</code> must be between <code class="reqn">1 / (1 - p)</code> and 1,
exclusively, to ensure that the covariance matrix is positive definite.
</p>


<h3>Value</h3>

<p>intraclass covariance matrix
</p>

<hr>
<h2 id='cov_list'>Computes the covariance-matrix maximum likelihood estimators for each class
and returns a list.</h2><span id='topic+cov_list'></span>

<h3>Description</h3>

<p>For a sample matrix, <code>x</code>, we compute the MLE for the covariance matrix
for each class given in the vector, <code>y</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_list(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_list_+3A_x">x</code></td>
<td>
<p>data matrix with <code>n</code> observations and <code>p</code> feature vectors</p>
</td></tr>
<tr><td><code id="cov_list_+3A_y">y</code></td>
<td>
<p>class labels for observations (rows) in <code>x</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of the sample covariance matrices of size <code class="reqn">p \times p</code> for
each class given in <code>y</code>.
</p>

<hr>
<h2 id='cov_mle'>Computes the maximum likelihood estimator for the sample covariance matrix
under the assumption of multivariate normality.</h2><span id='topic+cov_mle'></span>

<h3>Description</h3>

<p>For a sample matrix, <code>x</code>, we compute the sample covariance matrix of the
data as the maximum likelihood estimator (MLE) of the population covariance
matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_mle(x, diag = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_mle_+3A_x">x</code></td>
<td>
<p>data matrix with <code>n</code> observations and <code>p</code> feature vectors</p>
</td></tr>
<tr><td><code id="cov_mle_+3A_diag">diag</code></td>
<td>
<p>logical value. If TRUE, assumes the population covariance matrix
is diagonal. By default, we assume that <code>diag</code> is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the <code>diag</code> option is set to <code>TRUE</code>, then we assume the population
covariance matrix is diagonal, and the MLE is computed under this assumption.
In this case, we return a vector of length <code>p</code> instead.
</p>


<h3>Value</h3>

<p>sample covariance matrix of size <code class="reqn">p \times p</code>. If <code>diag</code> is
<code>TRUE</code>, then a vector of length <code>p</code> is returned instead.
</p>

<hr>
<h2 id='cov_pool'>Computes the pooled maximum likelihood estimator (MLE) for the common
covariance matrix</h2><span id='topic+cov_pool'></span>

<h3>Description</h3>

<p>For the matrix <code>x</code>, we compute the MLE for the population covariance
matrix under the assumption that the data are sampled from <code class="reqn">K</code>
multivariate normal populations having equal covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_pool(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_pool_+3A_x">x</code></td>
<td>
<p>data matrix with <code>n</code> observations and <code>p</code> feature vectors</p>
</td></tr>
<tr><td><code id="cov_pool_+3A_y">y</code></td>
<td>
<p>class labels for observations (rows) in <code>x</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>pooled sample covariance matrix of size <code class="reqn">p \times p</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cov_pool(iris[, -5], iris$Species)
</code></pre>

<hr>
<h2 id='cov_shrink_diag'>Computes a shrunken version of the maximum likelihood estimator for the
sample covariance matrix under the assumption of multivariate normality.</h2><span id='topic+cov_shrink_diag'></span>

<h3>Description</h3>

<p>For a sample matrix, <code>x</code>, we compute the sample covariance matrix as the
maximum likelihood estimator (MLE) of the population covariance matrix and
shrink it towards its diagonal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_shrink_diag(x, gamma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_shrink_diag_+3A_x">x</code></td>
<td>
<p>data matrix with <code>n</code> observations and <code>p</code> feature vectors</p>
</td></tr>
<tr><td><code id="cov_shrink_diag_+3A_gamma">gamma</code></td>
<td>
<p>the shrinkage parameter. Must be between 0 and 1, inclusively.
By default, the shrinkage parameter is 1, which simply yields the MLE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">\widehat{\Sigma}</code> be the MLE of the covariance matrix <code class="reqn">\Sigma</code>.
Then, we shrink the MLE towards its diagonal by computing
</p>
<p style="text-align: center;"><code class="reqn">\widehat{\Sigma}(\gamma) = \gamma \widehat{\Sigma} + (1 - \gamma)
\widehat{\Sigma} \circ I_p,</code>
</p>
<p> where <code class="reqn">\circ</code> denotes the Hadamard product
and <code class="reqn">\gamma \in [0,1]</code>.
</p>
<p>For <code class="reqn">\gamma &lt; 1</code>, the resulting shrunken covariance matrix estimator is
positive definite, and for <code class="reqn">\gamma = 1</code>, we simply have the MLE, which can
potentially be positive semidefinite (singular).
</p>
<p>The estimator given here is based on Section 18.3.1 of the Hastie et
al. (2008) text.
</p>


<h3>Value</h3>

<p>shrunken sample covariance matrix of size <code class="reqn">p \times p</code>
</p>


<h3>References</h3>

<p>Hastie, T., Tibshirani, R., and Friedman, J. (2008), &quot;The
Elements of Statistical Learning: Data Mining, Inference, and Prediction,&quot;
2nd edition. <a href="http://web.stanford.edu/~hastie/ElemStatLearn/">http://web.stanford.edu/~hastie/ElemStatLearn/</a>
</p>

<hr>
<h2 id='cv_partition'>Randomly partitions data for cross-validation.</h2><span id='topic+cv_partition'></span>

<h3>Description</h3>

<p>For a vector of training labels, we return a list of cross-validation folds,
where each fold has the indices of the observations to leave out in the fold.
In terms of classification error rate estimation, one can think of a fold as a
the observations to hold out as a test sample set. Either the <code>hold_out</code>
size or the number of folds, <code>num_folds</code>, can be specified. The number
of folds defaults to 10, but if the <code>hold_out</code> size is specified, then
<code>num_folds</code> is ignored.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_partition(y, num_folds = 10, hold_out = NULL, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_partition_+3A_y">y</code></td>
<td>
<p>a vector of class labels</p>
</td></tr>
<tr><td><code id="cv_partition_+3A_num_folds">num_folds</code></td>
<td>
<p>the number of cross-validation folds. Ignored if
<code>hold_out</code> is not <code>NULL</code>. See Details.</p>
</td></tr>
<tr><td><code id="cv_partition_+3A_hold_out">hold_out</code></td>
<td>
<p>the hold-out size for cross-validation. See Details.</p>
</td></tr>
<tr><td><code id="cv_partition_+3A_seed">seed</code></td>
<td>
<p>optional random number seed for splitting the data for cross-validation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We partition the vector <code>y</code> based on its length, which we treat as the
sample size, 'n'. If an object other than a vector is used in <code>y</code>, its
length can yield unexpected results. For example, the output of
<code>length(diag(3))</code> is 9.
</p>


<h3>Value</h3>

<p>list the indices of the training and test observations for each fold.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The following three calls to `cv_partition` yield the same partitions.
set.seed(42)
cv_partition(iris$Species)
cv_partition(iris$Species, num_folds = 10, seed = 42)
cv_partition(iris$Species, hold_out = 15, seed = 42)
</code></pre>

<hr>
<h2 id='diag_estimates'>Computes estimates and ancillary information for diagonal classifiers</h2><span id='topic+diag_estimates'></span>

<h3>Description</h3>

<p>Computes the maximum likelihood estimators (MLEs) for each class under the
assumption of multivariate normality for each class. Also, computes ancillary
information necessary for classifier summary, such as sample size, the number
of features, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diag_estimates(x, y, prior = NULL, pool = FALSE, est_mean = c("mle", "tong"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diag_estimates_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="diag_estimates_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="diag_estimates_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="diag_estimates_+3A_pool">pool</code></td>
<td>
<p>logical value. If TRUE, calculates the pooled sample variances
for each class.</p>
</td></tr>
<tr><td><code id="diag_estimates_+3A_est_mean">est_mean</code></td>
<td>
<p>the estimator for the class means. By default, we use the
maximum likelihood estimator (MLE). To improve the estimation, we provide the
option to use a shrunken mean estimator proposed by Tong et al. (2012).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the common estimates and ancillary information used in
all of the diagonal classifiers in the <code>sparsediscrim</code> package.
</p>
<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations. If other data have zero variances, these will be removed with
a warning.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p>named list with estimators for each class and necessary ancillary
information
</p>


<h3>References</h3>

<p>Tong, T., Chen, L., and Zhao, H. (2012), &quot;Improved Mean
Estimation and Its Application to Diagonal Discriminant Analysis,&quot;
Bioinformatics, 28, 4, 531-537.
<a href="https://academic.oup.com/bioinformatics/article/28/4/531/211887">https://academic.oup.com/bioinformatics/article/28/4/531/211887</a>
</p>

<hr>
<h2 id='dmvnorm_diag'>Computes multivariate normal density with a diagonal covariance matrix</h2><span id='topic+dmvnorm_diag'></span>

<h3>Description</h3>

<p>Alternative to <code>mvtnorm::dmvnorm</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmvnorm_diag(x, mean, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dmvnorm_diag_+3A_x">x</code></td>
<td>
<p>matrix</p>
</td></tr>
<tr><td><code id="dmvnorm_diag_+3A_mean">mean</code></td>
<td>
<p>vector of means</p>
</td></tr>
<tr><td><code id="dmvnorm_diag_+3A_sigma">sigma</code></td>
<td>
<p>vector containing diagonal covariance matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>multivariate normal density
</p>

<hr>
<h2 id='generate_blockdiag'>Generates data from <code>K</code> multivariate normal data populations, where each
population (class) has a covariance matrix consisting of block-diagonal
autocorrelation matrices.</h2><span id='topic+generate_blockdiag'></span>

<h3>Description</h3>

<p>This function generates <code>K</code> multivariate normal data sets, where each
class is generated with a constant mean vector and a covariance matrix
consisting of block-diagonal autocorrelation matrices. The data are returned
as a single matrix <code>x</code> along with a vector of class labels <code>y</code> that
indicates class membership.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_blockdiag(n, mu, num_blocks, block_size, rho, sigma2 = rep(1, K))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_blockdiag_+3A_n">n</code></td>
<td>
<p>vector of the sample sizes of each class. The length of <code>n</code>
determines the number of classes <code>K</code>.</p>
</td></tr>
<tr><td><code id="generate_blockdiag_+3A_mu">mu</code></td>
<td>
<p>matrix containing the mean vectors for each class. Expected to have
<code>p</code> rows and <code>K</code> columns.</p>
</td></tr>
<tr><td><code id="generate_blockdiag_+3A_num_blocks">num_blocks</code></td>
<td>
<p>the number of block matrices. See details.</p>
</td></tr>
<tr><td><code id="generate_blockdiag_+3A_block_size">block_size</code></td>
<td>
<p>the dimensions of the square block matrix. See details.</p>
</td></tr>
<tr><td><code id="generate_blockdiag_+3A_rho">rho</code></td>
<td>
<p>vector of the values of the autocorrelation parameter for each
class covariance matrix. Must equal the length of <code>n</code> (i.e., equal to
<code>K</code>).</p>
</td></tr>
<tr><td><code id="generate_blockdiag_+3A_sigma2">sigma2</code></td>
<td>
<p>vector of the variance coefficients for each class covariance
matrix. Must equal the length of <code>n</code> (i.e., equal to <code>K</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For simplicity, we assume that a class mean vector is constant for each
feature. That is, we assume that the mean vector of the <code class="reqn">k</code>th class is
<code class="reqn">c_k * j_p</code>, where <code class="reqn">j_p</code> is a <code class="reqn">p \times 1</code> vector of ones and
<code class="reqn">c_k</code> is a real scalar.
</p>
<p>The <code class="reqn">k</code>th class covariance matrix is defined as
</p>
<p style="text-align: center;"><code class="reqn">\Sigma_k = \Sigma^{(\rho)} \oplus \Sigma^{(-\rho)} \oplus \ldots
\oplus \Sigma^{(\rho)},</code>
</p>
<p> where <code class="reqn">\oplus</code> denotes the direct sum and the
<code class="reqn">(i,j)</code>th entry of <code class="reqn">\Sigma^{(\rho)}</code> is
</p>
<p style="text-align: center;"><code class="reqn">\Sigma_{ij}^{(\rho)} = \{ \rho^{|i - j|} \}.</code>
</p>

<p>The matrix <code class="reqn">\Sigma^{(\rho)}</code> is referred to as a block. Its dimensions
are provided in the <code>block_size</code> argument, and the number of blocks are
specified in the <code>num_blocks</code> argument.
</p>
<p>Each matrix <code class="reqn">\Sigma_k</code> is generated by the
<code><a href="#topic+cov_block_autocorrelation">cov_block_autocorrelation()</a></code> function.
</p>
<p>The number of classes <code>K</code> is determined with lazy evaluation as the
length of <code>n</code>.
</p>
<p>The number of features <code>p</code> is computed as <code>block_size * num_blocks</code>.
</p>


<h3>Value</h3>

<p>named list with elements:
</p>

<ul>
<li> <p><code>x</code>: matrix of observations with <code>n</code> rows and <code>p</code>
columns
</p>
</li>
<li> <p><code>y</code>: vector of class labels that indicates class membership for
each observation (row) in <code>x</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Generates data from K = 3 classes.
means &lt;- matrix(rep(1:3, each=9), ncol=3)
data &lt;- generate_blockdiag(n = c(15, 15, 15), block_size = 3, num_blocks = 3,
rho = seq(.1, .9, length = 3), mu = means)
data$x
data$y

# Generates data from K = 4 classes. Notice that we use specify a variance.
means &lt;- matrix(rep(1:4, each=9), ncol=4)
data &lt;- generate_blockdiag(n = c(15, 15, 15, 20), block_size = 3, num_blocks = 3,
rho = seq(.1, .9, length = 4), mu = means)
data$x
data$y
</code></pre>

<hr>
<h2 id='generate_intraclass'>Generates data from <code>K</code> multivariate normal data populations, where each
population (class) has an intraclass covariance matrix.</h2><span id='topic+generate_intraclass'></span>

<h3>Description</h3>

<p>This function generates <code>K</code> multivariate normal data sets, where each
class is generated with a constant mean vector and an intraclass covariance
matrix. The data are returned as a single matrix <code>x</code> along with a vector
of class labels <code>y</code> that indicates class membership.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_intraclass(n, p, rho, mu, sigma2 = rep(1, K))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_intraclass_+3A_n">n</code></td>
<td>
<p>vector of the sample sizes of each class. The length of <code>n</code>
determines the number of classes <code>K</code>.</p>
</td></tr>
<tr><td><code id="generate_intraclass_+3A_p">p</code></td>
<td>
<p>the number of features (variables) in the data</p>
</td></tr>
<tr><td><code id="generate_intraclass_+3A_rho">rho</code></td>
<td>
<p>vector of the values of the off-diagonal elements for each
intraclass covariance matrix. Must equal the length of <code>n</code>.</p>
</td></tr>
<tr><td><code id="generate_intraclass_+3A_mu">mu</code></td>
<td>
<p>vector containing the mean for each class. Must equal the length of
<code>n</code> (i.e., equal to <code>K</code>).</p>
</td></tr>
<tr><td><code id="generate_intraclass_+3A_sigma2">sigma2</code></td>
<td>
<p>vector of variances for each class. Must equal the length of
<code>n</code>. Default is 1 for each class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For simplicity, we assume that a class mean vector is constant for each
feature. That is, we assume that the mean vector of the <code class="reqn">k</code>th class is
<code class="reqn">c_k * j_p</code>, where <code class="reqn">j_p</code> is a <code class="reqn">p \times 1</code> vector of ones and
<code class="reqn">c_k</code> is a real scalar.
</p>
<p>The intraclass covariance matrix for the <code class="reqn">k</code>th class is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\sigma_k^2 * (\rho_k * J_p + (1 - \rho_k) * I_p),</code>
</p>

<p>where <code class="reqn">J_p</code> is the <code class="reqn">p \times p</code> matrix of ones and <code class="reqn">I_p</code> is the
<code class="reqn">p \times p</code> identity matrix.
</p>
<p>By default, with <code class="reqn">\sigma_k^2 = 1</code>, the diagonal elements of the intraclass
covariance matrix are all 1, while the off-diagonal elements of the matrix
are all <code>rho</code>.
</p>
<p>The values of <code>rho</code> must be between <code class="reqn">1 / (1 - p)</code> and 1,
exclusively, to ensure that the covariance matrix is positive definite.
</p>
<p>The number of classes <code>K</code> is determined with lazy evaluation as the
length of <code>n</code>.
</p>


<h3>Value</h3>

<p>named list with elements:
</p>

<ul>
<li> <p><code>x</code>: matrix of observations with <code>n</code> rows and <code>p</code>
columns
</p>
</li>
<li> <p><code>y</code>: vector of class labels that indicates class membership for
each observation (row) in <code>x</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Generates data from K = 3 classes.
data &lt;- generate_intraclass(n = 3:5, p = 5, rho = seq(.1, .9, length = 3),
                            mu = c(0, 3, -2))
data$x
data$y

# Generates data from K = 4 classes. Notice that we use specify a variance.
data &lt;- generate_intraclass(n = 3:6, p = 4, rho = seq(0, .9, length = 4),
                            mu = c(0, 3, -2, 6), sigma2 = 1:4)
data$x
data$y
</code></pre>

<hr>
<h2 id='h'>Bias correction function from Pang et al. (2009).</h2><span id='topic+h'></span>

<h3>Description</h3>

<p>This function computes the function <code class="reqn">h_{\nu, p}(t)</code> on page 1023 of Pang
et al. (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h(nu, p, t = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h_+3A_nu">nu</code></td>
<td>
<p>a specified constant (nu = N - K)</p>
</td></tr>
<tr><td><code id="h_+3A_p">p</code></td>
<td>
<p>the feature space dimension.</p>
</td></tr>
<tr><td><code id="h_+3A_t">t</code></td>
<td>
<p>a constant specified by the user that indicates the exponent to use
with the variance estimator. By default, t = -1 as in Pang et al. See the
paper for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the bias correction value
</p>


<h3>References</h3>

<p>Pang, H., Tong, T., &amp; Zhao, H. (2009). &quot;Shrinkage-based Diagonal
Discriminant Analysis and Its Applications in High-Dimensional Data,&quot;
Biometrics, 65, 4, 1021-1029.
<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01200.x">https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01200.x</a>
</p>

<hr>
<h2 id='lda_diag'>Diagonal Linear Discriminant Analysis (DLDA)</h2><span id='topic+lda_diag'></span><span id='topic+lda_diag.default'></span><span id='topic+lda_diag.formula'></span><span id='topic+predict.lda_diag'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the Diagonal Linear
Discriminant Analysis (DLDA) classifier, which is often attributed to Dudoit
et al. (2002). The DLDA classifier belongs to the family of Naive Bayes
classifiers, where the distributions of each class are assumed to be
multivariate normal and to share a common covariance matrix.
</p>
<p>The DLDA classifier is a modification to LDA, where the off-diagonal elements
of the pooled sample covariance matrix are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_diag(x, ...)

## Default S3 method:
lda_diag(x, y, prior = NULL, ...)

## S3 method for class 'formula'
lda_diag(formula, data, prior = NULL, ...)

## S3 method for class 'lda_diag'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_diag_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="lda_diag_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="lda_diag_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="lda_diag_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="lda_diag_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="lda_diag_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="lda_diag_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="lda_diag_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="lda_diag_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The DLDA classifier is a modification to the well-known LDA classifier, where
the off-diagonal elements of the pooled sample covariance matrix are assumed
to be zero &ndash; the features are assumed to be uncorrelated. Under multivariate
normality, the assumption uncorrelated features is equivalent to the
assumption of independent features. The feature-independence assumption is a
notable attribute of the Naive Bayes classifier family. The benefit of these
classifiers is that they are fast and have much fewer parameters to estimate,
especially when the number of features is quite large.
</p>
<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p>The model fitting function returns the fitted classifier. The
<code>predict()</code> method returns either a vector (<code>type = "class"</code>) or a data
frame (all other <code>type</code> values).
</p>


<h3>References</h3>

<p>Dudoit, S., Fridlyand, J., &amp; Speed, T. P. (2002). &quot;Comparison of
Discrimination Methods for the Classification of Tumors Using Gene Expression
Data,&quot; Journal of the American Statistical Association, 97, 457, 77-87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
dlda_out &lt;- lda_diag(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(dlda_out, penguins[pred_rows, -1], type = "class")

dlda_out2 &lt;- lda_diag(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(dlda_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='lda_eigen'>The Minimum Distance Rule using Moore-Penrose Inverse (MDMP) classifier</h2><span id='topic+lda_eigen'></span><span id='topic+lda_eigen.default'></span><span id='topic+lda_eigen.formula'></span><span id='topic+predict.lda_eigen'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the MDMP classifier from
Srivistava and Kubokawa (2007). The MDMP classifier is an adaptation of the
linear discriminant analysis (LDA) classifier that is designed for
small-sample, high-dimensional data. Srivastava and Kubokawa (2007) have
proposed a modification of the standard maximum likelihood estimator of the
pooled covariance matrix, where only the largest 95% of the eigenvalues and
their corresponding eigenvectors are kept. The value of 95% is the default
and can be changed via the <code>eigen_pct</code> argument.
</p>
<p>The MDMP classifier from Srivistava and Kubokawa (2007) is an adaptation of the
linear discriminant analysis (LDA) classifier that is designed for
small-sample, high-dimensional data. Srivastava and Kubokawa (2007) have
proposed a modification of the standard maximum likelihood estimator of the
pooled covariance matrix, where only the largest 95% of the eigenvalues and
their corresponding eigenvectors are kept.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_eigen(x, ...)

## Default S3 method:
lda_eigen(x, y, prior = NULL, eigen_pct = 0.95, ...)

## S3 method for class 'formula'
lda_eigen(formula, data, prior = NULL, ...)

## S3 method for class 'lda_eigen'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_eigen_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="lda_eigen_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="lda_eigen_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="lda_eigen_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="lda_eigen_+3A_eigen_pct">eigen_pct</code></td>
<td>
<p>the percentage of eigenvalues kept</p>
</td></tr>
<tr><td><code id="lda_eigen_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="lda_eigen_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="lda_eigen_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="lda_eigen_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="lda_eigen_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>lda_eigen</code> object that contains the trained MDMP classifier
</p>


<h3>References</h3>

<p>Srivastava, M. and Kubokawa, T. (2007). &quot;Comparison of
Discrimination Methods for High Dimensional Data,&quot; Journal of the Japanese
Statistical Association, 37, 1, 123-134.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
mdmp_out &lt;- lda_eigen(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(mdmp_out, penguins[pred_rows, -1], type = "class")

mdmp_out2 &lt;- lda_eigen(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(mdmp_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='lda_emp_bayes'>The Minimum Distance Empirical Bayesian Estimator (MDEB) classifier</h2><span id='topic+lda_emp_bayes'></span><span id='topic+lda_emp_bayes.default'></span><span id='topic+lda_emp_bayes.formula'></span><span id='topic+predict.lda_emp_bayes'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the MDEB classifier from
Srivistava and Kubokawa (2007). The MDEB classifier is an adaptation of the
linear discriminant analysis (LDA) classifier that is designed for
small-sample, high-dimensional data. Rather than using the standard maximum
likelihood estimator of the pooled covariance matrix, Srivastava and Kubokawa
(2007) have proposed an Empirical Bayes estimator where the eigenvalues of
the pooled sample covariance matrix are shrunken towards the identity matrix:
the shrinkage constant has a closed form and is quick to calculate.
</p>
<p>The MDEB classifier from Srivistava and Kubokawa (2007) is an adaptation of the
linear discriminant analysis (LDA) classifier that is designed for
small-sample, high-dimensional data. Rather than using the standard maximum
likelihood estimator of the pooled covariance matrix, Srivastava and Kubokawa
(2007) have proposed an Empirical Bayes estimator where the eigenvalues of
the pooled sample covariance matrix are shrunken towards the identity matrix:
the shrinkage constant has a closed form and is quick to calculate
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_emp_bayes(x, ...)

## Default S3 method:
lda_emp_bayes(x, y, prior = NULL, ...)

## S3 method for class 'formula'
lda_emp_bayes(formula, data, prior = NULL, ...)

## S3 method for class 'lda_emp_bayes'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_emp_bayes_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>lda_emp_bayes</code> object that contains the trained MDEB classifier
</p>


<h3>References</h3>

<p>Srivastava, M. and Kubokawa, T. (2007). &quot;Comparison of
Discrimination Methods for High Dimensional Data,&quot; Journal of the Japanese
Statistical Association, 37, 1, 123-134.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
mdeb_out &lt;- lda_emp_bayes(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(mdeb_out, penguins[pred_rows, -1], type = "class")

mdeb_out2 &lt;- lda_emp_bayes(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(mdeb_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='lda_emp_bayes_eigen'>The Minimum Distance Rule using Modified Empirical Bayes (MDMEB) classifier</h2><span id='topic+lda_emp_bayes_eigen'></span><span id='topic+lda_emp_bayes_eigen.default'></span><span id='topic+lda_emp_bayes_eigen.formula'></span><span id='topic+predict.lda_emp_bayes_eigen'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the MDMEB classifier from
Srivistava and Kubokawa (2007). The MDMEB classifier is an adaptation of the
linear discriminant analysis (LDA) classifier that is designed for
small-sample, high-dimensional data. Srivastava and Kubokawa (2007) have
proposed a modification of the standard maximum likelihood estimator of the
pooled covariance matrix, where only the largest 95% of the eigenvalues and
their corresponding eigenvectors are kept. The resulting covariance matrix is
then shrunken towards a scaled identity matrix. The value of 95% is the
default and can be changed via the <code>eigen_pct</code> argument.
</p>
<p>The MDMEB classifier is an adaptation of the linear discriminant analysis
(LDA) classifier that is designed for small-sample, high-dimensional
data. Srivastava and Kubokawa (2007) have proposed a modification of the
standard maximum likelihood estimator of the pooled covariance matrix, where
only the largest 95% of the eigenvalues and their corresponding eigenvectors
are kept. The resulting covariance matrix is then shrunken towards a scaled
identity matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_emp_bayes_eigen(x, ...)

## Default S3 method:
lda_emp_bayes_eigen(x, y, prior = NULL, eigen_pct = 0.95, ...)

## S3 method for class 'formula'
lda_emp_bayes_eigen(formula, data, prior = NULL, ...)

## S3 method for class 'lda_emp_bayes_eigen'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_emp_bayes_eigen_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_eigen_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_eigen_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_eigen_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_eigen_+3A_eigen_pct">eigen_pct</code></td>
<td>
<p>the percentage of eigenvalues kept</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_eigen_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_eigen_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_eigen_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_eigen_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="lda_emp_bayes_eigen_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>lda_emp_bayes_eigen</code> object that contains the trained MDMEB classifier
</p>


<h3>References</h3>

<p>Srivastava, M. and Kubokawa, T. (2007). &quot;Comparison of
Discrimination Methods for High Dimensional Data,&quot; Journal of the Japanese
Statistical Association, 37, 1, 123-134.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
mdmeb_out &lt;- lda_emp_bayes_eigen(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(mdmeb_out, penguins[pred_rows, -1], type = "class")

mdmeb_out2 &lt;- lda_emp_bayes_eigen(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(mdmeb_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='lda_pseudo'>Linear Discriminant Analysis (LDA) with the Moore-Penrose Pseudo-Inverse</h2><span id='topic+lda_pseudo'></span><span id='topic+lda_pseudo.default'></span><span id='topic+lda_pseudo.formula'></span><span id='topic+predict.lda_pseudo'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the Linear Discriminant
Analysis (LDA) classifier, where the distributions of each class are assumed
to be multivariate normal and share a common covariance matrix.
When the pooled sample covariance matrix is singular, the linear discriminant
function is incalculable. A common method to overcome this issue is to
replace the inverse of the pooled sample covariance matrix with the
Moore-Penrose pseudo-inverse, which is unique and always exists. Note that
when the pooled sample covariance matrix is nonsingular, it is equal to the
pseudo-inverse.
</p>
<p>The Linear Discriminant Analysis (LDA) classifier involves the assumption
that the distributions of each class are assumed to be multivariate normal
and share a common covariance matrix. When the pooled sample covariance
matrix is singular, the linear discriminant function is incalculable. A
common method to overcome this issue is to replace the inverse of the pooled
sample covariance matrix with the Moore-Penrose pseudo-inverse, which is
unique and always exists. Note that when the pooled sample covariance matrix
is nonsingular, it is equal to the pseudo-inverse.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_pseudo(x, ...)

## Default S3 method:
lda_pseudo(x, y, prior = NULL, tol = 1e-08, ...)

## S3 method for class 'formula'
lda_pseudo(formula, data, prior = NULL, tol = 1e-08, ...)

## S3 method for class 'lda_pseudo'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_pseudo_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="lda_pseudo_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="lda_pseudo_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="lda_pseudo_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="lda_pseudo_+3A_tol">tol</code></td>
<td>
<p>tolerance value below which eigenvalues are considered numerically
equal to 0</p>
</td></tr>
<tr><td><code id="lda_pseudo_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="lda_pseudo_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="lda_pseudo_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="lda_pseudo_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="lda_pseudo_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>lda_pseudo</code> object that contains the trained lda_pseudo
classifier
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
lda_pseudo_out &lt;- lda_pseudo(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(lda_pseudo_out, penguins[pred_rows, -1], type = "class")

lda_pseudo_out2 &lt;- lda_pseudo(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(lda_pseudo_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='lda_schafer'>Linear Discriminant Analysis using the Schafer-Strimmer Covariance Matrix
Estimator</h2><span id='topic+lda_schafer'></span><span id='topic+lda_schafer.default'></span><span id='topic+lda_schafer.formula'></span><span id='topic+predict.lda_schafer'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the Linear Discriminant
Analysis (LDA) classifier, where the distributions of each class are assumed
to be multivariate normal and share a common covariance matrix. When the
pooled sample covariance matrix is singular, the linear discriminant function
is incalculable. This function replaces the inverse of pooled sample
covariance matrix with an estimator proposed by Schafer and Strimmer
(2005). The estimator is calculated via <code><a href="corpcor.html#topic+invcov.shrink">corpcor::invcov.shrink()</a></code>.
</p>
<p>The Linear Discriminant Analysis (LDA) classifier involves the assumption
that the distributions of each class are assumed to be multivariate normal
and share a common covariance matrix. When the pooled sample covariance
matrix is singular, the linear discriminant function is incalculable. Here,
the inverse of the pooled sample covariance matrix is replaced with an
estimator from Schafer and Strimmer (2005).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_schafer(x, ...)

## Default S3 method:
lda_schafer(x, y, prior = NULL, ...)

## S3 method for class 'formula'
lda_schafer(formula, data, prior = NULL, ...)

## S3 method for class 'lda_schafer'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_schafer_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="lda_schafer_+3A_...">...</code></td>
<td>
<p>Options passed to <code><a href="corpcor.html#topic+invcov.shrink">corpcor::invcov.shrink()</a></code></p>
</td></tr>
<tr><td><code id="lda_schafer_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="lda_schafer_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="lda_schafer_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="lda_schafer_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="lda_schafer_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="lda_schafer_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="lda_schafer_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>lda_schafer</code> object that contains the trained classifier
</p>


<h3>References</h3>

<p>Schafer, J., and Strimmer, K. (2005). &quot;A shrinkage approach to
large-scale covariance estimation and implications for functional genomics,&quot;
Statist. Appl. Genet. Mol. Biol. 4, 32.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
lda_schafer_out &lt;- lda_schafer(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(lda_schafer_out, penguins[pred_rows, -1], type = "class")

lda_schafer_out2 &lt;- lda_schafer(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(lda_schafer_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='lda_shrink_cov'>Shrinkage-based Diagonal Linear Discriminant Analysis (SDLDA)</h2><span id='topic+lda_shrink_cov'></span><span id='topic+lda_shrink_cov.default'></span><span id='topic+lda_shrink_cov.formula'></span><span id='topic+predict.lda_shrink_cov'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the Shrinkage-based
Diagonal Linear Discriminant Analysis (SDLDA) classifier, which is based on
the DLDA classifier, often attributed to Dudoit et al. (2002). The DLDA
classifier belongs to the family of Naive Bayes classifiers, where the
distributions of each class are assumed to be multivariate normal and to
share a common covariance matrix. To improve the estimation of the pooled
variances, Pang et al. (2009) proposed the SDLDA classifier which uses a
shrinkage-based estimators of the pooled covariance matrix.
</p>
<p>The SDLDA classifier is a modification to LDA, where the off-diagonal
elements of the pooled sample covariance matrix are set to zero. To improve
the estimation of the pooled variances, we use a shrinkage method from Pang
et al.  (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_shrink_cov(x, ...)

## Default S3 method:
lda_shrink_cov(x, y, prior = NULL, num_alphas = 101, ...)

## S3 method for class 'formula'
lda_shrink_cov(formula, data, prior = NULL, num_alphas = 101, ...)

## S3 method for class 'lda_shrink_cov'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_shrink_cov_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="lda_shrink_cov_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="lda_shrink_cov_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="lda_shrink_cov_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="lda_shrink_cov_+3A_num_alphas">num_alphas</code></td>
<td>
<p>the number of values used to find the optimal amount of
shrinkage</p>
</td></tr>
<tr><td><code id="lda_shrink_cov_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="lda_shrink_cov_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="lda_shrink_cov_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="lda_shrink_cov_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="lda_shrink_cov_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The DLDA classifier is a modification to the well-known LDA classifier, where
the off-diagonal elements of the pooled covariance matrix are assumed to be
zero &ndash; the features are assumed to be uncorrelated. Under multivariate
normality, the assumption uncorrelated features is equivalent to the
assumption of independent features. The feature-independence assumption is a
notable attribute of the Naive Bayes classifier family. The benefit of these
classifiers is that they are fast and have much fewer parameters to estimate,
especially when the number of features is quite large.
</p>
<p>The matrix of training observations are given in <code>x</code>. The rows of
<code>x</code> contain the sample observations, and the columns contain the
features for each training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations
belonging to each class. Otherwise, <code>prior</code> should be a vector with the
same length as the number of classes in <code>y</code>. The <code>prior</code>
probabilities should be nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>lda_shrink_cov</code> object that contains the trained SDLDA classifier
</p>


<h3>References</h3>

<p>Dudoit, S., Fridlyand, J., &amp; Speed, T. P. (2002). &quot;Comparison of
Discrimination Methods for the Classification of Tumors Using Gene Expression
Data,&quot; Journal of the American Statistical Association, 97, 457, 77-87.
</p>
<p>Pang, H., Tong, T., &amp; Zhao, H. (2009). &quot;Shrinkage-based Diagonal
Discriminant Analysis and Its Applications in High-Dimensional Data,&quot;
Biometrics, 65, 4, 1021-1029.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
sdlda_out &lt;- lda_shrink_cov(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(sdlda_out, penguins[pred_rows, -1], type = "class")

sdlda_out2 &lt;- lda_shrink_cov(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(sdlda_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='lda_shrink_mean'>Shrinkage-mean-based Diagonal Linear Discriminant Analysis (SmDLDA) from
Tong, Chen, and Zhao (2012)</h2><span id='topic+lda_shrink_mean'></span><span id='topic+lda_shrink_mean.default'></span><span id='topic+lda_shrink_mean.formula'></span><span id='topic+predict.lda_shrink_mean'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the Shrinkage-mean-based
Diagonal Linear Discriminant Analysis (SmDLDA) classifier from Tong, Chen,
and Zhao (2012). The SmDLDA classifier incorporates a Lindley-type shrunken
mean estimator into the DLDA classifier from Dudoit et al. (2002). For more
about the DLDA classifier, see <code><a href="#topic+lda_diag">lda_diag()</a></code>.
</p>
<p>The SmDLDA classifier is a modification to LDA, where the off-diagonal
elements of the pooled sample covariance matrix are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_shrink_mean(x, ...)

## Default S3 method:
lda_shrink_mean(x, y, prior = NULL, ...)

## S3 method for class 'formula'
lda_shrink_mean(formula, data, prior = NULL, ...)

## S3 method for class 'lda_shrink_mean'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_shrink_mean_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="lda_shrink_mean_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="lda_shrink_mean_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="lda_shrink_mean_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="lda_shrink_mean_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="lda_shrink_mean_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="lda_shrink_mean_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="lda_shrink_mean_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="lda_shrink_mean_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The DLDA classifier belongs to the family of Naive Bayes classifiers, where
the distributions of each class are assumed to be multivariate normal and to
share a common covariance matrix.
</p>
<p>The DLDA classifier is a modification to the well-known LDA classifier, where
the off-diagonal elements of the pooled sample covariance matrix are assumed
to be zero &ndash; the features are assumed to be uncorrelated. Under multivariate
normality, the assumption uncorrelated features is equivalent to the
assumption of independent features. The feature-independence assumption is a
notable attribute of the Naive Bayes classifier family. The benefit of these
classifiers is that they are fast and have much fewer parameters to estimate,
especially when the number of features is quite large.
</p>
<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>lda_shrink_mean</code> object that contains the trained SmDLDA classifier
</p>


<h3>References</h3>

<p>Tong, T., Chen, L., and Zhao, H. (2012), &quot;Improved Mean
Estimation and Its Application to Diagonal Discriminant Analysis,&quot;
Bioinformatics, 28, 4, 531-537.
<a href="https://academic.oup.com/bioinformatics/article/28/4/531/211887">https://academic.oup.com/bioinformatics/article/28/4/531/211887</a>
</p>
<p>Dudoit, S., Fridlyand, J., &amp; Speed, T. P. (2002). &quot;Comparison of
Discrimination Methods for the Classification of Tumors Using Gene Expression
Data,&quot; Journal of the American Statistical Association, 97, 457, 77-87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
smdlda_out &lt;- lda_shrink_mean(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(smdlda_out, penguins[pred_rows, -1], type = "class")

smdlda_out2 &lt;- lda_shrink_mean(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(smdlda_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='lda_thomaz'>Linear Discriminant Analysis using the Thomaz-Kitani-Gillies Covariance
Matrix Estimator</h2><span id='topic+lda_thomaz'></span><span id='topic+lda_thomaz.default'></span><span id='topic+lda_thomaz.formula'></span><span id='topic+predict.lda_thomaz'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the Linear Discriminant
Analysis (LDA) classifier, where the distributions of each class are assumed
to be multivariate normal and share a common covariance matrix. When the
pooled sample covariance matrix is singular, the linear discriminant function
is incalculable. This function replaces the pooled sample covariance matrix
with a regularized estimator from Thomaz et al. (2006), where the smallest
eigenvalues are replaced with the average eigenvalue. Specifically, small
eigenvalues here means that the eigenvalues are less than the average
eigenvalue.
</p>
<p>Given a set of training data, this function builds the Linear Discriminant
Analysis (LDA) classifier, where the distributions of each class are assumed
to be multivariate normal and share a common covariance matrix. When the
pooled sample covariance matrix is singular, the linear discriminant function
is incalculable. This function replaces the pooled sample covariance matrix
with a regularized estimator from Thomaz et al. (2006), where the smallest
eigenvalues are replaced with the average eigenvalue. Specifically, small
eigenvalues here means that the eigenvalues are less than the average
eigenvalue.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_thomaz(x, ...)

## Default S3 method:
lda_thomaz(x, y, prior = NULL, ...)

## S3 method for class 'formula'
lda_thomaz(formula, data, prior = NULL, ...)

## S3 method for class 'lda_thomaz'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_thomaz_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="lda_thomaz_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="lda_thomaz_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="lda_thomaz_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="lda_thomaz_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="lda_thomaz_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="lda_thomaz_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="lda_thomaz_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="lda_thomaz_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>lda_thomaz</code> object that contains the trained classifier
</p>


<h3>References</h3>

<p>Thomaz, C. E., Kitani, E. C., and Gillies, D. F. (2006). &quot;A
maximum uncertainty LDA-based approach for limited sample size problems with
application to face recognition,&quot; J. Braz. Comp. Soc., 12, 2, 7-18.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
lda_thomaz_out &lt;- lda_thomaz(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(lda_thomaz_out, penguins[pred_rows, -1], type = "class")

lda_thomaz_out2 &lt;- lda_thomaz(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(lda_thomaz_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='log_determinant'>Computes the log determinant of a matrix.</h2><span id='topic+log_determinant'></span>

<h3>Description</h3>

<p>Computes the log determinant of a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_determinant(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_determinant_+3A_x">x</code></td>
<td>
<p>matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>log determinant of <code>x</code>
</p>

<hr>
<h2 id='no_intercept'>Removes the intercept term from a formula if it is included</h2><span id='topic+no_intercept'></span>

<h3>Description</h3>

<p>Often, we prefer not to have an intercept term in a model, but user-specified
formulas might have included the intercept term. In this case, we wish to
update the formula but without the intercept term. This is especially true in
numerous classification models, where errors and doom can occur if an
intercept is included in the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>no_intercept(formula, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="no_intercept_+3A_formula">formula</code></td>
<td>
<p>a model formula to remove its intercept term</p>
</td></tr>
<tr><td><code id="no_intercept_+3A_data">data</code></td>
<td>
<p>data frame</p>
</td></tr>
</table>


<h3>Value</h3>

<p>formula with no intercept term
</p>


<h3>Examples</h3>

<pre><code class='language-R'>iris_formula &lt;- formula(Species ~ .)
no_intercept(iris_formula, data = iris)
</code></pre>

<hr>
<h2 id='plot.rda_high_dim_cv'>Plots a heatmap of cross-validation error grid for a HDRDA classifier object.</h2><span id='topic+plot.rda_high_dim_cv'></span>

<h3>Description</h3>

<p>Uses <code><a href="ggplot2.html#topic+ggplot2-package">ggplot2::ggplot2()</a></code> to plot a heatmap of the training error
grid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rda_high_dim_cv'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.rda_high_dim_cv_+3A_x">x</code></td>
<td>
<p>object to plot</p>
</td></tr>
<tr><td><code id="plot.rda_high_dim_cv_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>

<hr>
<h2 id='posterior_probs'>Computes posterior probabilities via Bayes Theorem under normality</h2><span id='topic+posterior_probs'></span>

<h3>Description</h3>

<p>Computes posterior probabilities via Bayes Theorem under normality
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posterior_probs(x, means, covs, priors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="posterior_probs_+3A_x">x</code></td>
<td>
<p>matrix of observations</p>
</td></tr>
<tr><td><code id="posterior_probs_+3A_means">means</code></td>
<td>
<p>list of means for each class</p>
</td></tr>
<tr><td><code id="posterior_probs_+3A_covs">covs</code></td>
<td>
<p>list of covariance matrices for each class</p>
</td></tr>
<tr><td><code id="posterior_probs_+3A_priors">priors</code></td>
<td>
<p>list of prior probabilities for each class</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix of posterior probabilities for each observation
</p>

<hr>
<h2 id='print.lda_diag'>Outputs the summary for a DLDA classifier object.</h2><span id='topic+print.lda_diag'></span>

<h3>Description</h3>

<p>Summarizes the trained DLDA classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_diag'
print(x, ...)
</code></pre>


<h3>Value</h3>

<p><code>x</code> (invisibly).
</p>

<hr>
<h2 id='print.lda_eigen'>Outputs the summary for a MDMP classifier object.</h2><span id='topic+print.lda_eigen'></span>

<h3>Description</h3>

<p>Summarizes the trained lda_eigen classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_eigen'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.lda_emp_bayes'>Outputs the summary for a MDEB classifier object.</h2><span id='topic+print.lda_emp_bayes'></span>

<h3>Description</h3>

<p>Summarizes the trained lda_emp_bayes classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_emp_bayes'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.lda_emp_bayes_eigen'>Outputs the summary for a MDMEB classifier object.</h2><span id='topic+print.lda_emp_bayes_eigen'></span>

<h3>Description</h3>

<p>Summarizes the trained lda_emp_bayes_eigen classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_emp_bayes_eigen'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.lda_pseudo'>Outputs the summary for a lda_pseudo classifier object.</h2><span id='topic+print.lda_pseudo'></span>

<h3>Description</h3>

<p>Summarizes the trained lda_pseudo classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_pseudo'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.lda_schafer'>Outputs the summary for a lda_schafer classifier object.</h2><span id='topic+print.lda_schafer'></span>

<h3>Description</h3>

<p>Summarizes the trained lda_schafer classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_schafer'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.lda_shrink_cov'>Outputs the summary for a SDLDA classifier object.</h2><span id='topic+print.lda_shrink_cov'></span>

<h3>Description</h3>

<p>Summarizes the trained SDLDA classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_shrink_cov'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.lda_shrink_mean'>Outputs the summary for a SmDLDA classifier object.</h2><span id='topic+print.lda_shrink_mean'></span>

<h3>Description</h3>

<p>Summarizes the trained SmDLDA classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_shrink_mean'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.lda_thomaz'>Outputs the summary for a lda_thomaz classifier object.</h2><span id='topic+print.lda_thomaz'></span>

<h3>Description</h3>

<p>Summarizes the trained lda_thomaz classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_thomaz'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.qda_diag'>Outputs the summary for a DQDA classifier object.</h2><span id='topic+print.qda_diag'></span>

<h3>Description</h3>

<p>Summarizes the trained DQDA classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'qda_diag'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.qda_shrink_cov'>Outputs the summary for a SDQDA classifier object.</h2><span id='topic+print.qda_shrink_cov'></span>

<h3>Description</h3>

<p>Summarizes the trained SDQDA classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'qda_shrink_cov'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.qda_shrink_mean'>Outputs the summary for a SmDQDA classifier object.</h2><span id='topic+print.qda_shrink_mean'></span>

<h3>Description</h3>

<p>Summarizes the trained SmDQDA classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'qda_shrink_mean'
print(x, ...)
</code></pre>

<hr>
<h2 id='print.rda_high_dim'>Outputs the summary for a HDRDA classifier object.</h2><span id='topic+print.rda_high_dim'></span>

<h3>Description</h3>

<p>Summarizes the trained rda_high_dim classifier in a nice manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rda_high_dim'
print(x, ...)
</code></pre>

<hr>
<h2 id='qda_diag'>Diagonal Quadratic Discriminant Analysis (DQDA)</h2><span id='topic+qda_diag'></span><span id='topic+qda_diag.default'></span><span id='topic+qda_diag.formula'></span><span id='topic+predict.qda_diag'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the Diagonal Quadratic
Discriminant Analysis (DQDA) classifier, which is often attributed to Dudoit
et al. (2002). The DQDA classifier belongs to the family of Naive Bayes
classifiers, where the distributions of each class are assumed to be
multivariate normal. Note that the DLDA classifier is a special case of the
DQDA classifier.
</p>
<p>The DQDA classifier is a modification to QDA, where the off-diagonal elements
of the pooled sample covariance matrix are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qda_diag(x, ...)

## Default S3 method:
qda_diag(x, y, prior = NULL, ...)

## S3 method for class 'formula'
qda_diag(formula, data, prior = NULL, ...)

## S3 method for class 'qda_diag'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qda_diag_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="qda_diag_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="qda_diag_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="qda_diag_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="qda_diag_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="qda_diag_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="qda_diag_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="qda_diag_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="qda_diag_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The DQDA classifier is a modification to the well-known QDA classifier, where
the off-diagonal elements of each class covariance matrix are assumed
to be zero &ndash; the features are assumed to be uncorrelated. Under multivariate
normality, the assumption uncorrelated features is equivalent to the
assumption of independent features. The feature-independence assumption is a
notable attribute of the Naive Bayes classifier family. The benefit of these
classifiers is that they are fast and have much fewer parameters to estimate,
especially when the number of features is quite large.
</p>
<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>qda_diag</code> object that contains the trained DQDA classifier
</p>


<h3>References</h3>

<p>Dudoit, S., Fridlyand, J., &amp; Speed, T. P. (2002). &quot;Comparison of
Discrimination Methods for the Classification of Tumors Using Gene Expression
Data,&quot; Journal of the American Statistical Association, 97, 457, 77-87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
dqda_out &lt;- qda_diag(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(dqda_out, penguins[pred_rows, -1], type = "class")

dqda_out2 &lt;- qda_diag(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(dqda_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='qda_shrink_cov'>Shrinkage-based Diagonal Quadratic Discriminant Analysis (SDQDA)</h2><span id='topic+qda_shrink_cov'></span><span id='topic+qda_shrink_cov.default'></span><span id='topic+qda_shrink_cov.formula'></span><span id='topic+predict.qda_shrink_cov'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the Shrinkage-based
Diagonal Quadratic Discriminant Analysis (SDQDA) classifier, which is based
on the DQDA classifier, often attributed to Dudoit et al. (2002). The DQDA
classifier belongs to the family of Naive Bayes classifiers, where the
distributions of each class are assumed to be multivariate normal. To improve
the estimation of the class variances, Pang et al. (2009) proposed the SDQDA
classifier which uses a shrinkage-based estimators of each class covariance
matrix.
</p>
<p>The SDQDA classifier is a modification to QDA, where the off-diagonal
elements of the pooled sample covariance matrix are set to zero. To improve
the estimation of the pooled variances, we use a shrinkage method from Pang
et al.  (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qda_shrink_cov(x, ...)

## Default S3 method:
qda_shrink_cov(x, y, prior = NULL, num_alphas = 101, ...)

## S3 method for class 'formula'
qda_shrink_cov(formula, data, prior = NULL, num_alphas = 101, ...)

## S3 method for class 'qda_shrink_cov'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qda_shrink_cov_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="qda_shrink_cov_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="qda_shrink_cov_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="qda_shrink_cov_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="qda_shrink_cov_+3A_num_alphas">num_alphas</code></td>
<td>
<p>the number of values used to find the optimal amount of
shrinkage</p>
</td></tr>
<tr><td><code id="qda_shrink_cov_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="qda_shrink_cov_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="qda_shrink_cov_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="qda_shrink_cov_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="qda_shrink_cov_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The DQDA classifier is a modification to the well-known QDA classifier, where
the off-diagonal elements of the pooled covariance matrix are assumed to be
zero &ndash; the features are assumed to be uncorrelated. Under multivariate
normality, the assumption uncorrelated features is equivalent to the
assumption of independent features. The feature-independence assumption is a
notable attribute of the Naive Bayes classifier family. The benefit of these
classifiers is that they are fast and have much fewer parameters to estimate,
especially when the number of features is quite large.
</p>
<p>The matrix of training observations are given in <code>x</code>. The rows of
<code>x</code> contain the sample observations, and the columns contain the
features for each training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations
belonging to each class. Otherwise, <code>prior</code> should be a vector with the
same length as the number of classes in <code>y</code>. The <code>prior</code>
probabilities should be nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>qda_shrink_cov</code> object that contains the trained SDQDA classifier
</p>


<h3>References</h3>

<p>Dudoit, S., Fridlyand, J., &amp; Speed, T. P. (2002). &quot;Comparison of
Discrimination Methods for the Classification of Tumors Using Gene Expression
Data,&quot; Journal of the American Statistical Association, 97, 457, 77-87.
</p>
<p>Pang, H., Tong, T., &amp; Zhao, H. (2009). &quot;Shrinkage-based Diagonal
Discriminant Analysis and Its Applications in High-Dimensional Data,&quot;
Biometrics, 65, 4, 1021-1029.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]#' set.seed(42)
sdqda_out &lt;- qda_shrink_cov(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(sdqda_out, penguins[pred_rows, -1], type = "class")

sdqda_out2 &lt;- qda_shrink_cov(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(sdqda_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='qda_shrink_mean'>Shrinkage-mean-based Diagonal Quadratic Discriminant Analysis (SmDQDA) from
Tong, Chen, and Zhao (2012)</h2><span id='topic+qda_shrink_mean'></span><span id='topic+qda_shrink_mean.default'></span><span id='topic+qda_shrink_mean.formula'></span><span id='topic+predict.qda_shrink_mean'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the Shrinkage-mean-based
Diagonal Quadratic Discriminant Analysis (SmDQDA) classifier from Tong, Chen,
and Zhao (2012). The SmDQDA classifier incorporates a Lindley-type shrunken
mean estimator into the DQDA classifier from Dudoit et al. (2002). For more
about the DQDA classifier, see <code><a href="#topic+qda_diag">qda_diag()</a></code>.
</p>
<p>The SmDQDA classifier is a modification to QDA, where the off-diagonal elements
of the pooled sample covariance matrix are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qda_shrink_mean(x, ...)

## Default S3 method:
qda_shrink_mean(x, y, prior = NULL, ...)

## S3 method for class 'formula'
qda_shrink_mean(formula, data, prior = NULL, ...)

## S3 method for class 'qda_shrink_mean'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qda_shrink_mean_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="qda_shrink_mean_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="qda_shrink_mean_+3A_y">y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td></tr>
<tr><td><code id="qda_shrink_mean_+3A_prior">prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="qda_shrink_mean_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="qda_shrink_mean_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="qda_shrink_mean_+3A_object">object</code></td>
<td>
<p>Fitted model object</p>
</td></tr>
<tr><td><code id="qda_shrink_mean_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="qda_shrink_mean_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The DQDA classifier is a modification to the well-known QDA classifier, where
the off-diagonal elements of each class covariance matrix are assumed
to be zero &ndash; the features are assumed to be uncorrelated. Under multivariate
normality, the assumption uncorrelated features is equivalent to the
assumption of independent features. The feature-independence assumption is a
notable attribute of the Naive Bayes classifier family. The benefit of these
classifiers is that they are fast and have much fewer parameters to estimate,
especially when the number of features is quite large.
</p>
<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>qda_shrink_mean</code> object that contains the trained SmDQDA classifier
</p>


<h3>References</h3>

<p>Tong, T., Chen, L., and Zhao, H. (2012), &quot;Improved Mean
Estimation and Its Application to Diagonal Discriminant Analysis,&quot;
Bioinformatics, 28, 4, 531-537.
<a href="https://academic.oup.com/bioinformatics/article/28/4/531/211887">https://academic.oup.com/bioinformatics/article/28/4/531/211887</a>
</p>
<p>Dudoit, S., Fridlyand, J., &amp; Speed, T. P. (2002). &quot;Comparison of
Discrimination Methods for the Classification of Tumors Using Gene Expression
Data,&quot; Journal of the American Statistical Association, 97, 457, 77-87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
smdqda_out &lt;- qda_shrink_mean(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(smdqda_out, penguins[pred_rows, -1], type = "class")

smdqda_out2 &lt;- qda_shrink_mean(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(smdqda_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>

<hr>
<h2 id='quadform'>Quadratic form of a matrix and a vector</h2><span id='topic+quadform'></span>

<h3>Description</h3>

<p>We compute the quadratic form of a vector and a matrix in an efficient
manner. Let <code>x</code> be a real vector of length <code>p</code>, and let <code>A</code> be
a p x p real matrix. Then, we compute the quadratic form <code class="reqn">q = x' A x</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quadform(A, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quadform_+3A_a">A</code></td>
<td>
<p>matrix of dimension p x p</p>
</td></tr>
<tr><td><code id="quadform_+3A_x">x</code></td>
<td>
<p>vector of length p</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A naive way to compute the quadratic form is to explicitly write
<code style="white-space: pre;">&#8288;t(x) \%*\% A \%*\% x&#8288;</code>, but for large <code>p</code>, this operation is
inefficient. We provide a more efficient method below.
</p>
<p>Note that we have adapted the code from:
<a href="https://stat.ethz.ch/pipermail/r-help/2005-November/081940.html">https://stat.ethz.ch/pipermail/r-help/2005-November/081940.html</a>
</p>


<h3>Value</h3>

<p>scalar value
</p>

<hr>
<h2 id='quadform_inv'>Quadratic Form of the inverse of a matrix and a vector</h2><span id='topic+quadform_inv'></span>

<h3>Description</h3>

<p>We compute the quadratic form of a vector and the inverse of a matrix in an
efficient manner. Let <code>x</code> be a real vector of length <code>p</code>, and let
<code>A</code> be a p x p nonsingular matrix. Then, we compute the quadratic form
<code class="reqn">q = x' A^{-1} x</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quadform_inv(A, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quadform_inv_+3A_a">A</code></td>
<td>
<p>matrix that is p x p and nonsingular</p>
</td></tr>
<tr><td><code id="quadform_inv_+3A_x">x</code></td>
<td>
<p>vector of length p</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A naive way to compute the quadratic form is to explicitly write
<code style="white-space: pre;">&#8288;t(x) \%*\% solve(A) \%*\% x&#8288;</code>, but for large <code>p</code>, this operation is
inefficient. We provide a more efficient method below.
</p>
<p>Note that we have adapted the code from:
<a href="https://stat.ethz.ch/pipermail/r-help/2005-November/081940.html">https://stat.ethz.ch/pipermail/r-help/2005-November/081940.html</a>
</p>


<h3>Value</h3>

<p>scalar value
</p>

<hr>
<h2 id='rda_cov'>Calculates the RDA covariance-matrix estimators for each class</h2><span id='topic+rda_cov'></span>

<h3>Description</h3>

<p>For the classes given in the vector <code>y</code>, this function calculates the
class covariance-matrix estimators employed in the HDRDA classifier,
implemented in <code><a href="#topic+rda_high_dim">rda_high_dim()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rda_cov(x, y, lambda = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rda_cov_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="rda_cov_+3A_y">y</code></td>
<td>
<p>vector of class labels for each training observation</p>
</td></tr>
<tr><td><code id="rda_cov_+3A_lambda">lambda</code></td>
<td>
<p>the RDA pooling parameter. Must be between 0 and 1, inclusively.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list containing the RDA covariance-matrix estimators for each class
given in <code>y</code>
</p>


<h3>References</h3>

<p>Ramey, J. A., Stein, C. K., and Young, D. M. (2013),
&quot;High-Dimensional Regularized Discriminant Analysis.&quot;
</p>

<hr>
<h2 id='rda_high_dim'>High-Dimensional Regularized Discriminant Analysis (HDRDA)</h2><span id='topic+rda_high_dim'></span><span id='topic+rda_high_dim.default'></span><span id='topic+rda_high_dim.formula'></span><span id='topic+predict.rda_high_dim'></span>

<h3>Description</h3>

<p>Given a set of training data, this function builds the HDRDA classifier from
Ramey, Stein, and Young (2017). Specially designed for small-sample,
high-dimensional data, the HDRDA classifier incorporates dimension reduction
and covariance-matrix shrinkage to enable a computationally efficient
classifier.
</p>
<p>For a given <code>rda_high_dim</code> object, we predict the class of each observation
(row) of the the matrix given in <code>newdata</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rda_high_dim(x, ...)

## Default S3 method:
rda_high_dim(
  x,
  y,
  lambda = 1,
  gamma = 0,
  shrinkage_type = c("ridge", "convex"),
  prior = NULL,
  tol = 1e-06,
  ...
)

## S3 method for class 'formula'
rda_high_dim(formula, data, ...)

## S3 method for class 'rda_high_dim'
predict(
  object,
  newdata,
  projected = FALSE,
  type = c("class", "prob", "score"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rda_high_dim_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_...">...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_y">y</code></td>
<td>
<p>vector of class labels for each training observation</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_lambda">lambda</code></td>
<td>
<p>the HDRDA pooling parameter. Must be between 0 and 1,
inclusively.</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_gamma">gamma</code></td>
<td>
<p>a numeric values used for the shrinkage parameter.</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_shrinkage_type">shrinkage_type</code></td>
<td>
<p>the type of covariance-matrix shrinkage to apply. By
default, a ridge-like shrinkage is applied. If <code>convex</code> is given, then
shrinkage similar to Friedman (1989) is applied. See Ramey et al. (2017) for
details.</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_prior">prior</code></td>
<td>
<p>vector with prior probabilities for each class. If <code>NULL</code>
(default), then the sample proportion of observations belonging to each class
equal probabilities are used. See details.</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_tol">tol</code></td>
<td>
<p>a threshold for determining nonzero eigenvalues.</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_data">data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_object">object</code></td>
<td>
<p>Object of type <code>rda_high_dim</code> that contains the trained HDRDA
classifier</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_newdata">newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_projected">projected</code></td>
<td>
<p>logical indicating whether <code>newdata</code> have already been
projected to a q-dimensional subspace. This argument can yield large gains in
speed when the linear transformation has already been performed.</p>
</td></tr>
<tr><td><code id="rda_high_dim_+3A_type">type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The HDRDA classifier utilizes a covariance-matrix estimator that is a convex
combination of the covariance-matrix estimators used in the Linear
Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)
classifiers. For each of the <code>K</code> classes given in <code>y</code>,
<code class="reqn">(k = 1, \ldots, K)</code>, we first define this convex combination as
</p>
<p style="text-align: center;"><code class="reqn">\hat{\Sigma}_k(\lambda) = (1 - \lambda) \hat{\Sigma}_k
+ \lambda \hat{\Sigma},</code>
</p>

<p>where <code class="reqn">\lambda \in [0, 1]</code> is the <em>pooling</em> parameter. We then
calculate the covariance-matrix estimator
</p>
<p style="text-align: center;"><code class="reqn">\tilde{\Sigma}_k = \alpha_k \hat{\Sigma}_k(\lambda) + \gamma I_p,</code>
</p>

<p>where <code class="reqn">I_p</code> is the <code class="reqn">p \times p</code> identity matrix. The matrix
<code class="reqn">\tilde{\Sigma}_k</code> is substituted into the HDRDA classifier. See Ramey et
al. (2017) for more details.
</p>
<p>The matrix of training observations are given in <code>x</code>. The rows of
<code>x</code> contain the sample observations, and the columns contain the features
for each training observation. The vector of class labels given in <code>y</code>
are coerced to a <code>factor</code>. The length of <code>y</code> should match the number
of rows in <code>x</code>.
</p>
<p>The vector <code>prior</code> contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is <code>NULL</code> (default), the class membership
probabilities are estimated as the sample proportion of observations
belonging to each class. Otherwise, <code>prior</code> should be a vector with the
same length as the number of classes in <code>y</code>. The <code>prior</code>
probabilities should be nonnegative and sum to one. The order of the prior
probabilities is assumed to match the levels of <code>factor(y)</code>.
</p>


<h3>Value</h3>

<p><code>rda_high_dim</code> object that contains the trained HDRDA classifier
</p>
<p>list with predicted class and discriminant scores for each of the K
classes
</p>


<h3>References</h3>

<p>Ramey, J. A., Stein, C. K., and Young, D. M. (2017),
&quot;High-Dimensional Regularized Discriminant Analysis.&quot;
<a href="https://arxiv.org/abs/1602.01182">https://arxiv.org/abs/1602.01182</a>.
</p>
<p>Friedman, J. H. (1989), &quot;Regularized Discriminant Analysis,&quot;
Journal of American Statistical Association, 84, 405, 165-175.
<a href="http://www.jstor.org/stable/2289860">http://www.jstor.org/stable/2289860</a> (Requires full-text access).
</p>

<hr>
<h2 id='rda_high_dim_cv'>Helper function to optimize the HDRDA classifier via cross-validation</h2><span id='topic+rda_high_dim_cv'></span>

<h3>Description</h3>

<p>For a given data set, we apply cross-validation (cv) to select the optimal
HDRDA tuning parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rda_high_dim_cv(
  x,
  y,
  num_folds = 10,
  num_lambda = 21,
  num_gamma = 8,
  shrinkage_type = c("ridge", "convex"),
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rda_high_dim_cv_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="rda_high_dim_cv_+3A_y">y</code></td>
<td>
<p>vector of class labels for each training observation</p>
</td></tr>
<tr><td><code id="rda_high_dim_cv_+3A_num_folds">num_folds</code></td>
<td>
<p>the number of cross-validation folds.</p>
</td></tr>
<tr><td><code id="rda_high_dim_cv_+3A_num_lambda">num_lambda</code></td>
<td>
<p>The number of values of <code>lambda</code> to consider</p>
</td></tr>
<tr><td><code id="rda_high_dim_cv_+3A_num_gamma">num_gamma</code></td>
<td>
<p>The number of values of <code>gamma</code> to consider</p>
</td></tr>
<tr><td><code id="rda_high_dim_cv_+3A_shrinkage_type">shrinkage_type</code></td>
<td>
<p>the type of covariance-matrix shrinkage to apply. By
default, a ridge-like shrinkage is applied. If <code>convex</code> is given, then
shrinkage similar to Friedman (1989) is applied. See Ramey et al. (2017) for
details.</p>
</td></tr>
<tr><td><code id="rda_high_dim_cv_+3A_verbose">verbose</code></td>
<td>
<p>If set to <code>TRUE</code>, summary information will be outputted
as the optimal model is being determined.</p>
</td></tr>
<tr><td><code id="rda_high_dim_cv_+3A_...">...</code></td>
<td>
<p>Options passed to <code><a href="#topic+rda_high_dim">rda_high_dim()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of cross-validation folds is given in <code>num_folds</code>.
</p>


<h3>Value</h3>

<p>list containing the HDRDA model that minimizes cross-validation as
well as a <code>data.frame</code> that summarizes the cross-validation results.
</p>

<hr>
<h2 id='rda_weights'>Computes the observation weights for each class for the HDRDA classifier</h2><span id='topic+rda_weights'></span>

<h3>Description</h3>

<p>This function calculates the weight for each observation in the data matrix
<code>x</code> in order to calculate the covariance matrices employed in the HDRDA
classifier, implemented in <code><a href="#topic+rda_high_dim">rda_high_dim()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rda_weights(x, y, lambda = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rda_weights_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="rda_weights_+3A_y">y</code></td>
<td>
<p>vector of class labels for each training observation</p>
</td></tr>
<tr><td><code id="rda_weights_+3A_lambda">lambda</code></td>
<td>
<p>the RDA pooling parameter. Must be between 0 and 1, inclusively.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list containing the observations for each class given in <code>y</code>
</p>


<h3>References</h3>

<p>Ramey, J. A., Stein, C. K., and Young, D. M. (2013),
&quot;High-Dimensional Regularized Discriminant Analysis.&quot;
</p>

<hr>
<h2 id='regdiscrim_estimates'>Computes estimates and ancillary information for regularized discriminant
classifiers</h2><span id='topic+regdiscrim_estimates'></span>

<h3>Description</h3>

<p>Computes the maximum likelihood estimators (MLEs) for each class under the
assumption of multivariate normality for each class. Also, computes ancillary
information necessary for classifier summary, such as sample size, the number
of features, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regdiscrim_estimates(x, y, cov = TRUE, prior = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regdiscrim_estimates_+3A_x">x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td></tr>
<tr><td><code id="regdiscrim_estimates_+3A_y">y</code></td>
<td>
<p>vector of class labels for each training observation</p>
</td></tr>
<tr><td><code id="regdiscrim_estimates_+3A_cov">cov</code></td>
<td>
<p>logical. Should the sample covariance matrices be computed?
(Default: yes)</p>
</td></tr>
<tr><td><code id="regdiscrim_estimates_+3A_prior">prior</code></td>
<td>
<p>vector with prior probabilities for each class. If NULL
(default), then the sample proportions are used. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the common estimates and ancillary information used in
all of the regularized discriminant classifiers in the <code>sparsediscrim</code>
package.
</p>
<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p>named list with estimators for each class and necessary ancillary
information
</p>

<hr>
<h2 id='risk_stein'>Stein Risk function from Pang et al. (2009).</h2><span id='topic+risk_stein'></span>

<h3>Description</h3>

<p>This function finds the value for <code class="reqn">\alpha \in [0,1]</code> that empirically
minimizes the average risk under a Stein loss function, which is given on
page 1023 of Pang et al. (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>risk_stein(N, K, var_feature, num_alphas = 101, t = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="risk_stein_+3A_n">N</code></td>
<td>
<p>the sample size.</p>
</td></tr>
<tr><td><code id="risk_stein_+3A_k">K</code></td>
<td>
<p>the number of classes.</p>
</td></tr>
<tr><td><code id="risk_stein_+3A_var_feature">var_feature</code></td>
<td>
<p>a vector of the sample variances for each dimension.</p>
</td></tr>
<tr><td><code id="risk_stein_+3A_num_alphas">num_alphas</code></td>
<td>
<p>The number of values used to find the optimal amount of
shrinkage.</p>
</td></tr>
<tr><td><code id="risk_stein_+3A_t">t</code></td>
<td>
<p>a constant specified by the user that indicates the exponent to use
with the variance estimator. By default, t = -1 as in Pang et al. See the
paper for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with
</p>

<ul>
<li> <p><code>alpha</code>: the alpha that minimizes the average risk under a Stein
loss function. If the minimum is not unique, we randomly select an
<code>alpha</code> from the minimizers.
</p>
</li>
<li> <p><code>risk</code>: the minimum average risk attained.
</p>
</li></ul>



<h3>References</h3>

<p>Pang, H., Tong, T., &amp; Zhao, H. (2009). &quot;Shrinkage-based Diagonal
Discriminant Analysis and Its Applications in High-Dimensional Data,&quot;
Biometrics, 65, 4, 1021-1029.
<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01200.x">https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01200.x</a>
</p>

<hr>
<h2 id='solve_chol'>Computes the inverse of a symmetric, positive-definite matrix using the
Cholesky decomposition</h2><span id='topic+solve_chol'></span>

<h3>Description</h3>

<p>This often faster than <code><a href="Matrix.html#topic+solve">solve()</a></code> for larger matrices.
See, for example:
<a href="http://blog.phytools.org/2012/12/faster-inversion-of-square-symmetric.html">http://blog.phytools.org/2012/12/faster-inversion-of-square-symmetric.html</a>
and
<a href="https://stats.stackexchange.com/questions/14951/efficient-calculation-of-matrix-inverse-in-r">https://stats.stackexchange.com/questions/14951/efficient-calculation-of-matrix-inverse-in-r</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solve_chol(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve_chol_+3A_x">x</code></td>
<td>
<p>symmetric, positive-definite matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the inverse of <code>x</code>
</p>

<hr>
<h2 id='tong_mean_shrinkage'>Tong et al. (2012)'s Lindley-type Shrunken Mean Estimator</h2><span id='topic+tong_mean_shrinkage'></span>

<h3>Description</h3>

<p>An implementation of the Lindley-type shrunken mean estimator utilized in
shrinkage-mean-based diagonal linear discriminant analysis (SmDLDA).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tong_mean_shrinkage(x, r_opt = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tong_mean_shrinkage_+3A_x">x</code></td>
<td>
<p>a matrix with <code>n</code> rows and <code>p</code> columns.</p>
</td></tr>
<tr><td><code id="tong_mean_shrinkage_+3A_r_opt">r_opt</code></td>
<td>
<p>the shrinkage coefficient. If <code>NULL</code> (default), we calculate
the shrinkage coefficient with the formula given just above Equation 5 on page
533 and denoted by <code class="reqn">\hat{r}_{opt}</code>. We allow the user to specify an
alternative value to investigate better approximations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of length <code>p</code> with the shrunken mean estimator
</p>


<h3>References</h3>

<p>Tong, T., Chen, L., and Zhao, H. (2012), &quot;Improved Mean
Estimation and Its Application to Diagonal Discriminant Analysis,&quot;
Bioinformatics, 28, 4, 531-537.
<a href="https://academic.oup.com/bioinformatics/article/28/4/531/211887">https://academic.oup.com/bioinformatics/article/28/4/531/211887</a>
</p>

<hr>
<h2 id='two_class_sim_data'>Example bivariate classification data from caret</h2><span id='topic+two_class_sim_data'></span>

<h3>Description</h3>

<p>Example bivariate classification data from caret
</p>


<h3>Details</h3>

<p>These data were generated using by invoking the <code>twoClassSim()</code>
function in the <code>caret</code> package.
</p>


<h3>Value</h3>

<table>
<tr><td><code>two_class_sim_data</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(two_class_sim_data)
</code></pre>

<hr>
<h2 id='update_rda_high_dim'>Helper function to update tuning parameters for the HDRDA classifier</h2><span id='topic+update_rda_high_dim'></span>

<h3>Description</h3>

<p>This function updates some of the quantities in the HDRDA classifier based on
updated values of <code>lambda</code> and <code>gamma</code>. The update can greatly
expedite cross-validation to examine a large grid of values for <code>lambda</code>
and <code>gamma</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_rda_high_dim(obj, lambda = 1, gamma = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_rda_high_dim_+3A_obj">obj</code></td>
<td>
<p>a <code>rda_high_dim</code> object</p>
</td></tr>
<tr><td><code id="update_rda_high_dim_+3A_lambda">lambda</code></td>
<td>
<p>a numeric value between 0 and 1, inclusively</p>
</td></tr>
<tr><td><code id="update_rda_high_dim_+3A_gamma">gamma</code></td>
<td>
<p>a numeric value (nonnegative)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>rda_high_dim</code> object with updated estimates
</p>

<hr>
<h2 id='var_shrinkage'>Shrinkage-based estimator of variances for each feature from Pang et al.
(2009).</h2><span id='topic+var_shrinkage'></span>

<h3>Description</h3>

<p>This function computes the shrinkage-based estimator of variance of each
feature (variable) from Pang et al. (2009) for the SDLDA classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_shrinkage(N, K, var_feature, num_alphas = 101, t = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_shrinkage_+3A_n">N</code></td>
<td>
<p>the sample size.</p>
</td></tr>
<tr><td><code id="var_shrinkage_+3A_k">K</code></td>
<td>
<p>the number of classes.</p>
</td></tr>
<tr><td><code id="var_shrinkage_+3A_var_feature">var_feature</code></td>
<td>
<p>a vector of the sample variances for each feature.</p>
</td></tr>
<tr><td><code id="var_shrinkage_+3A_num_alphas">num_alphas</code></td>
<td>
<p>The number of values used to find the optimal amount of
shrinkage.</p>
</td></tr>
<tr><td><code id="var_shrinkage_+3A_t">t</code></td>
<td>
<p>a constant specified by the user that indicates the exponent to use
with the variance estimator. By default, t = -1 as in Pang et al. See the
paper for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of the shrunken variances for each feature.
</p>


<h3>References</h3>

<p>Pang, H., Tong, T., &amp; Zhao, H. (2009). &quot;Shrinkage-based Diagonal
Discriminant Analysis and Its Applications in High-Dimensional Data,&quot;
Biometrics, 65, 4, 1021-1029.
<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01200.x">https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01200.x</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
