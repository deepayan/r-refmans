<!DOCTYPE html><html><head><title>Help for package sgs</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sgs}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#arma_mv'><p>Matrix Product in RcppArmadillo.</p></a></li>
<li><a href='#as_sgs'><p>fits the adaptively scaled SGS model (AS-SGS)</p></a></li>
<li><a href='#atos'><p>adaptive three operator splitting (ATOS)</p></a></li>
<li><a href='#fit_sgs'><p>fit an SGS model</p></a></li>
<li><a href='#fit_sgs_cv'><p>fit an SGS model using CV</p></a></li>
<li><a href='#generate_penalties'><p>generate penalty sequences for SGS</p></a></li>
<li><a href='#generate_toy_data'><p>generate toy data</p></a></li>
<li><a href='#plot.sgs_cv'><p>plot a <code>"sgs_cv"</code> object</p></a></li>
<li><a href='#predict.sgs'><p>predict using a <code>"sgs"</code> object</p></a></li>
<li><a href='#print.sgs'><p>print a <code>"sgs"</code> object</p></a></li>
<li><a href='#scaled_sgs'><p>fits a scaled SGS model</p></a></li>
<li><a href='#sgs-package'><p>sgs: Sparse-Group SLOPE: Adaptive Bi-Level Selection with FDR Control</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Sparse-Group SLOPE: Adaptive Bi-Level Selection with FDR Control</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-21</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Fabio Feser &lt;ff120@ic.ac.uk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of Sparse-group SLOPE: Adaptive bi-level with FDR-control (Feser et al. (2023) &lt;<a href="https://arxiv.org/abs/2305.09467">arXiv:2305.09467</a>&gt;). Linear and logistic regression models are supported, both of which can be fit using k-fold cross-validation. Dense and sparse input matrices are supported. In addition, a general adaptive three operator splitting (ATOS) implementation is provided.</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, MASS, caret, grDevices, graphics, methods, stats,
faux, SLOPE, Rlab, Rcpp (&ge; 1.0.10)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>SGL, gglasso, glmnet, testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ff1201/sgs">https://github.com/ff1201/sgs</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ff1201/sgs/issues">https://github.com/ff1201/sgs/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-22 00:27:08 UTC; ff120</td>
</tr>
<tr>
<td>Author:</td>
<td>Fabio Feser <a href="https://orcid.org/0009-0007-3088-9727"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Marina Evangelou <a href="https://orcid.org/0000-0003-0789-8944"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-22 15:50:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='arma_mv'>Matrix Product in RcppArmadillo.</h2><span id='topic+arma_mv'></span>

<h3>Description</h3>

<p>Matrix Product in RcppArmadillo.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arma_mv(m, v)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arma_mv_+3A_m">m</code></td>
<td>
<p>numeric matrix</p>
</td></tr>
<tr><td><code id="arma_mv_+3A_v">v</code></td>
<td>
<p>numeric vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix product of m and v
</p>

<hr>
<h2 id='as_sgs'>fits the adaptively scaled SGS model (AS-SGS)</h2><span id='topic+as_sgs'></span>

<h3>Description</h3>

<p>Fits an SGS model using the noise estimation procedure, termed adaptively scaled SGS (Algorithm 2 from Feser et al (2023)).
This adaptively estimates <code class="reqn">\lambda</code> and then fits the model using the estimated value. It is an alternative approach to
cross-validation (<code><a href="#topic+fit_sgs_cv">fit_sgs_cv()</a></code>). The approach is only compatible with the SGS penalties.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_sgs(
  X,
  y,
  groups,
  type = "linear",
  pen_method = 2,
  alpha = 0.95,
  vFDR = 0.1,
  gFDR = 0.1,
  standardise = "l2",
  intercept = TRUE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_sgs_+3A_x">X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package).</p>
</td></tr>
<tr><td><code id="as_sgs_+3A_y">y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> should be continuous and for <code>type="logistic"</code> should be a binary variable.</p>
</td></tr>
<tr><td><code id="as_sgs_+3A_groups">groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td></tr>
<tr><td><code id="as_sgs_+3A_type">type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td></tr>
<tr><td><code id="as_sgs_+3A_pen_method">pen_method</code></td>
<td>
<p>The type of penalty sequences to use.
</p>

<ul>
<li> <p><code>"1"</code> uses the vMean and gMean SGS sequences.
</p>
</li>
<li> <p><code>"2"</code> uses the vMax and gMax SGS sequences.
</p>
</li></ul>
</td></tr>
<tr><td><code id="as_sgs_+3A_alpha">alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, which defines the convex balance between SLOPE and gSLOPE. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="as_sgs_+3A_vfdr">vFDR</code></td>
<td>
<p>Defines the desired variable false discovery rate (FDR) level, which determines the shape of the variable penalties. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="as_sgs_+3A_gfdr">gFDR</code></td>
<td>
<p>Defines the desired group false discovery rate (FDR) level, which determines the shape of the group penalties. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="as_sgs_+3A_standardise">standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li></ul>
</td></tr>
<tr><td><code id="as_sgs_+3A_intercept">intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td></tr>
<tr><td><code id="as_sgs_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>"sgs"</code> containing model fit information (see <code><a href="#topic+fit_sgs">fit_sgs()</a></code>).
</p>


<h3>References</h3>

<p>F. Feser, M. Evangelou <em>Sparse-group SLOPE: adaptive bi-level selection with FDR-control</em>, <a href="https://arxiv.org/abs/2305.09467">https://arxiv.org/abs/2305.09467</a>
</p>

<hr>
<h2 id='atos'>adaptive three operator splitting (ATOS)</h2><span id='topic+atos'></span>

<h3>Description</h3>

<p>Function for fitting adaptive three operator splitting (ATOS) with general convex penalties. Supports both linear and logistic regression, both with dense and sparse matrix implementations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>atos(
  X,
  y,
  type = "linear",
  prox_1,
  prox_2,
  pen_prox_1 = 0.5,
  pen_prox_2 = 0.5,
  max_iter = 5000,
  backtracking = 0.7,
  max_iter_backtracking = 100,
  tol = 1e-05,
  prox_1_opts = NULL,
  prox_2_opts = NULL,
  standardise = "l2",
  intercept = TRUE,
  x0 = NULL,
  u = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="atos_+3A_x">X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package)</p>
</td></tr>
<tr><td><code id="atos_+3A_y">y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> needs to be continuous and for <code>type="logistic"</code> needs to be a binary variable.</p>
</td></tr>
<tr><td><code id="atos_+3A_type">type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td></tr>
<tr><td><code id="atos_+3A_prox_1">prox_1</code></td>
<td>
<p>The proximal operator for the first function, <code class="reqn">h(x)</code>.</p>
</td></tr>
<tr><td><code id="atos_+3A_prox_2">prox_2</code></td>
<td>
<p>The proximal operator for the second function, <code class="reqn">g(x)</code>.</p>
</td></tr>
<tr><td><code id="atos_+3A_pen_prox_1">pen_prox_1</code></td>
<td>
<p>The penalty for the first proximal operator. For the lasso, this would be the sparsity parameter, <code class="reqn">\lambda</code>. If operator does not include a penalty, set to 1.</p>
</td></tr>
<tr><td><code id="atos_+3A_pen_prox_2">pen_prox_2</code></td>
<td>
<p>The penalty for the second proximal operator.</p>
</td></tr>
<tr><td><code id="atos_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of ATOS iterations to perform.</p>
</td></tr>
<tr><td><code id="atos_+3A_backtracking">backtracking</code></td>
<td>
<p>The backtracking parameter, <code class="reqn">\tau</code>, as defined in Pedregosa et. al. (2018).</p>
</td></tr>
<tr><td><code id="atos_+3A_max_iter_backtracking">max_iter_backtracking</code></td>
<td>
<p>Maximum number of backtracking line search iterations to perform per global iteration.</p>
</td></tr>
<tr><td><code id="atos_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for the stopping criteria.</p>
</td></tr>
<tr><td><code id="atos_+3A_prox_1_opts">prox_1_opts</code></td>
<td>
<p>Optional argument for first proximal operator. For the group lasso, this would be the group IDs. Note: this must be inserted as a list.</p>
</td></tr>
<tr><td><code id="atos_+3A_prox_2_opts">prox_2_opts</code></td>
<td>
<p>Optional argument for second proximal operator.</p>
</td></tr>
<tr><td><code id="atos_+3A_standardise">standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li></ul>
</td></tr>
<tr><td><code id="atos_+3A_intercept">intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td></tr>
<tr><td><code id="atos_+3A_x0">x0</code></td>
<td>
<p>Optional initial vector for <code class="reqn">x_0</code>.</p>
</td></tr>
<tr><td><code id="atos_+3A_u">u</code></td>
<td>
<p>Optional initial vector for <code class="reqn">u</code>.</p>
</td></tr>
<tr><td><code id="atos_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>atos()</code> solves convex minimization problems of the form
</p>
<p style="text-align: center;"><code class="reqn">
  f(x) + g(x) + h(x),
</code>
</p>

<p>where <code class="reqn">f</code> is convex and differentiable with <code class="reqn">L_f</code>-Lipschitz gradient, and <code class="reqn">g</code> and <code class="reqn">h</code> are both convex.
The algorithm is not symmetrical, but usually the difference between variations are only small numerical values, which are filtered out.
However, both variations should be checked regardless, by looking at <code>x</code> and <code>u</code>. An example for the sparse-group lasso (SGL) is given.
</p>


<h3>Value</h3>

<p>An object of class <code>"atos"</code> containing:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>The fitted values from the regression. Taken to be the more stable fit between <code>x</code> and <code>u</code>, which is usually the former.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The solution to the original problem (see Pedregosa et. al. (2018)).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>The solution to the dual problem (see Pedregosa et. al. (2018)).</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>The updated values from applying the first proximal operator (see Pedregosa et. al. (2018)).</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Indicates which type of regression was performed.</p>
</td></tr>
<tr><td><code>success</code></td>
<td>
<p>Logical flag indicating whether ATOS converged, according to <code>tol</code>.</p>
</td></tr>
<tr><td><code>num_it</code></td>
<td>
<p>Number of iterations performed. If convergence is not reached, this will be <code>max_iter</code>.</p>
</td></tr>
<tr><td><code>certificate</code></td>
<td>
<p>Final value of convergence criteria.</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>Logical flag indicating whether an intercept was fit.</p>
</td></tr>
</table>


<h3>References</h3>

<p>F. Pedregosa, G. Gidel (2018) <em>Adaptive Three Operator Splitting</em>, <a href="https://proceedings.mlr.press/v80/pedregosa18a.html">https://proceedings.mlr.press/v80/pedregosa18a.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(rep(1:20, each=3),
          rep(21:40, each=4),
          rep(41:60, each=5),
          rep(61:80, each=6),
          rep(81:100, each=7))
# define proximal operators
L1_prox &lt;- function(input, lambda){ # Lasso proximal operator
 out = sign(input) * pmax(0, abs(input) - lambda)
 return(out)
}
group_L1_prox = function(input,lambda,group_info){ 
 n_groups = length(unique(group_info))
 out = rep(0,length(input))
 for (i in 1:n_groups){
   grp_idx = which(group_info == unique(group_info)[i])
   if (lambda == 0 &amp; norm(input[grp_idx],type="2") == 0){ # 0/0 = 0
     out[grp_idx] = 0
   } else {
     out[grp_idx] = max((1-(lambda/norm(input[grp_idx],type="2"))),0) * input[grp_idx]}
 }
 return(out)
}
# generate data
data = generate_toy_data(p=500, n=400, groups = groups, seed_id=3)
# run atos (the proximal functions can be found in utils.R)
out = atos(X=data$X, y=data$y, type="linear", prox_1 = L1_prox, prox_2 = group_L1_prox, 
standardise="none", intercept=FALSE, prox_2_opts = list(groups))
</code></pre>

<hr>
<h2 id='fit_sgs'>fit an SGS model</h2><span id='topic+fit_sgs'></span>

<h3>Description</h3>

<p>Sparse-group SLOPE (SGS) main fitting function. Supports both linear and logistic regression, both with dense and sparse matrix implementations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_sgs(
  X,
  y,
  groups,
  pen_method = 1,
  type = "linear",
  lambda,
  alpha = 0.95,
  vFDR = 0.1,
  gFDR = 0.1,
  max_iter = 5000,
  backtracking = 0.7,
  max_iter_backtracking = 100,
  tol = 1e-05,
  standardise = "l2",
  intercept = TRUE,
  w_weights = NULL,
  v_weights = NULL,
  x0 = NULL,
  u = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_sgs_+3A_x">X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package).</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_y">y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> should be continuous and for <code>type="logistic"</code> should be a binary variable.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_groups">groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_pen_method">pen_method</code></td>
<td>
<p>The type of penalty sequences to use (see Feser et al. (2023)):
</p>

<ul>
<li> <p><code>"1"</code> uses the vMean SGS and gMean gSLOPE sequences.
</p>
</li>
<li> <p><code>"2"</code> uses the vMax SGS and gMean gSLOPE sequences.
</p>
</li>
<li> <p><code>"3"</code> uses the BH SLOPE and gMean gSLOPE sequences, also known as SGS Original.
</p>
</li></ul>
</td></tr>
<tr><td><code id="fit_sgs_+3A_type">type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_lambda">lambda</code></td>
<td>
<p>The value of <code class="reqn">\lambda</code>, which defines the level of sparsity in the model. Can be picked using cross-validation (see <code><a href="#topic+fit_sgs_cv">fit_sgs_cv()</a></code>). Must be a positive value.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_alpha">alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, which defines the convex balance between SLOPE and gSLOPE. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_vfdr">vFDR</code></td>
<td>
<p>Defines the desired variable false discovery rate (FDR) level, which determines the shape of the variable penalties. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_gfdr">gFDR</code></td>
<td>
<p>Defines the desired group false discovery rate (FDR) level, which determines the shape of the group penalties. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of ATOS iterations to perform.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_backtracking">backtracking</code></td>
<td>
<p>The backtracking parameter, <code class="reqn">\tau</code>, as defined in Pedregosa et. al. (2018).</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_max_iter_backtracking">max_iter_backtracking</code></td>
<td>
<p>Maximum number of backtracking line search iterations to perform per global iteration.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for the stopping criteria.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_standardise">standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li></ul>
</td></tr>
<tr><td><code id="fit_sgs_+3A_intercept">intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_w_weights">w_weights</code></td>
<td>
<p>Optional vector for the group penalty weights. Overrides the penalties from <code>pen_method</code> if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">1-\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code>.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_v_weights">v_weights</code></td>
<td>
<p>Optional vector for the variable penalty weights. Overrides the penalties from <code>pen_method</code> if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code>.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_x0">x0</code></td>
<td>
<p>Optional initial vector for <code class="reqn">x_0</code>.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_u">u</code></td>
<td>
<p>Optional initial vector for <code class="reqn">u</code>.</p>
</td></tr>
<tr><td><code id="fit_sgs_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fit_sgs()</code> fits an SGS model using adaptive three operator splitting (ATOS). SGS is a sparse-group method, so that it selects both variables and groups. Unlike group selection approaches, not every variable within a group is set as active.
It solves the convex optimisation problem given by
</p>
<p style="text-align: center;"><code class="reqn">
  \frac{1}{2n} f(b ; y, \mathbf{X}) + \lambda \alpha \sum_{i=1}^{p}v_i |b|_{(i)} + \lambda (1-\alpha)\sum_{g=1}^{m}w_g \sqrt{p_g} \|b^{(g)}\|_2,
</code>
</p>

<p>where <code class="reqn">f(\cdot)</code> is the loss function. In the case of the linear model, the loss function is given by the mean-squared error loss:
</p>
<p style="text-align: center;"><code class="reqn">
 f(b; y, \mathbf{X}) = \left\|y-\mathbf{X}b \right\|_2^2.
</code>
</p>

<p>In the logistic model, the loss function is given by
</p>
<p style="text-align: center;"><code class="reqn">
f(b;y,\mathbf{X})=-1/n \log(\mathcal{L}(b; y, \mathbf{X})).
</code>
</p>

<p>where the log-likelihood is given by
</p>
<p style="text-align: center;"><code class="reqn">
 \mathcal{L}(b; y, \mathbf{X}) = \sum_{i=1}^{n}\left\{y_i b^\intercal x_i - \log(1+\exp(b^\intercal x_i)) \right\}.
</code>
</p>

<p>SGS can be seen to be a convex combination of SLOPE and gSLOPE, balanced through <code>alpha</code>, such that it reduces to SLOPE for <code>alpha = 0</code> and to gSLOPE for <code>alpha = 1</code>.
The penalty parameters in SGS are sorted so that the largest coefficients are matched with the largest penalties, to reduce the FDR.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>The fitted values from the regression. Taken to be the more stable fit between <code>x</code> and <code>u</code>, which is usually the former.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The solution to the original problem (see Pedregosa et. al. (2018)).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>The solution to the dual problem (see Pedregosa et. al. (2018)).</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>The updated values from applying the first proximal operator (see Pedregosa et. al. (2018)).</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Indicates which type of regression was performed.</p>
</td></tr>
<tr><td><code>pen_slope</code></td>
<td>
<p>Vector of the variable penalty sequence.</p>
</td></tr>
<tr><td><code>pen_gslope</code></td>
<td>
<p>Vector of the group penalty sequence.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Value of <code class="reqn">\lambda</code> used to fit the model.</p>
</td></tr>
<tr><td><code>success</code></td>
<td>
<p>Logical flag indicating whether ATOS converged, according to <code>tol</code>.</p>
</td></tr>
<tr><td><code>num_it</code></td>
<td>
<p>Number of iterations performed. If convergence is not reached, this will be <code>max_iter</code>.</p>
</td></tr>
<tr><td><code>certificate</code></td>
<td>
<p>Final value of convergence criteria.</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>Logical flag indicating whether an intercept was fit.</p>
</td></tr>
</table>


<h3>References</h3>

<p>F. Feser, M. Evangelou <em>Sparse-group SLOPE: adaptive bi-level selection with FDR-control</em>, <a href="https://arxiv.org/abs/2305.09467">https://arxiv.org/abs/2305.09467</a>
</p>
<p>F. Pedregosa, G. Gidel (2018) <em>Adaptive Three Operator Splitting</em>, <a href="https://proceedings.mlr.press/v80/pedregosa18a.html">https://proceedings.mlr.press/v80/pedregosa18a.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,1,2,2,3,3,3,4,4)
# generate data
data = generate_toy_data(p=10, n=5, groups = groups, seed_id=3,group_sparsity=1)
# run SGS 
model = fit_sgs(X = data$X, y = data$y, groups = groups, type="linear", lambda = 1, alpha=0.95, 
vFDR=0.1, gFDR=0.1, standardise = "l2", intercept = TRUE, verbose=FALSE)
</code></pre>

<hr>
<h2 id='fit_sgs_cv'>fit an SGS model using CV</h2><span id='topic+fit_sgs_cv'></span>

<h3>Description</h3>

<p>Function to fit a pathwise solution of sparse-group SLOPE (SGS) models using k-fold cross-validation. Supports both linear and logistic regression, both with dense and sparse matrix implementations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_sgs_cv(
  X,
  y,
  groups,
  pen_method = 1,
  type = "linear",
  nlambda = 20,
  nfolds = 10,
  alpha = 0.95,
  vFDR = 0.1,
  gFDR = 0.1,
  backtracking = 0.7,
  max_iter = 5000,
  max_iter_backtracking = 100,
  tol = 1e-05,
  min_frac = 0.05,
  standardise = "l2",
  intercept = TRUE,
  verbose = FALSE,
  v_weights = NULL,
  w_weights = NULL,
  error_criteria = "mse",
  max_lambda = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_sgs_cv_+3A_x">X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package).</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_y">y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> should be continuous and for <code>type="logistic"</code> should be a binary variable.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_groups">groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_pen_method">pen_method</code></td>
<td>
<p>The type of penalty sequences to use (see Feser et al. (2023)):
</p>

<ul>
<li> <p><code>"1"</code> uses the vMean SGS and gMean gSLOPE sequences.
</p>
</li>
<li> <p><code>"2"</code> uses the vMax SGS and gMean gSLOPE sequences.
</p>
</li>
<li> <p><code>"3"</code> uses the BH SLOPE and gMean gSLOPE sequences, also known as SGS Original.
</p>
</li></ul>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_type">type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of pathwise <code class="reqn">\lambda</code> values to fit.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds to use in cross-validation.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_alpha">alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, which defines the convex balance between SLOPE and gSLOPE. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_vfdr">vFDR</code></td>
<td>
<p>Defines the desired variable false discovery rate (FDR) level, which determines the shape of the variable penalties. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_gfdr">gFDR</code></td>
<td>
<p>Defines the desired group false discovery rate (FDR) level, which determines the shape of the group penalties. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_backtracking">backtracking</code></td>
<td>
<p>The backtracking parameter, <code class="reqn">\tau</code>, as defined in Pedregosa et. al. (2018).</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of ATOS iterations to perform.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_max_iter_backtracking">max_iter_backtracking</code></td>
<td>
<p>Maximum number of backtracking line search iterations to perform per global iteration.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for the stopping criteria.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_min_frac">min_frac</code></td>
<td>
<p>Defines the termination point of the pathwise solution, so that <code class="reqn">\lambda_\text{min} = min_frac \cdot \lambda_\text{max}</code>.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_standardise">standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li></ul>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_intercept">intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_v_weights">v_weights</code></td>
<td>
<p>Optional vector for the variable penalty weights. Overrides the penalties from pen_method if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code></p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_w_weights">w_weights</code></td>
<td>
<p>Optional vector for the group penalty weights. Overrides the penalties from pen_method if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">1-\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code></p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_error_criteria">error_criteria</code></td>
<td>
<p>The criteria used to discriminate between models along the path. Supported values are: <code>"mse"</code> (mean squared error) and <code>"mae"</code> (mean absolute error).</p>
</td></tr>
<tr><td><code id="fit_sgs_cv_+3A_max_lambda">max_lambda</code></td>
<td>
<p>Optional parameter, <code class="reqn">\lambda_\text{max}</code>, which is used to fit the first model on the path. If not specificed, it is chosen to be just above the value which lets in the first variable (so that it is the null model).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fits SGS models under a pathwise solution using adaptive three operator splitting (ATOS), picking the 1se model as optimum. Warm starts are implemented.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>all_models</code></td>
<td>
<p>A list of all the models fitted along the path.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>The 1se chosen model, which is a <code>"sgs"</code> object type.</p>
</td></tr>
<tr><td><code>best_lambda</code></td>
<td>
<p>The value of <code class="reqn">\lambda</code> which generated the chosen model.</p>
</td></tr>
<tr><td><code>best_lambda_id</code></td>
<td>
<p>The path index for the chosen model.</p>
</td></tr>
<tr><td><code>errors</code></td>
<td>
<p>A table containing fitting information about the models on the path.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Indicates which type of regression was performed.</p>
</td></tr>
</table>


<h3>References</h3>

<p>F. Feser, M. Evangelou <em>Sparse-group SLOPE: adaptive bi-level selection with FDR-control</em>, <a href="https://arxiv.org/abs/2305.09467">https://arxiv.org/abs/2305.09467</a>
</p>
<p>F. Pedregosa, G. Gidel (2018) <em>Adaptive Three Operator Splitting</em>, <a href="https://proceedings.mlr.press/v80/pedregosa18a.html">https://proceedings.mlr.press/v80/pedregosa18a.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,1,2,2,3,3,3,4,4)
# generate data
data = generate_toy_data(p=10, n=5, groups = groups, seed_id=3,group_sparsity=1)
# run SGS with cross-validation (the proximal functions can be found in utils.R)
cv_model = fit_sgs_cv(X = data$X, y = data$y, groups=groups, type = "linear", 
nlambda = 5, nfolds=10, alpha = 0.95, vFDR = 0.1, gFDR = 0.1, min_frac = 0.05, 
standardise="l2",intercept=TRUE,verbose=TRUE)
</code></pre>

<hr>
<h2 id='generate_penalties'>generate penalty sequences for SGS</h2><span id='topic+generate_penalties'></span>

<h3>Description</h3>

<p>Generates variable and group penalties for SGS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_penalties(gFDR, vFDR, pen_method, groups, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_penalties_+3A_gfdr">gFDR</code></td>
<td>
<p>Defines the desired group false discovery rate (FDR) level, which determines the shape of the group penalties.</p>
</td></tr>
<tr><td><code id="generate_penalties_+3A_vfdr">vFDR</code></td>
<td>
<p>Defines the desired variable false discovery rate (FDR) level, which determines the shape of the variable penalties.</p>
</td></tr>
<tr><td><code id="generate_penalties_+3A_pen_method">pen_method</code></td>
<td>
<p>The type of penalty sequences to use (see Feser et al. (2023)):
</p>

<ul>
<li> <p><code>"1"</code> uses the vMean SGS and gMean gSLOPE sequences.
</p>
</li>
<li> <p><code>"2"</code> uses the vMax SGS and gMean gSLOPE sequences.
</p>
</li>
<li> <p><code>"3"</code> uses the BH SLOPE and gMean gSLOPE sequences, also known as SGS Original.
</p>
</li></ul>
</td></tr>
<tr><td><code id="generate_penalties_+3A_groups">groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td></tr>
<tr><td><code id="generate_penalties_+3A_alpha">alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, defines the convex balance between SLOPE and gSLOPE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The vMean and vMax SGS sequences are variable sequences derived specifically to give variable false discovery rate (FDR) control for SGS under orthogonal designs (see Feser et al. (2023)).
The BH SLOPE sequence is derived in Bodgan et. al. (2015) and has links to the Benjamini-Hochberg critical values. The sequence provides variable FDR-control for SLOPE under orthogonal designs.
The gMean gSLOPE sequence is derived in Brzyski et. al. (2015) and provides group FDR-control for gSLOPE under orthogonal designs.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>pen_slope_org</code></td>
<td>
<p>A vector of the variable penalty sequence.</p>
</td></tr>
<tr><td><code>pen_gslope_org</code></td>
<td>
<p>A vector of the group penalty sequence.</p>
</td></tr>
</table>


<h3>References</h3>

<p>F. Feser, M. Evangelou <em>Sparse-group SLOPE: adaptive bi-level selection with FDR-control</em>, <a href="https://arxiv.org/abs/2305.09467">https://arxiv.org/abs/2305.09467</a>
</p>
<p>M. Bogdan, E. Van den Berg, C. Sabatti, W. Su, E. Candes (2015) <em>SLOPE â€” Adaptive variable selection via convex optimization</em>, <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/SLOPEAdaptive-variable-selection-via-convex-optimization/10.1214/15-AOAS842.full">https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/SLOPEAdaptive-variable-selection-via-convex-optimization/10.1214/15-AOAS842.full</a>
</p>
<p>D. Brzyski, W. Su, M. Bodgdan (2015) <em>Group SLOPE - adaptive selection of groups of predictors</em>, <a href="https://arxiv.org/abs/1511.09078">https://arxiv.org/abs/1511.09078</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(rep(1:20, each=3),
          rep(21:40, each=4),
          rep(41:60, each=5),
          rep(61:80, each=6),
          rep(81:100, each=7))
# generate sequences
sequences = generate_penalties(gFDR=0.1, vFDR=0.1, pen_method=1, groups=groups, alpha=0.5)

</code></pre>

<hr>
<h2 id='generate_toy_data'>generate toy data</h2><span id='topic+generate_toy_data'></span>

<h3>Description</h3>

<p>Generates different types of datasets, which can then be fitted using sparse-group SLOPE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_toy_data(
  p,
  n,
  rho = 0,
  seed_id = 2,
  grouped = TRUE,
  groups,
  noise_level = 1,
  group_sparsity = 0.1,
  var_sparsity = 0.5,
  orthogonal = FALSE,
  data_mean = 0,
  data_sd = 1,
  signal_mean = 0,
  signal_sd = sqrt(10)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_toy_data_+3A_p">p</code></td>
<td>
<p>The number of input variables.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_n">n</code></td>
<td>
<p>The number of observations.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_rho">rho</code></td>
<td>
<p>Correlation coefficient. Must be in range <code class="reqn">[0,1]</code>.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_seed_id">seed_id</code></td>
<td>
<p>Seed to be used to generate the data matrix <code class="reqn">X</code>.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_grouped">grouped</code></td>
<td>
<p>A logical flag indicating whether grouped data is required.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_groups">groups</code></td>
<td>
<p>If itemgrouped=TRUE, the grouping structure is required. Each input variable should have a group id.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_noise_level">noise_level</code></td>
<td>
<p>Defines the level of noise (<code class="reqn">sigma</code>) to be used in generating the response vector <code class="reqn">y</code>.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_group_sparsity">group_sparsity</code></td>
<td>
<p>Defines the level of group sparsity. Must be in the range <code class="reqn">[0,1]</code>.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_var_sparsity">var_sparsity</code></td>
<td>
<p>Defines the level of variable sparsity. Must be in the range <code class="reqn">[0,1]</code>. If <code>grouped=TRUE</code>, this defines the level of sparsity within each group, not globally.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_orthogonal">orthogonal</code></td>
<td>
<p>Logical flag as to whether the input matrix should be orthogonal.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_data_mean">data_mean</code></td>
<td>
<p>Defines the mean of input predictors.</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_data_sd">data_sd</code></td>
<td>
<p>Defines the standard deviation of the signal (<code class="reqn">beta</code>).</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_signal_mean">signal_mean</code></td>
<td>
<p>Defines the mean of the signal (<code class="reqn">beta</code>).</p>
</td></tr>
<tr><td><code id="generate_toy_data_+3A_signal_sd">signal_sd</code></td>
<td>
<p>Defines the standard deviation of the signal (<code class="reqn">beta</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data is generated under a Gaussian linear model. The generated data can be grouped and sparsity can be provided at both a group and/or variable level.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>y</code></td>
<td>
<p>The response vector.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>The input matrix.</p>
</td></tr>
<tr><td><code>true_beta</code></td>
<td>
<p>The true values of <code class="reqn">beta</code> used to generate the response.</p>
</td></tr>
<tr><td><code>true_grp_id</code></td>
<td>
<p>Indices of which groups are non-zero in itemtrue_beta.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(rep(1:20, each=3),
          rep(21:40, each=4),
          rep(41:60, each=5),
          rep(61:80, each=6),
          rep(81:100, each=7))
# generate data
data = generate_toy_data(p=500, n=400, groups = groups, seed_id=3)

</code></pre>

<hr>
<h2 id='plot.sgs_cv'>plot a <code>"sgs_cv"</code> object</h2><span id='topic+plot.sgs_cv'></span>

<h3>Description</h3>

<p>Plots the pathwise solution of a cross-validation fit, from a call to <code><a href="#topic+fit_sgs_cv">fit_sgs_cv()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sgs_cv'
plot(x, how_many = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.sgs_cv_+3A_x">x</code></td>
<td>
<p>Object an object of class <code>"sgs_cv"</code> from a call to <code><a href="#topic+fit_sgs">fit_sgs()</a></code>.</p>
</td></tr>
<tr><td><code id="plot.sgs_cv_+3A_how_many">how_many</code></td>
<td>
<p>Defines how many predictors to plot. Plots the predictors in decreasing order of largest absolute value.</p>
</td></tr>
<tr><td><code id="plot.sgs_cv_+3A_...">...</code></td>
<td>
<p>further arguments passed to base function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>response</code></td>
<td>
<p>The predicted response. In the logistic case, this represents the predicted class probabilities.</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>The predicted class assignments. Only returned if type = &quot;logistic&quot; in the <code>"sgs"</code> object.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+fit_sgs_cv">fit_sgs_cv()</a></code>
</p>
<p>Other SGS-methods: 
<code><a href="#topic+predict.sgs">predict.sgs</a>()</code>,
<code><a href="#topic+print.sgs">print.sgs</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,2,2,3)
# generate data
data = generate_toy_data(p=5, n=4, groups = groups, seed_id=3,signal_mean=20,group_sparsity=1)
# run SGS 
cv_model = fit_sgs_cv(X = data$X, y = data$y, groups=groups, type = "linear", 
nlambda = 20, nfolds=10, alpha = 0.95, vFDR = 0.1, gFDR = 0.1, 
min_frac = 0.05, standardise="l2",intercept=TRUE,verbose=FALSE)
plot(cv_model, how_many = 10)
</code></pre>

<hr>
<h2 id='predict.sgs'>predict using a <code>"sgs"</code> object</h2><span id='topic+predict.sgs'></span>

<h3>Description</h3>

<p>Performs prediction from an <code><a href="#topic+fit_sgs">fit_sgs()</a></code> model fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sgs'
predict(object, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.sgs_+3A_object">object</code></td>
<td>
<p>an object of class <code>"sgs"</code> from a call to <code><a href="#topic+fit_sgs">fit_sgs()</a></code>.</p>
</td></tr>
<tr><td><code id="predict.sgs_+3A_x">x</code></td>
<td>
<p>Input data to use for prediction.</p>
</td></tr>
<tr><td><code id="predict.sgs_+3A_...">...</code></td>
<td>
<p>further arguments passed to stats function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
itemresponseThe predicted response. In the logistic case, this represents the predicted class probabilities.
itemclassThe predicted class assignments. Only returned if type = &quot;logistic&quot; in the <code>"sgs"</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit_sgs">fit_sgs()</a></code>
</p>
<p>Other SGS-methods: 
<code><a href="#topic+plot.sgs_cv">plot.sgs_cv</a>()</code>,
<code><a href="#topic+print.sgs">print.sgs</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,1,2,2,3,3,3,4,4)
# generate data
data = generate_toy_data(p=10, n=5, groups = groups, seed_id=3,group_sparsity=1)
# run SGS 
model = fit_sgs(X = data$X, y = data$y, groups = groups, type="linear", lambda = 1, alpha=0.95, 
vFDR=0.1, gFDR=0.1, standardise = "l2", intercept = TRUE, verbose=FALSE)
# use predict function
model_predictions = predict(model, x = data$X)
</code></pre>

<hr>
<h2 id='print.sgs'>print a <code>"sgs"</code> object</h2><span id='topic+print.sgs'></span>

<h3>Description</h3>

<p>Performs prediction from an <code><a href="#topic+fit_sgs">fit_sgs()</a></code> model fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sgs'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.sgs_+3A_x">x</code></td>
<td>
<p>Object an object of class <code>"sgs"</code> from a call to <code><a href="#topic+fit_sgs">fit_sgs()</a></code> or <code><a href="#topic+fit_sgs_cv">fit_sgs_cv()</a></code>.</p>
</td></tr>
<tr><td><code id="print.sgs_+3A_...">...</code></td>
<td>
<p>further arguments passed to base function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A summary of the model fit.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit_sgs">fit_sgs()</a></code>, <code><a href="#topic+fit_sgs_cv">fit_sgs_cv()</a></code>
</p>
<p>Other SGS-methods: 
<code><a href="#topic+plot.sgs_cv">plot.sgs_cv</a>()</code>,
<code><a href="#topic+predict.sgs">predict.sgs</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(rep(1:20, each=3),
          rep(21:40, each=4),
          rep(41:60, each=5),
          rep(61:80, each=6),
          rep(81:100, each=7))
# generate data
data = generate_toy_data(p=500, n=400, groups = groups, seed_id=3)
# run SGS 
model = fit_sgs(X = data$X, y = data$y, groups = groups, type="linear", lambda = 1, alpha=0.95, 
vFDR=0.1, gFDR=0.1, standardise = "l2", intercept = TRUE, verbose=FALSE)
# print model
print(model)
</code></pre>

<hr>
<h2 id='scaled_sgs'>fits a scaled SGS model</h2><span id='topic+scaled_sgs'></span>

<h3>Description</h3>

<p>Fits an SGS model using the noise estimation procedure (Algorithm 5 from Bogdan et. al. (2015)). This estimates <code class="reqn">\lambda</code> and then fits the model using the estimated value. It is an alternative approach to cross-validation (<code><a href="#topic+fit_sgs_cv">fit_sgs_cv()</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scaled_sgs(
  X,
  y,
  groups,
  type = "linear",
  pen_method = 1,
  alpha = 0.95,
  vFDR = 0.1,
  gFDR = 0.1,
  standardise = "l2",
  intercept = TRUE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scaled_sgs_+3A_x">X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package).</p>
</td></tr>
<tr><td><code id="scaled_sgs_+3A_y">y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> should be continuous and for <code>type="logistic"</code> should be a binary variable.</p>
</td></tr>
<tr><td><code id="scaled_sgs_+3A_groups">groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td></tr>
<tr><td><code id="scaled_sgs_+3A_type">type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td></tr>
<tr><td><code id="scaled_sgs_+3A_pen_method">pen_method</code></td>
<td>
<p>The type of penalty sequences to use.
</p>

<ul>
<li> <p><code>"1"</code> uses the vMean SGS and gMean gSLOPE sequences.
</p>
</li>
<li> <p><code>"2"</code> uses the vMax SGS and gMean gSLOPE sequences.
</p>
</li>
<li> <p><code>"1"</code> uses the BH SLOPE and gMean gSLOPE sequences, also known as SGS Original.
</p>
</li></ul>
</td></tr>
<tr><td><code id="scaled_sgs_+3A_alpha">alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, which defines the convex balance between SLOPE and gSLOPE. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="scaled_sgs_+3A_vfdr">vFDR</code></td>
<td>
<p>Defines the desired variable false discovery rate (FDR) level, which determines the shape of the variable penalties. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="scaled_sgs_+3A_gfdr">gFDR</code></td>
<td>
<p>Defines the desired group false discovery rate (FDR) level, which determines the shape of the group penalties. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="scaled_sgs_+3A_standardise">standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li></ul>
</td></tr>
<tr><td><code id="scaled_sgs_+3A_intercept">intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td></tr>
<tr><td><code id="scaled_sgs_+3A_verbose">verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>"sgs"</code> containing model fit information (see <code><a href="#topic+fit_sgs">fit_sgs()</a></code>).
</p>


<h3>References</h3>

<p>M. Bogdan, E. Van den Berg, C. Sabatti, W. Su, E. Candes (2015) <em>SLOPE â€” Adaptive variable selection via convex optimization</em>, <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/SLOPEAdaptive-variable-selection-via-convex-optimization/10.1214/15-AOAS842.full">https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/SLOPEAdaptive-variable-selection-via-convex-optimization/10.1214/15-AOAS842.full</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># specify a grouping structure
groups = c(1,1,2,2,3)
# generate data
data = generate_toy_data(p=5, n=4, groups = groups, seed_id=3,
signal_mean=20,group_sparsity=1,var_sparsity=1)
# run noise estimation 
model = scaled_sgs(X=data$X, y=data$y, groups=groups,pen_method=1)
</code></pre>

<hr>
<h2 id='sgs-package'>sgs: Sparse-Group SLOPE: Adaptive Bi-Level Selection with FDR Control</h2><span id='topic+sgs-package'></span><span id='topic+_PACKAGE'></span>

<h3>Description</h3>

<p>Implementation of Sparse-group SLOPE: Adaptive bi-level with FDR-control (Feser et al. (2023) <a href="https://arxiv.org/abs/2305.09467">arXiv:2305.09467</a>). Linear and logistic regression models are supported, both of which can be fit using k-fold cross-validation. Dense and sparse input matrices are supported. In addition, a general adaptive three operator splitting (ATOS) implementation is provided.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Fabio Feser <a href="mailto:ff120@ic.ac.uk">ff120@ic.ac.uk</a> (<a href="https://orcid.org/0009-0007-3088-9727">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Marina Evangelou (<a href="https://orcid.org/0000-0003-0789-8944">ORCID</a>)
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/ff1201/sgs">https://github.com/ff1201/sgs</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/ff1201/sgs/issues">https://github.com/ff1201/sgs/issues</a>
</p>
</li></ul>


</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
