<!DOCTYPE html><html><head><title>Help for package perplexR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {perplexR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#annotateCode'><p>Large Language Model: Annotate code</p></a></li>
<li><a href='#API_Request'><p>Get Large Language Model Completions Endpoint</p></a></li>
<li><a href='#AskMe'><p>Ask Large Language Model</p></a></li>
<li><a href='#buildUnitTests'><p>Large Language Model: Create Unit Tests</p></a></li>
<li><a href='#clarifyCode'><p>Large Language Model: Clarify Code</p></a></li>
<li><a href='#debugCode'><p>Large Language Model: Find Issues in Code</p></a></li>
<li><a href='#documentCode'><p>Large Language Model: Code Documentation (roxygen2 style)</p></a></li>
<li><a href='#execAddin'><p>Run a Large Language Model as RStudio add-in</p></a></li>
<li><a href='#execAddin_AskMe'><p>Ask Large Language Model</p></a></li>
<li><a href='#finishCode'><p>Large Language Model: Finish code</p></a></li>
<li><a href='#namingGenie'><p>Large Language Model: Create a Function or Variable Name</p></a></li>
<li><a href='#optimizeCode'><p>Large Language Model: Optimize Code</p></a></li>
<li><a href='#perplexR-package'><p>perplexR: A Coding Assistant using Perplexity's Large Language Models</p></a></li>
<li><a href='#responseParser'><p>Parse Perplexity API Response</p></a></li>
<li><a href='#responseReturn'><p>responseReturn</p></a></li>
<li><a href='#rewriteText'><p>Large Language Model: Rewrite Text</p></a></li>
<li><a href='#translateCode'><p>Translate Code from One Language to Another</p></a></li>
<li><a href='#translateText'><p>Large Language Model: Translate Text</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Coding Assistant using Perplexity's Large Language Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.3</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Gabriel Kaiser &lt;quantresearch.gk@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A coding assistant using Perplexity's Large Language Models
    <a href="https://www.perplexity.ai/">https://www.perplexity.ai/</a> API. A set of functions and 'RStudio' add-ins
    that aim to help R developers.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/GabrielKaiserQFin/perplexR">https://github.com/GabrielKaiserQFin/perplexR</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/GabrielKaiserQFin/perplexR/issues">https://github.com/GabrielKaiserQFin/perplexR/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>clipr, httr, jsonlite, miniUI, rstudioapi, shiny, utils</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-28 09:58:24 UTC; Gabriel</td>
</tr>
<tr>
<td>Author:</td>
<td>Gabriel Kaiser [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-29 20:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='annotateCode'>Large Language Model: Annotate code</h2><span id='topic+annotateCode'></span>

<h3>Description</h3>

<p>Large Language Model: Annotate code
</p>


<h3>Usage</h3>

<pre><code class='language-R'>annotateCode(
  code = clipr::read_clip(allow_non_interactive = TRUE),
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant with extensive programming skills.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="annotateCode_+3A_code">code</code></td>
<td>
<p>The code to be commented by Large Language Model. If not provided, it will use
what's copied on the clipboard.</p>
</td></tr>
<tr><td><code id="annotateCode_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="annotateCode_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="annotateCode_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="annotateCode_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="annotateCode_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="annotateCode_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="annotateCode_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="annotateCode_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="annotateCode_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="annotateCode_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
annotateCode("z &lt;- function(x) scale(x)^2")

## End(Not run)

</code></pre>

<hr>
<h2 id='API_Request'>Get Large Language Model Completions Endpoint</h2><span id='topic+API_Request'></span>

<h3>Description</h3>

<p>Get Large Language Model Completions Endpoint
</p>


<h3>Usage</h3>

<pre><code class='language-R'>API_Request(
  prompt,
  PERPLEXITY_API_KEY = PERPLEXITY_API_KEY,
  modelSelection = modelSelection,
  systemRole = systemRole,
  maxTokens = maxTokens,
  temperature = temperature,
  top_p = top_p,
  top_k = top_k,
  presence_penalty = presence_penalty,
  frequency_penalty = frequency_penalty,
  proxy = proxy
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="API_Request_+3A_prompt">prompt</code></td>
<td>
<p>The prompt to generate completions for.</p>
</td></tr>
<tr><td><code id="API_Request_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="API_Request_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="API_Request_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="API_Request_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="API_Request_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="API_Request_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="API_Request_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="API_Request_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="API_Request_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="API_Request_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>

<hr>
<h2 id='AskMe'>Ask Large Language Model</h2><span id='topic+AskMe'></span>

<h3>Description</h3>

<p>Note: See also <code>clearChatSession</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AskMe(
  question,
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AskMe_+3A_question">question</code></td>
<td>
<p>The question to ask Large Language Model.</p>
</td></tr>
<tr><td><code id="AskMe_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="AskMe_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="AskMe_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="AskMe_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="AskMe_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="AskMe_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="AskMe_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="AskMe_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="AskMe_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="AskMe_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
AskMe("What do you think about Large language models?")

## End(Not run)

</code></pre>

<hr>
<h2 id='buildUnitTests'>Large Language Model: Create Unit Tests</h2><span id='topic+buildUnitTests'></span>

<h3>Description</h3>

<p>Create <code>{testthat}</code> test cases for the code.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>buildUnitTests(
  code = clipr::read_clip(allow_non_interactive = TRUE),
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant with extensive programming skills.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="buildUnitTests_+3A_code">code</code></td>
<td>
<p>The code for which to create unit tests by Large Language Model. If not provided, it will use
what's copied on the clipboard.</p>
</td></tr>
<tr><td><code id="buildUnitTests_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="buildUnitTests_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="buildUnitTests_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="buildUnitTests_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="buildUnitTests_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="buildUnitTests_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="buildUnitTests_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="buildUnitTests_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="buildUnitTests_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="buildUnitTests_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
buildUnitTests("squared_numbers &lt;- function(numbers) {\n  numbers ^ 2\n}")

## End(Not run)

</code></pre>

<hr>
<h2 id='clarifyCode'>Large Language Model: Clarify Code</h2><span id='topic+clarifyCode'></span>

<h3>Description</h3>

<p>Large Language Model: Clarify Code
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clarifyCode(
  code = clipr::read_clip(allow_non_interactive = TRUE),
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant with extensive programming skills.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clarifyCode_+3A_code">code</code></td>
<td>
<p>The code to be explained by Large Language Model. If not provided, it will use what's copied on
the clipboard.</p>
</td></tr>
<tr><td><code id="clarifyCode_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="clarifyCode_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="clarifyCode_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="clarifyCode_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="clarifyCode_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="clarifyCode_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="clarifyCode_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="clarifyCode_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="clarifyCode_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="clarifyCode_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
clarifyCode("z &lt;- function(x) scale(x)^2")

## End(Not run)

</code></pre>

<hr>
<h2 id='debugCode'>Large Language Model: Find Issues in Code</h2><span id='topic+debugCode'></span>

<h3>Description</h3>

<p>Large Language Model: Find Issues in Code
</p>


<h3>Usage</h3>

<pre><code class='language-R'>debugCode(
  code = clipr::read_clip(allow_non_interactive = TRUE),
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant with extensive programming skills.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="debugCode_+3A_code">code</code></td>
<td>
<p>The code to be analyzed by Large Language Model. If not provided, it will use what's copied on
the clipboard.</p>
</td></tr>
<tr><td><code id="debugCode_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="debugCode_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="debugCode_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="debugCode_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="debugCode_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="debugCode_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="debugCode_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="debugCode_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="debugCode_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="debugCode_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
debugCode("z &lt;- function(x) scale(x)2")

## End(Not run)

</code></pre>

<hr>
<h2 id='documentCode'>Large Language Model: Code Documentation (roxygen2 style)</h2><span id='topic+documentCode'></span>

<h3>Description</h3>

<p>Large Language Model: Code Documentation (roxygen2 style)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>documentCode(
  code = clipr::read_clip(allow_non_interactive = TRUE),
  inLineDocumentation = "roxygen2",
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant with extensive programming skills.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="documentCode_+3A_code">code</code></td>
<td>
<p>The code to be documented by Large Language Model. If not provided, it will use what's copied on
the clipboard.</p>
</td></tr>
<tr><td><code id="documentCode_+3A_inlinedocumentation">inLineDocumentation</code></td>
<td>
<p>Formatting style of In-Line Documentation.</p>
</td></tr>
<tr><td><code id="documentCode_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="documentCode_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="documentCode_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="documentCode_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="documentCode_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="documentCode_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="documentCode_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="documentCode_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="documentCode_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="documentCode_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
documentCode("z &lt;- function(x) scale(x)^2")

## End(Not run)

</code></pre>

<hr>
<h2 id='execAddin'>Run a Large Language Model as RStudio add-in</h2><span id='topic+execAddin'></span>

<h3>Description</h3>

<p>Run a Large Language Model as RStudio add-in
</p>


<h3>Usage</h3>

<pre><code class='language-R'>execAddin(FUN)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="execAddin_+3A_fun">FUN</code></td>
<td>
<p>The function to be executed.</p>
</td></tr>
</table>

<hr>
<h2 id='execAddin_AskMe'>Ask Large Language Model</h2><span id='topic+execAddin_AskMe'></span>

<h3>Description</h3>

<p>Opens an interactive chat session with Large Language Model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>execAddin_AskMe()
</code></pre>

<hr>
<h2 id='finishCode'>Large Language Model: Finish code</h2><span id='topic+finishCode'></span>

<h3>Description</h3>

<p>Large Language Model: Finish code
</p>


<h3>Usage</h3>

<pre><code class='language-R'>finishCode(
  code = clipr::read_clip(allow_non_interactive = TRUE),
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant with extensive programming skills.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="finishCode_+3A_code">code</code></td>
<td>
<p>The code to be completed by Large Language Model. If not provided, it will use what's copied on
the clipboard.</p>
</td></tr>
<tr><td><code id="finishCode_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="finishCode_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="finishCode_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="finishCode_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="finishCode_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="finishCode_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="finishCode_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="finishCode_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="finishCode_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="finishCode_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
finishCode("# A function to square each element of a vector\nsquare_each &lt;- function(")

## End(Not run)

</code></pre>

<hr>
<h2 id='namingGenie'>Large Language Model: Create a Function or Variable Name</h2><span id='topic+namingGenie'></span>

<h3>Description</h3>

<p>Large Language Model: Create a Function or Variable Name
</p>


<h3>Usage</h3>

<pre><code class='language-R'>namingGenie(
  code = clipr::read_clip(allow_non_interactive = TRUE),
  namingConvention = "camelCase",
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant with extensive programming skills.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="namingGenie_+3A_code">code</code></td>
<td>
<p>The code for which to give a variable name to its result. If not provided, it will
use what's copied on the clipboard.</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_namingconvention">namingConvention</code></td>
<td>
<p>Naming convention. Default is camelCase.</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="namingGenie_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
namingGenie("sapply(1:10, function(i) i ** 2)")

## End(Not run)

</code></pre>

<hr>
<h2 id='optimizeCode'>Large Language Model: Optimize Code</h2><span id='topic+optimizeCode'></span>

<h3>Description</h3>

<p>Large Language Model: Optimize Code
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimizeCode(
  code = clipr::read_clip(allow_non_interactive = TRUE),
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant with extensive programming skills.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizeCode_+3A_code">code</code></td>
<td>
<p>The code to be optimized by Large Language Model. If not provided, it will use what's copied on
the clipboard.</p>
</td></tr>
<tr><td><code id="optimizeCode_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="optimizeCode_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="optimizeCode_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="optimizeCode_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="optimizeCode_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="optimizeCode_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="optimizeCode_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="optimizeCode_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="optimizeCode_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="optimizeCode_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
optimizeCode("z &lt;- function(x) scale(x)^2")

## End(Not run)

</code></pre>

<hr>
<h2 id='perplexR-package'>perplexR: A Coding Assistant using Perplexity's Large Language Models</h2><span id='topic+perplexR'></span><span id='topic+perplexR-package'></span>

<h3>Description</h3>

<p>A coding assistant using Perplexity's Large Language Models <a href="https://www.perplexity.ai/">https://www.perplexity.ai/</a> API. A set of functions and 'RStudio' add-ins that aim to help R developers.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Gabriel Kaiser <a href="mailto:quantresearch.gk@gmail.com">quantresearch.gk@gmail.com</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/GabrielKaiserQFin/perplexR">https://github.com/GabrielKaiserQFin/perplexR</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/GabrielKaiserQFin/perplexR/issues">https://github.com/GabrielKaiserQFin/perplexR/issues</a>
</p>
</li></ul>


<hr>
<h2 id='responseParser'>Parse Perplexity API Response</h2><span id='topic+responseParser'></span>

<h3>Description</h3>

<p>Takes the raw response from the Perplexity API and extracts the text content from it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>responseParser(raw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="responseParser_+3A_raw">raw</code></td>
<td>
<p>The raw object returned by the Perplexity API.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a character vector containing the text content of the response.
</p>

<hr>
<h2 id='responseReturn'>responseReturn</h2><span id='topic+responseReturn'></span>

<h3>Description</h3>

<p>responseReturn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>responseReturn(raw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="responseReturn_+3A_raw">raw</code></td>
<td>
<p>the chatresponse to return</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>

<hr>
<h2 id='rewriteText'>Large Language Model: Rewrite Text</h2><span id='topic+rewriteText'></span>

<h3>Description</h3>

<p>Large Language Model: Rewrite Text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rewriteText(
  text = clipr::read_clip(allow_non_interactive = TRUE),
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rewriteText_+3A_text">text</code></td>
<td>
<p>The text to be rewritten by Large Language Model. If not provided, it will use what's copied on the clipboard.</p>
</td></tr>
<tr><td><code id="rewriteText_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="rewriteText_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="rewriteText_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant.&quot;</p>
</td></tr>
<tr><td><code id="rewriteText_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="rewriteText_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="rewriteText_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="rewriteText_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="rewriteText_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="rewriteText_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="rewriteText_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
rewriteText("Dear Recipient, I hope this message finds you well.")

## End(Not run)

</code></pre>

<hr>
<h2 id='translateCode'>Translate Code from One Language to Another</h2><span id='topic+translateCode'></span>

<h3>Description</h3>

<p>This function takes a snippet of code and translates it from one programming
language to another using Perplexity API. The default behavior is to read the code
from the clipboard and translate from R to Python.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>translateCode(
  code = clipr::read_clip(allow_non_interactive = TRUE),
  from = "R",
  to = "Python",
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant with extensive programming skills.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="translateCode_+3A_code">code</code></td>
<td>
<p>A string containing the code to be translated.
If not provided, the function will attempt to read from the clipboard.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_from">from</code></td>
<td>
<p>The language of the input code. Default is &quot;R&quot;.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_to">to</code></td>
<td>
<p>The target language for translation. Default is &quot;Python&quot;.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant
with extensive knowledge of R programming.&quot;</p>
</td></tr>
<tr><td><code id="translateCode_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="translateCode_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A string containing the translated code.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
translateCode("your R code here", from = "R", to = "Python")

## End(Not run)

</code></pre>

<hr>
<h2 id='translateText'>Large Language Model: Translate Text</h2><span id='topic+translateText'></span>

<h3>Description</h3>

<p>Large Language Model: Translate Text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>translateText(
  text = clipr::read_clip(allow_non_interactive = TRUE),
  toLanguage = "German",
  PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
  modelSelection = c("mistral-7b-instruct", "mixtral-8x7b-instruct",
    "codellama-70b-instruct", "sonar-small-chat", "sonar-small-online",
    "sonar-medium-chat", "sonar-medium-online"),
  systemRole = "You are a helpful assistant.",
  maxTokens = 265,
  temperature = 1,
  top_p = NULL,
  top_k = 100,
  presence_penalty = 0,
  frequency_penalty = NULL,
  proxy = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="translateText_+3A_text">text</code></td>
<td>
<p>The text to be translated by Large Language Model. If not provided, it will use what's copied on the clipboard.</p>
</td></tr>
<tr><td><code id="translateText_+3A_tolanguage">toLanguage</code></td>
<td>
<p>The language to be translated into.</p>
</td></tr>
<tr><td><code id="translateText_+3A_perplexity_api_key">PERPLEXITY_API_KEY</code></td>
<td>
<p>PERPLEXITY API key.</p>
</td></tr>
<tr><td><code id="translateText_+3A_modelselection">modelSelection</code></td>
<td>
<p>model choice. Default is mistral-7b-instruct.</p>
</td></tr>
<tr><td><code id="translateText_+3A_systemrole">systemRole</code></td>
<td>
<p>Role for model. Default is: &quot;You are a helpful assistant.&quot;</p>
</td></tr>
<tr><td><code id="translateText_+3A_maxtokens">maxTokens</code></td>
<td>
<p>The maximum integer of completion tokens returned by API.</p>
</td></tr>
<tr><td><code id="translateText_+3A_temperature">temperature</code></td>
<td>
<p>The amount of randomness in the response,
valued between 0 inclusive and 2 exclusive. Higher values are more random,
and lower values are more deterministic. Set either temperature or top_p.</p>
</td></tr>
<tr><td><code id="translateText_+3A_top_p">top_p</code></td>
<td>
<p>Nucleus sampling threshold, valued between 0 and 1 inclusive.</p>
</td></tr>
<tr><td><code id="translateText_+3A_top_k">top_k</code></td>
<td>
<p>The number of tokens to keep for highest top-k filtering,
specified as an integer between 0 and 2048 inclusive.
If set to 0, top-k filtering is disabled.</p>
</td></tr>
<tr><td><code id="translateText_+3A_presence_penalty">presence_penalty</code></td>
<td>
<p>A value between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.
Incompatible with frequency_penalty.</p>
</td></tr>
<tr><td><code id="translateText_+3A_frequency_penalty">frequency_penalty</code></td>
<td>
<p>A multiplicative penalty greater than 0.
Values greater than 1.0 penalize new tokens based on their existing
frequency in the text so far, decreasing the model's likelihood to repeat
the same line verbatim. A value of 1.0 means no penalty.</p>
</td></tr>
<tr><td><code id="translateText_+3A_proxy">proxy</code></td>
<td>
<p>Default value is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character value with the response generated by Large Language Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
translateText("Dear Recipient, I hope this message finds you well.")

## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
