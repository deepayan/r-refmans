<!DOCTYPE html><html><head><title>Help for package mvGPS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mvGPS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bal'><p>Construct Covariate Balance Statistics for Models with Multivariate Exposure</p></a></li>
<li><a href='#D_C_check'><p>Internal function for formatting and checking specification of exposures and</p>
confounders</a></li>
<li><a href='#gen_D'><p>Generate Bivariate Multivariate Exposure</p></a></li>
<li><a href='#hull_sample'><p>Sample Points Along a Convex Hull</p></a></li>
<li><a href='#mvGPS'><p>Multivariate Generalized Propensity Score</p></a></li>
<li><a href='#X_check'><p>Checking that the exposure matrix is properly specified</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Causal Inference using Multivariate Generalized Propensity Score</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.2</td>
</tr>
<tr>
<td>Description:</td>
<td>
    Methods for estimating and utilizing the multivariate generalized
    propensity score (mvGPS) for multiple continuous exposures described in
    Williams, J.R, and Crespi, C.M. (2020) &lt;<a href="https://arxiv.org/abs/2008.13767">arXiv:2008.13767</a>&gt;. The methods allow
    estimation of a dose-response surface relating the joint distribution of multiple
    continuous exposure variables to an outcome. Weights are constructed assuming a
    multivariate normal density for the marginal and conditional distribution of
    exposures given a set of confounders. Confounders can be different for different
    exposure variables. The weights are designed to achieve balance across all
    exposure dimensions and can be used to estimate dose-response surfaces.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>false</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.0</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rdpack, MASS, WeightIt, cobalt, matrixNormal, geometry, sp,
gbm, CBPS</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/williazo/mvGPS/issues">https://github.com/williazo/mvGPS/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/williazo/mvGPS">https://github.com/williazo/mvGPS</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, dagitty, ggdag, dplyr, rmarkdown, ggplot2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-12-06 19:52:49 UTC; williazo</td>
</tr>
<tr>
<td>Author:</td>
<td>Justin Williams <a href="https://orcid.org/0000-0002-5045-2764"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Justin Williams &lt;williazo@ucla.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-12-07 08:20:15 UTC</td>
</tr>
</table>
<hr>
<h2 id='bal'>Construct Covariate Balance Statistics for Models with Multivariate Exposure</h2><span id='topic+bal'></span>

<h3>Description</h3>

<p>Assessing balance between exposure(s) and confounders is key when performing causal
analysis using propensity scores. We provide a list of several models to generate
weights to use in causal inference for multivariate exposures, and test the balancing property of these weights
using weighted Pearson correlations. In addition, returns the effective sample
size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bal(
  model_list,
  D,
  C,
  common = FALSE,
  trim_w = FALSE,
  trim_quantile = 0.99,
  all_uni = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bal_+3A_model_list">model_list</code></td>
<td>
<p>character string identifying which methods to use when
constructing weights. See details for a list of available models</p>
</td></tr>
<tr><td><code id="bal_+3A_d">D</code></td>
<td>
<p>numeric matrix of dimension <code class="reqn">n</code> by <code class="reqn">m</code> designating values of the exposures</p>
</td></tr>
<tr><td><code id="bal_+3A_c">C</code></td>
<td>
<p>either a list of numeric matrices of length <code class="reqn">m</code> of dimension 
<code class="reqn">n</code> by <code class="reqn">p_j</code> designating values of the confounders for each exposure 
value or if <code>common</code> is TRUE a single matrix of of dimension <code class="reqn">n</code> by
<code class="reqn">p</code> that represents common confounders for all exposures.</p>
</td></tr>
<tr><td><code id="bal_+3A_common">common</code></td>
<td>
<p>logical indicator for whether C is a single matrix of common
confounders for all exposures. default is FALSE meaning C must be specified
as list of confounders of length <code class="reqn">m</code>.</p>
</td></tr>
<tr><td><code id="bal_+3A_trim_w">trim_w</code></td>
<td>
<p>logical indicator for whether to trim weights. default is FALSE</p>
</td></tr>
<tr><td><code id="bal_+3A_trim_quantile">trim_quantile</code></td>
<td>
<p>numeric scalar used to specify the upper quantile to 
trim weights if applicable. default is 0.99</p>
</td></tr>
<tr><td><code id="bal_+3A_all_uni">all_uni</code></td>
<td>
<p>logical indicator. If TRUE then all univariate models specified
in model_list will be estimated for each exposure. If FALSE will only estimate weights
for the first exposure</p>
</td></tr>
<tr><td><code id="bal_+3A_...">...</code></td>
<td>
<p>additional arguments to pass to <code><a href="WeightIt.html#topic+weightit">weightit</a></code> function
if specifying one of these models in the model_list</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When using propensity score methods for causal inference it is crucial to
check the balancing property of the covariates and exposure(s). To do this in
the multivariate case we first use a weight generating method from the available
list shown below.
</p>


<h4>Methods Available</h4>


<ul>
<li><p> &quot;mvGPS&quot;: Multivariate generalized propensity score using Gaussian densities
</p>
</li>
<li><p> &quot;entropy&quot;: Estimating weights using entropy loss
function without specifying propensity score (TÃ¼bbicke 2020)
</p>
</li>
<li><p> &quot;CBPS&quot;: Covariate balancing propensity score for continuous treatments
which adds balance penalty while solving for propensity score parameters (Fong et al. 2018)
</p>
</li>
<li><p> &quot;PS&quot;: Generalized propensity score estimated using univariate Gaussian densities
</p>
</li>
<li><p> &quot;GBM&quot;: Gradient boosting to estimate the mean function of the propensity score,
but still maintains Gaussian distributional assumptions (Zhu et al. 2015)
</p>
</li></ul>

<p>Note that only the <code>mvGPS</code> method is multivariate and all others are strictly univariate.
For univariate methods weights are estimated for each exposure separately
using the <code><a href="WeightIt.html#topic+weightit">weightit</a></code> function given the
confounders for that exposure in <code>C</code> when <code>all_uni=TRUE</code>. To estimate
weights for only the first exposure set <code>all_uni=FALSE</code>.
</p>

<p>It is also important to note that the weights for each method can be trimmed at
the desired quantile by setting <code>trim_w=TRUE</code> and setting <code>trim_quantile</code>
in \[0.5, 1\]. Trimming is done at both the upper and lower bounds. For further details
see <code><a href="#topic+mvGPS">mvGPS</a></code> on how trimming is performed.
</p>


<h4>Balance Metrics</h4>

<p>In this package we include three key balancing metrics to summarize balance
across all of the exposures.
</p>

<ul>
<li><p> Euclidean distance
</p>
</li>
<li><p> Maximum absolute correlation
</p>
</li>
<li><p> Average absolute correlation
</p>
</li></ul>

<p><em>Euclidean distance</em> is calculated using the origin point as reference, e.g. for <code>m=2</code>
exposures the reference point is \[0, 0\]. In this way we are calculating how far
the observed set of correlation points are from perfect balance.
</p>
<p><em>Maximum absolute correlation</em> reports the largest single imbalance between
the exposures and the set of confounders. It is often a key diagnostic as
even a single confounder that is sufficiently out of balance can reduce performance.
</p>
<p><em>Average absolute correlation</em> is the sum of the exposure-confounder correlations.
This metric summarizes how well, on average, the entire set of exposures is balanced.
</p>



<h4>Effective Sample Size</h4>

<p>Effective sample size, ESS, is defined as
</p>
<p style="text-align: center;"><code class="reqn">ESS=(\Sigma_i w_i)^{2}/\Sigma_i w_i^2,</code>
</p>

<p>where <code class="reqn">w_i</code> are the estimated weights for a particular method (Kish 1965).
Note that when <code class="reqn">w=1</code> for all units that the <code class="reqn">ESS</code> is equal to the sample size <code class="reqn">n</code>.
<code class="reqn">ESS</code> decreases when there are extreme weights or high variability in the weights.
</p>



<h3>Value</h3>


<ul>
<li> <p><code>W</code>: list of weights generated for each model
</p>
</li>
<li> <p><code>cor_list</code>: list of weighted Pearson correlation coefficients for all confounders specified
</p>
</li>
<li> <p><code>bal_metrics</code>: data.frame with the Euclidean distance, maximum absolute correlation, and average absolute correlation by method
</p>
</li>
<li> <p><code>ess</code>: effective sample size for each of the methods used to generate weights
</p>
</li>
<li> <p><code>models</code>: vector of models used
</p>
</li></ul>



<h3>References</h3>

<p>Fong C, Hazlett C, Imai K (2018).
&ldquo;Covariate balancing propensity score for a continuous treatment: application to the efficacy of political advertisements.&rdquo;
<em>Annals of Applied Statistics</em>, <b>In-Press</b>.<br /><br /> Kish L (1965).
<em>Survey Sampling</em>.
John Wiley \&amp; Sons, New York.<br /><br /> TÃ¼bbicke S (2020).
&ldquo;Entropy Balancing for Continuous Treatments.&rdquo;
<em>arXiv e-prints</em>.
2001.06281.<br /><br /> Zhu Y, Coffman DL, Ghosh D (2015).
&ldquo;A boosting algorithm for estimating generalized propensity scores with continuous treatments.&rdquo;
<em>Journal of Causal Inference</em>, <b>3</b>(1), 25-40.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulating data
sim_dt &lt;- gen_D(method="u", n=150, rho_cond=0.2, s_d1_cond=2, s_d2_cond=2,
k=3, C_mu=rep(0, 3), C_cov=0.1, C_var=1, d1_beta=c(0.5, 1, 0),
d2_beta=c(0, 0.3, 0.75), seed=06112020)
D &lt;- sim_dt$D
C &lt;- sim_dt$C

#generating weights using mvGPS and potential univariate alternatives
require(WeightIt)
bal_sim &lt;- bal(model_list=c("mvGPS", "entropy", "CBPS", "PS", "GBM"), D,
C=list(C[, 1:2], C[, 2:3]))

#overall summary statistics
bal_sim$bal_metrics

#effective sample sizes
bal_sim$ess

#we can also trim weights for all methods
bal_sim_trim &lt;- bal(model_list=c("mvGPS", "entropy", "CBPS", "PS", "GBM"), D,
C=list(C[, 1:2], C[, 2:3]), trim_w=TRUE, trim_quantile=0.9, p.mean=0.5)
#note that in this case we can also pass additional arguments using in
#WeighIt package for entropy, CBPS, PS, and GBM such as specifying the p.mean

#can check to ensure all the weights have been properly trimmed at upper and
#lower bound
all.equal(unname(unlist(lapply(bal_sim$W, quantile, 0.99))),
unname(unlist(lapply(bal_sim_trim$W, max))))
all.equal(unname(unlist(lapply(bal_sim$W, quantile, 1-0.99))),
unname(unlist(lapply(bal_sim_trim$W, min))))

</code></pre>

<hr>
<h2 id='D_C_check'>Internal function for formatting and checking specification of exposures and
confounders</h2><span id='topic+D_C_check'></span>

<h3>Description</h3>

<p>Internal function for formatting and checking specification of exposures and
confounders
</p>


<h3>Usage</h3>

<pre><code class='language-R'>D_C_check(D, C, common)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="D_C_check_+3A_d">D</code></td>
<td>
<p>numeric matrix of dimension <code class="reqn">n</code> by <code class="reqn">m</code> designating values of the exposures</p>
</td></tr>
<tr><td><code id="D_C_check_+3A_c">C</code></td>
<td>
<p>either a list of numeric matrices of length <code class="reqn">m</code> of dimension 
<code class="reqn">n</code> by <code class="reqn">p_j</code> designating values of the confounders for each exposure 
value or if <code>common</code> is TRUE a single matrix of of dimension <code class="reqn">n</code> by
<code class="reqn">p</code> that represents common confounders for all exposures.</p>
</td></tr>
<tr><td><code id="D_C_check_+3A_common">common</code></td>
<td>
<p>logical indicator for whether C is a single matrix of common
confounders for all exposures. default is FALSE meaning C must be specified
as list of confounders of length <code class="reqn">m</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='gen_D'>Generate Bivariate Multivariate Exposure</h2><span id='topic+gen_D'></span>

<h3>Description</h3>

<p>Generate exposure from a bivariate normal distribution confounded by a set of
variables <code>C</code>=\(C1, C2).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen_D(
  method,
  n,
  rho_cond,
  s_d1_cond,
  s_d2_cond,
  k,
  C_mu,
  C_cov,
  C_var,
  C_sigma = NULL,
  d1_beta,
  d2_beta,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gen_D_+3A_method">method</code></td>
<td>
<p>character value identifying which method to use when generating
bivariate exposure. Options include &quot;matrix_normal&quot;, &quot;uni_cond&quot;, and &quot;vector_normal&quot;.
See details for a brief explanation of each method. <code>uni_cond</code> is fastest</p>
</td></tr>
<tr><td><code id="gen_D_+3A_n">n</code></td>
<td>
<p>integer value total number of units</p>
</td></tr>
<tr><td><code id="gen_D_+3A_rho_cond">rho_cond</code></td>
<td>
<p>scalar value identifying conditional correlation of exposures given covariates between \[0, 1\]</p>
</td></tr>
<tr><td><code id="gen_D_+3A_s_d1_cond">s_d1_cond</code></td>
<td>
<p>scalar value for conditional standard deviation of <code>D1</code></p>
</td></tr>
<tr><td><code id="gen_D_+3A_s_d2_cond">s_d2_cond</code></td>
<td>
<p>scalar value for conditional standard deviation of <code>D2</code></p>
</td></tr>
<tr><td><code id="gen_D_+3A_k">k</code></td>
<td>
<p>integer value determining number of covariates to generate in <code>C</code>.</p>
</td></tr>
<tr><td><code id="gen_D_+3A_c_mu">C_mu</code></td>
<td>
<p>numeric vector of mean values for covariates. Must be same length as <code>k</code></p>
</td></tr>
<tr><td><code id="gen_D_+3A_c_cov">C_cov</code></td>
<td>
<p>scalar value representing constant correlation between covariates</p>
</td></tr>
<tr><td><code id="gen_D_+3A_c_var">C_var</code></td>
<td>
<p>scalar value representing constant variance of covariates</p>
</td></tr>
<tr><td><code id="gen_D_+3A_c_sigma">C_sigma</code></td>
<td>
<p>numeric matrix representing the covariance matrix of covariates.
Default is NULL and will use <code>C_var</code> and <code>C_var</code> otherwise.</p>
</td></tr>
<tr><td><code id="gen_D_+3A_d1_beta">d1_beta</code></td>
<td>
<p>numeric vector of length <code>k</code> defining the mean of <code>D1</code> with respect to the covariates</p>
</td></tr>
<tr><td><code id="gen_D_+3A_d2_beta">d2_beta</code></td>
<td>
<p>numeric vector of length <code>k</code> defining the mean of <code>D2</code> with respect to the covariates</p>
</td></tr>
<tr><td><code id="gen_D_+3A_seed">seed</code></td>
<td>
<p>integer value setting the seed of random generator to produce repeatable results. set to NULL by default</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Generating Confounders</h4>

<p>We assume that there are a total of <code>k</code> confounders that are generated
from a multivariate normal distribution with equicorrelation covariance, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">\Sigma_{C}=\phi(\mathbf{1}\mathbf{1}^{T}-\mathbf{I})+\mathbf{I}\sigma^{2}_{C},</code>
</p>

<p>where <code class="reqn">\mathbf{1}</code> is the column vector with all entries equal to 1,
<code class="reqn">\mathbf{I}</code> is the identity matrix, <code class="reqn">\sigma^{2}_{C}</code> is a constant
standard deviation for all confounders, and <code class="reqn">\phi</code> is the covariance of
any two confounders. Therefore, our random confounders
<code>C</code> follow the distribution
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{C}\sim N_{k}(\boldsymbol{\mu}_{C}, \Sigma_{C}).</code>
</p>

<p>We draw a total of <code>n</code> samples from this multivariate normal distribution
using <code><a href="MASS.html#topic+mvrnorm">mvrnorm</a></code>.
</p>



<h4>Generating Bivariate Exposure</h4>

<p>The first step when generating the bivariate exposure is to specify the
effects of the confounders <code>C</code>. We control this for each exposure value
using the arguments <code>d1_beta</code> and <code>d2_beta</code> such that
</p>
<p style="text-align: center;"><code class="reqn">E[D_{1}\mid \mathbf{C}]=\boldsymbol{\beta}^{T}_{D1}\mathbf{C}</code>
</p>
<p> and
</p>
<p style="text-align: center;"><code class="reqn">E[D_{2}\mid \mathbf{C}]=\boldsymbol{\beta}^{T}_{D2}\mathbf{C}</code>
</p>
<p>.
</p>
<p>Note that by specifying <code>d1_beta</code> and <code>d2_beta</code> separately that the
user can control the amount of overlap in the confounders for each exposure,
and how many of the variables in <code>C</code> are truly related to the exposures.
For instance to have the exposure have identical confounding effects
<code>d1_beta</code>=<code>d2_beta</code>, and they have separate confounding if there are
zero non-zero elements in common between <code>d1_beta</code> and <code>d2_beta</code>.
</p>
<p>To generate the bivariate conditional distribution of exposures given the set
of confounders <code>C</code> we have the following three methods:
</p>

<ul>
<li><p> &quot;matrix_normal&quot;
</p>
</li>
<li><p> &quot;uni_cond&quot;
</p>
</li>
<li><p> &quot;vector_normal&quot;
</p>
</li></ul>

<p>&quot;matrix_normal&quot; uses the function <code><a href="matrixNormal.html#topic+rmatnorm">rmatnorm</a></code> to
generate all <code>n</code> samples as
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{D}\mid\mathbf{C}\sim N_{n \times 2}(\boldsymbol{\beta}\mathbf{C}, \mathbf{I}_{n}, \Omega)</code>
</p>

<p>where <code class="reqn">\boldsymbol{\beta}</code> is a column vector containing <code class="reqn">\boldsymbol{\beta}^{T}_{D1}</code>
and <code class="reqn">\boldsymbol{\beta}^{T}_{D2}</code>, and <code class="reqn">\Omega</code> is the conditional covariance matrix.
</p>
<p>&quot;vector_normal&quot; simply vectorizes the matrix_normal method above to generate
a vector of length <code class="reqn">n \times 2</code>.
</p>
<p>&quot;uni_cond&quot; specifies the bivariate exposure using univariate conditional
factorization, which in the case of bivariate normal results in two univariate
normal expressions.
</p>
<p>In general, we suggest using the univariate conditional, &quot;uni_cond&quot;, method
when generating exposures as it is substantially faster than both the
matrix normal and vector normal approaches.
</p>
<p>Note that the options use regular expression matching and can be specified
uniquely using either &quot;m&quot;, &quot;u&quot;, or &quot;v&quot;.
</p>



<h4>Marginal Covariance of Exposures</h4>

<p>As described above the exposures are drawn conditional on the set <code>C</code>,
so the marginal covariance of exposures is defined as
</p>
<p style="text-align: center;"><code class="reqn">\Sigma_{D}= \boldsymbol{\beta}\Sigma_{C}\boldsymbol{\beta}^{T}+\Omega.</code>
</p>

<p>In our function we return the true marginal covariance <code class="reqn">\Sigma_{D}</code> as well
as the true marginal correlation <code class="reqn">\rho_{D}</code>.
</p>



<h3>Value</h3>


<ul>
<li> <p><code>D</code>: nx2 numeric matrix of the sample values for the exposures given the set <code>C</code>
</p>
</li>
<li> <p><code>C</code>: nxk numeric matrix of the sampled values for the confounding set <code>C</code>
</p>
</li>
<li> <p><code>D_Sigma</code>: 2x2 numeric matrix of the true marginal covariance of exposures
</p>
</li>
<li> <p><code>rho</code>: numeric scalar representing the true marginal correlation of exposures
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>#generate bivariate exposures. D1 confounded by C1 and C2. D2 by C2 and C3
#uses univariate conditional normal to draw samples
sim_dt &lt;- gen_D(method="u", n=200, rho_cond=0.2, s_d1_cond=2, s_d2_cond=2, k=3,
C_mu=rep(0, 3), C_cov=0.1, C_var=1, d1_beta=c(0.5, 1, 0), d2_beta=c(0, 0.3, 0.75), seed=06112020)
D &lt;- sim_dt$D
C &lt;- sim_dt$C

#observed correlation should be close to true marginal value
cor(D); sim_dt$rho


#Use vector normal method instead of univariate method to draw samples
sim_dt &lt;- gen_D(method="v", n=200, rho_cond=0.2, s_d1_cond=2, s_d2_cond=2, k=3,
C_mu=rep(0, 3), C_cov=0.1, C_var=1, d1_beta=c(0.5, 1, 0), d2_beta=c(0, 0.3, 0.75), seed=06112020)

</code></pre>

<hr>
<h2 id='hull_sample'>Sample Points Along a Convex Hull</h2><span id='topic+hull_sample'></span>

<h3>Description</h3>

<p>To define a proper estimable region with multivariate exposure we construct 
a convex hull of the data in order to maintain the positivity identifying assumption.
We also provide options to create trimmed versions of the convex hull to further
restrict to high density regions in multidimensional space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hull_sample(
  X,
  num_grid_pts = 500,
  grid_type = "regular",
  trim_hull = FALSE,
  trim_quantile = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hull_sample_+3A_x">X</code></td>
<td>
<p>numeric matrix of n by m dimensions. Each row corresponds to a point in m-dimensional space.</p>
</td></tr>
<tr><td><code id="hull_sample_+3A_num_grid_pts">num_grid_pts</code></td>
<td>
<p>integer scalar denoting the number of parameters to 
search for over the convex hull. Default is 500.</p>
</td></tr>
<tr><td><code id="hull_sample_+3A_grid_type">grid_type</code></td>
<td>
<p>character value indicating the type of grid to sample from
the convex hull from <code><a href="sp.html#topic+spsample">spsample</a></code></p>
</td></tr>
<tr><td><code id="hull_sample_+3A_trim_hull">trim_hull</code></td>
<td>
<p>logical indicator of whether to restrict convex hull. Default is FALSE</p>
</td></tr>
<tr><td><code id="hull_sample_+3A_trim_quantile">trim_quantile</code></td>
<td>
<p>numeric scalar between \[0.5, 1\] representing the 
quantile value to trim the convex hull. Only used if trim_hull is set to TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that <code class="reqn">X</code> is an <code class="reqn">n\times m</code> matrix representing the multivariate
exposure of interest. We can define the convex hull of these observations as
<strong>H</strong>. There are two distinct processes for defining <strong>H</strong> depending
on whether <code class="reqn">m=2</code> or <code class="reqn">m&gt;2</code>.
</p>
<p>If <code class="reqn">m=2</code>, we use the <code><a href="grDevices.html#topic+chull">chull</a></code> function to define the 
vertices of the convex hull. The algorithm implemented is described in Eddy (1977).
</p>
<p>If <code class="reqn">m&gt;2</code>, we use the <code><a href="geometry.html#topic+convhulln">convhulln</a></code> function. This algorithm 
for obtaining the convex hull in m-dimensional space uses Qhull described in
Barber et al. (1996). Currently this function returns
only the vertex set <code>hpts_vs</code> without the grid sample points. There are
options to visualize the convex hull when <code class="reqn">m=3</code> using triangular facets,
but there are no implementable solutions to sample along convex hulls in higher
dimensions.
</p>
<p>To restrict the convex hull to higher density regions of the exposure we can 
apply trimming. To apply trimming set <code>trim_hull=TRUE</code> and specify 
<code>trim_quantile=q</code> where <code>q</code> must be in \[0.5, 1\]. Along each
exposure dimension we then calculate the upper and lower bounds using the 
<code><a href="stats.html#topic+quantile">quantile</a></code> function, i.e., <code>quantile(q)</code> and 
<code>quantile(1-q)</code>. Any observations that have a value above or below these
sample quantiles is excluded. The remaining observations that fall completely 
within the sample quantiles across all dimensions are used to estimate the 
convex hull. We return <code>X</code> that represents the observations used. 
If <code>trim_hull=FALSE</code>, then <code>X</code> is unchanged. However, if trimming
is applied then <code>X</code> contains only the remaining observations after trimming.
</p>


<h3>Value</h3>


<ul>
<li> <p><code>hpts_vs</code>: vertices of the convex hull in m-dimensional space
</p>
</li>
<li> <p><code>grid_pts</code>: values of grid points sampled over the corresponding convex hull
</p>
</li>
<li> <p><code>X</code>: data used to generate convex hull which may be trimmed
</p>
</li></ul>



<h3>References</h3>

<p>Barber CB, Dobkin DP, Huhdanpaa H (1996).
&ldquo;The quickhull algorithm for convex hulls.&rdquo;
<em>ACM Transactions on Mathematical Software (TOMS)</em>, <b>22</b>(4), 469-483.<br /><br /> Eddy WF (1977).
&ldquo;A new convex hull algorithm for planar sets.&rdquo;
<em>ACM Transactions on Mathematical Software (TOMS)</em>, <b>3</b>(4), 398-403.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#generating exposure with m=3
D &lt;- matrix(unlist(lapply(seq_len(3), function(m) rnorm(100))), nrow=100)

#first using only the first two variables we can return hpts_vs and grid_pts
D_hull &lt;- hull_sample(D[, 1:2])

#when m&gt;2 we only return hpts_vs and grid_pts is NULL
D_hull_large &lt;- hull_sample(D)
is.null(D_hull_large$grid_pts)

#we can also apply trimming to the convex hull and return this reduced matrix
D_hull_trim &lt;- hull_sample(D[, 1:2], trim_hull=TRUE, trim_quantile=0.95)
dim(D_hull$X)
dim(D_hull_trim$X)

#alternatively, we can also define the number of points to sample from for grid_pts
small_grid &lt;- hull_sample(D[, 1:2], num_grid_pts=100)
length(D_hull$grid_pts)
length(small_grid$grid_pts)

</code></pre>

<hr>
<h2 id='mvGPS'>Multivariate Generalized Propensity Score</h2><span id='topic+mvGPS'></span>

<h3>Description</h3>

<p>Estimate propensity scores for multivariate continuous exposure
by assuming joint normal conditional densities. Simultaneously estimate 
stabilized inverse probability of treatment weights (IPTW) using joint normal
density for marginal distribution of exposure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvGPS(D, C, common = FALSE, trim_w = FALSE, trim_quantile = 0.99)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvGPS_+3A_d">D</code></td>
<td>
<p>numeric matrix of dimension <code class="reqn">n</code> by <code class="reqn">m</code> designating values of the exposures</p>
</td></tr>
<tr><td><code id="mvGPS_+3A_c">C</code></td>
<td>
<p>either a list of numeric matrices of length <code class="reqn">m</code> of dimension 
<code class="reqn">n</code> by <code class="reqn">p_j</code> designating values of the confounders for each exposure 
value or if <code>common</code> is TRUE a single matrix of of dimension <code class="reqn">n</code> by
<code class="reqn">p</code> that represents common confounders for all exposures.</p>
</td></tr>
<tr><td><code id="mvGPS_+3A_common">common</code></td>
<td>
<p>logical indicator for whether C is a single matrix of common
confounders for all exposures. default is FALSE meaning C must be specified
as list of confounders of length <code class="reqn">m</code>.</p>
</td></tr>
<tr><td><code id="mvGPS_+3A_trim_w">trim_w</code></td>
<td>
<p>logical indicator for whether to trim weights. default is FALSE</p>
</td></tr>
<tr><td><code id="mvGPS_+3A_trim_quantile">trim_quantile</code></td>
<td>
<p>numeric scalar used to specify the upper quantile to 
trim weights if applicable. default is 0.99</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generalized propensity scores (GPS) were proposed by 
Hirano and Imbens (2004) and 
Imai and Van Dyk (2004) to extend propensity scores to
handle continuous exposures. The GPS is constructed using the conditional 
density of the exposure given a set of confounders. These original methods 
and the subsequent literature have focused on the case of a single continuous
exposure where the GPS could be estimated using normal densities, kernel 
smoothing (Kennedy et al. 2017), gradient boosting 
(Zhu et al. 2015), and empirical likelihoods 
(Fong et al. 2018). In this package we provide an extension to this
literature to allow for multivariate continuous exposures to be estimated.
</p>


<h4>Notation</h4>

<p>Assume that we have a set of continuous exposures, <code class="reqn">D</code>, of length 
<code>m</code>, i.e., <code class="reqn">\mathbf{D}=D_{1}, \dots, D_{m}</code> collected on <code class="reqn">n</code> 
units. Further, we assume that there exists a set of confounders 
<code class="reqn">\mathbf{C}_{1},\dots,\mathbf{C}_{m}</code> for each
exposure of length <code class="reqn">p_{j}</code> for <code class="reqn">j=1,\dots,m</code>. The confounders are 
related to both the exposures and the outcome of interest such. Note that
the confounders need not be identical for all exposures. 
</p>
<p>In our function we therefore expect that the argument <code>D</code> is a numeric
matrix of dimension <code class="reqn">n\times m</code>, and that <code>C</code> is a list of length
<code class="reqn">m</code> where each element is a matrix of dimension <code class="reqn">n\times p_{j}</code>. For 
the case where we assume that all exposures have common confounders we 
set <code>common=TRUE</code> and <code>C</code> must be a matrix of dimension 
<code class="reqn">n\times p</code>.
</p>



<h4>mvGPS</h4>

<p>We define the multivariate generalized propensity score, mvGPS, as
</p>
<p style="text-align: center;"><code class="reqn">mvGPS=f_{\mathbf{D}\mid \mathbf{C}_{1},\dots,\mathbf{C}_{m}}</code>
</p>

<p>where <code class="reqn">f</code> represents a joint multivariate conditional density function.
For our current development we specify <code class="reqn">f</code> as multivariate normal, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{D}\mid \mathbf{C}_{1},\dots,\mathbf{C}_{m}\sim N_{m}(\boldsymbol{\mu}, \Sigma).</code>
</p>

<p>Factorizing this joint density we can rewrite the mvGPS as the product of 
univariate conditional densities, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">mvGPS=f_{D_{m}\mid \mathbf{C}_{m}, D_{m-1},\dots,D_{1}}\times\cdots\times f_{D_{1}\mid\mathbf{C}_{1}}.</code>
</p>

<p>We use this factorized version in our implementation, with parameters for each
distribution estimated through least squares.
</p>



<h4>Constructing Weights</h4>

<p>Following Robins et al. (2000), we use the mvGPS to 
construct stabilized inverse probability of treatment (IPTW) weights. These 
have been shown to balance confounders and return unbiased estimated of the
dose-response. Weights are constructed as
</p>
<p style="text-align: center;"><code class="reqn">w=\frac{f_{\mathbf{D}}}{f_{\mathbf{D}\mid \mathbf{C}_{1},\dots,\mathbf{C}_{m}}},</code>
</p>

<p>where the marginal density <code class="reqn">f_{\mathbf{D}}</code> of the exposures is assumed 
to be multivariate normal as well.
</p>
<p>Writing the weights using completely factorized densities we have
</p>
<p style="text-align: center;"><code class="reqn">w=\frac{f_{D_{m}\mid D_{m-1},\dots, D_{1}}\times\cdots\times f_{D_{1}}}{f_{D_{m}\mid \mathbf{C}_{m}, D_{m-1},\dots,D_{1}}\times\cdots\times f_{D_{1}\mid\mathbf{C}_{1}}}.</code>
</p>




<h4>Trimming</h4>

<p>Often when using weights based on the propensity score, practitioners are 
concerned about the effect of extreme weights. It has been shown that an 
effective way to protect extreme weights is to trim them at a particular 
percentile (Crump et al. 2009; Lee et al. 2011). We allow
users to specify whether trimmed weights should be returned and at which
threshold. To trim weights set <code>trim_w=TRUE</code> and specify the desired
percentile as <code>trim_quantile=q</code>. Note that trimming is applied at 
both the upper and lower percentile thresholds, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">w^{*}=w 1_{\{Q(w, 1-q)\le w \le Q(w, q)\}}+Q(w, 1-q) 1_{\{w &lt; Q(w, 1-q)\}} + Q(w, q) 1_{\{w &gt; Q(w, q)\}}</code>
</p>




<h3>Value</h3>

<p>list of score and wts, where score is the mvGPS score values and 
wts are the corresponding stabilized inverse probability of treatment weights
</p>


<h3>References</h3>

<p>Crump RK, Hotz VJ, Imbens GW, Mitnik OA (2009).
&ldquo;Dealing with limited overlap in estimation of average treatment effects.&rdquo;
<em>Biometrika</em>, <b>96</b>(1), 187-199.<br /><br /> Fong C, Hazlett C, Imai K (2018).
&ldquo;Covariate balancing propensity score for a continuous treatment: application to the efficacy of political advertisements.&rdquo;
<em>Annals of Applied Statistics</em>, <b>In-Press</b>.<br /><br /> Hirano K, Imbens GW (2004).
&ldquo;The propensity score with continuous treatments.&rdquo;
In Gelman A, Meng X (eds.), <em>Applied Bayesian Modeling and Causal Inference from Incomplete-Data Perspectives</em>, 73-84.<br /><br /> Imai K, Van Dyk DA (2004).
&ldquo;Causal inference with general treatment regimes: generalizing the propensity score.&rdquo;
<em>Journal of the American Statistical Association</em>, <b>99</b>(467), 854-866.<br /><br /> Kennedy EH, Ma Z, McHugh MD, Small DS (2017).
&ldquo;Non-parametric methods for doubly robust estimation of continuous treatment effects.&rdquo;
<em>Journal of the Royal Statistical Society: Series B</em>, <b>79</b>(4), 1229-1245.<br /><br /> Lee BK, Lessler J, Stuart EA (2011).
&ldquo;Weight trimming and propensity score weighting.&rdquo;
<em>PloS One</em>, <b>6</b>(3).<br /><br /> Robins JM, Hernan MA, Brumback B (2000).
&ldquo;Marginal structural models and causal inference in epidemiology.&rdquo;
<em>Epidemiology</em>, <b>11</b>(5), 550-560.<br /><br /> Zhu Y, Coffman DL, Ghosh D (2015).
&ldquo;A boosting algorithm for estimating generalized propensity scores with continuous treatments.&rdquo;
<em>Journal of Causal Inference</em>, <b>3</b>(1), 25-40.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#generating confounded bivariate exposure
sim_dt &lt;- gen_D(method="u", n=200, rho_cond=0.2, s_d1_cond=2, s_d2_cond=2, k=3, 
C_mu=rep(0, 3), C_cov=0.1, C_var=1, d1_beta=c(0.5, 1, 0), d2_beta=c(0, 0.3, 0.75), seed=06112020)
D &lt;- sim_dt$D
C &lt;- sim_dt$C

#generating weights and mvGPS
out_mvGPS &lt;- mvGPS(D=D, C=list(C[, 1:2], C[, 2:3]))

# can apply trimming with default 99th percentile
out_mvGPS_trim &lt;- mvGPS(D=D, C=list(C[, 1:2], C[, 2:3]), trim_w=TRUE)

#or assume all exposures have equivalent confounders
out_mvGPS_common &lt;- mvGPS(D=D, C=C, common=TRUE)

</code></pre>

<hr>
<h2 id='X_check'>Checking that the exposure matrix is properly specified</h2><span id='topic+X_check'></span>

<h3>Description</h3>

<p>Checking that the exposure matrix is properly specified
</p>


<h3>Usage</h3>

<pre><code class='language-R'>X_check(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="X_check_+3A_x">X</code></td>
<td>
<p>numeric matrix of n by m dimensions. Each row corresponds to a point in m-dimensional space.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
