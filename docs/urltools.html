<!DOCTYPE html><html lang="en"><head><title>Help for package urltools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {urltools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#urltools'><p>Tools for handling URLs</p></a></li>
<li><a href='#domain'><p>Get or set a URL's domain</p></a></li>
<li><a href='#fragment'><p>Get or set a URL's fragment</p></a></li>
<li><a href='#host_extract'><p>Extract hosts</p></a></li>
<li><a href='#param_get'><p>get the values of a URL's parameters</p></a></li>
<li><a href='#param_remove'><p>Remove key-value pairs from query strings</p></a></li>
<li><a href='#param_set'><p>Set the value associated with a parameter in a URL's query.</p></a></li>
<li><a href='#parameters'><p>Get or set a URL's parameters</p></a></li>
<li><a href='#path'><p>Get or set a URL's path</p></a></li>
<li><a href='#port'><p>Get or set a URL's port</p></a></li>
<li><a href='#puny_encode'><p>Encode or Decode Internationalised Domains</p></a></li>
<li><a href='#scheme'><p>Get or set a URL's scheme</p></a></li>
<li><a href='#strip_credentials'><p>Get or remove user authentication credentials</p></a></li>
<li><a href='#suffix_dataset'><p>Dataset of public suffixes</p></a></li>
<li><a href='#suffix_extract'><p>extract the suffix from domain names</p></a></li>
<li><a href='#suffix_refresh'><p>Retrieve a public suffix dataset</p></a></li>
<li><a href='#tld_dataset'><p>Dataset of top-level domains (TLDs)</p></a></li>
<li><a href='#tld_extract'><p>Extract TLDs</p></a></li>
<li><a href='#tld_refresh'><p>Retrieve a TLD dataset</p></a></li>
<li><a href='#url_compose'><p>Recompose Parsed URLs</p></a></li>
<li><a href='#url_decode'><p>Encode or decode a URI</p></a></li>
<li><a href='#url_parse'><p>split URLs into their component parts</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Vectorised Tools for URL Handling and Parsing</td>
</tr>
<tr>
<td>Version:</td>
<td>1.7.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-04-14</td>
</tr>
<tr>
<td>Author:</td>
<td>Os Keyes [aut, cre], Jay Jacobs [aut, cre], Drew Schmidt [aut],
    Mark Greenaway [ctb], Bob Rudis [ctb], Alex Pinto [ctb], Maryam Khezrzadeh [ctb], Peter Meilstrup [ctb],
    Adam M. Costello [cph], Jeff Bezanson [cph], Peter Meilstrup [ctb], Xueyuan Jiang [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Os Keyes &lt;ironholds@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A toolkit for all URL-handling needs, including encoding and decoding,
    parsing, parameter extraction and modification. All functions are
    designed to be both fast and entirely vectorised. It is intended to be
    useful for people dealing with web-related datasets, such as server-side
    logs, although may be useful for other situations involving large sets of
    URLs.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, methods, triebeard</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/Ironholds/urltools/">https://github.com/Ironholds/urltools/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/Ironholds/urltools/issues">https://github.com/Ironholds/urltools/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-04-14 22:25:08 UTC; ironholds</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-04-14 23:02:47 UTC</td>
</tr>
</table>
<hr>
<h2 id='urltools'>Tools for handling URLs</h2><span id='topic+urltools'></span><span id='topic+urltools-package'></span>

<h3>Description</h3>

<p>This package provides functions for URL encoding and decoding,
parsing, and parameter extraction, designed to be both fast and
entirely vectorised. It is intended to be useful for people dealing with
web-related datasets, such as server-side logs.
</p>


<h3>See Also</h3>

<p>the <a href="https://CRAN.R-project.org/package=urltools/vignettes/urltools.html">package vignette</a>.
</p>

<hr>
<h2 id='domain'>Get or set a URL's domain</h2><span id='topic+domain'></span><span id='topic+domain+3C-'></span>

<h3>Description</h3>

<p>as in the lubridate package, individual components of a URL
can be both extracted or set using the relevant function call - see the
examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>domain(x)

domain(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="domain_+3A_x">x</code></td>
<td>
<p>a URL, or vector of URLs</p>
</td></tr>
<tr><td><code id="domain_+3A_value">value</code></td>
<td>
<p>a replacement value (or vector of replacement values)
for x's scheme.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+scheme">scheme</a></code>, <code><a href="#topic+port">port</a></code>, <code><a href="#topic+path">path</a></code>,
<code><a href="#topic+parameters">parameters</a></code> and <code><a href="#topic+fragment">fragment</a></code> for other accessors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Get a component
example_url &lt;- "http://cran.r-project.org/submit.html"
domain(example_url)

#Set a component
domain(example_url) &lt;- "en.wikipedia.org"
</code></pre>

<hr>
<h2 id='fragment'>Get or set a URL's fragment</h2><span id='topic+fragment'></span><span id='topic+fragment+3C-'></span>

<h3>Description</h3>

<p>as in the lubridate package, individual components of a URL
can be both extracted or set using the relevant function call - see the
examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fragment(x)

fragment(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fragment_+3A_x">x</code></td>
<td>
<p>a URL, or vector of URLs</p>
</td></tr>
<tr><td><code id="fragment_+3A_value">value</code></td>
<td>
<p>a replacement value (or vector of replacement values)
for x's fragment. If NULL, the fragment will be removed entirely.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+scheme">scheme</a></code>, <code><a href="#topic+domain">domain</a></code>, <code><a href="#topic+port">port</a></code>,
<code><a href="#topic+path">path</a></code> and <code><a href="#topic+parameters">parameters</a></code> for other accessors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Get a component
example_url &lt;- "http://en.wikipedia.org/wiki/Aaron_Halfaker?debug=true#test"
fragment(example_url)

#Set a component
fragment(example_url) &lt;- "production"

#Remove a component
fragment(example_url) &lt;- NULL
</code></pre>

<hr>
<h2 id='host_extract'>Extract hosts</h2><span id='topic+host_extract'></span>

<h3>Description</h3>

<p><code>host_extract</code> extracts the host from
a vector of domain names. A host isn't the same as a domain - it could be
the subdomain, if there are one or more subdomains. The host of <code>en.wikipedia.org</code>
is <code>en</code>, while the host of <code>wikipedia.org</code> is <code>wikipedia</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>host_extract(domains)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="host_extract_+3A_domains">domains</code></td>
<td>
<p>a vector of domains, retrieved through <code><a href="#topic+url_parse">url_parse</a></code> or
<code><a href="#topic+domain">domain</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame of two columns: <code>domain</code>, with the original domain names,
and <code>host</code>, the identified host from the domain.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># With subdomains
has_subdomain &lt;- domain("https://en.wikipedia.org/wiki/Main_Page")
host_extract(has_subdomain)

# Without
no_subdomain &lt;- domain("https://ironholds.org/projects/r_shiny/")
host_extract(no_subdomain)
</code></pre>

<hr>
<h2 id='param_get'>get the values of a URL's parameters</h2><span id='topic+param_get'></span><span id='topic+url_parameter'></span>

<h3>Description</h3>

<p>URLs can have parameters, taking the form of <code>name=value</code>, chained together
with <code>&amp;</code> symbols. <code>param_get</code>, when provided with a vector of URLs and a vector
of parameter names, will generate a data.frame consisting of the values of each parameter
for each URL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>param_get(urls, parameter_names = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="param_get_+3A_urls">urls</code></td>
<td>
<p>a vector of URLs</p>
</td></tr>
<tr><td><code id="param_get_+3A_parameter_names">parameter_names</code></td>
<td>
<p>a vector of parameter names. If <code>NULL</code> (default), will extract
all parameters that are present.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame containing one column for each provided parameter name. Values that
cannot be found within a particular URL are represented by an NA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+url_parse">url_parse</a></code> for decomposing URLs into their constituent parts and
<code><a href="#topic+param_set">param_set</a></code> for inserting or modifying key/value pairs within a query string.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#A very simple example
url &lt;- "https://google.com:80/foo.php?this_parameter=selfreferencing&amp;hiphop=awesome"
parameter_values &lt;- param_get(url, c("this_parameter","hiphop"))

</code></pre>

<hr>
<h2 id='param_remove'>Remove key-value pairs from query strings</h2><span id='topic+param_remove'></span>

<h3>Description</h3>

<p>URLs often have queries associated with them, particularly URLs for
APIs, that look like <code>?key=value&amp;key=value&amp;key=value</code>. <code>param_remove</code>
allows you to remove key/value pairs while leaving the rest of the URL intact.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>param_remove(urls, keys)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="param_remove_+3A_urls">urls</code></td>
<td>
<p>a vector of URLs. These should be decoded with <code>url_decode</code> but don't
have to have been otherwise processed.</p>
</td></tr>
<tr><td><code id="param_remove_+3A_keys">keys</code></td>
<td>
<p>a vector of parameter keys to remove.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the original URLs but with the key/value pairs specified by <code>keys</code> removed.
If the original URL is <code>NA</code>, <code>NA</code> will be returned; if a specified key is <code>NA</code>,
nothing will be done with it.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+param_set">param_set</a></code> to modify values associated with keys, or <code><a href="#topic+param_get">param_get</a></code>
to retrieve those values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Remove multiple parameters from a URL
param_remove(urls = "https://en.wikipedia.org/wiki/api.php?action=list&amp;type=query&amp;format=json",
            keys = c("action","format"))
</code></pre>

<hr>
<h2 id='param_set'>Set the value associated with a parameter in a URL's query.</h2><span id='topic+param_set'></span>

<h3>Description</h3>

<p>URLs often have queries associated with them, particularly URLs for
APIs, that look like <code>?key=value&amp;key=value&amp;key=value</code>. <code>param_set</code>
allows you to modify key/value pairs within query strings, or even add new ones
if they don't exist within the URL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>param_set(urls, key, value)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="param_set_+3A_urls">urls</code></td>
<td>
<p>a vector of URLs. These should be decoded (with <code>url_decode</code>)
but do not have to have been otherwise manipulated.</p>
</td></tr>
<tr><td><code id="param_set_+3A_key">key</code></td>
<td>
<p>a string representing the key to modify the value of (or insert wholesale
if it doesn't exist within the URL).</p>
</td></tr>
<tr><td><code id="param_set_+3A_value">value</code></td>
<td>
<p>a value to associate with the key. This can be a single string,
or a vector the same length as <code>urls</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the original vector of URLs, but with modified/inserted key-value pairs. If the
URL is <code>NA</code>, the returned value will be - if the key or value are, no insertion
will be made.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+param_get">param_get</a></code> to retrieve the values associated with multiple keys in
a vector of URLs, and <code><a href="#topic+param_remove">param_remove</a></code> to strip key/value pairs from a URL entirely.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Set a URL parameter where there's already a key for that
param_set("https://en.wikipedia.org/api.php?action=query", "action", "pageinfo")

# Set a URL parameter where there isn't.
param_set("https://en.wikipedia.org/api.php?list=props", "action", "pageinfo")

</code></pre>

<hr>
<h2 id='parameters'>Get or set a URL's parameters</h2><span id='topic+parameters'></span><span id='topic+parameters+3C-'></span>

<h3>Description</h3>

<p>as in the lubridate package, individual components of a URL
can be both extracted or set using the relevant function call - see the
examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parameters(x)

parameters(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="parameters_+3A_x">x</code></td>
<td>
<p>a URL, or vector of URLs</p>
</td></tr>
<tr><td><code id="parameters_+3A_value">value</code></td>
<td>
<p>a replacement value (or vector of replacement values)
for x's parameters. If NULL, the parameters will be removed entirely.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+scheme">scheme</a></code>, <code><a href="#topic+domain">domain</a></code>, <code><a href="#topic+port">port</a></code>,
<code><a href="#topic+path">path</a></code> and <code><a href="#topic+fragment">fragment</a></code> for other accessors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Get the parameters
example_url &lt;- "http://en.wikipedia.org/wiki/Aaron_Halfaker?debug=true"
parameters(example_url)

# Set the parameters
parameters(example_url) &lt;- "debug=false"

# Remove the parameters
parameters(example_url) &lt;- NULL
</code></pre>

<hr>
<h2 id='path'>Get or set a URL's path</h2><span id='topic+path'></span><span id='topic+path+3C-'></span>

<h3>Description</h3>

<p>as in the lubridate package, individual components of a URL
can be both extracted or set using the relevant function call - see the
examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>path(x)

path(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="path_+3A_x">x</code></td>
<td>
<p>a URL, or vector of URLs</p>
</td></tr>
<tr><td><code id="path_+3A_value">value</code></td>
<td>
<p>a replacement value (or vector of replacement values)
for x's path. If NULL, the path will be removed entirely.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+scheme">scheme</a></code>, <code><a href="#topic+domain">domain</a></code>, <code><a href="#topic+port">port</a></code>,
<code><a href="#topic+parameters">parameters</a></code> and <code><a href="#topic+fragment">fragment</a></code> for other accessors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Get the path
example_url &lt;- "http://cran.r-project.org:80/submit.html"
path(example_url)

# Set the path
path(example_url) &lt;- "bin/windows/"

# Remove the path
path(example_url) &lt;- NULL
</code></pre>

<hr>
<h2 id='port'>Get or set a URL's port</h2><span id='topic+port'></span><span id='topic+port+3C-'></span>

<h3>Description</h3>

<p>as in the lubridate package, individual components of a URL
can be both extracted or set using the relevant function call - see the
examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>port(x)

port(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="port_+3A_x">x</code></td>
<td>
<p>a URL, or vector of URLs</p>
</td></tr>
<tr><td><code id="port_+3A_value">value</code></td>
<td>
<p>a replacement value (or vector of replacement values)
for x's port. If NULL, the port will be entirely removed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+scheme">scheme</a></code>, <code><a href="#topic+domain">domain</a></code>, <code><a href="#topic+path">path</a></code>,
<code><a href="#topic+parameters">parameters</a></code> and <code><a href="#topic+fragment">fragment</a></code> for other accessors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Get the port
example_url &lt;- "http://cran.r-project.org:80/submit.html"
port(example_url)

# Set the port
port(example_url) &lt;- "12"

# Remove the port
port(example_url) &lt;- NULL
</code></pre>

<hr>
<h2 id='puny_encode'>Encode or Decode Internationalised Domains</h2><span id='topic+puny_encode'></span><span id='topic+puny_decode'></span>

<h3>Description</h3>

<p><code>puny_encode</code> and <code>puny_decode</code> implement
the encoding standard for internationalised (non-ASCII) domains and
subdomains. You can use them to encode UTF-8 domain names, or decode
encoded names (which start &quot;xn&ndash;&quot;), or both.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>puny_encode(x)

puny_decode(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="puny_encode_+3A_x">x</code></td>
<td>
<p>a vector of URLs. These should be URL decoded using <code><a href="#topic+url_decode">url_decode</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a CharacterVector containing encoded or decoded versions of the entries in <code>x</code>.
Invalid URLs (ones that are <code>NA</code>, or ones that do not successfully map to an actual
decoded or encoded version) will be returned as <code>NA</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+url_decode">url_decode</a></code> and <code><a href="#topic+url_encode">url_encode</a></code> for percent-encoding.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Encode a URL
puny_encode("https://www.bücher.com/foo")

# Decode the result, back to the original
puny_decode("https://www.xn--bcher-kva.com/foo")

</code></pre>

<hr>
<h2 id='scheme'>Get or set a URL's scheme</h2><span id='topic+scheme'></span><span id='topic+scheme+3C-'></span>

<h3>Description</h3>

<p>as in the lubridate package, individual components of a URL
can be both extracted or set using the relevant function call - see the
examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scheme(x)

scheme(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="scheme_+3A_x">x</code></td>
<td>
<p>a URL, or vector of URLs</p>
</td></tr>
<tr><td><code id="scheme_+3A_value">value</code></td>
<td>
<p>a replacement value (or vector of replacement values)
for x's scheme.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+domain">domain</a></code>, <code><a href="#topic+port">port</a></code>, <code><a href="#topic+path">path</a></code>,
<code><a href="#topic+parameters">parameters</a></code> and <code><a href="#topic+fragment">fragment</a></code> for other accessors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Get a component
example_url &lt;- "http://cran.r-project.org/submit.html"
scheme(example_url)

#Set a component
scheme(example_url) &lt;- "https"

# NA out the URL
scheme(example_url) &lt;- NA_character_
</code></pre>

<hr>
<h2 id='strip_credentials'>Get or remove user authentication credentials</h2><span id='topic+strip_credentials'></span><span id='topic+creds'></span><span id='topic+get_credentials'></span>

<h3>Description</h3>

<p>authentication credentials appear before the domain
name and look like <em>user:password</em>. Sometimes you want the removed,
or retrieved; <code>strip_credentials</code> and <code>get_credentials</code> do
precisely that
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strip_credentials(urls)

get_credentials(urls)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="strip_credentials_+3A_urls">urls</code></td>
<td>
<p>a URL, or vector of URLs</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Remove credentials
strip_credentials("http://foo:bar@97.77.104.22:3128")

# Get credentials
get_credentials("http://foo:bar@97.77.104.22:3128")
</code></pre>

<hr>
<h2 id='suffix_dataset'>Dataset of public suffixes</h2><span id='topic+suffix_dataset'></span>

<h3>Description</h3>

<p>This dataset contains a registry of public suffixes, as retrieved from
and defined by the <a href="https://publicsuffix.org/">public suffix list</a>. It is
sorted by how many periods(&quot;.&quot;) appear in the suffix, to optimise it for
<code><a href="#topic+suffix_extract">suffix_extract</a></code>.  It is a data.frame with two columns, the first is
the list of suffixes and the second is our best guess at the comment or owner 
associated with the particular suffix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(suffix_dataset)
</code></pre>


<h3>Format</h3>

<p>A data.frame of 8030 rows and 2 columns</p>


<h3>Note</h3>

<p>Last updated 2016-07-31.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+suffix_extract">suffix_extract</a></code> for extracting suffixes from domain names,
and <code><a href="#topic+suffix_refresh">suffix_refresh</a></code> for getting a new, totally-up-to-date dataset
version.
</p>

<hr>
<h2 id='suffix_extract'>extract the suffix from domain names</h2><span id='topic+suffix_extract'></span>

<h3>Description</h3>

<p>domain names have suffixes - common endings that people
can or could register domains under. This includes things like &quot;.org&quot;, but
also things like &quot;.edu.co&quot;. A simple Top Level Domain list, as a
result, probably won't cut it.
</p>
<p><code><a href="#topic+suffix_extract">suffix_extract</a></code> takes the list of public suffixes,
as maintained by Mozilla (see <code><a href="#topic+suffix_dataset">suffix_dataset</a></code>) and
a vector of domain names, and produces a data.frame containing the
suffix that each domain uses, and the remaining fragment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>suffix_extract(domains, suffixes = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="suffix_extract_+3A_domains">domains</code></td>
<td>
<p>a vector of damains, from <code><a href="#topic+domain">domain</a></code>
or <code><a href="#topic+url_parse">url_parse</a></code>. Alternately, full URLs can be provided
and will then be run through <code><a href="#topic+domain">domain</a></code> internally.</p>
</td></tr>
<tr><td><code id="suffix_extract_+3A_suffixes">suffixes</code></td>
<td>
<p>a dataset of suffixes. By default, this is NULL and the function
relies on <code><a href="#topic+suffix_dataset">suffix_dataset</a></code>. Optionally, if you want more updated
suffix data, you can provide the result of <code><a href="#topic+suffix_refresh">suffix_refresh</a></code> for
this parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame of four columns, &quot;host&quot; &quot;subdomain&quot;, &quot;domain&quot; &amp; &quot;suffix&quot;.
&quot;host&quot; is what was passed in. &quot;subdomain&quot; is the subdomain of the suffix.
&quot;domain&quot; contains the part of the domain name that came before the matched suffix.
&quot;suffix&quot; is, well, the suffix.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+suffix_dataset">suffix_dataset</a></code> for the dataset of suffixes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Using url_parse
domain_name &lt;- url_parse("http://en.wikipedia.org")$domain
suffix_extract(domain_name)

# Using domain()
domain_name &lt;- domain("http://en.wikipedia.org")
suffix_extract(domain_name)

## Not run: 
#Relying on a fresh version of the suffix dataset
suffix_extract(domain("http://en.wikipedia.org"), suffix_refresh())

## End(Not run)

</code></pre>

<hr>
<h2 id='suffix_refresh'>Retrieve a public suffix dataset</h2><span id='topic+suffix_refresh'></span>

<h3>Description</h3>

<p><code>urltools</code> comes with an inbuilt
dataset of public suffixes, <code><a href="#topic+suffix_dataset">suffix_dataset</a></code>.
This is used in <code><a href="#topic+suffix_extract">suffix_extract</a></code> to identify the top-level domain
within a particular domain name.
</p>
<p>While updates to the dataset will be included in each new package release,
there's going to be a gap between changes to the suffixes list and changes to the package.
Accordingly, the package also includes <code>suffix_refresh</code>, which generates
and returns a <em>fresh</em> version of the dataset. This can then be passed through
to <code><a href="#topic+suffix_extract">suffix_extract</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>suffix_refresh()
</code></pre>


<h3>Value</h3>

<p>a dataset equivalent in format to <code><a href="#topic+suffix_dataset">suffix_dataset</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+suffix_extract">suffix_extract</a></code> to extract suffixes from domain names,
or <code><a href="#topic+suffix_dataset">suffix_dataset</a></code> for the inbuilt, default version of the data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
new_suffixes &lt;- suffix_refresh()

## End(Not run)

</code></pre>

<hr>
<h2 id='tld_dataset'>Dataset of top-level domains (TLDs)</h2><span id='topic+tld_dataset'></span>

<h3>Description</h3>

<p>This dataset contains a registry of top-level domains, as retrieved from
and defined by the <a href="http://data.iana.org/TLD/tlds-alpha-by-domain.txt">IANA</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(tld_dataset)
</code></pre>


<h3>Format</h3>

<p>A vector of 1275 elements.</p>


<h3>Note</h3>

<p>Last updated 2016-07-20.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tld_extract">tld_extract</a></code> for extracting TLDs from domain names,
and <code><a href="#topic+tld_refresh">tld_refresh</a></code> to get an updated version of this dataset.
</p>

<hr>
<h2 id='tld_extract'>Extract TLDs</h2><span id='topic+tld_extract'></span>

<h3>Description</h3>

<p><code>tld_extract</code> extracts the top-level domain (TLD) from
a vector of domain names. This is distinct from the suffixes, extracted with
<code><a href="#topic+suffix_extract">suffix_extract</a></code>; TLDs are <em>top</em> level, while suffixes are just
domains through which internet users can publicly register domains (the difference
between <code>.org.uk</code> and <code>.uk</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tld_extract(domains, tlds = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tld_extract_+3A_domains">domains</code></td>
<td>
<p>a vector of domains, retrieved through <code><a href="#topic+url_parse">url_parse</a></code> or
<code><a href="#topic+domain">domain</a></code>.</p>
</td></tr>
<tr><td><code id="tld_extract_+3A_tlds">tlds</code></td>
<td>
<p>a dataset of TLDs. If NULL (the default), <code>tld_extract</code> relies
on urltools' <code><a href="#topic+tld_dataset">tld_dataset</a></code>; otherwise, you can pass in the result of
<code><a href="#topic+tld_refresh">tld_refresh</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame of two columns: <code>domain</code>, with the original domain names,
and <code>tld</code>, the identified TLD from the domain.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+suffix_extract">suffix_extract</a></code> for retrieving suffixes (distinct from TLDs).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Using the inbuilt dataset
domains &lt;- domain("https://en.wikipedia.org/wiki/Main_Page")
tld_extract(domains)

# Using a refreshed one
tld_extract(domains, tld_refresh())

</code></pre>

<hr>
<h2 id='tld_refresh'>Retrieve a TLD dataset</h2><span id='topic+tld_refresh'></span>

<h3>Description</h3>

<p><code>urltools</code> comes with an inbuilt
dataset of top level domains (TLDs), <code><a href="#topic+tld_dataset">tld_dataset</a></code>.
This is used in <code><a href="#topic+tld_extract">tld_extract</a></code> to identify the top-level domain
within a particular domain name.
</p>
<p>While updates to the dataset will be included in each new package release,
there's going to be a gap between changes to TLDs and changes to the package.
Accordingly, the package also includes <code>tld_refresh</code>, which generates
and returns a <em>fresh</em> version of the dataset. This can then be passed through
to <code><a href="#topic+tld_extract">tld_extract</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tld_refresh()
</code></pre>


<h3>Value</h3>

<p>a dataset equivalent in format to <code><a href="#topic+tld_dataset">tld_dataset</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tld_extract">tld_extract</a></code> to extract suffixes from domain names,
or <code><a href="#topic+tld_dataset">tld_dataset</a></code> for the inbuilt, default version of the data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
new_tlds &lt;- tld_refresh()

## End(Not run)

</code></pre>

<hr>
<h2 id='url_compose'>Recompose Parsed URLs</h2><span id='topic+url_compose'></span>

<h3>Description</h3>

<p>Sometimes you want to take a vector of URLs, parse them, perform
some operations and then rebuild them. <code>url_compose</code> takes a data.frame produced
by <code><a href="#topic+url_parse">url_parse</a></code> and rebuilds it into a vector of full URLs (or: URLs as full
as the vector initially thrown into url_parse).
</p>
<p>This is currently a 'beta' feature; please do report bugs if you find them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>url_compose(parsed_urls)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="url_compose_+3A_parsed_urls">parsed_urls</code></td>
<td>
<p>a data.frame sourced from <code><a href="#topic+url_parse">url_parse</a></code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+scheme">scheme</a></code> and other accessors, which you may want to
run URLs through before composing them to modify individual values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Parse a URL and compose it
url &lt;- "http://en.wikipedia.org"
url_compose(url_parse(url))

</code></pre>

<hr>
<h2 id='url_decode'>Encode or decode a URI</h2><span id='topic+url_decode'></span><span id='topic+url_encode'></span>

<h3>Description</h3>

<p>encodes or decodes a URI/URL
</p>


<h3>Usage</h3>

<pre><code class='language-R'>url_decode(urls)

url_encode(urls)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="url_decode_+3A_urls">urls</code></td>
<td>
<p>a vector of URLs to decode or encode.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>URL encoding and decoding is an essential prerequisite to proper web interaction
and data analysis around things like server-side logs. The
<a href="http://tools.ietf.org/html/rfc3986">relevant IETF RfC</a> mandates the percentage-encoding
of non-Latin characters, including things like slashes, unless those are reserved.
</p>
<p>Base R provides <code><a href="utils.html#topic+URLdecode">URLdecode</a></code> and <code><a href="utils.html#topic+URLencode">URLencode</a></code>, which handle
URL encoding - in theory. In practise, they have a set of substantial problems
that the urltools implementation solves::
</p>

<ul>
<li><p>No vectorisation: Both base R functions operate on single URLs, not vectors of URLs.
This means that, when confronted with a vector of URLs that need encoding or
decoding, your only option is to loop from within R. This can be incredibly
computationally costly with large datasets. url_encode and url_decode are
implemented in C++ and entirely vectorised, allowing for a substantial
performance improvement.
</p>
</li>
<li><p>No scheme recognition: encoding the slashes in, say, http://, is a good way
of making sure your URL no longer works. Because of this, the only thing
you can encode in URLencode (unless you refuse to encode reserved characters)
is a partial URL, lacking the initial scheme, which requires additional operations
to set up and increases the complexity of encoding or decoding. url_encode
detects the protocol and silently splits it off, leaving it unencoded to ensure
that the resulting URL is valid.
</p>
</li>
<li><p>ASCII NULs: Server side data can get very messy and sometimes include out-of-range
characters. Unfortunately, URLdecode's response to these characters is to convert
them to NULs, which R can't handle, at which point your URLdecode call breaks.
<code>url_decode</code> simply ignores them.
</p>
</li></ul>



<h3>Value</h3>

<p>a character vector containing the encoded (or decoded) versions of &quot;urls&quot;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+puny_decode">puny_decode</a></code> and <code><a href="#topic+puny_encode">puny_encode</a></code>, for punycode decoding
and encoding.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
url_decode("https://en.wikipedia.org/wiki/File:Vice_City_Public_Radio_%28logo%29.jpg")
url_encode("https://en.wikipedia.org/wiki/File:Vice_City_Public_Radio_(logo).jpg")

## Not run: 
#A demonstrator of the contrasting behaviours around out-of-range characters
URLdecode("%gIL")
url_decode("%gIL")

## End(Not run)
</code></pre>

<hr>
<h2 id='url_parse'>split URLs into their component parts</h2><span id='topic+url_parse'></span>

<h3>Description</h3>

<p><code>url_parse</code> takes a vector of URLs and splits each one into its component
parts, as recognised by RfC 3986.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>url_parse(urls)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="url_parse_+3A_urls">urls</code></td>
<td>
<p>a vector of URLs</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It's useful to be able to take a URL and split it out into its component parts - 
for the purpose of hostname extraction, for example, or analysing API calls. This functionality
is not provided in base R, although it is provided in <code><a href="httr.html#topic+parse_url">parse_url</a></code>; that
implementation is entirely in R, uses regular expressions, and is not vectorised. It's
perfectly suitable for the intended purpose (decomposition in the context of automated
HTTP requests from R), but not for large-scale analysis.
</p>
<p>Note that user authentication/identification information is not extracted;
this can be found with <code><a href="#topic+get_credentials">get_credentials</a></code>.
</p>


<h3>Value</h3>

<p>a data.frame consisting of the columns scheme, domain, port, path, query
and fragment. See the '<a href="http://tools.ietf.org/html/rfc3986">relevant IETF RfC</a> for
definitions. If an element cannot be identified, it is represented by an empty string.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+param_get">param_get</a></code> for extracting values associated with particular keys in a URL's
query string, and <code><a href="#topic+url_compose">url_compose</a></code>, which is <code>url_parse</code> in reverse.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>url_parse("https://en.wikipedia.org/wiki/Article")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
