<!DOCTYPE html><html><head><title>Help for package sjstats</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sjstats}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#anova_stats'><p>Effect size statistics for anova</p></a></li>
<li><a href='#auto_prior'><p>Create default priors for brms-models</p></a></li>
<li><a href='#boot_ci'><p>Standard error and confidence intervals for bootstrapped estimates</p></a></li>
<li><a href='#bootstrap'><p>Generate nonparametric bootstrap replications</p></a></li>
<li><a href='#chisq_gof'><p>Compute model quality</p></a></li>
<li><a href='#cramer'><p>Measures of association for contingency tables</p></a></li>
<li><a href='#cv'><p>Compute model quality</p></a></li>
<li><a href='#cv_error'><p>Test and training error from model cross-validation</p></a></li>
<li><a href='#design_effect'><p>Design effects for two-level mixed models</p></a></li>
<li><a href='#efc'><p>Sample dataset from the EUROFAMCARE project</p></a></li>
<li><a href='#find_beta'><p>Determining distribution parameters</p></a></li>
<li><a href='#gmd'><p>Gini's Mean Difference</p></a></li>
<li><a href='#inequ_trend'><p>Compute trends in status inequalities</p></a></li>
<li><a href='#is_prime'><p>Find prime numbers</p></a></li>
<li><a href='#mean_n'><p>Row means with min amount of valid values</p></a></li>
<li><a href='#means_by_group'><p>Summary of mean values by group</p></a></li>
<li><a href='#mwu'><p>Mann-Whitney-U-Test</p></a></li>
<li><a href='#nhanes_sample'><p>Sample dataset from the National Health and Nutrition Examination Survey</p></a></li>
<li><a href='#prop'><p>Proportions of values in a vector</p></a></li>
<li><a href='#r2'><p>Deprecated functions</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#samplesize_mixed'><p>Sample size for linear mixed models</p></a></li>
<li><a href='#se_ybar'><p>Standard error of sample mean for mixed models</p></a></li>
<li><a href='#survey_median'><p>Weighted statistics for tests and variables</p></a></li>
<li><a href='#svyglm.nb'><p>Survey-weighted negative binomial generalised linear model</p></a></li>
<li><a href='#svyglm.zip'><p>Survey-weighted zero-inflated Poisson model</p></a></li>
<li><a href='#table_values'><p>Expected and relative table values</p></a></li>
<li><a href='#var_pop'><p>Calculate population variance and standard deviation</p></a></li>
<li><a href='#weight'><p>Weight a variable</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Title:</td>
<td>Collection of Convenient Functions for Common Statistical
Computations</td>
</tr>
<tr>
<td>Version:</td>
<td>0.18.2</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Lüdecke &lt;d.luedecke@uke.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Collection of convenient functions for common statistical computations,
             which are not directly provided by R's base or stats packages.
             This package aims at providing, first, shortcuts for statistical measures, 
             which otherwise could only be calculated with additional effort 
             (like Cramer's V, Phi, or effect size statistics like Eta or Omega squared), 
             or for which currently no functions available. Second, another focus 
             lies on weighted variants of common statistical measures and tests 
             like weighted standard error, mean, t-test, correlation, and more.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4), utils</td>
</tr>
<tr>
<td>Imports:</td>
<td>bayestestR, broom, datawizard, dplyr, effectsize, emmeans,
insight, lme4, magrittr, MASS, modelr, parameters, performance,
purrr, rlang, sjlabelled, sjmisc, stats, tidyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>brms, car, coin, ggplot2, graphics, pscl, pwr, sjPlot,
survey, rstan, testthat</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://strengejacke.github.io/sjstats/">https://strengejacke.github.io/sjstats/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/strengejacke/sjstats/issues">https://github.com/strengejacke/sjstats/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.2</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-19 14:51:26 UTC; Daniel</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel Lüdecke <a href="https://orcid.org/0000-0002-8895-3206"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-11-19 22:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='anova_stats'>Effect size statistics for anova</h2><span id='topic+anova_stats'></span>

<h3>Description</h3>

<p>Returns the (partial) eta-squared, (partial) omega-squared,
epsilon-squared statistic or Cohen's F for all terms in an anovas.
<code>anova_stats()</code> returns a tidy summary, including all these statistics
and power for each term.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anova_stats(model, digits = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova_stats_+3A_model">model</code></td>
<td>
<p>A fitted anova-model of class <code>aov</code> or <code>anova</code>. Other
models are coerced to <code><a href="stats.html#topic+anova">anova</a></code>.</p>
</td></tr>
<tr><td><code id="anova_stats_+3A_digits">digits</code></td>
<td>
<p>Amount of digits for returned values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with all statistics is returned (excluding confidence intervals).
</p>


<h3>References</h3>

<p>Levine TR, Hullett CR (2002): Eta Squared, Partial Eta Squared, and Misreporting of Effect Size in Communication Research.
<br /> <br />
Tippey K, Longnecker MT (2016): An Ad Hoc Method for Computing Pseudo-Effect Size for Mixed Model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load sample data
data(efc)

# fit linear model
fit &lt;- aov(
  c12hour ~ as.factor(e42dep) + as.factor(c172code) + c160age,
  data = efc
)
## Not run: 
anova_stats(car::Anova(fit, type = 2))

## End(Not run)
</code></pre>

<hr>
<h2 id='auto_prior'>Create default priors for brms-models</h2><span id='topic+auto_prior'></span>

<h3>Description</h3>

<p>This function creates default priors for brms-regression
models, based on the same automatic prior-scale adjustment as in
<span class="pkg">rstanarm</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_prior(formula, data, gaussian, locations = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_prior_+3A_formula">formula</code></td>
<td>
<p>A formula describing the model, which just needs to contain
the model terms, but no notation of interaction, splines etc. Usually,
you want only those predictors in the formula, for which automatic
priors should be generated. Add informative priors afterwards to the
returned <code>brmsprior</code>-object.</p>
</td></tr>
<tr><td><code id="auto_prior_+3A_data">data</code></td>
<td>
<p>The data that will be used to fit the model.</p>
</td></tr>
<tr><td><code id="auto_prior_+3A_gaussian">gaussian</code></td>
<td>
<p>Logical, if the outcome is gaussian or not.</p>
</td></tr>
<tr><td><code id="auto_prior_+3A_locations">locations</code></td>
<td>
<p>A numeric vector with location values for the priors. If
<code>locations = NULL</code>, <code>0</code> is used as location parameter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>auto_prior()</code> is a small, convenient function to create
some default priors for brms-models with automatically adjusted prior
scales, in a similar way like <span class="pkg">rstanarm</span> does. The default scale for
the intercept is 10, for coefficients 2.5. If the outcome is gaussian,
both scales are multiplied with <code>sd(y)</code>. Then, for categorical
variables, nothing more is changed. For numeric variables, the scales
are divided by the standard deviation of the related variable.
<br /> <br />
All prior distributions are <em>normal</em> distributions. <code>auto_prior()</code>
is intended to quickly create default priors with feasible scales. If
more precise definitions of priors is necessary, this needs to be done
directly with brms-functions like <code>set_prior()</code>.
</p>


<h3>Value</h3>

<p>A <code>brmsprior</code>-object.
</p>


<h3>Note</h3>

<p>As <code>auto_prior()</code> also sets priors on the intercept, the model
formula used in <code>brms::brm()</code> must be rewritten to something like
<code>y ~ 0 + intercept ...</code>, see <code><a href="brms.html#topic+set_prior">set_prior</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sjmisc)
data(efc)
efc$c172code &lt;- as.factor(efc$c172code)
efc$c161sex &lt;- to_label(efc$c161sex)

mf &lt;- formula(neg_c_7 ~ c161sex + c160age + c172code)

if (requireNamespace("brms", quietly = TRUE))
  auto_prior(mf, efc, TRUE)

## compare to
# library(rstanarm)
# m &lt;- stan_glm(mf, data = efc, chains = 2, iter = 200)
# ps &lt;- prior_summary(m)
# ps$prior_intercept$adjusted_scale
# ps$prior$adjusted_scale

## usage
# ap &lt;- auto_prior(mf, efc, TRUE)
# brm(mf, data = efc, priors = ap)

# add informative priors
mf &lt;- formula(neg_c_7 ~ c161sex + c172code)

if (requireNamespace("brms", quietly = TRUE)) {
  auto_prior(mf, efc, TRUE) +
    brms::prior(normal(.1554, 40), class = "b", coef = "c160age")
}

# example with binary response
efc$neg_c_7d &lt;- ifelse(efc$neg_c_7 &lt; median(efc$neg_c_7, na.rm = TRUE), 0, 1)
mf &lt;- formula(neg_c_7d ~ c161sex + c160age + c172code + e17age)

if (requireNamespace("brms", quietly = TRUE))
  auto_prior(mf, efc, FALSE)
</code></pre>

<hr>
<h2 id='boot_ci'>Standard error and confidence intervals for bootstrapped estimates</h2><span id='topic+boot_ci'></span><span id='topic+boot_se'></span><span id='topic+boot_p'></span><span id='topic+boot_est'></span>

<h3>Description</h3>

<p>Compute nonparametric bootstrap estimate, standard error,
confidence intervals and p-value for a vector of bootstrap
replicate estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot_ci(data, ..., method = c("dist", "quantile"), ci.lvl = 0.95)

boot_se(data, ...)

boot_p(data, ...)

boot_est(data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boot_ci_+3A_data">data</code></td>
<td>
<p>A data frame that containts the vector with bootstrapped
estimates, or directly the vector (see 'Examples').</p>
</td></tr>
<tr><td><code id="boot_ci_+3A_...">...</code></td>
<td>
<p>Optional, unquoted names of variables with bootstrapped estimates.
Required, if either <code>data</code> is a data frame (and no vector),
and only selected variables from <code>data</code> should be processed.
You may also use functions like <code>:</code> or tidyselect's
<code>select_helpers()</code>.</p>
</td></tr>
<tr><td><code id="boot_ci_+3A_method">method</code></td>
<td>
<p>Character vector, indicating if confidence intervals should be
based on bootstrap standard error, multiplied by the value of the
quantile function of the t-distribution (default), or on sample
quantiles of the bootstrapped values. See 'Details' in <code>boot_ci()</code>.
May be abbreviated.</p>
</td></tr>
<tr><td><code id="boot_ci_+3A_ci.lvl">ci.lvl</code></td>
<td>
<p>Numeric, the level of the confidence intervals.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The methods require one or more vectors of bootstrap replicate estimates
as input.
</p>

<ul>
<li>
<p><code>boot_est()</code> returns the bootstrapped estimate, simply by
computing the mean value of all bootstrap estimates.

</p>
</li>
<li>
<p><code>boot_se()</code> computes the nonparametric bootstrap standard
error by calculating the standard deviation of the input vector.

</p>
</li>
<li>
<p>The mean value of the input vector and its standard error is used
by <code>boot_ci()</code> to calculate the lower and upper confidence
interval, assuming a t-distribution of bootstrap estimate replicates
(for <code>method = "dist"</code>, the default, which is
<code>mean(x) +/- qt(.975, df = length(x) - 1) * sd(x)</code>); for
<code>method = "quantile"</code>, 95% sample quantiles are used to compute
the confidence intervals (<code>quantile(x, probs = c(.025, .975))</code>).
Use <code>ci.lvl</code> to change the level for the confidence interval.

</p>
</li>
<li>
<p>P-values from <code>boot_p()</code> are also based on t-statistics,
assuming normal distribution.

</p>
</li></ul>



<h3>Value</h3>

<p>A data frame with either bootstrap estimate,
standard error, the lower and upper confidence intervals or the
p-value for all bootstrapped estimates.
</p>


<h3>References</h3>

<p>Carpenter J, Bithell J. Bootstrap confdence intervals: when, which, what? A practical guide for medical statisticians. Statist. Med. 2000; 19:1141-1164
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bootstrap">bootstrap</a></code> to generate nonparametric bootstrap samples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(purrr)
data(efc)
bs &lt;- bootstrap(efc, 100)

# now run models for each bootstrapped sample
bs$models &lt;- map(bs$strap, ~lm(neg_c_7 ~ e42dep + c161sex, data = .x))

# extract coefficient "dependency" and "gender" from each model
bs$dependency &lt;- map_dbl(bs$models, ~coef(.x)[2])
bs$gender &lt;- map_dbl(bs$models, ~coef(.x)[3])

# get bootstrapped confidence intervals
boot_ci(bs$dependency)

# compare with model fit
fit &lt;- lm(neg_c_7 ~ e42dep + c161sex, data = efc)
confint(fit)[2, ]

# alternative function calls.
boot_ci(bs$dependency)
boot_ci(bs, dependency)
boot_ci(bs, dependency, gender)
boot_ci(bs, dependency, gender, method = "q")


# compare coefficients
mean(bs$dependency)
boot_est(bs$dependency)
coef(fit)[2]


# bootstrap() and boot_ci() work fine within pipe-chains
efc %&gt;%
  bootstrap(100) %&gt;%
  mutate(
    models = map(strap, ~lm(neg_c_7 ~ e42dep + c161sex, data = .x)),
    dependency = map_dbl(models, ~coef(.x)[2])
  ) %&gt;%
  boot_ci(dependency)

# check p-value
boot_p(bs$gender)
summary(fit)$coefficients[3, ]

## Not run: 
# 'spread_coef()' from the 'sjmisc'-package makes it easy to generate
# bootstrapped statistics like confidence intervals or p-values
library(dplyr)
library(sjmisc)
efc %&gt;%
  # generate bootstrap replicates
  bootstrap(100) %&gt;%
  # apply lm to all bootstrapped data sets
  mutate(
    models = map(strap, ~lm(neg_c_7 ~ e42dep + c161sex + c172code, data = .x))
  ) %&gt;%
  # spread model coefficient for all 100 models
  spread_coef(models) %&gt;%
  # compute the CI for all bootstrapped model coefficients
  boot_ci(e42dep, c161sex, c172code)

# or...
efc %&gt;%
  # generate bootstrap replicates
  bootstrap(100) %&gt;%
  # apply lm to all bootstrapped data sets
  mutate(
    models = map(strap, ~lm(neg_c_7 ~ e42dep + c161sex + c172code, data = .x))
  ) %&gt;%
  # spread model coefficient for all 100 models
  spread_coef(models, append = FALSE) %&gt;%
  # compute the CI for all bootstrapped model coefficients
  boot_ci()
## End(Not run)
</code></pre>

<hr>
<h2 id='bootstrap'>Generate nonparametric bootstrap replications</h2><span id='topic+bootstrap'></span>

<h3>Description</h3>

<p>Generates <code>n</code> bootstrap samples of <code>data</code> and
returns the bootstrapped data frames as list-variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap(data, n, size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrap_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_n">n</code></td>
<td>
<p>Number of bootstraps to be generated.</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_size">size</code></td>
<td>
<p>Optional, size of the bootstrap samples. May either be a number
between 1 and <code>nrow(data)</code> or a value between 0 and 1 to sample
a proportion of observations from <code>data</code> (see 'Examples').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, each bootstrap sample has the same number of observations
as <code>data</code>. To generate bootstrap samples without resampling
same observations (i.e. sampling without replacement), use
<code>size</code> to get bootstrapped data with a specific number
of observations. However, specifying the <code>size</code>-argument is much
less memory-efficient than the bootstrap with replacement. Hence,
it is recommended to ignore the <code>size</code>-argument, if it is
not really needed.
</p>


<h3>Value</h3>

<p>A data frame with one column: a list-variable
<code>strap</code>, which contains resample-objects of class <code>sj_resample</code>.
These resample-objects are lists with three elements:
</p>

<ol>
<li><p> the original data frame, <code>data</code>
</p>
</li>
<li><p> the rownmumbers <code>id</code>, i.e. rownumbers of <code>data</code>, indicating the resampled rows with replacement
</p>
</li>
<li><p> the <code>resample.id</code>, indicating the index of the resample (i.e. the position of the <code>sj_resample</code>-object in the list <code>strap</code>)
</p>
</li></ol>



<h3>Note</h3>

<p>This function applies nonparametric bootstrapping, i.e. the function
draws samples with replacement.
<br /> <br />
There is an <code>as.data.frame</code>- and a <code>print</code>-method to get or
print the resampled data frames. See 'Examples'. The <code>as.data.frame</code>-
method automatically applies whenever coercion is done because a data
frame is required as input. See 'Examples' in <code><a href="#topic+boot_ci">boot_ci</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+boot_ci">boot_ci</a></code> to calculate confidence intervals from
bootstrap samples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(efc)
bs &lt;- bootstrap(efc, 5)

# now run models for each bootstrapped sample
lapply(bs$strap, function(x) lm(neg_c_7 ~ e42dep + c161sex, data = x))

# generate bootstrap samples with 600 observations for each sample
bs &lt;- bootstrap(efc, 5, 600)

# generate bootstrap samples with 70% observations of the original sample size
bs &lt;- bootstrap(efc, 5, .7)

# compute standard error for a simple vector from bootstraps
# use the `as.data.frame()`-method to get the resampled
# data frame
bs &lt;- bootstrap(efc, 100)
bs$c12hour &lt;- unlist(lapply(bs$strap, function(x) {
  mean(as.data.frame(x)$c12hour, na.rm = TRUE)
}))

# or as tidyverse-approach
if (require("dplyr") &amp;&amp; require("purrr")) {
  bs &lt;- efc %&gt;%
    bootstrap(100) %&gt;%
    mutate(
      c12hour = map_dbl(strap, ~mean(as.data.frame(.x)$c12hour, na.rm = TRUE))
    )

  # bootstrapped standard error
  boot_se(bs, c12hour)
}
</code></pre>

<hr>
<h2 id='chisq_gof'>Compute model quality</h2><span id='topic+chisq_gof'></span>

<h3>Description</h3>

<p>For logistic regression models, performs a Chi-squared
goodness-of-fit-test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chisq_gof(x, prob = NULL, weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chisq_gof_+3A_x">x</code></td>
<td>
<p>A numeric vector or a <code>glm</code>-object.</p>
</td></tr>
<tr><td><code id="chisq_gof_+3A_prob">prob</code></td>
<td>
<p>Vector of probabilities (indicating the population probabilities)
of the same length as <code>x</code>'s amount of categories / factor levels.
Use <code>nrow(table(x))</code> to determine the amount of necessary values
for <code>prob</code>. Only used, when <code>x</code> is a vector, and not a
<code>glm</code>-object.</p>
</td></tr>
<tr><td><code id="chisq_gof_+3A_weights">weights</code></td>
<td>
<p>Vector with weights, used to weight <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For vectors, this function is a convenient function for the
<code>chisq.test()</code>, performing goodness-of-fit test. For
<code>glm</code>-objects, this function performs a goodness-of-fit test.
A well-fitting model shows <em>no</em> significant difference between the
model and the observed data, i.e. the reported p-values should be
greater than 0.05.
</p>


<h3>Value</h3>

<p>For vectors, returns the object of the computed <code><a href="stats.html#topic+chisq.test">chisq.test</a></code>.
For <code>glm</code>-objects, an object of class <code>chisq_gof</code> with
following values: <code>p.value</code>, the p-value for the goodness-of-fit test;
<code>z.score</code>, the standardized z-score for the goodness-of-fit test;
<code>rss</code>, the residual sums of squares term and <code>chisq</code>, the pearson
chi-squared statistic.
</p>


<h3>References</h3>

<p>Hosmer, D. W., &amp; Lemeshow, S. (2000). Applied Logistic Regression. Hoboken, NJ, USA: John Wiley &amp; Sons, Inc.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(efc)
efc$neg_c_7d &lt;- ifelse(efc$neg_c_7 &lt; median(efc$neg_c_7, na.rm = TRUE), 0, 1)
m &lt;- glm(
  neg_c_7d ~ c161sex + barthtot + c172code,
  data = efc,
  family = binomial(link = "logit")
)

# goodness-of-fit test for logistic regression
chisq_gof(m)

# goodness-of-fit test for vectors against probabilities
# differing from population
chisq_gof(efc$e42dep, c(0.3,0.2,0.22,0.28))

# equal to population
chisq_gof(efc$e42dep, prop.table(table(efc$e42dep)))

</code></pre>

<hr>
<h2 id='cramer'>Measures of association for contingency tables</h2><span id='topic+cramer'></span><span id='topic+cramer.formula'></span><span id='topic+phi'></span><span id='topic+crosstable_statistics'></span><span id='topic+xtab_statistics'></span>

<h3>Description</h3>

<p>This function calculates various measure of association for
contingency tables and returns the statistic and p-value.
Supported measures are Cramer's V, Phi, Spearman's rho,
Kendall's tau and Pearson's r.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cramer(tab, ...)

## S3 method for class 'formula'
cramer(
  formula,
  data,
  ci.lvl = NULL,
  n = 1000,
  method = c("dist", "quantile"),
  ...
)

phi(tab, ...)

crosstable_statistics(
  data,
  x1 = NULL,
  x2 = NULL,
  statistics = c("auto", "cramer", "phi", "spearman", "kendall", "pearson", "fisher"),
  weights = NULL,
  ...
)

xtab_statistics(
  data,
  x1 = NULL,
  x2 = NULL,
  statistics = c("auto", "cramer", "phi", "spearman", "kendall", "pearson", "fisher"),
  weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cramer_+3A_tab">tab</code></td>
<td>
<p>A <code><a href="base.html#topic+table">table</a></code> or <code><a href="stats.html#topic+ftable">ftable</a></code>. Tables of class
<code><a href="stats.html#topic+xtabs">xtabs</a></code> and other will be coerced to <code>ftable</code>
objects.</p>
</td></tr>
<tr><td><code id="cramer_+3A_...">...</code></td>
<td>
<p>Other arguments, passed down to the statistic functions
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code>, <code><a href="stats.html#topic+fisher.test">fisher.test</a></code> or
<code><a href="stats.html#topic+cor.test">cor.test</a></code>.</p>
</td></tr>
<tr><td><code id="cramer_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>lhs ~ rhs</code> where <code>lhs</code> is a
numeric variable giving the data values and <code>rhs</code> a factor giving the
corresponding groups.</p>
</td></tr>
<tr><td><code id="cramer_+3A_data">data</code></td>
<td>
<p>A data frame or a table object. If a table object, <code>x1</code> and
<code>x2</code> will be ignored. For Kendall's <em>tau</em>, Spearman's <em>rho</em>
or Pearson's product moment correlation coefficient, <code>data</code> needs
to be a data frame. If <code>x1</code> and <code>x2</code> are not specified,
the first two columns of the data frames are used as variables
to compute the crosstab.</p>
</td></tr>
<tr><td><code id="cramer_+3A_ci.lvl">ci.lvl</code></td>
<td>
<p>Scalar between 0 and 1. If not <code>NULL</code>, returns a data
frame including lower and upper confidence intervals.</p>
</td></tr>
<tr><td><code id="cramer_+3A_n">n</code></td>
<td>
<p>Number of bootstraps to be generated.</p>
</td></tr>
<tr><td><code id="cramer_+3A_method">method</code></td>
<td>
<p>Character vector, indicating if confidence intervals should be
based on bootstrap standard error, multiplied by the value of the
quantile function of the t-distribution (default), or on sample
quantiles of the bootstrapped values. See 'Details' in <code>boot_ci()</code>.
May be abbreviated.</p>
</td></tr>
<tr><td><code id="cramer_+3A_x1">x1</code></td>
<td>
<p>Name of first variable that should be used to compute the
contingency table. If <code>data</code> is a table object, this argument
will be irgnored.</p>
</td></tr>
<tr><td><code id="cramer_+3A_x2">x2</code></td>
<td>
<p>Name of second variable that should be used to compute the
contingency table. If <code>data</code> is a table object, this argument
will be irgnored.</p>
</td></tr>
<tr><td><code id="cramer_+3A_statistics">statistics</code></td>
<td>
<p>Name of measure of association that should be computed. May
be one of <code>"auto"</code>, <code>"cramer"</code>, <code>"phi"</code>, <code>"spearman"</code>,
<code>"kendall"</code>, <code>"pearson"</code> or <code>"fisher"</code>. See 'Details'.</p>
</td></tr>
<tr><td><code id="cramer_+3A_weights">weights</code></td>
<td>
<p>Name of variable in <code>x</code> that indicated the vector of
weights that will be applied to weight all observations. Default is
<code>NULL</code>, so no weights are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The p-value for Cramer's V and the Phi coefficient are based
on <code>chisq.test()</code>. If any expected value of a table cell is
smaller than 5, or smaller than 10 and the df is 1, then <code>fisher.test()</code>
is used to compute the p-value, unless <code>statistics = "fisher"</code>; in
this case, the use of <code>fisher.test()</code> is forced to compute the
p-value. The test statistic is calculated with <code>cramer()</code> resp.
<code>phi()</code>.
<br /> <br />
Both test statistic and p-value for Spearman's rho, Kendall's tau
and Pearson's r are calculated with <code>cor.test()</code>.
<br /> <br />
When <code>statistics = "auto"</code>, only Cramer's V or Phi are calculated,
based on the dimension of the table (i.e. if the table has more than
two rows or columns, Cramer's V is calculated, else Phi).
</p>


<h3>Value</h3>

<p>For <code>phi()</code>, the table's Phi value. For <code>cramer()</code>, the
table's Cramer's V.
<br /> <br />
For <code>crosstable_statistics()</code>, a list with following components:
</p>

<dl>
<dt><code>estimate</code></dt><dd><p>the value of the estimated measure of association.</p>
</dd>
<dt><code>p.value</code></dt><dd><p>the p-value for the test.</p>
</dd>
<dt><code>statistic</code></dt><dd><p>the value of the test statistic.</p>
</dd>
<dt><code>stat.name</code></dt><dd><p>the name of the test statistic.</p>
</dd>
<dt><code>stat.html</code></dt><dd><p>if applicable, the name of the test statistic, in HTML-format.</p>
</dd>
<dt><code>df</code></dt><dd><p>the degrees of freedom for the contingency table.</p>
</dd>
<dt><code>method</code></dt><dd><p>character string indicating the name of the measure of association.</p>
</dd>
<dt><code>method.html</code></dt><dd><p>if applicable, the name of the measure of association, in HTML-format.</p>
</dd>
<dt><code>method.short</code></dt><dd><p>the short form of association measure, equals the <code>statistics</code>-argument.</p>
</dd>
<dt><code>fisher</code></dt><dd><p>logical, if Fisher's exact test was used to calculate the p-value.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'># Phi coefficient for 2x2 tables
tab &lt;- table(sample(1:2, 30, TRUE), sample(1:2, 30, TRUE))
phi(tab)

# Cramer's V for nominal variables with more than 2 categories
tab &lt;- table(sample(1:2, 30, TRUE), sample(1:3, 30, TRUE))
cramer(tab)

# formula notation
data(efc)
cramer(e16sex ~ c161sex, data = efc)

# bootstrapped confidence intervals
cramer(e16sex ~ c161sex, data = efc, ci.lvl = .95, n = 100)

# 2x2 table, compute Phi automatically
crosstable_statistics(efc, e16sex, c161sex)

# more dimensions than 2x2, compute Cramer's V automatically
crosstable_statistics(efc, c172code, c161sex)

# ordinal data, use Kendall's tau
crosstable_statistics(efc, e42dep, quol_5, statistics = "kendall")

# calcilate Spearman's rho, with continuity correction
crosstable_statistics(efc,
  e42dep,
  quol_5,
  statistics = "spearman",
  exact = FALSE,
  continuity = TRUE
)
</code></pre>

<hr>
<h2 id='cv'>Compute model quality</h2><span id='topic+cv'></span>

<h3>Description</h3>

<p>Compute the coefficient of variation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_+3A_x">x</code></td>
<td>
<p>Fitted linear model of class <code>lm</code>, <code>merMod</code> (<span class="pkg">lme4</span>)
or <code>lme</code> (<span class="pkg">nlme</span>).</p>
</td></tr>
<tr><td><code id="cv_+3A_...">...</code></td>
<td>
<p>More fitted model objects, to compute multiple coefficients of
variation at once.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The advantage of the cv is that it is unitless. This allows
coefficient of variation to be compared to each other in ways
that other measures, like standard deviations or root mean
squared residuals, cannot be.
</p>


<h3>Value</h3>

<p>Numeric, the coefficient of variation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(efc)
fit &lt;- lm(barthtot ~ c160age + c12hour, data = efc)
cv(fit)

</code></pre>

<hr>
<h2 id='cv_error'>Test and training error from model cross-validation</h2><span id='topic+cv_error'></span><span id='topic+cv_compare'></span>

<h3>Description</h3>

<p><code>cv_error()</code> computes the root mean squared error from a model fitted
to kfold cross-validated test-training-data. <code>cv_compare()</code>
does the same, for multiple formulas at once (by calling <code>cv_error()</code>
for each formula).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_error(data, formula, k = 5)

cv_compare(data, formulas, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_error_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="cv_error_+3A_formula">formula</code></td>
<td>
<p>The formula to fit the linear model for the test and training data.</p>
</td></tr>
<tr><td><code id="cv_error_+3A_k">k</code></td>
<td>
<p>The number of folds for the kfold-crossvalidation.</p>
</td></tr>
<tr><td><code id="cv_error_+3A_formulas">formulas</code></td>
<td>
<p>A list of formulas, to fit linear models for the test and training data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cv_error()</code> first generates cross-validated test-training pairs, using
<code><a href="modelr.html#topic+crossv_kfold">crossv_kfold</a></code> and then fits a linear model, which
is described in <code>formula</code>, to the training data. Then, predictions
for the test data are computed, based on the trained models.
The <em>training error</em> is the mean value of the <code><a href="#topic+rmse">rmse</a></code> for
all <em>trained</em> models; the <em>test error</em> is the rmse based on all
residuals from the test data.
</p>


<h3>Value</h3>

<p>A data frame with the root mean squared errors for the training and test data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(efc)
cv_error(efc, neg_c_7 ~ barthtot + c161sex)

cv_compare(efc, formulas = list(
  neg_c_7 ~ barthtot + c161sex,
  neg_c_7 ~ barthtot + c161sex + e42dep,
  neg_c_7 ~ barthtot + c12hour
))

</code></pre>

<hr>
<h2 id='design_effect'>Design effects for two-level mixed models</h2><span id='topic+design_effect'></span>

<h3>Description</h3>

<p>Compute the design effect (also called <em>Variance Inflation Factor</em>)
for mixed models with two-level design.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>design_effect(n, icc = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="design_effect_+3A_n">n</code></td>
<td>
<p>Average number of observations per grouping cluster (i.e. level-2 unit).</p>
</td></tr>
<tr><td><code id="design_effect_+3A_icc">icc</code></td>
<td>
<p>Assumed intraclass correlation coefficient for multilevel-model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The formula for the design effect is simply <code>(1 + (n - 1) * icc)</code>.
</p>


<h3>Value</h3>

<p>The design effect (Variance Inflation Factor) for the two-level model.
</p>


<h3>References</h3>

<p>Bland JM. 2000. Sample size in guidelines trials. Fam Pract. (17), 17-20.
<br /> <br />
Hsieh FY, Lavori PW, Cohen HJ, Feussner JR. 2003. An Overview of Variance Inflation Factors for Sample-Size Calculation. Evaluation and the Health Professions 26: 239-257. <a href="https://doi.org/10.1177/0163278703255230">doi:10.1177/0163278703255230</a>
<br /> <br />
Snijders TAB. 2005. Power and Sample Size in Multilevel Linear Models. In: Everitt BS, Howell DC (Hrsg.). Encyclopedia of Statistics in Behavioral Science. Chichester, UK: John Wiley and Sons, Ltd. <a href="https://doi.org/10.1002/0470013192.bsa492">doi:10.1002/0470013192.bsa492</a>
<br /> <br />
Thompson DM, Fernald DH, Mold JW. 2012. Intraclass Correlation Coefficients Typical of Cluster-Randomized Studies: Estimates From the Robert Wood Johnson Prescription for Health Projects. The Annals of Family Medicine;10(3):235-40. <a href="https://doi.org/10.1370/afm.1347">doi:10.1370/afm.1347</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Design effect for two-level model with 30 observations per
# cluster group (level-2 unit) and an assumed intraclass
# correlation coefficient of 0.05.
design_effect(n = 30)

# Design effect for two-level model with 24 observation per cluster
# group and an assumed intraclass correlation coefficient of 0.2.
design_effect(n = 24, icc = 0.2)

</code></pre>

<hr>
<h2 id='efc'>Sample dataset from the EUROFAMCARE project</h2><span id='topic+efc'></span>

<h3>Description</h3>

<p>German data set from the European study on family care of older people.
</p>


<h3>References</h3>

<p>Lamura G, Döhner H, Kofahl C, editors. Family carers of older people in Europe: a six-country comparative study. Münster: LIT, 2008.
</p>

<hr>
<h2 id='find_beta'>Determining distribution parameters</h2><span id='topic+find_beta'></span><span id='topic+find_beta2'></span><span id='topic+find_cauchy'></span><span id='topic+find_normal'></span>

<h3>Description</h3>

<p><code>find_beta()</code>, <code>find_normal()</code> and <code>find_cauchy()</code> find the
shape, mean and standard deviation resp. the location and scale parameters
to describe the beta, normal or cauchy distribution, based on two
percentiles. <code>find_beta2()</code> finds the shape parameters for a Beta
distribution, based on a probability value and its standard error
or confidence intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_beta(x1, p1, x2, p2)

find_beta2(x, se, ci, n)

find_cauchy(x1, p1, x2, p2)

find_normal(x1, p1, x2, p2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_beta_+3A_x1">x1</code></td>
<td>
<p>Value for the first percentile.</p>
</td></tr>
<tr><td><code id="find_beta_+3A_p1">p1</code></td>
<td>
<p>Probability of the first percentile.</p>
</td></tr>
<tr><td><code id="find_beta_+3A_x2">x2</code></td>
<td>
<p>Value for the second percentile.</p>
</td></tr>
<tr><td><code id="find_beta_+3A_p2">p2</code></td>
<td>
<p>Probability of the second percentile.</p>
</td></tr>
<tr><td><code id="find_beta_+3A_x">x</code></td>
<td>
<p>Numeric, a probability value between 0 and 1. Typically indicates
a prevalence rate of an outcome of interest; Or an integer value
with the number of observed events. In this case, specify <code>n</code>
to indicate the toral number of observations.</p>
</td></tr>
<tr><td><code id="find_beta_+3A_se">se</code></td>
<td>
<p>The standard error of <code>x</code>. Either <code>se</code> or <code>ci</code> must
be specified.</p>
</td></tr>
<tr><td><code id="find_beta_+3A_ci">ci</code></td>
<td>
<p>The upper limit of the confidence interval of <code>x</code>. Either
<code>se</code> or <code>ci</code> must be specified.</p>
</td></tr>
<tr><td><code id="find_beta_+3A_n">n</code></td>
<td>
<p>Numeric, number of total observations. Needs to be specified, if
<code>x</code> is an integer (number of observed events), and no
probability. See 'Examples'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions can be used to find parameter for various distributions,
to define prior probabilities for Bayesian analyses. <code>x1</code>,
<code>p1</code>, <code>x2</code> and <code>p2</code> are parameters that describe two
quantiles. Given this knowledge, the distribution parameters are
returned. <br /> <br />
Use <code>find_beta2()</code>, if the known parameters are, e.g. a prevalence
rate or similar probability, and its standard deviation or confidence
interval. In this case. <code>x</code> should be a probability,
for example a prevalence rate of a certain event. <code>se</code> then
needs to be the standard error for this probability. Alternatively,
<code>ci</code> can be specified, which should indicate the upper limit
of the confidence interval od the probability (prevalence rate) <code>x</code>.
If the number of events out of a total number of trials is known
(e.g. 12 heads out of 30 coin tosses), <code>x</code> can also be the number
of observed events, while <code>n</code> indicates the total amount of trials
(in the above example, the function call would be: <code>find_beta2(x = 12, n = 30)</code>).
</p>


<h3>Value</h3>

<p>A list of length two, with the two distribution parameters than can
be used to define the distribution, which (best) describes
the shape for the given input parameters.
</p>


<h3>References</h3>

<p>Cook JD. Determining distribution parameters from quantiles. 2010: Department of Biostatistics, Texas (<a href="https://www.johndcook.com/quantiles_parameters.pdf">PDF</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example from blogpost:
# https://www.johndcook.com/blog/2010/01/31/parameters-from-percentiles/
# 10% of patients respond within 30 days of treatment
# and 80% respond within 90 days of treatment
find_normal(x1 = 30, p1 = .1, x2 = 90, p2 = .8)
find_cauchy(x1 = 30, p1 = .1, x2 = 90, p2 = .8)

parms &lt;- find_normal(x1 = 30, p1 = .1, x2 = 90, p2 = .8)
curve(
  dnorm(x, mean = parms$mean, sd = parms$sd),
  from = 0, to = 200
)

parms &lt;- find_cauchy(x1 = 30, p1 = .1, x2 = 90, p2 = .8)
curve(
  dcauchy(x, location = parms$location, scale = parms$scale),
  from = 0, to = 200
)


find_beta2(x = .25, ci = .5)

shapes &lt;- find_beta2(x = .25, ci = .5)
curve(dbeta(x, shapes[[1]], shapes[[2]]))

# find Beta distribution for 3 events out of 20 observations
find_beta2(x = 3, n = 20)

shapes &lt;- find_beta2(x = 3, n = 20)
curve(dbeta(x, shapes[[1]], shapes[[2]]))

</code></pre>

<hr>
<h2 id='gmd'>Gini's Mean Difference</h2><span id='topic+gmd'></span>

<h3>Description</h3>

<p><code>gmd()</code> computes Gini's mean difference for a numeric vector
or for all numeric vectors in a data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmd(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gmd_+3A_x">x</code></td>
<td>
<p>A vector or data frame.</p>
</td></tr>
<tr><td><code id="gmd_+3A_...">...</code></td>
<td>
<p>Optional, unquoted names of variables that should be selected for
further processing. Required, if <code>x</code> is a data frame (and no vector)
and only selected variables from <code>x</code> should be processed. You may also
use functions like <code>:</code> or tidyselect's <code>select_helpers()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For numeric vectors, Gini's mean difference. For non-numeric vectors
or vectors of length &lt; 2, returns <code>NA</code>.
</p>


<h3>Note</h3>

<p>Gini's mean difference is defined as the mean absolute difference between
any two distinct elements of a vector. Missing values from <code>x</code> are
silently removed.
</p>


<h3>References</h3>

<p>David HA. Gini's mean difference rediscovered. Biometrika 1968(55): 573-575
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(efc)
gmd(efc$e17age)
gmd(efc, e17age, c160age, c12hour)

</code></pre>

<hr>
<h2 id='inequ_trend'>Compute trends in status inequalities</h2><span id='topic+inequ_trend'></span>

<h3>Description</h3>

<p>This method computes the proportional change of absolute
(rate differences) and relative (rate ratios) inequalities
of prevalence rates for two different status groups, as proposed
by Mackenbach et al. (2015).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inequ_trend(data, prev.low, prev.hi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inequ_trend_+3A_data">data</code></td>
<td>
<p>A data frame that contains the variables with prevalence rates for both low
and high status groups (see 'Examples').</p>
</td></tr>
<tr><td><code id="inequ_trend_+3A_prev.low">prev.low</code></td>
<td>
<p>The name of the variable with the prevalence rates for
the low status groups.</p>
</td></tr>
<tr><td><code id="inequ_trend_+3A_prev.hi">prev.hi</code></td>
<td>
<p>The name of the variable with the prevalence rates for
the hi status groups.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given the time trend of prevalence rates of an outcome for two status
groups (e.g. the mortality rates for people with lower and higher
socioeconomic status over 40 years), this function computes the
proportional change of absolute and relative inequalities, expressed
in changes in rate differences and rate ratios. The function implements
the algorithm proposed by <em>Mackenbach et al. 2015</em>.
</p>


<h3>Value</h3>

<p>A data frame with the prevalence rates as well as the values for the
proportional change in absolute (<code>rd</code>) and relative (<code>rr</code>)
ineqqualities.
</p>


<h3>References</h3>

<p>Mackenbach JP, Martikainen P, Menvielle G, de Gelder R. 2015. The Arithmetic of Reducing Relative and Absolute Inequalities in Health: A Theoretical Analysis Illustrated with European Mortality Data. Journal of Epidemiology and Community Health 70(7): 730-36. <a href="https://doi.org/10.1136/jech-2015-207018">doi:10.1136/jech-2015-207018</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example reproduces Fig. 1 of Mackenbach et al. 2015, p.5

# 40 simulated time points, with an initial rate ratio of 2 and
# a rate difference of 100 (i.e. low status group starts with a
# prevalence rate of 200, the high status group with 100)

# annual decline of prevalence is 1% for the low, and 3% for the
# high status group

n &lt;- 40
time &lt;- seq(1, n, by = 1)
lo &lt;- rep(200, times = n)
for (i in 2:n) lo[i] &lt;- lo[i - 1] * .99

hi &lt;- rep(100, times = n)
for (i in 2:n) hi[i] &lt;- hi[i - 1] * .97

prev.data &lt;- data.frame(lo, hi)

# print values
inequ_trend(prev.data, lo, hi)

# plot trends - here we see that the relative inequalities
# are increasing over time, while the absolute inequalities
# are first increasing as well, but later are decreasing
# (while rel. inequ. are still increasing)
plot(inequ_trend(prev.data, lo, hi))

</code></pre>

<hr>
<h2 id='is_prime'>Find prime numbers</h2><span id='topic+is_prime'></span>

<h3>Description</h3>

<p>This functions checks whether a number is, or numbers in a
vector are prime numbers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_prime(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_prime_+3A_x">x</code></td>
<td>
<p>An integer, or a vector of integers.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> for each prime number in <code>x</code>, <code>FALSE</code> otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>is_prime(89)
is_prime(15)
is_prime(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))

</code></pre>

<hr>
<h2 id='mean_n'>Row means with min amount of valid values</h2><span id='topic+mean_n'></span>

<h3>Description</h3>

<p>This function is similar to the SPSS <code>MEAN.n</code> function and computes
row means from a <code>data.frame</code> or <code>matrix</code> if at least <code>n</code>
values of a row are valid (and not <code>NA</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_n(dat, n, digits = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean_n_+3A_dat">dat</code></td>
<td>
<p>A data frame with at least two columns, where row means are applied.</p>
</td></tr>
<tr><td><code id="mean_n_+3A_n">n</code></td>
<td>
<p>May either be
</p>

<ul>
<li><p> a numeric value that indicates the amount of valid values per row to calculate the row mean;
</p>
</li>
<li><p> or a value between 0 and 1, indicating a proportion of valid values per row to calculate the row mean (see 'Details').
</p>
</li></ul>

<p>If a row's sum of valid values is less than <code>n</code>, <code>NA</code> will be returned as row mean value.</p>
</td></tr>
<tr><td><code id="mean_n_+3A_digits">digits</code></td>
<td>
<p>Numeric value indicating the number of decimal places to be used for rounding mean
value. Negative values are allowed (see 'Details').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Rounding to a negative number of <code>digits</code> means rounding to a power of
ten, so for example mean_n(df, 3, digits = -2) rounds to the
nearest hundred. <br /> <br />
For <code>n</code>, must be a numeric value from <code>0</code> to <code>ncol(dat)</code>. If
a <em>row</em> in <code>dat</code> has at least <code>n</code> non-missing values, the
row mean is returned. If <code>n</code> is a non-integer value from 0 to 1,
<code>n</code> is considered to indicate the proportion of necessary non-missing
values per row. E.g., if <code>n = .75</code>, a row must have at least <code>ncol(dat) * n</code>
non-missing values for the row mean to be calculated. See 'Examples'.
</p>


<h3>Value</h3>

<p>A vector with row mean values of <code>df</code> for those rows with at least <code>n</code>
valid values. Else, <code>NA</code> is returned.
</p>


<h3>References</h3>

<p><a href="https://r4stats.com/2014/09/03/adding-the-spss-mean-n-function-to-r/">r4stats.com</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- data.frame(c1 = c(1,2,NA,4),
                  c2 = c(NA,2,NA,5),
                  c3 = c(NA,4,NA,NA),
                  c4 = c(2,3,7,8))

# needs at least 4 non-missing values per row
mean_n(dat, 4) # 1 valid return value

# needs at least 3 non-missing values per row
mean_n(dat, 3) # 2 valid return values

# needs at least 2 non-missing values per row
mean_n(dat, 2)

# needs at least 1 non-missing value per row
mean_n(dat, 1) # all means are shown

# needs at least 50% of non-missing values per row
mean_n(dat, .5) # 3 valid return values

# needs at least 75% of non-missing values per row
mean_n(dat, .75) # 2 valid return values

</code></pre>

<hr>
<h2 id='means_by_group'>Summary of mean values by group</h2><span id='topic+means_by_group'></span><span id='topic+grpmean'></span>

<h3>Description</h3>

<p>Computes mean, sd and se for each sub-group (indicated by <code>grp</code>)
of <code>dv</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>means_by_group(
  x,
  dv,
  grp,
  weights = NULL,
  digits = 2,
  out = c("txt", "viewer", "browser"),
  encoding = "UTF-8",
  file = NULL
)

grpmean(
  x,
  dv,
  grp,
  weights = NULL,
  digits = 2,
  out = c("txt", "viewer", "browser"),
  encoding = "UTF-8",
  file = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="means_by_group_+3A_x">x</code></td>
<td>
<p>A (grouped) data frame.</p>
</td></tr>
<tr><td><code id="means_by_group_+3A_dv">dv</code></td>
<td>
<p>Name of the dependent variable, for which the mean value, grouped
by <code>grp</code>, is computed.</p>
</td></tr>
<tr><td><code id="means_by_group_+3A_grp">grp</code></td>
<td>
<p>Factor with the cross-classifying variable, where <code>dv</code> is
grouped into the categories represented by <code>grp</code>. Numeric vectors
are coerced to factors.</p>
</td></tr>
<tr><td><code id="means_by_group_+3A_weights">weights</code></td>
<td>
<p>Name of variable in <code>x</code> that indicated the vector of
weights that will be applied to weight all observations. Default is
<code>NULL</code>, so no weights are used.</p>
</td></tr>
<tr><td><code id="means_by_group_+3A_digits">digits</code></td>
<td>
<p>Numeric, amount of digits after decimal point when rounding
estimates and values.</p>
</td></tr>
<tr><td><code id="means_by_group_+3A_out">out</code></td>
<td>
<p>Character vector, indicating whether the results should be printed
to console (<code>out = "txt"</code>) or as HTML-table in the viewer-pane
(<code>out = "viewer"</code>) or browser (<code>out = "browser"</code>), of if the
results should be plotted (<code>out = "plot"</code>, only applies to certain
functions). May be abbreviated.</p>
</td></tr>
<tr><td><code id="means_by_group_+3A_encoding">encoding</code></td>
<td>
<p>Character vector, indicating the charset encoding used
for variable and value labels. Default is <code>"UTF-8"</code>. Only used
when <code>out</code> is not <code>"txt"</code>.</p>
</td></tr>
<tr><td><code id="means_by_group_+3A_file">file</code></td>
<td>
<p>Destination file, if the output should be saved as file.
Only used when <code>out</code> is not <code>"txt"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a One-Way-Anova with <code>dv</code> as dependent
and <code>grp</code> as independent variable, by calling
<code>lm(count ~ as.factor(grp))</code>. Then <code><a href="emmeans.html#topic+contrast">contrast</a></code>
is called to get p-values for each sub-group. P-values indicate whether
each group-mean is significantly different from the total mean.
</p>


<h3>Value</h3>

<p>For non-grouped data frames, <code>means_by_group()</code> returns a data frame with
following columns: <code>term</code>, <code>mean</code>, <code>N</code>, <code>std.dev</code>,
<code>std.error</code> and <code>p.value</code>. For grouped data frames, returns
a list of such data frames.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(efc)
means_by_group(efc, c12hour, e42dep)

data(iris)
means_by_group(iris, Sepal.Width, Species)

# also works for grouped data frames
if (require("dplyr")) {
  efc %&gt;%
    group_by(c172code) %&gt;%
    means_by_group(c12hour, e42dep)
}

# weighting
efc$weight &lt;- abs(rnorm(n = nrow(efc), mean = 1, sd = .5))
means_by_group(efc, c12hour, e42dep, weights = weight)
</code></pre>

<hr>
<h2 id='mwu'>Mann-Whitney-U-Test</h2><span id='topic+mwu'></span><span id='topic+mannwhitney'></span>

<h3>Description</h3>

<p>This function performs a Mann-Whitney-U-Test (or Wilcoxon rank sum test,
see <code><a href="stats.html#topic+wilcox.test">wilcox.test</a></code> and <code><a href="coin.html#topic+wilcox_test">wilcox_test</a></code>)
for <code>x</code>, for each group indicated by <code>grp</code>. If <code>grp</code>
has more than two categories, a comparison between each combination of
two groups is performed. <br /> <br />
The function reports U, p and Z-values as well as effect size r
and group-rank-means.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mwu(
  data,
  x,
  grp,
  distribution = "asymptotic",
  out = c("txt", "viewer", "browser"),
  encoding = "UTF-8",
  file = NULL
)

mannwhitney(
  data,
  x,
  grp,
  distribution = "asymptotic",
  out = c("txt", "viewer", "browser"),
  encoding = "UTF-8",
  file = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mwu_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="mwu_+3A_x">x</code></td>
<td>
<p>Bare (unquoted) variable name, or a character vector with the variable name.</p>
</td></tr>
<tr><td><code id="mwu_+3A_grp">grp</code></td>
<td>
<p>Bare (unquoted) name of the cross-classifying variable, where
<code>x</code> is grouped into the categories represented by <code>grp</code>,
or a character vector with the variable name.</p>
</td></tr>
<tr><td><code id="mwu_+3A_distribution">distribution</code></td>
<td>
<p>Indicates how the null distribution of the test statistic should be computed.
May be one of <code>"exact"</code>, <code>"approximate"</code> or <code>"asymptotic"</code>
(default). See <code><a href="coin.html#topic+wilcox_test">wilcox_test</a></code> for details.</p>
</td></tr>
<tr><td><code id="mwu_+3A_out">out</code></td>
<td>
<p>Character vector, indicating whether the results should be printed
to console (<code>out = "txt"</code>) or as HTML-table in the viewer-pane
(<code>out = "viewer"</code>) or browser (<code>out = "browser"</code>), of if the
results should be plotted (<code>out = "plot"</code>, only applies to certain
functions). May be abbreviated.</p>
</td></tr>
<tr><td><code id="mwu_+3A_encoding">encoding</code></td>
<td>
<p>Character vector, indicating the charset encoding used
for variable and value labels. Default is <code>"UTF-8"</code>. Only used
when <code>out</code> is not <code>"txt"</code>.</p>
</td></tr>
<tr><td><code id="mwu_+3A_file">file</code></td>
<td>
<p>Destination file, if the output should be saved as file.
Only used when <code>out</code> is not <code>"txt"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(Invisibly) returns a data frame with U, p and Z-values for each group-comparison
as well as effect-size r; additionally, group-labels and groups' n's are
also included.
</p>


<h3>Note</h3>

<p>This function calls the <code><a href="coin.html#topic+wilcox_test">wilcox_test</a></code> with formula. If <code>grp</code>
has more than two groups, additionally a Kruskal-Wallis-Test (see <code><a href="stats.html#topic+kruskal.test">kruskal.test</a></code>)
is performed. <br /> <br />
Interpretation of effect sizes, as a rule-of-thumb:
</p>

<ul>
<li><p> small effect &gt;= 0.1
</p>
</li>
<li><p> medium effect &gt;= 0.3
</p>
</li>
<li><p> large effect &gt;= 0.5
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(efc)
# Mann-Whitney-U-Tests for elder's age by elder's dependency.
mwu(efc, e17age, e42dep)

</code></pre>

<hr>
<h2 id='nhanes_sample'>Sample dataset from the National Health and Nutrition Examination Survey</h2><span id='topic+nhanes_sample'></span>

<h3>Description</h3>

<p>Selected variables from the National Health and Nutrition Examination
Survey that are used in the example from Lumley (2010), Appendix E.
See <code><a href="#topic+svyglm.nb">svyglm.nb</a></code> for examples.
</p>


<h3>References</h3>

<p>Lumley T (2010). Complex Surveys: a guide to analysis using R. Wiley
</p>

<hr>
<h2 id='prop'>Proportions of values in a vector</h2><span id='topic+prop'></span><span id='topic+props'></span>

<h3>Description</h3>

<p><code>prop()</code> calculates the proportion of a value or category
in a variable. <code>props()</code> does the same, but allows for
multiple logical conditions in one statement. It is similar
to <code>mean()</code> with logical predicates, however, both
<code>prop()</code> and <code>props()</code> work with grouped data frames.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prop(data, ..., weights = NULL, na.rm = TRUE, digits = 4)

props(data, ..., na.rm = TRUE, digits = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop_+3A_data">data</code></td>
<td>
<p>A data frame. May also be a grouped data frame (see 'Examples').</p>
</td></tr>
<tr><td><code id="prop_+3A_...">...</code></td>
<td>
<p>One or more value pairs of comparisons (logical predicates). Put
variable names the left-hand-side and values to match on the
right hand side. Expressions may be quoted or unquoted. See
'Examples'.</p>
</td></tr>
<tr><td><code id="prop_+3A_weights">weights</code></td>
<td>
<p>Vector of weights that will be applied to weight all observations.
Must be a vector of same length as the input vector. Default is
<code>NULL</code>, so no weights are used.</p>
</td></tr>
<tr><td><code id="prop_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical, whether to remove NA values from the vector when the
proportion is calculated. <code>na.rm = FALSE</code> gives you the raw
percentage of a value in a vector, <code>na.rm = TRUE</code> the valid
percentage.</p>
</td></tr>
<tr><td><code id="prop_+3A_digits">digits</code></td>
<td>
<p>Amount of digits for returned values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>prop()</code> only allows one logical statement per comparison,
while <code>props()</code> allows multiple logical statements per comparison.
However, <code>prop()</code> supports weighting of variables before calculating
proportions, and comparisons may also be quoted. Hence, <code>prop()</code>
also processes comparisons, which are passed as character vector
(see 'Examples').
</p>


<h3>Value</h3>

<p>For one condition, a numeric value with the proportion of the values
inside a vector. For more than one condition, a data frame with one column
of conditions and one column with proportions. For grouped data frames,
returns a data frame with one column per group with grouping categories,
followed by one column with proportions per condition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(efc)

# proportion of value 1 in e42dep
prop(efc, e42dep == 1)

# expression may also be completely quoted
prop(efc, "e42dep == 1")

# use "props()" for multiple logical statements
props(efc, e17age &gt; 70 &amp; e17age &lt; 80)

# proportion of value 1 in e42dep, and all values greater
# than 2 in e42dep, including missing values. will return a data frame
prop(efc, e42dep == 1, e42dep &gt; 2, na.rm = FALSE)

# for factors or character vectors, use quoted or unquoted values
library(sjmisc)
# convert numeric to factor, using labels as factor levels
efc$e16sex &lt;- to_label(efc$e16sex)
efc$n4pstu &lt;- to_label(efc$n4pstu)

# get proportion of female older persons
prop(efc, e16sex == female)

# get proportion of male older persons
prop(efc, e16sex == "male")

# "props()" needs quotes around non-numeric factor levels
props(efc,
  e17age &gt; 70 &amp; e17age &lt; 80,
  n4pstu == 'Care Level 1' | n4pstu == 'Care Level 3'
)

# also works with pipe-chains
library(dplyr)
efc %&gt;% prop(e17age &gt; 70)
efc %&gt;% prop(e17age &gt; 70, e16sex == 1)

# and with group_by
efc %&gt;%
  group_by(e16sex) %&gt;%
  prop(e42dep &gt; 2)

efc %&gt;%
  select(e42dep, c161sex, c172code, e16sex) %&gt;%
  group_by(c161sex, c172code) %&gt;%
  prop(e42dep &gt; 2, e16sex == 1)

# same for "props()"
efc %&gt;%
  select(e42dep, c161sex, c172code, c12hour, n4pstu) %&gt;%
  group_by(c161sex, c172code) %&gt;%
  props(
    e42dep &gt; 2,
    c12hour &gt; 20 &amp; c12hour &lt; 40,
    n4pstu == 'Care Level 1' | n4pstu == 'Care Level 3'
  )
</code></pre>

<hr>
<h2 id='r2'>Deprecated functions</h2><span id='topic+r2'></span><span id='topic+cohens_f'></span><span id='topic+eta_sq'></span><span id='topic+epsilon_sq'></span><span id='topic+omega_sq'></span><span id='topic+scale_weights'></span><span id='topic+robust'></span><span id='topic+icc'></span><span id='topic+p_value'></span><span id='topic+se'></span>

<h3>Description</h3>

<p>A list of deprecated functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2(x)

cohens_f(x, ...)

eta_sq(x, ...)

epsilon_sq(x, ...)

omega_sq(x, ...)

scale_weights(x, ...)

robust(x, ...)

icc(x)

p_value(x, ...)

se(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_+3A_x">x</code></td>
<td>
<p>An object.</p>
</td></tr>
<tr><td><code id="r2_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing.
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic++25+3E+25'></span><span id='topic+typical_value'></span><span id='topic+mse'></span><span id='topic+rmse'></span><span id='topic+ci'></span><span id='topic+equivalence_test'></span><span id='topic+link_inverse'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>bayestestR</dt><dd><p><code><a href="bayestestR.html#topic+ci">ci</a></code>, <code><a href="bayestestR.html#topic+equivalence_test">equivalence_test</a></code></p>
</dd>
<dt>insight</dt><dd><p><code><a href="insight.html#topic+link_inverse">link_inverse</a></code></p>
</dd>
<dt>magrittr</dt><dd><p><code><a href="magrittr.html#topic+pipe">%&gt;%</a></code></p>
</dd>
<dt>performance</dt><dd><p><code><a href="performance.html#topic+performance_mse">mse</a></code>, <code><a href="performance.html#topic+performance_rmse">rmse</a></code></p>
</dd>
<dt>sjmisc</dt><dd><p><code><a href="sjmisc.html#topic+typical_value">typical_value</a></code></p>
</dd>
</dl>

<hr>
<h2 id='samplesize_mixed'>Sample size for linear mixed models</h2><span id='topic+samplesize_mixed'></span><span id='topic+smpsize_lmm'></span>

<h3>Description</h3>

<p>Compute an approximated sample size for linear mixed models
(two-level-designs), based on power-calculation for standard
design and adjusted for design effect for 2-level-designs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>samplesize_mixed(
  eff.size,
  df.n = NULL,
  power = 0.8,
  sig.level = 0.05,
  k,
  n,
  icc = 0.05
)

smpsize_lmm(
  eff.size,
  df.n = NULL,
  power = 0.8,
  sig.level = 0.05,
  k,
  n,
  icc = 0.05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="samplesize_mixed_+3A_eff.size">eff.size</code></td>
<td>
<p>Effect size.</p>
</td></tr>
<tr><td><code id="samplesize_mixed_+3A_df.n">df.n</code></td>
<td>
<p>Optional argument for the degrees of freedom for numerator. See 'Details'.</p>
</td></tr>
<tr><td><code id="samplesize_mixed_+3A_power">power</code></td>
<td>
<p>Power of test (1 minus Type II error probability).</p>
</td></tr>
<tr><td><code id="samplesize_mixed_+3A_sig.level">sig.level</code></td>
<td>
<p>Significance level (Type I error probability).</p>
</td></tr>
<tr><td><code id="samplesize_mixed_+3A_k">k</code></td>
<td>
<p>Number of cluster groups (level-2-unit) in multilevel-design.</p>
</td></tr>
<tr><td><code id="samplesize_mixed_+3A_n">n</code></td>
<td>
<p>Optional, number of observations per cluster groups
(level-2-unit) in multilevel-design.</p>
</td></tr>
<tr><td><code id="samplesize_mixed_+3A_icc">icc</code></td>
<td>
<p>Expected intraclass correlation coefficient for multilevel-model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sample size calculation is based on a power-calculation for the
standard design. If <code>df.n</code> is not specified, a power-calculation
for an unpaired two-sample t-test will be computed (using
<code><a href="pwr.html#topic+pwr.t.test">pwr.t.test</a></code> of the <a href="https://CRAN.R-project.org/package=pwr"><span class="pkg">pwr</span></a>-package).
If <code>df.n</code> is given, a power-calculation for general linear models
will be computed (using <code><a href="pwr.html#topic+pwr.f2.test">pwr.f2.test</a></code> of the
<span class="pkg">pwr</span>-package). The sample size of the standard design
is then adjusted for the design effect of two-level-designs (see
<code><a href="#topic+design_effect">design_effect</a></code>). Thus, the sample size calculation is appropriate
in particular for two-level-designs (see <cite>Snijders 2005</cite>). Models that
additionally include repeated measures (three-level-designs) may work
as well, however, the computed sample size may be less accurate.
</p>


<h3>Value</h3>

<p>A list with two values: The number of subjects per cluster, and the
total sample size for the linear mixed model.
</p>


<h3>References</h3>

<p>Cohen J. 1988. Statistical power analysis for the behavioral sciences (2nd ed.). Hillsdale,NJ: Lawrence Erlbaum.
<br /> <br />
Hsieh FY, Lavori PW, Cohen HJ, Feussner JR. 2003. An Overview of Variance Inflation Factors for Sample-Size Calculation. Evaluation and the Health Professions 26: 239-257.
<br /> <br />
Snijders TAB. 2005. Power and Sample Size in Multilevel Linear Models. In: Everitt BS, Howell DC (Hrsg.). Encyclopedia of Statistics in Behavioral Science. Chichester, UK: John Wiley and Sons, Ltd.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Sample size for multilevel model with 30 cluster groups and a small to
# medium effect size (Cohen's d) of 0.3. 27 subjects per cluster and
# hence a total sample size of about 802 observations is needed.
samplesize_mixed(eff.size = .3, k = 30)

# Sample size for multilevel model with 20 cluster groups and a medium
# to large effect size for linear models of 0.2. Five subjects per cluster and
# hence a total sample size of about 107 observations is needed.
samplesize_mixed(eff.size = .2, df.n = 5, k = 20, power = .9)
</code></pre>

<hr>
<h2 id='se_ybar'>Standard error of sample mean for mixed models</h2><span id='topic+se_ybar'></span>

<h3>Description</h3>

<p>Compute the standard error for the sample mean for mixed models,
regarding the extent to which clustering affects the standard errors.
May be used as part of the multilevel power calculation for cluster sampling
(see <cite>Gelman and Hill 2007, 447ff</cite>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>se_ybar(fit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="se_ybar_+3A_fit">fit</code></td>
<td>
<p>Fitted mixed effects model (<code><a href="lme4.html#topic+merMod">merMod</a></code>-class).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The standard error of the sample mean of <code>fit</code>.
</p>


<h3>References</h3>

<p>Gelman A, Hill J. 2007. Data analysis using regression and multilevel/hierarchical models. Cambridge, New York: Cambridge University Press
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (require("lme4")) {
  fit &lt;- lmer(Reaction ~ 1 + (1 | Subject), sleepstudy)
  se_ybar(fit)
}
</code></pre>

<hr>
<h2 id='survey_median'>Weighted statistics for tests and variables</h2><span id='topic+survey_median'></span><span id='topic+weighted_chisqtest'></span><span id='topic+weighted_chisqtest.default'></span><span id='topic+weighted_chisqtest.formula'></span><span id='topic+weighted_correlation'></span><span id='topic+weighted_correlation.default'></span><span id='topic+weighted_correlation.formula'></span><span id='topic+weighted_mean'></span><span id='topic+weighted_median'></span><span id='topic+weighted_mannwhitney'></span><span id='topic+weighted_mannwhitney.default'></span><span id='topic+weighted_mannwhitney.formula'></span><span id='topic+weighted_sd'></span><span id='topic+wtd_sd'></span><span id='topic+weighted_se'></span><span id='topic+weighted_ttest'></span><span id='topic+weighted_ttest.default'></span><span id='topic+weighted_ttest.formula'></span>

<h3>Description</h3>

<p><strong>Weighted statistics for variables</strong>
<br /> <br />
<code>weighted_sd()</code>, <code>weighted_se()</code>, <code>weighted_mean()</code> and <code>weighted_median()</code>
compute weighted standard deviation, standard error, mean or median for a
variable or for all variables of a data frame. <code>survey_median()</code> computes the
median for a variable in a survey-design (see <code><a href="survey.html#topic+svydesign">svydesign</a></code>).
<code>weighted_correlation()</code> computes a weighted correlation for a two-sided alternative
hypothesis.
<br /> <br />
<strong>Weighted tests</strong>
<br /> <br />
<code>weighted_ttest()</code> computes a weighted t-test, while <code>weighted_mannwhitney()</code>
computes a weighted Mann-Whitney-U test or a Kruskal-Wallis test
(for more than two groups). <code>weighted_chisqtest()</code> computes a weighted
Chi-squared test for contigency tables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survey_median(x, design)

weighted_chisqtest(data, ...)

## Default S3 method:
weighted_chisqtest(data, x, y, weights, ...)

## S3 method for class 'formula'
weighted_chisqtest(formula, data, ...)

weighted_correlation(data, ...)

## Default S3 method:
weighted_correlation(data, x, y, weights, ci.lvl = 0.95, ...)

## S3 method for class 'formula'
weighted_correlation(formula, data, ci.lvl = 0.95, ...)

weighted_mean(x, weights = NULL)

weighted_median(x, weights = NULL)

weighted_mannwhitney(data, ...)

## Default S3 method:
weighted_mannwhitney(data, x, grp, weights, ...)

## S3 method for class 'formula'
weighted_mannwhitney(formula, data, ...)

weighted_sd(x, weights = NULL)

wtd_sd(x, weights = NULL)

weighted_se(x, weights = NULL)

weighted_ttest(data, ...)

## Default S3 method:
weighted_ttest(
  data,
  x,
  y = NULL,
  weights,
  mu = 0,
  paired = FALSE,
  ci.lvl = 0.95,
  alternative = c("two.sided", "less", "greater"),
  ...
)

## S3 method for class 'formula'
weighted_ttest(
  formula,
  data,
  mu = 0,
  paired = FALSE,
  ci.lvl = 0.95,
  alternative = c("two.sided", "less", "greater"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survey_median_+3A_x">x</code></td>
<td>
<p>(Numeric) vector or a data frame. For <code>survey_median()</code>, <code>weighted_ttest()</code>,
<code>weighted_mannwhitney()</code> and <code>weighted_chisqtest()</code> the bare (unquoted) variable
name, or a character vector with the variable name.</p>
</td></tr>
<tr><td><code id="survey_median_+3A_design">design</code></td>
<td>
<p>An object of class <code><a href="survey.html#topic+svydesign">svydesign</a></code>, providing
a specification of the survey design.</p>
</td></tr>
<tr><td><code id="survey_median_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="survey_median_+3A_...">...</code></td>
<td>
<p>For <code>weighted_ttest()</code> and <code>weighted_mannwhitney()</code>, currently not used.
For <code>weighted_chisqtest()</code>, further arguments passed down to
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code>.</p>
</td></tr>
<tr><td><code id="survey_median_+3A_y">y</code></td>
<td>
<p>Optional, bare (unquoted) variable name, or a character vector with
the variable name.</p>
</td></tr>
<tr><td><code id="survey_median_+3A_weights">weights</code></td>
<td>
<p>Bare (unquoted) variable name, or a character vector with
the variable name of the numeric vector of weights. If <code>weights = NULL</code>,
unweighted statistic is reported.</p>
</td></tr>
<tr><td><code id="survey_median_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>lhs ~ rhs1 + rhs2</code> where <code>lhs</code> is a
numeric variable giving the data values and <code>rhs1</code> a factor with two
levels giving the corresponding groups and <code>rhs2</code> a variable with weights.</p>
</td></tr>
<tr><td><code id="survey_median_+3A_ci.lvl">ci.lvl</code></td>
<td>
<p>Confidence level of the interval.</p>
</td></tr>
<tr><td><code id="survey_median_+3A_grp">grp</code></td>
<td>
<p>Bare (unquoted) name of the cross-classifying variable, where
<code>x</code> is grouped into the categories represented by <code>grp</code>,
or a character vector with the variable name.</p>
</td></tr>
<tr><td><code id="survey_median_+3A_mu">mu</code></td>
<td>
<p>A number indicating the true value of the mean (or difference in
means if you are performing a two sample test).</p>
</td></tr>
<tr><td><code id="survey_median_+3A_paired">paired</code></td>
<td>
<p>Logical, whether to compute a paired t-test.</p>
</td></tr>
<tr><td><code id="survey_median_+3A_alternative">alternative</code></td>
<td>
<p>A character string specifying the alternative hypothesis,
must be one of <code>"two.sided"</code> (default), <code>"greater"</code> or
<code>"less"</code>. You can specify just the initial letter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The weighted (test) statistic.
</p>


<h3>Note</h3>

<p><code>weighted_chisq()</code> is a convenient wrapper for <code><a href="#topic+crosstable_statistics">crosstable_statistics</a></code>.
For a weighted one-way Anova, use <code>means_by_group()</code> with
<code>weights</code>-argument.
<br /> <br />
<code>weighted_ttest()</code> assumes unequal variance between the two groups.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># weighted sd and se ----
weighted_sd(rnorm(n = 100, mean = 3), runif(n = 100))

data(efc)
weighted_sd(efc[, 1:3], runif(n = nrow(efc)))
weighted_se(efc[, 1:3], runif(n = nrow(efc)))

# survey_median ----
# median for variables from weighted survey designs
if (require("survey")) {
  data(nhanes_sample)

  des &lt;- svydesign(
    id = ~SDMVPSU,
    strat = ~SDMVSTRA,
    weights = ~WTINT2YR,
    nest = TRUE,
    data = nhanes_sample
  )

  survey_median(total, des)
  survey_median("total", des)
}

# weighted t-test ----
efc$weight &lt;- abs(rnorm(nrow(efc), 1, .3))
weighted_ttest(efc, e17age, weights = weight)
weighted_ttest(efc, e17age, c160age, weights = weight)
weighted_ttest(e17age ~ e16sex + weight, efc)

# weighted Mann-Whitney-U-test ----
weighted_mannwhitney(c12hour ~ c161sex + weight, efc)

# weighted Chi-squared-test ----
weighted_chisqtest(efc, c161sex, e16sex, weights = weight, correct = FALSE)
weighted_chisqtest(c172code ~ c161sex + weight, efc)

# weighted Chi-squared-test for given probabilities ----
weighted_chisqtest(c172code ~ weight, efc, p = c(.33, .33, .34))
</code></pre>

<hr>
<h2 id='svyglm.nb'>Survey-weighted negative binomial generalised linear model</h2><span id='topic+svyglm.nb'></span>

<h3>Description</h3>

<p><code>svyglm.nb()</code> is an extension to the <a href="https://CRAN.R-project.org/package=survey"><span class="pkg">survey</span></a>-package
to fit survey-weighted negative binomial models. It uses
<code><a href="survey.html#topic+svymle">svymle</a></code> to fit sampling-weighted
maximum likelihood estimates, based on starting values provided
by <code><a href="MASS.html#topic+glm.nb">glm.nb</a></code>, as proposed by <em>Lumley
(2010, pp249)</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyglm.nb(formula, design, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyglm.nb_+3A_formula">formula</code></td>
<td>
<p>An object of class <code>formula</code>, i.e. a symbolic description
of the model to be fitted. See 'Details' in <code><a href="stats.html#topic+glm">glm</a></code>.</p>
</td></tr>
<tr><td><code id="svyglm.nb_+3A_design">design</code></td>
<td>
<p>An object of class <code><a href="survey.html#topic+svydesign">svydesign</a></code>, providing
a specification of the survey design.</p>
</td></tr>
<tr><td><code id="svyglm.nb_+3A_...">...</code></td>
<td>
<p>Other arguments passed down to <code><a href="MASS.html#topic+glm.nb">glm.nb</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details on the computation method, see Lumley (2010), Appendix E
(especially 254ff.)
<br /> <br />
<span class="pkg">sjstats</span> implements following S3-methods for <code>svyglm.nb</code>-objects:
<code>family()</code>, <code>model.frame()</code>, <code>formula()</code>, <code>print()</code>,
<code>predict()</code> and <code>residuals()</code>. However, these functions have some
limitations:
</p>

<ul>
<li><p><code>family()</code> simply returns the family-object from the
underlying <code><a href="MASS.html#topic+glm.nb">glm.nb</a></code>-model.
</p>
</li>
<li><p>The <code>predict()</code>-method just re-fits the <code>svyglm.nb</code>-model
with <code><a href="MASS.html#topic+glm.nb">glm.nb</a></code>, overwrites the <code>$coefficients</code>
from this model-object with the coefficients from the returned
<code><a href="survey.html#topic+svymle">svymle</a></code>-object and finally calls
<code><a href="stats.html#topic+predict.glm">predict.glm</a></code> to compute the predicted values.
</p>
</li>
<li><p><code>residuals()</code> re-fits the <code>svyglm.nb</code>-model with
<code><a href="MASS.html#topic+glm.nb">glm.nb</a></code> and then computes the Pearson-residuals
from the <code>glm.nb</code>-object.
</p>
</li></ul>



<h3>Value</h3>

<p>An object of class <code><a href="survey.html#topic+svymle">svymle</a></code> and <code>svyglm.nb</code>,
with some additional information about the model.
</p>


<h3>References</h3>

<p>Lumley T (2010). Complex Surveys: a guide to analysis using R. Wiley
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ------------------------------------------
# This example reproduces the results from
# Lumley 2010, figure E.7 (Appendix E, p256)
# ------------------------------------------
if (require("survey")) {
  data(nhanes_sample)

  # create survey design
  des &lt;- svydesign(
    id = ~SDMVPSU,
    strat = ~SDMVSTRA,
    weights = ~WTINT2YR,
    nest = TRUE,
    data = nhanes_sample
  )

  # fit negative binomial regression
  fit &lt;- svyglm.nb(total ~ factor(RIAGENDR) * (log(age) + factor(RIDRETH1)), des)

  # print coefficients and standard errors
  fit
}
</code></pre>

<hr>
<h2 id='svyglm.zip'>Survey-weighted zero-inflated Poisson model</h2><span id='topic+svyglm.zip'></span>

<h3>Description</h3>

<p><code>svyglm.zip()</code> is an extension to the <a href="https://CRAN.R-project.org/package=survey"><span class="pkg">survey</span></a>-package
to fit survey-weighted zero-inflated Poisson models. It uses
<code><a href="survey.html#topic+svymle">svymle</a></code> to fit sampling-weighted
maximum likelihood estimates, based on starting values provided
by <code><a href="pscl.html#topic+zeroinfl">zeroinfl</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyglm.zip(formula, design, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyglm.zip_+3A_formula">formula</code></td>
<td>
<p>An object of class <code>formula</code>, i.e. a symbolic description
of the model to be fitted. See 'Details' in <code><a href="pscl.html#topic+zeroinfl">zeroinfl</a></code>.</p>
</td></tr>
<tr><td><code id="svyglm.zip_+3A_design">design</code></td>
<td>
<p>An object of class <code><a href="survey.html#topic+svydesign">svydesign</a></code>, providing
a specification of the survey design.</p>
</td></tr>
<tr><td><code id="svyglm.zip_+3A_...">...</code></td>
<td>
<p>Other arguments passed down to <code><a href="pscl.html#topic+zeroinfl">zeroinfl</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Code modified from https://notstatschat.rbind.io/2015/05/26/zero-inflated-poisson-from-complex-samples/.
</p>


<h3>Value</h3>

<p>An object of class <code><a href="survey.html#topic+svymle">svymle</a></code> and <code>svyglm.zip</code>,
with some additional information about the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (require("survey")) {
  data(nhanes_sample)
  set.seed(123)
  nhanes_sample$malepartners &lt;- rpois(nrow(nhanes_sample), 2)
  nhanes_sample$malepartners[sample(1:2992, 400)] &lt;- 0

  # create survey design
  des &lt;- svydesign(
    id = ~SDMVPSU,
    strat = ~SDMVSTRA,
    weights = ~WTINT2YR,
    nest = TRUE,
    data = nhanes_sample
  )

  # fit negative binomial regression
  fit &lt;- svyglm.zip(
    malepartners ~ age + factor(RIDRETH1) | age + factor(RIDRETH1),
    des
  )

  # print coefficients and standard errors
  fit
}
</code></pre>

<hr>
<h2 id='table_values'>Expected and relative table values</h2><span id='topic+table_values'></span>

<h3>Description</h3>

<p>This function calculates a table's cell, row and column percentages as
well as expected values and returns all results as lists of tables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>table_values(tab, digits = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="table_values_+3A_tab">tab</code></td>
<td>
<p>Simple <code><a href="base.html#topic+table">table</a></code> or <code><a href="stats.html#topic+ftable">ftable</a></code> of which
cell, row and column percentages as well as expected values are calculated.
Tables of class <code><a href="stats.html#topic+xtabs">xtabs</a></code> and other will be coerced to
<code>ftable</code> objects.</p>
</td></tr>
<tr><td><code id="table_values_+3A_digits">digits</code></td>
<td>
<p>Amount of digits for the table percentage values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(Invisibly) returns a list with four tables:
</p>

<ol>
<li> <p><code>cell</code> a table with cell percentages of <code>tab</code>
</p>
</li>
<li> <p><code>row</code> a table with row percentages of <code>tab</code>
</p>
</li>
<li> <p><code>col</code> a table with column percentages of <code>tab</code>
</p>
</li>
<li> <p><code>expected</code> a table with expected values of <code>tab</code>
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'>tab &lt;- table(sample(1:2, 30, TRUE), sample(1:3, 30, TRUE))
# show expected values
table_values(tab)$expected
# show cell percentages
table_values(tab)$cell

</code></pre>

<hr>
<h2 id='var_pop'>Calculate population variance and standard deviation</h2><span id='topic+var_pop'></span><span id='topic+sd_pop'></span>

<h3>Description</h3>

<p>Calculate the population variance or standard deviation of a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_pop(x)

sd_pop(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_pop_+3A_x">x</code></td>
<td>
<p>(Numeric) vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unlike <code><a href="stats.html#topic+var">var</a></code>, which returns the sample variance,
<code>var_pop()</code> returns the population variance. <code>sd_pop()</code>
returns the standard deviation based on the population variance.
</p>


<h3>Value</h3>

<p>The population variance or standard deviation of <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(efc)

# sampling variance
var(efc$c12hour, na.rm = TRUE)
# population variance
var_pop(efc$c12hour)

# sampling sd
sd(efc$c12hour, na.rm = TRUE)
# population sd
sd_pop(efc$c12hour)

</code></pre>

<hr>
<h2 id='weight'>Weight a variable</h2><span id='topic+weight'></span><span id='topic+weight2'></span>

<h3>Description</h3>

<p>These functions weight the variable <code>x</code> by
a specific vector of <code>weights</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weight(x, weights, digits = 0)

weight2(x, weights)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weight_+3A_x">x</code></td>
<td>
<p>(Unweighted) variable.</p>
</td></tr>
<tr><td><code id="weight_+3A_weights">weights</code></td>
<td>
<p>Vector with same length as <code>x</code>, which
contains weight factors. Each value of <code>x</code> has a
specific assigned weight in <code>weights</code>.</p>
</td></tr>
<tr><td><code id="weight_+3A_digits">digits</code></td>
<td>
<p>Numeric value indicating the number of decimal places to be
used for rounding the weighted values. By default, this value is
<code>0</code>, i.e. the returned values are integer values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>weight2()</code> sums up all <code>weights</code> values of the associated
categories of <code>x</code>, whereas <code>weight()</code> uses a
<code><a href="stats.html#topic+xtabs">xtabs</a></code> formula to weight cases. Thus, <code>weight()</code>
may return a vector of different length than <code>x</code>.
</p>


<h3>Value</h3>

<p>The weighted <code>x</code>.
</p>


<h3>Note</h3>

<p>The values of the returned vector are in sorted order, whereas the values'
order of the original <code>x</code> may be spread randomly. Hence, <code>x</code> can't be
used, for instance, for further cross tabulation. In case you want to have
weighted contingency tables or (grouped) box plots etc., use the <code>weightBy</code>
argument of most functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>v &lt;- sample(1:4, 20, TRUE)
table(v)
w &lt;- abs(rnorm(20))
table(weight(v, w))
table(weight2(v, w))

set.seed(1)
x &lt;- sample(letters[1:5], size = 20, replace = TRUE)
w &lt;- runif(n = 20)

table(x)
table(weight(x, w))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
