<!DOCTYPE html><html><head><title>Help for package doc2concrete</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {doc2concrete}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adviceModel'><p>Pre-trained Concreteness Detection Model for Advice</p></a></li>
<li><a href='#adviceNgrams'><p>Pre-trained advice concreteness features</p></a></li>
<li><a href='#bootstrap_list'><p>Concreteness mTurk Word List</p></a></li>
<li><a href='#cleanpunct'><p>Cleaning weird encodings</p></a></li>
<li><a href='#cleantext'><p>Text Cleaner</p></a></li>
<li><a href='#concDict'><p>Open-Domain Concreteness Dictionaries</p></a></li>
<li><a href='#ctxpand'><p>Contraction Expander</p></a></li>
<li><a href='#doc2concrete'><p>Concreteness Scores</p></a></li>
<li><a href='#doublestacker'><p>Doublestacker</p></a></li>
<li><a href='#feedback_dat'><p>Personal Feedback Dataset</p></a></li>
<li><a href='#mturk_list'><p>Concreteness mTurk Word List</p></a></li>
<li><a href='#ngramTokens'><p>Ngram Tokenizer</p></a></li>
<li><a href='#overlaps'><p>Overlap cleaner</p></a></li>
<li><a href='#planModel'><p>Pre-trained Concreteness Detection Model for Plan-Making</p></a></li>
<li><a href='#planNgrams'><p>Pre-trained plan concreteness features</p></a></li>
<li><a href='#stemexcept'><p>Conditional Stemmer</p></a></li>
<li><a href='#stemmer'><p>Stemmer</p></a></li>
<li><a href='#textformat'><p>Text Formatter</p></a></li>
<li><a href='#uk2us'><p>UK to US Conversion dictionary</p></a></li>
<li><a href='#usWords'><p>UK to US conversion</p></a></li>
<li><a href='#vocabmatcher'><p>Feature Count Matcher</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Measuring Concreteness in Natural Language</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Mike Yeomans</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mike Yeomans &lt;mk.yeomans@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Models for detecting concreteness in natural language. This package is built in support of Yeomans (2021) &lt;<a href="https://doi.org/10.1016%2Fj.obhdp.2020.10.008">doi:10.1016/j.obhdp.2020.10.008</a>&gt;, which reviews linguistic models of concreteness in several domains. Here, we provide an implementation of the best-performing domain-general model (from Brysbaert et al., (2014) &lt;<a href="https://doi.org/10.3758%2Fs13428-013-0403-5">doi:10.3758/s13428-013-0403-5</a>&gt;) as well as two pre-trained models for the feedback and plan-making domains.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>tm, quanteda, parallel, glmnet, stringr, english, textstem,
SnowballC, stringi</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-23 11:52:59 UTC; myeomans</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-23 12:32:52 UTC</td>
</tr>
</table>
<hr>
<h2 id='adviceModel'>Pre-trained Concreteness Detection Model for Advice</h2><span id='topic+adviceModel'></span>

<h3>Description</h3>

<p>This model was pre-trained on 3289 examples of feedback on different tasks (e.g. writing a cover letter, boggle, workplace annual reviews). All of those documents were annotated by research assistants for concreteness, and this model simulates those annotations on new documents.
</p>
<p>Model pre-trained on advice data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adviceModel

adviceModel(texts, num.mc.cores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adviceModel_+3A_texts">texts</code></td>
<td>
<p>character A vector of texts, each of which will be tallied for concreteness.</p>
</td></tr>
<tr><td><code id="adviceModel_+3A_num.mc.cores">num.mc.cores</code></td>
<td>
<p>numeric number of cores for parallel processing - see parallel::detectCores(). Default is 1.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>A pre-trained glmnet model
</p>


<h3>Value</h3>

<p>numeric Vector of concreteness ratings.
</p>


<h3>Source</h3>

<p>Yeomans (2020). A Concrete Application of Open Science for Natural Language Processing.
</p>

<hr>
<h2 id='adviceNgrams'>Pre-trained advice concreteness features</h2><span id='topic+adviceNgrams'></span>

<h3>Description</h3>

<p>For internal use only. This dataset demonstrates the ngram features that are used for the pre-trained adviceModel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adviceNgrams
</code></pre>


<h3>Format</h3>

<p>A (truncated) matrix of ngram feature counts for alignment to the pre-trained advice glmnet model.
</p>


<h3>Source</h3>

<p>Yeomans (2020). A Concrete Application of Open Science for Natural Language Processing.
</p>

<hr>
<h2 id='bootstrap_list'>Concreteness mTurk Word List</h2><span id='topic+bootstrap_list'></span>

<h3>Description</h3>

<p>Word list from Paetzold &amp; Specia (2016). A list of 85,942 words where concreteness was imputed using word embeddings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_list
</code></pre>


<h3>Format</h3>

<p>A data frame with 85,942 rows and 2 variables.
</p>

<dl>
<dt>Word</dt><dd><p>character text of a word with an entry in this dictionary</p>
</dd>
<dt>Conc.M</dt><dd><p>predicted concreteness score for that word (from 100-700)</p>
</dd>
</dl>



<h3>Source</h3>

<p>#' Paetzold, G., &amp; Specia, L. (2016, June). Inferring psycholinguistic properties of words. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 435-440).
</p>

<hr>
<h2 id='cleanpunct'>Cleaning weird encodings</h2><span id='topic+cleanpunct'></span>

<h3>Description</h3>

<p>Handles curly quotes, umlauts, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cleanpunct(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cleanpunct_+3A_text">text</code></td>
<td>
<p>character Vector of strings to clean.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character Vector of clean strings.
</p>

<hr>
<h2 id='cleantext'>Text Cleaner</h2><span id='topic+cleantext'></span>

<h3>Description</h3>

<p>background function to load.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cleantext(
  text,
  language = "english",
  punct = FALSE,
  stop.words = TRUE,
  number.words = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cleantext_+3A_text">text</code></td>
<td>
<p>character Vector of strings to clean.</p>
</td></tr>
<tr><td><code id="cleantext_+3A_language">language</code></td>
<td>
<p>character Language to use for cleaning. Default is &quot;english&quot;.</p>
</td></tr>
<tr><td><code id="cleantext_+3A_punct">punct</code></td>
<td>
<p>logical Should punctuation be kept as tokens? Default is TRUE.</p>
</td></tr>
<tr><td><code id="cleantext_+3A_stop.words">stop.words</code></td>
<td>
<p>logical Should stop words be kept? default is TRUE.</p>
</td></tr>
<tr><td><code id="cleantext_+3A_number.words">number.words</code></td>
<td>
<p>logical Should numbers be converted to words? default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character Vector of cleaned strings.
</p>

<hr>
<h2 id='concDict'>Open-Domain Concreteness Dictionaries</h2><span id='topic+concDict'></span>

<h3>Description</h3>

<p>background function to load
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concDict(
  texts,
  wordlist = NULL,
  stop.words = TRUE,
  number.words = TRUE,
  shrink = FALSE,
  fill = FALSE,
  minwords = 0,
  num.mc.cores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="concDict_+3A_texts">texts</code></td>
<td>
<p>character Vector of documents to classify</p>
</td></tr>
<tr><td><code id="concDict_+3A_wordlist">wordlist</code></td>
<td>
<p>Dictionary to be used.</p>
</td></tr>
<tr><td><code id="concDict_+3A_stop.words">stop.words</code></td>
<td>
<p>logical should stop words be kept? default is TRUE</p>
</td></tr>
<tr><td><code id="concDict_+3A_number.words">number.words</code></td>
<td>
<p>logical should numbers be converted to words? default is TRUE</p>
</td></tr>
<tr><td><code id="concDict_+3A_shrink">shrink</code></td>
<td>
<p>logical should scores on shorter documents be regularized? default is FALSE</p>
</td></tr>
<tr><td><code id="concDict_+3A_fill">fill</code></td>
<td>
<p>logical Should empty cells be assigned the mean rating? Default is FALSE.</p>
</td></tr>
<tr><td><code id="concDict_+3A_minwords">minwords</code></td>
<td>
<p>numeric all documents with less words than this return NA. default is 0 (i.e. keep all documents)</p>
</td></tr>
<tr><td><code id="concDict_+3A_num.mc.cores">num.mc.cores</code></td>
<td>
<p>numeric number of cores for parallel processing - see parallel::detectCores()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>concreteness score for each document
</p>

<hr>
<h2 id='ctxpand'>Contraction Expander</h2><span id='topic+ctxpand'></span>

<h3>Description</h3>

<p>background function to load.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ctxpand(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ctxpand_+3A_text">text</code></td>
<td>
<p>character vector of sentences to un-contract.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character Vector of sentences without contractions.
</p>

<hr>
<h2 id='doc2concrete'>Concreteness Scores</h2><span id='topic+doc2concrete'></span>

<h3>Description</h3>

<p>Detects linguistic markers of concreteness in natural language.
This function is the workhorse of the <code>doc2concrete</code> package, taking a vector of text documents and returning an equal-length vector of concreteness scores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>doc2concrete(
  texts,
  domain = c("open", "advice", "plans"),
  wordlist = doc2concrete::mturk_list,
  stop.words = TRUE,
  number.words = TRUE,
  shrink = FALSE,
  fill = FALSE,
  uk_english = FALSE,
  num.mc.cores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="doc2concrete_+3A_texts">texts</code></td>
<td>
<p>character A vector of texts, each of which will be tallied for concreteness.</p>
</td></tr>
<tr><td><code id="doc2concrete_+3A_domain">domain</code></td>
<td>
<p>character Indicates the domain from which the text data was collected (see details).</p>
</td></tr>
<tr><td><code id="doc2concrete_+3A_wordlist">wordlist</code></td>
<td>
<p>Dictionary to be used. Default is the Brysbaert et al. (2014) list.</p>
</td></tr>
<tr><td><code id="doc2concrete_+3A_stop.words">stop.words</code></td>
<td>
<p>logical Should stop words be kept? Default is TRUE</p>
</td></tr>
<tr><td><code id="doc2concrete_+3A_number.words">number.words</code></td>
<td>
<p>logical Should numbers be converted to words? Default is TRUE</p>
</td></tr>
<tr><td><code id="doc2concrete_+3A_shrink">shrink</code></td>
<td>
<p>logical Should open-domain concreteness models regularize low-count words? Default is FALSE.</p>
</td></tr>
<tr><td><code id="doc2concrete_+3A_fill">fill</code></td>
<td>
<p>logical Should empty cells be assigned the mean rating? Default is TRUE.</p>
</td></tr>
<tr><td><code id="doc2concrete_+3A_uk_english">uk_english</code></td>
<td>
<p>logical Does the text contain any British English spelling? Including variants (e.g. Canadian). Default is FALSE</p>
</td></tr>
<tr><td><code id="doc2concrete_+3A_num.mc.cores">num.mc.cores</code></td>
<td>
<p>numeric number of cores for parallel processing - see parallel::detectCores(). Default is 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In principle, concreteness could be measured from any english text. However, the
definition and interpretation of concreteness may vary based on the domain. Here, we provide
a domain-specific pre-trained classifier for concreteness in advice &amp; feedback data, which we have
empirically confirmed to be robust across a variety of contexts within that domain (Yeomans, 2021).
</p>
<p>The training data for the advice classifier includes both second-person (e.g. &quot;You should&quot;) and
third-person (e.g. &quot;She should&quot;) framing, including some names (e.g. &quot;Riley should&quot;). For consistency,
we anonymised all our training data to replace any names with &quot;Riley&quot;. If you are working with a
dataset that includes the names of advice recipients, we recommend you convert all those names to
&quot;Riley&quot; as well, to ensure optimal performance of the algorithm (and to respect their privacy).
</p>
<p>There are many domains where such pre-training is not yet possible. Accordingly, we provide
support for two off-the-shelf concreteness &quot;dictionaries&quot; - i.e. document-level aggregations of
word-level scores. We found that that have modest (but consistent) accuracy across domains and contexts.
However, we still encourage researchers to train a model of concreteness in their own domain, if possible.
</p>


<h3>Value</h3>

<p>A vector of concreteness scores, with one value for every item in 'text'.
</p>


<h3>References</h3>

<p>Yeomans, M. (2021). A Concrete Application of Open Science for Natural Language Processing. Organizational Behavior and Human Decision Processes, 162, 81-94.
</p>
<p>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46(3), 904-911.
</p>
<p>Paetzold, G., &amp; Specia, L. (2016, June). Inferring psycholinguistic properties of words. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 435-440).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data("feedback_dat")

doc2concrete(feedback_dat$feedback, domain="open")


cor(doc2concrete(feedback_dat$feedback, domain="open"),feedback_dat$concrete)


</code></pre>

<hr>
<h2 id='doublestacker'>Doublestacker</h2><span id='topic+doublestacker'></span>

<h3>Description</h3>

<p>background function to load
</p>


<h3>Usage</h3>

<pre><code class='language-R'>doublestacker(wdcts)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="doublestacker_+3A_wdcts">wdcts</code></td>
<td>
<p>matrix Token counts that will have doubled column names condensed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Token count matrix with no doubled column names.
</p>

<hr>
<h2 id='feedback_dat'>Personal Feedback Dataset</h2><span id='topic+feedback_dat'></span>

<h3>Description</h3>

<p>A dataset containing responses from people on Mechanical Turk, writing
feedback to a recent collaborator, that were then scored by other Turkers
for feedback specificity. Note that all proper names of advice recipients
have been substituted with &quot;Riley&quot; - we recommend the same in your data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>feedback_dat
</code></pre>


<h3>Format</h3>

<p>A data frame with 171 rows and 2 variables:
</p>

<dl>
<dt>feedback</dt><dd><p>character text of feedback from writers</p>
</dd>
<dt>concrete</dt><dd><p>numeric average specificity score from readers</p>
</dd>
</dl>



<h3>Source</h3>

<p>Blunden, H., Green, P., &amp; Gino, F. (2018).
</p>
<p>&quot;The Impersonal Touch: Improving Feedback-Giving with Interpersonal Distance.&quot;
</p>
<p>Academy of Management Proceedings, 2018.
</p>

<hr>
<h2 id='mturk_list'>Concreteness mTurk Word List</h2><span id='topic+mturk_list'></span>

<h3>Description</h3>

<p>Word list from Brysbaert, Warriner &amp; Kuperman (2014). A list of 39,954 words that have been hand-annotated by crowdsourced workers for concreteness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mturk_list
</code></pre>


<h3>Format</h3>

<p>A data frame with 39,954 rows and 2 variables.
</p>

<dl>
<dt>Word</dt><dd><p>character text of a word with an entry in this dictionary</p>
</dd>
<dt>Conc.M</dt><dd><p>average concreteness score for that word (from 1-5)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46(3), 904-911.
</p>

<hr>
<h2 id='ngramTokens'>Ngram Tokenizer</h2><span id='topic+ngramTokens'></span>

<h3>Description</h3>

<p>Tally bag-of-words ngram features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ngramTokens(
  texts,
  wstem = "all",
  ngrams = 1,
  language = "english",
  punct = TRUE,
  stop.words = TRUE,
  number.words = TRUE,
  per.100 = FALSE,
  overlap = 1,
  sparse = 0.995,
  verbose = FALSE,
  vocabmatch = NULL,
  num.mc.cores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ngramTokens_+3A_texts">texts</code></td>
<td>
<p>character vector of texts.</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_wstem">wstem</code></td>
<td>
<p>character Which words should be stemmed? Defaults to &quot;all&quot;.</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_ngrams">ngrams</code></td>
<td>
<p>numeric Vector of ngram lengths to be included. Default is 1 (i.e. unigrams only).</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_language">language</code></td>
<td>
<p>Language for stemming. Default is &quot;english&quot;</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_punct">punct</code></td>
<td>
<p>logical Should punctuation be kept as tokens? Default is TRUE</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_stop.words">stop.words</code></td>
<td>
<p>logical Should stop words be kept? Default is TRUE</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_number.words">number.words</code></td>
<td>
<p>logical Should numbers be kept as words? Default is TRUE</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_per.100">per.100</code></td>
<td>
<p>logical Should counts be expressed as frequency per 100 words? Default is FALSE</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_overlap">overlap</code></td>
<td>
<p>numeric Threshold (as cosine distance) for including ngrams that constitute other included phrases. Default is 1 (i.e. all ngrams included).</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_sparse">sparse</code></td>
<td>
<p>maximum feature sparsity for inclusion (1 = include all features)</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_verbose">verbose</code></td>
<td>
<p>logical Should the package report token counts after each ngram level? Useful for long-running code. Default is FALSE.</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_vocabmatch">vocabmatch</code></td>
<td>
<p>matrix Should the new token count matrix will be coerced to include the same tokens as a previous count matrix? Default is NULL (i.e. no token match).</p>
</td></tr>
<tr><td><code id="ngramTokens_+3A_num.mc.cores">num.mc.cores</code></td>
<td>
<p>numeric number of cores for parallel processing - see parallel::detectCores(). Default is 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function produces ngram featurizations of text based on the quanteda package. This provides a complement to the doc2concrete function by demonstrating
How to build a feature set for training a new detection algorithm in other contexts.
</p>


<h3>Value</h3>

<p>a matrix of feature counts
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dim(ngramTokens(feedback_dat$feedback, ngrams=1))
dim(ngramTokens(feedback_dat$feedback, ngrams=1:3))
</code></pre>

<hr>
<h2 id='overlaps'>Overlap cleaner</h2><span id='topic+overlaps'></span>

<h3>Description</h3>

<p>background function to load
</p>


<h3>Usage</h3>

<pre><code class='language-R'>overlaps(high, low, cutoff = 1, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="overlaps_+3A_high">high</code></td>
<td>
<p>matrix Token counts that will all be kept.</p>
</td></tr>
<tr><td><code id="overlaps_+3A_low">low</code></td>
<td>
<p>matrix Token counts that will evaluated (and pruned) for overlapping.</p>
</td></tr>
<tr><td><code id="overlaps_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric Threshold (as cosine distance) for including overlapping tokens. Default is 1 (i.e. all tokens included).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Combined token count matrix.
</p>

<hr>
<h2 id='planModel'>Pre-trained Concreteness Detection Model for Plan-Making</h2><span id='topic+planModel'></span>

<h3>Description</h3>

<p>This model was pre-trained on 5,172 examples of pre-course plans from online courses at HarvardX. Each plan was annotated by research assistants for concreteness, and this model simulates those annotations on new plans.
</p>
<p>Model pre-trained on planning data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>planModel

planModel(texts, num.mc.cores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="planModel_+3A_texts">texts</code></td>
<td>
<p>character A vector of texts, each of which will be tallied for concreteness.</p>
</td></tr>
<tr><td><code id="planModel_+3A_num.mc.cores">num.mc.cores</code></td>
<td>
<p>numeric number of cores for parallel processing - see parallel::detectCores(). Default is 1.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>A pre-trained glmnet model
</p>


<h3>Value</h3>

<p>numeric Vector of concreteness ratings.
</p>


<h3>Source</h3>

<p>Yeomans (2020). A Concrete Application of Open Science for Natural Language Processing.
</p>

<hr>
<h2 id='planNgrams'>Pre-trained plan concreteness features</h2><span id='topic+planNgrams'></span>

<h3>Description</h3>

<p>For internal use only. This dataset demonstrates the ngram features that are used for the pre-trained planModel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>planNgrams
</code></pre>


<h3>Format</h3>

<p>A (truncated) matrix of ngram feature counts for alignment to the pre-trained planning glmnet model.
</p>


<h3>Source</h3>

<p>Yeomans (2020). A Concrete Application of Open Science for Natural Language Processing.
</p>

<hr>
<h2 id='stemexcept'>Conditional Stemmer</h2><span id='topic+stemexcept'></span>

<h3>Description</h3>

<p>background function to load
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stemexcept(sentence, excepts, language = "english")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stemexcept_+3A_sentence">sentence</code></td>
<td>
<p>character Vector of sentences to stem.</p>
</td></tr>
<tr><td><code id="stemexcept_+3A_excepts">excepts</code></td>
<td>
<p>character Vector of words that should not be stemmed.</p>
</td></tr>
<tr><td><code id="stemexcept_+3A_language">language</code></td>
<td>
<p>Language for stemming. Default is &quot;english&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Sentence of stemmed words.
</p>

<hr>
<h2 id='stemmer'>Stemmer</h2><span id='topic+stemmer'></span>

<h3>Description</h3>

<p>background function to load.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stemmer(text, wstem = "all", language = "english")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stemmer_+3A_text">text</code></td>
<td>
<p>character vector of strings to clean.</p>
</td></tr>
<tr><td><code id="stemmer_+3A_wstem">wstem</code></td>
<td>
<p>character Which words should be stemmed? Defaults to &quot;all&quot;.</p>
</td></tr>
<tr><td><code id="stemmer_+3A_language">language</code></td>
<td>
<p>Language for stemming. Default is &quot;english&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Sentence of stemmed words.
</p>

<hr>
<h2 id='textformat'>Text Formatter</h2><span id='topic+textformat'></span>

<h3>Description</h3>

<p>background function to load.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textformat(text, punct = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textformat_+3A_text">text</code></td>
<td>
<p>character Vector of strings to clean.</p>
</td></tr>
<tr><td><code id="textformat_+3A_punct">punct</code></td>
<td>
<p>logical Should punctuation be kept as tokens? Default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character Vector of cleaned strings.
</p>

<hr>
<h2 id='uk2us'>UK to US Conversion dictionary</h2><span id='topic+uk2us'></span>

<h3>Description</h3>

<p>For internal use only. This dataset contains a quanteda dictionary for converting UK words to US words. The models in this package were all trained on US English.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uk2us
</code></pre>


<h3>Format</h3>

<p>A quanteda dictionary with named entries. Names are the US version, and entries are the UK version.
</p>


<h3>Source</h3>

<p>Borrowed from the quanteda.dictionaries package on github (from user kbenoit)
</p>

<hr>
<h2 id='usWords'>UK to US conversion</h2><span id='topic+usWords'></span>

<h3>Description</h3>

<p>background function to load.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>usWords(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="usWords_+3A_text">text</code></td>
<td>
<p>character Vector of strings to convert to US spelling.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character Vector of Americanized strings.
</p>

<hr>
<h2 id='vocabmatcher'>Feature Count Matcher</h2><span id='topic+vocabmatcher'></span>

<h3>Description</h3>

<p>background function to load
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vocabmatcher(hole, peg)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vocabmatcher_+3A_hole">hole</code></td>
<td>
<p>matrix Token counts in model data.</p>
</td></tr>
<tr><td><code id="vocabmatcher_+3A_peg">peg</code></td>
<td>
<p>matrix Token counts in new data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Token counts matrix from new data, with column names that match the model data.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
