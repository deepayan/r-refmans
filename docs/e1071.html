<!DOCTYPE html><html><head><title>Help for package e1071</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {e1071}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#allShortestPaths'><p>Find Shortest Paths Between All Nodes in a Directed Graph</p></a></li>
<li><a href='#bclust'><p>Bagged Clustering</p></a></li>
<li><a href='#bincombinations'><p>Binary Combinations</p></a></li>
<li><a href='#bootstrap.lca'><p>Bootstrap Samples of LCA Results</p></a></li>
<li><a href='#boxplot.bclust'><p>Boxplot of Cluster Profiles</p></a></li>
<li><a href='#classAgreement'><p>Coefficients Comparing Classification Agreement</p></a></li>
<li><a href='#cmeans'><p>Fuzzy C-Means Clustering</p></a></li>
<li><a href='#countpattern'><p>Count Binary Patterns</p></a></li>
<li><a href='#cshell'><p>Fuzzy C-Shell Clustering</p></a></li>
<li><a href='#Discrete'><p>Discrete Distribution</p></a></li>
<li><a href='#e1071-deprecated'><p>Deprecated Functions in Package e1071</p></a></li>
<li><a href='#element'><p>Extract Elements of an Array</p></a></li>
<li><a href='#fclustIndex'><p>Fuzzy Cluster Indexes (Validity/Performance Measures)</p></a></li>
<li><a href='#gknn'><p>Generalized k-Nearest Neighbors Classification or Regression</p></a></li>
<li><a href='#hamming.distance'><p>Hamming Distances of Vectors</p></a></li>
<li><a href='#hamming.window'><p>Computes the Coefficients of a Hamming Window.</p></a></li>
<li><a href='#hanning.window'><p>Computes the Coefficients of a Hanning Window.</p></a></li>
<li><a href='#hsv_palette'><p>Sequential color palette based on HSV colors</p></a></li>
<li><a href='#ica'><p>Independent Component Analysis</p></a></li>
<li><a href='#impute'><p>Replace Missing Values</p></a></li>
<li><a href='#interpolate'><p>Interpolate Values of Array</p></a></li>
<li><a href='#kurtosis'><p>Kurtosis</p></a></li>
<li><a href='#lca'><p>Latent Class Analysis (LCA)</p></a></li>
<li><a href='#matchClasses'><p>Find Similar Classes in Two-way Contingency Tables</p></a></li>
<li><a href='#matchControls'><p>Find Matched Control Group</p></a></li>
<li><a href='#moment'><p>Statistical Moment</p></a></li>
<li><a href='#naiveBayes'><p>Naive Bayes Classifier</p></a></li>
<li><a href='#permutations'><p>All Permutations of Integers 1:n</p></a></li>
<li><a href='#plot.stft'><p>Plot Short Time Fourier Transforms</p></a></li>
<li><a href='#plot.svm'><p>Plot SVM Objects</p></a></li>
<li><a href='#plot.tune'><p>Plot Tuning Object</p></a></li>
<li><a href='#predict.svm'><p>Predict Method for Support Vector Machines</p></a></li>
<li><a href='#probplot'><p>Probability Plot</p></a></li>
<li><a href='#rbridge'><p>Simulation of Brownian Bridge</p></a></li>
<li><a href='#read.matrix.csr'><p>Read/Write Sparse Data</p></a></li>
<li><a href='#rectangle.window'><p>Computes the Coefficients of a Rectangle Window.</p></a></li>
<li><a href='#rwiener'><p>Simulation of Wiener Process</p></a></li>
<li><a href='#scale_data_frame'><p>Scaling and Centering of Data Frames</p></a></li>
<li><a href='#sigmoid'><p>The Logistic Function and Derivatives</p></a></li>
<li><a href='#skewness'><p>Skewness</p></a></li>
<li><a href='#stft'><p>Computes the Short Time Fourier Transform of a Vector</p></a></li>
<li><a href='#svm'><p>Support Vector Machines</p></a></li>
<li><a href='#tune'><p>Parameter Tuning of Functions Using Grid Search</p></a></li>
<li><a href='#tune.control'><p>Control Parameters for the Tune Function</p></a></li>
<li><a href='#tune.wrapper'><p>Convenience Tuning Wrapper Functions</p></a></li>
<li><a href='#write.svm'><p>Write SVM Object to File</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.7-14</td>
</tr>
<tr>
<td>Title:</td>
<td>Misc Functions of the Department of Statistics, Probability
Theory Group (Formerly: E1071), TU Wien</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, grDevices, class, stats, methods, utils, proxy</td>
</tr>
<tr>
<td>Suggests:</td>
<td>cluster, mlbench, nnet, randomForest, rpart, SparseM, xtable,
Matrix, MASS, slam</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions for latent class analysis, short time Fourier
	     transform, fuzzy clustering, support vector machines,
	     shortest path computation, bagged clustering, naive Bayes
	     classifier, generalized k-nearest neighbour ...</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-05 17:29:44 UTC; meyer</td>
</tr>
<tr>
<td>Author:</td>
<td>David Meyer <a href="https://orcid.org/0000-0002-5196-3048"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Evgenia Dimitriadou [aut, cph],
  Kurt Hornik <a href="https://orcid.org/0000-0003-4198-9911"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Andreas Weingessel [aut],
  Friedrich Leisch [aut],
  Chih-Chung Chang [ctb, cph] (libsvm C++-code),
  Chih-Chen Lin [ctb, cph] (libsvm C++-code)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Meyer &lt;David.Meyer@R-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-06 09:22:29 UTC</td>
</tr>
<tr>
<td>Built:</td>
<td>R 4.4.0; x86_64-pc-linux-gnu; 2024-01-02 07:21:47 UTC; unix</td>
</tr>
</table>
<hr>
<h2 id='allShortestPaths'>Find Shortest Paths Between All Nodes in a Directed Graph</h2><span id='topic+allShortestPaths'></span><span id='topic+extractPath'></span>

<h3>Description</h3>

<p><code>allShortestPaths</code> finds all shortest paths in a directed (or
undirected) graph using Floyd's algorithm. <code>extractPath</code> can be
used to actually extract the path between a given pair of nodes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>allShortestPaths(x)
extractPath(obj, start, end)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="allShortestPaths_+3A_x">x</code></td>
<td>
<p>matrix or distance object</p>
</td></tr>
<tr><td><code id="allShortestPaths_+3A_obj">obj</code></td>
<td>
<p>return value of <code>allShortestPaths</code></p>
</td></tr>
<tr><td><code id="allShortestPaths_+3A_start">start</code></td>
<td>
<p>integer, starting point of path</p>
</td></tr>
<tr><td><code id="allShortestPaths_+3A_end">end</code></td>
<td>
<p>integer, end point of path</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> is a matrix, then <code>x[i,j]</code> has to be the length of the
direct path from point <code>i</code> to point <code>j</code>. If no direct
connection from point <code>i</code> to point <code>j</code> exist, then
<code>x[i,j]</code> should be either <code>NA</code> or <code>Inf</code>. Note that the
graph can be directed, hence <code>x[i,j]</code> need not be the same as
<code>x[j,i]</code>. The main diagonal of <code>x</code> is ignored.
Alternatively, <code>x</code> can be a distance object as returned by
<code><a href="stats.html#topic+dist">dist</a></code> (corresponding to an undirected graph).
</p>


<h3>Value</h3>

<p><code>allShortestPaths</code> returns a list with components
</p>
<table>
<tr><td><code>length</code></td>
<td>
<p>A matrix with the total lengths of the shortest
path between each pair of points.</p>
</td></tr>
<tr><td><code>middlePoints</code></td>
<td>
<p>A matrix giving a point in the middle of each
shortest path (or 0 if the direct connection is the shortest path),
this is mainly used as input for <code>extractPath</code>.</p>
</td></tr>
</table>
<p><code>extractPath</code> returns a vector of node numbers giving with the
shortest path between two points.
</p>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>References</h3>

<p>Kumar, V., Grama, A., Gupta, A. and Karypis, G. Introduction
to Parallel Programming - Design and Analysis of
Algorithms, Benjamin Cummings Publishing, 1994, ISBN 0-8053-3170-0</p>


<h3>Examples</h3>

<pre><code class='language-R'>## build a graph with 5 nodes
x &lt;- matrix(NA, 5, 5)
diag(x) &lt;- 0
x[1,2] &lt;- 30; x[1,3] &lt;- 10
x[2,4] &lt;- 70; x[2,5] &lt;- 40
x[3,4] &lt;- 50; x[3,5] &lt;- 20
x[4,5] &lt;- 60
x[5,4] &lt;- 10
print(x)

## compute all path lengths
z &lt;- allShortestPaths(x)
print(z)

## the following should give 1 -&gt; 3 -&gt; 5 -&gt; 4
extractPath(z, 1, 4)
</code></pre>

<hr>
<h2 id='bclust'>Bagged Clustering</h2><span id='topic+bclust'></span><span id='topic+hclust.bclust'></span><span id='topic+plot.bclust'></span><span id='topic+centers.bclust'></span><span id='topic+clusters.bclust'></span>

<h3>Description</h3>

<p>Cluster the data in <code>x</code> using the bagged clustering
algorithm. A partitioning cluster algorithm such as
<code><a href="stats.html#topic+kmeans">kmeans</a></code> is run repeatedly on bootstrap samples from the
original data. The resulting cluster centers are then combined using
the hierarchical cluster algorithm <code><a href="stats.html#topic+hclust">hclust</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bclust(x, centers=2, iter.base=10, minsize=0,
       dist.method="euclidean",
       hclust.method="average", base.method="kmeans",
       base.centers=20, verbose=TRUE,
       final.kmeans=FALSE, docmdscale=FALSE,
       resample=TRUE, weights=NULL, maxcluster=base.centers, ...)
hclust.bclust(object, x, centers, dist.method=object$dist.method,
              hclust.method=object$hclust.method, final.kmeans=FALSE,
              docmdscale = FALSE, maxcluster=object$maxcluster)
## S3 method for class 'bclust'
plot(x, maxcluster=x$maxcluster, main, ...)
centers.bclust(object, k)
clusters.bclust(object, k, x=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bclust_+3A_x">x</code></td>
<td>
<p>Matrix of inputs (or object of class <code>"bclust"</code> for plot).</p>
</td></tr>
<tr><td><code id="bclust_+3A_centers">centers</code>, <code id="bclust_+3A_k">k</code></td>
<td>
<p>Number of clusters.</p>
</td></tr>
<tr><td><code id="bclust_+3A_iter.base">iter.base</code></td>
<td>
<p>Number of runs of the base cluster algorithm.</p>
</td></tr>
<tr><td><code id="bclust_+3A_minsize">minsize</code></td>
<td>
<p>Minimum number of points in a base cluster.</p>
</td></tr>
<tr><td><code id="bclust_+3A_dist.method">dist.method</code></td>
<td>
<p>Distance method used for the hierarchical
clustering, see <code><a href="stats.html#topic+dist">dist</a></code> for available distances.</p>
</td></tr>
<tr><td><code id="bclust_+3A_hclust.method">hclust.method</code></td>
<td>
<p>Linkage method used for the hierarchical
clustering, see <code><a href="stats.html#topic+hclust">hclust</a></code> for available methods.</p>
</td></tr>
<tr><td><code id="bclust_+3A_base.method">base.method</code></td>
<td>
<p>Partitioning cluster method used as base algorithm.</p>
</td></tr>
<tr><td><code id="bclust_+3A_base.centers">base.centers</code></td>
<td>
<p>Number of centers used in each repetition of the
base method.</p>
</td></tr>
<tr><td><code id="bclust_+3A_verbose">verbose</code></td>
<td>
<p>Output status messages.</p>
</td></tr>
<tr><td><code id="bclust_+3A_final.kmeans">final.kmeans</code></td>
<td>
<p>If <code>TRUE</code>, a final kmeans step is performed
using the output of the bagged clustering as initialization.</p>
</td></tr>
<tr><td><code id="bclust_+3A_docmdscale">docmdscale</code></td>
<td>
<p>Logical, if <code>TRUE</code> a <code><a href="stats.html#topic+cmdscale">cmdscale</a></code>
result is included in the return value.</p>
</td></tr>
<tr><td><code id="bclust_+3A_resample">resample</code></td>
<td>
<p>Logical, if <code>TRUE</code> the base method is run on
bootstrap samples of <code>x</code>, else directly on <code>x</code>.</p>
</td></tr>
<tr><td><code id="bclust_+3A_weights">weights</code></td>
<td>
<p>Vector of length <code>nrow(x)</code>, weights for the
resampling. By default all observations have equal weight.</p>
</td></tr>
<tr><td><code id="bclust_+3A_maxcluster">maxcluster</code></td>
<td>
<p>Maximum number of clusters memberships are to be
computed for.</p>
</td></tr>
<tr><td><code id="bclust_+3A_object">object</code></td>
<td>
<p>Object of class <code>"bclust"</code>.</p>
</td></tr>
<tr><td><code id="bclust_+3A_main">main</code></td>
<td>
<p>Main title of the plot.</p>
</td></tr>
<tr><td><code id="bclust_+3A_...">...</code></td>
<td>
<p>Optional arguments top be passed to the base method
in <code>bclust</code>, ignored in <code>plot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>First, <code>iter.base</code> bootstrap samples of the original data in
<code>x</code> are created by drawing with replacement. The base cluster
method is run on each of these samples with <code>base.centers</code>
centers. The <code>base.method</code> must be the name of a partitioning
cluster function returning a list with the same components as the
return value of <code><a href="stats.html#topic+kmeans">kmeans</a></code>.
</p>
<p>This results in a collection of <code>iter.base *
	base.centers</code> centers, which are subsequently clustered using
the hierarchical method <code><a href="stats.html#topic+hclust">hclust</a></code>. Base centers with less
than <code>minsize</code> points in there respective partitions are removed
before the hierarchical clustering.
</p>
<p>The resulting dendrogram is then cut to produce <code>centers</code>
clusters. Hence, the name of the argument <code>centers</code> is a little
bit misleading as the resulting clusters need not be convex, e.g.,
when single linkage is used. The name was chosen for compatibility 
with standard partitioning cluster methods such as
<code><a href="stats.html#topic+kmeans">kmeans</a></code>.
</p>
<p>A new hierarchical clustering (e.g., using another
<code>hclust.method</code>) re-using previous base runs can be
performed by running <code>hclust.bclust</code> on the return value of
<code>bclust</code>. 
</p>


<h3>Value</h3>

<p><code>bclust</code> and <code>hclust.bclust</code> return objects of class
<code>"bclust"</code> including the components 
</p>
<table>
<tr><td><code>hclust</code></td>
<td>
<p>Return value of the hierarchical clustering of the
collection of base centers (Object of class <code>"hclust"</code>).</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>Vector with indices of the clusters the inputs are
assigned to.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>Matrix of centers of the final clusters. Only useful,
if the hierarchical clustering method produces convex clusters.</p>
</td></tr>
<tr><td><code>allcenters</code></td>
<td>
<p>Matrix of all <code>iter.base * base.centers</code>
centers found in the base runs.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>References</h3>

<p>Friedrich Leisch. Bagged clustering. Working Paper 51, SFB &ldquo;Adaptive
Information Systems and Modeling in Economics and Management
Science&rdquo;, August 1999. <a href="https://epub.wu.ac.at/1272/1/document.pdf">https://epub.wu.ac.at/1272/1/document.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="stats.html#topic+kmeans">kmeans</a></code>,
<code><a href="e1071.html#topic+boxplot.bclust">boxplot.bclust</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
bc1 &lt;- bclust(iris[,1:4], 3, base.centers=5)
plot(bc1)

table(clusters.bclust(bc1, 3))
centers.bclust(bc1, 3)
</code></pre>

<hr>
<h2 id='bincombinations'>Binary Combinations</h2><span id='topic+bincombinations'></span>

<h3>Description</h3>

<p>Returns a matrix containing the <code class="reqn">2^p</code> vectors of length <code>p</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bincombinations(p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bincombinations_+3A_p">p</code></td>
<td>
<p>Length of binary vectors</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>Examples</h3>

<pre><code class='language-R'>bincombinations(2)
bincombinations(3)
</code></pre>

<hr>
<h2 id='bootstrap.lca'>Bootstrap Samples of LCA Results</h2><span id='topic+bootstrap.lca'></span><span id='topic+print.bootstrap.lca'></span>

<h3>Description</h3>

<p>This function draws bootstrap samples from a given LCA model and refits
a new LCA model for each sample. The quality of fit of these models is
compared to the original model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap.lca(l, nsamples=10, lcaiter=30, verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrap.lca_+3A_l">l</code></td>
<td>
<p>An LCA model as created by <code><a href="e1071.html#topic+lca">lca</a></code></p>
</td></tr>
<tr><td><code id="bootstrap.lca_+3A_nsamples">nsamples</code></td>
<td>
<p>Number of bootstrap samples</p>
</td></tr>
<tr><td><code id="bootstrap.lca_+3A_lcaiter">lcaiter</code></td>
<td>
<p>Number of LCA iterations</p>
</td></tr>
<tr><td><code id="bootstrap.lca_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code> some output is printed during the
computations.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>From a given LCA model <code>l</code>, <code>nsamples</code> bootstrap samples are
drawn. For each sample a new LCA model is fitted. The goodness of fit
for each model is computed via Likelihood Ratio and Pearson's
Chisquare. The values for the fitted models are compared with the values
of the original model <code>l</code>. By this method it can be tested whether
the data to which <code>l</code> was originally fitted come from an LCA model.
</p>


<h3>Value</h3>

<p>An object of class <code>bootstrap.lca</code> is returned, containing
</p>
<table>
<tr><td><code>logl</code>, <code>loglsat</code></td>
<td>
<p>The LogLikelihood of the models and of the
corresponding saturated models</p>
</td></tr>
<tr><td><code>lratio</code></td>
<td>
<p>Likelihood quotient of the models and the corresponding
saturated models</p>
</td></tr>
<tr><td><code>lratiomean</code>, <code>lratiosd</code></td>
<td>
<p>Mean and Standard deviation of
<code>lratio</code></p>
</td></tr>
<tr><td><code>lratioorg</code></td>
<td>
<p>Likelihood quotient of the original model and the
corresponding saturated model</p>
</td></tr>
<tr><td><code>zratio</code></td>
<td>
<p>Z-Statistics of <code>lratioorg</code></p>
</td></tr>
<tr><td><code>pvalzratio</code>, <code>pvalratio</code></td>
<td>
<p>P-Values for <code>zratio</code>, computed via normal
distribution and empirical distribution</p>
</td></tr>
<tr><td><code>chisq</code></td>
<td>
<p>Pearson's Chisq of the models</p>
</td></tr>
<tr><td><code>chisqmean</code>, <code>chisqsd</code></td>
<td>
<p>Mean and Standard deviation of
<code>chisq</code></p>
</td></tr>
<tr><td><code>chisqorg</code></td>
<td>
<p>Pearson's Chisq of the original model</p>
</td></tr>
<tr><td><code>zchisq</code></td>
<td>
<p>Z-Statistics of <code>chisqorg</code></p>
</td></tr>
<tr><td><code>pvalzchisq</code>, <code>pvalchisq</code></td>
<td>
<p>P-Values for <code>zchisq</code>, computed via normal
distribution and empirical distribution</p>
</td></tr>
<tr><td><code>nsamples</code></td>
<td>
<p>Number of bootstrap samples</p>
</td></tr>
<tr><td><code>lcaiter</code></td>
<td>
<p>Number of LCA Iterations</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andreas Weingessel</p>


<h3>References</h3>

<p>Anton K. Formann: &ldquo;Die Latent-Class-Analysis&rdquo;, Beltz
Verlag 1984</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+lca">lca</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a 4-dim. sample with 2 latent classes of 500 data points each.
## The probabilities for the 2 classes are given by type1 and type2.
type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x &lt;- matrix(runif(4000), nrow = 1000)
x[1:500,] &lt;- t(t(x[1:500,]) &lt; type1) * 1
x[501:1000,] &lt;- t(t(x[501:1000,]) &lt; type2) * 1

l &lt;- lca(x, 2, niter=5)
bl &lt;- bootstrap.lca(l,nsamples=3,lcaiter=5)
bl
</code></pre>

<hr>
<h2 id='boxplot.bclust'>Boxplot of Cluster Profiles</h2><span id='topic+boxplot.bclust'></span>

<h3>Description</h3>

<p>Makes boxplots of the results of a bagged clustering run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bclust'
boxplot(x, n=nrow(x$centers), bycluster=TRUE,
        main=deparse(substitute(x)), oneplot=TRUE,
        which=1:n, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boxplot.bclust_+3A_x">x</code></td>
<td>
<p>Clustering result, object of class <code>"bclust"</code>.</p>
</td></tr>
<tr><td><code id="boxplot.bclust_+3A_n">n</code></td>
<td>
<p>Number of clusters to plot, by default the number of clusters
used in the call of <code><a href="e1071.html#topic+bclust">bclust</a></code>.</p>
</td></tr>
<tr><td><code id="boxplot.bclust_+3A_bycluster">bycluster</code></td>
<td>
<p>If <code>TRUE</code> (default), a boxplot for each cluster
is plotted. If <code>FALSE</code>, a boxplot for each variable is plotted.</p>
</td></tr>
<tr><td><code id="boxplot.bclust_+3A_main">main</code></td>
<td>
<p>Main title of the plot, by default the name of the cluster
object.</p>
</td></tr> 
<tr><td><code id="boxplot.bclust_+3A_oneplot">oneplot</code></td>
<td>
<p>If <code>TRUE</code>, all boxplots appear on one screen (using
an appropriate rectangular layout).</p>
</td></tr>
<tr><td><code id="boxplot.bclust_+3A_which">which</code></td>
<td>
<p>Number of clusters which should be plotted, default is all
clusters.</p>
</td></tr>
<tr><td><code id="boxplot.bclust_+3A_...">...</code></td>
<td>
<p>Additional arguments for <code><a href="graphics.html#topic+boxplot">boxplot</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
bc1 &lt;- bclust(iris[,1:4], 3, base.centers=5)
boxplot(bc1)
</code></pre>

<hr>
<h2 id='classAgreement'>Coefficients Comparing Classification Agreement</h2><span id='topic+classAgreement'></span>

<h3>Description</h3>

<p><code>classAgreement()</code> computes several coefficients of agreement
between the columns and rows of a 2-way contingency table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classAgreement(tab, match.names=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classAgreement_+3A_tab">tab</code></td>
<td>
<p>A 2-dimensional contingency table.</p>
</td></tr>
<tr><td><code id="classAgreement_+3A_match.names">match.names</code></td>
<td>
<p>Flag whether row and columns should be matched by name.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose we want to compare two classifications summarized by the
contingency table <code class="reqn">T=[t_{ij}]</code> where <code class="reqn">i,j=1,\ldots,K</code> and <code class="reqn">t_{ij}</code>
denotes the number of data points which are in class <code class="reqn">i</code> in the
first partition and in class <code class="reqn">j</code> in the second partition. If both
classifications use the same labels, then obviously the two
classification agree completely if only elements in the main diagonal
of the table are non-zero. On the other hand, large off-diagonal
elements correspond to smaller agreement between the two
classifications. If <code>match.names</code> is <code>TRUE</code>, the class labels
as given by the row and column names are matched, i.e. only columns and
rows with the same dimnames are used for the computation.
</p>
<p>If the two classification do not use the same set of labels, or if
identical labels can have different meaning (e.g., two outcomes of
cluster analysis on the same data set), then the situation is a little
bit more complicated. Let <code class="reqn">A</code> denote the number of all pairs of data
points which are either put into the same cluster by both partitions or
put into different clusters by both partitions. Conversely, let <code class="reqn">D</code>
denote the number of all pairs of data points that are put into one
cluster in one partition, but into different clusters by the other
partition.  Hence, the partitions disagree for all pairs <code class="reqn">D</code> and
agree for all pairs <code class="reqn">A</code>. We can measure the agreement by the Rand
index <code class="reqn">A/(A+D)</code> which is invariant with respect to permutations of
the columns or rows of <code class="reqn">T</code>.
</p>
<p>Both indices have to be corrected for agreement by chance if the sizes
of the classes are not uniform.
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>diag</code></td>
<td>
<p>Percentage of data points in the main diagonal of <code>tab</code>.</p>
</td></tr>
<tr><td><code>kappa</code></td>
<td>
<p><code>diag</code> corrected for agreement by chance.</p>
</td></tr>
<tr><td><code>rand</code></td>
<td>
<p>Rand index.</p>
</td></tr>
<tr><td><code>crand</code></td>
<td>
<p>Rand index corrected for agreement by chance.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>References</h3>

<p>J.~Cohen. A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 20, 37&ndash;46, 1960.
</p>
<p>Lawrence Hubert and Phipps Arabie. Comparing partitions.
Journal of Classification, 2, 193&ndash;218, 1985.
</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+matchClasses">matchClasses</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## no class correlations: both kappa and crand almost zero
g1 &lt;- sample(1:5, size=1000, replace=TRUE)
g2 &lt;- sample(1:5, size=1000, replace=TRUE)
tab &lt;- table(g1, g2)
classAgreement(tab)

## let pairs (g1=1,g2=1) and (g1=3,g2=3) agree better
k &lt;- sample(1:1000, size=200)
g1[k] &lt;- 1
g2[k] &lt;- 1

k &lt;- sample(1:1000, size=200)
g1[k] &lt;- 3
g2[k] &lt;- 3

tab &lt;- table(g1, g2)
## both kappa and crand should be significantly larger than before
classAgreement(tab)
</code></pre>

<hr>
<h2 id='cmeans'>Fuzzy C-Means Clustering</h2><span id='topic+cmeans'></span><span id='topic+print.fclust'></span>

<h3>Description</h3>

<p>The fuzzy version of the known <em>k</em>means clustering algorithm as
well as an on-line variant (Unsupervised Fuzzy Competitive learning).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cmeans(x, centers, iter.max = 100, verbose = FALSE,
       dist = "euclidean", method = "cmeans", m = 2,
       rate.par = NULL, weights = 1, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cmeans_+3A_x">x</code></td>
<td>
<p>The data matrix where columns correspond to variables and
rows to observations.</p>
</td></tr> 
<tr><td><code id="cmeans_+3A_centers">centers</code></td>
<td>
<p>Number of clusters or initial values for cluster
centers.</p>
</td></tr>
<tr><td><code id="cmeans_+3A_iter.max">iter.max</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="cmeans_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, make some output during learning.</p>
</td></tr>
<tr><td><code id="cmeans_+3A_dist">dist</code></td>
<td>
<p>Must be one of the following: If <code>"euclidean"</code>, the
mean square error, if <code>"manhattan"</code>, the mean absolute error is
computed.  Abbreviations are also accepted.</p>
</td></tr>
<tr><td><code id="cmeans_+3A_method">method</code></td>
<td>
<p>If <code>"cmeans"</code>, then we have the <code class="reqn">c</code>-means fuzzy
clustering method, if <code>"ufcl"</code> we have the on-line update.
Abbreviations are also accepted.</p>
</td></tr>
<tr><td><code id="cmeans_+3A_m">m</code></td>
<td>
<p>A number greater than 1 giving the degree of fuzzification.</p>
</td></tr>
<tr><td><code id="cmeans_+3A_rate.par">rate.par</code></td>
<td>
<p>A number between 0 and 1 giving the parameter of the
learning rate for the on-line variant.  The default corresponds to
<code class="reqn">0.3</code>.</p>
</td></tr>
<tr><td><code id="cmeans_+3A_weights">weights</code></td>
<td>
<p>a numeric vector with non-negative case weights.
Recycled to the number of observations in <code>x</code> if necessary.</p>
</td></tr>
<tr><td><code id="cmeans_+3A_control">control</code></td>
<td>
<p>a list of control parameters.  See <b>Details</b>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data given by <code>x</code> is clustered by generalized versions of the
fuzzy <em>c</em>-means algorithm, which use either a fixed-point or an
on-line heuristic for minimizing the objective function
</p>
<p style="text-align: center;"><code class="reqn">\sum_i \sum_j w_i u_{ij}^m d_{ij},</code>
</p>

<p>where <code class="reqn">w_i</code> is the weight of observation <code class="reqn">i</code>, <code class="reqn">u_{ij}</code> is
the membership of observation <code class="reqn">i</code> in cluster <code class="reqn">j</code>, and
<code class="reqn">d_{ij}</code> is the distance (dissimilarity) between observation
<code class="reqn">i</code> and center <code class="reqn">j</code>.  The dissimilarities used are the sums of
squares (<code>"euclidean"</code>) or absolute values (<code>"manhattan"</code>)
of the element-wise differences.
</p>
<p>If <code>centers</code> is a matrix, its rows are taken as the initial cluster
centers.  If <code>centers</code> is an integer, <code>centers</code> rows of
<code>x</code> are randomly chosen as initial values.
</p>
<p>The algorithm stops when the maximum number of iterations (given by
<code>iter.max</code>) is reached, or when the algorithm is unable to reduce
the current value <code>val</code> of the objective function by
<code>reltol * (abs(val) * reltol)</code> at a step.  The relative
convergence tolerance <code>reltol</code> can be specified as the
<code>reltol</code> component of the list of control parameters, and
defaults to <code>sqrt(.Machine$double.eps)</code>.
</p>
<p>If <code>verbose</code> is <code>TRUE</code>, each iteration displays its number
and the value of the objective function.
</p>
<p>If <code>method</code> is <code>"cmeans"</code>, then we have the <code class="reqn">c</code>-means
fuzzy clustering method, see for example Bezdek (1981).  If
<code>"ufcl"</code>, we have the On-line Update (Unsupervised Fuzzy
Competitive Learning) method due to Chung and Lee (1992), see also Pal
et al (1996).  This method works by performing an update directly
after each input signal (i.e., for each single observation).
</p>
<p>The parameters <code>m</code> defines the degree of fuzzification.  It is
defined for real values greater than 1 and the bigger it is the more
fuzzy the membership values of the clustered data points are.
</p>


<h3>Value</h3>

<p>An object of class <code>"fclust"</code> which is a list with components:
</p>
<table>
<tr><td><code>centers</code></td>
<td>
<p>the final cluster centers.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>the number of data points in each cluster of the closest
hard clustering.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a vector of integers containing the indices of the
clusters where the data points are assigned to for the closest hard
clustering, as obtained by assigning points to the (first) class with
maximal membership.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>the number of iterations performed.</p>
</td></tr>
<tr><td><code>membership</code></td>
<td>
<p>a matrix with the membership values of the data points
to the clusters.</p>
</td></tr>
<tr><td><code>withinerror</code></td>
<td>
<p>the value of the objective function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call used to create the object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Evgenia Dimitriadou and Kurt Hornik
</p>


<h3>References</h3>

<p>J. C. Bezdek (1981).
<em>Pattern recognition with fuzzy objective function algorithms</em>.
New York: Plenum.
</p>
<p>Fu Lai Chung and Tong Lee (1992).
Fuzzy competitive learning.
<em>Neural Networks</em>, <b>7</b>(3), 539&ndash;551.
</p>
<p>Nikhil R. Pal, James C. Bezdek, and Richard J. Hathaway (1996).
Sequential competitive learning and the fuzzy c-means clustering
algorithms.
<em>Neural Networks</em>, <b>9</b>(5), 787&ndash;796.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># a 2-dimensional example
x&lt;-rbind(matrix(rnorm(100,sd=0.3),ncol=2),
         matrix(rnorm(100,mean=1,sd=0.3),ncol=2))
cl&lt;-cmeans(x,2,20,verbose=TRUE,method="cmeans",m=2)
print(cl)

# a 3-dimensional example
x&lt;-rbind(matrix(rnorm(150,sd=0.3),ncol=3),
         matrix(rnorm(150,mean=1,sd=0.3),ncol=3),
         matrix(rnorm(150,mean=2,sd=0.3),ncol=3))
cl&lt;-cmeans(x,6,20,verbose=TRUE,method="cmeans")
print(cl)
</code></pre>

<hr>
<h2 id='countpattern'>Count Binary Patterns</h2><span id='topic+countpattern'></span>

<h3>Description</h3>

<p>Every row of the binary matrix <code>x</code> is transformed into a binary
pattern and these patterns are counted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>countpattern(x, matching=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="countpattern_+3A_x">x</code></td>
<td>
<p>A matrix of binary observations</p>
</td></tr>
<tr><td><code id="countpattern_+3A_matching">matching</code></td>
<td>
<p>If TRUE an additional vector is returned which stores
which row belongs to which pattern</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of length <code>2\^ncol(x)</code> giving the number of times each
pattern occurs in the rows of <code>x</code>. The names of this vector are the
binary patterns. They are sorted according to their numeric value.
If <code>matching</code> is <code>TRUE</code>, a list of the following two vectors is returned.
</p>
<table>
<tr><td><code>pat</code></td>
<td>
<p>Numbers of patterns as described above.</p>
</td></tr>
<tr><td><code>matching</code></td>
<td>
<p>Vector giving the position of the pattern of each row
of <code>x</code> in <code>pat</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andreas Weingessel</p>


<h3>Examples</h3>

<pre><code class='language-R'>xx &lt;- rbind(c(1,0,0),c(1,0,0),c(1,0,1),c(0,1,1),c(0,1,1))
countpattern(xx)
countpattern(xx, matching=TRUE)
</code></pre>

<hr>
<h2 id='cshell'>Fuzzy C-Shell Clustering</h2><span id='topic+cshell'></span>

<h3>Description</h3>

<p>The <em>c</em>-shell clustering algorithm, the shell prototype-based version
(ring prototypes) of the fuzzy <em>k</em>means clustering method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cshell(x, centers, iter.max=100, verbose=FALSE, dist="euclidean",
       method="cshell", m=2, radius = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cshell_+3A_x">x</code></td>
<td>
<p>The data matrix, were columns correspond to the variables and
rows to observations.</p>
</td></tr>
<tr><td><code id="cshell_+3A_centers">centers</code></td>
<td>
<p>Number of clusters or initial values for cluster centers</p>
</td></tr>
<tr><td><code id="cshell_+3A_iter.max">iter.max</code></td>
<td>
<p>Maximum number of iterations</p>
</td></tr>
<tr><td><code id="cshell_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, make some output during learning</p>
</td></tr>
<tr><td><code id="cshell_+3A_dist">dist</code></td>
<td>
<p>Must be one of the following: If <code>"euclidean"</code>, the
mean square error, if <code>"manhattan"</code>, the mean absolute error is
computed. Abbreviations are also accepted.</p>
</td></tr>
<tr><td><code id="cshell_+3A_method">method</code></td>
<td>
<p>Currently, only the <code>"cshell"</code> method; the c-shell fuzzy
clustering method</p>
</td></tr>
<tr><td><code id="cshell_+3A_m">m</code></td>
<td>
<p>The degree of fuzzification. It is defined for values greater
than <em>1</em></p>
</td></tr>
<tr><td><code id="cshell_+3A_radius">radius</code></td>
<td>
<p>The radius of resulting clusters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data given by <code>x</code> is clustered by the fuzzy <em>c</em>-shell algorithm.
</p>
<p>If <code>centers</code> is a matrix, its rows are taken as the initial cluster
centers. If <code>centers</code> is an integer, <code>centers</code> rows
of <code>x</code> are randomly chosen as initial values.
</p>
<p>The algorithm stops when the maximum number of iterations (given by
<code>iter.max</code>) is reached.
</p>
<p>If <code>verbose</code> is <code>TRUE</code>, it displays for each iteration the number
the value of the objective function.
</p>
<p>If <code>dist</code> is <code>"euclidean"</code>, the distance between the
cluster center and the data points is the Euclidean distance (ordinary
kmeans algorithm). If <code>"manhattan"</code>, the distance between the
cluster center and the data points is the sum of the absolute values
of the distances of the coordinates.
</p>
<p>If <code>method</code> is <code>"cshell"</code>, then we have the <em>c</em>-shell
fuzzy clustering method.
</p>
<p>The parameters <code>m</code> defines the degree of fuzzification. It is
defined for real values greater than 1 and the bigger it is the more
fuzzy the membership values of the clustered data points are.
</p>
<p>The parameter <code>radius</code> is by default set to <em>0.2</em> for every
cluster.
</p>


<h3>Value</h3>

<p><code>cshell</code> returns an object of class <code>"cshell"</code>.
</p>
<table>
<tr><td><code>centers</code></td>
<td>
<p>The final cluster centers.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of data points in each cluster.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>Vector containing the indices of the clusters where
the data points are assigned to. The maximum membership value of a
point is considered for partitioning it to a cluster.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of iterations performed.</p>
</td></tr>
<tr><td><code>membership</code></td>
<td>
<p>a matrix with the membership values of the data points
to the clusters.</p>
</td></tr>
<tr><td><code>withinerror</code></td>
<td>
<p>Returns the sum of square distances within the
clusters.</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>Returns a call in which all of the arguments are
specified by their names.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Evgenia Dimitriadou</p>


<h3>References</h3>

<p>Rajesh N. Dave. <em>Fuzzy Shell-Clustering and Applications to Circle
Detection in Digital Images.</em> Int. J. of General Systems, Vol. <b>16</b>,
pp. 343-355, 1996.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## a 2-dimensional example
x &lt;- rbind(matrix(rnorm(50, sd = 0.3), ncol = 2),
           matrix(rnorm(50, mean = 1, sd=0.3), ncol = 2))
cl &lt;- cshell(x, 2, 20, verbose = TRUE, method = "cshell", m = 2)
print(cl)
</code></pre>

<hr>
<h2 id='Discrete'>Discrete Distribution</h2><span id='topic+ddiscrete'></span><span id='topic+pdiscrete'></span><span id='topic+qdiscrete'></span><span id='topic+rdiscrete'></span>

<h3>Description</h3>

<p>These functions provide information about the discrete distribution
where the probability of the elements of <code>values</code> is proportional
to the values given in <code>probs</code>, which are normalized to sum up to
1.  <code>ddiscrete</code> gives the density, <code>pdiscrete</code> gives the
distribution function, <code>qdiscrete</code> gives the quantile function
and <code>rdiscrete</code> generates random deviates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddiscrete(x, probs, values = 1:length(probs))
pdiscrete(q, probs, values = 1:length(probs))
qdiscrete(p, probs, values = 1:length(probs))
rdiscrete(n, probs, values = 1:length(probs), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Discrete_+3A_x">x</code>, <code id="Discrete_+3A_q">q</code></td>
<td>
<p>vector or array of quantiles.</p>
</td></tr>
<tr><td><code id="Discrete_+3A_p">p</code></td>
<td>
<p>vector or array of probabilities.</p>
</td></tr>
<tr><td><code id="Discrete_+3A_n">n</code></td>
<td>
<p>number of observations.</p>
</td></tr>
<tr><td><code id="Discrete_+3A_probs">probs</code></td>
<td>
<p>probabilities of the distribution.</p>
</td></tr>
<tr><td><code id="Discrete_+3A_values">values</code></td>
<td>
<p>values of the distribution.</p>
</td></tr>
<tr><td><code id="Discrete_+3A_...">...</code></td>
<td>
<p>ignored (only there for backwards compatibility)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The random number generator is simply a wrapper for
<code><a href="base.html#topic+sample">sample</a></code> and provided for backwards compatibility only.
</p>


<h3>Author(s)</h3>

<p>Andreas Weingessel and Friedrich Leisch</p>


<h3>Examples</h3>

<pre><code class='language-R'>## a vector of length 30 whose elements are 1 with probability 0.2
## and 2 with probability 0.8.
rdiscrete (30, c(0.2, 0.8))

## a vector of length 100 whose elements are A, B, C, D.
## The probabilities of the four values have the relation 1:2:3:3
rdiscrete (100, c(1,2,3,3), c("A","B","C","D"))

</code></pre>

<hr>
<h2 id='e1071-deprecated'>Deprecated Functions in Package e1071</h2><span id='topic+e1071-deprecated'></span>

<h3>Description</h3>

<p>These functions are provided for compatibility with older versions of
package <span class="pkg">e1071</span> only, and may be defunct as soon as of the next
release.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+Deprecated">Deprecated</a></code>
</p>

<hr>
<h2 id='element'>Extract Elements of an Array</h2><span id='topic+element'></span>

<h3>Description</h3>

<p>Returns the element of <code>x</code> specified by <code>i</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>element(x, i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="element_+3A_x">x</code></td>
<td>
<p>Array of arbitrary dimensionality.</p>
</td></tr>
<tr><td><code id="element_+3A_i">i</code></td>
<td>
<p>Vector of the same length as <code>x</code> has dimension.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>See Also</h3>

<p><a href="base.html#topic+Extract">Extract</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- array(1:20, dim=c(2,5,2))
element(x, c(1,4,2))
</code></pre>

<hr>
<h2 id='fclustIndex'>Fuzzy Cluster Indexes (Validity/Performance Measures)</h2><span id='topic+fclustIndex'></span>

<h3>Description</h3>

<p>Calculates the values of several fuzzy validity measures. The values
of the indexes can be independently used in order to evaluate and compare
clustering partitions or even to determine the number of clusters
existing in a data set.</p>


<h3>Usage</h3>

<pre><code class='language-R'>fclustIndex(y, x, index = "all")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fclustIndex_+3A_y">y</code></td>
<td>
<p>An object of a fuzzy clustering result of class <code>"fclust"</code></p>
</td></tr>
<tr><td><code id="fclustIndex_+3A_x">x</code></td>
<td>
<p>Data matrix</p>
</td></tr>
<tr><td><code id="fclustIndex_+3A_index">index</code></td>
<td>
<p>The validity measures used: <code>"gath.geva"</code>, <code>"xie.beni"</code>,
<code>"fukuyama.sugeno"</code>, <code>"partition.coefficient"</code>,
<code>"partition.entropy"</code>, <code>"proportion.exponent"</code>,
<code>"separation.index"</code> and <code>"all"</code> for all the indexes.</p>
</td></tr></table>


<h3>Details</h3>

<p>The validity measures and a short description of them follows, where
<code class="reqn">N</code> is the number of data points, <code class="reqn">u_{ij}</code> the values of the
membership matrix, <code class="reqn">v_j</code> the centers of the clusters and <code class="reqn">k</code>
te number of clusters.
</p>

<dl>
<dt><b>gath.geva</b>:</dt><dd>
<p>Gath and Geva introduced 2 main criteria for comparing and finding
optimal partitions based on the heuristics that a better clustering
assumes clear separation between the clusters, minimal volume of the
clusters and maximal number of data points concentrated in the
vicinity of the cluster centroids. These indexes are only for the
cmeans clustering algorithm valid.
For the first, the &ldquo;fuzzy hypervolume&rdquo; we have:
<code class="reqn">F_{HV}=\sum_{j=1}^{c}{[\det(F_j)]}^{1/2}</code>, where
<code class="reqn">F_j=\frac{\sum_{i=1}^N
	  u_{ij}(x_i-v_j)(x_i-v_j)^T}{\sum_{i=1}^{N}u_{ij}}</code>, for the
case when the defuzzification parameter is 2.
For the second, the &ldquo;average partition density&rdquo;:
<code class="reqn">D_{PA}=\frac{1}{k}\sum_{j=1}^k\frac{S_j}{{[\det(F_j)]}^{1/2}}</code>,
where <code class="reqn">S_j=\sum_{i=1}^N u_{ij}</code>.
Moreover, the &ldquo;partition density&rdquo; which expresses the general
partition density according to the physical definition of density
is calculated by:
<code class="reqn">P_D=\frac{S}{F_{HV}}</code>, where <code class="reqn">S=\sum_{j=1}^k\sum_{i=1}^N
	u_{ij}</code>.
</p>
</dd>
<dt><b>xie.beni</b>:</dt><dd>
<p>This index is a function of the data set and the centroids of the
clusters. Xie and Beni explained this index by writing it as a ratio
of the total variation of the partition and the centroids $(U,V)$
and the separation of the centroids vectors. The minimum values of
this index under comparison support the best partitions.
<code class="reqn">u_{XB}(U,V;X)=\frac{\sum_{j=1}^k\sum_{i=1}^Nu_{ij}^2{||x_i-v_j||}^2}{N(\min_{j\neq l}\{{||v_j-v_l||}^2\})}</code>
</p>
</dd>
<dt><b>fukuyama.sugeno</b>:</dt><dd>
<p>This index consists of the difference of two terms, the first
combining the fuzziness in the membership matrix with the
geometrical compactness of the representation of the data set via
the prototypes, and the second the fuzziness in its row of the
partition matrix with the distance from the $i$th prototype to the
grand mean of the data. The minimum values of this index also
propose a good partition.
<code class="reqn">u_{FS}(U,V;X)=\sum_{i=1}^{N}\sum_{j=1}^k
	(u_{ij}^2)^q(||x_i-v_j||^2-||v_j-\bar v||^2)</code>
</p>
</dd>
<dt><b>partition.coefficient</b>:</dt><dd>
<p>An index which measures the fuzziness of the partition but without
considering the data set itself. It is a heuristic measure since it
has no connection to any property of the data. The maximum values of
it imply a good partition in the meaning of a least fuzzy
clustering.
<code class="reqn">F(U;k)=\frac{tr (UU^T)}{N}=\frac{&lt;U,U&gt;}{N}=\frac{||U||^2}{N}</code>
</p>

<ul>
<li> <p><code class="reqn">F(U;k)</code> shows the fuzziness or the overlap of the partition
and depends on <code class="reqn">kN</code> elements. 
</p>
</li>
<li> <p><code class="reqn">1/k\leq F(U;k)\leq 1</code>, where if <code class="reqn">F(U;k)=1</code> then <code class="reqn">U</code> is a hard
partition and if <code class="reqn">F(U;k)=1/k</code> then <code class="reqn">U=[1/k]</code> is the centroid of
the fuzzy partion space <code class="reqn">P_{fk}</code>. The converse is also valid.
</p>
</li></ul>

</dd>
<dt><b>partition.entropy</b>:</dt><dd>
<p>It is a measure that provides information about the membership
matrix without also considering the data itself. The minimum values
imply a good partition in the meaning of a more crisp partition.
<code class="reqn">H(U;k)=\sum_{i=1}^{N} h(u_i)/N</code>, where
<code class="reqn">h(u)=-\sum_{j=1}^{k} u_j\,\log _a (u_j)</code> the Shannon's entropy.
</p>

<ul>
<li> <p><code class="reqn">H(U;k)</code> shows the uncertainty of a fuzzy partition and
depends also on <code class="reqn">kN</code> elements. Specifically, <code class="reqn">h(u_i)</code> is
interpreted as the amount of fuzzy information about the
membership of <code class="reqn">x_i</code> in <code class="reqn">k</code> classes that is retained by column
<code class="reqn">u_j</code>. Thus, at <code class="reqn">U=[1/k]</code> the most information is withheld since
the membership is the fuzziest possible.
</p>
</li>
<li> <p><code class="reqn">0\leq H(U;k)\leq \log_a(k)</code>, where for <code class="reqn">H(U;k)=0</code> <code class="reqn">U</code> is a
hard partition and for <code class="reqn">H(U;k)=\log_a(k)</code> <code class="reqn">U=[1/k]</code>.
</p>
</li></ul>

</dd>
<dt><b>proportion.exponent</b>:</dt><dd>
<p>It is a measure <code class="reqn">P(U;k)</code> of fuzziness adept to detect structural variations
in the partition matrix as it becomes more fuzzier. A crisp cluster
in the partition matrix can drive it to infinity when the partition
coefficient and the partition entropy are more sensitive to small
changes when approaching a hard partition. Its evaluation does not also
involve the data or the algorithm used to partition them and
its maximum implies the optimal partition but without knowing what
maximum is a statistically significant maximum.
</p>

<ul>
<li> <p><code class="reqn">0\leq P(U;k)&lt;\infty</code>, since the <code class="reqn">[0,1]</code> values explode to
<code class="reqn">[0,\infty)</code> due to the natural logarithm. Specifically, <code class="reqn">P=0</code>
when and only when <code class="reqn">U=[1/k]</code>, while <code class="reqn">P\rightarrow\infty</code> when
any column of <code class="reqn">U</code> is crisp. 
</p>
</li>
<li> <p><code class="reqn">P(U;k)</code> can easily explode and it is good for partitions
with large column maximums and at detecting structural
variations.
</p>
</li></ul>

</dd>
<dt><b>separation.index (known as CS Index)</b>:</dt><dd>
<p>This index identifies unique cluster structure with well-defined
properties that depend on the data and a measure of distance. It
answers the question if the clusters are compact and separated, but
it rather seems computationally infeasible for big data sets since a
distance matrix between all the data membership values has to be
calculated. It also presupposes that a hard partition is derived
from the fuzzy one.<br />
<code class="reqn">D_1(U;k;X,d)=\min_{i+1\,\leq\,l\,\leq\,k-1}\left\{\min_{1\,\leq\,j\,\leq\,k}\left\{\frac{dis(u_j,u_l)}{\max_{1\leq m\leq k}\{dia(u_m)\}}\right\}\right\}</code>, where <code class="reqn">dia</code>  is the diameter of the subset, <code class="reqn">dis</code> the distance of
two subsets, and <code class="reqn">d</code> a metric.
<code class="reqn">U</code> is a CS partition of <code class="reqn">X</code> <code class="reqn">\Leftrightarrow D_1&gt;1</code>. When this
holds then <code class="reqn">U</code> is unique.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>Returns a vector with the validity measures values.
</p>


<h3>Author(s)</h3>

<p>Evgenia Dimitriadou</p>


<h3>References</h3>

<p>James C. Bezdek, <em>Pattern Recognition with Fuzzy Objective
Function Algorithms</em>, Plenum Press, 1981, NY.<br />
L. X. Xie and G. Beni, <em>Validity measure for fuzzy
clustering</em>, IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. <b>3</b>, n. 8, p. 841-847, 1991.<br />
I. Gath and A. B. Geva, <em>Unsupervised Optimal Fuzzy
Clustering</em>, IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. <b>11</b>, n. 7, p. 773-781, 1989.<br />
Y. Fukuyama and M. Sugeno, <em>A new method of choosing the
number of clusters for the fuzzy $c$-means method</em>, Proc. 5th Fuzzy
Syst. Symp., p. 247-250, 1989 (in japanese).</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+cmeans">cmeans</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># a 2-dimensional example
x&lt;-rbind(matrix(rnorm(100,sd=0.3),ncol=2),
         matrix(rnorm(100,mean=1,sd=0.3),ncol=2))
cl&lt;-cmeans(x,2,20,verbose=TRUE,method="cmeans")
resultindexes &lt;- fclustIndex(cl,x, index="all")
resultindexes   
</code></pre>

<hr>
<h2 id='gknn'>Generalized k-Nearest Neighbors Classification or Regression</h2><span id='topic+gknn'></span><span id='topic+gknn.default'></span><span id='topic+gknn.formula'></span><span id='topic+print.gknn'></span><span id='topic+predict.gknn'></span>

<h3>Description</h3>

<p><code>gknn</code> is an implementation of the k-nearest neighbours algorithm making use of general distance measures. A formula interface is provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
gknn(formula, data = NULL, ..., subset, na.action = na.pass, scale = TRUE)
## Default S3 method:
gknn(x, y, k = 1, method = NULL, 
                       scale = TRUE, use_all = TRUE, 
                       FUN = mean, ...)
## S3 method for class 'gknn'
predict(object, newdata, 
                         type = c("class", "votes", "prob"), 
                         ...,
                         na.action = na.pass)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gknn_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of the model to be fit.</p>
</td></tr>
<tr><td><code id="gknn_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
&lsquo;gknn&rsquo; is called from.</p>
</td></tr>
<tr><td><code id="gknn_+3A_x">x</code></td>
<td>
<p>a data matrix.</p>
</td></tr>
<tr><td><code id="gknn_+3A_y">y</code></td>
<td>
<p>a response vector with one label for each row/component of
<code>x</code>. Can be either a factor (for classification tasks)
or a numeric vector (for regression).</p>
</td></tr>
<tr><td><code id="gknn_+3A_k">k</code></td>
<td>
<p>number of neighbours considered.</p>
</td></tr>
<tr><td><code id="gknn_+3A_scale">scale</code></td>
<td>
<p>a logical vector indicating the variables to be
scaled. If <code>scale</code> is of length 1, the value is recycled as
many times as needed.
By default, numeric <em>matrices</em> are scaled to zero mean and unit variance. The center and scale
values are returned and used for later predictions.
Note that the default metric for data frames is the Gower metric
which <em>standardizes</em> the values to the unit interval.</p>
</td></tr>
<tr><td><code id="gknn_+3A_method">method</code></td>
<td>
<p>Argument passed to <code>dist()</code> from the <code>proxy</code> package to select the distance metric used: a function, or a mnemonic string referencing the distance measure. Defaults to <code>"Euclidean"</code> for metric matrices, to <code>"Jaccard"</code> for logical matrices and to <code>"Gower"</code> for data frames.
</p>
</td></tr>
<tr><td><code id="gknn_+3A_use_all">use_all</code></td>
<td>
<p>controls handling of ties. If true, all distances equal to the kth largest are included. If false, a random selection of distances equal to the kth is chosen to use exactly k neighbours.</p>
</td></tr>
<tr><td><code id="gknn_+3A_fun">FUN</code></td>
<td>
<p>function used to aggregate the k nearest target values in case of regression.</p>
</td></tr>
<tr><td><code id="gknn_+3A_object">object</code></td>
<td>
<p>object of class <code>gknn</code>.</p>
</td></tr>
<tr><td><code id="gknn_+3A_newdata">newdata</code></td>
<td>
<p>matrix or data frame with new instances.</p>
</td></tr>
<tr><td><code id="gknn_+3A_type">type</code></td>
<td>
<p>character specifying the return type in case of class
predictions: for <code>"class"</code>, the class labels; for <code>"prob"</code>, the class distribution for all k neighbours considered; for <code>"votes"</code>, the raw counts.</p>
</td></tr>
<tr><td><code id="gknn_+3A_...">...</code></td>
<td>
<p>additional parameters passed to <code>dist()</code></p>
</td></tr>
<tr><td><code id="gknn_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample.  (NOTE: If given, this argument must be
named.)</p>
</td></tr>
<tr><td><code id="gknn_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.pass</code>. (NOTE: If given, this argument must be named.)</p>
</td></tr>	
</table>


<h3>Value</h3>

<p>For <code>gknn()</code>, an object of class <code>"gknn"</code> containing the data and the specified parameters. For <code>predict.gknn()</code>, a vector of predictions, or a matrix with votes for all classes. In case of an overall class tie, the predicted class is chosen by random.
</p>


<h3>Author(s)</h3>

<p>David Meyer (<a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a>)
</p>


<h3>See Also</h3>

<p><code><a href="proxy.html#topic+dist">dist</a></code> (in package <span class="pkg">proxy</span>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)

model &lt;- gknn(Species ~ ., data = iris)
predict(model, iris[c(1, 51, 101),])

test = c(45:50, 95:100, 145:150)

model &lt;- gknn(Species ~ ., data = iris[-test,], k = 3, method = "Manhattan")
predict(model, iris[test,], type = "votes")

model &lt;- gknn(Species ~ ., data = iris[-test], k = 3, method = "Manhattan")
predict(model, iris[test,], type = "prob")

</code></pre>

<hr>
<h2 id='hamming.distance'>Hamming Distances of Vectors</h2><span id='topic+hamming.distance'></span>

<h3>Description</h3>

<p> If both <code>x</code> and
<code>y</code> are vectors, <code>hamming.distance</code> returns the Hamming
distance (number of different elements) between this two vectors. If
<code>x</code> is a matrix, the Hamming distances between the rows of <code>x</code>
are computed and <code>y</code> is ignored.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> hamming.distance(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hamming.distance_+3A_x">x</code></td>
<td>
<p>a vector or matrix.</p>
</td></tr>
<tr><td><code id="hamming.distance_+3A_y">y</code></td>
<td>
<p>an optional vector.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(1, 0, 0)
y &lt;- c(1, 0, 1)
hamming.distance(x, y)
z &lt;- rbind(x,y)
rownames(z) &lt;- c("Fred", "Tom")
hamming.distance(z)

hamming.distance(1:3, 3:1)
</code></pre>

<hr>
<h2 id='hamming.window'>Computes the Coefficients of a Hamming Window.</h2><span id='topic+hamming.window'></span>

<h3>Description</h3>

<p>The filter coefficients <code class="reqn">w_i</code> of a Hamming
window of length 
<code>n</code> are computed according to the formula
</p>
<p style="text-align: center;"><code class="reqn">w_i = 0.54 - 0.46 \cos\frac{2\pi i}{n-1}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>hamming.window(n)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hamming.window_+3A_n">n</code></td>
<td>
<p>The length of the window.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector containing the filter coefficients.</p>


<h3>Author(s)</h3>

<p>Andreas Weingessel</p>


<h3>References</h3>

<p>For a definition of the Hamming window, see for example<br />
Alan V. Oppenheim and Roland W. Schafer: &quot;Discrete-Time Signal
Processing&quot;, Prentice-Hall, 1989.</p>


<h3>See Also</h3>

<p>stft, hanning.window</p>


<h3>Examples</h3>

<pre><code class='language-R'>hamming.window(10)

x&lt;-rnorm(500)
y&lt;-stft(x, wtype="hamming.window")
plot(y)
</code></pre>

<hr>
<h2 id='hanning.window'>Computes the Coefficients of a Hanning Window.</h2><span id='topic+hanning.window'></span>

<h3>Description</h3>

<p>The filter coefficients <code class="reqn">w_i</code> of a Hanning
window of length 
<code>n</code> are computed according to the formula
</p>
<p style="text-align: center;"><code class="reqn">w_i = 0.5 - 0.5 \cos\frac{2\pi i}{n-1}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>hanning.window(n)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hanning.window_+3A_n">n</code></td>
<td>
<p>The length of the window.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector containing the filter coefficients.</p>


<h3>Author(s)</h3>

<p>Andreas Weingessel</p>


<h3>References</h3>

<p>For a definition of the Hanning window, see for example<br />
Alan V. Oppenheim and Roland W. Schafer: &quot;Discrete-Time Signal
Processing&quot;, Prentice-Hall, 1989.</p>


<h3>See Also</h3>

<p>stft, hamming.window</p>


<h3>Examples</h3>

<pre><code class='language-R'>hanning.window(10)

x&lt;-rnorm(500)
y&lt;-stft(x, wtype="hanning.window")
plot(y)
</code></pre>

<hr>
<h2 id='hsv_palette'>Sequential color palette based on HSV colors</h2><span id='topic+hsv_palette'></span>

<h3>Description</h3>

<p>Computes a sequential color palette based on HSV colors by varying the
saturation, given hue and value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hsv_palette(h = 2/3, from = 0.7, to = 0.2, v = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hsv_palette_+3A_h">h</code></td>
<td>
<p>hue</p>
</td></tr>
<tr><td><code id="hsv_palette_+3A_from">from</code></td>
<td>
<p>lower bound for saturation</p>
</td></tr>
<tr><td><code id="hsv_palette_+3A_to">to</code></td>
<td>
<p>upper bound for saturation</p>
</td></tr>
<tr><td><code id="hsv_palette_+3A_v">v</code></td>
<td>
<p>value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A function with one argument: the size of the palette, i.e., the number
of colors.
</p>


<h3>Author(s)</h3>

<p>David Meyer <a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a></p>


<h3>See Also</h3>

<p><code><a href="grDevices.html#topic+hsv">hsv</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>pie(rep(1, 10), col = hsv_palette()(10))
pie(rep(1, 10), col = hsv_palette(h = 0)(10))
</code></pre>

<hr>
<h2 id='ica'>Independent Component Analysis</h2><span id='topic+ica'></span><span id='topic+plot.ica'></span><span id='topic+print.ica'></span>

<h3>Description</h3>

<p>This is an R-implementation of the Matlab-Function of
Petteri.Pajunen@hut.fi.
</p>
<p>For a data matrix X independent components are extracted by applying a
nonlinear PCA algorithm. The parameter <code>fun</code> determines which
nonlinearity is used. <code>fun</code> can either be a function or one of the
following strings &quot;negative kurtosis&quot;, &quot;positive kurtosis&quot;, &quot;4th
moment&quot; which can be abbreviated to uniqueness. If <code>fun</code> equals
&quot;negative (positive) kurtosis&quot; the function tanh (x-tanh(x)) is used
which provides ICA for sources with negative (positive) kurtosis. For
<code>fun == "4th moments"</code> the signed square function is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ica(X, lrate, epochs=100, ncomp=dim(X)[2], fun="negative")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ica_+3A_x">X</code></td>
<td>
<p>The matrix for which the ICA is to be computed</p>
</td></tr>
<tr><td><code id="ica_+3A_lrate">lrate</code></td>
<td>
<p>learning rate</p>
</td></tr>
<tr><td><code id="ica_+3A_epochs">epochs</code></td>
<td>
<p>number of iterations</p>
</td></tr>
<tr><td><code id="ica_+3A_ncomp">ncomp</code></td>
<td>
<p>number of independent components</p>
</td></tr>
<tr><td><code id="ica_+3A_fun">fun</code></td>
<td>
<p>function used for the nonlinear computation part</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"ica"</code> which is a list with components
</p>
<table>
<tr><td><code>weights</code></td>
<td>
<p>ICA weight matrix</p>
</td></tr>
<tr><td><code>projection</code></td>
<td>
<p>Projected data</p>
</td></tr>
<tr><td><code>epochs</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code>fun</code></td>
<td>
<p>Name of the used function</p>
</td></tr>
<tr><td><code>lrate</code></td>
<td>
<p>Learning rate used</p>
</td></tr>
<tr><td><code>initweights</code></td>
<td>
<p>Initial weight matrix</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Currently, there is no reconstruction from the ICA subspace to the
original input space.</p>


<h3>Author(s)</h3>

<p>Andreas Weingessel</p>


<h3>References</h3>

<p>Oja et al., &ldquo;Learning in Nonlinear Constrained Hebbian Networks&rdquo;, in
Proc. ICANN-91, pp. 385&ndash;390.
</p>
<p>Karhunen and Joutsensalo, &ldquo;Generalizations of Principal Component
Analysis, Optimization Problems, and Neural Networks&rdquo;, Neural Networks,
v. 8, no. 4, pp. 549&ndash;562, 1995.
</p>

<hr>
<h2 id='impute'>Replace Missing Values</h2><span id='topic+impute'></span>

<h3>Description</h3>

<p>Replaces missing values of a matrix or dataframe with the
medians (<code>what="median"</code>) or means (<code>what="mean"</code>) of the
respective columns.</p>


<h3>Usage</h3>

<pre><code class='language-R'>impute(x, what = c("median", "mean"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="impute_+3A_x">x</code></td>
<td>
<p>A matrix or dataframe.</p>
</td></tr>
<tr><td><code id="impute_+3A_what">what</code></td>
<td>
<p>What to impute.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix or dataframe.
</p>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;- matrix(1:10, ncol=2)
x[c(1,3,7)] &lt;- NA
print(x)
print(impute(x))
</code></pre>

<hr>
<h2 id='interpolate'>Interpolate Values of Array</h2><span id='topic+interpolate'></span>

<h3>Description</h3>

<p>For each row in matrix <code>x</code>, the hypercube of <code>a</code> containing
this point is searched. The corners of the hypercube are
linearly interpolated. By default, <code>dimnames(a)</code> is taken to
contain the coordinate values for each point in <code>a</code>. This can be
overridden using <code>adims</code>. If <code>method=="constant"</code>, the value
of the &ldquo;lower left&rdquo; corner of the hypercube is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interpolate(x, a, adims=lapply(dimnames(a), as.numeric),
            method="linear")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interpolate_+3A_x">x</code></td>
<td>
<p>Matrix of values at which interpolation shall take place.</p>
</td></tr>
<tr><td><code id="interpolate_+3A_a">a</code></td>
<td>
<p>Array of arbitrary dimension.</p>
</td></tr>
<tr><td><code id="interpolate_+3A_adims">adims</code></td>
<td>
<p>List of the same structure as <code>dimnames(a)</code>.</p>
</td></tr>
<tr><td><code id="interpolate_+3A_method">method</code></td>
<td>
<p>Interpolation method, one of <code>"linear"</code> or
<code>"constant"</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+approx">approx</a></code>, <code><a href="stats.html#topic+spline">spline</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(0,3,0.2)
z &lt;- outer(x,x, function(x,y) sin(x*y))
dimnames(z) &lt;- list(x,x)
sin(1.1*2.1)
interpolate(c(1.1, 2.1),z)
</code></pre>

<hr>
<h2 id='kurtosis'>Kurtosis</h2><span id='topic+kurtosis'></span>

<h3>Description</h3>

<p>Computes the kurtosis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kurtosis(x, na.rm = FALSE, type = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kurtosis_+3A_x">x</code></td>
<td>
<p>a numeric vector containing the values whose kurtosis is to
be computed.</p>
</td></tr>
<tr><td><code id="kurtosis_+3A_na.rm">na.rm</code></td>
<td>
<p>a logical value indicating whether <code>NA</code> values
should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="kurtosis_+3A_type">type</code></td>
<td>
<p>an integer between 1 and 3 selecting one of the algorithms
for computing kurtosis detailed below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> contains missings and these are not removed, the kurtosis
is <code>NA</code>.
</p>
<p>Otherwise, write <code class="reqn">x_i</code> for the non-missing elements of <code>x</code>,
<code class="reqn">n</code> for their number, <code class="reqn">\mu</code> for their mean, <code class="reqn">s</code> for
their standard deviation, and
<code class="reqn">m_r = \sum_i (x_i - \mu)^r / n</code>
for the sample moments of order <code class="reqn">r</code>.
</p>
<p>Joanes and Gill (1998) discuss three methods for estimating kurtosis:
</p>

<dl>
<dt>Type 1:</dt><dd>
<p><code class="reqn">g_2 = m_4 / m_2^2 - 3</code>.
This is the typical definition used in many older textbooks.</p>
</dd>
<dt>Type 2:</dt><dd>
<p><code class="reqn">G_2 = ((n+1) g_2 + 6) * (n-1) / ((n-2)(n-3))</code>.
Used in SAS and SPSS.
</p>
</dd>
<dt>Type 3:</dt><dd>
<p><code class="reqn">b_2 = m_4 / s^4 - 3 = (g_2 + 3) (1 - 1/n)^2 - 3</code>.
Used in MINITAB and BMDP.</p>
</dd>
</dl>

<p>Only <code class="reqn">G_2</code> (corresponding to <code>type = 2</code>) is unbiased under
normality.
</p>


<h3>Value</h3>

<p>The estimated kurtosis of <code>x</code>.
</p>


<h3>References</h3>

<p>D. N. Joanes and C. A. Gill (1998),
Comparing measures of sample skewness and kurtosis.
<em>The Statistician</em>, <b>47</b>, 183&ndash;189.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(100)
kurtosis(x)
</code></pre>

<hr>
<h2 id='lca'>Latent Class Analysis (LCA)</h2><span id='topic+lca'></span><span id='topic+print.lca'></span><span id='topic+summary.lca'></span><span id='topic+print.summary.lca'></span><span id='topic+predict.lca'></span>

<h3>Description</h3>

<p>A latent class analysis with <code>k</code> classes is performed on the data
given by <code>x</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lca(x, k, niter=100, matchdata=FALSE, verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lca_+3A_x">x</code></td>
<td>
<p>Either a data matrix of binary observations or a list of
patterns as created by <code><a href="e1071.html#topic+countpattern">countpattern</a></code></p>
</td></tr>
<tr><td><code id="lca_+3A_k">k</code></td>
<td>
<p>Number of classes used for LCA</p>
</td></tr>
<tr><td><code id="lca_+3A_niter">niter</code></td>
<td>
<p>Number of Iterations</p>
</td></tr>
<tr><td><code id="lca_+3A_matchdata">matchdata</code></td>
<td>
<p>If <code>TRUE</code> and <code>x</code> is a data matrix, the class
membership of every data point is returned, otherwise the class
membership of every pattern is returned.</p>
</td></tr>
<tr><td><code id="lca_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code> some output is printed during the
computations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"lca"</code> is returned, containing
</p>
<table>
<tr><td><code>w</code></td>
<td>
<p>Probabilities to belong to each class</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>Probabilities of a &lsquo;1&rsquo; for each variable in each class</p>
</td></tr>
<tr><td><code>matching</code></td>
<td>
<p>Depending on <code>matchdata</code> either the class
membership of each pattern or of each data point</p>
</td></tr>
<tr><td><code>logl</code>, <code>loglsat</code></td>
<td>
<p>The LogLikelihood of the model and of the
saturated model</p>
</td></tr>
<tr><td><code>bic</code>, <code>bicsat</code></td>
<td>
<p>The BIC of the model and of the
saturated model</p>
</td></tr>
<tr><td><code>chisq</code></td>
<td>
<p>Pearson's Chisq</p>
</td></tr>
<tr><td><code>lhquot</code></td>
<td>
<p>Likelihood quotient of the model and the saturated
model</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Number of data points.</p>
</td></tr>
<tr><td><code>np</code></td>
<td>
<p>Number of free parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andreas Weingessel</p>


<h3>References</h3>

<p>Anton K. Formann: &ldquo;Die Latent-Class-Analysis&rdquo;, Beltz
Verlag 1984</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+countpattern">countpattern</a></code>,
<code><a href="e1071.html#topic+bootstrap.lca">bootstrap.lca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a 4-dim. sample with 2 latent classes of 500 data points each.
## The probabilities for the 2 classes are given by type1 and type2.
type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x &lt;- matrix(runif(4000), nrow = 1000)
x[1:500,] &lt;- t(t(x[1:500,]) &lt; type1) * 1
x[501:1000,] &lt;- t(t(x[501:1000,]) &lt; type2) * 1

l &lt;- lca(x, 2, niter=5)
print(l)
summary(l)
p &lt;- predict(l, x)
table(p, c(rep(1,500),rep(2,500)))
</code></pre>

<hr>
<h2 id='matchClasses'>Find Similar Classes in Two-way Contingency Tables</h2><span id='topic+matchClasses'></span><span id='topic+compareMatchedClasses'></span>

<h3>Description</h3>

<p>Try to find a mapping between the two groupings, such that as many
cases as possible are in one of the matched pairs. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matchClasses(tab, method="rowmax", iter=1, maxexact=9, verbose=TRUE)
compareMatchedClasses(x, y, method="rowmax", iter=1,
                      maxexact=9, verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matchClasses_+3A_tab">tab</code></td>
<td>
<p>Two-way contingency table of class memberships</p>
</td></tr>
<tr><td><code id="matchClasses_+3A_method">method</code></td>
<td>
<p>One of <code>"rowmax"</code>, <code>"greedy"</code> or
<code>"exact"</code>.</p>
</td></tr>
<tr><td><code id="matchClasses_+3A_iter">iter</code></td>
<td>
<p>Number of iterations used in greedy search.</p>
</td></tr>
<tr><td><code id="matchClasses_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, display some status messages during
computation.</p>
</td></tr>
<tr><td><code id="matchClasses_+3A_maxexact">maxexact</code></td>
<td>
<p>Maximum number of variables for which all possible
permutations are computed.</p>
</td></tr>
<tr><td><code id="matchClasses_+3A_x">x</code>, <code id="matchClasses_+3A_y">y</code></td>
<td>
<p>Vectors or matrices with class memberships.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>method="rowmax"</code>, then each class defining a row in the
contingency table is mapped to the column of the corresponding row
maximum. Hence, some columns may be mapped to more than one row
(while each row is mapped to a single column).
</p>
<p>If <code>method="greedy"</code> or <code>method="exact"</code>, then the
contingency table must be a square matrix and a unique mapping is
computed. This corresponds to a permutation of columns and rows,
such that sum of the main diagonal, i.e., the trace of the matrix,
gets as large as possible. For both methods, first all pairs where
row and columns maxima correspond and are bigger than the sum of all
other elements in the corresponding columns and rows together are
located and fixed (this is a necessary condition for maximal trace).
</p>
<p>If <code>method="exact"</code>, then for the remaining rows and columns,
all possible permutations are computed and the optimum is
returned. This can get computationally infeasible very fast. If more
than <code>maxexact</code> rows and columns remain after applying the
necessary condition, then <code>method</code> is reset to <code>"greedy"</code>. If
<code>method="greedy"</code>, then a greedy heuristic is tried <code>iter</code>
times. Repeatedly a row is picked at random and matched to the free
column with the maximum value.
</p>
<p><code>compareMatchedClasses()</code> computes the contingency table for
each combination of columns from <code>x</code> and <code>y</code> and applies
<code>matchClasses</code> to that table. The columns of the table are
permuted accordingly and then the table is
passed to <code><a href="e1071.html#topic+classAgreement">classAgreement</a></code>. The resulting agreement
coefficients (diag, kappa, ...) are returned. The return value of
<code>compareMatchedClasses()</code> is a list containing a matrix for
each coefficient; with element (k,l) corresponding to the k-th
column of <code>x</code> and l-th column of <code>y</code>. If <code>y</code> is
missing, then the columns of <code>x</code> are compared with each other.
</p>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+classAgreement">classAgreement</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## a stupid example with no class correlations:
g1 &lt;- sample(1:5, size=1000, replace=TRUE)
g2 &lt;- sample(1:5, size=1000, replace=TRUE)
tab &lt;- table(g1, g2)
matchClasses(tab, "exact")

## let pairs (g1=1,g2=4) and (g1=3,g2=1) agree better
k &lt;- sample(1:1000, size=200)
g1[k] &lt;- 1
g2[k] &lt;- 4

k &lt;- sample(1:1000, size=200)
g1[k] &lt;- 3
g2[k] &lt;- 1

tab &lt;- table(g1, g2)
matchClasses(tab, "exact")

## get agreement coefficients:
compareMatchedClasses(g1, g2, method="exact")
</code></pre>

<hr>
<h2 id='matchControls'>Find Matched Control Group</h2><span id='topic+matchControls'></span>

<h3>Description</h3>

<p>Finds controls matching the cases as good as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matchControls(formula, data = list(), subset, contlabel = "con",
               caselabel = NULL, dogrep = TRUE, replace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matchControls_+3A_formula">formula</code></td>
<td>
<p>A formula indicating cases, controls and the
variables to be matched. Details are described below.</p>
</td></tr>
<tr><td><code id="matchControls_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the
model.  By default the variables are taken from the environment
which <code>matchControls</code> is called from.</p>
</td></tr>
<tr><td><code id="matchControls_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used in the matching process.</p>
</td></tr>
<tr><td><code id="matchControls_+3A_contlabel">contlabel</code></td>
<td>
<p>A string giving the label of the control group.</p>
</td></tr>
<tr><td><code id="matchControls_+3A_caselabel">caselabel</code></td>
<td>
<p>A string giving the labels of the cases.</p>
</td></tr>
<tr><td><code id="matchControls_+3A_dogrep">dogrep</code></td>
<td>
<p>If <code>TRUE</code>, then <code>contlabel</code> and
<code>contlabel</code> are matched using <code><a href="base.html#topic+grep">grep</a></code>, else
string comparison (exact equality) is used.</p>
</td></tr>
<tr><td><code id="matchControls_+3A_replace">replace</code></td>
<td>
<p>If <code>FALSE</code>, then every control is used only
once.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The left hand side of the <code>formula</code> must be a factor
determining whether an observation belongs to the case or the
control group.  By default, all observations where a grep of
<code>contlabel</code> matches, are used as possible controls, the rest is
taken as cases.  If <code>caselabel</code> is given, then only those
observations are taken as cases.  If <code>dogrep = TRUE</code>, then both
<code>contlabel</code> and <code>caselabel</code> can be regular expressions.
</p>
<p>The right hand side of the <code>formula</code> gives the variables that
should be matched.  The matching is done using the
<code><a href="cluster.html#topic+daisy">daisy</a></code> distance from the <code>cluster</code> package, i.e.,
a model frame is built from the formula and used as input for
<code><a href="cluster.html#topic+daisy">daisy</a></code>. For each case, the nearest control is
selected. If <code>replace = FALSE</code>, each control is used only
once.
</p>


<h3>Value</h3>

<p>Returns a list with components
</p>
<table>
<tr><td><code>cases</code></td>
<td>
<p>Row names of cases.</p>
</td></tr>
<tr><td><code>controls</code></td>
<td>
<p>Row names of matched controls.</p>
</td></tr>
<tr><td><code>factor</code></td>
<td>
<p>A factor with 2 levels indicating cases and controls
(the rest is set to <code>NA</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>Examples</h3>

<pre><code class='language-R'>Age.case &lt;- 40 + 5 * rnorm(50)
Age.cont &lt;- 45 + 10 * rnorm(150)
Age &lt;- c(Age.case, Age.cont)

Sex.case &lt;- sample(c("M", "F"), 50, prob = c(.4, .6), replace = TRUE)
Sex.cont &lt;- sample(c("M", "F"), 150, prob = c(.6, .4), replace = TRUE)
Sex &lt;- as.factor(c(Sex.case, Sex.cont))

casecont &lt;- as.factor(c(rep("case", 50), rep("cont", 150)))

## now look at the group properties:
boxplot(Age ~ casecont)
barplot(table(Sex, casecont), beside = TRUE)

m &lt;- matchControls(casecont ~ Sex + Age)

## properties of the new groups:
boxplot(Age ~ m$factor)
barplot(table(Sex, m$factor))
</code></pre>

<hr>
<h2 id='moment'>Statistical Moment</h2><span id='topic+moment'></span>

<h3>Description</h3>

<p>Computes the (optionally centered and/or absolute) sample moment of a
certain order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>moment(x, order=1, center=FALSE, absolute=FALSE, na.rm=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="moment_+3A_x">x</code></td>
<td>
<p>a numeric vector containing the values whose moment is to be
computed.</p>
</td></tr>
<tr><td><code id="moment_+3A_order">order</code></td>
<td>
<p>order of the moment to be computed, the default is to
compute the first moment, i.e., the mean.</p>
</td></tr>
<tr><td><code id="moment_+3A_center">center</code></td>
<td>
<p>a logical value indicating whether centered moments are
to be computed.</p>
</td></tr>
<tr><td><code id="moment_+3A_absolute">absolute</code></td>
<td>
<p>a logical value indicating whether absolute moments are
to be computed.</p>
</td></tr>
<tr><td><code id="moment_+3A_na.rm">na.rm</code></td>
<td>
<p>a logical value indicating whether <code>NA</code> values
should be stripped before the computation proceeds.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>center</code> and <code>absolute</code> are both <code>FALSE</code>, the
moment is simply <code>sum(x ^ order) / length(x)</code>.
</p>


<h3>Author(s)</h3>

<p>Kurt Hornik and Friedrich Leisch</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+mean">mean</a></code>,
<code><a href="stats.html#topic+var">var</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(100)

## Compute the mean
moment(x)
## Compute the 2nd centered moment (!= var)
moment(x, order=2, center=TRUE)

## Compute the 3rd absolute centered moment
moment(x, order=3, center=TRUE, absolute=TRUE)
</code></pre>

<hr>
<h2 id='naiveBayes'>Naive Bayes Classifier</h2><span id='topic+naiveBayes'></span><span id='topic+naiveBayes.default'></span><span id='topic+naiveBayes.formula'></span><span id='topic+print.naiveBayes'></span><span id='topic+predict.naiveBayes'></span>

<h3>Description</h3>

<p>Computes the conditional a-posterior probabilities of a categorical
class variable given independent predictor variables using
the Bayes rule.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
naiveBayes(formula, data, laplace = 0, ..., subset, na.action = na.pass)
## Default S3 method:
naiveBayes(x, y, laplace = 0, ...)


## S3 method for class 'naiveBayes'
predict(object, newdata,
  type = c("class", "raw"), threshold = 0.001, eps = 0, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="naiveBayes_+3A_x">x</code></td>
<td>
<p>A numeric matrix, or a data frame of categorical and/or
numeric variables.</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_y">y</code></td>
<td>
<p>Class vector.</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>class ~ x1 + x2 +
      ...</code>. Interactions are not allowed.</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_data">data</code></td>
<td>
<p>Either a data frame of predictors (categorical and/or
numeric) or a contingency table.</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_laplace">laplace</code></td>
<td>
<p>positive double controlling Laplace smoothing. The
default (0) disables Laplace smoothing.</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_subset">subset</code></td>
<td>
<p>For data given in a data frame, an index vector
specifying the cases to be used in the
training sample.  (NOTE: If given, this argument must be
named.)</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is not to count them for the
computation of the probability factors. An
alternative is na.omit, which leads to rejection of cases
with missing values on any required variable.  (NOTE: If
given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_object">object</code></td>
<td>
<p>An object of class <code>"naiveBayes"</code>.</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_newdata">newdata</code></td>
<td>
<p>A dataframe with new predictors (with possibly fewer
columns than the training data). Note that the column names of
<code>newdata</code> are matched against the training data ones.</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_type">type</code></td>
<td>
<p>If <code>"raw"</code>, the conditional a-posterior
probabilities for each class are returned, and the class with
maximal probability else.</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_threshold">threshold</code></td>
<td>
<p>Value replacing cells with probabilities within <code>eps</code> range.</p>
</td></tr>
<tr><td><code id="naiveBayes_+3A_eps">eps</code></td>
<td>
<p>double for specifying an epsilon-range to apply laplace
smoothing (to replace zero or close-zero probabilities by <code>theshold</code>.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The standard naive Bayes classifier (at least this implementation)
assumes independence of the predictor
variables, and Gaussian distribution (given the target class) of
metric predictors.
For attributes with missing values, the
corresponding table entries are omitted for prediction.
</p>


<h3>Value</h3>

<p>An object of class <code>"naiveBayes"</code> including components:
</p>
<table>
<tr><td><code>apriori</code></td>
<td>
<p>Class distribution for the dependent variable.</p>
</td></tr>
<tr><td><code>tables</code></td>
<td>
<p>A list of tables, one for each predictor variable. For each
categorical variable a table giving, for each attribute level, the conditional
probabilities given the target class. For each numeric variable, a
table giving, for each target class, mean and standard deviation of
the (sub-)variable.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>David Meyer <a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a>. Laplace smoothing
enhancement by Jinghao Xue.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Categorical data only:
data(HouseVotes84, package = "mlbench")
model &lt;- naiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,])
predict(model, HouseVotes84[1:10,], type = "raw")

pred &lt;- predict(model, HouseVotes84)
table(pred, HouseVotes84$Class)

## using laplace smoothing:
model &lt;- naiveBayes(Class ~ ., data = HouseVotes84, laplace = 3)
pred &lt;- predict(model, HouseVotes84[,-1])
table(pred, HouseVotes84$Class)


## Example of using a contingency table:
data(Titanic)
m &lt;- naiveBayes(Survived ~ ., data = Titanic)
m
predict(m, as.data.frame(Titanic))

## Example with metric predictors:
data(iris)
m &lt;- naiveBayes(Species ~ ., data = iris)
## alternatively:
m &lt;- naiveBayes(iris[,-5], iris[,5])
m
table(predict(m, iris), iris[,5])
</code></pre>

<hr>
<h2 id='permutations'>All Permutations of Integers 1:n</h2><span id='topic+permutations'></span>

<h3>Description</h3>

<p>Returns a matrix containing all permutations of the integers
<code>1:n</code> (one permutation per row).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permutations(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="permutations_+3A_n">n</code></td>
<td>
<p>Number of element to permute.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>Examples</h3>

<pre><code class='language-R'>permutations(3)
</code></pre>

<hr>
<h2 id='plot.stft'>Plot Short Time Fourier Transforms</h2><span id='topic+plot.stft'></span>

<h3>Description</h3>

<p>An object of class <code>"stft"</code> is plotted as a gray scale image.
The x-axis corresponds to time, the y-axis to frequency.  If the
default colormap is used, dark regions in the plot correspond to high
values at the particular time/frequency location.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stft'
plot(x, col = gray(63:0/63), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.stft_+3A_x">x</code></td>
<td>
<p>An object of class <code>"stft"</code> as obtained by the function
<code>stft</code>.</p>
</td></tr>
<tr><td><code id="plot.stft_+3A_col">col</code></td>
<td>
<p>An optional colormap.  By default 64 gray values are used,
where white corresponds to the minimum value and black to the
maximum.</p>
</td></tr>
<tr><td><code id="plot.stft_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value. This function is only for plotting.</p>


<h3>Author(s)</h3>

<p>Andreas Weingessel</p>


<h3>See Also</h3>

<p>stft</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-rnorm(500)
y&lt;-stft(x)
plot(y)
</code></pre>

<hr>
<h2 id='plot.svm'>Plot SVM Objects</h2><span id='topic+plot.svm'></span>

<h3>Description</h3>

<p>Generates a scatter plot of the input data of a <code>svm</code> fit for
classification models by highlighting the classes and support
vectors. Optionally, draws a filled contour plot of the class regions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'svm'
plot(x, data, formula, fill = TRUE, grid = 50, slice = list(),
symbolPalette = palette(), svSymbol = "x", dataSymbol = "o", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.svm_+3A_x">x</code></td>
<td>
<p>An object of class <code>svm</code></p>
</td></tr>
<tr><td><code id="plot.svm_+3A_data">data</code></td>
<td>
<p>data to visualize. Should be the same used for fitting.</p>
</td></tr>
<tr><td><code id="plot.svm_+3A_formula">formula</code></td>
<td>
<p>formula selecting the visualized two dimensions. Only
needed if more than two input variables are used.</p>
</td></tr>
<tr><td><code id="plot.svm_+3A_fill">fill</code></td>
<td>
<p>switch indicating whether a contour plot for the class
regions should be added.</p>
</td></tr>
<tr><td><code id="plot.svm_+3A_grid">grid</code></td>
<td>
<p>granularity for the contour plot.</p>
</td></tr>
<tr><td><code id="plot.svm_+3A_slice">slice</code></td>
<td>
<p>a list of named values for the dimensions held
constant (only needed if more than two variables are
used). The defaults for unspecified dimensions are 0 (for numeric
variables) and the first level (for factors). Factor levels can
either be specified as factors or character vectors of length 1.</p>
</td></tr>
<tr><td><code id="plot.svm_+3A_symbolpalette">symbolPalette</code></td>
<td>
<p>Color palette used for the class the data points and support
vectors belong to.</p>
</td></tr>
<tr><td><code id="plot.svm_+3A_svsymbol">svSymbol</code></td>
<td>
<p>Symbol used for support vectors.</p>
</td></tr>
<tr><td><code id="plot.svm_+3A_datasymbol">dataSymbol</code></td>
<td>
<p>Symbol used for data points (other than support vectors).</p>
</td></tr>
<tr><td><code id="plot.svm_+3A_...">...</code></td>
<td>
<p>additional graphics parameters passed to
<code>filled.contour</code> and <code>plot</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>David Meyer<br />
<a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a></p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+svm">svm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## a simple example
data(cats, package = "MASS")
m &lt;- svm(Sex~., data = cats)
plot(m, cats)

## more than two variables: fix 2 dimensions
data(iris)
m2 &lt;- svm(Species~., data = iris)
plot(m2, iris, Petal.Width ~ Petal.Length,
     slice = list(Sepal.Width = 3, Sepal.Length = 4))

## plot with custom symbols and colors
plot(m, cats, svSymbol = 1, dataSymbol = 2, symbolPalette = rainbow(4),
color.palette = terrain.colors)


</code></pre>

<hr>
<h2 id='plot.tune'>Plot Tuning Object</h2><span id='topic+plot.tune'></span>

<h3>Description</h3>

<p>Visualizes the results of parameter tuning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tune'
plot(x, type = c("contour", "perspective"), theta = 60,
          col = "lightblue", main = NULL, xlab = NULL, ylab = NULL,
          swapxy = FALSE, transform.x = NULL, transform.y = NULL,
          transform.z = NULL, color.palette = hsv_palette(),
          nlevels = 20, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.tune_+3A_x">x</code></td>
<td>
<p>an object of class <code>tune</code></p>
</td></tr>
<tr><td><code id="plot.tune_+3A_type">type</code></td>
<td>
<p>choose whether a contour plot or a perspective plot is
used if two parameters are to be visualized. Ignored if only one
parameter has been tuned.</p>
</td></tr>
<tr><td><code id="plot.tune_+3A_theta">theta</code></td>
<td>
<p>angle of azimuthal direction.</p>
</td></tr>
<tr><td><code id="plot.tune_+3A_col">col</code></td>
<td>
<p>the color(s) of the surface facets.  Transparent colors are
ignored.</p>
</td></tr>
<tr><td><code id="plot.tune_+3A_main">main</code></td>
<td>
<p>main title</p>
</td></tr>
<tr><td><code id="plot.tune_+3A_xlab">xlab</code>, <code id="plot.tune_+3A_ylab">ylab</code></td>
<td>
<p>titles for the axes.  N.B. These must be character
strings; expressions are not accepted.  Numbers will be
coerced to character strings.</p>
</td></tr>
<tr><td><code id="plot.tune_+3A_swapxy">swapxy</code></td>
<td>
<p>if <code>TRUE</code>, the parameter axes are swaped (only used
in case of two parameters).</p>
</td></tr>
<tr><td><code id="plot.tune_+3A_transform.x">transform.x</code>, <code id="plot.tune_+3A_transform.y">transform.y</code>, <code id="plot.tune_+3A_transform.z">transform.z</code></td>
<td>
<p>functions to transform
the parameters (<code>x</code> and <code>y</code>) and the error measures
(<code>z</code>). Ignored if <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="plot.tune_+3A_color.palette">color.palette</code></td>
<td>
<p>color palette used in contour plot.</p>
</td></tr>
<tr><td><code id="plot.tune_+3A_nlevels">nlevels</code></td>
<td>
<p>number of levels used in contour plot.</p>
</td></tr>
<tr><td><code id="plot.tune_+3A_...">...</code></td>
<td>
<p>Further graphics parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>David Meyer (based on C/C++-code by Chih-Chung Chang and Chih-Jen Lin)<br />
<a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+tune">tune</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  obj &lt;- tune.svm(Species~., data = iris, sampling = "fix",
                  gamma = 2^c(-8,-4,0,4), cost = 2^c(-8,-4,-2,0))
  plot(obj, transform.x = log2, transform.y = log2)
  plot(obj, type = "perspective", theta = 120, phi = 45)
</code></pre>

<hr>
<h2 id='predict.svm'>Predict Method for Support Vector Machines</h2><span id='topic+predict.svm'></span>

<h3>Description</h3>

<p>This function predicts values based upon a model trained by <code>svm</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'svm'
predict(object, newdata, decision.values = FALSE,
probability = FALSE, ..., na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.svm_+3A_object">object</code></td>
<td>
<p>Object of class <code>"svm"</code>, created by <code>svm</code>.</p>
</td></tr>
<tr><td><code id="predict.svm_+3A_newdata">newdata</code></td>
<td>
<p>An object containing the new input data: either a
matrix or a sparse matrix (object of class
<code><a href="Matrix.html#topic+Matrix">Matrix</a></code> provided by the <span class="pkg">Matrix</span> package,
or of class <code><a href="SparseM.html#topic+matrix.csr">matrix.csr</a></code>
provided by the <span class="pkg">SparseM</span> package, or of class
<code><a href="slam.html#topic+simple_triplet_matrix">simple_triplet_matrix</a></code> provided by the <span class="pkg">slam</span>
package). A vector will
be transformed to a n x 1 matrix.</p>
</td></tr>
<tr><td><code id="predict.svm_+3A_decision.values">decision.values</code></td>
<td>
<p>Logical controlling whether the decision values
of all binary classifiers computed in multiclass classification
shall be computed and returned.</p>
</td></tr>
<tr><td><code id="predict.svm_+3A_probability">probability</code></td>
<td>
<p>Logical indicating whether class probabilities
should be computed and returned. Only possible if the model was
fitted with the <code>probability</code> option enabled.</p>
</td></tr>
<tr><td><code id="predict.svm_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if &lsquo;NA&rsquo;s are
found. The default action is <code>na.omit</code>, which leads to rejection of cases
with missing values on any required variable. An alternative
is <code>na.fail</code>, which causes an error if <code>NA</code> cases
are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>	
<tr><td><code id="predict.svm_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predicted values (for classification: a vector of labels, for density
estimation: a logical vector). If <code>decision.value</code> is
<code>TRUE</code>, the vector gets a <code>"decision.values"</code> attribute
containing a n x c matrix (n number of predicted values, c number of
classifiers) of all c binary classifiers' decision values. There are k
* (k - 1) / 2 classifiers (k number of classes). The colnames of
the matrix indicate the labels of the two classes. If <code>probability</code> is
<code>TRUE</code>, the vector gets a <code>"probabilities"</code> attribute
containing a n x k matrix (n number of predicted values, k number of
classes) of the class probabilities.
</p>


<h3>Note</h3>

<p>If the training set was scaled by <code>svm</code> (done by default), the
new data is scaled accordingly using scale and center of
the training data.
</p>


<h3>Author(s)</h3>

<p>David Meyer (based on C++-code by Chih-Chung Chang and Chih-Jen Lin)<br />
<a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+svm">svm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
attach(iris)

## classification mode
# default with factor response:
model &lt;- svm(Species ~ ., data = iris)

# alternatively the traditional interface:
x &lt;- subset(iris, select = -Species)
y &lt;- Species
model &lt;- svm(x, y, probability = TRUE) 

print(model)
summary(model)

# test with train data
pred &lt;- predict(model, x)
# (same as:)
pred &lt;- fitted(model)

# compute decision values and probabilites
pred &lt;- predict(model, x, decision.values = TRUE, probability = TRUE)
attr(pred, "decision.values")[1:4,]
attr(pred, "probabilities")[1:4,]

## try regression mode on two dimensions

# create data
x &lt;- seq(0.1, 5, by = 0.05)
y &lt;- log(x) + rnorm(x, sd = 0.2)

# estimate model and predict input values
m   &lt;- svm(x, y)
new &lt;- predict(m, x)

# visualize
plot   (x, y)
points (x, log(x), col = 2)
points (x, new, col = 4)

## density-estimation

# create 2-dim. normal with rho=0:
X &lt;- data.frame(a = rnorm(1000), b = rnorm(1000))
attach(X)

# traditional way:
m &lt;- svm(X, gamma = 0.1)

# formula interface:
m &lt;- svm(~., data = X, gamma = 0.1)
# or:
m &lt;- svm(~ a + b, gamma = 0.1)

# test:
newdata &lt;- data.frame(a = c(0, 4), b = c(0, 4))
predict (m, newdata)

# visualize:
plot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))
points(newdata, pch = "+", col = 2, cex = 5)
</code></pre>

<hr>
<h2 id='probplot'>Probability Plot</h2><span id='topic+probplot'></span><span id='topic+lines.probplot'></span>

<h3>Description</h3>

<p>Generates a probability plot for a specified theoretical
distribution, i.e., basically a
<code><a href="stats.html#topic+qqplot">qqplot</a></code> where the y-axis is labeled with
probabilities instead
of quantiles. The function
is mainly intended for teaching the concept of quantile plots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>probplot(x, qdist=qnorm, probs=NULL, line=TRUE,
         xlab=NULL, ylab="Probability in %", ...)
## S3 method for class 'probplot'
lines(x, h=NULL, v=NULL, bend=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="probplot_+3A_x">x</code></td>
<td>
<p>A data vector for <code>probplot</code>, an object of class
<code>probplot</code> for the <code>lines</code> method.</p>
</td></tr>
<tr><td><code id="probplot_+3A_qdist">qdist</code></td>
<td>
<p>A character string or a function for the quantiles of the
target distribution.</p>
</td></tr>
<tr><td><code id="probplot_+3A_probs">probs</code></td>
<td>
<p>Vector of probabilities at which horizontal lines should
be drawn.</p>
</td></tr>
<tr><td><code id="probplot_+3A_line">line</code></td>
<td>
<p>Add a line passing through the quartiles to the plot?</p>
</td></tr>
<tr><td><code id="probplot_+3A_xlab">xlab</code>, <code id="probplot_+3A_ylab">ylab</code></td>
<td>
<p>Graphical parameters.</p>
</td></tr>
<tr><td><code id="probplot_+3A_h">h</code></td>
<td>
<p>The y-value for a horizontal line.</p>
</td></tr>
<tr><td><code id="probplot_+3A_v">v</code></td>
<td>
<p>The x-value for a vertical line.</p>
</td></tr>
<tr><td><code id="probplot_+3A_bend">bend</code></td>
<td>
<p>If <code>TRUE</code>, lines are &ldquo;bent&rdquo; at the quartile line, else
regular <code>abline</code>s are added. See examples.</p>
</td></tr>
<tr><td><code id="probplot_+3A_...">...</code></td>
<td>
<p>Further arguments for <code>qdist</code> and graphical
parameters for lines.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+qqplot">qqplot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## a simple example
x &lt;- rnorm(100, mean=5)
probplot(x)

## the same with horizontal tickmarks at the y-axis
opar &lt;- par("las")
par(las=1)
probplot(x)

## this should show the lack of fit at the tails
probplot(x, "qunif")

## for increasing degrees of freedom the t-distribution converges to
## normal
probplot(x, qt, df=1)
probplot(x, qt, df=3)
probplot(x, qt, df=10)
probplot(x, qt, df=100)

## manually add the line through the quartiles
p &lt;- probplot(x, line=FALSE)
lines(p, col="green", lty=2, lwd=2)


## Make the line at prob=0.5 red
lines(p, h=0.5, col="red")

### The following use the estimted distribution given by the green
### line:

## What is the probability that x is smaller than 7?
lines(p, v=7, bend=TRUE, col="blue")

## Median and 90% confidence interval
lines(p, h=.5, col="red", lwd=3, bend=TRUE)
lines(p, h=c(.05, .95), col="red", lwd=2, lty=3, bend=TRUE)

par(opar)
</code></pre>

<hr>
<h2 id='rbridge'>Simulation of Brownian Bridge</h2><span id='topic+rbridge'></span>

<h3>Description</h3>

<p><code>rwiener</code> returns a time series containing a simulated realization
of the Brownian bridge on the interval [0,<code>end</code>]. If W(t) is a
Wiener process, then the Brownian bridge is defined as W(t) - t W(1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbridge(end = 1, frequency = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbridge_+3A_end">end</code></td>
<td>
<p>the time of the last observation.</p>
</td></tr>
<tr><td><code id="rbridge_+3A_frequency">frequency</code></td>
<td>
<p>the number of observations per unit of time.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>rwiener
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a Brownian bridge on [0,1] and plot it

x &lt;- rbridge()
plot(x,type="l")
</code></pre>

<hr>
<h2 id='read.matrix.csr'>Read/Write Sparse Data</h2><span id='topic+read.matrix.csr'></span><span id='topic+write.matrix.csr'></span>

<h3>Description</h3>

<p>reads and writes a file in sparse data format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.matrix.csr(file, fac = TRUE, ncol = NULL)
write.matrix.csr(x, file = "out.dat", y = NULL, fac = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.matrix.csr_+3A_x">x</code></td>
<td>
<p>An object of class <code>matrix.csr</code></p>
</td></tr>
<tr><td><code id="read.matrix.csr_+3A_y">y</code></td>
<td>
<p>A vector (either numeric or a factor)</p>
</td></tr>
<tr><td><code id="read.matrix.csr_+3A_file">file</code></td>
<td>
<p>The filename.</p>
</td></tr>
<tr><td><code id="read.matrix.csr_+3A_fac">fac</code></td>
<td>
<p>If <code>TRUE</code>, the
y-values (if any) are interpreted as factor levels.</p>
</td></tr>
<tr><td><code id="read.matrix.csr_+3A_ncol">ncol</code></td>
<td>
<p>Number of columns, detected automatically. Can be used to
add empty columns (possibly not stored in the sparse format).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If the data file includes no y variable, <code>read.matrix.csr</code> returns an object of class
<code>matrix.csr</code>, else a list with components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>object of class <code>matrix.csr</code></p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>vector of numeric values or factor levels, depending on <code>fac</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>David Meyer<br />
<a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="SparseM.html#topic+matrix.csr">matrix.csr</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(methods)
if (require(SparseM)) {
    data(iris)
    x &lt;- as.matrix(iris[,1:4])
    y &lt;- iris[,5]
    xs &lt;- as.matrix.csr(x)
    write.matrix.csr(xs, y = y, file = "iris.dat")
    xs2 &lt;- read.matrix.csr("iris.dat")$x
    if (!all(as.matrix(xs) == as.matrix(xs2)))
        stop("Error: objects are not equal!")
}

## End(Not run)
</code></pre>

<hr>
<h2 id='rectangle.window'>Computes the Coefficients of a Rectangle Window.</h2><span id='topic+rectangle.window'></span>

<h3>Description</h3>

<p>Returns the filter coefficients of a rectangle
window. That is a vector of <code>n</code> 1.
</p>
<p>The purpose of this function is just to have a name for the R command
<code>rep (1, n)</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rectangle.window(n)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rectangle.window_+3A_n">n</code></td>
<td>
<p>The length of the window.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of length <code>n</code> filled with 1.</p>


<h3>Author(s)</h3>

<p>Andreas Weingessel</p>


<h3>See Also</h3>

<p>stft</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-rnorm(500)
y&lt;-stft(x, wtype="rectangle.window")
plot(y)
</code></pre>

<hr>
<h2 id='rwiener'>Simulation of Wiener Process</h2><span id='topic+rwiener'></span>

<h3>Description</h3>

<p><code>rwiener</code> returns a time series containing a simulated realization
of the Wiener process on the interval [0,<code>end</code>]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rwiener(end = 1, frequency = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rwiener_+3A_end">end</code></td>
<td>
<p>the time of the last observation.</p>
</td></tr>
<tr><td><code id="rwiener_+3A_frequency">frequency</code></td>
<td>
<p>the number of observations per unit of time.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a Wiener process on [0,1] and plot it

x &lt;- rwiener()
plot(x,type="l")
</code></pre>

<hr>
<h2 id='scale_data_frame'>Scaling and Centering of Data Frames</h2><span id='topic+scale_data_frame'></span>

<h3>Description</h3>

<p><code>scale_data_frame</code> centers and/or
scales the columns of a data frame (or matrix).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scale_data_frame(x, center = TRUE, scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scale_data_frame_+3A_x">x</code></td>
<td>
<p>a data frame or a numeric matrix (or vector). For matrices or vectors, 
<code>scale()</code> is used.</p>
</td></tr>
<tr><td><code id="scale_data_frame_+3A_center">center</code></td>
<td>
<p>either a logical value or numeric-alike vector of length
equal to the number of columns of <code>x</code>, where
&lsquo;numeric-alike&rsquo; means that <code><a href="base.html#topic+as.numeric">as.numeric</a>(.)</code> will
be applied successfully if <code><a href="base.html#topic+is.numeric">is.numeric</a>(.)</code> is not true.</p>
</td></tr>
<tr><td><code id="scale_data_frame_+3A_scale">scale</code></td>
<td>
<p>either a logical value or a numeric-alike vector of length
equal to the number of columns of <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The value of <code>center</code> determines how column centering is
performed.  If <code>center</code> is a numeric-alike vector with length equal to
the number of numeric/logical columns of <code>x</code>, then each column of <code>x</code> has
the corresponding value from <code>center</code> subtracted from it.  If
<code>center</code> is <code>TRUE</code> then centering is done by subtracting the
column means (omitting <code>NA</code>s) of <code>x</code> from their
corresponding columns, and if <code>center</code> is <code>FALSE</code>, no
centering is done.
</p>
<p>The value of <code>scale</code> determines how column scaling is performed
(after centering).  If <code>scale</code> is a numeric-alike vector with length
equal to the number of numeric/logiocal columns of <code>x</code>, then each column of
<code>x</code> is divided by the corresponding value from <code>scale</code>.
If <code>scale</code> is <code>TRUE</code> then scaling is done by dividing the
(centered) columns of <code>x</code> by their standard deviations if
<code>center</code> is <code>TRUE</code>, and the root mean square otherwise.
If <code>scale</code> is <code>FALSE</code>, no scaling is done.
</p>
<p>The root-mean-square for a (possibly centered) column is defined as
<code class="reqn">\sqrt{\sum(x^2)/(n-1)}</code>, where <code class="reqn">x</code> is
a vector of the non-missing values and <code class="reqn">n</code> is the number of
non-missing values.  In the case <code>center = TRUE</code>, this is the
same as the standard deviation, but in general it is not.  (To scale
by the standard deviations without centering, use
<code>scale(x, center = FALSE, scale = apply(x, 2, sd, na.rm = TRUE))</code>.)
</p>


<h3>Value</h3>

<p>For <code>scale.default</code>, the centered, scaled data frame. Non-numeric columns are ignored.
Note that logicals are treated as 0/1-numerics to be consistent with <code>scale()</code>.
The numeric centering and scalings used (if any) are returned as attributes
<code>"scaled:center"</code> and <code>"scaled:scale"</code> - but only for the numeric/logical columns.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+sweep">sweep</a></code> which allows centering (and scaling) with
arbitrary statistics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(stats)
data(iris)
summary(scale_data_frame(iris))
</code></pre>

<hr>
<h2 id='sigmoid'>The Logistic Function and Derivatives</h2><span id='topic+sigmoid'></span><span id='topic+dsigmoid'></span><span id='topic+d2sigmoid'></span>

<h3>Description</h3>

<p>Sigmoid <code class="reqn">1/(1 + \exp(-x))</code>, first and second
derivative.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sigmoid(x)
dsigmoid(x)
d2sigmoid(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sigmoid_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Friedrich Leisch</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(sigmoid, -5, 5, ylim = c(-.2, 1))
plot(dsigmoid, -5, 5, add = TRUE, col = 2)
plot(d2sigmoid, -5, 5, add = TRUE, col = 3)
</code></pre>

<hr>
<h2 id='skewness'>Skewness</h2><span id='topic+skewness'></span>

<h3>Description</h3>

<p>Computes the skewness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skewness(x, na.rm = FALSE, type = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="skewness_+3A_x">x</code></td>
<td>
<p>a numeric vector containing the values whose skewness is to
be computed.</p>
</td></tr>
<tr><td><code id="skewness_+3A_na.rm">na.rm</code></td>
<td>
<p>a logical value indicating whether <code>NA</code> values
should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="skewness_+3A_type">type</code></td>
<td>
<p>an integer between 1 and 3 selecting one of the algorithms
for computing skewness detailed below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> contains missings and these are not removed, the skewness
is <code>NA</code>.
</p>
<p>Otherwise, write <code class="reqn">x_i</code> for the non-missing elements of <code>x</code>,
<code class="reqn">n</code> for their number, <code class="reqn">\mu</code> for their mean, <code class="reqn">s</code> for
their standard deviation, and
<code class="reqn">m_r = \sum_i (x_i - \mu)^r / n</code>
for the sample moments of order <code class="reqn">r</code>.
</p>
<p>Joanes and Gill (1998) discuss three methods for estimating skewness:
</p>

<dl>
<dt>Type 1:</dt><dd>
<p><code class="reqn">g_1 = m_3 / m_2^{3/2}</code>.
This is the typical definition used in many older textbooks.</p>
</dd>
<dt>Type 2:</dt><dd>
<p><code class="reqn">G_1 = g_1 \sqrt{n(n-1)} / (n-2)</code>.
Used in SAS and SPSS.
</p>
</dd>
<dt>Type 3:</dt><dd>
<p><code class="reqn">b_1 = m_3 / s^3 = g_1 ((n-1)/n)^{3/2}</code>.
Used in MINITAB and BMDP.</p>
</dd>
</dl>

<p>All three skewness measures are unbiased under normality.
</p>


<h3>Value</h3>

<p>The estimated skewness of <code>x</code>.
</p>


<h3>References</h3>

<p>D. N. Joanes and C. A. Gill (1998),
Comparing measures of sample skewness and kurtosis.
<em>The Statistician</em>, <b>47</b>, 183&ndash;189.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(100)
skewness(x)
</code></pre>

<hr>
<h2 id='stft'>Computes the Short Time Fourier Transform of a Vector</h2><span id='topic+stft'></span>

<h3>Description</h3>

<p>This function computes the Short Time Fourier Transform
of a given vector <code>X</code>. 
</p>
<p>First, time-slices of length <code>win</code> are
extracted from the vector. The shift of one time-slice to the next one is
given by <code>inc</code>. The values of these time-slices are smoothed by
mulitplying them with a window function specified in <code>wtype</code>. For
the thus obtained windows, the Fast Fourier Transform is computed.</p>


<h3>Usage</h3>

<pre><code class='language-R'>stft(X, win=min(80,floor(length(X)/10)), inc=min(24,
floor(length(X)/30)), coef=64, wtype="hanning.window")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stft_+3A_x">X</code></td>
<td>
<p>The vector from which the stft is computed.</p>
</td></tr>
<tr><td><code id="stft_+3A_win">win</code></td>
<td>
<p>Length of the window. For long vectors the default window
size is 80, for short vectors the window size is chosen so that 10
windows fit in the vector.</p>
</td></tr>
<tr><td><code id="stft_+3A_inc">inc</code></td>
<td>
<p>Increment by which the window is shifted. For long vectors
the default increment is 24, for short vectors the increment is chosen
so that 30 increments fit in the vector.</p>
</td></tr> 
<tr><td><code id="stft_+3A_coef">coef</code></td>
<td>
<p>Number of Fourier coefficients</p>
</td></tr>
<tr><td><code id="stft_+3A_wtype">wtype</code></td>
<td>
<p>Type of window used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of type stft. Contains the values of the stft and
information about the parameters. 
</p>
<table>
<tr><td><code>values</code></td>
<td>
<p>A matrix containing the results of the stft. Each row of
the matrix contains the <code>coef</code> Fourier coefficients of one
window.</p>
</td></tr> 
<tr><td><code>windowsize</code></td>
<td>
<p>The value of the parameter <code>win</code></p>
</td></tr>
<tr><td><code>increment</code></td>
<td>
<p>The value of the parameter <code>inc</code></p>
</td></tr>
<tr><td><code>windowtype</code></td>
<td>
<p>The value of the parameter <code>wtype</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andreas Weingessel</p>


<h3>See Also</h3>

<p>plot.stft</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-rnorm(500)
y&lt;-stft(x)
plot(y)
</code></pre>

<hr>
<h2 id='svm'>Support Vector Machines</h2><span id='topic+svm'></span><span id='topic+svm.default'></span><span id='topic+svm.formula'></span><span id='topic+summary.svm'></span><span id='topic+print.summary.svm'></span><span id='topic+coef.svm'></span><span id='topic+print.svm'></span>

<h3>Description</h3>

<p><code>svm</code> is used to train a support vector machine. It can be used to carry
out general regression and classification (of nu and epsilon-type), as
well as density-estimation. A formula interface is provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
svm(formula, data = NULL, ..., subset, na.action =
na.omit, scale = TRUE)
## Default S3 method:
svm(x, y = NULL, scale = TRUE, type = NULL, kernel =
"radial", degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),
coef0 = 0, cost = 1, nu = 0.5,
class.weights = NULL, cachesize = 40, tolerance = 0.001, epsilon = 0.1,
shrinking = TRUE, cross = 0, probability = FALSE, fitted = TRUE,
..., subset, na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svm_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of the model to be fit.</p>
</td></tr>
<tr><td><code id="svm_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
&lsquo;svm&rsquo; is called from.</p>
</td></tr>
<tr><td><code id="svm_+3A_x">x</code></td>
<td>
<p>a data matrix, a vector, or a sparse matrix (object of class
<code><a href="Matrix.html#topic+Matrix">Matrix</a></code> provided by the <span class="pkg">Matrix</span> package,
or of class <code><a href="SparseM.html#topic+matrix.csr">matrix.csr</a></code>
provided by the <span class="pkg">SparseM</span> package, or of class
<code><a href="slam.html#topic+simple_triplet_matrix">simple_triplet_matrix</a></code> provided by the <span class="pkg">slam</span>
package).</p>
</td></tr>
<tr><td><code id="svm_+3A_y">y</code></td>
<td>
<p>a response vector with one label for each row/component of
<code>x</code>. Can be either a factor (for classification tasks)
or a numeric vector (for regression).</p>
</td></tr>
<tr><td><code id="svm_+3A_scale">scale</code></td>
<td>
<p>A logical vector indicating the variables to be
scaled. If <code>scale</code> is of length 1, the value is recycled as
many times as needed.
Per default, data are scaled internally (both <code>x</code> and <code>y</code>
variables) to zero mean and unit variance. The center and scale
values are returned and used for later predictions.</p>
</td></tr>
<tr><td><code id="svm_+3A_type">type</code></td>
<td>
<p><code>svm</code> can be used as a classification
machine, as a regression machine, or for novelty detection.
Depending of whether <code>y</code> is
a factor or not, the default setting for <code>type</code> is <code>C-classification</code> or <code>eps-regression</code>, respectively, but may be overwritten by setting an explicit value.<br />
Valid options are:
</p>

<ul>
<li> <p><code>C-classification</code>
</p>
</li>
<li> <p><code>nu-classification</code>
</p>
</li>
<li> <p><code>one-classification</code> (for novelty detection)
</p>
</li>
<li> <p><code>eps-regression</code>
</p>
</li>
<li> <p><code>nu-regression</code>
</p>
</li></ul>

</td></tr>
<tr><td><code id="svm_+3A_kernel">kernel</code></td>
<td>
<p>the kernel used in training and predicting. You
might consider changing some of the following parameters, depending
on the kernel type.<br />
</p>

<dl>
<dt>linear:</dt><dd><p><code class="reqn">u'v</code></p>
</dd>
<dt>polynomial:</dt><dd><p><code class="reqn">(\gamma u'v + coef0)^{degree}</code></p>
</dd>
<dt>radial basis:</dt><dd><p><code class="reqn">e^(-\gamma |u-v|^2)</code></p>
</dd>
<dt>sigmoid:</dt><dd><p><code class="reqn">tanh(\gamma u'v + coef0)</code></p>
</dd>
</dl>

</td></tr>
<tr><td><code id="svm_+3A_degree">degree</code></td>
<td>
<p>parameter needed for kernel of type <code>polynomial</code> (default: 3)</p>
</td></tr>
<tr><td><code id="svm_+3A_gamma">gamma</code></td>
<td>
<p>parameter needed for all kernels except <code>linear</code>
(default: 1/(data dimension))</p>
</td></tr>
<tr><td><code id="svm_+3A_coef0">coef0</code></td>
<td>
<p>parameter needed for kernels of type <code>polynomial</code>
and <code>sigmoid</code> (default: 0)</p>
</td></tr>
<tr><td><code id="svm_+3A_cost">cost</code></td>
<td>
<p>cost of constraints violation (default: 1)&mdash;it is the
&lsquo;C&rsquo;-constant of the regularization term in the Lagrange formulation.</p>
</td></tr>
<tr><td><code id="svm_+3A_nu">nu</code></td>
<td>
<p>parameter needed for <code>nu-classification</code>,
<code>nu-regression</code>, and <code>one-classification</code></p>
</td></tr>
<tr><td><code id="svm_+3A_class.weights">class.weights</code></td>
<td>
<p>a named vector of weights for the different
classes, used for asymmetric class sizes. Not all factor levels have
to be supplied (default weight: 1). All components have to be
named. Specifying <code>"inverse"</code> will choose the weights <em>inversely</em>
proportional to the class distribution.</p>
</td></tr>
<tr><td><code id="svm_+3A_cachesize">cachesize</code></td>
<td>
<p>cache memory in MB (default 40)</p>
</td></tr>
<tr><td><code id="svm_+3A_tolerance">tolerance</code></td>
<td>
<p>tolerance of termination criterion (default: 0.001)</p>
</td></tr>
<tr><td><code id="svm_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon in the insensitive-loss function (default: 0.1)</p>
</td></tr>
<tr><td><code id="svm_+3A_shrinking">shrinking</code></td>
<td>
<p>option whether to use the shrinking-heuristics
(default: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="svm_+3A_cross">cross</code></td>
<td>
<p>if a integer value k&gt;0 is specified, a k-fold cross
validation on the training data is performed to assess the quality
of the model: the accuracy rate for classification and the Mean
Squared Error for regression</p>
</td></tr>
<tr><td><code id="svm_+3A_fitted">fitted</code></td>
<td>
<p>logical indicating whether the fitted values should be computed
and included in the model or not (default: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="svm_+3A_probability">probability</code></td>
<td>
<p>logical indicating whether the model should
allow for probability predictions.</p>
</td></tr>
<tr><td><code id="svm_+3A_...">...</code></td>
<td>
<p>additional parameters for the low level fitting function
<code>svm.default</code></p>
</td></tr>
<tr><td><code id="svm_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample.  (NOTE: If given, this argument must be
named.)</p>
</td></tr>
<tr><td><code id="svm_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to rejection of cases
with missing values on any required variable. An alternative
is <code>na.fail</code>, which causes an error if <code>NA</code> cases
are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>	
</table>


<h3>Details</h3>

<p>For multiclass-classification with k levels, k&gt;2, <code>libsvm</code> uses the
&lsquo;one-against-one&rsquo;-approach, in which k(k-1)/2 binary classifiers are
trained; the appropriate class is found by a voting scheme.
</p>
<p><code>libsvm</code> internally uses a sparse data representation, which is 
also high-level supported by the package <span class="pkg">SparseM</span>.
</p>
<p>If the predictor variables include factors, the formula interface must be used to get a
correct model matrix.
</p>
<p><code>plot.svm</code> allows a simple graphical
visualization of classification models.
</p>
<p>The probability model for classification fits a logistic distribution
using maximum likelihood to the decision values of all binary
classifiers, and computes the a-posteriori class probabilities for the
multi-class problem using quadratic optimization. The probabilistic
regression model assumes (zero-mean) laplace-distributed errors for the
predictions, and estimates the scale parameter using maximum
likelihood.
</p>
<p>For linear kernel, the coefficients of the regression/decision hyperplane
can be extracted using the <code>coef</code> method (see examples).
</p>


<h3>Value</h3>

<p>An object of class <code>"svm"</code> containing the fitted model, including:
</p>
<table>
<tr><td><code>SV</code></td>
<td>
<p>The resulting support vectors (possibly scaled).</p>
</td></tr>
<tr><td><code>index</code></td>
<td>
<p>The index of the resulting support vectors in the data
matrix. Note that this index refers to the preprocessed data (after
the possible effect of <code>na.omit</code> and <code>subset</code>)</p>
</td></tr>
<tr><td><code>coefs</code></td>
<td>
<p>The corresponding coefficients times the training labels.</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>The negative intercept.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>In case of a probabilistic regression model, the scale
parameter of the hypothesized (zero-mean) laplace distribution estimated by
maximum likelihood.</p>
</td></tr>
<tr><td><code>probA</code>, <code>probB</code></td>
<td>
<p>numeric vectors of length k(k-1)/2, k number of
classes, containing the parameters of the logistic distributions fitted to
the decision values of the binary classifiers (1 / (1 + exp(a x + b))).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Data are scaled internally, usually yielding better results.
</p>
<p>Parameters of SVM-models usually <em>must</em> be tuned to yield sensible results!
</p>


<h3>Author(s)</h3>

<p>David Meyer (based on C/C++-code by Chih-Chung Chang and Chih-Jen Lin)<br />
<a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a>
</p>


<h3>References</h3>


<ul>
<li>
<p>Chang, Chih-Chung and Lin, Chih-Jen:<br />
<em>LIBSVM: a library for Support Vector Machines</em><br />
<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">https://www.csie.ntu.edu.tw/~cjlin/libsvm/</a>
</p>
</li>
<li>
<p>Exact formulations of models, algorithms, etc. can be found in the
document:<br />
Chang, Chih-Chung and Lin, Chih-Jen:<br />
<em>LIBSVM: a library for Support Vector Machines</em><br />
<a href="https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz">https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz</a>
</p>
</li>
<li>
<p>More implementation details and speed benchmarks can be found on:
Rong-En Fan and Pai-Hsune Chen and Chih-Jen Lin:<br />
<em>Working Set Selection Using the Second Order Information for Training SVM</em><br />
<a href="https://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="e1071.html#topic+predict.svm">predict.svm</a></code>
<code><a href="e1071.html#topic+plot.svm">plot.svm</a></code>
<code><a href="e1071.html#topic+tune.svm">tune.svm</a></code>
<code><a href="SparseM.html#topic+matrix.csr">matrix.csr</a></code> (in package <span class="pkg">SparseM</span>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
attach(iris)

## classification mode
# default with factor response:
model &lt;- svm(Species ~ ., data = iris)

# alternatively the traditional interface:
x &lt;- subset(iris, select = -Species)
y &lt;- Species
model &lt;- svm(x, y) 

print(model)
summary(model)

# test with train data
pred &lt;- predict(model, x)
# (same as:)
pred &lt;- fitted(model)

# Check accuracy:
table(pred, y)

# compute decision values and probabilities:
pred &lt;- predict(model, x, decision.values = TRUE)
attr(pred, "decision.values")[1:4,]

# visualize (classes by color, SV by crosses):
plot(cmdscale(dist(iris[,-5])),
     col = as.integer(iris[,5]),
     pch = c("o","+")[1:150 %in% model$index + 1])

## try regression mode on two dimensions

# create data
x &lt;- seq(0.1, 5, by = 0.05)
y &lt;- log(x) + rnorm(x, sd = 0.2)

# estimate model and predict input values
m   &lt;- svm(x, y)
new &lt;- predict(m, x)

# visualize
plot(x, y)
points(x, log(x), col = 2)
points(x, new, col = 4)

## density-estimation

# create 2-dim. normal with rho=0:
X &lt;- data.frame(a = rnorm(1000), b = rnorm(1000))
attach(X)

# traditional way:
m &lt;- svm(X, gamma = 0.1)

# formula interface:
m &lt;- svm(~., data = X, gamma = 0.1)
# or:
m &lt;- svm(~ a + b, gamma = 0.1)

# test:
newdata &lt;- data.frame(a = c(0, 4), b = c(0, 4))
predict (m, newdata)

# visualize:
plot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))
points(newdata, pch = "+", col = 2, cex = 5)

## weights: (example not particularly sensible)
i2 &lt;- iris
levels(i2$Species)[3] &lt;- "versicolor"
summary(i2$Species)
wts &lt;- 100 / table(i2$Species)
wts
m &lt;- svm(Species ~ ., data = i2, class.weights = wts)

## extract coefficients for linear kernel

# a. regression
x &lt;- 1:100
y &lt;- x + rnorm(100)
m &lt;- svm(y ~ x, scale = FALSE, kernel = "linear")
coef(m)
plot(y ~ x)
abline(m, col = "red")

# b. classification
# transform iris data to binary problem, and scale data
setosa &lt;- as.factor(iris$Species == "setosa")
iris2 = scale(iris[,-5])

# fit binary C-classification model
m &lt;- svm(setosa ~ Petal.Width + Petal.Length,
        data = iris2, kernel = "linear")

# plot data and separating hyperplane
plot(Petal.Length ~ Petal.Width, data = iris2, col = setosa)
(cf &lt;- coef(m))
abline(-cf[1]/cf[3], -cf[2]/cf[3], col = "red")

# plot margin and mark support vectors
abline(-(cf[1] + 1)/cf[3], -cf[2]/cf[3], col = "blue")
abline(-(cf[1] - 1)/cf[3], -cf[2]/cf[3], col = "blue")
points(m$SV, pch = 5, cex = 2)
</code></pre>

<hr>
<h2 id='tune'>Parameter Tuning of Functions Using Grid Search</h2><span id='topic+tune'></span><span id='topic+best.tune'></span><span id='topic+print.tune'></span><span id='topic+summary.tune'></span><span id='topic+print.summary.tune'></span>

<h3>Description</h3>

<p>This generic function tunes hyperparameters of statistical methods
using a grid search over supplied parameter ranges.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune(METHOD, train.x, train.y = NULL, data = list(), validation.x =
     NULL, validation.y = NULL, ranges = NULL, predict.func = predict,
     tunecontrol = tune.control(), ...)
best.tune(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_+3A_method">METHOD</code></td>
<td>
<p>either the function to be tuned, or a character string
naming such a function.</p>
</td></tr>
<tr><td><code id="tune_+3A_train.x">train.x</code></td>
<td>
<p>either a formula or a matrix of predictors.</p>
</td></tr>
<tr><td><code id="tune_+3A_train.y">train.y</code></td>
<td>
<p>the response variable if <code>train.x</code> is a predictor
matrix. Ignored if <code>train.x</code> is a formula.</p>
</td></tr>
<tr><td><code id="tune_+3A_data">data</code></td>
<td>
<p>data, if a formula interface is used. Ignored, if
predictor matrix and response are supplied directly.</p>
</td></tr>
<tr><td><code id="tune_+3A_validation.x">validation.x</code></td>
<td>
<p>an optional validation set. Depending on whether a
formula interface is used or not, the response can be
included in <code>validation.x</code> or separately specified using
<code>validation.y</code>. Only used for bootstrap and fixed validation
set (see <code><a href="e1071.html#topic+tune.control">tune.control</a></code>)</p>
</td></tr>
<tr><td><code id="tune_+3A_validation.y">validation.y</code></td>
<td>
<p>if no formula interface is used, the response of
the (optional) validation set. Only used for bootstrap and fixed validation
set (see <code><a href="e1071.html#topic+tune.control">tune.control</a></code>)</p>
</td></tr>
<tr><td><code id="tune_+3A_ranges">ranges</code></td>
<td>
<p>a named list of parameter vectors spanning the sampling
space. The vectors will usually be created by <code>seq</code>.</p>
</td></tr>
<tr><td><code id="tune_+3A_predict.func">predict.func</code></td>
<td>
<p>optional predict function, if the standard <code>predict</code>
behavior is inadequate.</p>
</td></tr>
<tr><td><code id="tune_+3A_tunecontrol">tunecontrol</code></td>
<td>
<p>object of class <code>"tune.control"</code>, as created by the
function <code>tune.control()</code>. If omitted, <code>tune.control()</code>
gives the defaults.</p>
</td></tr>
<tr><td><code id="tune_+3A_...">...</code></td>
<td>
<p>Further parameters passed to the training functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As performance measure, the classification error is used
for classification, and the mean squared error for regression. It is
possible to specify only one parameter combination (i.e., vectors of
length 1) to obtain an error estimation of the specified type
(bootstrap, cross-classification, etc.) on the given data set. For
convenience, there
are several <code>tune.foo()</code> wrappers defined, e.g., for
<code>nnet()</code>, <code>randomForest()</code>,
<code>rpart()</code>, <code>svm()</code>, and <code>knn()</code>.
</p>
<p>Cross-validation randomizes the data set before building the splits
which&mdash;once created&mdash;remain constant during the training
process. The splits can be recovered through the <code>train.ind</code>
component of the returned object.
</p>


<h3>Value</h3>

<p>For <code>tune</code>, an object of class <code>tune</code>, including the components:
</p>
<table>
<tr><td><code>best.parameters</code></td>
<td>
<p>a 1 x k data frame, k number of parameters.</p>
</td></tr>
<tr><td><code>best.performance</code></td>
<td>
<p>best achieved performance.</p>
</td></tr>
<tr><td><code>performances</code></td>
<td>
<p>if requested, a data frame of all parameter
combinations along with the corresponding performance results.</p>
</td></tr>
<tr><td><code>train.ind</code></td>
<td>
<p>list of index vectors used for splits into
training and validation sets.</p>
</td></tr>
<tr><td><code>best.model</code></td>
<td>
<p>if requested, the model trained on the complete training data
using the best parameter combination.</p>
</td></tr>
</table>
<p><code>best.tune()</code> returns the best model detected by <code>tune</code>.
</p>


<h3>Author(s)</h3>

<p>David Meyer<br />
<a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+tune.control">tune.control</a></code>, <code><a href="e1071.html#topic+plot.tune">plot.tune</a></code>,
<code><a href="e1071.html#topic+tune.svm">tune.svm</a></code>, <a href="e1071.html#topic+tune.wrapper">tune.wrapper</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  ## tune `svm' for classification with RBF-kernel (default in svm),
  ## using one split for training/validation set
  
  obj &lt;- tune(svm, Species~., data = iris, 
              ranges = list(gamma = 2^(-1:1), cost = 2^(2:4)),
              tunecontrol = tune.control(sampling = "fix")
             )

  ## alternatively:
  ## obj &lt;- tune.svm(Species~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))

  summary(obj)
  plot(obj)

  ## tune `knn' using a convenience function; this time with the
  ## conventional interface and bootstrap sampling:
  x &lt;- iris[,-5]
  y &lt;- iris[,5]
  obj2 &lt;- tune.knn(x, y, k = 1:5, tunecontrol = tune.control(sampling = "boot"))
  summary(obj2)
  plot(obj2)

  ## tune `gknn' using the formula interface.
  ## (Use Euclidean distances instead of Gower metric)
  obj3 &lt;- tune.gknn(Species ~ ., data = iris, k = 1:5, method = "Euclidean")
  summary(obj3)
  plot(obj3)
  

  ## tune `rpart' for regression, using 10-fold cross validation (default)
  data(mtcars)
  obj4 &lt;- tune.rpart(mpg~., data = mtcars, minsplit = c(5,10,15))
  summary(obj4)
  plot(obj4)

  ## simple error estimation for lm using 10-fold cross validation
  tune(lm, mpg~., data = mtcars)
</code></pre>

<hr>
<h2 id='tune.control'>Control Parameters for the Tune Function</h2><span id='topic+tune.control'></span>

<h3>Description</h3>

<p>Creates an object of class <code>tune.control</code> to be used with
the <code>tune</code> function, containing various control parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune.control(random = FALSE, nrepeat = 1, repeat.aggregate = mean,
sampling = c("cross", "fix", "bootstrap"), sampling.aggregate = mean,
sampling.dispersion = sd,
cross = 10, fix = 2/3, nboot = 10, boot.size = 9/10, best.model = TRUE,
performances = TRUE, error.fun = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune.control_+3A_random">random</code></td>
<td>
<p>if an integer value is specified, <code>random</code>
parameter vectors are drawn from the parameter space.</p>
</td></tr>
<tr><td><code id="tune.control_+3A_nrepeat">nrepeat</code></td>
<td>
<p>specifies how often training shall be repeated.</p>
</td></tr>
<tr><td><code id="tune.control_+3A_repeat.aggregate">repeat.aggregate</code></td>
<td>
<p>function for aggregating the repeated training results.</p>
</td></tr>
<tr><td><code id="tune.control_+3A_sampling">sampling</code></td>
<td>
<p>sampling scheme. If <code>sampling = "cross"</code>, a
<code>cross</code>-times cross validation is performed. If <code>sampling
      = "boot"</code>, <code>nboot</code> training sets of size <code>boot.size</code> (part)
are sampled (with replacement) from the supplied data. If <code>sampling
      = "fix"</code>, a single split into training/validation set is
used, the training set containing a <code>fix</code> part of the supplied
data. Note that a separate validation set can be supplied via
<code>validation.x</code> and <code>validation.y</code>. It is only used for
<code>sampling = "boot"</code> and <code>sampling = "fix"</code>; in the latter
case, <code>fix</code> is set to 1.</p>
</td></tr>
<tr><td><code id="tune.control_+3A_sampling.aggregate">sampling.aggregate</code>, <code id="tune.control_+3A_sampling.dispersion">sampling.dispersion</code></td>
<td>
<p>functions for aggregating the training
results on the generated training samples (default: mean and
standard deviation).</p>
</td></tr>
<tr><td><code id="tune.control_+3A_cross">cross</code></td>
<td>
<p>number of partitions for cross-validation.</p>
</td></tr>
<tr><td><code id="tune.control_+3A_fix">fix</code></td>
<td>
<p>part of the data used for training in fixed sampling.</p>
</td></tr>
<tr><td><code id="tune.control_+3A_nboot">nboot</code></td>
<td>
<p>number of bootstrap replications.</p>
</td></tr>
<tr><td><code id="tune.control_+3A_boot.size">boot.size</code></td>
<td>
<p>size of the bootstrap samples.</p>
</td></tr>
<tr><td><code id="tune.control_+3A_best.model">best.model</code></td>
<td>
<p>if <code>TRUE</code>, the best model is trained and
returned (the best parameter set is used for
training on the complete training set).</p>
</td></tr>
<tr><td><code id="tune.control_+3A_performances">performances</code></td>
<td>
<p>if <code>TRUE</code>, the performance results for all
parameter combinations are returned.</p>
</td></tr>
<tr><td><code id="tune.control_+3A_error.fun">error.fun</code></td>
<td>
<p>function returning the error measure to be minimized.
It takes two arguments: a vector of true values and a vector of
predicted values. If <code>NULL</code>, the misclassification error is used
for categorical predictions and the mean squared error for numeric
predictions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"tune.control"</code> containing all the above
parameters (either the defaults or the user specified values).
</p>


<h3>Author(s)</h3>

<p>David Meyer<br />
<a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+tune">tune</a></code></p>

<hr>
<h2 id='tune.wrapper'>Convenience Tuning Wrapper Functions</h2><span id='topic+tune.wrapper'></span><span id='topic+tune.rpart'></span><span id='topic+best.rpart'></span><span id='topic+tune.svm'></span><span id='topic+best.svm'></span><span id='topic+tune.nnet'></span><span id='topic+best.nnet'></span><span id='topic+tune.randomForest'></span><span id='topic+best.randomForest'></span><span id='topic+tune.gknn'></span><span id='topic+best.gknn'></span><span id='topic+tune.knn'></span>

<h3>Description</h3>

<p>Convenience tuning wrapper functions, using <code>tune</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune.svm(x, y = NULL, data = NULL, degree = NULL, gamma = NULL, coef0 = NULL,
         cost = NULL, nu = NULL, class.weights = NULL, epsilon = NULL, ...)
best.svm(x, tunecontrol = tune.control(), ...)
 
tune.nnet(x, y = NULL, data = NULL, size = NULL, decay = NULL,
          trace = FALSE, tunecontrol = tune.control(nrepeat = 5), 
          ...)
best.nnet(x, tunecontrol = tune.control(nrepeat = 5), ...)

tune.rpart(formula, data, na.action = na.omit, minsplit = NULL,
           minbucket = NULL, cp = NULL, maxcompete = NULL, maxsurrogate = NULL,
           usesurrogate = NULL, xval = NULL, surrogatestyle = NULL, maxdepth =
           NULL, predict.func = NULL, ...)
best.rpart(formula, tunecontrol = tune.control(), ...)

tune.randomForest(x, y = NULL, data = NULL, nodesize = NULL, 
                  mtry = NULL, ntree = NULL, ...)
best.randomForest(x, tunecontrol = tune.control(), ...)

tune.gknn(x, y = NULL, data = NULL, k = NULL, ...)

best.gknn(x, tunecontrol = tune.control(), ...)

tune.knn(x, y, k = NULL, l = NULL, ...) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune.wrapper_+3A_formula">formula</code>, <code id="tune.wrapper_+3A_x">x</code>, <code id="tune.wrapper_+3A_y">y</code>, <code id="tune.wrapper_+3A_data">data</code></td>
<td>
<p>formula and data arguments of function to be tuned.</p>
</td></tr>
<tr><td><code id="tune.wrapper_+3A_predict.func">predict.func</code></td>
<td>
<p>predicting function.</p>
</td></tr>
<tr><td><code id="tune.wrapper_+3A_na.action">na.action</code></td>
<td>
<p>function handling missingness.</p>
</td></tr>
<tr><td><code id="tune.wrapper_+3A_minsplit">minsplit</code>, <code id="tune.wrapper_+3A_minbucket">minbucket</code>, <code id="tune.wrapper_+3A_cp">cp</code>, <code id="tune.wrapper_+3A_maxcompete">maxcompete</code>, <code id="tune.wrapper_+3A_maxsurrogate">maxsurrogate</code>, <code id="tune.wrapper_+3A_usesurrogate">usesurrogate</code>, <code id="tune.wrapper_+3A_xval">xval</code>, <code id="tune.wrapper_+3A_surrogatestyle">surrogatestyle</code>, <code id="tune.wrapper_+3A_maxdepth">maxdepth</code></td>
<td>
<p><code>rpart</code> parameters.</p>
</td></tr>
<tr><td><code id="tune.wrapper_+3A_degree">degree</code>, <code id="tune.wrapper_+3A_gamma">gamma</code>, <code id="tune.wrapper_+3A_coef0">coef0</code>, <code id="tune.wrapper_+3A_cost">cost</code>, <code id="tune.wrapper_+3A_nu">nu</code>, <code id="tune.wrapper_+3A_class.weights">class.weights</code>, <code id="tune.wrapper_+3A_epsilon">epsilon</code></td>
<td>
<p><code>svm</code>
parameters.</p>
</td></tr>
<tr><td><code id="tune.wrapper_+3A_k">k</code>, <code id="tune.wrapper_+3A_l">l</code></td>
<td>
<p><code>(g)knn</code> parameters.</p>
</td></tr>
<tr><td><code id="tune.wrapper_+3A_mtry">mtry</code>, <code id="tune.wrapper_+3A_nodesize">nodesize</code>, <code id="tune.wrapper_+3A_ntree">ntree</code></td>
<td>
<p><code>randomForest</code> parameters.</p>
</td></tr>
<tr><td><code id="tune.wrapper_+3A_size">size</code>, <code id="tune.wrapper_+3A_decay">decay</code>, <code id="tune.wrapper_+3A_trace">trace</code></td>
<td>
<p>parameters passed to
<code>nnet</code>.</p>
</td></tr>
<tr><td><code id="tune.wrapper_+3A_tunecontrol">tunecontrol</code></td>
<td>
<p>object of class <code>"tune.control"</code> containing
tuning parameters.</p>
</td></tr>
<tr><td><code id="tune.wrapper_+3A_...">...</code></td>
<td>
<p>Further parameters passed to <code>tune</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For examples, see the help page of <code>tune()</code>.</p>


<h3>Value</h3>

<p><code>tune.foo()</code> returns a tuning object including the best parameter set obtained
by optimizing over the specified parameter vectors. <code>best.foo()</code>
directly returns the best model, i.e. the fit of a new model using the
optimal parameters found by <code>tune.foo</code>.
</p>


<h3>Author(s)</h3>

<p>David Meyer<br />
<a href="mailto:David.Meyer@R-project.org">David.Meyer@R-project.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+tune">tune</a></code></p>

<hr>
<h2 id='write.svm'>Write SVM Object to File</h2><span id='topic+write.svm'></span>

<h3>Description</h3>

<p>This function exports an SVM object (trained by <code>svm</code>) to two
specified files. One is in the format that the
function 'svm_load_model()' of libsvm can read. The other is for
scaling data, containing a data with centers and scales for all variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write.svm(object, svm.file = "Rdata.svm",
          scale.file = "Rdata.scale", yscale.file = "Rdata.yscale")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write.svm_+3A_object">object</code></td>
<td>
<p>Object of class <code>"svm"</code>, created by <code>svm</code>.</p>
</td></tr>
<tr><td><code id="write.svm_+3A_svm.file">svm.file</code></td>
<td>
<p>filename to export the svm object to.</p>
</td></tr>
<tr><td><code id="write.svm_+3A_scale.file">scale.file</code></td>
<td>
<p>filename to export the scaling data of the
explanatory variables to.</p>
</td></tr>
<tr><td><code id="write.svm_+3A_yscale.file">yscale.file</code></td>
<td>
<p>filename to export the scaling data of the dependent
variable to, if any.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is useful when SVM models trained in R shall be used in 
other environments. The SVM model is saved in the standard format of
libsvm. The scaling data are written to separate files because scaling
data are not included in the standard format of libsvm. The format
of the scaling data file is a n times 2 matrix: the n-th row
corresponds to the n-th dimension of the data, the columns being formed
of the corresponding mean and scale. If scaling information for the
dependent variable exists (in case of regression models), it is stored
in yet another file (1 times 2 matrix).
</p>


<h3>Author(s)</h3>

<p>Tomomi TAKASHINA (based on 'predict.svm' by David Meyer)
<a href="mailto:t.takashina@computer.org">t.takashina@computer.org</a>
</p>


<h3>See Also</h3>

<p><code><a href="e1071.html#topic+svm">svm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
attach(iris)

## classification mode
# default with factor response:
model &lt;- svm (Species~., data=iris)

# export SVM object to (temporary) files
svm_file &lt;- tempfile()
scale_file &lt;- tempfile()

write.svm(model, svm.file = svm_file, scale.file = scale_file)

# read scale file
# the n-th row is corresponding to n-th dimension. The 1st column contains the
# center value, the 2nd column is the scale value.
read.table(scale_file)

# clean up
unlink(svm_file)
unlink(scale_file)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
