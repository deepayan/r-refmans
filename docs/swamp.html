<!DOCTYPE html><html><head><title>Help for package swamp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {swamp}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adjust.linearmodel'><p>Batch adjustment using a linear model</p></a></li>
<li><a href='#combat'>
<p>ComBat algorithm to combine batches.</p></a></li>
<li><a href='#confounding'><p>Heatmap of interrelation of sample annotations</p></a></li>
<li><a href='#corrected.p'><p>Correction of p-values for associations between features and sample annotation</p></a></li>
<li><a href='#dense.plot'><p>Density plots of feature associations in observed and permuted data</p></a></li>
<li><a href='#feature.assoc'><p>Associations of the features to a sample annotation in observed and reshuffled data.</p></a></li>
<li><a href='#hca.plot'>
<p>Dendrogram with according sample annotations</p></a></li>
<li><a href='#hca.test'>
<p>Tests for annotation differences among sample clusters</p></a></li>
<li><a href='#kill.pc'><p>Removes principal components from a data matrix</p></a></li>
<li><a href='#prince'>
<p>Linear models of prinicipal conponents dependent on sample annotations</p></a></li>
<li><a href='#prince.plot'>
<p>Heatmap of the associations between principal components and sample annotations</p></a></li>
<li><a href='#prince.var.plot'><p>ScreePlot of the data variation covered by the principal components</p></a></li>
<li><a href='#quickadjust.ref'>
<p>Batch adjustment by median-scaling to a reference batch</p></a></li>
<li><a href='#quickadjust.zero'><p>Batch adjustment by median-centering</p></a></li>
<li><a href='#swamp-package'>
<p>Visualization, Analysis and Adjustment of High-Dimensional Data in Respect to Sample Annotations</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Visualization, Analysis and Adjustment of High-Dimensional Data
in Respect to Sample Annotations</td>
</tr>
<tr>
<td>Version:</td>
<td>1.5.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>impute, amap, gplots, MASS</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods</td>
</tr>
<tr>
<td>Author:</td>
<td>Martin Lauss</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Martin Lauss &lt;martin.lauss@med.lu.se&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Collection of functions to connect the structure of the data with the information on the samples. Three types of associations are covered: 1. linear model of principal components. 2. hierarchical clustering analysis. 3. distribution of features-sample annotation associations. Additionally, the inter-relation between sample annotations can be analyzed. Simple methods are provided for the correction of batch effects and removal of principal components.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-12-06 10:51:24 UTC; med-mlu</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-12-06 15:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='adjust.linearmodel'>Batch adjustment using a linear model
</h2><span id='topic+adjust.linearmodel'></span>

<h3>Description</h3>

<p>The function uses a linear model for each feature: with the feature as dependent
variable and technical variables (batches) as regressors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjust.linearmodel(g, o.batches, robust.lm = F, small.memory = F)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjust.linearmodel_+3A_g">g</code></td>
<td>
<p>the input data in form of a matrix with features as rows and samples
as columns. NAs are allowed.
</p>
</td></tr>
<tr><td><code id="adjust.linearmodel_+3A_o.batches">o.batches</code></td>
<td>
<p>contains the batch variable(s). a numeric or factor vector,
or a dataframe.
</p>
</td></tr>
<tr><td><code id="adjust.linearmodel_+3A_robust.lm">robust.lm</code></td>
<td>
<p>default=F, if set to true robust linear models are performed
by rlm of package MASS.
The calculations take longer than using lm because a loop is used.
</p>
</td></tr>
<tr><td><code id="adjust.linearmodel_+3A_small.memory">small.memory</code></td>
<td>
<p>default=F, if set to true robust a loop through the rows
is used in the lm function.
This reduces the risk of running out of memory, however computation time is
longer.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each feature a lm(feature~., batches) is performed. The residuals of the
fitted model are returned.
(The means of the features of g are added to the residuals, as the residuals are centered,
which may not be desired.) 
Note the following possibilties of using a linear model for batch adjustment: 
1. Technical variables (Batches) can be numeric.
2. Numerical technical variables can be used in transformed forms (log, exp,...).
3. Several batch variables, be it numeric or factors, can be corrected at once,
by just adding more regressors to the model.
By default the function performs lm. If robust.lm = T, robust linear models are performed
using the rlm funcion of MASS.
NAs are not allowed in g. Samples that contain NAs in o.batches are returned unadjusted. 
</p>


<h3>Value</h3>

<p>A numeric matrix which is the adjusted dataset.
</p>


<h3>Note</h3>

<p>robust linear models require the package MASS
</p>


<h3>Author(s)</h3>

<p>Martin Lauss
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
   paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
# patient annotations as a data.frame, annotations should be numbers and factors
# but not characters.
# rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50), Numeric2=colMeans(g),row.names=colnames(g))

##unadjusted.data
res1&lt;-prince(g,o,top=10)
prince.plot(res1)

##batch adjustment
lin1&lt;-adjust.linearmodel(g,o$Numeric2)
lin2&lt;-adjust.linearmodel(g,o[,c("Numeric2","Factor2")]) # also correct for Factor2

##prince.plot
prince.plot(prince(lin1,o,top=10)) 
prince.plot(prince(lin2,o,top=10)) 


</code></pre>

<hr>
<h2 id='combat'>
ComBat algorithm to combine batches.
</h2><span id='topic+combat'></span>

<h3>Description</h3>

<p>Performs ComBat as described by Johnson et al.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combat(g, o.withbatch, batchcolumn = NULL, par.prior = T, prior.plots = T)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combat_+3A_g">g</code></td>
<td>

<p>the input data in form of a matrix with features as rows and samples as columns.
</p>
</td></tr>
<tr><td><code id="combat_+3A_o.withbatch">o.withbatch</code></td>
<td>

<p>the batch annotation as a factor vector or within a dataframe that contains
additional biological co-variates. make sure that the order of annotation is the same as in g. 
rownames (o) must be identical to colnames (g). when submitting a data.frame o.withbatch it
can contain only factors.
</p>
</td></tr>
<tr><td><code id="combat_+3A_batchcolumn">batchcolumn</code></td>
<td>
<p>Required. Specify the batch column number of a dataframe ;
set to 1 for a vector. All columns have to be factors, no NAs allowed.
</p>
</td></tr>
<tr><td><code id="combat_+3A_par.prior">par.prior</code></td>
<td>
<p>if 'T' uses the parametric adjustments, if 'F' uses the
nonparametric adjustments. if you are unsure what to use, try the parametric adjustments
(they run faster) and check the plots to see if these priors are reasonable.
</p>
</td></tr>
<tr><td><code id="combat_+3A_prior.plots">prior.plots</code></td>
<td>
<p> if 'T' will give prior plots with black as a kernal estimate
of the empirical batch effect density and red as the parametric estimate. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The R-code of the ComBat algorithm has been taken from the webpage jlab.byu.edu/ComBat
and input and output were adopted to the swamp package. ComBat uses parametric and non-parametric
empirical Bayes frameworks for adjusting data for batch effects. The method is robust to outliers
and performs particularly well with small sample sizes. ComBat can handle only categorical batch
variables in its current development stage. Biological covariates can be added to the model
(also categorical).
</p>


<h3>Value</h3>

<p>A numeric matrix which is the adjusted dataset.
</p>


<h3>Note</h3>

<p>R coded algorithm directly from Johnson WE
</p>


<h3>Author(s)</h3>

<p>Martin Lauss 
</p>


<h3>References</h3>

<p>Johnson WE, Li C, Rabinovic A. Adjusting batch effects in microarray
expression data using empirical Bayes methods. Biostatistics. 2007
Jan;8(1):118-27.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
          paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
# patient annotations as a data.frame, annotations should be numbers and factors
# but not characters.
# rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Factor3=factor(c(rep("X",15),rep("Y",20),rep("Z",15))),
              Numeric1=rnorm(50),
              row.names=colnames(g))

##unadjusted.data
res1&lt;-prince(g,o,top=10)
prince.plot(res1)

##batch adjustment for Factor 3
com1&lt;-combat(g,o$Factor3,batchcolumn=1)
##batch adjustment for Factor 3; with covariate
com2&lt;-combat(g,o[,c("Factor2","Factor3")],batchcolumn=2)

##prince.plot
prince.plot(prince(com1,o,top=10)) 
prince.plot(prince(com2,o,top=10)) 
</code></pre>

<hr>
<h2 id='confounding'>Heatmap of interrelation of sample annotations
</h2><span id='topic+confounding'></span>

<h3>Description</h3>

<p>The function tests the relationships of the sample annotations and plots
the heatmap of the p-values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confounding(o, method = "chisq", workspace = 2e+07, smallest = -20, 
            diagonal.zero = F, label = colnames(o), note = T, notecol = "black", 
            notecex = 1, breaks = 50, col = c(heat.colors(48), "white"), key = T, 
            cexRow = 1, cexCol = 1, margins = c(7,7), colsep = NULL, 
            rowsep = NULL, sepcolor = "black", sepwidth = c(0.05,0.05))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confounding_+3A_o">o</code></td>
<td>
<p>the sample annotations in the form of a data.frame, with the sample names as
rownames(o). o can contain factors with 2 or more levels and numeric variables;
no character variables are allowed. NAs are allowed and cases are removed at calculations.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_method">method</code></td>
<td>
<p>statistical test to be used when two factors are tested, this can be either
&quot;fisher&quot; or &quot;chisq&quot; to use fisher.test() or chisq.test(), respectively. 
default = &quot;chisq&quot;. fisher.test is however preferable as it is an exact test.
Note that fisher.test() is computationally expensive and can cause R to crash.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_workspace">workspace</code></td>
<td>
<p>workspace to use if test=&quot;fisher&quot;.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_smallest">smallest</code></td>
<td>
<p>a numeric value. log10(p-values) less than smallest are set to
smallest for plotting. default = -20.
e.g. a log10 p-value of -37 will be set to -20. Smallest has to be less than 0.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_diagonal.zero">diagonal.zero</code></td>
<td>
<p>set to TRUE to force diagonal p-values to be 0. 
</p>
</td></tr>
<tr><td><code id="confounding_+3A_label">label</code></td>
<td>
<p>vector containing names of the sample annotation. default=colnames(o)
</p>
</td></tr>
<tr><td><code id="confounding_+3A_note">note</code></td>
<td>
<p>set to TRUE to print the p-values in the cells of the plot.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_notecol">notecol</code></td>
<td>
<p>to determine the color of the notes.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_notecex">notecex</code></td>
<td>
<p>to determine the font size of the notes.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_breaks">breaks</code></td>
<td>
<p>either a number (default=50) or a numeric vector
(default would be seq(-20,0,length.out=50)) of breaks for the colors.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_col">col</code></td>
<td>
<p>a vector of colors with a length of breaks-1.
default=c(heat.colors(48), &quot;white&quot;)).
</p>
</td></tr>
<tr><td><code id="confounding_+3A_key">key</code></td>
<td>
<p>whether the color key should be printed, default=TRUE.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_cexrow">cexRow</code></td>
<td>
<p>font size of row label. default=1.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_cexcol">cexCol</code></td>
<td>
<p>font size of column label. default=1.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_margins">margins</code></td>
<td>
<p>a vector with the margins for columns and rows. default=c(7,7).
</p>
</td></tr>
<tr><td><code id="confounding_+3A_colsep">colsep</code></td>
<td>
<p>same as in heatmap.2 function.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_rowsep">rowsep</code></td>
<td>
<p>same as in heatmap.2 function.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_sepcolor">sepcolor</code></td>
<td>
<p>same as in heatmap.2 function.
</p>
</td></tr>
<tr><td><code id="confounding_+3A_sepwidth">sepwidth</code></td>
<td>
<p>same as in heatmap.2 function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Technical and biological annotations are often interrelated, leading to confounding.
This function tests the interelation of all sample annotations, be they technical batch
surrogates or biological measures.
Two sample annotations are compared at a time.
If both are factors, fisher.test() or chisq() test can be used.
Note that fisher.test() is computationally expensive and might cause R to crash at
large sample numbers.
If one sample annotation is numeric a linear modeal is used in
the form of lm(numeric sample annotation~other sample annotation).
The p-value is derived from the F-statistic of the linear model.
The p-value from lm() is equivalent to the cor.test() p-value in the case
of two numeric variables.
NAs in the sample annotations are allowed and result in deletion of the NA case.
It should be noted however, that different number of NAs in various
sample annotations lead to different power of the comparisons. Matrices that
specify for each comparison the test and sample number used are returned.
With NAs in the data it is possible that a pair of sample annotations does not
provide two different values each. In such a pair that does not show variance for
both annotations the output is set to NA.
The function uses heatmap.2() from the package gplots to plot the p-values.
</p>


<h3>Value</h3>

<p>a list with components
</p>
<table>
<tr><td><code>p.values</code></td>
<td>
<p>a numeric square matrix that contains the p-values for associations
between sample annotations.
</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>a numeric square matrix that contains the number of samples at each test.
</p>
</td></tr>
<tr><td><code>test.function</code></td>
<td>
<p>a character square matrix that contains the test
function used at each test.
</p>
</td></tr>
<tr><td><code>classes</code></td>
<td>
<p>a character vector that contains the classes of the variables in o.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>requires the package gplots
</p>


<h3>Author(s)</h3>

<p>Martin Lauss
</p>


<h3>Examples</h3>

<pre><code class='language-R'># patient annotations as a data.frame, annotations should be numbers and factors
# but not characters.
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Factor3=factor(c(rep("X",15),rep("Y",20),rep("Z",15))),
              Numeric1=rnorm(50))
              
## calculate and plot interrelations
res4&lt;-confounding(o,method="fisher")


</code></pre>

<hr>
<h2 id='corrected.p'>Correction of p-values for associations between features and sample annotation
</h2><span id='topic+corrected.p'></span>

<h3>Description</h3>

<p>The function corrects for multiple testing of associations of features to sample annotation.
Adjustment is done by padjust() or by the p-values from permuted data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corrected.p(feature.assoc, correction = "fdr", adjust.permute = T, 
            adjust.rank = T, ties.method = "first")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="corrected.p_+3A_feature.assoc">feature.assoc</code></td>
<td>
<p>a list with the p-values of feature associations,
typically created by the function feature.assoc().
(If not created by feature.assoc() the list has to contain the elements observed.p
and permuted.p.)
</p>
</td></tr>
<tr><td><code id="corrected.p_+3A_correction">correction</code></td>
<td>
<p>adjustment method to use for padjust().
default=&quot;fdr&quot;. must be one of &quot;holm&quot;, &quot;hochberg&quot;, &quot;hommel&quot;, &quot;bonferroni&quot;,
&quot;BH&quot;, &quot;BY&quot;, &quot;fdr&quot; or &quot;none&quot;.
</p>
</td></tr>
<tr><td><code id="corrected.p_+3A_adjust.permute">adjust.permute</code></td>
<td>
<p>if set to TRUE (default), the p-values will be adjusted
by observed.p divided by permuted.p for each rank in observed.p and permuted.p.
</p>
</td></tr>
<tr><td><code id="corrected.p_+3A_adjust.rank">adjust.rank</code></td>
<td>
<p>if set to TRUE (default), the p-values will be adjusted
by calculating for every observed p-value the proportion of smaller permuted.p
values to smaller observed.p values.
</p>
</td></tr>
<tr><td><code id="corrected.p_+3A_ties.method">ties.method</code></td>
<td>
<p>if adjust.permute=TRUE or adjust.rank=TRUE the method for
handling ties can be either &quot;first&quot; or &quot;random&quot;. Tied p-values are likely
when using &quot;AUC&quot; as method to measure feature associations. default=&quot;first&quot;. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As high-dimensional data contains many features, the p-values of
feature associations have to be corrected for multiple testing.
The number of features that are significantly associated with sample annotation can
show how strog the data is connected to the respective sample annotation.
The p-values can be adjusted using the standard correction methods of padjust().
Additionally two methods that use the p-values from permuted data are proposed.
First, p-values are adjusted by observed.p divided by permuted.p for each rank in
observed.p and permuted.p. For instance if the third lowest p-value in 
the observed associations is 1e-9 and the third lowest p value in
the permuted data is 1e-4, this p-value is correced by 1e-9 divided
by 1e-4  which is 1e-5.
Second, p-values are adjusted by calculating for every observed p-value the
proportion of smaller permuted.
p values to smaller observed.p values. For instance, we have a p-value
of 1e-3 which is ranked as the 300-lowest p-value in the observed data.
In the permuted data there are 3 p-values that are lower than 1e-3.
In the 300 p-values we suspect 3 of them to occur by chance, hence the
adjusted p-value is 3 divided by 300 which is 0.01. 
This correction method can yield p-values of 0 and is less robust when
only a few permuted.p are smaller than the observed.p. Both proposed
correction methods may acutally show similar results
to padjust(observed.p,method=&quot;fdr&quot;)  
</p>


<h3>Value</h3>

<p>a list with components
</p>
<table>
<tr><td><code>padjust</code></td>
<td>
<p>a numeric vector containing the corrected p-values using padjust().
</p>
</td></tr>
<tr><td><code>adjust.permute</code></td>
<td>
<p>a numeric vector containing the corrected p-values using
observed.p divided by permuted.p at each rank
</p>
</td></tr>
<tr><td><code>adjust.rank</code></td>
<td>
<p>a numeric vector containing the corrected p-values by
calculating for every observed p-value the proportion of smaller permuted.p values
to smaller observed.p values
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Martin Lauss</p>


<h3>Examples</h3>

<pre><code class='language-R'># data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
          paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
# patient annotations as a data.frame, annotations should be numbers and factor
# but not characters.
# rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))
              

#calculate feature associations
res4a&lt;-feature.assoc(g,o$Factor1,method="correlation")
#correct the p-values
res5&lt;-corrected.p(res4a)
  names(which(res5$padjust&lt;0.05))
  names(which(res5$adjust.permute&lt;0.05))
  names(which(res5$adjust.rank&lt;0.05))
</code></pre>

<hr>
<h2 id='dense.plot'>Density plots of feature associations in observed and permuted data
</h2><span id='topic+dense.plot'></span>

<h3>Description</h3>

<p>The function plots the distribution of feature associations for a specified sample
annotation for both observed and reshuffled data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dense.plot(feature.assoc, lty = 1:2, col = 1:2, lwd = c(2, 2), ylab = "", 
           main = "", cex.main = 1, cex.lab = 1, cex.axis = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dense.plot_+3A_feature.assoc">feature.assoc</code></td>
<td>
<p>A list of feature associations, typically created by
the function feature.assoc(). (If not created by feature.assoc() the list
has to contain the elements observed, permuted and method.)
</p>
</td></tr>
<tr><td><code id="dense.plot_+3A_lty">lty</code></td>
<td>
<p>a numeric vector containing the line types for the observed
and permuted density lines. default=1:2.
</p>
</td></tr>
<tr><td><code id="dense.plot_+3A_col">col</code></td>
<td>
<p>the colors for the observed and permuted density lines. default=1:2.
</p>
</td></tr>
<tr><td><code id="dense.plot_+3A_lwd">lwd</code></td>
<td>
<p>the line widths. default=c(2,2).
</p>
</td></tr>
<tr><td><code id="dense.plot_+3A_ylab">ylab</code></td>
<td>
<p>optional labeling of y-axis.
</p>
</td></tr>
<tr><td><code id="dense.plot_+3A_main">main</code></td>
<td>
<p>optional titel.
</p>
</td></tr>
<tr><td><code id="dense.plot_+3A_cex.main">cex.main</code></td>
<td>
<p>optional titel font size.
</p>
</td></tr>
<tr><td><code id="dense.plot_+3A_cex.lab">cex.lab</code></td>
<td>
<p>optional axis label font size.
</p>
</td></tr>
<tr><td><code id="dense.plot_+3A_cex.axis">cex.axis</code></td>
<td>
<p>optional axis font size.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function plots the distribution of associations of features with a
sample annotation calculated by feature.assoc(). The function uses plot.density()
for the observed data and adds the permuted data using lines(density()).
The x-axis is dependent on the method used to measure association,
e.g. if the method was &quot;correlation&quot;, then xlim is c(-1,1) and xlab=&quot;Corrlation&quot;.
</p>


<h3>Author(s)</h3>

<p>Martin Lauss</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
   paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
## patient annotations as a data.frame, annotations should be numbers and factor
# but not characters.
## rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))

# calculate the associations to Factor 1
res4a&lt;-feature.assoc(g,o$Factor1,method="correlation")
res4b&lt;-feature.assoc(g,o$Factor1,method="t.test",g1=res4a$permuted.data) 
   # uses t.test instead, reuses the permuted data generated in res4a
res4c&lt;-feature.assoc(g,o$Factor1,method="AUC",g1=res4a$permuted.data) 
   # uses AUC instead, reuses the permuted data generated in res4a

# plot distribution of associations in observed and permuted data
dense.plot(res4a)
dense.plot(res4b)
dense.plot(res4c)
</code></pre>

<hr>
<h2 id='feature.assoc'>Associations of the features to a sample annotation in observed and reshuffled data.
</h2><span id='topic+feature.assoc'></span>

<h3>Description</h3>

<p>This function calculates the associations of each feature of the data matrix to a specified
sample annotation. Either Pearson correlation, t-test statistic,
Area Under Curve or R squared is used as measure of association. 
In parallel, the features in permuted data are tested for comparison.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>feature.assoc(g, y, method = "correlation", g1 = NULL, exact = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feature.assoc_+3A_g">g</code></td>
<td>
<p>the input data in form of a matrix with features as rows and samples as columns.
Missing values are allowed.
</p>
</td></tr>
<tr><td><code id="feature.assoc_+3A_y">y</code></td>
<td>
<p>a factor or numeric vector which contains the sample information.
Typically a variable of the data.frame o used in the remaining functions of this package.
y can be a factor with 2 or more levels or a numeric vector. y cannot be a character vecor.
y has to be of the same length as ncol (g). Missing values are allowed and those cases are
removed from the calculations.
</p>
</td></tr>
<tr><td><code id="feature.assoc_+3A_method">method</code></td>
<td>
<p>if y is a factor with two levels, this method is used for calculation of the
association. The method can be one of &quot;correlation&quot;, &quot;t.test&quot;, or &quot;AUC&quot;. If y is a factor
with &gt;2 levels lm() is used automatically, if y is numeric cor() is used automatically to
determine the associations.  
</p>
</td></tr>
<tr><td><code id="feature.assoc_+3A_g1">g1</code></td>
<td>
<p>As there are different ways to generate a randomized dataset, a pre-calculated
permutation set can be specified here. Else the permutation data is generated within the
function by reshuffling the values for each feature. g1 has to be a matrix with the same
dimensions as g. 
</p>
</td></tr>
<tr><td><code id="feature.assoc_+3A_exact">exact</code></td>
<td>
<p>if method=&quot;AUC&quot;, exact determines how wilcox.test() treats ties.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each feature the association to the sample annotation is calculated. If the
sample annotaion is a factor with 2 levels, it can be chosen whether Pearson correlation,
t.test statistic or Area Under Curve (AUC) is used as measure of association. The
uncorrected p-values for the strength of associations are calculated by cor.test(),
t.test() and wilcox.test() respectively.
The distribution of these associations can be seen using dense.plot() function.
For instance this can reveal a group of positively associated features. The order
of the levels in levels(y) is decisive, e.g. for correlation the factors are
transformed by as.numeric(), whereby the first level becomes 1 and the second level
becomes 2. Hence, a positive association means higher values in samples with
level 2 and a negative assocation means higher values in level 1. This should also
be true for t.test and AUC, but please re-check.
If the annotation is a factor with more than 2 levels, lm() is automatically used
with R squared as the measure of association and the p-value as obtained from the
F statistic. If the annotation is a numeric vector, correlation
is used (with cor.test() for p-value). NAs are allowed in both the data matrix
and the annotation vector and is treated by case-wise deletion for the calculations.
To see the relelvance of the associations, the calculations are repeated with permuted
data, which can be either pre-entered as g1 or otherwise is calculated within the
function by reshuffling the values for each feature.
</p>


<h3>Value</h3>

<p>a list with components
</p>
<table>
<tr><td><code>observed</code></td>
<td>
<p>a numeric vector containing the association of features to sample
annotation in the observed data.
</p>
</td></tr>
<tr><td><code>permuted</code></td>
<td>
<p>a numeric vector containing the association of features to sample
annotation in the permuted data.
</p>
</td></tr>
<tr><td><code>observed.p</code></td>
<td>
<p>a numeric vector containing the p-values for association of features
to sample annotation in the observed data.
</p>
</td></tr>
<tr><td><code>permuted.p</code></td>
<td>
<p>a numeric vector containing the p-values for association of features
to sample annotation in the permuted data.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the method used as measure of association, which can be one of &quot;correlation&quot;,
&quot;t.test&quot;, &quot;AUC&quot; or &quot;linear.model&quot;.
</p>
</td></tr>
<tr><td><code>class.of.y</code></td>
<td>
<p>a character that states the class of y.
</p>
</td></tr> 
<tr><td><code>permuted.data</code></td>
<td>
<p>the matrix of the permuted data.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Martin Lauss</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
   paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
## patient annotations as a data.frame, annotations should be numbers and factor
# but not characters.
## rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))

# calculate the associations to Factor 1
res4a&lt;-feature.assoc(g,o$Factor1,method="correlation")
res4b&lt;-feature.assoc(g,o$Factor1,method="t.test",g1=res4a$permuted.data) 
    # uses t.test instead, reuses the permuted data generated in res4a
res4c&lt;-feature.assoc(g,o$Factor1,method="AUC",g1=res4a$permuted.data) 
    # uses AUC instead, reuses the permuted data generated in res4a
str(res4a)
 
</code></pre>

<hr>
<h2 id='hca.plot'>
Dendrogram with according sample annotations
</h2><span id='topic+hca.plot'></span>

<h3>Description</h3>

<p>The function plots the dendrogram from hierarchical cluster analysis with colorcoded
sample annotations below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hca.plot(g, o, method = "correlation", link = "ward", colored = palette(), 
         border = NA, code = colnames(o), cex.code = 1, 
         breaks = round(nrow(oreihe)/4), 
         cutcolors = colorpanel(breaks, low = "green", mid = "black", high = "red"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hca.plot_+3A_g">g</code></td>
<td>
<p> the input data in form of a matrix with features as rows and samples as columns.
</p>
</td></tr>
<tr><td><code id="hca.plot_+3A_o">o</code></td>
<td>

<p>the corresponding sample annotations in the form of a data.frame. A single sample annotation
variable as a vector is allowed and will be transformed to a data.frame.
rownames (o) must be identical to colnames (g). o can contain factors and numeric variables.
No character variables are allowed. NAs are allowed and blank spaces are plotted. 
</p>
</td></tr>
<tr><td><code id="hca.plot_+3A_method">method</code></td>
<td>
<p>the distance method for the clustering. default=&quot;correlation&quot;. hcluster
from the package amap is used and method must be one of &quot;euclidean&quot;, &quot;maximum&quot;,
&quot;manhattan&quot;, &quot;canberra&quot; &quot;binary&quot; &quot;pearson&quot;, &quot;correlation&quot;, &quot;spearman&quot; or &quot;kendall&quot;.
</p>
</td></tr>
<tr><td><code id="hca.plot_+3A_link">link</code></td>
<td>
<p>the agglomeration principle for the clustering. default=&quot;ward&quot;.
hcluster from the package amap is used and link must be one of &quot;ward&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot; or &quot;centroid&quot;.
</p>
</td></tr>
<tr><td><code id="hca.plot_+3A_colored">colored</code></td>
<td>
<p>a vector of colors in which factor variables of o will be colorcoded.
default are the 8 colors of palette(). the first level is plotted in the first color,
the second in the second color and so on. for annotation with more than 8 levels
colors should be added here.
</p>
</td></tr>
<tr><td><code id="hca.plot_+3A_border">border</code></td>
<td>
<p>a color for the borders in the annotation rectangels rect(). default=NA.
</p>
</td></tr>
<tr><td><code id="hca.plot_+3A_code">code</code></td>
<td>
<p>vector containing names of the sample annotations. default=colnames(o).
</p>
</td></tr>
<tr><td><code id="hca.plot_+3A_cex.code">cex.code</code></td>
<td>
<p>font size of code.
</p>
</td></tr>
<tr><td><code id="hca.plot_+3A_breaks">breaks</code></td>
<td>
<p>a number that determines in how many bins a numeric annotation is cut
using the cut() function.
</p>
</td></tr>
<tr><td><code id="hca.plot_+3A_cutcolors">cutcolors</code></td>
<td>
<p>a vector of color in which numeric variables will be colored.
length(cutcolors) has to be the number of breaks. a colorpanel is default to plot
the numeric values as a color gradient, with low values in green and high values in red.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data is clustered using the amap package. The plot works for sample annotations
as a data.frame or as a single vector. NAs are allowed in both data matrix and
sample annotation data.frame.
If the annotation is a factor, the annotations come in the colororder specified by colored.
If the annotation is numeric, breaks and cutcolors is used which is currently
set to be a colorpanel().
</p>


<h3>Note</h3>

<p>requires the packages amap and gplots 
</p>


<h3>Author(s)</h3>

<p>Martin Lauss
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
   paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show higher values in the samples 26:50
# patient annotations as a data.frame, annotations should be numbers and factor but not characters.
# rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))

## hca plot
hca.plot(g,o) 
 
</code></pre>

<hr>
<h2 id='hca.test'>
Tests for annotation differences among sample clusters
</h2><span id='topic+hca.test'></span>

<h3>Description</h3>

<p>The main clusters of a dendrogram are tested for different patient annotations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hca.test(g, o, dendcut = 2, method = "correlation", link = "ward", 
         test = "chisq", workspace = 2e+07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hca.test_+3A_g">g</code></td>
<td>
<p>the input data in form of a matrix with features as rows and samples as columns.
</p>
</td></tr>
<tr><td><code id="hca.test_+3A_o">o</code></td>
<td>

<p>the corresponding sample annotations in the form of a data.frame. Sample annotation
as a single vector is allowed and will be transformed to a data.frame.
rownames (o) must be identical to colnames (g). o can contain factors and numeric
variables. No character variables are allowed. NAs are allowed. 
</p>
</td></tr>
<tr><td><code id="hca.test_+3A_dendcut">dendcut</code></td>
<td>

<p>the number of clusters to cut the dendrogram tree (using cutree()). default=2.
</p>
</td></tr>
<tr><td><code id="hca.test_+3A_method">method</code></td>
<td>
<p>the distance method for the clustering. default=&quot;correlation&quot;.
hcluster from the package amap is used and method must be one of &quot;euclidean&quot;,
&quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot; &quot;binary&quot; &quot;pearson&quot;, &quot;correlation&quot;, &quot;spearman&quot;
or &quot;kendall&quot;.
</p>
</td></tr>
<tr><td><code id="hca.test_+3A_link">link</code></td>
<td>
<p>the agglomeration principle for the clustering. default=&quot;ward&quot;.
hcluster from the package amap is used and link must be one of &quot;ward&quot;, &quot;single&quot;,
&quot;complete&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot; or &quot;centroid&quot;.
</p>
</td></tr>
<tr><td><code id="hca.test_+3A_test">test</code></td>
<td>
<p>the test to use for the annotations that are factors. this can be either
&quot;fisher&quot; or &quot;chisq&quot; to use fisher.test() or chisq.test(), respectively. 
default = &quot;chisq&quot;. However fisher.test is preferable as it is
an exact test. Note that fisher.test() is computationally expensive
and can cause R to crash.
</p>
</td></tr>
<tr><td><code id="hca.test_+3A_workspace">workspace</code></td>
<td>
<p>workspace to use if test=&quot;fisher&quot;
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function clusters the samples using amap and then cuts the dendrogram into a
specified number of clusters. The obtained sample clusters are tested for differences
in sample annotations.
fisher.test() or chisq.test() is used if the annotation is a factor,
lm(annotation~clusters) is used for numeric annotations. The p-values for the dependence
of sample annotation and sample clusters are returned.
</p>


<h3>Value</h3>

<p>a list with components
</p>
<table>
<tr><td><code>p.values</code></td>
<td>
<p>a numeric vector containing the p.values for the annotation variable.
</p>
</td></tr>
<tr><td><code>classes</code></td>
<td>
<p>the classes of the annotation variables in o.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>requires the package amap
</p>


<h3>Author(s)</h3>

<p>Martin Lauss
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
          paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
## patient annotations as a data.frame, annotations should be numbers and factor
# but not characters.
## rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))

# perform the test for the main 2 clusters
res3&lt;-hca.test(g,o,dendcut=2,test="fisher") 
        # use test="chisq" for large ncol(g) to avoid crash of R
res3$p.values
</code></pre>

<hr>
<h2 id='kill.pc'>Removes principal components from a data matrix
</h2><span id='topic+kill.pc'></span>

<h3>Description</h3>

<p>Does not destroy your personal computer. Really. (No warranty).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kill.pc(g, pc, imputeknn = F, center = T)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kill.pc_+3A_g">g</code></td>
<td>
<p>the input data in form of a matrix with features as rows and samples as columns.
</p>
</td></tr>
<tr><td><code id="kill.pc_+3A_pc">pc</code></td>
<td>
<p>the principal components to be removed in form of a numeric vector of
length 1 or more. e.g. to remove pc1 and pc3 use pc=c(1,3), to remove only pc3 use pc=3.
</p>
</td></tr>
<tr><td><code id="kill.pc_+3A_imputeknn">imputeknn</code></td>
<td>
<p>default=FALSE. missing values in the data matrix can be imputed
by imputeknn=TRUE. The function knn.impute from the package impute is used with default settings. 
</p>
</td></tr>
<tr><td><code id="kill.pc_+3A_center">center</code></td>
<td>

<p>default=TRUE. the features are mean-centered before singular value decompositon.
this is a pre-requisite for principal component analysis, change only if you are
really convinced that centering is not necessary. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A specific principal component might be associated with several interelated
batch surrogate variables but free from biological associations. In such a case it may
be useful to take out such a principal component from the data.
The svd() function resolves the data matrix X into X = U*D*V. D is then set to
zero for the unwanted principal components and the data X is recalculated.
If you use the default center=TRUE make sure you also use prince() with the
default center=TRUE. Using different settings for center for the two functions
is not fully compatible.
</p>


<h3>Value</h3>

<p>a matrix which is the new data with the specified principal components removed.
</p>


<h3>Note</h3>

<p>requires the package impute.
</p>


<h3>Author(s)</h3>

<p>Martin Lauss
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
   paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
# patient annotations as a data.frame, annotations should be numbers and factors
# but not characters.
# rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))
  
## pca on unadjusted data
res1&lt;-prince(g,o,top=10)
prince.plot(res1)

## take out pc1
gadj3&lt;-kill.pc(g,pc=1)
prince.plot(prince(gadj3,o,top=10))  
</code></pre>

<hr>
<h2 id='prince'>
Linear models of prinicipal conponents dependent on sample annotations 
</h2><span id='topic+prince'></span>

<h3>Description</h3>

<p>The function calculates the principal components of the data and finds associations
between the prinicpal
components and the sample annotations using linear regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prince(g, o, top = 25, imputeknn = F, center = T, permute = F)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prince_+3A_g">g</code></td>
<td>

<p>the input data in form of a matrix with features as rows and samples as columns.
</p>
</td></tr>
<tr><td><code id="prince_+3A_o">o</code></td>
<td>

<p>the corresponding sample annotations in the form of a data.frame. rownames (o) must be
identical to colnames (g). 
o can contain factors with 2 or more levels and numeric variables; no character variables
are allowed. NAs are allowed (these samples are ommited in lm()). 
</p>
</td></tr>
<tr><td><code id="prince_+3A_top">top</code></td>
<td>

<p>the number of top principal components to be analyzed, default is set to 25.
</p>
</td></tr>
<tr><td><code id="prince_+3A_imputeknn">imputeknn</code></td>
<td>

<p>default=FALSE. missing values in the data matrix can be imputed by imputeknn=TRUE.
The function knn.impute from the package impute is used with default settings. 
</p>
</td></tr>
<tr><td><code id="prince_+3A_center">center</code></td>
<td>

<p>default=TRUE. the features are mean-centered before singular value decompositon.
this is a pre-requisite for principal component analysis, change only if you are really
convinced that centering is not necessary. 
</p>
</td></tr>
<tr><td><code id="prince_+3A_permute">permute</code></td>
<td>

<p>default=FALSE. if set to TRUE a permuted data matrix is generated with the values for
each feature shuffled. The linear models are also calculated for this permuted dataset.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To calculate the principal components of a data matrix, the function prcomp is used.
The function prcomp uses singular value decomposition instead of eigen decomposition
of the covariance matrix, the actual principal component analysis. As prcomp cannot
handle missing values they have to be imputed beforehands, imputeknn=TRUE can be used
for k-nearest neighbor imputation from package impute, k is 10.
A linear model is perfomed to find associations of the principal components with a dataframe
of sample annotations. The f-statistics of lm(principal component i ~ sample annotation j)
is used to derive the p-value for these associations. The results can be plotted using
the prince.plot() function. If permute=TRUE the analysis will be repeated on permuted data,
in which for each feature the values have been randomly shuffled.
</p>


<h3>Value</h3>

<p>a list as a prince object with components
</p>
<table>
<tr><td><code>pr</code></td>
<td>
<p>the output list of the prcomp function with the principal components
contained in pr$x. 
</p>
</td></tr>
<tr><td><code>linp</code></td>
<td>
<p>matrix containing the F-test p-values
from lm(principal component~sample annotation).
</p>
</td></tr>
<tr><td><code>rsquared</code></td>
<td>
<p>matrix containing the R-squared values
from lm(principal component~sample annotation).
</p>
</td></tr>  
<tr><td><code>prop</code></td>
<td>
<p>numeric vector containing the percentage of the overall variation for
each principal component.
</p>
</td></tr>
<tr><td><code>o</code></td>
<td>
<p>the input data.frame containing the patient annotations.
</p>
</td></tr>
<tr><td><code>pr.perm</code></td>
<td>
<p>if permute=T it contains the outcome list from prcomp for the permuted data.
</p>
</td></tr>
<tr><td><code>linpperm</code></td>
<td>
<p>if permute=T it contains the p-values for the permuted data.
</p>
</td></tr>
<tr><td><code>rsquaredperm</code></td>
<td>
<p>if permute=T it contains the R-squared values for the permuted data.
</p>
</td></tr>   
<tr><td><code>propperm</code></td>
<td>
<p>if permute=T it contains the percentages of variation for the permuted data.
</p>
</td></tr>
<tr><td><code>imputed</code></td>
<td>
<p>if imputeknn=T it contains the data matrix with the imputed values.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>requires the package impute</p>


<h3>Author(s)</h3>

<p>Martin Lauss</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
          paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
## patient annotations as a data.frame, annotations should be numbers and factor
# but not characters.
## rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))
              
## calculate principal components and use linear models to calculate 
## their dependence on sample annotations
res1&lt;-prince(g,o,top=10,permute=TRUE)
str(res1)
res1$linp # to see the p values
</code></pre>

<hr>
<h2 id='prince.plot'>
Heatmap of the associations between principal components and sample annotations
</h2><span id='topic+prince.plot'></span>

<h3>Description</h3>

<p>This function uses the calculations performed by prince() and plots the log10
of p-values obtained by the linear models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prince.plot(prince, label = colnames(prince$o), smallest = -20, note = F, 
            notecol = "black", notecex = 1, 
            breaks = seq(-20,0,length.out=100), col = heat.colors(99), 
            margins = c(5, 7), key = T, cexRow = 1, cexCol = 1, 
            xlab = "Principal Components (Variation)", colsep = NULL, 
            rowsep = NULL, sepcolor = "black", sepwidth = c(0.05,0.05),
            Rsquared = F, breaksRsquared = seq(0,1,length.out=100) )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prince.plot_+3A_prince">prince</code></td>
<td>
<p>an object generated by the function prince()
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_label">label</code></td>
<td>
<p>vector containing names of the sample annotation.
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_smallest">smallest</code></td>
<td>
<p>a numeric value. log10(p-values) less than smallest are set to
smallest for plotting. default = -20.
e.g. a log10 p-value of -37 will be set to -20. Smallest has
to be less than 0.  
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_note">note</code></td>
<td>
<p>set to TRUE to print the p-values in the cells of the plot.
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_notecol">notecol</code></td>
<td>
<p>to determine the color of the notes
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_notecex">notecex</code></td>
<td>
<p>to determine the font size of the notes
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_breaks">breaks</code></td>
<td>
<p>either a number (default=100) or a numeric
vector (default would be seq(-20,0,length.out=100)) of breaks for the colors
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_col">col</code></td>
<td>
<p>a vector of colors with a length of breaks-1. default=heat.colors(99).
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_margins">margins</code></td>
<td>
<p>a vector with the margins for columns and rows. default=c(5,7).
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_key">key</code></td>
<td>
<p>whether the color key should be printed, default=TRUE.
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_cexrow">cexRow</code></td>
<td>
<p>font size of label. default=1.
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_cexcol">cexCol</code></td>
<td>
<p>font size of column labeling. default=1.
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_xlab">xlab</code></td>
<td>
<p>an additional character vector to print at the bottom.
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_colsep">colsep</code></td>
<td>
<p>same as in heatmap.2 function.
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_rowsep">rowsep</code></td>
<td>
<p>same as in heatmap.2 function.
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_sepcolor">sepcolor</code></td>
<td>
<p>same as in heatmap.2 function.
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_sepwidth">sepwidth</code></td>
<td>
<p>same as in heatmap.2 function.
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_rsquared">Rsquared</code></td>
<td>
<p>set to TRUE to print Rsquared values instead of log10 p-values. 
Missing values in object o will flaw the comparability of p-values. 
</p>
</td></tr>
<tr><td><code id="prince.plot_+3A_breaksrsquared">breaksRsquared</code></td>
<td>
<p>same format as argument breaks. will be used when Rsquared=TRUE
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This plot indicates batch effects, and shows the influence of the biological annotations
on the overall variation. The input has to be an object generated by the prince() function.
The plot is based on the heatmap.2 function from the gplots package. Colors, breaks,
font size and margins can be changed and cell notes can be added.
The log10 of p-values from linear model are plotted. If Rsquared is TRUE the
R-squared values from linear model are plotted instead.
</p>


<h3>Note</h3>

<p>requires the package gplots</p>


<h3>Author(s)</h3>

<p>Martin Lauss</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
          paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
## patient annotations as a data.frame, annotations should be numbers and factors
# but not characters.
## rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))
              
## calculate principal components and use linear models to calculate their 
## dependence on sample annotations
res1&lt;-prince(g,o,top=10)
## plot p-values as heatmap
prince.plot(prince=res1)
</code></pre>

<hr>
<h2 id='prince.var.plot'>ScreePlot of the data variation covered by the principal components
</h2><span id='topic+prince.var.plot'></span>

<h3>Description</h3>

<p>To identify the number of top principal components with relevant variation,
this function plots the variation contained in the pc for both observed data
and reshuffled data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prince.var.plot(g, show.top = dim(g)[2], imputeknn = F, 
                center = T, npermute = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prince.var.plot_+3A_g">g</code></td>
<td>

<p>the input data in form of a matrix with features as rows and samples as columns.
</p>
</td></tr>
<tr><td><code id="prince.var.plot_+3A_show.top">show.top</code></td>
<td>

<p>the number of top principal components to be shown in the
plot (cannot exceed ncol(g) or nrow(g)).
</p>
</td></tr>
<tr><td><code id="prince.var.plot_+3A_imputeknn">imputeknn</code></td>
<td>

<p>default=FALSE. missing values in the data matrix can be imputed by imputeknn=TRUE.
The function knn.impute from the package impute is used with default settings. 
</p>
</td></tr>
<tr><td><code id="prince.var.plot_+3A_center">center</code></td>
<td>

<p>default=TRUE. the features are mean-centered before singular value decompositon.
this is a pre-requisite for principal component analysis, change only if you are
really convinced that centering is not necessary. 
</p>
</td></tr>
<tr><td><code id="prince.var.plot_+3A_npermute">npermute</code></td>
<td>

<p>the number of reshuffled datasets. default=10. A permuted data matrix is generated
with the values for each feature shuffled. From the permutation sets the median
percentage of variation for each principal component is taken.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function prcomp() is used to calculate the variation of the data
contained in the principal components.  As prcomp cannot handle missing values
they have to be imputed beforehands, using imputeknn=TRUE.</p>


<h3>Value</h3>

<p>a list with components
</p>
<table>
<tr><td><code>real.variation</code></td>
<td>
<p>a vector containing the percentage of variation for
each principal component in the observed data.
</p>
</td></tr>
<tr><td><code>permuted.variation</code></td>
<td>
<p>a matrix containing the percentages of variation
for each principal component in the reshuffled data sets.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>requires the package impute</p>


<h3>Author(s)</h3>

<p>Martin Lauss</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
          paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 
     # the first 100 features show higher values in the samples 26:50

## to plot the variations
res2&lt;-prince.var.plot(g,show.top=50,npermute=10)
str(res2)
</code></pre>

<hr>
<h2 id='quickadjust.ref'>
Batch adjustment by median-scaling to a reference batch
</h2><span id='topic+quickadjust.ref'></span>

<h3>Description</h3>

<p>The function adjusts for batches by adjusting the median of the features to the
median of a reference batch.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quickadjust.ref(g, batches, refbatch)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quickadjust.ref_+3A_g">g</code></td>
<td>
<p>the input data in form of a matrix with features as rows and samples
as columns. NAs are allowed.
</p>
</td></tr>
<tr><td><code id="quickadjust.ref_+3A_batches">batches</code></td>
<td>
<p>a factor with two or more levels and with same length as
ncol(g), each level has to contain at least 2 samples. 
</p>
</td></tr>
<tr><td><code id="quickadjust.ref_+3A_refbatch">refbatch</code></td>
<td>
<p>a character that determines the reference batch.
this character has to be a level of batches.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The batches are adjusted to a reference batch. The values of the
reference batch remain unchanged. For each feature the median of the batches is
determined. NAs are removed for median() calculations. The median of the reference
batch is divided by the median of the non-reference batch, which is the scaling factor. 
All values in the non-reference batch are multiplied by the scaling factor.
This way all batches will have the same median as the reference batch for each feature.
Scaling factors get inflated when data was already feature-centered before.
Hence, this method is only advisable for uncentered data.
This is a quick and simple method of batch adjustment, that probably does
not work for every batch effect, especially when sample numbers per batch
are low. The efficiency of batch adjustment can be checked
by prince.plot(prince(g,o)) or prince.plot(prince(g, data.frame(batch,batch))).
</p>


<h3>Value</h3>

<p>a list with components
</p>
<table>
<tr><td><code>adjusted.data</code></td>
<td>
<p>A numeric matrix which is the adjusted dataset.
</p>
</td></tr>
<tr><td><code>scaling.factors</code></td>
<td>
<p>A numeric matrix containing the scaling factors for
each feature in each batch.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Martin Lauss
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The function is currently defined as
# data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
          paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
# patient annotations as a data.frame, annotations should be numbers and factors
# but not characters.
# rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))

##unadjusted.data
res1&lt;-prince(g,o,top=10)
prince.plot(res1)

##batch adjustment
gadj2&lt;-quickadjust.ref(g,o$Factor1,"B")
str(gadj2)
##prince.plot
prince.plot(prince(gadj2$adjusted.data,o,top=10)) 
    # note the high number of variation covered by the first principal component. 
    # This is caused by infalted scaling factor as the features of the 
    # input matrix g are already centered around zero.
    # this adjustment method should be used only on uncentered data.
</code></pre>

<hr>
<h2 id='quickadjust.zero'>Batch adjustment by median-centering
</h2><span id='topic+quickadjust.zero'></span>

<h3>Description</h3>

<p>The function centers the median of each feature in each batch to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quickadjust.zero(g, batches)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quickadjust.zero_+3A_g">g</code></td>
<td>
<p>the input data in form of a matrix with features as rows and samples
as columns. NAs are allowed.
</p>
</td></tr>
<tr><td><code id="quickadjust.zero_+3A_batches">batches</code></td>
<td>
<p>a factor with two or more levels and with same length as
ncol(g), each level has to contain at least 2 samples. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The values of a feature are split up into batches and each part is
median-centered to zero, i.e. the batch median is subtracted from every value
in the batch. As a result the feature will have a median of zero in all batches.
NAs are removed for median() calculations.
This is a quick and simple method of batch adjustment, that probably does
not work for every batch effect, especially when sample numbers per batch
are low. The efficiency of batch adjustment can be checked by
prince.plot(prince(g,o)) or prince.plot(prince(g, data.frame(batch,batch))).
</p>


<h3>Value</h3>

<p>A numeric matrix which is the adjusted dataset.
</p>


<h3>Author(s)</h3>

<p>Martin Lauss
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data as a matrix
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
          paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1 # the first 100 features show
# higher values in the samples 26:50
# patient annotations as a data.frame, annotations should be numbers and factors
# but not characters.
# rownames have to be the same as colnames of the data matrix 
set.seed(200)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))

##unadjusted.data
res1&lt;-prince(g,o,top=10)
prince.plot(res1)

##batch adjustment
gadj1&lt;-quickadjust.zero(g,o$Factor1)
##prince.plot
prince.plot(prince(gadj1,o,top=10)) 

</code></pre>

<hr>
<h2 id='swamp-package'>
Visualization, Analysis and Adjustment of High-Dimensional Data in Respect to Sample Annotations
</h2><span id='topic+swamp-package'></span><span id='topic+swamp'></span>

<h3>Description</h3>

<p>The package contains functions to connect the structure of the data with sample information.
Three types of analyses are covered: 1. linear models of principal components. 2. hierarchical
clustering analysis. 
3. distribution of associations between individual features and sample annotations.
Additionally, the inter-relation between sample annotations can be analyzed. 
We include methods for batch adjustment by a. median-centering,
b. linear models and c. empirical bayes (combat).
A method to remove principal components from the data is also included. 
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> swamp</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Visualization, Analysis and Adjustment of High-Dimensional Data in Respect to Sample Annotations</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.5.1</td>
</tr>
<tr>
 <td style="text-align: left;">
biocViews: </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> impute, amap, gplots, MASS</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> methods</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Martin Lauss</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Martin Lauss &lt;martin.lauss@med.lu.se&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Collection of functions to connect the structure of the data with the information on the samples. Three types of associations are covered: 1. linear model of principal components. 2. hierarchical clustering analysis. 3. distribution of features-sample annotation associations. Additionally, the inter-relation between sample annotations can be analyzed. Simple methods are provided for the correction of batch effects and removal of principal components.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
Packaged: </td><td style="text-align: left;"> Wed Dec  06 09:58:46 2019; lauss</td>
</tr>
<tr>
 <td style="text-align: left;">
RoxygenNote: </td><td style="text-align: left;"> 7.0.2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
adjust.linearmodel      Batch adjustment using a linear model
combat                  ComBat algorithm to combine batches.
confounding             Heatmap of interrelation of sample annotations
corrected.p             Correction of p-values for associations between
                        features and sample annotation
dense.plot              Density plots of feature associations in
                        observed and permuted data
feature.assoc           Associations of the features to a sample
                        annotation in observed and reshuffled data.
hca.plot                Dendrogram with according sample annotations
hca.test                Tests for annotation differences among sample
                        clusters
kill.pc                 Removes principal components from a data matrix
prince                  Linear models of prinicipal conponents
                        dependent on sample annotations
prince.plot             Heatmap of the associations between principal
                        components and sample annotations
prince.var.plot         ScreePlot of the data variation covered by the
                        principal components
quickadjust.ref         Batch adjustment by median-scaling to a
                        reference batch
quickadjust.zero        Batch adjustment by median-centering
swamp-package           Visualization, Analysis and Adjustment of
                        High-Dimensional Data in Respect to Sample
                        Annotations
</pre>
<p>This package aims to find the associations between a high-dimensional data matrix,
typically obtained form high-throughput analysis, to the sample annotations,
be they technical (e.g. batch surrogates) or biological information.
High-dimensional data usually has a much larger number of features than samples.
This requires specific analysis to see &quot;how the data looks like&quot; and in which
way the technical and biological information on the samples is reflected in the
dataset.
Sample annotations can be analyzed for their association to principal
componenents (prince(), prince.plot(), prince.var.plot()) as well as clusters
from HCA (hca.plot(), hca.test()).
The distribution of association of sample annotations to the features can be
plotted and analysed (feature.assoc(), dense.plot(), corrected.p()).
Batch surrogate variables might be associated with biological annotations,
hence making batch adjustment of the data problematic. The associations between
the sample annotations can be calculated and plotted (confounding()).
If unwanted batch effects have been identified in the previous steps, two
simple methods are provided to adjust the data (quickadjust.zero(),
quickadjust.ref()). Another batch adjustment uses linear models,
with the advantage to remove numerical variables and several variables at
once (adjust.linearmodel()). In addition, the popular ComBat batch adjustment
has been adopted for the package to combine batches of small sample
size (combat()). Principal components confounded by batch variables can be
reomved from the data (kill.pc()). 
To use the functions of this package, the data matrix has to be a numeric
matrix and the sample annotations have to be a data.frame with numeric or
factors with 2 or more levels. NAs are allowed in both the data matrix and
the annotation dataframe, except for the functions adjust.linearmodel()
and combat(). 
</p>


<h3>Author(s)</h3>

<p>Martin Lauss
</p>
<p>Maintainer: Martin Lauss &lt;martin.lauss@med.lu.se&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##### first you need a dataset (matrix)
set.seed(100)
g&lt;-matrix(nrow=1000,ncol=50,rnorm(1000*50),dimnames=list(paste("Feature",1:1000),
          paste("Sample",1:50)))
g[1:100,26:50]&lt;-g[1:100,26:50]+1  ## lets put in some larger values
set.seed(200)
##### second you need sample annotations (data.frame)
o&lt;-data.frame(Factor1=factor(c(rep("A",25),rep("B",25))),
              Factor2=factor(rep(c("A","B"),25)),
              Numeric1=rnorm(50),row.names=colnames(g))  
              # Level "B" of Factor 1 marks the samples with the larger values
              


##### perform the functions

## principal components
res1&lt;-prince(g,o,top=10,permute=TRUE)
prince.plot(prince=res1) 
  # to plot p values of linear models: lm(principal components ~ sapmle annotations). 
  # to see if the variation in the data is associated with sample annotations.
res2&lt;-prince.var.plot(g,show.top=50,npermute=10) 
  # to see how many principal components carry informative variation

## hierarchical clustering analysis
hca.plot(g,o) 
  # to show a dendrogram with sample annotations below
res3&lt;-hca.test(g,o,dendcut=2,test="fisher") 
  # to test if the major clusters show differences in sample annotations

## feature associations
res4a&lt;-feature.assoc(g,o$Factor1,method="correlation") 
  # to calculate correlation between one sample annotation and each feature
res4b&lt;-feature.assoc(g,o$Factor1,method="t.test",g1=res4a$permuted.data) 
res4c&lt;-feature.assoc(g,o$Factor1,method="AUC",g1=res4a$permuted.data)
dense.plot(res4a) 
  # to plot the distribution of correlations in the observed data 
  # in comparison to permuted data
dense.plot(res4b)
dense.plot(res4c)
res5&lt;-corrected.p(res4a) 
  # to correct for multiple testing and find out how many features are 
  # significantly associated to the sample annotation
names(which(res5$padjust&lt;0.05))
names(which(res5$adjust.permute&lt;0.05))
names(which(res5$adjust.rank&lt;0.05))

## associations between sample annotations
res4&lt;-confounding(o,method="fisher") 
  # to see how biological and technical annotations are inter-related

## adjustment for batch effects
gadj1&lt;-quickadjust.zero(g,o$Factor1) 
  # to adjust for batches (o$Factor1) 
  # using median centering of the features for each batch
prince.plot(prince(gadj1,o,top=10))

gadj2&lt;-quickadjust.ref(g,o$Factor1,"B") 
  # to adjust for batches (o$Factor) 
  # by adjusting the median of the features to the median of a reference batch.
prince.plot(prince(gadj2$adjusted.data,o,top=10))

gadj3&lt;-kill.pc(g,pc=1)  
  # to remove one or more principal components (here pc1) from the data
prince.plot(prince(gadj3,o,top=10))

lin1&lt;-adjust.linearmodel(g,o$Numeric1)  
  # to adjust for numerical variables using a linear model
prince.plot(prince(lin1,o,top=10))

com1&lt;-combat(g,o$Factor1,batchcolumn=1)  
  # to use the empirical Bayes framework of ComBat, popular for small-sample sizes
prince.plot(prince(com1,o,top=10))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
