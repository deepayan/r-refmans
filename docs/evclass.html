<!DOCTYPE html><html><head><title>Help for package evclass</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {evclass}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calcAB'><p>Determination of optimal coefficients for computing weights of evidence in logistic regression</p></a></li>
<li><a href='#calcm'><p>Determination of optimal coefficients for computing weights of evidence in logistic regression</p></a></li>
<li><a href='#decision'><p>Decision rules for evidential classifiers</p></a></li>
<li><a href='#EkNNfit'><p>Training of the EkNN classifier</p></a></li>
<li><a href='#EkNNinit'><p>Initialization of parameters for the EkNN classifier</p></a></li>
<li><a href='#EkNNval'><p>Classification of a test set by the EkNN classifier</p></a></li>
<li><a href='#evclass'><p>evclass: A package for evidential classification</p></a></li>
<li><a href='#glass'><p>Glass dataset</p></a></li>
<li><a href='#ionosphere'><p>Ionosphere dataset</p></a></li>
<li><a href='#proDSfit'><p>Training of the evidential neural network classifier</p></a></li>
<li><a href='#proDSinit'><p>Initialization of parameters for the evidential neural network classifier</p></a></li>
<li><a href='#proDSval'><p>Classification of a test set by the evidential neural network classifier</p></a></li>
<li><a href='#RBFfit'><p>Training of a radial basis function classifier</p></a></li>
<li><a href='#RBFinit'><p>Initialization of parameters for a Radial Basis Function classifier</p></a></li>
<li><a href='#RBFval'><p>Classification of a test set by a radial basis function classifier</p></a></li>
<li><a href='#vehicles'><p>Vehicles dataset</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Evidential Distance-Based Classification</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-9</td>
</tr>
<tr>
<td>Author:</td>
<td>Thierry Denoeux</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Thierry Denoeux &lt;tdenoeux@utc.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Different evidential classifiers, which provide
    outputs in the form of Dempster-Shafer mass functions. The methods are: 
    the evidential K-nearest neighbor rule, the evidential neural 
    network, radial basis function neural networks, logistic regression,
    feed-forward neural networks.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>FNN, ibelief, R.utils</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr,rmarkdown,datasets,stats,nnet</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-09 09:39:44 UTC; Thierry</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-09 11:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='calcAB'>Determination of optimal coefficients for computing weights of evidence in logistic regression</h2><span id='topic+calcAB'></span>

<h3>Description</h3>

<p><code>calcAB</code> computes optimal coefficients alpha and beta needed to transform coefficients
from logistic regression (or connections weights between the last hidden layer and the output
layer of multilayer neural networks) into weights of evidence. These weights of evidence
can then be used to express the outputs of logistic regression or multilayer neural networks
as &quot;latent&quot; mass functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcAB(W, mu = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcAB_+3A_w">W</code></td>
<td>
<p>Vector of coefficients of length (d+1), where d is the number of features, in the
case of M=2 classes, or (d+1,M) matrix of coefficients (or connection weights) in the case
of M&gt;2 classes.</p>
</td></tr>
<tr><td><code id="calcAB_+3A_mu">mu</code></td>
<td>
<p>Optional vector containing the means of the d features.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements:
</p>

<dl>
<dt>A</dt><dd><p>Vector of length d (M=2) or matrix of size (d,M) (for M&gt;2) of coefficients alpha.</p>
</dd>
<dt>B</dt><dd><p>Vector of length d (M=2) or matrix of size (d,M) (for M&gt;2) of coefficients beta.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. Logistic Regression, Neural Networks and Dempster-Shafer Theory: a New Perspective.
Knowledge-Based Systems, Vol. 176, Pages 54–67, 2019.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calcm">calcm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example with 2 classes and logistic regression
data(ionosphere)
x&lt;-ionosphere$x[,-2]
y&lt;-ionosphere$y-1
fit&lt;-glm(y ~ x,family='binomial')
AB&lt;-calcAB(fit$coefficients,colMeans(x))
AB
## Example with K&gt;2 classes and multilayer neural network
library(nnet)
data(glass)
K&lt;-max(glass$y)
d&lt;-ncol(glass$x)
n&lt;-nrow(x)
x&lt;-scale(glass$x)
y&lt;-as.factor(glass$y)
p&lt;-3 # number of hidden units
fit&lt;-nnet(y~x,size=p)  # training a neural network with 3 hidden units
W1&lt;-matrix(fit$wts[1:(p*(d+1))],d+1,p) # Input-to-hidden weights
W2&lt;-matrix(fit$wts[(p*(d+1)+1):(p*(d+1) + K*(p+1))],p+1,K) # hidden-to-output weights
a1&lt;-cbind(rep(1,n),x)%*%W1  # hidden unit activations
o1&lt;-1/(1+exp(-a1)) # hidden unit outputs
AB&lt;-calcAB(W2,colMeans(o1))
AB
</code></pre>

<hr>
<h2 id='calcm'>Determination of optimal coefficients for computing weights of evidence in logistic regression</h2><span id='topic+calcm'></span>

<h3>Description</h3>

<p><code>calcAB</code> transforms coefficients alpha and beta computed by <code>calcm</code> into weights of
evidence, and then into mass and contour (plausibility) functions. These mass functions
can be used to express uncertainty about the prediction of logistic regression or multilayer
neural network classifiers (See Denoeux, 2019).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcm(x, A, B)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcm_+3A_x">x</code></td>
<td>
<p>Matrix (n,d) of feature values, where d is the number of features, and n is the number
of observations. Can be a vector if $d=1$.</p>
</td></tr>
<tr><td><code id="calcm_+3A_a">A</code></td>
<td>
<p>Vector of length d (for M=2) or matrix of size (d,M) (for M&gt;2) of coefficients alpha.</p>
</td></tr>
<tr><td><code id="calcm_+3A_b">B</code></td>
<td>
<p>Vector of length d (for M=2) or matrix of size (d,M) (for M&gt;2) of coefficients beta</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An error may occur if the absolute values of some coefficients are too high. It is then advised
to recompute these coefficients by training the logistic regression or neural network classifier
with L2 regularization. With M classes, the output mass functions have 2^M focal sets.
Using this function with large M may cause memory issues.
</p>


<h3>Value</h3>

<p>A list with six elements:
</p>

<dl>
<dt>F</dt><dd><p>Matrix (2^M,M) of focal sets.</p>
</dd>
<dt>mass</dt><dd><p>Matrix (n,2^M) of mass functions (one in each row).</p>
</dd>
<dt>pl</dt><dd><p>Matrix (n,M) containing the plausibilities of singletons.</p>
</dd>
<dt>bel</dt><dd><p>Matrix (n,M) containing the degrees of belief of singletons.</p>
</dd>
<dt>prob</dt><dd><p>Matrix (n,M) containing the normalized plausibilities of singletons.</p>
</dd>
<dt>conf</dt><dd><p>Vector of length n containing the degrees of conflict.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. Logistic Regression, Neural Networks and Dempster-Shafer Theory: a New Perspective.
Knowledge-Based Systems, Vol. 176, Pages 54–67, 2019.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calcAB">calcAB</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example with 2 classes and logistic regression
data(ionosphere)
x&lt;-ionosphere$x[,-2]
y&lt;-ionosphere$y-1
fit&lt;-glm(y ~ x,family='binomial')
AB&lt;-calcAB(fit$coefficients,colMeans(x))
Bel&lt;-calcm(x,AB$A,AB$B)
Bel$focal
Bel$mass[1:5,]
Bel$pl[1:5,]
Bel$conf[1:5]
## Example with K&gt;2 classes and multilayer neural network
library(nnet)
data(glass)
K&lt;-max(glass$y)
d&lt;-ncol(glass$x)
n&lt;-nrow(x)
x&lt;-scale(glass$x)
y&lt;-as.factor(glass$y)
p&lt;-3 # number of hidden units
fit&lt;-nnet(y~x,size=p)  # training a neural network with 3 hidden units
W1&lt;-matrix(fit$wts[1:(p*(d+1))],d+1,p) # Input-to-hidden weights
W2&lt;-matrix(fit$wts[(p*(d+1)+1):(p*(d+1) + K*(p+1))],p+1,K) # hidden-to-output weights
a1&lt;-cbind(rep(1,n),x)%*%W1  # hidden unit activations
o1&lt;-1/(1+exp(-a1)) # hidden unit outputs
AB&lt;-calcAB(W2,colMeans(o1))
Bel&lt;-calcm(o1,AB$A,AB$B)
Bel$focal
Bel$mass[1:5,]
Bel$pl[1:5,]
Bel$conf[1:5]
</code></pre>

<hr>
<h2 id='decision'>Decision rules for evidential classifiers</h2><span id='topic+decision'></span>

<h3>Description</h3>

<p><code>decision</code> returns decisions from a loss matrix and mass functions computed
by an evidential classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decision(
  m,
  L = 1 - diag(ncol(m) - 1),
  rule = c("upper", "lower", "pignistic", "hurwicz"),
  rho = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decision_+3A_m">m</code></td>
<td>
<p>Matrix of masses for n test cases. Each row is a mass function. The first M columns
correspond to the mass assigned to each of the M classes. The last column
corresponds to the mass assigned to the whole set of classes.</p>
</td></tr>
<tr><td><code id="decision_+3A_l">L</code></td>
<td>
<p>The loss matrix of dimension (M,na) or (M+1,na), where na is the number
of actions. L[k,j] is the loss incurred if action j is chosen and the true class
is <code class="reqn">\omega_k</code>. If L has M+1 rows, the last row corresponds to the unknown
class.</p>
</td></tr>
<tr><td><code id="decision_+3A_rule">rule</code></td>
<td>
<p>Decision rule to be used. Must be one of these: 'upper' (upper
expectation), 'lower' (lower expectations), 'pignistic' (pignistic expectation),
'hurwicz' (weighted sum of the lower and upper expectations).</p>
</td></tr>
<tr><td><code id="decision_+3A_rho">rho</code></td>
<td>
<p>Parameter between 0 and 1. Used only is rule='hurwicz'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the decision rules described in Denoeux (1997), with an
arbitrary loss function. The decision rules are the minimization of the lower,
upper or pignistic expectation, and Jaffray's decision rule based on minimizing a
convex combination of the lower and upper expectations. The function also handles
the case where there is an &quot;unknown&quot; class, in addition to the classes represented
in the training set.
</p>


<h3>Value</h3>

<p>A n-vector with the decisions (integers between 1 and na).
</p>


<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. Analysis of evidence-theoretic decision rules for pattern
classification. Pattern Recognition, 30(7):1095&ndash;1107, 1997.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EkNNval">EkNNval</a></code>, <code><a href="#topic+proDSval">proDSval</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example with M=2 classes
m&lt;-matrix(c(0.9,0.1,0,0.4,0.6,0,0.1,0.1,0.8),3,3,byrow=TRUE)
## Loss matrix with na=4 acts: assignment to class 1, assignment to class2,
# rejection, and assignment to the unknown class.
L&lt;-matrix(c(0,1,1,1,0,1,0.2,0.2,0.2,0.25,0.25,0),3,4)
d&lt;-decision(m,L,'upper') ## instances 2 and 3 are rejected
d&lt;-decision(m,L,'lower') ## instance 2 is rejected, instance 3 is
# assigned to the unknown class

</code></pre>

<hr>
<h2 id='EkNNfit'>Training of the EkNN classifier</h2><span id='topic+EkNNfit'></span>

<h3>Description</h3>

<p><code>EkNNfit</code> optimizes the parameters of the EkNN classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EkNNfit(
  x,
  y,
  K,
  param = NULL,
  alpha = 0.95,
  lambda = 1/max(as.numeric(y)),
  optimize = TRUE,
  options = list(maxiter = 300, eta = 0.1, gain_min = 1e-06, disp = TRUE)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EkNNfit_+3A_x">x</code></td>
<td>
<p>Input matrix of size n x d, where n is the number of objects and d the number of
attributes.</p>
</td></tr>
<tr><td><code id="EkNNfit_+3A_y">y</code></td>
<td>
<p>Vector of class labels (of length n). May be a factor, or a vector of
integers from 1 to M (number of classes).</p>
</td></tr>
<tr><td><code id="EkNNfit_+3A_k">K</code></td>
<td>
<p>Number of neighbors.</p>
</td></tr>
<tr><td><code id="EkNNfit_+3A_param">param</code></td>
<td>
<p>Initial parameters (default: NULL).</p>
</td></tr>
<tr><td><code id="EkNNfit_+3A_alpha">alpha</code></td>
<td>
<p>Parameter <code class="reqn">\alpha</code> (default: 0.95)</p>
</td></tr>
<tr><td><code id="EkNNfit_+3A_lambda">lambda</code></td>
<td>
<p>Parameter of the cost function. If <code>lambda=1</code>, the
cost function measures the error between the plausibilities and the 0-1 target values.
If <code>lambda=1/M</code>, where M is the number of classes (default), the piginistic probabilities
are considered in the cost function. If <code>lambda=0</code>, the beliefs are used.</p>
</td></tr>
<tr><td><code id="EkNNfit_+3A_optimize">optimize</code></td>
<td>
<p>Boolean. If TRUE (default), the parameters are optimized.</p>
</td></tr>
<tr><td><code id="EkNNfit_+3A_options">options</code></td>
<td>
<p>A list of parameters for the optimization algorithm: maxiter
(maximum number of iterations), eta (initial step of gradient variation),
gain_min (minimum gain in the optimisation loop), disp (Boolean; if TRUE, intermediate
results are displayed during the optimization).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the argument <code>param</code> is not supplied, the function <code><a href="#topic+EkNNinit">EkNNinit</a></code> is called.
</p>


<h3>Value</h3>

<p>A list with five elements:
</p>

<dl>
<dt>param</dt><dd><p>The optimized parameters.</p>
</dd>
<dt>cost</dt><dd><p>Final value of the cost function.</p>
</dd>
<dt>err</dt><dd><p>Leave-one-out error rate.</p>
</dd>
<dt>ypred</dt><dd><p>Leave-one-out predicted class labels (coded as integers from 1 to M).</p>
</dd>
<dt>m</dt><dd><p>Leave-one-out predicted mass functions. The first M columns correspond
to the mass assigned to each class. The last column corresponds to the mass
assigned to the whole set of classes.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. A k-nearest neighbor classification rule based on Dempster-Shafer
theory. IEEE Transactions on Systems, Man and Cybernetics, 25(05):804&ndash;813, 1995.
</p>
<p>L. M. Zouhal and T. Denoeux. An evidence-theoretic k-NN rule with parameter
optimization. IEEE Transactions on Systems, Man and Cybernetics Part C,
28(2):263&ndash;271,1998.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EkNNinit">EkNNinit</a></code>, <code><a href="#topic+EkNNval">EkNNval</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Iris dataset
data(iris)
x&lt;-iris[,1:4]
y&lt;-iris[,5]
fit&lt;-EkNNfit(x,y,K=5)
</code></pre>

<hr>
<h2 id='EkNNinit'>Initialization of parameters for the EkNN classifier</h2><span id='topic+EkNNinit'></span>

<h3>Description</h3>

<p><code>EkNNinit</code> returns initial parameter values for the EkNN classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EkNNinit(x, y, alpha = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EkNNinit_+3A_x">x</code></td>
<td>
<p>Input matrix of size n x d, where n is the number of objects and d the number of
attributes.</p>
</td></tr>
<tr><td><code id="EkNNinit_+3A_y">y</code></td>
<td>
<p>Vector of class lables (of length n). May be a factor, or a vector of
integers from 1 to M (number of classes).</p>
</td></tr>
<tr><td><code id="EkNNinit_+3A_alpha">alpha</code></td>
<td>
<p>Parameter <code class="reqn">\alpha</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each parameter <code class="reqn">\gamma_k</code> is set ot the inverse of the square root of the mean
Euclidean distances wihin class k. Note that <code class="reqn">\gamma_k</code> here is the square root
of the <code class="reqn">\gamma_k</code> as defined in (Zouhal and Denoeux, 1998). By default, parameter alpha is set
to 0.95. This value normally does not have to be changed.
</p>


<h3>Value</h3>

<p>A list with two elements:
</p>

<dl>
<dt>gamma</dt><dd><p>Vector of parameters <code class="reqn">\gamma_k</code>, of length c, the number of classes.</p>
</dd>
<dt>alpha</dt><dd><p>Parameter <code class="reqn">\alpha</code>, set to 0.95.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. A k-nearest neighbor classification rule based on Dempster-Shafer
theory. IEEE Transactions on Systems, Man and Cybernetics, 25(05):804&ndash;813, 1995.
</p>
<p>L. M. Zouhal and T. Denoeux. An evidence-theoretic k-NN rule with parameter
optimization. IEEE Transactions on Systems, Man and Cybernetics Part C,
28(2):263&ndash;271,1998.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EkNNfit">EkNNfit</a></code>, <code><a href="#topic+EkNNval">EkNNval</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Iris dataset
data(iris)
x&lt;-iris[,1:4]
y&lt;-iris[,5]
param&lt;-EkNNinit(x,y)
param
</code></pre>

<hr>
<h2 id='EkNNval'>Classification of a test set by the EkNN classifier</h2><span id='topic+EkNNval'></span>

<h3>Description</h3>

<p><code>EkNNval</code> classifies instances in a test set using the EkNN classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EkNNval(xtrain, ytrain, xtst, K, ytst = NULL, param = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EkNNval_+3A_xtrain">xtrain</code></td>
<td>
<p>Matrix of size ntrain x d, containing the values of the d attributes for the
training data.</p>
</td></tr>
<tr><td><code id="EkNNval_+3A_ytrain">ytrain</code></td>
<td>
<p>Vector of class labels for the training data (of length ntrain). May
be a factor, or a vector of integers from 1 to M (number of classes).</p>
</td></tr>
<tr><td><code id="EkNNval_+3A_xtst">xtst</code></td>
<td>
<p>Matrix of size ntst x d, containing the values of the d attributes for the
test data.</p>
</td></tr>
<tr><td><code id="EkNNval_+3A_k">K</code></td>
<td>
<p>Number of neighbors.</p>
</td></tr>
<tr><td><code id="EkNNval_+3A_ytst">ytst</code></td>
<td>
<p>Vector of class labels for the test data (optional). May
be a factor, or a vector of integers from 1 to M (number of classes).</p>
</td></tr>
<tr><td><code id="EkNNval_+3A_param">param</code></td>
<td>
<p>Parameters, as returned by <code><a href="#topic+EkNNfit">EkNNfit</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If class labels for the test set are provided, the test error rate is also returned.
If parameters are not supplied, they are given default values by <code><a href="#topic+EkNNinit">EkNNinit</a></code>.
</p>


<h3>Value</h3>

<p>A list with three elements:
</p>

<dl>
<dt>m</dt><dd><p>Predicted mass functions for the test data. The first M columns correspond
to the mass assigned to each class. The last column corresponds to the mass
assigned to the whole set of classes.</p>
</dd>
<dt>ypred</dt><dd><p>Predicted class labels for the test data (coded as integers from 1 to M).</p>
</dd>
<dt>err</dt><dd><p>Test error rate.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. A k-nearest neighbor classification rule based on Dempster-Shafer
theory. IEEE Transactions on Systems, Man and Cybernetics, 25(05):804&ndash;813, 1995.
</p>
<p>L. M. Zouhal and T. Denoeux. An evidence-theoretic k-NN rule with parameter
optimization. IEEE Transactions on Systems, Man and Cybernetics Part C,
28(2):263&ndash;271,1998.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EkNNinit">EkNNinit</a></code>, <code><a href="#topic+EkNNfit">EkNNfit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Iris dataset
data(iris)
train&lt;-sample(150,100)
xtrain&lt;-iris[train,1:4]
ytrain&lt;-iris[train,5]
xtst&lt;-iris[-train,1:4]
ytst&lt;-iris[-train,5]
K&lt;-5
fit&lt;-EkNNfit(xtrain,ytrain,K)
test&lt;-EkNNval(xtrain,ytrain,xtst,K,ytst,fit$param)
</code></pre>

<hr>
<h2 id='evclass'>evclass: A package for evidential classification</h2><span id='topic+evclass'></span>

<h3>Description</h3>

<p>The evclass package currently contains functions for three evidential classifiers: the evidential
K-nearest neighbor (EK-NN) rule (Denoeux, 1995; Zouhal and Denoeux, 1998), the evidential
neural network (Denoeux, 2000) and the RBF classifier with weight-of-evidence interpretation
(Denoeux, 2019; Huang et al., 2022), as well as methods to compute output mass functions from
trained logistic regression or multilayer classifiers as described in (Denoeux, 2019). In contrast
with classical statistical classifiers, evidential classifiers quantify the uncertainty of the
classification using Dempster-Shafer mass functions.
</p>


<h3>Details</h3>

<p>The main functions are: <code><a href="#topic+EkNNinit">EkNNinit</a></code>, <code><a href="#topic+EkNNfit">EkNNfit</a></code> and <code><a href="#topic+EkNNval">EkNNval</a></code>
for the initialization, training and evaluation of the EK-NN classifier;
<code><a href="#topic+proDSinit">proDSinit</a></code>, <code><a href="#topic+proDSfit">proDSfit</a></code> and <code><a href="#topic+proDSval">proDSval</a></code> for the
evidential neural network classifier; <code><a href="#topic+decision">decision</a></code> for decision-making;
<code><a href="#topic+RBFinit">RBFinit</a></code>, <code><a href="#topic+RBFfit">RBFfit</a></code> and <code><a href="#topic+RBFval">RBFval</a></code> for the RBF classifier;
<code><a href="#topic+calcAB">calcAB</a></code> and <code><a href="#topic+calcm">calcm</a></code> for computing output mass functions from trained
logistic regression or multilayer classifiers.
</p>


<h3>References</h3>

<p>T. Denoeux. A k-nearest neighbor classification rule based on Dempster-Shafer
theory. IEEE Transactions on Systems, Man and Cybernetics, 25(05):804&ndash;813, 1995.
</p>
<p>T. Denoeux. Analysis of evidence-theoretic decision rules for pattern
classification. Pattern Recognition, 30(7):1095&ndash;1107, 1997.
</p>
<p>T. Denoeux. A neural network classifier based on Dempster-Shafer theory.
IEEE Trans. on Systems, Man and Cybernetics A, 30(2):131&ndash;150, 2000.
</p>
<p>L. M. Zouhal and T. Denoeux. An evidence-theoretic k-NN rule with parameter
optimization. IEEE Transactions on Systems, Man and Cybernetics Part C,
28(2):263&ndash;271,1998.
</p>
<p>T. Denoeux. Logistic Regression, Neural Networks and Dempster-Shafer Theory: a New Perspective.
Knowledge-Based Systems, Vol. 176, Pages 54–67, 2019.
</p>
<p>L., S. Ruan, P. Decazes and T. Denoeux. Lymphoma segmentation from 3D PET-CT images using a
deep evidential network. International Journal of Approximate Reasoning, Vol. 149, Pages 39-60,
2022.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EkNNinit">EkNNinit</a></code>, <code><a href="#topic+EkNNfit">EkNNfit</a></code>,
<code><a href="#topic+EkNNval">EkNNval</a></code>, <code><a href="#topic+proDSinit">proDSinit</a></code>, <code><a href="#topic+proDSfit">proDSfit</a></code>, <code><a href="#topic+proDSval">proDSval</a></code>,
<code><a href="#topic+RBFinit">RBFinit</a></code>, <code><a href="#topic+RBFfit">RBFfit</a></code> and <code><a href="#topic+RBFval">RBFval</a></code>, <code><a href="#topic+decision">decision</a></code>,
<code><a href="#topic+calcAB">calcAB</a></code>, <code><a href="#topic+calcm">calcm</a></code>.
</p>

<hr>
<h2 id='glass'>Glass dataset</h2><span id='topic+glass'></span>

<h3>Description</h3>

<p>This data set contains the description of 214 fragments of glass originally
collected for a study in the context of criminal investigation. Each fragment has a measured
reflectivity index and chemical composition (weight percent of Na, Mg, Al, Si, K, Ca, Ba and Fe).
As suggested by Ripley (1994), 29 instances were discarded, and the remaining 185
were re-grouped in four classes: window float glass (70), window non-float glass (76), vehicle window
glass (17) and other (22). The data set was split randomly in a training set of size 89 and a test
set of size 96.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(glass)
</code></pre>


<h3>Format</h3>

<p>A list with two elements:
</p>

<dl>
<dt>x</dt><dd><p>The 185 x 9 object-attribute matrix.</p>
</dd>
<dt>y</dt><dd><p>A 185-vector containing the class labels.</p>
</dd>
</dl>



<h3>References</h3>

<p>P. M.  Murphy and D. W. Aha.  UCI Reposition of machine learning databases.
[Machine readable data repository]. University of California, Departement of
Information and Computer Science, Irvine, CA.
</p>
<p>B.D.Ripley, Flexible nonlinear approaches to classification, in &quot;From Statistics
to Neural Networks&quot;, V. Cherkassly, J. H. Friedman, and H. Wechsler, Eds.,
Berlin, Germany: Springer-Verlag, 1994, pp. 105&ndash;126.
</p>
<p>T. Denoeux. A neural network classifier based on Dempster-Shafer theory.
IEEE Trans. on Systems, Man and Cybernetics A, 30(2):131&ndash;150, 2000.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(glass)
table(glass$y)
</code></pre>

<hr>
<h2 id='ionosphere'>Ionosphere dataset</h2><span id='topic+ionosphere'></span>

<h3>Description</h3>

<p>This dataset was collected by a radar system and consists of phased array of 16
high-frequency antennas with a total transmitted power of the order of 6.4 kilowatts.
The targets were free electrons in the ionosphere. &quot;Good&quot; radar returns are those
showing evidence of some type of structure in the ionosphere. &quot;Bad&quot; returns are those
that do not.  There are 351 instances and 34 numeric attributes. The first 175
instances are training data, the rest are test data. This version of dataset was
used by Zouhal and Denoeux (1998).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ionosphere)
</code></pre>


<h3>Format</h3>

<p>A list with two elements:
</p>

<dl>
<dt>x</dt><dd><p>The 351 x 34 object-attribute matrix.</p>
</dd>
<dt>y</dt><dd><p>A 351-vector containing the class labels.</p>
</dd>
</dl>



<h3>References</h3>

<p>P. M.  Murphy and D. W. Aha.  UCI Reposition of machine learning databases.
[Machine readable data repository]. University of California, Departement of
Information and Computer Science, Irvine, CA.
</p>
<p>L. M. Zouhal and T. Denoeux. An evidence-theoretic k-NN rule with parameter
optimization. IEEE Transactions on Systems, Man and Cybernetics Part C,
28(2):263&ndash;271,1998.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ionosphere)
table(vehicles$y)
</code></pre>

<hr>
<h2 id='proDSfit'>Training of the evidential neural network classifier</h2><span id='topic+proDSfit'></span>

<h3>Description</h3>

<p><code>proDSfit</code> performs parameter optimization for the evidential neural network classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proDSfit(
  x,
  y,
  param,
  lambda = 1/max(as.numeric(y)),
  mu = 0,
  optimProto = TRUE,
  options = list(maxiter = 500, eta = 0.1, gain_min = 1e-04, disp = 10)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proDSfit_+3A_x">x</code></td>
<td>
<p>Input matrix of size n x d, where n is the number of objects and d the number of
attributes.</p>
</td></tr>
<tr><td><code id="proDSfit_+3A_y">y</code></td>
<td>
<p>Vector of class lables (of length n). May be a factor, or a vector of
integers from 1 to M (number of classes).</p>
</td></tr>
<tr><td><code id="proDSfit_+3A_param">param</code></td>
<td>
<p>Initial parameters (see <code>link{proDSinit}</code>).</p>
</td></tr>
<tr><td><code id="proDSfit_+3A_lambda">lambda</code></td>
<td>
<p>Parameter of the cost function. If <code>lambda=1</code>, the
cost function measures the error between the plausibilities and the 0-1 target values.
If <code>lambda=1/M</code>, where M is the number of classes (default), the piginistic probabilities
are considered in the cost function. If <code>lambda=0</code>, the beliefs are used.</p>
</td></tr>
<tr><td><code id="proDSfit_+3A_mu">mu</code></td>
<td>
<p>Regularization hyperparameter (default=0).</p>
</td></tr>
<tr><td><code id="proDSfit_+3A_optimproto">optimProto</code></td>
<td>
<p>Boolean. If TRUE, the prototypes are optimized (default). Otherwise, they are fixed.</p>
</td></tr>
<tr><td><code id="proDSfit_+3A_options">options</code></td>
<td>
<p>A list of parameters for the optimization algorithm: maxiter
(maximum number of iterations), eta (initial step of gradient variation),
gain_min (minimum gain in the optimisation loop), disp (integer; if &gt;0, intermediate
results are displayed every disp iterations).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>optimProto=TRUE</code> (default), the prototypes are optimized. Otherwise, they are fixed to
their initial value.
</p>


<h3>Value</h3>

<p>A list with three elements:
</p>

<dl>
<dt>param</dt><dd><p>Optimized network parameters.</p>
</dd>
<dt>cost</dt><dd><p>Final value of the cost function.</p>
</dd>
<dt>err</dt><dd><p>Training error rate.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. A neural network classifier based on Dempster-Shafer theory.
IEEE Trans. on Systems, Man and Cybernetics A, 30(2):131&ndash;150, 2000.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+proDSinit">proDSinit</a></code>, <code><a href="#topic+proDSval">proDSval</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Glass dataset
data(glass)
xapp&lt;-glass$x[1:89,]
yapp&lt;-glass$y[1:89]
xtst&lt;-glass$x[90:185,]
ytst&lt;-glass$y[90:185]
## Initialization
param0&lt;-proDSinit(xapp,yapp,nproto=7)
## Training
fit&lt;-proDSfit(xapp,yapp,param0)
</code></pre>

<hr>
<h2 id='proDSinit'>Initialization of parameters for the evidential neural network classifier</h2><span id='topic+proDSinit'></span>

<h3>Description</h3>

<p><code>proDSinit</code> returns initial parameter values for the evidential neural network classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proDSinit(x, y, nproto, nprotoPerClass = FALSE, crisp = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proDSinit_+3A_x">x</code></td>
<td>
<p>Input matrix of size n x d, where n is the number of objects and d the number of
attributes.</p>
</td></tr>
<tr><td><code id="proDSinit_+3A_y">y</code></td>
<td>
<p>Vector of class labels (of length n). May be a factor, or a vector of
integers from 1 to M (number of classes).</p>
</td></tr>
<tr><td><code id="proDSinit_+3A_nproto">nproto</code></td>
<td>
<p>Number of prototypes.</p>
</td></tr>
<tr><td><code id="proDSinit_+3A_nprotoperclass">nprotoPerClass</code></td>
<td>
<p>Boolean. If TRUE, there are <code>nproto</code> prototypes per class. If
FALSE (default), the total number of prototypes is equal to <code>nproto</code>.</p>
</td></tr>
<tr><td><code id="proDSinit_+3A_crisp">crisp</code></td>
<td>
<p>Boolean. If TRUE, the prototypes have full membership to only one class. (Available only if
nprotoPerClass=TRUE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prototypes are initialized by the k-means algorithms. The initial membership values <code class="reqn">u_{ik}</code> of
each prototype <code class="reqn">p_i</code> to class <code class="reqn">\omega_k</code> are normally defined as the proportion of training samples
from class <code class="reqn">\omega_k</code> in the neighborhood of prototype <code class="reqn">p_i</code>. If arguments <code>crisp</code> and
<code>nprotoPerClass</code> are set to TRUE, the prototypes are assigned to one and only one class.
</p>


<h3>Value</h3>

<p>A list with four elements containing the initialized network parameters
</p>

<dl>
<dt>alpha</dt><dd><p>Vector of length r, where r is the number of prototypes.</p>
</dd>
<dt>gamma</dt><dd><p>Vector of length r</p>
</dd>
<dt>beta</dt><dd><p>Matrix of size (r,M), where M is the number of classes.</p>
</dd>
<dt>W</dt><dd><p>Matrix of size (r,d), containing the prototype coordinates.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. A neural network classifier based on Dempster-Shafer theory.
IEEE Trans. on Systems, Man and Cybernetics A, 30(2):131&ndash;150, 2000.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+proDSfit">proDSfit</a></code>, <code><a href="#topic+proDSval">proDSval</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Glass dataset
data(glass)
xapp&lt;-glass$x[1:89,]
yapp&lt;-glass$y[1:89]
param0&lt;-proDSinit(xapp,yapp,nproto=7)
param0
</code></pre>

<hr>
<h2 id='proDSval'>Classification of a test set by the evidential neural network classifier</h2><span id='topic+proDSval'></span>

<h3>Description</h3>

<p><code>proDSval</code> classifies instances in a test set using the evidential neural network classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proDSval(x, param, y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proDSval_+3A_x">x</code></td>
<td>
<p>Matrix of size n x d, containing the values of the d attributes for the test data.</p>
</td></tr>
<tr><td><code id="proDSval_+3A_param">param</code></td>
<td>
<p>Neural network parameters, as provided by <code><a href="#topic+proDSfit">proDSfit</a></code>.</p>
</td></tr>
<tr><td><code id="proDSval_+3A_y">y</code></td>
<td>
<p>Optional vector of class labels for the test data. May be a factor, or a vector of
integers from 1 to M (number of classes).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If class labels for the test set are provided, the test error rate is also returned.
</p>


<h3>Value</h3>

<p>A list with three elements:
</p>

<dl>
<dt>m</dt><dd><p>Predicted mass functions for the test data. The first M columns correspond
to the mass assigned to each class. The last column corresponds to the mass
assigned to the whole set of classes.</p>
</dd>
<dt>ypred</dt><dd><p>Predicted class labels for the test data.</p>
</dd>
<dt>err</dt><dd><p>Test error rate (if the class label of test data has been provided).</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. A neural network classifier based on Dempster-Shafer theory.
IEEE Trans. on Systems, Man and Cybernetics A, 30(2):131&ndash;150, 2000.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+proDSinit">proDSinit</a></code>, <code><a href="#topic+proDSfit">proDSfit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Glass dataset
data(glass)
xapp&lt;-glass$x[1:89,]
yapp&lt;-glass$y[1:89]
xtst&lt;-glass$x[90:185,]
ytst&lt;-glass$y[90:185]
## Initialization
param0&lt;-proDSinit(xapp,yapp,nproto=7)
## Training
fit&lt;-proDSfit(xapp,yapp,param0)
## Test
val&lt;-proDSval(xtst,fit$param,ytst)
## Confusion matrix
table(ytst,val$ypred)
</code></pre>

<hr>
<h2 id='RBFfit'>Training of a radial basis function classifier</h2><span id='topic+RBFfit'></span>

<h3>Description</h3>

<p><code>RBFfit</code> performs parameter optimization for a radial basis function (RBF) classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RBFfit(
  x,
  y,
  param,
  lambda = 0,
  control = list(fnscale = -1, trace = 2, maxit = 1000),
  optimProto = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RBFfit_+3A_x">x</code></td>
<td>
<p>Input matrix of size n x d, where n is the number of objects and d the number of
attributes.</p>
</td></tr>
<tr><td><code id="RBFfit_+3A_y">y</code></td>
<td>
<p>Vector of class labels (of length n). May be a factor, or a vector of
integers from 1 to M (number of classes).</p>
</td></tr>
<tr><td><code id="RBFfit_+3A_param">param</code></td>
<td>
<p>Initial parameters (see <code><a href="#topic+RBFinit">RBFinit</a></code>).</p>
</td></tr>
<tr><td><code id="RBFfit_+3A_lambda">lambda</code></td>
<td>
<p>Regularization hyperparameter (default=0).</p>
</td></tr>
<tr><td><code id="RBFfit_+3A_control">control</code></td>
<td>
<p>Parameters passed to function <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code id="RBFfit_+3A_optimproto">optimProto</code></td>
<td>
<p>Boolean. If TRUE, the prototypes are optimized (default). Otherwise,
they are fixed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The RBF neural network is trained by maximizing the conditional log-likelihood (or, equivalently,
by minimizing the cross-entropy loss function). The optimization procedure is the BFGS
algorithm implemented in function <code>optim</code>.
</p>


<h3>Value</h3>

<p>A list with three elements:
</p>

<dl>
<dt>param</dt><dd><p>Optimized network parameters.</p>
</dd>
<dt>loglik</dt><dd><p>Final value of the log-likelihood objective function.</p>
</dd>
<dt>err</dt><dd><p>Training error rate.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+proDSinit">proDSinit</a></code>, <code><a href="#topic+proDSval">proDSval</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Glass dataset
data(glass)
xapp&lt;-glass$x[1:89,]
yapp&lt;-glass$y[1:89]
## Initialization
param0&lt;-RBFinit(xapp,yapp,nproto=7)
## Training
fit&lt;-RBFfit(xapp,yapp,param0,control=list(fnscale=-1,trace=2))
</code></pre>

<hr>
<h2 id='RBFinit'>Initialization of parameters for a Radial Basis Function classifier</h2><span id='topic+RBFinit'></span>

<h3>Description</h3>

<p><code>RBFinit</code> returns initial parameter values for a Radial Basis Function classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RBFinit(x, y, nproto)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RBFinit_+3A_x">x</code></td>
<td>
<p>Input matrix of size n x d, where n is the number of objects and d the number of
attributes.</p>
</td></tr>
<tr><td><code id="RBFinit_+3A_y">y</code></td>
<td>
<p>Vector of class labels (of length n). May be a factor, or a vector of
integers from 1 to M (number of classes).</p>
</td></tr>
<tr><td><code id="RBFinit_+3A_nproto">nproto</code></td>
<td>
<p>Number of prototypes</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prototypes are initialized by the k-means algorithms. The hidden-to-output weights are initialized
by linear regression. The scale parameter for each prototype is computed as the inverse of the square
root of the mean squared distances to this prototype. The final number of prototypes may be different
from the desired number <code>nproto</code> depending on the result of the k-means clustering (clusters
composed of only one input vector are discarded).
</p>


<h3>Value</h3>

<p>A list with three elements containing the initialized network parameters
</p>

<dl>
<dt>P</dt><dd><p>Matrix of size (R,d), containing the R prototype coordinates.</p>
</dd>
<dt>Gamma</dt><dd><p>Vector of length R, containing the scale parameters.</p>
</dd>
<dt>W</dt><dd><p>Matrix of size (R,M), containing the hidden-to-output weights.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+RBFfit">RBFfit</a></code>, <code><a href="#topic+RBFval">RBFval</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Glass dataset
data(glass)
xapp&lt;-glass$x[1:89,]
yapp&lt;-glass$y[1:89]
param0&lt;-RBFinit(xapp,yapp,nproto=7)
param0
</code></pre>

<hr>
<h2 id='RBFval'>Classification of a test set by a radial basis function classifier</h2><span id='topic+RBFval'></span>

<h3>Description</h3>

<p><code>RBFval</code> classifies instances in a test set using a radial basis function classifier. Function
<code><a href="#topic+calcm">calcm</a></code> is called for computing output belief functions. It is recommended to set
<code>calc.belief=FALSE</code> when the number of classes is very large, to avoid memory problems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RBFval(x, param, y = NULL, calc.belief = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RBFval_+3A_x">x</code></td>
<td>
<p>Matrix of size n x d, containing the values of the d attributes for the test data.</p>
</td></tr>
<tr><td><code id="RBFval_+3A_param">param</code></td>
<td>
<p>Neural network parameters, as provided by <code><a href="#topic+RBFfit">RBFfit</a></code>.</p>
</td></tr>
<tr><td><code id="RBFval_+3A_y">y</code></td>
<td>
<p>Optional vector of class labels for the test data. May be a factor, or a vector of
integers from 1 to M (number of classes).</p>
</td></tr>
<tr><td><code id="RBFval_+3A_calc.belief">calc.belief</code></td>
<td>
<p>If TRUE (default), output belief functions are calculated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If class labels for the test set are provided, the test error rate is also returned.
</p>


<h3>Value</h3>

<p>A list with four elements:
</p>

<dl>
<dt>ypred</dt><dd><p>Predicted class labels for the test data.</p>
</dd>
<dt>err</dt><dd><p>Test error rate (if the class label of test data has been provided).</p>
</dd>
<dt>Prob</dt><dd><p>Output probabilities.</p>
</dd>
<dt>Belief</dt><dd><p>If <code>calc.belief=TRUE</code>, output belief function, provided as a list
output by function <code><a href="#topic+calcm">calcm</a></code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. Logistic Regression, Neural Networks and Dempster-Shafer Theory: a New Perspective.
Knowledge-Based Systems, Vol. 176, Pages 54–67, 2019.
</p>
<p>Ling Huang, Su Ruan, Pierre Decazes and Thierry Denoeux. Lymphoma segmentation from 3D PET-CT
images using a deep evidential network. International Journal of Approximate Reasoning,
Vol. 149, Pages 39-60, 2022.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+RBFinit">RBFinit</a></code>, <code><a href="#topic+RBFfit">RBFfit</a></code>, <code><a href="#topic+calcm">calcm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Glass dataset
data(glass)
xapp&lt;-glass$x[1:89,]
yapp&lt;-glass$y[1:89]
xtst&lt;-glass$x[90:185,]
ytst&lt;-glass$y[90:185]
## Initialization
param0&lt;-RBFinit(xapp,yapp,nproto=7)
## Training
fit&lt;-RBFfit(xapp,yapp,param0)
## Test
val&lt;-RBFval(xtst,fit$param,ytst)
## Confusion matrix
table(ytst,val$ypred)
</code></pre>

<hr>
<h2 id='vehicles'>Vehicles dataset</h2><span id='topic+vehicles'></span>

<h3>Description</h3>

<p>This dataset was collected from silhouettes by the HIPS (Hierarchical Image
Processing System) extension BINATTS  Four model vehicles were used for the
experiment: bus, Chevrolet van,  Saab 9000 and Opel Manta. The data were used to
distinguish 3D objects within a 2-D silhouette of the objects. There are 846
instances and 18 numeric attributes. The first 564 objects are training data,
the rest are test data. This version of dataset was used by Zouhal and Denoeux (1998).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(vehicles)
</code></pre>


<h3>Format</h3>

<p>A list with two elements:
</p>

<dl>
<dt>x</dt><dd><p>The 846 x 18 object-attribute matrix.</p>
</dd>
<dt>y</dt><dd><p>A 846-vector containing the class labels.</p>
</dd>
</dl>



<h3>References</h3>

<p>P. M.  Murphy and D. W. Aha.  UCI Reposition of machine learning databases.
[Machine readable data repository]. University of California, Departement of
Information and Computer Science, Irvine, CA.
</p>
<p>L. M. Zouhal and T. Denoeux. An evidence-theoretic k-NN rule with parameter
optimization. IEEE Transactions on Systems, Man and Cybernetics Part C,
28(2):263&ndash;271,1998.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vehicles)
table(vehicles$y)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
