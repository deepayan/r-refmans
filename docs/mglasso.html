<!DOCTYPE html><html lang="en"><head><title>Help for package mglasso</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mglasso}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adj_mat'><p>Adjacency matrix</p></a></li>
<li><a href='#beta_ols'><p>Initialize regression matrix</p></a></li>
<li><a href='#beta_to_vector'><p>Transform a matrix of regression coefficients to vector removing the diagonal</p></a></li>
<li><a href='#conesta'><p>CONESTA solver.</p></a></li>
<li><a href='#cost'><p><code>Mglasso</code> cost function</p></a></li>
<li><a href='#dist_beta'><p>Compute distance matrix between regression vectors</p></a></li>
<li><a href='#fun_lines'><p>weighted sum/difference of two regression vectors</p></a></li>
<li><a href='#image_sparse'><p>Plot the image of a matrix</p></a></li>
<li><a href='#install_pylearn_parsimony'><p>Install the python library pylearn-parsimony and other required libraries</p></a></li>
<li><a href='#merge_clusters'><p>compute clusters partition from pairs of variables to merge</p></a></li>
<li><a href='#mglasso'><p>Inference of Multiscale Gaussian Graphical Model.</p></a></li>
<li><a href='#plot_clusterpath'><p>Plot MGLasso Clusterpath</p></a></li>
<li><a href='#plot_mglasso'><p>Plot <code>mglasso</code> function output.</p></a></li>
<li><a href='#precision_to_regression'><p>Compute precision matrix from regression vectors</p></a></li>
<li><a href='#symmetrize'><p>Apply symmetrization on estimated graph</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Multiscale Graphical Lasso</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Inference of Multiscale graphical models with neighborhood
    selection approach.  The method is based on solving a convex
    optimization problem combining a Lasso and fused-group Lasso
    penalties.  This allows to infer simultaneously a conditional
    independence graph and a clustering partition. The optimization is
    based on the Continuation with Nesterov smoothing in a
    Shrinkage-Thresholding Algorithm solver (Hadj-Selem et al. 2018)
    &lt;<a href="https://doi.org/10.1109%2FTMI.2018.2829802">doi:10.1109/TMI.2018.2829802</a>&gt; implemented in python.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Imports:</td>
<td>corpcor, ggplot2, ggrepel, gridExtra, Matrix, methods,
R.utils, reticulate (&ge; 1.25), rstudioapi</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, mvtnorm, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Config/reticulate:</td>
<td>list( packages = list( list(package = "scipy",
version = "1.7.1"), list(package = "numpy", version =
"1.22.4"), list(package = "matplotlib"), list(package =
"scikit-learn"), list(package = "six"), list(package =
"pylearn-parsimony", version = "0.3.1", pip = TRUE) ) )</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://desanou.github.io/mglasso/">https://desanou.github.io/mglasso/</a></td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-09-05 22:06:02 UTC; doedm</td>
</tr>
<tr>
<td>Author:</td>
<td>Edmond Sanou [aut, cre],
  Tung Le [ctb],
  Christophe Ambroise [ths],
  Genevi√®ve Robin [ths]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Edmond Sanou &lt;doedmond.sanou@univ-evry.fr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-08 13:12:55 UTC</td>
</tr>
</table>
<hr>
<h2 id='adj_mat'>Adjacency matrix</h2><span id='topic+adj_mat'></span>

<h3>Description</h3>

<p>Adjacency matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adj_mat(mat, sym_rule = "and")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="adj_mat_+3A_mat">mat</code></td>
<td>
<p>matrix of regression coefficients</p>
</td></tr>
<tr><td><code id="adj_mat_+3A_sym_rule">sym_rule</code></td>
<td>
<p>symmetrization rule, either AND or OR</p>
</td></tr>
</table>


<h3>Value</h3>

<p>adjacency matrix
</p>

<hr>
<h2 id='beta_ols'>Initialize regression matrix</h2><span id='topic+beta_ols'></span>

<h3>Description</h3>

<p>Initialize regression matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>beta_ols(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="beta_ols_+3A_x">X</code></td>
<td>
<p>data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A zero-diagonal matrix of regression vectors.
</p>

<hr>
<h2 id='beta_to_vector'>Transform a matrix of regression coefficients to vector removing the diagonal</h2><span id='topic+beta_to_vector'></span>

<h3>Description</h3>

<p>Transform a matrix of regression coefficients to vector removing the diagonal
</p>


<h3>Usage</h3>

<pre><code class='language-R'>beta_to_vector(beta_mat)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="beta_to_vector_+3A_beta_mat">beta_mat</code></td>
<td>
<p>matrix of regressions vectors</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of all regression coefficients.
</p>

<hr>
<h2 id='conesta'>CONESTA solver.</h2><span id='topic+conesta'></span>

<h3>Description</h3>

<p>Solve the MGLasso optimization problem using CONESTA algorithm. Interface to
the pylearn.parsimony python library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conesta(
  X,
  lam1,
  lam2,
  beta_warm = c(0),
  type_ = "initial",
  W_ = NULL,
  mean_ = FALSE,
  max_iter_ = 10000,
  prec_ = 0.01
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="conesta_+3A_x">X</code></td>
<td>
<p>Data matrix nxp.</p>
</td></tr>
<tr><td><code id="conesta_+3A_lam1">lam1</code></td>
<td>
<p>Sparsity penalty.</p>
</td></tr>
<tr><td><code id="conesta_+3A_lam2">lam2</code></td>
<td>
<p>Total variation penalty.</p>
</td></tr>
<tr><td><code id="conesta_+3A_beta_warm">beta_warm</code></td>
<td>
<p>Warm initialization vector.</p>
</td></tr>
<tr><td><code id="conesta_+3A_type_">type_</code></td>
<td>
<p>Character scalar. By default set to initial version which doesn't
use weights</p>
</td></tr>
<tr><td><code id="conesta_+3A_w_">W_</code></td>
<td>
<p>Weights matrix for total variation penalties.</p>
</td></tr>
<tr><td><code id="conesta_+3A_mean_">mean_</code></td>
<td>
<p>Logical scalar. If TRUE weights the optimization function by the
inverse of sample size.</p>
</td></tr>
<tr><td><code id="conesta_+3A_max_iter_">max_iter_</code></td>
<td>
<p>Numeric scalar. Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="conesta_+3A_prec_">prec_</code></td>
<td>
<p>Numeric scalar. Tolerance for the stopping criterion (duality gap).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>COntinuation with NEsterov smoothing in a Shrinkage-Thresholding
Algorithm</em> (CONESTA, Hadj-Selem et al. 2018) <a href="doi:10.1109/TMI.2018.2829802">doi:10.1109/TMI.2018.2829802</a>
is an algorithm design for solving optimization problems including group-wise
penalties. This function is an interface with the python solver. The MGLasso
problem is first reformulated in a problem of the form </p>
<p style="text-align: center;"><code class="reqn">argmin 1/2 ||Y -
\tilde{X} \tilde{\beta}||_2^2 + \lambda_1 ||\tilde{\beta}||_1 + \lambda_2
\sum_{i&lt;j} ||\boldsymbol A_{ij} \tilde{\beta}||_2</code>
</p>
<p> where vector <code class="reqn">Y</code> is
the vectorized form of matrix <code class="reqn">X</code>.
</p>


<h3>Value</h3>

<p>Numeric matrix of size pxp. Line <code>k</code> of the matrix represents
the coefficients obtained from the L1-L2 penalized regression of variable
<code>k</code> on the others.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mglasso">mglasso()</a></code> for the MGLasso model estimation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: # because of installation of external packages during checks
mglasso::install_pylearn_parsimony(envname = "rmglasso", method = "conda")
reticulate::use_condaenv("rmglasso", required = TRUE)
reticulate::py_config()
n = 30
K = 2
p = 4
rho = 0.85
blocs &lt;- list()
for (j in 1:K) {
 bloc &lt;- matrix(rho, nrow = p/K, ncol = p/K)
   for(i in 1:(p/K)) { bloc[i,i] &lt;- 1 }
   blocs[[j]] &lt;- bloc
   }

mat.covariance &lt;- Matrix::bdiag(blocs)
mat.covariance
set.seed(11)
X &lt;- mvtnorm::rmvnorm(n, mean = rep(0,p), sigma = as.matrix(mat.covariance))
X &lt;- scale(X)
res &lt;- conesta(X, 0.1, 0.1)

## End(Not run)
</code></pre>

<hr>
<h2 id='cost'><code>Mglasso</code> cost function</h2><span id='topic+cost'></span>

<h3>Description</h3>

<p><code>cost</code> computes the cost function of <code>Mglasso</code> method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cost(beta, x, lambda1 = 0, lambda2 = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cost_+3A_beta">beta</code></td>
<td>
<p>p by p numeric matrix. In rows, regression vectors coefficients after node-wise regression. <code>diag(beta) = 0</code>.</p>
</td></tr>
<tr><td><code id="cost_+3A_x">x</code></td>
<td>
<p>n by p numeric matrix. Data with variables in columns.</p>
</td></tr>
<tr><td><code id="cost_+3A_lambda1">lambda1</code></td>
<td>
<p>numeric scalar. Lasso penalization parameter.</p>
</td></tr>
<tr><td><code id="cost_+3A_lambda2">lambda2</code></td>
<td>
<p>numeric scalar. Fused-group Lasso penalization parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric scalar. The cost.
</p>

<hr>
<h2 id='dist_beta'>Compute distance matrix between regression vectors</h2><span id='topic+dist_beta'></span>

<h3>Description</h3>

<p>Compute distance matrix between regression vectors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist_beta(beta, distance = "euclidean")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dist_beta_+3A_beta">beta</code></td>
<td>
<p>matrix of regression vectors</p>
</td></tr>
<tr><td><code id="dist_beta_+3A_distance">distance</code></td>
<td>
<p>euclidean or relative distance</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix of distances.
</p>

<hr>
<h2 id='fun_lines'>weighted sum/difference of two regression vectors</h2><span id='topic+fun_lines'></span>

<h3>Description</h3>

<p><code>fun_lines</code> applies function <code>fun</code> to regression vectors while reordering the coefficients,
such that the <code>j</code>-th coefficient in <code>beta[j, ]</code> is permuted with the <code>i</code>-th coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fun_lines(i, j, beta, fun = `-`, ni = 1, nj = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fun_lines_+3A_i">i</code></td>
<td>
<p>integer scalar. Index of the first vector.</p>
</td></tr>
<tr><td><code id="fun_lines_+3A_j">j</code></td>
<td>
<p>integer scalar. Index of the second vector.</p>
</td></tr>
<tr><td><code id="fun_lines_+3A_beta">beta</code></td>
<td>
<p>p by p numeric matrix. In rows, regression vectors coefficients after node-wise regression. <code>diag(beta) = 0</code>.</p>
</td></tr>
<tr><td><code id="fun_lines_+3A_fun">fun</code></td>
<td>
<p>function. Applied on lines.</p>
</td></tr>
<tr><td><code id="fun_lines_+3A_ni">ni</code></td>
<td>
<p>integer scalar. Weight for vector <code>i</code>.</p>
</td></tr>
<tr><td><code id="fun_lines_+3A_nj">nj</code></td>
<td>
<p>integer scalar. Weight for vector <code>j</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>beta &lt;- matrix(round(rnorm(9),2), ncol = 3)
diag(beta) &lt;- 0
beta
fun_lines(1, 2, beta)
fun_lines(2, 1, beta)
</code></pre>

<hr>
<h2 id='image_sparse'>Plot the image of a matrix</h2><span id='topic+image_sparse'></span>

<h3>Description</h3>

<p>Plot the image of a matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>image_sparse(matrix, main_ = "", sub_ = "", col_names = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="image_sparse_+3A_matrix">matrix</code></td>
<td>
<p>matrix of regression coefficients</p>
</td></tr>
<tr><td><code id="image_sparse_+3A_main_">main_</code></td>
<td>
<p>title</p>
</td></tr>
<tr><td><code id="image_sparse_+3A_sub_">sub_</code></td>
<td>
<p>subtitle</p>
</td></tr>
<tr><td><code id="image_sparse_+3A_col_names">col_names</code></td>
<td>
<p>columns names</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value.
</p>

<hr>
<h2 id='install_pylearn_parsimony'>Install the python library pylearn-parsimony and other required libraries</h2><span id='topic+install_pylearn_parsimony'></span>

<h3>Description</h3>

<p>pylearn-parsimony contains the solver CONESTA used for the mglasso problem
and is available on github at https://github.com/neurospin/pylearn-parsimony
It is advised to use a python version &quot;&gt;=3.7,&lt;3.10&quot;.
Indeed, the latest version of scipy under which mglasso was developped is scipy 1.7.1 which is based on python &quot;&gt;=3.7,&lt;3.10&quot;.
In turn, this version of scipy can only be associated with a version of numpy &quot;&gt;=1.16.5,&lt;1.23.0&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_pylearn_parsimony(
  method = c("auto", "virtualenv", "conda"),
  conda = "auto",
  extra_pack = c("scipy == 1.7.1", "scikit-learn", "numpy == 1.22.4", "six",
    "matplotlib"),
  python_version = "3.8",
  restart_session = TRUE,
  envname = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="install_pylearn_parsimony_+3A_method">method</code></td>
<td>
<p>Installation method. By default, &quot;auto&quot; automatically finds a
method that will work in the local environment. Change the default to force
a specific installation method. Note that the &quot;virtualenv&quot; method is not
available on Windows.</p>
</td></tr>
<tr><td><code id="install_pylearn_parsimony_+3A_conda">conda</code></td>
<td>
<p>The path to a <code>conda</code> executable. Use <code>"auto"</code> to allow
<code>reticulate</code> to automatically find an appropriate <code>conda</code> binary.
See <strong>Finding Conda</strong> and <code><a href="reticulate.html#topic+conda_binary">conda_binary()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="install_pylearn_parsimony_+3A_extra_pack">extra_pack</code></td>
<td>
<p>Character vector. Extra-packages to be installed.</p>
</td></tr>
<tr><td><code id="install_pylearn_parsimony_+3A_python_version">python_version</code></td>
<td>
<p>The requested Python version. Ignored when attempting
to install with a Python virtual environment.</p>
</td></tr>
<tr><td><code id="install_pylearn_parsimony_+3A_restart_session">restart_session</code></td>
<td>
<p>Restart R session after installing (note this will
only occur within RStudio)</p>
</td></tr>
<tr><td><code id="install_pylearn_parsimony_+3A_envname">envname</code></td>
<td>
<p>The name, or full path, of the environment in which Python
packages are to be installed. When <code>NULL</code> (the default), the active
environment as set by the <code>RETICULATE_PYTHON_ENV</code> variable will be used;
if that is unset, then the <code>r-reticulate</code> environment will be used.</p>
</td></tr>
<tr><td><code id="install_pylearn_parsimony_+3A_...">...</code></td>
<td>
<p>additionnal arguments passed to <code><a href="reticulate.html#topic+py_install">reticulate::py_install()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value.
</p>

<hr>
<h2 id='merge_clusters'>compute clusters partition from pairs of variables to merge</h2><span id='topic+merge_clusters'></span>

<h3>Description</h3>

<p>compute clusters partition from pairs of variables to merge
</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_clusters(pairs_to_merge, clusters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="merge_clusters_+3A_pairs_to_merge">pairs_to_merge</code></td>
<td>
<p>table of the indices of variables to be merge</p>
</td></tr>
<tr><td><code id="merge_clusters_+3A_clusters">clusters</code></td>
<td>
<p>numeric vector. By default 1:p where p is the number of variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector.
</p>

<hr>
<h2 id='mglasso'>Inference of Multiscale Gaussian Graphical Model.</h2><span id='topic+mglasso'></span>

<h3>Description</h3>

<p>Cluster variables using L2 fusion penalty and simultaneously estimates a
gaussian graphical model structure with the addition of L1 sparsity penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mglasso(
  x,
  lambda1 = 0,
  fuse_thresh = 0.001,
  maxit = NULL,
  distance = c("euclidean", "relative"),
  lambda2_start = 1e-04,
  lambda2_factor = 1.5,
  precision = 0.01,
  weights_ = NULL,
  type = c("initial"),
  compact = TRUE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mglasso_+3A_x">x</code></td>
<td>
<p>Numeric matrix (<code class="reqn">n x p</code>). Multivariate normal sample with
<code class="reqn">n</code> independent observations.</p>
</td></tr>
<tr><td><code id="mglasso_+3A_lambda1">lambda1</code></td>
<td>
<p>Positive numeric scalar. Lasso penalty.</p>
</td></tr>
<tr><td><code id="mglasso_+3A_fuse_thresh">fuse_thresh</code></td>
<td>
<p>Positive numeric scalar. Threshold for clusters fusion.</p>
</td></tr>
<tr><td><code id="mglasso_+3A_maxit">maxit</code></td>
<td>
<p>Integer scalar. Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="mglasso_+3A_distance">distance</code></td>
<td>
<p>Character. Distance between regression vectors with
permutation on symmetric coefficients.</p>
</td></tr>
<tr><td><code id="mglasso_+3A_lambda2_start">lambda2_start</code></td>
<td>
<p>Numeric scalar. Starting value for fused-group Lasso
penalty (clustering penalty).</p>
</td></tr>
<tr><td><code id="mglasso_+3A_lambda2_factor">lambda2_factor</code></td>
<td>
<p>Numeric scalar. Step used to update fused-group Lasso
penalty in a multiplicative way..</p>
</td></tr>
<tr><td><code id="mglasso_+3A_precision">precision</code></td>
<td>
<p>Tolerance for the stopping criterion (duality gap).</p>
</td></tr>
<tr><td><code id="mglasso_+3A_weights_">weights_</code></td>
<td>
<p>Matrix of weights.</p>
</td></tr>
<tr><td><code id="mglasso_+3A_type">type</code></td>
<td>
<p>If &quot;initial&quot; use classical version of <strong>MGLasso</strong> without
weights.</p>
</td></tr>
<tr><td><code id="mglasso_+3A_compact">compact</code></td>
<td>
<p>Logical scalar. If TRUE, only save results when previous
clusters are different from current.</p>
</td></tr>
<tr><td><code id="mglasso_+3A_verbose">verbose</code></td>
<td>
<p>Logical scalar. Print trace. Default value is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimates a gaussian graphical model structure while hierarchically grouping
variables by optimizing a pseudo-likelihood function combining Lasso and
fuse-group Lasso penalties. The problem is solved via the <em>COntinuation
with NEsterov smoothing in a Shrinkage-Thresholding Algorithm</em> (Hadj-Selem et
al. 2018). Varying the fusion penalty <code class="reqn">\lambda_2</code> in a multiplicative
fashion allow to uncover a seemingly hierarchical clustering structure. For
<code class="reqn">\lambda_2 = 0</code>, the approach is theoretically equivalent to the
Meinshausen-B√ºhlmann (2006) <em>neighborhood selection</em> as resuming to the
optimization of <em>pseudo-likelihood</em> function with <code class="reqn">\ell_1</code> penalty
(Rocha et al., 2008). The algorithm stops when all the variables have merged
into one cluster. The criterion used to merge clusters is the
<code class="reqn">\ell_2</code>-norm distance between regression vectors.
</p>
<p>For each iteration of the algorithm, the following function is optimized :
</p>
<p style="text-align: center;"><code class="reqn">1/2 \sum_{i=1}^p || X^i - X^{\ i} \beta^i ||_2 ^2  + \lambda_1 \sum_{i
= 1}^p || \beta^i ||_1 + \lambda_2 \sum_{i &lt; j} || \beta^i -
\tau_{ij}(\beta^j) ||_2.</code>
</p>

<p>where <code class="reqn">\beta^i</code> is the vector of coefficients obtained after regression
<code class="reqn">X^i</code> on the others and <code class="reqn">\tau_{ij}</code> is a permutation exchanging
<code class="reqn">\beta_j^i</code> with <code class="reqn">\beta_i^j</code>.
</p>


<h3>Value</h3>

<p>A list-like object of class <code>mglasso</code> is returned.
</p>
<table role = "presentation">
<tr><td><code>out</code></td>
<td>
<p>List of lists. Each element of the list corresponds to a
clustering level. An element named <code>levelk</code> contains the regression
matrix <code>beta</code> and clusters vector <code>clusters</code> for a clustering in
<code>k</code> clusters. When <code>compact = TRUE</code> <code>out</code> has as many
elements as the number of unique partitions. When set to <code>FALSE</code>, the
function returns as many items as the the range of values taken by
<code>lambda2</code>.</p>
</td></tr> <tr><td><code>l1</code></td>
<td>
<p>the sparsity penalty <code>lambda1</code> used in the
problem solving.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+conesta">conesta()</a></code> for the problem solver,
<code><a href="#topic+plot_mglasso">plot_mglasso()</a></code> for plotting the output of <code>mglasso</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
reticulate::use_condaenv("rmglasso", required = TRUE)
n = 50
K = 3
p = 9
rho = 0.85
blocs &lt;- list()
for (j in 1:K) {
  bloc &lt;- matrix(rho, nrow = p/K, ncol = p/K)
  for(i in 1:(p/K)) { bloc[i,i] &lt;- 1 }
  blocs[[j]] &lt;- bloc
}

mat.covariance &lt;- Matrix::bdiag(blocs)
mat.covariance

set.seed(11)
X &lt;- mvtnorm::rmvnorm(n, mean = rep(0,p), sigma = as.matrix(mat.covariance))
X &lt;- scale(X)

res &lt;- mglasso(X, 0.1, lambda2_start = 0.1)
res$out[[1]]$clusters
res$out[[1]]$beta

## End(Not run)
</code></pre>

<hr>
<h2 id='plot_clusterpath'>Plot MGLasso Clusterpath</h2><span id='topic+plot_clusterpath'></span>

<h3>Description</h3>

<p>Plot MGLasso Clusterpath
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_clusterpath(X, mglasso_res, colnames_ = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_clusterpath_+3A_x">X</code></td>
<td>
<p>numeric matrix</p>
</td></tr>
<tr><td><code id="plot_clusterpath_+3A_mglasso_res">mglasso_res</code></td>
<td>
<p>object of class <code>mglasso</code></p>
</td></tr>
<tr><td><code id="plot_clusterpath_+3A_colnames_">colnames_</code></td>
<td>
<p>columns labels</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function plot the clustering path of mglasso method on the 2 principal components
axis of X. As the centroids matrices are not of the same dimension as X, we choose to plot the
predicted X matrix path.
</p>


<h3>Value</h3>

<p>no return value.
</p>

<hr>
<h2 id='plot_mglasso'>Plot <code>mglasso</code> function output.</h2><span id='topic+plot_mglasso'></span>

<h3>Description</h3>

<p>Plot the object returned by the <code>mglasso</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_mglasso(mglasso_, levels_ = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_mglasso_+3A_mglasso_">mglasso_</code></td>
<td>
<p>Object of class <code>mglasso</code>.</p>
</td></tr>
<tr><td><code id="plot_mglasso_+3A_levels_">levels_</code></td>
<td>
<p>Character vector. Selected levels for which estimated matrices
will be plot. If NULL plot all levels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value.
</p>

<hr>
<h2 id='precision_to_regression'>Compute precision matrix from regression vectors</h2><span id='topic+precision_to_regression'></span>

<h3>Description</h3>

<p>Compute precision matrix from regression vectors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>precision_to_regression(K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="precision_to_regression_+3A_k">K</code></td>
<td>
<p>precision matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix.
</p>

<hr>
<h2 id='symmetrize'>Apply symmetrization on estimated graph</h2><span id='topic+symmetrize'></span>

<h3>Description</h3>

<p>Apply symmetrization on estimated graph
</p>


<h3>Usage</h3>

<pre><code class='language-R'>symmetrize(mat, rule = "and")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="symmetrize_+3A_mat">mat</code></td>
<td>
<p>graph or precision matrix</p>
</td></tr>
<tr><td><code id="symmetrize_+3A_rule">rule</code></td>
<td>
<p>&quot;and&quot; or &quot;or&quot; rule</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
