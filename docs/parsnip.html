<!DOCTYPE html><html lang="en"><head><title>Help for package parsnip</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {parsnip}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#parsnip-package'><p>parsnip</p></a></li>
<li><a href='#.check_glmnet_penalty_fit'><p>Helper functions for checking the penalty of glmnet models</p></a></li>
<li><a href='#.convert_form_to_xy_fit'><p>Helper functions to convert between formula and matrix interface</p></a></li>
<li><a href='#.extract_surv_status'><p>Extract survival status</p></a></li>
<li><a href='#.extract_surv_time'><p>Extract survival time</p></a></li>
<li><a href='#.get_prediction_column_names'><p>Obtain names of prediction columns for a fitted model or workflow</p></a></li>
<li><a href='#.model_param_name_key'><p>Translate names of model tuning parameters</p></a></li>
<li><a href='#.organize_glmnet_pred'><p>Organize glmnet predictions</p></a></li>
<li><a href='#add_rowindex'><p>Add a column of row numbers to a data frame</p></a></li>
<li><a href='#augment.model_fit'><p>Augment data with predictions</p></a></li>
<li><a href='#auto_ml'><p>Automatic Machine Learning</p></a></li>
<li><a href='#autoplot.model_fit'><p>Create a ggplot for a model object</p></a></li>
<li><a href='#bag_mars'><p>Ensembles of MARS models</p></a></li>
<li><a href='#bag_mlp'><p>Ensembles of neural networks</p></a></li>
<li><a href='#bag_tree'><p>Ensembles of decision trees</p></a></li>
<li><a href='#bart'><p>Bayesian additive regression trees (BART)</p></a></li>
<li><a href='#bart-internal'><p>Developer functions for predictions via BART models</p></a></li>
<li><a href='#boost_tree'><p>Boosted trees</p></a></li>
<li><a href='#C5_rules'><p>C5.0 rule-based classification models</p></a></li>
<li><a href='#C5.0_train'><p>Boosted trees via C5.0</p></a></li>
<li><a href='#case_weights'><p>Using case weights with parsnip</p></a></li>
<li><a href='#case_weights_allowed'><p>Determine if case weights are used</p></a></li>
<li><a href='#censoring_weights'><p>Calculations for inverse probability of censoring weights (IPCW)</p></a></li>
<li><a href='#check_empty_ellipse'><p>Check to ensure that ellipses are empty</p></a></li>
<li><a href='#condense_control'><p>Condense control object into strictly smaller control object</p></a></li>
<li><a href='#control_parsnip'><p>Control the fit function</p></a></li>
<li><a href='#convert_stan_interval'><p>Convenience function for intervals</p></a></li>
<li><a href='#ctree_train'><p>A wrapper function for conditional inference tree models</p></a></li>
<li><a href='#cubist_rules'><p>Cubist rule-based regression models</p></a></li>
<li><a href='#decision_tree'><p>Decision trees</p></a></li>
<li><a href='#descriptors'><p>Data Set Characteristics Available when Fitting Models</p></a></li>
<li><a href='#details_auto_ml_h2o'><p>Automatic machine learning via h2o</p></a></li>
<li><a href='#details_bag_mars_earth'><p>Bagged MARS via earth</p></a></li>
<li><a href='#details_bag_mlp_nnet'><p>Bagged neural networks via nnet</p></a></li>
<li><a href='#details_bag_tree_C5.0'><p>Bagged trees via C5.0</p></a></li>
<li><a href='#details_bag_tree_rpart'><p>Bagged trees via rpart</p></a></li>
<li><a href='#details_bart_dbarts'><p>Bayesian additive regression trees via dbarts</p></a></li>
<li><a href='#details_boost_tree_C5.0'><p>Boosted trees via C5.0</p></a></li>
<li><a href='#details_boost_tree_h2o'><p>Boosted trees via h2o</p></a></li>
<li><a href='#details_boost_tree_lightgbm'><p>Boosted trees via lightgbm</p></a></li>
<li><a href='#details_boost_tree_mboost'><p>Boosted trees</p></a></li>
<li><a href='#details_boost_tree_spark'><p>Boosted trees via Spark</p></a></li>
<li><a href='#details_boost_tree_xgboost'><p>Boosted trees via xgboost</p></a></li>
<li><a href='#details_C5_rules_C5.0'><p>C5.0 rule-based classification models</p></a></li>
<li><a href='#details_cubist_rules_Cubist'><p>Cubist rule-based regression models</p></a></li>
<li><a href='#details_decision_tree_C5.0'><p>Decision trees via C5.0</p></a></li>
<li><a href='#details_decision_tree_partykit'><p>Decision trees via partykit</p></a></li>
<li><a href='#details_decision_tree_rpart'><p>Decision trees via CART</p></a></li>
<li><a href='#details_decision_tree_spark'><p>Decision trees via Spark</p></a></li>
<li><a href='#details_discrim_flexible_earth'><p>Flexible discriminant analysis via earth</p></a></li>
<li><a href='#details_discrim_linear_MASS'><p>Linear discriminant analysis via MASS</p></a></li>
<li><a href='#details_discrim_linear_mda'><p>Linear discriminant analysis via flexible discriminant analysis</p></a></li>
<li><a href='#details_discrim_linear_sda'><p>Linear discriminant analysis via James-Stein-type shrinkage estimation</p></a></li>
<li><a href='#details_discrim_linear_sparsediscrim'><p>Linear discriminant analysis via regularization</p></a></li>
<li><a href='#details_discrim_quad_MASS'><p>Quadratic discriminant analysis via MASS</p></a></li>
<li><a href='#details_discrim_quad_sparsediscrim'><p>Quadratic discriminant analysis via regularization</p></a></li>
<li><a href='#details_discrim_regularized_klaR'><p>Regularized discriminant analysis via klaR</p></a></li>
<li><a href='#details_gen_additive_mod_mgcv'><p>Generalized additive models via mgcv</p></a></li>
<li><a href='#details_linear_reg_brulee'><p>Linear regression via brulee</p></a></li>
<li><a href='#details_linear_reg_gee'><p>Linear regression via generalized estimating equations (GEE)</p></a></li>
<li><a href='#details_linear_reg_glm'><p>Linear regression via glm</p></a></li>
<li><a href='#details_linear_reg_glmer'><p>Linear regression via generalized mixed models</p></a></li>
<li><a href='#details_linear_reg_glmnet'><p>Linear regression via glmnet</p></a></li>
<li><a href='#details_linear_reg_gls'><p>Linear regression via generalized least squares</p></a></li>
<li><a href='#details_linear_reg_h2o'><p>Linear regression via h2o</p></a></li>
<li><a href='#details_linear_reg_keras'><p>Linear regression via keras/tensorflow</p></a></li>
<li><a href='#details_linear_reg_lm'><p>Linear regression via lm</p></a></li>
<li><a href='#details_linear_reg_lme'><p>Linear regression via mixed models</p></a></li>
<li><a href='#details_linear_reg_lmer'><p>Linear regression via mixed models</p></a></li>
<li><a href='#details_linear_reg_quantreg'><p>Linear quantile regression via the quantreg package</p></a></li>
<li><a href='#details_linear_reg_spark'><p>Linear regression via spark</p></a></li>
<li><a href='#details_linear_reg_stan'><p>Linear regression via Bayesian Methods</p></a></li>
<li><a href='#details_linear_reg_stan_glmer'><p>Linear regression via hierarchical Bayesian methods</p></a></li>
<li><a href='#details_logistic_reg_brulee'><p>Logistic regression via brulee</p></a></li>
<li><a href='#details_logistic_reg_gee'><p>Logistic regression via generalized estimating equations (GEE)</p></a></li>
<li><a href='#details_logistic_reg_glm'><p>Logistic regression via glm</p></a></li>
<li><a href='#details_logistic_reg_glmer'><p>Logistic regression via mixed models</p></a></li>
<li><a href='#details_logistic_reg_glmnet'><p>Logistic regression via glmnet</p></a></li>
<li><a href='#details_logistic_reg_h2o'><p>Logistic regression via h2o</p></a></li>
<li><a href='#details_logistic_reg_keras'><p>Logistic regression via keras</p></a></li>
<li><a href='#details_logistic_reg_LiblineaR'><p>Logistic regression via LiblineaR</p></a></li>
<li><a href='#details_logistic_reg_spark'><p>Logistic regression via spark</p></a></li>
<li><a href='#details_logistic_reg_stan'><p>Logistic regression via stan</p></a></li>
<li><a href='#details_logistic_reg_stan_glmer'><p>Logistic regression via hierarchical Bayesian methods</p></a></li>
<li><a href='#details_mars_earth'><p>Multivariate adaptive regression splines (MARS) via earth</p></a></li>
<li><a href='#details_mlp_brulee'><p>Multilayer perceptron via brulee</p></a></li>
<li><a href='#details_mlp_brulee_two_layer'><p>Multilayer perceptron via brulee with two hidden layers</p></a></li>
<li><a href='#details_mlp_h2o'><p>Multilayer perceptron via h2o</p></a></li>
<li><a href='#details_mlp_keras'><p>Multilayer perceptron via keras</p></a></li>
<li><a href='#details_mlp_nnet'><p>Multilayer perceptron via nnet</p></a></li>
<li><a href='#details_multinom_reg_brulee'><p>Multinomial regression via brulee</p></a></li>
<li><a href='#details_multinom_reg_glmnet'><p>Multinomial regression via glmnet</p></a></li>
<li><a href='#details_multinom_reg_h2o'><p>Multinomial regression via h2o</p></a></li>
<li><a href='#details_multinom_reg_keras'><p>Multinomial regression via keras</p></a></li>
<li><a href='#details_multinom_reg_nnet'><p>Multinomial regression via nnet</p></a></li>
<li><a href='#details_multinom_reg_spark'><p>Multinomial regression via spark</p></a></li>
<li><a href='#details_naive_Bayes_h2o'><p>Naive Bayes models via naivebayes</p></a></li>
<li><a href='#details_naive_Bayes_klaR'><p>Naive Bayes models via klaR</p></a></li>
<li><a href='#details_naive_Bayes_naivebayes'><p>Naive Bayes models via naivebayes</p></a></li>
<li><a href='#details_nearest_neighbor_kknn'><p>K-nearest neighbors via kknn</p></a></li>
<li><a href='#details_pls_mixOmics'><p>Partial least squares via mixOmics</p></a></li>
<li><a href='#details_poisson_reg_gee'><p>Poisson regression via generalized estimating equations (GEE)</p></a></li>
<li><a href='#details_poisson_reg_glm'><p>Poisson regression via glm</p></a></li>
<li><a href='#details_poisson_reg_glmer'><p>Poisson regression via mixed models</p></a></li>
<li><a href='#details_poisson_reg_glmnet'><p>Poisson regression via glmnet</p></a></li>
<li><a href='#details_poisson_reg_h2o'><p>Poisson regression via h2o</p></a></li>
<li><a href='#details_poisson_reg_hurdle'><p>Poisson regression via pscl</p></a></li>
<li><a href='#details_poisson_reg_stan'><p>Poisson regression via stan</p></a></li>
<li><a href='#details_poisson_reg_stan_glmer'><p>Poisson regression via hierarchical Bayesian methods</p></a></li>
<li><a href='#details_poisson_reg_zeroinfl'><p>Poisson regression via pscl</p></a></li>
<li><a href='#details_proportional_hazards_glmnet'><p>Proportional hazards regression</p></a></li>
<li><a href='#details_proportional_hazards_survival'><p>Proportional hazards regression</p></a></li>
<li><a href='#details_rand_forest_aorsf'><p>Oblique random survival forests via aorsf</p></a></li>
<li><a href='#details_rand_forest_h2o'><p>Random forests via h2o</p></a></li>
<li><a href='#details_rand_forest_partykit'><p>Random forests via partykit</p></a></li>
<li><a href='#details_rand_forest_randomForest'><p>Random forests via randomForest</p></a></li>
<li><a href='#details_rand_forest_ranger'><p>Random forests via ranger</p></a></li>
<li><a href='#details_rand_forest_spark'><p>Random forests via spark</p></a></li>
<li><a href='#details_rule_fit_h2o'><p>RuleFit models via h2o</p></a></li>
<li><a href='#details_rule_fit_xrf'><p>RuleFit models via xrf</p></a></li>
<li><a href='#details_survival_reg_flexsurv'><p>Parametric survival regression</p></a></li>
<li><a href='#details_survival_reg_flexsurvspline'><p>Flexible parametric survival regression</p></a></li>
<li><a href='#details_survival_reg_survival'><p>Parametric survival regression</p></a></li>
<li><a href='#details_svm_linear_kernlab'><p>Linear support vector machines (SVMs) via kernlab</p></a></li>
<li><a href='#details_svm_linear_LiblineaR'><p>Linear support vector machines (SVMs) via LiblineaR</p></a></li>
<li><a href='#details_svm_poly_kernlab'><p>Polynomial support vector machines (SVMs) via kernlab</p></a></li>
<li><a href='#details_svm_rbf_kernlab'><p>Radial basis function support vector machines (SVMs) via kernlab</p></a></li>
<li><a href='#discrim_flexible'><p>Flexible discriminant analysis</p></a></li>
<li><a href='#discrim_linear'><p>Linear discriminant analysis</p></a></li>
<li><a href='#discrim_quad'><p>Quadratic discriminant analysis</p></a></li>
<li><a href='#discrim_regularized'><p>Regularized discriminant analysis</p></a></li>
<li><a href='#doc-tools'><p>Tools for documenting engines</p></a></li>
<li><a href='#eval_args'><p>Evaluate parsnip model arguments</p></a></li>
<li><a href='#extract-parsnip'><p>Extract elements of a parsnip model object</p></a></li>
<li><a href='#fit_control'><p>Control the fit function</p></a></li>
<li><a href='#fit.model_spec'><p>Fit a Model Specification to a Dataset</p></a></li>
<li><a href='#format-internals'><p>Internal functions that format predictions</p></a></li>
<li><a href='#gen_additive_mod'><p>Generalized additive models (GAMs)</p></a></li>
<li><a href='#get_model_env'><p>Working with the parsnip model environment</p></a></li>
<li><a href='#glance.model_fit'><p>Construct a single row summary &quot;glance&quot; of a model, fit, or other object</p></a></li>
<li><a href='#glm_grouped'><p>Fit a grouped binomial outcome from a data set with case weights</p></a></li>
<li><a href='#glmnet-details'><p>Technical aspects of the glmnet model</p></a></li>
<li><a href='#has_multi_predict'><p>Tools for models that predict on sub-models</p></a></li>
<li><a href='#keras_activations'><p>Activation functions for neural networks in keras</p></a></li>
<li><a href='#keras_mlp'><p>Simple interface to MLP models via keras</p></a></li>
<li><a href='#keras_predict_classes'><p>Wrapper for keras class predictions</p></a></li>
<li><a href='#knit_engine_docs'><p>Knit engine-specific documentation</p></a></li>
<li><a href='#linear_reg'><p>Linear regression</p></a></li>
<li><a href='#list_md_problems'><p>Locate and show errors/warnings in engine-specific documentation</p></a></li>
<li><a href='#logistic_reg'><p>Logistic regression</p></a></li>
<li><a href='#make_call'><p>Make a parsnip call expression</p></a></li>
<li><a href='#make_classes'><p>Prepend a new class</p></a></li>
<li><a href='#mars'><p>Multivariate adaptive regression splines (MARS)</p></a></li>
<li><a href='#matrix_to_quantile_pred'><p>Reformat quantile predictions</p></a></li>
<li><a href='#max_mtry_formula'><p>Determine largest value of mtry from formula.</p>
This function potentially caps the value of <code>mtry</code> based on a formula and
data set. This is a safe approach for survival and/or multivariate models.</a></li>
<li><a href='#maybe_matrix'><p>Fuzzy conversions</p></a></li>
<li><a href='#min_cols'><p>Execution-time data dimension checks</p></a></li>
<li><a href='#mlp'><p>Single layer neural network</p></a></li>
<li><a href='#model_db'><p>parsnip model specification database</p></a></li>
<li><a href='#model_fit'><p>Model Fit Objects</p></a></li>
<li><a href='#model_formula'><p>Formulas with special terms in tidymodels</p></a></li>
<li><a href='#model_printer'><p>Print helper for model objects</p></a></li>
<li><a href='#model_spec'><p>Model Specifications</p></a></li>
<li><a href='#multi_predict'><p>Model predictions across many sub-models</p></a></li>
<li><a href='#multinom_reg'><p>Multinomial regression</p></a></li>
<li><a href='#naive_Bayes'><p>Naive Bayes models</p></a></li>
<li><a href='#nearest_neighbor'><p>K-nearest neighbors</p></a></li>
<li><a href='#null_model'><p>Null model</p></a></li>
<li><a href='#null_value'><p>Functions required for parsnip-adjacent packages</p></a></li>
<li><a href='#nullmodel'><p>Fit a simple, non-informative model</p></a></li>
<li><a href='#parsnip_addin'><p>Start an RStudio Addin that can write model specifications</p></a></li>
<li><a href='#pls'><p>Partial least squares (PLS)</p></a></li>
<li><a href='#poisson_reg'><p>Poisson regression models</p></a></li>
<li><a href='#predict_class.model_fit'><p>Other predict methods.</p></a></li>
<li><a href='#predict.model_fit'><p>Model predictions</p></a></li>
<li><a href='#prepare_data'><p>Prepare data based on parsnip encoding information</p></a></li>
<li><a href='#proportional_hazards'><p>Proportional hazards regression</p></a></li>
<li><a href='#rand_forest'><p>Random forest</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#repair_call'><p>Repair a model call object</p></a></li>
<li><a href='#req_pkgs'><p>Determine required packages for a model</p></a></li>
<li><a href='#required_pkgs.model_spec'><p>Determine required packages for a model</p></a></li>
<li><a href='#rule_fit'><p>RuleFit models</p></a></li>
<li><a href='#set_args'><p>Change elements of a model specification</p></a></li>
<li><a href='#set_engine'><p>Declare a computational engine and specific arguments</p></a></li>
<li><a href='#set_new_model'><p>Tools to Register Models</p></a></li>
<li><a href='#set_tf_seed'><p>Set seed in R and TensorFlow at the same time</p></a></li>
<li><a href='#show_call'><p>Print the model call</p></a></li>
<li><a href='#show_engines'><p>Display currently available engines for a model</p></a></li>
<li><a href='#sparse_data'><p>Using sparse data with parsnip</p></a></li>
<li><a href='#spec_is_possible'><p>Model Specification Checking:</p></a></li>
<li><a href='#stan_conf_int'><p>Wrapper for stan confidence intervals</p></a></li>
<li><a href='#surv_reg'><p>Parametric survival regression</p></a></li>
<li><a href='#survival_reg'><p>Parametric survival regression</p></a></li>
<li><a href='#svm_linear'><p>Linear support vector machines</p></a></li>
<li><a href='#svm_poly'><p>Polynomial support vector machines</p></a></li>
<li><a href='#svm_rbf'><p>Radial basis function support vector machines</p></a></li>
<li><a href='#tidy._elnet'><p>tidy methods for glmnet models</p></a></li>
<li><a href='#tidy._LiblineaR'><p>tidy methods for LiblineaR models</p></a></li>
<li><a href='#tidy.model_fit'><p>Turn a parsnip model object into a tidy tibble</p></a></li>
<li><a href='#tidy.nullmodel'><p>Tidy method for null models</p></a></li>
<li><a href='#translate'><p>Resolve a Model Specification for a Computational Engine</p></a></li>
<li><a href='#type_sum.model_spec'><p>Succinct summary of parsnip object</p></a></li>
<li><a href='#update_model_info_file'><p>Save information about models</p></a></li>
<li><a href='#update.bag_mars'><p>Updating a model specification</p></a></li>
<li><a href='#varying'><p>A placeholder function for argument values</p></a></li>
<li><a href='#varying_args.model_spec'><p>Determine varying arguments</p></a></li>
<li><a href='#xgb_train'><p>Boosted trees via xgboost</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>A Common API to Modeling and Analysis Functions</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Max Kuhn &lt;max@posit.co&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A common interface is provided to allow users to specify a
    model without having to remember the different argument names across
    different functions or computational engines (e.g. 'R', 'Spark',
    'Stan', 'H2O', etc).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/tidymodels/parsnip">https://github.com/tidymodels/parsnip</a>,
<a href="https://parsnip.tidymodels.org/">https://parsnip.tidymodels.org/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tidymodels/parsnip/issues">https://github.com/tidymodels/parsnip/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, dplyr (&ge; 1.1.0), generics (&ge; 0.1.2), ggplot2, globals,
glue, hardhat (&ge; 1.4.1), lifecycle, magrittr, pillar,
prettyunits, purrr (&ge; 1.0.0), rlang (&ge; 1.1.0), sparsevctrs
(&ge; 0.2.0), stats, tibble (&ge; 2.1.1), tidyr (&ge; 1.3.0), utils,
vctrs (&ge; 0.6.0), withr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>bench, C50, covr, dials (&ge; 1.1.0), earth, ggrepel, keras,
kernlab, kknn, knitr, LiblineaR, MASS, Matrix, methods, mgcv,
modeldata, nlme, prodlim, ranger (&ge; 0.12.0), remotes,
rmarkdown, rpart, sparklyr (&ge; 1.0.0), survival, tensorflow,
testthat (&ge; 3.0.0), xgboost (&ge; 1.5.0.1)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>brulee, C50, dbarts, earth, glmnet, keras,
kernlab, kknn, LiblineaR, mgcv, nnet, parsnip, quantreg,
randomForest, ranger, rpart, rstanarm, tidymodels/tidymodels,
tidyverse/tidytemplate, rstudio/reticulate, xgboost, rmarkdown</td>
</tr>
<tr>
<td>Config/rcmdcheck/ignore-inconsequential-notes:</td>
<td>true</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-11 19:17:07 UTC; max</td>
</tr>
<tr>
<td>Author:</td>
<td>Max Kuhn [aut, cre],
  Davis Vaughan [aut],
  Emil Hvitfeldt [ctb],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-12 00:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='parsnip-package'>parsnip</h2><span id='topic+parsnip'></span><span id='topic+parsnip-package'></span>

<h3>Description</h3>

<p>The goal of parsnip is to provide a tidy, unified interface to models that
can be used to try a range of models without getting bogged down in the
syntactical minutiae of the underlying packages.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Max Kuhn <a href="mailto:max@posit.co">max@posit.co</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Davis Vaughan <a href="mailto:davis@posit.co">davis@posit.co</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Emil Hvitfeldt <a href="mailto:emil.hvitfeldt@posit.co">emil.hvitfeldt@posit.co</a> [contributor]
</p>
</li>
<li><p> Posit Software, PBC [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/tidymodels/parsnip">https://github.com/tidymodels/parsnip</a>
</p>
</li>
<li> <p><a href="https://parsnip.tidymodels.org/">https://parsnip.tidymodels.org/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tidymodels/parsnip/issues">https://github.com/tidymodels/parsnip/issues</a>
</p>
</li></ul>


<hr>
<h2 id='.check_glmnet_penalty_fit'>Helper functions for checking the penalty of glmnet models</h2><span id='topic+.check_glmnet_penalty_fit'></span><span id='topic+.check_glmnet_penalty_predict'></span>

<h3>Description</h3>

<p>These functions are for developer use.
</p>
<p><code>.check_glmnet_penalty_fit()</code> checks that the model specification for fitting a
glmnet model contains a single value.
</p>
<p><code>.check_glmnet_penalty_predict()</code> checks that the penalty value used for prediction is valid.
If called by <code>predict()</code>, it needs to be a single value. Multiple values are
allowed for <code>multi_predict()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.check_glmnet_penalty_fit(x, call = rlang::caller_env())

.check_glmnet_penalty_predict(
  penalty = NULL,
  object,
  multi = FALSE,
  call = rlang::caller_env()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".check_glmnet_penalty_fit_+3A_x">x</code></td>
<td>
<p>An object of class <code>model_spec</code>.</p>
</td></tr>
<tr><td><code id=".check_glmnet_penalty_fit_+3A_penalty">penalty</code></td>
<td>
<p>A penalty value to check.</p>
</td></tr>
<tr><td><code id=".check_glmnet_penalty_fit_+3A_object">object</code></td>
<td>
<p>An object of class <code>model_fit</code>.</p>
</td></tr>
<tr><td><code id=".check_glmnet_penalty_fit_+3A_multi">multi</code></td>
<td>
<p>A logical indicating if multiple values are allowed.</p>
</td></tr>
</table>

<hr>
<h2 id='.convert_form_to_xy_fit'>Helper functions to convert between formula and matrix interface</h2><span id='topic+.convert_form_to_xy_fit'></span><span id='topic+.convert_form_to_xy_new'></span><span id='topic+.convert_xy_to_form_fit'></span><span id='topic+.convert_xy_to_form_new'></span>

<h3>Description</h3>

<p>Functions to take a formula interface and get the resulting
objects (y, x, weights, etc) back or the other way around. The functions are
intended for developer use. For the most part, this emulates the internals
of <code>lm()</code> (and also see the notes at
https://developer.r-project.org/model-fitting-functions.html).
</p>
<p><code>.convert_form_to_xy_fit()</code> and <code>.convert_xy_to_form_fit()</code> are for when the
data are created for modeling.
<code>.convert_form_to_xy_fit()</code> saves both the data objects as well as the objects
needed when new data are predicted (e.g. <code>terms</code>, etc.).
</p>
<p><code>.convert_form_to_xy_new()</code> and <code>.convert_xy_to_form_new()</code> are used when new
samples are being predicted and only require the predictors to be available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.convert_form_to_xy_fit(
  formula,
  data,
  ...,
  na.action = na.omit,
  indicators = "traditional",
  composition = "data.frame",
  remove_intercept = TRUE,
  call = rlang::caller_env()
)

.convert_form_to_xy_new(
  object,
  new_data,
  na.action = na.pass,
  composition = "data.frame",
  call = rlang::caller_env()
)

.convert_xy_to_form_fit(
  x,
  y,
  weights = NULL,
  y_name = "..y",
  remove_intercept = TRUE,
  call = rlang::caller_env()
)

.convert_xy_to_form_new(object, new_data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".convert_form_to_xy_fit_+3A_formula">formula</code></td>
<td>
<p>An object of class <code>formula</code> (or one that can
be coerced to that class): a symbolic description of the model
to be fitted.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_data">data</code></td>
<td>
<p>A data frame containing all relevant variables (e.g. outcome(s),
predictors, case weights, etc).</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="stats.html#topic+model.frame">stats::model.frame()</a></code>.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_na.action">na.action</code></td>
<td>
<p>A function which indicates what should happen when the data
contain NAs.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_indicators">indicators</code></td>
<td>
<p>A string describing whether and how to create
indicator/dummy variables from factor predictors. Possible options are
<code>"none"</code>, <code>"traditional"</code>, and <code>"one_hot"</code>.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_composition">composition</code></td>
<td>
<p>A string describing whether the resulting <code>x</code> and <code>y</code>
should be returned as a <code>"matrix"</code> or a <code>"data.frame"</code>.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_remove_intercept">remove_intercept</code></td>
<td>
<p>A logical indicating whether to remove the intercept
column after <code>model.matrix()</code> is finished.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_object">object</code></td>
<td>
<p>A <a href="#topic+model_fit">model fit</a>.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_new_data">new_data</code></td>
<td>
<p>A rectangular data object, such as a data frame.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_x">x</code></td>
<td>
<p>A matrix, sparse matrix, or data frame of predictors. Only some
models have support for sparse matrix input. See <code>parsnip::get_encoding()</code>
for details. <code>x</code> should have column names.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_y">y</code></td>
<td>
<p>A vector, matrix or data frame of outcome data.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_weights">weights</code></td>
<td>
<p>A numeric vector containing the weights.</p>
</td></tr>
<tr><td><code id=".convert_form_to_xy_fit_+3A_y_name">y_name</code></td>
<td>
<p>A string specifying the name of the outcome.</p>
</td></tr>
</table>

<hr>
<h2 id='.extract_surv_status'>Extract survival status</h2><span id='topic+.extract_surv_status'></span>

<h3>Description</h3>

<p>Extract the status from a <code><a href="survival.html#topic+Surv">survival::Surv()</a></code> object.
</p>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".extract_surv_status_+3A_surv">surv</code></td>
<td>
<p>A single <code><a href="survival.html#topic+Surv">survival::Surv()</a></code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector.
</p>

<hr>
<h2 id='.extract_surv_time'>Extract survival time</h2><span id='topic+.extract_surv_time'></span>

<h3>Description</h3>

<p>Extract the time component(s) from a <code><a href="survival.html#topic+Surv">survival::Surv()</a></code> object.
</p>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".extract_surv_time_+3A_surv">surv</code></td>
<td>
<p>A single <code><a href="survival.html#topic+Surv">survival::Surv()</a></code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector when the type is <code>"right"</code> or <code>"left"</code> and a tibble otherwise.
</p>

<hr>
<h2 id='.get_prediction_column_names'>Obtain names of prediction columns for a fitted model or workflow</h2><span id='topic+.get_prediction_column_names'></span>

<h3>Description</h3>

<p><code><a href="#topic+.get_prediction_column_names">.get_prediction_column_names()</a></code> returns a list that has the names of the
columns for the primary prediction types for a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.get_prediction_column_names(x, syms = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".get_prediction_column_names_+3A_x">x</code></td>
<td>
<p>A fitted parsnip model (class <code>"model_fit"</code>) or a fitted workflow.</p>
</td></tr>
<tr><td><code id=".get_prediction_column_names_+3A_syms">syms</code></td>
<td>
<p>Should the column names be converted to symbols? Defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements <code>"estimate"</code> and <code>"probabilities"</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)
library(modeldata)
data("two_class_dat")

levels(two_class_dat$Class)
lr_fit &lt;- logistic_reg() %&gt;% fit(Class ~ ., data = two_class_dat)

.get_prediction_column_names(lr_fit)
.get_prediction_column_names(lr_fit, syms = TRUE)

</code></pre>

<hr>
<h2 id='.model_param_name_key'>Translate names of model tuning parameters</h2><span id='topic+.model_param_name_key'></span>

<h3>Description</h3>

<p>This function creates a key that connects the identifiers users make for
tuning parameter names, the standardized parsnip parameter names, and the
argument names to the underlying fit function for the engine.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.model_param_name_key(object, as_tibble = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".model_param_name_key_+3A_object">object</code></td>
<td>
<p>A workflow or parsnip model specification.</p>
</td></tr>
<tr><td><code id=".model_param_name_key_+3A_as_tibble">as_tibble</code></td>
<td>
<p>A logical. Should the results be in a tibble (the default)
or in a list that can facilitate renaming grid objects?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with columns <code>user</code>, <code>parsnip</code>, and <code>engine</code>, or a list
with named character vectors <code>user_to_parsnip</code> and <code>parsnip_to_engine</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mod &lt;-
 linear_reg(penalty = tune("regularization"), mixture = tune()) %&gt;%
 set_engine("glmnet")

mod %&gt;% .model_param_name_key()

rn &lt;- mod %&gt;% .model_param_name_key(as_tibble = FALSE)
rn

grid &lt;- tidyr::crossing(regularization = c(0, 1), mixture = (0:3) / 3)

grid %&gt;%
  dplyr::rename(!!!rn$user_to_parsnip)

grid %&gt;%
  dplyr::rename(!!!rn$user_to_parsnip) %&gt;%
  dplyr::rename(!!!rn$parsnip_to_engine)

</code></pre>

<hr>
<h2 id='.organize_glmnet_pred'>Organize glmnet predictions</h2><span id='topic+.organize_glmnet_pred'></span>

<h3>Description</h3>

<p>This function is for developer use and organizes predictions from glmnet
models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.organize_glmnet_pred(x, object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".organize_glmnet_pred_+3A_x">x</code></td>
<td>
<p>Predictions as returned by the <code>predict()</code> method for glmnet models.</p>
</td></tr>
<tr><td><code id=".organize_glmnet_pred_+3A_object">object</code></td>
<td>
<p>An object of class <code>model_fit</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='add_rowindex'>Add a column of row numbers to a data frame</h2><span id='topic+add_rowindex'></span>

<h3>Description</h3>

<p>Add a column of row numbers to a data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_rowindex(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="add_rowindex_+3A_x">x</code></td>
<td>
<p>A data frame</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same data frame with a column of 1-based integers named <code>.row</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mtcars %&gt;% add_rowindex()

</code></pre>

<hr>
<h2 id='augment.model_fit'>Augment data with predictions</h2><span id='topic+augment.model_fit'></span>

<h3>Description</h3>

<p><code>augment()</code> will add column(s) for predictions to the given data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_fit'
augment(x, new_data, eval_time = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="augment.model_fit_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+model_fit">model fit</a> produced by <code><a href="#topic+fit.model_spec">fit.model_spec()</a></code> or
<code><a href="#topic+fit_xy.model_spec">fit_xy.model_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="augment.model_fit_+3A_new_data">new_data</code></td>
<td>
<p>A data frame or matrix.</p>
</td></tr>
<tr><td><code id="augment.model_fit_+3A_eval_time">eval_time</code></td>
<td>
<p>For censored regression models, a vector of time points at
which the survival probability is estimated.</p>
</td></tr>
<tr><td><code id="augment.model_fit_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Regression</h4>

<p>For regression models, a <code>.pred</code> column is added. If <code>x</code> was created using
<code><a href="#topic+fit.model_spec">fit.model_spec()</a></code> and <code>new_data</code> contains a regression outcome column, a
<code>.resid</code> column is also added.
</p>



<h4>Classification</h4>

<p>For classification models, the results can include a column called
<code>.pred_class</code> as well as class probability columns named <code style="white-space: pre;">&#8288;.pred_{level}&#8288;</code>.
This depends on what type of prediction types are available for the model.
</p>



<h4>Censored Regression</h4>

<p>For these models, predictions for the expected time and survival probability
are created (if the model engine supports them). If the model supports
survival prediction, the <code>eval_time</code> argument is required.
</p>
<p>If survival predictions are created and <code>new_data</code> contains a
<code><a href="survival.html#topic+Surv">survival::Surv()</a></code> object, additional columns are added for inverse
probability of censoring weights (IPCW) are also created (see <code>tidymodels.org</code>
page in the references below). This enables the user to compute performance
metrics in the <span class="pkg">yardstick</span> package.
</p>



<h3>References</h3>

<p><a href="https://www.tidymodels.org/learn/statistics/survival-metrics/">https://www.tidymodels.org/learn/statistics/survival-metrics/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
car_trn &lt;- mtcars[11:32,]
car_tst &lt;- mtcars[ 1:10,]

reg_form &lt;-
  linear_reg() %&gt;%
  set_engine("lm") %&gt;%
  fit(mpg ~ ., data = car_trn)
reg_xy &lt;-
  linear_reg() %&gt;%
  set_engine("lm") %&gt;%
  fit_xy(car_trn[, -1], car_trn$mpg)

augment(reg_form, car_tst)
augment(reg_form, car_tst[, -1])

augment(reg_xy, car_tst)
augment(reg_xy, car_tst[, -1])

# ------------------------------------------------------------------------------

data(two_class_dat, package = "modeldata")
cls_trn &lt;- two_class_dat[-(1:10), ]
cls_tst &lt;- two_class_dat[  1:10 , ]

cls_form &lt;-
  logistic_reg() %&gt;%
  set_engine("glm") %&gt;%
  fit(Class ~ ., data = cls_trn)
cls_xy &lt;-
  logistic_reg() %&gt;%
  set_engine("glm") %&gt;%
  fit_xy(cls_trn[, -3],
  cls_trn$Class)

augment(cls_form, cls_tst)
augment(cls_form, cls_tst[, -3])

augment(cls_xy, cls_tst)
augment(cls_xy, cls_tst[, -3])

</code></pre>

<hr>
<h2 id='auto_ml'>Automatic Machine Learning</h2><span id='topic+auto_ml'></span>

<h3>Description</h3>

<p><code>auto_ml()</code> defines an automated searching and tuning process where
many models of different families are trained and ranked given their
performance on the training data.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_auto_ml_h2o">h2o</a>¹²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for classification and regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_ml(mode = "unknown", engine = "h2o")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="auto_ml_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="auto_ml_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
auto_ml(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_auto_ml_h2o">h2o engine details</a></code>
</p>

<hr>
<h2 id='autoplot.model_fit'>Create a ggplot for a model object</h2><span id='topic+autoplot.model_fit'></span><span id='topic+autoplot.glmnet'></span>

<h3>Description</h3>

<p>This method provides a good visualization method for model results.
Currently, only methods for glmnet models are implemented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_fit'
autoplot(object, ...)

## S3 method for class 'glmnet'
autoplot(object, ..., min_penalty = 0, best_penalty = NULL, top_n = 3L)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="autoplot.model_fit_+3A_object">object</code></td>
<td>
<p>A model fit object.</p>
</td></tr>
<tr><td><code id="autoplot.model_fit_+3A_...">...</code></td>
<td>
<p>For <code><a href="#topic+autoplot.glmnet">autoplot.glmnet()</a></code>, options to pass to
<code><a href="ggrepel.html#topic+geom_text_repel">ggrepel::geom_label_repel()</a></code>. Otherwise, this argument is ignored.</p>
</td></tr>
<tr><td><code id="autoplot.model_fit_+3A_min_penalty">min_penalty</code></td>
<td>
<p>A single, non-negative number for the smallest penalty
value that should be shown in the plot. If left <code>NULL</code>, the whole data
range is used.</p>
</td></tr>
<tr><td><code id="autoplot.model_fit_+3A_best_penalty">best_penalty</code></td>
<td>
<p>A single, non-negative number that will show a vertical
line marker. If left <code>NULL</code>, no line is shown. When this argument is used,
the <span class="pkg">ggrepl</span> package is required.</p>
</td></tr>
<tr><td><code id="autoplot.model_fit_+3A_top_n">top_n</code></td>
<td>
<p>A non-negative integer for how many model predictors to label.
The top predictors are ranked by their absolute coefficient value. For
multinomial or multivariate models, the <code>top_n</code> terms are selected within
class or response, respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <span class="pkg">glmnet</span> package will need to be attached or loaded for
its <code>autoplot()</code> method to work correctly.
</p>


<h3>Value</h3>

<p>A ggplot object with penalty on the x-axis and coefficients on the
y-axis. For multinomial or multivariate models, the plot is faceted.
</p>

<hr>
<h2 id='bag_mars'>Ensembles of MARS models</h2><span id='topic+bag_mars'></span>

<h3>Description</h3>

<p><code>bag_mars()</code> defines an ensemble of generalized linear models that use
artificial features for some predictors. These features resemble hinge
functions and the result is a model that is a segmented regression in small
dimensions. This function can fit classification and regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_bag_mars_earth">earth</a>¹²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for classification and regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bag_mars(
  mode = "unknown",
  num_terms = NULL,
  prod_degree = NULL,
  prune_method = NULL,
  engine = "earth"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bag_mars_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="bag_mars_+3A_num_terms">num_terms</code></td>
<td>
<p>The number of features that will be retained in the
final model, including the intercept.</p>
</td></tr>
<tr><td><code id="bag_mars_+3A_prod_degree">prod_degree</code></td>
<td>
<p>The highest possible interaction degree.</p>
</td></tr>
<tr><td><code id="bag_mars_+3A_prune_method">prune_method</code></td>
<td>
<p>The pruning method.</p>
</td></tr>
<tr><td><code id="bag_mars_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
bag_mars(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_bag_mars_earth">earth engine details</a></code>
</p>

<hr>
<h2 id='bag_mlp'>Ensembles of neural networks</h2><span id='topic+bag_mlp'></span>

<h3>Description</h3>

<p><code>bag_mlp()</code> defines an ensemble of single layer, feed-forward neural networks.
This function can fit classification and regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_bag_mlp_nnet">nnet</a>¹²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for classification and regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bag_mlp(
  mode = "unknown",
  hidden_units = NULL,
  penalty = NULL,
  epochs = NULL,
  engine = "nnet"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bag_mlp_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="bag_mlp_+3A_hidden_units">hidden_units</code></td>
<td>
<p>An integer for the number of units in the hidden model.</p>
</td></tr>
<tr><td><code id="bag_mlp_+3A_penalty">penalty</code></td>
<td>
<p>A non-negative numeric value for the amount of weight
decay.</p>
</td></tr>
<tr><td><code id="bag_mlp_+3A_epochs">epochs</code></td>
<td>
<p>An integer for the number of training iterations.</p>
</td></tr>
<tr><td><code id="bag_mlp_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
bag_mlp(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_bag_mlp_nnet">nnet engine details</a></code>
</p>

<hr>
<h2 id='bag_tree'>Ensembles of decision trees</h2><span id='topic+bag_tree'></span>

<h3>Description</h3>

<p><code>bag_tree()</code> defines an ensemble of decision trees. This function can fit
classification, regression, and censored regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_bag_tree_rpart">rpart</a>¹²</code></p>
</li>
<li><p><code><a href="#topic+details_bag_tree_C5.0">C5.0</a>²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for censored regression, classification, and regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bag_tree(
  mode = "unknown",
  cost_complexity = 0,
  tree_depth = NULL,
  min_n = 2,
  class_cost = NULL,
  engine = "rpart"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bag_tree_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;,
&quot;classification&quot;, or &quot;censored regression&quot;.</p>
</td></tr>
<tr><td><code id="bag_tree_+3A_cost_complexity">cost_complexity</code></td>
<td>
<p>A positive number for the the cost/complexity
parameter (a.k.a. <code>Cp</code>) used by CART models (specific engines only).</p>
</td></tr>
<tr><td><code id="bag_tree_+3A_tree_depth">tree_depth</code></td>
<td>
<p>An integer for the maximum depth of the tree (i.e. number
of splits) (specific engines only).</p>
</td></tr>
<tr><td><code id="bag_tree_+3A_min_n">min_n</code></td>
<td>
<p>An integer for the minimum number of data points
in a node that is required for the node to be split further.</p>
</td></tr>
<tr><td><code id="bag_tree_+3A_class_cost">class_cost</code></td>
<td>
<p>A non-negative scalar for a class cost (where a cost of 1
means no extra cost). This is useful for when the first level of the outcome
factor is the minority class. If this is not the case, values between zero
and one can be used to bias to the second level of the factor.</p>
</td></tr>
<tr><td><code id="bag_tree_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
bag_tree(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_bag_tree_rpart">rpart engine details</a></code>, <code><a href="#topic+details_bag_tree_C5.0">C5.0 engine details</a></code>
</p>

<hr>
<h2 id='bart'>Bayesian additive regression trees (BART)</h2><span id='topic+bart'></span>

<h3>Description</h3>

<p><code>bart()</code> defines a tree ensemble model that uses Bayesian analysis to
assemble the ensemble. This function can fit classification and regression
models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_bart_dbarts">dbarts</a>¹</code></p>
</li></ul>
<p>¹ The default engine.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bart(
  mode = "unknown",
  engine = "dbarts",
  trees = NULL,
  prior_terminal_node_coef = NULL,
  prior_terminal_node_expo = NULL,
  prior_outcome_range = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bart_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="bart_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="bart_+3A_trees">trees</code></td>
<td>
<p>An integer for the number of trees contained in
the ensemble.</p>
</td></tr>
<tr><td><code id="bart_+3A_prior_terminal_node_coef">prior_terminal_node_coef</code></td>
<td>
<p>A coefficient for the prior probability that
a node is a terminal node. Values are usually between 0 and one with
a default of 0.95. This affects the baseline probability; smaller numbers
make the probabilities larger overall. See Details below.</p>
</td></tr>
<tr><td><code id="bart_+3A_prior_terminal_node_expo">prior_terminal_node_expo</code></td>
<td>
<p>An exponent in the prior probability that
a node is a terminal node.  Values are usually non-negative with
a default of 2 This affects the rate that the prior probability decreases as
the depth of the tree increases. Larger values make deeper trees less likely.</p>
</td></tr>
<tr><td><code id="bart_+3A_prior_outcome_range">prior_outcome_range</code></td>
<td>
<p>A positive value that defines the width of a prior
that the predicted outcome is within a certain range. For regression it is
related to the observed range of the data; the prior is the number of standard
deviations of a Gaussian distribution defined by the observed range of the
data. For classification, it is defined as the range of +/-3 (assumed to be
on the logit scale). The default value is 2.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prior for the terminal node probability is expressed as
<code>prior = a * (1 + d)^(-b)</code> where <code>d</code> is the depth of the node, <code>a</code> is
<code>prior_terminal_node_coef</code> and <code>b</code> is <code>prior_terminal_node_expo</code>. See the
Examples section below for an example graph of the prior probability of a
terminal node for different values of these parameters.
</p>
<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
bart(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_bart_dbarts">dbarts engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("bart")

bart(mode = "regression", trees = 5)

# ------------------------------------------------------------------------------
# Examples for terminal node prior

library(ggplot2)
library(dplyr)

prior_test &lt;- function(coef = 0.95, expo = 2, depths = 1:10) {
  tidyr::crossing(coef = coef, expo = expo, depth = depths) %&gt;%
    mutate(
      `terminial node prior` = coef * (1 + depth)^(-expo),
      coef = format(coef),
      expo = format(expo))
}

prior_test(coef = c(0.05, 0.5, .95), expo = c(1/2, 1, 2)) %&gt;%
  ggplot(aes(depth, `terminial node prior`, col = coef)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ expo)

</code></pre>

<hr>
<h2 id='bart-internal'>Developer functions for predictions via BART models</h2><span id='topic+bart-internal'></span><span id='topic+dbart_predict_calc'></span>

<h3>Description</h3>

<p>Developer functions for predictions via BART models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dbart_predict_calc(obj, new_data, type, level = 0.95, std_err = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bart-internal_+3A_obj">obj</code></td>
<td>
<p>A parsnip object.</p>
</td></tr>
<tr><td><code id="bart-internal_+3A_new_data">new_data</code></td>
<td>
<p>A rectangular data object, such as a data frame.</p>
</td></tr>
<tr><td><code id="bart-internal_+3A_type">type</code></td>
<td>
<p>A single character value or <code>NULL</code>. Possible values
are <code>"numeric"</code>, <code>"class"</code>, <code>"prob"</code>, <code>"conf_int"</code>, <code>"pred_int"</code>,
<code>"quantile"</code>, <code>"time"</code>, <code>"hazard"</code>, <code>"survival"</code>, or <code>"raw"</code>. When <code>NULL</code>,
<code>predict()</code> will choose an appropriate value based on the model's mode.</p>
</td></tr>
<tr><td><code id="bart-internal_+3A_level">level</code></td>
<td>
<p>Confidence level.</p>
</td></tr>
<tr><td><code id="bart-internal_+3A_std_err">std_err</code></td>
<td>
<p>Attach column for standard error of prediction or not.</p>
</td></tr>
</table>

<hr>
<h2 id='boost_tree'>Boosted trees</h2><span id='topic+boost_tree'></span>

<h3>Description</h3>

<p><code>boost_tree()</code> defines a model that creates a series of decision trees
forming an ensemble. Each tree depends on the results of previous trees.
All trees in the ensemble are combined to produce a final prediction. This
function can fit classification, regression, and censored regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_boost_tree_xgboost">xgboost</a>¹</code></p>
</li>
<li><p><code><a href="#topic+details_boost_tree_C5.0">C5.0</a></code></p>
</li>
<li><p><code><a href="#topic+details_boost_tree_h2o">h2o</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_boost_tree_lightgbm">lightgbm</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_boost_tree_mboost">mboost</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_boost_tree_spark">spark</a></code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for censored regression, classification, and regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boost_tree(
  mode = "unknown",
  engine = "xgboost",
  mtry = NULL,
  trees = NULL,
  min_n = NULL,
  tree_depth = NULL,
  learn_rate = NULL,
  loss_reduction = NULL,
  sample_size = NULL,
  stop_iter = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="boost_tree_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;,
&quot;classification&quot;, or &quot;censored regression&quot;.</p>
</td></tr>
<tr><td><code id="boost_tree_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="boost_tree_+3A_mtry">mtry</code></td>
<td>
<p>A number for the number (or proportion) of predictors that will
be randomly sampled at each split when creating the tree models
(specific engines only).</p>
</td></tr>
<tr><td><code id="boost_tree_+3A_trees">trees</code></td>
<td>
<p>An integer for the number of trees contained in
the ensemble.</p>
</td></tr>
<tr><td><code id="boost_tree_+3A_min_n">min_n</code></td>
<td>
<p>An integer for the minimum number of data points
in a node that is required for the node to be split further.</p>
</td></tr>
<tr><td><code id="boost_tree_+3A_tree_depth">tree_depth</code></td>
<td>
<p>An integer for the maximum depth of the tree (i.e. number
of splits) (specific engines only).</p>
</td></tr>
<tr><td><code id="boost_tree_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A number for the rate at which the boosting algorithm adapts
from iteration-to-iteration (specific engines only). This is sometimes referred to
as the shrinkage parameter.</p>
</td></tr>
<tr><td><code id="boost_tree_+3A_loss_reduction">loss_reduction</code></td>
<td>
<p>A number for the reduction in the loss function required
to split further (specific engines only).</p>
</td></tr>
<tr><td><code id="boost_tree_+3A_sample_size">sample_size</code></td>
<td>
<p>A number for the number (or proportion) of data that is
exposed to the fitting routine. For <code>xgboost</code>, the sampling is done at
each iteration while <code>C5.0</code> samples once during training.</p>
</td></tr>
<tr><td><code id="boost_tree_+3A_stop_iter">stop_iter</code></td>
<td>
<p>The number of iterations without improvement before
stopping (specific engines only).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
boost_tree(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_boost_tree_xgboost">xgboost engine details</a></code>, <code><a href="#topic+details_boost_tree_C5.0">C5.0 engine details</a></code>, <code><a href="#topic+details_boost_tree_h2o">h2o engine details</a></code>, <code><a href="#topic+details_boost_tree_lightgbm">lightgbm engine details</a></code>, <code><a href="#topic+details_boost_tree_mboost">mboost engine details</a></code>, <code><a href="#topic+details_boost_tree_spark">spark engine details</a></code>,
<code><a href="#topic+xgb_train">xgb_train()</a></code>, <code><a href="#topic+C5.0_train">C5.0_train()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("boost_tree")

boost_tree(mode = "classification", trees = 20)

</code></pre>

<hr>
<h2 id='C5_rules'>C5.0 rule-based classification models</h2><span id='topic+C5_rules'></span>

<h3>Description</h3>

<p><code>C5_rules()</code> defines a model that derives feature rules from a tree for
prediction. A single tree or boosted ensemble can be used. This function can
fit classification models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_C5_rules_C5.0">C5.0</a>¹²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>C5_rules(mode = "classification", trees = NULL, min_n = NULL, engine = "C5.0")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="C5_rules_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model.
The only possible value for this model is &quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="C5_rules_+3A_trees">trees</code></td>
<td>
<p>A non-negative integer (no greater than 100) for the number
of members of the ensemble.</p>
</td></tr>
<tr><td><code id="C5_rules_+3A_min_n">min_n</code></td>
<td>
<p>An integer greater between zero and nine for the minimum number
of data points in a node that are required for the node to be split further.</p>
</td></tr>
<tr><td><code id="C5_rules_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>C5.0 is a classification model that is an extension of the C4.5
model of Quinlan (1993). It has tree- and rule-based versions that also
include boosting capabilities. <code>C5_rules()</code> enables the version of the model
that uses a series of rules (see the examples below). To make a set of
rules, an initial C5.0 tree is created and flattened into rules. The rules
are pruned, simplified, and ordered. Rule sets are created within each
iteration of boosting.
</p>
<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
C5_rules(argument = !!value)
</pre></div>


<h3>References</h3>

<p>Quinlan R (1993). <em>C4.5: Programs for Machine Learning</em>. Morgan
Kaufmann Publishers.
</p>
<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="C50.html#topic+C5.0">C50::C5.0()</a></code>, <code><a href="C50.html#topic+C5.0Control">C50::C5.0Control()</a></code>,
<code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_C5_rules_C5.0">C5.0 engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("C5_rules")

C5_rules()

</code></pre>

<hr>
<h2 id='C5.0_train'>Boosted trees via C5.0</h2><span id='topic+C5.0_train'></span>

<h3>Description</h3>

<p><code>C5.0_train</code> is a wrapper for the <code>C5.0()</code> function in the
<span class="pkg">C50</span> package that fits tree-based models
where all of the model arguments are in the main function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>C5.0_train(x, y, weights = NULL, trials = 15, minCases = 2, sample = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="C5.0_train_+3A_x">x</code></td>
<td>
<p>A data frame or matrix of predictors.</p>
</td></tr>
<tr><td><code id="C5.0_train_+3A_y">y</code></td>
<td>
<p>A factor vector with 2 or more levels</p>
</td></tr>
<tr><td><code id="C5.0_train_+3A_weights">weights</code></td>
<td>
<p>An optional numeric vector of case weights. Note
that the data used for the case weights will not be used as a
splitting variable in the model (see
<a href="https://www.rulequest.com/see5-info.html">https://www.rulequest.com/see5-info.html</a> for
Quinlan's notes on case weights).</p>
</td></tr>
<tr><td><code id="C5.0_train_+3A_trials">trials</code></td>
<td>
<p>An integer specifying the number of boosting
iterations. A value of one indicates that a single model is
used.</p>
</td></tr>
<tr><td><code id="C5.0_train_+3A_mincases">minCases</code></td>
<td>
<p>An integer for the smallest number of samples
that must be put in at least two of the splits.</p>
</td></tr>
<tr><td><code id="C5.0_train_+3A_sample">sample</code></td>
<td>
<p>A value between (0, .999) that specifies the
random proportion of the data should be used to train the model.
By default, all the samples are used for model training. Samples
not used for training are used to evaluate the accuracy of the
model in the printed output. A value of zero means that all the training
data are used.</p>
</td></tr>
<tr><td><code id="C5.0_train_+3A_...">...</code></td>
<td>
<p>Other arguments to pass.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A fitted C5.0 model.
</p>

<hr>
<h2 id='case_weights'>Using case weights with parsnip</h2><span id='topic+case_weights'></span>

<h3>Description</h3>

<p>Case weights are positive numeric values that influence how much each data
point has during the model fitting process. There are a variety of situations
where case weights can be used.
</p>


<h3>Details</h3>

<p>tidymodels packages differentiate <em>how</em> different types of case weights
should be used during the entire data analysis process, including
preprocessing data, model fitting, performance calculations, etc.
</p>
<p>The tidymodels packages require users to convert their numeric vectors to a
vector class that reflects how these should be used. For example, there are
some situations where the weights should not affect operations such as
centering and scaling or other preprocessing operations.
</p>
<p>The types of weights allowed in tidymodels are:
</p>

<ul>
<li><p> Frequency weights via <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>
</p>
</li>
<li><p> Importance weights via <code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>
</p>
</li></ul>

<p>More types can be added by request.
</p>
<p>For parsnip, the <code><a href="#topic+fit">fit()</a></code> and <code><a href="#topic+fit_xy">fit_xy()</a></code> functions contain a <code>case_weight</code>
argument that takes these data. For Spark models, the argument value should
be a character value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+frequency_weights">frequency_weights()</a></code>, <code><a href="#topic+importance_weights">importance_weights()</a></code>, <code><a href="#topic+fit">fit()</a></code>, <code><a href="#topic+fit_xy">fit_xy()</a></code>
</p>

<hr>
<h2 id='case_weights_allowed'>Determine if case weights are used</h2><span id='topic+case_weights_allowed'></span>

<h3>Description</h3>

<p>Not all modeling engines can incorporate case weights into their
calculations. This function can determine whether they can be used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>case_weights_allowed(spec)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="case_weights_allowed_+3A_spec">spec</code></td>
<td>
<p>A parsnip <a href="#topic+model_spec">model specification</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single logical.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>case_weights_allowed(linear_reg())
case_weights_allowed(linear_reg(engine = "keras"))
</code></pre>

<hr>
<h2 id='censoring_weights'>Calculations for inverse probability of censoring weights (IPCW)</h2><span id='topic+censoring_weights'></span><span id='topic+.censoring_weights_graf'></span><span id='topic+.censoring_weights_graf.default'></span><span id='topic+.censoring_weights_graf.model_fit'></span>

<h3>Description</h3>

<p>The method of Graf <em>et al</em> (1999) is used to compute weights at specific
evaluation times that can be used to help measure a model's time-dependent
performance (e.g. the time-dependent Brier score or the area under the ROC
curve). This is an internal function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.censoring_weights_graf(object, ...)

## Default S3 method:
.censoring_weights_graf(object, ...)

## S3 method for class 'model_fit'
.censoring_weights_graf(
  object,
  predictions,
  cens_predictors = NULL,
  trunc = 0.05,
  eps = 10^-10,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="censoring_weights_+3A_object">object</code></td>
<td>
<p>A fitted parsnip model object or fitted workflow with a mode
of &quot;censored regression&quot;.</p>
</td></tr>
<tr><td><code id="censoring_weights_+3A_predictions">predictions</code></td>
<td>
<p>A data frame with a column containing a <code><a href="survival.html#topic+Surv">survival::Surv()</a></code>
object as well as a list column called <code>.pred</code> that contains the data
structure produced by <code><a href="#topic+predict.model_fit">predict.model_fit()</a></code>.</p>
</td></tr>
<tr><td><code id="censoring_weights_+3A_cens_predictors">cens_predictors</code></td>
<td>
<p>Not currently used. A potential future slot for models with
informative censoring based on columns in <code>predictions</code>.</p>
</td></tr>
<tr><td><code id="censoring_weights_+3A_trunc">trunc</code></td>
<td>
<p>A potential lower bound for the probability of censoring to avoid
very large weight values.</p>
</td></tr>
<tr><td><code id="censoring_weights_+3A_eps">eps</code></td>
<td>
<p>A small value that is subtracted from the evaluation time when
computing the censoring probabilities. See Details below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A probability that the data are censored immediately prior to a specific
time is computed. To do this, we must determine what time to
make the prediction. There are two time values for each row of the data set:
the observed time (either censored or not) and the time that the model is
being evaluated at (e.g. the survival function prediction at some time point),
which is constant across rows. .
</p>
<p>From  Graf <em>et al</em> (1999) there are three cases:
</p>

<ul>
<li><p> If the observed time is a censoring time and that is before the
evaluation time, the data point should make no contribution to the
performance metric (their &quot;category 3&quot;). These values have a missing
value for their probability estimate (and also for their weight column).
</p>
</li>
<li><p> If the observed time corresponds to an actual event, and that time is
prior to the evaluation time (category 1), the probability of being
censored is predicted at the observed time (minus an epsilon).
</p>
</li>
<li><p> If the observed time is <em>after</em> the evaluation time (category 2), regardless of
the status, the probability of being censored is predicted at the evaluation
time (minus an epsilon).
</p>
</li></ul>

<p>The epsilon is used since, we would not have actual information at time <code>t</code>
for a data point being predicted at time <code>t</code> (only data prior to time <code>t</code>
should be available).
</p>
<p>After the censoring probability is computed, the <code>trunc</code> option is used to
avoid using numbers pathologically close to zero. After this, the weight is
computed by inverting the censoring probability.
</p>
<p>The <code>eps</code> argument is used to avoid information leakage when computing the
censoring probability. Subtracting a small number avoids using data that
would not be known at the time of prediction. For example, if we are making
survival probability predictions at <code>eval_time = 3.0</code>, we would <em>not</em> know the
about the probability of being censored at that exact time (since it has not
occurred yet).
</p>
<p>When creating weights by inverting probabilities, there is the risk that a few
cases will have severe outliers due to probabilities close to zero. To
mitigate this, the <code>trunc</code> argument can be used to put a cap on the weights.
If the smallest probability is greater than <code>trunc</code>, the probabilities with
values less than <code>trunc</code> are given that value. Otherwise,  <code>trunc</code> is
adjusted to be half of the smallest probability and that value is used as the
lower bound..
</p>
<p>Note that if there are <code>n</code> rows in <code>data</code> and <code>t</code> time points, the resulting
data, once unnested, has <code>n * t</code> rows. Computations will not easily scale
well as <code>t</code> becomes very large.
</p>


<h3>Value</h3>

<p>The same data are returned with the <code>pred</code> tibbles containing
several new columns:
</p>

<ul>
<li> <p><code>.weight_time</code>: the time at which the inverse censoring probability weights
are computed. This is a function of the observed time and the time of
analysis (i.e., <code>eval_time</code>). See Details for more information.
</p>
</li>
<li> <p><code>.pred_censored</code>: the probability of being censored at <code>.weight_time</code>.
</p>
</li>
<li> <p><code>.weight_censored</code>: The inverse of the censoring probability.
</p>
</li></ul>



<h3>References</h3>

<p>Graf, E., Schmoor, C., Sauerbrei, W. and Schumacher, M. (1999),
Assessment and comparison of prognostic classification schemes for survival
data. <em>Statist. Med.</em>, 18: 2529-2545.
</p>

<hr>
<h2 id='check_empty_ellipse'>Check to ensure that ellipses are empty</h2><span id='topic+check_empty_ellipse'></span>

<h3>Description</h3>

<p>Check to ensure that ellipses are empty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_empty_ellipse(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_empty_ellipse_+3A_...">...</code></td>
<td>
<p>Extra arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If an error is not thrown (from non-empty ellipses), a NULL list.
</p>

<hr>
<h2 id='condense_control'>Condense control object into strictly smaller control object</h2><span id='topic+condense_control'></span>

<h3>Description</h3>

<p>This function is used to help the hierarchy of control functions used
throughout the tidymodels packages. It is now assumed that each control
function is either a subset or a superset of another control function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>condense_control(x, ref, ..., call = rlang::caller_env())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="condense_control_+3A_x">x</code></td>
<td>
<p>A control object to be condensed.</p>
</td></tr>
<tr><td><code id="condense_control_+3A_ref">ref</code></td>
<td>
<p>A control object that is used to determine what element should be
kept.</p>
</td></tr>
<tr><td><code id="condense_control_+3A_call">call</code></td>
<td>
<p>The execution environment of a currently running function, e.g.
<code>caller_env()</code>. The function will be mentioned in error messages as the
source of the error. See the call argument of <code><a href="rlang.html#topic+abort">rlang::abort()</a></code> for more
information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A control object with the same elements and classes of <code>ref</code>, with
values of <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
ctrl &lt;- control_parsnip(catch = TRUE)
ctrl$allow_par &lt;- TRUE
str(ctrl)

ctrl &lt;- condense_control(ctrl, control_parsnip())
str(ctrl)

</code></pre>

<hr>
<h2 id='control_parsnip'>Control the fit function</h2><span id='topic+control_parsnip'></span>

<h3>Description</h3>

<p>Pass options to the <code><a href="#topic+fit.model_spec">fit.model_spec()</a></code> function to control its
output and computations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_parsnip(verbosity = 1L, catch = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="control_parsnip_+3A_verbosity">verbosity</code></td>
<td>
<p>An integer to control how verbose the output is. For a
value of zero, no messages or output are shown when packages are loaded or
when the model is fit. For a value of 1, package loading is quiet but model
fits can produce output to the screen (depending on if they contain their
own <code>verbose</code>-type argument). For a value of 2 or more, any output at all
is displayed and the execution time of the fit is recorded and printed.</p>
</td></tr>
<tr><td><code id="control_parsnip_+3A_catch">catch</code></td>
<td>
<p>A logical where a value of <code>TRUE</code> will evaluate the model
inside of <code>try(, silent = TRUE)</code>. If the model fails, an object is still
returned (without an error) that inherits the class &quot;try-error&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An S3 object with class &quot;control_parsnip&quot; that is a named list
with the results of the function call
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
control_parsnip(verbosity = 2L)

</code></pre>

<hr>
<h2 id='convert_stan_interval'>Convenience function for intervals</h2><span id='topic+convert_stan_interval'></span>

<h3>Description</h3>

<p>Convenience function for intervals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_stan_interval(x, level = 0.95, lower = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="convert_stan_interval_+3A_x">x</code></td>
<td>
<p>A fitted model object</p>
</td></tr>
<tr><td><code id="convert_stan_interval_+3A_level">level</code></td>
<td>
<p>Level of uncertainty for intervals</p>
</td></tr>
<tr><td><code id="convert_stan_interval_+3A_lower">lower</code></td>
<td>
<p>Is <code>level</code> the lower level?</p>
</td></tr>
</table>

<hr>
<h2 id='ctree_train'>A wrapper function for conditional inference tree models</h2><span id='topic+ctree_train'></span><span id='topic+cforest_train'></span>

<h3>Description</h3>

<p>These functions are slightly different APIs for <code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code> and
<code><a href="partykit.html#topic+cforest">partykit::cforest()</a></code> that have several important arguments as top-level
arguments (as opposed to being specified in <code><a href="partykit.html#topic+ctree_control">partykit::ctree_control()</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ctree_train(
  formula,
  data,
  weights = NULL,
  minsplit = 20L,
  maxdepth = Inf,
  teststat = "quadratic",
  testtype = "Bonferroni",
  mincriterion = 0.95,
  ...
)

cforest_train(
  formula,
  data,
  weights = NULL,
  minsplit = 20L,
  maxdepth = Inf,
  teststat = "quadratic",
  testtype = "Univariate",
  mincriterion = 0,
  mtry = ceiling(sqrt(ncol(data) - 1)),
  ntree = 500L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ctree_train_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fit.</p>
</td></tr>
<tr><td><code id="ctree_train_+3A_data">data</code></td>
<td>
<p>A data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="ctree_train_+3A_weights">weights</code></td>
<td>
<p>A vector of weights whose length is the same as <code>nrow(data)</code>.
For <code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code> models, these are required to be non-negative
integers while for <code><a href="partykit.html#topic+cforest">partykit::cforest()</a></code> they can be non-negative integers
or doubles.</p>
</td></tr>
<tr><td><code id="ctree_train_+3A_minsplit">minsplit</code></td>
<td>
<p>The minimum sum of weights in a node in order to be
considered for splitting.</p>
</td></tr>
<tr><td><code id="ctree_train_+3A_maxdepth">maxdepth</code></td>
<td>
<p>maximum depth of the tree. The default <code>maxdepth = Inf</code>
means that no restrictions are applied to tree sizes.</p>
</td></tr>
<tr><td><code id="ctree_train_+3A_teststat">teststat</code></td>
<td>
<p>A character specifying the type of the test statistic to be
applied.</p>
</td></tr>
<tr><td><code id="ctree_train_+3A_testtype">testtype</code></td>
<td>
<p>A character specifying how to compute the distribution of
the test statistic.</p>
</td></tr>
<tr><td><code id="ctree_train_+3A_mincriterion">mincriterion</code></td>
<td>
<p>The value of the test statistic (for <code>testtype ==
"Teststatistic"</code>), or 1 - p-value (for other values of <code>testtype</code>) that
must be exceeded in order to implement a split.</p>
</td></tr>
<tr><td><code id="ctree_train_+3A_...">...</code></td>
<td>
<p>Other options to pass to <code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code> or <code><a href="partykit.html#topic+cforest">partykit::cforest()</a></code>.</p>
</td></tr>
<tr><td><code id="ctree_train_+3A_mtry">mtry</code></td>
<td>
<p>Number of input variables randomly sampled as candidates at each
node for random forest like algorithms. The default <code>mtry = Inf</code> means
that no random selection takes place.</p>
</td></tr>
<tr><td><code id="ctree_train_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees to grow in a forest.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>party</code> (for <code>ctree</code>) or <code>cforest</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (rlang::is_installed(c("modeldata", "partykit"))) {
  data(bivariate, package = "modeldata")
  ctree_train(Class ~ ., data = bivariate_train)
  ctree_train(Class ~ ., data = bivariate_train, maxdepth = 1)
}

</code></pre>

<hr>
<h2 id='cubist_rules'>Cubist rule-based regression models</h2><span id='topic+cubist_rules'></span>

<h3>Description</h3>

<p><code>cubist_rules()</code> defines a model that derives simple feature rules from a tree
ensemble and creates regression models within each rule. This function can fit
regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_cubist_rules_Cubist">Cubist</a>¹²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cubist_rules(
  mode = "regression",
  committees = NULL,
  neighbors = NULL,
  max_rules = NULL,
  engine = "Cubist"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cubist_rules_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model.
The only possible value for this model is &quot;regression&quot;.</p>
</td></tr>
<tr><td><code id="cubist_rules_+3A_committees">committees</code></td>
<td>
<p>A non-negative integer (no greater than 100) for the number
of members of the ensemble.</p>
</td></tr>
<tr><td><code id="cubist_rules_+3A_neighbors">neighbors</code></td>
<td>
<p>An integer between zero and nine for the number of training
set instances that are used to adjust the model-based prediction.</p>
</td></tr>
<tr><td><code id="cubist_rules_+3A_max_rules">max_rules</code></td>
<td>
<p>The largest number of rules.</p>
</td></tr>
<tr><td><code id="cubist_rules_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cubist is a rule-based ensemble regression model. A basic model tree
(Quinlan, 1992) is created that has a separate linear regression model
corresponding for each terminal node. The paths along the model tree are
flattened into rules and these rules are simplified and pruned. The parameter
<code>min_n</code> is the primary method for controlling the size of each tree while
<code>max_rules</code> controls the number of rules.
</p>
<p>Cubist ensembles are created using <em>committees</em>, which are similar to
boosting. After the first model in the committee is created, the second
model uses a modified version of the outcome data based on whether the
previous model under- or over-predicted the outcome. For iteration <em>m</em>, the
new outcome <code style="white-space: pre;">&#8288;y*&#8288;</code> is computed using
</p>
<p><img src="../help/figures/comittees.png" alt="comittees.png" />
</p>
<p>If a sample is under-predicted on the previous iteration, the outcome is
adjusted so that the next time it is more likely to be over-predicted to
compensate. This adjustment continues for each ensemble iteration. See
Kuhn and Johnson (2013) for details.
</p>
<p>After the model is created, there is also an option for a post-hoc
adjustment that uses the training set (Quinlan, 1993). When a new sample is
predicted by the model, it can be modified by its nearest neighbors in the
original training set. For <em>K</em> neighbors, the model-based predicted value is
adjusted by the neighbor using:
</p>
<p><img src="../help/figures/adjust.png" alt="adjust.png" />
</p>
<p>where <code>t</code> is the training set prediction and <code>w</code> is a weight that is inverse
to the distance to the neighbor.
</p>
<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
cubist_rules(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>
<p>Quinlan R (1992). &quot;Learning with Continuous Classes.&quot; Proceedings
of the 5th Australian Joint Conference On Artificial Intelligence, pp.
343-348.
</p>
<p>Quinlan R (1993).&quot;Combining Instance-Based and Model-Based Learning.&quot;
Proceedings of the Tenth International Conference on Machine Learning, pp.
236-243.
</p>
<p>Kuhn M and Johnson K (2013). <em>Applied Predictive Modeling</em>. Springer.
</p>


<h3>See Also</h3>

<p><code><a href="Cubist.html#topic+cubist.default">Cubist::cubist()</a></code>, <code><a href="Cubist.html#topic+cubistControl">Cubist::cubistControl()</a></code>, <code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_cubist_rules_Cubist">Cubist engine details</a></code>
</p>

<hr>
<h2 id='decision_tree'>Decision trees</h2><span id='topic+decision_tree'></span>

<h3>Description</h3>

<p><code>decision_tree()</code> defines a model as a set of <code style="white-space: pre;">&#8288;if/then&#8288;</code> statements that
creates a tree-based structure. This function can fit classification,
regression, and censored regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_decision_tree_rpart">rpart</a>¹²</code></p>
</li>
<li><p><code><a href="#topic+details_decision_tree_C5.0">C5.0</a></code></p>
</li>
<li><p><code><a href="#topic+details_decision_tree_partykit">partykit</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_decision_tree_spark">spark</a></code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for censored regression, classification, and regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decision_tree(
  mode = "unknown",
  engine = "rpart",
  cost_complexity = NULL,
  tree_depth = NULL,
  min_n = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="decision_tree_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;,
&quot;classification&quot;, or &quot;censored regression&quot;.</p>
</td></tr>
<tr><td><code id="decision_tree_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="decision_tree_+3A_cost_complexity">cost_complexity</code></td>
<td>
<p>A positive number for the the cost/complexity
parameter (a.k.a. <code>Cp</code>) used by CART models (specific engines only).</p>
</td></tr>
<tr><td><code id="decision_tree_+3A_tree_depth">tree_depth</code></td>
<td>
<p>An integer for maximum depth of the tree.</p>
</td></tr>
<tr><td><code id="decision_tree_+3A_min_n">min_n</code></td>
<td>
<p>An integer for the minimum number of data points
in a node that are required for the node to be split further.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
decision_tree(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_decision_tree_rpart">rpart engine details</a></code>, <code><a href="#topic+details_decision_tree_C5.0">C5.0 engine details</a></code>, <code><a href="#topic+details_decision_tree_partykit">partykit engine details</a></code>, <code><a href="#topic+details_decision_tree_spark">spark engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("decision_tree")

decision_tree(mode = "classification", tree_depth = 5)

</code></pre>

<hr>
<h2 id='descriptors'>Data Set Characteristics Available when Fitting Models</h2><span id='topic+descriptors'></span><span id='topic+.obs'></span><span id='topic+.cols'></span><span id='topic+.preds'></span><span id='topic+.facts'></span><span id='topic+.lvls'></span><span id='topic+.x'></span><span id='topic+.y'></span><span id='topic+.dat'></span>

<h3>Description</h3>

<p>When using the <code>fit()</code> functions there are some
variables that will be available for use in arguments. For
example, if the user would like to choose an argument value
based on the current number of rows in a data set, the <code>.obs()</code>
function can be used. See Details below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.cols()

.preds()

.obs()

.lvls()

.facts()

.x()

.y()

.dat()
</code></pre>


<h3>Details</h3>

<p>Existing functions:
</p>

<ul>
<li> <p><code>.obs()</code>: The current number of rows in the data set.
</p>
</li>
<li> <p><code>.preds()</code>: The number of columns in the data set that is
associated with the predictors prior to dummy variable creation.
</p>
</li>
<li> <p><code>.cols()</code>: The number of predictor columns available after dummy
variables are created (if any).
</p>
</li>
<li> <p><code>.facts()</code>: The number of factor predictors in the data set.
</p>
</li>
<li> <p><code>.lvls()</code>: If the outcome is a factor, this is a table
with the counts for each level (and <code>NA</code> otherwise).
</p>
</li>
<li> <p><code>.x()</code>: The predictors returned in the format given. Either a
data frame or a matrix.
</p>
</li>
<li> <p><code>.y()</code>: The known outcomes returned in the format given. Either
a vector, matrix, or data frame.
</p>
</li>
<li> <p><code>.dat()</code>: A data frame containing all of the predictors and the
outcomes. If <code>fit_xy()</code> was used, the outcomes are attached as the
column, <code>..y</code>.
</p>
</li></ul>

<p>For example, if you use the model formula <code>circumference ~ .</code> with the
built-in <code>Orange</code> data, the values would be
</p>
<pre>
 .preds() =   2          (the 2 remaining columns in `Orange`)
 .cols()  =   5          (1 numeric column + 4 from Tree dummy variables)
 .obs()   = 35
 .lvls()  =  NA          (no factor outcome)
 .facts() =   1          (the Tree predictor)
 .y()     = &lt;vector&gt;     (circumference as a vector)
 .x()     = &lt;data.frame&gt; (The other 2 columns as a data frame)
 .dat()   = &lt;data.frame&gt; (The full data set)
</pre>
<p>If the formula <code>Tree ~ .</code> were used:
</p>
<pre>
 .preds() =   2          (the 2 numeric columns in `Orange`)
 .cols()  =   2          (same)
 .obs()   = 35
 .lvls()  =  c("1" = 7, "2" = 7, "3" = 7, "4" = 7, "5" = 7)
 .facts() =   0
 .y()     = &lt;vector&gt;     (Tree as a vector)
 .x()     = &lt;data.frame&gt; (The other 2 columns as a data frame)
 .dat()   = &lt;data.frame&gt; (The full data set)
</pre>
<p>To use these in a model fit, pass them to a model specification.
The evaluation is delayed until the time when the
model is run via <code>fit()</code> (and the variables listed above are available).
For example:
</p>
<pre>

library(modeldata)
data("lending_club")

rand_forest(mode = "classification", mtry = .cols() - 2)
</pre>
<p>When no descriptors are found, the computation of the descriptor values
is not executed.
</p>

<hr>
<h2 id='details_auto_ml_h2o'>Automatic machine learning via h2o</h2><span id='topic+details_auto_ml_h2o'></span>

<h3>Description</h3>

<p><a href="h2o.html#topic+h2o.automl">h2o::h2o.automl</a> defines an automated model training process and returns a
leaderboard of models with best performances.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>
<p>Engine arguments of interest
</p>

<ul>
<li> <p><code>max_runtime_secs</code> and <code>max_models</code>: controls the maximum running time
and number of models to build in the automatic process.
</p>
</li>
<li> <p><code>exclude_algos</code> and <code>include_algos</code>: a character vector indicating the
excluded or included algorithms during model building. To see a full
list of supported models, see the details section in
<code><a href="h2o.html#topic+h2o.automl">h2o::h2o.automl()</a></code>.
</p>
</li>
<li> <p><code>validation</code>: An integer between 0 and 1 specifying the <em>proportion</em>
of training data reserved as validation set. This is used by h2o for
performance assessment and potential early stopping.
</p>
</li></ul>




<h4>Translation from parsnip to the original package (regression)</h4>

<p><code><a href="agua.html#topic+h2o_train">agua::h2o_train_auto()</a></code> is a wrapper around
<code><a href="h2o.html#topic+h2o.automl">h2o::h2o.automl()</a></code>.
</p>
<div class="sourceCode r"><pre>auto_ml() %&gt;%  
  set_engine("h2o") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Automatic Machine Learning Model Specification (regression)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_auto(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), verbosity = NULL)
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>auto_ml() %&gt;%  
  set_engine("h2o") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Automatic Machine Learning Model Specification (classification)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_auto(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), verbosity = NULL)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Initializing h2o</h4>

<p>To use the h2o engine with tidymodels, please run <code>h2o::h2o.init()</code>
first. By default, This connects R to the local h2o server. This needs
to be done in every new R session. You can also connect to a remote h2o
server with an IP address, for more details see
<code><a href="h2o.html#topic+h2o.init">h2o::h2o.init()</a></code>.
</p>
<p>You can control the number of threads in the thread pool used by h2o
with the <code>nthreads</code> argument. By default, it uses all CPUs on the host.
This is different from the usual parallel processing mechanism in
tidymodels for tuning, while tidymodels parallelizes over resamples, h2o
parallelizes over hyperparameter combinations for a given resample.
</p>
<p>h2o will automatically shut down the local h2o instance started by R
when R is terminated. To manually stop the h2o server, run
<code>h2o::h2o.shutdown()</code>.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>


<hr>
<h2 id='details_bag_mars_earth'>Bagged MARS via earth</h2><span id='topic+details_bag_mars_earth'></span>

<h3>Description</h3>

<p><code><a href="baguette.html#topic+bagger">baguette::bagger()</a></code> creates an collection of MARS models forming an
ensemble. All models in the ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>prod_degree</code>: Degree of Interaction (type: integer, default: 1L)
</p>
</li>
<li> <p><code>prune_method</code>: Pruning Method (type: character, default: ‘backward’)
</p>
</li>
<li> <p><code>num_terms</code>: # Model Terms (type: integer, default: see below)
</p>
</li></ul>

<p>The default value of <code>num_terms</code> depends on the number of predictor
columns. For a data frame <code>x</code>, the default is
<code>min(200, max(20, 2 * ncol(x))) + 1</code> (see
<code><a href="earth.html#topic+earth">earth::earth()</a></code> and the reference below).
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<p>The <strong>baguette</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>bag_mars(num_terms = integer(1), prod_degree = integer(1), prune_method = character(1)) %&gt;% 
  set_engine("earth") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Bagged MARS Model Specification (regression)
## 
## Main Arguments:
##   num_terms = integer(1)
##   prod_degree = integer(1)
##   prune_method = character(1)
## 
## Computational engine: earth 
## 
## Model fit template:
## baguette::bagger(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), nprune = integer(1), degree = integer(1), 
##     pmethod = character(1), base_model = "MARS")
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<p>The <strong>baguette</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(baguette)

bag_mars(
  num_terms = integer(1),
  prod_degree = integer(1),
  prune_method = character(1)
) %&gt;% 
  set_engine("earth") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Bagged MARS Model Specification (classification)
## 
## Main Arguments:
##   num_terms = integer(1)
##   prod_degree = integer(1)
##   prune_method = character(1)
## 
## Computational engine: earth 
## 
## Model fit template:
## baguette::bagger(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), nprune = integer(1), degree = integer(1), 
##     pmethod = character(1), base_model = "MARS")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p>Note that the <code>earth</code> package documentation has: “In the current
implementation, <em>building models with weights can be slow</em>.”
</p>



<h4>References</h4>


<ul>
<li><p> Breiman, L. 1996. “Bagging predictors”. Machine Learning. 24 (2):
123-140
</p>
</li>
<li><p> Friedman, J. 1991. “Multivariate Adaptive Regression Splines.” <em>The
Annals of Statistics</em>, vol. 19, no. 1, pp. 1-67.
</p>
</li>
<li><p> Milborrow, S. <a href="http://www.milbo.org/doc/earth-notes.pdf">“Notes on the earth package.”</a>
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_bag_mlp_nnet'>Bagged neural networks via nnet</h2><span id='topic+details_bag_mlp_nnet'></span>

<h3>Description</h3>

<p><code><a href="baguette.html#topic+bagger">baguette::bagger()</a></code> creates a collection of neural networks forming an
ensemble. All trees in the ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>hidden_units</code>: # Hidden Units (type: integer, default: 10L)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li>
<li> <p><code>epochs</code>: # Epochs (type: integer, default: 1000L)
</p>
</li></ul>

<p>These defaults are set by the <code>baguette</code> package and are different than
those in <code><a href="nnet.html#topic+nnet">nnet::nnet()</a></code>.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<p>The <strong>baguette</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(baguette)

bag_mlp(penalty = double(1), hidden_units = integer(1)) %&gt;% 
  set_engine("nnet") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Bagged Neural Network Model Specification (classification)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
## 
## Computational engine: nnet 
## 
## Model fit template:
## baguette::bagger(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), size = integer(1), decay = double(1), 
##     base_model = "nnet")
</pre></div>



<h4>Translation from parsnip to the original package (regression)</h4>

<p>The <strong>baguette</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(baguette)

bag_mlp(penalty = double(1), hidden_units = integer(1)) %&gt;% 
  set_engine("nnet") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Bagged Neural Network Model Specification (regression)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
## 
## Computational engine: nnet 
## 
## Model fit template:
## baguette::bagger(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), size = integer(1), decay = double(1), 
##     base_model = "nnet")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Breiman L. 1996. “Bagging predictors”. Machine Learning. 24 (2):
123-140
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_bag_tree_C5.0'>Bagged trees via C5.0</h2><span id='topic+details_bag_tree_C5.0'></span>

<h3>Description</h3>

<p><code><a href="baguette.html#topic+bagger">baguette::bagger()</a></code> creates an collection of decision trees forming an
ensemble. All trees in the ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 1 tuning parameters:
</p>

<ul>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 2L)
</p>
</li></ul>




<h4>Translation from parsnip to the original package (classification)</h4>

<p>The <strong>baguette</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(baguette)

bag_tree(min_n = integer()) %&gt;% 
  set_engine("C5.0") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Bagged Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   cost_complexity = 0
##   min_n = integer()
## 
## Computational engine: C5.0 
## 
## Model fit template:
## baguette::bagger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     minCases = integer(), base_model = "C5.0")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Breiman, L. 1996. “Bagging predictors”. Machine Learning. 24 (2):
123-140
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_bag_tree_rpart'>Bagged trees via rpart</h2><span id='topic+details_bag_tree_rpart'></span>

<h3>Description</h3>

<p><code><a href="baguette.html#topic+bagger">baguette::bagger()</a></code> and <code><a href="ipred.html#topic+bagging">ipred::bagging()</a></code> create collections of decision
trees forming an ensemble. All trees in the ensemble are combined to produce
a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification, regression,
and censored regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 4 tuning parameters:
</p>

<ul>
<li> <p><code>class_cost</code>: Class Cost (type: double, default: (see below))
</p>
</li>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 30L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 2L)
</p>
</li>
<li> <p><code>cost_complexity</code>: Cost-Complexity Parameter (type: double, default:
0.01)
</p>
</li></ul>

<p>For the <code>class_cost</code> parameter, the value can be a non-negative scalar
for a class cost (where a cost of 1 means no extra cost). This is useful
for when the first level of the outcome factor is the minority class. If
this is not the case, values between zero and one can be used to bias to
the second level of the factor.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<p>The <strong>baguette</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(baguette)

bag_tree(tree_depth = integer(1), min_n = integer(1), cost_complexity = double(1)) %&gt;% 
  set_engine("rpart") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Bagged Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   cost_complexity = double(1)
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: rpart 
## 
## Model fit template:
## baguette::bagger(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), cp = double(1), maxdepth = integer(1), 
##     minsplit = integer(1), base_model = "CART")
</pre></div>



<h4>Translation from parsnip to the original package (regression)</h4>

<p>The <strong>baguette</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(baguette)

bag_tree(tree_depth = integer(1), min_n = integer(1), cost_complexity = double(1)) %&gt;% 
  set_engine("rpart") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Bagged Decision Tree Model Specification (regression)
## 
## Main Arguments:
##   cost_complexity = double(1)
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: rpart 
## 
## Model fit template:
## baguette::bagger(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), cp = double(1), maxdepth = integer(1), 
##     minsplit = integer(1), base_model = "CART")
</pre></div>



<h4>Translation from parsnip to the original package (censored regression)</h4>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

bag_tree(tree_depth = integer(1), min_n = integer(1), cost_complexity = double(1)) %&gt;% 
  set_engine("rpart") %&gt;% 
  set_mode("censored regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Bagged Decision Tree Model Specification (censored regression)
## 
## Main Arguments:
##   cost_complexity = double(1)
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: rpart 
## 
## Model fit template:
## ipred::bagging(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), cp = double(1), maxdepth = integer(1), 
##     minsplit = integer(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Other details</h4>

<p>Predictions of type <code>"time"</code> are predictions of the median survival
time.
</p>



<h4>References</h4>


<ul>
<li><p> Breiman L. 1996. “Bagging predictors”. Machine Learning. 24 (2):
123-140
</p>
</li>
<li><p> Hothorn T, Lausen B, Benner A, Radespiel-Troeger M. 2004. Bagging
Survival Trees. <em>Statistics in Medicine</em>, 23(1), 77–91.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_bart_dbarts'>Bayesian additive regression trees via dbarts</h2><span id='topic+details_bart_dbarts'></span>

<h3>Description</h3>

<p><code><a href="dbarts.html#topic+bart">dbarts::bart()</a></code> creates an ensemble of tree-based model whose training
and assembly is determined using Bayesian analysis.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 4 tuning parameters:
</p>

<ul>
<li> <p><code>trees</code>: # Trees (type: integer, default: 200L)
</p>
</li>
<li> <p><code>prior_terminal_node_coef</code>: Terminal Node Prior Coefficient (type:
double, default: 0.95)
</p>
</li>
<li> <p><code>prior_terminal_node_expo</code>: Terminal Node Prior Exponent (type:
double, default: 2.00)
</p>
</li>
<li> <p><code>prior_outcome_range</code>: Prior for Outcome Range (type: double, default:
2.00)
</p>
</li></ul>




<h4>Important engine-specific options</h4>

<p>Some relevant arguments that can be passed to <code>set_engine()</code>:
</p>

<ul>
<li> <p><code>keepevery</code>, <code>n.thin</code>: Every <code>keepevery</code> draw is kept to be returned
to the user. Useful for “thinning” samples.
</p>
</li>
<li> <p><code>ntree</code>, <code>n.trees</code>: The number of trees in the sum-of-trees
formulation.
</p>
</li>
<li> <p><code>ndpost</code>, <code>n.samples</code>: The number of posterior draws after burn in,
<code>ndpost</code> / <code>keepevery</code> will actually be returned.
</p>
</li>
<li> <p><code>nskip</code>, <code>n.burn</code>: Number of MCMC iterations to be treated as burn in.
</p>
</li>
<li> <p><code>nchain</code>, <code>n.chains</code>: Integer specifying how many independent tree
sets and fits should be calculated.
</p>
</li>
<li> <p><code>nthread</code>, <code>n.threads</code>: Integer specifying how many threads to use.
Depending on the CPU architecture, using more than the number of
chains can degrade performance for small/medium data sets. As such
some calculations may be executed single threaded regardless.
</p>
</li>
<li> <p><code>combinechains</code>, <code>combineChains</code>: Logical; if <code>TRUE</code>, samples will be
returned in arrays of dimensions equal to <code>nchain</code> times <code>ndpost</code>
times number of observations.
</p>
</li></ul>




<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>bart(
  trees = integer(1),
  prior_terminal_node_coef = double(1),
  prior_terminal_node_expo = double(1),
  prior_outcome_range = double(1)
) %&gt;% 
  set_engine("dbarts") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## BART Model Specification (classification)
## 
## Main Arguments:
##   trees = integer(1)
##   prior_terminal_node_coef = double(1)
##   prior_terminal_node_expo = double(1)
##   prior_outcome_range = double(1)
## 
## Computational engine: dbarts 
## 
## Model fit template:
## dbarts::bart(x = missing_arg(), y = missing_arg(), ntree = integer(1), 
##     base = double(1), power = double(1), k = double(1), verbose = FALSE, 
##     keeptrees = TRUE, keepcall = FALSE)
</pre></div>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>bart(
  trees = integer(1),
  prior_terminal_node_coef = double(1),
  prior_terminal_node_expo = double(1),
  prior_outcome_range = double(1)
) %&gt;% 
  set_engine("dbarts") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## BART Model Specification (regression)
## 
## Main Arguments:
##   trees = integer(1)
##   prior_terminal_node_coef = double(1)
##   prior_terminal_node_expo = double(1)
##   prior_outcome_range = double(1)
## 
## Computational engine: dbarts 
## 
## Model fit template:
## dbarts::bart(x = missing_arg(), y = missing_arg(), ntree = integer(1), 
##     base = double(1), power = double(1), k = double(1), verbose = FALSE, 
##     keeptrees = TRUE, keepcall = FALSE)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p><code><a href="dbarts.html#topic+bart">dbarts::bart()</a></code> will also convert the factors to
indicators if the user does not create them first.
</p>



<h4>References</h4>


<ul>
<li><p> Chipman, George, McCulloch. “BART: Bayesian additive regression
trees.” <em>Ann. Appl. Stat.</em> 4 (1) 266 - 298, March 2010.
</p>
</li></ul>



<hr>
<h2 id='details_boost_tree_C5.0'>Boosted trees via C5.0</h2><span id='topic+details_boost_tree_C5.0'></span>

<h3>Description</h3>

<p><code><a href="C50.html#topic+C5.0">C50::C5.0()</a></code> creates a series of classification trees forming an
ensemble. Each tree depends on the results of previous trees. All trees in
the ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>trees</code>: # Trees (type: integer, default: 15L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 2L)
</p>
</li>
<li> <p><code>sample_size</code>: Proportion Observations Sampled (type: double, default:
1.0)
</p>
</li></ul>

<p>The implementation of C5.0 limits the number of trees to be between 1
and 100.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>boost_tree(trees = integer(), min_n = integer(), sample_size = numeric()) %&gt;% 
  set_engine("C5.0") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (classification)
## 
## Main Arguments:
##   trees = integer()
##   min_n = integer()
##   sample_size = numeric()
## 
## Computational engine: C5.0 
## 
## Model fit template:
## parsnip::C5.0_train(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     trials = integer(), minCases = integer(), sample = numeric())
</pre></div>
<p><code><a href="#topic+C5.0_train">C5.0_train()</a></code> is a wrapper around
<code><a href="C50.html#topic+C5.0">C50::C5.0()</a></code> that makes it easier to run this model.
</p>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Other details</h4>



<h5>Early stopping</h5>

<p>By default, early stopping is used. To use the complete set of boosting
iterations, pass <code>earlyStopping = FALSE</code> to
<code><a href="#topic+set_engine">set_engine()</a></code>. Also, it is unlikely that early stopping
will occur if <code>sample_size = 1</code>.
</p>




<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#boost-tree-C5.0">examples</a>
for <code>boost_tree()</code> with the <code>"C5.0"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_boost_tree_h2o'>Boosted trees via h2o</h2><span id='topic+details_boost_tree_h2o'></span>

<h3>Description</h3>

<p><code><a href="h2o.html#topic+h2o.xgboost">h2o::h2o.xgboost()</a></code> creates a series of decision trees
forming an ensemble. Each tree depends on the results of previous trees.
All trees in the ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 8 tuning parameters:
</p>

<ul>
<li> <p><code>trees</code>: # Trees (type: integer, default: 50)
</p>
</li>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 6)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 1)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.3)
</p>
</li>
<li> <p><code>sample_size</code>: # Observations Sampled (type: integer, default: 1)
</p>
</li>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: 1)
</p>
</li>
<li> <p><code>loss_reduction</code>: Minimum Loss Reduction (type: double, default: 0)
</p>
</li>
<li> <p><code>stop_iter</code>: # Iterations Before Stopping (type: integer, default: 0)
</p>
</li></ul>

<p><code>min_n</code> represents the fewest allowed observations in a terminal node,
<code><a href="h2o.html#topic+h2o.xgboost">h2o::h2o.xgboost()</a></code> allows only one row in a leaf
by default.
</p>
<p><code>stop_iter</code> controls early stopping rounds based on the convergence of
the engine parameter <code>stopping_metric</code>. By default,
<code><a href="h2o.html#topic+h2o.xgboost">h2o::h2o.xgboost()</a></code> does not use early stopping.
When <code>stop_iter</code> is not 0, <code><a href="h2o.html#topic+h2o.xgboost">h2o::h2o.xgboost()</a></code>
uses logloss for classification, deviance for regression and anonomaly
score for Isolation Forest. This is mostly useful when used alongside
the engine parameter <code>validation</code>, which is the <strong>proportion</strong> of
train-validation split, parsnip will split and pass the two data frames
to h2o. Then <code><a href="h2o.html#topic+h2o.xgboost">h2o::h2o.xgboost()</a></code> will evaluate
the metric and early stopping criteria on the validation set.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<p><code><a href="agua.html#topic+h2o_train">agua::h2o_train_xgboost()</a></code> is a wrapper
around <code><a href="h2o.html#topic+h2o.xgboost">h2o::h2o.xgboost()</a></code>.
</p>
<p>The <strong>agua</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), tree_depth = integer(), 
  learn_rate = numeric(), min_n = integer(), loss_reduction = numeric(), stop_iter = integer()
) %&gt;%
  set_engine("h2o") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
##   stop_iter = integer()
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_xgboost(x = missing_arg(), y = missing_arg(), 
##     weights = missing_arg(), validation_frame = missing_arg(), 
##     col_sample_rate = integer(), ntrees = integer(), min_rows = integer(), 
##     max_depth = integer(), learn_rate = numeric(), min_split_improvement = numeric(), 
##     stopping_rounds = integer())
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<p>The <strong>agua</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), tree_depth = integer(), 
  learn_rate = numeric(), min_n = integer(), loss_reduction = numeric(), stop_iter = integer()
) %&gt;% 
  set_engine("h2o") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
##   stop_iter = integer()
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_xgboost(x = missing_arg(), y = missing_arg(), 
##     weights = missing_arg(), validation_frame = missing_arg(), 
##     col_sample_rate = integer(), ntrees = integer(), min_rows = integer(), 
##     max_depth = integer(), learn_rate = numeric(), min_split_improvement = numeric(), 
##     stopping_rounds = integer())
</pre></div>



<h4>Preprocessing</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>
<p>Non-numeric predictors (i.e., factors) are internally converted to
numeric. In the classification context, non-numeric outcomes (i.e.,
factors) are also internally converted to numeric.
</p>



<h4>Interpreting <code>mtry</code></h4>

<p>The <code>mtry</code> argument denotes the number of predictors that will be
randomly sampled at each split when creating tree models.
</p>
<p>Some engines, such as <code>"xgboost"</code>, <code>"xrf"</code>, and <code>"lightgbm"</code>, interpret
their analogue to the <code>mtry</code> argument as the <em>proportion</em> of predictors
that will be randomly sampled at each split rather than the <em>count</em>. In
some settings, such as when tuning over preprocessors that influence the
number of predictors, this parameterization is quite
helpful—interpreting <code>mtry</code> as a proportion means that <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> is
always a valid range for that parameter, regardless of input data.
</p>
<p>parsnip and its extensions accommodate this parameterization using the
<code>counts</code> argument: a logical indicating whether <code>mtry</code> should be
interpreted as the number of predictors that will be randomly sampled at
each split. <code>TRUE</code> indicates that <code>mtry</code> will be interpreted in its
sense as a count, <code>FALSE</code> indicates that the argument will be
interpreted in its sense as a proportion.
</p>
<p><code>mtry</code> is a main model argument for
<code><a href="#topic+boost_tree">boost_tree()</a></code> and
<code><a href="#topic+rand_forest">rand_forest()</a></code>, and thus should not have an
engine-specific interface. So, regardless of engine, <code>counts</code> defaults
to <code>TRUE</code>. For engines that support the proportion interpretation
(currently <code>"xgboost"</code> and <code>"xrf"</code>, via the rules package, and
<code>"lightgbm"</code> via the bonsai package) the user can pass the
<code>counts = FALSE</code> argument to <code>set_engine()</code> to supply <code>mtry</code> values
within <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>.
</p>



<h4>Initializing h2o</h4>

<p>To use the h2o engine with tidymodels, please run <code>h2o::h2o.init()</code>
first. By default, This connects R to the local h2o server. This needs
to be done in every new R session. You can also connect to a remote h2o
server with an IP address, for more details see
<code><a href="h2o.html#topic+h2o.init">h2o::h2o.init()</a></code>.
</p>
<p>You can control the number of threads in the thread pool used by h2o
with the <code>nthreads</code> argument. By default, it uses all CPUs on the host.
This is different from the usual parallel processing mechanism in
tidymodels for tuning, while tidymodels parallelizes over resamples, h2o
parallelizes over hyperparameter combinations for a given resample.
</p>
<p>h2o will automatically shut down the local h2o instance started by R
when R is terminated. To manually stop the h2o server, run
<code>h2o::h2o.shutdown()</code>.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>


<hr>
<h2 id='details_boost_tree_lightgbm'>Boosted trees via lightgbm</h2><span id='topic+details_boost_tree_lightgbm'></span>

<h3>Description</h3>

<p><code><a href="lightgbm.html#topic+lgb.train">lightgbm::lgb.train()</a></code> creates a series of decision trees
forming an ensemble. Each tree depends on the results of previous trees.
All trees in the ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: regression and classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 6 tuning parameters:
</p>

<ul>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: -1)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 100)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.1)
</p>
</li>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: see
below)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 20)
</p>
</li>
<li> <p><code>loss_reduction</code>: Minimum Loss Reduction (type: double, default: 0)
</p>
</li></ul>

<p>The <code>mtry</code> parameter gives the <em>number</em> of predictors that will be
randomly sampled at each split. The default is to use all predictors.
</p>
<p>Rather than as a number,
<code><a href="lightgbm.html#topic+lgb.train">lightgbm::lgb.train()</a></code>’s <code>feature_fraction</code>
argument encodes <code>mtry</code> as the <em>proportion</em> of predictors that will be
randomly sampled at each split. parsnip translates <code>mtry</code>, supplied as
the <em>number</em> of predictors, to a proportion under the hood. That is, the
user should still supply the argument as <code>mtry</code> to <code>boost_tree()</code>, and
do so in its sense as a number rather than a proportion; before passing
<code>mtry</code> to <code><a href="lightgbm.html#topic+lgb.train">lightgbm::lgb.train()</a></code>, parsnip will
convert the <code>mtry</code> value to a proportion.
</p>
<p>Note that parsnip’s translation can be overridden via the <code>counts</code>
argument, supplied to <code>set_engine()</code>. By default, <code>counts</code> is set to
<code>TRUE</code>, but supplying the argument <code>counts = FALSE</code> allows the user to
supply <code>mtry</code> as a proportion rather than a number.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<p>The <strong>bonsai</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), tree_depth = integer(), 
  learn_rate = numeric(), min_n = integer(), loss_reduction = numeric()
) %&gt;%
  set_engine("lightgbm") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
## 
## Computational engine: lightgbm 
## 
## Model fit template:
## bonsai::train_lightgbm(x = missing_arg(), y = missing_arg(), 
##     weights = missing_arg(), feature_fraction_bynode = integer(), 
##     num_iterations = integer(), min_data_in_leaf = integer(), 
##     max_depth = integer(), learning_rate = numeric(), min_gain_to_split = numeric(), 
##     verbose = -1, num_threads = 0, seed = sample.int(10^5, 1), 
##     deterministic = TRUE)
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<p>The <strong>bonsai</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), tree_depth = integer(), 
  learn_rate = numeric(), min_n = integer(), loss_reduction = numeric()
) %&gt;% 
  set_engine("lightgbm") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
## 
## Computational engine: lightgbm 
## 
## Model fit template:
## bonsai::train_lightgbm(x = missing_arg(), y = missing_arg(), 
##     weights = missing_arg(), feature_fraction_bynode = integer(), 
##     num_iterations = integer(), min_data_in_leaf = integer(), 
##     max_depth = integer(), learning_rate = numeric(), min_gain_to_split = numeric(), 
##     verbose = -1, num_threads = 0, seed = sample.int(10^5, 1), 
##     deterministic = TRUE)
</pre></div>
<p><code><a href="bonsai.html#topic+train_lightgbm">bonsai::train_lightgbm()</a></code> is a wrapper
around <code><a href="lightgbm.html#topic+lgb.train">lightgbm::lgb.train()</a></code> (and other
functions) that make it easier to run this model.
</p>



<h4>Other details</h4>



<h5>Preprocessing</h5>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>
<p>Non-numeric predictors (i.e., factors) are internally converted to
numeric. In the classification context, non-numeric outcomes (i.e.,
factors) are also internally converted to numeric.
</p>



<h5>Interpreting <code>mtry</code></h5>

<p>The <code>mtry</code> argument denotes the number of predictors that will be
randomly sampled at each split when creating tree models.
</p>
<p>Some engines, such as <code>"xgboost"</code>, <code>"xrf"</code>, and <code>"lightgbm"</code>, interpret
their analogue to the <code>mtry</code> argument as the <em>proportion</em> of predictors
that will be randomly sampled at each split rather than the <em>count</em>. In
some settings, such as when tuning over preprocessors that influence the
number of predictors, this parameterization is quite
helpful—interpreting <code>mtry</code> as a proportion means that <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> is
always a valid range for that parameter, regardless of input data.
</p>
<p>parsnip and its extensions accommodate this parameterization using the
<code>counts</code> argument: a logical indicating whether <code>mtry</code> should be
interpreted as the number of predictors that will be randomly sampled at
each split. <code>TRUE</code> indicates that <code>mtry</code> will be interpreted in its
sense as a count, <code>FALSE</code> indicates that the argument will be
interpreted in its sense as a proportion.
</p>
<p><code>mtry</code> is a main model argument for
<code><a href="#topic+boost_tree">boost_tree()</a></code> and
<code><a href="#topic+rand_forest">rand_forest()</a></code>, and thus should not have an
engine-specific interface. So, regardless of engine, <code>counts</code> defaults
to <code>TRUE</code>. For engines that support the proportion interpretation
(currently <code>"xgboost"</code> and <code>"xrf"</code>, via the rules package, and
<code>"lightgbm"</code> via the bonsai package) the user can pass the
<code>counts = FALSE</code> argument to <code>set_engine()</code> to supply <code>mtry</code> values
within <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>.
</p>



<h5>Bagging</h5>

<p>The <code>sample_size</code> argument is translated to the <code>bagging_fraction</code>
parameter in the <code>param</code> argument of <code>lgb.train</code>. The argument is
interpreted by lightgbm as a <em>proportion</em> rather than a count, so bonsai
internally reparameterizes the <code>sample_size</code> argument with
<code><a href="dials.html#topic+trees">dials::sample_prop()</a></code> during tuning.
</p>
<p>To effectively enable bagging, the user would also need to set the
<code>bagging_freq</code> argument to lightgbm. <code>bagging_freq</code> defaults to 0, which
means bagging is disabled, and a <code>bagging_freq</code> argument of <code>k</code> means
that the booster will perform bagging at every <code>k</code>th boosting iteration.
Thus, by default, the <code>sample_size</code> argument would be ignored without
setting this argument manually. Other boosting libraries, like xgboost,
do not have an analogous argument to <code>bagging_freq</code> and use <code>k = 1</code> when
the analogue to <code>bagging_fraction</code> is in $(0, 1)$. <em>bonsai will thus
automatically set</em> <code>bagging_freq = 1</code> <em>in</em> <code>set_engine("lightgbm", ...)</code>
if <code>sample_size</code> (i.e. <code>bagging_fraction</code>) is not equal to 1 and no
<code>bagging_freq</code> value is supplied. This default can be overridden by
setting the <code>bagging_freq</code> argument to <code>set_engine()</code> manually.
</p>



<h5>Verbosity</h5>

<p>bonsai quiets much of the logging output from
<code><a href="lightgbm.html#topic+lgb.train">lightgbm::lgb.train()</a></code> by default. With
default settings, logged warnings and errors will still be passed on to
the user. To print out all logs during training, set <code>quiet = TRUE</code>.
</p>




<h4>Sparse Data</h4>

<p>This model can utilize sparse data during model fitting and prediction.
Both sparse matrices such as dgCMatrix from the <code>Matrix</code> package and
sparse tibbles from the <code>sparsevctrs</code> package are supported. See
<a href="#topic+sparse_data">sparse_data</a> for more information.
</p>



<h4>Examples</h4>

<p>The “Introduction to bonsai” article contains
<a href="https://bonsai.tidymodels.org/articles/bonsai.html">examples</a> of
<code>boost_tree()</code> with the <code>"lightgbm"</code> engine.
</p>



<h4>References</h4>


<ul>
<li> <p><a href="https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</a>
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_boost_tree_mboost'>Boosted trees</h2><span id='topic+details_boost_tree_mboost'></span>

<h3>Description</h3>

<p><code><a href="mboost.html#topic+blackboost">mboost::blackboost()</a></code> fits a series of decision trees forming an ensemble.
Each tree depends on the results of previous trees. All trees in the
ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: censored regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 5 tuning parameters:
</p>

<ul>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: see
below)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 100L)
</p>
</li>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 2L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 10L)
</p>
</li>
<li> <p><code>loss_reduction</code>: Minimum Loss Reduction (type: double, default: 0)
</p>
</li></ul>

<p>The <code>mtry</code> parameter is related to the number of predictors. The default
is to use all predictors.
</p>



<h4>Translation from parsnip to the original package (censored regression)</h4>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

boost_tree() %&gt;% 
  set_engine("mboost") %&gt;% 
  set_mode("censored regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (censored regression)
## 
## Computational engine: mboost 
## 
## Model fit template:
## censored::blackboost_train(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), family = mboost::CoxPH())
</pre></div>
<p><code>censored::blackboost_train()</code> is a wrapper around
<code><a href="mboost.html#topic+blackboost">mboost::blackboost()</a></code> (and other functions)
that makes it easier to run this model.
</p>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Other details</h4>

<p>Predictions of type <code>"time"</code> are predictions of the mean survival time.
</p>



<h4>References</h4>


<ul>
<li><p> Buehlmann P, Hothorn T. 2007. Boosting algorithms: regularization,
prediction and model fitting. <em>Statistical Science</em>, 22(4), 477–505.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_boost_tree_spark'>Boosted trees via Spark</h2><span id='topic+details_boost_tree_spark'></span>

<h3>Description</h3>

<p><code><a href="sparklyr.html#topic+ml_gradient_boosted_trees">sparklyr::ml_gradient_boosted_trees()</a></code> creates a series of decision trees
forming an ensemble. Each tree depends on the results of previous trees.
All trees in the ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and
regression. However, multiclass classification is not supported yet.
</p>


<h4>Tuning Parameters</h4>

<p>This model has 7 tuning parameters:
</p>

<ul>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 5L)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 20L)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.1)
</p>
</li>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: see
below)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 1L)
</p>
</li>
<li> <p><code>loss_reduction</code>: Minimum Loss Reduction (type: double, default: 0.0)
</p>
</li>
<li> <p><code>sample_size</code>: # Observations Sampled (type: integer, default: 1.0)
</p>
</li></ul>

<p>The <code>mtry</code> parameter is related to the number of predictors. The default
depends on the model mode. For classification, the square root of the
number of predictors is used and for regression, one third of the
predictors are sampled.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric()
) %&gt;%
  set_engine("spark") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
##   sample_size = numeric()
## 
## Computational engine: spark 
## 
## Model fit template:
## sparklyr::ml_gradient_boosted_trees(x = missing_arg(), formula = missing_arg(), 
##     type = "regression", feature_subset_strategy = integer(), 
##     max_iter = integer(), min_instances_per_node = min_rows(integer(0), 
##         x), max_depth = integer(), step_size = numeric(), min_info_gain = numeric(), 
##     subsampling_rate = numeric(), seed = sample.int(10^5, 1))
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric()
) %&gt;% 
  set_engine("spark") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
##   sample_size = numeric()
## 
## Computational engine: spark 
## 
## Model fit template:
## sparklyr::ml_gradient_boosted_trees(x = missing_arg(), formula = missing_arg(), 
##     type = "classification", feature_subset_strategy = integer(), 
##     max_iter = integer(), min_instances_per_node = min_rows(integer(0), 
##         x), max_depth = integer(), step_size = numeric(), min_info_gain = numeric(), 
##     subsampling_rate = numeric(), seed = sample.int(10^5, 1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p>Note that, for spark engines, the <code>case_weight</code> argument value should be
a character string to specify the column with the numeric case weights.
</p>



<h4>Other details</h4>

<p>For models created using the <code>"spark"</code> engine, there are several things
to consider.
</p>

<ul>
<li><p> Only the formula interface to via <code>fit()</code> is available; using
<code>fit_xy()</code> will generate an error.
</p>
</li>
<li><p> The predictions will always be in a Spark table format. The names will
be the same as documented but without the dots.
</p>
</li>
<li><p> There is no equivalent to factor columns in Spark tables so class
predictions are returned as character columns.
</p>
</li>
<li><p> To retain the model object for a new R session (via <code>save()</code>), the
<code>model$fit</code> element of the parsnip object should be serialized via
<code>ml_save(object$fit)</code> and separately saved to disk. In a new session,
the object can be reloaded and reattached to the parsnip object.
</p>
</li></ul>




<h4>References</h4>


<ul>
<li><p> Luraschi, J, K Kuo, and E Ruiz. 2019. <em>Mastering Spark with R</em>.
O’Reilly Media
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_boost_tree_xgboost'>Boosted trees via xgboost</h2><span id='topic+details_boost_tree_xgboost'></span>

<h3>Description</h3>

<p><code><a href="xgboost.html#topic+xgb.train">xgboost::xgb.train()</a></code> creates a series of decision trees forming an
ensemble. Each tree depends on the results of previous trees. All trees in
the ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 8 tuning parameters:
</p>

<ul>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 6L)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 15L)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.3)
</p>
</li>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: see
below)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 1L)
</p>
</li>
<li> <p><code>loss_reduction</code>: Minimum Loss Reduction (type: double, default: 0.0)
</p>
</li>
<li> <p><code>sample_size</code>: Proportion Observations Sampled (type: double, default:
1.0)
</p>
</li>
<li> <p><code>stop_iter</code>: # Iterations Before Stopping (type: integer, default:
Inf)
</p>
</li></ul>

<p>For <code>mtry</code>, the default value of <code>NULL</code> translates to using all
available columns.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric(),
  stop_iter = integer()
) %&gt;%
  set_engine("xgboost") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
##   sample_size = numeric()
##   stop_iter = integer()
## 
## Computational engine: xgboost 
## 
## Model fit template:
## parsnip::xgb_train(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     colsample_bynode = integer(), nrounds = integer(), min_child_weight = integer(), 
##     max_depth = integer(), eta = numeric(), gamma = numeric(), 
##     subsample = numeric(), early_stop = integer(), nthread = 1, 
##     verbose = 0)
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric(),
  stop_iter = integer()
) %&gt;% 
  set_engine("xgboost") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
##   sample_size = numeric()
##   stop_iter = integer()
## 
## Computational engine: xgboost 
## 
## Model fit template:
## parsnip::xgb_train(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     colsample_bynode = integer(), nrounds = integer(), min_child_weight = integer(), 
##     max_depth = integer(), eta = numeric(), gamma = numeric(), 
##     subsample = numeric(), early_stop = integer(), nthread = 1, 
##     verbose = 0)
</pre></div>
<p><code><a href="#topic+xgb_train">xgb_train()</a></code> is a wrapper around
<code><a href="xgboost.html#topic+xgb.train">xgboost::xgb.train()</a></code> (and other functions)
that makes it easier to run this model.
</p>



<h4>Preprocessing requirements</h4>

<p>xgboost does not have a means to translate factor predictors to grouped
splits. Factor/categorical predictors need to be converted to numeric
values (e.g., dummy or indicator variables) for this engine. When using
the formula method via <code><a href="#topic+fit.model_spec">fit.model_spec()</a></code>, parsnip
will convert factor columns to indicators using a one-hot encoding.
</p>
<p>For classification, non-numeric outcomes (i.e., factors) are internally
converted to numeric. For binary classification, the <code>event_level</code>
argument of <code>set_engine()</code> can be set to either <code>"first"</code> or <code>"second"</code>
to specify which level should be used as the event. This can be helpful
when a watchlist is used to monitor performance from with the xgboost
training process.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Sparse Data</h4>

<p>This model can utilize sparse data during model fitting and prediction.
Both sparse matrices such as dgCMatrix from the <code>Matrix</code> package and
sparse tibbles from the <code>sparsevctrs</code> package are supported. See
<a href="#topic+sparse_data">sparse_data</a> for more information.
</p>



<h4>Other details</h4>



<h5>Interfacing with the <code>params</code> argument</h5>

<p>The xgboost function that parsnip indirectly wraps,
<code><a href="xgboost.html#topic+xgb.train">xgboost::xgb.train()</a></code>, takes most arguments via
the <code>params</code> list argument. To supply engine-specific arguments that are
documented in <code><a href="xgboost.html#topic+xgb.train">xgboost::xgb.train()</a></code> as
arguments to be passed via <code>params</code>, supply the list elements directly
as named arguments to <code><a href="#topic+set_engine">set_engine()</a></code> rather than as
elements in <code>params</code>. For example, pass a non-default evaluation metric
like this:
</p>
<div class="sourceCode r"><pre># good
boost_tree() %&gt;%
  set_engine("xgboost", eval_metric = "mae")
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (unknown mode)
## 
## Engine-Specific Arguments:
##   eval_metric = mae
## 
## Computational engine: xgboost
</pre></div>
<p>…rather than this:
</p>
<div class="sourceCode r"><pre># bad
boost_tree() %&gt;%
  set_engine("xgboost", params = list(eval_metric = "mae"))
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (unknown mode)
## 
## Engine-Specific Arguments:
##   params = list(eval_metric = "mae")
## 
## Computational engine: xgboost
</pre></div>
<p>parsnip will then route arguments as needed. In the case that arguments
are passed to <code>params</code> via <code><a href="#topic+set_engine">set_engine()</a></code>, parsnip will
warn and re-route the arguments as needed. Note, though, that arguments
passed to <code>params</code> cannot be tuned.
</p>



<h5>Sparse matrices</h5>

<p>xgboost requires the data to be in a sparse format. If your predictor
data are already in this format, then use
<code><a href="#topic+fit_xy.model_spec">fit_xy.model_spec()</a></code> to pass it to the model
function. Otherwise, parsnip converts the data to this format.
</p>



<h5>Parallel processing</h5>

<p>By default, the model is trained without parallel processing. This can
be change by passing the <code>nthread</code> parameter to
<code><a href="#topic+set_engine">set_engine()</a></code>. However, it is unwise to combine this
with external parallel processing when using the package.
</p>



<h5>Interpreting <code>mtry</code></h5>

<p>The <code>mtry</code> argument denotes the number of predictors that will be
randomly sampled at each split when creating tree models.
</p>
<p>Some engines, such as <code>"xgboost"</code>, <code>"xrf"</code>, and <code>"lightgbm"</code>, interpret
their analogue to the <code>mtry</code> argument as the <em>proportion</em> of predictors
that will be randomly sampled at each split rather than the <em>count</em>. In
some settings, such as when tuning over preprocessors that influence the
number of predictors, this parameterization is quite
helpful—interpreting <code>mtry</code> as a proportion means that <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> is
always a valid range for that parameter, regardless of input data.
</p>
<p>parsnip and its extensions accommodate this parameterization using the
<code>counts</code> argument: a logical indicating whether <code>mtry</code> should be
interpreted as the number of predictors that will be randomly sampled at
each split. <code>TRUE</code> indicates that <code>mtry</code> will be interpreted in its
sense as a count, <code>FALSE</code> indicates that the argument will be
interpreted in its sense as a proportion.
</p>
<p><code>mtry</code> is a main model argument for
<code><a href="#topic+boost_tree">boost_tree()</a></code> and
<code><a href="#topic+rand_forest">rand_forest()</a></code>, and thus should not have an
engine-specific interface. So, regardless of engine, <code>counts</code> defaults
to <code>TRUE</code>. For engines that support the proportion interpretation
(currently <code>"xgboost"</code> and <code>"xrf"</code>, via the rules package, and
<code>"lightgbm"</code> via the bonsai package) the user can pass the
<code>counts = FALSE</code> argument to <code>set_engine()</code> to supply <code>mtry</code> values
within <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>.
</p>



<h5>Early stopping</h5>

<p>The <code>stop_iter()</code> argument allows the model to prematurely stop training
if the objective function does not improve within <code>early_stop</code>
iterations.
</p>
<p>The best way to use this feature is in conjunction with an <em>internal
validation set</em>. To do this, pass the <code>validation</code> parameter of
<code><a href="#topic+xgb_train">xgb_train()</a></code> via the parsnip
<code><a href="#topic+set_engine">set_engine()</a></code> function. This is the
proportion of the training set that should be reserved for measuring
performance (and stopping early).
</p>
<p>If the model specification has <code>early_stop &gt;= trees</code>, <code>early_stop</code> is
converted to <code>trees - 1</code> and a warning is issued.
</p>
<p>Note that, since the <code>validation</code> argument provides an alternative
interface to <code>watchlist</code>, the <code>watchlist</code> argument is guarded by parsnip
and will be ignored (with a warning) if passed.
</p>



<h5>Objective function</h5>

<p>parsnip chooses the objective function based on the characteristics of
the outcome. To use a different loss, pass the <code>objective</code> argument to
<code><a href="#topic+set_engine">set_engine()</a></code> directly.
</p>




<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>
<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#boost-tree-xgboost">examples</a>
for <code>boost_tree()</code> with the <code>"xgboost"</code> engine.
</p>



<h4>References</h4>


<ul>
<li> <p><a href="https://arxiv.org/abs/1603.02754">XGBoost: A Scalable Tree Boosting System</a>
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_C5_rules_C5.0'>C5.0 rule-based classification models</h2><span id='topic+details_C5_rules_C5.0'></span>

<h3>Description</h3>

<p><code><a href="C50.html#topic+C5.0">C50::C5.0()</a></code> fits a model that derives feature rules from a tree for
prediction. A single tree or boosted ensemble can be used. <code><a href="rules.html#topic+rules-internal">rules::c5_fit()</a></code>
is a wrapper around this function.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>trees</code>: # Trees (type: integer, default: 1L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 2L)
</p>
</li></ul>

<p>Note that C5.0 has a tool for <em>early stopping</em> during boosting where
less iterations of boosting are performed than the number requested.
<code>C5_rules()</code> turns this feature off (although it can be re-enabled using
<code><a href="C50.html#topic+C5.0Control">C50::C5.0Control()</a></code>).
</p>



<h4>Translation from parsnip to the underlying model call (classification)</h4>

<p>The <strong>rules</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(rules)

C5_rules(
  trees = integer(1),
  min_n = integer(1)
) %&gt;%
  set_engine("C5.0") %&gt;%
  set_mode("classification") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## C5.0 Model Specification (classification)
## 
## Main Arguments:
##   trees = integer(1)
##   min_n = integer(1)
## 
## Computational engine: C5.0 
## 
## Model fit template:
## rules::c5_fit(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     trials = integer(1), minCases = integer(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>References</h4>


<ul>
<li><p> Quinlan R (1992). “Learning with Continuous Classes.” Proceedings of
the 5th Australian Joint Conference On Artificial Intelligence,
pp. 343-348.
</p>
</li>
<li><p> Quinlan R (1993).”Combining Instance-Based and Model-Based Learning.”
Proceedings of the Tenth International Conference on Machine Learning,
pp. 236-243.
</p>
</li>
<li><p> Kuhn M and Johnson K (2013). <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_cubist_rules_Cubist'>Cubist rule-based regression models</h2><span id='topic+details_cubist_rules_Cubist'></span>

<h3>Description</h3>

<p><code><a href="Cubist.html#topic+cubist.default">Cubist::cubist()</a></code> fits a model that derives simple feature rules from a tree
ensemble and uses creates regression models within each rule.
<code><a href="rules.html#topic+rules-internal">rules::cubist_fit()</a></code> is a wrapper around this function.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>committees</code>: # Committees (type: integer, default: 1L)
</p>
</li>
<li> <p><code>neighbors</code>: # Nearest Neighbors (type: integer, default: 0L)
</p>
</li>
<li> <p><code>max_rules</code>: Max. Rules (type: integer, default: NA_integer)
</p>
</li></ul>




<h4>Translation from parsnip to the underlying model call (regression)</h4>

<p>The <strong>rules</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(rules)

cubist_rules(
  committees = integer(1),
  neighbors = integer(1),
  max_rules = integer(1)
) %&gt;%
  set_engine("Cubist") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Cubist Model Specification (regression)
## 
## Main Arguments:
##   committees = integer(1)
##   neighbors = integer(1)
##   max_rules = integer(1)
## 
## Computational engine: Cubist 
## 
## Model fit template:
## rules::cubist_fit(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     committees = integer(1), neighbors = integer(1), max_rules = integer(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>References</h4>


<ul>
<li><p> Quinlan R (1992). “Learning with Continuous Classes.” Proceedings of
the 5th Australian Joint Conference On Artificial Intelligence,
pp. 343-348.
</p>
</li>
<li><p> Quinlan R (1993).”Combining Instance-Based and Model-Based Learning.”
Proceedings of the Tenth International Conference on Machine Learning,
pp. 236-243.
</p>
</li>
<li><p> Kuhn M and Johnson K (2013). <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_decision_tree_C5.0'>Decision trees via C5.0</h2><span id='topic+details_decision_tree_C5.0'></span>

<h3>Description</h3>

<p><code><a href="C50.html#topic+C5.0">C50::C5.0()</a></code> fits a model as a set of <code style="white-space: pre;">&#8288;if/then&#8288;</code> statements that
creates a tree-based structure.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 1 tuning parameters:
</p>

<ul>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 2L)
</p>
</li></ul>




<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>decision_tree(min_n = integer()) %&gt;% 
  set_engine("C5.0") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   min_n = integer()
## 
## Computational engine: C5.0 
## 
## Model fit template:
## parsnip::C5.0_train(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     minCases = integer(), trials = 1)
</pre></div>
<p><code><a href="#topic+C5.0_train">C5.0_train()</a></code> is a wrapper around
<code><a href="C50.html#topic+C5.0">C50::C5.0()</a></code> that makes it easier to run this model.
</p>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#decision-tree-C5.0">examples</a>
for <code>decision_tree()</code> with the <code>"C5.0"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_decision_tree_partykit'>Decision trees via partykit</h2><span id='topic+details_decision_tree_partykit'></span>

<h3>Description</h3>

<p><code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code> fits a model as a set of if/then statements that creates a
tree-based structure using hypothesis testing methods.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: censored regression,
regression, and classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: see below)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 20L)
</p>
</li></ul>

<p>The <code>tree_depth</code> parameter defaults to <code>0</code> which means no restrictions
are applied to tree depth.
</p>
<p>An engine-specific parameter for this model is:
</p>

<ul>
<li> <p><code>mtry</code>: the number of predictors, selected at random, that are
evaluated for splitting. The default is to use all predictors.
</p>
</li></ul>




<h4>Translation from parsnip to the original package (regression)</h4>

<p>The <strong>bonsai</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(bonsai)

decision_tree(tree_depth = integer(1), min_n = integer(1)) %&gt;% 
  set_engine("partykit") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Decision Tree Model Specification (regression)
## 
## Main Arguments:
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: partykit 
## 
## Model fit template:
## parsnip::ctree_train(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), maxdepth = integer(1), minsplit = min_rows(0L, 
##         data))
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<p>The <strong>bonsai</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(bonsai)

decision_tree(tree_depth = integer(1), min_n = integer(1)) %&gt;% 
  set_engine("partykit") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: partykit 
## 
## Model fit template:
## parsnip::ctree_train(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), maxdepth = integer(1), minsplit = min_rows(0L, 
##         data))
</pre></div>
<p><code>parsnip::ctree_train()</code> is a wrapper around
<code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code> (and other functions) that
makes it easier to run this model.
</p>



<h4>Translation from parsnip to the original package (censored regression)</h4>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

decision_tree(tree_depth = integer(1), min_n = integer(1)) %&gt;% 
  set_engine("partykit") %&gt;% 
  set_mode("censored regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Decision Tree Model Specification (censored regression)
## 
## Main Arguments:
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: partykit 
## 
## Model fit template:
## parsnip::ctree_train(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), maxdepth = integer(1), minsplit = min_rows(0L, 
##         data))
</pre></div>
<p><code>censored::cond_inference_surv_ctree()</code> is a wrapper around
<code><a href="partykit.html#topic+ctree">partykit::ctree()</a></code> (and other functions) that
makes it easier to run this model.
</p>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Other details</h4>

<p>Predictions of type <code>"time"</code> are predictions of the median survival
time.
</p>



<h4>References</h4>


<ul>
<li> <p><a href="https://jmlr.org/papers/v16/hothorn15a.html">partykit: A Modular Toolkit for Recursive Partytioning in R</a>
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_decision_tree_rpart'>Decision trees via CART</h2><span id='topic+details_decision_tree_rpart'></span>

<h3>Description</h3>

<p><code><a href="rpart.html#topic+rpart">rpart::rpart()</a></code> fits a model as a set of <code style="white-space: pre;">&#8288;if/then&#8288;</code> statements that
creates a tree-based structure.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification, regression,
and censored regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 30L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 2L)
</p>
</li>
<li> <p><code>cost_complexity</code>: Cost-Complexity Parameter (type: double, default:
0.01)
</p>
</li></ul>




<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>decision_tree(tree_depth = integer(1), min_n = integer(1), cost_complexity = double(1)) %&gt;% 
  set_engine("rpart") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   cost_complexity = double(1)
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: rpart 
## 
## Model fit template:
## rpart::rpart(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     cp = double(1), maxdepth = integer(1), minsplit = min_rows(0L, 
##         data))
</pre></div>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>decision_tree(tree_depth = integer(1), min_n = integer(1), cost_complexity = double(1)) %&gt;% 
  set_engine("rpart") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Decision Tree Model Specification (regression)
## 
## Main Arguments:
##   cost_complexity = double(1)
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: rpart 
## 
## Model fit template:
## rpart::rpart(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     cp = double(1), maxdepth = integer(1), minsplit = min_rows(0L, 
##         data))
</pre></div>



<h4>Translation from parsnip to the original package (censored regression)</h4>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

decision_tree(
  tree_depth = integer(1),
  min_n = integer(1),
  cost_complexity = double(1)
) %&gt;% 
  set_engine("rpart") %&gt;% 
  set_mode("censored regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Decision Tree Model Specification (censored regression)
## 
## Main Arguments:
##   cost_complexity = double(1)
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: rpart 
## 
## Model fit template:
## pec::pecRpart(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), cp = double(1), maxdepth = integer(1), 
##     minsplit = min_rows(0L, data))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Other details</h4>

<p>Predictions of type <code>"time"</code> are predictions of the mean survival time.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#decision-tree-rpart">examples</a>
for <code>decision_tree()</code> with the <code>"rpart"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_decision_tree_spark'>Decision trees via Spark</h2><span id='topic+details_decision_tree_spark'></span>

<h3>Description</h3>

<p><code><a href="sparklyr.html#topic+ml_decision_tree">sparklyr::ml_decision_tree()</a></code> fits a model as a set of <code style="white-space: pre;">&#8288;if/then&#8288;</code>
statements that creates a tree-based structure.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 5L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 1L)
</p>
</li></ul>




<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>decision_tree(tree_depth = integer(1), min_n = integer(1)) %&gt;% 
  set_engine("spark") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: spark 
## 
## Model fit template:
## sparklyr::ml_decision_tree_classifier(x = missing_arg(), formula = missing_arg(), 
##     max_depth = integer(1), min_instances_per_node = min_rows(0L, 
##         x), seed = sample.int(10^5, 1))
</pre></div>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>decision_tree(tree_depth = integer(1), min_n = integer(1)) %&gt;% 
  set_engine("spark") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Decision Tree Model Specification (regression)
## 
## Main Arguments:
##   tree_depth = integer(1)
##   min_n = integer(1)
## 
## Computational engine: spark 
## 
## Model fit template:
## sparklyr::ml_decision_tree_regressor(x = missing_arg(), formula = missing_arg(), 
##     max_depth = integer(1), min_instances_per_node = min_rows(0L, 
##         x), seed = sample.int(10^5, 1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p>Note that, for spark engines, the <code>case_weight</code> argument value should be
a character string to specify the column with the numeric case weights.
</p>



<h4>Other details</h4>

<p>For models created using the <code>"spark"</code> engine, there are several things
to consider.
</p>

<ul>
<li><p> Only the formula interface to via <code>fit()</code> is available; using
<code>fit_xy()</code> will generate an error.
</p>
</li>
<li><p> The predictions will always be in a Spark table format. The names will
be the same as documented but without the dots.
</p>
</li>
<li><p> There is no equivalent to factor columns in Spark tables so class
predictions are returned as character columns.
</p>
</li>
<li><p> To retain the model object for a new R session (via <code>save()</code>), the
<code>model$fit</code> element of the parsnip object should be serialized via
<code>ml_save(object$fit)</code> and separately saved to disk. In a new session,
the object can be reloaded and reattached to the parsnip object.
</p>
</li></ul>




<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_discrim_flexible_earth'>Flexible discriminant analysis via earth</h2><span id='topic+details_discrim_flexible_earth'></span>

<h3>Description</h3>

<p><code><a href="mda.html#topic+fda">mda::fda()</a></code> (in conjunction with <code><a href="earth.html#topic+earth">earth::earth()</a></code> can fit a nonlinear
discriminant analysis model that uses nonlinear features created using
multivariate adaptive regression splines (MARS). This function can fit
classification models.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameter:
</p>

<ul>
<li> <p><code>num_terms</code>: # Model Terms (type: integer, default: (see below))
</p>
</li>
<li> <p><code>prod_degree</code>: Degree of Interaction (type: integer, default: 1L)
</p>
</li>
<li> <p><code>prune_method</code>: Pruning Method (type: character, default: ‘backward’)
</p>
</li></ul>

<p>The default value of <code>num_terms</code> depends on the number of columns (<code>p</code>):
<code>min(200, max(20, 2 * p)) + 1</code>. Note that <code>num_terms = 1</code> is an
intercept-only model.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>discrim</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(discrim)

discrim_flexible(
  num_terms = integer(0),
  prod_degree = integer(0),
  prune_method = character(0)
) %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Flexible Discriminant Model Specification (classification)
## 
## Main Arguments:
##   num_terms = integer(0)
##   prod_degree = integer(0)
##   prune_method = character(0)
## 
## Computational engine: earth 
## 
## Model fit template:
## mda::fda(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     nprune = integer(0), degree = integer(0), pmethod = character(0), 
##     method = earth::earth)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Hastie, Tibshirani &amp; Buja (1994) Flexible Discriminant Analysis by
Optimal Scoring, <em>Journal of the American Statistical Association</em>,
89:428, 1255-1270
</p>
</li>
<li><p> Friedman (1991). Multivariate Adaptive Regression Splines. <em>The Annals
of Statistics</em>, 19(1), 1-67.
</p>
</li></ul>



<hr>
<h2 id='details_discrim_linear_MASS'>Linear discriminant analysis via MASS</h2><span id='topic+details_discrim_linear_MASS'></span>

<h3>Description</h3>

<p><code><a href="MASS.html#topic+lda">MASS::lda()</a></code> fits a model that estimates a multivariate
distribution for the predictors separately for the data in each class
(Gaussian with a common covariance matrix). Bayes' theorem is used
to compute the probability of each class, given the predictor values.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>discrim</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(discrim)

discrim_linear() %&gt;% 
  set_engine("MASS") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Discriminant Model Specification (classification)
## 
## Computational engine: MASS 
## 
## Model fit template:
## MASS::lda(formula = missing_arg(), data = missing_arg())
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Variance calculations are used in these computations so <em>zero-variance</em>
predictors (i.e., with a single unique value) should be eliminated
before fitting the model.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_discrim_linear_mda'>Linear discriminant analysis via flexible discriminant analysis</h2><span id='topic+details_discrim_linear_mda'></span>

<h3>Description</h3>

<p><code><a href="mda.html#topic+fda">mda::fda()</a></code> (in conjunction with <code><a href="mda.html#topic+gen.ridge">mda::gen.ridge()</a></code> can fit a linear
discriminant analysis model that penalizes the predictor coefficients with a
quadratic penalty (i.e., a ridge or weight decay approach).
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 1 tuning parameter:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 1.0)
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>discrim</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(discrim)

discrim_linear(penalty = numeric(0)) %&gt;% 
  set_engine("mda") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Discriminant Model Specification (classification)
## 
## Main Arguments:
##   penalty = numeric(0)
## 
## Computational engine: mda 
## 
## Model fit template:
## mda::fda(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     lambda = numeric(0), method = mda::gen.ridge, keep.fitted = FALSE)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Variance calculations are used in these computations so <em>zero-variance</em>
predictors (i.e., with a single unique value) should be eliminated
before fitting the model.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Hastie, Tibshirani &amp; Buja (1994) Flexible Discriminant Analysis by
Optimal Scoring, <em>Journal of the American Statistical Association</em>,
89:428, 1255-1270
</p>
</li></ul>



<hr>
<h2 id='details_discrim_linear_sda'>Linear discriminant analysis via James-Stein-type shrinkage estimation</h2><span id='topic+details_discrim_linear_sda'></span>

<h3>Description</h3>

<p><code><a href="sda.html#topic+sda">sda::sda()</a></code> can fit a linear discriminant analysis model that can fit models
between classical discriminant analysis and diagonal discriminant analysis.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameter arguments in
<code><a href="#topic+discrim_linear">discrim_linear()</a></code>.
</p>
<p>However, there are a few engine-specific parameters that can be set or
optimized when calling <code><a href="#topic+set_engine">set_engine()</a></code>:
</p>

<ul>
<li> <p><code>lambda</code>: the shrinkage parameters for the correlation matrix. This
maps to the parameter
<code><a href="dials.html#topic+shrinkage_correlation">dials::shrinkage_correlation()</a></code>.
</p>
</li>
<li> <p><code>lambda.var</code>: the shrinkage parameters for the predictor variances.
This maps to
<code><a href="dials.html#topic+shrinkage_correlation">dials::shrinkage_variance()</a></code>.
</p>
</li>
<li> <p><code>lambda.freqs</code>: the shrinkage parameters for the class frequencies.
This maps to
<code><a href="dials.html#topic+shrinkage_correlation">dials::shrinkage_frequencies()</a></code>.
</p>
</li>
<li> <p><code>diagonal</code>: a logical to make the model covariance diagonal or not.
This maps to
<code><a href="dials.html#topic+shrinkage_correlation">dials::diagonal_covariance()</a></code>.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>discrim</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(discrim)

discrim_linear() %&gt;% 
  set_engine("sda") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Discriminant Model Specification (classification)
## 
## Computational engine: sda 
## 
## Model fit template:
## sda::sda(Xtrain = missing_arg(), L = missing_arg(), verbose = FALSE)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Variance calculations are used in these computations so <em>zero-variance</em>
predictors (i.e., with a single unique value) should be eliminated
before fitting the model.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Ahdesmaki, A., and K. Strimmer. 2010. Feature selection in omics
prediction problems using cat scores and false non-discovery rate
control. Ann. Appl. Stat. 4: 503-519.
<a href="https://arxiv.org/abs/0903.2003">Preprint</a>.
</p>
</li></ul>



<hr>
<h2 id='details_discrim_linear_sparsediscrim'>Linear discriminant analysis via regularization</h2><span id='topic+details_discrim_linear_sparsediscrim'></span>

<h3>Description</h3>

<p>Functions in the <span class="pkg">sparsediscrim</span> package fit different types of linear
discriminant analysis model that regularize the estimates (like the mean or
covariance).
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 1 tuning parameter:
</p>

<ul>
<li> <p><code>regularization_method</code>: Regularization Method (type: character,
default: ‘diagonal’)
</p>
</li></ul>

<p>The possible values of this parameter, and the functions that they
execute, are:
</p>

<ul>
<li> <p><code>"diagonal"</code>: <code><a href="sparsediscrim.html#topic+lda_diag">sparsediscrim::lda_diag()</a></code>
</p>
</li>
<li> <p><code>"min_distance"</code>:
<code><a href="sparsediscrim.html#topic+lda_emp_bayes_eigen">sparsediscrim::lda_emp_bayes_eigen()</a></code>
</p>
</li>
<li> <p><code>"shrink_mean"</code>:
<code><a href="sparsediscrim.html#topic+lda_shrink_mean">sparsediscrim::lda_shrink_mean()</a></code>
</p>
</li>
<li> <p><code>"shrink_cov"</code>:
<code><a href="sparsediscrim.html#topic+lda_shrink_cov">sparsediscrim::lda_shrink_cov()</a></code>
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>discrim</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(discrim)

discrim_linear(regularization_method = character(0)) %&gt;% 
  set_engine("sparsediscrim") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Discriminant Model Specification (classification)
## 
## Main Arguments:
##   regularization_method = character(0)
## 
## Computational engine: sparsediscrim 
## 
## Model fit template:
## discrim::fit_regularized_linear(x = missing_arg(), y = missing_arg(), 
##     regularization_method = character(0))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Variance calculations are used in these computations so <em>zero-variance</em>
predictors (i.e., with a single unique value) should be eliminated
before fitting the model.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li> <p><code>lda_diag()</code>: Dudoit, Fridlyand and Speed (2002) Comparison of
Discrimination Methods for the Classification of Tumors Using Gene
Expression Data, <em>Journal of the American Statistical Association</em>,
97:457, 77-87.
</p>
</li>
<li> <p><code>lda_shrink_mean()</code>: Tong, Chen, Zhao, Improved mean estimation and
its application to diagonal discriminant analysis, <em>Bioinformatics</em>,
Volume 28, Issue 4, 15 February 2012, Pages 531-537.
</p>
</li>
<li> <p><code>lda_shrink_cov()</code>: Pang, Tong and Zhao (2009), Shrinkage-based
Diagonal Discriminant Analysis and Its Applications in
High-Dimensional Data. <em>Biometrics</em>, 65, 1021-1029.
</p>
</li>
<li> <p><code>lda_emp_bayes_eigen()</code>: Srivistava and Kubokawa (2007), Comparison of
Discrimination Methods for High Dimensional Data, <em>Journal of the
Japan Statistical Society</em>, 37:1, 123-134.
</p>
</li></ul>



<hr>
<h2 id='details_discrim_quad_MASS'>Quadratic discriminant analysis via MASS</h2><span id='topic+details_discrim_quad_MASS'></span>

<h3>Description</h3>

<p><code><a href="MASS.html#topic+qda">MASS::qda()</a></code> fits a model that estimates a multivariate
distribution for the predictors separately for the data in each class
(Gaussian with separate covariance matrices). Bayes' theorem is used
to compute the probability of each class, given the predictor values.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>discrim</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(discrim)

discrim_quad() %&gt;% 
  set_engine("MASS") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Quadratic Discriminant Model Specification (classification)
## 
## Computational engine: MASS 
## 
## Model fit template:
## MASS::qda(formula = missing_arg(), data = missing_arg())
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Variance calculations are used in these computations within each outcome
class. For this reason, <em>zero-variance</em> predictors (i.e., with a single
unique value) within each class should be eliminated before fitting the
model.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_discrim_quad_sparsediscrim'>Quadratic discriminant analysis via regularization</h2><span id='topic+details_discrim_quad_sparsediscrim'></span>

<h3>Description</h3>

<p>Functions in the <span class="pkg">sparsediscrim</span> package fit different types of quadratic
discriminant analysis model that regularize the estimates (like the mean or
covariance).
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 1 tuning parameter:
</p>

<ul>
<li> <p><code>regularization_method</code>: Regularization Method (type: character,
default: ‘diagonal’)
</p>
</li></ul>

<p>The possible values of this parameter, and the functions that they
execute, are:
</p>

<ul>
<li> <p><code>"diagonal"</code>: <code><a href="sparsediscrim.html#topic+qda_diag">sparsediscrim::qda_diag()</a></code>
</p>
</li>
<li> <p><code>"shrink_mean"</code>:
<code><a href="sparsediscrim.html#topic+qda_shrink_mean">sparsediscrim::qda_shrink_mean()</a></code>
</p>
</li>
<li> <p><code>"shrink_cov"</code>:
<code><a href="sparsediscrim.html#topic+qda_shrink_cov">sparsediscrim::qda_shrink_cov()</a></code>
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>discrim</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(discrim)

discrim_quad(regularization_method = character(0)) %&gt;% 
  set_engine("sparsediscrim") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Quadratic Discriminant Model Specification (classification)
## 
## Main Arguments:
##   regularization_method = character(0)
## 
## Computational engine: sparsediscrim 
## 
## Model fit template:
## discrim::fit_regularized_quad(x = missing_arg(), y = missing_arg(), 
##     regularization_method = character(0))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Variance calculations are used in these computations within each outcome
class. For this reason, <em>zero-variance</em> predictors (i.e., with a single
unique value) within each class should be eliminated before fitting the
model.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li> <p><code>qda_diag()</code>: Dudoit, Fridlyand and Speed (2002) Comparison of
Discrimination Methods for the Classification of Tumors Using Gene
Expression Data, <em>Journal of the American Statistical Association</em>,
97:457, 77-87.
</p>
</li>
<li> <p><code>qda_shrink_mean()</code>: Tong, Chen, Zhao, Improved mean estimation and
its application to diagonal discriminant analysis, <em>Bioinformatics</em>,
Volume 28, Issue 4, 15 February 2012, Pages 531-537.
</p>
</li>
<li> <p><code>qda_shrink_cov()</code>: Pang, Tong and Zhao (2009), Shrinkage-based
Diagonal Discriminant Analysis and Its Applications in
High-Dimensional Data. <em>Biometrics</em>, 65, 1021-1029.
</p>
</li></ul>



<hr>
<h2 id='details_discrim_regularized_klaR'>Regularized discriminant analysis via klaR</h2><span id='topic+details_discrim_regularized_klaR'></span>

<h3>Description</h3>

<p><code><a href="klaR.html#topic+rda">klaR::rda()</a></code> fits a a model that estimates a multivariate
distribution for the predictors separately for the data in each class. The
structure of the model can be LDA, QDA, or some amalgam of the two. Bayes'
theorem is used to compute the probability of each class, given the
predictor values.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameter:
</p>

<ul>
<li> <p><code>frac_common_cov</code>: Fraction of the Common Covariance Matrix (type:
double, default: (see below))
</p>
</li>
<li> <p><code>frac_identity</code>: Fraction of the Identity Matrix (type: double,
default: (see below))
</p>
</li></ul>

<p>Some special cases for the RDA model:
</p>

<ul>
<li> <p><code>frac_identity = 0</code> and <code>frac_common_cov = 1</code> is a linear discriminant
analysis (LDA) model.
</p>
</li>
<li> <p><code>frac_identity = 0</code> and <code>frac_common_cov = 0</code> is a quadratic
discriminant analysis (QDA) model.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>discrim</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(discrim)

discrim_regularized(frac_identity = numeric(0), frac_common_cov = numeric(0)) %&gt;% 
  set_engine("klaR") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Regularized Discriminant Model Specification (classification)
## 
## Main Arguments:
##   frac_common_cov = numeric(0)
##   frac_identity = numeric(0)
## 
## Computational engine: klaR 
## 
## Model fit template:
## klaR::rda(formula = missing_arg(), data = missing_arg(), lambda = numeric(0), 
##     gamma = numeric(0))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Variance calculations are used in these computations within each outcome
class. For this reason, <em>zero-variance</em> predictors (i.e., with a single
unique value) within each class should be eliminated before fitting the
model.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Friedman, J (1989). Regularized Discriminant Analysis. <em>Journal of the
American Statistical Association</em>, 84, 165-175.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_gen_additive_mod_mgcv'>Generalized additive models via mgcv</h2><span id='topic+details_gen_additive_mod_mgcv'></span>

<h3>Description</h3>

<p><code><a href="mgcv.html#topic+gam">mgcv::gam()</a></code> fits a generalized linear model with additive smoother terms
for continuous predictors.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: regression and classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>select_features</code>: Select Features? (type: logical, default: FALSE)
</p>
</li>
<li> <p><code>adjust_deg_free</code>: Smoothness Adjustment (type: double, default: 1.0)
</p>
</li></ul>




<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>gen_additive_mod(adjust_deg_free = numeric(1), select_features = logical(1)) %&gt;% 
  set_engine("mgcv") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## GAM Model Specification (regression)
## 
## Main Arguments:
##   select_features = logical(1)
##   adjust_deg_free = numeric(1)
## 
## Computational engine: mgcv 
## 
## Model fit template:
## mgcv::gam(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     select = logical(1), gamma = numeric(1))
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>gen_additive_mod(adjust_deg_free = numeric(1), select_features = logical(1)) %&gt;% 
  set_engine("mgcv") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## GAM Model Specification (classification)
## 
## Main Arguments:
##   select_features = logical(1)
##   adjust_deg_free = numeric(1)
## 
## Computational engine: mgcv 
## 
## Model fit template:
## mgcv::gam(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     select = logical(1), gamma = numeric(1), family = stats::binomial(link = "logit"))
</pre></div>



<h4>Model fitting</h4>

<p>This model should be used with a model formula so that smooth terms can
be specified. For example:
</p>
<div class="sourceCode r"><pre>library(mgcv)
gen_additive_mod() %&gt;% 
  set_engine("mgcv") %&gt;% 
  set_mode("regression") %&gt;% 
  fit(mpg ~ wt + gear + cyl + s(disp, k = 10), data = mtcars)
</pre></div>
<div class="sourceCode"><pre>## parsnip model object
## 
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## mpg ~ wt + gear + cyl + s(disp, k = 10)
## 
## Estimated degrees of freedom:
## 7.52  total = 11.52 
## 
## GCV score: 4.225228
</pre></div>
<p>The smoothness of the terms will need to be manually specified (e.g.,
using <code>s(x, df = 10)</code>) in the formula. Tuning can be accomplished using
the <code>adjust_deg_free</code> parameter.
</p>
<p>When using a workflow, pass the <em>model formula</em> to
<code><a href="workflows.html#topic+add_model">workflows::add_model()</a></code>’s <code>formula</code> argument,
and a simplified <em>preprocessing formula</em> elsewhere.
</p>
<div class="sourceCode r"><pre>spec &lt;- 
  gen_additive_mod() %&gt;% 
  set_engine("mgcv") %&gt;% 
  set_mode("regression")

workflow() %&gt;% 
  add_model(spec, formula = mpg ~ wt + gear + cyl + s(disp, k = 10)) %&gt;% 
  add_formula(mpg ~ wt + gear + cyl + disp) %&gt;% 
  fit(data = mtcars) %&gt;% 
  extract_fit_engine()
</pre></div>
<div class="sourceCode"><pre>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## mpg ~ wt + gear + cyl + s(disp, k = 10)
## 
## Estimated degrees of freedom:
## 7.52  total = 11.52 
## 
## GCV score: 4.225228
</pre></div>
<p>To learn more about the differences between these formulas, see
<code><a href="#topic+model_formula">?model_formula</a></code>.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>References</h4>


<ul>
<li><p> Ross, W. 2021. <a href="https://noamross.github.io/gams-in-r-course/"><em>Generalized Additive Models in R: A Free, Interactive Course using mgcv</em></a>
</p>
</li>
<li><p> Wood, S. 2017. <em>Generalized Additive Models: An Introduction with R</em>.
Chapman and Hall/CRC.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_brulee'>Linear regression via brulee</h2><span id='topic+details_linear_reg_brulee'></span>

<h3>Description</h3>

<p><code><a href="brulee.html#topic+brulee_linear_reg">brulee::brulee_linear_reg()</a></code> uses ordinary least squares to fit models with
numeric outcomes.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameter:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.001)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 0.0)
</p>
</li></ul>

<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as glmnet).
The zeroing out of parameters is a specific feature the optimization
method used in those packages.
</p>
<p>Other engine arguments of interest:
</p>

<ul>
<li> <p><code>optimizer()</code>: The optimization method. See
<code><a href="brulee.html#topic+brulee_linear_reg">brulee::brulee_linear_reg()</a></code>.
</p>
</li>
<li> <p><code>epochs()</code>: An integer for the number of passes through the training
set.
</p>
</li>
<li> <p><code>lean_rate()</code>: A number used to accelerate the gradient decsent
process.
</p>
</li>
<li> <p><code>momentum()</code>: A number used to use historical gradient infomration
during optimization (<code>optimizer = "SGD"</code> only).
</p>
</li>
<li> <p><code>batch_size()</code>: An integer for the number of training set points in
each batch.
</p>
</li>
<li> <p><code>stop_iter()</code>: A non-negative integer for how many iterations with no
improvement before stopping. (default: 5L).
</p>
</li></ul>




<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>linear_reg(penalty = double(1)) %&gt;%  
  set_engine("brulee") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = double(1)
## 
## Computational engine: brulee 
## 
## Model fit template:
## brulee::brulee_linear_reg(x = missing_arg(), y = missing_arg(), 
##     penalty = double(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_gee'>Linear regression via generalized estimating equations (GEE)</h2><span id='topic+details_linear_reg_gee'></span>

<h3>Description</h3>

<p><code>gee::gee()</code> uses generalized least squares to fit different types of models
with errors that are not independent.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no formal tuning parameters. It may be beneficial to
determine the appropriate correlation structure to use, but this
typically does not affect the predicted value of the model. It <em>does</em>
have an effect on the inferential results and parameter covariance
values.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

linear_reg() %&gt;% 
  set_engine("gee") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Computational engine: gee 
## 
## Model fit template:
## multilevelmod::gee_fit(formula = missing_arg(), data = missing_arg(), 
##     family = gaussian)
</pre></div>
<p><code>multilevelmod::gee_fit()</code> is a wrapper model around <code>gee::gee()</code>.
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model cannot accept case weights.
</p>
<p>Both <code>gee:gee()</code> and <code>gee:geepack()</code> specify the id/cluster variable
using an argument <code>id</code> that requires a vector. parsnip doesn’t work that
way so we enable this model to be fit using a artificial function
<code>id_var()</code> to be used in the formula. So, in the original package, the
call would look like:
</p>
<div class="sourceCode r"><pre>gee(breaks ~ tension, id = wool, data = warpbreaks, corstr = "exchangeable")
</pre></div>
<p>With parsnip, we suggest using the formula method when fitting:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

linear_reg() %&gt;% 
  set_engine("gee", corstr = "exchangeable") %&gt;% 
  fit(breaks ~ tension + id_var(wool), data = warpbreaks)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the GEE formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

gee_spec &lt;- 
  linear_reg() %&gt;% 
  set_engine("gee", corstr = "exchangeable")

gee_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = breaks, predictors = c(tension, wool)) %&gt;% 
  add_model(gee_spec, formula = breaks ~ tension + id_var(wool))

fit(gee_wflow, data = warpbreaks)
</pre></div>
<p>The <code>gee::gee()</code> function always prints out warnings and output even
when <code>silent = TRUE</code>. The parsnip <code>"gee"</code> engine, by contrast, silences
all console output coming from <code>gee::gee()</code>, even if <code>silent = FALSE</code>.
</p>
<p>Also, because of issues with the <code>gee()</code> function, a supplementary call
to <code>glm()</code> is needed to get the rank and QR decomposition objects so
that <code>predict()</code> can be used.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Liang, K.Y. and Zeger, S.L. (1986) Longitudinal data analysis using
generalized linear models. <em>Biometrika</em>, 73 13–22.
</p>
</li>
<li><p> Zeger, S.L. and Liang, K.Y. (1986) Longitudinal data analysis for
discrete and continuous outcomes. <em>Biometrics</em>, 42 121–130.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_glm'>Linear regression via glm</h2><span id='topic+details_linear_reg_glm'></span>

<h3>Description</h3>

<p><code><a href="stats.html#topic+glm">stats::glm()</a></code> fits a generalized linear model for numeric outcomes. A
linear combination of the predictors is used to model the numeric outcome
via a link function.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters but you can set the <code>family</code>
parameter (and/or <code>link</code>) as an engine argument (see below).
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>linear_reg() %&gt;% 
  set_engine("glm") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Computational engine: glm 
## 
## Model fit template:
## stats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     family = stats::gaussian)
</pre></div>
<p>To use a non-default <code>family</code> and/or <code>link</code>, pass in as an argument to
<code>set_engine()</code>:
</p>
<div class="sourceCode r"><pre>linear_reg() %&gt;% 
  set_engine("glm", family = stats::poisson(link = "sqrt")) %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Engine-Specific Arguments:
##   family = stats::poisson(link = "sqrt")
## 
## Computational engine: glm 
## 
## Model fit template:
## stats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     family = stats::poisson(link = "sqrt"))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p><em>However</em>, the documentation in <code><a href="stats.html#topic+glm">stats::glm()</a></code> assumes
that is specific type of case weights are being used:“Non-NULL weights
can be used to indicate that different observations have different
dispersions (with the values in weights being inversely proportional to
the dispersions); or equivalently, when the elements of weights are
positive integers <code>w_i</code>, that each response <code>y_i</code> is the mean of <code>w_i</code>
unit-weight observations. For a binomial GLM prior weights are used to
give the number of trials when the response is the proportion of
successes: they would rarely be used for a Poisson GLM.”
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#linear-reg-glm">examples</a>
for <code>linear_reg()</code> with the <code>"glm"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_glmer'>Linear regression via generalized mixed models</h2><span id='topic+details_linear_reg_glmer'></span>

<h3>Description</h3>

<p>The <code>"glmer"</code> engine estimates fixed and random effect regression parameters
using maximum likelihood (or restricted maximum likelihood) estimation.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

linear_reg() %&gt;% 
  set_engine("glmer") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Computational engine: glmer 
## 
## Model fit template:
## lme4::glmer(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     family = stats::gaussian)
</pre></div>
<p>Note that using this engine with a linear link function will result in a
warning:
</p>
<div class="sourceCode"><pre>calling glmer() with family=gaussian (identity link) as a shortcut 
to lmer() is deprecated; please call lmer() directly
</pre></div>



<h4>Predicting new samples</h4>

<p>This model can use subject-specific coefficient estimates to make
predictions (i.e. partial pooling). For example, this equation shows the
linear predictor (<code style="white-space: pre;">&#8288;\eta&#8288;</code>) for a random intercept:
</p>
<div class="sourceCode"><pre>\eta_{i} = (\beta_0 + b_{0i}) + \beta_1x_{i1}
</pre></div>
<p>where <code>i</code> denotes the <code>i</code>th independent experimental unit
(e.g. subject). When the model has seen subject <code>i</code>, it can use that
subject’s data to adjust the <em>population</em> intercept to be more specific
to that subjects results.
</p>
<p>What happens when data are being predicted for a subject that was not
used in the model fit? In that case, this package uses <em>only</em> the
population parameter estimates for prediction:
</p>
<div class="sourceCode"><pre>\hat{\eta}_{i'} = \hat{\beta}_0+ \hat{\beta}x_{i'1}
</pre></div>
<p>Depending on what covariates are in the model, this might have the
effect of making the same prediction for all new samples. The population
parameters are the “best estimate” for a subject that was not included
in the model fit.
</p>
<p>The tidymodels framework deliberately constrains predictions for new
data to not use the training set or other data (to prevent information
leakage).
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model can accept case weights.
</p>
<p>With parsnip, we suggest using the formula method when fitting:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
data("riesby")

linear_reg() %&gt;% 
  set_engine("glmer") %&gt;% 
  fit(depr_score ~ week + (1|subject), data = riesby)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the typical formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

glmer_spec &lt;- 
  linear_reg() %&gt;% 
  set_engine("glmer")

glmer_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = depr_score, predictors = c(week, subject)) %&gt;% 
  add_model(glmer_spec, formula = depr_score ~ week + (1|subject))

fit(glmer_wflow, data = riesby)
</pre></div>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> J Pinheiro, and D Bates. 2000. <em>Mixed-effects models in S and S-PLUS</em>.
Springer, New York, NY
</p>
</li>
<li><p> West, K, Band Welch, and A Galecki. 2014. <em>Linear Mixed Models: A
Practical Guide Using Statistical Software</em>. CRC Press.
</p>
</li>
<li><p> Thorson, J, Minto, C. 2015, Mixed effects: a unifying framework for
statistical modelling in fisheries biology. <em>ICES Journal of Marine
Science</em>, Volume 72, Issue 5, Pages 1245–1256.
</p>
</li>
<li><p> Harrison, XA, Donaldson, L, Correa-Cano, ME, Evans, J, Fisher, DN,
Goodwin, CED, Robinson, BS, Hodgson, DJ, Inger, R. 2018. <em>A brief
introduction to mixed effects modelling and multi-model inference in
ecology</em>. PeerJ 6:e4794.
</p>
</li>
<li><p> DeBruine LM, Barr DJ. Understanding Mixed-Effects Models Through Data
Simulation. 2021. <em>Advances in Methods and Practices in Psychological
Science</em>.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_glmnet'>Linear regression via glmnet</h2><span id='topic+details_linear_reg_glmnet'></span>

<h3>Description</h3>

<p><code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> uses regularized least squares to fit models with numeric outcomes.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: see below)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 1.0)
</p>
</li></ul>

<p>A value of <code>mixture = 1</code> corresponds to a pure lasso model, while
<code>mixture = 0</code> indicates ridge regression.
</p>
<p>The <code>penalty</code> parameter has no default and requires a single numeric
value. For more details about this, and the <code>glmnet</code> model in general,
see <a href="#topic+glmnet-details">glmnet-details</a>.
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>linear_reg(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("glmnet") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0
##   mixture = double(1)
## 
## Computational engine: glmnet 
## 
## Model fit template:
## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     alpha = double(1), family = "gaussian")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one. By default, <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> uses
the argument <code>standardize = TRUE</code> to center and scale the data.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Sparse Data</h4>

<p>This model can utilize sparse data during model fitting and prediction.
Both sparse matrices such as dgCMatrix from the <code>Matrix</code> package and
sparse tibbles from the <code>sparsevctrs</code> package are supported. See
<a href="#topic+sparse_data">sparse_data</a> for more information.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#linear-reg-glmnet">examples</a>
for <code>linear_reg()</code> with the <code>"glmnet"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Hastie, T, R Tibshirani, and M Wainwright. 2015. <em>Statistical Learning
with Sparsity</em>. CRC Press.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_gls'>Linear regression via generalized least squares</h2><span id='topic+details_linear_reg_gls'></span>

<h3>Description</h3>

<p>The <code>"gls"</code> engine estimates linear regression for models where the rows of the
data are not independent.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

linear_reg() %&gt;% 
  set_engine("gls") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Computational engine: gls 
## 
## Model fit template:
## nlme::gls(formula = missing_arg(), data = missing_arg())
</pre></div>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model can accept case weights.
</p>
<p>With parsnip, we suggest using the <em>fixed effects</em> formula method when
fitting, but the details of the correlation structure should be passed
to <code>set_engine()</code> since it is an irregular (but required) argument:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
# load nlme to be able to use the `cor*()` functions
library(nlme)

data("riesby")

linear_reg() %&gt;% 
  set_engine("gls", correlation =  corCompSymm(form = ~ 1 | subject)) %&gt;% 
  fit(depr_score ~ week, data = riesby)
</pre></div>
<div class="sourceCode"><pre>## parsnip model object
## 
## Generalized least squares fit by REML
##   Model: depr_score ~ week 
##   Data: data 
##   Log-restricted-likelihood: -765.0148
## 
## Coefficients:
## (Intercept)        week 
##   -4.953439   -2.119678 
## 
## Correlation Structure: Compound symmetry
##  Formula: ~1 | subject 
##  Parameter estimate(s):
##       Rho 
## 0.6820145 
## Degrees of freedom: 250 total; 248 residual
## Residual standard error: 6.868785
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the typical formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

gls_spec &lt;- 
  linear_reg() %&gt;% 
  set_engine("gls", correlation =  corCompSymm(form = ~ 1 | subject))

gls_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = depr_score, predictors = c(week, subject)) %&gt;% 
  add_model(gls_spec, formula = depr_score ~ week)

fit(gls_wflow, data = riesby)
</pre></div>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> J Pinheiro, and D Bates. 2000. <em>Mixed-effects models in S and S-PLUS</em>.
Springer, New York, NY
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_h2o'>Linear regression via h2o</h2><span id='topic+details_linear_reg_h2o'></span>

<h3>Description</h3>

<p>This model uses regularized least squares to fit models with numeric outcomes.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: see
below)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: see below)
</p>
</li></ul>

<p>By default, when not given a fixed <code>penalty</code>,
<code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> uses a heuristic approach to select
the optimal value of <code>penalty</code> based on training data. Setting the
engine parameter <code>lambda_search</code> to <code>TRUE</code> enables an efficient version
of the grid search, see more details at
<a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/lambda_search.html">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/lambda_search.html</a>.
</p>
<p>The choice of <code>mixture</code> depends on the engine parameter <code>solver</code>, which
is automatically chosen given training data and the specification of
other model parameters. When <code>solver</code> is set to <code>'L-BFGS'</code>, <code>mixture</code>
defaults to 0 (ridge regression) and 0.5 otherwise.
</p>



<h4>Translation from parsnip to the original package</h4>

<p><code><a href="agua.html#topic+h2o_train">agua::h2o_train_glm()</a></code> for <code>linear_reg()</code> is a
wrapper around <code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> with
<code>family = "gaussian"</code>.
</p>
<div class="sourceCode r"><pre>linear_reg(penalty = 1, mixture = 0.5) %&gt;% 
  set_engine("h2o") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 1
##   mixture = 0.5
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_glm(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), lambda = 1, alpha = 0.5, 
##     family = "gaussian")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>
<p>By default, <code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> uses the argument
<code>standardize = TRUE</code> to center and scale the data.
</p>



<h4>Initializing h2o</h4>

<p>To use the h2o engine with tidymodels, please run <code>h2o::h2o.init()</code>
first. By default, This connects R to the local h2o server. This needs
to be done in every new R session. You can also connect to a remote h2o
server with an IP address, for more details see
<code><a href="h2o.html#topic+h2o.init">h2o::h2o.init()</a></code>.
</p>
<p>You can control the number of threads in the thread pool used by h2o
with the <code>nthreads</code> argument. By default, it uses all CPUs on the host.
This is different from the usual parallel processing mechanism in
tidymodels for tuning, while tidymodels parallelizes over resamples, h2o
parallelizes over hyperparameter combinations for a given resample.
</p>
<p>h2o will automatically shut down the local h2o instance started by R
when R is terminated. To manually stop the h2o server, run
<code>h2o::h2o.shutdown()</code>.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>


<hr>
<h2 id='details_linear_reg_keras'>Linear regression via keras/tensorflow</h2><span id='topic+details_linear_reg_keras'></span>

<h3>Description</h3>

<p>This model uses regularized least squares to fit models with numeric outcomes.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has one tuning parameter:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li></ul>

<p>For <code>penalty</code>, the amount of regularization is <em>only</em> L2 penalty (i.e.,
ridge or weight decay).
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>linear_reg(penalty = double(1)) %&gt;% 
  set_engine("keras") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = double(1)
## 
## Computational engine: keras 
## 
## Model fit template:
## parsnip::keras_mlp(x = missing_arg(), y = missing_arg(), penalty = double(1), 
##     hidden_units = 1, act = "linear")
</pre></div>
<p><code><a href="#topic+keras_mlp">keras_mlp()</a></code> is a parsnip wrapper around keras code for
neural networks. This model fits a linear regression as a network with a
single hidden unit.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#linear-reg-keras">examples</a>
for <code>linear_reg()</code> with the <code>"keras"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Hoerl, A., &amp; Kennard, R. (2000). <em>Ridge Regression: Biased Estimation
for Nonorthogonal Problems</em>. Technometrics, 42(1), 80-86.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_lm'>Linear regression via lm</h2><span id='topic+details_linear_reg_lm'></span>

<h3>Description</h3>

<p><code><a href="stats.html#topic+lm">stats::lm()</a></code> uses ordinary least squares to fit models with numeric outcomes.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>linear_reg() %&gt;% 
  set_engine("lm") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Computational engine: lm 
## 
## Model fit template:
## stats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p><em>However</em>, the documentation in <code><a href="stats.html#topic+lm">stats::lm()</a></code> assumes
that is specific type of case weights are being used: “Non-NULL weights
can be used to indicate that different observations have different
variances (with the values in weights being inversely proportional to
the variances); or equivalently, when the elements of weights are
positive integers <code>w_i</code>, that each response <code>y_i</code> is the mean of <code>w_i</code>
unit-weight observations (including the case that there are w_i
observations equal to <code>y_i</code> and the data have been summarized). However,
in the latter case, notice that within-group variation is not used.
Therefore, the sigma estimate and residual degrees of freedom may be
suboptimal; in the case of replication weights, <strong>even wrong</strong>. Hence,
standard errors and analysis of variance tables should be treated with
care” (emphasis added)
</p>
<p>Depending on your application, the degrees of freedom for the model (and
other statistics) might be incorrect.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#linear-reg-lm">examples</a>
for <code>linear_reg()</code> with the <code>"lm"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_lme'>Linear regression via mixed models</h2><span id='topic+details_linear_reg_lme'></span>

<h3>Description</h3>

<p>The <code>"lme"</code> engine estimates fixed and random effect regression parameters
using maximum likelihood (or restricted maximum likelihood) estimation.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

linear_reg() %&gt;% 
  set_engine("lme") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Computational engine: lme 
## 
## Model fit template:
## nlme::lme(fixed = missing_arg(), data = missing_arg())
</pre></div>



<h4>Predicting new samples</h4>

<p>This model can use subject-specific coefficient estimates to make
predictions (i.e. partial pooling). For example, this equation shows the
linear predictor (<code style="white-space: pre;">&#8288;\eta&#8288;</code>) for a random intercept:
</p>
<div class="sourceCode"><pre>\eta_{i} = (\beta_0 + b_{0i}) + \beta_1x_{i1}
</pre></div>
<p>where <code>i</code> denotes the <code>i</code>th independent experimental unit
(e.g. subject). When the model has seen subject <code>i</code>, it can use that
subject’s data to adjust the <em>population</em> intercept to be more specific
to that subjects results.
</p>
<p>What happens when data are being predicted for a subject that was not
used in the model fit? In that case, this package uses <em>only</em> the
population parameter estimates for prediction:
</p>
<div class="sourceCode"><pre>\hat{\eta}_{i'} = \hat{\beta}_0+ \hat{\beta}x_{i'1}
</pre></div>
<p>Depending on what covariates are in the model, this might have the
effect of making the same prediction for all new samples. The population
parameters are the “best estimate” for a subject that was not included
in the model fit.
</p>
<p>The tidymodels framework deliberately constrains predictions for new
data to not use the training set or other data (to prevent information
leakage).
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model can accept case weights.
</p>
<p>With parsnip, we suggest using the <em>fixed effects</em> formula method when
fitting, but the random effects formula should be passed to
<code>set_engine()</code> since it is an irregular (but required) argument:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
data("riesby")

linear_reg() %&gt;% 
  set_engine("lme", random =  ~ 1|subject) %&gt;% 
  fit(depr_score ~ week, data = riesby)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the typical formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

lme_spec &lt;- 
  linear_reg() %&gt;% 
  set_engine("lme", random =  ~ 1|subject)

lme_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = depr_score, predictors = c(week, subject)) %&gt;% 
  add_model(lme_spec, formula = depr_score ~ week)

fit(lme_wflow, data = riesby)
</pre></div>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> J Pinheiro, and D Bates. 2000. <em>Mixed-effects models in S and S-PLUS</em>.
Springer, New York, NY
</p>
</li>
<li><p> West, K, Band Welch, and A Galecki. 2014. <em>Linear Mixed Models: A
Practical Guide Using Statistical Software</em>. CRC Press.
</p>
</li>
<li><p> Thorson, J, Minto, C. 2015, Mixed effects: a unifying framework for
statistical modelling in fisheries biology. <em>ICES Journal of Marine
Science</em>, Volume 72, Issue 5, Pages 1245–1256.
</p>
</li>
<li><p> Harrison, XA, Donaldson, L, Correa-Cano, ME, Evans, J, Fisher, DN,
Goodwin, CED, Robinson, BS, Hodgson, DJ, Inger, R. 2018. <em>A brief
introduction to mixed effects modelling and multi-model inference in
ecology</em>. PeerJ 6:e4794.
</p>
</li>
<li><p> DeBruine LM, Barr DJ. Understanding Mixed-Effects Models Through Data
Simulation. 2021. <em>Advances in Methods and Practices in Psychological
Science</em>.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_lmer'>Linear regression via mixed models</h2><span id='topic+details_linear_reg_lmer'></span>

<h3>Description</h3>

<p>The <code>"lmer"</code> engine estimates fixed and random effect regression parameters
using maximum likelihood (or restricted maximum likelihood) estimation.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

linear_reg() %&gt;% 
  set_engine("lmer") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Computational engine: lmer 
## 
## Model fit template:
## lme4::lmer(formula = missing_arg(), data = missing_arg(), weights = missing_arg())
</pre></div>



<h4>Predicting new samples</h4>

<p>This model can use subject-specific coefficient estimates to make
predictions (i.e. partial pooling). For example, this equation shows the
linear predictor (<code style="white-space: pre;">&#8288;\eta&#8288;</code>) for a random intercept:
</p>
<div class="sourceCode"><pre>\eta_{i} = (\beta_0 + b_{0i}) + \beta_1x_{i1}
</pre></div>
<p>where <code>i</code> denotes the <code>i</code>th independent experimental unit
(e.g. subject). When the model has seen subject <code>i</code>, it can use that
subject’s data to adjust the <em>population</em> intercept to be more specific
to that subjects results.
</p>
<p>What happens when data are being predicted for a subject that was not
used in the model fit? In that case, this package uses <em>only</em> the
population parameter estimates for prediction:
</p>
<div class="sourceCode"><pre>\hat{\eta}_{i'} = \hat{\beta}_0+ \hat{\beta}x_{i'1}
</pre></div>
<p>Depending on what covariates are in the model, this might have the
effect of making the same prediction for all new samples. The population
parameters are the “best estimate” for a subject that was not included
in the model fit.
</p>
<p>The tidymodels framework deliberately constrains predictions for new
data to not use the training set or other data (to prevent information
leakage).
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model can accept case weights.
</p>
<p>With parsnip, we suggest using the formula method when fitting:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
data("riesby")

linear_reg() %&gt;% 
  set_engine("lmer") %&gt;% 
  fit(depr_score ~ week + (1|subject), data = riesby)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the typical formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

lmer_spec &lt;- 
  linear_reg() %&gt;% 
  set_engine("lmer")

lmer_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = depr_score, predictors = c(week, subject)) %&gt;% 
  add_model(lmer_spec, formula = depr_score ~ week + (1|subject))

fit(lmer_wflow, data = riesby)
</pre></div>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> J Pinheiro, and D Bates. 2000. <em>Mixed-effects models in S and S-PLUS</em>.
Springer, New York, NY
</p>
</li>
<li><p> West, K, Band Welch, and A Galecki. 2014. <em>Linear Mixed Models: A
Practical Guide Using Statistical Software</em>. CRC Press.
</p>
</li>
<li><p> Thorson, J, Minto, C. 2015, Mixed effects: a unifying framework for
statistical modelling in fisheries biology. <em>ICES Journal of Marine
Science</em>, Volume 72, Issue 5, Pages 1245–1256.
</p>
</li>
<li><p> Harrison, XA, Donaldson, L, Correa-Cano, ME, Evans, J, Fisher, DN,
Goodwin, CED, Robinson, BS, Hodgson, DJ, Inger, R. 2018. <em>A brief
introduction to mixed effects modelling and multi-model inference in
ecology</em>. PeerJ 6:e4794.
</p>
</li>
<li><p> DeBruine LM, Barr DJ. Understanding Mixed-Effects Models Through Data
Simulation. 2021. <em>Advances in Methods and Practices in Psychological
Science</em>.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_quantreg'>Linear quantile regression via the quantreg package</h2><span id='topic+details_linear_reg_quantreg'></span>

<h3>Description</h3>

<p><code><a href="quantreg.html#topic+rq">quantreg::rq()</a></code> optimizes quantile loss to fit models with numeric outcomes.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: quantile regression
</p>
<p>This model has the same structure as the model fit by <code>lm()</code>, but
instead of optimizing the sum of squared errors, it optimizes “quantile
loss” in order to produce better estimates of the predictive
distribution.
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>This model only works with the <code>"quantile regression"</code> model and
requires users to specify which areas of the distribution to predict via
the <code>quantile_levels</code> argument. For example:
</p>
<div class="sourceCode r"><pre>linear_reg() %&gt;% 
  set_engine("quantreg") %&gt;% 
  set_mode("quantile regression", quantile_levels = (1:3) / 4) %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (quantile regression)
## 
## Computational engine: quantreg 
## 
## Model fit template:
## quantreg::rq(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     tau = quantile_levels)

## Quantile levels: 0.25, 0.5, and 0.75.
</pre></div>



<h4>Output format</h4>

<p>When multiple quantile levels are predicted, there are multiple
predicted values for each row of new data. The <code>predict()</code> method for
this mode produces a column named <code>.pred_quantile</code> that has a special
class of <code>"quantile_pred"</code>, and it contains the predictions for each
row.
</p>
<p>For example:
</p>
<div class="sourceCode r"><pre>library(modeldata)
rlang::check_installed("quantreg")

n &lt;- nrow(Chicago)
Chicago &lt;- Chicago %&gt;% select(ridership, Clark_Lake)

Chicago_train &lt;- Chicago[1:(n - 7), ]
Chicago_test  &lt;- Chicago[(n - 6):n, ]

qr_fit &lt;- 
  linear_reg() %&gt;% 
  set_engine("quantreg") %&gt;% 
  set_mode("quantile regression", quantile_levels = (1:3) / 4) %&gt;% 
  fit(ridership ~ Clark_Lake, data = Chicago_train)
qr_fit
</pre></div>
<div class="sourceCode"><pre>## parsnip model object
## 
## Call:
## quantreg::rq(formula = ridership ~ Clark_Lake, tau = quantile_levels, 
##     data = data)
## 
## Coefficients:
##              tau= 0.25 tau= 0.50 tau= 0.75
## (Intercept) -0.2064189 0.2051549 0.8112286
## Clark_Lake   0.9820582 0.9862306 0.9777820
## 
## Degrees of freedom: 5691 total; 5689 residual
</pre></div>
<div class="sourceCode r"><pre>qr_pred &lt;- predict(qr_fit, Chicago_test)
qr_pred
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 7 x 1
##   .pred_quantile
##        &lt;qtls(3)&gt;
## 1         [21.1]
## 2         [21.4]
## 3         [21.7]
## 4         [21.4]
## 5         [19.5]
## 6         [6.88]
## # i 1 more row
</pre></div>
<p>We can unnest these values and/or convert them to a rectangular format:
</p>
<div class="sourceCode r"><pre>as_tibble(qr_pred$.pred_quantile)
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 21 x 3
##   .pred_quantile .quantile_levels  .row
##            &lt;dbl&gt;            &lt;dbl&gt; &lt;int&gt;
## 1           20.6             0.25     1
## 2           21.1             0.5      1
## 3           21.5             0.75     1
## 4           20.9             0.25     2
## 5           21.4             0.5      2
## 6           21.8             0.75     2
## # i 15 more rows
</pre></div>
<div class="sourceCode r"><pre>as.matrix(qr_pred$.pred_quantile)
</pre></div>
<div class="sourceCode"><pre>##           [,1]      [,2]      [,3]
## [1,] 20.590627 21.090561 21.517717
## [2,] 20.863639 21.364733 21.789541
## [3,] 21.190665 21.693148 22.115142
## [4,] 20.879352 21.380513 21.805185
## [5,] 19.047814 19.541193 19.981622
## [6,]  6.435241  6.875033  7.423968
## [7,]  6.062058  6.500265  7.052411
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#linear-reg-quantreg">examples</a>
for <code>linear_reg()</code> with the <code>"quantreg"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Waldmann, E. (2018). Quantile regression: a short story on how and
why. <em>Statistical Modelling</em>, 18(3-4), 203-218.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_spark'>Linear regression via spark</h2><span id='topic+details_linear_reg_spark'></span>

<h3>Description</h3>

<p><code><a href="sparklyr.html#topic+ml_linear_regression">sparklyr::ml_linear_regression()</a></code> uses regularized least squares to fit
models with numeric outcomes.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 0.0)
</p>
</li></ul>

<p>For <code>penalty</code>, the amount of regularization includes both the L1 penalty
(i.e., lasso) and the L2 penalty (i.e., ridge or weight decay). As for
<code>mixture</code>:
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code> specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso
and ridge.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>linear_reg(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("spark") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = double(1)
##   mixture = double(1)
## 
## Computational engine: spark 
## 
## Model fit template:
## sparklyr::ml_linear_regression(x = missing_arg(), formula = missing_arg(), 
##     weights = missing_arg(), reg_param = double(1), elastic_net_param = double(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>
<p>By default, <code>ml_linear_regression()</code> uses the argument
<code>standardization = TRUE</code> to center and scale the data.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p>Note that, for spark engines, the <code>case_weight</code> argument value should be
a character string to specify the column with the numeric case weights.
</p>



<h4>Other details</h4>

<p>For models created using the <code>"spark"</code> engine, there are several things
to consider.
</p>

<ul>
<li><p> Only the formula interface to via <code>fit()</code> is available; using
<code>fit_xy()</code> will generate an error.
</p>
</li>
<li><p> The predictions will always be in a Spark table format. The names will
be the same as documented but without the dots.
</p>
</li>
<li><p> There is no equivalent to factor columns in Spark tables so class
predictions are returned as character columns.
</p>
</li>
<li><p> To retain the model object for a new R session (via <code>save()</code>), the
<code>model$fit</code> element of the parsnip object should be serialized via
<code>ml_save(object$fit)</code> and separately saved to disk. In a new session,
the object can be reloaded and reattached to the parsnip object.
</p>
</li></ul>




<h4>References</h4>


<ul>
<li><p> Luraschi, J, K Kuo, and E Ruiz. 2019. <em>Mastering Spark with R</em>.
O’Reilly Media
</p>
</li>
<li><p> Hastie, T, R Tibshirani, and M Wainwright. 2015. <em>Statistical Learning
with Sparsity</em>. CRC Press.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_stan'>Linear regression via Bayesian Methods</h2><span id='topic+details_linear_reg_stan'></span>

<h3>Description</h3>

<p>The <code>"stan"</code> engine estimates regression parameters using Bayesian estimation.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters.
</p>



<h4>Important engine-specific options</h4>

<p>Some relevant arguments that can be passed to <code>set_engine()</code>:
</p>

<ul>
<li> <p><code>chains</code>: A positive integer specifying the number of Markov chains.
The default is 4.
</p>
</li>
<li> <p><code>iter</code>: A positive integer specifying the number of iterations for
each chain (including warmup). The default is 2000.
</p>
</li>
<li> <p><code>seed</code>: The seed for random number generation.
</p>
</li>
<li> <p><code>cores</code>: Number of cores to use when executing the chains in parallel.
</p>
</li>
<li> <p><code>prior</code>: The prior distribution for the (non-hierarchical) regression
coefficients. The <code>"stan"</code> engine does not fit any hierarchical terms.
See the <code>"stan_glmer"</code> engine from the multilevelmod package for that
type of model.
</p>
</li>
<li> <p><code>prior_intercept</code>: The prior distribution for the intercept (after
centering all predictors).
</p>
</li></ul>

<p>See <code><a href="rstan.html#topic+stanmodel-method-sampling">rstan::sampling()</a></code> and
<code><a href="rstanarm.html#topic+priors">rstanarm::priors()</a></code> for more information on these
and other options.
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>linear_reg() %&gt;% 
  set_engine("stan") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Computational engine: stan 
## 
## Model fit template:
## rstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), family = stats::gaussian, refresh = 0)
</pre></div>
<p>Note that the <code>refresh</code> default prevents logging of the estimation
process. Change this value in <code>set_engine()</code> to show the MCMC logs.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Other details</h4>

<p>For prediction, the <code>"stan"</code> engine can compute posterior intervals
analogous to confidence and prediction intervals. In these instances,
the units are the original outcome and when <code>std_error = TRUE</code>, the
standard deviation of the posterior distribution (or posterior
predictive distribution as appropriate) is returned.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#linear-reg-stan">examples</a>
for <code>linear_reg()</code> with the <code>"stan"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> McElreath, R. 2020 <em>Statistical Rethinking</em>. CRC Press.
</p>
</li></ul>



<hr>
<h2 id='details_linear_reg_stan_glmer'>Linear regression via hierarchical Bayesian methods</h2><span id='topic+details_linear_reg_stan_glmer'></span>

<h3>Description</h3>

<p>The <code>"stan_glmer"</code> engine estimates hierarchical regression parameters using
Bayesian estimation.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>



<h4>Important engine-specific options</h4>

<p>Some relevant arguments that can be passed to <code>set_engine()</code>:
</p>

<ul>
<li> <p><code>chains</code>: A positive integer specifying the number of Markov chains.
The default is 4.
</p>
</li>
<li> <p><code>iter</code>: A positive integer specifying the number of iterations for
each chain (including warmup). The default is 2000.
</p>
</li>
<li> <p><code>seed</code>: The seed for random number generation.
</p>
</li>
<li> <p><code>cores</code>: Number of cores to use when executing the chains in parallel.
</p>
</li>
<li> <p><code>prior</code>: The prior distribution for the (non-hierarchical) regression
coefficients.
</p>
</li>
<li> <p><code>prior_intercept</code>: The prior distribution for the intercept (after
centering all predictors).
</p>
</li></ul>

<p>See <code>?rstanarm::stan_glmer</code> and <code>?rstan::sampling</code> for more information.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

linear_reg() %&gt;% 
  set_engine("stan_glmer") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Computational engine: stan_glmer 
## 
## Model fit template:
## rstanarm::stan_glmer(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), family = stats::gaussian, refresh = 0)
</pre></div>



<h4>Predicting new samples</h4>

<p>This model can use subject-specific coefficient estimates to make
predictions (i.e. partial pooling). For example, this equation shows the
linear predictor (<code style="white-space: pre;">&#8288;\eta&#8288;</code>) for a random intercept:
</p>
<div class="sourceCode"><pre>\eta_{i} = (\beta_0 + b_{0i}) + \beta_1x_{i1}
</pre></div>
<p>where <code>i</code> denotes the <code>i</code>th independent experimental unit
(e.g. subject). When the model has seen subject <code>i</code>, it can use that
subject’s data to adjust the <em>population</em> intercept to be more specific
to that subjects results.
</p>
<p>What happens when data are being predicted for a subject that was not
used in the model fit? In that case, this package uses <em>only</em> the
population parameter estimates for prediction:
</p>
<div class="sourceCode"><pre>\hat{\eta}_{i'} = \hat{\beta}_0+ \hat{\beta}x_{i'1}
</pre></div>
<p>Depending on what covariates are in the model, this might have the
effect of making the same prediction for all new samples. The population
parameters are the “best estimate” for a subject that was not included
in the model fit.
</p>
<p>The tidymodels framework deliberately constrains predictions for new
data to not use the training set or other data (to prevent information
leakage).
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model can accept case weights.
</p>
<p>With parsnip, we suggest using the formula method when fitting:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
data("riesby")

linear_reg() %&gt;% 
  set_engine("stan_glmer") %&gt;% 
  fit(depr_score ~ week + (1|subject), data = riesby)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the typical formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

glmer_spec &lt;- 
  linear_reg() %&gt;% 
  set_engine("stan_glmer")

glmer_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = depr_score, predictors = c(week, subject)) %&gt;% 
  add_model(glmer_spec, formula = depr_score ~ week + (1|subject))

fit(glmer_wflow, data = riesby)
</pre></div>
<p>For prediction, the <code>"stan_glmer"</code> engine can compute posterior
intervals analogous to confidence and prediction intervals. In these
instances, the units are the original outcome. When <code>std_error = TRUE</code>,
the standard deviation of the posterior distribution (or posterior
predictive distribution as appropriate) is returned.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> McElreath, R. 2020 <em>Statistical Rethinking</em>. CRC Press.
</p>
</li>
<li><p> Sorensen, T, Vasishth, S. 2016. Bayesian linear mixed models using
Stan: A tutorial for psychologists, linguists, and cognitive
scientists, arXiv:1506.06201.
</p>
</li></ul>



<hr>
<h2 id='details_logistic_reg_brulee'>Logistic regression via brulee</h2><span id='topic+details_logistic_reg_brulee'></span>

<h3>Description</h3>

<p><code><a href="brulee.html#topic+brulee_logistic_reg">brulee::brulee_logistic_reg()</a></code> fits a generalized linear model for binary
outcomes. A linear combination of the predictors is used to model the log
odds of an event.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameter:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.001)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 0.0)
</p>
</li></ul>

<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as glmnet).
The zeroing out of parameters is a specific feature the optimization
method used in those packages.
</p>
<p>Other engine arguments of interest:
</p>

<ul>
<li> <p><code>optimizer()</code>: The optimization method. See
<code><a href="brulee.html#topic+brulee_linear_reg">brulee::brulee_linear_reg()</a></code>.
</p>
</li>
<li> <p><code>epochs()</code>: An integer for the number of passes through the training
set.
</p>
</li>
<li> <p><code>lean_rate()</code>: A number used to accelerate the gradient decsent
process.
</p>
</li>
<li> <p><code>momentum()</code>: A number used to use historical gradient information
during optimization (<code>optimizer = "SGD"</code> only).
</p>
</li>
<li> <p><code>batch_size()</code>: An integer for the number of training set points in
each batch.
</p>
</li>
<li> <p><code>stop_iter()</code>: A non-negative integer for how many iterations with no
improvement before stopping. (default: 5L).
</p>
</li>
<li> <p><code>class_weights()</code>: Numeric class weights. See
<code><a href="brulee.html#topic+brulee_logistic_reg">brulee::brulee_logistic_reg()</a></code>.
</p>
</li></ul>




<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>logistic_reg(penalty = double(1)) %&gt;% 
  set_engine("brulee") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = double(1)
## 
## Computational engine: brulee 
## 
## Model fit template:
## brulee::brulee_logistic_reg(x = missing_arg(), y = missing_arg(), 
##     penalty = double(1))
</pre></div>
<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_logistic_reg_gee'>Logistic regression via generalized estimating equations (GEE)</h2><span id='topic+details_logistic_reg_gee'></span>

<h3>Description</h3>

<p><code>gee::gee()</code> uses generalized least squares to fit different types of models
with errors that are not independent.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has no formal tuning parameters. It may be beneficial to
determine the appropriate correlation structure to use, but this
typically does not affect the predicted value of the model. It <em>does</em>
have an effect on the inferential results and parameter covariance
values.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

logistic_reg() %&gt;% 
  set_engine("gee") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Computational engine: gee 
## 
## Model fit template:
## multilevelmod::gee_fit(formula = missing_arg(), data = missing_arg(), 
##     family = binomial)
</pre></div>
<p><code>multilevelmod::gee_fit()</code> is a wrapper model around <code>gee::gee()</code>.
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model cannot accept case weights.
</p>
<p>Both <code>gee:gee()</code> and <code>gee:geepack()</code> specify the id/cluster variable
using an argument <code>id</code> that requires a vector. parsnip doesn’t work that
way so we enable this model to be fit using a artificial function
<code>id_var()</code> to be used in the formula. So, in the original package, the
call would look like:
</p>
<div class="sourceCode r"><pre>gee(breaks ~ tension, id = wool, data = warpbreaks, corstr = "exchangeable")
</pre></div>
<p>With <code>parsnip</code>, we suggest using the formula method when fitting:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
data("toenail", package = "HSAUR3")

logistic_reg() %&gt;% 
  set_engine("gee", corstr = "exchangeable") %&gt;% 
  fit(outcome ~ treatment * visit + id_var(patientID), data = toenail)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the GEE formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

gee_spec &lt;- 
  logistic_reg() %&gt;% 
  set_engine("gee", corstr = "exchangeable")

gee_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = outcome, predictors = c(treatment, visit, patientID)) %&gt;% 
  add_model(gee_spec, formula = outcome ~ treatment * visit + id_var(patientID))

fit(gee_wflow, data = toenail)
</pre></div>
<p>The <code>gee::gee()</code> function always prints out warnings and output even
when <code>silent = TRUE</code>. The parsnip <code>"gee"</code> engine, by contrast, silences
all console output coming from <code>gee::gee()</code>, even if <code>silent = FALSE</code>.
</p>
<p>Also, because of issues with the <code>gee()</code> function, a supplementary call
to <code>glm()</code> is needed to get the rank and QR decomposition objects so
that <code>predict()</code> can be used.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Liang, K.Y. and Zeger, S.L. (1986) Longitudinal data analysis using
generalized linear models. <em>Biometrika</em>, 73 13–22.
</p>
</li>
<li><p> Zeger, S.L. and Liang, K.Y. (1986) Longitudinal data analysis for
discrete and continuous outcomes. <em>Biometrics</em>, 42 121–130.
</p>
</li></ul>



<hr>
<h2 id='details_logistic_reg_glm'>Logistic regression via glm</h2><span id='topic+details_logistic_reg_glm'></span>

<h3>Description</h3>

<p><code><a href="stats.html#topic+glm">stats::glm()</a></code> fits a generalized linear model for binary outcomes. A
linear combination of the predictors is used to model the log odds of an
event.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters but you can set the <code>family</code>
parameter (and/or <code>link</code>) as an engine argument (see below).
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>logistic_reg() %&gt;% 
  set_engine("glm") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Computational engine: glm 
## 
## Model fit template:
## stats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     family = stats::binomial)
</pre></div>
<p>To use a non-default <code>family</code> and/or <code>link</code>, pass in as an argument to
<code>set_engine()</code>:
</p>
<div class="sourceCode r"><pre>logistic_reg() %&gt;% 
  set_engine("glm", family = stats::binomial(link = "probit")) %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Engine-Specific Arguments:
##   family = stats::binomial(link = "probit")
## 
## Computational engine: glm 
## 
## Model fit template:
## stats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     family = stats::binomial(link = "probit"))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p><em>However</em>, the documentation in <code><a href="stats.html#topic+glm">stats::glm()</a></code> assumes
that is specific type of case weights are being used:“Non-NULL weights
can be used to indicate that different observations have different
dispersions (with the values in weights being inversely proportional to
the dispersions); or equivalently, when the elements of weights are
positive integers <code>w_i</code>, that each response <code>y_i</code> is the mean of <code>w_i</code>
unit-weight observations. For a binomial GLM prior weights are used to
give the number of trials when the response is the proportion of
successes: they would rarely be used for a Poisson GLM.”
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#logistic-reg-glm">examples</a>
for <code>logistic_reg()</code> with the <code>"glm"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_logistic_reg_glmer'>Logistic regression via mixed models</h2><span id='topic+details_logistic_reg_glmer'></span>

<h3>Description</h3>

<p>The <code>"glmer"</code> engine estimates fixed and random effect regression parameters
using maximum likelihood (or restricted maximum likelihood) estimation.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

logistic_reg() %&gt;% 
  set_engine("glmer") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Computational engine: glmer 
## 
## Model fit template:
## lme4::glmer(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     family = binomial)
</pre></div>



<h4>Predicting new samples</h4>

<p>This model can use subject-specific coefficient estimates to make
predictions (i.e. partial pooling). For example, this equation shows the
linear predictor (<code style="white-space: pre;">&#8288;\eta&#8288;</code>) for a random intercept:
</p>
<div class="sourceCode"><pre>\eta_{i} = (\beta_0 + b_{0i}) + \beta_1x_{i1}
</pre></div>
<p>where <code>i</code> denotes the <code>i</code>th independent experimental unit
(e.g. subject). When the model has seen subject <code>i</code>, it can use that
subject’s data to adjust the <em>population</em> intercept to be more specific
to that subjects results.
</p>
<p>What happens when data are being predicted for a subject that was not
used in the model fit? In that case, this package uses <em>only</em> the
population parameter estimates for prediction:
</p>
<div class="sourceCode"><pre>\hat{\eta}_{i'} = \hat{\beta}_0+ \hat{\beta}x_{i'1}
</pre></div>
<p>Depending on what covariates are in the model, this might have the
effect of making the same prediction for all new samples. The population
parameters are the “best estimate” for a subject that was not included
in the model fit.
</p>
<p>The tidymodels framework deliberately constrains predictions for new
data to not use the training set or other data (to prevent information
leakage).
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model can accept case weights.
</p>
<p>With parsnip, we suggest using the formula method when fitting:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
data("toenail", package = "HSAUR3")

logistic_reg() %&gt;% 
  set_engine("glmer") %&gt;% 
  fit(outcome ~ treatment * visit + (1 | patientID), data = toenail)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the typical formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

glmer_spec &lt;- 
  logistic_reg() %&gt;% 
  set_engine("glmer")

glmer_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = outcome, predictors = c(treatment, visit, patientID)) %&gt;% 
  add_model(glmer_spec, formula = outcome ~ treatment * visit + (1 | patientID))

fit(glmer_wflow, data = toenail)
</pre></div>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> J Pinheiro, and D Bates. 2000. <em>Mixed-effects models in S and S-PLUS</em>.
Springer, New York, NY
</p>
</li>
<li><p> West, K, Band Welch, and A Galecki. 2014. <em>Linear Mixed Models: A
Practical Guide Using Statistical Software</em>. CRC Press.
</p>
</li>
<li><p> Thorson, J, Minto, C. 2015, Mixed effects: a unifying framework for
statistical modelling in fisheries biology. <em>ICES Journal of Marine
Science</em>, Volume 72, Issue 5, Pages 1245–1256.
</p>
</li>
<li><p> Harrison, XA, Donaldson, L, Correa-Cano, ME, Evans, J, Fisher, DN,
Goodwin, CED, Robinson, BS, Hodgson, DJ, Inger, R. 2018. <em>A brief
introduction to mixed effects modelling and multi-model inference in
ecology</em>. PeerJ 6:e4794.
</p>
</li>
<li><p> DeBruine LM, Barr DJ. Understanding Mixed-Effects Models Through Data
Simulation. 2021. <em>Advances in Methods and Practices in Psychological
Science</em>.
</p>
</li></ul>



<hr>
<h2 id='details_logistic_reg_glmnet'>Logistic regression via glmnet</h2><span id='topic+details_logistic_reg_glmnet'></span>

<h3>Description</h3>

<p><code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> fits a generalized linear model for binary outcomes. A
linear combination of the predictors is used to model the log odds of an
event.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: see below)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 1.0)
</p>
</li></ul>

<p>The <code>penalty</code> parameter has no default and requires a single numeric
value. For more details about this, and the <code>glmnet</code> model in general,
see <a href="#topic+glmnet-details">glmnet-details</a>. As for <code>mixture</code>:
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code> specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso
and ridge.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>logistic_reg(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("glmnet") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = 0
##   mixture = double(1)
## 
## Computational engine: glmnet 
## 
## Model fit template:
## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     alpha = double(1), family = "binomial")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one. By default, <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> uses
the argument <code>standardize = TRUE</code> to center and scale the data.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Sparse Data</h4>

<p>This model can utilize sparse data during model fitting and prediction.
Both sparse matrices such as dgCMatrix from the <code>Matrix</code> package and
sparse tibbles from the <code>sparsevctrs</code> package are supported. See
<a href="#topic+sparse_data">sparse_data</a> for more information.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#logistic-reg-glmnet">examples</a>
for <code>logistic_reg()</code> with the <code>"glmnet"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Hastie, T, R Tibshirani, and M Wainwright. 2015. <em>Statistical Learning
with Sparsity</em>. CRC Press.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_logistic_reg_h2o'>Logistic regression via h2o</h2><span id='topic+details_logistic_reg_h2o'></span>

<h3>Description</h3>

<p><code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> fits a generalized linear model for binary outcomes.
A linear combination of the predictors is used to model the log odds of an
event.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: see
below)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: see below)
</p>
</li></ul>

<p>By default, when not given a fixed <code>penalty</code>,
<code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> uses a heuristic approach to select
the optimal value of <code>penalty</code> based on training data. Setting the
engine parameter <code>lambda_search</code> to <code>TRUE</code> enables an efficient version
of the grid search, see more details at
<a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/lambda_search.html">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/lambda_search.html</a>.
</p>
<p>The choice of <code>mixture</code> depends on the engine parameter <code>solver</code>, which
is automatically chosen given training data and the specification of
other model parameters. When <code>solver</code> is set to <code>'L-BFGS'</code>, <code>mixture</code>
defaults to 0 (ridge regression) and 0.5 otherwise.
</p>



<h4>Translation from parsnip to the original package</h4>

<p><code><a href="agua.html#topic+h2o_train">agua::h2o_train_glm()</a></code> for <code>logistic_reg()</code> is
a wrapper around <code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code>. h2o will
automatically picks the link function and distribution family or
binomial responses.
</p>
<div class="sourceCode r"><pre>logistic_reg() %&gt;% 
  set_engine("h2o") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_glm(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), family = "binomial")
</pre></div>
<p>To use a non-default argument in <code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code>,
pass in as an engine argument to <code>set_engine()</code>:
</p>
<div class="sourceCode r"><pre>logistic_reg() %&gt;% 
  set_engine("h2o", compute_p_values = TRUE) %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Engine-Specific Arguments:
##   compute_p_values = TRUE
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_glm(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), compute_p_values = TRUE, 
##     family = "binomial")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>
<p>By default, <code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> uses the argument
<code>standardize = TRUE</code> to center and scale all numeric columns.
</p>



<h4>Initializing h2o</h4>

<p>To use the h2o engine with tidymodels, please run <code>h2o::h2o.init()</code>
first. By default, This connects R to the local h2o server. This needs
to be done in every new R session. You can also connect to a remote h2o
server with an IP address, for more details see
<code><a href="h2o.html#topic+h2o.init">h2o::h2o.init()</a></code>.
</p>
<p>You can control the number of threads in the thread pool used by h2o
with the <code>nthreads</code> argument. By default, it uses all CPUs on the host.
This is different from the usual parallel processing mechanism in
tidymodels for tuning, while tidymodels parallelizes over resamples, h2o
parallelizes over hyperparameter combinations for a given resample.
</p>
<p>h2o will automatically shut down the local h2o instance started by R
when R is terminated. To manually stop the h2o server, run
<code>h2o::h2o.shutdown()</code>.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>


<hr>
<h2 id='details_logistic_reg_keras'>Logistic regression via keras</h2><span id='topic+details_logistic_reg_keras'></span>

<h3>Description</h3>

<p><code><a href="#topic+keras_mlp">keras_mlp()</a></code> fits a generalized linear model for binary outcomes. A
linear combination of the predictors is used to model the log odds of an
event.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has one tuning parameter:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li></ul>

<p>For <code>penalty</code>, the amount of regularization is <em>only</em> L2 penalty (i.e.,
ridge or weight decay).
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>logistic_reg(penalty = double(1)) %&gt;% 
  set_engine("keras") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = double(1)
## 
## Computational engine: keras 
## 
## Model fit template:
## parsnip::keras_mlp(x = missing_arg(), y = missing_arg(), penalty = double(1), 
##     hidden_units = 1, act = "linear")
</pre></div>
<p><code><a href="#topic+keras_mlp">keras_mlp()</a></code> is a parsnip wrapper around keras code for
neural networks. This model fits a linear regression as a network with a
single hidden unit.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#logistic-reg-keras">examples</a>
for <code>logistic_reg()</code> with the <code>"keras"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Hoerl, A., &amp; Kennard, R. (2000). <em>Ridge Regression: Biased Estimation
for Nonorthogonal Problems</em>. Technometrics, 42(1), 80-86.
</p>
</li></ul>



<hr>
<h2 id='details_logistic_reg_LiblineaR'>Logistic regression via LiblineaR</h2><span id='topic+details_logistic_reg_LiblineaR'></span>

<h3>Description</h3>

<p><code><a href="LiblineaR.html#topic+LiblineaR">LiblineaR::LiblineaR()</a></code> fits a generalized linear model for binary outcomes. A
linear combination of the predictors is used to model the log odds of an
event.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: see below)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 0)
</p>
</li></ul>

<p>For <code>LiblineaR</code> models, the value for <code>mixture</code> can either be 0 (for
ridge) or 1 (for lasso) but not other intermediate values. In the
<code><a href="LiblineaR.html#topic+LiblineaR">LiblineaR::LiblineaR()</a></code> documentation, these
correspond to types 0 (L2-regularized) and 6 (L1-regularized).
</p>
<p>Be aware that the <code>LiblineaR</code> engine regularizes the intercept. Other
regularized regression models do not, which will result in different
parameter estimates.
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>logistic_reg(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("LiblineaR") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = double(1)
##   mixture = double(1)
## 
## Computational engine: LiblineaR 
## 
## Model fit template:
## LiblineaR::LiblineaR(x = missing_arg(), y = missing_arg(), cost = Inf, 
##     type = double(1), verbose = FALSE)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Sparse Data</h4>

<p>This model can utilize sparse data during model fitting and prediction.
Both sparse matrices such as dgCMatrix from the <code>Matrix</code> package and
sparse tibbles from the <code>sparsevctrs</code> package are supported. See
<a href="#topic+sparse_data">sparse_data</a> for more information.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#logistic-reg-LiblineaR">examples</a>
for <code>logistic_reg()</code> with the <code>"LiblineaR"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Hastie, T, R Tibshirani, and M Wainwright. 2015. <em>Statistical Learning
with Sparsity</em>. CRC Press.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_logistic_reg_spark'>Logistic regression via spark</h2><span id='topic+details_logistic_reg_spark'></span>

<h3>Description</h3>

<p><code><a href="sparklyr.html#topic+ml_logistic_regression">sparklyr::ml_logistic_regression()</a></code> fits a generalized linear model for
binary outcomes. A linear combination of the predictors is used to model the
log odds of an event.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 0.0)
</p>
</li></ul>

<p>For <code>penalty</code>, the amount of regularization includes both the L1 penalty
(i.e., lasso) and the L2 penalty (i.e., ridge or weight decay). As for
<code>mixture</code>:
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code> specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso
and ridge.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>logistic_reg(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("spark") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = double(1)
##   mixture = double(1)
## 
## Computational engine: spark 
## 
## Model fit template:
## sparklyr::ml_logistic_regression(x = missing_arg(), formula = missing_arg(), 
##     weights = missing_arg(), reg_param = double(1), elastic_net_param = double(1), 
##     family = "binomial")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>
<p>By default, <code>ml_logistic_regression()</code> uses the argument
<code>standardization = TRUE</code> to center and scale the data.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p>Note that, for spark engines, the <code>case_weight</code> argument value should be
a character string to specify the column with the numeric case weights.
</p>



<h4>Other details</h4>

<p>For models created using the <code>"spark"</code> engine, there are several things
to consider.
</p>

<ul>
<li><p> Only the formula interface to via <code>fit()</code> is available; using
<code>fit_xy()</code> will generate an error.
</p>
</li>
<li><p> The predictions will always be in a Spark table format. The names will
be the same as documented but without the dots.
</p>
</li>
<li><p> There is no equivalent to factor columns in Spark tables so class
predictions are returned as character columns.
</p>
</li>
<li><p> To retain the model object for a new R session (via <code>save()</code>), the
<code>model$fit</code> element of the parsnip object should be serialized via
<code>ml_save(object$fit)</code> and separately saved to disk. In a new session,
the object can be reloaded and reattached to the parsnip object.
</p>
</li></ul>




<h4>References</h4>


<ul>
<li><p> Luraschi, J, K Kuo, and E Ruiz. 2019. <em>Mastering Spark with R</em>.
O’Reilly Media
</p>
</li>
<li><p> Hastie, T, R Tibshirani, and M Wainwright. 2015. <em>Statistical Learning
with Sparsity</em>. CRC Press.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_logistic_reg_stan'>Logistic regression via stan</h2><span id='topic+details_logistic_reg_stan'></span>

<h3>Description</h3>

<p><code><a href="rstanarm.html#topic+stan_glm">rstanarm::stan_glm()</a></code> fits a generalized linear model for binary outcomes.
A linear combination of the predictors is used to model the log odds of an
event.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters.
</p>



<h4>Important engine-specific options</h4>

<p>Some relevant arguments that can be passed to <code>set_engine()</code>:
</p>

<ul>
<li> <p><code>chains</code>: A positive integer specifying the number of Markov chains.
The default is 4.
</p>
</li>
<li> <p><code>iter</code>: A positive integer specifying the number of iterations for
each chain (including warmup). The default is 2000.
</p>
</li>
<li> <p><code>seed</code>: The seed for random number generation.
</p>
</li>
<li> <p><code>cores</code>: Number of cores to use when executing the chains in parallel.
</p>
</li>
<li> <p><code>prior</code>: The prior distribution for the (non-hierarchical) regression
coefficients. This <code>"stan"</code> engine does not fit any hierarchical
terms.
</p>
</li>
<li> <p><code>prior_intercept</code>: The prior distribution for the intercept (after
centering all predictors).
</p>
</li></ul>

<p>See <code><a href="rstan.html#topic+stanmodel-method-sampling">rstan::sampling()</a></code> and
<code><a href="rstanarm.html#topic+priors">rstanarm::priors()</a></code> for more information on these
and other options.
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>logistic_reg() %&gt;% 
  set_engine("stan") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Computational engine: stan 
## 
## Model fit template:
## rstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), family = stats::binomial, refresh = 0)
</pre></div>
<p>Note that the <code>refresh</code> default prevents logging of the estimation
process. Change this value in <code>set_engine()</code> to show the MCMC logs.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Other details</h4>

<p>For prediction, the <code>"stan"</code> engine can compute posterior intervals
analogous to confidence and prediction intervals. In these instances,
the units are the original outcome and when <code>std_error = TRUE</code>, the
standard deviation of the posterior distribution (or posterior
predictive distribution as appropriate) is returned.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#logistic-reg-stan">examples</a>
for <code>logistic_reg()</code> with the <code>"stan"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> McElreath, R. 2020 <em>Statistical Rethinking</em>. CRC Press.
</p>
</li></ul>



<hr>
<h2 id='details_logistic_reg_stan_glmer'>Logistic regression via hierarchical Bayesian methods</h2><span id='topic+details_logistic_reg_stan_glmer'></span>

<h3>Description</h3>

<p>The <code>"stan_glmer"</code> engine estimates hierarchical regression parameters using
Bayesian estimation.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>



<h4>Important engine-specific options</h4>

<p>Some relevant arguments that can be passed to <code>set_engine()</code>:
</p>

<ul>
<li> <p><code>chains</code>: A positive integer specifying the number of Markov chains.
The default is 4.
</p>
</li>
<li> <p><code>iter</code>: A positive integer specifying the number of iterations for
each chain (including warmup). The default is 2000.
</p>
</li>
<li> <p><code>seed</code>: The seed for random number generation.
</p>
</li>
<li> <p><code>cores</code>: Number of cores to use when executing the chains in parallel.
</p>
</li>
<li> <p><code>prior</code>: The prior distribution for the (non-hierarchical) regression
coefficients.
</p>
</li>
<li> <p><code>prior_intercept</code>: The prior distribution for the intercept (after
centering all predictors).
</p>
</li></ul>

<p>See <code>?rstanarm::stan_glmer</code> and <code>?rstan::sampling</code> for more information.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

logistic_reg() %&gt;% 
  set_engine("stan_glmer") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Logistic Regression Model Specification (classification)
## 
## Computational engine: stan_glmer 
## 
## Model fit template:
## rstanarm::stan_glmer(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), family = stats::binomial, refresh = 0)
</pre></div>



<h4>Predicting new samples</h4>

<p>This model can use subject-specific coefficient estimates to make
predictions (i.e. partial pooling). For example, this equation shows the
linear predictor (<code style="white-space: pre;">&#8288;\eta&#8288;</code>) for a random intercept:
</p>
<div class="sourceCode"><pre>\eta_{i} = (\beta_0 + b_{0i}) + \beta_1x_{i1}
</pre></div>
<p>where <code>i</code> denotes the <code>i</code>th independent experimental unit
(e.g. subject). When the model has seen subject <code>i</code>, it can use that
subject’s data to adjust the <em>population</em> intercept to be more specific
to that subjects results.
</p>
<p>What happens when data are being predicted for a subject that was not
used in the model fit? In that case, this package uses <em>only</em> the
population parameter estimates for prediction:
</p>
<div class="sourceCode"><pre>\hat{\eta}_{i'} = \hat{\beta}_0+ \hat{\beta}x_{i'1}
</pre></div>
<p>Depending on what covariates are in the model, this might have the
effect of making the same prediction for all new samples. The population
parameters are the “best estimate” for a subject that was not included
in the model fit.
</p>
<p>The tidymodels framework deliberately constrains predictions for new
data to not use the training set or other data (to prevent information
leakage).
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model can accept case weights.
</p>
<p>With parsnip, we suggest using the formula method when fitting:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
data("toenail", package = "HSAUR3")

logistic_reg() %&gt;% 
  set_engine("stan_glmer") %&gt;% 
  fit(outcome ~ treatment * visit + (1 | patientID), data = toenail)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the typical formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

glmer_spec &lt;- 
  logistic_reg() %&gt;% 
  set_engine("stan_glmer")

glmer_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = outcome, predictors = c(treatment, visit, patientID)) %&gt;% 
  add_model(glmer_spec, formula = outcome ~ treatment * visit + (1 | patientID))

fit(glmer_wflow, data = toenail)
</pre></div>
<p>For prediction, the <code>"stan_glmer"</code> engine can compute posterior
intervals analogous to confidence and prediction intervals. In these
instances, the units are the original outcome. When <code>std_error = TRUE</code>,
the standard deviation of the posterior distribution (or posterior
predictive distribution as appropriate) is returned.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> McElreath, R. 2020 <em>Statistical Rethinking</em>. CRC Press.
</p>
</li>
<li><p> Sorensen, T, Vasishth, S. 2016. Bayesian linear mixed models using
Stan: A tutorial for psychologists, linguists, and cognitive
scientists, arXiv:1506.06201.
</p>
</li></ul>



<hr>
<h2 id='details_mars_earth'>Multivariate adaptive regression splines (MARS) via earth</h2><span id='topic+details_mars_earth'></span>

<h3>Description</h3>

<p><code><a href="earth.html#topic+earth">earth::earth()</a></code> fits a generalized linear model that uses artificial features for
some predictors. These features resemble hinge functions and the result is
a model that is a segmented regression in small dimensions.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>num_terms</code>: # Model Terms (type: integer, default: see below)
</p>
</li>
<li> <p><code>prod_degree</code>: Degree of Interaction (type: integer, default: 1L)
</p>
</li>
<li> <p><code>prune_method</code>: Pruning Method (type: character, default: ‘backward’)
</p>
</li></ul>

<p>Parsnip changes the default range for <code>num_terms</code> to <code>c(50, 500)</code>.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>mars(num_terms = integer(1), prod_degree = integer(1), prune_method = character(1)) %&gt;% 
  set_engine("earth") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## MARS Model Specification (regression)
## 
## Main Arguments:
##   num_terms = integer(1)
##   prod_degree = integer(1)
##   prune_method = character(1)
## 
## Computational engine: earth 
## 
## Model fit template:
## earth::earth(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     nprune = integer(1), degree = integer(1), pmethod = character(1), 
##     keepxy = TRUE)
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>mars(num_terms = integer(1), prod_degree = integer(1), prune_method = character(1)) %&gt;% 
  set_engine("earth") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## MARS Model Specification (classification)
## 
## Main Arguments:
##   num_terms = integer(1)
##   prod_degree = integer(1)
##   prune_method = character(1)
## 
## Engine-Specific Arguments:
##   glm = list(family = stats::binomial)
## 
## Computational engine: earth 
## 
## Model fit template:
## earth::earth(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     nprune = integer(1), degree = integer(1), pmethod = character(1), 
##     glm = list(family = stats::binomial), keepxy = TRUE)
</pre></div>
<p>An alternate method for using MARs for categorical outcomes can be found
in <code><a href="#topic+discrim_flexible">discrim_flexible()</a></code>.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p>Note that the <code>earth</code> package documentation has: “In the current
implementation, <em>building models with weights can be slow</em>.”
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#mars-earth">examples</a>
for <code>mars()</code> with the <code>"earth"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Friedman, J. 1991. “Multivariate Adaptive Regression Splines.” <em>The
Annals of Statistics</em>, vol. 19, no. 1, pp. 1-67.
</p>
</li>
<li><p> Milborrow, S. <a href="http://www.milbo.org/doc/earth-notes.pdf">“Notes on the earth package.”</a>
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_mlp_brulee'>Multilayer perceptron via brulee</h2><span id='topic+details_mlp_brulee'></span>

<h3>Description</h3>

<p><code><a href="brulee.html#topic+brulee_mlp">brulee::brulee_mlp()</a></code> fits a neural network.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 7 tuning parameters:
</p>

<ul>
<li> <p><code>epochs</code>: # Epochs (type: integer, default: 100L)
</p>
</li>
<li> <p><code>hidden_units</code>: # Hidden Units (type: integer, default: 3L)
</p>
</li>
<li> <p><code>activation</code>: Activation Function (type: character, default: ‘relu’)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.001)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 0.0)
</p>
</li>
<li> <p><code>dropout</code>: Dropout Rate (type: double, default: 0.0)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.01)
</p>
</li></ul>

<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as glmnet).
The zeroing out of parameters is a specific feature the optimization
method used in those packages.
</p>
<p>Both <code>penalty</code> and <code>dropout</code> should be not be used in the same model.
</p>
<p>Other engine arguments of interest:
</p>

<ul>
<li> <p><code>momentum</code>: A number used to use historical gradient infomration
during optimization.
</p>
</li>
<li> <p><code>batch_size</code>: An integer for the number of training set points in each
batch.
</p>
</li>
<li> <p><code>class_weights</code>: Numeric class weights. See
<code><a href="brulee.html#topic+brulee_mlp">brulee::brulee_mlp()</a></code>.
</p>
</li>
<li> <p><code>stop_iter</code>: A non-negative integer for how many iterations with no
improvement before stopping. (default: 5L).
</p>
</li>
<li> <p><code>rate_schedule</code>: A function to change the learning rate over epochs.
See <code><a href="brulee.html#topic+schedule_decay_time">brulee::schedule_decay_time()</a></code>
for details.
</p>
</li></ul>




<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) %&gt;%  
  set_engine("brulee") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (regression)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
##   learn_rate = double(1)
## 
## Computational engine: brulee 
## 
## Model fit template:
## brulee::brulee_mlp(x = missing_arg(), y = missing_arg(), hidden_units = integer(1), 
##     penalty = double(1), dropout = double(1), epochs = integer(1), 
##     activation = character(1), learn_rate = double(1))
</pre></div>
<p>Note that parsnip automatically sets linear activation in the last
layer.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) %&gt;% 
  set_engine("brulee") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (classification)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
##   learn_rate = double(1)
## 
## Computational engine: brulee 
## 
## Model fit template:
## brulee::brulee_mlp(x = missing_arg(), y = missing_arg(), hidden_units = integer(1), 
##     penalty = double(1), dropout = double(1), epochs = integer(1), 
##     activation = character(1), learn_rate = double(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_mlp_brulee_two_layer'>Multilayer perceptron via brulee with two hidden layers</h2><span id='topic+details_mlp_brulee_two_layer'></span>

<h3>Description</h3>

<p><code><a href="brulee.html#topic+brulee_mlp">brulee::brulee_mlp_two_layer()</a></code> fits a neural network (with version 0.3.0.9000 or higher of brulee)
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 7 tuning parameters:
</p>

<ul>
<li> <p><code>epochs</code>: # Epochs (type: integer, default: 100L)
</p>
</li>
<li> <p><code>hidden_units</code>: # Hidden Units (type: integer, default: 3L)
</p>
</li>
<li> <p><code>activation</code>: Activation Function (type: character, default: ‘relu’)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.001)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 0.0)
</p>
</li>
<li> <p><code>dropout</code>: Dropout Rate (type: double, default: 0.0)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.01)
</p>
</li></ul>

<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as glmnet).
The zeroing out of parameters is a specific feature the optimization
method used in those packages.
</p>
<p>Both <code>penalty</code> and <code>dropout</code> should be not be used in the same model.
</p>
<p>Other engine arguments of interest:
</p>

<ul>
<li> <p><code>hidden_layer_2</code> and <code>activation_2</code> control the format of the second
layer.
</p>
</li>
<li> <p><code>momentum</code>: A number used to use historical gradient information
during optimization.
</p>
</li>
<li> <p><code>batch_size</code>: An integer for the number of training set points in each
batch.
</p>
</li>
<li> <p><code>class_weights</code>: Numeric class weights. See
<code><a href="brulee.html#topic+brulee_mlp">brulee::brulee_mlp()</a></code>.
</p>
</li>
<li> <p><code>stop_iter</code>: A non-negative integer for how many iterations with no
improvement before stopping. (default: 5L).
</p>
</li>
<li> <p><code>rate_schedule</code>: A function to change the learning rate over epochs.
See <code><a href="brulee.html#topic+schedule_decay_time">brulee::schedule_decay_time()</a></code>
for details.
</p>
</li></ul>




<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) %&gt;%
  set_engine("brulee_two_layer",
             hidden_units_2 = integer(1),
             activation_2 = character(1)) %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (regression)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
##   learn_rate = double(1)
## 
## Engine-Specific Arguments:
##   hidden_units_2 = integer(1)
##   activation_2 = character(1)
## 
## Computational engine: brulee_two_layer 
## 
## Model fit template:
## brulee::brulee_mlp_two_layer(x = missing_arg(), y = missing_arg(), 
##     hidden_units = integer(1), penalty = double(1), dropout = double(1), 
##     epochs = integer(1), activation = character(1), learn_rate = double(1), 
##     hidden_units_2 = integer(1), activation_2 = character(1))
</pre></div>
<p>Note that parsnip automatically sets the linear activation in the last
layer.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) %&gt;% 
  set_engine("brulee_two_layer",
             hidden_units_2 = integer(1),
             activation_2 = character(1)) %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (classification)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
##   learn_rate = double(1)
## 
## Engine-Specific Arguments:
##   hidden_units_2 = integer(1)
##   activation_2 = character(1)
## 
## Computational engine: brulee_two_layer 
## 
## Model fit template:
## brulee::brulee_mlp_two_layer(x = missing_arg(), y = missing_arg(), 
##     hidden_units = integer(1), penalty = double(1), dropout = double(1), 
##     epochs = integer(1), activation = character(1), learn_rate = double(1), 
##     hidden_units_2 = integer(1), activation_2 = character(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_mlp_h2o'>Multilayer perceptron via h2o</h2><span id='topic+details_mlp_h2o'></span>

<h3>Description</h3>

<p><code><a href="h2o.html#topic+h2o.deeplearning">h2o::h2o.deeplearning()</a></code> fits a feed-forward neural network.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 6 tuning parameters:
</p>

<ul>
<li> <p><code>hidden_units</code>: # Hidden Units (type: integer, default: 200L)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li>
<li> <p><code>dropout</code>: Dropout Rate (type: double, default: 0.5)
</p>
</li>
<li> <p><code>epochs</code>: # Epochs (type: integer, default: 10)
</p>
</li>
<li> <p><code>activation</code>: Activation function (type: character, default: ‘see
below’)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.005)
</p>
</li></ul>

<p>The naming of activation functions in
<code><a href="h2o.html#topic+h2o.deeplearning">h2o::h2o.deeplearning()</a></code> differs from
parsnip’s conventions. Currently, only “relu” and “tanh” are supported
and will be converted internally to “Rectifier” and “Tanh” passed to the
fitting function.
</p>
<p><code>penalty</code> corresponds to l2 penalty.
<code><a href="h2o.html#topic+h2o.deeplearning">h2o::h2o.deeplearning()</a></code> also supports
specifying the l1 penalty directly with the engine argument <code>l1</code>.
</p>
<p>Other engine arguments of interest:
</p>

<ul>
<li> <p><code>stopping_rounds</code> controls early stopping rounds based on the
convergence of another engine parameter <code>stopping_metric</code>. By default,
<a href="h2o.html#topic+h2o.deeplearning">h2o::h2o.deeplearning</a> stops training if
simple moving average of length 5 of the stopping_metric does not
improve for 5 scoring events. This is mostly useful when used
alongside the engine parameter <code>validation</code>, which is the
<strong>proportion</strong> of train-validation split, parsnip will split and pass
the two data frames to h2o. Then
<a href="h2o.html#topic+h2o.deeplearning">h2o::h2o.deeplearning</a> will evaluate the
metric and early stopping criteria on the validation set.
</p>
</li>
<li><p> h2o uses a 50% dropout ratio controlled by <code>dropout</code> for hidden layers
by default. <code><a href="h2o.html#topic+h2o.deeplearning">h2o::h2o.deeplearning()</a></code>
provides an engine argument <code>input_dropout_ratio</code> for dropout ratios
in the input layer, which defaults to 0.
</p>
</li></ul>




<h4>Translation from parsnip to the original package (regression)</h4>

<p><a href="agua.html#topic+h2o_train">agua::h2o_train_mlp</a> is a wrapper around
<code><a href="h2o.html#topic+h2o.deeplearning">h2o::h2o.deeplearning()</a></code>.
</p>
<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) %&gt;%  
  set_engine("h2o") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (regression)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
##   learn_rate = double(1)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_mlp(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), hidden = integer(1), l2 = double(1), 
##     hidden_dropout_ratios = double(1), epochs = integer(1), activation = character(1), 
##     rate = double(1))
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) %&gt;% 
  set_engine("h2o") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (classification)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
##   learn_rate = double(1)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_mlp(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), hidden = integer(1), l2 = double(1), 
##     hidden_dropout_ratios = double(1), epochs = integer(1), activation = character(1), 
##     rate = double(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>
<p>By default, <code><a href="h2o.html#topic+h2o.deeplearning">h2o::h2o.deeplearning()</a></code> uses
the argument <code>standardize = TRUE</code> to center and scale all numeric
columns.
</p>



<h4>Initializing h2o</h4>

<p>To use the h2o engine with tidymodels, please run <code>h2o::h2o.init()</code>
first. By default, This connects R to the local h2o server. This needs
to be done in every new R session. You can also connect to a remote h2o
server with an IP address, for more details see
<code><a href="h2o.html#topic+h2o.init">h2o::h2o.init()</a></code>.
</p>
<p>You can control the number of threads in the thread pool used by h2o
with the <code>nthreads</code> argument. By default, it uses all CPUs on the host.
This is different from the usual parallel processing mechanism in
tidymodels for tuning, while tidymodels parallelizes over resamples, h2o
parallelizes over hyperparameter combinations for a given resample.
</p>
<p>h2o will automatically shut down the local h2o instance started by R
when R is terminated. To manually stop the h2o server, run
<code>h2o::h2o.shutdown()</code>.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>


<hr>
<h2 id='details_mlp_keras'>Multilayer perceptron via keras</h2><span id='topic+details_mlp_keras'></span>

<h3>Description</h3>

<p><code><a href="#topic+keras_mlp">keras_mlp()</a></code> fits a single layer, feed-forward neural network.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 5 tuning parameters:
</p>

<ul>
<li> <p><code>hidden_units</code>: # Hidden Units (type: integer, default: 5L)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li>
<li> <p><code>dropout</code>: Dropout Rate (type: double, default: 0.0)
</p>
</li>
<li> <p><code>epochs</code>: # Epochs (type: integer, default: 20L)
</p>
</li>
<li> <p><code>activation</code>: Activation Function (type: character, default:
‘softmax’)
</p>
</li></ul>




<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  activation = character(1)
) %&gt;%  
  set_engine("keras") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (regression)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
## 
## Computational engine: keras 
## 
## Model fit template:
## parsnip::keras_mlp(x = missing_arg(), y = missing_arg(), hidden_units = integer(1), 
##     penalty = double(1), dropout = double(1), epochs = integer(1), 
##     activation = character(1))
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  activation = character(1)
) %&gt;% 
  set_engine("keras") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (classification)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
## 
## Computational engine: keras 
## 
## Model fit template:
## parsnip::keras_mlp(x = missing_arg(), y = missing_arg(), hidden_units = integer(1), 
##     penalty = double(1), dropout = double(1), epochs = integer(1), 
##     activation = character(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#mlp-keras">examples</a>
for <code>mlp()</code> with the <code>"keras"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_mlp_nnet'>Multilayer perceptron via nnet</h2><span id='topic+details_mlp_nnet'></span>

<h3>Description</h3>

<p><code><a href="nnet.html#topic+nnet">nnet::nnet()</a></code> fits a single layer, feed-forward neural network.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>hidden_units</code>: # Hidden Units (type: integer, default: none)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li>
<li> <p><code>epochs</code>: # Epochs (type: integer, default: 100L)
</p>
</li></ul>

<p>Note that, in <code><a href="nnet.html#topic+nnet">nnet::nnet()</a></code>, the maximum number of
parameters is an argument with a fairly low value of <code>maxit = 1000</code>. For
some models, you may need to pass this value in via
<code><a href="#topic+set_engine">set_engine()</a></code> so that the model does not fail.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  epochs = integer(1)
) %&gt;%  
  set_engine("nnet") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (regression)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   epochs = integer(1)
## 
## Computational engine: nnet 
## 
## Model fit template:
## nnet::nnet(formula = missing_arg(), data = missing_arg(), size = integer(1), 
##     decay = double(1), maxit = integer(1), trace = FALSE, linout = TRUE)
</pre></div>
<p>Note that parsnip automatically sets linear activation in the last
layer.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  epochs = integer(1)
) %&gt;% 
  set_engine("nnet") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (classification)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   epochs = integer(1)
## 
## Computational engine: nnet 
## 
## Model fit template:
## nnet::nnet(formula = missing_arg(), data = missing_arg(), size = integer(1), 
##     decay = double(1), maxit = integer(1), trace = FALSE, linout = FALSE)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#mlp-nnet">examples</a>
for <code>mlp()</code> with the <code>"nnet"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_multinom_reg_brulee'>Multinomial regression via brulee</h2><span id='topic+details_multinom_reg_brulee'></span>

<h3>Description</h3>

<p><code><a href="brulee.html#topic+brulee_multinomial_reg">brulee::brulee_multinomial_reg()</a></code> fits a model that uses linear predictors
to predict multiclass data using the multinomial distribution.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameter:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.001)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 0.0)
</p>
</li></ul>

<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as glmnet).
The zeroing out of parameters is a specific feature the optimization
method used in those packages.
</p>
<p>Other engine arguments of interest:
</p>

<ul>
<li> <p><code>optimizer()</code>: The optimization method. See
<code><a href="brulee.html#topic+brulee_linear_reg">brulee::brulee_linear_reg()</a></code>.
</p>
</li>
<li> <p><code>epochs()</code>: An integer for the number of passes through the training
set.
</p>
</li>
<li> <p><code>lean_rate()</code>: A number used to accelerate the gradient decsent
process.
</p>
</li>
<li> <p><code>momentum()</code>: A number used to use historical gradient information
during optimization (<code>optimizer = "SGD"</code> only).
</p>
</li>
<li> <p><code>batch_size()</code>: An integer for the number of training set points in
each batch.
</p>
</li>
<li> <p><code>stop_iter()</code>: A non-negative integer for how many iterations with no
improvement before stopping. (default: 5L).
</p>
</li>
<li> <p><code>class_weights()</code>: Numeric class weights. See
<code><a href="brulee.html#topic+brulee_multinomial_reg">brulee::brulee_multinomial_reg()</a></code>.
</p>
</li></ul>




<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>multinom_reg(penalty = double(1)) %&gt;% 
  set_engine("brulee") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Multinomial Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = double(1)
## 
## Computational engine: brulee 
## 
## Model fit template:
## brulee::brulee_multinomial_reg(x = missing_arg(), y = missing_arg(), 
##     penalty = double(1))
</pre></div>
<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_multinom_reg_glmnet'>Multinomial regression via glmnet</h2><span id='topic+details_multinom_reg_glmnet'></span>

<h3>Description</h3>

<p><code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> fits a model that uses linear predictors to predict
multiclass data using the multinomial distribution.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: see below)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 1.0)
</p>
</li></ul>

<p>The <code>penalty</code> parameter has no default and requires a single numeric
value. For more details about this, and the <code>glmnet</code> model in general,
see <a href="#topic+glmnet-details">glmnet-details</a>. As for <code>mixture</code>:
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code> specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso
and ridge.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>multinom_reg(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("glmnet") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Multinomial Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = 0
##   mixture = double(1)
## 
## Computational engine: glmnet 
## 
## Model fit template:
## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     alpha = double(1), family = "multinomial")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one. By default, <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> uses
the argument <code>standardize = TRUE</code> to center and scale the data.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#multinom-reg-glmnet">examples</a>
for <code>multinom_reg()</code> with the <code>"glmnet"</code> engine.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Sparse Data</h4>

<p>This model can utilize sparse data during model fitting and prediction.
Both sparse matrices such as dgCMatrix from the <code>Matrix</code> package and
sparse tibbles from the <code>sparsevctrs</code> package are supported. See
<a href="#topic+sparse_data">sparse_data</a> for more information.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>References</h4>


<ul>
<li><p> Hastie, T, R Tibshirani, and M Wainwright. 2015. <em>Statistical Learning
with Sparsity</em>. CRC Press.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_multinom_reg_h2o'>Multinomial regression via h2o</h2><span id='topic+details_multinom_reg_h2o'></span>

<h3>Description</h3>

<p><code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> fits a model that uses linear predictors to predict
multiclass data for multinomial responses.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: see
below)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: see below)
</p>
</li></ul>

<p>By default, when not given a fixed <code>penalty</code>,
<code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> uses a heuristic approach to select
the optimal value of <code>penalty</code> based on training data. Setting the
engine parameter <code>lambda_search</code> to <code>TRUE</code> enables an efficient version
of the grid search, see more details at
<a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/lambda_search.html">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/lambda_search.html</a>.
</p>
<p>The choice of <code>mixture</code> depends on the engine parameter <code>solver</code>, which
is automatically chosen given training data and the specification of
other model parameters. When <code>solver</code> is set to <code>'L-BFGS'</code>, <code>mixture</code>
defaults to 0 (ridge regression) and 0.5 otherwise.
</p>



<h4>Translation from parsnip to the original package</h4>

<p><code><a href="agua.html#topic+h2o_train">agua::h2o_train_glm()</a></code> for <code>multinom_reg()</code> is
a wrapper around <code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> with
<code>family = 'multinomial'</code>.
</p>
<div class="sourceCode r"><pre>multinom_reg(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("h2o") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Multinomial Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = double(1)
##   mixture = double(1)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_glm(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), lambda = double(1), alpha = double(1), 
##     family = "multinomial")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>
<p>By default, <code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> uses the argument
<code>standardize = TRUE</code> to center and scale the data.
</p>



<h4>Initializing h2o</h4>

<p>To use the h2o engine with tidymodels, please run <code>h2o::h2o.init()</code>
first. By default, This connects R to the local h2o server. This needs
to be done in every new R session. You can also connect to a remote h2o
server with an IP address, for more details see
<code><a href="h2o.html#topic+h2o.init">h2o::h2o.init()</a></code>.
</p>
<p>You can control the number of threads in the thread pool used by h2o
with the <code>nthreads</code> argument. By default, it uses all CPUs on the host.
This is different from the usual parallel processing mechanism in
tidymodels for tuning, while tidymodels parallelizes over resamples, h2o
parallelizes over hyperparameter combinations for a given resample.
</p>
<p>h2o will automatically shut down the local h2o instance started by R
when R is terminated. To manually stop the h2o server, run
<code>h2o::h2o.shutdown()</code>.
</p>


<hr>
<h2 id='details_multinom_reg_keras'>Multinomial regression via keras</h2><span id='topic+details_multinom_reg_keras'></span>

<h3>Description</h3>

<p><code><a href="#topic+keras_mlp">keras_mlp()</a></code> fits a model that uses linear predictors to predict
multiclass data using the multinomial distribution.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has one tuning parameter:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li></ul>

<p>For <code>penalty</code>, the amount of regularization is <em>only</em> L2 penalty (i.e.,
ridge or weight decay).
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>multinom_reg(penalty = double(1)) %&gt;% 
  set_engine("keras") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Multinomial Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = double(1)
## 
## Computational engine: keras 
## 
## Model fit template:
## parsnip::keras_mlp(x = missing_arg(), y = missing_arg(), penalty = double(1), 
##     hidden_units = 1, act = "linear")
</pre></div>
<p><code><a href="#topic+keras_mlp">keras_mlp()</a></code> is a parsnip wrapper around keras code for
neural networks. This model fits a linear regression as a network with a
single hidden unit.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#multinom-reg-keras">examples</a>
for <code>multinom_reg()</code> with the <code>"keras"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Hoerl, A., &amp; Kennard, R. (2000). <em>Ridge Regression: Biased Estimation
for Nonorthogonal Problems</em>. Technometrics, 42(1), 80-86.
</p>
</li></ul>



<hr>
<h2 id='details_multinom_reg_nnet'>Multinomial regression via nnet</h2><span id='topic+details_multinom_reg_nnet'></span>

<h3>Description</h3>

<p><code><a href="nnet.html#topic+multinom">nnet::multinom()</a></code> fits a model that uses linear predictors to predict
multiclass data using the multinomial distribution.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 1 tuning parameters:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li></ul>

<p>For <code>penalty</code>, the amount of regularization includes only the L2 penalty
(i.e., ridge or weight decay).
</p>



<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>multinom_reg(penalty = double(1)) %&gt;% 
  set_engine("nnet") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Multinomial Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = double(1)
## 
## Computational engine: nnet 
## 
## Model fit template:
## nnet::multinom(formula = missing_arg(), data = missing_arg(), 
##     decay = double(1), trace = FALSE)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#multinom-reg-nnet">examples</a>
for <code>multinom_reg()</code> with the <code>"nnet"</code> engine.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>References</h4>


<ul>
<li><p> Luraschi, J, K Kuo, and E Ruiz. 2019. <em>Mastering nnet with R</em>.
O’Reilly Media
</p>
</li>
<li><p> Hastie, T, R Tibshirani, and M Wainwright. 2015. <em>Statistical Learning
with Sparsity</em>. CRC Press.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_multinom_reg_spark'>Multinomial regression via spark</h2><span id='topic+details_multinom_reg_spark'></span>

<h3>Description</h3>

<p><code><a href="sparklyr.html#topic+ml_logistic_regression">sparklyr::ml_logistic_regression()</a></code> fits a model that uses linear
predictors to predict multiclass data using the multinomial distribution.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 0.0)
</p>
</li></ul>

<p>For <code>penalty</code>, the amount of regularization includes both the L1 penalty
(i.e., lasso) and the L2 penalty (i.e., ridge or weight decay). As for
<code>mixture</code>:
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code> specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso
and ridge.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<div class="sourceCode r"><pre>multinom_reg(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("spark") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Multinomial Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = double(1)
##   mixture = double(1)
## 
## Computational engine: spark 
## 
## Model fit template:
## sparklyr::ml_logistic_regression(x = missing_arg(), formula = missing_arg(), 
##     weights = missing_arg(), reg_param = double(1), elastic_net_param = double(1), 
##     family = "multinomial")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>
<p>By default, <code>ml_multinom_regression()</code> uses the argument
<code>standardization = TRUE</code> to center and scale the data.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p>Note that, for spark engines, the <code>case_weight</code> argument value should be
a character string to specify the column with the numeric case weights.
</p>



<h4>Other details</h4>

<p>For models created using the <code>"spark"</code> engine, there are several things
to consider.
</p>

<ul>
<li><p> Only the formula interface to via <code>fit()</code> is available; using
<code>fit_xy()</code> will generate an error.
</p>
</li>
<li><p> The predictions will always be in a Spark table format. The names will
be the same as documented but without the dots.
</p>
</li>
<li><p> There is no equivalent to factor columns in Spark tables so class
predictions are returned as character columns.
</p>
</li>
<li><p> To retain the model object for a new R session (via <code>save()</code>), the
<code>model$fit</code> element of the parsnip object should be serialized via
<code>ml_save(object$fit)</code> and separately saved to disk. In a new session,
the object can be reloaded and reattached to the parsnip object.
</p>
</li></ul>




<h4>References</h4>


<ul>
<li><p> Luraschi, J, K Kuo, and E Ruiz. 2019. <em>Mastering Spark with R</em>.
O’Reilly Media
</p>
</li>
<li><p> Hastie, T, R Tibshirani, and M Wainwright. 2015. <em>Statistical Learning
with Sparsity</em>. CRC Press.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_naive_Bayes_h2o'>Naive Bayes models via naivebayes</h2><span id='topic+details_naive_Bayes_h2o'></span>

<h3>Description</h3>

<p><code><a href="h2o.html#topic+h2o.naiveBayes">h2o::h2o.naiveBayes()</a></code> fits a model that uses Bayes' theorem to compute
the probability of each class, given the predictor values.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 1 tuning parameter:
</p>

<ul>
<li> <p><code>Laplace</code>: Laplace Correction (type: double, default: 0.0)
</p>
</li></ul>

<p><code><a href="h2o.html#topic+h2o.naiveBayes">h2o::h2o.naiveBayes()</a></code> provides several engine
arguments to deal with imbalances and rare classes:
</p>

<ul>
<li> <p><code>balance_classes</code> A logical value controlling over/under-sampling (for
imbalanced data). Defaults to <code>FALSE</code>.
</p>
</li>
<li> <p><code>class_sampling_factors</code> The over/under-sampling ratios per class (in
lexicographic order). If not specified, sampling factors will be
automatically computed to obtain class balance during training.
Require <code>balance_classes</code> to be <code>TRUE</code>.
</p>
</li>
<li> <p><code>min_sdev</code>: The minimum standard deviation to use for observations
without enough data, must be greater than 1e-10.
</p>
</li>
<li> <p><code>min_prob</code>: The minimum probability to use for observations with not
enough data.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>agua</strong> extension package is required to fit this model.
</p>
<p><code><a href="agua.html#topic+h2o_train">agua::h2o_train_nb()</a></code> is a wrapper around
<code><a href="h2o.html#topic+h2o.naiveBayes">h2o::h2o.naiveBayes()</a></code>.
</p>
<div class="sourceCode r"><pre>naive_Bayes(Laplace = numeric(0)) %&gt;% 
  set_engine("h2o") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Naive Bayes Model Specification (classification)
## 
## Main Arguments:
##   Laplace = numeric(0)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_nb(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), laplace = numeric(0))
</pre></div>



<h4>Initializing h2o</h4>

<p>To use the h2o engine with tidymodels, please run <code>h2o::h2o.init()</code>
first. By default, This connects R to the local h2o server. This needs
to be done in every new R session. You can also connect to a remote h2o
server with an IP address, for more details see
<code><a href="h2o.html#topic+h2o.init">h2o::h2o.init()</a></code>.
</p>
<p>You can control the number of threads in the thread pool used by h2o
with the <code>nthreads</code> argument. By default, it uses all CPUs on the host.
This is different from the usual parallel processing mechanism in
tidymodels for tuning, while tidymodels parallelizes over resamples, h2o
parallelizes over hyperparameter combinations for a given resample.
</p>
<p>h2o will automatically shut down the local h2o instance started by R
when R is terminated. To manually stop the h2o server, run
<code>h2o::h2o.shutdown()</code>.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>


<hr>
<h2 id='details_naive_Bayes_klaR'>Naive Bayes models via klaR</h2><span id='topic+details_naive_Bayes_klaR'></span>

<h3>Description</h3>

<p><code><a href="klaR.html#topic+NaiveBayes">klaR::NaiveBayes()</a></code> fits a model that uses Bayes' theorem to compute the
probability of each class, given the predictor values.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameter:
</p>

<ul>
<li> <p><code>smoothness</code>: Kernel Smoothness (type: double, default: 1.0)
</p>
</li>
<li> <p><code>Laplace</code>: Laplace Correction (type: double, default: 0.0)
</p>
</li></ul>

<p>Note that the engine argument <code>usekernel</code> is set to <code>TRUE</code> by default
when using the <code>klaR</code> engine.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>discrim</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(discrim)

naive_Bayes(smoothness = numeric(0), Laplace = numeric(0)) %&gt;% 
  set_engine("klaR") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Naive Bayes Model Specification (classification)
## 
## Main Arguments:
##   smoothness = numeric(0)
##   Laplace = numeric(0)
## 
## Computational engine: klaR 
## 
## Model fit template:
## discrim::klar_bayes_wrapper(x = missing_arg(), y = missing_arg(), 
##     adjust = numeric(0), fL = numeric(0), usekernel = TRUE)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>The columns for qualitative predictors should always be represented as
factors (as opposed to dummy/indicator variables). When the predictors
are factors, the underlying code treats them as multinomial data and
appropriately computes their conditional distributions.
</p>
<p>Variance calculations are used in these computations so <em>zero-variance</em>
predictors (i.e., with a single unique value) should be eliminated
before fitting the model.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_naive_Bayes_naivebayes'>Naive Bayes models via naivebayes</h2><span id='topic+details_naive_Bayes_naivebayes'></span>

<h3>Description</h3>

<p><code><a href="naivebayes.html#topic+naive_bayes">naivebayes::naive_bayes()</a></code> fits a model that uses Bayes' theorem to compute
the probability of each class, given the predictor values.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameter:
</p>

<ul>
<li> <p><code>smoothness</code>: Kernel Smoothness (type: double, default: 1.0)
</p>
</li>
<li> <p><code>Laplace</code>: Laplace Correction (type: double, default: 0.0)
</p>
</li></ul>

<p>Note that the engine argument <code>usekernel</code> is set to <code>TRUE</code> by default
when using the <code>naivebayes</code> engine.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>discrim</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(discrim)

naive_Bayes(smoothness = numeric(0), Laplace = numeric(0)) %&gt;% 
  set_engine("naivebayes") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Naive Bayes Model Specification (classification)
## 
## Main Arguments:
##   smoothness = numeric(0)
##   Laplace = numeric(0)
## 
## Computational engine: naivebayes 
## 
## Model fit template:
## naivebayes::naive_bayes(x = missing_arg(), y = missing_arg(), 
##     adjust = numeric(0), laplace = numeric(0), usekernel = TRUE)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>The columns for qualitative predictors should always be represented as
factors (as opposed to dummy/indicator variables). When the predictors
are factors, the underlying code treats them as multinomial data and
appropriately computes their conditional distributions.
</p>
<p>For count data, integers can be estimated using a Poisson distribution
if the argument <code>usepoisson = TRUE</code> is passed as an engine argument.
</p>
<p>Variance calculations are used in these computations so <em>zero-variance</em>
predictors (i.e., with a single unique value) should be eliminated
before fitting the model.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_nearest_neighbor_kknn'>K-nearest neighbors via kknn</h2><span id='topic+details_nearest_neighbor_kknn'></span>

<h3>Description</h3>

<p><code><a href="kknn.html#topic+train.kknn">kknn::train.kknn()</a></code> fits a model that uses the <code>K</code> most similar data points
from the training set to predict new samples.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>neighbors</code>: # Nearest Neighbors (type: integer, default: 5L)
</p>
</li>
<li> <p><code>weight_func</code>: Distance Weighting Function (type: character, default:
‘optimal’)
</p>
</li>
<li> <p><code>dist_power</code>: Minkowski Distance Order (type: double, default: 2.0)
</p>
</li></ul>

<p>Parsnip changes the default range for <code>neighbors</code> to <code>c(1, 15)</code> and
<code>dist_power</code> to <code>c(1/10, 2)</code>.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>nearest_neighbor(
  neighbors = integer(1),
  weight_func = character(1),
  dist_power = double(1)
) %&gt;%  
  set_engine("kknn") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## K-Nearest Neighbor Model Specification (regression)
## 
## Main Arguments:
##   neighbors = integer(1)
##   weight_func = character(1)
##   dist_power = double(1)
## 
## Computational engine: kknn 
## 
## Model fit template:
## kknn::train.kknn(formula = missing_arg(), data = missing_arg(), 
##     ks = min_rows(0L, data, 5), kernel = character(1), distance = double(1))
</pre></div>
<p><code>min_rows()</code> will adjust the number of neighbors if the chosen value if
it is not consistent with the actual data dimensions.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>nearest_neighbor(
  neighbors = integer(1),
  weight_func = character(1),
  dist_power = double(1)
) %&gt;% 
  set_engine("kknn") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## K-Nearest Neighbor Model Specification (classification)
## 
## Main Arguments:
##   neighbors = integer(1)
##   weight_func = character(1)
##   dist_power = double(1)
## 
## Computational engine: kknn 
## 
## Model fit template:
## kknn::train.kknn(formula = missing_arg(), data = missing_arg(), 
##     ks = min_rows(0L, data, 5), kernel = character(1), distance = double(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#nearest-neighbor-kknn">examples</a>
for <code>nearest_neighbor()</code> with the <code>"kknn"</code> engine.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>References</h4>


<ul>
<li><p> Hechenbichler K. and Schliep K.P. (2004) <a href="https://epub.ub.uni-muenchen.de/1769/">Weighted k-Nearest-Neighbor Techniques and Ordinal Classification</a>, Discussion
Paper 399, SFB 386, Ludwig-Maximilians University Munich
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_pls_mixOmics'>Partial least squares via mixOmics</h2><span id='topic+details_pls_mixOmics'></span>

<h3>Description</h3>

<p>The mixOmics package can fit several different types of PLS models.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>predictor_prop</code>: Proportion of Predictors (type: double, default: see
below)
</p>
</li>
<li> <p><code>num_comp</code>: # Components (type: integer, default: 2L)
</p>
</li></ul>




<h4>Translation from parsnip to the underlying model call (regression)</h4>

<p>The <strong>plsmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(plsmod)

pls(num_comp = integer(1), predictor_prop = double(1)) %&gt;%
  set_engine("mixOmics") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## PLS Model Specification (regression)
## 
## Main Arguments:
##   predictor_prop = double(1)
##   num_comp = integer(1)
## 
## Computational engine: mixOmics 
## 
## Model fit template:
## plsmod::pls_fit(x = missing_arg(), y = missing_arg(), predictor_prop = double(1), 
##     ncomp = integer(1))
</pre></div>
<p><code><a href="plsmod.html#topic+pls_fit">plsmod::pls_fit()</a></code> is a function that:
</p>

<ul>
<li><p> Determines the number of predictors in the data.
</p>
</li>
<li><p> Adjusts <code>num_comp</code> if the value is larger than the number of factors.
</p>
</li>
<li><p> Determines whether sparsity is required based on the value of
<code>predictor_prop</code>.
</p>
</li>
<li><p> Sets the <code>keepX</code> argument of <code>mixOmics::spls()</code> for sparse models.
</p>
</li></ul>




<h4>Translation from parsnip to the underlying model call (classification)</h4>

<p>The <strong>plsmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(plsmod)

pls(num_comp = integer(1), predictor_prop = double(1)) %&gt;%
  set_engine("mixOmics") %&gt;%
  set_mode("classification") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## PLS Model Specification (classification)
## 
## Main Arguments:
##   predictor_prop = double(1)
##   num_comp = integer(1)
## 
## Computational engine: mixOmics 
## 
## Model fit template:
## plsmod::pls_fit(x = missing_arg(), y = missing_arg(), predictor_prop = double(1), 
##     ncomp = integer(1))
</pre></div>
<p>In this case, <code><a href="plsmod.html#topic+pls_fit">plsmod::pls_fit()</a></code> has the same role
as above but eventually targets <code>mixOmics::plsda()</code> or
<code>mixOmics::splsda()</code>.
</p>



<h4>Installing mixOmics</h4>

<p>This package is available via the Bioconductor repository and is not
accessible via CRAN. You can install using:
</p>
<div class="sourceCode r"><pre>  if (!require("remotes", quietly = TRUE)) {
    install.packages("remotes")
  }
  
  remotes::install_bioc("mixOmics")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Variance calculations are used in these computations so <em>zero-variance</em>
predictors (i.e., with a single unique value) should be eliminated
before fitting the model.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Rohart F and Gautier B and Singh A and Le Cao K-A (2017). “mixOmics:
An R package for ’omics feature selection and multiple data
integration.” PLoS computational biology, 13(11), e1005752.
</p>
</li></ul>



<hr>
<h2 id='details_poisson_reg_gee'>Poisson regression via generalized estimating equations (GEE)</h2><span id='topic+details_poisson_reg_gee'></span>

<h3>Description</h3>

<p><code>gee::gee()</code> uses generalized least squares to fit different types of models
with errors that are not independent.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no formal tuning parameters. It may be beneficial to
determine the appropriate correlation structure to use, but this
typically does not affect the predicted value of the model. It <em>does</em>
have an effect on the inferential results and parameter covariance
values.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

poisson_reg(engine = "gee") %&gt;% 
  set_engine("gee") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Poisson Regression Model Specification (regression)
## 
## Computational engine: gee 
## 
## Model fit template:
## multilevelmod::gee_fit(formula = missing_arg(), data = missing_arg(), 
##     family = stats::poisson)
</pre></div>
<p><code>multilevelmod::gee_fit()</code> is a wrapper model around <code>gee()</code>.
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Other details</h4>

<p>Both <code>gee:gee()</code> and <code>gee:geepack()</code> specify the id/cluster variable
using an argument <code>id</code> that requires a vector. parsnip doesn’t work that
way so we enable this model to be fit using a artificial function
<code>id_var()</code> to be used in the formula. So, in the original package, the
call would look like:
</p>
<div class="sourceCode r"><pre>gee(breaks ~ tension, id = wool, data = warpbreaks, corstr = "exchangeable")
</pre></div>
<p>With parsnip, we suggest using the formula method when fitting:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

poisson_reg() %&gt;% 
  set_engine("gee", corstr = "exchangeable") %&gt;% 
  fit(y ~ time + x + id_var(subject), data = longitudinal_counts)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the GEE formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

gee_spec &lt;- 
  poisson_reg() %&gt;% 
  set_engine("gee", corstr = "exchangeable")

gee_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = y, predictors = c(time, x, subject)) %&gt;% 
  add_model(gee_spec, formula = y ~ time + x + id_var(subject))

fit(gee_wflow, data = longitudinal_counts)
</pre></div>
<p>The <code>gee::gee()</code> function always prints out warnings and output even
when <code>silent = TRUE</code>. The parsnip <code>"gee"</code> engine, by contrast, silences
all console output coming from <code>gee::gee()</code>, even if <code>silent = FALSE</code>.
</p>
<p>Also, because of issues with the <code>gee()</code> function, a supplementary call
to <code>glm()</code> is needed to get the rank and QR decomposition objects so
that <code>predict()</code> can be used.
</p>



<h4>References</h4>


<ul>
<li><p> Liang, K.Y. and Zeger, S.L. (1986) Longitudinal data analysis using
generalized linear models. <em>Biometrika</em>, 73 13–22.
</p>
</li>
<li><p> Zeger, S.L. and Liang, K.Y. (1986) Longitudinal data analysis for
discrete and continuous outcomes. <em>Biometrics</em>, 42 121–130.
</p>
</li></ul>



<hr>
<h2 id='details_poisson_reg_glm'>Poisson regression via glm</h2><span id='topic+details_poisson_reg_glm'></span>

<h3>Description</h3>

<p><code><a href="stats.html#topic+glm">stats::glm()</a></code> uses maximum likelihood to fit a model for count data.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters.
</p>



<h4>Translation from parsnip to the underlying model call (regression)</h4>

<p>The <strong>poissonreg</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(poissonreg)

poisson_reg() %&gt;%
  set_engine("glm") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Poisson Regression Model Specification (regression)
## 
## Computational engine: glm 
## 
## Model fit template:
## stats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     family = stats::poisson)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p><em>However</em>, the documentation in <code><a href="stats.html#topic+glm">stats::glm()</a></code> assumes
that is specific type of case weights are being used:“Non-NULL weights
can be used to indicate that different observations have different
dispersions (with the values in weights being inversely proportional to
the dispersions); or equivalently, when the elements of weights are
positive integers <code>w_i</code>, that each response <code>y_i</code> is the mean of <code>w_i</code>
unit-weight observations. For a binomial GLM prior weights are used to
give the number of trials when the response is the proportion of
successes: they would rarely be used for a Poisson GLM.”
</p>
<p>If frequency weights are being used in your application, the
<code><a href="#topic+glm_grouped">glm_grouped()</a></code> model (and corresponding engine) may be
more appropriate.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>


<hr>
<h2 id='details_poisson_reg_glmer'>Poisson regression via mixed models</h2><span id='topic+details_poisson_reg_glmer'></span>

<h3>Description</h3>

<p>The <code>"glmer"</code> engine estimates fixed and random effect regression parameters
using maximum likelihood (or restricted maximum likelihood) estimation.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

poisson_reg(engine = "glmer") %&gt;% 
  set_engine("glmer") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Poisson Regression Model Specification (regression)
## 
## Computational engine: glmer 
## 
## Model fit template:
## lme4::glmer(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     family = stats::poisson)
</pre></div>



<h4>Predicting new samples</h4>

<p>This model can use subject-specific coefficient estimates to make
predictions (i.e. partial pooling). For example, this equation shows the
linear predictor (<code style="white-space: pre;">&#8288;\eta&#8288;</code>) for a random intercept:
</p>
<div class="sourceCode"><pre>\eta_{i} = (\beta_0 + b_{0i}) + \beta_1x_{i1}
</pre></div>
<p>where <code>i</code> denotes the <code>i</code>th independent experimental unit
(e.g. subject). When the model has seen subject <code>i</code>, it can use that
subject’s data to adjust the <em>population</em> intercept to be more specific
to that subjects results.
</p>
<p>What happens when data are being predicted for a subject that was not
used in the model fit? In that case, this package uses <em>only</em> the
population parameter estimates for prediction:
</p>
<div class="sourceCode"><pre>\hat{\eta}_{i'} = \hat{\beta}_0+ \hat{\beta}x_{i'1}
</pre></div>
<p>Depending on what covariates are in the model, this might have the
effect of making the same prediction for all new samples. The population
parameters are the “best estimate” for a subject that was not included
in the model fit.
</p>
<p>The tidymodels framework deliberately constrains predictions for new
data to not use the training set or other data (to prevent information
leakage).
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model can accept case weights.
</p>
<p>With parsnip, we suggest using the formula method when fitting:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

poisson_reg() %&gt;% 
  set_engine("glmer") %&gt;% 
  fit(y ~ time + x + (1 | subject), data = longitudinal_counts)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the typical formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

glmer_spec &lt;- 
  poisson_reg() %&gt;% 
  set_engine("glmer")

glmer_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = y, predictors = c(time, x, subject)) %&gt;% 
  add_model(glmer_spec, formula = y ~ time + x + (1 | subject))

fit(glmer_wflow, data = longitudinal_counts)
</pre></div>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> J Pinheiro, and D Bates. 2000. <em>Mixed-effects models in S and S-PLUS</em>.
Springer, New York, NY
</p>
</li>
<li><p> West, K, Band Welch, and A Galecki. 2014. <em>Linear Mixed Models: A
Practical Guide Using Statistical Software</em>. CRC Press.
</p>
</li>
<li><p> Thorson, J, Minto, C. 2015, Mixed effects: a unifying framework for
statistical modelling in fisheries biology. <em>ICES Journal of Marine
Science</em>, Volume 72, Issue 5, Pages 1245–1256.
</p>
</li>
<li><p> Harrison, XA, Donaldson, L, Correa-Cano, ME, Evans, J, Fisher, DN,
Goodwin, CED, Robinson, BS, Hodgson, DJ, Inger, R. 2018. <em>A brief
introduction to mixed effects modelling and multi-model inference in
ecology</em>. PeerJ 6:e4794.
</p>
</li>
<li><p> DeBruine LM, Barr DJ. Understanding Mixed-Effects Models Through Data
Simulation. 2021. <em>Advances in Methods and Practices in Psychological
Science</em>.
</p>
</li></ul>



<hr>
<h2 id='details_poisson_reg_glmnet'>Poisson regression via glmnet</h2><span id='topic+details_poisson_reg_glmnet'></span>

<h3>Description</h3>

<p><code>glmnet::glmnet()</code> uses penalized maximum likelihood to fit a model for
count data.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: see below)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 1.0)
</p>
</li></ul>

<p>The <code>penalty</code> parameter has no default and requires a single numeric
value. For more details about this, and the <code>glmnet</code> model in general,
see <a href="#topic+glmnet-details">glmnet-details</a>. As for <code>mixture</code>:
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code> specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso
and ridge.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>poissonreg</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(poissonreg)

poisson_reg(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("glmnet") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Poisson Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0
##   mixture = double(1)
## 
## Computational engine: glmnet 
## 
## Model fit template:
## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     alpha = double(1), family = "poisson")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one. By default, <code>glmnet::glmnet()</code> uses the argument
<code>standardize = TRUE</code> to center and scale the data.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>


<hr>
<h2 id='details_poisson_reg_h2o'>Poisson regression via h2o</h2><span id='topic+details_poisson_reg_h2o'></span>

<h3>Description</h3>

<p><code>h2o::h2o.glm()</code> uses penalized maximum likelihood to fit a model for
count data.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: see
below)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: see below)
</p>
</li></ul>

<p>By default, when not given a fixed <code>penalty</code>,
<code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> uses a heuristic approach to select
the optimal value of <code>penalty</code> based on training data. Setting the
engine parameter <code>lambda_search</code> to <code>TRUE</code> enables an efficient version
of the grid search, see more details at
<a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/lambda_search.html">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/lambda_search.html</a>.
</p>
<p>The choice of <code>mixture</code> depends on the engine parameter <code>solver</code>, which
is automatically chosen given training data and the specification of
other model parameters. When <code>solver</code> is set to <code>'L-BFGS'</code>, <code>mixture</code>
defaults to 0 (ridge regression) and 0.5 otherwise.
</p>



<h4>Translation from parsnip to the original package</h4>

<p><code><a href="agua.html#topic+h2o_train">agua::h2o_train_glm()</a></code> for <code>poisson_reg()</code> is
a wrapper around <code><a href="h2o.html#topic+h2o.glm">h2o::h2o.glm()</a></code> with
<code>family = 'poisson'</code>.
</p>
<p>The <strong>agua</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(poissonreg)

poisson_reg(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("h2o") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Poisson Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = double(1)
##   mixture = double(1)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_glm(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), lambda = double(1), alpha = double(1), 
##     family = "poisson")
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>
<p>By default, <code>h2o::h2o.glm()</code> uses the argument <code>standardize = TRUE</code> to
center and scale all numerical columns.
</p>



<h4>Initializing h2o</h4>

<p>To use the h2o engine with tidymodels, please run <code>h2o::h2o.init()</code>
first. By default, This connects R to the local h2o server. This needs
to be done in every new R session. You can also connect to a remote h2o
server with an IP address, for more details see
<code><a href="h2o.html#topic+h2o.init">h2o::h2o.init()</a></code>.
</p>
<p>You can control the number of threads in the thread pool used by h2o
with the <code>nthreads</code> argument. By default, it uses all CPUs on the host.
This is different from the usual parallel processing mechanism in
tidymodels for tuning, while tidymodels parallelizes over resamples, h2o
parallelizes over hyperparameter combinations for a given resample.
</p>
<p>h2o will automatically shut down the local h2o instance started by R
when R is terminated. To manually stop the h2o server, run
<code>h2o::h2o.shutdown()</code>.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>


<hr>
<h2 id='details_poisson_reg_hurdle'>Poisson regression via pscl</h2><span id='topic+details_poisson_reg_hurdle'></span>

<h3>Description</h3>

<p><code><a href="pscl.html#topic+hurdle">pscl::hurdle()</a></code> uses maximum likelihood estimation to fit a model for
count data that has separate model terms for predicting the counts and for
predicting the probability of a zero count.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters.
</p>



<h4>Translation from parsnip to the underlying model call (regression)</h4>

<p>The <strong>poissonreg</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(poissonreg)

poisson_reg() %&gt;%
  set_engine("hurdle") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Poisson Regression Model Specification (regression)
## 
## Computational engine: hurdle 
## 
## Model fit template:
## pscl::hurdle(formula = missing_arg(), data = missing_arg(), weights = missing_arg())
</pre></div>



<h4>Preprocessing and special formulas for zero-inflated Poisson models</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Specifying the statistical model details</h4>

<p>For this particular model, a special formula is used to specify which
columns affect the counts and which affect the model for the probability
of zero counts. These sets of terms are separated by a bar. For example,
<code>y ~ x | z</code>. This type of formula is not used by the base R
infrastructure (e.g. <code>model.matrix()</code>)
</p>
<p>When fitting a parsnip model with this engine directly, the formula
method is required and the formula is just passed through. For example:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
tidymodels_prefer()

data("bioChemists", package = "pscl")
poisson_reg() %&gt;% 
  set_engine("hurdle") %&gt;% 
  fit(art ~ fem + mar | ment, data = bioChemists)
</pre></div>
<div class="sourceCode"><pre>## parsnip model object
## 
## 
## Call:
## pscl::hurdle(formula = art ~ fem + mar | ment, data = data)
## 
## Count model coefficients (truncated poisson with log link):
## (Intercept)     femWomen   marMarried  
##    0.847598    -0.237351     0.008846  
## 
## Zero hurdle model coefficients (binomial with logit link):
## (Intercept)         ment  
##     0.24871      0.08092
</pre></div>
<p>However, when using a workflow, the best approach is to avoid using
<code><a href="workflows.html#topic+add_formula">workflows::add_formula()</a></code> and use
<code><a href="workflows.html#topic+add_variables">workflows::add_variables()</a></code> in
conjunction with a model formula:
</p>
<div class="sourceCode r"><pre>data("bioChemists", package = "pscl")
spec &lt;- 
  poisson_reg() %&gt;% 
  set_engine("hurdle")

workflow() %&gt;% 
  add_variables(outcomes = c(art), predictors = c(fem, mar, ment)) %&gt;% 
  add_model(spec, formula = art ~ fem + mar | ment) %&gt;% 
  fit(data = bioChemists) %&gt;% 
  extract_fit_engine()
</pre></div>
<div class="sourceCode"><pre>## 
## Call:
## pscl::hurdle(formula = art ~ fem + mar | ment, data = data)
## 
## Count model coefficients (truncated poisson with log link):
## (Intercept)     femWomen   marMarried  
##    0.847598    -0.237351     0.008846  
## 
## Zero hurdle model coefficients (binomial with logit link):
## (Intercept)         ment  
##     0.24871      0.08092
</pre></div>
<p>The reason for this is that
<code><a href="workflows.html#topic+add_formula">workflows::add_formula()</a></code> will try to
create the model matrix and either fail or create dummy variables
prematurely.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>


<hr>
<h2 id='details_poisson_reg_stan'>Poisson regression via stan</h2><span id='topic+details_poisson_reg_stan'></span>

<h3>Description</h3>

<p><code><a href="rstanarm.html#topic+stan_glm">rstanarm::stan_glm()</a></code> uses Bayesian estimation to fit a model for
count data.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters.
</p>



<h4>Important engine-specific options</h4>

<p>Some relevant arguments that can be passed to <code>set_engine()</code>:
</p>

<ul>
<li> <p><code>chains</code>: A positive integer specifying the number of Markov chains.
The default is 4.
</p>
</li>
<li> <p><code>iter</code>: A positive integer specifying the number of iterations for
each chain (including warmup). The default is 2000.
</p>
</li>
<li> <p><code>seed</code>: The seed for random number generation.
</p>
</li>
<li> <p><code>cores</code>: Number of cores to use when executing the chains in parallel.
</p>
</li>
<li> <p><code>prior</code>: The prior distribution for the (non-hierarchical) regression
coefficients. The <code>"stan"</code> engine does not fit any hierarchical terms.
</p>
</li>
<li> <p><code>prior_intercept</code>: The prior distribution for the intercept (after
centering all predictors).
</p>
</li></ul>

<p>See <code><a href="rstan.html#topic+stanmodel-method-sampling">rstan::sampling()</a></code> and
<code><a href="rstanarm.html#topic+priors">rstanarm::priors()</a></code> for more information on these
and other options.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>poissonreg</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(poissonreg)

poisson_reg() %&gt;% 
  set_engine("stan") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Poisson Regression Model Specification (regression)
## 
## Computational engine: stan 
## 
## Model fit template:
## rstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), family = stats::poisson)
</pre></div>
<p>Note that the <code>refresh</code> default prevents logging of the estimation
process. Change this value in <code>set_engine()</code> to show the MCMC logs.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Other details</h4>

<p>For prediction, the <code>"stan"</code> engine can compute posterior intervals
analogous to confidence and prediction intervals. In these instances,
the units are the original outcome. When <code>std_error = TRUE</code>, the
standard deviation of the posterior distribution (or posterior
predictive distribution as appropriate) is returned.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#linear-reg-stan">examples</a>
for <code>poisson_reg()</code> with the <code>"stan"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> McElreath, R. 2020 <em>Statistical Rethinking</em>. CRC Press.
</p>
</li></ul>



<hr>
<h2 id='details_poisson_reg_stan_glmer'>Poisson regression via hierarchical Bayesian methods</h2><span id='topic+details_poisson_reg_stan_glmer'></span>

<h3>Description</h3>

<p>The <code>"stan_glmer"</code> engine estimates hierarchical regression parameters using
Bayesian estimation.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>



<h4>Important engine-specific options</h4>

<p>Some relevant arguments that can be passed to <code>set_engine()</code>:
</p>

<ul>
<li> <p><code>chains</code>: A positive integer specifying the number of Markov chains.
The default is 4.
</p>
</li>
<li> <p><code>iter</code>: A positive integer specifying the number of iterations for
each chain (including warmup). The default is 2000.
</p>
</li>
<li> <p><code>seed</code>: The seed for random number generation.
</p>
</li>
<li> <p><code>cores</code>: Number of cores to use when executing the chains in parallel.
</p>
</li>
<li> <p><code>prior</code>: The prior distribution for the (non-hierarchical) regression
coefficients.
</p>
</li>
<li> <p><code>prior_intercept</code>: The prior distribution for the intercept (after
centering all predictors).
</p>
</li></ul>

<p>See <code>?rstanarm::stan_glmer</code> and <code>?rstan::sampling</code> for more information.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>multilevelmod</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(multilevelmod)

poisson_reg(engine = "stan_glmer") %&gt;% 
  set_engine("stan_glmer") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Poisson Regression Model Specification (regression)
## 
## Computational engine: stan_glmer 
## 
## Model fit template:
## rstanarm::stan_glmer(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), family = stats::poisson, refresh = 0)
</pre></div>



<h4>Predicting new samples</h4>

<p>This model can use subject-specific coefficient estimates to make
predictions (i.e. partial pooling). For example, this equation shows the
linear predictor (<code style="white-space: pre;">&#8288;\eta&#8288;</code>) for a random intercept:
</p>
<div class="sourceCode"><pre>\eta_{i} = (\beta_0 + b_{0i}) + \beta_1x_{i1}
</pre></div>
<p>where <code>i</code> denotes the <code>i</code>th independent experimental unit
(e.g. subject). When the model has seen subject <code>i</code>, it can use that
subject’s data to adjust the <em>population</em> intercept to be more specific
to that subjects results.
</p>
<p>What happens when data are being predicted for a subject that was not
used in the model fit? In that case, this package uses <em>only</em> the
population parameter estimates for prediction:
</p>
<div class="sourceCode"><pre>\hat{\eta}_{i'} = \hat{\beta}_0+ \hat{\beta}x_{i'1}
</pre></div>
<p>Depending on what covariates are in the model, this might have the
effect of making the same prediction for all new samples. The population
parameters are the “best estimate” for a subject that was not included
in the model fit.
</p>
<p>The tidymodels framework deliberately constrains predictions for new
data to not use the training set or other data (to prevent information
leakage).
</p>



<h4>Preprocessing requirements</h4>

<p>There are no specific preprocessing needs. However, it is helpful to
keep the clustering/subject identifier column as factor or character
(instead of making them into dummy variables). See the examples in the
next section.
</p>



<h4>Other details</h4>

<p>The model can accept case weights.
</p>
<p>With parsnip, we suggest using the formula method when fitting:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

poisson_reg() %&gt;% 
  set_engine("stan_glmer") %&gt;% 
  fit(y ~ time + x + (1 | subject), data = longitudinal_counts)
</pre></div>
<p>When using tidymodels infrastructure, it may be better to use a
workflow. In this case, you can add the appropriate columns using
<code>add_variables()</code> then supply the typical formula when adding the model:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

glmer_spec &lt;- 
  poisson_reg() %&gt;% 
  set_engine("stan_glmer")

glmer_wflow &lt;- 
  workflow() %&gt;% 
  # The data are included as-is using:
  add_variables(outcomes = y, predictors = c(time, x, subject)) %&gt;% 
  add_model(glmer_spec, formula = y ~ time + x + (1 | subject))

fit(glmer_wflow, data = longitudinal_counts)
</pre></div>
<p>For prediction, the <code>"stan_glmer"</code> engine can compute posterior
intervals analogous to confidence and prediction intervals. In these
instances, the units are the original outcome. When <code>std_error = TRUE</code>,
the standard deviation of the posterior distribution (or posterior
predictive distribution as appropriate) is returned.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> McElreath, R. 2020 <em>Statistical Rethinking</em>. CRC Press.
</p>
</li>
<li><p> Sorensen, T, Vasishth, S. 2016. Bayesian linear mixed models using
Stan: A tutorial for psychologists, linguists, and cognitive
scientists, arXiv:1506.06201.
</p>
</li></ul>



<hr>
<h2 id='details_poisson_reg_zeroinfl'>Poisson regression via pscl</h2><span id='topic+details_poisson_reg_zeroinfl'></span>

<h3>Description</h3>

<p><code><a href="pscl.html#topic+zeroinfl">pscl::zeroinfl()</a></code> uses maximum likelihood estimation to fit a model for
count data that has separate model terms for predicting the counts and for
predicting the probability of a zero count.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: regression
</p>


<h4>Tuning Parameters</h4>

<p>This engine has no tuning parameters.
</p>



<h4>Translation from parsnip to the underlying model call (regression)</h4>

<p>The <strong>poissonreg</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(poissonreg)

poisson_reg() %&gt;%
  set_engine("zeroinfl") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Poisson Regression Model Specification (regression)
## 
## Computational engine: zeroinfl 
## 
## Model fit template:
## pscl::zeroinfl(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg())
</pre></div>



<h4>Preprocessing and special formulas for zero-inflated Poisson models</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Specifying the statistical model details</h4>

<p>For this particular model, a special formula is used to specify which
columns affect the counts and which affect the model for the probability
of zero counts. These sets of terms are separated by a bar. For example,
<code>y ~ x | z</code>. This type of formula is not used by the base R
infrastructure (e.g. <code>model.matrix()</code>)
</p>
<p>When fitting a parsnip model with this engine directly, the formula
method is required and the formula is just passed through. For example:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
tidymodels_prefer()

data("bioChemists", package = "pscl")
poisson_reg() %&gt;% 
  set_engine("zeroinfl") %&gt;% 
  fit(art ~ fem + mar | ment, data = bioChemists)
</pre></div>
<div class="sourceCode"><pre>## parsnip model object
## 
## 
## Call:
## pscl::zeroinfl(formula = art ~ fem + mar | ment, data = data)
## 
## Count model coefficients (poisson with log link):
## (Intercept)     femWomen   marMarried  
##     0.82840     -0.21365      0.02576  
## 
## Zero-inflation model coefficients (binomial with logit link):
## (Intercept)         ment  
##      -0.363       -0.166
</pre></div>
<p>However, when using a workflow, the best approach is to avoid using
<code><a href="workflows.html#topic+add_formula">workflows::add_formula()</a></code> and use
<code><a href="workflows.html#topic+add_variables">workflows::add_variables()</a></code> in
conjunction with a model formula:
</p>
<div class="sourceCode r"><pre>data("bioChemists", package = "pscl")
spec &lt;- 
  poisson_reg() %&gt;% 
  set_engine("zeroinfl")

workflow() %&gt;% 
  add_variables(outcomes = c(art), predictors = c(fem, mar, ment)) %&gt;% 
  add_model(spec, formula = art ~ fem + mar | ment) %&gt;% 
  fit(data = bioChemists) %&gt;% 
  extract_fit_engine()
</pre></div>
<div class="sourceCode"><pre>## 
## Call:
## pscl::zeroinfl(formula = art ~ fem + mar | ment, data = data)
## 
## Count model coefficients (poisson with log link):
## (Intercept)     femWomen   marMarried  
##     0.82840     -0.21365      0.02576  
## 
## Zero-inflation model coefficients (binomial with logit link):
## (Intercept)         ment  
##      -0.363       -0.166
</pre></div>
<p>The reason for this is that
<code><a href="workflows.html#topic+add_formula">workflows::add_formula()</a></code> will try to
create the model matrix and either fail or create dummy variables
prematurely.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>


<hr>
<h2 id='details_proportional_hazards_glmnet'>Proportional hazards regression</h2><span id='topic+details_proportional_hazards_glmnet'></span>

<h3>Description</h3>

<p><code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> fits a regularized Cox proportional hazards model.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: censored regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: see below)
</p>
</li>
<li> <p><code>mixture</code>: Proportion of Lasso Penalty (type: double, default: 1.0)
</p>
</li></ul>

<p>The <code>penalty</code> parameter has no default and requires a single numeric
value. For more details about this, and the <code>glmnet</code> model in general,
see <a href="#topic+glmnet-details">glmnet-details</a>. As for
<code>mixture</code>:
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code> specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso
and ridge.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

proportional_hazards(penalty = double(1), mixture = double(1)) %&gt;% 
  set_engine("glmnet") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Proportional Hazards Model Specification (censored regression)
## 
## Main Arguments:
##   penalty = 0
##   mixture = double(1)
## 
## Computational engine: glmnet 
## 
## Model fit template:
## censored::coxnet_train(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), alpha = double(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one. By default, <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> uses
the argument <code>standardize = TRUE</code> to center and scale the data.
</p>



<h4>Other details</h4>

<p>The model does not fit an intercept.
</p>
<p>The model formula (which is required) can include <em>special</em> terms, such
as <code><a href="survival.html#topic+strata">survival::strata()</a></code>. This allows the baseline
hazard to differ between groups contained in the function. (To learn
more about using special terms in formulas with tidymodels, see
<code><a href="#topic+model_formula">?model_formula</a></code>.) The column used inside
<code>strata()</code> is treated as qualitative no matter its type. This is
different than the syntax offered by the
<code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> package (i.e.,
<code><a href="glmnet.html#topic+stratifySurv">glmnet::stratifySurv()</a></code>) which is not
recommended here.
</p>
<p>For example, in this model, the numeric column <code>rx</code> is used to estimate
two different baseline hazards for each value of the column:
</p>
<div class="sourceCode r"><pre>library(survival)
library(censored)
library(dplyr)
library(tidyr)

mod &lt;- 
  proportional_hazards(penalty = 0.01) %&gt;% 
  set_engine("glmnet", nlambda = 5) %&gt;% 
  fit(Surv(futime, fustat) ~ age + ecog.ps + strata(rx), data = ovarian)

pred_data &lt;- data.frame(age = c(50, 50), ecog.ps = c(1, 1), rx = c(1, 2))

# Different survival probabilities for different values of 'rx'
predict(mod, pred_data, type = "survival", time = 500) %&gt;% 
  bind_cols(pred_data) %&gt;% 
  unnest(.pred)
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 2 x 5
##   .eval_time .pred_survival   age ecog.ps    rx
##        &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
## 1        500          0.666    50       1     1
## 2        500          0.769    50       1     2
</pre></div>
<p>Note that columns used in the <code>strata()</code> function <em>will</em> also be
estimated in the regular portion of the model (i.e., within the linear
predictor).
</p>
<p>Predictions of type <code>"time"</code> are predictions of the mean survival time.
</p>



<h4>Linear predictor values</h4>

<p>Since risk regression and parametric survival models are modeling
different characteristics (e.g. relative hazard versus event time),
their linear predictors will be going in opposite directions.
</p>
<p>For example, for parametric models, the linear predictor <em>increases with
time</em>. For proportional hazards models the linear predictor <em>decreases
with time</em> (since hazard is increasing). As such, the linear predictors
for these two quantities will have opposite signs.
</p>
<p>tidymodels does not treat different models differently when computing
performance metrics. To standardize across model types, the default for
proportional hazards models is to have <em>increasing values with time</em>. As
a result, the sign of the linear predictor will be the opposite of the
value produced by the <code>predict()</code> method in the engine package.
</p>
<p>This behavior can be changed by using the <code>increasing</code> argument when
calling <code>predict()</code> on a model object.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h3>References</h3>


<ul>
<li><p> Simon N, Friedman J, Hastie T, Tibshirani R. 2011. “Regularization
Paths for Cox’s Proportional Hazards Model via Coordinate Descent.”
<em>Journal of Statistical Software</em>, Articles 39 (5): 1–13. .
</p>
</li>
<li><p> Hastie T, Tibshirani R, Wainwright M. 2015. <em>Statistical Learning with
Sparsity</em>. CRC Press.
</p>
</li>
<li><p> Kuhn M, Johnson K. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>


<hr>
<h2 id='details_proportional_hazards_survival'>Proportional hazards regression</h2><span id='topic+details_proportional_hazards_survival'></span>

<h3>Description</h3>

<p><code><a href="survival.html#topic+coxph">survival::coxph()</a></code> fits a Cox proportional hazards model.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: censored regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has no tuning parameters.
</p>



<h4>Translation from parsnip to the original package</h4>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

proportional_hazards() %&gt;% 
  set_engine("survival") %&gt;% 
  set_mode("censored regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Proportional Hazards Model Specification (censored regression)
## 
## Computational engine: survival 
## 
## Model fit template:
## survival::coxph(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), x = TRUE, model = TRUE)
</pre></div>



<h4>Other details</h4>

<p>The model does not fit an intercept.
</p>
<p>The main interface for this model uses the formula method since the
model specification typically involved the use of
<code><a href="survival.html#topic+Surv">survival::Surv()</a></code>.
</p>
<p>The model formula can include <em>special</em> terms, such as
<code><a href="survival.html#topic+strata">survival::strata()</a></code>. The allows the baseline
hazard to differ between groups contained in the function. The column
used inside <code>strata()</code> is treated as qualitative no matter its type. To
learn more about using special terms in formulas with tidymodels, see
<code><a href="#topic+model_formula">?model_formula</a></code>.
</p>
<p>For example, in this model, the numeric column <code>rx</code> is used to estimate
two different baseline hazards for each value of the column:
</p>
<div class="sourceCode r"><pre>library(survival)

proportional_hazards() %&gt;% 
  fit(Surv(futime, fustat) ~ age + strata(rx), data = ovarian) %&gt;% 
  extract_fit_engine() %&gt;% 
  # Two different hazards for each value of 'rx'
  basehaz()
</pre></div>
<div class="sourceCode"><pre>##        hazard time strata
## 1  0.02250134   59   rx=1
## 2  0.05088586  115   rx=1
## 3  0.09467873  156   rx=1
## 4  0.14809975  268   rx=1
## 5  0.30670509  329   rx=1
## 6  0.46962698  431   rx=1
## 7  0.46962698  448   rx=1
## 8  0.46962698  477   rx=1
## 9  1.07680229  638   rx=1
## 10 1.07680229  803   rx=1
## 11 1.07680229  855   rx=1
## 12 1.07680229 1040   rx=1
## 13 1.07680229 1106   rx=1
## 14 0.05843331  353   rx=2
## 15 0.12750063  365   rx=2
## 16 0.12750063  377   rx=2
## 17 0.12750063  421   rx=2
## 18 0.23449656  464   rx=2
## 19 0.35593895  475   rx=2
## 20 0.50804209  563   rx=2
## 21 0.50804209  744   rx=2
## 22 0.50804209  769   rx=2
## 23 0.50804209  770   rx=2
## 24 0.50804209 1129   rx=2
## 25 0.50804209 1206   rx=2
## 26 0.50804209 1227   rx=2
</pre></div>
<p>Note that columns used in the <code>strata()</code> function will not be estimated
in the regular portion of the model (i.e., within the linear predictor).
</p>
<p>Predictions of type <code>"time"</code> are predictions of the mean survival time.
</p>



<h4>Linear predictor values</h4>

<p>Since risk regression and parametric survival models are modeling
different characteristics (e.g. relative hazard versus event time),
their linear predictors will be going in opposite directions.
</p>
<p>For example, for parametric models, the linear predictor <em>increases with
time</em>. For proportional hazards models the linear predictor <em>decreases
with time</em> (since hazard is increasing). As such, the linear predictors
for these two quantities will have opposite signs.
</p>
<p>tidymodels does not treat different models differently when computing
performance metrics. To standardize across model types, the default for
proportional hazards models is to have <em>increasing values with time</em>. As
a result, the sign of the linear predictor will be the opposite of the
value produced by the <code>predict()</code> method in the engine package.
</p>
<p>This behavior can be changed by using the <code>increasing</code> argument when
calling <code>predict()</code> on a model object.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Andersen P, Gill R. 1982. Cox’s regression model for counting
processes, a large sample study. <em>Annals of Statistics</em> 10, 1100-1120.
</p>
</li></ul>



<hr>
<h2 id='details_rand_forest_aorsf'>Oblique random survival forests via aorsf</h2><span id='topic+details_rand_forest_aorsf'></span>

<h3>Description</h3>

<p><code><a href="aorsf.html#topic+orsf">aorsf::orsf()</a></code> fits a model that creates a large number of oblique decision
trees, each de-correlated from the others. The final prediction uses all
predictions from the individual trees and combines them.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: censored regression,
classification, and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>trees</code>: # Trees (type: integer, default: 500L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 5L)
</p>
</li>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default:
ceiling(sqrt(n_predictors)))
</p>
</li></ul>

<p>Additionally, this model has one engine-specific tuning parameter:
</p>

<ul>
<li> <p><code>split_min_stat</code>: Minimum test statistic required to split a node.
Defaults are <code>3.841459</code> for censored regression (which is roughly a
p-value of 0.05) and <code>0</code> for classification and regression. For
classification, this tuning parameter should be between 0 and 1, and
for regression it should be greater than or equal to 0. Higher values
of this parameter cause trees grown by <code>aorsf</code> to have less depth.
</p>
</li></ul>




<h4>Translation from parsnip to the original package (censored regression)</h4>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

rand_forest() %&gt;%
  set_engine("aorsf") %&gt;%
  set_mode("censored regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (censored regression)
## 
## Computational engine: aorsf 
## 
## Model fit template:
## aorsf::orsf(formula = missing_arg(), data = missing_arg(), weights = missing_arg())
</pre></div>



<h4>Translation from parsnip to the original package (regression)</h4>

<p>The <strong>bonsai</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(bonsai)

rand_forest() %&gt;%
  set_engine("aorsf") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (regression)
## 
## Computational engine: aorsf 
## 
## Model fit template:
## aorsf::orsf(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     n_thread = 1, verbose_progress = FALSE)
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<p>The <strong>bonsai</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(bonsai)

rand_forest() %&gt;%
  set_engine("aorsf") %&gt;%
  set_mode("classification") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (classification)
## 
## Computational engine: aorsf 
## 
## Model fit template:
## aorsf::orsf(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), 
##     n_thread = 1, verbose_progress = FALSE)
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Other details</h4>

<p>Predictions of survival probability at a time exceeding the maximum
observed event time are the predicted survival probability at the
maximum observed time in the training data.
</p>
<p>The class predict method in <code>aorsf</code> uses the standard ‘each tree gets
one vote’ approach, which is usually but not always consistent with the
picking the class that has highest predicted probability. It is okay for
this inconsistency to occur in <code>aorsf</code> because it is intentionally
applying the traditional class prediction method for random forests, but
in <code>tidymodels</code> it is preferable to embrace consistency. Thus, we opted
to make predicted probability consistent with predicted class all the
time by making the predicted class a function of predicted probability
(see
<a href="https://github.com/tidymodels/bonsai/pull/78">tidymodels/bonsai#78</a>).
</p>



<h4>References</h4>


<ul>
<li><p> Jaeger BC, Long DL, Long DM, Sims M, Szychowski JM, Min YI, Mcclure
LA, Howard G, Simon N. Oblique random survival forests. Annals of
applied statistics 2019 Sep; 13(3):1847-83. DOI: 10.1214/19-AOAS1261
</p>
</li>
<li><p> Jaeger BC, Welden S, Lenoir K, Pajewski NM. aorsf: An R package for
supervised learning using the oblique random survival forest. Journal
of Open Source Software 2022, 7(77), 1 4705. .
</p>
</li>
<li><p> Jaeger BC, Welden S, Lenoir K, Speiser JL, Segar MW, Pandey A,
Pajewski NM. Accelerated and interpretable oblique random survival
forests. arXiv e-prints 2022 Aug; arXiv-2208. URL:
<a href="https://arxiv.org/abs/2208.01129">https://arxiv.org/abs/2208.01129</a>
</p>
</li></ul>



<hr>
<h2 id='details_rand_forest_h2o'>Random forests via h2o</h2><span id='topic+details_rand_forest_h2o'></span>

<h3>Description</h3>

<p><code><a href="h2o.html#topic+h2o.randomForest">h2o::h2o.randomForest()</a></code> fits a model that creates a large number of
decision trees, each independent of the others. The final prediction uses all
predictions from the individual trees and combines them.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>trees</code>: # Trees (type: integer, default: 50L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 1)
</p>
</li>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: see
below)
</p>
</li></ul>

<p><code>mtry</code> depends on the number of columns and the model mode. The default
in <code><a href="h2o.html#topic+h2o.randomForest">h2o::h2o.randomForest()</a></code> is
<code>floor(sqrt(ncol(x)))</code> for classification and <code>floor(ncol(x)/3)</code> for
regression.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<p><code><a href="agua.html#topic+h2o_train">agua::h2o_train_rf()</a></code> is a wrapper around
<code><a href="h2o.html#topic+h2o.randomForest">h2o::h2o.randomForest()</a></code>.
</p>
<div class="sourceCode r"><pre>rand_forest(
  mtry = integer(1),
  trees = integer(1),
  min_n = integer(1)
) %&gt;%  
  set_engine("h2o") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer(1)
##   trees = integer(1)
##   min_n = integer(1)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_rf(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), mtries = integer(1), ntrees = integer(1), 
##     min_rows = integer(1))
</pre></div>
<p><code>min_rows()</code> and <code>min_cols()</code> will adjust the number of neighbors if the
chosen value if it is not consistent with the actual data dimensions.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>rand_forest(
  mtry = integer(1),
  trees = integer(1),
  min_n = integer(1)
) %&gt;% 
  set_engine("h2o") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer(1)
##   trees = integer(1)
##   min_n = integer(1)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_rf(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), mtries = integer(1), ntrees = integer(1), 
##     min_rows = integer(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Initializing h2o</h4>

<p>To use the h2o engine with tidymodels, please run <code>h2o::h2o.init()</code>
first. By default, This connects R to the local h2o server. This needs
to be done in every new R session. You can also connect to a remote h2o
server with an IP address, for more details see
<code><a href="h2o.html#topic+h2o.init">h2o::h2o.init()</a></code>.
</p>
<p>You can control the number of threads in the thread pool used by h2o
with the <code>nthreads</code> argument. By default, it uses all CPUs on the host.
This is different from the usual parallel processing mechanism in
tidymodels for tuning, while tidymodels parallelizes over resamples, h2o
parallelizes over hyperparameter combinations for a given resample.
</p>
<p>h2o will automatically shut down the local h2o instance started by R
when R is terminated. To manually stop the h2o server, run
<code>h2o::h2o.shutdown()</code>.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>


<hr>
<h2 id='details_rand_forest_partykit'>Random forests via partykit</h2><span id='topic+details_rand_forest_partykit'></span>

<h3>Description</h3>

<p><code><a href="partykit.html#topic+cforest">partykit::cforest()</a></code> fits a model that creates a large number of decision
trees, each independent of the others. The final prediction uses all
predictions from the individual trees and combines them.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: censored regression,
regression, and classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>trees</code>: # Trees (type: integer, default: 500L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 20L)
</p>
</li>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: 5L)
</p>
</li></ul>




<h4>Translation from parsnip to the original package (regression)</h4>

<p>The <strong>bonsai</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(bonsai)

rand_forest() %&gt;% 
  set_engine("partykit") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (regression)
## 
## Computational engine: partykit 
## 
## Model fit template:
## parsnip::cforest_train(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg())
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<p>The <strong>bonsai</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(bonsai)

rand_forest() %&gt;% 
  set_engine("partykit") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (classification)
## 
## Computational engine: partykit 
## 
## Model fit template:
## parsnip::cforest_train(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg())
</pre></div>
<p><code>parsnip::cforest_train()</code> is a wrapper around
<code><a href="partykit.html#topic+cforest">partykit::cforest()</a></code> (and other functions) that
makes it easier to run this model.
</p>



<h3>Translation from parsnip to the original package (censored regression)</h3>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

rand_forest() %&gt;% 
  set_engine("partykit") %&gt;% 
  set_mode("censored regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (censored regression)
## 
## Computational engine: partykit 
## 
## Model fit template:
## parsnip::cforest_train(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg())
</pre></div>
<p><code>censored::cond_inference_surv_cforest()</code> is a wrapper around
<code><a href="partykit.html#topic+cforest">partykit::cforest()</a></code> (and other functions) that
makes it easier to run this model.
</p>


<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Other details</h4>

<p>Predictions of type <code>"time"</code> are predictions of the median survival
time.
</p>



<h4>References</h4>


<ul>
<li> <p><a href="https://jmlr.org/papers/v16/hothorn15a.html">partykit: A Modular Toolkit for Recursive Partytioning in R</a>
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_rand_forest_randomForest'>Random forests via randomForest</h2><span id='topic+details_rand_forest_randomForest'></span>

<h3>Description</h3>

<p><code><a href="randomForest.html#topic+randomForest">randomForest::randomForest()</a></code> fits a model that creates a large number of
decision trees, each independent of the others. The final prediction uses all
predictions from the individual trees and combines them.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: see
below)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 500L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: see below)
</p>
</li></ul>

<p><code>mtry</code> depends on the number of columns and the model mode. The default
in <code><a href="randomForest.html#topic+randomForest">randomForest::randomForest()</a></code> is
<code>floor(sqrt(ncol(x)))</code> for classification and <code>floor(ncol(x)/3)</code> for
regression.
</p>
<p><code>min_n</code> depends on the mode. For regression, a value of 5 is the
default. For classification, a value of 10 is used.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>rand_forest(
  mtry = integer(1),
  trees = integer(1),
  min_n = integer(1)
) %&gt;%  
  set_engine("randomForest") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer(1)
##   trees = integer(1)
##   min_n = integer(1)
## 
## Computational engine: randomForest 
## 
## Model fit template:
## randomForest::randomForest(x = missing_arg(), y = missing_arg(), 
##     mtry = min_cols(~integer(1), x), ntree = integer(1), nodesize = min_rows(~integer(1), 
##         x))
</pre></div>
<p><code>min_rows()</code> and <code>min_cols()</code> will adjust the number of neighbors if the
chosen value if it is not consistent with the actual data dimensions.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>rand_forest(
  mtry = integer(1),
  trees = integer(1),
  min_n = integer(1)
) %&gt;% 
  set_engine("randomForest") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer(1)
##   trees = integer(1)
##   min_n = integer(1)
## 
## Computational engine: randomForest 
## 
## Model fit template:
## randomForest::randomForest(x = missing_arg(), y = missing_arg(), 
##     mtry = min_cols(~integer(1), x), ntree = integer(1), nodesize = min_rows(~integer(1), 
##         x))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#rand-forest-randomForest">examples</a>
for <code>rand_forest()</code> with the <code>"randomForest"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_rand_forest_ranger'>Random forests via ranger</h2><span id='topic+details_rand_forest_ranger'></span>

<h3>Description</h3>

<p><code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code> fits a model that creates a large number of decision
trees, each independent of the others. The final prediction uses all
predictions from the individual trees and combines them.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: see
below)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 500L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: see below)
</p>
</li></ul>

<p><code>mtry</code> depends on the number of columns. The default in
<code><a href="ranger.html#topic+ranger">ranger::ranger()</a></code> is <code>floor(sqrt(ncol(x)))</code>.
</p>
<p><code>min_n</code> depends on the mode. For regression, a value of 5 is the
default. For classification, a value of 10 is used.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>rand_forest(
  mtry = integer(1),
  trees = integer(1),
  min_n = integer(1)
) %&gt;%  
  set_engine("ranger") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer(1)
##   trees = integer(1)
##   min_n = integer(1)
## 
## Computational engine: ranger 
## 
## Model fit template:
## ranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     mtry = min_cols(~integer(1), x), num.trees = integer(1), 
##     min.node.size = min_rows(~integer(1), x), num.threads = 1, 
##     verbose = FALSE, seed = sample.int(10^5, 1))
</pre></div>
<p><code>min_rows()</code> and <code>min_cols()</code> will adjust the number of neighbors if the
chosen value if it is not consistent with the actual data dimensions.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>rand_forest(
  mtry = integer(1),
  trees = integer(1),
  min_n = integer(1)
) %&gt;% 
  set_engine("ranger") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer(1)
##   trees = integer(1)
##   min_n = integer(1)
## 
## Computational engine: ranger 
## 
## Model fit template:
## ranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     mtry = min_cols(~integer(1), x), num.trees = integer(1), 
##     min.node.size = min_rows(~integer(1), x), num.threads = 1, 
##     verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE)
</pre></div>
<p>Note that a <code>ranger</code> probability forest is always fit (unless the
<code>probability</code> argument is changed by the user via
<code><a href="#topic+set_engine">set_engine()</a></code>).
</p>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Other notes</h4>

<p>By default, parallel processing is turned off. When tuning, it is more
efficient to parallelize over the resamples and tuning parameters. To
parallelize the construction of the trees within the <code>ranger</code> model,
change the <code>num.threads</code> argument via <code><a href="#topic+set_engine">set_engine()</a></code>.
</p>
<p>For <code>ranger</code> confidence intervals, the intervals are constructed using
the form <code style="white-space: pre;">&#8288;estimate +/- z * std_error&#8288;</code>. For classification probabilities,
these values can fall outside of <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> and will be coerced to be in
this range.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Sparse Data</h4>

<p>This model can utilize sparse data during model fitting and prediction.
Both sparse matrices such as dgCMatrix from the <code>Matrix</code> package and
sparse tibbles from the <code>sparsevctrs</code> package are supported. See
<a href="#topic+sparse_data">sparse_data</a> for more information.
</p>
<p>While this engine supports sparse data as an input, it doesn’t use it
any differently than dense data. Hence there it no reason to convert
back and forth.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#rand-forest-ranger">examples</a>
for <code>rand_forest()</code> with the <code>"ranger"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_rand_forest_spark'>Random forests via spark</h2><span id='topic+details_rand_forest_spark'></span>

<h3>Description</h3>

<p><code><a href="sparklyr.html#topic+ml_random_forest">sparklyr::ml_random_forest()</a></code> fits a model that creates a large number of
decision trees, each independent of the others. The final prediction uses all
predictions from the individual trees and combines them.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: see
below)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 20L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 1L)
</p>
</li></ul>

<p><code>mtry</code> depends on the number of columns and the model mode. The default
in <code><a href="sparklyr.html#topic+ml_random_forest">sparklyr::ml_random_forest()</a></code> is
<code>floor(sqrt(ncol(x)))</code> for classification and <code>floor(ncol(x)/3)</code> for
regression.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>rand_forest(
  mtry = integer(1),
  trees = integer(1),
  min_n = integer(1)
) %&gt;%  
  set_engine("spark") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer(1)
##   trees = integer(1)
##   min_n = integer(1)
## 
## Computational engine: spark 
## 
## Model fit template:
## sparklyr::ml_random_forest(x = missing_arg(), formula = missing_arg(), 
##     type = "regression", feature_subset_strategy = integer(1), 
##     num_trees = integer(1), min_instances_per_node = min_rows(~integer(1), 
##         x), seed = sample.int(10^5, 1))
</pre></div>
<p><code>min_rows()</code> and <code>min_cols()</code> will adjust the number of neighbors if the
chosen value if it is not consistent with the actual data dimensions.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>rand_forest(
  mtry = integer(1),
  trees = integer(1),
  min_n = integer(1)
) %&gt;% 
  set_engine("spark") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer(1)
##   trees = integer(1)
##   min_n = integer(1)
## 
## Computational engine: spark 
## 
## Model fit template:
## sparklyr::ml_random_forest(x = missing_arg(), formula = missing_arg(), 
##     type = "classification", feature_subset_strategy = integer(1), 
##     num_trees = integer(1), min_instances_per_node = min_rows(~integer(1), 
##         x), seed = sample.int(10^5, 1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">&#8288;{a, c}&#8288;</code> vs <code style="white-space: pre;">&#8288;{b, d}&#8288;</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>



<h4>Other details</h4>

<p>For models created using the <code>"spark"</code> engine, there are several things
to consider.
</p>

<ul>
<li><p> Only the formula interface to via <code>fit()</code> is available; using
<code>fit_xy()</code> will generate an error.
</p>
</li>
<li><p> The predictions will always be in a Spark table format. The names will
be the same as documented but without the dots.
</p>
</li>
<li><p> There is no equivalent to factor columns in Spark tables so class
predictions are returned as character columns.
</p>
</li>
<li><p> To retain the model object for a new R session (via <code>save()</code>), the
<code>model$fit</code> element of the parsnip object should be serialized via
<code>ml_save(object$fit)</code> and separately saved to disk. In a new session,
the object can be reloaded and reattached to the parsnip object.
</p>
</li></ul>




<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>
<p>Note that, for spark engines, the <code>case_weight</code> argument value should be
a character string to specify the column with the numeric case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_rule_fit_h2o'>RuleFit models via h2o</h2><span id='topic+details_rule_fit_h2o'></span>

<h3>Description</h3>

<p><code><a href="h2o.html#topic+h2o.rulefit">h2o::h2o.rulefit()</a></code> fits a model that derives simple feature rules from a tree
ensemble and uses the rules as features to a regularized (LASSO) model. <code><a href="agua.html#topic+h2o_train">agua::h2o_train_rule()</a></code>
is a wrapper around this function.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>trees</code>: # Trees (type: integer, default: 50L)
</p>
</li>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 3L)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0) Note
that <code>penalty</code> for the h2o engine in 'rule_fit()&ldquo; corresponds to
the L1 penalty (LASSO).
</p>
</li></ul>

<p>Other engine arguments of interest:
</p>

<ul>
<li> <p><code>algorithm</code>: The algorithm to use to generate rules. should be one of
“AUTO”, “DRF”, “GBM”, defaults to “AUTO”.
</p>
</li>
<li> <p><code>min_rule_length</code>: Minimum length of tree depth, opposite of
<code>tree_dpeth</code>, defaults to 3.
</p>
</li>
<li> <p><code>max_num_rules</code>: The maximum number of rules to return. The default
value of -1 means the number of rules is selected by diminishing
returns in model deviance.
</p>
</li>
<li> <p><code>model_type</code>: The type of base learners in the ensemble, should be one
of: “rules_and_linear”, “rules”, “linear”, defaults to
“rules_and_linear”.
</p>
</li></ul>




<h4>Translation from parsnip to the underlying model call (regression)</h4>

<p><code><a href="agua.html#topic+h2o_train">agua::h2o_train_rule()</a></code> is a wrapper around
<code><a href="h2o.html#topic+h2o.rulefit">h2o::h2o.rulefit()</a></code>.
</p>
<p>The <strong>agua</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(rules)

rule_fit(
  trees = integer(1),
  tree_depth = integer(1),
  penalty = numeric(1)
) %&gt;%
  set_engine("h2o") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## RuleFit Model Specification (regression)
## 
## Main Arguments:
##   trees = integer(1)
##   tree_depth = integer(1)
##   penalty = numeric(1)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_rule(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), rule_generation_ntrees = integer(1), 
##     max_rule_length = integer(1), lambda = numeric(1))
</pre></div>



<h4>Translation from parsnip to the underlying model call (classification)</h4>

<p><code><a href="agua.html#topic+h2o_train">agua::h2o_train_rule()</a></code> for <code>rule_fit()</code> is a
wrapper around <code><a href="h2o.html#topic+h2o.rulefit">h2o::h2o.rulefit()</a></code>.
</p>
<p>The <strong>agua</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>rule_fit(
  trees = integer(1),
  tree_depth = integer(1),
  penalty = numeric(1)
) %&gt;%
  set_engine("h2o") %&gt;%
  set_mode("classification") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## RuleFit Model Specification (classification)
## 
## Main Arguments:
##   trees = integer(1)
##   tree_depth = integer(1)
##   penalty = numeric(1)
## 
## Computational engine: h2o 
## 
## Model fit template:
## agua::h2o_train_rule(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     validation_frame = missing_arg(), rule_generation_ntrees = integer(1), 
##     max_rule_length = integer(1), lambda = numeric(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Other details</h4>

<p>To use the h2o engine with tidymodels, please run <code>h2o::h2o.init()</code>
first. By default, This connects R to the local h2o server. This needs
to be done in every new R session. You can also connect to a remote h2o
server with an IP address, for more details see
<code><a href="h2o.html#topic+h2o.init">h2o::h2o.init()</a></code>.
</p>
<p>You can control the number of threads in the thread pool used by h2o
with the <code>nthreads</code> argument. By default, it uses all CPUs on the host.
This is different from the usual parallel processing mechanism in
tidymodels for tuning, while tidymodels parallelizes over resamples, h2o
parallelizes over hyperparameter combinations for a given resample.
</p>
<p>h2o will automatically shut down the local h2o instance started by R
when R is terminated. To manually stop the h2o server, run
<code>h2o::h2o.shutdown()</code>.
</p>



<h4>Saving fitted model objects</h4>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>


<hr>
<h2 id='details_rule_fit_xrf'>RuleFit models via xrf</h2><span id='topic+details_rule_fit_xrf'></span>

<h3>Description</h3>

<p><code><a href="xrf.html#topic+xrf">xrf::xrf()</a></code> fits a model that derives simple feature rules from a tree
ensemble and uses the rules as features to a regularized model. <code><a href="rules.html#topic+rules-internal">rules::xrf_fit()</a></code>
is a wrapper around this function.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 8 tuning parameters:
</p>

<ul>
<li> <p><code>mtry</code>: Proportion Randomly Selected Predictors (type: double,
default: see below)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 15L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 1L)
</p>
</li>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 6L)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.3)
</p>
</li>
<li> <p><code>loss_reduction</code>: Minimum Loss Reduction (type: double, default: 0.0)
</p>
</li>
<li> <p><code>sample_size</code>: Proportion Observations Sampled (type: double, default:
1.0)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.1)
</p>
</li></ul>




<h4>Translation from parsnip to the underlying model call (regression)</h4>

<p>The <strong>rules</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(rules)

rule_fit(
  mtry = numeric(1),
  trees = integer(1),
  min_n = integer(1),
  tree_depth = integer(1),
  learn_rate = numeric(1),
  loss_reduction = numeric(1),
  sample_size = numeric(1),
  penalty = numeric(1)
) %&gt;%
  set_engine("xrf") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## RuleFit Model Specification (regression)
## 
## Main Arguments:
##   mtry = numeric(1)
##   trees = integer(1)
##   min_n = integer(1)
##   tree_depth = integer(1)
##   learn_rate = numeric(1)
##   loss_reduction = numeric(1)
##   sample_size = numeric(1)
##   penalty = numeric(1)
## 
## Computational engine: xrf 
## 
## Model fit template:
## rules::xrf_fit(formula = missing_arg(), data = missing_arg(), 
##     xgb_control = missing_arg(), colsample_bynode = numeric(1), 
##     nrounds = integer(1), min_child_weight = integer(1), max_depth = integer(1), 
##     eta = numeric(1), gamma = numeric(1), subsample = numeric(1), 
##     lambda = numeric(1))
</pre></div>



<h4>Translation from parsnip to the underlying model call (classification)</h4>

<p>The <strong>rules</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(rules)

rule_fit(
  mtry = numeric(1),
  trees = integer(1),
  min_n = integer(1),
  tree_depth = integer(1),
  learn_rate = numeric(1),
  loss_reduction = numeric(1),
  sample_size = numeric(1),
  penalty = numeric(1)
) %&gt;%
  set_engine("xrf") %&gt;%
  set_mode("classification") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## RuleFit Model Specification (classification)
## 
## Main Arguments:
##   mtry = numeric(1)
##   trees = integer(1)
##   min_n = integer(1)
##   tree_depth = integer(1)
##   learn_rate = numeric(1)
##   loss_reduction = numeric(1)
##   sample_size = numeric(1)
##   penalty = numeric(1)
## 
## Computational engine: xrf 
## 
## Model fit template:
## rules::xrf_fit(formula = missing_arg(), data = missing_arg(), 
##     xgb_control = missing_arg(), colsample_bynode = numeric(1), 
##     nrounds = integer(1), min_child_weight = integer(1), max_depth = integer(1), 
##     eta = numeric(1), gamma = numeric(1), subsample = numeric(1), 
##     lambda = numeric(1))
</pre></div>



<h4>Differences from the xrf package</h4>

<p>Note that, per the documentation in <code>?xrf</code>, transformations of the
response variable are not supported. To use these with <code>rule_fit()</code>, we
recommend using a recipe instead of the formula method.
</p>
<p>Also, there are several configuration differences in how <code>xrf()</code> is fit
between that package and the wrapper used in <strong>rules</strong>. Some differences
in default values are:</p>

<table>
<tr>
 <td style="text-align: left;">
   parameter </td><td style="text-align: left;"> <strong>xrf</strong> </td><td style="text-align: left;"> <strong>rules</strong> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>trees</code> </td><td style="text-align: left;"> 100 </td><td style="text-align: left;"> 15 </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>max_depth</code> </td><td style="text-align: left;"> 3 </td><td style="text-align: left;"> 6 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>These differences will create a disparity in the values of the <code>penalty</code>
argument that <strong>glmnet</strong> uses. Also, <strong>rules</strong> can also set <code>penalty</code>
whereas <strong>xrf</strong> uses an internal 5-fold cross-validation to determine it
(by default).
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Other details</h4>



<h5>Interpreting <code>mtry</code></h5>

<p>The <code>mtry</code> argument denotes the number of predictors that will be
randomly sampled at each split when creating tree models.
</p>
<p>Some engines, such as <code>"xgboost"</code>, <code>"xrf"</code>, and <code>"lightgbm"</code>, interpret
their analogue to the <code>mtry</code> argument as the <em>proportion</em> of predictors
that will be randomly sampled at each split rather than the <em>count</em>. In
some settings, such as when tuning over preprocessors that influence the
number of predictors, this parameterization is quite
helpful—interpreting <code>mtry</code> as a proportion means that <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> is
always a valid range for that parameter, regardless of input data.
</p>
<p>parsnip and its extensions accommodate this parameterization using the
<code>counts</code> argument: a logical indicating whether <code>mtry</code> should be
interpreted as the number of predictors that will be randomly sampled at
each split. <code>TRUE</code> indicates that <code>mtry</code> will be interpreted in its
sense as a count, <code>FALSE</code> indicates that the argument will be
interpreted in its sense as a proportion.
</p>
<p><code>mtry</code> is a main model argument for
<code><a href="#topic+boost_tree">boost_tree()</a></code> and
<code><a href="#topic+rand_forest">rand_forest()</a></code>, and thus should not have an
engine-specific interface. So, regardless of engine, <code>counts</code> defaults
to <code>TRUE</code>. For engines that support the proportion interpretation
(currently <code>"xgboost"</code> and <code>"xrf"</code>, via the rules package, and
<code>"lightgbm"</code> via the bonsai package) the user can pass the
<code>counts = FALSE</code> argument to <code>set_engine()</code> to supply <code>mtry</code> values
within <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>.
</p>



<h5>Early stopping</h5>

<p>The <code>stop_iter()</code> argument allows the model to prematurely stop training
if the objective function does not improve within <code>early_stop</code>
iterations.
</p>
<p>The best way to use this feature is in conjunction with an <em>internal
validation set</em>. To do this, pass the <code>validation</code> parameter of
<code><a href="#topic+xgb_train">xgb_train()</a></code> via the parsnip
<code><a href="#topic+set_engine">set_engine()</a></code> function. This is the
proportion of the training set that should be reserved for measuring
performance (and stopping early).
</p>
<p>If the model specification has <code>early_stop &gt;= trees</code>, <code>early_stop</code> is
converted to <code>trees - 1</code> and a warning is issued.
</p>




<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul>
<li><p> Friedman and Popescu. “Predictive learning via rule ensembles.” Ann.
Appl. Stat. 2 (3) 916- 954, September 2008
</p>
</li></ul>



<hr>
<h2 id='details_survival_reg_flexsurv'>Parametric survival regression</h2><span id='topic+details_survival_reg_flexsurv'></span>

<h3>Description</h3>

<p><code><a href="flexsurv.html#topic+flexsurvreg">flexsurv::flexsurvreg()</a></code> fits a parametric survival model.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: censored regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 1 tuning parameters:
</p>

<ul>
<li> <p><code>dist</code>: Distribution (type: character, default: ‘weibull’)
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

survival_reg(dist = character(1)) %&gt;% 
  set_engine("flexsurv") %&gt;% 
  set_mode("censored regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Parametric Survival Regression Model Specification (censored regression)
## 
## Main Arguments:
##   dist = character(1)
## 
## Computational engine: flexsurv 
## 
## Model fit template:
## flexsurv::flexsurvreg(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), dist = character(1))
</pre></div>



<h4>Other details</h4>

<p>The main interface for this model uses the formula method since the
model specification typically involved the use of
<code><a href="survival.html#topic+Surv">survival::Surv()</a></code>.
</p>
<p>For this engine, stratification cannot be specified via
<code><a href="survival.html#topic+strata">survival::strata()</a></code>, please see
<code><a href="flexsurv.html#topic+flexsurvreg">flexsurv::flexsurvreg()</a></code> for alternative
specifications.
</p>
<p>Predictions of type <code>"time"</code> are predictions of the mean survival time.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>References</h4>


<ul>
<li><p> Jackson, C. 2016. <code>flexsurv</code>: A Platform for Parametric Survival
Modeling in R. <em>Journal of Statistical Software</em>, 70(8), 1 - 33.
</p>
</li></ul>



<hr>
<h2 id='details_survival_reg_flexsurvspline'>Flexible parametric survival regression</h2><span id='topic+details_survival_reg_flexsurvspline'></span>

<h3>Description</h3>

<p><code><a href="flexsurv.html#topic+flexsurvspline">flexsurv::flexsurvspline()</a></code> fits a flexible parametric survival model.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: censored regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has one engine-specific tuning parameter:
</p>

<ul>
<li> <p><code>k</code>: Number of knots in the spline. The default is <code>k = 0</code>.
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

survival_reg() %&gt;% 
  set_engine("flexsurvspline") %&gt;% 
  set_mode("censored regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Parametric Survival Regression Model Specification (censored regression)
## 
## Computational engine: flexsurvspline 
## 
## Model fit template:
## flexsurv::flexsurvspline(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg())
</pre></div>



<h4>Other details</h4>

<p>The main interface for this model uses the formula method since the
model specification typically involved the use of
<code><a href="survival.html#topic+Surv">survival::Surv()</a></code>.
</p>
<p>For this engine, stratification cannot be specified via
<code><a href="survival.html#topic+strata">survival::strata()</a></code>, please see
<code><a href="flexsurv.html#topic+flexsurvspline">flexsurv::flexsurvspline()</a></code> for
alternative specifications.
</p>
<p>Predictions of type <code>"time"</code> are predictions of the mean survival time.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>References</h4>


<ul>
<li><p> Jackson, C. 2016. <code>flexsurv</code>: A Platform for Parametric Survival
Modeling in R. <em>Journal of Statistical Software</em>, 70(8), 1 - 33.
</p>
</li></ul>



<hr>
<h2 id='details_survival_reg_survival'>Parametric survival regression</h2><span id='topic+details_survival_reg_survival'></span>

<h3>Description</h3>

<p><code><a href="survival.html#topic+survreg">survival::survreg()</a></code> fits a parametric survival model.
</p>


<h3>Details</h3>

<p>For this engine, there is a single mode: censored regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 1 tuning parameters:
</p>

<ul>
<li> <p><code>dist</code>: Distribution (type: character, default: ‘weibull’)
</p>
</li></ul>




<h4>Translation from parsnip to the original package</h4>

<p>The <strong>censored</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(censored)

survival_reg(dist = character(1)) %&gt;% 
  set_engine("survival") %&gt;% 
  set_mode("censored regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Parametric Survival Regression Model Specification (censored regression)
## 
## Main Arguments:
##   dist = character(1)
## 
## Computational engine: survival 
## 
## Model fit template:
## survival::survreg(formula = missing_arg(), data = missing_arg(), 
##     weights = missing_arg(), dist = character(1), model = TRUE)
</pre></div>



<h4>Other details</h4>

<p>In the translated syntax above, note that <code>model = TRUE</code> is needed to
produce quantile predictions when there is a stratification variable and
can be overridden in other cases.
</p>
<p>The main interface for this model uses the formula method since the
model specification typically involved the use of
<code><a href="survival.html#topic+Surv">survival::Surv()</a></code>.
</p>
<p>The model formula can include <em>special</em> terms, such as
<code><a href="survival.html#topic+strata">survival::strata()</a></code>. The allows the model scale
parameter to differ between groups contained in the function. The column
used inside <code>strata()</code> is treated as qualitative no matter its type. To
learn more about using special terms in formulas with tidymodels, see
<code><a href="#topic+model_formula">?model_formula</a></code>.
</p>
<p>For example, in this model, the numeric column <code>rx</code> is used to estimate
two different scale parameters for each value of the column:
</p>
<div class="sourceCode r"><pre>library(survival)

survival_reg() %&gt;% 
  fit(Surv(futime, fustat) ~ age + strata(rx), data = ovarian) %&gt;% 
  extract_fit_engine()
</pre></div>
<div class="sourceCode"><pre>## Call:
## survival::survreg(formula = Surv(futime, fustat) ~ age + strata(rx), 
##     data = data, model = TRUE)
## 
## Coefficients:
## (Intercept)         age 
##  12.8734120  -0.1033569 
## 
## Scale:
##      rx=1      rx=2 
## 0.7695509 0.4703602 
## 
## Loglik(model)= -89.4   Loglik(intercept only)= -97.1
##  Chisq= 15.36 on 1 degrees of freedom, p= 8.88e-05 
## n= 26
</pre></div>
<p>Predictions of type <code>"time"</code> are predictions of the mean survival time.
</p>



<h4>Case weights</h4>

<p>This model can utilize case weights during model fitting. To use them,
see the documentation in <a href="#topic+case_weights">case_weights</a> and the examples
on <code>tidymodels.org</code>.
</p>
<p>The <code>fit()</code> and <code>fit_xy()</code> arguments have arguments called
<code>case_weights</code> that expect vectors of case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>References</h4>


<ul>
<li><p> Kalbfleisch, J. D. and Prentice, R. L. 2002 <em>The statistical analysis
of failure time data</em>, Wiley.
</p>
</li></ul>



<hr>
<h2 id='details_svm_linear_kernlab'>Linear support vector machines (SVMs) via kernlab</h2><span id='topic+details_svm_linear_kernlab'></span>

<h3>Description</h3>

<p><code><a href="kernlab.html#topic+ksvm">kernlab::ksvm()</a></code> fits a support vector machine model. For classification,
the model tries to maximize the width of the margin between classes.
For regression, the model optimizes a robust loss function that is only
affected by very large model residuals.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>cost</code>: Cost (type: double, default: 1.0)
</p>
</li>
<li> <p><code>margin</code>: Insensitivity Margin (type: double, default: 0.1)
</p>
</li></ul>

<p>Parsnip changes the default range for <code>cost</code> to <code>c(-10, 5)</code>.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>svm_linear(
  cost = double(1),
  margin = double(1)
) %&gt;%  
  set_engine("kernlab") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Support Vector Machine Model Specification (regression)
## 
## Main Arguments:
##   cost = double(1)
##   margin = double(1)
## 
## Computational engine: kernlab 
## 
## Model fit template:
## kernlab::ksvm(x = missing_arg(), data = missing_arg(), C = double(1), 
##     epsilon = double(1), kernel = "vanilladot")
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>svm_linear(
  cost = double(1)
) %&gt;% 
  set_engine("kernlab") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Support Vector Machine Model Specification (classification)
## 
## Main Arguments:
##   cost = double(1)
## 
## Computational engine: kernlab 
## 
## Model fit template:
## kernlab::ksvm(x = missing_arg(), data = missing_arg(), C = double(1), 
##     kernel = "vanilladot", prob.model = TRUE)
</pre></div>
<p>The <code>margin</code> parameter does not apply to classification models.
</p>
<p>Note that the <code>"kernlab"</code> engine does not naturally estimate class
probabilities. To produce them, the decision values of the model are
converted to probabilities using Platt scaling. This method fits an
additional model on top of the SVM model. When fitting the Platt scaling
model, random numbers are used that are not reproducible or controlled
by R’s random number stream.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#svm-linear-kernlab">examples</a>
for <code>svm_linear()</code> with the <code>"kernlab"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Lin, HT, and R Weng. <a href="https://www.csie.ntu.edu.tw/~cjlin/papers/plattprob.pdf">“A Note on Platt’s Probabilistic Outputs for Support Vector Machines”</a>
</p>
</li>
<li><p> Karatzoglou, A, Smola, A, Hornik, K, and A Zeileis. 2004. <a href="https://www.jstatsoft.org/article/view/v011i09">“kernlab - An S4 Package for Kernel Methods in R.”</a>, <em>Journal of
Statistical Software</em>.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_svm_linear_LiblineaR'>Linear support vector machines (SVMs) via LiblineaR</h2><span id='topic+details_svm_linear_LiblineaR'></span>

<h3>Description</h3>

<p><code><a href="LiblineaR.html#topic+LiblineaR">LiblineaR::LiblineaR()</a></code> fits a support vector machine model. For classification,
the model tries to maximize the width of the margin between classes.
For regression, the model optimizes a robust loss function that is only
affected by very large model residuals.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 2 tuning parameters:
</p>

<ul>
<li> <p><code>cost</code>: Cost (type: double, default: 1.0)
</p>
</li>
<li> <p><code>margin</code>: Insensitivity Margin (type: double, default: no default)
</p>
</li></ul>

<p>This engine fits models that are L2-regularized for L2-loss. In the
<code><a href="LiblineaR.html#topic+LiblineaR">LiblineaR::LiblineaR()</a></code> documentation, these
are types 1 (classification) and 11 (regression).
</p>
<p>Parsnip changes the default range for <code>cost</code> to <code>c(-10, 5)</code>.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>svm_linear(
  cost = double(1),
  margin = double(1)
) %&gt;%  
  set_engine("LiblineaR") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Support Vector Machine Model Specification (regression)
## 
## Main Arguments:
##   cost = double(1)
##   margin = double(1)
## 
## Computational engine: LiblineaR 
## 
## Model fit template:
## LiblineaR::LiblineaR(x = missing_arg(), y = missing_arg(), C = double(1), 
##     svr_eps = double(1), type = 11)
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>svm_linear(
  cost = double(1)
) %&gt;% 
  set_engine("LiblineaR") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Support Vector Machine Model Specification (classification)
## 
## Main Arguments:
##   cost = double(1)
## 
## Computational engine: LiblineaR 
## 
## Model fit template:
## LiblineaR::LiblineaR(x = missing_arg(), y = missing_arg(), C = double(1), 
##     type = 1)
</pre></div>
<p>The <code>margin</code> parameter does not apply to classification models.
</p>
<p>Note that the <code>LiblineaR</code> engine does not produce class probabilities.
When optimizing the model using the tune package, the default metrics
require class probabilities. To use the <code style="white-space: pre;">&#8288;tune_*()&#8288;</code> functions, a metric
set must be passed as an argument that only contains metrics for hard
class predictions (e.g., accuracy).
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Sparse Data</h4>

<p>This model can utilize sparse data during model fitting and prediction.
Both sparse matrices such as dgCMatrix from the <code>Matrix</code> package and
sparse tibbles from the <code>sparsevctrs</code> package are supported. See
<a href="#topic+sparse_data">sparse_data</a> for more information.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#svm-linear-LiblineaR">examples</a>
for <code>svm_linear()</code> with the <code>"LiblineaR"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_svm_poly_kernlab'>Polynomial support vector machines (SVMs) via kernlab</h2><span id='topic+details_svm_poly_kernlab'></span>

<h3>Description</h3>

<p><code><a href="kernlab.html#topic+ksvm">kernlab::ksvm()</a></code> fits a support vector machine model. For classification,
the model tries to maximize the width of the margin between classes.
For regression, the model optimizes a robust loss function that is only
affected by very large model residuals.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 4 tuning parameters:
</p>

<ul>
<li> <p><code>cost</code>: Cost (type: double, default: 1.0)
</p>
</li>
<li> <p><code>degree</code>: Degree of Interaction (type: integer, default: 1L1)
</p>
</li>
<li> <p><code>scale_factor</code>: Scale Factor (type: double, default: 1.0)
</p>
</li>
<li> <p><code>margin</code>: Insensitivity Margin (type: double, default: 0.1)
</p>
</li></ul>

<p>Parsnip changes the default range for <code>cost</code> to <code>c(-10, 5)</code>.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>svm_poly(
  cost = double(1),
  degree = integer(1),
  scale_factor = double(1), 
  margin = double(1)
) %&gt;%  
  set_engine("kernlab") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Polynomial Support Vector Machine Model Specification (regression)
## 
## Main Arguments:
##   cost = double(1)
##   degree = integer(1)
##   scale_factor = double(1)
##   margin = double(1)
## 
## Computational engine: kernlab 
## 
## Model fit template:
## kernlab::ksvm(x = missing_arg(), data = missing_arg(), C = double(1), 
##     epsilon = double(1), kernel = "polydot", kpar = list(degree = ~integer(1), 
##         scale = ~double(1)))
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>svm_poly(
  cost = double(1),
  degree = integer(1),
  scale_factor = double(1)
) %&gt;% 
  set_engine("kernlab") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Polynomial Support Vector Machine Model Specification (classification)
## 
## Main Arguments:
##   cost = double(1)
##   degree = integer(1)
##   scale_factor = double(1)
## 
## Computational engine: kernlab 
## 
## Model fit template:
## kernlab::ksvm(x = missing_arg(), data = missing_arg(), C = double(1), 
##     kernel = "polydot", prob.model = TRUE, kpar = list(degree = ~integer(1), 
##         scale = ~double(1)))
</pre></div>
<p>The <code>margin</code> parameter does not apply to classification models.
</p>
<p>Note that the <code>"kernlab"</code> engine does not naturally estimate class
probabilities. To produce them, the decision values of the model are
converted to probabilities using Platt scaling. This method fits an
additional model on top of the SVM model. When fitting the Platt scaling
model, random numbers are used that are not reproducible or controlled
by R’s random number stream.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#svm-poly-kernlab">examples</a>
for <code>svm_poly()</code> with the <code>"kernlab"</code> engine.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>References</h4>


<ul>
<li><p> Lin, HT, and R Weng. <a href="https://www.csie.ntu.edu.tw/~cjlin/papers/plattprob.pdf">“A Note on Platt’s Probabilistic Outputs for Support Vector Machines”</a>
</p>
</li>
<li><p> Karatzoglou, A, Smola, A, Hornik, K, and A Zeileis. 2004. <a href="https://www.jstatsoft.org/article/view/v011i09">“kernlab - An S4 Package for Kernel Methods in R.”</a>, <em>Journal of
Statistical Software</em>.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='details_svm_rbf_kernlab'>Radial basis function support vector machines (SVMs) via kernlab</h2><span id='topic+details_svm_rbf_kernlab'></span>

<h3>Description</h3>

<p><code><a href="kernlab.html#topic+ksvm">kernlab::ksvm()</a></code> fits a support vector machine model. For classification,
the model tries to maximize the width of the margin between classes.
For regression, the model optimizes a robust loss function that is only
affected by very large model residuals.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 3 tuning parameters:
</p>

<ul>
<li> <p><code>cost</code>: Cost (type: double, default: 1.0)
</p>
</li>
<li> <p><code>rbf_sigma</code>: Radial Basis Function sigma (type: double, default: see
below)
</p>
</li>
<li> <p><code>margin</code>: Insensitivity Margin (type: double, default: 0.1)
</p>
</li></ul>

<p>There is no default for the radial basis function kernel parameter.
kernlab estimates it from the data using a heuristic method. See
<code><a href="kernlab.html#topic+sigest">kernlab::sigest()</a></code>. This method uses random
numbers so, without setting the seed before fitting, the model will not
be reproducible.
</p>
<p>Parsnip changes the default range for <code>cost</code> to <code>c(-10, 5)</code>.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>svm_rbf(
  cost = double(1),
  rbf_sigma = double(1), 
  margin = double(1)
) %&gt;%  
  set_engine("kernlab") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Radial Basis Function Support Vector Machine Model Specification (regression)
## 
## Main Arguments:
##   cost = double(1)
##   rbf_sigma = double(1)
##   margin = double(1)
## 
## Computational engine: kernlab 
## 
## Model fit template:
## kernlab::ksvm(x = missing_arg(), data = missing_arg(), C = double(1), 
##     epsilon = double(1), kernel = "rbfdot", kpar = list(sigma = ~double(1)))
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>svm_rbf(
  cost = double(1),
  rbf_sigma = double(1)
) %&gt;% 
  set_engine("kernlab") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Radial Basis Function Support Vector Machine Model Specification (classification)
## 
## Main Arguments:
##   cost = double(1)
##   rbf_sigma = double(1)
## 
## Computational engine: kernlab 
## 
## Model fit template:
## kernlab::ksvm(x = missing_arg(), data = missing_arg(), C = double(1), 
##     kernel = "rbfdot", prob.model = TRUE, kpar = list(sigma = ~double(1)))
</pre></div>
<p>The <code>margin</code> parameter does not apply to classification models.
</p>
<p>Note that the <code>"kernlab"</code> engine does not naturally estimate class
probabilities. To produce them, the decision values of the model are
converted to probabilities using Platt scaling. This method fits an
additional model on top of the SVM model. When fitting the Platt scaling
model, random numbers are used that are not reproducible or controlled
by R’s random number stream.
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code><a href="#topic+fit.model_spec">fit()</a></code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#svm-rbf-kernlab">examples</a>
for <code>svm_rbf()</code> with the <code>"kernlab"</code> engine.
</p>



<h4>References</h4>


<ul>
<li><p> Lin, HT, and R Weng. <a href="https://www.csie.ntu.edu.tw/~cjlin/papers/plattprob.pdf">“A Note on Platt’s Probabilistic Outputs for Support Vector Machines”</a>
</p>
</li>
<li><p> Karatzoglou, A, Smola, A, Hornik, K, and A Zeileis. 2004. <a href="https://www.jstatsoft.org/article/view/v011i09">“kernlab - An S4 Package for Kernel Methods in R.”</a>, <em>Journal of
Statistical Software</em>.
</p>
</li>
<li><p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>



<hr>
<h2 id='discrim_flexible'>Flexible discriminant analysis</h2><span id='topic+discrim_flexible'></span>

<h3>Description</h3>

<p><code>discrim_flexible()</code> defines a model that fits a discriminant analysis model
that can use nonlinear features created using multivariate adaptive
regression splines (MARS). This function can fit classification models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_discrim_flexible_earth">earth</a>¹²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discrim_flexible(
  mode = "classification",
  num_terms = NULL,
  prod_degree = NULL,
  prune_method = NULL,
  engine = "earth"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="discrim_flexible_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="discrim_flexible_+3A_num_terms">num_terms</code></td>
<td>
<p>The number of features that will be retained in the
final model, including the intercept.</p>
</td></tr>
<tr><td><code id="discrim_flexible_+3A_prod_degree">prod_degree</code></td>
<td>
<p>The highest possible interaction degree.</p>
</td></tr>
<tr><td><code id="discrim_flexible_+3A_prune_method">prune_method</code></td>
<td>
<p>The pruning method.</p>
</td></tr>
<tr><td><code id="discrim_flexible_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
discrim_flexible(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_discrim_flexible_earth">earth engine details</a></code>
</p>

<hr>
<h2 id='discrim_linear'>Linear discriminant analysis</h2><span id='topic+discrim_linear'></span>

<h3>Description</h3>

<p><code>discrim_linear()</code> defines a model that estimates a multivariate
distribution for the predictors separately for the data in each class
(usually Gaussian with a common covariance matrix). Bayes' theorem is used
to compute the probability of each class, given the predictor values. This
function can fit classification models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_discrim_linear_MASS">MASS</a>¹²</code></p>
</li>
<li><p><code><a href="#topic+details_discrim_linear_mda">mda</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_discrim_linear_sda">sda</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_discrim_linear_sparsediscrim">sparsediscrim</a>²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discrim_linear(
  mode = "classification",
  penalty = NULL,
  regularization_method = NULL,
  engine = "MASS"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="discrim_linear_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model. The only
possible value for this model is &quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="discrim_linear_+3A_penalty">penalty</code></td>
<td>
<p>An non-negative number representing the amount of
regularization used by some of the engines.</p>
</td></tr>
<tr><td><code id="discrim_linear_+3A_regularization_method">regularization_method</code></td>
<td>
<p>A character string for the type of regularized
estimation. Possible values are: &quot;<code>diagonal</code>&quot;, &quot;<code>min_distance</code>&quot;,
&quot;<code>shrink_cov</code>&quot;, and &quot;<code>shrink_mean</code>&quot; (<code>sparsediscrim</code> engine only).</p>
</td></tr>
<tr><td><code id="discrim_linear_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
discrim_linear(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_discrim_linear_MASS">MASS engine details</a></code>, <code><a href="#topic+details_discrim_linear_mda">mda engine details</a></code>, <code><a href="#topic+details_discrim_linear_sda">sda engine details</a></code>, <code><a href="#topic+details_discrim_linear_sparsediscrim">sparsediscrim engine details</a></code>
</p>

<hr>
<h2 id='discrim_quad'>Quadratic discriminant analysis</h2><span id='topic+discrim_quad'></span>

<h3>Description</h3>

<p><code>discrim_quad()</code> defines a model that estimates a multivariate
distribution for the predictors separately for the data in each class
(usually Gaussian with separate covariance matrices). Bayes' theorem is used
to compute the probability of each class, given the predictor values. This
function can fit classification models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_discrim_quad_MASS">MASS</a>¹²</code></p>
</li>
<li><p><code><a href="#topic+details_discrim_quad_sparsediscrim">sparsediscrim</a>²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discrim_quad(
  mode = "classification",
  regularization_method = NULL,
  engine = "MASS"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="discrim_quad_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model. The only
possible value for this model is &quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="discrim_quad_+3A_regularization_method">regularization_method</code></td>
<td>
<p>A character string for the type of regularized
estimation. Possible values are: &quot;<code>diagonal</code>&quot;, &quot;<code>shrink_cov</code>&quot;, and
&quot;<code>shrink_mean</code>&quot; (<code>sparsediscrim</code> engine only).</p>
</td></tr>
<tr><td><code id="discrim_quad_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
discrim_quad(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_discrim_quad_MASS">MASS engine details</a></code>, <code><a href="#topic+details_discrim_quad_sparsediscrim">sparsediscrim engine details</a></code>
</p>

<hr>
<h2 id='discrim_regularized'>Regularized discriminant analysis</h2><span id='topic+discrim_regularized'></span>

<h3>Description</h3>

<p><code>discrim_regularized()</code> defines a model that estimates a multivariate
distribution for the predictors separately for the data in each class. The
structure of the model can be LDA, QDA, or some amalgam of the two. Bayes'
theorem is used to compute the probability of each class, given the
predictor values. This function can fit classification models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_discrim_regularized_klaR">klaR</a>¹²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discrim_regularized(
  mode = "classification",
  frac_common_cov = NULL,
  frac_identity = NULL,
  engine = "klaR"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="discrim_regularized_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="discrim_regularized_+3A_frac_common_cov">frac_common_cov</code>, <code id="discrim_regularized_+3A_frac_identity">frac_identity</code></td>
<td>
<p>Numeric values between zero and one.</p>
</td></tr>
<tr><td><code id="discrim_regularized_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are many ways of regularizing models. For example, one form of
regularization is to penalize model parameters. Similarly, the classic
James–Stein regularization approach shrinks the model structure to a less
complex form.
</p>
<p>The model fits a very specific type of regularized model by Friedman (1989)
that uses two types of regularization. One modulates how class-specific the
covariance matrix should be. This allows the model to balance between LDA
and QDA. The second regularization component shrinks the covariance matrix
towards the identity matrix.
</p>
<p>For the penalization approach, <code><a href="#topic+discrim_linear">discrim_linear()</a></code> with a <code>mda</code> engine can be
used. Other regularization methods can be used with <code><a href="#topic+discrim_linear">discrim_linear()</a></code> and
<code><a href="#topic+discrim_quad">discrim_quad()</a></code> can used via the <code>sparsediscrim</code> engine for those functions.
</p>
<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
discrim_regularized(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>
<p>Friedman, J (1989). Regularized Discriminant Analysis. <em>Journal of the
American Statistical Association</em>, 84, 165-175.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_discrim_regularized_klaR">klaR engine details</a></code>
</p>

<hr>
<h2 id='doc-tools'>Tools for documenting engines</h2><span id='topic+doc-tools'></span><span id='topic+find_engine_files'></span><span id='topic+make_engine_list'></span><span id='topic+make_seealso_list'></span>

<h3>Description</h3>

<p>parsnip has a fairly complex documentation system where the engines for
each model have detailed documentation about the syntax, tuning parameters,
preprocessing needs, and so on.
</p>
<p>The functions below are called from <code>.R</code> files to programmatically
generate content in the help files for a model.
</p>

<ul>
<li> <p><code><a href="#topic+find_engine_files">find_engine_files()</a></code> identifies engines for a model and creates a
bulleted list of links to those specific help files.
</p>
</li>
<li> <p><code><a href="#topic+make_seealso_list">make_seealso_list()</a></code> creates a set of links for the &quot;See Also&quot; list at
the bottom of the help pages.
</p>
</li>
<li> <p><code><a href="#topic+find_engine_files">find_engine_files()</a></code> is a function, used by the above, to find the
engines for each model function.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>find_engine_files(mod)

make_engine_list(mod)

make_seealso_list(mod, pkg = "parsnip")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="doc-tools_+3A_mod">mod</code></td>
<td>
<p>A character string for the model file (e.g. &quot;linear_reg&quot;)</p>
</td></tr>
<tr><td><code id="doc-tools_+3A_pkg">pkg</code></td>
<td>
<p>A character string for the package where the function is invoked.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>parsnip includes a document (<code>README-DOCS.md</code>) with step-by-step instructions
and details. See the code below to determine where it is installed (or see
the References section).
</p>
<p>Most parsnip users will not need to use these functions or documentation.
</p>


<h3>Value</h3>

<p><code>make_engine_list()</code> returns a character string that creates a
bulleted list of links to more specific help files.
</p>
<p><code>make_seealso_list()</code> returns a formatted character string of links.
</p>
<p><code>find_engine_files()</code> returns a tibble.
</p>


<h3>References</h3>

<p><a href="https://github.com/tidymodels/parsnip/blob/main/inst/README-DOCS.md">https://github.com/tidymodels/parsnip/blob/main/inst/README-DOCS.md</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# See this file for step-by-step instructions.
system.file("README-DOCS.md", package = "parsnip")

# Code examples:
make_engine_list("linear_reg")

cat(make_engine_list("linear_reg"))

</code></pre>

<hr>
<h2 id='eval_args'>Evaluate parsnip model arguments</h2><span id='topic+eval_args'></span>

<h3>Description</h3>

<p>Evaluate parsnip model arguments
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eval_args(spec, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="eval_args_+3A_spec">spec</code></td>
<td>
<p>A <a href="#topic+model_spec">model specification</a>.</p>
</td></tr>
<tr><td><code id="eval_args_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>

<hr>
<h2 id='extract-parsnip'>Extract elements of a parsnip model object</h2><span id='topic+extract-parsnip'></span><span id='topic+extract_spec_parsnip.model_fit'></span><span id='topic+extract_fit_engine.model_fit'></span><span id='topic+extract_parameter_set_dials.model_spec'></span><span id='topic+extract_parameter_dials.model_spec'></span><span id='topic+extract_fit_time.model_fit'></span>

<h3>Description</h3>

<p>These functions extract various elements from a parsnip object. If they do
not exist yet, an error is thrown.
</p>

<ul>
<li> <p><code>extract_spec_parsnip()</code> returns the parsnip <a href="#topic+model_spec">model specification</a>.
</p>
</li>
<li> <p><code>extract_fit_engine()</code> returns the engine specific fit embedded within
a parsnip model fit. For example, when using <code><a href="#topic+linear_reg">linear_reg()</a></code>
with the <code>"lm"</code> engine, this returns the underlying <code>lm</code> object.
</p>
</li>
<li> <p><code>extract_parameter_dials()</code> returns a single dials parameter object.
</p>
</li>
<li> <p><code>extract_parameter_set_dials()</code> returns a set of dials parameter objects.
</p>
</li>
<li> <p><code>extract_fit_time()</code> returns a tibble with fit times. The fit times
correspond to the time for the parsnip engine to fit and do not include
other portions of the elapsed time in <code><a href="#topic+fit.model_spec">fit.model_spec()</a></code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_fit'
extract_spec_parsnip(x, ...)

## S3 method for class 'model_fit'
extract_fit_engine(x, ...)

## S3 method for class 'model_spec'
extract_parameter_set_dials(x, ...)

## S3 method for class 'model_spec'
extract_parameter_dials(x, parameter, ...)

## S3 method for class 'model_fit'
extract_fit_time(x, summarize = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract-parsnip_+3A_x">x</code></td>
<td>
<p>A parsnip <code>model_fit</code> object or a parsnip <code>model_spec</code> object.</p>
</td></tr>
<tr><td><code id="extract-parsnip_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="extract-parsnip_+3A_parameter">parameter</code></td>
<td>
<p>A single string for the parameter ID.</p>
</td></tr>
<tr><td><code id="extract-parsnip_+3A_summarize">summarize</code></td>
<td>
<p>A logical for whether the elapsed fit time should be
returned as a single row or multiple rows. Doesn't support <code>FALSE</code> for
parsnip models.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extracting the underlying engine fit can be helpful for describing the
model (via <code>print()</code>, <code>summary()</code>, <code>plot()</code>, etc.) or for variable
importance/explainers.
</p>
<p>However, users should not invoke the <code>predict()</code> method on an extracted
model. There may be preprocessing operations that parsnip has executed on
the data prior to giving it to the model. Bypassing these can lead to errors
or silently generating incorrect predictions.
</p>
<p><strong>Good</strong>:
</p>
<div class="sourceCode r"><pre>   parsnip_fit %&gt;% predict(new_data)
</pre></div>
<p><strong>Bad</strong>:
</p>
<div class="sourceCode r"><pre>   parsnip_fit %&gt;% extract_fit_engine() %&gt;% predict(new_data)
</pre></div>


<h3>Value</h3>

<p>The extracted value from the parsnip object, <code>x</code>, as described in the description
section.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
lm_spec &lt;- linear_reg() %&gt;% set_engine("lm")
lm_fit &lt;- fit(lm_spec, mpg ~ ., data = mtcars)

lm_spec
extract_spec_parsnip(lm_fit)

extract_fit_engine(lm_fit)
lm(mpg ~ ., data = mtcars)

</code></pre>

<hr>
<h2 id='fit_control'>Control the fit function</h2><span id='topic+fit_control'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p>Pass options to the <code><a href="#topic+fit.model_spec">fit.model_spec()</a></code> function to control its
output and computations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_control(verbosity = 1L, catch = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit_control_+3A_verbosity">verbosity</code></td>
<td>
<p>An integer to control how verbose the output is. For a
value of zero, no messages or output are shown when packages are loaded or
when the model is fit. For a value of 1, package loading is quiet but model
fits can produce output to the screen (depending on if they contain their
own <code>verbose</code>-type argument). For a value of 2 or more, any output at all
is displayed and the execution time of the fit is recorded and printed.</p>
</td></tr>
<tr><td><code id="fit_control_+3A_catch">catch</code></td>
<td>
<p>A logical where a value of <code>TRUE</code> will evaluate the model
inside of <code>try(, silent = TRUE)</code>. If the model fails, an object is still
returned (without an error) that inherits the class &quot;try-error&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fit_control()</code> is deprecated in favor of <code>control_parsnip()</code>.
</p>


<h3>Value</h3>

<p>An S3 object with class &quot;control_parsnip&quot; that is a named list
with the results of the function call
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
fit_control(verbosity = 2L)

</code></pre>

<hr>
<h2 id='fit.model_spec'>Fit a Model Specification to a Dataset</h2><span id='topic+fit.model_spec'></span><span id='topic+fit_xy.model_spec'></span>

<h3>Description</h3>

<p><code>fit()</code> and <code>fit_xy()</code> take a model specification, translate the required
code by substituting arguments, and execute the model fit
routine.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_spec'
fit(
  object,
  formula,
  data,
  case_weights = NULL,
  control = control_parsnip(),
  ...
)

## S3 method for class 'model_spec'
fit_xy(object, x, y, case_weights = NULL, control = control_parsnip(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.model_spec_+3A_object">object</code></td>
<td>
<p>An object of class <code>model_spec</code> that has a chosen engine
(via <code><a href="#topic+set_engine">set_engine()</a></code>).</p>
</td></tr>
<tr><td><code id="fit.model_spec_+3A_formula">formula</code></td>
<td>
<p>An object of class <code>formula</code> (or one that can
be coerced to that class): a symbolic description of the model
to be fitted.</p>
</td></tr>
<tr><td><code id="fit.model_spec_+3A_data">data</code></td>
<td>
<p>Optional, depending on the interface (see Details
below). A data frame containing all relevant variables (e.g.
outcome(s), predictors, case weights, etc). Note: when needed, a
<em>named argument</em> should be used.</p>
</td></tr>
<tr><td><code id="fit.model_spec_+3A_case_weights">case_weights</code></td>
<td>
<p>An optional classed vector of numeric case weights. This
must return <code>TRUE</code> when <code><a href="hardhat.html#topic+is_case_weights">hardhat::is_case_weights()</a></code> is run on it. See
<code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code> and <code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code> for
examples.</p>
</td></tr>
<tr><td><code id="fit.model_spec_+3A_control">control</code></td>
<td>
<p>A named list with elements <code>verbosity</code> and
<code>catch</code>. See <code><a href="#topic+control_parsnip">control_parsnip()</a></code>.</p>
</td></tr>
<tr><td><code id="fit.model_spec_+3A_...">...</code></td>
<td>
<p>Not currently used; values passed here will be
ignored. Other options required to fit the model should be
passed using <code>set_engine()</code>.</p>
</td></tr>
<tr><td><code id="fit.model_spec_+3A_x">x</code></td>
<td>
<p>A matrix, sparse matrix, or data frame of predictors. Only some
models have support for sparse matrix input. See <code>parsnip::get_encoding()</code>
for details. <code>x</code> should have column names.</p>
</td></tr>
<tr><td><code id="fit.model_spec_+3A_y">y</code></td>
<td>
<p>A vector, matrix or data frame of outcome data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fit()</code> and <code>fit_xy()</code> substitute the current arguments in the model
specification into the computational engine's code, check them
for validity, then fit the model using the data and the
engine-specific code. Different model functions have different
interfaces (e.g. formula or <code>x</code>/<code>y</code>) and these functions translate
between the interface used when <code>fit()</code> or <code>fit_xy()</code> was invoked and the one
required by the underlying model.
</p>
<p>When possible, these functions attempt to avoid making copies of the
data. For example, if the underlying model uses a formula and
<code>fit()</code> is invoked, the original data are references
when the model is fit. However, if the underlying model uses
something else, such as <code>x</code>/<code>y</code>, the formula is evaluated and
the data are converted to the required format. In this case, any
calls in the resulting model objects reference the temporary
objects used to fit the model.
</p>
<p>If the model engine has not been set, the model's default engine will be used
(as discussed on each model page). If the <code>verbosity</code> option of
<code><a href="#topic+control_parsnip">control_parsnip()</a></code> is greater than zero, a warning will be produced.
</p>
<p>If you would like to use an alternative method for generating contrasts when
supplying a formula to <code>fit()</code>, set the global option <code>contrasts</code> to your
preferred method. For example, you might set it to:
<code>options(contrasts = c(unordered = "contr.helmert", ordered = "contr.poly"))</code>.
See the help page for <code><a href="stats.html#topic+contrast">stats::contr.treatment()</a></code> for more possible contrast
types.
</p>
<p>For models with <code>"censored regression"</code> modes, an additional computation is
executed and saved in the parsnip object. The <code>censor_probs</code> element contains
a &quot;reverse Kaplan-Meier&quot; curve that models the probability of censoring. This
may be used later to compute inverse probability censoring weights for
performance measures.
</p>
<p>Sparse data is supported, with the use of the <code>x</code> argument in <code>fit_xy()</code>. See
<code>allow_sparse_x</code> column of <code><a href="#topic+get_encoding">get_encoding()</a></code> for sparse input
compatibility.
</p>


<h3>Value</h3>

<p>A <code>model_fit</code> object that contains several elements:
</p>

<ul>
<li> <p><code>lvl</code>: If the outcome is a factor, this contains
the factor levels at the time of model fitting.
</p>
</li>
<li> <p><code>ordered</code>: If the outcome is a factor, was it an ordered factor?
</p>
</li>
<li> <p><code>spec</code>: The model specification object
(<code>object</code> in the call to <code>fit</code>)
</p>
</li>
<li> <p><code>fit</code>: when the model is executed without error,
this is the model object. Otherwise, it is a <code>try-error</code>
object with the error message.
</p>
</li>
<li> <p><code>preproc</code>: any objects needed to convert between
a formula and non-formula interface (such as the <code>terms</code>
object)
</p>
</li></ul>

<p>The return value will also have a class related to the fitted model (e.g.
<code>"_glm"</code>) before the base class of <code>"model_fit"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="#topic+control_parsnip">control_parsnip()</a></code>, <code>model_spec</code>, <code>model_fit</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Although `glm()` only has a formula interface, different
# methods for specifying the model can be used

library(dplyr)
library(modeldata)
data("lending_club")

lr_mod &lt;- logistic_reg()

using_formula &lt;-
  lr_mod %&gt;%
  set_engine("glm") %&gt;%
  fit(Class ~ funded_amnt + int_rate, data = lending_club)

using_xy &lt;-
  lr_mod %&gt;%
   set_engine("glm") %&gt;%
  fit_xy(x = lending_club[, c("funded_amnt", "int_rate")],
         y = lending_club$Class)

using_formula
using_xy

</code></pre>

<hr>
<h2 id='format-internals'>Internal functions that format predictions</h2><span id='topic+format-internals'></span><span id='topic+format_num'></span><span id='topic+format_class'></span><span id='topic+format_classprobs'></span><span id='topic+format_time'></span><span id='topic+format_survival'></span><span id='topic+format_linear_pred'></span><span id='topic+format_hazard'></span><span id='topic+ensure_parsnip_format'></span>

<h3>Description</h3>

<p>These are used to ensure that we have appropriate column names inside of
tibbles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_num(x)

format_class(x)

format_classprobs(x)

format_time(x)

format_survival(x)

format_linear_pred(x)

format_hazard(x)

ensure_parsnip_format(x, col_name, overwrite = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="format-internals_+3A_x">x</code></td>
<td>
<p>A data frame or vector (depending on the context and function).</p>
</td></tr>
<tr><td><code id="format-internals_+3A_col_name">col_name</code></td>
<td>
<p>A string for a prediction column name.</p>
</td></tr>
<tr><td><code id="format-internals_+3A_overwrite">overwrite</code></td>
<td>
<p>A logical for whether to overwrite the column name.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble
</p>

<hr>
<h2 id='gen_additive_mod'>Generalized additive models (GAMs)</h2><span id='topic+gen_additive_mod'></span>

<h3>Description</h3>

<p><code>gen_additive_mod()</code> defines a model that can use smoothed functions of
numeric predictors in a generalized linear model. This function can fit
classification and regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_gen_additive_mod_mgcv">mgcv</a>¹</code></p>
</li></ul>
<p>¹ The default engine.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen_additive_mod(
  mode = "unknown",
  select_features = NULL,
  adjust_deg_free = NULL,
  engine = "mgcv"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gen_additive_mod_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="gen_additive_mod_+3A_select_features">select_features</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE.</code> If <code>TRUE</code>, the model has the
ability to eliminate a predictor (via penalization). Increasing
<code>adjust_deg_free</code> will increase the likelihood of removing predictors.</p>
</td></tr>
<tr><td><code id="gen_additive_mod_+3A_adjust_deg_free">adjust_deg_free</code></td>
<td>
<p>If <code>select_features = TRUE</code>, then acts as a multiplier
for smoothness. Increase this beyond 1 to produce smoother models.</p>
</td></tr>
<tr><td><code id="gen_additive_mod_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
gen_additive_mod(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_gen_additive_mod_mgcv">mgcv engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("gen_additive_mod")

gen_additive_mod()

</code></pre>

<hr>
<h2 id='get_model_env'>Working with the parsnip model environment</h2><span id='topic+get_model_env'></span><span id='topic+get_from_env'></span><span id='topic+set_in_env'></span><span id='topic+set_env_val'></span>

<h3>Description</h3>

<p>These functions read and write to the environment where the package stores
information about model specifications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_model_env()

get_from_env(items)

set_in_env(...)

set_env_val(name, value)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_model_env_+3A_items">items</code></td>
<td>
<p>A character string of objects in the model environment.</p>
</td></tr>
<tr><td><code id="get_model_env_+3A_...">...</code></td>
<td>
<p>Named values that will be assigned to the model environment.</p>
</td></tr>
<tr><td><code id="get_model_env_+3A_name">name</code></td>
<td>
<p>A single character value for a new symbol in the model environment.</p>
</td></tr>
<tr><td><code id="get_model_env_+3A_value">value</code></td>
<td>
<p>A single value for a new value in the model environment.</p>
</td></tr>
</table>


<h3>References</h3>

<p>&quot;How to build a parsnip model&quot;
<a href="https://www.tidymodels.org/learn/develop/models/">https://www.tidymodels.org/learn/develop/models/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Access the model data:
current_code &lt;- get_model_env()
ls(envir = current_code)

</code></pre>

<hr>
<h2 id='glance.model_fit'>Construct a single row summary &quot;glance&quot; of a model, fit, or other object</h2><span id='topic+glance.model_fit'></span>

<h3>Description</h3>

<p>This method glances the model in a parsnip model object, if it exists.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_fit'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="glance.model_fit_+3A_x">x</code></td>
<td>
<p>model or other R object to convert to single-row data frame</p>
</td></tr>
<tr><td><code id="glance.model_fit_+3A_...">...</code></td>
<td>
<p>other arguments passed to methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble
</p>

<hr>
<h2 id='glm_grouped'>Fit a grouped binomial outcome from a data set with case weights</h2><span id='topic+glm_grouped'></span>

<h3>Description</h3>

<p><code><a href="stats.html#topic+glm">stats::glm()</a></code> assumes that a tabular data set with case weights corresponds
to &quot;different observations have different dispersions&quot; (see <code>?glm</code>).
</p>
<p>In some cases, the case weights reflect that the same covariate pattern was
observed multiple times (i.e., <em>frequency weights</em>). In this case,
<code><a href="stats.html#topic+glm">stats::glm()</a></code> expects the data to be formatted as the number of events for
each factor level so that the outcome can be given to the formula as
<code>cbind(events_1, events_2)</code>.
</p>
<p><code><a href="#topic+glm_grouped">glm_grouped()</a></code> converts data with integer case weights to the expected
&quot;number of events&quot; format for binomial data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm_grouped(formula, data, weights, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="glm_grouped_+3A_formula">formula</code></td>
<td>
<p>A formula object with one outcome that is a two-level factors.</p>
</td></tr>
<tr><td><code id="glm_grouped_+3A_data">data</code></td>
<td>
<p>A data frame with the outcomes and predictors (but not case
weights).</p>
</td></tr>
<tr><td><code id="glm_grouped_+3A_weights">weights</code></td>
<td>
<p>An integer vector of weights whose length is the same as the
number of rows in <code>data</code>. If it is a non-integer numeric, it will be converted
to integer (with a warning).</p>
</td></tr>
<tr><td><code id="glm_grouped_+3A_...">...</code></td>
<td>
<p>Options to pass to <code><a href="stats.html#topic+glm">stats::glm()</a></code>. If <code>family</code> is not set, it will
automatically be assigned the basic binomial family.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A object produced by <code><a href="stats.html#topic+glm">stats::glm()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#----------------------------------------------------------------------------
# The same data set formatted three ways

# First with basic case weights that, from ?glm, are used inappropriately.
ucb_weighted &lt;- as.data.frame(UCBAdmissions)
ucb_weighted$Freq &lt;- as.integer(ucb_weighted$Freq)
head(ucb_weighted)
nrow(ucb_weighted)

# Format when yes/no data are in individual rows (probably still inappropriate)
library(tidyr)
ucb_long &lt;- uncount(ucb_weighted, Freq)
head(ucb_long)
nrow(ucb_long)

# Format where the outcome is formatted as number of events
ucb_events &lt;-
  ucb_weighted %&gt;%
  tidyr::pivot_wider(
    id_cols = c(Gender, Dept),
    names_from = Admit,
    values_from = Freq,
    values_fill = 0L
  )
head(ucb_events)
nrow(ucb_events)

#----------------------------------------------------------------------------
# Different model fits

# Treat data as separate Bernoulli data:
glm(Admit ~ Gender + Dept, data = ucb_long, family = binomial)

# Weights produce the same statistics
glm(
  Admit ~ Gender + Dept,
  data = ucb_weighted,
  family = binomial,
  weights = ucb_weighted$Freq
)

# Data as binomial "x events out of n trials" format. Note that, to get the same
# coefficients, the order of the levels must be reversed.
glm(
  cbind(Rejected, Admitted) ~ Gender + Dept,
  data = ucb_events,
  family = binomial
)

# The new function that starts with frequency weights and gets the correct place:
glm_grouped(Admit ~ Gender + Dept, data = ucb_weighted, weights = ucb_weighted$Freq)

</code></pre>

<hr>
<h2 id='glmnet-details'>Technical aspects of the glmnet model</h2><span id='topic+glmnet-details'></span>

<h3>Description</h3>

<p>glmnet is a popular statistical model for regularized generalized linear
models. These notes reflect common questions about this particular model.
</p>


<h3>tidymodels and glmnet</h3>

<p>The implementation of the glmnet package has some nice features. For
example, one of the main tuning parameters, the regularization penalty,
does not need to be specified when fitting the model. The package fits a
compendium of values, called the regularization path. These values
depend on the data set and the value of <code>alpha</code>, the mixture parameter
between a pure ridge model (<code>alpha = 0</code>) and a pure lasso model
(<code>alpha = 1</code>). When predicting, any penalty values can be simultaneously
predicted, even those that are not exactly on the regularization path.
For those, the model approximates between the closest path values to
produce a prediction. There is an argument called <code>lambda</code> to the
<code>glmnet()</code> function that is used to specify the path.
</p>
<p>In the discussion below, <code>linear_reg()</code> is used. The information is true
for all parsnip models that have a <code>"glmnet"</code> engine.
</p>


<h4>Fitting and predicting using parsnip</h4>

<p>Recall that tidymodels uses standardized parameter names across models
chosen to be low on jargon. The argument <code>penalty</code> is the equivalent of
what glmnet calls the <code>lambda</code> value and <code>mixture</code> is the same as their
<code>alpha</code> value.
</p>
<p>In tidymodels, our <code>predict()</code> methods are defined to make one
prediction at a time. For this model, that means predictions are for a
single penalty value. For this reason, models that have glmnet engines
require the user to always specify a single penalty value when the model
is defined. For example, for linear regression:
</p>
<div class="sourceCode r"><pre>linear_reg(penalty = 1) %&gt;% set_engine("glmnet")
</pre></div>
<p>When the <code>predict()</code> method is called, it automatically uses the penalty
that was given when the model was defined. For example:
</p>
<div class="sourceCode r"><pre>library(tidymodels)

fit &lt;- 
  linear_reg(penalty = 1) %&gt;% 
  set_engine("glmnet") %&gt;% 
  fit(mpg ~ ., data = mtcars)

# predict at penalty = 1
predict(fit, mtcars[1:3,])
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 3 x 1
##   .pred
##   &lt;dbl&gt;
## 1  22.2
## 2  21.5
## 3  24.9
</pre></div>
<p>However, any penalty values can be predicted simultaneously using the
<code>multi_predict()</code> method:
</p>
<div class="sourceCode r"><pre># predict at c(0.00, 0.01)
multi_predict(fit, mtcars[1:3,], penalty = c(0.00, 0.01))
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 3 x 1
##   .pred           
##   &lt;list&gt;          
## 1 &lt;tibble [2 x 2]&gt;
## 2 &lt;tibble [2 x 2]&gt;
## 3 &lt;tibble [2 x 2]&gt;
</pre></div>
<div class="sourceCode r"><pre># unnested:
multi_predict(fit, mtcars[1:3,], penalty = c(0.00, 0.01)) %&gt;% 
  add_rowindex() %&gt;% 
  unnest(cols = ".pred")
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 6 x 3
##   penalty .pred  .row
##     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1    0     22.6     1
## 2    0.01  22.5     1
## 3    0     22.1     2
## 4    0.01  22.1     2
## 5    0     26.3     3
## 6    0.01  26.3     3
</pre></div>


<h5>Where did <code>lambda</code> go?</h5>

<p>It may appear odd that the <code>lambda</code> value does not get used in the fit:
</p>
<div class="sourceCode r"><pre>linear_reg(penalty = 1) %&gt;% 
  set_engine("glmnet") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 1
## 
## Computational engine: glmnet 
## 
## Model fit template:
## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     family = "gaussian")
</pre></div>
<p>Internally, the value of <code>penalty = 1</code> is saved in the parsnip object
and no value is set for <code>lambda</code>. This enables the full path to be fit
by <code>glmnet()</code>. See the section below about setting the path.
</p>




<h4>How do I set the regularization path?</h4>

<p>Regardless of what value you use for <code>penalty</code>, the full coefficient
path is used when <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code> is called.
</p>
<p>What if you want to manually set this path? Normally, you would pass a
vector to <code>lambda</code> in <code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code>.
</p>
<p>parsnip models that use a <code>glmnet</code> engine can use a special optional
argument called <code>path_values</code>. This is <em>not</em> an argument to
<code><a href="glmnet.html#topic+glmnet">glmnet::glmnet()</a></code>; it is used by parsnip to
independently set the path.
</p>
<p>For example, we have found that if you want a fully ridge regression
model (i.e., <code>mixture = 0</code>), you can get the <em>wrong coefficients</em> if the
path does not contain zero (see <a href="https://github.com/tidymodels/parsnip/issues/431#issuecomment-782883848">issue #431</a>).
</p>
<p>If we want to use our own path, the argument is passed as an
engine-specific option:
</p>
<div class="sourceCode r"><pre>coef_path_values &lt;- c(0, 10^seq(-5, 1, length.out = 7))

fit_ridge &lt;- 
  linear_reg(penalty = 1, mixture = 0) %&gt;% 
  set_engine("glmnet", path_values = coef_path_values) %&gt;% 
  fit(mpg ~ ., data = mtcars)

all.equal(sort(fit_ridge$fit$lambda), coef_path_values)
</pre></div>
<div class="sourceCode"><pre>## [1] TRUE
</pre></div>
<div class="sourceCode r"><pre># predict at penalty = 1
predict(fit_ridge, mtcars[1:3,])
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 3 x 1
##   .pred
##   &lt;dbl&gt;
## 1  22.1
## 2  21.8
## 3  26.6
</pre></div>



<h4>Tidying the model object</h4>

<p><code><a href="broom.html#topic+reexports">broom::tidy()</a></code> is a function that gives a summary of
the object as a tibble.
</p>
<p><strong>tl;dr</strong> <code>tidy()</code> on a <code>glmnet</code> model produced by parsnip gives the
coefficients for the value given by <code>penalty</code>.
</p>
<p>When parsnip makes a model, it gives it an extra class. Use the <code>tidy()</code>
method on the object, it produces coefficients for the penalty that was
originally requested:
</p>
<div class="sourceCode r"><pre>tidy(fit)
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 11 x 3
##   term        estimate penalty
##   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)  35.3          1
## 2 cyl          -0.872        1
## 3 disp          0            1
## 4 hp           -0.0101       1
## 5 drat          0            1
## 6 wt           -2.59         1
## # i 5 more rows
</pre></div>
<p>Note that there is a <code>tidy()</code> method for <code>glmnet</code> objects in the <code>broom</code>
package. If this is used directly on the underlying <code>glmnet</code> object, it
returns <em>all of coefficients on the path</em>:
</p>
<div class="sourceCode r"><pre># Use the basic tidy() method for glmnet
all_tidy_coefs &lt;- broom:::tidy.glmnet(fit$fit)
all_tidy_coefs
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 640 x 5
##   term         step estimate lambda dev.ratio
##   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)     1     20.1   5.15     0    
## 2 (Intercept)     2     21.6   4.69     0.129
## 3 (Intercept)     3     23.2   4.27     0.248
## 4 (Intercept)     4     24.7   3.89     0.347
## 5 (Intercept)     5     26.0   3.55     0.429
## 6 (Intercept)     6     27.2   3.23     0.497
## # i 634 more rows
</pre></div>
<div class="sourceCode r"><pre>length(unique(all_tidy_coefs$lambda))
</pre></div>
<div class="sourceCode"><pre>## [1] 79
</pre></div>
<p>This can be nice for plots but it might not contain the penalty value
that you are interested in.
</p>


<hr>
<h2 id='has_multi_predict'>Tools for models that predict on sub-models</h2><span id='topic+has_multi_predict'></span><span id='topic+has_multi_predict.default'></span><span id='topic+has_multi_predict.model_fit'></span><span id='topic+has_multi_predict.workflow'></span><span id='topic+multi_predict_args'></span><span id='topic+multi_predict_args.default'></span><span id='topic+multi_predict_args.model_fit'></span><span id='topic+multi_predict_args.workflow'></span>

<h3>Description</h3>

<p><code>has_multi_predict()</code> tests to see if an object can make multiple
predictions on submodels from the same object. <code>multi_predict_args()</code>
returns the names of the arguments to <code>multi_predict()</code> for this model
(if any).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>has_multi_predict(object, ...)

## Default S3 method:
has_multi_predict(object, ...)

## S3 method for class 'model_fit'
has_multi_predict(object, ...)

## S3 method for class 'workflow'
has_multi_predict(object, ...)

multi_predict_args(object, ...)

## Default S3 method:
multi_predict_args(object, ...)

## S3 method for class 'model_fit'
multi_predict_args(object, ...)

## S3 method for class 'workflow'
multi_predict_args(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="has_multi_predict_+3A_object">object</code></td>
<td>
<p>An object to test.</p>
</td></tr>
<tr><td><code id="has_multi_predict_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>has_multi_predict()</code> returns single logical value while
<code>multi_predict_args()</code> returns a character vector of argument names (or <code>NA</code>
if none exist).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
lm_model_idea &lt;- linear_reg() %&gt;% set_engine("lm")
has_multi_predict(lm_model_idea)
lm_model_fit &lt;- fit(lm_model_idea, mpg ~ ., data = mtcars)
has_multi_predict(lm_model_fit)

multi_predict_args(lm_model_fit)

library(kknn)

knn_fit &lt;-
  nearest_neighbor(mode = "regression", neighbors = 5) %&gt;%
  set_engine("kknn") %&gt;%
  fit(mpg ~ ., mtcars)

multi_predict_args(knn_fit)

multi_predict(knn_fit, mtcars[1, -1], neighbors = 1:4)$.pred

</code></pre>

<hr>
<h2 id='keras_activations'>Activation functions for neural networks in keras</h2><span id='topic+keras_activations'></span>

<h3>Description</h3>

<p>Activation functions for neural networks in keras
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keras_activations()
</code></pre>


<h3>Value</h3>

<p>A character vector of values.
</p>

<hr>
<h2 id='keras_mlp'>Simple interface to MLP models via keras</h2><span id='topic+keras_mlp'></span>

<h3>Description</h3>

<p>Instead of building a <code>keras</code> model sequentially, <code>keras_mlp</code> can be used to
create a feedforward network with a single hidden layer. Regularization is
via either weight decay or dropout.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keras_mlp(
  x,
  y,
  hidden_units = 5,
  penalty = 0,
  dropout = 0,
  epochs = 20,
  activation = "softmax",
  seeds = sample.int(10^5, size = 3),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="keras_mlp_+3A_x">x</code></td>
<td>
<p>A data frame or matrix of predictors</p>
</td></tr>
<tr><td><code id="keras_mlp_+3A_y">y</code></td>
<td>
<p>A vector (factor or numeric) or matrix (numeric) of outcome data.</p>
</td></tr>
<tr><td><code id="keras_mlp_+3A_hidden_units">hidden_units</code></td>
<td>
<p>An integer for the number of hidden units.</p>
</td></tr>
<tr><td><code id="keras_mlp_+3A_penalty">penalty</code></td>
<td>
<p>A non-negative real number for the amount of weight decay. Either
this parameter <em>or</em> <code>dropout</code> can specified.</p>
</td></tr>
<tr><td><code id="keras_mlp_+3A_dropout">dropout</code></td>
<td>
<p>The proportion of parameters to set to zero. Either
this parameter <em>or</em> <code>penalty</code> can specified.</p>
</td></tr>
<tr><td><code id="keras_mlp_+3A_epochs">epochs</code></td>
<td>
<p>An integer for the number of passes through the data.</p>
</td></tr>
<tr><td><code id="keras_mlp_+3A_activation">activation</code></td>
<td>
<p>A character string for the type of activation function between layers.</p>
</td></tr>
<tr><td><code id="keras_mlp_+3A_seeds">seeds</code></td>
<td>
<p>A vector of three positive integers to control randomness of the
calculations.</p>
</td></tr>
<tr><td><code id="keras_mlp_+3A_...">...</code></td>
<td>
<p>Additional named arguments to pass to <code>keras::compile()</code> or
<code>keras::fit()</code>. Arguments will be sorted and passed to either function
internally.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>keras</code> model object.
</p>

<hr>
<h2 id='keras_predict_classes'>Wrapper for keras class predictions</h2><span id='topic+keras_predict_classes'></span>

<h3>Description</h3>

<p>Wrapper for keras class predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keras_predict_classes(object, x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="keras_predict_classes_+3A_object">object</code></td>
<td>
<p>A keras model fit</p>
</td></tr>
<tr><td><code id="keras_predict_classes_+3A_x">x</code></td>
<td>
<p>A data set.</p>
</td></tr>
</table>

<hr>
<h2 id='knit_engine_docs'>Knit engine-specific documentation</h2><span id='topic+knit_engine_docs'></span>

<h3>Description</h3>

<p>Knit engine-specific documentation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knit_engine_docs(pattern = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="knit_engine_docs_+3A_pattern">pattern</code></td>
<td>
<p>A regular expression to specify which files to knit. The
default knits all engine documentation files.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function will check whether the known parsnip extension packages,
engine specific packages, and a few other ancillary packages are installed.
Users will be prompted to install anything required to create the engine
documentation.
</p>


<h3>Value</h3>

<p>A tibble with column <code>file</code> for the file name and <code>result</code> (a
character vector that echos the output file name or, when there is
a failure, the error message).
</p>

<hr>
<h2 id='linear_reg'>Linear regression</h2><span id='topic+linear_reg'></span>

<h3>Description</h3>

<p><code>linear_reg()</code> defines a model that can predict numeric values from
predictors using a linear function. This function can fit regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_linear_reg_lm">lm</a>¹</code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_brulee">brulee</a></code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_gee">gee</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_glm">glm</a></code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_glmer">glmer</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_glmnet">glmnet</a></code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_gls">gls</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_h2o">h2o</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_keras">keras</a></code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_lme">lme</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_lmer">lmer</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_quantreg">quantreg</a></code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_spark">spark</a></code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_stan">stan</a></code></p>
</li>
<li><p><code><a href="#topic+details_linear_reg_stan_glmer">stan_glmer</a>²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linear_reg(mode = "regression", engine = "lm", penalty = NULL, mixture = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="linear_reg_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model.
The only possible value for this model is &quot;regression&quot;.</p>
</td></tr>
<tr><td><code id="linear_reg_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting. Possible engines are listed below. The default for this
model is <code>"lm"</code>.</p>
</td></tr>
<tr><td><code id="linear_reg_+3A_penalty">penalty</code></td>
<td>
<p>A non-negative number representing the total
amount of regularization (specific engines only).</p>
</td></tr>
<tr><td><code id="linear_reg_+3A_mixture">mixture</code></td>
<td>
<p>A number between zero and one (inclusive) denoting the
proportion of L1 regularization (i.e. lasso) in the model.
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code>  specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso and ridge.
</p>
</li></ul>

<p>Available for specific engines only.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
linear_reg(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_linear_reg_lm">lm engine details</a></code>, <code><a href="#topic+details_linear_reg_brulee">brulee engine details</a></code>, <code><a href="#topic+details_linear_reg_gee">gee engine details</a></code>, <code><a href="#topic+details_linear_reg_glm">glm engine details</a></code>, <code><a href="#topic+details_linear_reg_glmer">glmer engine details</a></code>, <code><a href="#topic+details_linear_reg_glmnet">glmnet engine details</a></code>, <code><a href="#topic+details_linear_reg_gls">gls engine details</a></code>, <code><a href="#topic+details_linear_reg_h2o">h2o engine details</a></code>, <code><a href="#topic+details_linear_reg_keras">keras engine details</a></code>, <code><a href="#topic+details_linear_reg_lme">lme engine details</a></code>, <code><a href="#topic+details_linear_reg_lmer">lmer engine details</a></code>, <code><a href="#topic+details_linear_reg_quantreg">quantreg engine details</a></code>, <code><a href="#topic+details_linear_reg_spark">spark engine details</a></code>, <code><a href="#topic+details_linear_reg_stan">stan engine details</a></code>, <code><a href="#topic+details_linear_reg_stan_glmer">stan_glmer engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("linear_reg")

linear_reg()

</code></pre>

<hr>
<h2 id='list_md_problems'>Locate and show errors/warnings in engine-specific documentation</h2><span id='topic+list_md_problems'></span>

<h3>Description</h3>

<p>Locate and show errors/warnings in engine-specific documentation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_md_problems()
</code></pre>


<h3>Value</h3>

<p>A tibble with column <code>file</code> for the file name, <code>line</code> indicating
the line where the error/warning occurred, and <code>problem</code> showing the
error/warning message.
</p>

<hr>
<h2 id='logistic_reg'>Logistic regression</h2><span id='topic+logistic_reg'></span>

<h3>Description</h3>

<p><code><a href="#topic+logistic_reg">logistic_reg()</a></code> defines a generalized linear model for binary outcomes. A
linear combination of the predictors is used to model the log odds of an
event. This function can fit classification models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_logistic_reg_glm">glm</a>¹</code></p>
</li>
<li><p><code><a href="#topic+details_logistic_reg_brulee">brulee</a></code></p>
</li>
<li><p><code><a href="#topic+details_logistic_reg_gee">gee</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_logistic_reg_glmer">glmer</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_logistic_reg_glmnet">glmnet</a></code></p>
</li>
<li><p><code><a href="#topic+details_logistic_reg_h2o">h2o</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_logistic_reg_keras">keras</a></code></p>
</li>
<li><p><code><a href="#topic+details_logistic_reg_LiblineaR">LiblineaR</a></code></p>
</li>
<li><p><code><a href="#topic+details_logistic_reg_spark">spark</a></code></p>
</li>
<li><p><code><a href="#topic+details_logistic_reg_stan">stan</a></code></p>
</li>
<li><p><code><a href="#topic+details_logistic_reg_stan_glmer">stan_glmer</a>²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logistic_reg(
  mode = "classification",
  engine = "glm",
  penalty = NULL,
  mixture = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logistic_reg_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model.
The only possible value for this model is &quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="logistic_reg_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting. Possible engines are listed below. The default for this
model is <code>"glm"</code>.</p>
</td></tr>
<tr><td><code id="logistic_reg_+3A_penalty">penalty</code></td>
<td>
<p>A non-negative number representing the total
amount of regularization (specific engines only).
For <code>keras</code> models, this corresponds to purely L2 regularization
(aka weight decay) while the other models can be either or a combination
of L1 and L2 (depending on the value of <code>mixture</code>).</p>
</td></tr>
<tr><td><code id="logistic_reg_+3A_mixture">mixture</code></td>
<td>
<p>A number between zero and one (inclusive) giving the
proportion of L1 regularization (i.e. lasso) in the model.
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code>  specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso and ridge.
</p>
</li></ul>

<p>Available for specific engines only. For <code>LiblineaR</code> models, <code>mixture</code> must
be exactly 1 or 0 only.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
logistic_reg(argument = !!value)
</pre></div>
<p>This model fits a classification model for binary outcomes; for
multiclass outcomes, see <code><a href="#topic+multinom_reg">multinom_reg()</a></code>.
</p>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_logistic_reg_glm">glm engine details</a></code>, <code><a href="#topic+details_logistic_reg_brulee">brulee engine details</a></code>, <code><a href="#topic+details_logistic_reg_gee">gee engine details</a></code>, <code><a href="#topic+details_logistic_reg_glmer">glmer engine details</a></code>, <code><a href="#topic+details_logistic_reg_glmnet">glmnet engine details</a></code>, <code><a href="#topic+details_logistic_reg_h2o">h2o engine details</a></code>, <code><a href="#topic+details_logistic_reg_keras">keras engine details</a></code>, <code><a href="#topic+details_logistic_reg_LiblineaR">LiblineaR engine details</a></code>, <code><a href="#topic+details_logistic_reg_spark">spark engine details</a></code>, <code><a href="#topic+details_logistic_reg_stan">stan engine details</a></code>, <code><a href="#topic+details_logistic_reg_stan_glmer">stan_glmer engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("logistic_reg")

logistic_reg()

</code></pre>

<hr>
<h2 id='make_call'>Make a parsnip call expression</h2><span id='topic+make_call'></span>

<h3>Description</h3>

<p>Make a parsnip call expression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_call(fun, ns, args, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="make_call_+3A_fun">fun</code></td>
<td>
<p>A character string of a function name.</p>
</td></tr>
<tr><td><code id="make_call_+3A_ns">ns</code></td>
<td>
<p>A character string of a package name.</p>
</td></tr>
<tr><td><code id="make_call_+3A_args">args</code></td>
<td>
<p>A named list of argument values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The arguments are spliced into the <code>ns::fun()</code> call. If they are
missing, null, or a single logical, then are not spliced.
</p>


<h3>Value</h3>

<p>A call.
</p>

<hr>
<h2 id='make_classes'>Prepend a new class</h2><span id='topic+make_classes'></span>

<h3>Description</h3>

<p>This adds an extra class to a base class of &quot;model_spec&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_classes(prefix)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="make_classes_+3A_prefix">prefix</code></td>
<td>
<p>A character string for a class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>

<hr>
<h2 id='mars'>Multivariate adaptive regression splines (MARS)</h2><span id='topic+mars'></span>

<h3>Description</h3>

<p><code>mars()</code> defines a generalized linear model that uses artificial features for
some predictors. These features resemble hinge functions and the result is
a model that is a segmented regression in small dimensions. This function can
fit classification and regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_mars_earth">earth</a>¹</code></p>
</li></ul>
<p>¹ The default engine.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mars(
  mode = "unknown",
  engine = "earth",
  num_terms = NULL,
  prod_degree = NULL,
  prune_method = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mars_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="mars_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="mars_+3A_num_terms">num_terms</code></td>
<td>
<p>The number of features that will be retained in the
final model, including the intercept.</p>
</td></tr>
<tr><td><code id="mars_+3A_prod_degree">prod_degree</code></td>
<td>
<p>The highest possible interaction degree.</p>
</td></tr>
<tr><td><code id="mars_+3A_prune_method">prune_method</code></td>
<td>
<p>The pruning method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
mars(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_mars_earth">earth engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("mars")

mars(mode = "regression", num_terms = 5)

</code></pre>

<hr>
<h2 id='matrix_to_quantile_pred'>Reformat quantile predictions</h2><span id='topic+matrix_to_quantile_pred'></span>

<h3>Description</h3>

<p>Reformat quantile predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrix_to_quantile_pred(x, object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matrix_to_quantile_pred_+3A_x">x</code></td>
<td>
<p>A matrix of predictions with rows as samples and columns as quantile
levels.</p>
</td></tr>
<tr><td><code id="matrix_to_quantile_pred_+3A_object">object</code></td>
<td>
<p>A parsnip <code>model_fit</code> object from a quantile regression model.</p>
</td></tr>
</table>

<hr>
<h2 id='max_mtry_formula'>Determine largest value of mtry from formula.
This function potentially caps the value of <code>mtry</code> based on a formula and
data set. This is a safe approach for survival and/or multivariate models.</h2><span id='topic+max_mtry_formula'></span>

<h3>Description</h3>

<p>Determine largest value of mtry from formula.
This function potentially caps the value of <code>mtry</code> based on a formula and
data set. This is a safe approach for survival and/or multivariate models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>max_mtry_formula(mtry, formula, data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="max_mtry_formula_+3A_mtry">mtry</code></td>
<td>
<p>An initial value of <code>mtry</code> (which may be too large).</p>
</td></tr>
<tr><td><code id="max_mtry_formula_+3A_formula">formula</code></td>
<td>
<p>A model formula.</p>
</td></tr>
<tr><td><code id="max_mtry_formula_+3A_data">data</code></td>
<td>
<p>The training set (data frame).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A value for <code>mtry</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# should be 9
max_mtry_formula(200, cbind(wt, mpg) ~ ., data = mtcars)

</code></pre>

<hr>
<h2 id='maybe_matrix'>Fuzzy conversions</h2><span id='topic+maybe_matrix'></span><span id='topic+maybe_data_frame'></span>

<h3>Description</h3>

<p>These are substitutes for <code>as.matrix()</code> and <code>as.data.frame()</code> that leave
a sparse matrix as-is.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maybe_matrix(x)

maybe_data_frame(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="maybe_matrix_+3A_x">x</code></td>
<td>
<p>A data frame, matrix, or sparse matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame, matrix, or sparse matrix.
</p>

<hr>
<h2 id='min_cols'>Execution-time data dimension checks</h2><span id='topic+min_cols'></span><span id='topic+min_rows'></span>

<h3>Description</h3>

<p>For some tuning parameters, the range of values depend on the data
dimensions (e.g. <code>mtry</code>). Some packages will fail if the parameter values are
outside of these ranges. Since the model might receive resampled versions of
the data, these ranges can't be set prior to the point where the model is
fit.  These functions check the possible range of the data and adjust them
if needed (with a warning).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>min_cols(num_cols, source)

min_rows(num_rows, source, offset = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="min_cols_+3A_num_cols">num_cols</code>, <code id="min_cols_+3A_num_rows">num_rows</code></td>
<td>
<p>The parameter value requested by the user.</p>
</td></tr>
<tr><td><code id="min_cols_+3A_source">source</code></td>
<td>
<p>A data frame for the data to be used in the fit. If the source
is named &quot;data&quot;, it is assumed that one column of the data corresponds to
an outcome (and is subtracted off).</p>
</td></tr>
<tr><td><code id="min_cols_+3A_offset">offset</code></td>
<td>
<p>A number subtracted off of the number of rows available in the
data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer (and perhaps a warning).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
nearest_neighbor(neighbors= 100) %&gt;%
  set_engine("kknn") %&gt;%
  set_mode("regression") %&gt;%
  translate()

library(ranger)
rand_forest(mtry = 2, min_n = 100, trees = 3) %&gt;%
  set_engine("ranger") %&gt;%
  set_mode("regression") %&gt;%
  fit(mpg ~ ., data = mtcars)

</code></pre>

<hr>
<h2 id='mlp'>Single layer neural network</h2><span id='topic+mlp'></span>

<h3>Description</h3>

<p><code>mlp()</code> defines a multilayer perceptron model (a.k.a. a single layer,
feed-forward neural network). This function can fit classification and
regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_mlp_nnet">nnet</a>¹</code></p>
</li>
<li><p><code><a href="#topic+details_mlp_brulee">brulee</a></code></p>
</li>
<li><p><code><a href="#topic+details_mlp_brulee_two_layer">brulee_two_layer</a></code></p>
</li>
<li><p><code><a href="#topic+details_mlp_h2o">h2o</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_mlp_keras">keras</a></code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for classification and regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mlp(
  mode = "unknown",
  engine = "nnet",
  hidden_units = NULL,
  penalty = NULL,
  dropout = NULL,
  epochs = NULL,
  activation = NULL,
  learn_rate = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mlp_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="mlp_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="mlp_+3A_hidden_units">hidden_units</code></td>
<td>
<p>An integer for the number of units in the hidden model.</p>
</td></tr>
<tr><td><code id="mlp_+3A_penalty">penalty</code></td>
<td>
<p>A non-negative numeric value for the amount of weight
decay.</p>
</td></tr>
<tr><td><code id="mlp_+3A_dropout">dropout</code></td>
<td>
<p>A number between 0 (inclusive) and 1 denoting the proportion
of model parameters randomly set to zero during model training.</p>
</td></tr>
<tr><td><code id="mlp_+3A_epochs">epochs</code></td>
<td>
<p>An integer for the number of training iterations.</p>
</td></tr>
<tr><td><code id="mlp_+3A_activation">activation</code></td>
<td>
<p>A single character string denoting the type of relationship
between the original predictors and the hidden unit layer. The activation
function between the hidden and output layers is automatically set to either
&quot;linear&quot; or &quot;softmax&quot; depending on the type of outcome. Possible values
depend on the engine being used.</p>
</td></tr>
<tr><td><code id="mlp_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A number for the rate at which the boosting algorithm adapts
from iteration-to-iteration (specific engines only). This is sometimes referred to
as the shrinkage parameter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
mlp(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_mlp_nnet">nnet engine details</a></code>, <code><a href="#topic+details_mlp_brulee">brulee engine details</a></code>, <code><a href="#topic+details_mlp_brulee_two_layer">brulee_two_layer engine details</a></code>, <code><a href="#topic+details_mlp_h2o">h2o engine details</a></code>, <code><a href="#topic+details_mlp_keras">keras engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("mlp")

mlp(mode = "classification", penalty = 0.01)

</code></pre>

<hr>
<h2 id='model_db'>parsnip model specification database</h2><span id='topic+model_db'></span>

<h3>Description</h3>

<p>This is used in the RStudio add-in and captures information about mode
specifications in various R packages.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>model_db</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data(model_db)

</code></pre>

<hr>
<h2 id='model_fit'>Model Fit Objects</h2><span id='topic+model_fit'></span>

<h3>Description</h3>

<p>Model fits are trained <a href="#topic+model_spec">model specifications</a> that are
ready to <a href="#topic+predict.model_fit">predict</a> on new data. Model fits have class
<code>model_fit</code> and, usually, a subclass referring to the engine
used to fit the model.
</p>


<h3>Details</h3>

<p>An object with class <code>"model_fit"</code> is a container for
information about a model that has been fit to the data.
</p>
<p>The main elements of the object are:
</p>

<ul>
<li> <p><code>lvl</code>: A vector of factor levels when the outcome is
a factor. This is <code>NULL</code> when the outcome is not a factor
vector.
</p>
</li>
<li> <p><code>spec</code>: A <code>model_spec</code> object.
</p>
</li>
<li> <p><code>fit</code>: The object produced by the fitting function.
</p>
</li>
<li> <p><code>preproc</code>: This contains any data-specific information
required to process new a sample point for prediction. For
example, if the underlying model function requires arguments <code>x</code>
and <code>y</code> and the user passed a formula to <code>fit</code>, the <code>preproc</code>
object would contain items such as the terms object and so on.
When no information is required, this is <code>NA</code>.
</p>
</li></ul>

<p>As discussed in the documentation for <code><a href="#topic+model_spec">model_spec</a></code>, the
original arguments to the specification are saved as quosures.
These are evaluated for the <code>model_fit</code> object prior to fitting.
If the resulting model object prints its call, any user-defined
options are shown in the call preceded by a tilde (see the
example below). This is a result of the use of quosures in the
specification.
</p>
<p>This class and structure is the basis for how <span class="pkg">parsnip</span>
stores model objects after seeing the data and applying a model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Keep the `x` matrix if the data are not too big.
spec_obj &lt;-
  linear_reg() %&gt;%
  set_engine("lm", x = ifelse(.obs() &lt; 500, TRUE, FALSE))
spec_obj

fit_obj &lt;- fit(spec_obj, mpg ~ ., data = mtcars)
fit_obj

nrow(fit_obj$fit$x)

</code></pre>

<hr>
<h2 id='model_formula'>Formulas with special terms in tidymodels</h2><span id='topic+model_formula'></span>

<h3>Description</h3>

<p>In R, formulas provide a compact, symbolic notation to specify model terms.
Many modeling functions in R make use of <a href="stats.html#topic+terms.formula">&quot;specials&quot;</a>,
or nonstandard notations used in formulas. Specials are defined and handled as
a special case by a given modeling package. For example, the mgcv package,
which provides support for
<a href="#topic+gen_additive_mod">generalized additive models</a> in R, defines a
function <code>s()</code> to be in-lined into formulas. It can be used like so:
</p>
<div class="sourceCode r"><pre>mgcv::gam(mpg ~ wt + s(disp, k = 5), data = mtcars)
</pre></div>
<p>In this example, the <code>s()</code> special defines a smoothing term that the mgcv
package knows to look for when preprocessing model input.
</p>
<p>The parsnip package can handle most specials without issue. The analogous
code for specifying this generalized additive model
<a href="#topic+details_gen_additive_mod_mgcv">with the parsnip &quot;mgcv&quot; engine</a>
looks like:
</p>
<div class="sourceCode r"><pre>gen_additive_mod() %&gt;%
  set_mode("regression") %&gt;%
  set_engine("mgcv") %&gt;%
  fit(mpg ~ wt + s(disp, k = 5), data = mtcars)
</pre></div>
<p>However, parsnip is often used in conjunction with the greater tidymodels
package ecosystem, which defines its own pre-processing infrastructure and
functionality via packages like hardhat and recipes. The specials defined
in many modeling packages introduce conflicts with that infrastructure.
</p>
<p>To support specials while also maintaining consistent syntax elsewhere in
the ecosystem, <strong>tidymodels delineates between two types of formulas:
preprocessing formulas and model formulas</strong>. Preprocessing formulas specify
the input variables, while model formulas determine the model structure.
</p>


<h3>Example</h3>

<p>To create the preprocessing formula from the model formula, just remove
the specials, retaining references to input variables themselves. For example:
</p>
<div class="sourceCode"><pre>model_formula &lt;- mpg ~ wt + s(disp, k = 5)
preproc_formula &lt;- mpg ~ wt + disp
</pre></div>

<ul>
<li> <p><strong>With parsnip,</strong> use the model formula:
</p>
<div class="sourceCode r"><pre>model_spec &lt;-
  gen_additive_mod() %&gt;%
  set_mode("regression") %&gt;%
  set_engine("mgcv")

model_spec %&gt;%
  fit(model_formula, data = mtcars)
</pre></div>
</li>
<li> <p><strong>With recipes</strong>, use the preprocessing formula only:
</p>
<div class="sourceCode r"><pre>library(recipes)

recipe(preproc_formula, mtcars)
</pre></div>
<p>The recipes package supplies a large variety of preprocessing techniques
that may replace the need for specials altogether, in some cases.
</p>
</li>
<li> <p><strong>With workflows,</strong> use the preprocessing formula everywhere, but
pass the model formula to the <code>formula</code> argument in <code>add_model()</code>:
</p>
<div class="sourceCode r"><pre>library(workflows)

wflow &lt;-
  workflow() %&gt;%
  add_formula(preproc_formula) %&gt;%
  add_model(model_spec, formula = model_formula)

fit(wflow, data = mtcars)
</pre></div>
<p>The workflow will then pass the model formula to parsnip, using the
preprocessor formula elsewhere. We would still use the preprocessing
formula if we had added a recipe preprocessor using <code>add_recipe()</code>
instead a formula via <code>add_formula()</code>.
</p>
</li></ul>


<hr>
<h2 id='model_printer'>Print helper for model objects</h2><span id='topic+model_printer'></span>

<h3>Description</h3>

<p>A common format function that prints information about the model object (e.g.
arguments, calls, packages, etc).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_printer(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="model_printer_+3A_x">x</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="model_printer_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>

<hr>
<h2 id='model_spec'>Model Specifications</h2><span id='topic+model_spec'></span>

<h3>Description</h3>

<p>The parsnip package splits the process of fitting models into two steps:
</p>

<ol>
<li><p> Specify how a model will be fit using a <em>model specification</em>
</p>
</li>
<li><p> Fit a model using the model specification
</p>
</li></ol>

<p>This is a different approach to many other model interfaces in R, like <code>lm()</code>,
where both the specification of the model and the fitting happens in one
function call. Splitting the process into two steps allows users to
iteratively define model specifications throughout the model development
process.
</p>
<p>This intermediate object that defines how the model will be fit is called
a <em>model specification</em> and has class <code>model_spec</code>. Model type functions,
like <code><a href="#topic+linear_reg">linear_reg()</a></code> or <code><a href="#topic+boost_tree">boost_tree()</a></code>, return <code>model_spec</code> objects.
</p>
<p>Fitted model objects, resulting from passing a <code>model_spec</code> to
<a href="#topic+fit.model_spec">fit()</a> or <a href="#topic+fit_xy.model_spec">fit_xy</a>, have
class <code>model_fit</code>, and contain the original <code>model_spec</code> objects inside
them. See <a href="#topic+model_fit">?model_fit</a> for more on that object type, and
<a href="#topic+extract_spec_parsnip.model_fit">?extract_spec_parsnip</a> to
extract <code>model_spec</code>s from <code>model_fit</code>s.
</p>


<h3>Details</h3>

<p>An object with class <code>"model_spec"</code> is a container for
information about a model that will be fit.
</p>
<p>The main elements of the object are:
</p>

<ul>
<li> <p><code>args</code>: A vector of the main arguments for the model. The
names of these arguments may be different from their
counterparts n the underlying model function. For example, for a
<code>glmnet</code> model, the argument name for the amount of the penalty
is called &quot;penalty&quot; instead of &quot;lambda&quot; to make it more general
and usable across different types of models (and to not be
specific to a particular model function). The elements of <code>args</code>
can <code>tune()</code> with the use of the
<a href="https://tune.tidymodels.org/">tune package</a>. For more information
see <a href="https://www.tidymodels.org/start/tuning/">https://www.tidymodels.org/start/tuning/</a>. If left to their
defaults (<code>NULL</code>), the
arguments will use the underlying model functions default value.
As discussed below, the arguments in <code>args</code> are captured as
quosures and are not immediately executed.
</p>
</li>
<li> <p><code>...</code>: Optional model-function-specific
parameters. As with <code>args</code>, these will be quosures and can be
<code>tune()</code>.
</p>
</li>
<li> <p><code>mode</code>: The type of model, such as &quot;regression&quot; or
&quot;classification&quot;. Other modes will be added once the package
adds more functionality.
</p>
</li>
<li> <p><code>method</code>: This is a slot that is filled in later by the
model's constructor function. It generally contains lists of
information that are used to create the fit and prediction code
as well as required packages and similar data.
</p>
</li>
<li> <p><code>engine</code>: This character string declares exactly what
software will be used. It can be a package name or a technology
type.
</p>
</li></ul>

<p>This class and structure is the basis for how parsnip
stores model objects prior to seeing the data.
</p>


<h3>Argument Details</h3>

<p>An important detail to understand when creating model
specifications is that they are intended to be functionally
independent of the data. While it is true that some tuning
parameters are <em>data dependent</em>, the model specification does
not interact with the data at all.
</p>
<p>For example, most R functions immediately evaluate their
arguments. For example, when calling <code>mean(dat_vec)</code>, the object
<code>dat_vec</code> is immediately evaluated inside of the function.
</p>
<p>parsnip model functions do not do this. For example, using
</p>
<pre>
 rand_forest(mtry = ncol(mtcars) - 1)
</pre>
<p><strong>does not</strong> execute <code>ncol(mtcars) - 1</code> when creating the specification.
This can be seen in the output:
</p>
<pre>
 &gt; rand_forest(mtry = ncol(mtcars) - 1)
 Random Forest Model Specification (unknown)

 Main Arguments:
   mtry = ncol(mtcars) - 1
</pre>
<p>The model functions save the argument <em>expressions</em> and their
associated environments (a.k.a. a quosure) to be evaluated later
when either <code><a href="#topic+fit.model_spec">fit.model_spec()</a></code> or <code><a href="#topic+fit_xy.model_spec">fit_xy.model_spec()</a></code>  are
called with the actual data.
</p>
<p>The consequence of this strategy is that any data required to
get the parameter values must be available when the model is
fit. The two main ways that this can fail is if:
</p>

<ol>
<li><p> The data have been modified between the creation of the
model specification and when the model fit function is invoked.
</p>
</li>
<li><p> If the model specification is saved and loaded into a new
session where those same data objects do not exist.
</p>
</li></ol>

<p>The best way to avoid these issues is to not reference any data
objects in the global environment but to use data descriptors
such as <code>.cols()</code>. Another way of writing the previous
specification is
</p>
<pre>
 rand_forest(mtry = .cols() - 1)
</pre>
<p>This is not dependent on any specific data object and
is evaluated immediately before the model fitting process begins.
</p>
<p>One less advantageous approach to solving this issue is to use
quasiquotation. This would insert the actual R object into the
model specification and might be the best idea when the data
object is small. For example, using
</p>
<pre>
 rand_forest(mtry = ncol(!!mtcars) - 1)
</pre>
<p>would work (and be reproducible between sessions) but embeds
the entire mtcars data set into the <code>mtry</code> expression:
</p>
<pre>
 &gt; rand_forest(mtry = ncol(!!mtcars) - 1)
 Random Forest Model Specification (unknown)

 Main Arguments:
   mtry = ncol(structure(list(Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5, &lt;snip&gt;
</pre>
<p>However, if there were an object with the number of columns in
it, this wouldn't be too bad:
</p>
<pre>
 &gt; mtry_val &lt;- ncol(mtcars) - 1
 &gt; mtry_val
 [1] 10
 &gt; rand_forest(mtry = !!mtry_val)
 Random Forest Model Specification (unknown)

 Main Arguments:
   mtry = 10
</pre>
<p>More information on quosures and quasiquotation can be found at
<a href="https://adv-r.hadley.nz/quasiquotation.html">https://adv-r.hadley.nz/quasiquotation.html</a>.
</p>

<hr>
<h2 id='multi_predict'>Model predictions across many sub-models</h2><span id='topic+multi_predict'></span><span id='topic+multi_predict.default'></span><span id='topic+multi_predict._xgb.Booster'></span><span id='topic+multi_predict._C5.0'></span><span id='topic+multi_predict._elnet'></span><span id='topic+multi_predict._lognet'></span><span id='topic+multi_predict._multnet'></span><span id='topic+multi_predict._glmnetfit'></span><span id='topic+multi_predict._earth'></span><span id='topic+multi_predict._torch_mlp'></span><span id='topic+multi_predict._train.kknn'></span>

<h3>Description</h3>

<p>For some models, predictions can be made on sub-models in the model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multi_predict(object, ...)

## Default S3 method:
multi_predict(object, ...)

## S3 method for class ''_xgb.Booster''
multi_predict(object, new_data, type = NULL, trees = NULL, ...)

## S3 method for class ''_C5.0''
multi_predict(object, new_data, type = NULL, trees = NULL, ...)

## S3 method for class ''_elnet''
multi_predict(object, new_data, type = NULL, penalty = NULL, ...)

## S3 method for class ''_lognet''
multi_predict(object, new_data, type = NULL, penalty = NULL, ...)

## S3 method for class ''_multnet''
multi_predict(object, new_data, type = NULL, penalty = NULL, ...)

## S3 method for class ''_glmnetfit''
multi_predict(object, new_data, type = NULL, penalty = NULL, ...)

## S3 method for class ''_earth''
multi_predict(object, new_data, type = NULL, num_terms = NULL, ...)

## S3 method for class ''_torch_mlp''
multi_predict(object, new_data, type = NULL, epochs = NULL, ...)

## S3 method for class ''_train.kknn''
multi_predict(object, new_data, type = NULL, neighbors = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="multi_predict_+3A_object">object</code></td>
<td>
<p>A <a href="#topic+model_fit">model fit</a>.</p>
</td></tr>
<tr><td><code id="multi_predict_+3A_...">...</code></td>
<td>
<p>Optional arguments to pass to <code>predict.model_fit(type = "raw")</code>
such as <code>type</code>.</p>
</td></tr>
<tr><td><code id="multi_predict_+3A_new_data">new_data</code></td>
<td>
<p>A rectangular data object, such as a data frame.</p>
</td></tr>
<tr><td><code id="multi_predict_+3A_type">type</code></td>
<td>
<p>A single character value or <code>NULL</code>. Possible values are
<code>"numeric"</code>, <code>"class"</code>, <code>"prob"</code>, <code>"conf_int"</code>, <code>"pred_int"</code>, <code>"quantile"</code>,
or <code>"raw"</code>. When <code>NULL</code>, <code>predict()</code> will choose an appropriate value
based on the model's mode.</p>
</td></tr>
<tr><td><code id="multi_predict_+3A_trees">trees</code></td>
<td>
<p>An integer vector for the number of trees in the ensemble.</p>
</td></tr>
<tr><td><code id="multi_predict_+3A_penalty">penalty</code></td>
<td>
<p>A numeric vector of penalty values.</p>
</td></tr>
<tr><td><code id="multi_predict_+3A_num_terms">num_terms</code></td>
<td>
<p>An integer vector for the number of MARS terms to retain.</p>
</td></tr>
<tr><td><code id="multi_predict_+3A_epochs">epochs</code></td>
<td>
<p>An integer vector for the number of training epochs.</p>
</td></tr>
<tr><td><code id="multi_predict_+3A_neighbors">neighbors</code></td>
<td>
<p>An integer vector for the number of nearest neighbors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with the same number of rows as the data being predicted.
There is a list-column named <code>.pred</code> that contains tibbles with
multiple rows per sub-model. Note that, within the tibbles, the column names
follow the usual standard based on prediction <code>type</code> (i.e. <code>.pred_class</code> for
<code>type = "class"</code> and so on).
</p>

<hr>
<h2 id='multinom_reg'>Multinomial regression</h2><span id='topic+multinom_reg'></span>

<h3>Description</h3>

<p><code>multinom_reg()</code> defines a model that uses linear predictors to predict
multiclass data using the multinomial distribution. This function can fit
classification models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_multinom_reg_nnet">nnet</a>¹</code></p>
</li>
<li><p><code><a href="#topic+details_multinom_reg_brulee">brulee</a></code></p>
</li>
<li><p><code><a href="#topic+details_multinom_reg_glmnet">glmnet</a></code></p>
</li>
<li><p><code><a href="#topic+details_multinom_reg_h2o">h2o</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_multinom_reg_keras">keras</a></code></p>
</li>
<li><p><code><a href="#topic+details_multinom_reg_spark">spark</a></code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinom_reg(
  mode = "classification",
  engine = "nnet",
  penalty = NULL,
  mixture = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="multinom_reg_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model.
The only possible value for this model is &quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="multinom_reg_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting. Possible engines are listed below. The default for this
model is <code>"nnet"</code>.</p>
</td></tr>
<tr><td><code id="multinom_reg_+3A_penalty">penalty</code></td>
<td>
<p>A non-negative number representing the total
amount of regularization (specific engines only).
For <code>keras</code> models, this corresponds to purely L2 regularization
(aka weight decay) while the other models can be a combination
of L1 and L2 (depending on the value of <code>mixture</code>).</p>
</td></tr>
<tr><td><code id="multinom_reg_+3A_mixture">mixture</code></td>
<td>
<p>A number between zero and one (inclusive) giving the
proportion of L1 regularization (i.e. lasso) in the model.
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code>  specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso and ridge.
</p>
</li></ul>

<p>Available for specific engines only.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
multinom_reg(argument = !!value)
</pre></div>
<p>This model fits a classification model for multiclass outcomes; for
binary outcomes, see <code><a href="#topic+logistic_reg">logistic_reg()</a></code>.
</p>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_multinom_reg_nnet">nnet engine details</a></code>, <code><a href="#topic+details_multinom_reg_brulee">brulee engine details</a></code>, <code><a href="#topic+details_multinom_reg_glmnet">glmnet engine details</a></code>, <code><a href="#topic+details_multinom_reg_h2o">h2o engine details</a></code>, <code><a href="#topic+details_multinom_reg_keras">keras engine details</a></code>, <code><a href="#topic+details_multinom_reg_spark">spark engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("multinom_reg")

multinom_reg()

</code></pre>

<hr>
<h2 id='naive_Bayes'>Naive Bayes models</h2><span id='topic+naive_Bayes'></span>

<h3>Description</h3>

<p><code>naive_Bayes()</code> defines a model that uses Bayes' theorem to compute the
probability of each class, given the predictor values. This function can fit
classification models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_naive_Bayes_klaR">klaR</a>¹²</code></p>
</li>
<li><p><code><a href="#topic+details_naive_Bayes_h2o">h2o</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_naive_Bayes_naivebayes">naivebayes</a>²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>naive_Bayes(
  mode = "classification",
  smoothness = NULL,
  Laplace = NULL,
  engine = "klaR"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="naive_Bayes_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="naive_Bayes_+3A_smoothness">smoothness</code></td>
<td>
<p>An non-negative number representing the the relative
smoothness of the class boundary. Smaller examples result in model flexible
boundaries and larger values generate class boundaries that are less
adaptable</p>
</td></tr>
<tr><td><code id="naive_Bayes_+3A_laplace">Laplace</code></td>
<td>
<p>A non-negative value for the Laplace correction to smoothing
low-frequency counts.</p>
</td></tr>
<tr><td><code id="naive_Bayes_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
naive_Bayes(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_naive_Bayes_klaR">klaR engine details</a></code>, <code><a href="#topic+details_naive_Bayes_h2o">h2o engine details</a></code>, <code><a href="#topic+details_naive_Bayes_naivebayes">naivebayes engine details</a></code>
</p>

<hr>
<h2 id='nearest_neighbor'>K-nearest neighbors</h2><span id='topic+nearest_neighbor'></span>

<h3>Description</h3>

<p><code>nearest_neighbor()</code> defines a model that uses the <code>K</code> most similar data
points from the training set to predict new samples. This function can
fit classification and regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_nearest_neighbor_kknn">kknn</a>¹</code></p>
</li></ul>
<p>¹ The default engine.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nearest_neighbor(
  mode = "unknown",
  engine = "kknn",
  neighbors = NULL,
  weight_func = NULL,
  dist_power = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nearest_neighbor_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="nearest_neighbor_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="nearest_neighbor_+3A_neighbors">neighbors</code></td>
<td>
<p>A single integer for the number of neighbors
to consider (often called <code>k</code>). For <span class="pkg">kknn</span>, a value of 5
is used if <code>neighbors</code> is not specified.</p>
</td></tr>
<tr><td><code id="nearest_neighbor_+3A_weight_func">weight_func</code></td>
<td>
<p>A <em>single</em> character for the type of kernel function used
to weight distances between samples. Valid choices are: <code>"rectangular"</code>,
<code>"triangular"</code>, <code>"epanechnikov"</code>, <code>"biweight"</code>, <code>"triweight"</code>,
<code>"cos"</code>, <code>"inv"</code>, <code>"gaussian"</code>, <code>"rank"</code>, or <code>"optimal"</code>.</p>
</td></tr>
<tr><td><code id="nearest_neighbor_+3A_dist_power">dist_power</code></td>
<td>
<p>A single number for the parameter used in
calculating Minkowski distance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
nearest_neighbor(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_nearest_neighbor_kknn">kknn engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("nearest_neighbor")

nearest_neighbor(neighbors = 11)

</code></pre>

<hr>
<h2 id='null_model'>Null model</h2><span id='topic+null_model'></span>

<h3>Description</h3>

<p><code>null_model()</code> defines a simple, non-informative model. It doesn't have any
main arguments. This function can fit classification and regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>null_model(mode = "classification", engine = "parsnip")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="null_model_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model. The only
possible values for this model are <code>"regression"</code> and <code>"classification"</code>.</p>
</td></tr>
<tr><td><code id="null_model_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting. Possible engines are listed below. The default for this
model is <code>"parsnip"</code>.</p>
</td></tr>
</table>


<h3>Engine Details</h3>

<p>Engines may have pre-set default arguments when executing the model fit
call. For this type of model, the template of the fit calls are below:
</p>


<h4>parsnip</h4>

<div class="sourceCode r"><pre>null_model() %&gt;% 
  set_engine("parsnip") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Null Model Specification (regression)
## 
## Computational engine: parsnip 
## 
## Model fit template:
## parsnip::nullmodel(x = missing_arg(), y = missing_arg())
</pre></div>
<div class="sourceCode r"><pre>null_model() %&gt;% 
  set_engine("parsnip") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Null Model Specification (classification)
## 
## Computational engine: parsnip 
## 
## Model fit template:
## parsnip::nullmodel(x = missing_arg(), y = missing_arg())
</pre></div>



<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit.model_spec()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
null_model(mode = "regression")

</code></pre>

<hr>
<h2 id='null_value'>Functions required for parsnip-adjacent packages</h2><span id='topic+null_value'></span><span id='topic+show_fit'></span><span id='topic+check_args'></span><span id='topic+update_dot_check'></span><span id='topic+new_model_spec'></span><span id='topic+check_final_param'></span><span id='topic+update_main_parameters'></span><span id='topic+update_engine_parameters'></span><span id='topic+print_model_spec'></span><span id='topic+update_spec'></span><span id='topic+is_varying'></span>

<h3>Description</h3>

<p>These functions are helpful when creating new packages that will register
new model specifications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>null_value(x)

show_fit(model, eng)

check_args(object, call = rlang::caller_env())

update_dot_check(...)

new_model_spec(
  cls,
  args,
  eng_args,
  mode,
  user_specified_mode = TRUE,
  method,
  engine,
  user_specified_engine = TRUE
)

check_final_param(x, call = rlang::caller_env())

update_main_parameters(args, param, call = rlang::caller_env())

update_engine_parameters(eng_args, fresh, ...)

print_model_spec(x, cls = class(x)[1], desc = get_model_desc(cls), ...)

update_spec(
  object,
  parameters,
  args_enquo_list,
  fresh,
  cls,
  ...,
  call = caller_env()
)

is_varying(x)
</code></pre>

<hr>
<h2 id='nullmodel'>Fit a simple, non-informative model</h2><span id='topic+nullmodel'></span><span id='topic+nullmodel.default'></span><span id='topic+predict.nullmodel'></span><span id='topic+print.nullmodel'></span>

<h3>Description</h3>

<p>Fit a single mean or largest class model. <code>nullmodel()</code> is the underlying
computational function for the <code>null_model()</code> specification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nullmodel(x, ...)

## Default S3 method:
nullmodel(x = NULL, y, ...)

## S3 method for class 'nullmodel'
print(x, ...)

## S3 method for class 'nullmodel'
predict(object, new_data = NULL, type = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nullmodel_+3A_x">x</code></td>
<td>
<p>An optional matrix or data frame of predictors. These values are
not used in the model fit</p>
</td></tr>
<tr><td><code id="nullmodel_+3A_...">...</code></td>
<td>
<p>Optional arguments (not yet used)</p>
</td></tr>
<tr><td><code id="nullmodel_+3A_y">y</code></td>
<td>
<p>A numeric vector (for regression) or factor (for classification) of
outcomes</p>
</td></tr>
<tr><td><code id="nullmodel_+3A_object">object</code></td>
<td>
<p>An object of class <code>nullmodel</code></p>
</td></tr>
<tr><td><code id="nullmodel_+3A_new_data">new_data</code></td>
<td>
<p>A matrix or data frame of predictors (only used to determine
the number of predictions to return)</p>
</td></tr>
<tr><td><code id="nullmodel_+3A_type">type</code></td>
<td>
<p>Either &quot;raw&quot; (for regression), &quot;class&quot; or &quot;prob&quot; (for
classification)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nullmodel()</code> emulates other model building functions, but returns the
simplest model possible given a training set: a single mean for numeric
outcomes and the most prevalent class for factor outcomes. When class
probabilities are requested, the percentage of the training set samples with
the most prevalent class is returned.
</p>


<h3>Value</h3>

<p>The output of <code>nullmodel()</code> is a list of class <code>nullmodel</code>
with elements </p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the function call</p>
</td></tr> <tr><td><code>value</code></td>
<td>
<p>the mean of
<code>y</code> or the most prevalent class</p>
</td></tr> <tr><td><code>levels</code></td>
<td>
<p>when <code>y</code> is a
factor, a vector of levels. <code>NULL</code> otherwise</p>
</td></tr> <tr><td><code>pct</code></td>
<td>
<p>when <code>y</code>
is a factor, a data frame with a column for each class (<code>NULL</code>
otherwise). The column for the most prevalent class has the proportion of
the training samples with that class (the other columns are zero). </p>
</td></tr> <tr><td><code>n</code></td>
<td>
<p>the number of elements in <code>y</code></p>
</td></tr>
</table>
<p><code>predict.nullmodel()</code> returns either a factor or numeric vector
depending on the class of <code>y</code>. All predictions are always the same.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

outcome &lt;- factor(sample(letters[1:2],
                         size = 100,
                         prob = c(.1, .9),
                         replace = TRUE))
useless &lt;- nullmodel(y = outcome)
useless
predict(useless, matrix(NA, nrow = 5))

</code></pre>

<hr>
<h2 id='parsnip_addin'>Start an RStudio Addin that can write model specifications</h2><span id='topic+parsnip_addin'></span>

<h3>Description</h3>

<p><code>parsnip_addin()</code> starts a process in the RStudio IDE Viewer window
that allows users to write code for parsnip model specifications from
various R packages. The new code is written to the current document at the
location of the cursor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parsnip_addin()
</code></pre>

<hr>
<h2 id='pls'>Partial least squares (PLS)</h2><span id='topic+pls'></span>

<h3>Description</h3>

<p><code>pls()</code> defines a partial least squares model that uses latent variables to
model the data. It is similar to a supervised version of principal component.
This function can fit classification and regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_pls_mixOmics">mixOmics</a>¹²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for classification and regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pls(
  mode = "unknown",
  predictor_prop = NULL,
  num_comp = NULL,
  engine = "mixOmics"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pls_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="pls_+3A_predictor_prop">predictor_prop</code></td>
<td>
<p>The maximum proportion of original predictors that can
have <em>non-zero</em> coefficients for each PLS component (via regularization).
This value is used for all PLS components for X.</p>
</td></tr>
<tr><td><code id="pls_+3A_num_comp">num_comp</code></td>
<td>
<p>The number of PLS components to retain.</p>
</td></tr>
<tr><td><code id="pls_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
pls(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_pls_mixOmics">mixOmics engine details</a></code>
</p>

<hr>
<h2 id='poisson_reg'>Poisson regression models</h2><span id='topic+poisson_reg'></span>

<h3>Description</h3>

<p><code>poisson_reg()</code> defines a generalized linear model for count data that follow
a Poisson distribution. This function can fit regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_poisson_reg_glm">glm</a>¹²</code></p>
</li>
<li><p><code><a href="#topic+details_poisson_reg_gee">gee</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_poisson_reg_glmer">glmer</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_poisson_reg_glmnet">glmnet</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_poisson_reg_h2o">h2o</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_poisson_reg_hurdle">hurdle</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_poisson_reg_stan">stan</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_poisson_reg_stan_glmer">stan_glmer</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_poisson_reg_zeroinfl">zeroinfl</a>²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poisson_reg(
  mode = "regression",
  penalty = NULL,
  mixture = NULL,
  engine = "glm"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="poisson_reg_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model.
The only possible value for this model is &quot;regression&quot;.</p>
</td></tr>
<tr><td><code id="poisson_reg_+3A_penalty">penalty</code></td>
<td>
<p>A non-negative number representing the total
amount of regularization (<code>glmnet</code> only).</p>
</td></tr>
<tr><td><code id="poisson_reg_+3A_mixture">mixture</code></td>
<td>
<p>A number between zero and one (inclusive) giving the
proportion of L1 regularization (i.e. lasso) in the model.
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code>  specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso and ridge.
</p>
</li></ul>

<p>Available for <code>glmnet</code> and <code>spark</code> only.</p>
</td></tr>
<tr><td><code id="poisson_reg_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
poisson_reg(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_poisson_reg_glm">glm engine details</a></code>, <code><a href="#topic+details_poisson_reg_gee">gee engine details</a></code>, <code><a href="#topic+details_poisson_reg_glmer">glmer engine details</a></code>, <code><a href="#topic+details_poisson_reg_glmnet">glmnet engine details</a></code>, <code><a href="#topic+details_poisson_reg_h2o">h2o engine details</a></code>, <code><a href="#topic+details_poisson_reg_hurdle">hurdle engine details</a></code>, <code><a href="#topic+details_poisson_reg_stan">stan engine details</a></code>, <code><a href="#topic+details_poisson_reg_stan_glmer">stan_glmer engine details</a></code>, <code><a href="#topic+details_poisson_reg_zeroinfl">zeroinfl engine details</a></code>
</p>

<hr>
<h2 id='predict_class.model_fit'>Other predict methods.</h2><span id='topic+predict_class.model_fit'></span><span id='topic+predict_classprob.model_fit'></span><span id='topic+predict_hazard.model_fit'></span><span id='topic+predict_confint.model_fit'></span><span id='topic+predict_confint'></span><span id='topic+predict_predint'></span><span id='topic+predict_predint.model_fit'></span><span id='topic+predict_linear_pred.model_fit'></span><span id='topic+predict_linear_pred'></span><span id='topic+predict_numeric.model_fit'></span><span id='topic+predict_numeric'></span><span id='topic+predict_quantile.model_fit'></span><span id='topic+predict_survival.model_fit'></span><span id='topic+predict_survival'></span><span id='topic+predict_time.model_fit'></span><span id='topic+predict_time'></span>

<h3>Description</h3>

<p>These are internal functions not meant to be directly called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_fit'
predict_class(object, new_data, ...)

## S3 method for class 'model_fit'
predict_classprob(object, new_data, ...)

## S3 method for class 'model_fit'
predict_hazard(object, new_data, eval_time, time = deprecated(), ...)

## S3 method for class 'model_fit'
predict_confint(object, new_data, level = 0.95, std_error = FALSE, ...)

predict_confint(object, ...)

predict_predint(object, ...)

## S3 method for class 'model_fit'
predict_predint(object, new_data, level = 0.95, std_error = FALSE, ...)

predict_predint(object, ...)

## S3 method for class 'model_fit'
predict_linear_pred(object, new_data, ...)

predict_linear_pred(object, ...)

## S3 method for class 'model_fit'
predict_numeric(object, new_data, ...)

predict_numeric(object, ...)

## S3 method for class 'model_fit'
predict_quantile(
  object,
  new_data,
  quantile_levels = NULL,
  quantile = deprecated(),
  interval = "none",
  level = 0.95,
  ...
)

## S3 method for class 'model_fit'
predict_survival(
  object,
  new_data,
  eval_time,
  time = deprecated(),
  interval = "none",
  level = 0.95,
  ...
)

predict_survival(object, ...)

## S3 method for class 'model_fit'
predict_time(object, new_data, ...)

predict_time(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_class.model_fit_+3A_object">object</code></td>
<td>
<p>A <a href="#topic+model_fit">model fit</a>.</p>
</td></tr>
<tr><td><code id="predict_class.model_fit_+3A_new_data">new_data</code></td>
<td>
<p>A rectangular data object, such as a data frame.</p>
</td></tr>
<tr><td><code id="predict_class.model_fit_+3A_...">...</code></td>
<td>
<p>Additional <code>parsnip</code>-related options, depending on the
value of <code>type</code>. Arguments to the underlying model's prediction
function cannot be passed here (use the <code>opts</code> argument instead).
Possible arguments are:
</p>

<ul>
<li> <p><code>interval</code>: for <code>type</code> equal to <code>"survival"</code> or <code>"quantile"</code>, should
interval estimates be added, if available? Options are <code>"none"</code>
and <code>"confidence"</code>.
</p>
</li>
<li> <p><code>level</code>: for <code>type</code> equal to <code>"conf_int"</code>, <code>"pred_int"</code>, or <code>"survival"</code>,
this is the parameter for the tail area of the intervals
(e.g. confidence level for confidence intervals).
Default value is <code>0.95</code>.
</p>
</li>
<li> <p><code>std_error</code>: for <code>type</code> equal to <code>"conf_int"</code> or <code>"pred_int"</code>, add
the standard error of fit or prediction (on the scale of the
linear predictors). Default value is <code>FALSE</code>.
</p>
</li>
<li> <p><code>quantile</code>: for <code>type</code> equal to <code>quantile</code>, the quantiles of the
distribution. Default is <code>(1:9)/10</code>.
</p>
</li>
<li> <p><code>eval_time</code>: for <code>type</code> equal to <code>"survival"</code> or <code>"hazard"</code>, the
time points at which the survival probability or hazard is estimated.
</p>
</li></ul>
</td></tr>
<tr><td><code id="predict_class.model_fit_+3A_level">level</code></td>
<td>
<p>A single numeric value between zero and one for the
interval estimates.</p>
</td></tr>
<tr><td><code id="predict_class.model_fit_+3A_std_error">std_error</code></td>
<td>
<p>A single logical for whether the standard error should be
returned (assuming that the model can compute it).</p>
</td></tr>
<tr><td><code id="predict_class.model_fit_+3A_quantile">quantile</code>, <code id="predict_class.model_fit_+3A_quantile_levels">quantile_levels</code></td>
<td>
<p>A vector of values between 0 and 1 for the
quantile to be predicted. If the model has a <code>"quantile regression"</code> mode,
this value should be <code>NULL</code>. For other modes, the default is <code>(1:9)/10</code>.
Note that, as of version 1.3.0 of parsnip, the <code>quantile</code> is deprecated. Use
<code>quantile_levels</code> instead.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.model_fit'>Model predictions</h2><span id='topic+predict.model_fit'></span><span id='topic+predict_raw.model_fit'></span><span id='topic+predict_raw'></span>

<h3>Description</h3>

<p>Apply a model to create different types of predictions.
<code>predict()</code> can be used for all types of models and uses the
&quot;type&quot; argument for more specificity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_fit'
predict(object, new_data, type = NULL, opts = list(), ...)

## S3 method for class 'model_fit'
predict_raw(object, new_data, opts = list(), ...)

predict_raw(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.model_fit_+3A_object">object</code></td>
<td>
<p>A <a href="#topic+model_fit">model fit</a>.</p>
</td></tr>
<tr><td><code id="predict.model_fit_+3A_new_data">new_data</code></td>
<td>
<p>A rectangular data object, such as a data frame.</p>
</td></tr>
<tr><td><code id="predict.model_fit_+3A_type">type</code></td>
<td>
<p>A single character value or <code>NULL</code>. Possible values
are <code>"numeric"</code>, <code>"class"</code>, <code>"prob"</code>, <code>"conf_int"</code>, <code>"pred_int"</code>,
<code>"quantile"</code>, <code>"time"</code>, <code>"hazard"</code>, <code>"survival"</code>, or <code>"raw"</code>. When <code>NULL</code>,
<code>predict()</code> will choose an appropriate value based on the model's mode.</p>
</td></tr>
<tr><td><code id="predict.model_fit_+3A_opts">opts</code></td>
<td>
<p>A list of optional arguments to the underlying
predict function that will be used when <code>type = "raw"</code>. The
list should not include options for the model object or the
new data being predicted.</p>
</td></tr>
<tr><td><code id="predict.model_fit_+3A_...">...</code></td>
<td>
<p>Additional <code>parsnip</code>-related options, depending on the
value of <code>type</code>. Arguments to the underlying model's prediction
function cannot be passed here (use the <code>opts</code> argument instead).
Possible arguments are:
</p>

<ul>
<li> <p><code>interval</code>: for <code>type</code> equal to <code>"survival"</code> or <code>"quantile"</code>, should
interval estimates be added, if available? Options are <code>"none"</code>
and <code>"confidence"</code>.
</p>
</li>
<li> <p><code>level</code>: for <code>type</code> equal to <code>"conf_int"</code>, <code>"pred_int"</code>, or <code>"survival"</code>,
this is the parameter for the tail area of the intervals
(e.g. confidence level for confidence intervals).
Default value is <code>0.95</code>.
</p>
</li>
<li> <p><code>std_error</code>: for <code>type</code> equal to <code>"conf_int"</code> or <code>"pred_int"</code>, add
the standard error of fit or prediction (on the scale of the
linear predictors). Default value is <code>FALSE</code>.
</p>
</li>
<li> <p><code>quantile</code>: for <code>type</code> equal to <code>quantile</code>, the quantiles of the
distribution. Default is <code>(1:9)/10</code>.
</p>
</li>
<li> <p><code>eval_time</code>: for <code>type</code> equal to <code>"survival"</code> or <code>"hazard"</code>, the
time points at which the survival probability or hazard is estimated.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>type = NULL</code>, <code>predict()</code> uses
</p>

<ul>
<li> <p><code>type = "numeric"</code> for regression models,
</p>
</li>
<li> <p><code>type = "class"</code> for classification, and
</p>
</li>
<li> <p><code>type = "time"</code> for censored regression.
</p>
</li></ul>



<h4>Interval predictions</h4>

<p>When using <code>type = "conf_int"</code> and <code>type = "pred_int"</code>, the options
<code>level</code> and <code>std_error</code> can be used. The latter is a logical for an
extra column of standard error values (if available).
</p>



<h4>Censored regression predictions</h4>

<p>For censored regression, a numeric vector for <code>eval_time</code> is required when
survival or hazard probabilities are requested. The time values are required
to be unique, finite, non-missing, and non-negative. The <code>predict()</code>
functions will adjust the values to fit this specification by removing
offending points (with a warning).
</p>
<p><code>predict.model_fit()</code> does not require the outcome to be present. For
performance metrics on the predicted survival probability, inverse probability
of censoring weights (IPCW) are required (see the <code>tidymodels.org</code> reference
below). Those require the outcome and are thus not returned by <code>predict()</code>.
They can be added via <code><a href="#topic+augment.model_fit">augment.model_fit()</a></code> if <code>new_data</code> contains a column
with the outcome as a <code>Surv</code> object.
</p>
<p>Also, when <code>type = "linear_pred"</code>, censored regression models will by default
be formatted such that the linear predictor <em>increases</em> with time. This may
have the opposite sign as what the underlying model's <code>predict()</code> method
produces. Set <code>increasing = FALSE</code> to suppress this behavior.
</p>



<h3>Value</h3>

<p>With the exception of <code>type = "raw"</code>, the result of
<code>predict.model_fit()</code>
</p>

<ul>
<li><p> is a tibble
</p>
</li>
<li><p> has as many rows as there are rows in <code>new_data</code>
</p>
</li>
<li><p> has standardized column names, see below:
</p>
</li></ul>

<p>For <code>type = "numeric"</code>, the tibble has a <code>.pred</code> column for a single
outcome and <code>.pred_Yname</code> columns for a multivariate outcome.
</p>
<p>For <code>type = "class"</code>, the tibble has a <code>.pred_class</code> column.
</p>
<p>For <code>type = "prob"</code>, the tibble has <code>.pred_classlevel</code> columns.
</p>
<p>For <code>type = "conf_int"</code> and <code>type = "pred_int"</code>, the tibble has
<code>.pred_lower</code> and <code>.pred_upper</code> columns with an attribute for
the confidence level. In the case where intervals can be
produces for class probabilities (or other non-scalar outputs),
the columns are named <code>.pred_lower_classlevel</code> and so on.
</p>
<p>For <code>type = "quantile"</code>, the tibble has a <code>.pred</code> column, which is
a list-column. Each list element contains a tibble with columns
<code>.pred</code> and <code>.quantile</code> (and perhaps other columns).
</p>
<p>For <code>type = "time"</code>, the tibble has a <code>.pred_time</code> column.
</p>
<p>For <code>type = "survival"</code>, the tibble has a <code>.pred</code> column, which is
a list-column. Each list element contains a tibble with columns
<code>.eval_time</code> and <code>.pred_survival</code> (and perhaps other columns).
</p>
<p>For <code>type = "hazard"</code>, the tibble has a <code>.pred</code> column, which is
a list-column. Each list element contains a tibble with columns
<code>.eval_time</code> and <code>.pred_hazard</code> (and perhaps other columns).
</p>
<p>Using <code>type = "raw"</code> with <code>predict.model_fit()</code> will return
the unadulterated results of the prediction function.
</p>
<p>In the case of Spark-based models, since table columns cannot
contain dots, the same convention is used except 1) no dots
appear in names and 2) vectors are never returned but
type-specific prediction functions.
</p>
<p>When the model fit failed and the error was captured, the
<code>predict()</code> function will return the same structure as above but
filled with missing values. This does not currently work for
multivariate models.
</p>


<h3>References</h3>

<p><a href="https://www.tidymodels.org/learn/statistics/survival-metrics/">https://www.tidymodels.org/learn/statistics/survival-metrics/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

lm_model &lt;-
  linear_reg() %&gt;%
  set_engine("lm") %&gt;%
  fit(mpg ~ ., data = mtcars %&gt;% dplyr::slice(11:32))

pred_cars &lt;-
  mtcars %&gt;%
  dplyr::slice(1:10) %&gt;%
  dplyr::select(-mpg)

predict(lm_model, pred_cars)

predict(
  lm_model,
  pred_cars,
  type = "conf_int",
  level = 0.90
)

predict(
  lm_model,
  pred_cars,
  type = "raw",
  opts = list(type = "terms")
)

</code></pre>

<hr>
<h2 id='prepare_data'>Prepare data based on parsnip encoding information</h2><span id='topic+prepare_data'></span>

<h3>Description</h3>

<p>Prepare data based on parsnip encoding information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data(object, new_data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_data_+3A_object">object</code></td>
<td>
<p>A parsnip model object</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_new_data">new_data</code></td>
<td>
<p>A data frame</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame or matrix
</p>

<hr>
<h2 id='proportional_hazards'>Proportional hazards regression</h2><span id='topic+proportional_hazards'></span>

<h3>Description</h3>

<p><code>proportional_hazards()</code> defines a model for the hazard function
as a multiplicative function of covariates times a baseline hazard. This
function can fit censored regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_proportional_hazards_survival">survival</a>¹²</code></p>
</li>
<li><p><code><a href="#topic+details_proportional_hazards_glmnet">glmnet</a>²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proportional_hazards(
  mode = "censored regression",
  engine = "survival",
  penalty = NULL,
  mixture = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="proportional_hazards_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
The only possible value for this model is &quot;censored regression&quot;.</p>
</td></tr>
<tr><td><code id="proportional_hazards_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="proportional_hazards_+3A_penalty">penalty</code></td>
<td>
<p>A non-negative number representing the total
amount of regularization (specific engines only).</p>
</td></tr>
<tr><td><code id="proportional_hazards_+3A_mixture">mixture</code></td>
<td>
<p>A number between zero and one (inclusive) denoting the
proportion of L1 regularization (i.e. lasso) in the model.
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code>  specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso and ridge.
</p>
</li></ul>

<p>Available for specific engines only.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
proportional_hazards(argument = !!value)
</pre></div>
<p>Since survival models typically involve censoring (and require the use of
<code><a href="survival.html#topic+Surv">survival::Surv()</a></code> objects), the <code><a href="#topic+fit.model_spec">fit.model_spec()</a></code> function will require that the
survival model be specified via the formula interface.
</p>
<p>Proportional hazards models include the Cox model.
</p>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_proportional_hazards_survival">survival engine details</a></code>, <code><a href="#topic+details_proportional_hazards_glmnet">glmnet engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("proportional_hazards")

proportional_hazards(mode = "censored regression")

</code></pre>

<hr>
<h2 id='rand_forest'>Random forest</h2><span id='topic+rand_forest'></span>

<h3>Description</h3>

<p><code>rand_forest()</code> defines a model that creates a large number of decision
trees, each independent of the others. The final prediction uses all
predictions from the individual trees and combines them. This function can fit
classification, regression, and censored regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_rand_forest_ranger">ranger</a>¹</code></p>
</li>
<li><p><code><a href="#topic+details_rand_forest_aorsf">aorsf</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_rand_forest_h2o">h2o</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_rand_forest_partykit">partykit</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_rand_forest_randomForest">randomForest</a></code></p>
</li>
<li><p><code><a href="#topic+details_rand_forest_spark">spark</a></code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for censored regression, classification, and regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rand_forest(
  mode = "unknown",
  engine = "ranger",
  mtry = NULL,
  trees = NULL,
  min_n = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rand_forest_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;,
&quot;classification&quot;, or &quot;censored regression&quot;.</p>
</td></tr>
<tr><td><code id="rand_forest_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="rand_forest_+3A_mtry">mtry</code></td>
<td>
<p>An integer for the number of predictors that will
be randomly sampled at each split when creating the tree models.</p>
</td></tr>
<tr><td><code id="rand_forest_+3A_trees">trees</code></td>
<td>
<p>An integer for the number of trees contained in
the ensemble.</p>
</td></tr>
<tr><td><code id="rand_forest_+3A_min_n">min_n</code></td>
<td>
<p>An integer for the minimum number of data points
in a node that are required for the node to be split further.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
rand_forest(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_rand_forest_ranger">ranger engine details</a></code>, <code><a href="#topic+details_rand_forest_aorsf">aorsf engine details</a></code>, <code><a href="#topic+details_rand_forest_h2o">h2o engine details</a></code>, <code><a href="#topic+details_rand_forest_partykit">partykit engine details</a></code>, <code><a href="#topic+details_rand_forest_randomForest">randomForest engine details</a></code>, <code><a href="#topic+details_rand_forest_spark">spark engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("rand_forest")

rand_forest(mode = "classification", trees = 2000)

</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+autoplot'></span><span id='topic++25+3E+25'></span><span id='topic+fit'></span><span id='topic+fit_xy'></span><span id='topic+tidy'></span><span id='topic+glance'></span><span id='topic+augment'></span><span id='topic+required_pkgs'></span><span id='topic+contr_one_hot'></span><span id='topic+extract_spec_parsnip'></span><span id='topic+extract_fit_engine'></span><span id='topic+extract_parameter_set_dials'></span><span id='topic+extract_parameter_dials'></span><span id='topic+tune'></span><span id='topic+frequency_weights'></span><span id='topic+importance_weights'></span><span id='topic+extract_fit_time'></span><span id='topic+varying_args'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+augment">augment</a></code>, <code><a href="generics.html#topic+fit">fit</a></code>, <code><a href="generics.html#topic+fit_xy">fit_xy</a></code>, <code><a href="generics.html#topic+glance">glance</a></code>, <code><a href="generics.html#topic+required_pkgs">required_pkgs</a></code>, <code><a href="generics.html#topic+tidy">tidy</a></code>, <code><a href="generics.html#topic+varying_args">varying_args</a></code></p>
</dd>
<dt>ggplot2</dt><dd><p><code><a href="ggplot2.html#topic+autoplot">autoplot</a></code></p>
</dd>
<dt>hardhat</dt><dd><p><code><a href="hardhat.html#topic+contr_one_hot">contr_one_hot</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_fit_engine</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_fit_time</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_parameter_dials</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_parameter_set_dials</a></code>, <code><a href="hardhat.html#topic+hardhat-extract">extract_spec_parsnip</a></code>, <code><a href="hardhat.html#topic+frequency_weights">frequency_weights</a></code>, <code><a href="hardhat.html#topic+importance_weights">importance_weights</a></code>, <code><a href="hardhat.html#topic+tune">tune</a></code></p>
</dd>
<dt>magrittr</dt><dd><p><code><a href="magrittr.html#topic+pipe">%&gt;%</a></code></p>
</dd>
</dl>

<hr>
<h2 id='repair_call'>Repair a model call object</h2><span id='topic+repair_call'></span>

<h3>Description</h3>

<p>When the user passes a formula to <code>fit()</code> <em>and</em> the underlying model function
uses a formula, the call object produced by <code>fit()</code> may not be usable by
other functions. For example, some arguments may still be quosures and the
<code>data</code> portion of the call will not correspond to the original data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>repair_call(x, data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="repair_call_+3A_x">x</code></td>
<td>
<p>A fitted parsnip model. An error will occur if the underlying model
does not have a <code>call</code> element.</p>
</td></tr>
<tr><td><code id="repair_call_+3A_data">data</code></td>
<td>
<p>A data object that is relevant to the call. In most cases, this
is the data frame that was given to parsnip for the model fit (i.e., the
training set data). The name of this data object is inserted into the call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>repair_call()</code> call can adjust the model objects call to be usable by other
functions and methods.
</p>


<h3>Value</h3>

<p>A modified <code>parsnip</code> fitted model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

fitted_model &lt;-
  linear_reg() %&gt;%
  set_engine("lm", model = TRUE) %&gt;%
  fit(mpg ~ ., data = mtcars)

# In this call, note that `data` is not `mtcars` and the `model = ~TRUE`
# indicates that the `model` argument is an rlang quosure.
fitted_model$fit$call

# All better:
repair_call(fitted_model, mtcars)$fit$call

</code></pre>

<hr>
<h2 id='req_pkgs'>Determine required packages for a model</h2><span id='topic+req_pkgs'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>req_pkgs(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="req_pkgs_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+model_spec">model specification</a> or <a href="#topic+model_fit">fit</a>.</p>
</td></tr>
<tr><td><code id="req_pkgs_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function has been deprecated in favor of <code>required_pkgs()</code>.
</p>


<h3>Value</h3>

<p>A character string of package names (if any).
</p>

<hr>
<h2 id='required_pkgs.model_spec'>Determine required packages for a model</h2><span id='topic+required_pkgs.model_spec'></span><span id='topic+required_pkgs.model_fit'></span>

<h3>Description</h3>

<p>Determine required packages for a model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_spec'
required_pkgs(x, infra = TRUE, ...)

## S3 method for class 'model_fit'
required_pkgs(x, infra = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="required_pkgs.model_spec_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+model_spec">model specification</a> or <a href="#topic+model_fit">fit</a>.</p>
</td></tr>
<tr><td><code id="required_pkgs.model_spec_+3A_infra">infra</code></td>
<td>
<p>Should parsnip itself be included in the result?</p>
</td></tr>
<tr><td><code id="required_pkgs.model_spec_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
should_fail &lt;- try(required_pkgs(linear_reg(engine = NULL)), silent = TRUE)
should_fail

linear_reg() %&gt;%
  set_engine("glmnet") %&gt;%
  required_pkgs()

linear_reg() %&gt;%
  set_engine("glmnet") %&gt;%
  required_pkgs(infra = FALSE)

linear_reg() %&gt;%
  set_engine("lm") %&gt;%
  fit(mpg ~ ., data = mtcars) %&gt;%
  required_pkgs()

</code></pre>

<hr>
<h2 id='rule_fit'>RuleFit models</h2><span id='topic+rule_fit'></span>

<h3>Description</h3>

<p><code>rule_fit()</code> defines a model that derives simple feature rules from a tree
ensemble and uses them as features in a regularized model. This function can
fit classification and regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_rule_fit_xrf">xrf</a>¹²</code></p>
</li>
<li><p><code><a href="#topic+details_rule_fit_h2o">h2o</a>²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package for classification and regression.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rule_fit(
  mode = "unknown",
  mtry = NULL,
  trees = NULL,
  min_n = NULL,
  tree_depth = NULL,
  learn_rate = NULL,
  loss_reduction = NULL,
  sample_size = NULL,
  stop_iter = NULL,
  penalty = NULL,
  engine = "xrf"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rule_fit_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="rule_fit_+3A_mtry">mtry</code></td>
<td>
<p>A number for the number (or proportion) of predictors that will
be randomly sampled at each split when creating the tree models
(specific engines only).</p>
</td></tr>
<tr><td><code id="rule_fit_+3A_trees">trees</code></td>
<td>
<p>An integer for the number of trees contained in
the ensemble.</p>
</td></tr>
<tr><td><code id="rule_fit_+3A_min_n">min_n</code></td>
<td>
<p>An integer for the minimum number of data points
in a node that is required for the node to be split further.</p>
</td></tr>
<tr><td><code id="rule_fit_+3A_tree_depth">tree_depth</code></td>
<td>
<p>An integer for the maximum depth of the tree (i.e. number
of splits) (specific engines only).</p>
</td></tr>
<tr><td><code id="rule_fit_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A number for the rate at which the boosting algorithm adapts
from iteration-to-iteration (specific engines only). This is sometimes referred to
as the shrinkage parameter.</p>
</td></tr>
<tr><td><code id="rule_fit_+3A_loss_reduction">loss_reduction</code></td>
<td>
<p>A number for the reduction in the loss function required
to split further (specific engines only).</p>
</td></tr>
<tr><td><code id="rule_fit_+3A_sample_size">sample_size</code></td>
<td>
<p>A number for the number (or proportion) of data that is
exposed to the fitting routine. For <code>xgboost</code>, the sampling is done at
each iteration while <code>C5.0</code> samples once during training.</p>
</td></tr>
<tr><td><code id="rule_fit_+3A_stop_iter">stop_iter</code></td>
<td>
<p>The number of iterations without improvement before
stopping (specific engines only).</p>
</td></tr>
<tr><td><code id="rule_fit_+3A_penalty">penalty</code></td>
<td>
<p>L1 regularization parameter.</p>
</td></tr>
<tr><td><code id="rule_fit_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The RuleFit model creates a regression model of rules in two stages. The
first stage uses a tree-based model that is used to generate a set of rules
that can be filtered, modified, and simplified. These rules are then added
as predictors to a regularized generalized linear model that can also
conduct feature selection during model training.
</p>
<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
rule_fit(argument = !!value)
</pre></div>


<h3>References</h3>

<p>Friedman, J. H., and Popescu, B. E. (2008). &quot;Predictive learning
via rule ensembles.&quot; <em>The Annals of Applied Statistics</em>, 2(3), 916-954.
</p>
<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="xrf.html#topic+xrf.formula">xrf::xrf.formula()</a></code>, <code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_rule_fit_xrf">xrf engine details</a></code>, <code><a href="#topic+details_rule_fit_h2o">h2o engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("rule_fit")

rule_fit()

</code></pre>

<hr>
<h2 id='set_args'>Change elements of a model specification</h2><span id='topic+set_args'></span><span id='topic+set_mode'></span><span id='topic+set_mode.model_spec'></span>

<h3>Description</h3>

<p><code>set_args()</code> can be used to modify the arguments of a model specification while
<code>set_mode()</code> is used to change the model's mode.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_args(object, ...)

set_mode(object, mode, ...)

## S3 method for class 'model_spec'
set_mode(object, mode, quantile_levels = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_args_+3A_object">object</code></td>
<td>
<p>A <a href="#topic+model_spec">model specification</a>.</p>
</td></tr>
<tr><td><code id="set_args_+3A_...">...</code></td>
<td>
<p>One or more named model arguments.</p>
</td></tr>
<tr><td><code id="set_args_+3A_mode">mode</code></td>
<td>
<p>A character string for the model type (e.g. &quot;classification&quot; or
&quot;regression&quot;)</p>
</td></tr>
<tr><td><code id="set_args_+3A_quantile_levels">quantile_levels</code></td>
<td>
<p>A vector of values between zero and one (only for the
<code>"quantile regression"</code> mode); otherwise, it is <code>NULL</code>. The model uses these
values to appropriately train quantile regression models to make predictions
for these values (e.g., <code>quantile_levels = 0.5</code> is the median).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>set_args()</code> will replace existing values of the arguments.
</p>


<h3>Value</h3>

<p>An updated model object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
rand_forest()

rand_forest() %&gt;%
  set_args(mtry = 3, importance = TRUE) %&gt;%
  set_mode("regression")

linear_reg() %&gt;%
  set_mode("quantile regression", quantile_levels = c(0.2, 0.5, 0.8))

</code></pre>

<hr>
<h2 id='set_engine'>Declare a computational engine and specific arguments</h2><span id='topic+set_engine'></span>

<h3>Description</h3>

<p><code>set_engine()</code> is used to specify which package or system will be used
to fit the model, along with any arguments specific to that software.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_engine(object, engine, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_engine_+3A_object">object</code></td>
<td>
<p>A <a href="#topic+model_spec">model specification</a>.</p>
</td></tr>
<tr><td><code id="set_engine_+3A_engine">engine</code></td>
<td>
<p>A character string for the software that should
be used to fit the model. This is highly dependent on the type
of model (e.g. linear regression, random forest, etc.).</p>
</td></tr>
<tr><td><code id="set_engine_+3A_...">...</code></td>
<td>
<p>Any optional arguments associated with the chosen computational
engine. These are captured as quosures and can be tuned with <code>tune()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In parsnip,
</p>

<ul>
<li><p> the model <strong>type</strong> differentiates basic modeling approaches, such as random
forests, logistic regression, linear support vector machines, etc.,
</p>
</li>
<li><p> the <strong>mode</strong> denotes in what kind of modeling context it will be used
(most commonly, classification or regression), and
</p>
</li>
<li><p> the computational <strong>engine</strong> indicates how the model is fit, such as with
a specific R package implementation or even methods outside of R like Keras
or Stan.
</p>
</li></ul>

<p>Use <code><a href="#topic+show_engines">show_engines()</a></code> to get a list of possible engines for the model of
interest.
</p>
<p>Modeling functions in parsnip separate model arguments into two categories:
</p>

<ul>
<li> <p><em>Main arguments</em> are more commonly used and tend to be available across
engines. These names are standardized to work with different engines in a
consistent way, so you can use the parsnip main argument <code>trees</code>,
instead of the heterogeneous arguments for this parameter from <span class="pkg">ranger</span>
and  <span class="pkg">randomForest</span> packages (<code>num.trees</code> and <code>ntree</code>, respectively). Set
these in your model type function, like <code>rand_forest(trees = 2000)</code>.
</p>
</li>
<li> <p><em>Engine arguments</em> are either specific to a particular engine or used
more rarely; there is no change for these argument names from the underlying
engine. The <code>...</code> argument of <code>set_engine()</code> allows any engine-specific
argument to be passed directly to the engine fitting function, like
<code>set_engine("ranger", importance = "permutation")</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>An updated model specification.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# First, set main arguments using the standardized names
logistic_reg(penalty = 0.01, mixture = 1/3) %&gt;%
  # Now specify how you want to fit the model with another argument
  set_engine("glmnet", nlambda = 10) %&gt;%
  translate()

# Many models have possible engine-specific arguments
decision_tree(tree_depth = 5) %&gt;%
  set_engine("rpart", parms = list(prior = c(.65,.35))) %&gt;%
  set_mode("classification") %&gt;%
  translate()

</code></pre>

<hr>
<h2 id='set_new_model'>Tools to Register Models</h2><span id='topic+set_new_model'></span><span id='topic+set_model_mode'></span><span id='topic+set_model_engine'></span><span id='topic+set_model_arg'></span><span id='topic+set_dependency'></span><span id='topic+get_dependency'></span><span id='topic+set_fit'></span><span id='topic+get_fit'></span><span id='topic+set_pred'></span><span id='topic+get_pred_type'></span><span id='topic+show_model_info'></span><span id='topic+pred_value_template'></span><span id='topic+set_encoding'></span><span id='topic+get_encoding'></span>

<h3>Description</h3>

<p>These functions are similar to constructors and can be used to validate
that there are no conflicts with the underlying model structures used by the
package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_new_model(model)

set_model_mode(model, mode)

set_model_engine(model, mode, eng)

set_model_arg(model, eng, parsnip, original, func, has_submodel)

set_dependency(model, eng, pkg = "parsnip", mode = NULL)

get_dependency(model)

set_fit(model, mode, eng, value)

get_fit(model)

set_pred(model, mode, eng, type, value)

get_pred_type(model, type)

show_model_info(model)

pred_value_template(pre = NULL, post = NULL, func, ...)

set_encoding(model, mode, eng, options)

get_encoding(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_new_model_+3A_model">model</code></td>
<td>
<p>A single character string for the model type (e.g.
<code>"rand_forest"</code>, etc).</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_mode">mode</code></td>
<td>
<p>A single character string for the model mode (e.g. &quot;regression&quot;).</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_eng">eng</code></td>
<td>
<p>A single character string for the model engine.</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_parsnip">parsnip</code></td>
<td>
<p>A single character string for the &quot;harmonized&quot; argument name
that parsnip exposes.</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_original">original</code></td>
<td>
<p>A single character string for the argument name that
underlying model function uses.</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_func">func</code></td>
<td>
<p>A named character vector that describes how to call
a function. <code>func</code> should have elements <code>pkg</code> and <code>fun</code>. The
former is optional but is recommended and the latter is
required. For example, <code>c(pkg = "stats", fun = "lm")</code> would be
used to invoke the usual linear regression function. In some
cases, it is helpful to use <code>c(fun = "predict")</code> when using a
package's <code>predict</code> method.</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_has_submodel">has_submodel</code></td>
<td>
<p>A single logical for whether the argument
can make predictions on multiple submodels at once.</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_pkg">pkg</code></td>
<td>
<p>An options character string for a package name.</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_value">value</code></td>
<td>
<p>A list that conforms to the <code>fit_obj</code> or <code>pred_obj</code> description
below, depending on context.</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_type">type</code></td>
<td>
<p>A single character value for the type of prediction. Possible
values are: <code>class</code>, <code>conf_int</code>, <code>numeric</code>, <code>pred_int</code>, <code>prob</code>, <code>quantile</code>,
and <code>raw</code>.</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_pre">pre</code>, <code id="set_new_model_+3A_post">post</code></td>
<td>
<p>Optional functions for pre- and post-processing of prediction
results.</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_...">...</code></td>
<td>
<p>Optional arguments that should be passed into the <code>args</code> slot for
prediction objects.</p>
</td></tr>
<tr><td><code id="set_new_model_+3A_options">options</code></td>
<td>
<p>A list of options for engine-specific preprocessing encodings.
See Details below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are available for users to add their
own models or engines (in a package or otherwise) so that they can
be accessed using parsnip. This is more thoroughly documented
on the package web site (see references below).
</p>
<p>In short, <code>parsnip</code> stores an environment object that contains
all of the information and code about how models are used (e.g.
fitting, predicting, etc). These functions can be used to add
models to that environment as well as helper functions that can
be used to makes sure that the model data is in the right
format.
</p>
<p><code>check_model_exists()</code> checks the model value and ensures that the model has
already been registered. <code>check_model_doesnt_exist()</code> checks the model value
and also checks to see if it is novel in the environment.
</p>
<p>The options for engine-specific encodings dictate how the predictors should be
handled. These options ensure that the data
that <code>parsnip</code> gives to the underlying model allows for a model fit that is
as similar as possible to what it would have produced directly.
</p>
<p>For example, if <code>fit()</code> is used to fit a model that does not have
a formula interface, typically some predictor preprocessing must
be conducted. <code>glmnet</code> is a good example of this.
</p>
<p>There are four options that can be used for the encodings:
</p>
<p><code>predictor_indicators</code> describes whether and how to create indicator/dummy
variables from factor predictors. There are three options: <code>"none"</code> (do not
expand factor predictors), <code>"traditional"</code> (apply the standard
<code>model.matrix()</code> encodings), and <code>"one_hot"</code> (create the complete set
including the baseline level for all factors). This encoding only affects
cases when <code><a href="#topic+fit.model_spec">fit.model_spec()</a></code> is used and the underlying model has an x/y
interface.
</p>
<p>Another option is <code>compute_intercept</code>; this controls whether <code>model.matrix()</code>
should include the intercept in its formula. This affects more than the
inclusion of an intercept column. With an intercept, <code>model.matrix()</code>
computes dummy variables for all but one factor levels. Without an
intercept, <code>model.matrix()</code> computes a full set of indicators for the
<em>first</em> factor variable, but an incomplete set for the remainder.
</p>
<p>Next, the option <code>remove_intercept</code> will remove the intercept column
<em>after</em> <code>model.matrix()</code> is finished. This can be useful if the model
function (e.g. <code>lm()</code>) automatically generates an intercept.
</p>
<p>Finally, <code>allow_sparse_x</code> specifies whether the model function can natively
accommodate a sparse matrix representation for predictors during fitting
and tuning.
</p>


<h3>References</h3>

<p>&quot;How to build a parsnip model&quot;
<a href="https://www.tidymodels.org/learn/develop/models/">https://www.tidymodels.org/learn/develop/models/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# set_new_model("shallow_learning_model")

# Show the information about a model:
show_model_info("rand_forest")

</code></pre>

<hr>
<h2 id='set_tf_seed'>Set seed in R and TensorFlow at the same time</h2><span id='topic+set_tf_seed'></span>

<h3>Description</h3>

<p>Some Keras models requires seeds to be set in both R and TensorFlow to
achieve reproducible results. This function sets these seeds at the same
time using version appropriate functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_tf_seed(seed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_tf_seed_+3A_seed">seed</code></td>
<td>
<p>1 integer value.</p>
</td></tr>
</table>

<hr>
<h2 id='show_call'>Print the model call</h2><span id='topic+show_call'></span>

<h3>Description</h3>

<p>Print the model call
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show_call(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="show_call_+3A_object">object</code></td>
<td>
<p>A &quot;model_spec&quot; object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string.
</p>

<hr>
<h2 id='show_engines'>Display currently available engines for a model</h2><span id='topic+show_engines'></span>

<h3>Description</h3>

<p>The possible engines for a model can depend on what packages are loaded.
Some parsnip extension add engines to existing models. For example,
the <span class="pkg">poissonreg</span> package adds additional engines for the <code><a href="#topic+poisson_reg">poisson_reg()</a></code>
model and these are not available unless <span class="pkg">poissonreg</span> is loaded.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show_engines(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="show_engines_+3A_x">x</code></td>
<td>
<p>The name of a parsnip model (e.g., &quot;linear_reg&quot;, &quot;mars&quot;, etc.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("linear_reg")

</code></pre>

<hr>
<h2 id='sparse_data'>Using sparse data with parsnip</h2><span id='topic+sparse_data'></span>

<h3>Description</h3>

<p>You can figure out whether a given model engine supports sparse data by
calling <code>get_encoding("name of model")</code> and looking at the <code>allow_sparse_x</code>
column.
</p>


<h3>Details</h3>

<p>Using sparse data for model fitting and prediction shouldn't require any
additional configurations. Just pass in a sparse matrix such as dgCMatrix
from the <code>Matrix</code> package or a sparse tibble from the sparsevctrs package
to the data argument of <code><a href="#topic+fit">fit()</a></code>, <code><a href="#topic+fit_xy">fit_xy()</a></code>, and <code><a href="stats.html#topic+predict">predict()</a></code>.
</p>
<p>Models that don't support sparse data will try to convert to non-sparse data
with warnings. If conversion isn’t possible, an informative error will be
thrown.
</p>

<hr>
<h2 id='spec_is_possible'>Model Specification Checking:</h2><span id='topic+spec_is_possible'></span><span id='topic+spec_is_loaded'></span><span id='topic+prompt_missing_implementation'></span>

<h3>Description</h3>

<p>The helpers <code>spec_is_possible()</code>, <code>spec_is_loaded()</code>, and
<code>prompt_missing_implementation()</code> provide tooling for checking
model specifications. In addition to the <code>spec</code>, <code>engine</code>, and <code>mode</code>
arguments, the functions take arguments <code>user_specified_engine</code> and
<code>user_specified_mode</code>, denoting whether the user themselves has
specified the engine or mode, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spec_is_possible(
  spec,
  engine = spec$engine,
  user_specified_engine = spec$user_specified_engine,
  mode = spec$mode,
  user_specified_mode = spec$user_specified_mode
)

spec_is_loaded(
  spec,
  engine = spec$engine,
  user_specified_engine = spec$user_specified_engine,
  mode = spec$mode,
  user_specified_mode = spec$user_specified_mode
)

prompt_missing_implementation(
  spec,
  engine = spec$engine,
  user_specified_engine = spec$user_specified_engine,
  mode = spec$mode,
  user_specified_mode = spec$user_specified_mode,
  prompt,
  ...
)
</code></pre>


<h3>Details</h3>

<p><code>spec_is_possible()</code> checks against the union of
</p>

<ul>
<li><p> the current parsnip model environment and
</p>
</li>
<li><p> the <code>model_info_table</code> of &quot;pre-registered&quot; model specifications
</p>
</li></ul>

<p>to determine whether a model is well-specified. See
<code>parsnip:::model_info_table</code> for this table.
</p>
<p><code>spec_is_loaded()</code> checks only against the current parsnip model environment.
</p>
<p><code>spec_is_possible()</code> is executed automatically on <code>new_model_spec()</code>,
<code>set_mode()</code>, and <code>set_engine()</code>, and <code>spec_is_loaded()</code> is executed
automatically in <code>print.model_spec()</code>, among other places. <code>spec_is_possible()</code>
should be used when a model specification is still &quot;in progress&quot; of being
specified, while <code>spec_is_loaded</code> should only be called when parsnip or an
extension receives some indication that the user is &quot;done&quot; specifying a model
specification: at print, fit, addition to a workflow, or <code style="white-space: pre;">&#8288;extract_*()&#8288;</code>, for
example.
</p>
<p>When <code>spec_is_loaded()</code> is <code>FALSE</code>, the <code>prompt_missing_implementation()</code>
helper will construct an informative message to prompt users to load or
install needed packages. It's <code>prompt</code> argument refers to the prompting
function to use, usually <a href="cli.html#topic+cli_abort">cli::cli_inform</a> or <a href="cli.html#topic+cli_abort">cli::cli_abort</a>, and the
ellipses are passed to that function.
</p>

<hr>
<h2 id='stan_conf_int'>Wrapper for stan confidence intervals</h2><span id='topic+stan_conf_int'></span>

<h3>Description</h3>

<p>Wrapper for stan confidence intervals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stan_conf_int(object, newdata)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stan_conf_int_+3A_object">object</code></td>
<td>
<p>A stan model fit</p>
</td></tr>
<tr><td><code id="stan_conf_int_+3A_newdata">newdata</code></td>
<td>
<p>A data set.</p>
</td></tr>
</table>

<hr>
<h2 id='surv_reg'>Parametric survival regression</h2><span id='topic+surv_reg'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p>This function is deprecated in favor of <code>survival_reg()</code> which uses the
<code>"censored regression"</code> mode.
</p>
<p><code>surv_reg()</code> defines a parametric survival model.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>surv_reg(mode = "regression", engine = "survival", dist = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="surv_reg_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
The only possible value for this model is &quot;regression&quot;.</p>
</td></tr>
<tr><td><code id="surv_reg_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="surv_reg_+3A_dist">dist</code></td>
<td>
<p>A character string for the probability distribution of the
outcome. The default is &quot;weibull&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
surv_reg(argument = !!value)
</pre></div>
<p>Since survival models typically involve censoring (and require the use of
<code><a href="survival.html#topic+Surv">survival::Surv()</a></code> objects), the <code><a href="#topic+fit.model_spec">fit.model_spec()</a></code> function will require that the
survival model be specified via the formula interface.
</p>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>

<hr>
<h2 id='survival_reg'>Parametric survival regression</h2><span id='topic+survival_reg'></span>

<h3>Description</h3>

<p><code>survival_reg()</code> defines a parametric survival model. This function can fit
censored regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_survival_reg_survival">survival</a>¹²</code></p>
</li>
<li><p><code><a href="#topic+details_survival_reg_flexsurv">flexsurv</a>²</code></p>
</li>
<li><p><code><a href="#topic+details_survival_reg_flexsurvspline">flexsurvspline</a>²</code></p>
</li></ul>
<p>¹ The default engine. ² Requires a parsnip extension package.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survival_reg(mode = "censored regression", engine = "survival", dist = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="survival_reg_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
The only possible value for this model is &quot;censored regression&quot;.</p>
</td></tr>
<tr><td><code id="survival_reg_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="survival_reg_+3A_dist">dist</code></td>
<td>
<p>A character string for the probability distribution of the
outcome. The default is &quot;weibull&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
survival_reg(argument = !!value)
</pre></div>
<p>Since survival models typically involve censoring (and require the use of
<code><a href="survival.html#topic+Surv">survival::Surv()</a></code> objects), the <code><a href="#topic+fit.model_spec">fit.model_spec()</a></code> function will require that the
survival model be specified via the formula interface.
</p>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_survival_reg_survival">survival engine details</a></code>, <code><a href="#topic+details_survival_reg_flexsurv">flexsurv engine details</a></code>, <code><a href="#topic+details_survival_reg_flexsurvspline">flexsurvspline engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("survival_reg")

survival_reg(mode = "censored regression", dist = "weibull")

</code></pre>

<hr>
<h2 id='svm_linear'>Linear support vector machines</h2><span id='topic+svm_linear'></span>

<h3>Description</h3>

<p><code>svm_linear()</code> defines a support vector machine model. For classification,
the model tries to maximize the width of the margin between classes (using a
linear class boundary). For regression, the model optimizes a robust loss
function that is only affected by very large model residuals and uses a
linear fit. This function can fit classification and regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_svm_linear_LiblineaR">LiblineaR</a>¹</code></p>
</li>
<li><p><code><a href="#topic+details_svm_linear_kernlab">kernlab</a></code></p>
</li></ul>
<p>¹ The default engine.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svm_linear(mode = "unknown", engine = "LiblineaR", cost = NULL, margin = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="svm_linear_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="svm_linear_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="svm_linear_+3A_cost">cost</code></td>
<td>
<p>A positive number for the cost of predicting a sample within
or on the wrong side of the margin</p>
</td></tr>
<tr><td><code id="svm_linear_+3A_margin">margin</code></td>
<td>
<p>A positive number for the epsilon in the SVM insensitive
loss function (regression only)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
svm_linear(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_svm_linear_LiblineaR">LiblineaR engine details</a></code>, <code><a href="#topic+details_svm_linear_kernlab">kernlab engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("svm_linear")

svm_linear(mode = "classification")

</code></pre>

<hr>
<h2 id='svm_poly'>Polynomial support vector machines</h2><span id='topic+svm_poly'></span>

<h3>Description</h3>

<p><code>svm_poly()</code> defines a support vector machine model. For classification,
the model tries to maximize the width of the margin between classes using a
polynomial class boundary. For regression, the model optimizes a robust loss
function that is only affected by very large model residuals and uses polynomial
functions of the predictors. This function can fit classification and
regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_svm_poly_kernlab">kernlab</a>¹</code></p>
</li></ul>
<p>¹ The default engine.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svm_poly(
  mode = "unknown",
  engine = "kernlab",
  cost = NULL,
  degree = NULL,
  scale_factor = NULL,
  margin = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="svm_poly_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="svm_poly_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting.</p>
</td></tr>
<tr><td><code id="svm_poly_+3A_cost">cost</code></td>
<td>
<p>A positive number for the cost of predicting a sample within
or on the wrong side of the margin</p>
</td></tr>
<tr><td><code id="svm_poly_+3A_degree">degree</code></td>
<td>
<p>A positive number for polynomial degree.</p>
</td></tr>
<tr><td><code id="svm_poly_+3A_scale_factor">scale_factor</code></td>
<td>
<p>A positive number for the polynomial scaling factor.</p>
</td></tr>
<tr><td><code id="svm_poly_+3A_margin">margin</code></td>
<td>
<p>A positive number for the epsilon in the SVM insensitive
loss function (regression only)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
svm_poly(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_svm_poly_kernlab">kernlab engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("svm_poly")

svm_poly(mode = "classification", degree = 1.2)

</code></pre>

<hr>
<h2 id='svm_rbf'>Radial basis function support vector machines</h2><span id='topic+svm_rbf'></span>

<h3>Description</h3>

<p><code>svm_rbf()</code> defines a support vector machine model. For classification,
the model tries to maximize the width of the margin between classes using a
nonlinear class boundary. For regression, the model optimizes a robust loss
function that is only affected by very large model residuals and uses
nonlinear functions of the predictors. The function can fit classification
and regression models.
</p>
<p>There are different ways to fit this model, and the method of estimation is  chosen by setting the model <em>engine</em>. The engine-specific pages  for this model are listed  below.
</p>
<ul>
<li><p><code><a href="#topic+details_svm_rbf_kernlab">kernlab</a>¹</code></p>
</li></ul>
<p>¹ The default engine.
</p>
<p>More information on how <span class="pkg">parsnip</span> is used for modeling is at
<a href="https://www.tidymodels.org/">https://www.tidymodels.org/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svm_rbf(
  mode = "unknown",
  engine = "kernlab",
  cost = NULL,
  rbf_sigma = NULL,
  margin = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="svm_rbf_+3A_mode">mode</code></td>
<td>
<p>A single character string for the prediction outcome mode.
Possible values for this model are &quot;unknown&quot;, &quot;regression&quot;, or
&quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="svm_rbf_+3A_engine">engine</code></td>
<td>
<p>A single character string specifying what computational engine
to use for fitting. Possible engines are listed below. The default for this
model is <code>"kernlab"</code>.</p>
</td></tr>
<tr><td><code id="svm_rbf_+3A_cost">cost</code></td>
<td>
<p>A positive number for the cost of predicting a sample within
or on the wrong side of the margin</p>
</td></tr>
<tr><td><code id="svm_rbf_+3A_rbf_sigma">rbf_sigma</code></td>
<td>
<p>A positive number for radial basis function.</p>
</td></tr>
<tr><td><code id="svm_rbf_+3A_margin">margin</code></td>
<td>
<p>A positive number for the epsilon in the SVM insensitive
loss function (regression only)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only defines what <em>type</em> of model is being fit. Once an engine
is specified, the <em>method</em> to fit the model is also defined. See
<code><a href="#topic+set_engine">set_engine()</a></code> for more on setting the engine, including how to set engine
arguments.
</p>
<p>The model is not trained or fit until the <code><a href="#topic+fit.model_spec">fit()</a></code> function is used
with the data.
</p>
<p>Each of the arguments in this function other than <code>mode</code> and <code>engine</code> are
captured as <a href="rlang.html#topic+topic-quosure">quosures</a>. To pass values
programmatically, use the <a href="rlang.html#topic+injection-operator">injection operator</a> like so:
</p>
<div class="sourceCode r"><pre>value &lt;- 1
svm_rbf(argument = !!value)
</pre></div>


<h3>References</h3>

<p><a href="https://www.tidymodels.org">https://www.tidymodels.org</a>, <a href="https://www.tmwr.org/"><em>Tidy Modeling with R</em></a>, <a href="https://www.tidymodels.org/find/parsnip/">searchable table of parsnip models</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.model_spec">fit()</a></code>, <code><a href="#topic+set_engine">set_engine()</a></code>, <code><a href="stats.html#topic+update">update()</a></code>, <code><a href="#topic+details_svm_rbf_kernlab">kernlab engine details</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
show_engines("svm_rbf")

svm_rbf(mode = "classification", rbf_sigma = 0.2)

</code></pre>

<hr>
<h2 id='tidy._elnet'>tidy methods for glmnet models</h2><span id='topic+tidy._elnet'></span><span id='topic+tidy._lognet'></span><span id='topic+tidy._multnet'></span><span id='topic+tidy._fishnet'></span><span id='topic+tidy._coxnet'></span>

<h3>Description</h3>

<p><code>tidy()</code> methods for the various <code>glmnet</code> models that return the coefficients
for the specific penalty value used by the parsnip model fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class ''_elnet''
tidy(x, penalty = NULL, ...)

## S3 method for class ''_lognet''
tidy(x, penalty = NULL, ...)

## S3 method for class ''_multnet''
tidy(x, penalty = NULL, ...)

## S3 method for class ''_fishnet''
tidy(x, penalty = NULL, ...)

## S3 method for class ''_coxnet''
tidy(x, penalty = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidy._elnet_+3A_x">x</code></td>
<td>
<p>A fitted parsnip model that used the <code>glmnet</code> engine.</p>
</td></tr>
<tr><td><code id="tidy._elnet_+3A_penalty">penalty</code></td>
<td>
<p>A <em>single</em> numeric value. If none is given, the value specified
in the model specification is used.</p>
</td></tr>
<tr><td><code id="tidy._elnet_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with columns <code>term</code>, <code>estimate</code>, and <code>penalty</code>. When a
multinomial mode is used, an additional <code>class</code> column is included.
</p>

<hr>
<h2 id='tidy._LiblineaR'>tidy methods for LiblineaR models</h2><span id='topic+tidy._LiblineaR'></span>

<h3>Description</h3>

<p><code>tidy()</code> methods for the various <code>LiblineaR</code> models that return the
coefficients from the parsnip model fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class ''_LiblineaR''
tidy(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidy._LiblineaR_+3A_x">x</code></td>
<td>
<p>A fitted parsnip model that used the <code>LiblineaR</code> engine.</p>
</td></tr>
<tr><td><code id="tidy._LiblineaR_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with columns <code>term</code> and <code>estimate</code>.
</p>

<hr>
<h2 id='tidy.model_fit'>Turn a parsnip model object into a tidy tibble</h2><span id='topic+tidy.model_fit'></span>

<h3>Description</h3>

<p>This method tidies the model in a parsnip model object, if it exists.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_fit'
tidy(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidy.model_fit_+3A_x">x</code></td>
<td>
<p>An object to be converted into a tidy <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code>.</p>
</td></tr>
<tr><td><code id="tidy.model_fit_+3A_...">...</code></td>
<td>
<p>Additional arguments to tidying method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble
</p>

<hr>
<h2 id='tidy.nullmodel'>Tidy method for null models</h2><span id='topic+tidy.nullmodel'></span>

<h3>Description</h3>

<p>Return the results of <code>nullmodel</code> as a tibble
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nullmodel'
tidy(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidy.nullmodel_+3A_x">x</code></td>
<td>
<p>A <code>nullmodel</code> object.</p>
</td></tr>
<tr><td><code id="tidy.nullmodel_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with column <code>value</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

nullmodel(mtcars[,-1], mtcars$mpg) %&gt;% tidy()

</code></pre>

<hr>
<h2 id='translate'>Resolve a Model Specification for a Computational Engine</h2><span id='topic+translate'></span><span id='topic+translate.default'></span>

<h3>Description</h3>

<p><code>translate()</code> will translate a <a href="#topic+model_spec">model specification</a> into a code
object that is specific to a particular engine (e.g. R package).
It translates generic parameters to their counterparts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>translate(x, ...)

## Default S3 method:
translate(x, engine = x$engine, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="translate_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+model_spec">model specification</a>.</p>
</td></tr>
<tr><td><code id="translate_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="translate_+3A_engine">engine</code></td>
<td>
<p>The computational engine for the model (see <code>?set_engine</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>translate()</code> produces a <em>template</em> call that lacks the specific
argument values (such as <code>data</code>, etc). These are filled in once
<code>fit()</code> is called with the specifics of the data for the model.
The call may also include <code>tune()</code> arguments if these are in
the specification. To handle the <code>tune()</code> arguments, you need to use the
<a href="https://tune.tidymodels.org/">tune package</a>. For more information
see <a href="https://www.tidymodels.org/start/tuning/">https://www.tidymodels.org/start/tuning/</a>
</p>
<p>It does contain the resolved argument names that are specific to
the model fitting function/engine.
</p>
<p>This function can be useful when you need to understand how
parsnip goes from a generic model specific to a model fitting
function.
</p>
<p><strong>Note</strong>: this function is used internally and users should only use it
to understand what the underlying syntax would be. It should not be used
to modify the model specification.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
lm_spec &lt;- linear_reg(penalty = 0.01)

# `penalty` is tranlsated to `lambda`
translate(lm_spec, engine = "glmnet")

# `penalty` not applicable for this model.
translate(lm_spec, engine = "lm")

# `penalty` is tranlsated to `reg_param`
translate(lm_spec, engine = "spark")

# with a placeholder for an unknown argument value:
translate(linear_reg(penalty = tune(), mixture = tune()), engine = "glmnet")

</code></pre>

<hr>
<h2 id='type_sum.model_spec'>Succinct summary of parsnip object</h2><span id='topic+type_sum.model_spec'></span><span id='topic+type_sum.model_fit'></span>

<h3>Description</h3>

<p><code>type_sum</code> controls how objects are shown when inside tibble
columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_spec'
type_sum(x)

## S3 method for class 'model_fit'
type_sum(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="type_sum.model_spec_+3A_x">x</code></td>
<td>
<p>A <code>model_spec</code> or <code>model_fit</code> object to summarise.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>model_spec</code> objects, the summary is &quot;<code style="white-space: pre;">&#8288;spec[?]&#8288;</code>&quot;
or &quot;<code style="white-space: pre;">&#8288;spec[+]&#8288;</code>&quot;. The former indicates that either the model
mode has not been declared or that the specification has
<code>tune()</code> parameters. Otherwise, the latter is shown.
</p>
<p>For fitted models, either &quot;<code>fit[x]</code>&quot; or &quot;<code style="white-space: pre;">&#8288;fit[+]&#8288;</code>&quot; are used
where the &quot;x&quot; implies that the model fit failed in some way.
</p>


<h3>Value</h3>

<p>A character value.
</p>

<hr>
<h2 id='update_model_info_file'>Save information about models</h2><span id='topic+update_model_info_file'></span>

<h3>Description</h3>

<p>This function writes a tab delimited file to the package to capture
information about the known models. This information includes packages in
the tidymodels GitHub repository as well as packages that are known to work
well with tidymodels packages (e.g. not only <span class="pkg">parsnip</span> but also
<span class="pkg">tune</span>, etc.). There may be more model definitions in other extension
packages that are not included here.
</p>
<p>These data are used to document engines for each model function man page.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_model_info_file(path = "inst/models.tsv")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update_model_info_file_+3A_path">path</code></td>
<td>
<p>A character string for the location of the tab delimited file.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See our
<a href="https://tidymodels.github.io/model-implementation-principles/">model implementation guidelines</a>
on best practices for modeling and modeling packages.
</p>
<p>It is highly recommended that the known parsnip extension packages are loaded.
The unexported <span class="pkg">parsnip</span> function <code>extensions()</code> will list these.
</p>

<hr>
<h2 id='update.bag_mars'>Updating a model specification</h2><span id='topic+update.bag_mars'></span><span id='topic+update.bag_mlp'></span><span id='topic+update.bag_tree'></span><span id='topic+update.bart'></span><span id='topic+update.boost_tree'></span><span id='topic+update.C5_rules'></span><span id='topic+update.cubist_rules'></span><span id='topic+update.decision_tree'></span><span id='topic+update.discrim_flexible'></span><span id='topic+update.discrim_linear'></span><span id='topic+update.discrim_quad'></span><span id='topic+update.discrim_regularized'></span><span id='topic+update.gen_additive_mod'></span><span id='topic+update.linear_reg'></span><span id='topic+update.logistic_reg'></span><span id='topic+update.mars'></span><span id='topic+update.mlp'></span><span id='topic+update.multinom_reg'></span><span id='topic+update.naive_Bayes'></span><span id='topic+update.nearest_neighbor'></span><span id='topic+update.pls'></span><span id='topic+update.poisson_reg'></span><span id='topic+update.proportional_hazards'></span><span id='topic+update.rand_forest'></span><span id='topic+update.rule_fit'></span><span id='topic+update.surv_reg'></span><span id='topic+update.survival_reg'></span><span id='topic+update.svm_linear'></span><span id='topic+update.svm_poly'></span><span id='topic+update.svm_rbf'></span><span id='topic+parsnip_update'></span>

<h3>Description</h3>

<p>If parameters of a model specification need to be modified, <code>update()</code> can
be used in lieu of recreating the object from scratch.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bag_mars'
update(
  object,
  parameters = NULL,
  num_terms = NULL,
  prod_degree = NULL,
  prune_method = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'bag_mlp'
update(
  object,
  parameters = NULL,
  hidden_units = NULL,
  penalty = NULL,
  epochs = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'bag_tree'
update(
  object,
  parameters = NULL,
  cost_complexity = NULL,
  tree_depth = NULL,
  min_n = NULL,
  class_cost = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'bart'
update(
  object,
  parameters = NULL,
  trees = NULL,
  prior_terminal_node_coef = NULL,
  prior_terminal_node_expo = NULL,
  prior_outcome_range = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'boost_tree'
update(
  object,
  parameters = NULL,
  mtry = NULL,
  trees = NULL,
  min_n = NULL,
  tree_depth = NULL,
  learn_rate = NULL,
  loss_reduction = NULL,
  sample_size = NULL,
  stop_iter = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'C5_rules'
update(
  object,
  parameters = NULL,
  trees = NULL,
  min_n = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'cubist_rules'
update(
  object,
  parameters = NULL,
  committees = NULL,
  neighbors = NULL,
  max_rules = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'decision_tree'
update(
  object,
  parameters = NULL,
  cost_complexity = NULL,
  tree_depth = NULL,
  min_n = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'discrim_flexible'
update(
  object,
  num_terms = NULL,
  prod_degree = NULL,
  prune_method = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'discrim_linear'
update(
  object,
  penalty = NULL,
  regularization_method = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'discrim_quad'
update(object, regularization_method = NULL, fresh = FALSE, ...)

## S3 method for class 'discrim_regularized'
update(
  object,
  frac_common_cov = NULL,
  frac_identity = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'gen_additive_mod'
update(
  object,
  select_features = NULL,
  adjust_deg_free = NULL,
  parameters = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'linear_reg'
update(
  object,
  parameters = NULL,
  penalty = NULL,
  mixture = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'logistic_reg'
update(
  object,
  parameters = NULL,
  penalty = NULL,
  mixture = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'mars'
update(
  object,
  parameters = NULL,
  num_terms = NULL,
  prod_degree = NULL,
  prune_method = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'mlp'
update(
  object,
  parameters = NULL,
  hidden_units = NULL,
  penalty = NULL,
  dropout = NULL,
  epochs = NULL,
  activation = NULL,
  learn_rate = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'multinom_reg'
update(
  object,
  parameters = NULL,
  penalty = NULL,
  mixture = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'naive_Bayes'
update(object, smoothness = NULL, Laplace = NULL, fresh = FALSE, ...)

## S3 method for class 'nearest_neighbor'
update(
  object,
  parameters = NULL,
  neighbors = NULL,
  weight_func = NULL,
  dist_power = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'pls'
update(
  object,
  parameters = NULL,
  predictor_prop = NULL,
  num_comp = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'poisson_reg'
update(
  object,
  parameters = NULL,
  penalty = NULL,
  mixture = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'proportional_hazards'
update(
  object,
  parameters = NULL,
  penalty = NULL,
  mixture = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'rand_forest'
update(
  object,
  parameters = NULL,
  mtry = NULL,
  trees = NULL,
  min_n = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'rule_fit'
update(
  object,
  parameters = NULL,
  mtry = NULL,
  trees = NULL,
  min_n = NULL,
  tree_depth = NULL,
  learn_rate = NULL,
  loss_reduction = NULL,
  sample_size = NULL,
  penalty = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'surv_reg'
update(object, parameters = NULL, dist = NULL, fresh = FALSE, ...)

## S3 method for class 'survival_reg'
update(object, parameters = NULL, dist = NULL, fresh = FALSE, ...)

## S3 method for class 'svm_linear'
update(
  object,
  parameters = NULL,
  cost = NULL,
  margin = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'svm_poly'
update(
  object,
  parameters = NULL,
  cost = NULL,
  degree = NULL,
  scale_factor = NULL,
  margin = NULL,
  fresh = FALSE,
  ...
)

## S3 method for class 'svm_rbf'
update(
  object,
  parameters = NULL,
  cost = NULL,
  rbf_sigma = NULL,
  margin = NULL,
  fresh = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update.bag_mars_+3A_object">object</code></td>
<td>
<p>A <a href="#topic+model_spec">model specification</a>.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_parameters">parameters</code></td>
<td>
<p>A 1-row tibble or named list with <em>main</em>
parameters to update. Use <strong>either</strong> <code>parameters</code> <strong>or</strong> the main arguments
directly when updating. If the main arguments are used,
these will supersede the values in <code>parameters</code>. Also, using
engine arguments in this object will result in an error.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_num_terms">num_terms</code></td>
<td>
<p>The number of features that will be retained in the
final model, including the intercept.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_prod_degree">prod_degree</code></td>
<td>
<p>The highest possible interaction degree.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_prune_method">prune_method</code></td>
<td>
<p>The pruning method.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_fresh">fresh</code></td>
<td>
<p>A logical for whether the arguments should be
modified in-place or replaced wholesale.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_...">...</code></td>
<td>
<p>Not used for <code>update()</code>.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_hidden_units">hidden_units</code></td>
<td>
<p>An integer for the number of units in the hidden model.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_penalty">penalty</code></td>
<td>
<p>An non-negative number representing the amount of
regularization used by some of the engines.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_epochs">epochs</code></td>
<td>
<p>An integer for the number of training iterations.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_cost_complexity">cost_complexity</code></td>
<td>
<p>A positive number for the the cost/complexity
parameter (a.k.a. <code>Cp</code>) used by CART models (specific engines only).</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_tree_depth">tree_depth</code></td>
<td>
<p>An integer for maximum depth of the tree.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_min_n">min_n</code></td>
<td>
<p>An integer for the minimum number of data points
in a node that are required for the node to be split further.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_class_cost">class_cost</code></td>
<td>
<p>A non-negative scalar for a class cost (where a cost of 1
means no extra cost). This is useful for when the first level of the outcome
factor is the minority class. If this is not the case, values between zero
and one can be used to bias to the second level of the factor.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_trees">trees</code></td>
<td>
<p>An integer for the number of trees contained in
the ensemble.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_prior_terminal_node_coef">prior_terminal_node_coef</code></td>
<td>
<p>A coefficient for the prior probability that
a node is a terminal node.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_prior_terminal_node_expo">prior_terminal_node_expo</code></td>
<td>
<p>An exponent in the prior probability that
a node is a terminal node.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_prior_outcome_range">prior_outcome_range</code></td>
<td>
<p>A positive value that defines the width of a prior
that the predicted outcome is within a certain range. For regression it is
related to the observed range of the data; the prior is the number of standard
deviations of a Gaussian distribution defined by the observed range of the
data. For classification, it is defined as the range of +/-3 (assumed to be
on the logit scale). The default value is 2.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_mtry">mtry</code></td>
<td>
<p>A number for the number (or proportion) of predictors that will
be randomly sampled at each split when creating the tree models
(specific engines only).</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A number for the rate at which the boosting algorithm adapts
from iteration-to-iteration (specific engines only). This is sometimes referred to
as the shrinkage parameter.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_loss_reduction">loss_reduction</code></td>
<td>
<p>A number for the reduction in the loss function required
to split further (specific engines only).</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_sample_size">sample_size</code></td>
<td>
<p>A number for the number (or proportion) of data that is
exposed to the fitting routine. For <code>xgboost</code>, the sampling is done at
each iteration while <code>C5.0</code> samples once during training.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_stop_iter">stop_iter</code></td>
<td>
<p>The number of iterations without improvement before
stopping (specific engines only).</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_committees">committees</code></td>
<td>
<p>A non-negative integer (no greater than 100) for the number
of members of the ensemble.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_neighbors">neighbors</code></td>
<td>
<p>An integer between zero and nine for the number of training
set instances that are used to adjust the model-based prediction.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_max_rules">max_rules</code></td>
<td>
<p>The largest number of rules.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_regularization_method">regularization_method</code></td>
<td>
<p>A character string for the type of regularized
estimation. Possible values are: &quot;<code>diagonal</code>&quot;, &quot;<code>min_distance</code>&quot;,
&quot;<code>shrink_cov</code>&quot;, and &quot;<code>shrink_mean</code>&quot; (<code>sparsediscrim</code> engine only).</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_frac_common_cov">frac_common_cov</code>, <code id="update.bag_mars_+3A_frac_identity">frac_identity</code></td>
<td>
<p>Numeric values between zero and one.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_select_features">select_features</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE.</code> If <code>TRUE</code>, the model has the
ability to eliminate a predictor (via penalization). Increasing
<code>adjust_deg_free</code> will increase the likelihood of removing predictors.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_adjust_deg_free">adjust_deg_free</code></td>
<td>
<p>If <code>select_features = TRUE</code>, then acts as a multiplier
for smoothness. Increase this beyond 1 to produce smoother models.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_mixture">mixture</code></td>
<td>
<p>A number between zero and one (inclusive) denoting the
proportion of L1 regularization (i.e. lasso) in the model.
</p>

<ul>
<li> <p><code>mixture = 1</code> specifies a pure lasso model,
</p>
</li>
<li> <p><code>mixture = 0</code>  specifies a ridge regression model, and
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;0 &lt; mixture &lt; 1&#8288;</code> specifies an elastic net model, interpolating lasso and ridge.
</p>
</li></ul>

<p>Available for specific engines only.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_dropout">dropout</code></td>
<td>
<p>A number between 0 (inclusive) and 1 denoting the proportion
of model parameters randomly set to zero during model training.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_activation">activation</code></td>
<td>
<p>A single character string denoting the type of relationship
between the original predictors and the hidden unit layer. The activation
function between the hidden and output layers is automatically set to either
&quot;linear&quot; or &quot;softmax&quot; depending on the type of outcome. Possible values
depend on the engine being used.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_smoothness">smoothness</code></td>
<td>
<p>An non-negative number representing the the relative
smoothness of the class boundary. Smaller examples result in model flexible
boundaries and larger values generate class boundaries that are less
adaptable</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_laplace">Laplace</code></td>
<td>
<p>A non-negative value for the Laplace correction to smoothing
low-frequency counts.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_weight_func">weight_func</code></td>
<td>
<p>A <em>single</em> character for the type of kernel function used
to weight distances between samples. Valid choices are: <code>"rectangular"</code>,
<code>"triangular"</code>, <code>"epanechnikov"</code>, <code>"biweight"</code>, <code>"triweight"</code>,
<code>"cos"</code>, <code>"inv"</code>, <code>"gaussian"</code>, <code>"rank"</code>, or <code>"optimal"</code>.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_dist_power">dist_power</code></td>
<td>
<p>A single number for the parameter used in
calculating Minkowski distance.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_predictor_prop">predictor_prop</code></td>
<td>
<p>The maximum proportion of original predictors that can
have <em>non-zero</em> coefficients for each PLS component (via regularization).
This value is used for all PLS components for X.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_num_comp">num_comp</code></td>
<td>
<p>The number of PLS components to retain.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_dist">dist</code></td>
<td>
<p>A character string for the probability distribution of the
outcome. The default is &quot;weibull&quot;.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_cost">cost</code></td>
<td>
<p>A positive number for the cost of predicting a sample within
or on the wrong side of the margin</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_margin">margin</code></td>
<td>
<p>A positive number for the epsilon in the SVM insensitive
loss function (regression only)</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_degree">degree</code></td>
<td>
<p>A positive number for polynomial degree.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_scale_factor">scale_factor</code></td>
<td>
<p>A positive number for the polynomial scaling factor.</p>
</td></tr>
<tr><td><code id="update.bag_mars_+3A_rbf_sigma">rbf_sigma</code></td>
<td>
<p>A positive number for radial basis function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated model specification.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# ------------------------------------------------------------------------------

model &lt;- C5_rules(trees = 10, min_n = 2)
model
update(model, trees = 1)
update(model, trees = 1, fresh = TRUE)



# ------------------------------------------------------------------------------

model &lt;- cubist_rules(committees = 10, neighbors = 2)
model
update(model, committees = 1)
update(model, committees = 1, fresh = TRUE)


model &lt;- pls(predictor_prop =  0.1)
model
update(model, predictor_prop = 1)
update(model, predictor_prop = 1, fresh = TRUE)


# ------------------------------------------------------------------------------

model &lt;- rule_fit(trees = 10, min_n = 2)
model
update(model, trees = 1)
update(model, trees = 1, fresh = TRUE)


model &lt;- boost_tree(mtry = 10, min_n = 3)
model
update(model, mtry = 1)
update(model, mtry = 1, fresh = TRUE)

param_values &lt;- tibble::tibble(mtry = 10, tree_depth = 5)

model %&gt;% update(param_values)
model %&gt;% update(param_values, mtry = 3)

param_values$verbose &lt;- 0
# Fails due to engine argument
# model %&gt;% update(param_values)

model &lt;- linear_reg(penalty = 10, mixture = 0.1)
model
update(model, penalty = 1)
update(model, penalty = 1, fresh = TRUE)

</code></pre>

<hr>
<h2 id='varying'>A placeholder function for argument values</h2><span id='topic+varying'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p><code><a href="#topic+varying">varying()</a></code> is used when a parameter will be specified at a later date.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varying()
</code></pre>

<hr>
<h2 id='varying_args.model_spec'>Determine varying arguments</h2><span id='topic+varying_args.model_spec'></span><span id='topic+varying_args.recipe'></span><span id='topic+varying_args.step'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p><code>varying_args()</code> takes a model specification or a recipe and returns a tibble
of information on all possible varying arguments and whether or not they
are actually varying.
</p>
<p>The <code>id</code> column is determined differently depending on whether a <code>model_spec</code>
or a <code>recipe</code> is used. For a <code>model_spec</code>, the first class is used. For
a <code>recipe</code>, the unique step <code>id</code> is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'model_spec'
varying_args(object, full = TRUE, ...)

## S3 method for class 'recipe'
varying_args(object, full = TRUE, ...)

## S3 method for class 'step'
varying_args(object, full = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varying_args.model_spec_+3A_object">object</code></td>
<td>
<p>A <code>model_spec</code> or a <code>recipe</code>.</p>
</td></tr>
<tr><td><code id="varying_args.model_spec_+3A_full">full</code></td>
<td>
<p>A single logical. Should all possible varying parameters be
returned? If <code>FALSE</code>, then only the parameters that
are actually varying are returned.</p>
</td></tr>
<tr><td><code id="varying_args.model_spec_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with columns for the parameter name (<code>name</code>), whether it
contains <em>any</em> varying value (<code>varying</code>), the <code>id</code> for the object (<code>id</code>),
and the class that was used to call the method (<code>type</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# List all possible varying args for the random forest spec
rand_forest() %&gt;% varying_args()

# mtry is now recognized as varying
rand_forest(mtry = varying()) %&gt;% varying_args()

# Even engine specific arguments can vary
rand_forest() %&gt;%
  set_engine("ranger", sample.fraction = varying()) %&gt;%
  varying_args()

# List only the arguments that actually vary
rand_forest() %&gt;%
  set_engine("ranger", sample.fraction = varying()) %&gt;%
  varying_args(full = FALSE)

rand_forest() %&gt;%
  set_engine(
    "randomForest",
    strata = Class,
    sampsize = varying()
  ) %&gt;%
  varying_args()

</code></pre>

<hr>
<h2 id='xgb_train'>Boosted trees via xgboost</h2><span id='topic+xgb_train'></span><span id='topic+xgb_predict'></span>

<h3>Description</h3>

<p><code>xgb_train()</code> and <code>xgb_predict()</code> are wrappers for <code>xgboost</code> tree-based
models where all of the model arguments are in the main function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgb_train(
  x,
  y,
  weights = NULL,
  max_depth = 6,
  nrounds = 15,
  eta = 0.3,
  colsample_bynode = NULL,
  colsample_bytree = NULL,
  min_child_weight = 1,
  gamma = 0,
  subsample = 1,
  validation = 0,
  early_stop = NULL,
  counts = TRUE,
  event_level = c("first", "second"),
  ...
)

xgb_predict(object, new_data, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgb_train_+3A_x">x</code></td>
<td>
<p>A data frame or matrix of predictors</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_y">y</code></td>
<td>
<p>A vector (factor or numeric) or matrix (numeric) of outcome data.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_max_depth">max_depth</code></td>
<td>
<p>An integer for the maximum depth of the tree.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_nrounds">nrounds</code></td>
<td>
<p>An integer for the number of boosting iterations.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_eta">eta</code></td>
<td>
<p>A numeric value between zero and one to control the learning rate.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_colsample_bynode">colsample_bynode</code></td>
<td>
<p>Subsampling proportion of columns for each node
within each tree. See the <code>counts</code> argument below. The default uses all
columns.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_colsample_bytree">colsample_bytree</code></td>
<td>
<p>Subsampling proportion of columns for each tree.
See the <code>counts</code> argument below. The default uses all columns.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_min_child_weight">min_child_weight</code></td>
<td>
<p>A numeric value for the minimum sum of instance
weights needed in a child to continue to split.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_gamma">gamma</code></td>
<td>
<p>A number for the minimum loss reduction required to make a
further partition on a leaf node of the tree</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_subsample">subsample</code></td>
<td>
<p>Subsampling proportion of rows. By default, all of the
training data are used.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_validation">validation</code></td>
<td>
<p>The <em>proportion</em> of the data that are used for performance
assessment and potential early stopping.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_early_stop">early_stop</code></td>
<td>
<p>An integer or <code>NULL</code>. If not <code>NULL</code>, it is the number of
training iterations without improvement before stopping. If <code>validation</code> is
used, performance is base on the validation set; otherwise, the training set
is used.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_counts">counts</code></td>
<td>
<p>A logical. If <code>FALSE</code>, <code>colsample_bynode</code> and
<code>colsample_bytree</code> are both assumed to be <em>proportions</em> of the proportion of
columns affects (instead of counts).</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_event_level">event_level</code></td>
<td>
<p>For binary classification, this is a single string of either
<code>"first"</code> or <code>"second"</code> to pass along describing which level of the outcome
should be considered the &quot;event&quot;.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_...">...</code></td>
<td>
<p>Other options to pass to <code>xgb.train()</code> or xgboost's method for <code>predict()</code>.</p>
</td></tr>
<tr><td><code id="xgb_train_+3A_new_data">new_data</code></td>
<td>
<p>A rectangular data object, such as a data frame.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A fitted <code>xgboost</code> object.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
