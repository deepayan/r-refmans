<!DOCTYPE html><html lang="en"><head><title>Help for package PerfMeas</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PerfMeas}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#PerfMeas-package'>
<p>Performance Measures for Ranking and Classification Tasks</p></a></li>
<li><a href='#AUC.measures'>
<p>AUC measures</p></a></li>
<li><a href='#AUPRC'>
<p>Area Under the Precision Recall Curve</p></a></li>
<li><a href='#example.data'><p> Datasets used in the examples of the package</p></a></li>
<li><a href='#F.measures'>
<p>F-measures</p></a></li>
<li><a href='#get.all.nodes.by.depth'>
<p>Getting nodes by their depth</p></a></li>
<li><a href='#graphics'>
<p>Graphics function to plot precision/recall or f.score/recall curves</p></a></li>
<li><a href='#PXR'>
<p>Precision at a given recall level measures</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Performance Measures for Ranking and Classification Tasks</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-09-09</td>
</tr>
<tr>
<td>Author:</td>
<td>Giorgio Valentini [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Giorgio Valentini &lt;valentini@di.unimi.it&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of different performance measures for classification and ranking tasks  including  Area Under the Receiving Characteristic Curve (AUROC) and Area Under the Precision Recall Curve (AUPRC), precision at a given recall, F-score for single and multiple classes.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Depends:</td>
<td>limma, graph, RBGL</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-09-13 17:41:31 UTC; valenti</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-14 06:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='PerfMeas-package'>
Performance Measures for Ranking and Classification Tasks
</h2><span id='topic+PerfMeas-package'></span><span id='topic+PerfMeas'></span>

<h3>Description</h3>

<p>Metrics for ranking and classification tasks: Area Under the Receiving Operating Characteristic Curve (AUROC), Area Under the Precision and Recall Curve (AUPRC), F-scores, and precision at given recall level are implemented.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> PerfMeas</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.2.5</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2022-09-09</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2) </td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>This package implements a set of functions to estimate the AUC, F-score, precision, recall, specificity, accuracy according to the 0/1 loss, and precision at given recall level for ranking and classification problems.
</p>
<p>Functions to compute the above measures for single classes or for sets of classes are provided. 
</p>


<h3>Author(s)</h3>

<p><em>Giorgio Valentini</em>
</p>
<p>DI, Dipartimento di Informatica
</p>
<p>Universita' degli Studi di Milano
</p>
<p><a href="mailto:%7Bvalentini%7D@di.unimi.it">{valentini}@di.unimi.it</a>
</p>
<p>Maintainer: 
<em>Giorgio Valentini</em> 
</p>

<hr>
<h2 id='AUC.measures'>
AUC measures
</h2><span id='topic+AUC.measures'></span><span id='topic+AUC.single'></span><span id='topic+AUC.single.over.classes'></span><span id='topic+AUC.n.single'></span><span id='topic+AUC.n.single.over.classes'></span><span id='topic+compute.mean.AUC.single.over.classes'></span>

<h3>Description</h3>

<p>Set of functions to compute the Area Under the ROC Curve (AUC)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AUC.single(pred, labels)
AUC.single.over.classes(target, predicted, g, root = "00")
AUC.n.single(pred, labels, n=50)
AUC.n.single.over.classes(target, predicted, g, n=50, root = "00")
compute.mean.AUC.single.over.classes(y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="AUC.measures_+3A_pred">pred</code></td>
<td>

<p>numeric vector (scores) of  the values  of the predicted labels
</p>
</td></tr>
<tr><td><code id="AUC.measures_+3A_labels">labels</code></td>
<td>

<p>numeric vector of the true labels (0 negative, 1 positive examples)</p>
</td></tr>
<tr><td><code id="AUC.measures_+3A_target">target</code></td>
<td>

<p>matrix with the target multilabels: rows correspond to examples and columns to classes.
target[i,j] = 1 if example i belongs to class j, target[i,j] = 0 otherwise.
</p>
</td></tr>
<tr><td><code id="AUC.measures_+3A_predicted">predicted</code></td>
<td>

<p>a numeric matrix with predicted values (scores): rows correspond to examples and columns to classes.
</p>
</td></tr>
<tr><td><code id="AUC.measures_+3A_g">g</code></td>
<td>

<p>a graph of class <em>graphNEL</em> (package <span class="pkg">graph</span>) of the classes. If g is missing no per.level results are computed
</p>
</td></tr>
<tr><td><code id="AUC.measures_+3A_n">n</code></td>
<td>

<p>number of negatives (def=50)
</p>
</td></tr>
<tr><td><code id="AUC.measures_+3A_root">root</code></td>
<td>

<p>the name of the root node (def. &quot;00&quot;)
</p>
</td></tr>
<tr><td><code id="AUC.measures_+3A_y">y</code></td>
<td>

<p>a list of lists. The components of the outer list is a list returned from the function <code>AUC.single.over.classes</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>AUC.single</code> computes the AUC for a single class.
</p>
<p><code>AUC.single.over.classes</code> computes AUC for a set of classes, including their average values across classes and
the average values across the levels of the hierarchy (if any); level 1 classes are at distance 1 from the root,
level 2 the second level, till to last level correponding to the leaves. Note that if the argument g is missing no   per-level values are computed.
</p>
<p><code>AUC.n.single</code> computes the AUCn for a single class, i.e. the AUC by considering only the first n top ranked negatives, where n is the absolute
number of negative examples receiving the highest scores.
</p>
<p><code>AUC.n.single.over.classes</code> computes AUCn for a set of classes, including their average values across classes and
the average values across the levels of the hierarchy (if any); level 1 classes are at distance 1 from the root,
level 2 the second level, till to last level correponding to the leaves. Note that if the argument g is missing no   per-level values are computed.
</p>
<p><code>compute.mean.AUC.single.over.classes</code> compute means across folds of AUC.single.over.classes. It can be used to automatically computed average values (for each class, level, or average across classes) across folds.
</p>


<h3>Value</h3>

<p><code>AUC.single</code> returns a numeric value corresponding to the AUC.
</p>
<p><code>AUC.single.over.classes</code> returns  a list with three elements:
</p>
<table role = "presentation">
<tr><td><code>- average</code></td>
<td>
<p>the average AUC across classes</p>
</td></tr>
<tr><td><code>- per.level</code></td>
<td>
<p>a named vector with average  AUC for each level of the hierarchy; names correspond to levels</p>
</td></tr>
<tr><td><code>- per.class</code></td>
<td>
<p>a named vector with AUC for each class; names correspond to classes</p>
</td></tr>
</table>
<p><code>AUC.n.single</code> returns a numeric value corresponding to the AUCn.
</p>
<p><code>AUC.n.single.over.classes</code> returns  a list with three elements:
</p>
<table role = "presentation">
<tr><td><code>- average</code></td>
<td>
<p>the average AUCn across classes</p>
</td></tr>
<tr><td><code>- per.level</code></td>
<td>
<p>a named vector with average  AUCn for each level of the hierarchy; names correspond to levels</p>
</td></tr>
<tr><td><code>- per.class</code></td>
<td>
<p>a named vector with AUCn for each class; names correspond to classes</p>
</td></tr>
</table>
<p><code>compute.mean.AUC.single.over.classes</code> returns a list obtained by averaging the results across folds of the input y. 
The components are:
</p>
<table role = "presentation">
<tr><td><code>- average</code></td>
<td>
<p>the average AUC across classes</p>
</td></tr>
<tr><td><code>- per.level</code></td>
<td>
<p>a named vector with average  AUC for each level of the hierarchy; names correspond to levels</p>
</td></tr>
<tr><td><code>- per.class</code></td>
<td>
<p>a named vector with AUC for each class; names correspond to classes</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+F.measures">F.measures</a></code>, <code><a href="#topic+PXR">PXR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># preparing pseudo.random scores and target-labels for examples: 100 examples
# and 10 classes
Scores &lt;- matrix(runif(1000),nrow=100);
Targets &lt;- matrix(integer(1000),nrow=100);
Targets[Scores&gt;0.5] &lt;- 1;
# adding noise to scores
Scores &lt;- Scores + matrix(rnorm(1000, sd=0.3),nrow=100);
colnames(Scores) &lt;-colnames(Targets) &lt;- LETTERS[1:10];
# getting scores and labels of class "A"
scores &lt;- Scores[,"A"];
labels &lt;- Targets[,"A"];
# AUC for a single class
AUC.single(scores,labels);
# AUC for the 10 classes
AUC.single.over.classes(Targets, Scores);
# AUCn for a single class considering only the first top scored negatives
AUC.n.single(scores,labels, n=20);
# AUCn for the 10 classes considering only the first top scored negatives
AUC.n.single.over.classes(Targets, Scores, n=20);
</code></pre>

<hr>
<h2 id='AUPRC'>
Area Under the Precision Recall Curve
</h2><span id='topic+AUPRC'></span><span id='topic+trap.rule.integral'></span>

<h3>Description</h3>

<p>Functions to compute the Area Under the Precision Recall Curve (AUPRC) and the Area Under the F-score Recall Curve (AUFRC)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AUPRC(z, comp.precision=TRUE)
trap.rule.integral(x,y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="AUPRC_+3A_z">z</code></td>
<td>

<p>a list of lists. The components of the outer list is a list returned from the function <code>precision.at.all.recall.levels</code> that reports precision, recall and f-score results at different levels for different methods or tasks.
</p>
</td></tr>
<tr><td><code id="AUPRC_+3A_comp.precision">comp.precision</code></td>
<td>

<p>boolean. It TRUE (default) the AUPRC is computed otherwise the area under the F-score curve is computed
</p>
</td></tr>
<tr><td><code id="AUPRC_+3A_x">x</code></td>
<td>

<p>vector of the x values in increasing order
</p>
</td></tr>
<tr><td><code id="AUPRC_+3A_y">y</code></td>
<td>

<p>vector of the corresponding y=f(x) values
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>AUPRC</code> computes the Area Under the Precision Recall Curve or the Area Under the F-score Recall Curve (AUFRC) 
for multiple curves by using the output of the function <code>precision.at.all.recall.levels</code>.
</p>
<p>The function  <code>trap.rule.integral</code> implements the trapezoidal rule of integration and can be used to compute the integral of any empirical function expressed as a set of pair values (a vector of <code>x</code> values and a vector of <code>y = f(x)</code> values). In particular if <code>x</code> is the recall (with values in ascending order) and <code>y</code> the corresponding precision, <code>trap.rule.integral</code> copmutes the AUPRC.
</p>


<h3>Value</h3>

<p><code>AUPRC</code> returns the value of the AUPRC (if the argument <code>comp.precision = TRUE</code>), otherwise the value of the AUFRC.
</p>
<p><code>trap.rule.integral</code>  returns the value of the integral.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+AUC.measures">AUC.measures</a></code>, <code><a href="#topic+PXR">PXR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># loading matrices of scores an correponding table of classes
data(T);
data(Scores);
res=list();
classes=1:10
# computing precision recall values
for (j in classes) res=c(res, list(precision.at.all.recall.levels(Scores[,j], T[,j])));
names(res)&lt;-seq(0.1, 1, by=0.1);
# computing AUPRC
AUPRC (res, comp.precision=TRUE);
# computing AU F-score recall curve
AUPRC (res, comp.precision=TRUE);

# Loading precision at given recall levels for different methods
data(PrecRec);
# computing AUPRC for different methods
x &lt;- seq(0.1, 1, by=0.1);
res &lt;- numeric(nrow(PrecRec));
names(res) &lt;- rownames(PrecRec);
for (i in 1:nrow(PrecRec))  
  res[i] &lt;- trap.rule.integral(x, PrecRec[i,]);
print(res);

</code></pre>

<hr>
<h2 id='example.data'> Datasets used in the examples of the package</h2><span id='topic+example.data'></span><span id='topic+Scores'></span><span id='topic+T'></span><span id='topic+PrecRec'></span>

<h3>Description</h3>

<p>Collection of datasets used in the examples of the package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Scores)
data(T)
data(PrecRec)
</code></pre>


<h3>Details</h3>

<p>The <code>T</code> data is a named 1901 X 10 matrix whose rows correspondes to yest genes, while columns correspond to 
10 FunCat (Functional Categories) classes. If T_ij = 1 gene i belong to class j, if T_ij = 0 gene i does not belong to class j.
The <code>Scores</code> data is a named 1901 X 10 matrix representing scores (likelihood) that a given gene belongs to a given class: higher the value higher the likelihood. 
<code>PrecRec</code> is  a matrix representing precision at 10 different recall values of 7 methods for gene function prediction.
</p>

<hr>
<h2 id='F.measures'>
F-measures
</h2><span id='topic+F.measures'></span><span id='topic+F.measure.single'></span><span id='topic+F.measure.single.over.classes'></span><span id='topic+compute.mean.F.measure.single.over.classes'></span>

<h3>Description</h3>

<p>Set of functions to compute the F-measure, precision, recall, specificity and 0/1 loss accuracy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>F.measure.single(pred, labels)
F.measure.single.over.classes(target, predicted, g, root = "00")
compute.mean.F.measure.single.over.classes(y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="F.measures_+3A_pred">pred</code></td>
<td>

<p>vector of the predicted labels. 0  stands for negative and 1 for positive
</p>
</td></tr>
<tr><td><code id="F.measures_+3A_labels">labels</code></td>
<td>

<p>vector of the true labels. 0  stands for negative and 1 for positive
</p>
</td></tr>
<tr><td><code id="F.measures_+3A_target">target</code></td>
<td>

<p>matrix with the target multilabels. 0  stands for negative and 1 for positive.
Rows correspond to examples and columns to classes.
</p>
</td></tr>
<tr><td><code id="F.measures_+3A_predicted">predicted</code></td>
<td>

<p>matrix with the predicted multilabels. 0  stands for negative and 1 for positive.
Rows correspond to examples and columns to classes.
</p>
</td></tr>
<tr><td><code id="F.measures_+3A_g">g</code></td>
<td>

<p>graph of the classes (object of class graphNEL, package <span class="pkg">graph</span>). 
If missing, no per level results are computed.
</p>
</td></tr>
<tr><td><code id="F.measures_+3A_root">root</code></td>
<td>

<p>the name of the root node (def. &quot;00&quot;) of the graph g.
</p>
</td></tr>
<tr><td><code id="F.measures_+3A_y">y</code></td>
<td>

<p>a list of lists. The components of the outer list is a list returned from the function <code>F.measure.single.over.classes</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>F.measure.single</code> computes the F.score, precision, recall, specificity and accuracy for a single class.
</p>
<p><code>F.measure.single.over.classes</code>  computes  precision, recall, specificity, accuracy and F-measure for a set of classes. In particualr it computes the correponding average values across classes, the average values across levels of the hierarchy of the classes (if any), and the values of the measures for each class. Note that if there is no hierarchy between classes (represented by the graph g), you can miss the g parameter and no per-level values are computed.
</p>
<p><code>compute.mean.F.measure.single.over.classes</code> computes means across folds <br /> 
of F.measure.single.over.classes.   
This function could be useful in cross-validated or multiple hold-out experimental settings.
</p>


<h3>Value</h3>

<p><code>F.measure.single</code> returns a named numeric vector with six elements:
</p>
<table role = "presentation">
<tr><td><code>- P</code></td>
<td>
<p>precision</p>
</td></tr>
<tr><td><code>- R</code></td>
<td>
<p>recall (sensitivity)</p>
</td></tr>
<tr><td><code>- S</code></td>
<td>
<p>specificity</p>
</td></tr>
<tr><td><code>- F</code></td>
<td>
<p>F measure</p>
</td></tr>
<tr><td><code>- A</code></td>
<td>
<p>0/1 loss accuracy</p>
</td></tr>
<tr><td><code>- npos</code></td>
<td>
<p>number of positive examples</p>
</td></tr>
</table>
<p><code>F.measure.single.over.classes</code>  returns  a list with three elements:
</p>
<table role = "presentation">
<tr><td><code>- average</code></td>
<td>
<p>a named vector with the average precision, recall, specificity, F-measure, accuracy and average number of positive examples across classes.</p>
</td></tr>
<tr><td><code>- per.level</code></td>
<td>
<p>a named matrix with average  precision, recall, specificity, F-measure and accuracy for each level of the hierarchy. Named rows correspond to levels, named columns correspond respectively to precision, recall, specificity, F-measure, accuracy and number of positive examples.</p>
</td></tr>
<tr><td><code>- per.class</code></td>
<td>
<p>a named matrix with  precision, recall, specificity, F-measure, accuracy and number of positive examples for each class. Named rows correspond to classes, named columns correspond respectively to precision, recall, specificity, F-measure, accuracy and and number of positive examples.</p>
</td></tr>
</table>
<p><code>compute.mean.F.measure.single.over.classes</code>  returns a list obtained by averaging the results across folds of the input y. 
The components are:
</p>
<table role = "presentation">
<tr><td><code>- average</code></td>
<td>
<p>a named vector with the average precision, recall, specificity, F-measure and accuracy across classes across folds.</p>
</td></tr>
<tr><td><code>- per.level</code></td>
<td>
<p>a named matrix with average  precision, recall, specificity, F-measure and accuracy for each level of the hierarchy across folds. Named rows correspond to levels, named columns correspond respectively to precision, recall, specificity, F-measure and accuracy</p>
</td></tr>
<tr><td><code>- per.class</code></td>
<td>
<p>a named matrix with  precision, recall, specificity, F-measure and accuracy for each class across folds. Named rows correspond to classes, named columns correspond respectively to precision, recall, specificity, F-measure and accuracy.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+AUC.measures">AUC.measures</a></code>, <code><a href="#topic+PXR">PXR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># preparing pseudo-random predictions and target-labels for examples: 100 examples
# and 10 classes
Scores &lt;- matrix(runif(1000),nrow=100);
Targets &lt;- Pred &lt;- matrix(integer(1000),nrow=100);
Targets[Scores&gt;0.5] &lt;- 1;
# adding noise to scores
Scores &lt;- Scores + matrix(rnorm(1000, sd=0.3),nrow=100);
Pred[Scores&gt;0.5] &lt;- 1;
colnames(Pred) &lt;-colnames(Targets) &lt;- LETTERS[1:10];
# getting predictions and labels of class "A"
pred &lt;- Pred[,"A"];
labels &lt;- Targets[,"A"];
# F.score and other metrics for a single class
F.measure.single(pred,labels);
# F.score and other metrics for the 10 classes
F.measure.single.over.classes(Targets, Pred);
</code></pre>

<hr>
<h2 id='get.all.nodes.by.depth'>
Getting nodes by their depth
</h2><span id='topic+get.all.nodes.by.depth'></span>

<h3>Description</h3>

<p>Grouping classes by level in a given hierarchy. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get.all.nodes.by.depth(g, root = "00")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get.all.nodes.by.depth_+3A_g">g</code></td>
<td>

<p>graph of the classes (object of class graphNEL, package <span class="pkg">graph</span>). 
</p>
</td></tr>
<tr><td><code id="get.all.nodes.by.depth_+3A_root">root</code></td>
<td>

<p>name of the root node (def. 00)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The minimum paths between the &ldquo;root&rdquo; and all the other classes/nodes are computed.  Levels are numbered from 1 in increasing order by their distance from the &ldquo;root&rdquo; class.</p>


<h3>Value</h3>

<p>a  list of the nodes, grouped w.r.t. the distance from the root.
The first element of the list corresponds to the nodes at distance 1, 
the second to nodes at distance 2 and so on.
</p>

<hr>
<h2 id='graphics'>
Graphics function to plot precision/recall or f.score/recall curves
</h2><span id='topic+precision.recall.curves.plot'></span><span id='topic+performance.curves.plot'></span>

<h3>Description</h3>

<p>Function to plot multiple precision/recall or f.score/recall curves
</p>


<h3>Usage</h3>

<pre><code class='language-R'>precision.recall.curves.plot(y, range=seq(from=0, to=1, by=0.1), 
curve.names=1:length(y), cex.val=0.6, f="", height=9, width=11,
col=c("black","red1","blue1","green1","darkgrey","brown1","yellow1","orange1",
"red4","blue4","green4","lightgrey","brown4","yellow4","orange4"),
line.type=1, leg=TRUE, pos=c(range[length(range)-2], range[length(range)]), do.grid=TRUE,
plot.precision=TRUE, trap.rule=TRUE)

performance.curves.plot(m, x.range=seq(from=0.1, to=1, by=0.1), 
y.range=c(0,1), curve.names=1:nrow(m), cex.val=0.6, f="", height=9, width=11,
col=c("black","red1","blue1","green1","darkgrey","brown1","yellow1","orange1",
"red4","blue4","green4","lightgrey","brown4","yellow4","orange4"), line.type=1, 
patch.type=1:16, leg=TRUE, pos=c(x.range[length(x.range)-2], y.range[2]), do.grid=TRUE,
x.label="Recall", y.label="Precision")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="graphics_+3A_y">y</code></td>
<td>

<p>a list of lists. 
Each component list is a list returned from <code>precision.at.all.recall.levels</code>
that reports precision and recall results at different levels for different methods or tasks
</p>
</td></tr>
<tr><td><code id="graphics_+3A_range">range</code></td>
<td>

<p>numeric vector of the precision/recall values to be represented (def: values between 0 and 1 step 0.1)
</p>
</td></tr>
<tr><td><code id="graphics_+3A_curve.names">curve.names</code></td>
<td>

<p>names of the compared methods to be reported in the legenda (def: numbers)
</p>
</td></tr>
<tr><td><code id="graphics_+3A_cex.val">cex.val</code></td>
<td>

<p>magnification value for characters (def. 0.6)
</p>
</td></tr>
<tr><td><code id="graphics_+3A_f">f</code></td>
<td>

<p>file name. If is given, an encapsulated postscript file is created, otherwise the output is rendered on
a window.
</p>
</td></tr>
<tr><td><code id="graphics_+3A_height">height</code></td>
<td>

<p>relative heigth of the graph (def. 9)
</p>
</td></tr>
<tr><td><code id="graphics_+3A_width">width</code></td>
<td>

<p>relative width of the graph (def. 11)
</p>
</td></tr>
<tr><td><code id="graphics_+3A_col">col</code></td>
<td>

<p>colors of the lines. 14 different colors are given as default, but any vector of color from colors() (package <span class="pkg">graphics</span>) can be used. Colors are recycled if length(col) &lt; length(y).
</p>
</td></tr>
<tr><td><code id="graphics_+3A_line.type">line.type</code></td>
<td>

<p>type of the line. Any valid vector of integer can be assigned (values between 1  and 6, see lty in par(), package <span class="pkg">graphics</span> for details). Values are recycled if length(line.type) &lt; length(y). Def.: 1 (solid lines).
</p>
</td></tr>
<tr><td><code id="graphics_+3A_leg">leg</code></td>
<td>
<p> boolean: if TRUE (def.) a legend is depicted.</p>
</td></tr>
<tr><td><code id="graphics_+3A_pos">pos</code></td>
<td>
<p>coordinates of the position of the legend.</p>
</td></tr>
<tr><td><code id="graphics_+3A_plot.precision">plot.precision</code></td>
<td>
<p> boolean: if TRUE (def.) precision/recall curves are plotted, othewise f-score/recall curves.</p>
</td></tr>
<tr><td><code id="graphics_+3A_trap.rule">trap.rule</code></td>
<td>
<p> boolean: if TRUE (def.) the integral of the curves are computed.</p>
</td></tr>
<tr><td><code id="graphics_+3A_m">m</code></td>
<td>
<p> a numeric matrix. Rows correspond to different methods and columns to precision or f-score at given recall values </p>
</td></tr>
<tr><td><code id="graphics_+3A_x.range">x.range</code></td>
<td>
<p>vector of the recall values to be represented</p>
</td></tr>
<tr><td><code id="graphics_+3A_y.range">y.range</code></td>
<td>
<p>vector with 2 elements: range of the precision/f-score to be represented </p>
</td></tr>
<tr><td><code id="graphics_+3A_patch.type">patch.type</code></td>
<td>
<p> numeric vector corresponding to the symbols to be plotted for different recall values (def. 1:16)</p>
</td></tr>
<tr><td><code id="graphics_+3A_do.grid">do.grid</code></td>
<td>
<p>a bollen value indicating whether a grid will be plotted or not (def: TRUE)</p>
</td></tr>
<tr><td><code id="graphics_+3A_x.label">x.label</code></td>
<td>
<p>label of the abscissa (def: Recall)</p>
</td></tr>
<tr><td><code id="graphics_+3A_y.label">y.label</code></td>
<td>
<p>label of the ordinate (def: Precision)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>precision.recall.curves.plot</code> plots multiple precision/recall curves (or f-score/recall curves, if the argument <code>plot.precision=FALSE</code>) by using the output of the function <code>precision.at.all.recall.levels</code>, that compute several precison/recall pairs by moving the threshold from the lowest to the highest score achieved by each example.
</p>
<p>The function <code>performance.curves.plot</code> plots precision of F-score/recall curves ofr a predefined set of recall levels.
Thsi function can be useful to plot and to compare the average results between methods across multiple classes.
</p>
<p>The curves can differ by color, type of line and for <code>performance.curves.plot</code> for each recall value a symbol can be also plotted.  A legend can be automatically constructed.
</p>


<h3>Value</h3>

<p>The functions output  a graphic file either on a window or on an encapsulated postscript file.
The function <code>precision.recall.curves.plot</code>, if the parameter <code>trap.rule = TRUE</code> (default), outputs a vector with the AUPRC (or the Area Under the F-score Curve if the parameter <code>plot.precision=FALSE</code>) for each curve.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># loading  an example matrix of scores an the correponding table of classes
data(T);
data(Scores);
res=list();
classes=c(1,2,7,8)
# computing precison recall values
for (j in classes) res=c(res, list(precision.at.all.recall.levels(Scores[,j], T[,j])));
# plotting precision/recall curves
precision.recall.curves.plot(res,curve.names=colnames(T)[classes], 
pos=c(0.7,1), plot.precision=TRUE);
# black and white version
precision.recall.curves.plot(res,curve.names=colnames(T)[classes], pos=c(0.7,1), 
plot.precision=TRUE, line.type=1:4, col=1);
# plotting f-score/recall curves
precision.recall.curves.plot(res,curve.names=colnames(T)[classes], pos=c(0.7,1), 
plot.precision=FALSE);
# black and white version
precision.recall.curves.plot(res,curve.names=colnames(T)[classes], pos=c(0.7,1), 
plot.precision=TRUE, line.type=1:4, col=1);
</code></pre>

<hr>
<h2 id='PXR'>
Precision at a given recall level measures
</h2><span id='topic+PXR'></span><span id='topic+precision.at.recall.level'></span><span id='topic+precision.at.recall.level.over.classes'></span><span id='topic+precision.at.multiple.recall.level'></span><span id='topic+precision.at.multiple.recall.level.over.classes'></span><span id='topic+precision.at.all.recall.levels'></span>

<h3>Description</h3>

<p>Set of functions to compute the precision at fixed recall levels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>precision.at.recall.level(scores, labels, rec.level = 0.2)
precision.at.recall.level.over.classes(target, predicted,  
                          g, rec.level = 0.2, root = "00")
precision.at.multiple.recall.level(scores, labels, 
           rec.levels = seq(from = 0.1, to = 1, by = 0.1))
precision.at.multiple.recall.level.over.classes(target, 
   predicted, rec.levels = seq(from = 0.1, to = 1, by = 0.1))
precision.at.all.recall.levels(scores, labels, resolution=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PXR_+3A_scores">scores</code></td>
<td>

<p>vector of the predicted scores in [0,1]
</p>
</td></tr>
<tr><td><code id="PXR_+3A_labels">labels</code></td>
<td>

<p>0/1 vector of the true labels 
</p>
</td></tr>
<tr><td><code id="PXR_+3A_rec.level">rec.level</code></td>
<td>

<p>rec.level: the desired recall level (def: 0.2)
</p>
</td></tr>
<tr><td><code id="PXR_+3A_target">target</code></td>
<td>

<p>matrix with the target multilabels; rows correspond to examples, columns to classes
</p>
</td></tr>
<tr><td><code id="PXR_+3A_predicted">predicted</code></td>
<td>

<p>matrix with the predicted multilabels; rows correspond to examples, columns to classes
</p>
</td></tr>
<tr><td><code id="PXR_+3A_g">g</code></td>
<td>

<p>graph of the classes (object of class graphNEL, package graph). 
If missing, no per level results are computed.
</p>
</td></tr>
<tr><td><code id="PXR_+3A_root">root</code></td>
<td>

<p>the name of the root node (def. &quot;00&quot;) of the graph g.
</p>
</td></tr>
<tr><td><code id="PXR_+3A_rec.levels">rec.levels</code></td>
<td>

<p>a vector with the desired recall levels (def. 0.1 to 1 by 0.1 step)
</p>
</td></tr>
<tr><td><code id="PXR_+3A_resolution">resolution</code></td>
<td>

<p>a number between 0 and 1 (def. 1). This represents the fraction of precision, recall and f-score values returned. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>precision.at.recall.level</code> computes the precision at a given recall level for a single class.
</p>
<p><code>precision.at.recall.level.over.classes</code> computes precision at a given recall level for a set of classes.
</p>
<p><code>precision.at.multiple.recall.level</code> computes the precision at multiple levels of recall for a single class.
</p>
<p><code>precision.at.multiple.recall.level.over.classes</code> computes the precision at multiple levels of recall for multiple classes.
</p>
<p><code>precision.at.all.recall.levels</code> compute the precision at all recall levels  for a single class.
It returns a pair of precision and recall values by moving a threshold from the lowest to the highest score: a number of precision and recall values equal to the number n of available examples is returned
if resolution=1, otherwise a number of values equal to n * resolution.
</p>


<h3>Value</h3>

<p><code>precision.at.recall.level</code> returns the precision at the requested recall
</p>
<p><code>precision.at.recall.level.over.classes</code> a list with three elements:
</p>
<table role = "presentation">
<tr><td><code>- average</code></td>
<td>
<p> the average precision at a given recall level across classes. </p>
</td></tr>
<tr><td><code>- per.level</code></td>
<td>
<p> a named vector with average  precision at a given recall level for each level of the hierarchy; names correspond to levels </p>
</td></tr>
<tr><td><code>- per.class</code></td>
<td>
<p> a named vector with precision at a given recall level for each class. Names correspond to classes</p>
</td></tr>
</table>
<p><code>precision.at.multiple.recall.level</code> a list with 2 elements:
</p>
<table role = "presentation">
<tr><td><code>- precisions</code></td>
<td>
<p>a vector with the precision at different recall levels</p>
</td></tr>
<tr><td><code>- f.score</code></td>
<td>
<p>a vector with the f-score at different recall levels</p>
</td></tr>
</table>
<p><code>precision.at.multiple.recall.level.over.classes</code> 
</p>
<table role = "presentation">
<tr><td><code>- PXR</code></td>
<td>
<p>a matrix with the precisions at different recall levels: rows are classes, columns    precisions at different recall levels</p>
</td></tr>
<tr><td><code>- avgPXR</code></td>
<td>
<p>a vector with the the average precisions at different recall levels across classes</p>
</td></tr>
</table>
<p><code>precision.at.all.recall.levels</code> a list with 3 elements:
</p>
<table role = "presentation">
<tr><td><code>- precision</code></td>
<td>
<p>precision at different thresholds</p>
</td></tr>
<tr><td><code>- recall</code></td>
<td>
<p>recall at different thresholds</p>
</td></tr>
<tr><td><code>- f.score</code></td>
<td>
<p>f.score at different thresholds</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+AUC.measures">AUC.measures</a></code>, <code><a href="#topic+F.measures">F.measures</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># preparing pseudo-random predictions and target-labels for examples: 
# 100 examples and 10 classes
Scores &lt;- matrix(runif(1000),nrow=100);
Targets &lt;- matrix(integer(1000),nrow=100);
Targets[Scores&gt;0.5] &lt;- 1;
# adding noise to scores
Scores &lt;- Scores + matrix(rnorm(1000, sd=0.3),nrow=100);
colnames(Scores) &lt;-colnames(Targets) &lt;- LETTERS[1:10];
# getting scores and labels of class "A"
scores &lt;- Scores[,"A"];
labels &lt;- Targets[,"A"];
# precsion at 0.4 recall level for class A
precision.at.recall.level(scores, labels, rec.level=0.4);
# precision at 0.4 recall level for all the 10 classes
precision.at.recall.level.over.classes(Targets, Scores, rec.level=0.4);
# precision at multiple recall levels for class A
levels &lt;- seq(from=0.1, to=1, by=0.1);
precision.at.multiple.recall.level(scores, labels, rec.levels=levels);
# precision at multiple recall levels for all the 10 classes
precision.at.multiple.recall.level.over.classes(Targets, Scores);
# precision, recall and f-score for a single class obtained 
# by moving the threshold across the examples
precision.at.all.recall.levels(scores, labels);

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
