<!DOCTYPE html><html><head><title>Help for package SuperLearner</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SuperLearner}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#create.Learner'><p>Factory for learner wrappers</p></a></li>
<li><a href='#create.SL.xgboost'><p>Factory for XGBoost SL wrappers</p></a></li>
<li><a href='#CV.SuperLearner'>
<p>Function to get V-fold cross-validated risk estimate for super learner</p></a></li>
<li><a href='#CVFolds'>
<p>Generate list of row numbers for each fold in the cross-validation</p></a></li>
<li><a href='#listWrappers'>
<p>list all wrapper functions in SuperLearner</p></a></li>
<li><a href='#plot.CV.SuperLearner'>
<p>Graphical display of the V-fold CV risk estimates</p></a></li>
<li><a href='#predict.SL.bartMachine'><p>bartMachine prediction</p></a></li>
<li><a href='#predict.SL.biglasso'><p>Prediction wrapper for SL.biglasso</p></a></li>
<li><a href='#predict.SL.glm'><p>Prediction for SL.glm</p></a></li>
<li><a href='#predict.SL.glmnet'><p>Prediction for an SL.glmnet object</p></a></li>
<li><a href='#predict.SL.kernelKnn'><p>Prediction for SL.kernelKnn</p></a></li>
<li><a href='#predict.SL.ksvm'><p>Prediction for SL.ksvm</p></a></li>
<li><a href='#predict.SL.lda'><p>Prediction wrapper for SL.lda</p></a></li>
<li><a href='#predict.SL.lm'><p>Prediction for SL.lm</p></a></li>
<li><a href='#predict.SL.qda'><p>Prediction wrapper for SL.qda</p></a></li>
<li><a href='#predict.SL.ranger'><p>Prediction wrapper for ranger random forests</p></a></li>
<li><a href='#predict.SL.speedglm'><p>Prediction for SL.speedglm</p></a></li>
<li><a href='#predict.SL.speedlm'><p>Prediction for SL.speedlm</p></a></li>
<li><a href='#predict.SL.xgboost'><p>XGBoost prediction on new data</p></a></li>
<li><a href='#predict.SuperLearner'><p>Predict method for SuperLearner object</p></a></li>
<li><a href='#recombineCVSL'>
<p>Recombine a CV.SuperLearner fit using a new metalearning method</p></a></li>
<li><a href='#recombineSL'><p>Recombine a SuperLearner fit using a new metalearning method</p></a></li>
<li><a href='#SampleSplitSuperLearner'><p>Super Learner Prediction Function</p></a></li>
<li><a href='#SL.bartMachine'><p>Wrapper for bartMachine learner</p></a></li>
<li><a href='#SL.biglasso'><p>SL wrapper for biglasso</p></a></li>
<li><a href='#SL.cforest'><p>cforest (party)</p></a></li>
<li><a href='#SL.glm'><p>Wrapper for glm</p></a></li>
<li><a href='#SL.glmnet'><p>Elastic net regression, including lasso and ridge</p></a></li>
<li><a href='#SL.kernelKnn'><p>SL wrapper for KernelKNN</p></a></li>
<li><a href='#SL.ksvm'><p>Wrapper for Kernlab's SVM algorithm</p></a></li>
<li><a href='#SL.lda'><p>SL wrapper for MASS:lda</p></a></li>
<li><a href='#SL.lm'><p>Wrapper for lm</p></a></li>
<li><a href='#SL.qda'><p>SL wrapper for MASS:qda</p></a></li>
<li><a href='#SL.ranger'><p>SL wrapper for ranger</p></a></li>
<li><a href='#SL.speedglm'><p>Wrapper for speedglm</p></a></li>
<li><a href='#SL.speedlm'><p>Wrapper for speedlm</p></a></li>
<li><a href='#SL.xgboost'><p>XGBoost SuperLearner wrapper</p></a></li>
<li><a href='#summary.CV.SuperLearner'>
<p>Summary Function for Cross-Validated Super Learner</p></a></li>
<li><a href='#SuperLearner'><p>Super Learner Prediction Function</p></a></li>
<li><a href='#SuperLearner.control'>
<p>Control parameters for the SuperLearner</p></a></li>
<li><a href='#SuperLearner.CV.control'>
<p>Control parameters for the cross validation steps in <code>SuperLearner</code></p></a></li>
<li><a href='#SuperLearnerNews'><p>Show the NEWS file for the SuperLearner package</p></a></li>
<li><a href='#trimLogit'>
<p>truncated-probabilities logit transformation</p></a></li>
<li><a href='#write.method.template'>
<p>Method to estimate the coefficients for the super learner</p></a></li>
<li><a href='#write.screen.template'>
<p>screening algorithms for SuperLearner</p></a></li>
<li><a href='#write.SL.template'>
<p>Wrapper functions for prediction algorithms in SuperLearner</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Super Learner Prediction</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0-29</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-06</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Eric Polley &lt;epolley@uchicago.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements the super learner prediction method and contains a
    library of prediction algorithms to be used in the super learner.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ecpolley/SuperLearner">https://github.com/ecpolley/SuperLearner</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14.0), nnls, gam (&ge; 1.15)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cvAUC, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>arm, bartMachine, biglasso, bigmemory, caret, class,
devtools, e1071, earth, gbm, genefilter, ggplot2, glmnet,
ipred, KernelKnn, kernlab, knitr, lattice, LogicReg, MASS,
mlbench, nloptr, nnet, party, polspline, prettydoc, quadprog,
randomForest, ranger, RhpcBLASctl, ROCR, rmarkdown, rpart, SIS,
speedglm, spls, sva, testthat, xgboost (&ge; 0.6)</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-20 15:39:36 UTC; epolley</td>
</tr>
<tr>
<td>Author:</td>
<td>Eric Polley [aut, cre],
  Erin LeDell [aut],
  Chris Kennedy [aut],
  Sam Lendle [ctb],
  Mark van der Laan [aut, ths]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-20 17:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='create.Learner'>Factory for learner wrappers</h2><span id='topic+create.Learner'></span>

<h3>Description</h3>

<p>Create custom learners and/or a sequence of learners with hyperparameter
combinations defined over a grid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create.Learner(base_learner, params = list(), tune = list(),
  env = parent.frame(), name_prefix = base_learner, detailed_names = F,
  verbose = F)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create.Learner_+3A_base_learner">base_learner</code></td>
<td>
<p>Character string of the learner function that will be
customized.</p>
</td></tr>
<tr><td><code id="create.Learner_+3A_params">params</code></td>
<td>
<p>List with parameters to customize.</p>
</td></tr>
<tr><td><code id="create.Learner_+3A_tune">tune</code></td>
<td>
<p>List of hyperparameter settings that will define custom learners.</p>
</td></tr>
<tr><td><code id="create.Learner_+3A_env">env</code></td>
<td>
<p>Environment in which to create the functions. Defaults
to the current environment (e.g. often the global environment).</p>
</td></tr>
<tr><td><code id="create.Learner_+3A_name_prefix">name_prefix</code></td>
<td>
<p>The prefix string for the name of each function that is
generated.</p>
</td></tr>
<tr><td><code id="create.Learner_+3A_detailed_names">detailed_names</code></td>
<td>
<p>Set to T to have the function names include the
parameter configurations.</p>
</td></tr>
<tr><td><code id="create.Learner_+3A_verbose">verbose</code></td>
<td>
<p>Display extra details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with expanded tuneGrid and the names of the created
functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Create a randomForest learner with ntree set to 1000 rather than the
# default of 500.
create_rf = create.Learner("SL.randomForest", list(ntree = 1000))
create_rf
sl = SuperLearner(Y = Y, X = X, SL.library = create_rf$names, family = binomial())
sl
# Clean up global environment.
rm(list = create_rf$names)
# Create a randomForest learner that optimizes over mtry
create_rf = create.Learner("SL.randomForest",
                     tune = list(mtry = round(c(1, sqrt(ncol(X)), ncol(X)))))
create_rf
sl = SuperLearner(Y = Y, X = X, SL.library = create_rf$names, family = binomial())
sl
# Clean up global environment.
rm(list = create_rf$names)

# Optimize elastic net over alpha, with a custom environment and detailed names.
learners = new.env()
create_enet = create.Learner("SL.glmnet", env = learners, detailed_names = T,
                           tune = list(alpha = seq(0, 1, length.out=5)))
create_enet
# List the environment to review what functions were created.
ls(learners)
# We can simply list the environment to specify the library.
sl = SuperLearner(Y = Y, X = X, SL.library = ls(learners), family = binomial(), env = learners)
sl

## End(Not run)

</code></pre>

<hr>
<h2 id='create.SL.xgboost'>Factory for XGBoost SL wrappers</h2><span id='topic+create.SL.xgboost'></span>

<h3>Description</h3>

<p>Create multiple configurations of XGBoost learners based on the desired combinations of hyperparameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create.SL.xgboost(tune = list(ntrees = c(1000), max_depth = c(4), shrinkage =
  c(0.1), minobspernode = c(10)), detailed_names = F, env = .GlobalEnv,
  name_prefix = "SL.xgb")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create.SL.xgboost_+3A_tune">tune</code></td>
<td>
<p>List of hyperparameter settings to test. If specified, each hyperparameter will need to be defined.</p>
</td></tr>
<tr><td><code id="create.SL.xgboost_+3A_detailed_names">detailed_names</code></td>
<td>
<p>Set to T to have the function names include the parameter configurations.</p>
</td></tr>
<tr><td><code id="create.SL.xgboost_+3A_env">env</code></td>
<td>
<p>Environment in which to create the SL.xgboost functions. Defaults to the global environment.</p>
</td></tr>
<tr><td><code id="create.SL.xgboost_+3A_name_prefix">name_prefix</code></td>
<td>
<p>The prefix string for the name of each function that is generated.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create a new environment to store the learner functions.
# This keeps the global environment organized.
sl_env = new.env()
# Create 2 * 2 * 1 * 3 = 12 combinations of hyperparameters.
tune = list(ntrees = c(100, 500), max_depth = c(1, 2), minobspernode = 10,
            shrinkage = c(0.1, 0.01, 0.001))
# Generate a separate learner for each combination.
xgb_grid = create.SL.xgboost(tune = tune, env = sl_env)
# Review the function configurations.
xgb_grid
# Attach the environment so that the custom learner functions can be accessed.
attach(sl_env)
## Not run: 
sl = SuperLearner(Y = Y, X = X, SL.library = xgb_grid$names)

## End(Not run)
detach(sl_env)
</code></pre>

<hr>
<h2 id='CV.SuperLearner'>
Function to get V-fold cross-validated risk estimate for super learner
</h2><span id='topic+CV.SuperLearner'></span><span id='topic+print.CV.SuperLearner'></span><span id='topic+coef.CV.SuperLearner'></span>

<h3>Description</h3>

<p>Function to get V-fold cross-validated risk estimate for super learner. This function simply splits the data into V folds and then calls SuperLearner. Most of the arguments are passed directly to SuperLearner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CV.SuperLearner(Y, X, V = NULL, family = gaussian(), SL.library,
  method = "method.NNLS", id = NULL, verbose = FALSE,
  control = list(saveFitLibrary = FALSE), cvControl = list(),
  innerCvControl = list(),              
  obsWeights = NULL, saveAll = TRUE, parallel = "seq", env = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CV.SuperLearner_+3A_y">Y</code></td>
<td>

<p>The outcome.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_x">X</code></td>
<td>

<p>The covariates.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_v">V</code></td>
<td>

<p>The number of folds for <code>CV.SuperLearner</code>. This argument will be depreciated and moved into the <code>cvControl</code>. If Both <code>V</code> and <code>cvControl</code> set the number of cross-validation folds, an error message will appear. The recommendation is to use <code>cvControl</code>. This is not the number of folds for <code>SuperLearner</code>. The number of folds for <code>SuperLearner</code> is controlled with <code>innerCvControl</code>.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_family">family</code></td>
<td>

<p>Currently allows <code>gaussian</code> or <code>binomial</code> to describe the error distribution. Link function information will be ignored and should be contained in the method argument below.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_sl.library">SL.library</code></td>
<td>

<p>Either a character vector of prediction algorithms or a list containing character vectors. See details below for examples on the structure. A list of functions included in the SuperLearner package can be found with <code>listWrappers()</code>.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_method">method</code></td>
<td>

<p>A list (or a function to create a list) containing details on estimating the coefficients for the super learner and the model to combine the individual algorithms in the library. See <code>?method.template</code> for details.  Currently, the built in options are either &quot;method.NNLS&quot; (the default), &quot;method.NNLS2&quot;, &quot;method.NNloglik&quot;, &quot;method.CC_LS&quot;, &quot;method.CC_nloglik&quot;, or &quot;method.AUC&quot;.  NNLS and NNLS2 are non-negative least squares based on the Lawson-Hanson algorithm and the dual method of Goldfarb and Idnani, respectively.  NNLS and NNLS2 will work for both gaussian and binomial outcomes.  NNloglik is a non-negative binomial likelihood maximization using the BFGS quasi-Newton optimization method. NN* methods are normalized so weights sum to one. CC_LS uses Goldfarb and Idnani's quadratic programming algorithm to calculate the best convex combination of weights to minimize the squared error loss. CC_nloglik calculates the convex combination of weights that minimize the negative binomial log likelihood on the logistic scale using the sequential quadratic programming algorithm.  AUC, which only works for binary outcomes, uses the Nelder-Mead method via the optim function to minimize rank loss (equivalent to maximizing AUC).
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_id">id</code></td>
<td>

<p>Optional cluster identification variable. For the cross-validation splits, <code>id</code> forces observations in the same cluster to be in the same validation fold. <code>id</code> is passed to the prediction and screening algorithms in SL.library, but be sure to check the individual wrappers as many of them ignore the information.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_verbose">verbose</code></td>
<td>

<p>Logical; TRUE for printing progress during the computation (helpful for debugging).
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_control">control</code></td>
<td>

<p>A list of parameters to control the estimation process. Parameters include <code>saveFitLibrary</code> and <code>trimLogit</code>. See <code><a href="#topic+SuperLearner.control">SuperLearner.control</a></code> for details.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_cvcontrol">cvControl</code></td>
<td>

<p>A list of parameters to control the outer cross-validation process. The outer cross-validation is the sample spliting for evaluating the SuperLearner. Parameters include <code>V</code>, <code>stratifyCV</code>, <code>shuffle</code> and <code>validRows</code>. See <code><a href="#topic+SuperLearner.CV.control">SuperLearner.CV.control</a></code> for details.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_innercvcontrol">innerCvControl</code></td>
<td>

<p>A list of lists of parameters to control the inner cross-validation process. It should have <code>V</code> elements in the list, each a valid <code>cvControl</code> list. If only a single value, then replicated across all folds. The inner cross-validation are the values passed to each of the <code>V</code> <code>SuperLearner</code> calls. Parameters include <code>V</code>, <code>stratifyCV</code>, <code>shuffle</code> and <code>validRows</code>. See <code><a href="#topic+SuperLearner.CV.control">SuperLearner.CV.control</a></code> for details.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_obsweights">obsWeights</code></td>
<td>

<p>Optional observation weights variable. As with <code>id</code> above, <code>obsWeights</code> is passed to the prediction and screening algorithms, but many of the built in wrappers ignore (or can't use) the information. If you are using observation weights, make sure the library you specify uses the information.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_saveall">saveAll</code></td>
<td>

<p>Logical; Should the entire <code>SuperLearner</code> object be saved for each fold?
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_parallel">parallel</code></td>
<td>

<p>Options for parallel computation of the V-fold step. Use &quot;seq&quot; (the default) for sequential computation. <code>parallel = 'multicore'</code> to use <code>mclapply</code> for the V-fold step (but note that <code>SuperLearner()</code> will still be sequential). The default for mclapply is to check the <code>mc.cores</code> option, and if not set to default to 2 cores. Be sure to set <code>options()$mc.cores</code> to the desired number of cores if you don't want the default. Or <code>parallel</code> can be the name of a snow cluster and will use <code>parLapply</code> for the V-fold step. For both multicore and snow, the inner <code>SuperLearner</code> calls will be sequential.
</p>
</td></tr>
<tr><td><code id="CV.SuperLearner_+3A_env">env</code></td>
<td>

<p>Environment containing the learner functions. Defaults to the calling environment.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>SuperLearner</code> function builds a estimator, but does not contain an estimate on the performance of the estimator. Various methods exist for estimator performance evaluation. If you are familiar with the super learner algorithm, it should be no surprise we recommend using cross-validation to evaluate the honest performance of the super learner estimator. The function <code>CV.SuperLearner</code> computes the usual V-fold cross-validated risk estimate for the super learner (and all algorithms in <code>SL.library</code> for comparison).
</p>


<h3>Value</h3>

<p>An object of class <code>CV.SuperLearner</code> (a list) with components:
</p>
<table>
<tr><td><code>call</code></td>
<td>

<p>The matched call.
</p>
</td></tr>
<tr><td><code>AllSL</code></td>
<td>

<p>If <code>saveAll = TRUE</code>, a list with output from each call to <code>SuperLearner</code>, otherwise NULL.
</p>
</td></tr>
<tr><td><code>SL.predict</code></td>
<td>

<p>The predicted values from the super learner when each particular row was part of the validation fold.
</p>
</td></tr>
<tr><td><code>discreteSL.predict</code></td>
<td>

<p>The traditional cross-validated selector. Picks the algorithm with the smallest cross-validated risk (in super learner terms, gives that algorithm coefficient 1 and all others 0).
</p>
</td></tr>
<tr><td><code>whichDiscreteSL</code></td>
<td>

<p>A list of length <code>V</code>. The elements in the list are the algorithm that had the smallest cross-validated risk estimate for that fold.
</p>
</td></tr>
<tr><td><code>library.predict</code></td>
<td>

<p>A matrix with the predicted values from each algorithm in <code>SL.library</code>. The columns are the algorithms in <code>SL.library</code> and the rows represent the predicted values when that particular row was in the validation fold (i.e. not used to fit that estimator).
</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>

<p>A matrix with the coefficients for the super learner on each fold. The columns are the algorithms in <code>SL.library</code> the rows are the folds.
</p>
</td></tr>
<tr><td><code>folds</code></td>
<td>

<p>A list containing the row numbers for each validation fold.
</p>
</td></tr>
<tr><td><code>V</code></td>
<td>

<p>Number of folds for <code>CV.SuperLearner</code>.
</p>
</td></tr>
<tr><td><code>libraryNames</code></td>
<td>

<p>A character vector with the names of the algorithms in the library. The format is 'predictionAlgorithm_screeningAlgorithm' with '_All' used to denote the prediction algorithm run on all variables in X.
</p>
</td></tr>
<tr><td><code>SL.library</code></td>
<td>

<p>Returns <code>SL.library</code> in the same format as the argument with the same name above.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>A list with the method functions.
</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>

<p>The outcome
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Eric C Polley <a href="mailto:polley.eric@mayo.edu">polley.eric@mayo.edu</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+SuperLearner">SuperLearner</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(23432)
## training set
n &lt;- 500
p &lt;- 50
X &lt;- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) &lt;- paste("X", 1:p, sep="")
X &lt;- data.frame(X)
Y &lt;- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)

## build Library and run Super Learner
SL.library &lt;- c("SL.glm", "SL.randomForest", "SL.gam", "SL.polymars", "SL.mean")

test &lt;- CV.SuperLearner(Y = Y, X = X, V = 10, SL.library = SL.library,
  verbose = TRUE, method = "method.NNLS")
test
summary(test)
## Look at the coefficients across folds
coef(test)

# Example with specifying cross-validation options for both 
# CV.SuperLearner (cvControl) and the internal SuperLearners (innerCvControl)
test &lt;- CV.SuperLearner(Y = Y, X = X, SL.library = SL.library,
  cvControl = list(V = 10, shuffle = FALSE),
  innerCvControl = list(list(V = 5)),
  verbose = TRUE, method = "method.NNLS")

## examples with snow
library(parallel)
cl &lt;- makeCluster(2, type = "PSOCK") # can use different types here
clusterSetRNGStream(cl, iseed = 2343)
testSNOW &lt;- CV.SuperLearner(Y = Y, X = X, SL.library = SL.library, method = "method.NNLS",
  parallel = cl)
summary(testSNOW)
stopCluster(cl)

## End(Not run)
</code></pre>

<hr>
<h2 id='CVFolds'>
Generate list of row numbers for each fold in the cross-validation
</h2><span id='topic+CVFolds'></span>

<h3>Description</h3>

<p>Generate list of row numbers for each fold in the cross-validation. <code>CVFolds</code> is used in the <code>SuperLearner</code> to create the cross-validation splits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CVFolds(N, id, Y, cvControl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CVFolds_+3A_n">N</code></td>
<td>

<p>Sample size
</p>
</td></tr>
<tr><td><code id="CVFolds_+3A_id">id</code></td>
<td>

<p>Optional cluster id variable. If present, all observations in the same cluster will always be in the same split.
</p>
</td></tr>
<tr><td><code id="CVFolds_+3A_y">Y</code></td>
<td>

<p>outcome
</p>
</td></tr>
<tr><td><code id="CVFolds_+3A_cvcontrol">cvControl</code></td>
<td>

<p>Control parameters for the cross-validation step. See <code><a href="#topic+SuperLearner.CV.control">SuperLearner.CV.control</a></code> for details.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>validRows</code></td>
<td>

<p>A list of length V where each element in the list is a vector with the row numbers of the corresponding validation sample.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Eric C Polley <a href="mailto:polley.eric@mayo.edu">polley.eric@mayo.edu</a> </p>

<hr>
<h2 id='listWrappers'>
list all wrapper functions in SuperLearner
</h2><span id='topic+listWrappers'></span>

<h3>Description</h3>

<p>List all wrapper functions in <code><a href="#topic+SuperLearner">SuperLearner</a></code> package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>listWrappers(what = "both")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="listWrappers_+3A_what">what</code></td>
<td>

<p>What list to return. Can be <code>both</code> for both prediction algorithms and screening algorithms, <code>SL</code> for the prediction algorithms, <code>screen</code> for the screening algorithms, <code>method</code> for the estimation method details, or anything else will return a list of all (exported) functions in the <code>SuperLearner</code> package. Additional wrapper functions are available at <a href="https://github.com/ecpolley/SuperLearnerExtra">https://github.com/ecpolley/SuperLearnerExtra</a>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisible character vector with all exported functions in the SuperLearner package
</p>


<h3>Author(s)</h3>

<p> Eric C Polley <a href="mailto:epolley@uchicago.edu">epolley@uchicago.edu</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+SuperLearner">SuperLearner</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>listWrappers(what = "SL")
listWrappers(what = "screen")
</code></pre>

<hr>
<h2 id='plot.CV.SuperLearner'>
Graphical display of the V-fold CV risk estimates
</h2><span id='topic+plot.CV.SuperLearner'></span>

<h3>Description</h3>

<p>The function plots the V-fold cross-validated risk estimates for the super learner, the discrete super learner and each algorithm in the library. By default the estimates will be sorted and include an asymptotic 95% confidence interval.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'CV.SuperLearner'
plot(x, package = "ggplot2", constant = qnorm(0.975), sort = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.CV.SuperLearner_+3A_x">x</code></td>
<td>

<p>The output from <code>CV.SuperLearner</code>.
</p>
</td></tr>
<tr><td><code id="plot.CV.SuperLearner_+3A_package">package</code></td>
<td>

<p>Either &quot;ggplot2&quot; or &quot;lattice&quot;. The package selected must be available.
</p>
</td></tr>
<tr><td><code id="plot.CV.SuperLearner_+3A_constant">constant</code></td>
<td>

<p>A numeric value. The confidence interval is defined as p +/- constant * se, where p is the point estimate and se is the standard error. The default is the quantile of the standard normal corresponding to a 95% CI.
</p>
</td></tr>
<tr><td><code id="plot.CV.SuperLearner_+3A_sort">sort</code></td>
<td>

<p>Logical. Should the rows in the plot be sorted from the smallest to the largest point estimate. If FALSE, then the order is super learner, discrete super learner, then the estimators in <code>SL.library</code>.
</p>
</td></tr>
<tr><td><code id="plot.CV.SuperLearner_+3A_...">...</code></td>
<td>

<p>Additional arguments for <code>summary.CV.SuperLearner</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>see <a href="#topic+summary.CV.SuperLearner">summary.CV.SuperLearner</a> for details on how the estimates are computed
</p>


<h3>Value</h3>

<p>Returns the plot (either a ggplot2 object (class <code>ggplot</code>) or a lattice object (class <code>trellis</code>))
</p>


<h3>Author(s)</h3>

<p> Eric C Polley <a href="mailto:epolley@uchicago.edu">epolley@uchicago.edu</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.CV.SuperLearner">summary.CV.SuperLearner</a></code> and <code><a href="#topic+CV.SuperLearner">CV.SuperLearner</a></code>
</p>

<hr>
<h2 id='predict.SL.bartMachine'>bartMachine prediction</h2><span id='topic+predict.SL.bartMachine'></span>

<h3>Description</h3>

<p>bartMachine prediction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.bartMachine'
predict(object, newdata, family, X = NULL,
  Y = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.bartMachine_+3A_object">object</code></td>
<td>
<p>SuperLearner object</p>
</td></tr>
<tr><td><code id="predict.SL.bartMachine_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to predict the outcome</p>
</td></tr>
<tr><td><code id="predict.SL.bartMachine_+3A_family">family</code></td>
<td>
<p>&quot;gaussian&quot; for regression, &quot;binomial&quot; for binary
classification. (Not used)</p>
</td></tr>
<tr><td><code id="predict.SL.bartMachine_+3A_x">X</code></td>
<td>
<p>Covariate dataframe (not used)</p>
</td></tr>
<tr><td><code id="predict.SL.bartMachine_+3A_y">Y</code></td>
<td>
<p>Outcome variable (not used)</p>
</td></tr>
<tr><td><code id="predict.SL.bartMachine_+3A_...">...</code></td>
<td>
<p>Additional arguments (not used)</p>
</td></tr>
</table>

<hr>
<h2 id='predict.SL.biglasso'>Prediction wrapper for SL.biglasso</h2><span id='topic+predict.SL.biglasso'></span>

<h3>Description</h3>

<p>Prediction wrapper for SL.biglasso objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.biglasso'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.biglasso_+3A_object">object</code></td>
<td>
<p>SL.kernlab object</p>
</td></tr>
<tr><td><code id="predict.SL.biglasso_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to generate predictions</p>
</td></tr>
<tr><td><code id="predict.SL.biglasso_+3A_...">...</code></td>
<td>
<p>Unused additional arguments</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SL.biglasso">SL.biglasso</a></code> <code><a href="biglasso.html#topic+biglasso">biglasso</a></code>
<code><a href="biglasso.html#topic+predict.biglasso">predict.biglasso</a></code>
</p>

<hr>
<h2 id='predict.SL.glm'>Prediction for SL.glm</h2><span id='topic+predict.SL.glm'></span>

<h3>Description</h3>

<p>Prediction for SL.glm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.glm'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.glm_+3A_object">object</code></td>
<td>
<p>SL.glm object</p>
</td></tr>
<tr><td><code id="predict.SL.glm_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to generate predictions</p>
</td></tr>
<tr><td><code id="predict.SL.glm_+3A_...">...</code></td>
<td>
<p>Unused additional arguments</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SL.glm">SL.glm</a></code> <code><a href="stats.html#topic+glm">glm</a></code>
<code><a href="stats.html#topic+predict.glm">predict.glm</a></code>  <code><a href="#topic+SL.speedglm">SL.speedglm</a></code>
</p>

<hr>
<h2 id='predict.SL.glmnet'>Prediction for an SL.glmnet object</h2><span id='topic+predict.SL.glmnet'></span>

<h3>Description</h3>

<p>Prediction for the glmnet wrapper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.glmnet'
predict(object, newdata, remove_extra_cols = T,
  add_missing_cols = T, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.glmnet_+3A_object">object</code></td>
<td>
<p>Result object from SL.glmnet</p>
</td></tr>
<tr><td><code id="predict.SL.glmnet_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe or matrix that will generate predictions.</p>
</td></tr>
<tr><td><code id="predict.SL.glmnet_+3A_remove_extra_cols">remove_extra_cols</code></td>
<td>
<p>Remove any extra columns in the new data that were
not part of the original model.</p>
</td></tr>
<tr><td><code id="predict.SL.glmnet_+3A_add_missing_cols">add_missing_cols</code></td>
<td>
<p>Add any columns from original data that do not exist
in the new data, and set values to 0.</p>
</td></tr>
<tr><td><code id="predict.SL.glmnet_+3A_...">...</code></td>
<td>
<p>Any additional arguments (not used).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SL.glmnet">SL.glmnet</a></code>
</p>

<hr>
<h2 id='predict.SL.kernelKnn'>Prediction for SL.kernelKnn</h2><span id='topic+predict.SL.kernelKnn'></span>

<h3>Description</h3>

<p>Prediction for SL.kernelKnn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.kernelKnn'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.kernelKnn_+3A_object">object</code></td>
<td>
<p>SL.kernelKnn object</p>
</td></tr>
<tr><td><code id="predict.SL.kernelKnn_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to generate predictions</p>
</td></tr>
<tr><td><code id="predict.SL.kernelKnn_+3A_...">...</code></td>
<td>
<p>Unused additional arguments</p>
</td></tr>
</table>

<hr>
<h2 id='predict.SL.ksvm'>Prediction for SL.ksvm</h2><span id='topic+predict.SL.ksvm'></span>

<h3>Description</h3>

<p>Prediction for SL.ksvm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.ksvm'
predict(object, newdata, family, coupler = "minpair", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.ksvm_+3A_object">object</code></td>
<td>
<p>SL.kernlab object</p>
</td></tr>
<tr><td><code id="predict.SL.ksvm_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to generate predictions</p>
</td></tr>
<tr><td><code id="predict.SL.ksvm_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="predict.SL.ksvm_+3A_coupler">coupler</code></td>
<td>
<p>Coupling method used in the multiclass case, can be one of
minpair or pkpd (see kernlab package for details). For future usage.</p>
</td></tr>
<tr><td><code id="predict.SL.ksvm_+3A_...">...</code></td>
<td>
<p>Unused additional arguments</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SL.ksvm">SL.ksvm</a></code> <code><a href="kernlab.html#topic+ksvm">ksvm</a></code> <code><a href="kernlab.html#topic+predict.ksvm">predict.ksvm</a></code>
</p>

<hr>
<h2 id='predict.SL.lda'>Prediction wrapper for SL.lda</h2><span id='topic+predict.SL.lda'></span>

<h3>Description</h3>

<p>Prediction wrapper for SL.lda
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.lda'
predict(object, newdata, prior = object$object$prior,
  dimen = NULL, method = "plug-in", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.lda_+3A_object">object</code></td>
<td>
<p>SL.lda object</p>
</td></tr>
<tr><td><code id="predict.SL.lda_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to generate predictions</p>
</td></tr>
<tr><td><code id="predict.SL.lda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of the classes, by default the
proportions in the training set or what was set in the call to lda.</p>
</td></tr>
<tr><td><code id="predict.SL.lda_+3A_dimen">dimen</code></td>
<td>
<p>the dimension of the space to be used. If this is less than
min(p, ng-1), only the first dimen discriminant components are used (except
for method=&quot;predictive&quot;), and only those dimensions are returned in x.</p>
</td></tr>
<tr><td><code id="predict.SL.lda_+3A_method">method</code></td>
<td>
<p>This determines how the parameter estimation is handled. With
&quot;plug-in&quot; (the default) the usual unbiased parameter estimates are used and
assumed to be correct. With &quot;debiased&quot; an unbiased estimator of the log
posterior probabilities is used, and with &quot;predictive&quot; the parameter
estimates are integrated out using a vague prior.</p>
</td></tr>
<tr><td><code id="predict.SL.lda_+3A_...">...</code></td>
<td>
<p>Unused additional arguments</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SL.lda">SL.lda</a></code> <code><a href="MASS.html#topic+lda">lda</a></code>
<code><a href="MASS.html#topic+predict.lda">predict.lda</a></code>
</p>

<hr>
<h2 id='predict.SL.lm'>Prediction for SL.lm</h2><span id='topic+predict.SL.lm'></span>

<h3>Description</h3>

<p>Prediction for SL.lm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.lm'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.lm_+3A_object">object</code></td>
<td>
<p>SL.lm object</p>
</td></tr>
<tr><td><code id="predict.SL.lm_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to generate predictions</p>
</td></tr>
<tr><td><code id="predict.SL.lm_+3A_...">...</code></td>
<td>
<p>Unused additional arguments</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SL.lm">SL.lm</a></code> <code><a href="stats.html#topic+lm">lm</a></code>
<code><a href="stats.html#topic+predict.lm">predict.lm</a></code>  <code><a href="#topic+SL.speedlm">SL.speedlm</a></code>
</p>

<hr>
<h2 id='predict.SL.qda'>Prediction wrapper for SL.qda</h2><span id='topic+predict.SL.qda'></span>

<h3>Description</h3>

<p>Prediction wrapper for SL.qda
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.qda'
predict(object, newdata, prior = object$object$prior,
  dimen = NULL, method = "plug-in", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.qda_+3A_object">object</code></td>
<td>
<p>SL.lda object</p>
</td></tr>
<tr><td><code id="predict.SL.qda_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to generate predictions</p>
</td></tr>
<tr><td><code id="predict.SL.qda_+3A_prior">prior</code></td>
<td>
<p>The prior probabilities of the classes, by default the
proportions in the training set or what was set in the call to lda.</p>
</td></tr>
<tr><td><code id="predict.SL.qda_+3A_dimen">dimen</code></td>
<td>
<p>the dimension of the space to be used. If this is less than
min(p, ng-1), only the first dimen discriminant components are used (except
for method=&quot;predictive&quot;), and only those dimensions are returned in x.</p>
</td></tr>
<tr><td><code id="predict.SL.qda_+3A_method">method</code></td>
<td>
<p>This determines how the parameter estimation is handled. With
&quot;plug-in&quot; (the default) the usual unbiased parameter estimates are used and
assumed to be correct. With &quot;debiased&quot; an unbiased estimator of the log
posterior probabilities is used, and with &quot;predictive&quot; the parameter
estimates are integrated out using a vague prior.</p>
</td></tr>
<tr><td><code id="predict.SL.qda_+3A_...">...</code></td>
<td>
<p>Unused additional arguments</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SL.qda">SL.qda</a></code> <code><a href="MASS.html#topic+qda">qda</a></code>
<code><a href="MASS.html#topic+predict.qda">predict.qda</a></code>
</p>

<hr>
<h2 id='predict.SL.ranger'>Prediction wrapper for ranger random forests</h2><span id='topic+predict.SL.ranger'></span>

<h3>Description</h3>

<p>Prediction wrapper for SL.ranger objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.ranger'
predict(object, newdata, family, num.threads = 1,
  verbose = object$verbose, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.ranger_+3A_object">object</code></td>
<td>
<p>SL.kernlab object</p>
</td></tr>
<tr><td><code id="predict.SL.ranger_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to generate predictions</p>
</td></tr>
<tr><td><code id="predict.SL.ranger_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="predict.SL.ranger_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used for parallelization</p>
</td></tr>
<tr><td><code id="predict.SL.ranger_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE output additional information during execution.</p>
</td></tr>
<tr><td><code id="predict.SL.ranger_+3A_...">...</code></td>
<td>
<p>Unused additional arguments</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SL.ranger">SL.ranger</a></code> <code><a href="ranger.html#topic+ranger">ranger</a></code>
<code><a href="ranger.html#topic+predict.ranger">predict.ranger</a></code>
</p>

<hr>
<h2 id='predict.SL.speedglm'>Prediction for SL.speedglm</h2><span id='topic+predict.SL.speedglm'></span>

<h3>Description</h3>

<p>Prediction for SL.speedglm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.speedglm'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.speedglm_+3A_object">object</code></td>
<td>
<p>SL.speedglm object</p>
</td></tr>
<tr><td><code id="predict.SL.speedglm_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to generate predictions</p>
</td></tr>
<tr><td><code id="predict.SL.speedglm_+3A_...">...</code></td>
<td>
<p>Unused additional arguments</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SL.speedglm">SL.speedglm</a></code> <code><a href="speedglm.html#topic+speedglm">speedglm</a></code>
<code><a href="speedglm.html#topic+predict.speedglm">predict.speedglm</a></code>
</p>

<hr>
<h2 id='predict.SL.speedlm'>Prediction for SL.speedlm</h2><span id='topic+predict.SL.speedlm'></span>

<h3>Description</h3>

<p>Prediction for SL.speedlm, a fast lm()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.speedlm'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.speedlm_+3A_object">object</code></td>
<td>
<p>SL.speedlm object</p>
</td></tr>
<tr><td><code id="predict.SL.speedlm_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe to generate predictions</p>
</td></tr>
<tr><td><code id="predict.SL.speedlm_+3A_...">...</code></td>
<td>
<p>Unused additional arguments</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+SL.speedlm">SL.speedlm</a></code> <code><a href="speedglm.html#topic+speedlm">speedlm</a></code>
<code><a href="speedglm.html#topic+predict.speedlm">predict.speedlm</a></code>  <code><a href="#topic+SL.speedglm">SL.speedglm</a></code>
</p>

<hr>
<h2 id='predict.SL.xgboost'>XGBoost prediction on new data</h2><span id='topic+predict.SL.xgboost'></span>

<h3>Description</h3>

<p>XGBoost prediction on new data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SL.xgboost'
predict(object, newdata, family, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SL.xgboost_+3A_object">object</code></td>
<td>
<p>Model fit object from SuperLearner</p>
</td></tr>
<tr><td><code id="predict.SL.xgboost_+3A_newdata">newdata</code></td>
<td>
<p>Dataframe that will be converted to an xgb.DMatrix</p>
</td></tr>
<tr><td><code id="predict.SL.xgboost_+3A_family">family</code></td>
<td>
<p>Binomial or gaussian</p>
</td></tr>
<tr><td><code id="predict.SL.xgboost_+3A_...">...</code></td>
<td>
<p>Any remaining arguments (not supported though).</p>
</td></tr>
</table>

<hr>
<h2 id='predict.SuperLearner'>Predict method for SuperLearner object</h2><span id='topic+predict.SuperLearner'></span>

<h3>Description</h3>

<p>Obtains predictions on a new data set from a SuperLearner fit.  May require
the original data if one of the library algorithms uses the original data in
its predict method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SuperLearner'
predict(object, newdata, X = NULL, Y = NULL,
  onlySL = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SuperLearner_+3A_object">object</code></td>
<td>
<p>Fitted object from <code>SuperLearner</code></p>
</td></tr>
<tr><td><code id="predict.SuperLearner_+3A_newdata">newdata</code></td>
<td>
<p>New X values for prediction</p>
</td></tr>
<tr><td><code id="predict.SuperLearner_+3A_x">X</code></td>
<td>
<p>Original data set used to fit <code>object</code>, if needed by fit object.</p>
</td></tr>
<tr><td><code id="predict.SuperLearner_+3A_y">Y</code></td>
<td>
<p>Original outcome used to fit <code>object</code>, if needed by fit object.</p>
</td></tr>
<tr><td><code id="predict.SuperLearner_+3A_onlysl">onlySL</code></td>
<td>
<p>Logical. If TRUE, only compute predictions for algorithms with
non-zero coefficients in the super learner object. Default is FALSE
(computes predictions for all algorithms in library).</p>
</td></tr>
<tr><td><code id="predict.SuperLearner_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the <code>predict.SL.*</code>
functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>newdata</code> is omitted the predicted values from <code>object</code> are
returned.  Each algorithm in the Super Learner library needs to have a
corresponding prediction function with the &ldquo;predict.&rdquo; prefixed onto the
algorithm name (e.g. <code>predict.SL.glm</code> for <code>SL.glm</code>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>pred</code></td>
<td>
<p> Predicted values from Super Learner fit</p>
</td></tr>
<tr><td><code>library.predict</code></td>
<td>
<p> Predicted values for each algorithm in library</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric C Polley <a href="mailto:epolley@uchicago.edu">epolley@uchicago.edu</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SuperLearner">SuperLearner</a></code>
</p>

<hr>
<h2 id='recombineCVSL'>
Recombine a CV.SuperLearner fit using a new metalearning method
</h2><span id='topic+recombineCVSL'></span>

<h3>Description</h3>

<p>Function to re-compute the V-fold cross-validated risk estimate for super learner using a new metalearning method.  This function takes as input an existing CV.SuperLearner fit and applies the <code>recombineSL</code> fit to each of the V Super Learner fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recombineCVSL(object, method = "method.NNloglik", verbose = FALSE, 
  saveAll = TRUE, parallel = "seq")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recombineCVSL_+3A_object">object</code></td>
<td>

<p>Fitted object from <code>CV.SuperLearner</code>. 
</p>
</td></tr>
<tr><td><code id="recombineCVSL_+3A_method">method</code></td>
<td>

<p>A list (or a function to create a list) containing details on estimating the coefficients for the super learner and the model to combine the individual algorithms in the library. See <code>?method.template</code> for details.  Currently, the built in options are either &quot;method.NNLS&quot; (the default), &quot;method.NNLS2&quot;, &quot;method.NNloglik&quot;, &quot;method.CC_LS&quot;, &quot;method.CC_nloglik&quot;, or &quot;method.AUC&quot;.  NNLS and NNLS2 are non-negative least squares based on the Lawson-Hanson algorithm and the dual method of Goldfarb and Idnani, respectively.  NNLS and NNLS2 will work for both gaussian and binomial outcomes.  NNloglik is a non-negative binomial likelihood maximization using the BFGS quasi-Newton optimization method. NN* methods are normalized so weights sum to one. CC_LS uses Goldfarb and Idnani's quadratic programming algorithm to calculate the best convex combination of weights to minimize the squared error loss. CC_nloglik calculates the convex combination of weights that minimize the negative binomial log likelihood on the logistic scale using the sequential quadratic programming algorithm.  AUC, which only works for binary outcomes, uses the Nelder-Mead method via the optim function to minimize rank loss (equivalent to maximizing AUC).
</p>
</td></tr>
<tr><td><code id="recombineCVSL_+3A_verbose">verbose</code></td>
<td>

<p>logical; TRUE for printing progress during the computation (helpful for debugging).
</p>
</td></tr>
<tr><td><code id="recombineCVSL_+3A_saveall">saveAll</code></td>
<td>

<p>Logical; Should the entire <code>SuperLearner</code> object be saved for each fold?
</p>
</td></tr>
<tr><td><code id="recombineCVSL_+3A_parallel">parallel</code></td>
<td>

<p>Options for parallel computation of the V-fold step. Use &quot;seq&quot; (the default) for sequential computation. <code>parallel = 'multicore'</code> to use <code>mclapply</code> for the V-fold step (but note that <code>SuperLearner()</code> will still be sequential). Or <code>parallel</code> can be the name of a snow cluster and will use <code>parLapply</code> for the V-fold step. For both multicore and snow, the inner <code>SuperLearner</code> calls will be sequential.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>recombineCVSL</code> computes the usual V-fold cross-validated risk estimate for the super learner (and all algorithms in <code>SL.library</code> for comparison), using a newly specified metalearning method. The weights for each algorithm in <code>SL.library</code> are re-estimated using the new metalearner, however the base learner fits are not regenerated, so this function saves a lot of computation time as opposed to using the <code>CV.SuperLearner</code> function with a new <code>method</code> argument.  The output is identical to the output from the <code>CV.SuperLearner</code> function.
</p>


<h3>Value</h3>

<p>An object of class <code>CV.SuperLearner</code> (a list) with components:
</p>
<table>
<tr><td><code>call</code></td>
<td>

<p>The matched call.
</p>
</td></tr>
<tr><td><code>AllSL</code></td>
<td>

<p>If <code>saveAll = TRUE</code>, a list with output from each call to <code>SuperLearner</code>, otherwise NULL.
</p>
</td></tr>
<tr><td><code>SL.predict</code></td>
<td>

<p>The predicted values from the super learner when each particular row was part of the validation fold.
</p>
</td></tr>
<tr><td><code>discreteSL.predict</code></td>
<td>

<p>The traditional cross-validated selector. Picks the algorithm with the smallest cross-validated risk (in super learner terms, gives that algorithm coefficient 1 and all others 0).
</p>
</td></tr>
<tr><td><code>whichDiscreteSL</code></td>
<td>

<p>A list of length <code>V</code>. The elements in the list are the algorithm that had the smallest cross-validated risk estimate for that fold.
</p>
</td></tr>
<tr><td><code>library.predict</code></td>
<td>

<p>A matrix with the predicted values from each algorithm in <code>SL.library</code>. The columns are the algorithms in <code>SL.library</code> and the rows represent the predicted values when that particular row was in the validation fold (i.e. not used to fit that estimator).
</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>

<p>A matrix with the coefficients for the super learner on each fold. The columns are the algorithms in <code>SL.library</code> the rows are the folds.
</p>
</td></tr>
<tr><td><code>folds</code></td>
<td>

<p>A list containing the row numbers for each validation fold.
</p>
</td></tr>
<tr><td><code>V</code></td>
<td>

<p>Number of folds for <code>CV.SuperLearner</code>.
</p>
</td></tr>
<tr><td><code>libraryNames</code></td>
<td>

<p>A character vector with the names of the algorithms in the library. The format is 'predictionAlgorithm_screeningAlgorithm' with '_All' used to denote the prediction algorithm run on all variables in X.
</p>
</td></tr>
<tr><td><code>SL.library</code></td>
<td>

<p>Returns <code>SL.library</code> in the same format as the argument with the same name above.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>A list with the method functions.
</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>

<p>The outcome
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Erin LeDell <a href="mailto:ledell@berkeley.edu">ledell@berkeley.edu</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+recombineSL">recombineSL</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# Binary outcome example adapted from SuperLearner examples

set.seed(1)
N &lt;- 200
X &lt;- matrix(rnorm(N*10), N, 10)
X &lt;- as.data.frame(X)
Y &lt;- rbinom(N, 1, plogis(.2*X[, 1] + .1*X[, 2] - .2*X[, 3] + 
  .1*X[, 3]*X[, 4] - .2*abs(X[, 4])))

SL.library &lt;- c("SL.glmnet", "SL.glm", "SL.knn", "SL.gam", "SL.mean")

# least squares loss function
set.seed(1) # for reproducibility
cvfit_nnls &lt;- CV.SuperLearner(Y = Y, X = X, V = 10, SL.library = SL.library, 
  verbose = TRUE, method = "method.NNLS", family = binomial())
cvfit_nnls$coef
#    SL.glmnet_All SL.glm_All  SL.knn_All SL.gam_All SL.mean_All
# 1      0.0000000 0.00000000 0.000000000  0.4143862   0.5856138
# 2      0.0000000 0.00000000 0.304802397  0.3047478   0.3904498
# 3      0.0000000 0.00000000 0.002897533  0.5544075   0.4426950
# 4      0.0000000 0.20322642 0.000000000  0.1121891   0.6845845
# 5      0.1743973 0.00000000 0.032471026  0.3580624   0.4350693
# 6      0.0000000 0.00000000 0.099881535  0.3662309   0.5338876
# 7      0.0000000 0.00000000 0.234876082  0.2942472   0.4708767
# 8      0.0000000 0.06424676 0.113988158  0.5600208   0.2617443
# 9      0.0000000 0.00000000 0.338030342  0.2762604   0.3857093
# 10     0.3022442 0.00000000 0.294226204  0.1394534   0.2640762


# negative log binomial likelihood loss function
cvfit_nnloglik &lt;- recombineCVSL(cvfit_nnls, method = "method.NNloglik")
cvfit_nnloglik$coef
#    SL.glmnet_All SL.glm_All SL.knn_All SL.gam_All SL.mean_All
# 1      0.0000000  0.0000000 0.00000000  0.5974799  0.40252010
# 2      0.0000000  0.0000000 0.31177345  0.6882266  0.00000000
# 3      0.0000000  0.0000000 0.01377469  0.8544238  0.13180152
# 4      0.0000000  0.1644188 0.00000000  0.2387919  0.59678930
# 5      0.2142254  0.0000000 0.00000000  0.3729426  0.41283197
# 6      0.0000000  0.0000000 0.00000000  0.5847150  0.41528502
# 7      0.0000000  0.0000000 0.47538172  0.5080311  0.01658722
# 8      0.0000000  0.0000000 0.00000000  1.0000000  0.00000000
# 9      0.0000000  0.0000000 0.45384961  0.2923480  0.25380243
# 10     0.3977816  0.0000000 0.27927906  0.1606384  0.16230097

# If we use the same seed as the original `cvfit_nnls`, then
# the recombineCVSL and CV.SuperLearner results will be identical
# however, the recombineCVSL version will be much faster since
# it doesn't have to re-fit all the base learners, V times each.
set.seed(1)
cvfit_nnloglik2 &lt;- CV.SuperLearner(Y = Y, X = X, V = 10, SL.library = SL.library,
  verbose = TRUE, method = "method.NNloglik", family = binomial())
cvfit_nnloglik2$coef
#    SL.glmnet_All SL.glm_All SL.knn_All SL.gam_All SL.mean_All
# 1      0.0000000  0.0000000 0.00000000  0.5974799  0.40252010
# 2      0.0000000  0.0000000 0.31177345  0.6882266  0.00000000
# 3      0.0000000  0.0000000 0.01377469  0.8544238  0.13180152
# 4      0.0000000  0.1644188 0.00000000  0.2387919  0.59678930
# 5      0.2142254  0.0000000 0.00000000  0.3729426  0.41283197
# 6      0.0000000  0.0000000 0.00000000  0.5847150  0.41528502
# 7      0.0000000  0.0000000 0.47538172  0.5080311  0.01658722
# 8      0.0000000  0.0000000 0.00000000  1.0000000  0.00000000
# 9      0.0000000  0.0000000 0.45384961  0.2923480  0.25380243
# 10     0.3977816  0.0000000 0.27927906  0.1606384  0.16230097


## End(Not run)
</code></pre>

<hr>
<h2 id='recombineSL'>Recombine a SuperLearner fit using a new metalearning method</h2><span id='topic+recombineSL'></span>

<h3>Description</h3>

<p>The <code>recombineSL</code> function takes an existing SuperLearner fit and a new metalearning method and returns a new SuperLearner fit with updated base learner weights.</p>


<h3>Usage</h3>

<pre><code class='language-R'>recombineSL(object, Y, method = "method.NNloglik", verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recombineSL_+3A_object">object</code></td>
<td>

<p>Fitted object from <code>SuperLearner</code>.
</p>
</td></tr>
<tr><td><code id="recombineSL_+3A_y">Y</code></td>
<td>
 
<p>The outcome in the training data set. Must be a numeric vector.
</p>
</td></tr>
<tr><td><code id="recombineSL_+3A_method">method</code></td>
<td>

<p>A list (or a function to create a list) containing details on estimating the coefficients for the super learner and the model to combine the individual algorithms in the library. See <code>?method.template</code> for details.  Currently, the built in options are either &quot;method.NNLS&quot; (the default), &quot;method.NNLS2&quot;, &quot;method.NNloglik&quot;, &quot;method.CC_LS&quot;, &quot;method.CC_nloglik&quot;, or &quot;method.AUC&quot;.  NNLS and NNLS2 are non-negative least squares based on the Lawson-Hanson algorithm and the dual method of Goldfarb and Idnani, respectively.  NNLS and NNLS2 will work for both gaussian and binomial outcomes.  NNloglik is a non-negative binomial likelihood maximization using the BFGS quasi-Newton optimization method. NN* methods are normalized so weights sum to one. CC_LS uses Goldfarb and Idnani's quadratic programming algorithm to calculate the best convex combination of weights to minimize the squared error loss. CC_nloglik calculates the convex combination of weights that minimize the negative binomial log likelihood on the logistic scale using the sequential quadratic programming algorithm.  AUC, which only works for binary outcomes, uses the Nelder-Mead method via the optim function to minimize rank loss (equivalent to maximizing AUC).
</p>
</td></tr>
<tr><td><code id="recombineSL_+3A_verbose">verbose</code></td>
<td>

<p>logical; TRUE for printing progress during the computation (helpful for debugging).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>recombineSL</code> re-fits the super learner prediction algorithm using a new metalearning method.  The weights for each algorithm in <code>SL.library</code> are re-estimated using the new metalearner, however the base learner fits are not regenerated, so this function saves a lot of computation time as opposed to using the <code>SuperLearner</code> function with a new <code>method</code> argument.  The output is identical to the output from the <code>SuperLearner</code> function.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>

<p>The matched call.
</p>
</td></tr>
<tr><td><code>libraryNames</code></td>
<td>

<p>A character vector with the names of the algorithms in the library. The format is 'predictionAlgorithm_screeningAlgorithm' with '_All' used to denote the prediction algorithm run on all variables in X.
</p>
</td></tr>
<tr><td><code>SL.library</code></td>
<td>

<p>Returns <code>SL.library</code> in the same format as the argument with the same name above.
</p>
</td></tr>
<tr><td><code>SL.predict</code></td>
<td>

<p>The predicted values from the super learner for the rows in <code>newX</code>.
</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>

<p>Coefficients for the super learner.
</p>
</td></tr>
<tr><td><code>library.predict</code></td>
<td>

<p>A matrix with the predicted values from each algorithm in <code>SL.library</code> for the rows in <code>newX</code>.
</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>

<p>The Z matrix (the cross-validated predicted values for each algorithm in <code>SL.library</code>).
</p>
</td></tr>
<tr><td><code>cvRisk</code></td>
<td>

<p>A numeric vector with the V-fold cross-validated risk estimate for each algorithm in <code>SL.library</code>. Note that this does not contain the CV risk estimate for the SuperLearner, only the individual algorithms in the library.
</p>
</td></tr>
<tr><td><code>family</code></td>
<td>

<p>Returns the <code>family</code> value from above
</p>
</td></tr>
<tr><td><code>fitLibrary</code></td>
<td>

<p>A list with the fitted objects for each algorithm in <code>SL.library</code> on the full training data set.
</p>
</td></tr>
<tr><td><code>varNames</code></td>
<td>

<p>A character vector with the names of the variables in <code>X</code>.
</p>
</td></tr>
<tr><td><code>validRows</code></td>
<td>

<p>A list containing the row numbers for the V-fold cross-validation step.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>A list with the method functions.
</p>
</td></tr>
<tr><td><code>whichScreen</code></td>
<td>

<p>A logical matrix indicating which variables passed each screening algorithm.
</p>
</td></tr>
<tr><td><code>control</code></td>
<td>

<p>The <code>control</code> list.
</p>
</td></tr>
<tr><td><code>cvControl</code></td>
<td>

<p>The <code>cvControl</code> list.
</p>
</td></tr>
<tr><td><code>errorsInCVLibrary</code></td>
<td>

<p>A logical vector indicating if any algorithms experienced an error within the CV step.
</p>
</td></tr>
<tr><td><code>errorsInLibrary</code></td>
<td>

<p>A logical vector indicating if any algorithms experienced an error on the full data.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Erin LeDell <a href="mailto:ledell@berkeley.edu">ledell@berkeley.edu</a> </p>


<h3>References</h3>

 
<p>van der Laan, M. J., Polley, E. C. and Hubbard, A. E. (2008) Super Learner, <em>Statistical Applications of Genetics and Molecular Biology</em>, <b>6</b>, article 25.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# Binary outcome example adapted from SuperLearner examples

set.seed(1)
N &lt;- 200
X &lt;- matrix(rnorm(N*10), N, 10)
X &lt;- as.data.frame(X)
Y &lt;- rbinom(N, 1, plogis(.2*X[, 1] + .1*X[, 2] - .2*X[, 3] + 
  .1*X[, 3]*X[, 4] - .2*abs(X[, 4])))

SL.library &lt;- c("SL.glmnet", "SL.glm", "SL.knn", "SL.gam", "SL.mean")

# least squares loss function
set.seed(1) # for reproducibility
fit_nnls &lt;- SuperLearner(Y = Y, X = X, SL.library = SL.library, 
  verbose = TRUE, method = "method.NNLS", family = binomial())
fit_nnls
#                    Risk       Coef
# SL.glmnet_All 0.2439433 0.01293059
# SL.glm_All    0.2461245 0.08408060
# SL.knn_All    0.2604000 0.09600353
# SL.gam_All    0.2471651 0.40761918
# SL.mean_All   0.2486049 0.39936611


# negative log binomial likelihood loss function
fit_nnloglik &lt;- recombineSL(fit_nnls, Y = Y, method = "method.NNloglik")
fit_nnloglik
#                    Risk      Coef
# SL.glmnet_All 0.6815911 0.1577228
# SL.glm_All    0.6918926 0.0000000
# SL.knn_All          Inf 0.0000000
# SL.gam_All    0.6935383 0.6292881
# SL.mean_All   0.6904050 0.2129891

# If we use the same seed as the original `fit_nnls`, then
# the recombineSL and SuperLearner results will be identical
# however, the recombineSL version will be much faster since
# it doesn't have to re-fit all the base learners.
set.seed(1)
fit_nnloglik2 &lt;- SuperLearner(Y = Y, X = X, SL.library = SL.library,
  verbose = TRUE, method = "method.NNloglik", family = binomial())
fit_nnloglik2
#                    Risk      Coef
# SL.glmnet_All 0.6815911 0.1577228
# SL.glm_All    0.6918926 0.0000000
# SL.knn_All          Inf 0.0000000
# SL.gam_All    0.6935383 0.6292881
# SL.mean_All   0.6904050 0.2129891


## End(Not run)
</code></pre>

<hr>
<h2 id='SampleSplitSuperLearner'>Super Learner Prediction Function</h2><span id='topic+SampleSplitSuperLearner'></span>

<h3>Description</h3>

<p>A Prediction Function for the Super Learner.  The <code>SuperLearner</code> function takes a training set pair (X,Y) and returns the predicted values based on a validation set. SampleSplitSuperLearner uses sample split validation whereas SuperLearner uses V-fold cross-validation.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SampleSplitSuperLearner(Y, X, newX = NULL, family = gaussian(), SL.library, 
  method = "method.NNLS", id = NULL, verbose = FALSE, 
  control = list(), split = 0.8, obsWeights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SampleSplitSuperLearner_+3A_y">Y</code></td>
<td>
 
<p>The outcome in the training data set. Must be a numeric vector.
</p>
</td></tr>
<tr><td><code id="SampleSplitSuperLearner_+3A_x">X</code></td>
<td>

<p>The predictor variables in the training data set, usually a data.frame.
</p>
</td></tr>
<tr><td><code id="SampleSplitSuperLearner_+3A_newx">newX</code></td>
<td>

<p>The predictor variables in the validation data set. The structure should match X. If missing, uses X for newX.
</p>
</td></tr>
<tr><td><code id="SampleSplitSuperLearner_+3A_sl.library">SL.library</code></td>
<td>

<p>Either a character vector of prediction algorithms or a list containing character vectors. See details below for examples on the structure. A list of functions included in the SuperLearner package can be found with <code>listWrappers()</code>.</p>
</td></tr>
<tr><td><code id="SampleSplitSuperLearner_+3A_verbose">verbose</code></td>
<td>

<p>logical; TRUE for printing progress during the computation (helpful for debugging).
</p>
</td></tr>
<tr><td><code id="SampleSplitSuperLearner_+3A_family">family</code></td>
<td>
 
<p>Currently allows <code>gaussian</code> or <code>binomial</code> to describe the error distribution. Link function information will be ignored and should be contained in the method argument below.
</p>
</td></tr>
<tr><td><code id="SampleSplitSuperLearner_+3A_method">method</code></td>
<td>

<p>A list (or a function to create a list) containing details on estimating the coefficients for the super learner and the model to combine the individual algorithms in the library. See <code>?method.template</code> for details.  Currently, the built in options are either &quot;method.NNLS&quot; (the default), &quot;method.NNLS2&quot;, &quot;method.NNloglik&quot;, &quot;method.CC_LS&quot;, or &quot;method.CC_nloglik&quot;.  NNLS and NNLS2 are non-negative least squares based on the Lawson-Hanson algorithm and the dual method of Goldfarb and Idnani, respectively.  NNLS and NNLS2 will work for both gaussian and binomial outcomes.  NNloglik is a non-negative binomial likelihood maximization using the BFGS quasi-Newton optimization method. NN* methods are normalized so weights sum to one. CC_LS uses Goldfarb and Idnani's quadratic programming algorithm to calculate the best convex combination of weights to minimize the squared error loss. CC_nloglik calculates the convex combination of weights that minimize the negative binomial log likelihood on the logistic scale using the sequential quadratic programming algorithm. 
</p>
</td></tr>
<tr><td><code id="SampleSplitSuperLearner_+3A_id">id</code></td>
<td>

<p>Optional cluster identification variable. For the cross-validation splits, <code>id</code> forces observations in the same cluster to be in the same validation fold. <code>id</code> is passed to the prediction and screening algorithms in SL.library, but be sure to check the individual wrappers as many of them ignore the information.
</p>
</td></tr>
<tr><td><code id="SampleSplitSuperLearner_+3A_obsweights">obsWeights</code></td>
<td>

<p>Optional observation weights variable. As with <code>id</code> above, <code>obsWeights</code> is passed to the prediction and screening algorithms, but many of the built in wrappers ignore (or can't use) the information. If you are using observation weights, make sure the library you specify uses the information.
</p>
</td></tr>
<tr><td><code id="SampleSplitSuperLearner_+3A_control">control</code></td>
<td>

<p>A list of parameters to control the estimation process. Parameters include <code>saveFitLibrary</code> and <code>trimLogit</code>. See <code><a href="#topic+SuperLearner.control">SuperLearner.control</a></code> for details.
</p>
</td></tr>
<tr><td><code id="SampleSplitSuperLearner_+3A_split">split</code></td>
<td>

<p>Either a single value between 0 and 1 indicating the fraction of the samples for the training split. A value of 0.8 will randomly assign 80 percent of the samples to the training split and the other 20 percent to the validation split. Alternatively, split can be a numeric vector with the row numbers of <code>X</code> corresponding to the validation split. All other rows not in the vector will be considered in the training split.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>SuperLearner</code> fits the super learner prediction algorithm.  The weights for each algorithm in <code>SL.library</code> is estimated, along with the fit of each algorithm.
</p>
<p>The prescreen algorithms.  These algorithms first rank the variables in <code>X</code> based on either a univariate regression p-value of the <code>randomForest</code> variable importance.  A subset of the variables in <code>X</code> is selected based on a pre-defined cut-off.  With this subset of the X variables, the algorithms in <code>SL.library</code> are then fit.
</p>
<p>The SuperLearner package contains a few prediction and screening algorithm wrappers. The full list of wrappers can be viewed with <code>listWrappers()</code>. The design of the SuperLearner package is such that the user can easily add their own wrappers. We also maintain a website with additional examples of wrapper functions at <a href="https://github.com/ecpolley/SuperLearnerExtra">https://github.com/ecpolley/SuperLearnerExtra</a>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>

<p>The matched call.
</p>
</td></tr>
<tr><td><code>libraryNames</code></td>
<td>

<p>A character vector with the names of the algorithms in the library. The format is 'predictionAlgorithm_screeningAlgorithm' with '_All' used to denote the prediction algorithm run on all variables in X.
</p>
</td></tr>
<tr><td><code>SL.library</code></td>
<td>

<p>Returns <code>SL.library</code> in the same format as the argument with the same name above.
</p>
</td></tr>
<tr><td><code>SL.predict</code></td>
<td>

<p>The predicted values from the super learner for the rows in <code>newX</code>.
</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>

<p>Coefficients for the super learner.
</p>
</td></tr>
<tr><td><code>library.predict</code></td>
<td>

<p>A matrix with the predicted values from each algorithm in <code>SL.library</code> for the rows in <code>newX</code>.
</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>

<p>The Z matrix (the cross-validated predicted values for each algorithm in <code>SL.library</code>).
</p>
</td></tr>
<tr><td><code>cvRisk</code></td>
<td>

<p>A numeric vector with the V-fold cross-validated risk estimate for each algorithm in <code>SL.library</code>. Note that this does not contain the CV risk estimate for the SuperLearner, only the individual algorithms in the library.
</p>
</td></tr>
<tr><td><code>family</code></td>
<td>

<p>Returns the <code>family</code> value from above
</p>
</td></tr>
<tr><td><code>fitLibrary</code></td>
<td>

<p>A list with the fitted objects for each algorithm in <code>SL.library</code> on the full training data set.
</p>
</td></tr>
<tr><td><code>varNames</code></td>
<td>

<p>A character vector with the names of the variables in <code>X</code>.
</p>
</td></tr>
<tr><td><code>validRows</code></td>
<td>

<p>A list containing the row numbers for the V-fold cross-validation step.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>A list with the method functions.
</p>
</td></tr>
<tr><td><code>whichScreen</code></td>
<td>

<p>A logical matrix indicating which variables passed each screening algorithm.
</p>
</td></tr>
<tr><td><code>control</code></td>
<td>

<p>The <code>control</code> list.
</p>
</td></tr>
<tr><td><code>split</code></td>
<td>

<p>The <code>split</code> value.
</p>
</td></tr>
<tr><td><code>errorsInCVLibrary</code></td>
<td>

<p>A logical vector indicating if any algorithms experienced an error within the CV step.
</p>
</td></tr>
<tr><td><code>errorsInLibrary</code></td>
<td>

<p>A logical vector indicating if any algorithms experienced an error on the full data.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Eric C Polley <a href="mailto:epolley@uchicago.edu">epolley@uchicago.edu</a> </p>


<h3>References</h3>

 
<p>van der Laan, M. J., Polley, E. C. and Hubbard, A. E. (2008) Super Learner, <em>Statistical Applications of Genetics and Molecular Biology</em>, <b>6</b>, article 25.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## simulate data
set.seed(23432)
## training set
n &lt;- 500
p &lt;- 50
X &lt;- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) &lt;- paste("X", 1:p, sep="")
X &lt;- data.frame(X)
Y &lt;- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)

## test set
m &lt;- 1000
newX &lt;- matrix(rnorm(m*p), nrow = m, ncol = p)
colnames(newX) &lt;- paste("X", 1:p, sep="")
newX &lt;- data.frame(newX)
newY &lt;- newX[, 1] + sqrt(abs(newX[, 2] * newX[, 3])) + newX[, 2] -
  newX[, 3] + rnorm(m)

# generate Library and run Super Learner
SL.library &lt;- c("SL.glm", "SL.randomForest", "SL.gam",
  "SL.polymars", "SL.mean")
test &lt;- SampleSplitSuperLearner(Y = Y, X = X, newX = newX, SL.library = SL.library,
  verbose = TRUE, method = "method.NNLS")
test

# library with screening
SL.library &lt;- list(c("SL.glmnet", "All"), c("SL.glm", "screen.randomForest",
  "All", "screen.SIS"), "SL.randomForest", c("SL.polymars", "All"), "SL.mean")
test &lt;- SuperLearner(Y = Y, X = X, newX = newX, SL.library = SL.library,
  verbose = TRUE, method = "method.NNLS")
test

# binary outcome
set.seed(1)
N &lt;- 200
X &lt;- matrix(rnorm(N*10), N, 10)
X &lt;- as.data.frame(X)
Y &lt;- rbinom(N, 1, plogis(.2*X[, 1] + .1*X[, 2] - .2*X[, 3] + 
  .1*X[, 3]*X[, 4] - .2*abs(X[, 4])))

SL.library &lt;- c("SL.glmnet", "SL.glm", "SL.knn", "SL.gam", "SL.mean")

# least squares loss function
test.NNLS &lt;- SampleSplitSuperLearner(Y = Y, X = X, SL.library = SL.library, 
  verbose = TRUE, method = "method.NNLS", family = binomial())
test.NNLS

## End(Not run)
</code></pre>

<hr>
<h2 id='SL.bartMachine'>Wrapper for bartMachine learner</h2><span id='topic+SL.bartMachine'></span>

<h3>Description</h3>

<p>Support bayesian additive regression trees via the bartMachine package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.bartMachine(Y, X, newX, family, obsWeights, id, num_trees = 50,
  num_burn_in = 250, verbose = F, alpha = 0.95, beta = 2, k = 2,
  q = 0.9, nu = 3, num_iterations_after_burn_in = 1000, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.bartMachine_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_x">X</code></td>
<td>
<p>Covariate dataframe</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_newx">newX</code></td>
<td>
<p>Optional dataframe to predict the outcome</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_family">family</code></td>
<td>
<p>&quot;gaussian&quot; for regression, &quot;binomial&quot; for binary
classification</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_obsweights">obsWeights</code></td>
<td>
<p>Optional observation-level weights (supported but not tested)</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_id">id</code></td>
<td>
<p>Optional id to group observations from the same unit (not used
currently).</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_num_trees">num_trees</code></td>
<td>
<p>The number of trees to be grown in the sum-of-trees model.</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_num_burn_in">num_burn_in</code></td>
<td>
<p>Number of MCMC samples to be discarded as &quot;burn-in&quot;.</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_verbose">verbose</code></td>
<td>
<p>Prints information about progress of the algorithm to the
screen.</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_alpha">alpha</code></td>
<td>
<p>Base hyperparameter in tree prior for whether a node is
nonterminal or not.</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_beta">beta</code></td>
<td>
<p>Power hyperparameter in tree prior for whether a node is
nonterminal or not.</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_k">k</code></td>
<td>
<p>For regression, k determines the prior probability that E(Y|X) is
contained in the interval (y_min, y_max), based on a normal
distribution. For example, when k=2, the prior probability is 95%. For
classification, k determines the prior probability that E(Y|X) is between
(-3,3). Note that a larger value of k results in more shrinkage and a more
conservative fit.</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_q">q</code></td>
<td>
<p>Quantile of the prior on the error variance at which the data-based
estimate is placed. Note that the larger the value of q, the more
aggressive the fit as you are placing more prior weight on values lower
than the data-based estimate. Not used for classification.</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_nu">nu</code></td>
<td>
<p>Degrees of freedom for the inverse chi^2 prior. Not used for
classification.</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_num_iterations_after_burn_in">num_iterations_after_burn_in</code></td>
<td>
<p>Number of MCMC samples to draw from the
posterior distribution of f(x).</p>
</td></tr>
<tr><td><code id="SL.bartMachine_+3A_...">...</code></td>
<td>
<p>Additional arguments (not used)</p>
</td></tr>
</table>

<hr>
<h2 id='SL.biglasso'>SL wrapper for biglasso</h2><span id='topic+SL.biglasso'></span>

<h3>Description</h3>

<p>SL wrapper for biglasso
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.biglasso(Y, X, newX, family, obsWeights, penalty = "lasso",
  alg.logistic = "Newton", screen = "SSR", alpha = 1, nlambda = 100,
  eval.metric = "default", ncores = 1, nfolds = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.biglasso_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_obsweights">obsWeights</code></td>
<td>
<p>Observation-level weights</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_penalty">penalty</code></td>
<td>
<p>The penalty to be applied to the model. Either &quot;lasso&quot;
(default), &quot;ridge&quot;, or &quot;enet&quot; (elastic net).</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_alg.logistic">alg.logistic</code></td>
<td>
<p>The algorithm used in logistic regression. If &quot;Newton&quot;
then the exact hessian is used (default); if &quot;MM&quot; then a
majorization-minimization algorithm is used to set an upper-bound on the
hessian matrix. This can be faster, particularly in data-larger-than-RAM
case.</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_screen">screen</code></td>
<td>
<p>&quot;SSR&quot; (default) is the sequential strong rule; &quot;SEDPP&quot; is the
(sequential) EDPP rule. &quot;SSR-BEDPP&quot;, &quot;SSR-Dome&quot;, and &quot;SSR-Slores&quot; are our
newly proposed screening rules which combine the strong rule with a safe
rule (BEDPP, Dome test, or Slores rule). Among the three, the first two are
for lasso-penalized linear regression, and the last one is for
lasso-penalized logistic regression. &quot;None&quot; is to not apply a screening
rule.</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_alpha">alpha</code></td>
<td>
<p>The elastic-net mixing parameter that controls the relative
contribution from the lasso (l1) and the ridge (l2) penalty.</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_nlambda">nlambda</code></td>
<td>
<p>The number of lambda values to check.  Default is 100.</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_eval.metric">eval.metric</code></td>
<td>
<p>The evaluation metric for the cross-validated error and
for choosing optimal <code>lambda</code>. &quot;default&quot; for linear regression is MSE
(mean squared error), for logistic regression is misclassification error.
&quot;MAPE&quot;, for linear regression only, is the Mean Absolute Percentage Error.</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores to use for parallel execution across a
cluster created by the <code>parallel</code> package.</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of cross-validation folds.  Default is 5.</p>
</td></tr>
<tr><td><code id="SL.biglasso_+3A_...">...</code></td>
<td>
<p>Any additional arguments, not currently used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Zeng Y, Breheny P (2017). biglasso: Extending Lasso Model Fitting to Big
Data. https://CRAN.R-project.org/package=biglasso.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.SL.biglasso">predict.SL.biglasso</a></code> <code><a href="biglasso.html#topic+biglasso">biglasso</a></code>
<code><a href="biglasso.html#topic+cv.biglasso">cv.biglasso</a></code>
<code><a href="biglasso.html#topic+predict.biglasso">predict.biglasso</a></code> <code><a href="#topic+SL.glmnet">SL.glmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston, package = "MASS")
Y = Boston$medv
# Remove outcome from covariate dataframe.
X = Boston[, -14]

set.seed(1)

# Sample rows to speed up example.
row_subset = sample(nrow(X), 30)

# Subset rows and columns &amp; use only 2 folds to speed up example.
sl = SuperLearner(Y[row_subset], X[row_subset, 1:2, drop = FALSE],
                  family = gaussian(), cvControl = list(V = 2),
                  SL.library = "SL.biglasso")
sl

pred = predict(sl, X)
summary(pred$pred)

</code></pre>

<hr>
<h2 id='SL.cforest'>cforest (party)</h2><span id='topic+SL.cforest'></span>

<h3>Description</h3>

<p>These defaults emulate cforest_unbiased() but allow customization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.cforest(Y, X, newX, family, obsWeights, id, ntree = 1000,
  mtry = max(floor(ncol(X)/3), 1), mincriterion = 0, teststat = "quad",
  testtype = "Univ", replace = F, fraction = 0.632, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.cforest_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_x">X</code></td>
<td>
<p>Covariate dataframe</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_newx">newX</code></td>
<td>
<p>Optional dataframe to predict the outcome</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_family">family</code></td>
<td>
<p>&quot;gaussian&quot; for regression, &quot;binomial&quot; for binary
classification</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_obsweights">obsWeights</code></td>
<td>
<p>Optional observation-level weights (supported but not tested)</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_id">id</code></td>
<td>
<p>Optional id to group observations from the same unit (not used
currently).</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_mtry">mtry</code></td>
<td>
<p>Number of randomly selected features per node</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_mincriterion">mincriterion</code></td>
<td>
<p>See ?cforest_control</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_teststat">teststat</code></td>
<td>
<p>See ?cforest_control</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_testtype">testtype</code></td>
<td>
<p>See ?cforest_control</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_replace">replace</code></td>
<td>
<p>See ?cforest_control</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_fraction">fraction</code></td>
<td>
<p>See ?cforest_control</p>
</td></tr>
<tr><td><code id="SL.cforest_+3A_...">...</code></td>
<td>
<p>Remaining arguments (unused)</p>
</td></tr>
</table>

<hr>
<h2 id='SL.glm'>Wrapper for glm</h2><span id='topic+SL.glm'></span>

<h3>Description</h3>

<p>Wrapper for generalized linear models via glm().
</p>
<p>Note that for outcomes bounded by [0, 1] the binomial family can be used in
addition to gaussian.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.glm(Y, X, newX, family, obsWeights, model = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.glm_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.glm_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.glm_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.glm_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="SL.glm_+3A_obsweights">obsWeights</code></td>
<td>
<p>Observation-level weights</p>
</td></tr>
<tr><td><code id="SL.glm_+3A_model">model</code></td>
<td>
<p>Whether to save model.matrix of data in fit object. Set to FALSE
to save memory.</p>
</td></tr>
<tr><td><code id="SL.glm_+3A_...">...</code></td>
<td>
<p>Any remaining arguments, not used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Fox, J. (2015). Applied regression analysis and generalized linear models.
Sage Publications.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.SL.glm">predict.SL.glm</a></code> <code><a href="stats.html#topic+glm">glm</a></code>
<code><a href="stats.html#topic+predict.glm">predict.glm</a></code>  <code><a href="#topic+SL.speedglm">SL.speedglm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston, package = "MASS")
Y = Boston$medv
# Remove outcome from covariate dataframe.
X = Boston[, -14]

set.seed(1)

sl = SuperLearner(Y, X, family = gaussian(),
                  SL.library = c("SL.mean", "SL.glm"))

print(sl)

</code></pre>

<hr>
<h2 id='SL.glmnet'>Elastic net regression, including lasso and ridge</h2><span id='topic+SL.glmnet'></span>

<h3>Description</h3>

<p>Penalized regression using elastic net. Alpha = 0 corresponds to ridge
regression and alpha = 1 corresponds to Lasso.
</p>
<p>See <code>vignette("glmnet_beta", package = "glmnet")</code> for a nice tutorial on
glmnet.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.glmnet(Y, X, newX, family, obsWeights, id, alpha = 1, nfolds = 10,
  nlambda = 100, useMin = TRUE, loss = "deviance", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.glmnet_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_x">X</code></td>
<td>
<p>Covariate dataframe</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_newx">newX</code></td>
<td>
<p>Dataframe to predict the outcome</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_family">family</code></td>
<td>
<p>&quot;gaussian&quot; for regression, &quot;binomial&quot; for binary
classification. Untested options: &quot;multinomial&quot; for multiple classification
or &quot;mgaussian&quot; for multiple response, &quot;poisson&quot; for non-negative outcome
with proportional mean and variance, &quot;cox&quot;.</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_obsweights">obsWeights</code></td>
<td>
<p>Optional observation-level weights</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_id">id</code></td>
<td>
<p>Optional id to group observations from the same unit (not used
currently).</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_alpha">alpha</code></td>
<td>
<p>Elastic net mixing parameter, range [0, 1]. 0 = ridge regression
and 1 = lasso.</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds for internal cross-validation to optimize lambda.</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_nlambda">nlambda</code></td>
<td>
<p>Number of lambda values to check, recommended to be 100 or more.</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_usemin">useMin</code></td>
<td>
<p>If TRUE use lambda that minimizes risk, otherwise use 1
standard-error rule which chooses a higher penalty with performance within
one standard error of the minimum (see Breiman et al. 1984 on CART for
background).</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_loss">loss</code></td>
<td>
<p>Loss function, can be &quot;deviance&quot;, &quot;mse&quot;, or &quot;mae&quot;. If family =
binomial can also be &quot;auc&quot; or &quot;class&quot; (misclassification error).</p>
</td></tr>
<tr><td><code id="SL.glmnet_+3A_...">...</code></td>
<td>
<p>Any additional arguments are passed through to cv.glmnet.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010). Regularization paths for
generalized linear models via coordinate descent. Journal of statistical
software, 33(1), 1.
</p>
<p>Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge regression: Biased estimation
for nonorthogonal problems. Technometrics, 12(1), 55-67.
</p>
<p>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society. Series B (Methodological), 267-288.
</p>
<p>Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the
elastic net. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 67(2), 301-320.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.SL.glmnet">predict.SL.glmnet</a></code> <code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code>
<code><a href="glmnet.html#topic+glmnet">glmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Load a test dataset.
data(PimaIndiansDiabetes2, package = "mlbench")
data = PimaIndiansDiabetes2

# Omit observations with missing data.
data = na.omit(data)

Y = as.numeric(data$diabetes == "pos")
X = subset(data, select = -diabetes)

set.seed(1, "L'Ecuyer-CMRG")

sl = SuperLearner(Y, X, family = binomial(),
                  SL.library = c("SL.mean", "SL.glm", "SL.glmnet"))
sl

</code></pre>

<hr>
<h2 id='SL.kernelKnn'>SL wrapper for KernelKNN</h2><span id='topic+SL.kernelKnn'></span>

<h3>Description</h3>

<p>Wrapper for a configurable implementation of k-nearest
neighbors. Supports both binomial and gaussian outcome distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.kernelKnn(Y, X, newX, family, k = 10, method = "euclidean",
  weights_function = NULL, extrema = F, h = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.kernelKnn_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.kernelKnn_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.kernelKnn_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.kernelKnn_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="SL.kernelKnn_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors to use</p>
</td></tr>
<tr><td><code id="SL.kernelKnn_+3A_method">method</code></td>
<td>
<p>Distance method, can be 'euclidean' (default), 'manhattan',
'chebyshev', 'canberra', 'braycurtis', 'pearson_correlation',
'simple_matching_coefficient', 'minkowski' (by default the order 'p' of the
minkowski parameter equals k), 'hamming', 'mahalanobis',
'jaccard_coefficient', 'Rao_coefficient'</p>
</td></tr>
<tr><td><code id="SL.kernelKnn_+3A_weights_function">weights_function</code></td>
<td>
<p>Weighting method for combining the nearest neighbors.
Can be 'uniform' (default), 'triangular', 'epanechnikov', 'biweight',
'triweight', 'tricube', 'gaussian', 'cosine', 'logistic', 'gaussianSimple',
'silverman', 'inverse', 'exponential'.</p>
</td></tr>
<tr><td><code id="SL.kernelKnn_+3A_extrema">extrema</code></td>
<td>
<p>if TRUE then the minimum and maximum values from the
k-nearest-neighbors will be removed (can be thought as outlier removal).</p>
</td></tr>
<tr><td><code id="SL.kernelKnn_+3A_h">h</code></td>
<td>
<p>the bandwidth, applicable if the weights_function is not NULL.
Defaults to 1.0.</p>
</td></tr>
<tr><td><code id="SL.kernelKnn_+3A_...">...</code></td>
<td>
<p>Any additional parameters, not currently passed through.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with predictions and the original training data &amp;
hyperparameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Load a test dataset.
data(PimaIndiansDiabetes2, package = "mlbench")

data = PimaIndiansDiabetes2

# Omit observations with missing data.
data = na.omit(data)

Y_bin = as.numeric(data$diabetes)
X = subset(data, select = -diabetes)

set.seed(1)

sl = SuperLearner(Y_bin, X, family = binomial(),
                 SL.library = c("SL.mean", "SL.kernelKnn"))
sl

</code></pre>

<hr>
<h2 id='SL.ksvm'>Wrapper for Kernlab's SVM algorithm</h2><span id='topic+SL.ksvm'></span>

<h3>Description</h3>

<p>Wrapper for Kernlab's support vector machine algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.ksvm(Y, X, newX, family, type = NULL, kernel = "rbfdot",
  kpar = "automatic", scaled = T, C = 1, nu = 0.2, epsilon = 0.1,
  cross = 0, prob.model = family$family == "binomial",
  class.weights = NULL, cache = 40, tol = 0.001, shrinking = T, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.ksvm_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_type">type</code></td>
<td>
<p>ksvm can be used for classification , for regression, or for
novelty detection. Depending on whether y is a factor or not, the default
setting for type is C-svc or eps-svr, respectively, but can be overwritten
by setting an explicit value. See ?ksvm for more details.</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting. This
parameter can be set to any function, of class kernel, which computes the
inner product in feature space between two vector arguments. See ?ksvm for
more details.</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters). This is a list
which contains the parameters to be used with the kernel function. See
?ksvm for more details.</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_scaled">scaled</code></td>
<td>
<p>A logical vector indicating the variables to be scaled. If
scaled is of length 1, the value is recycled as many times as needed and
all non-binary variables are scaled. Per default, data are scaled
internally (both x and y variables) to zero mean and unit variance. The
center and scale values are returned and used for later predictions.</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_c">C</code></td>
<td>
<p>cost of constraints violation (default: 1) this is the 'C'-constant
of the regularization term in the Lagrange formulation.</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_nu">nu</code></td>
<td>
<p>parameter needed for nu-svc, one-svc, and nu-svr. The nu parameter
sets the upper bound on the training error and the lower bound on the
fraction of data points to become Support Vectors (default: 0.2).</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon in the insensitive-loss function used for eps-svr,
nu-svr and eps-bsvm (default: 0.1)</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_cross">cross</code></td>
<td>
<p>if a integer value k&gt;0 is specified, a k-fold cross validation
on the training data is performed to assess the quality of the model: the
accuracy rate for classification and the Mean Squared Error for regression</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_prob.model">prob.model</code></td>
<td>
<p>if set to TRUE builds a model for calculating class
probabilities or in case of regression, calculates the scaling parameter of
the Laplacian distribution fitted on the residuals. Fitting is done on
output data created by performing a 3-fold cross-validation on the training
data. (default: FALSE)</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_class.weights">class.weights</code></td>
<td>
<p>a named vector of weights for the different classes,
used for asymmetric class sizes. Not all factor levels have to be supplied
(default weight: 1). All components have to be named.</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_cache">cache</code></td>
<td>
<p>cache memory in MB (default 40)</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_tol">tol</code></td>
<td>
<p>tolerance of termination criterion (default: 0.001)</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_shrinking">shrinking</code></td>
<td>
<p>option whether to use the shrinking-heuristics (default: TRUE)</p>
</td></tr>
<tr><td><code id="SL.ksvm_+3A_...">...</code></td>
<td>
<p>Any additional parameters, not currently passed through.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with predictions and the original training data &amp;
hyperparameters.
</p>


<h3>References</h3>

<p>Hsu, C. W., Chang, C. C., &amp; Lin, C. J. (2016). A practical guide to support
vector classification. <a href="https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf</a>
</p>
<p>Scholkopf, B., &amp; Smola, A. J. (2001). Learning with kernels: support vector
machines, regularization, optimization, and beyond. MIT press.
</p>
<p>Vapnik, V. N. (1998). Statistical learning theory (Vol. 1). New York: Wiley.
</p>
<p>Zeileis, A., Hornik, K., Smola, A., &amp; Karatzoglou, A. (2004). kernlab-an S4
package for kernel methods in R. Journal of statistical software, 11(9),
1-20.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.SL.ksvm">predict.SL.ksvm</a></code> <code><a href="kernlab.html#topic+ksvm">ksvm</a></code>
<code><a href="kernlab.html#topic+predict.ksvm">predict.ksvm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston, package = "MASS")
Y = Boston$medv
# Remove outcome from covariate dataframe.
X = Boston[, -14]

set.seed(1)

sl = SuperLearner(Y, X, family = gaussian(),
                 SL.library = c("SL.mean", "SL.ksvm"))
sl

pred = predict(sl, X)
summary(pred$pred)

</code></pre>

<hr>
<h2 id='SL.lda'>SL wrapper for MASS:lda</h2><span id='topic+SL.lda'></span>

<h3>Description</h3>

<p>Linear discriminant analysis, used for classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.lda(Y, X, newX, family, obsWeights = rep(1, nrow(X)), id = NULL,
  verbose = F, prior = as.vector(prop.table(table(Y))), method = "mle",
  tol = 1e-04, CV = F, nu = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.lda_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_family">family</code></td>
<td>
<p>Binomial only, cannot be used for regression.</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_obsweights">obsWeights</code></td>
<td>
<p>Observation-level weights</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_id">id</code></td>
<td>
<p>Not supported.</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, display additional output during execution.</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_prior">prior</code></td>
<td>
<p>the prior probabilities of class membership. If unspecified, the
class proportions for the training set are used. If present, the
probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_method">method</code></td>
<td>
<p>&quot;moment&quot; for standard estimators of the mean and variance,
&quot;mle&quot; for MLEs, &quot;mve&quot; to use cov.mve, or &quot;t&quot; for robust estimates based on
a t distribution.</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_tol">tol</code></td>
<td>
<p>tolerance</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_cv">CV</code></td>
<td>
<p>If true, returns results (classes and posterior probabilities) for
leave-one-out cross-validation. Note that if the prior is estimated, the
proportions in the whole dataset are used.</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_nu">nu</code></td>
<td>
<p>degrees of freedom for method = &quot;t&quot;.</p>
</td></tr>
<tr><td><code id="SL.lda_+3A_...">...</code></td>
<td>
<p>Any additional arguments, not currently used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction
to Statistical Learning (Vol. 6). New York: Springer. Section 4.4.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.SL.lda">predict.SL.lda</a></code> <code><a href="MASS.html#topic+lda">lda</a></code>
<code><a href="MASS.html#topic+predict.lda">predict.lda</a></code> <code><a href="#topic+SL.qda">SL.qda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston, package = "MASS")
Y = as.numeric(Boston$medv &gt; 23)
# Remove outcome from covariate dataframe.
X = Boston[, -14]

set.seed(1)

# Use only 2 CV folds to speed up example.
sl = SuperLearner(Y, X, family = binomial(), cvControl = list(V = 2),
                 SL.library = c("SL.mean", "SL.lda"))
sl

pred = predict(sl, X)
summary(pred$pred)

</code></pre>

<hr>
<h2 id='SL.lm'>Wrapper for lm</h2><span id='topic+SL.lm'></span>

<h3>Description</h3>

<p>Wrapper for OLS via lm(), which may be faster than glm().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.lm(Y, X, newX, family, obsWeights, model = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.lm_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.lm_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.lm_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.lm_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="SL.lm_+3A_obsweights">obsWeights</code></td>
<td>
<p>Observation-level weights</p>
</td></tr>
<tr><td><code id="SL.lm_+3A_model">model</code></td>
<td>
<p>Whether to save model.matrix of data in fit object. Set to FALSE
to save memory.</p>
</td></tr>
<tr><td><code id="SL.lm_+3A_...">...</code></td>
<td>
<p>Any remaining arguments, not used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Fox, J. (2015). Applied regression analysis and generalized linear models.
Sage Publications.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.SL.lm">predict.SL.lm</a></code> <code><a href="stats.html#topic+lm">lm</a></code>
<code><a href="stats.html#topic+predict.lm">predict.lm</a></code>  <code><a href="#topic+SL.speedlm">SL.speedlm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston, package = "MASS")
Y = Boston$medv
# Remove outcome from covariate dataframe.
X = Boston[, -14]

set.seed(1)

sl = SuperLearner(Y, X, family = gaussian(),
                  SL.library = c("SL.mean", "SL.lm"))

print(sl)

</code></pre>

<hr>
<h2 id='SL.qda'>SL wrapper for MASS:qda</h2><span id='topic+SL.qda'></span>

<h3>Description</h3>

<p>Quadratic discriminant analysis, used for classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.qda(Y, X, newX, family, obsWeights = rep(1, nrow(X)), verbose = F,
  id = NULL, prior = as.vector(prop.table(table(Y))), method = "mle",
  tol = 1e-04, CV = F, nu = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.qda_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_family">family</code></td>
<td>
<p>Binomial only, cannot be used for regression.</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_obsweights">obsWeights</code></td>
<td>
<p>Observation-level weights</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, display additional output during execution.</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_id">id</code></td>
<td>
<p>Not supported.</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_prior">prior</code></td>
<td>
<p>the prior probabilities of class membership. If unspecified, the
class proportions for the training set are used. If present, the
probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_method">method</code></td>
<td>
<p>&quot;moment&quot; for standard estimators of the mean and variance,
&quot;mle&quot; for MLEs, &quot;mve&quot; to use cov.mve, or &quot;t&quot; for robust estimates based on
a t distribution.</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_tol">tol</code></td>
<td>
<p>tolerance</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_cv">CV</code></td>
<td>
<p>If true, returns results (classes and posterior probabilities) for
leave-one-out cross-validation. Note that if the prior is estimated, the
proportions in the whole dataset are used.</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_nu">nu</code></td>
<td>
<p>degrees of freedom for method = &quot;t&quot;.</p>
</td></tr>
<tr><td><code id="SL.qda_+3A_...">...</code></td>
<td>
<p>Any additional arguments, not currently used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction
to Statistical Learning (Vol. 6). New York: Springer. Section 4.4.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.SL.qda">predict.SL.qda</a></code> <code><a href="MASS.html#topic+qda">qda</a></code>
<code><a href="MASS.html#topic+predict.qda">predict.qda</a></code> <code><a href="#topic+SL.lda">SL.lda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston, package = "MASS")
Y = as.numeric(Boston$medv &gt; 23)
# Remove outcome from covariate dataframe.
X = Boston[, -14]

set.seed(1)

# Use only 2 CV folds to speed up example.
sl = SuperLearner(Y, X, family = binomial(), cvControl = list(V = 2),
                 SL.library = c("SL.mean", "SL.qda"))
sl

pred = predict(sl, X)
summary(pred$pred)


</code></pre>

<hr>
<h2 id='SL.ranger'>SL wrapper for ranger</h2><span id='topic+SL.ranger'></span>

<h3>Description</h3>

<p>Ranger is a fast implementation of Random Forest (Breiman 2001)
or recursive partitioning, particularly suited for high dimensional data.
</p>
<p>Extending code by Eric Polley from the SuperLearnerExtra package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.ranger(Y, X, newX, family, obsWeights, num.trees = 500,
  mtry = floor(sqrt(ncol(X))), write.forest = TRUE,
  probability = family$family == "binomial",
  min.node.size = ifelse(family$family == "gaussian", 5, 1), replace = TRUE,
  sample.fraction = ifelse(replace, 1, 0.632), num.threads = 1,
  verbose = T, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.ranger_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_obsweights">obsWeights</code></td>
<td>
<p>Observation-level weights</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees.</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables to possibly split at in each node. Default is
the (rounded down) square root of the number variables.</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_write.forest">write.forest</code></td>
<td>
<p>Save ranger.forest object, required for prediction. Set
to FALSE to reduce memory usage if no prediction intended.</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_probability">probability</code></td>
<td>
<p>Grow a probability forest as in Malley et al. (2012).</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_min.node.size">min.node.size</code></td>
<td>
<p>Minimal node size. Default 1 for classification, 5 for
regression, 3 for survival, and 10 for probability.</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_replace">replace</code></td>
<td>
<p>Sample with replacement.</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of observations to sample. Default is 1 for
sampling with replacement and 0.632 for sampling without replacement.</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, display additional output during execution.</p>
</td></tr>
<tr><td><code id="SL.ranger_+3A_...">...</code></td>
<td>
<p>Any additional arguments, not currently used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Breiman, L. (2001). Random forests. Machine learning 45:5-32.
</p>
<p>Wright, M. N. &amp; Ziegler, A. (2016). ranger: A Fast Implementation of Random
Forests for High Dimensional Data in C++ and R. Journal of Statistical
Software, in press. http://arxiv.org/abs/1508.04409.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SL.ranger">SL.ranger</a></code> <code><a href="ranger.html#topic+ranger">ranger</a></code>
<code><a href="ranger.html#topic+predict.ranger">predict.ranger</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston, package = "MASS")
Y = Boston$medv
# Remove outcome from covariate dataframe.
X = Boston[, -14]

set.seed(1)

# Use only 2 CV folds to speed up example.
sl = SuperLearner(Y, X, family = gaussian(), cvControl = list(V = 2),
                 SL.library = c("SL.mean", "SL.ranger"))
sl

pred = predict(sl, X)
summary(pred$pred)

</code></pre>

<hr>
<h2 id='SL.speedglm'>Wrapper for speedglm</h2><span id='topic+SL.speedglm'></span>

<h3>Description</h3>

<p>Speedglm is a fast version of glm()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.speedglm(Y, X, newX, family, obsWeights, maxit = 25, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.speedglm_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.speedglm_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.speedglm_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.speedglm_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="SL.speedglm_+3A_obsweights">obsWeights</code></td>
<td>
<p>Observation-level weights</p>
</td></tr>
<tr><td><code id="SL.speedglm_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations before stopping.</p>
</td></tr>
<tr><td><code id="SL.speedglm_+3A_k">k</code></td>
<td>
<p>numeric, the penalty per parameter to be used; the default k = 2 is
the classical AIC.</p>
</td></tr>
<tr><td><code id="SL.speedglm_+3A_...">...</code></td>
<td>
<p>Any remaining arguments, not used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Enea, M. A. R. C. O. (2013). Fitting linear models and generalized linear
models with large data sets in R. Statistical Methods for the Analysis of
Large Datasets: book of short papers, 411-414.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.SL.speedglm">predict.SL.speedglm</a></code> <code><a href="speedglm.html#topic+speedglm">speedglm</a></code>
<code><a href="speedglm.html#topic+predict.speedglm">predict.speedglm</a></code>
</p>

<hr>
<h2 id='SL.speedlm'>Wrapper for speedlm</h2><span id='topic+SL.speedlm'></span>

<h3>Description</h3>

<p>Speedlm is a fast version of lm()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.speedlm(Y, X, newX, family, obsWeights, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.speedlm_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.speedlm_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.speedlm_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.speedlm_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="SL.speedlm_+3A_obsweights">obsWeights</code></td>
<td>
<p>Observation-level weights</p>
</td></tr>
<tr><td><code id="SL.speedlm_+3A_...">...</code></td>
<td>
<p>Any remaining arguments, not used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Enea, M. A. R. C. O. (2013). Fitting linear models and generalized linear
models with large data sets in R. Statistical Methods for the Analysis of
Large Datasets: book of short papers, 411-414.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.SL.speedlm">predict.SL.speedlm</a></code> <code><a href="speedglm.html#topic+speedlm">speedlm</a></code>
<code><a href="speedglm.html#topic+predict.speedlm">predict.speedlm</a></code>  <code><a href="#topic+SL.speedglm">SL.speedglm</a></code>
</p>

<hr>
<h2 id='SL.xgboost'>XGBoost SuperLearner wrapper</h2><span id='topic+SL.xgboost'></span>

<h3>Description</h3>

<p>Supports the Extreme Gradient Boosting package for SuperLearnering, which is
a variant of gradient boosted machines (GBM).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.xgboost(Y, X, newX, family, obsWeights, id, ntrees = 1000, max_depth = 4,
  shrinkage = 0.1, minobspernode = 10, params = list(), nthread = 1,
  verbose = 0, save_period = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SL.xgboost_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_x">X</code></td>
<td>
<p>Covariate dataframe</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_newx">newX</code></td>
<td>
<p>Optional dataframe to predict the outcome</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_family">family</code></td>
<td>
<p>&quot;gaussian&quot; for regression, &quot;binomial&quot; for binary
classification, &quot;multinomial&quot; for multiple classification (not yet supported).</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_obsweights">obsWeights</code></td>
<td>
<p>Optional observation-level weights (supported but not tested)</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_id">id</code></td>
<td>
<p>Optional id to group observations from the same unit (not used
currently).</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_ntrees">ntrees</code></td>
<td>
<p>How many trees to fit. Low numbers may underfit but high
numbers may overfit, depending also on the shrinkage.</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_max_depth">max_depth</code></td>
<td>
<p>How deep each tree can be. 1 means no interactions, aka tree
stubs.</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_shrinkage">shrinkage</code></td>
<td>
<p>How much to shrink the predictions, in order to reduce
overfitting.</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_minobspernode">minobspernode</code></td>
<td>
<p>Minimum observations allowed per tree node, after which
no more splitting will occur.</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_params">params</code></td>
<td>
<p>Many other parameters can be customized. See
<a href="https://xgboost.readthedocs.io/en/latest/parameter.html">https://xgboost.readthedocs.io/en/latest/parameter.html</a></p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_nthread">nthread</code></td>
<td>
<p>How many threads (cores) should xgboost use. Generally we want
to keep this to 1 so that XGBoost does not compete with SuperLearner
parallelization.</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_verbose">verbose</code></td>
<td>
<p>Verbosity of XGB fitting.</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_save_period">save_period</code></td>
<td>
<p>How often (in tree iterations) to save current model to
disk during processing. If NULL does not save model, and if 0 saves model
at the end.</p>
</td></tr>
<tr><td><code id="SL.xgboost_+3A_...">...</code></td>
<td>
<p>Any remaining arguments (not supported though).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The performance of XGBoost, like GBM, is sensitive to the configuration
settings. Therefore it is best to create multiple configurations using
create.SL.xgboost and allow the SuperLearner to choose the best weights based
on cross-validated performance.
</p>
<p>If you run into errors please first try installing the latest version of
XGBoost from drat as described here:
<a href="https://xgboost.readthedocs.io/en/latest/build.html">https://xgboost.readthedocs.io/en/latest/build.html</a>
</p>

<hr>
<h2 id='summary.CV.SuperLearner'>
Summary Function for Cross-Validated Super Learner
</h2><span id='topic+summary.CV.SuperLearner'></span><span id='topic+print.summary.CV.SuperLearner'></span>

<h3>Description</h3>

<p>summary method for the <code>CV.SuperLearner</code> function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'CV.SuperLearner'
summary(object, obsWeights = NULL, ...)

## S3 method for class 'summary.CV.SuperLearner'
print(x, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.CV.SuperLearner_+3A_object">object</code></td>
<td>

<p>An object of class &quot;CV.SuperLearner&quot;, the result of a call to <code>CV.SuperLearner</code>.
</p>
</td></tr>
<tr><td><code id="summary.CV.SuperLearner_+3A_x">x</code></td>
<td>

<p>An object of class &quot;summary.CV.SuperLearner&quot;, the result of a call to <code>summary.CV.SuperLearner</code>. 
</p>
</td></tr>
<tr><td><code id="summary.CV.SuperLearner_+3A_obsweights">obsWeights</code></td>
<td>

<p>Optional vector for observation weights.
</p>
</td></tr>
<tr><td><code id="summary.CV.SuperLearner_+3A_digits">digits</code></td>
<td>

<p>The number of significant digits to use when printing.
</p>
</td></tr>
<tr><td><code id="summary.CV.SuperLearner_+3A_...">...</code></td>
<td>
<p> additional arguments ...</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Summary method for <code>CV.SuperLearner</code>.  Calculates the V-fold cross-validated estimate of either the mean squared error or the -2*log(L) depending on the loss function used.
</p>


<h3>Value</h3>

<p><code>summary.CV.SuperLearner</code> returns a list with components
</p>
<table>
<tr><td><code>call</code></td>
<td>

<p>The function call from <code>CV.SuperLearner</code>
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>Describes the loss function used.  Currently either least squares of negative log Likelihood.
</p>
</td></tr>
<tr><td><code>V</code></td>
<td>

<p>Number of folds
</p>
</td></tr>
<tr><td><code>Risk.SL</code></td>
<td>

<p>Risk estimate for the super learner
</p>
</td></tr>
<tr><td><code>Risk.dSL</code></td>
<td>

<p>Risk estimate for the discrete super learner (the cross-validation selector)
</p>
</td></tr>
<tr><td><code>Risk.library</code></td>
<td>

<p>A matrix with the risk estimates for each algorithm in the library
</p>
</td></tr>
<tr><td><code>Table</code></td>
<td>

<p>A table with the mean risk estimate and standard deviation across the folds for the super learner and all algorithms in the library
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Eric C Polley <a href="mailto:eric.polley@nih.gov">eric.polley@nih.gov</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+CV.SuperLearner">CV.SuperLearner</a></code>
</p>

<hr>
<h2 id='SuperLearner'>Super Learner Prediction Function</h2><span id='topic+SuperLearner'></span><span id='topic+mcSuperLearner'></span><span id='topic+snowSuperLearner'></span><span id='topic+print.SuperLearner'></span><span id='topic+coef.SuperLearner'></span>

<h3>Description</h3>

<p>A Prediction Function for the Super Learner.  The <code>SuperLearner</code> function takes a training set pair (X,Y) and returns the predicted values based on a validation set.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SuperLearner(Y, X, newX = NULL, family = gaussian(), SL.library,
  method = "method.NNLS", id = NULL, verbose = FALSE,
  control = list(), cvControl = list(), obsWeights = NULL, env = parent.frame())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SuperLearner_+3A_y">Y</code></td>
<td>

<p>The outcome in the training data set. Must be a numeric vector.
</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_x">X</code></td>
<td>

<p>The predictor variables in the training data set, usually a data.frame.
</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_newx">newX</code></td>
<td>

<p>The predictor variables in the validation data set. The structure should match X. If missing, uses X for newX.
</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_sl.library">SL.library</code></td>
<td>

<p>Either a character vector of prediction algorithms or a list containing character vectors. See details below for examples on the structure. A list of functions included in the SuperLearner package can be found with <code>listWrappers()</code>.</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_verbose">verbose</code></td>
<td>

<p>logical; TRUE for printing progress during the computation (helpful for debugging).
</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_family">family</code></td>
<td>

<p>Currently allows <code>gaussian</code> or <code>binomial</code> to describe the error distribution. Link function information will be ignored and should be contained in the method argument below.
</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_method">method</code></td>
<td>

<p>A list (or a function to create a list) containing details on estimating the coefficients for the super learner and the model to combine the individual algorithms in the library. See <code>?method.template</code> for details.  Currently, the built in options are either &quot;method.NNLS&quot; (the default), &quot;method.NNLS2&quot;, &quot;method.NNloglik&quot;, &quot;method.CC_LS&quot;, &quot;method.CC_nloglik&quot;, or &quot;method.AUC&quot;.  NNLS and NNLS2 are non-negative least squares based on the Lawson-Hanson algorithm and the dual method of Goldfarb and Idnani, respectively.  NNLS and NNLS2 will work for both gaussian and binomial outcomes.  NNloglik is a non-negative binomial likelihood maximization using the BFGS quasi-Newton optimization method. NN* methods are normalized so weights sum to one. CC_LS uses Goldfarb and Idnani's quadratic programming algorithm to calculate the best convex combination of weights to minimize the squared error loss. CC_nloglik calculates the convex combination of weights that minimize the negative binomial log likelihood on the logistic scale using the sequential quadratic programming algorithm.  AUC, which only works for binary outcomes, uses the Nelder-Mead method via the optim function to minimize rank loss (equivalent to maximizing AUC).
</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_id">id</code></td>
<td>

<p>Optional cluster identification variable. For the cross-validation splits, <code>id</code> forces observations in the same cluster to be in the same validation fold. <code>id</code> is passed to the prediction and screening algorithms in SL.library, but be sure to check the individual wrappers as many of them ignore the information.
</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_obsweights">obsWeights</code></td>
<td>

<p>Optional observation weights variable. As with <code>id</code> above, <code>obsWeights</code> is passed to the prediction and screening algorithms, but many of the built in wrappers ignore (or can't use) the information. If you are using observation weights, make sure the library you specify uses the information.
</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_control">control</code></td>
<td>

<p>A list of parameters to control the estimation process. Parameters include <code>saveFitLibrary</code> and <code>trimLogit</code>. See <code><a href="#topic+SuperLearner.control">SuperLearner.control</a></code> for details.
</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_cvcontrol">cvControl</code></td>
<td>

<p>A list of parameters to control the cross-validation process. Parameters include <code>V</code>, <code>stratifyCV</code>, <code>shuffle</code> and <code>validRows</code>. See <code><a href="#topic+SuperLearner.CV.control">SuperLearner.CV.control</a></code> for details.
</p>
</td></tr>
<tr><td><code id="SuperLearner_+3A_env">env</code></td>
<td>

<p>Environment containing the learner functions. Defaults to the calling environment.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>SuperLearner</code> fits the super learner prediction algorithm.  The weights for each algorithm in <code>SL.library</code> is estimated, along with the fit of each algorithm.
</p>
<p>The prescreen algorithms.  These algorithms first rank the variables in <code>X</code> based on either a univariate regression p-value of the <code>randomForest</code> variable importance.  A subset of the variables in <code>X</code> is selected based on a pre-defined cut-off.  With this subset of the X variables, the algorithms in <code>SL.library</code> are then fit.
</p>
<p>The SuperLearner package contains a few prediction and screening algorithm wrappers. The full list of wrappers can be viewed with <code>listWrappers()</code>. The design of the SuperLearner package is such that the user can easily add their own wrappers. We also maintain a website with additional examples of wrapper functions at <a href="https://github.com/ecpolley/SuperLearnerExtra">https://github.com/ecpolley/SuperLearnerExtra</a>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>

<p>The matched call.
</p>
</td></tr>
<tr><td><code>libraryNames</code></td>
<td>

<p>A character vector with the names of the algorithms in the library. The format is 'predictionAlgorithm_screeningAlgorithm' with '_All' used to denote the prediction algorithm run on all variables in X.
</p>
</td></tr>
<tr><td><code>SL.library</code></td>
<td>

<p>Returns <code>SL.library</code> in the same format as the argument with the same name above.
</p>
</td></tr>
<tr><td><code>SL.predict</code></td>
<td>

<p>The predicted values from the super learner for the rows in <code>newX</code>.
</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>

<p>Coefficients for the super learner.
</p>
</td></tr>
<tr><td><code>library.predict</code></td>
<td>

<p>A matrix with the predicted values from each algorithm in <code>SL.library</code> for the rows in <code>newX</code>.
</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>

<p>The Z matrix (the cross-validated predicted values for each algorithm in <code>SL.library</code>).
</p>
</td></tr>
<tr><td><code>cvRisk</code></td>
<td>

<p>A numeric vector with the V-fold cross-validated risk estimate for each algorithm in <code>SL.library</code>. Note that this does not contain the CV risk estimate for the SuperLearner, only the individual algorithms in the library.
</p>
</td></tr>
<tr><td><code>family</code></td>
<td>

<p>Returns the <code>family</code> value from above
</p>
</td></tr>
<tr><td><code>fitLibrary</code></td>
<td>

<p>A list with the fitted objects for each algorithm in <code>SL.library</code> on the full training data set.
</p>
</td></tr>
<tr><td><code>cvFitLibrary</code></td>
<td>

<p>A list with fitted objects for each algorithm in <code>SL.library</code> on each of 
<code>V</code> different training data sets. 
</p>
</td></tr>
<tr><td><code>varNames</code></td>
<td>

<p>A character vector with the names of the variables in <code>X</code>.
</p>
</td></tr>
<tr><td><code>validRows</code></td>
<td>

<p>A list containing the row numbers for the V-fold cross-validation step.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>A list with the method functions.
</p>
</td></tr>
<tr><td><code>whichScreen</code></td>
<td>

<p>A logical matrix indicating which variables passed each screening algorithm.
</p>
</td></tr>
<tr><td><code>control</code></td>
<td>

<p>The <code>control</code> list.
</p>
</td></tr>
<tr><td><code>cvControl</code></td>
<td>

<p>The <code>cvControl</code> list.
</p>
</td></tr>
<tr><td><code>errorsInCVLibrary</code></td>
<td>

<p>A logical vector indicating if any algorithms experienced an error within the CV step.
</p>
</td></tr>
<tr><td><code>errorsInLibrary</code></td>
<td>

<p>A logical vector indicating if any algorithms experienced an error on the full data.
</p>
</td></tr>
<tr><td><code>env</code></td>
<td>

<p>Environment passed into function which will be searched to find the learner functions. Defaults to the calling environment.
</p>
</td></tr>
<tr><td><code>times</code></td>
<td>

<p>A list that contains the execution time of the SuperLearner, plus separate times for model fitting and prediction.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Eric C Polley <a href="mailto:epolley@uchicago.edu">epolley@uchicago.edu</a> </p>


<h3>References</h3>

<p>van der Laan, M. J., Polley, E. C. and Hubbard, A. E. (2008) Super Learner, <em>Statistical Applications of Genetics and Molecular Biology</em>, <b>6</b>, article 25.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## simulate data
set.seed(23432)
## training set
n &lt;- 500
p &lt;- 50
X &lt;- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) &lt;- paste("X", 1:p, sep="")
X &lt;- data.frame(X)
Y &lt;- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)

## test set
m &lt;- 1000
newX &lt;- matrix(rnorm(m*p), nrow = m, ncol = p)
colnames(newX) &lt;- paste("X", 1:p, sep="")
newX &lt;- data.frame(newX)
newY &lt;- newX[, 1] + sqrt(abs(newX[, 2] * newX[, 3])) + newX[, 2] -
  newX[, 3] + rnorm(m)

# generate Library and run Super Learner
SL.library &lt;- c("SL.glm", "SL.randomForest", "SL.gam",
  "SL.polymars", "SL.mean")
test &lt;- SuperLearner(Y = Y, X = X, newX = newX, SL.library = SL.library,
  verbose = TRUE, method = "method.NNLS")
test

# library with screening
SL.library &lt;- list(c("SL.glmnet", "All"), c("SL.glm", "screen.randomForest",
  "All", "screen.SIS"), "SL.randomForest", c("SL.polymars", "All"), "SL.mean")
test &lt;- SuperLearner(Y = Y, X = X, newX = newX, SL.library = SL.library,
  verbose = TRUE, method = "method.NNLS")
test

# binary outcome
set.seed(1)
N &lt;- 200
X &lt;- matrix(rnorm(N*10), N, 10)
X &lt;- as.data.frame(X)
Y &lt;- rbinom(N, 1, plogis(.2*X[, 1] + .1*X[, 2] - .2*X[, 3] +
  .1*X[, 3]*X[, 4] - .2*abs(X[, 4])))

SL.library &lt;- c("SL.glmnet", "SL.glm", "SL.knn", "SL.gam", "SL.mean")

# least squares loss function
test.NNLS &lt;- SuperLearner(Y = Y, X = X, SL.library = SL.library,
  verbose = TRUE, method = "method.NNLS", family = binomial())
test.NNLS

# negative log binomial likelihood loss function
test.NNloglik &lt;- SuperLearner(Y = Y, X = X, SL.library = SL.library,
  verbose = TRUE, method = "method.NNloglik", family = binomial())
test.NNloglik

# 1 - AUC loss function
test.AUC &lt;- SuperLearner(Y = Y, X = X, SL.library = SL.library,
  verbose = TRUE, method = "method.AUC", family = binomial())
test.AUC

# 2
# adapted from library(SIS)
set.seed(1)
# training
b &lt;- c(2, 2, 2, -3*sqrt(2))
n &lt;- 150
p &lt;- 200
truerho &lt;- 0.5
corrmat &lt;- diag(rep(1-truerho, p)) + matrix(truerho, p, p)
corrmat[, 4] = sqrt(truerho)
corrmat[4, ] = sqrt(truerho)
corrmat[4, 4] = 1
cholmat &lt;- chol(corrmat)
x &lt;- matrix(rnorm(n*p, mean=0, sd=1), n, p)
x &lt;- x 
feta &lt;- x[, 1:4] 
fprob &lt;- exp(feta) / (1 + exp(feta))
y &lt;- rbinom(n, 1, fprob)

# test
m &lt;- 10000
newx &lt;- matrix(rnorm(m*p, mean=0, sd=1), m, p)
newx &lt;- newx 
newfeta &lt;- newx[, 1:4] 
newfprob &lt;- exp(newfeta) / (1 + exp(newfeta))
newy &lt;- rbinom(m, 1, newfprob)

DATA2 &lt;- data.frame(Y = y, X = x)
newDATA2 &lt;- data.frame(Y = newy, X=newx)

create.SL.knn &lt;- function(k = c(20, 30)) {
  for(mm in seq(length(k))){
    eval(parse(text = paste('SL.knn.', k[mm], '&lt;- function(..., k = ', k[mm],
      ') SL.knn(..., k = k)', sep = '')), envir = .GlobalEnv)
  }
  invisible(TRUE)
}
create.SL.knn(c(20, 30, 40, 50, 60, 70))

# library with screening
SL.library &lt;- list(c("SL.glmnet", "All"), c("SL.glm", "screen.randomForest"),
  "SL.randomForest", "SL.knn", "SL.knn.20", "SL.knn.30", "SL.knn.40",
  "SL.knn.50", "SL.knn.60", "SL.knn.70",
  c("SL.polymars", "screen.randomForest"))
test &lt;- SuperLearner(Y = DATA2$Y, X = DATA2[, -1], newX = newDATA2[, -1],
  SL.library = SL.library, verbose = TRUE, family = binomial())
test

## examples with multicore
set.seed(23432, "L'Ecuyer-CMRG")  # use L'Ecuyer for multicore seeds. see ?set.seed for details
## training set
n &lt;- 500
p &lt;- 50
X &lt;- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) &lt;- paste("X", 1:p, sep="")
X &lt;- data.frame(X)
Y &lt;- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)

## test set
m &lt;- 1000
newX &lt;- matrix(rnorm(m*p), nrow = m, ncol = p)
colnames(newX) &lt;- paste("X", 1:p, sep="")
newX &lt;- data.frame(newX)
newY &lt;- newX[, 1] + sqrt(abs(newX[, 2] * newX[, 3])) + newX[, 2] - newX[, 3] + rnorm(m)

# generate Library and run Super Learner
SL.library &lt;- c("SL.glm", "SL.randomForest", "SL.gam",
  "SL.polymars", "SL.mean")

testMC &lt;- mcSuperLearner(Y = Y, X = X, newX = newX, SL.library = SL.library,
  method = "method.NNLS")
testMC

## examples with snow
library(parallel)
cl &lt;- makeCluster(2, type = "PSOCK") # can use different types here
clusterSetRNGStream(cl, iseed = 2343)
# make SL functions available on the clusters, use assignment to avoid printing
foo &lt;- clusterEvalQ(cl, library(SuperLearner))  
testSNOW &lt;- snowSuperLearner(cluster = cl, Y = Y, X = X, newX = newX,
  SL.library = SL.library, method = "method.NNLS")
testSNOW
stopCluster(cl)

## snow example with user-generated wrappers
# If you write your own wrappers and are using snowSuperLearner()
# These new wrappers need to be added to the SuperLearner namespace and exported to the clusters
# Using a simple example here, but can define any new SuperLearner wrapper
my.SL.wrapper &lt;- function(...) SL.glm(...)
# assign function into SuperLearner namespace
environment(my.SL.wrapper) &lt;-asNamespace("SuperLearner")

cl &lt;- makeCluster(2, type = "PSOCK") # can use different types here
clusterSetRNGStream(cl, iseed = 2343)
# make SL functions available on the clusters, use assignment to avoid printing	
foo &lt;- clusterEvalQ(cl, library(SuperLearner))  
clusterExport(cl, c("my.SL.wrapper"))  # copy the function to all clusters
testSNOW &lt;- snowSuperLearner(cluster = cl, Y = Y, X = X, newX = newX,
  SL.library = c("SL.glm", "SL.mean", "my.SL.wrapper"), method = "method.NNLS")
testSNOW
stopCluster(cl)

## timing
replicate(5, system.time(SuperLearner(Y = Y, X = X, newX = newX,
  SL.library = SL.library, method = "method.NNLS")))

replicate(5, system.time(mcSuperLearner(Y = Y, X = X, newX = newX,
  SL.library = SL.library, method = "method.NNLS")))

cl &lt;- makeCluster(2, type = 'PSOCK')
# make SL functions available on the clusters, use assignment to avoid printing	
foo &lt;- clusterEvalQ(cl, library(SuperLearner))  
replicate(5, system.time(snowSuperLearner(cl, Y = Y, X = X, newX = newX,
  SL.library = SL.library, method = "method.NNLS")))
stopCluster(cl)


## End(Not run)
</code></pre>

<hr>
<h2 id='SuperLearner.control'>
Control parameters for the SuperLearner
</h2><span id='topic+SuperLearner.control'></span>

<h3>Description</h3>

<p>Control parameters for the <code>SuperLearner</code></p>


<h3>Usage</h3>

<pre><code class='language-R'>SuperLearner.control(saveFitLibrary = TRUE, saveCVFitLibrary = FALSE, trimLogit = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SuperLearner.control_+3A_savefitlibrary">saveFitLibrary</code></td>
<td>

<p>Logical. Should the fit for each algorithm be saved in the output from <code>SuperLearner</code>.
</p>
</td></tr>
<tr><td><code id="SuperLearner.control_+3A_savecvfitlibrary">saveCVFitLibrary</code></td>
<td>

<p>Logical. Should cross-validated fits for each algorithm be saved in the output from <code>SuperLearner</code>.
</p>
</td></tr>
<tr><td><code id="SuperLearner.control_+3A_trimlogit">trimLogit</code></td>
<td>

<p>number between 0.0 and 0.5. What level to truncate the logit transformation to maintain a bounded loss function when using the NNloglik method.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the control parameters.
</p>

<hr>
<h2 id='SuperLearner.CV.control'>
Control parameters for the cross validation steps in <code>SuperLearner</code>
</h2><span id='topic+SuperLearner.CV.control'></span>

<h3>Description</h3>

<p>Control parameters for the cross validation steps in <code>SuperLearner</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SuperLearner.CV.control(V = 10L, stratifyCV = FALSE, shuffle = TRUE, 
  validRows = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SuperLearner.CV.control_+3A_v">V</code></td>
<td>

<p>Integer. Number of splits for the V-fold cross-validation step. The default is 10. In most cases, between 10 and 20 splits works well.
</p>
</td></tr>
<tr><td><code id="SuperLearner.CV.control_+3A_stratifycv">stratifyCV</code></td>
<td>

<p>Logical. Should the data splits be stratified by a binary response? Attempts to maintain the same ratio in each training and validation sample.
</p>
</td></tr>
<tr><td><code id="SuperLearner.CV.control_+3A_shuffle">shuffle</code></td>
<td>

<p>Logical. Should the rows of <code>X</code> be shuffled before creating the splits.
</p>
</td></tr>
<tr><td><code id="SuperLearner.CV.control_+3A_validrows">validRows</code></td>
<td>

<p>A List. Use this to pass pre-specified rows for the sample splits. The length of the list should be <code>V</code> and each entry in the list should contain a vector with the row numbers of the corresponding validation sample.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the control parameters
</p>

<hr>
<h2 id='SuperLearnerNews'>Show the NEWS file for the SuperLearner package</h2><span id='topic+SuperLearnerNews'></span><span id='topic+SuperLearnerDocs'></span>

<h3>Description</h3>

<p>Show the NEWS file of the SuperLearner package. The function is simply a wrapper for the <code>RShowDoc</code> function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SuperLearnerNews(...)
SuperLearnerDocs(what = 'SuperLearnerR.pdf', ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SuperLearnerNews_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>RShowDoc</code></p>
</td></tr>
<tr><td><code id="SuperLearnerNews_+3A_what">what</code></td>
<td>
<p>specify what document to open. Currently supports the NEWS file and the PDF files 'SuperLearner.pdf' and 'SuperLearnerR.pdf'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A invisible character string given the path to the SuperLearner NEWS file
</p>

<hr>
<h2 id='trimLogit'>
truncated-probabilities logit transformation 
</h2><span id='topic+trimLogit'></span>

<h3>Description</h3>

<p>computes the logit transformation on the truncated probabilities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trimLogit(x, trim = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trimLogit_+3A_x">x</code></td>
<td>

<p>vector of probabilities.
</p>
</td></tr>
<tr><td><code id="trimLogit_+3A_trim">trim</code></td>
<td>

<p>value to truncate probabilities at. Currently symmetric truncation (trim and 1-trim).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>logit transformed values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(0.00000001, 0.0001, 0.001, 0.01, 0.1, 0.3, 0.7, 0.9, 0.99, 
  0.999, 0.9999, 0.99999999)
trimLogit(x, trim = 0.001)
data.frame(Prob = x, Logit = qlogis(x), trimLogit = trimLogit(x, 0.001))
</code></pre>

<hr>
<h2 id='write.method.template'>
Method to estimate the coefficients for the super learner
</h2><span id='topic+write.method.template'></span><span id='topic+method.template'></span><span id='topic+method.NNLS'></span><span id='topic+method.NNLS2'></span><span id='topic+method.NNloglik'></span><span id='topic+method.CC_LS'></span><span id='topic+method.CC_nloglik'></span><span id='topic+method.AUC'></span>

<h3>Description</h3>

<p>These functions contain the information on the loss function and the model to combine algorithms
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write.method.template(file = "", ...)

## a few built in options:
method.NNLS()
method.NNLS2()
method.NNloglik()
method.CC_LS()
method.CC_nloglik()
method.AUC(nlopt_method=NULL, optim_method="L-BFGS-B", bounds=c(0, Inf), normalize=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write.method.template_+3A_file">file</code></td>
<td>

<p>A connection, or a character string naming a file to print to. Passed to <code><a href="base.html#topic+cat">cat</a></code>.
</p>
</td></tr>
<tr><td><code id="write.method.template_+3A_optim_method">optim_method</code></td>
<td>

<p>Passed to the <code>optim</code> call method. See <code><a href="stats.html#topic+optim">optim</a></code> for details.
</p>
</td></tr>
<tr><td><code id="write.method.template_+3A_nlopt_method">nlopt_method</code></td>
<td>

<p>Either <code>optim_method</code> or <code>nlopt_method</code> must be provided, the other must be <code>NULL</code>
</p>
</td></tr>
<tr><td><code id="write.method.template_+3A_bounds">bounds</code></td>
<td>

<p>Bounds for parameter estimates
</p>
</td></tr>
<tr><td><code id="write.method.template_+3A_normalize">normalize</code></td>
<td>

<p>Logical. Should the parameters be normalized to sum up to 1
</p>
</td></tr>  
<tr><td><code id="write.method.template_+3A_...">...</code></td>
<td>

<p>Additional arguments passed to <code><a href="base.html#topic+cat">cat</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A <code>SuperLearner</code> method must be a list (or a function to create a list) with exactly 3 elements. The 3 elements must be named <code>require</code>, <code>computeCoef</code> and <code>computePred</code>.
</p>


<h3>Value</h3>

<p>A list containing 3 elements:
</p>
<table>
<tr><td><code>require</code></td>
<td>

<p>A character vector listing any required packages. Use <code>NULL</code> if no additional packages are required
</p>
</td></tr>
<tr><td><code>computeCoef</code></td>
<td>

<p>A function. The arguments are: <code>Z</code>, <code>Y</code>, <code>libraryNames</code>, <code>obsWeights</code>, <code>control</code>, <code>verbose</code>. The value is a list with two items: <code>cvRisk</code> and <code>coef</code>. This function computes the coefficients of the super learner. As the super learner minimizes the cross-validated risk, the loss function information is contained in this function as well as the model to combine the algorithms in <code>SL.library</code>.
</p>
</td></tr>
<tr><td><code>computePred</code></td>
<td>

<p>A function. The arguments are: <code>predY</code>, <code>coef</code>, <code>control</code>. The value is a numeric vector with the super learner predicted values.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Eric C Polley <a href="mailto:Polley.Eric@mayo.edu">Polley.Eric@mayo.edu</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+SuperLearner">SuperLearner</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>write.method.template(file = '')
</code></pre>

<hr>
<h2 id='write.screen.template'>
screening algorithms for SuperLearner
</h2><span id='topic+write.screen.template'></span><span id='topic+screen.template'></span><span id='topic+All'></span><span id='topic+screen.randomForest'></span><span id='topic+screen.SIS'></span><span id='topic+screen.ttest'></span><span id='topic+screen.corP'></span><span id='topic+screen.corRank'></span><span id='topic+screen.glmnet'></span>

<h3>Description</h3>

<p>Screening algorithms for <code>SuperLearner</code> to be used with <code>SL.library</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write.screen.template(file = "", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write.screen.template_+3A_file">file</code></td>
<td>

<p>A connection, or a character string naming a file to print to. Passed to <code><a href="base.html#topic+cat">cat</a></code>.
</p>
</td></tr>
<tr><td><code id="write.screen.template_+3A_...">...</code></td>
<td>

<p>Additional arguments passed to <code><a href="base.html#topic+cat">cat</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Explain structure of a screening algorithm here:
</p>


<h3>Value</h3>

<table>
<tr><td><code>whichVariable</code></td>
<td>

<p>A logical vector with the length equal to the number of columns in <code>X</code>.  TRUE indicates the variable (column of X) should be included.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Eric C Polley <a href="mailto:polley.eric@mayo.edu">polley.eric@mayo.edu</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+SuperLearner">SuperLearner</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>write.screen.template(file = '')
</code></pre>

<hr>
<h2 id='write.SL.template'>
Wrapper functions for prediction algorithms in SuperLearner
</h2><span id='topic+write.SL.template'></span><span id='topic+SL.template'></span><span id='topic+predict.SL.template'></span><span id='topic+SL.bayesglm'></span><span id='topic+predict.SL.bayesglm'></span><span id='topic+SL.caret'></span><span id='topic+predict.SL.caret'></span><span id='topic+SL.caret.rpart'></span><span id='topic+predict.SL.cforest'></span><span id='topic+SL.earth'></span><span id='topic+predict.SL.earth'></span><span id='topic+SL.gam'></span><span id='topic+predict.SL.gam'></span><span id='topic+SL.gbm'></span><span id='topic+predict.SL.gbm'></span><span id='topic+SL.glm.interaction'></span><span id='topic+SL.ipredbagg'></span><span id='topic+predict.SL.ipredbagg'></span><span id='topic+SL.knn'></span><span id='topic+predict.SL.knn'></span><span id='topic+SL.loess'></span><span id='topic+predict.SL.loess'></span><span id='topic+SL.logreg'></span><span id='topic+predict.SL.logreg'></span><span id='topic+SL.mean'></span><span id='topic+predict.SL.mean'></span><span id='topic+SL.nnet'></span><span id='topic+predict.SL.nnet'></span><span id='topic+SL.polymars'></span><span id='topic+predict.SL.polymars'></span><span id='topic+SL.randomForest'></span><span id='topic+predict.SL.randomForest'></span><span id='topic+SL.rpart'></span><span id='topic+SL.rpartPrune'></span><span id='topic+predict.SL.rpart'></span><span id='topic+SL.step'></span><span id='topic+predict.SL.step'></span><span id='topic+SL.step.forward'></span><span id='topic+SL.step.interaction'></span><span id='topic+SL.stepAIC'></span><span id='topic+predict.SL.stepAIC'></span><span id='topic+SL.svm'></span><span id='topic+predict.SL.svm'></span><span id='topic+SL.ridge'></span><span id='topic+predict.SL.ridge'></span><span id='topic+SL.leekasso'></span><span id='topic+predict.SL.leekasso'></span><span id='topic+SL.nnls'></span><span id='topic+predict.SL.nnls'></span>

<h3>Description</h3>

<p>Template function for SuperLearner prediction wrappers and built in options.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write.SL.template(file = "", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write.SL.template_+3A_file">file</code></td>
<td>

<p>A connection, or a character string naming a file to print to. Passed to <code><a href="base.html#topic+cat">cat</a></code>.
</p>
</td></tr>
<tr><td><code id="write.SL.template_+3A_...">...</code></td>
<td>

<p>Additional arguments passed to <code><a href="base.html#topic+cat">cat</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Describe SL.* structure here
</p>


<h3>Value</h3>

<p>A list with two elements:
</p>
<table>
<tr><td><code>pred</code></td>
<td>

<p>The predicted values for the rows in <code>newX</code>.
</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>

<p>A list. Contains all objects necessary to get predictions for new observations from specific algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Eric C Polley <a href="mailto:epolley@uchicago.edu">epolley@uchicago.edu</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+SuperLearner">SuperLearner</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>write.SL.template(file = '')
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
