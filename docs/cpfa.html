<!DOCTYPE html><html lang="en"><head><title>Help for package cpfa</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cpfa}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cpfa'>
<p>Classification with Parallel Factor Analysis</p></a></li>
<li><a href='#cpfa-internals'><p>Internal Functions for &quot;cpfa&quot;</p></a></li>
<li><a href='#cpm'>
<p>Classification Performance Measures</p></a></li>
<li><a href='#cpm.all'>
<p>Wrapper for Calculating Classification Performance Measures</p></a></li>
<li><a href='#plotcpfa'>
<p>Plot Optimal Model from Classification with Parallel Factor Analysis</p></a></li>
<li><a href='#predict.tunecpfa'>
<p>Predict Method for Tuning for Classification with Parallel Factor Analysis</p></a></li>
<li><a href='#print.tunecpfa'>
<p>Print Method for Tuning for Classification with Parallel Factor Analysis</p></a></li>
<li><a href='#tunecpfa'>
<p>Tuning for Classification with Parallel Factor Analysis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Classification with Parallel Factor Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1-7</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-02-22</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matthew A. Snodgress &lt;mattgress@protonmail.ch&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>multiway</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet, e1071, randomForest, nnet, rda, xgboost, foreach,
doParallel</td>
</tr>
<tr>
<td>Description:</td>
<td>Classification using Richard A. Harshman's Parallel Factor Analysis-1 (Parafac) model or Parallel Factor Analysis-2 (Parafac2) model fit to a three-way or four-way data array. See Harshman and Lundy (1994): &lt;<a href="https://doi.org/10.1016%2F0167-9473%2894%2990132-5">doi:10.1016/0167-9473(94)90132-5</a>&gt;. Uses component weights from one mode of a Parafac or Parafac2 model as features to tune parameters for one or more classification methods via a k-fold cross-validation procedure. Allows for constraints on different tensor modes. Supports penalized logistic regression, support vector machine, random forest, feed-forward neural network, regularized discriminant analysis, and gradient boosting machine. Supports binary and multiclass classification. Predicts class labels or class probabilities and calculates multiple classification performance measures. Implements parallel computing via the 'parallel' and 'doParallel' packages.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-22 19:37:55 UTC; nr2</td>
</tr>
<tr>
<td>Author:</td>
<td>Matthew A. Snodgress [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-22 23:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='cpfa'>
Classification with Parallel Factor Analysis
</h2><span id='topic+cpfa'></span>

<h3>Description</h3>

<p>Fits Richard A. Harshman's Parallel Factor Analysis-1 (Parafac) model or Parallel Factor Analysis-2 (Parafac2) model to a three-way or four-way data array. Allows for different constraint options on multiple tensor modes. Uses Parafac component weights from a single mode of this model as predictors to tune parameters for one or more classification methods via a k-fold cross-validation procedure. Predicts class labels and calculates multiple performance measures for binary or multiclass classification over some number of replications with different train-test splits. Provides descriptive statistics to pool output across replications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpfa(x, y, model = c("parafac", "parafac2"), nfac = 1, nrep = 5, ratio = 0.8,
     nfolds = 10, method = c("PLR", "SVM", "RF", "NN", "RDA", "GBM"), 
     family = c("binomial", "multinomial"), parameters = list(), 
     type.out = c("measures", "descriptives"), foldid = NULL, 
     prior = NULL, cmode = NULL, seeds = NULL, plot.out = FALSE, 
     plot.measures = NULL, parallel = FALSE, cl = NULL, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cpfa_+3A_x">x</code></td>
<td>

<p>A three-way or four-way data array. For Parafac2, can be a list of length <code>K</code> where the <code>k</code>-th element is a matrix or three-way array associated with the <code>k</code>-th element. Array or list must contain only real numbers. See note below.
</p>
</td></tr> 
<tr><td><code id="cpfa_+3A_y">y</code></td>
<td>

<p>A vector containing at least two unique class labels. Should be a factor that contains two or more levels . For binary case, ensure the order of factor levels (left to right) is such that negative class is first and positive class is second.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_model">model</code></td>
<td>

<p>Character designating the Parafac model to use, either <code>model = "parafac"</code> to fit the Parafac model or <code>model = "parafac2"</code> to fit the Parafac2 model.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_nfac">nfac</code></td>
<td>

<p>Number of components for each Parafac or Parafac2 model to fit. Default is <code>nfac = 1</code>.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_nrep">nrep</code></td>
<td>

<p>Number of replications to repeat the procedure. Default is <code>nrep = 5</code>.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_ratio">ratio</code></td>
<td>

<p>Split ratio for dividing data into train and test sets. Default is <code>ratio = 0.8</code>.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_nfolds">nfolds</code></td>
<td>

<p>Numeric setting number of folds for k-fold cross-validation. Must be 2 or greater. Default is <code>nfolds = 10</code>.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_method">method</code></td>
<td>

<p>Character vector indicating classification methods to use. Possible methods include penalized logistic regression (PLR); support vector machine (SVM); random forest (RF); feed-forward neural network (NN); regularized discriminant analysis (RDA); and gradient boosting machine (GBM). If none are selected, default is to use all methods with <code>method = c("PLR", "SVM", "RF", "NN", "RDA", "GBM")</code>.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_family">family</code></td>
<td>

<p>Character value specifying binary classification (<code>family = "binomial"</code>) or multiclass classification (<code>family = "multinomial"</code>). If not provided, number of levels of input <code>y</code> is used, where two levels is binary, and where three or more levels is multiclass.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_parameters">parameters</code></td>
<td>

<p>List containing arguments related to classification methods. When specified, must contain one or more of the following:
</p>

<dl>
<dt>alpha</dt><dd>
<p>Values for penalized logistic regression alpha parameter; default is <code>alpha = seq(0, 1, length = 6)</code>. Must be numeric and contain only real numbers between 0 and 1, inclusive.
</p>
</dd>
<dt>lambda</dt><dd>
<p>Optional user-supplied lambda sequence for <code>cv.glmnet</code> for penalized logistic regression. Default is NULL.
</p>
</dd>
<dt>cost</dt><dd>
<p>Values for support vector machine cost parameter; default is <code>cost = c(1, 2, 4, 8, 16, 32, 64)</code>. Must be numeric and contain only real numbers greater than or equal to zero.
</p>
</dd>
<dt>gamma</dt><dd>
<p>Values for support vector machine gamma parameter; default is <code>gamma = c(0, 0.01, 0.1, 1, 10, 100, 1000)</code>. Must be numeric and greater than or equal to 0.
</p>
</dd>
<dt>ntree</dt><dd>
<p>Values for random forest number of trees parameter; default is <code>ntree = c(100, 200, 400, 600, 800, 1600, 3200)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
<dt>nodesize</dt><dd>
<p>Values for random forest node size parameter; default is <code>nodesize = c(1, 2, 4, 8, 16, 32, 64)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
<dt>size</dt><dd>
<p>Values for neural network size parameter; default is <code>size = c(1, 2, 4, 8, 16, 32, 64)</code>. Must be numeric and contain only integers greater than or equal to 0.
</p>
</dd>
<dt>decay</dt><dd>
<p>Values for neural network decay parameter; default is <code>decay = c(0.001, 0.01, 0.1, 1, 2, 4, 8, 16)</code>. Must be numeric and contain only real numbers.
</p>
</dd>
<dt>rda.alpha</dt><dd>
<p>Values for regularized discriminant analysis alpha parameter; default is <code>rda.alpha = seq(0, 0.999, length = 6)</code>. Must be numeric and contain only real numbers between 0 (inclusive) and 1 (exclusive).
</p>
</dd>
<dt>delta</dt><dd>
<p>Values for regularized discriminant analysis delta parameter; default is <code>delta = c(0, 0.1, 1, 2, 3, 4)</code>. Must be numeric and contain only real numbers greater than or equal to 0.
</p>
</dd>
<dt>eta</dt><dd>
<p>Values for gradient boosting machine eta parameter; default is <code>eta = c(0.1, 0.3, 0.5, 0.7, 0.9)</code>. Must be numeric and contain only real numbers greater than 0 and less than 1.
</p>
</dd>
<dt>max.depth</dt><dd>
<p>Values for gradient boosting machine max.depth parameter; default is <code>max.depth = c(1, 2, 3, 4)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
<dt>subsample</dt><dd>
<p>Values for gradient boosting machine subsample parameter; default is <code>subsample = c(0.6, 0.7, 0.8, 0.9)</code>. Must be numeric and contain only real numbers greater than 0 and less than or equal to 1.
</p>
</dd>
<dt>nrounds</dt><dd>
<p>Values for gradient boosting machine nrounds parameter; default is <code>nrounds = c(100, 200, 300, 500)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="cpfa_+3A_type.out">type.out</code></td>
<td>

<p>Type of output desired: <code>type.out = "measures"</code> gives array containing classification performance measures for all replications while <code>type.out = "descriptives"</code> gives list of descriptive statistics calculated across all replications for each performance measure. Both options also provide the estimated training weights and classification weights. Defaults to <code>type.out = "descriptives"</code>.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_foldid">foldid</code></td>
<td>

<p>Integer vector containing fold IDs for k-fold cross-validation. If not provided, fold IDs are generated randomly for number of folds <code>nfolds</code>.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_prior">prior</code></td>
<td>

<p>Prior probabilities of class membership. If unspecified, the class proportions for input <code>y</code> are used. If specified, the probabilities should be in the order of the factor levels of input <code>y</code>.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_cmode">cmode</code></td>
<td>

<p>Integer value of 1, 2, or 3 (or 4 if <code>x</code> is a four-way array) specifying the mode whose component weights will be predictors for classification. Defaults to the last mode of the inputted array (i.e., defaults to 3 for three-way array, and to 4 for four-way array). If <code>model = "parafac2"</code>, last mode will be used.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_seeds">seeds</code></td>
<td>

<p>Random seeds to be associated with each replication. Default is <code>seeds = 1:nrep</code>.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_plot.out">plot.out</code></td>
<td>

<p>Logical indicating whether to output one or more box plots of classification performance measures that are plotted across classification methods and number of components.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_plot.measures">plot.measures</code></td>
<td>

<p>Character vector containing values that specify for plotting one or more of 11 possible classification performance measures. Only relevant when <code>plot.out = T</code>. Should contain one or more of the following labels: <code>c("err", "acc", "tpr", "fpr", "tnr", "fnr", "ppv", "npv", "fdr", "fom", "fs")</code>. A box plot will be created for each measure that is specified, summarizing output across replications. Note that additional information about each label is available in the Details section of the help file for function <code>cpm</code>. Note also that there are a few cases where the x-axis tick labels for a plot might not appear. This issue will be resolved in a future update.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_parallel">parallel</code></td>
<td>

<p>Logical indicating if parallel computing should be implemented. If TRUE, the package <b>parallel</b> is used for parallel computing. For all classification methods except penalized logistic regression, the <b>doParallel</b> package is used as a wrapper. Defaults to FALSE, which implements sequential computing.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_cl">cl</code></td>
<td>

<p>Cluster for parallel computing, which is used when <code>parallel = T</code>. Note that if <code>parallel = T</code> and <code>cl = NULL</code>, then the cluster is defined as <code>makeCluster(detectCores())</code>.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_verbose">verbose</code></td>
<td>

<p>If TRUE, progress is printed.
</p>
</td></tr>
<tr><td><code id="cpfa_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed to function <code>parafac</code> for fitting a Parafac model or function <code>parafac2</code> for fitting a Parafac2 model. Example: can impose different constraints on different modes of the input array using the argument <code>const</code>. See help file for function <code>parafac</code> or for function <code>parafac2</code> for additional details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data are split into a training set and a testing set. After fitting a Parafac or Parafac2 model with the training set using package <b>multiway</b> (see <code>parafac</code> or <code>parafac2</code> in <b>multiway</b> for details), the estimated classification mode weight matrix is passed to one or several of six classification methods. The methods include: penalized logistic regression (PLR); support vector machine (SVM); random forest (RF); feed-forward neural network (NN); regularized discriminant analysis (RDA); and gradient boosting machine (GBM).
</p>
<p>Package <b>glmnet</b> fits models for PLR. PLR tunes penalty parameter lambda while the elastic net parameter alpha is set by the user (see the help file for function <code>cv.glmnet</code> in package <b>glmnet</b>). For SVM, package <b>e1071</b> is used with a radial basis kernel. Penalty parameter cost and radial basis parameter gamma are used (see <code>svm</code> in package <b>e1071</b>). For RF, package <b>randomForest</b> is used and implements Breiman's random forest algorithm. The number of predictors sampled at each node split is set at the default of sqrt(R), where R is the number of Parafac or Parafac2 components. Two tuning parameters allowed are ntree, the number of trees to be grown, and nodesize, the minimum size of terminal nodes (see <code>randomForest</code> in package <b>randomForest</b>). For NN, package <b>nnet</b> fits a single-hidden-layer, feed-forward neural network model. Penalty parameters size (i.e., number of hidden layer units) and decay (i.e., weight decay) are used (see <b>nnet</b>). For RDA, package <b>rda</b> fits a shrunken centroids regularized discriminant analysis model. Tuning parameters include rda.alpha, the shrinkage penalty for the within-class covariance matrix, and delta, the shrinkage penalty of class centroids towards the overall dataset centroid. For GBM, package <b>xgboost</b> fits a gradient boosting machine model. Four tuning parameters are allowed: (1) eta, the learning rate; (2) max.depth, the maximum tree depth; (3) subsample, the fraction of samples per tree; and (4) nrounds, the number of boosting trees to build.
</p>
<p>For all six methods, k-fold cross-validation is implemented to tune classification parameters where the number of folds is set by argument <code>nfolds</code>. Separately, the trained Parafac or Parafac2 model is used to predict the classification mode's component weights using the testing set data. The predicted component weights and the optimized classification method are then used to predict class labels. Finally, classification performance measures are calculated. The process is repeated over a number of replications with different random splits of the input array and of the class labels at each replication.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>wrapcpfa</code> either with a three-way array with classification performance measures for each model and for each replication, or with a list containing matrices with descriptive statistics for performance measures calculated across all replications. Specify <code>type.out = "measures"</code> to output the array of performance measures. Specify <code>type.out = "descriptives"</code> to output descriptive statistics across replications. In addition, for both options, the following are also provided:
</p>
<table role = "presentation">
<tr><td><code>predweights</code></td>
<td>

<p>List of predicted classification weights for each Parafac or Parafac2 model and for each replication.
</p>
</td></tr>
<tr><td><code>train.weights</code></td>
<td>

<p>List of lists of training weights for each Parafac or Parafac2 model and for each replication.
</p>
</td></tr>
<tr><td><code>opt.tune</code></td>
<td>

<p>List of optimal tuning parameters for classification methods for each Parafac or Parafac2 model and for each replication.
</p>
</td></tr>
<tr><td><code>mean.opt.tune</code></td>
<td>

<p>Mean across all replications of optimal tuning parameters for classification methods for each Parafac or Parafac2 model. 
</p>
</td></tr>
<tr><td><code>X</code></td>
<td>

<p>Three-way or four-way data array or list used in argument <code>x</code>.
</p>
</td></tr>
<tr><td><code>nfac</code></td>
<td>

<p>Number of components used to fit each Parafac or Parafac2 model.
</p>
</td></tr>
<tr><td><code>model</code></td>
<td>

<p>Character designating the Parafac model that was used, either <code>model = "parafac"</code> for the Parafac model or <code>model = "parafac2"</code> for the Parafac2 model.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>Classification methods used.
</p>
</td></tr>
<tr><td><code>const</code></td>
<td>

<p>Constraints used in fitting Parafac or Parafac2 models.
</p>
</td></tr>
<tr><td><code>cmode</code></td>
<td>

<p>Integer value used to specify the mode whose component weights were predictors for classification. 
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If argument <code>cmode</code> is not null, input array <code>x</code> is reshaped with function <code>aperm</code> such that the <code>cmode</code> dimension of <code>x</code> is ordered last. Estimated mode A and B (and mode C for a four-way array) weights that are outputted as <code>Aweights</code> and <code>Bweights</code> (and <code>Cweights</code>) reflect this permutation. For example, if <code>x</code> is a four-way array and <code>cmode = 2</code>, the original input modes 1, 2, 3, and 4 will correspond to output modes 1, 3, 4, 2. Here, output A = input 1; B = 3, and C = 4 (i.e., the second mode specified by <code>cmode</code> has been moved to the D mode/last mode). For <code>model = "parafac2"</code>, classification mode is assumed to be the last mode (i.e., mode C for three-way array and mode D for four-way array). 
</p>
<p>In addition, note that the following combination of arguments will give an error: <code>nfac = 1, family = "multinomial", method = "PLR"</code>. The issue arises from providing <code>glmnet::cv.glmnet</code> input <code>x</code> a matrix with a single column. The issue is resolved for <code>family = "binomial"</code> because a column of 0s is appended to the single column, but this solution does not appear to work for the multiclass case. As such, this combination of arguments is not currently allowed. This issue will be resolved in a future update.
</p>


<h3>Author(s)</h3>

<p>Matthew A. Snodgress &lt;snodg031@umn.edu&gt;
</p>


<h3>References</h3>

<p>Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.
</p>
<p>Chen, T., He, T., Benesty, M., Khotilovich, V., Tang, Y., Cho, H., Chen, K., Mitchell, R., Cano, I., Zhou, T., Li, M., Xie, J., Lin, M., Geng, Y., Li, Y., Yuan, J. (2024). xgboost: Extreme gradient boosting. R Package Version 1.7.7.1.
</p>
<p>Cortes, C. and Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3), 273-297.
</p>
<p>Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of Statistics, 29(5), 1189-1232.
</p>
<p>Friedman, J. H. (1989). Regularized discriminant analysis. Journal of the American Statistical Association, 84(405), 165-175.
</p>
<p>Friedman, J. Hastie, T., and Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 1-22.
</p>
<p>Guo, Y., Hastie, T., and Tibshirani, R. (2007). Regularized linear discriminant analysis and its application in microarrays. Biostatistics, 8(1), 86-100.
</p>
<p>Guo Y., Hastie T., and Tibshirani, R. (2023). rda: Shrunken centroids regularized discriminant analysis. R Package Version 1.2-1.
</p>
<p>Harshman, R. (1970). Foundations of the PARAFAC procedure: Models and conditions for an &quot;explanatory&quot; multimodal factor analysis. UCLA Working Papers in Phonetics, 16, 1-84.
</p>
<p>Harshman, R. (1972). PARAFAC2: Mathematical and technical notes. UCLA Working Papers in Phonetics, 22, 30-44.
</p>
<p>Harshman, R. and Lundy, M. (1994). PARAFAC: Parallel factor analysis. Computational Statistics and Data Analysis, 18, 39-72.
</p>
<p>Helwig, N. (2017). Estimating latent trends in multivariate longitudinal data via Parafac2 with functional and structural constraints. Biometrical Journal, 59(4), 783-803.
</p>
<p>Helwig, N. (2019). multiway: Component models for multi-way data. R Package Version 1.0-6.
</p>
<p>Liaw, A. and Wiener, M. (2002). Classification and regression by randomForest. R News 2(3), 18&ndash;22.
</p>
<p>Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., and Leisch, F. (2023). e1071: Misc functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. R Package Version 1.7-13.
</p>
<p>Ripley, B. (1994). Neural networks and related methods for classification. Journal of the Royal Statistical Society: Series B (Methodological), 56(3), 409-437.
</p>
<p>Venables, W. and Ripley, B. (2002). Modern applied statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0.
</p>
<p>Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301-320.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>########## Parafac2 example with 4-way array and multiclass response ##########
## Not run: 
# set seed and specify dimensions of a four-way tensor
set.seed(5)
mydim &lt;- c(10, 11, 12, 100)
nf &lt;- 3

# create correlation matrix between response and fourth mode's weights 
rho.dd &lt;- .35 
rho.dy &lt;- .75 
cormat.values &lt;- c(1, rho.dd, rho.dd, rho.dy, rho.dd, 1, rho.dd, rho.dy, 
                   rho.dd, rho.dd, 1, rho.dy, rho.dy, rho.dy, rho.dy, 1)
cormat &lt;- matrix(cormat.values, nrow = (nf + 1), ncol = (nf + 1))

# sample from a multivariate normal with specified correlation structure
ymean &lt;- Dmean &lt;- 2
mu &lt;- as.matrix(c(Dmean, Dmean, Dmean, ymean))
eidecomp &lt;- eigen(cormat, symmetric = TRUE)
L.sqrt &lt;- diag(eidecomp$values^0.5)
cormat.sqrt &lt;- eidecomp$vectors %*% L.sqrt %*% t(eidecomp$vectors)
Z &lt;- matrix(rnorm(mydim[4] * (nf + 1)), nrow = mydim[4], ncol = (nf + 1))
Xw &lt;- rep(1, mydim[4]) %*% t(mu) + Z %*% cormat.sqrt
Dmat &lt;- Xw[, 1:nf]

# create a random four-way data tensor with D weights related to a response
Bmat &lt;- matrix(runif(mydim[2] * nf), nrow = mydim[2], ncol = nf)
Cmat &lt;- matrix(runif(mydim[3] * nf), nrow = mydim[3], ncol = nf)
nDd &lt;- rep(c(10, 12, 14), length.out = mydim[4])
Gmat &lt;- matrix(rnorm(nf * nf), nrow = nf)
Amat &lt;- vector("list", mydim[4])
X &lt;- Xmat &lt;- Emat &lt;- Amat
for (Dd in 1:mydim[4]) {
   Amat[[Dd]] &lt;- matrix(nf * rnorm(nDd[Dd]), nrow = nDd[Dd], ncol = nf)
   Amat[[Dd]] &lt;- svd(Amat[[Dd]], nv = 0)$u %*% Gmat
   leftMat &lt;- Amat[[Dd]] %*% diag(Dmat[Dd,])
   Xmat[[Dd]] &lt;- array(tcrossprod(leftMat, krprod(Cmat, Bmat)), 
                       dim = c(nDd[Dd], mydim[2], mydim[3]))
   Emat[[Dd]] &lt;- array(rnorm(nDd[Dd] * mydim[2] * mydim[3]), 
                       dim = c(nDd[Dd], mydim[2], mydim[3]))
   X[[Dd]] &lt;- Xmat[[Dd]] + Emat[[Dd]]
}

# create a multiclass response
stor &lt;- matrix(rep(1, nrow(Xw)), nrow = nrow(Xw))
stor[which(Xw[, (nf + 1)] &lt; (ymean - 0.4 * sd(Xw[, (nf + 1)])))] &lt;- 2
stor[which(Xw[, (nf + 1)] &gt; (ymean + 0.4 * sd(Xw[, (nf + 1)])))] &lt;- 0
y &lt;- factor(stor)

# initialize
alpha &lt;- seq(0, 1, length = 2)
gamma &lt;- c(0, 1)
cost &lt;- c(0.1, 5)
ntree &lt;- c(200, 300)
nodesize &lt;- c(1, 2)
size &lt;- c(1, 2)
decay &lt;- c(0, 1)
rda.alpha &lt;- seq(0.1, 0.9, length = 2)
delta &lt;- c(0.1, 2)
eta &lt;- c(0.3, 0.7)
max.depth &lt;- c(1, 2)
subsample &lt;- c(0.75)
nrounds &lt;- c(100)
method &lt;- c("PLR", "SVM", "RF", "NN", "RDA", "GBM")
family &lt;- "multinomial"
parameters &lt;- list(alpha = alpha, gamma = gamma, cost = cost, ntree = ntree,
                   nodesize = nodesize, size = size, decay = decay, 
                   rda.alpha = rda.alpha, delta = delta, eta = eta,
                   max.depth = max.depth, subsample = subsample,
                   nrounds = nrounds)
model &lt;- "parafac2"
nfolds &lt;- 3
nstart &lt;- 3

# constrain first mode weights to be orthogonal, fourth mode to be nonnegative
const &lt;- c("orthog", "uncons", "uncons", "nonneg")

# fit Parafac2 model and use fourth mode weights to tune classification
# methods, to predict class labels, and to return classification 
# performance measures pooled across multiple train-test splits
output &lt;- cpfa(x = X, y = y, model = model, nfac = nf, nrep = 2, ratio = 0.8, 
               nfolds = nfolds, method = method, family = family, 
               parameters = parameters, type.out = "descriptives", 
               seeds = NULL, plot.out = TRUE, parallel = FALSE, const = const,
               nstart = nstart)

# print performance measure means across train-test splits
output$descriptive$mean

## End(Not run)
</code></pre>

<hr>
<h2 id='cpfa-internals'>Internal Functions for &quot;cpfa&quot;</h2><span id='topic+kcv.plr'></span><span id='topic+kcv.svm'></span><span id='topic+kcv.rf'></span><span id='topic+kcv.nn'></span><span id='topic+kcv.rda'></span><span id='topic+kcv.gbm'></span><span id='topic+kcvcheck'></span>

<h3>Description</h3>

<p>Internal functions for the &quot;cpfa&quot; package.
</p>


<h3>Details</h3>

<p>These functions are not intended to be called by the user.
</p>

<hr>
<h2 id='cpm'>
Classification Performance Measures
</h2><span id='topic+cpm'></span>

<h3>Description</h3>

<p>Calculates multiple performance measures for binary or multiclass classification. Uses known class labels and evaluates against predicted labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpm(x, y, level = NULL, fbeta = NULL, prior = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cpm_+3A_x">x</code></td>
<td>

<p>Known class labels of class numeric, factor, or integer. If factor, converted to class integer in the order of factor levels with integers beginning at 0 (i.e., for binary classification, factor levels become 0 and 1; for multiclass, levels become 0, 1, 2, etc.). 
</p>
</td></tr>
<tr><td><code id="cpm_+3A_y">y</code></td>
<td>

<p>Predicted class labels of class numeric, factor, or integer. If factor, converted to class integer in the order of factor levels with integers beginning at 0 (i.e., for binary classification, factor levels become 0 and 1; for multiclass, 0, 1, 2, etc.).
</p>
</td></tr>
<tr><td><code id="cpm_+3A_level">level</code></td>
<td>

<p>Optional argument specifying possible class labels. For cases when <code>x</code> or <code>y</code> do not contain all possible classes. Can be of class numeric, integer, or character. Must contain two elements for binary classification, and contain three or more elements for multiclass classification. If integer, integers should be ordered (e.g., binary with <code>c(0, 1)</code>; or three-class with <code>c(0, 1, 2)</code>). Note: if both <code>x</code> and <code>y</code> jointly contain only a single value (e.g., 1), must specify argument <code>level</code> in order to identify classification as binary or multiclass. 
</p>
</td></tr>
<tr><td><code id="cpm_+3A_fbeta">fbeta</code></td>
<td>

<p>Optional numeric argument specifying beta value for F-score. Defaults to <code>fbeta = 1</code>, providing an F1-score (i.e., the balanced harmonic mean between precision and recall). Can be any real number.
</p>
</td></tr>
<tr><td><code id="cpm_+3A_prior">prior</code></td>
<td>

<p>Optional numeric argument specifying weights for classes. Currently only implemented with multiclass problems. Defaults to <code>prior = c(rep(1/llev, llev))</code>, where <code>llev</code> is the number of classes, providing equal importance across classes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Selecting one class as a negative class and one class as a positive class, binary classification generates four possible outcomes: (1) negative cases classified as positives, called false positives (FP); (2) negative cases classified as negatives, called true negatives (TN); (3) positive cases classified as negatives, called false negatives (FN); and (4) positive cases classified as positives, called true positives (TP). 
</p>
<p>Multiple evaluation measures are calculated using these four outcomes. Measures include: overall error (ERR), also called fraction incorrect;  overall accuracy (ACC), also called fraction correct; true positive rate (TPR), also called recall, hit rate, or sensitivity; false negative rate (FNR), also called miss rate; false positive rate (FPR), also called fall-out; true negative rate (TNR), also called specificity or selectivity; positive predictive value (PPV), also called precision; false discovery rate (FDR); negative predictive value (NPV); false omission rate (FOR); and F-score (FS).
</p>
<p>In multiclass classification, the four outcomes are possible for each individual class in macro-averaging, and performance measures are averaged over classes. Macro-averaging gives equal importance to all classes. For multiclass classification, calculated measures are currently only macro-averaged. See the listed reference in this help file for additional details on micro-averaging.
</p>
<p>For binary classification, this function assumes a negative class and a positive class (i.e., it contains a reference group) and is ordered. Multiclass classification is currently assumed to be unordered. 
</p>
<p>Computational details:
</p>
<p>ERR = (FP + FN) / (TP + TN + FP + FN).
</p>
<p>ACC = (TP + TN) / (TP + TN + FP + FN), and ACC = 1 - ERR.
</p>
<p>TPR = TP / (TP + FN).
</p>
<p>FNR = FN / (FN + TP), and FNR = 1 - TPR.
</p>
<p>FPR = FP / (FP + TN).
</p>
<p>TNR = TN / (TN + FP), and TNR = 1 - FPR.
</p>
<p>PPV = TP / (TP + FP).
</p>
<p>FDR = FP / (FP + TP), and FDR = 1 - PPV.
</p>
<p>NPV = TN / (TN + FN).
</p>
<p>FOR = FN / (FN + TN), and FOR = 1 - NPV.
</p>
<p>FS = (1 + beta^2) * ((PPV * TPR) / (((beta^2)*PPV) + TPR)).
</p>
<p>All performance measures calculated are between 0 and 1, inclusive. For multiclass classification, macro-averaged values are provided for each performance measure. Note that 'beta' in FS represents the relative weight such that recall (TPR) is beta times more important than precision (PPV). See reference for more details.
</p>


<h3>Value</h3>

<p>Returns list where first element is a full confusion matrix <code>cm</code> and where the second element is a data frame containing performance measures. For multiclass classification, macro-averaged values are provided (i.e., each measure is calculated for each class, then averaged over all classes; the average is weighted by argument <code>prior</code> if provided). The second list element contains the following performance measures:
</p>
<table role = "presentation">
<tr><td><code>cm</code></td>
<td>

<p>A confusion matrix with counts for each of the possible outcomes.
</p>
</td></tr>
<tr><td><code>err</code></td>
<td>

<p>Overall error (ERR). Also called fraction incorrect.
</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>

<p>Overall accuracy (ACC). Also called fraction correct.
</p>
</td></tr>
<tr><td><code>tpr</code></td>
<td>

<p>True positive rate (TPR). Also called recall, hit rate, or sensitivity.
</p>
</td></tr>
<tr><td><code>fpr</code></td>
<td>

<p>False positive rate (FPR). Also called fall-out.
</p>
</td></tr>
<tr><td><code>tnr</code></td>
<td>

<p>True negative rate (TNR). Also called specificity or selectivity.
</p>
</td></tr>
<tr><td><code>fnr</code></td>
<td>

<p>False negative rate (FNR). Also called miss rate.
</p>
</td></tr>
<tr><td><code>ppv</code></td>
<td>

<p>Positive predictive value (PPV). Also called precision.
</p>
</td></tr>
<tr><td><code>npv</code></td>
<td>

<p>Negative predicted value (NPV).
</p>
</td></tr>
<tr><td><code>fdr</code></td>
<td>

<p>False discovery rate (FDR).
</p>
</td></tr>
<tr><td><code>fom</code></td>
<td>

<p>False omission rate (FOR).
</p>
</td></tr>
<tr><td><code>fs</code></td>
<td>

<p>F-score. Mean between TPR (recall) and PPV (precision) varying by importance 
given to recall over precision (see Details section and argument <code>fbeta</code>). 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthew Snodgress &lt;snodg031@umn.edu&gt;
</p>


<h3>References</h3>

<p>Sokolova, M. and Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks. Information Processing and Management, 45(4), 427-437.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>########## Parafac example with 3-way array and binary response ##########
## Not run: 
# set seed and specify dimensions of a three-way tensor
set.seed(3)
mydim &lt;- c(10, 11, 80)
nf &lt;- 3

# create correlation matrix between response and third mode's weights 
rho.cc &lt;- .35 
rho.cy &lt;- .75 
cormat.values &lt;- c(1, rho.cc, rho.cc, rho.cy, rho.cc, 1, rho.cc, rho.cy, 
                   rho.cc, rho.cc, 1, rho.cy, rho.cy, rho.cy, rho.cy, 1)
cormat &lt;- matrix(cormat.values, nrow = (nf + 1), ncol = (nf + 1))

# sample from a multivariate normal with specified correlation structure
ymean &lt;- Cmean &lt;- 2
mu &lt;- as.matrix(c(Cmean, Cmean, Cmean, ymean))
eidecomp &lt;- eigen(cormat, symmetric = TRUE)
L.sqrt &lt;- diag(eidecomp$values^0.5)
cormat.sqrt &lt;- eidecomp$vectors %*% L.sqrt %*% t(eidecomp$vectors)
Z &lt;- matrix(rnorm(mydim[3]*(nf + 1)), nrow = mydim[3], ncol = (nf + 1))
Xw &lt;- rep(1, mydim[3]) %*% t(mu) + Z %*% cormat.sqrt
Cmat &lt;- Xw[, 1:nf]

# create a random three-way data tensor with C weights related to a response
Amat &lt;- matrix(rnorm(mydim[1]*nf), nrow = mydim[1], ncol = nf)
Bmat &lt;- matrix(runif(mydim[2]*nf), nrow = mydim[2], ncol = nf)
Xmat &lt;- tcrossprod(Amat, krprod(Cmat, Bmat))
Xmat &lt;- array(Xmat, dim = mydim)
Emat &lt;- array(rnorm(prod(mydim)), dim = mydim)
Emat &lt;- nscale(Emat, 0, ssnew = sumsq(Xmat))  
X &lt;- Xmat + Emat

# create a binary response by dichotomizing at the specified response mean
y &lt;- factor(as.numeric(Xw[ , (nf + 1)] &gt; ymean))

# initialize
gamma &lt;- c(0, 0.01)
cost &lt;- c(1, 2)
method &lt;- c("SVM")
family &lt;- "binomial"
parameters &lt;- list(gamma = gamma, cost = cost)
model &lt;- "parafac"
nfolds &lt;- 3
nstart &lt;- 3

# constrain first mode weights to be orthogonal
const &lt;- c("orthog", "uncons", "uncons")

# fit Parafac models and use third mode to tune classification methods
tune.object &lt;- tunecpfa(x = X, y = y, model = model, nfac = nf, 
                        nfolds = nfolds, method = method, family = family, 
                        parameters = parameters, parallel = FALSE, 
                        const = const, nstart = nstart)
                         
# create new data with Parafac structure and C weights related to response
mydim.new &lt;- c(10, 11, 20)
Znew &lt;- matrix(rnorm(mydim.new[3]*(nf + 1)), 
               nrow = mydim.new[3], ncol = (nf + 1))
Xwnew &lt;- rep(1, mydim.new[3]) %*% t(mu) + Znew %*% cormat.sqrt
Cmatnew &lt;- Xwnew[, 1:nf]
Xnew0 &lt;- tcrossprod(Amat, krprod(Cmatnew, Bmat))
Xnew0 &lt;- array(Xnew0, dim = mydim.new)
Ematnew &lt;- array(rnorm(prod(mydim.new)), dim = mydim.new)
Ematnew &lt;- nscale(Ematnew, 0, ssnew = sumsq(Xnew0))  
Xnew &lt;- Xnew0 + Ematnew

# create new random class labels for two levels
newlabel &lt;- as.numeric(Xwnew[, (nf + 1)] &gt; ymean)

# predict class labels
predict.labels &lt;- predict(object = tune.object, newdata = Xnew, 
                          type = "response")
                        
# calculate performance measures for predicted class labels
y.pred &lt;- predict.labels[, 1]
evalmeasure &lt;- cpm(x = newlabel, y = y.pred)

# print performance measures
evalmeasure

## End(Not run)
</code></pre>

<hr>
<h2 id='cpm.all'>
Wrapper for Calculating Classification Performance Measures
</h2><span id='topic+cpm.all'></span>

<h3>Description</h3>

<p>Applies function <code>cpm</code> to multiple sets of class labels. Each set of class labels is evaluated against the same set of predicted labels. Works with output from function <code>predict.tunecpfa</code> and calculates classification performance measures for multiple classifiers or numbers of components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpm.all(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cpm.all_+3A_x">x</code></td>
<td>

<p>A data frame where each column contains a set of known class labels of class numeric, factor, or integer. If a set is of class factor, that set is converted to class integer in the order of factor levels with integers beginning at 0 (i.e., for binary classification, factor levels become 0 and 1; for multiclass, levels become 0, 1, 2, etc.). 
</p>
</td></tr>
<tr><td><code id="cpm.all_+3A_y">y</code></td>
<td>

<p>Predicted class labels of class numeric, factor, or integer. If factor, converted to class integer in order of factor levels with integers beginning at 0 (i.e., for binary classification, factor levels become 0 and 1; for multiclass, 0, 1, 2, etc.).
</p>
</td></tr>
<tr><td><code id="cpm.all_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed to function <code>cpm</code> for calculating classification performance measures.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Wrapper function that applies function <code>cpm</code> to multiple sets of class labels and one set of predicted labels. See help file for function <code>cpm</code> for additional details.
</p>


<h3>Value</h3>

<p>Returns a list with the following two elements: 
</p>
<table role = "presentation">
<tr><td><code>cm.list</code></td>
<td>

<p>A list of confusion matrices, denoted <code>cm</code>, where each confusion matrix is associated with one comparison.
</p>
</td></tr>
<tr><td><code>cpms</code></td>
<td>

<p>A data frame containing classification performance measures where each row contains measures for one comparison.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthew Snodgress &lt;snodg031@umn.edu&gt;
</p>


<h3>References</h3>

<p>Sokolova, M. and Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks. Information Processing and Management, 45(4), 427-437.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>########## Parafac example with 3-way array and binary response ##########
## Not run: 
# set seed and specify dimensions of a three-way tensor
set.seed(3)
mydim &lt;- c(10, 11, 80)
nf &lt;- 3

# create correlation matrix between response and third mode's weights 
rho.cc &lt;- .35 
rho.cy &lt;- .75 
cormat.values &lt;- c(1, rho.cc, rho.cc, rho.cy, rho.cc, 1, rho.cc, rho.cy, 
                   rho.cc, rho.cc, 1, rho.cy, rho.cy, rho.cy, rho.cy, 1)
cormat &lt;- matrix(cormat.values, nrow = (nf + 1), ncol = (nf + 1))

# sample from a multivariate normal with specified correlation structure
ymean &lt;- Cmean &lt;- 2
mu &lt;- as.matrix(c(Cmean, Cmean, Cmean, ymean))
eidecomp &lt;- eigen(cormat, symmetric = TRUE)
L.sqrt &lt;- diag(eidecomp$values^0.5)
cormat.sqrt &lt;- eidecomp$vectors %*% L.sqrt %*% t(eidecomp$vectors)
Z &lt;- matrix(rnorm(mydim[3] * (nf + 1)), nrow = mydim[3], ncol = (nf + 1))
Xw &lt;- rep(1, mydim[3]) %*% t(mu) + Z %*% cormat.sqrt
Cmat &lt;- Xw[, 1:nf]

# create a random three-way data tensor with C weights related to a response
Amat &lt;- matrix(rnorm(mydim[1] * nf), nrow = mydim[1], ncol = nf)
Bmat &lt;- matrix(runif(mydim[2] * nf), nrow = mydim[2], ncol = nf)
Xmat &lt;- tcrossprod(Amat, krprod(Cmat, Bmat))
Xmat &lt;- array(Xmat, dim = mydim)
Emat &lt;- array(rnorm(prod(mydim)), dim = mydim)
Emat &lt;- nscale(Emat, 0, ssnew = sumsq(Xmat))  
X &lt;- Xmat + Emat

# create a binary response by dichotomizing at the specified response mean
y &lt;- factor(as.numeric(Xw[ , (nf + 1)] &gt; ymean))

# initialize
alpha &lt;- seq(0, 1, length = 2)
gamma &lt;- c(0, 0.01)
cost &lt;- c(1, 2)
method &lt;- c("PLR", "SVM")
family &lt;- "binomial"
parameters &lt;- list(alpha = alpha, gamma = gamma, cost = cost)
model &lt;- "parafac"
nfolds &lt;- 3
nstart &lt;- 3

# constrain first mode weights to be orthogonal
const &lt;- c("orthog", "uncons", "uncons")

# fit Parafac models and use third mode to tune classification methods
tune.object &lt;- tunecpfa(x = X, y = y, model = model, nfac = nf, 
                        nfolds = nfolds, method = method, family = family, 
                        parameters = parameters, parallel = FALSE, 
                        const = const, nstart = nstart)
                         
# create new data with Parafac structure and C weights related to response
mydim.new &lt;- c(10, 11, 20)
Znew &lt;- matrix(rnorm(mydim.new[3] * (nf + 1)), 
               nrow = mydim.new[3], ncol = (nf + 1))
Xwnew &lt;- rep(1, mydim.new[3]) %*% t(mu) + Znew %*% cormat.sqrt
Cmatnew &lt;- Xwnew[, 1:nf]
Xnew0 &lt;- tcrossprod(Amat, krprod(Cmatnew, Bmat))
Xnew0 &lt;- array(Xnew0, dim = mydim.new)
Ematnew &lt;- array(rnorm(prod(mydim.new)), dim = mydim.new)
Ematnew &lt;- nscale(Ematnew, 0, ssnew = sumsq(Xnew0))  
Xnew &lt;- Xnew0 + Ematnew

# create new random class labels for two levels
newlabel &lt;- as.numeric(Xwnew[, (nf + 1)] &gt; ymean)

# predict class labels
predict.labels &lt;- predict(object = tune.object, newdata = Xnew, 
                          type = "response")
                        
# calculate performance measures for predicted class labels
evalmeasure &lt;- cpm.all(x = predict.labels, y = newlabel)

# print performance measures
evalmeasure

## End(Not run)
</code></pre>

<hr>
<h2 id='plotcpfa'>
Plot Optimal Model from Classification with Parallel Factor Analysis
</h2><span id='topic+plotcpfa'></span>

<h3>Description</h3>

<p>Plots optimal model based on results from a 'wrapcpfa' object obtained using function <code>cpfa</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotcpfa(object, cmeasure = "acc", meanvalue = TRUE, supNum = FALSE, 
         parallel = FALSE, cl = NULL, scale.remode = NULL, newscales = 1, 
         scale.abmode = NULL, sign.remode = NULL, newsigns = 1, 
         sign.abmode = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotcpfa_+3A_object">object</code></td>
<td>

<p>An object of class 'wrapcpfa' from function <code>cpfa</code>.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_cmeasure">cmeasure</code></td>
<td>

<p>Classification performance measure used to select the optimal number of components. Options include <code>c("err", "acc", "tpr", "fpr", "tnr", "fnr", "ppv", "npv", "fdr", "fom", "fs")</code>. If <code>cmeasure</code> is in <code>c("err", "fpr", "fnr", "fdr", "fom")</code>, the number of components that minimized <code>cmeasure</code> is selected among all classification methods. Otherwise, the number that maximized <code>cmeasure</code> is selected.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_meanvalue">meanvalue</code></td>
<td>

<p>Logical indicating whether to find the optimal number of components based on the mean performance across replications from the results generated by <code>cpfa</code>. If <code>meanvalue = F</code>, the median is used. 
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_supnum">supNum</code></td>
<td>

<p>Logical indicating whether to suppress text displaying component weight values within plot cells. If TRUE, values are not displayed.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_parallel">parallel</code></td>
<td>

<p>Logical indicating if parallel computing should be implemented. If TRUE, parallel computing is used.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_cl">cl</code></td>
<td>

<p>Cluster for parallel computing, which is used when <code>parallel = T</code>. Note that if <code>parallel = T</code> and <code>cl = NULL</code>, then the cluster is defined as <code>makeCluster(detectCores())</code>.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_scale.remode">scale.remode</code></td>
<td>

<p>Character that indicates a mode to rescale. Must be one of <code>c("A", "B", "C", "D")</code>. Sent directly to argument <code>mode</code> in function <code>rescale</code> from package <b>multiway</b>. See help file for <code>rescale</code> for additional details.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_newscales">newscales</code></td>
<td>

<p>The root mean-square for columns of the mode indicated by <code>scale.remode</code>. See help file for <code>rescale</code> for additional details.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_scale.abmode">scale.abmode</code></td>
<td>

<p>Character that indicates the mode that absorbs the inverse of rescalings applied to the mode indicated by <code>scale.remode</code>. Must be one of <code>c("A", "B", "C", "D")</code>. Sent directly to argument <code>absorb</code> in function <code>rescale</code> from package <b>multiway</b>. See help file for <code>rescale</code> for additional details.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_sign.remode">sign.remode</code></td>
<td>

<p>Character that indicates a mode to resign. Must be one of <code>c("A", "B", "C", "D")</code>. Sent directly to argument <code>mode</code> in function <code>resign</code> from package <b>multiway</b>. See help file for <code>resign</code> for additional details.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_newsigns">newsigns</code></td>
<td>

<p>Scalar or vector indicating resignings for columns of the mode indicated by <code>sign.remode</code>. See help file for <code>resign</code> for additional details.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_sign.abmode">sign.abmode</code></td>
<td>

<p>Character that indicates the mode that absorbs the negation of the resignings applied to the mode indicated by <code>sign.remode</code>. Must be one of <code>c("A", "B", "C", "D")</code>. Sent directly to argument <code>absorb</code> in function <code>resign</code> from package <b>multiway</b>. See help file for <code>resign</code> for additional details.
</p>
</td></tr>
<tr><td><code id="plotcpfa_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed to function <code>parafac</code> for fitting a Parafac model or function <code>parafac2</code> for fitting a Parafac2 model. See help file for function <code>parafac</code> or for function <code>parafac2</code> for additional details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Selects the number of components that optimized a performance measure across all classification methods used by <code>cpfa</code>. With this optimal number of components, fits the Parafac or Parafac2 model that was used by <code>cpfa</code> to create the input 'wrapcpfa' object. Uses same constraints used in <code>cpfa</code>. Plots component weights for this optimal model using heatmaps. Darker red indicates component weights that are more negative while darker green indicates component weights that are more positive. For three-way Parafac, plots A and B weights. For four-way Parafac, plots A, B, and C weights. For three-way Parafac2, plots B weights. For four-way Parafac2, plots B and C weights.
</p>


<h3>Value</h3>

<p>Returns one or more heatmap plots of component weights for the optimal Parafac or Parafac2 model. Returns list of estimated component weights used in the plots.
</p>


<h3>Author(s)</h3>

<p>Matthew Snodgress &lt;snodg031@umn.edu&gt;
</p>


<h3>References</h3>

<p>See help file for function <code>cpfa</code> for a list of references.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>########## Parafac2 example with 4-way array and multiclass response ##########
## Not run: 
# set seed and specify dimensions of a four-way tensor
set.seed(5)
mydim &lt;- c(10, 11, 12, 100)
nf &lt;- 3

# create correlation matrix between response and fourth mode's weights 
rho.dd &lt;- .35 
rho.dy &lt;- .75 
cormat.values &lt;- c(1, rho.dd, rho.dd, rho.dy, rho.dd, 1, rho.dd, rho.dy, 
                   rho.dd, rho.dd, 1, rho.dy, rho.dy, rho.dy, rho.dy, 1)
cormat &lt;- matrix(cormat.values, nrow = (nf + 1), ncol = (nf + 1))

# sample from a multivariate normal with specified correlation structure
ymean &lt;- Dmean &lt;- 2
mu &lt;- as.matrix(c(Dmean, Dmean, Dmean, ymean))
eidecomp &lt;- eigen(cormat, symmetric = TRUE)
L.sqrt &lt;- diag(eidecomp$values^0.5)
cormat.sqrt &lt;- eidecomp$vectors %*% L.sqrt %*% t(eidecomp$vectors)
Z &lt;- matrix(rnorm(mydim[4] * (nf + 1)), nrow = mydim[4], ncol = (nf + 1))
Xw &lt;- rep(1, mydim[4]) %*% t(mu) + Z %*% cormat.sqrt
Dmat &lt;- Xw[, 1:nf]

# create a random four-way data tensor with D weights related to a response
Bmat &lt;- matrix(runif(mydim[2] * nf), nrow = mydim[2], ncol = nf)
Cmat &lt;- matrix(runif(mydim[3] * nf), nrow = mydim[3], ncol = nf)
nDd &lt;- rep(c(10, 12, 14), length.out = mydim[4])
Gmat &lt;- matrix(rnorm(nf * nf), nrow = nf)
Amat &lt;- vector("list", mydim[4])
X &lt;- Xmat &lt;- Emat &lt;- Amat
for (Dd in 1:mydim[4]) {
   Amat[[Dd]] &lt;- matrix(nf * rnorm(nDd[Dd]), nrow = nDd[Dd], ncol = nf)
   Amat[[Dd]] &lt;- svd(Amat[[Dd]], nv = 0)$u %*% Gmat
   leftMat &lt;- Amat[[Dd]] %*% diag(Dmat[Dd,])
   Xmat[[Dd]] &lt;- array(tcrossprod(leftMat, krprod(Cmat, Bmat)), 
                       dim = c(nDd[Dd], mydim[2], mydim[3]))
   Emat[[Dd]] &lt;- array(rnorm(nDd[Dd] * mydim[2] * mydim[3]), 
                       dim = c(nDd[Dd], mydim[2], mydim[3]))
   X[[Dd]] &lt;- Xmat[[Dd]] + Emat[[Dd]]
}

# create a multiclass response
stor &lt;- matrix(rep(1, nrow(Xw)), nrow = nrow(Xw))
stor[which(Xw[, (nf + 1)] &lt; (ymean - 0.4 * sd(Xw[, (nf + 1)])))] &lt;- 2
stor[which(Xw[, (nf + 1)] &gt; (ymean + 0.4 * sd(Xw[, (nf + 1)])))] &lt;- 0
y &lt;- factor(stor)

# initialize
alpha &lt;- seq(0, 1, length = 2)
gamma &lt;- c(0, 1)
cost &lt;- c(0.1, 5)
rda.alpha &lt;- seq(0.1, 0.9, length = 2)
delta &lt;- c(0.1, 2)
method &lt;- c("PLR", "SVM", "RDA")
family &lt;- "multinomial"
parameters &lt;- list(alpha = alpha, gamma = gamma, cost = cost, 
                   rda.alpha = rda.alpha, delta = delta)
model &lt;- "parafac2"
nfolds &lt;- 3
nstart &lt;- 1

# constrain first mode weights to be orthogonal, fourth mode to be nonnegative
const &lt;- c("orthog", "uncons", "uncons", "nonneg")

# fit Parafac2 model and use fourth mode weights to tune classification
# methods, to predict class labels, and to return classificaiton 
# performance measures pooled across multiple train-test splits
output &lt;- cpfa(x = X, y = y, model = model, nfac = nf, nrep = 2, ratio = 0.8, 
               nfolds = nfolds, method = method, family = family, 
               parameters = parameters, type.out = "descriptives", 
               seeds = NULL, plot.out = TRUE, parallel = FALSE, const = const,
               nstart = nstart, ctol = 1e-2)

# plot heatmap of component weights for optimal model
plotcpfa(output, nstart = nstart, ctol = 1e-2)

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.tunecpfa'>
Predict Method for Tuning for Classification with Parallel Factor Analysis
</h2><span id='topic+predict.tunecpfa'></span>

<h3>Description</h3>

<p>Obtains predictions for class labels from a 'tunecpfa' model object obtained using function <code>tunecpfa</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tunecpfa'
predict(object, newdata = NULL, method = NULL, 
        type = c("response", "prob", "classify.weights"), 
        threshold = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.tunecpfa_+3A_object">object</code></td>
<td>

<p>A fit object of class 'tunecpfa' produced by function <code>tunecpfa</code>.
</p>
</td></tr>
<tr><td><code id="predict.tunecpfa_+3A_newdata">newdata</code></td>
<td>

<p>An optional three-way or four-way data array used to predict Parafac or Parafac2 component weights using estimated Parafac or Parafac2 model component weights from inputted object. For Parafac2, can be a list of length <code>K</code> where the <code>k</code>-th element is a matrix or three-way array associated with the <code>k</code>-th element. Array or list must contain only real numbers. Dimensions must match dimensions of original data for all modes except the classification mode. If omitted, the original data are used.
</p>
</td></tr>
<tr><td><code id="predict.tunecpfa_+3A_method">method</code></td>
<td>

<p>Character vector indicating classification methods to use. Possible methods include penalized logistic regression (PLR); support vector machine (SVM); random forest (RF); feed-forward neural network (NN); regularized discriminant analysis (RDA); and gradient boosting machine (GBM). If none selected, default is to use all methods.
</p>
</td></tr>
<tr><td><code id="predict.tunecpfa_+3A_type">type</code></td>
<td>

<p>Character vector indicating type of prediction to return. Possible values include: (1) <code>"response"</code>, returning predicted class labels; (2) <code>"prob"</code>, returning predicted class probabilities; or (3) <code>"classify.weights"</code>, returning predicted component weights used in classification from Parafac models specified. Defaults to <code>"response"</code>.
</p>
</td></tr>
<tr><td><code id="predict.tunecpfa_+3A_threshold">threshold</code></td>
<td>

<p>For binary classification, value indicating prediction threshold over which observations are classified as the positive class. If not provided, calculates threshold using class proportions in original data. For multiclass classification, <code>threshold</code> is not currently implemented.
</p>
</td></tr>
<tr><td><code id="predict.tunecpfa_+3A_...">...</code></td>
<td>

<p>Additional predict arguments. Currently ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Predicts class labels for a binary or a multiclass outcome. Specifically, predicts component weights for one mode of a Parallel Factor Analysis-1 (Parafac) model or a Parallel Factor Analysis-2 (Parafac2) model using new data and previously estimated mode weights from original data. Passes predicted component weights to one or several classification methods as new data for predicting class labels.
</p>
<p>Tuning parameters optimized by k-fold cross-validation are used for each classification method (see help for <code>tunecpfa</code>). If not supplied in argument <code>threshold</code>, prediction threshold for all classification methods is calculated using proportions of class labels for original data in the binary case (and the positive class proportion is set as the threshold). For multiclass case, class with highest probability is chosen.
</p>


<h3>Value</h3>

<p>Returns one of the following, depending on the choice for argument <code>type</code>:
</p>
<table role = "presentation">
<tr><td><code>type = "response"</code></td>
<td>
<p>A data frame containing predicted class labels or probabilities (binary case) for each Parafac model and classification method selected (see argument <code>type</code>). Number of columns is equal to number of methods times number of Parafac models. Number of rows is equal to number of predicted observations.
</p>
</td></tr>
<tr><td><code>type = "prob"</code></td>
<td>
<p>A list containing predicted probabilities for each Parafac model and classification method selected (see argument <code>type</code>). Only returned if original response was multiclass (i.e., contained three or more class labels). The number of list elements is equal to the number of methods times the number of Parafac models.
</p>
</td></tr>
<tr><td><code>type = "classify.weights"</code></td>
<td>
<p>List containing predicted component weights for each Parafac or Parafac2 model. Length is equal to number of Parafac models that were fit.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthew Snodgress &lt;snodg031@umn.edu&gt;
</p>


<h3>References</h3>

<p>See help file for function <code>tunecpfa</code> for a list of references.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>########## Parafac2 example with 4-way array and multiclass response ##########
## Not run: 
# set seed and specify dimensions of a four-way tensor
set.seed(5)
mydim &lt;- c(10, 11, 12, 90)
nf &lt;- 3

# create correlation matrix between response and fourth mode's weights 
rho.dd &lt;- .35 
rho.dy &lt;- .75 
cormat.values &lt;- c(1, rho.dd, rho.dd, rho.dy, rho.dd, 1, rho.dd, rho.dy, 
                   rho.dd, rho.dd, 1, rho.dy, rho.dy, rho.dy, rho.dy, 1)
cormat &lt;- matrix(cormat.values, nrow = (nf + 1), ncol = (nf + 1))

# sample from a multivariate normal with specified correlation structure
ymean &lt;- Dmean &lt;- 2
mu &lt;- as.matrix(c(Dmean, Dmean, Dmean, ymean))
eidecomp &lt;- eigen(cormat, symmetric = TRUE)
L.sqrt &lt;- diag(eidecomp$values^0.5)
cormat.sqrt &lt;- eidecomp$vectors %*% L.sqrt %*% t(eidecomp$vectors)
Z &lt;- matrix(rnorm(mydim[4] * (nf + 1)), nrow = mydim[4], ncol = (nf + 1))
Xw &lt;- rep(1, mydim[4]) %*% t(mu) + Z %*% cormat.sqrt
Dmat &lt;- Xw[, 1:nf]

# create a random four-way data tensor with D weights related to a response
Bmat &lt;- matrix(runif(mydim[2] * nf), nrow = mydim[2], ncol = nf)
Cmat &lt;- matrix(runif(mydim[3] * nf), nrow = mydim[3], ncol = nf)
nDd &lt;- rep(c(10, 12, 14), length.out = mydim[4])
Gmat &lt;- matrix(rnorm(nf * nf), nrow = nf)
Amat &lt;- vector("list", mydim[4])
X &lt;- Xmat &lt;- Emat &lt;- Amat
for (Dd in 1:mydim[4]) {
   Amat[[Dd]] &lt;- matrix(nf * rnorm(nDd[Dd]), nrow = nDd[Dd], ncol = nf)
   Amat[[Dd]] &lt;- svd(Amat[[Dd]], nv = 0)$u %*% Gmat
   leftMat &lt;- Amat[[Dd]] %*% diag(Dmat[Dd,])
   Xmat[[Dd]] &lt;- array(tcrossprod(leftMat, krprod(Cmat, Bmat)), 
                       dim = c(nDd[Dd], mydim[2], mydim[3]))
   Emat[[Dd]] &lt;- array(rnorm(nDd[Dd] * mydim[2] * mydim[3]), 
                       dim = c(nDd[Dd], mydim[2], mydim[3]))
   X[[Dd]] &lt;- Xmat[[Dd]] + Emat[[Dd]]
}

# create a multiclass response
stor &lt;- matrix(rep(1, nrow(Xw)), nrow = nrow(Xw))
stor[which(Xw[, (nf + 1)] &lt; (ymean - 0.4 * sd(Xw[, (nf + 1)])))] &lt;- 2
stor[which(Xw[, (nf + 1)] &gt; (ymean + 0.4 * sd(Xw[, (nf + 1)])))] &lt;- 0
y &lt;- factor(stor)

# initialize
rda.alpha &lt;- seq(0.1, 0.9, length = 2)
delta &lt;- c(0.1, 2)
eta &lt;- c(0.3, 0.7)
max.depth &lt;- c(1, 2)
subsample &lt;- c(0.75)
nrounds &lt;- c(100)
method &lt;- c("RDA", "GBM")
family &lt;- "multinomial"
parameters &lt;- list(rda.alpha = rda.alpha, delta = delta, eta = eta,
                   max.depth = max.depth, subsample = subsample, 
                   nrounds = nrounds)
model &lt;- "parafac2"
nfolds &lt;- 3
nstart &lt;- 3

# constrain first mode weights to be orthogonal, fourth mode to be nonnegative
const &lt;- c("orthog", "uncons", "uncons", "nonneg")

# fit Parafac2 model and use fourth mode to tune classification methods
tune.object &lt;- tunecpfa(x = X, y = y, model = model, nfac = nf, 
                        nfolds = nfolds, method = method, family = family, 
                        parameters = parameters, parallel = FALSE, 
                        const = const, nstart = nstart)

# create new data with Parafac2 structure and D weights related to response
mydim.new &lt;- c(10, 11, 12, 10)
Znew &lt;- matrix(rnorm(mydim.new[4] * (nf + 1)), nrow = mydim.new[4], 
               ncol = (nf + 1))
Xwnew &lt;- rep(1, mydim.new[4]) %*% t(mu) + Znew %*% cormat.sqrt
Dmatnew &lt;- Xwnew[, 1:nf]
Amat &lt;- vector("list", mydim.new[4])
Xnew &lt;- Xmat &lt;- Emat &lt;- Amat
for (Dd in 1:mydim.new[4]) {
   Amat[[Dd]] &lt;- matrix(nf * rnorm(nDd[Dd]), nrow = nDd[Dd], ncol = nf)
   Amat[[Dd]] &lt;- svd(Amat[[Dd]], nv = 0)$u %*% Gmat
   leftMat &lt;- Amat[[Dd]] %*% diag(Dmatnew[Dd, ])
   Xmat[[Dd]] &lt;- array(tcrossprod(leftMat, krprod(Cmat, Bmat)), 
                       dim = c(nDd[Dd], mydim.new[2], mydim.new[3]))
   Emat[[Dd]] &lt;- array(rnorm(nDd[Dd] * mydim.new[2] * mydim.new[3]), 
                       dim = c(nDd[Dd], mydim.new[2], mydim.new[3]))
   Xnew[[Dd]] &lt;- Xmat[[Dd]] + Emat[[Dd]]
}

# create new random class labels for two levels
stor &lt;- matrix(rep(1, nrow(Xwnew)), nrow = nrow(Xwnew))
stor[which(Xwnew[, (nf + 1)] &lt; (ymean - 0.4 * sd(Xwnew[, (nf + 1)])))] &lt;- 2
stor[which(Xwnew[, (nf + 1)] &gt; (ymean + 0.4 * sd(Xwnew[, (nf + 1)])))] &lt;- 0
newlabels &lt;- as.numeric(stor)

# predict class labels
predict.labels &lt;- predict(object = tune.object, newdata = Xnew, 
                          type = "response")

# print predicted labels
predict.labels

## End(Not run)
</code></pre>

<hr>
<h2 id='print.tunecpfa'>
Print Method for Tuning for Classification with Parallel Factor Analysis
</h2><span id='topic+print.tunecpfa'></span>

<h3>Description</h3>

<p>Prints summary of results from a 'tunecpfa' model object obtained using function <code>tunecpfa</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tunecpfa'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.tunecpfa_+3A_x">x</code></td>
<td>

<p>A fit object of class 'tunecpfa' from function <code>tunecpfa</code>.
</p>
</td></tr>
<tr><td><code id="print.tunecpfa_+3A_...">...</code></td>
<td>

<p>Additional print arguments.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Prints names of the models and methods used to create the input 'tunecpfa' model object. Prints misclassification error rates and estimation times in seconds.
</p>


<h3>Value</h3>

<p>Returns a summary of the 'tunecpfa' model object.
</p>


<h3>Author(s)</h3>

<p>Matthew Snodgress &lt;snodg031@umn.edu&gt;
</p>


<h3>References</h3>

<p>See help file for function <code>tunecpfa</code> for a list of references.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>########## Parafac example with 3-way array and binary response ##########
## Not run: 
# set seed and specify dimensions of a three-way tensor
set.seed(3)
mydim &lt;- c(10, 11, 80)
nf &lt;- 3

# create correlation matrix between response and third mode's weights 
rho.cc &lt;- .35
rho.cy &lt;- .75
cormat.values &lt;- c(1, rho.cc, rho.cc, rho.cy, rho.cc, 1, rho.cc, rho.cy, 
                   rho.cc, rho.cc, 1, rho.cy, rho.cy, rho.cy, rho.cy, 1)
cormat &lt;- matrix(cormat.values, nrow = (nf + 1), ncol = (nf + 1))

# sample from a multivariate normal with specified correlation structure
ymean &lt;- Cmean &lt;- 2
mu &lt;- as.matrix(c(Cmean, Cmean, Cmean, ymean))
eidecomp &lt;- eigen(cormat, symmetric = TRUE)
L.sqrt &lt;- diag(eidecomp$values^0.5)
cormat.sqrt &lt;- eidecomp$vectors %*% L.sqrt %*% t(eidecomp$vectors)
Z &lt;- matrix(rnorm(mydim[3] * (nf + 1)), nrow = mydim[3], ncol = (nf + 1))
Xw &lt;- rep(1, mydim[3]) %*% t(mu) + Z %*% cormat.sqrt
Cmat &lt;- Xw[, 1:nf]

# create a random three-way data tensor with C weights related to a response
Amat &lt;- matrix(rnorm(mydim[1] * nf), nrow = mydim[1], ncol = nf)
Bmat &lt;- matrix(runif(mydim[2] * nf), nrow = mydim[2], ncol = nf)
Xmat &lt;- tcrossprod(Amat, krprod(Cmat, Bmat))
Xmat &lt;- array(Xmat, dim = mydim)
Emat &lt;- array(rnorm(prod(mydim)), dim = mydim)
Emat &lt;- nscale(Emat, 0, ssnew = sumsq(Xmat))  
X &lt;- Xmat + Emat

# create a binary response by dichotomizing at the specified response mean
y &lt;- factor(as.numeric(Xw[ , (nf + 1)] &gt; ymean))

# initialize
alpha &lt;- seq(0, 1, length = 2)
gamma &lt;- c(0, 0.01)
cost &lt;- c(1, 2)
method &lt;- c("PLR", "SVM")
family &lt;- "binomial"
parameters &lt;- list(alpha = alpha, gamma = gamma, cost = cost)
model &lt;- "parafac"
nfolds &lt;- 3
nstart &lt;- 3

# constrain first mode weights to be orthogonal
const &lt;- c("orthog", "uncons", "uncons")

# fit Parafac models and use third mode to tune classification methods
tune.object &lt;- tunecpfa(x = X, y = y, model = model, nfac = nf, 
                        nfolds = nfolds, method = method, family = family, 
                        parameters = parameters, parallel = FALSE, 
                        const = const, nstart = nstart)
                         
# print summary of output
print(tune.object)

## End(Not run)
</code></pre>

<hr>
<h2 id='tunecpfa'>
Tuning for Classification with Parallel Factor Analysis
</h2><span id='topic+tunecpfa'></span>

<h3>Description</h3>

<p>Fits Richard A. Harshman's Parallel Factor Analysis-1 (Parafac) model or Parallel Factor Analysis-2 (Parafac2) model to a three-way or four-way data array. Allows for multiple constraint options on tensor modes. Uses component weights from a single mode of the model as predictors to tune parameters for one or more classification methods via a k-fold cross-validation procedure. Supports binary and multiclass classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tunecpfa(x, y, model = c("parafac", "parafac2"), nfac = 1, nfolds = 10,
         method = c("PLR", "SVM", "RF", "NN", "RDA", "GBM"), 
         family = c("binomial", "multinomial"), parameters = list(), 
         foldid = NULL, prior = NULL, cmode = NULL, parallel = FALSE, 
         cl = NULL, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tunecpfa_+3A_x">x</code></td>
<td>

<p>For Parafac or Parafac2, a three-way or four-way data array. For Parafac2, can be a list of length <code>K</code> where the <code>k</code>-th element is a matrix or three-way array associated with the <code>k</code>-th element. Array or list must contain real numbers. See note below.
</p>
</td></tr> 
<tr><td><code id="tunecpfa_+3A_y">y</code></td>
<td>

<p>A vector containing at least two unique class labels. Should be a factor that contains two or more levels. For binary case, ensure the order of factor levels (left to right) is such that negative class is first and positive class is second.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_model">model</code></td>
<td>

<p>Character designating the Parafac model to use, either <code>model = "parafac"</code> to fit the Parafac model or <code>model = "parafac2"</code> to fit the Parafac2 model.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_nfac">nfac</code></td>
<td>

<p>Number of components for each Parafac or Parafac2 model to fit. Default is <code>nfac = 1</code>.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_nfolds">nfolds</code></td>
<td>

<p>Numeric setting number of folds for k-fold cross-validation. Must be 2 or greater. Default is <code>nfolds = 10</code>.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_method">method</code></td>
<td>

<p>Character vector indicating classification methods to use. Possible methods include penalized logistic regression (PLR); support vector machine (SVM); random forest (RF); feed-forward neural network (NN); regularized discriminant analysis (RDA); and gradient boosting machine (GBM). If none selected, default is to use all methods.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_family">family</code></td>
<td>

<p>Character value specifying binary classification (<code>family = "binomial"</code>) or multiclass classification (<code>family = "multinomial"</code>). If not provided, number of levels of input <code>y</code> is used, where two levels is binary, and where three or more levels is multiclass.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_parameters">parameters</code></td>
<td>

<p>List containing arguments related to classification methods. When specified, must contain one or more of the following:
</p>

<dl>
<dt>alpha</dt><dd>
<p>Values for penalized logistic regression alpha parameter; default is <code>alpha = seq(0, 1, length = 6)</code>. Must be numeric and contain only real numbers between 0 and 1, inclusive.
</p>
</dd>
<dt>lambda</dt><dd>
<p>Optional user-supplied lambda sequence for <code>cv.glmnet</code> for penalized logistic regression. Default is NULL.
</p>
</dd>
<dt>cost</dt><dd>
<p>Values for support vector machine cost parameter; default is <code>cost = c(1, 2, 4, 8, 16, 32, 64)</code>. Must be numeric and contain only real numbers greater than or equal to zero.
</p>
</dd>
<dt>gamma</dt><dd>
<p>Values for support vector machine gamma parameter; default is <code>gamma = c(0, 0.01, 0.1, 1, 10, 100, 1000)</code>. Must be numeric and greater than or equal to 0.
</p>
</dd>
<dt>ntree</dt><dd>
<p>Values for random forest number of trees parameter; default is <code>ntree = c(100, 200, 400, 600, 800, 1600, 3200)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
<dt>nodesize</dt><dd>
<p>Values for random forest node size parameter; default is <code>nodesize = c(1, 2, 4, 8, 16, 32, 64)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
<dt>size</dt><dd>
<p>Values for neural network size parameter; default is <code>size = c(1, 2, 4, 8, 16, 32, 64)</code>. Must be numeric and contain only integers greater than or equal to 0.
</p>
</dd>
<dt>decay</dt><dd>
<p>Values for neural network decay parameter; default is <code>decay = c(0.001, 0.01, 0.1, 1, 2, 4, 8, 16)</code>. Must be numeric and contain only real numbers.
</p>
</dd>
<dt>rda.alpha</dt><dd>
<p>Values for regularized discriminant analysis alpha parameter; default is <code>rda.alpha = seq(0, 0.999, length = 6)</code>. Must be numeric and contain only real numbers between 0 (inclusive) and 1 (exclusive).
</p>
</dd>
<dt>delta</dt><dd>
<p>Values for regularized discriminant analysis delta parameter; default is <code>delta = c(0, 0.1, 1, 2, 3, 4)</code>. Must be numeric and contain only real numbers greater than or equal to 0.
</p>
</dd>
<dt>eta</dt><dd>
<p>Values for gradient boosting machine eta parameter; default is <code>eta = c(0.1, 0.3, 0.5, 0.7, 0.9)</code>. Must be numeric and contain only real numbers greater than 0 and less than 1.
</p>
</dd>
<dt>max.depth</dt><dd>
<p>Values for gradient boosting machine max.depth parameter; default is <code>max.depth = c(1, 2, 3, 4)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
<dt>subsample</dt><dd>
<p>Values for gradient boosting machine subsample parameter; default is <code>subsample = c(0.6, 0.7, 0.8, 0.9)</code>. Must be numeric and contain only real numbers greater than 0 and less than or equal to 1.
</p>
</dd>
<dt>nrounds</dt><dd>
<p>Values for gradient boosting machine nrounds parameter; default is <code>nrounds = c(100, 200, 300, 500)</code>. Must be numeric and contain only integers greater than or equal to 1.
</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="tunecpfa_+3A_foldid">foldid</code></td>
<td>

<p>Vector containing fold IDs for k-fold cross-validation. Can be of class integer, numeric, or data frame. Should contain integers from 1 through the number of folds. If not provided, fold IDs are generated randomly for observations using 1 through the number of folds <code>nfolds</code>.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_prior">prior</code></td>
<td>

<p>Prior probabilities of class membership. If unspecified, the class proportions for input <code>y</code> are used. If specified, the probabilities should be in the order of the factor levels of input <code>y</code>.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_cmode">cmode</code></td>
<td>

<p>Integer value of 1, 2, or 3 (or 4 if <code>x</code> is a four-way array) specifying the mode whose component weights will be predictors for classification. Defaults to the last mode of the inputted array (i.e., defaults to 3 for three-way array, and to 4 for four-way array). If <code>model = "parafac2"</code>, last mode will be used.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_parallel">parallel</code></td>
<td>

<p>Logical indicating if parallel computing should be implemented. If TRUE, the package <b>parallel</b> is used for parallel computing. For all classification methods except penalized logistic regression, the <b>doParallel</b> package is used as a wrapper. Defaults to FALSE, which implements sequential computing.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_cl">cl</code></td>
<td>

<p>Cluster for parallel computing, which is used when <code>parallel = T</code>. Note that if <code>parallel = T</code> and <code>cl = NULL</code>, then the cluster is defined as <code>makeCluster(detectCores())</code>.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_verbose">verbose</code></td>
<td>

<p>If TRUE, progress is printed.
</p>
</td></tr>
<tr><td><code id="tunecpfa_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed to function <code>parafac</code> for fitting a Parafac model or function <code>parafac2</code> for fitting a Parafac2 model. Example: can impose different constraints on different modes of the input array using the argument <code>const</code>. See help file for function <code>parafac</code> or for function <code>parafac2</code> for additional details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After fitting a Parafac or Parafac2 model with package <b>multiway</b> (see <code>parafac</code> or <code>parafac2</code> in <b>multiway</b> for details), the estimated classification mode weight matrix is passed to one or several of six classification methods&ndash;including penalized logistic regression (PLR); support vector machine (SVM); random forest (RF); feed-forward neural network (NN); regularized discriminant analysis (RDA); and gradient boosting machine (GBM).
</p>
<p>Package <b>glmnet</b> fits models for PLR. PLR tunes penalty parameter lambda while the elastic net parameter alpha is set by the user (see the help file for function <code>cv.glmnet</code> in package <b>glmnet</b>). For SVM, package <b>e1071</b> is used with a radial basis kernel. Penalty parameter cost and radial basis parameter gamma are used (see <code>svm</code> in package <b>e1071</b>). For RF, package <b>randomForest</b> is used and implements Breiman's random forest algorithm. The number of predictors sampled at each node split is set at the default of sqrt(R), where R is the number of Parafac or Parafac2 components. Two tuning parameters allowed are ntree, the number of trees to be grown, and nodesize, the minimum size of terminal nodes (see <code>randomForest</code> in package <b>randomForest</b>). For NN, package <b>nnet</b> fits a single-hidden-layer, feed-forward neural network model. Penalty parameters size (i.e., number of hidden layer units) and decay (i.e., weight decay) are used (see <b>nnet</b>). For RDA, package <b>rda</b> fits a shrunken centroids regularized discriminant analysis model. Tuning parameters include rda.alpha, the shrinkage penalty for the within-class covariance matrix, and delta, the shrinkage penalty of class centroids towards the overall dataset centroid. For GBM, package <b>xgboost</b> fits a gradient boosting machine model. Four tuning parameters are allowed: (1) eta, the learning rate; (2) max.depth, the maximum tree depth; (3) subsample, the fraction of samples per tree; and (4) nrounds, the number of boosting trees to build.
</p>
<p>For all six methods, k-fold cross-validation is implemented to tune classification parameters where the number of folds is set by argument <code>nfolds</code>.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>tunecpfa</code> with the following elements:
</p>
<table role = "presentation">
<tr><td><code>opt.model</code></td>
<td>

<p>List containing optimal model for tuned classification methods for each Parafac or Parafac2 model that was fit.
</p>
</td></tr>
<tr><td><code>opt.param</code></td>
<td>

<p>Data frame containing optimal parameters for tuned classification methods.
</p>
</td></tr>
<tr><td><code>kcv.error</code></td>
<td>

<p>Data frame containing KCV misclassification error for optimal parameters for tuned classification methods.
</p>
</td></tr>
<tr><td><code>est.time</code></td>
<td>

<p>Data frame containing times for fitting Parafac or Parafac2 model and for tuning classification methods.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>Numeric indicating classification methods used. Value of '1' indicates 'PLR'; value of '2' indicates 'SVM'; value of '3' indicates 'RF'; value of '4' indicates 'NN'; value of '5' indicates 'RDA'; and value of '6' indicates 'GBM'.
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>

<p>Three-way or four-way array used. If a list was used with <code>model = "parafac2"</code>, returns list of matrices or three-way arrays used.
</p>
</td></tr>
<tr><td><code>y</code></td>
<td>

<p>Factor containing class labels used. Note that output <code>y</code> is recoded such that the input labels of <code>y</code> are converted to numeric integers from 0 through the number of levels, which are then applied as labels for output <code>y</code>. 
</p>
</td></tr>
<tr><td><code>Aweights</code></td>
<td>

<p>List containing estimated A weights for each Parafac or Parafac2 model that was fit.
</p>
</td></tr>
<tr><td><code>Bweights</code></td>
<td>

<p>List containing estimated B weights for each Parafac or Parafac2 model that was fit.
</p>
</td></tr>
<tr><td><code>Cweights</code></td>
<td>

<p>List containing estimated C weights for each Parafac or Parafac2 model that was fit. Null if inputted argument <code>x</code> was a three-way array.
</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>

<p>If <code>model = "parafac2"</code>, a list containing estimated <code>Phi</code> from the Parafac2 model. <code>Phi</code> is the common cross product matrix shared by all levels of the last mode (see help file for function <code>parafac2</code> in package <b>multiway</b> for additional details). NULL if <code>model = "parafac"</code>.
</p>
</td></tr>
<tr><td><code>const</code></td>
<td>

<p>Constraints used in fitting Parafac or Parafac2 models. If argument <code>const</code> was not inputted, no constraints will be used.
</p>
</td></tr>
<tr><td><code>cmode</code></td>
<td>

<p>Integer value of 1, 2, or 3 (or 4 if <code>x</code> is a four-way array) specifying mode whose component weights were predictors for classification.
</p>
</td></tr>
<tr><td><code>family</code></td>
<td>

<p>Character value specifying whether classification was binary (<code>family = "binomial"</code>) or multiclass (<code>family = "multinomial"</code>).
</p>
</td></tr>
<tr><td><code>xdim</code></td>
<td>

<p>Numeric value specifying number of levels for each mode of input <code>x</code>. If <code>model = "parafac2"</code>, number of levels for first mode is designated as <code>NA</code> because the number of levels can differ across levels of the last mode.
</p>
</td></tr>
<tr><td><code>lxdim</code></td>
<td>

<p>Numeric value specifying number of modes of input <code>x</code>.
</p>
</td></tr>
<tr><td><code>train.weights</code></td>
<td>

<p>List containing classification component weights for each fit Parafac or Parafac2 model, for possibly different numbers of components. The weights used to train classifiers.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For fitting the Parafac model, if argument <code>cmode</code> is not null, input array <code>x</code> is reshaped with function <code>aperm</code> such that the <code>cmode</code> dimension of <code>x</code> is ordered last. Estimated mode A and B (and mode C for a four-way array) weights that are outputted as <code>Aweights</code> and <code>Bweights</code> (and <code>Cweights</code>) reflect this permutation. For example, if <code>x</code> is a four-way array and <code>cmode = 2</code>, the original input modes 1, 2, 3, and 4 will correspond to output modes 1, 3, 4, 2. Here, output A = input 1; B = 3, and C = 4 (i.e., the second mode specified by <code>cmode</code> has been moved to the D mode/last mode). For <code>model = "parafac2"</code>, classification mode is assumed to be the last mode (i.e., mode C for three-way array and mode D for four-way array).
</p>
<p>In addition, note that the following combination of arguments will give an error: <code>nfac = 1, family = "multinomial", method = "PLR"</code>. The issue arises from providing <code>glmnet::cv.glmnet</code> input <code>x</code> a matrix with a single column. The issue is resolved for <code>family = "binomial"</code> because a column of 0s is appended to the single column, but this solution does not appear to work for the multiclass case. As such, this combination of arguments is not currently allowed. This issue will be resolved in a future update.
</p>


<h3>Author(s)</h3>

<p>Matthew A. Snodgress &lt;snodg031@umn.edu&gt;
</p>


<h3>References</h3>

<p>Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.
</p>
<p>Chen, T., He, T., Benesty, M., Khotilovich, V., Tang, Y., Cho, H., Chen, K., Mitchell, R., Cano, I., Zhou, T., Li, M., Xie, J., Lin, M., Geng, Y., Li, Y., Yuan, J. (2024). xgboost: Extreme gradient boosting. R Package Version 1.7.7.1.
</p>
<p>Cortes, C. and Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3), 273-297.
</p>
<p>Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of Statistics, 29(5), 1189-1232.
</p>
<p>Friedman, J. H. (1989). Regularized discriminant analysis. Journal of the American Statistical Association, 84(405), 165-175.
</p>
<p>Friedman, J. Hastie, T., and Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 1-22.
</p>
<p>Guo, Y., Hastie, T., and Tibshirani, R. (2007). Regularized linear discriminant analysis and its application in microarrays. Biostatistics, 8(1), 86-100.
</p>
<p>Guo Y., Hastie T., and Tibshirani, R. (2023). rda: Shrunken centroids regularized discriminant analysis. R Package Version 1.2-1.
</p>
<p>Harshman, R. (1970). Foundations of the PARAFAC procedure: Models and conditions for an &quot;explanatory&quot; multimodal factor analysis. UCLA Working Papers in Phonetics, 16, 1-84.
</p>
<p>Harshman, R. (1972). PARAFAC2: Mathematical and technical notes. UCLA Working Papers in Phonetics, 22, 30-44.
</p>
<p>Harshman, R. and Lundy, M. (1994). PARAFAC: Parallel factor analysis. Computational Statistics and Data Analysis, 18, 39-72.
</p>
<p>Helwig, N. (2017). Estimating latent trends in multivariate longitudinal data via Parafac2 with functional and structural constraints. Biometrical Journal, 59(4), 783-803.
</p>
<p>Helwig, N. (2019). multiway: Component models for multi-way data. R Package Version 1.0-6.
</p>
<p>Liaw, A. and Wiener, M. (2002). Classification and regression by randomForest. R News 2(3), 18&ndash;22.
</p>
<p>Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., and Leisch, F. (2023). e1071: Misc functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. R Package Version 1.7-13.
</p>
<p>Ripley, B. (1994). Neural networks and related methods for classification. Journal of the Royal Statistical Society: Series B (Methodological), 56(3), 409-437.
</p>
<p>Venables, W. and Ripley, B. (2002). Modern applied statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0.
</p>
<p>Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301-320.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>########## Parafac example with 3-way array and binary response ##########
## Not run: 
# set seed and specify dimensions of a three-way tensor
set.seed(3)
mydim &lt;- c(10, 11, 80)
nf &lt;- 3

# create correlation matrix between response and third mode's weights 
rho.cc &lt;- .35 
rho.cy &lt;- .75 
cormat.values &lt;- c(1, rho.cc, rho.cc, rho.cy, rho.cc, 1, rho.cc, rho.cy, 
                   rho.cc, rho.cc, 1, rho.cy, rho.cy, rho.cy, rho.cy, 1)
cormat &lt;- matrix(cormat.values, nrow = (nf + 1), ncol = (nf + 1))
 
# sample from a multivariate normal with specified correlation structure
ymean &lt;- Cmean &lt;- 2
mu &lt;- as.matrix(c(Cmean, Cmean, Cmean, ymean)) 
eidecomp &lt;- eigen(cormat, symmetric = TRUE)
L.sqrt &lt;- diag(eidecomp$values^0.5)
cormat.sqrt &lt;- eidecomp$vectors %*% L.sqrt %*% t(eidecomp$vectors)
Z &lt;- matrix(rnorm(mydim[3] * (nf + 1)), nrow = mydim[3], ncol = (nf + 1))
Xw &lt;- rep(1, mydim[3]) %*% t(mu) + Z %*% cormat.sqrt
Cmat &lt;- Xw[, 1:nf]

# create a random three-way data tensor with C weights related to a response
Amat &lt;- matrix(rnorm(mydim[1] * nf), nrow = mydim[1], ncol = nf)
Bmat &lt;- matrix(runif(mydim[2] * nf), nrow = mydim[2], ncol = nf)
Xmat &lt;- tcrossprod(Amat, krprod(Cmat, Bmat))
Xmat &lt;- array(Xmat, dim = mydim)
Emat &lt;- array(rnorm(prod(mydim)), dim = mydim)
Emat &lt;- nscale(Emat, 0, ssnew = sumsq(Xmat))  
X &lt;- Xmat + Emat

# create a binary response by dichotomizing at the specified response mean
y &lt;- factor(as.numeric(Xw[ , (nf + 1)] &gt; ymean))

# initialize
alpha &lt;- seq(0, 1, length = 2)
gamma &lt;- c(0, 0.01)
cost &lt;- c(1, 2)
ntree &lt;- c(100, 200)
nodesize &lt;- c(1, 2)
size &lt;- c(1, 2)
decay &lt;- c(0, 1)
rda.alpha &lt;- c(0.1, 0.6)
delta &lt;- c(0.1, 2)
eta &lt;- c(0.3, 0.7)
max.depth &lt;- c(1, 2)
subsample &lt;- c(0.75)
nrounds &lt;- c(100)
method &lt;- c("PLR", "SVM", "RF", "NN", "RDA", "GBM")
family &lt;- "binomial"
parameters &lt;- list(alpha = alpha, gamma = gamma, cost = cost, ntree = ntree,
                   nodesize = nodesize, size = size, decay = decay, 
                   rda.alpha = rda.alpha, delta = delta, eta = eta,
                   max.depth = max.depth, subsample = subsample,
                   nrounds = nrounds)
model &lt;- "parafac"
nfolds &lt;- 3
nstart &lt;- 3

# constrain first mode weights to be orthogonal
const &lt;- c("orthog", "uncons", "uncons")

# fit Parafac models and use third mode to tune classification methods
tune.object &lt;- tunecpfa(x = X, y = y, model = model, nfac = nf, 
                        nfolds = nfolds, method = method, family = family, 
                        parameters = parameters, parallel = FALSE, 
                        const = const, nstart = nstart)

# print tuning object
tune.object

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
