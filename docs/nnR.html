<!DOCTYPE html><html><head><title>Help for package nnR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nnR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#A'><p>This is an intermediate variable. See the reference</p></a></li>
<li><a href='#A_k'><p>A_k: The function that returns the matrix A_k</p></a></li>
<li><a href='#Aff'><p>Aff</p></a></li>
<li><a href='#B'><p>This is an intermediate variable, see reference.</p></a></li>
<li><a href='#C_k'><p>C_k: The function that returns the C_k matrix</p></a></li>
<li><a href='#ck'><p>The ck function</p></a></li>
<li><a href='#comp'><p>comp</p></a></li>
<li><a href='#Cpy'><p>Cpy</p></a></li>
<li><a href='#create_block_diagonal'><p>Function for creating a block diagonal given two matrices.</p></a></li>
<li><a href='#create_nn'><p>create_nn</p></a></li>
<li><a href='#Csn'><p>Csn</p></a></li>
<li><a href='#dep'><p>dep</p></a></li>
<li><a href='#Etr'><p>Etr</p></a></li>
<li><a href='#generate_random_matrix'><p>Function to generate a random matrix with specified dimensions.</p></a></li>
<li><a href='#hid'><p>hid</p></a></li>
<li><a href='#i'><p>i</p></a></li>
<li><a href='#Id'><p>: Id</p></a></li>
<li><a href='#inn'><p>inn</p></a></li>
<li><a href='#inst'><p>inst</p></a></li>
<li><a href='#is_nn'><p>is_nn</p></a></li>
<li><a href='#lay'><p>lay</p></a></li>
<li><a href='#MC'><p>The MC neural network</p></a></li>
<li><a href='#Mxm'><p>Mxm</p></a></li>
<li><a href='#nn_sum'><p>nn_sum</p></a></li>
<li><a href='#Nrm'><p>Nrm</p></a></li>
<li><a href='#out'><p>out</p></a></li>
<li><a href='#param'><p>param</p></a></li>
<li><a href='#Phi'><p>The Phi function</p></a></li>
<li><a href='#Phi_k'><p>The Phi_k function</p></a></li>
<li><a href='#Prd'><p>Prd</p></a></li>
<li><a href='#Pwr'><p>Pwr</p></a></li>
<li><a href='#ReLU'><p>: ReLU</p></a></li>
<li><a href='#Sigmoid'><p>: Sigmoid</p></a></li>
<li><a href='#slm'><p>slm</p></a></li>
<li><a href='#Sne'><p>Sne</p></a></li>
<li><a href='#Sqr'><p>Sqr</p></a></li>
<li><a href='#srm'><p>srm</p></a></li>
<li><a href='#stk'><p>stk</p></a></li>
<li><a href='#Sum'><p>Sum</p></a></li>
<li><a href='#Tanh'><p>Tanh</p></a></li>
<li><a href='#Tay'><p>The Tay function</p></a></li>
<li><a href='#Trp'><p>Trp</p></a></li>
<li><a href='#Tun'><p>Tun: The function that returns tunneling neural networks</p></a></li>
<li><a href='#view_nn'><p>view_nn</p></a></li>
<li><a href='#Xpn'><p>The Xpn function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Neural Networks Made Algebraic</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Shakil Rafi &lt;sarafi@uark.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Do algebraic operations on neural networks. We seek here to implement
  in R, operations on neural networks and their resulting approximations. Our operations derive
  their descriptions mainly from
  Rafi S., Padgett, J.L., and Nakarmi, U. (2024), "Towards an Algebraic Framework For Approximating Functions Using Neural Network Polynomials", &lt;<a href="https://doi.org/10.48550%2FarXiv.2402.01058">doi:10.48550/arXiv.2402.01058</a>&gt;, 
  Grohs P., Hornung, F., Jentzen, A. et al. (2023), "Space-time error estimates for deep neural network approximations for differential equations", &lt;<a href="https://doi.org/10.1007%2Fs10444-022-09970-2">doi:10.1007/s10444-022-09970-2</a>&gt;,
  Jentzen A., Kuckuck B., von Wurstemberger, P. (2023), "Mathematical Introduction to Deep Learning Methods, Implementations, and Theory" &lt;<a href="https://doi.org/10.48550%2FarXiv.2310.20360">doi:10.48550/arXiv.2310.20360</a>&gt;.
  Our implementation is meant mainly as a pedagogical tool, and proof of concept. Faster implementations with 
  deeper vectorizations may be made in future versions. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.0</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/2shakilrafi/nnR/">https://github.com/2shakilrafi/nnR/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/2shakilrafi/nnR/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc">https://github.com/2shakilrafi/nnR/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-13 20:41:38 UTC; shakilrafi</td>
</tr>
<tr>
<td>Author:</td>
<td>Shakil Rafi <a href="https://orcid.org/0000-0003-3791-9697"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Joshua Lee Padgett
    <a href="https://orcid.org/0000-0001-9369-351X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Ukash Nakarmi <a href="https://orcid.org/0000-0002-5351-3956"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-14 21:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='A'>This is an intermediate variable. See the reference</h2><span id='topic+A'></span>

<h3>Description</h3>

<p>This is an intermediate variable. See the reference
</p>


<h3>Usage</h3>

<pre><code class='language-R'>A
</code></pre>


<h3>Format</h3>

<p>An object of class <code>matrix</code> (inherits from <code>array</code>) with 4 rows and 1 columns.
</p>


<h3>References</h3>

<p>Definition 2.22. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>

<hr>
<h2 id='A_k'>A_k: The function that returns the matrix A_k</h2><span id='topic+A_k'></span>

<h3>Description</h3>

<p>A_k: The function that returns the matrix A_k
</p>


<h3>Usage</h3>

<pre><code class='language-R'>A_k(k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="A_k_+3A_k">k</code></td>
<td>
<p>Natural number, the precision with which to approximate squares
within <code class="reqn">[0,1]</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>An intermediate matrix in a neural network that approximates the square of any real within
<code class="reqn">[0,1]</code> upon ReLU instantiation.
</p>


<h3>References</h3>

<p>Definition 2.22. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A_k(4)
A_k(45)

</code></pre>

<hr>
<h2 id='Aff'>Aff</h2><span id='topic+Aff'></span>

<h3>Description</h3>

<p>The function that returns <code class="reqn">\mathsf{Aff}</code> neural networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Aff(W, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Aff_+3A_w">W</code></td>
<td>
<p>An <code class="reqn">m \times n</code> matrix representing the weight of the affine
neural network</p>
</td></tr>
<tr><td><code id="Aff_+3A_b">b</code></td>
<td>
<p>An <code class="reqn">m \times 1</code> vector representing the bias of the affine
neural network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the network <code class="reqn">((W,b))</code> representing an affine neural network. Also
denoted as <code class="reqn">\mathsf{Aff}_{W,b}</code>
See also <code><a href="#topic+Cpy">Cpy</a></code> and <code><a href="#topic+Sum">Sum</a></code>.
</p>


<h3>References</h3>

<p>Definition 2.3.1. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>
</p>
<p>And especially:
</p>
<p>Definition 2.8. Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Aff(4, 5)
c(5, 6, 7, 8, 9, 10) |&gt;
  matrix(2, 3) |&gt;
  Aff(5)

</code></pre>

<hr>
<h2 id='B'>This is an intermediate variable, see reference.</h2><span id='topic+B'></span>

<h3>Description</h3>

<p>This is an intermediate variable, see reference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>B
</code></pre>


<h3>Format</h3>

<p>An object of class <code>matrix</code> (inherits from <code>array</code>) with 4 rows and 1 columns.
</p>

<hr>
<h2 id='C_k'>C_k: The function that returns the C_k matrix</h2><span id='topic+C_k'></span>

<h3>Description</h3>

<p>C_k: The function that returns the C_k matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>C_k(k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="C_k_+3A_k">k</code></td>
<td>
<p>Natural number, the precision with which to approximate squares
within <code class="reqn">[0,1]</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>An intermediate matrix in a
neural network that approximates the square of any real within
<code class="reqn">[0,1]</code> upon ReLU instantiation.
</p>


<h3>References</h3>

<p>Definition 2.22. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>C_k(5)

</code></pre>

<hr>
<h2 id='ck'>The ck function</h2><span id='topic+ck'></span>

<h3>Description</h3>

<p>The ck function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ck(k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ck_+3A_k">k</code></td>
<td>
<p>input value, any real number</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the ck function
</p>


<h3>References</h3>

<p>Definition 2.22. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ck(1)
ck(-1)

</code></pre>

<hr>
<h2 id='comp'>comp</h2><span id='topic+comp'></span><span id='topic++25comp+25'></span>

<h3>Description</h3>

<p>The function that takes the composition of two neural
networks assuming they are compatible, i.e., given
<code class="reqn">\nu_1, \nu_2 \in \mathsf{NN}</code>, it must be the case that
<code class="reqn">\mathsf{I}(\nu)_1 = \mathsf{O}(\nu_2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp(phi_1, phi_2)

phi_1 %comp% phi_2
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comp_+3A_phi_1">phi_1</code></td>
<td>
<p>first neural network to be composed, goes on the left</p>
</td></tr>
<tr><td><code id="comp_+3A_phi_2">phi_2</code></td>
<td>
<p>second neural network to be composed, goes on right</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The composed neural network. See also <code><a href="#topic+dep">dep</a></code>.
</p>
<p>Our definition derive specifically from:
</p>


<h3>References</h3>

<p>Definition 2.1.1. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>
</p>
<p><em>Remark:</em> We have two versions of this function, an
infix version for close resemblance to mathematical notation and
prefix version.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(5, 4, 6, 7)) |&gt; comp(create_nn(c(4, 1, 5)))
</code></pre>

<hr>
<h2 id='Cpy'>Cpy</h2><span id='topic+Cpy'></span>

<h3>Description</h3>

<p>The function that returns <code class="reqn">\mathsf{Cpy}</code> neural networks.
These are neural networks defined as such
</p>
<p style="text-align: center;"><code class="reqn">
\mathsf{Aff}_{\left[ \mathbb{I}_k \: \mathbb{I}_k \: \cdots \: \mathbb{I}_k\right]^T,0_{k}}
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>Cpy(n, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cpy_+3A_n">n</code></td>
<td>
<p>number of copies to make.</p>
</td></tr>
<tr><td><code id="Cpy_+3A_k">k</code></td>
<td>
<p>the size of the input vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an affine network that makes a concatenated vector that is <code class="reqn">n</code>
copies of the input vector of size <code class="reqn">k</code>. See <code><a href="#topic+Aff">Aff</a></code> and <code><a href="#topic+Sum">Sum</a></code>.
</p>


<h3>References</h3>

<p>Definition 2.9. Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an
Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>

<hr>
<h2 id='create_block_diagonal'>Function for creating a block diagonal given two matrices.</h2><span id='topic+create_block_diagonal'></span>

<h3>Description</h3>

<p>Function for creating a block diagonal given two matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_block_diagonal(matrix1, matrix2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_block_diagonal_+3A_matrix1">matrix1</code></td>
<td>
<p>A matrix.</p>
</td></tr>
<tr><td><code id="create_block_diagonal_+3A_matrix2">matrix2</code></td>
<td>
<p>A matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A block diagonal matrix with matrix1 on top left
and matrix2 on bottom right.
</p>

<hr>
<h2 id='create_nn'>create_nn</h2><span id='topic+create_nn'></span>

<h3>Description</h3>

<p>Function to create a list of lists for neural network layers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_nn(layer_architecture)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_nn_+3A_layer_architecture">layer_architecture</code></td>
<td>
<p>a list specifying the width of each layer</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An ordered list of ordered pairs of <code class="reqn">(W,b)</code>. Where <code class="reqn">W</code> is the matrix
representing the weight matrix at that layer and <code class="reqn">b</code> the bias vector. Entries
on the matrix come from a standard normal distribution.
</p>


<h3>References</h3>

<p>Definition 2.1 in Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>
<p>Which in turn is a modified version of the one found in:
</p>
<p>Definition 2.3. Grohs, P., Hornung, F., Jentzen, A. et al.
Space-time error estimates for deep neural network approximations
for differential equations. (2019).
<a href="https://arxiv.org/abs/1908.03833">https://arxiv.org/abs/1908.03833</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(8, 7, 8))
create_nn(c(4,4))

</code></pre>

<hr>
<h2 id='Csn'>Csn</h2><span id='topic+Csn'></span>

<h3>Description</h3>

<p>The function that returns <code class="reqn">\mathsf{Csn}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Csn(n, q, eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Csn_+3A_n">n</code></td>
<td>
<p>The number of Taylor iterations. Accuracy as well as computation
time increases as <code class="reqn">n</code> increases</p>
</td></tr>
<tr><td><code id="Csn_+3A_q">q</code></td>
<td>
<p>a real number in <code class="reqn">(2,\infty)</code>. Accuracy as well as computation
time increases as <code class="reqn">q</code> gets closer to <code class="reqn">2</code> increases</p>
</td></tr>
<tr><td><code id="Csn_+3A_eps">eps</code></td>
<td>
<p>a real number in <code class="reqn">(0,\infty)</code>. ccuracy as well as computation
time increases as <code class="reqn">\varepsilon</code> gets closer to <code class="reqn">0</code> increases
</p>
<p><em>Note: </em> In practice for most desktop uses
<code class="reqn">q &lt; 2.05</code> and <code class="reqn">\varepsilon&lt; 0.05</code> tends to cause problems in
&quot;too long a vector&quot;, atleaast as tested on my computer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A neural network that approximates <code class="reqn">\cos</code> under instantiation
with ReLU activation. See also <code><a href="#topic+Sne">Sne</a></code>.
</p>


<h3>References</h3>

<p>Definition 2.29 in Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Csn(2, 2.5, 0.5) # this may take some time

Csn(2, 2.5, 0.5) |&gt; inst(ReLU, 1.50)

</code></pre>

<hr>
<h2 id='dep'>dep</h2><span id='topic+dep'></span>

<h3>Description</h3>

<p>The function that returns the depth of a neural network. Denoted
<code class="reqn">\mathsf{D}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dep(nu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dep_+3A_nu">nu</code></td>
<td>
<p>a neural network of the type generated by
create_nn(). Very straightforwardly it is the
length of the list where neural networks are defined as an odered list of
lists.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer representing the depth of the neural network.
</p>


<h3>References</h3>

<p>Definition 1.3.1. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(4, 5, 6, 2)) |&gt; dep()
</code></pre>

<hr>
<h2 id='Etr'>Etr</h2><span id='topic+Etr'></span>

<h3>Description</h3>

<p>The function that returns the <code class="reqn">\mathsf{Etr}</code> networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Etr(n, h)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Etr_+3A_n">n</code></td>
<td>
<p>number of trapezoids to make. Note this will result in a set of
trapezoids. A natural number.</p>
</td></tr>
<tr><td><code id="Etr_+3A_h">h</code></td>
<td>
<p>width of trapezoids. A positive real number.
</p>
<p><em>Note: </em> Upon instantiation with any continuous function this neural
network must be fed with <code class="reqn">n+1</code> real numbers representing the values
of the function being approximated at the <code class="reqn">n+1</code> meshpoints which are
the legs of the <code class="reqn">n</code> trapezoids as stipulated in the input parameter <code class="reqn">n</code>..</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An approximation for value of the integral of a function. Must be instantiated
with a list of <code class="reqn">n+1</code> reals
</p>


<h3>References</h3>

<p>Definition 2.33. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Etr(5, 0.1)
seq(0, pi, length.out = 1000) |&gt; sin() -&gt; samples
Etr(1000 - 1, pi / 1000) |&gt; inst(ReLU, samples)

seq(0, 2, length.out = 1000)^2 -&gt; samples
Etr(1000 - 1, 2 / 1000) |&gt; inst(Tanh, samples)

</code></pre>

<hr>
<h2 id='generate_random_matrix'>Function to generate a random matrix with specified dimensions.</h2><span id='topic+generate_random_matrix'></span>

<h3>Description</h3>

<p>Function to generate a random matrix with specified dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_random_matrix(rows, cols)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_random_matrix_+3A_rows">rows</code></td>
<td>
<p>number of rows.</p>
</td></tr>
<tr><td><code id="generate_random_matrix_+3A_cols">cols</code></td>
<td>
<p>number of columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a random matrix of dimension rows times columns with elements from
a standard normal distribution
</p>

<hr>
<h2 id='hid'>hid</h2><span id='topic+hid'></span>

<h3>Description</h3>

<p>The function that returns the number of hidden layers of a
neural network. Denoted <code class="reqn">\mathsf{H}</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hid(nu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hid_+3A_nu">nu</code></td>
<td>
<p>a neural network of the type generated by create_nn()
</p>
<p>By definition <code class="reqn">\mathsf{H}(\nu) = \mathsf{D}(\nu) - 1</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer representing the number of hidden layers.
</p>


<h3>References</h3>

<p>Definition 1.3.1. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(4, 5, 6, 2)) |&gt; hid()

</code></pre>

<hr>
<h2 id='i'>i</h2><span id='topic+i'></span>

<h3>Description</h3>

<p>The function that returns the <code class="reqn">\mathbb{i}</code> network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>i(d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="i_+3A_d">d</code></td>
<td>
<p>the size of the <code class="reqn">\mathsf{i}</code> network</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns the i_d network
</p>


<h3>References</h3>

<p>Definition 2.2.6. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>i(5)
i(10)

</code></pre>

<hr>
<h2 id='Id'>: Id</h2><span id='topic+Id'></span>

<h3>Description</h3>

<p>The function that returns the <code class="reqn">\mathsf{Id_1}</code> networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Id(d = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Id_+3A_d">d</code></td>
<td>
<p>the dimension of the <code class="reqn">Id</code> network, by default it is <code class="reqn">1</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the <code class="reqn">\mathsf{Id_1}</code> network.
</p>


<h3>References</h3>

<p>Definition 2.17. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Id()
Id(3)

</code></pre>

<hr>
<h2 id='inn'>inn</h2><span id='topic+inn'></span>

<h3>Description</h3>

<p>The function that returns the input layer size of a neural
network. Denoted <code class="reqn">\mathsf{I}</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inn(nu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inn_+3A_nu">nu</code></td>
<td>
<p>A neural network of the type generated by
create_nn().</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer representing the input width of the neural network.
</p>


<h3>References</h3>

<p>Definition 1.3.1. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(4, 5, 6, 2)) |&gt; inn()
</code></pre>

<hr>
<h2 id='inst'>inst</h2><span id='topic+inst'></span>

<h3>Description</h3>

<p>The function that instantiates a neural network as created
by create_nn().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inst(neural_network, activation_function, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inst_+3A_neural_network">neural_network</code></td>
<td>
<p>An ordered list of lists, of the type generated by
create_nn() where each element in the
list of lists is a pair <code class="reqn">(W,b)</code> representing the weights and biases of
that layer.
</p>
<p><em>NOTE:</em> We will call istantiation what Grohs et. al. call &quot;realization&quot;.</p>
</td></tr>
<tr><td><code id="inst_+3A_activation_function">activation_function</code></td>
<td>
<p>A continuous function applied to the output of each layer. For now we only
have ReLU, Sigmoid, and Tanh. Note, all proofs are only valid for ReLU activation.</p>
</td></tr>
<tr><td><code id="inst_+3A_x">x</code></td>
<td>
<p>our input to the continuous function formed from activation. Our input will
be an element in <code class="reqn">\mathbb{R}^d</code> for some appropriate <code class="reqn">d</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output of the continuous function that is the instantiation of the given
neural network with the given activation function at the given <code class="reqn">x</code>. Where <code class="reqn">x</code>
is of vector size equal to the input layer of the neural network.
</p>


<h3>References</h3>

<p>Grohs, P., Hornung, F., Jentzen, A. et al. Space-time error estimates for deep
neural network approximations for differential equations. (2019).
<a href="https://arxiv.org/abs/1908.03833">https://arxiv.org/abs/1908.03833</a>.
</p>
<p>Definition 1.3.4. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>
</p>
<p>Very precisely we will use the definition in:
</p>
<p>Definition 2.3 in Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(1, 3, 5, 6)) |&gt; inst(ReLU, 5)
create_nn(c(3, 3, 5, 6)) |&gt; inst(ReLU, c(4, 4, 4))

</code></pre>

<hr>
<h2 id='is_nn'>is_nn</h2><span id='topic+is_nn'></span>

<h3>Description</h3>

<p>Function to create a list of lists for neural network layers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_nn(nn)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_nn_+3A_nn">nn</code></td>
<td>
<p>A neural network. Neural networks are defined to be an ordered
list of ordered pairs of <code class="reqn">(W,b)</code>. Where <code class="reqn">W</code> is the matrix
representing the weight matrix <code class="reqn">W</code> at that layer and <code class="reqn">b</code> the bias vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE or FALSE on whether nn is indeed a neural network as defined above.
</p>
<p>We will use the definition of neural networks as found in:
</p>


<h3>References</h3>

<p>Definition 2.1 in Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>
<p>Which in turn is a modified version of the one found in:
</p>
<p>Definition 2.3. Grohs, P., Hornung, F., Jentzen, A. et al.
Space-time error estimates for deep neural network approximations
for differential equations. (2019).
<a href="https://arxiv.org/abs/1908.03833">https://arxiv.org/abs/1908.03833</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(5, 6, 7)) |&gt; is_nn()
Sqr(2.1, 0.1) |&gt; is_nn()

</code></pre>

<hr>
<h2 id='lay'>lay</h2><span id='topic+lay'></span>

<h3>Description</h3>

<p>The function that returns the layer architecture of a neural
network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lay(nu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lay_+3A_nu">nu</code></td>
<td>
<p>A neural network of the type generated by
create_nn(). Denoted <code class="reqn">\mathsf{L}</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tuple representing the layer architecture of our neural network.
</p>


<h3>References</h3>

<p>Definition 1.3.1. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(4, 5, 6, 2)) |&gt; lay()
</code></pre>

<hr>
<h2 id='MC'>The MC neural network</h2><span id='topic+MC'></span>

<h3>Description</h3>

<p>This function implements the 1-D approximation scheme outlined in the References.
</p>
<p><strong>Note:</strong> Only 1-D interpolation is implemented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MC(X, y, L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MC_+3A_x">X</code></td>
<td>
<p>a list of samples from the functions domain.</p>
</td></tr>
<tr><td><code id="MC_+3A_y">y</code></td>
<td>
<p>the function applied componentwise to each point in the domain.</p>
</td></tr>
<tr><td><code id="MC_+3A_l">L</code></td>
<td>
<p>the Lipschitz constant for the function. Not necessarily global,
but could be an absolute upper limit of slope, over the domain.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A neural network that gives the maximum convolution approximation
of a function whose outputs is <code class="reqn">y</code> at <code class="reqn">n</code> sample points given by
each row of <code class="reqn">X</code>, when instantiated with ReLU.
</p>


<h3>References</h3>

<p>Lemma 4.2.9. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
seq(0, 3.1416, length.out = 200) -&gt; X
sin(X) -&gt; y
MC(X, y, 1) |&gt; inst(ReLU, 0.25) # compare to sin(0.25)

</code></pre>

<hr>
<h2 id='Mxm'>Mxm</h2><span id='topic+Mxm'></span>

<h3>Description</h3>

<p>The function that returns the <code class="reqn">\mathsf{Mxm}</code> neural networks.
</p>
<p><em>Note:</em> Because of certain quirks of R we will have split
into five cases. We add an extra case for <code class="reqn">d = 3</code>. Unlike the paper
we will simply reverse engineer the appropriate <em>d</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Mxm(d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Mxm_+3A_d">d</code></td>
<td>
<p>The dimension of the input vector on instantiation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The neural network that will ouput the maximum of a vector of
size <code class="reqn">d</code> when activated with the ReLU function.
</p>
<p>For a specific definition, see:
</p>


<h3>References</h3>

<p>Lemma 4.2.4. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Mxm(1) |&gt; inst(ReLU, -5)
Mxm(3) |&gt; inst(ReLU, c(4, 5, 1))
Mxm(5) |&gt; inst(ReLU, c(5, 3, -1, 6, 6))

</code></pre>

<hr>
<h2 id='nn_sum'>nn_sum</h2><span id='topic+nn_sum'></span><span id='topic++25nn_sum+25'></span>

<h3>Description</h3>

<p>A function that performs the neural network sum for two
neural networks of the type generated by
create_nn().
</p>
<p>For a specific definition, see:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_sum(nu_1, nu_2)

nu_1 %nn_sum% nu_2
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_sum_+3A_nu_1">nu_1</code></td>
<td>
<p>A neural network.</p>
</td></tr>
<tr><td><code id="nn_sum_+3A_nu_2">nu_2</code></td>
<td>
<p>A neural network.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A neural network that is the neural network sum of <code class="reqn">\nu_1</code> and <code class="reqn">\nu_2</code>
i.e. <code class="reqn">\nu_1 \oplus \nu_2</code>.
</p>
<p><em>Note:</em> We have two versions, an infix version and a prefix version.
</p>


<h3>References</h3>

<p>Proposition 2.25. Grohs, P., Hornung, F., Jentzen, A. et al.
Space-time error estimates for deep neural network approximations
for differential equations. (2019).
<a href="https://arxiv.org/abs/1908.03833">https://arxiv.org/abs/1908.03833</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Prd(2.1, 0.1) |&gt; nn_sum(Prd(2.1, 0.1))

</code></pre>

<hr>
<h2 id='Nrm'>Nrm</h2><span id='topic+Nrm'></span>

<h3>Description</h3>

<p>A function that creates the <code class="reqn">\mathsf{Nrm}</code> neural networks.that take
the 1- norm of a <code class="reqn">d</code>-dimensional vector when instantiated with ReLU
activation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Nrm(d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Nrm_+3A_d">d</code></td>
<td>
<p>the dimensions of the vector or list being normed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a neural network that takes the 1-norm of a vector of
size d.under ReLU activation.
</p>
<p><em>Note:</em> This function is split into two cases
much like the definition itself.
</p>
<p><em>Note:</em> If you choose to specify a <code class="reqn">d</code> other that <code class="reqn">0</code> you must instantiate with
a vector or list of that length.
</p>
<p>For a specific definition, see:
</p>


<h3>References</h3>

<p>Lemma 4.2.1. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Nrm(2) |&gt; inst(ReLU, c(5,6))
Nrm(5) |&gt; inst(ReLU,c(0,-9,3,4,-11))


</code></pre>

<hr>
<h2 id='out'>out</h2><span id='topic+out'></span>

<h3>Description</h3>

<p>The function that returns the output layer size of a neural
network. Denoted <code class="reqn">\mathsf{O}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>out(nu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="out_+3A_nu">nu</code></td>
<td>
<p>A neural network of the type generated by create_nn().</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer representing the output width of the neural network.
</p>


<h3>References</h3>

<p>Definition 1.3.1. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(4, 5, 6, 2)) |&gt; out()
</code></pre>

<hr>
<h2 id='param'>param</h2><span id='topic+param'></span>

<h3>Description</h3>

<p>The function that returns the numbe of parameters of a neural
network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>param(nu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="param_+3A_nu">nu</code></td>
<td>
<p>A neural network of the type generated by
create_nn(). Denoted <code class="reqn">\mathsf{P}</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer representing the parameter count of our neural network.
</p>


<h3>References</h3>

<p>Definition 1.3.1. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(4, 5, 6, 2)) |&gt; param()
</code></pre>

<hr>
<h2 id='Phi'>The Phi function</h2><span id='topic+Phi'></span>

<h3>Description</h3>

<p>The Phi function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Phi(eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Phi_+3A_eps">eps</code></td>
<td>
<p>parameter for Phi in <code class="reqn">(0,\infty)</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>neural network Phi that approximately squares a number between
0 and 1.
</p>


<h3>References</h3>

<p>Definition 2.23. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Phi(0.5) |&gt; view_nn()
Phi(0.1) |&gt; view_nn()

</code></pre>

<hr>
<h2 id='Phi_k'>The Phi_k function</h2><span id='topic+Phi_k'></span>

<h3>Description</h3>

<p>The Phi_k function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Phi_k(k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Phi_k_+3A_k">k</code></td>
<td>
<p>an integer <code class="reqn">k \in (2,\infty)</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Phi_k neural network
</p>


<h3>References</h3>

<p>Definition 2.22. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Phi_k(4) |&gt; view_nn()
Phi_k(5) |&gt; view_nn()

</code></pre>

<hr>
<h2 id='Prd'>Prd</h2><span id='topic+Prd'></span>

<h3>Description</h3>

<p>A function that returns the <code class="reqn">\mathsf{Prd}</code> neural networks that
approximates the product of two real numbers when given an appropriate
<code class="reqn">q</code>, <code class="reqn">\varepsilon</code>, a real number <code class="reqn">x</code> and instantiation with ReLU.
activation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Prd(q, eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Prd_+3A_q">q</code></td>
<td>
<p>a real number in <code class="reqn">(2,\infty)</code>. Accuracy as well as computation
time increases as <code class="reqn">q</code> gets closer to <code class="reqn">2</code> increases</p>
</td></tr>
<tr><td><code id="Prd_+3A_eps">eps</code></td>
<td>
<p>a real number in <code class="reqn">(0,\infty)</code>. ccuracy as well as computation
time increases as <code class="reqn">\varepsilon</code> gets closer to <code class="reqn">0</code> increases</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A neural network that takes in <code class="reqn">x</code> and <code class="reqn">y</code> and approximately
returns <code class="reqn">xy</code> when instantiated with ReLU activation, and given a list
c(x,y), the two numbers to be multiplied.
</p>
<p><em>Note that this must be instantiated with a tuple c(x,y)</em>
</p>


<h3>References</h3>

<p>Proposition 3.5. Grohs, P., Hornung, F., Jentzen, A. et al. Space-time error estimates for deep
neural network approximations for differential equations. (2019).
<a href="https://arxiv.org/abs/1908.03833">https://arxiv.org/abs/1908.03833</a>
</p>
<p>Definition 2.25. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Prd(2.1, 0.1) |&gt; inst(ReLU, c(4, 5)) # This may take some time, please only click once

</code></pre>

<hr>
<h2 id='Pwr'>Pwr</h2><span id='topic+Pwr'></span>

<h3>Description</h3>

<p>A function that returns the <code class="reqn">\mathsf{Pwr}</code> neural networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Pwr(q, eps, exponent)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Pwr_+3A_q">q</code></td>
<td>
<p>a real number in <code class="reqn">(2,\infty)</code>. Accuracy as well as computation
time increases as <code class="reqn">q</code> gets closer to <code class="reqn">2</code> increases</p>
</td></tr>
<tr><td><code id="Pwr_+3A_eps">eps</code></td>
<td>
<p>a real number in <code class="reqn">(0,\infty)</code>. ccuracy as well as computation
time increases as <code class="reqn">\varepsilon</code> gets closer to <code class="reqn">0</code> increases</p>
</td></tr>
<tr><td><code id="Pwr_+3A_exponent">exponent</code></td>
<td>
<p>The power to which we will raise. Computation
time increases as exponent increases</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A neural network that approximates raising a number to exponent, when
given appropriate <code class="reqn">q,\varepsilon</code> and exponent when instantiated
under ReLU activation at <code class="reqn">x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Pwr(2.1, 0.1, 2) |&gt; inst(ReLU, 3) # This may take some time, please only click once.

</code></pre>

<hr>
<h2 id='ReLU'>: ReLU</h2><span id='topic+ReLU'></span>

<h3>Description</h3>

<p>The ReLU activation function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReLU(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ReLU_+3A_x">x</code></td>
<td>
<p>A real number that is the input to our ReLU function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output of the standard ReLU function, i.e. <code class="reqn">\max\{0,x\}</code>. See also <code><a href="#topic+Sigmoid">Sigmoid</a></code>.
and <code><a href="#topic+Tanh">Tanh</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ReLU(5)
ReLU(-5)

</code></pre>

<hr>
<h2 id='Sigmoid'>: Sigmoid</h2><span id='topic+Sigmoid'></span>

<h3>Description</h3>

<p>The Sigmoid activation function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sigmoid(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sigmoid_+3A_x">x</code></td>
<td>
<p>a real number that is the input to our Sigmoid function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output of a standard Sigmoid function,
i,e. <code class="reqn">\frac{1}{1 + \exp(-x)}</code>.
See also <code><a href="#topic+Tanh">Tanh</a></code>.and <code><a href="#topic+ReLU">ReLU</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Sigmoid(0)
Sigmoid(-1)

</code></pre>

<hr>
<h2 id='slm'>slm</h2><span id='topic+slm'></span><span id='topic++25slm+25'></span>

<h3>Description</h3>

<p>The function that returns the left  scalar multiplication
neural network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slm(a, nu)

a %slm% nu
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slm_+3A_a">a</code></td>
<td>
<p>A real number.</p>
</td></tr>
<tr><td><code id="slm_+3A_nu">nu</code></td>
<td>
<p>A neural network of the type generated by create_nn().</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a neural network that is <code class="reqn">a \triangleright \nu</code>. This
instantiates as <code class="reqn">a \cdot f(x)</code> under continuous function activation. More specifically
we define operation as:
</p>
<p>Let <code class="reqn">\lambda \in \mathbb{R}</code>. We will denote by <code class="reqn">(\cdot) \triangleright (\cdot):
\mathbb{R} \times \mathsf{NN} \rightarrow \mathsf{NN}</code> the function satisfying for all
<code class="reqn">\nu \in \mathsf{NN}</code> and <code class="reqn">\lambda \in \mathbb{R}</code> that <code class="reqn">\lambda \triangleright \nu =
\mathsf{Aff}_{\lambda \mathbb{I}_{\mathsf{I}(\nu)},0} \bullet \nu</code>.
</p>


<h3>References</h3>

<p>Definition 2.3.4. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>.
</p>
<p><em>Note:</em> We will have two versions of this operation, a prefix and an
infix version.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
5 |&gt; slm(Prd(2.1, 0.1))
Prd(2.1, 0.1) |&gt; srm(5)

</code></pre>

<hr>
<h2 id='Sne'>Sne</h2><span id='topic+Sne'></span>

<h3>Description</h3>

<p>Returns the <code class="reqn">\mathsf{Sne}</code> neural networks
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sne(n, q, eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sne_+3A_n">n</code></td>
<td>
<p>The number of Taylor iterations. Accuracy as well as computation
time increases as <code class="reqn">n</code> increases</p>
</td></tr>
<tr><td><code id="Sne_+3A_q">q</code></td>
<td>
<p>a real number in <code class="reqn">(2,\infty)</code>. Accuracy as well as computation
time increases as <code class="reqn">q</code> gets closer to <code class="reqn">2</code> increases</p>
</td></tr>
<tr><td><code id="Sne_+3A_eps">eps</code></td>
<td>
<p>a real number in <code class="reqn">(0,\infty)</code>. ccuracy as well as computation
time increases as <code class="reqn">\varepsilon</code> gets closer to <code class="reqn">0</code> increases
</p>
<p><em>Note: </em> In practice for most desktop uses
<code class="reqn">q &lt; 2.05</code> and <code class="reqn">\varepsilon&lt; 0.05</code> tends to cause problems in
&quot;too long a vector&quot;, atleaast as tested on my computer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A neural network that approximates <code class="reqn">\sin</code> when given
an appropriate <code class="reqn">n,q,\varepsilon</code> and instantiated with ReLU
activation and given value <code class="reqn">x</code>.
</p>


<h3>References</h3>

<p>Definition 2.30. Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Sne(2, 2.3, 0.3) # this may take some time, click only once and wait

Sne(2, 2.3, 0.3) |&gt; inst(ReLU, 1.57) # this may take some time, click only once and wait
</code></pre>

<hr>
<h2 id='Sqr'>Sqr</h2><span id='topic+Sqr'></span>

<h3>Description</h3>

<p>A function that returns the <code class="reqn">\mathsf{Sqr}</code> neural networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sqr(q, eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sqr_+3A_q">q</code></td>
<td>
<p>a real number in <code class="reqn">(2,\infty)</code>. Accuracy as well as computation
time increases as <code class="reqn">q</code> gets closer to <code class="reqn">2</code> increases</p>
</td></tr>
<tr><td><code id="Sqr_+3A_eps">eps</code></td>
<td>
<p>a real number in <code class="reqn">(0,\infty)</code>. ccuracy as well as computation
time increases as <code class="reqn">\varepsilon</code> gets closer to <code class="reqn">0</code> increases</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A neural network that approximates the square of a real number.when
provided appropriate <code class="reqn">q,\varepsilon</code> and upon instantiation with ReLU,
and a real number <code class="reqn">x</code>
</p>


<h3>References</h3>

<p>Proposition 3.4. Grohs, P., Hornung, F., Jentzen, A. et al. Space-time error estimates for deep
neural network approximations for differential equations. (2019).
<a href="https://arxiv.org/abs/1908.03833">https://arxiv.org/abs/1908.03833</a>
</p>
<p>#' @references Definition 2.24. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Sqr(2.5, 0.1)
Sqr(2.5, 0.1) |&gt; inst(ReLU, 4)

</code></pre>

<hr>
<h2 id='srm'>srm</h2><span id='topic+srm'></span><span id='topic++25srm+25'></span>

<h3>Description</h3>

<p>The function that returns the right scalar multiplication
neural network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>srm(nu, a)

nu %srm% a
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="srm_+3A_nu">nu</code></td>
<td>
<p>A neural network</p>
</td></tr>
<tr><td><code id="srm_+3A_a">a</code></td>
<td>
<p>A real number.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a neural network that is <code class="reqn">\nu \triangleleft a</code>. This
instantiates as <code class="reqn">f(a \cdot x)</code>.under continuous function activation. More
specifically we will define this operation as:
</p>
<p>Let <code class="reqn">\lambda \in \mathbb{R}</code>. We will denote by <code class="reqn">(\cdot) \triangleleft (\cdot):
\mathsf{NN} \times \mathbb{R} \rightarrow \mathsf{NN}</code> the function satisfying for all
<code class="reqn">\nu \in \mathsf{NN}</code> and <code class="reqn">\lambda \in \mathbb{R}</code> that <code class="reqn">\nu \triangleleft \lambda =
\nu \bullet \mathsf{Aff}_{\lambda \mathbb{I}_{\mathsf{I}(\nu)},0}</code>.
</p>


<h3>References</h3>

<p>Definition 2.3.4. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>.
</p>
<p><em>Note:</em> We will have two versions of this operation, a prefix
and an infix version.
</p>

<hr>
<h2 id='stk'>stk</h2><span id='topic+stk'></span><span id='topic++25stk+25'></span>

<h3>Description</h3>

<p>A function that stacks neural networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stk(nu, mu)

nu %stk% mu
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stk_+3A_nu">nu</code></td>
<td>
<p>neural network.</p>
</td></tr>
<tr><td><code id="stk_+3A_mu">mu</code></td>
<td>
<p>neural network.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A stacked neural network of <code class="reqn">\nu</code> and <code class="reqn">\mu</code>, i.e. <code class="reqn">\nu \boxminus \mu</code>
</p>
<p><strong>NOTE:</strong> This is different than the one given in Grohs, et. al. 2023.
While we use padding to equalize neural networks being parallelized our
padding is via the Tun network whereas Grohs et. al. uses repetitive
composition of the i network. We use repetitive composition of the <code class="reqn">\mathsf{Id_1}</code>
network. See <code><a href="#topic+Id">Id</a></code> <code><a href="#topic+comp">comp</a></code>
</p>
<p><strong>NOTE:</strong> The terminology is also different from Grohs et. al. 2023.
We call stacking what they call parallelization. This terminology change was
inspired by the fact that parallelization implies commutativity but this
operation is not quite commutative. It is commutative up to transposition
of our input x under instantiation with a continuous activation function.
</p>
<p>Also the word parallelization has a lot of baggage when it comes to
artificial neural networks in that it often means many different CPUs working
together.
</p>
<p><em>Remark:</em> We will use only one symbol for stacking equal and unequal depth
neural networks, namely &quot;stk&quot;. This is for usability but also that
for all practical purposes only the general stacking of neural networks
of different sizes is what is needed.
</p>
<p><em>Remark:</em> We have two versions, a prefix and an infix version.
</p>
<p>This operation on neural networks, called &quot;parallelization&quot; is found in:
</p>
<p>A stacked neural network of nu and mu.
</p>


<h3>References</h3>

<p>Grohs, P., Hornung, F., Jentzen, A. et al. Space-time error estimates for deep
neural network approximations for differential equations. (2023).
<a href="https://arxiv.org/abs/1908.03833">https://arxiv.org/abs/1908.03833</a>
</p>
<p>And especially in:
</p>
<p>' Definition 2.14 in Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_nn(c(4,5,6)) |&gt; stk(create_nn(c(6,7)))
create_nn(c(9,1,67)) |&gt; stk(create_nn(c(4,4,4,4,4)))


</code></pre>

<hr>
<h2 id='Sum'>Sum</h2><span id='topic+Sum'></span>

<h3>Description</h3>

<p>The function that returns <code class="reqn">\mathsf{Sum}</code> neural networks.
</p>
<p>These are neural networks defined as such
</p>
<p style="text-align: center;"><code class="reqn">
\mathsf{Aff}_{\left[ \mathbb{I}_k \: \mathbb{I}_k \: \cdots \: \mathbb{I}_k\right],0_{k}}
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>Sum(n, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sum_+3A_n">n</code></td>
<td>
<p>number of copies of a certain vector to be summed.</p>
</td></tr>
<tr><td><code id="Sum_+3A_k">k</code></td>
<td>
<p>the size of the summation vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An affine neural network that will take a vector of size
<code class="reqn">n \times k</code> and return the summation vector that is of length
<code class="reqn">k</code>. See also <code><a href="#topic+Aff">Aff</a></code> and <code><a href="#topic+Cpy">Cpy</a></code>.
</p>


<h3>References</h3>

<p>Definition 2.10. Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>

<hr>
<h2 id='Tanh'>Tanh</h2><span id='topic+Tanh'></span>

<h3>Description</h3>

<p>The tanh activation function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Tanh(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tanh_+3A_x">x</code></td>
<td>
<p>a real number</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the <code class="reqn">tanh</code> of x. See also <code><a href="#topic+Sigmoid">Sigmoid</a></code> and
<code><a href="#topic+ReLU">ReLU</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Tanh(0)
Tanh(0.1)

</code></pre>

<hr>
<h2 id='Tay'>The Tay function</h2><span id='topic+Tay'></span>

<h3>Description</h3>

<p>The Tay function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Tay(f, n, q, eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tay_+3A_f">f</code></td>
<td>
<p>the function to be Taylor approximated, for now &quot;exp&quot;, &quot;sin&quot;
and &quot;cos&quot;. NOTE use the quotation marks when using this argument.</p>
</td></tr>
<tr><td><code id="Tay_+3A_n">n</code></td>
<td>
<p>The number of Taylor iterations. Accuracy as well as computation
time increases as <code class="reqn">n</code> increases</p>
</td></tr>
<tr><td><code id="Tay_+3A_q">q</code></td>
<td>
<p>a real number in <code class="reqn">(2,\infty)</code>. Accuracy as well as computation
time increases as <code class="reqn">q</code> gets closer to <code class="reqn">2</code> increases</p>
</td></tr>
<tr><td><code id="Tay_+3A_eps">eps</code></td>
<td>
<p>a real number in <code class="reqn">(0,\infty)</code>. ccuracy as well as computation
time increases as <code class="reqn">\varepsilon</code> gets closer to <code class="reqn">0</code> increases</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a neural network that approximates the function f. For now only
<code class="reqn">sin</code>, <code class="reqn">cos</code>, and <code class="reqn">e^x</code> are available.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Tay("sin", 2, 2.3, 0.3) |&gt; inst(ReLU, 1.5) # May take some time, please only click once
Tay("cos", 2, 2.3, 0.3) |&gt; inst(ReLU, 1) # May take some time, please only click once
Tay("exp", 4, 2.3, 0.3) |&gt; inst(ReLU, 1.5) # May take some time, please only click once


</code></pre>

<hr>
<h2 id='Trp'>Trp</h2><span id='topic+Trp'></span>

<h3>Description</h3>

<p>The function that returns the <code class="reqn">\mathsf{Trp}</code> networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Trp(h)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Trp_+3A_h">h</code></td>
<td>
<p>the horizontal distance between two mesh points</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code class="reqn">\mathsf{Trp}</code> network that gives the area
when activated with ReLU or any continuous function and two
meshpoint values <code class="reqn">x_1</code> and <code class="reqn">x_2</code>.
</p>


<h3>References</h3>

<p>Definition 2.31. Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Trp(0.1)
Trp(0.5) |&gt; inst(ReLU, c(9, 7))
Trp(0.1) |&gt; inst(Sigmoid, c(9, 8))

</code></pre>

<hr>
<h2 id='Tun'>Tun: The function that returns tunneling neural networks</h2><span id='topic+Tun'></span>

<h3>Description</h3>

<p>Tun: The function that returns tunneling neural networks
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Tun(n, d = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tun_+3A_n">n</code></td>
<td>
<p>The depth of the tunnel network where <code class="reqn">n \in \mathbb{N} \cap [1,\infty)</code>.</p>
</td></tr>
<tr><td><code id="Tun_+3A_d">d</code></td>
<td>
<p>The dimension of the tunneling network. By default it is assumed to be <code class="reqn">1</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tunnel neural network of depth n. A tunneling neural
network is defined as the neural network <code class="reqn">\mathsf{Aff}_{1,0}</code> for <code class="reqn">n=1</code>,
the neural network <code class="reqn">\mathsf{Id}_1</code> for <code class="reqn">n=1</code> and the neural network
<code class="reqn">\bullet^{n-2}\mathsf{Id}_1</code> for <code class="reqn">n &gt;2</code>. For this to work we
must provide an appropriate <code class="reqn">n</code> and instantiate with ReLU at some
real number <code class="reqn">x</code>.
</p>


<h3>References</h3>

<p>Definition 2.17. Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Tun(4)
Tun(4, 3) |&gt; view_nn()

Tun(5)
Tun(5, 3)

</code></pre>

<hr>
<h2 id='view_nn'>view_nn</h2><span id='topic+view_nn'></span>

<h3>Description</h3>

<p>Takes a neural network shown in vectorized form and explicitly
displays it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>view_nn(nn)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="view_nn_+3A_nn">nn</code></td>
<td>
<p>A neural network., i.e.
a list of lists of <code class="reqn">W</code> and <code class="reqn">b</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A displayed version of the neural network. This may be required if
the neural network is very deep.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>c(5, 6, 7, 9) |&gt;
  create_nn() |&gt;
  view_nn()
Sqr(2.1, 0.1) |&gt; view_nn()

Xpn(3, 2.1, 1.1) |&gt; view_nn()
Pwr(2.1, 0.1, 3) |&gt; view_nn()
</code></pre>

<hr>
<h2 id='Xpn'>The Xpn function</h2><span id='topic+Xpn'></span>

<h3>Description</h3>

<p>The Xpn function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Xpn(n, q, eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Xpn_+3A_n">n</code></td>
<td>
<p>The number of Taylor iterations. Accuracy as well as computation
time increases as <code class="reqn">n</code> increases</p>
</td></tr>
<tr><td><code id="Xpn_+3A_q">q</code></td>
<td>
<p>a real number in <code class="reqn">(2,\infty)</code>. Accuracy as well as computation
time increases as <code class="reqn">q</code> gets closer to <code class="reqn">2</code> increases</p>
</td></tr>
<tr><td><code id="Xpn_+3A_eps">eps</code></td>
<td>
<p>a real number in <code class="reqn">(0,\infty)</code>. ccuracy as well as computation
time increases as <code class="reqn">\varepsilon</code> gets closer to <code class="reqn">0</code> increases
</p>
<p><em>Note: </em> In practice for most desktop uses
<code class="reqn">q &lt; 2.05</code> and <code class="reqn">\varepsilon&lt; 0.05</code> tends to cause problems in
&quot;too long a vector&quot;, atleaast as tested on my computer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A neural network that approximates <code class="reqn">e^x</code> for real <code class="reqn">x</code> when
given appropriate <code class="reqn">n,q,\varepsilon</code> and instnatiated with ReLU
activation at point<code class="reqn">x</code>.
</p>


<h3>References</h3>

<p>Definition 2.28 in Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Xpn(3, 2.25, 0.25) # this may take some time

Xpn(3, 2.2, 0.2) |&gt; inst(ReLU, 1.5) # this may take some time

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
