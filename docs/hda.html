<!DOCTYPE html><html><head><title>Help for package hda</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {hda}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#hda'><p>Heteroscedastic discriminant analysis</p></a></li>
<li><a href='#plot.hda'><p>Plot transformed data</p></a></li>
<li><a href='#predict.hda'><p>Heteroscedastic discriminant analysis</p></a></li>
<li><a href='#showloadings'><p>Loadings plot for heteroscedastic discriminant analysis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.2-14</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-03-04</td>
</tr>
<tr>
<td>Title:</td>
<td>Heteroscedastic Discriminant Analysis</td>
</tr>
<tr>
<td>Author:</td>
<td>Gero Szepannek</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Gero Szepannek &lt;gero.szepannek@web.de&gt;</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mvtnorm, klaR, MASS</td>
</tr>
<tr>
<td>Imports:</td>
<td>e1071</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions to perform dimensionality reduction for classification if the covariance matrices of the classes are unequal. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-03-04 17:16:27 UTC; Gero</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-03-04 22:44:09</td>
</tr>
</table>
<hr>
<h2 id='hda'>Heteroscedastic discriminant analysis</h2><span id='topic+hda'></span><span id='topic+hda.default'></span><span id='topic+hda.formula'></span><span id='topic+print.hda'></span>

<h3>Description</h3>

<p>Computes a linear transformation loadings matrix for discrimination of classes with unequal covariance matrices.</p>


<h3>Usage</h3>

<pre><code class='language-R'>hda(x, ...)
## Default S3 method:
hda(x, grouping, newdim = 1:(ncol(x)-1), crule = FALSE, 
             reg.lamb = NULL, reg.gamm = NULL, initial.loadings = NULL, 
             sig.levs = c(0.05,0.05), noutit = 7, ninit = 10, verbose = TRUE, ...)
## S3 method for class 'formula'
hda(formula, data = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hda_+3A_x">x</code></td>
<td>
<p>A matrix or data frame containing the explanatory variables. The method is restricted to numerical data.</p>
</td></tr>
<tr><td><code id="hda_+3A_grouping">grouping</code></td>
<td>
<p>A factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="hda_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>grouping ~ x1 + x2 + ...</code> That is, the response is the grouping factor and the right hand side specifies the (non-factor) discriminators.</p>
</td></tr>
<tr><td><code id="hda_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in formula are to be taken.</p>
</td></tr>
<tr><td><code id="hda_+3A_newdim">newdim</code></td>
<td>
<p>Dimension of the discriminative subspace. The class distributions are assumed to be equal in the remaining dimensions. 
Alternatively, a vector of integers can be specified which is then computed until for the first time both tests on equal means as well 
as homoscedasticity do not reject. This option is to be be applied with care and the resulting dimension should be 
checked manually.</p>
</td></tr>  
<tr><td><code id="hda_+3A_crule">crule</code></td>
<td>
<p>Logical specifying whether a <code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code> classification rule should be computed. Requires package <code>e1071</code>.</p>
</td></tr>
<tr><td><code id="hda_+3A_reg.lamb">reg.lamb</code></td>
<td>
<p>Parameter in [0,1] for regularization towards equal covariance matrix estimations of the classes (in the original space): 
0 means equal covariances, 1 (default) means complete heteroscedasticity.</p>
</td></tr>
<tr><td><code id="hda_+3A_reg.gamm">reg.gamm</code></td>
<td>
<p>Similar to <code>reg.lambd</code>: parameter for shrinkage towards diagonal covariance matrices of equal variance in all variables where 0 means diagonality. Default is no shrinkage.</p>
</td></tr>
<tr><td><code id="hda_+3A_initial.loadings">initial.loadings</code></td>
<td>
<p>Initial guess of the matrix of loadings. Must be quadratic of size <code>ncol(x)</code> Default is the identity matrix. By specification of <code>initial.loadings = "random"</code> a random orthonormal matrix will be generated using <code>qr.Q(qr())</code> of a random matrix with uniformly distributed elements.</p>
</td></tr>
<tr><td><code id="hda_+3A_sig.levs">sig.levs</code></td>
<td>
<p>Vector of significance levels for eqmean.test (position 1) and homog.test (pos. 2) to stop search for an appropriate dimension of the reduced space.</p>
</td></tr>
<tr><td><code id="hda_+3A_noutit">noutit</code></td>
<td>
<p>Number iterations of the outer loop, i.e. iterations of the likelihood. Default is 7.</p>
</td></tr>
<tr><td><code id="hda_+3A_ninit">ninit</code></td>
<td>
<p>Number of iterations of the inner loop, i.e. reiterations of the loadings matrix within one iteration step of the likelihood.</p>
</td></tr>
<tr><td><code id="hda_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating whether iteration process should be displayed.</p>
</td></tr>
<tr><td><code id="hda_+3A_...">...</code></td>
<td>
<p>For <code>hda.formula</code>: Further arguments passed to function <code>hda.default</code> such as <code>newdim</code>. For <code>hda.default</code>: currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the transformation that maximizes the likelihood if the classes are normally distributed 
but differ only in a <code>newdim</code> dimensional subspace and have equal distributions in the remaining dimensions  
(see Kumar and Andreou, 1998). The scores are uncorrelated for all classes. The algorithm is implemented as it is proposed by 
Burget (2006). Regularization is computed as proposed by Friedman et al. (1989) and Szepannek et al. (2009).
</p>


<h3>Value</h3>

<p>Returns an object of class <em>hda</em>. 
</p>
<table>
<tr><td><code>hda.loadings</code></td>
<td>
<p>Transformation matrix to be post-multiplied to new data.</p>
</td></tr>
<tr><td><code>hda.scores</code></td>
<td>
<p>Input data after hda transformation. Reduced discriminative space are the first <code>newdim</code> dimensions.</p>
</td></tr>
<tr><td><code>grouping</code></td>
<td>
<p>Corresponding class labels for <code>hda.scores</code> data. Identical to input grouping.</p>
</td></tr>
<tr><td><code>class.dist</code></td>
<td>
<p>Estimated class means and covariance matrices in the transformed space.</p>
</td></tr>
<tr><td><code>reduced.dimension</code></td>
<td>
<p>Input parameter: dimension of the reduced space.</p>
</td></tr>
<tr><td><code>naivebayes</code></td>
<td>
<p>Object of class <code>naiveBayes</code> trained on input data in the reduced space for classification 
of new (transformed) data. Its computation must be specified by input the parameter <code>crule</code>.</p>
</td></tr>
<tr><td><code>comp.acc</code></td>
<td>
<p>Matrix of accuracies per component and class: reports up to which degree each class k can be classified (<code class="reqn">P(f_k&gt;f_{\ne k})</code>) correctly according to the estimated (normal) distribution in any single component in the identified subspace. Meaningful for reasons of interpretability as HDA is invariant to reordering of the components.</p>
</td></tr>        
<tr><td><code>vlift</code></td>
<td>
<p>Returns the variable importance in terms of ratio between the accuracy <code>comp.acc</code> and the resulting accuracy that results if single variable loadings are set to 0. The first element describes overall accuracy lift where  the second element is an array of dimension (number of classes, number of components in reduced space, number of variables) specifying the lifts for recognition each class separately.</p>
</td></tr>
<tr><td><code>reg.lambd</code></td>
<td>
<p>Input regularization parameter.</p>
</td></tr>
<tr><td><code>reg.gamm</code></td>
<td>
<p>Input regularization parameter.</p>
</td></tr>
<tr><td><code>eqmean.test</code></td>
<td>
<p>Test on equal means of the classes in the remaining dimensions like in <code>manova</code> based on Wilk's lambda.</p>
</td></tr>
<tr><td><code>homog.test</code></td>
<td>
<p>Test on homoscedasticity of the classes in the remaining dimensions (see e.g. Fahrmeir et al., 1984, p.75.)</p>
</td></tr>
<tr><td><code>hda.call</code></td>
<td>
<p>(Matched) function call.</p>
</td></tr>
<tr><td><code>initial.loadings</code></td>
<td>
<p>Initialization of the loadings matrix.</p>
</td></tr>
<tr><td><code>trace.dimensions</code></td>
<td>
<p>Matrix of p values for different subspace dimensions (as specified in <code>newdim</code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gero Szepannek</p>


<h3>References</h3>

<p>Burget, L. (2006): <em>Combination of speech features using smoothed heteroscedastic discriminant analysis.</em> 
Proceedings of Interspeech 2004, pp. 2549-2552.
</p>
<p>Fahrmeir, L. and Hamerle, A. (1984): <em>Multivariate statistische Verfahren.</em> de Gruyter, Berlin.
</p>
<p>Friedman, J. (1989): <em>Regularized discriminant analysis.</em> JASA 84, 165-175.
</p>
<p>Kumar, N. and Andreou, A. (1998): <em>Heteroscedastic discriminant analysis and reduced rank HMMs 
for improved speech recognition.</em> Speech Communication 25, pp.283-297.
</p>
<p>Szepannek G., Harczos, T., Klefenz, F. and Weihs, C. (2009): <em>Extending features for automatic speech recognition 
by means of auditory modelling.</em> In: Proceedings of European Signal Processing Conference (EUSIPCO) 2009, Glasgow, pp.1235-1239. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.hda">predict.hda</a></code>, <code><a href="#topic+showloadings">showloadings</a></code>, <code><a href="#topic+plot.hda">plot.hda</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mvtnorm)
library(MASS)

# simulate data for two classes
n           &lt;- 50
meana       &lt;- meanb &lt;- c(0,0,0,0,0)
cova        &lt;- diag(5)
cova[1,1]   &lt;- 0.2
for(i in 3:4){
  for(j in (i+1):5){
    cova[i,j] &lt;- cova[j,i] &lt;- 0.75^(j-i)}
  }
covb       &lt;- cova
diag(covb)[1:2]  &lt;- c(1,0.2)

xa      &lt;- rmvnorm(n, meana, cova)
xb      &lt;- rmvnorm(n, meanb, covb)
x       &lt;- rbind(xa, xb)
classes &lt;- as.factor(c(rep(1,n), rep(2,n)))
# rotate simulated data
symmat &lt;- matrix(runif(5^2),5)
symmat &lt;- symmat + t(symmat)
even   &lt;- eigen(symmat)$vectors
rotatedspace &lt;- x %*% even
plot(as.data.frame(rotatedspace), col = classes)

# apply linear discriminant analysis and plot data on (single) discriminant axis
lda.res &lt;- lda(rotatedspace, classes)
plot(rotatedspace %*% lda.res$scaling, col = classes, 
     ylab = "discriminant axis", xlab = "Observation index")

# apply heteroscedastic discriminant analysis and plot data in discriminant space
hda.res &lt;- hda(rotatedspace, classes)
plot(hda.res$hda.scores, col = classes)

# compare with principal component analysis
pca.res  &lt;- prcomp(as.data.frame(rotatedspace), retx = TRUE)
plot(as.data.frame(pca.res$x), col=classes)

# Automatically build classification rule
# this requires package e1071
hda.res2 &lt;- hda(rotatedspace, classes, crule = TRUE)
</code></pre>

<hr>
<h2 id='plot.hda'>Plot transformed data</h2><span id='topic+plot.hda'></span>

<h3>Description</h3>

<p>Visualizes the scores on selected components of the  
discriminant space of reduced dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hda'
plot(x, comps = 1:x$reduced.dimension, scores = TRUE, col = x$grouping, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.hda_+3A_x">x</code></td>
<td>
<p>An object of class <code>hda</code>.</p>
</td></tr>
<tr><td><code id="plot.hda_+3A_comps">comps</code></td>
<td>
<p>A vector of component ids for which the data should be displayed.</p>
</td></tr>
<tr><td><code id="plot.hda_+3A_scores">scores</code></td>
<td>
<p>Logical indicating whether the scores in the projected space should be plotted. 
If FALSE estimated densities are plotted.</p>
</td></tr>
<tr><td><code id="plot.hda_+3A_col">col</code></td>
<td>
<p>Color vector for the data to be displayed. Per default, different colors represent the classes.</p>
</td></tr>
<tr><td><code id="plot.hda_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the plot function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Scatterplots of the scores or estimated densities.        
</p>


<h3>Value</h3>

<p>No value is returned.
</p>


<h3>Author(s)</h3>

<p>Gero Szepannek</p>


<h3>References</h3>

<p>Kumar, N. and Andreou, A. (1998): <em>Heteroscedastic discriminant analysis and reduced rank HMMs 
for improved speech recognition.</em> Speech Communication 25, pp.283-297.
</p>
<p>Szepannek G., Harczos, T., Klefenz, F. and Weihs, C. (2009): <em>Extending features for automatic speech recognition 
by means of auditory modelling.</em> In: Proceedings of European Signal Processing Conference (EUSIPCO) 2009, Glasgow, pp.1235-1239. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hda">hda</a></code>, <code><a href="#topic+predict.hda">predict.hda</a></code>, <code><a href="#topic+showloadings">showloadings</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library("mvtnorm")
library("MASS")

# simulate data for two classes
n           &lt;- 50
meana       &lt;- meanb &lt;- c(0,0,0,0,0)
cova        &lt;- diag(5)
cova[1,1]   &lt;- 0.2
for(i in 3:4){
  for(j in (i+1):5){
    cova[i,j] &lt;- cova[j,i] &lt;- 0.75^(j-i)}
  }
covb       &lt;- cova
diag(covb)[1:2]  &lt;- c(1,0.2)

xa      &lt;- rmvnorm(n, meana, cova)
xb      &lt;- rmvnorm(n, meanb, covb)
x       &lt;- rbind(xa,xb)
classes &lt;- as.factor(c(rep(1,n), rep(2,n)))
## rotate simulated data
symmat &lt;- matrix(runif(5^2),5)
symmat &lt;- symmat + t(symmat)
even   &lt;- eigen(symmat)$vectors
rotatedspace &lt;- x %*% even
plot(as.data.frame(rotatedspace), col = classes)

# apply heteroscedastic discriminant analysis and plot data in discriminant space
hda.res &lt;- hda(rotatedspace, classes)

# plot scores
plot(hda.res)
</code></pre>

<hr>
<h2 id='predict.hda'>Heteroscedastic discriminant analysis</h2><span id='topic+predict.hda'></span>

<h3>Description</h3>

<p>Computes linear transformation of new data into lower dimensional discriminative space using some model produced by <code><a href="#topic+hda">hda</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hda'
predict(object, newdata, alldims = FALSE, task = c("dr", "c"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.hda_+3A_object">object</code></td>
<td>
<p>Model resulting from a call of <code>hda</code>.</p>
</td></tr>
<tr><td><code id="predict.hda_+3A_newdata">newdata</code></td>
<td>
<p>A matrix or data frame to be transformed into lower dimensional space of the same dimension as the data used for building the model.</p>
</td></tr>
<tr><td><code id="predict.hda_+3A_alldims">alldims</code></td>
<td>
<p>Logical flag specifying whether the result should contain only the reduced space (default) or should also 
include the redundant dimensions and thus be of the same dimension as the input data. In this case the reduced
space is given by the first <code>newdim</code> columns.</p>
</td></tr>
<tr><td><code id="predict.hda_+3A_task">task</code></td>
<td>
<p><code>"dr"</code> for standard application of the <code>hda</code> model to <code>newdata</code>. Choose <code>"c"</code> for classification of new data. 
This is an interface to predict function of <code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code>. 
The option can be chosen if <code>crule = TRUE</code> has been specified in the <code>hda()</code> call.</p>
</td></tr>
<tr><td><code id="predict.hda_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the <code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code> predict function.</p>
</td></tr>        
</table>


<h3>Value</h3>

<p>If option <code>type = "dr"</code> the transformed data are returned. For <code>type = "c"</code> both the transformed data as well as
the resulting object of the naive Bayes prediction are returned.
</p>


<h3>Author(s)</h3>

<p>Gero Szepannek</p>


<h3>References</h3>

<p>Kumar, N. and Andreou, A. (1998): <em>Heteroscedastic discriminant analysis and reduced rank HMMs 
for improved speech recognition.</em> Speech Communication 25, pp. 283-297.
</p>
<p>Szepannek G., Harczos, T., Klefenz, F. and Weihs, C. (2009): <em>Extending features for automatic speech recognition 
by means of auditory modelling.</em> In: Proceedings of European Signal Processing Conference (EUSIPCO) 2009, Glasgow, pp. 1235-1239. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hda">hda</a></code>, <code><a href="#topic+showloadings">showloadings</a></code>, <code><a href="#topic+plot.hda">plot.hda</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mvtnorm)
library(MASS)

# simulate data for two classes
n           &lt;- 50
meana       &lt;- meanb &lt;- c(0,0,0,0,0)
cova        &lt;- diag(5)
cova[1,1]   &lt;- 0.2
for(i in 3:4){
  for(j in (i+1):5){cova[i,j] &lt;- cova[j,i] &lt;- 0.75^(j-i)}
  }
covb       &lt;- cova
diag(covb)[1:2]  &lt;- c(1,0.2)

xa      &lt;- rmvnorm(n,meana,cova)
xb      &lt;- rmvnorm(n,meanb,covb)
x       &lt;- rbind(xa,xb)
classes &lt;- as.factor(c(rep(1,n),rep(2,n)))
# rotate simulated data
symmat &lt;- matrix(runif(5^2),5)
symmat &lt;- symmat + t(symmat)
even   &lt;- eigen(symmat)$vectors
rotatedspace &lt;- x %*% even

# apply heteroscedastic discriminant analysis and plot data in discriminant space
hda.res &lt;- hda(rotatedspace, classes)

# simulate new data
xanew      &lt;- rmvnorm(n,meana,cova)
xbnew      &lt;- rmvnorm(n,meanb,covb)
xnew       &lt;- rbind(xanew,xbnew)
classes &lt;- as.factor(c(rep(1,n),rep(2,n)))
newrotateddata &lt;- x %*% even
plot(as.data.frame(newrotateddata), col = classes)

# transform new data 
prediction &lt;- predict(hda.res, newrotateddata)
plot(as.data.frame(prediction), col = classes)

# predict classes for new data on automatically computed naive Bayes classification rule 
# this requires package e1071
hda.res2 &lt;- hda(rotatedspace, classes, crule = TRUE)
prediction2 &lt;- predict(hda.res2, newrotateddata, task = "c")
prediction2
</code></pre>

<hr>
<h2 id='showloadings'>Loadings plot for heteroscedastic discriminant analysis</h2><span id='topic+showloadings'></span>

<h3>Description</h3>

<p>Visualizes the loadings of the original variables on the components of the transformed 
discriminant space of reduced dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>showloadings(object, comps = 1:object$reduced.dimension, loadings = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="showloadings_+3A_object">object</code></td>
<td>
<p>An object of class <code>hda</code>.</p>
</td></tr>
<tr><td><code id="showloadings_+3A_comps">comps</code></td>
<td>
<p>A vector of component ids for which the loadings should be displayed.</p>
</td></tr>
<tr><td><code id="showloadings_+3A_loadings">loadings</code></td>
<td>
<p>Logical indicating whether loadings or variable importance lifts should be plotted.</p>
</td></tr>  
<tr><td><code id="showloadings_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the plot functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Scatterplots of loadings (or lifts) of any variable on any hda component to give an idea of what variables do mainly contribute to the different discriminant components (see corresponding values of <code>object</code>). Note that as opposed to linear discriminant analysis not only location but also scale differences contribute to class discrimination of the hda components.    
</p>


<h3>Value</h3>

<p>No value is returned.
</p>


<h3>Author(s)</h3>

<p>Gero Szepannek</p>


<h3>References</h3>

<p>Kumar, N. and Andreou, A. (1998): <em>Heteroscedastic discriminant analysis and reduced rank HMMs 
for improved speech recognition.</em> Speech Communication 25, pp.283-297.
</p>
<p>Szepannek G., Harczos, T., Klefenz, F. and Weihs, C. (2009): <em>Extending features for automatic speech recognition 
by means of auditory modelling.</em> In: Proceedings of European Signal Processing Conference (EUSIPCO) 2009, Glasgow, pp.1235-1239. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hda">hda</a></code>, <code><a href="#topic+predict.hda">predict.hda</a></code>, <code><a href="#topic+plot.hda">plot.hda</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mvtnorm)
library(MASS)

# simulate data for two classes
n           &lt;- 50
meana       &lt;- meanb &lt;- c(0,0,0,0,0)
cova        &lt;- diag(5)
cova[1,1]   &lt;- 0.2
for(i in 3:4){
  for(j in (i+1):5){
    cova[i,j] &lt;- cova[j,i] &lt;- 0.75^(j-i)}
  }
covb       &lt;- cova
diag(covb)[1:2]  &lt;- c(1,0.2)

xa      &lt;- rmvnorm(n, meana, cova)
xb      &lt;- rmvnorm(n, meanb, covb)
x       &lt;- rbind(xa,xb)
classes &lt;- as.factor(c(rep(1,n), rep(2,n)))
# rotate simulated data
symmat &lt;- matrix(runif(5^2),5)
symmat &lt;- symmat + t(symmat)
even   &lt;- eigen(symmat)$vectors
rotatedspace &lt;- x %*% even
plot(as.data.frame(rotatedspace), col = classes)

# apply heteroscedastic discriminant analysis and plot data in discriminant space
hda.res &lt;- hda(rotatedspace, classes)

# visualize loadings
showloadings(hda.res)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
