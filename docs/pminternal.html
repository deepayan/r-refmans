<!DOCTYPE html><html lang="en"><head><title>Help for package pminternal</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pminternal}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#pminternal-package'><p>pminternal: Internal Validation of Clinical Prediction Models</p></a></li>
<li><a href='#boot_optimism'><p>Calculate optimism and bias-corrected scores via bootstrap resampling</p></a></li>
<li><a href='#cal_defaults'><p>Get default settings for calibration curves</p></a></li>
<li><a href='#cal_plot'><p>Plot apparent and bias-corrected calibration curves</p></a></li>
<li><a href='#calibration_stability'><p>Plot calibration stability across bootstrap replicates</p></a></li>
<li><a href='#classification_stability'><p>Classification instability plot</p></a></li>
<li><a href='#confint.internal_validate'><p>Confidence intervals for bias-corrected performance measures</p></a></li>
<li><a href='#crossval'><p>Calculate bias-corrected scores via cross-validation</p></a></li>
<li><a href='#dcurve_stability'><p>Plot decision curve stability across bootstrap replicates</p></a></li>
<li><a href='#get_stability'><p>Get stability from internal_validate or internal_boot object</p></a></li>
<li><a href='#mape_stability'><p>Mean absolute predictor error (MAPE) stability plot</p></a></li>
<li><a href='#prediction_stability'><p>Plot prediction stability across bootstrap replicates</p></a></li>
<li><a href='#print.internal_boot'><p>Print a internal_boot object</p></a></li>
<li><a href='#print.internal_cv'><p>Print a internal_cv object</p></a></li>
<li><a href='#print.internal_validate'><p>print a internal_validate object</p></a></li>
<li><a href='#print.internal_validatesummary'><p>Print summary of internal_validate object</p></a></li>
<li><a href='#score_binary'><p>Score predictions for binary events</p></a></li>
<li><a href='#summary.internal_validate'><p>Summarize a internal_validate object</p></a></li>
<li><a href='#validate'><p>Get bias-corrected performance measures via bootstrapping or cross-validation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Internal Validation of Clinical Prediction Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Stephen Rhodes &lt;steverho89@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Conduct internal validation of a clinical prediction model for a binary outcome.
  Produce bias corrected performance metrics (c-statistic, Brier score, calibration intercept/slope)
  via bootstrap (simple bootstrap, bootstrap optimism, .632 optimism) and cross-validation (CV optimism,
  CV average). Also includes functions to assess model stability via bootstrap resampling.
  See Steyerberg et al. (2001) &lt;<a href="https://doi.org/10.1016%2Fs0895-4356%2801%2900341-9">doi:10.1016/s0895-4356(01)00341-9</a>&gt;;
  Harrell (2015) &lt;<a href="https://doi.org/10.1007%2F978-3-319-19425-7">doi:10.1007/978-3-319-19425-7</a>&gt;;
  Riley and Collins (2023) &lt;<a href="https://doi.org/10.1002%2Fbimj.202200302">doi:10.1002/bimj.202200302</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/stephenrho/pminternal">https://github.com/stephenrho/pminternal</a>,
<a href="https://stephenrho.github.io/pminternal/">https://stephenrho.github.io/pminternal/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/stephenrho/pminternal/issues">https://github.com/stephenrho/pminternal/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>dcurves, graphics, grDevices, insight, marginaleffects,
methods, pmcalibration, pROC, stats, parallel, pbapply, purrr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>ggplot2, glmnet, Hmisc, knitr, rmarkdown, ranger, gbm, rms,
mgcv, mice</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>rmarkdown</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-15 17:24:31 UTC; stephenrhodes</td>
</tr>
<tr>
<td>Author:</td>
<td>Stephen Rhodes [aut, cre, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-18 08:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='pminternal-package'>pminternal: Internal Validation of Clinical Prediction Models</h2><span id='topic+pminternal'></span><span id='topic+pminternal-package'></span>

<h3>Description</h3>

<p>Conduct internal validation of a clinical prediction model for a binary outcome. Produce bias corrected performance metrics (c-statistic, Brier score, calibration intercept/slope) via bootstrap (simple bootstrap, bootstrap optimism, .632 optimism) and cross-validation (CV optimism, CV average). Also includes functions to assess model stability via bootstrap resampling. See Steyerberg et al. (2001) <a href="https://doi.org/10.1016/s0895-4356%2801%2900341-9">doi:10.1016/s0895-4356(01)00341-9</a>; Harrell (2015) <a href="https://doi.org/10.1007/978-3-319-19425-7">doi:10.1007/978-3-319-19425-7</a>; Riley and Collins (2023) <a href="https://doi.org/10.1002/bimj.202200302">doi:10.1002/bimj.202200302</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Stephen Rhodes <a href="mailto:steverho89@gmail.com">steverho89@gmail.com</a> [copyright holder]
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/stephenrho/pminternal">https://github.com/stephenrho/pminternal</a>
</p>
</li>
<li> <p><a href="https://stephenrho.github.io/pminternal/">https://stephenrho.github.io/pminternal/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/stephenrho/pminternal/issues">https://github.com/stephenrho/pminternal/issues</a>
</p>
</li></ul>


<hr>
<h2 id='boot_optimism'>Calculate optimism and bias-corrected scores via bootstrap resampling</h2><span id='topic+boot_optimism'></span>

<h3>Description</h3>

<p>Estimate bias-corrected scores via calculation of bootstrap optimism (standard or .632).
Can also produce estimates for assessing the stability of prediction model predictions.
This function is called by <code><a href="#topic+validate">validate</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot_optimism(
  data,
  outcome,
  model_fun,
  pred_fun,
  score_fun,
  method = c("boot", ".632"),
  B = 200,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="boot_optimism_+3A_data">data</code></td>
<td>
<p>the data used in developing the model. Should contain all variables considered (i.e., even those excluded by variable selection in the development sample)</p>
</td></tr>
<tr><td><code id="boot_optimism_+3A_outcome">outcome</code></td>
<td>
<p>character denoting the column name of the outcome in <code>data</code>.</p>
</td></tr>
<tr><td><code id="boot_optimism_+3A_model_fun">model_fun</code></td>
<td>
<p>a function that takes at least one argument, <code>data</code>. This function should implement the entire model development procedure (i.e., hyperparameter tuning, variable selection, imputation). Additional arguments can be provided via <code>...</code>. This function should return an object that works with <code>pred_fun</code>.</p>
</td></tr>
<tr><td><code id="boot_optimism_+3A_pred_fun">pred_fun</code></td>
<td>
<p>function that takes at least two arguments, <code>model</code> and <code>data</code>. This function should return a numeric vector of predicted probabilities of the outcome with the same length as the number of rows in <code>data</code> so it is important to take into account how missing data is treated (e.g., <code>predict.glm</code> omits predictions for rows with missing values). see <code>vignette("missing-data", package = "pminternal")</code>.</p>
</td></tr>
<tr><td><code id="boot_optimism_+3A_score_fun">score_fun</code></td>
<td>
<p>a function to calculate the metrics of interest. If this is not specified <code><a href="#topic+score_binary">score_binary</a></code> is used.</p>
</td></tr>
<tr><td><code id="boot_optimism_+3A_method">method</code></td>
<td>
<p>&quot;boot&quot; or &quot;.632&quot;. The former estimates bootstrap optimism for each score and subtracts
from apparent scores (simple bootstrap estimates are also produced as a by-product).
The latter estimates &quot;.632&quot; optimism as described in Harrell (2015). See <code><a href="#topic+validate">validate</a></code> details.</p>
</td></tr>
<tr><td><code id="boot_optimism_+3A_b">B</code></td>
<td>
<p>number of bootstrap resamples to run</p>
</td></tr>
<tr><td><code id="boot_optimism_+3A_...">...</code></td>
<td>
<p>additional arguments for <code>model_fun</code>, <code>pred_fun</code>, and/or <code>score_fun</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of class <code>internal_boot</code> containing:
</p>

<ul>
<li><p><code>apparent</code> - scores calculated on the original data using the original model.
</p>
</li>
<li><p><code>optimism</code> - estimates of optimism for each score (average difference in score for bootstrap models evaluated on bootstrap vs original sample) which can be subtracted from 'apparent' performance calculated using the original model on the original data.
</p>
</li>
<li><p><code>corrected</code> - 'bias corrected' scores (apparent - optimism)
</p>
</li>
<li><p><code>simple</code> - if method = &quot;boot&quot;, estimates of scores derived from the 'simple bootstrap'. This is the average of each score calculated from the bootstrap models evaluated on the original outcome data. NULL if method = &quot;.632&quot;
</p>
</li>
<li><p><code>stability</code> - if method = &quot;boot&quot;, a N,(B+1) matrix where N is the number of observations in <code>data</code> and <code>B</code> is the number of bootstrap samples. The first column contains the original predictions and each of subsequent B columns contain the predicted probabilities of the outcome from each bootstrap model evaluated on the original data. There may be fewer than B+1 columns if errors occur during resamples (when model_fun throws an error all scores are NA). NULL if method = &quot;.632&quot;
</p>
</li></ul>



<h3>References</h3>

<p>Steyerberg, E. W., Harrell Jr, F. E., Borsboom, G. J., Eijkemans, M. J. C., Vergouwe, Y., &amp; Habbema, J. D. F. (2001). Internal validation of predictive models: efficiency of some procedures for logistic regression analysis. Journal of clinical epidemiology, 54(8), 774-781.
</p>
<p>Harrell Jr F. E. (2015). Regression Modeling Strategies: with applications to linear models, logistic and ordinal regression, and survival analysis. New York: Springer Science, LLC.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(pminternal)
set.seed(456)
# simulate data with two predictors that interact
dat &lt;- pmcalibration::sim_dat(N = 1000, a1 = -2, a3 = -.3)
mean(dat$y)
dat$LP &lt;- NULL # remove linear predictor

# fit a (misspecified) logistic regression model
model_fun &lt;- function(data, ...){
  glm(y ~ x1 + x2, data=data, family="binomial")
}

pred_fun &lt;- function(model, data, ...){
  predict(model, newdata=data, type="response")
}

boot_optimism(data=dat, outcome="y", model_fun=model_fun, pred_fun=pred_fun,
              method="boot", B=20) # B set to 20 for example but should be &gt;= 200

</code></pre>

<hr>
<h2 id='cal_defaults'>Get default settings for calibration curves</h2><span id='topic+cal_defaults'></span>

<h3>Description</h3>

<p>Get default settings for calibration curves
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_defaults(x = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cal_defaults_+3A_x">x</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing default arguments to supply to <code>pmcalibration::pmcalibration</code>
</p>

<hr>
<h2 id='cal_plot'>Plot apparent and bias-corrected calibration curves</h2><span id='topic+cal_plot'></span>

<h3>Description</h3>

<p>Plot apparent and bias-corrected calibration curves
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_plot(
  x,
  xlim,
  ylim,
  xlab,
  ylab,
  app_col,
  bc_col,
  app_lty,
  bc_lty,
  plotci = c("if", "yes", "no")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cal_plot_+3A_x">x</code></td>
<td>
<p>an object returned from <code><a href="#topic+validate">validate</a></code>. Original call should
have specified 'eval' argument. See <code><a href="#topic+score_binary">score_binary</a></code>.</p>
</td></tr>
<tr><td><code id="cal_plot_+3A_xlim">xlim</code></td>
<td>
<p>x limits (default = c(0, max of either curve))</p>
</td></tr>
<tr><td><code id="cal_plot_+3A_ylim">ylim</code></td>
<td>
<p>y limits (default = c(0, max of either curve))</p>
</td></tr>
<tr><td><code id="cal_plot_+3A_xlab">xlab</code></td>
<td>
<p>a title for the x axis</p>
</td></tr>
<tr><td><code id="cal_plot_+3A_ylab">ylab</code></td>
<td>
<p>a title for the y axis</p>
</td></tr>
<tr><td><code id="cal_plot_+3A_app_col">app_col</code></td>
<td>
<p>color of the apparent calibration curve (default = 'black')</p>
</td></tr>
<tr><td><code id="cal_plot_+3A_bc_col">bc_col</code></td>
<td>
<p>color of the bias-corrected calibration curve (default = 'black')</p>
</td></tr>
<tr><td><code id="cal_plot_+3A_app_lty">app_lty</code></td>
<td>
<p>line type of the apparent calibration curve (default = 1)</p>
</td></tr>
<tr><td><code id="cal_plot_+3A_bc_lty">bc_lty</code></td>
<td>
<p>line type of the bias-corrected calibration curve (default = 2)</p>
</td></tr>
<tr><td><code id="cal_plot_+3A_plotci">plotci</code></td>
<td>
<p>plot confidence intervals ('yes') or not ('no'). If 'yes' x should have confidence intervals
added by <code><a href="#topic+confint.internal_validate">confint.internal_validate</a></code>. 'if' (default) plots CIs if they are
available.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plots apparent and bias-corrected curves. Silently returns a data.frame
that can be used to produce a more 'publication ready' plot. Columns are as
follows: predicted = values for the x-axis, apparent = value of apparent curve,
bias_corrected = value of bias-corrected curve. Confidence intervals are included
if available.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(pminternal)
set.seed(456)
# simulate data with two predictors that interact
dat &lt;- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3)
mean(dat$y)
dat$LP &lt;- NULL # remove linear predictor

# fit a (misspecified) logistic regression model
m1 &lt;- glm(y ~ x1 + x2, data=dat, family="binomial")

# to get a plot of bias-corrected calibration we need
# to specify 'eval' argument via 'calib_args'
# this argument specifies at what points to evalulate the
# calibration curve for plotting. The example below uses
# 100 equally spaced points between the min and max
# original prediction.

p &lt;- predict(m1, type="response")
p100 &lt;- seq(min(p), max(p), length.out=100)

m1_iv &lt;- validate(m1, method="cv_optimism", B=10,
                  calib_args = list(eval=p100))
# calib_ags can be used to set other calibration curve
# settings: see pmcalibration::pmcalibration

cal_plot(m1_iv)

</code></pre>

<hr>
<h2 id='calibration_stability'>Plot calibration stability across bootstrap replicates</h2><span id='topic+calibration_stability'></span>

<h3>Description</h3>

<p>A calibration (in)stability plot shows calibration curves for bootstrap
models evaluated on original outcome. A stable model should produce
boot calibration curves that differ minimally from the 'apparent' curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibration_stability(
  x,
  calib_args,
  xlim,
  ylim,
  xlab,
  ylab,
  col,
  subset,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calibration_stability_+3A_x">x</code></td>
<td>
<p>an object produced by <code><a href="#topic+validate">validate</a></code> with method = &quot;boot_*&quot; (or <code><a href="#topic+boot_optimism">boot_optimism</a></code> with method=&quot;boot&quot;)</p>
</td></tr>
<tr><td><code id="calibration_stability_+3A_calib_args">calib_args</code></td>
<td>
<p>settings for calibration curve (see <code>pmcalibration::pmcalibration</code>). If unspecified settings
are given by <code><a href="#topic+cal_defaults">cal_defaults</a></code> with 'eval' set to 100 (evaluate each curve at 100 points between min and max prediction).</p>
</td></tr>
<tr><td><code id="calibration_stability_+3A_xlim">xlim</code></td>
<td>
<p>x limits (default = c(0,1))</p>
</td></tr>
<tr><td><code id="calibration_stability_+3A_ylim">ylim</code></td>
<td>
<p>y limits (default = c(0,1))</p>
</td></tr>
<tr><td><code id="calibration_stability_+3A_xlab">xlab</code></td>
<td>
<p>a title for the x axis</p>
</td></tr>
<tr><td><code id="calibration_stability_+3A_ylab">ylab</code></td>
<td>
<p>a title for the y axis</p>
</td></tr>
<tr><td><code id="calibration_stability_+3A_col">col</code></td>
<td>
<p>color of lines for bootstrap models (default = grDevices::grey(.5, .3))</p>
</td></tr>
<tr><td><code id="calibration_stability_+3A_subset">subset</code></td>
<td>
<p>vector of observations to include (row indices). If dataset is large fitting B curves is demanding. This can be used to select a random subset of observations.</p>
</td></tr>
<tr><td><code id="calibration_stability_+3A_plot">plot</code></td>
<td>
<p>if FALSE just returns curves (see value)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plots calibration (in)stability.
Invisibly returns a list containing data for each curve (p=x-axis, pc=y-axis).
The first element of this list is the apparent curve (original model on original outcome).
</p>


<h3>References</h3>

<p>Riley, R. D., &amp; Collins, G. S. (2023). Stability of clinical prediction models developed using statistical or machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(456)
# simulate data with two predictors that interact
dat &lt;- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3)
mean(dat$y)
dat$LP &lt;- NULL # remove linear predictor

# fit a (misspecified) logistic regression model
m1 &lt;- glm(y ~ ., data=dat, family="binomial")

# internal validation of m1 via bootstrap optimism with 10 resamples
# B = 10 for example but should be &gt;= 200 in practice
m1_iv &lt;- validate(m1, method="boot_optimism", B=10)

calibration_stability(m1_iv)

</code></pre>

<hr>
<h2 id='classification_stability'>Classification instability plot</h2><span id='topic+classification_stability'></span>

<h3>Description</h3>

<p>Classification instability plot shows the relationship between original model estimated risk
and the classification instability index (CII). The CII is the proportion of bootstrap replicates
where the predicted class (0 if p &lt;= threshold; 1 if p &gt; threshold) is different to that
obtained from the original model. Those with risk predictions around the threshold will exhibit
elevated CII but an unstable model will exhibit high CII across a range of risk predictions.
See Riley and Collins (2023).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classification_stability(
  x,
  threshold,
  xlim,
  ylim,
  xlab,
  ylab,
  pch,
  cex,
  col,
  subset,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="classification_stability_+3A_x">x</code></td>
<td>
<p>an object produced by <code><a href="#topic+validate">validate</a></code> with method = &quot;boot_*&quot; (or <code><a href="#topic+boot_optimism">boot_optimism</a></code> with method=&quot;boot&quot;)</p>
</td></tr>
<tr><td><code id="classification_stability_+3A_threshold">threshold</code></td>
<td>
<p>estimated risks above the threshold get a predicted 'class' of 1, otherwise 0.</p>
</td></tr>
<tr><td><code id="classification_stability_+3A_xlim">xlim</code></td>
<td>
<p>x limits (default = range of estimated risks)</p>
</td></tr>
<tr><td><code id="classification_stability_+3A_ylim">ylim</code></td>
<td>
<p>y limits (default = c(0, maximum CII))</p>
</td></tr>
<tr><td><code id="classification_stability_+3A_xlab">xlab</code></td>
<td>
<p>a title for the x axis</p>
</td></tr>
<tr><td><code id="classification_stability_+3A_ylab">ylab</code></td>
<td>
<p>a title for the y axis</p>
</td></tr>
<tr><td><code id="classification_stability_+3A_pch">pch</code></td>
<td>
<p>plotting character (default = 16)</p>
</td></tr>
<tr><td><code id="classification_stability_+3A_cex">cex</code></td>
<td>
<p>controls point size (default = 1)</p>
</td></tr>
<tr><td><code id="classification_stability_+3A_col">col</code></td>
<td>
<p>color of points (default = grDevices::grey(.5, .5))</p>
</td></tr>
<tr><td><code id="classification_stability_+3A_subset">subset</code></td>
<td>
<p>vector of observations to include (row indices). This can be used to select a random subset of observations.</p>
</td></tr>
<tr><td><code id="classification_stability_+3A_plot">plot</code></td>
<td>
<p>if FALSE just returns CII values (see value)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plots classification (in)stability.
Invisibly returns estimates of CII for each observation.
</p>


<h3>References</h3>

<p>Riley, R. D., &amp; Collins, G. S. (2023). Stability of clinical prediction models developed using statistical or machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(456)
# simulate data with two predictors that interact
dat &lt;- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3)
mean(dat$y)
dat$LP &lt;- NULL # remove linear predictor

# fit a (misspecified) logistic regression model
m1 &lt;- glm(y ~ ., data=dat, family="binomial")

# internal validation of m1 via bootstrap optimism with 10 resamples
# B = 10 for example but should be &gt;= 200 in practice
m1_iv &lt;- validate(m1, method="boot_optimism", B=10)

classification_stability(m1_iv, threshold=.2)

</code></pre>

<hr>
<h2 id='confint.internal_validate'>Confidence intervals for bias-corrected performance measures</h2><span id='topic+confint.internal_validate'></span>

<h3>Description</h3>

<p>Implements the methods discussed in Noma et al. (2021), plus some others that have not been tested.
Specifically, Noma et al. discuss bootstrap optimism correction (&quot;boot_optimism&quot; and &quot;.632&quot;) and the percentile
bootstrap (<code>ci_type = "perc"</code>). Their paper contains some simulation results on coverage properties of these
CIs. If you used <code><a href="#topic+validate">validate</a></code> to do something other than bootstrap optimism correction or if you request
normal approximation CIs please note that these approaches have (to my knowledge) not been thoroughly tested.
<code>ci_type = "norm"</code> is included as it might be able to reduce the number of runs needed for &quot;twostage&quot; CIs.
See details for the difference between &quot;shifted&quot; and &quot;twostage&quot;. &quot;norm&quot; CIs are likely to perform poorly for some
performance measures, such as calibration Intercept and Slope, which for regular glms are always 0 and 1, respectively,
on assessment of apparent performance. As &quot;shifted&quot; CIs are based on apparent performance they will be meaningless for these measures.
Use the untested methods with caution!
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'internal_validate'
confint(
  object,
  parm,
  level = 0.95,
  method = c("shifted", "twostage"),
  ci_type = c("perc", "norm"),
  R = 1000,
  add = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="confint.internal_validate_+3A_object">object</code></td>
<td>
<p>created by call to <code><a href="#topic+validate">validate</a></code></p>
</td></tr>
<tr><td><code id="confint.internal_validate_+3A_parm">parm</code></td>
<td>
<p>a specification of which performance measures are
to be given confidence intervals, either a vector of numbers
or a vector of names. If missing, all scores are considered.</p>
</td></tr>
<tr><td><code id="confint.internal_validate_+3A_level">level</code></td>
<td>
<p>the confidence level required</p>
</td></tr>
<tr><td><code id="confint.internal_validate_+3A_method">method</code></td>
<td>
<p>&quot;shifted&quot; or &quot;twostage&quot; (see details)</p>
</td></tr>
<tr><td><code id="confint.internal_validate_+3A_ci_type">ci_type</code></td>
<td>
<p>percentile (&quot;perc&quot;) or normal approximation (&quot;norm&quot;) bootstrap CIs</p>
</td></tr>
<tr><td><code id="confint.internal_validate_+3A_r">R</code></td>
<td>
<p>number of replicates</p>
</td></tr>
<tr><td><code id="confint.internal_validate_+3A_add">add</code></td>
<td>
<p>return the object with an additional slot containing CIs (default) or
just return the CIs</p>
</td></tr>
<tr><td><code id="confint.internal_validate_+3A_...">...</code></td>
<td>
<p>additional arguments (currently ignored)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The two methods are as follows (see Noma et al. (2021) for more details):
</p>

<dl>
<dt>shifted</dt><dd><p> (default) This approach is based on shifting bootstrap CIs for apparent performance
by optimism. This makes it the faster option as only the calculation of apparent performance is needed for
each replicate. If the CI for apparent performance is [lower, upper], the resulting CI for bias-corrected performance
is [lower - optimism, upper - optimism]. Note this method is only available when using an optimism based approach
(and &quot;cv_optimism&quot; was untested in Noma et al).</p>
</dd>
<dt>twostage</dt><dd><p>This approach creates a bootstrap resample of the data and runs the entire
validation procedure on the resample (with the same number of 'inner' replicates, determined by B in the original
validate call). The CI is then constructed using the corrected estimates from the R 'outer' replicates.
As this involves R*B replicates, this could take a long time. Note <code><a href="#topic+validate">validate</a></code>
takes a <code>cores</code> argument that can allow the inner samples to run in parallel. </p>
</dd>
</dl>



<h3>Value</h3>

<p>A list with two elements, each a matrix with columns giving lower and upper confidence limits for each measure. One for apparent and one for bias-corrected measures.
Columns will be labelled as (1-level)/2 and 1 - (1-level)/2 in % (by default 2.5% and 97.5%).
</p>


<h3>References</h3>

<p>Noma, H., Shinozaki, T., Iba, K., Teramukai, S., &amp; Furukawa, T. A. (2021). Confidence intervals of prediction accuracy measures for multivariable prediction models based on the bootstrap‐based optimism correction methods. Statistics in Medicine, 40(26), 5691-5701.
</p>

<hr>
<h2 id='crossval'>Calculate bias-corrected scores via cross-validation</h2><span id='topic+crossval'></span>

<h3>Description</h3>

<p>Estimate bias-corrected scores via cross-validation. CV is used to calculate optimism
which is then subtracted from apparent scores and to calculate average performance in the
out of sample (held out) data.
This function is called by <code><a href="#topic+validate">validate</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crossval(data, outcome, model_fun, pred_fun, score_fun, k = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="crossval_+3A_data">data</code></td>
<td>
<p>the data used in developing the model. Should contain all variables considered (i.e., even those excluded by variable selection in the development sample)</p>
</td></tr>
<tr><td><code id="crossval_+3A_outcome">outcome</code></td>
<td>
<p>character denoting the column name of the outcome in <code>data</code>.</p>
</td></tr>
<tr><td><code id="crossval_+3A_model_fun">model_fun</code></td>
<td>
<p>a function that takes at least one argument, <code>data</code>. This function should implement the entire model development procedure (i.e., hyperparameter tuning, variable selection, imputation). Additional arguments can be provided via <code>...</code>. This function should return an object that works with <code>pred_fun</code>.</p>
</td></tr>
<tr><td><code id="crossval_+3A_pred_fun">pred_fun</code></td>
<td>
<p>function that takes at least two arguments, <code>model</code> and <code>data</code>. This function should return a numeric vector of predicted probabilities of the outcome with the same length as the number of rows in <code>data</code> so it is important to take into account how missing data is treated (e.g., <code>predict.glm</code> omits predictions for rows with missing values).</p>
</td></tr>
<tr><td><code id="crossval_+3A_score_fun">score_fun</code></td>
<td>
<p>a function to calculate the metrics of interest. If this is not specified <code><a href="#topic+score_binary">score_binary</a></code> is used.</p>
</td></tr>
<tr><td><code id="crossval_+3A_k">k</code></td>
<td>
<p>number of folds. Typically scores need greater than 2 observations to be calculated so folds should be chosen with this in mind.</p>
</td></tr>
<tr><td><code id="crossval_+3A_...">...</code></td>
<td>
<p>additional arguments for <code>model_fun</code>, <code>pred_fun</code>, and/or <code>score_fun</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of class <code>internal_cv</code> containing:
</p>

<ul>
<li><p><code>apparent</code> - scores calculated on the original data using the original model.
</p>
</li>
<li><p><code>optimism</code> - estimates of optimism for each score (average difference in score for training data vs test data on each fold) which can be subtracted from 'apparent' performance calculated using the original model on the original data.
</p>
</li>
<li><p><code>cv_optimism_corrected</code> - 'bias corrected' scores (apparent - optimism). This is what is produced by <code>rms::validate</code>, <code>rms::predab.resample</code>.
</p>
</li>
<li><p><code>cv_average</code> - average of scores calculated on the test (held out) data. This is the metric described in Steyerberg et al. (2001).
</p>
</li>
<li><p><code>indices</code> - indices used to define test set on each fold.
</p>
</li></ul>



<h3>References</h3>

<p>Steyerberg, E. W., Harrell Jr, F. E., Borsboom, G. J., Eijkemans, M. J. C., Vergouwe, Y., &amp; Habbema, J. D. F. (2001). Internal validation of predictive models: efficiency of some procedures for logistic regression analysis. Journal of clinical epidemiology, 54(8), 774-781.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(pminternal)
set.seed(456)
# simulate data with two predictors that interact
dat &lt;- pmcalibration::sim_dat(N = 1000, a1 = -2, a3 = -.3)
mean(dat$y)
dat$LP &lt;- NULL # remove linear predictor

# fit a (misspecified) logistic regression model
#m1 &lt;- glm(y ~ x1 + x2, data=dat, family="binomial")

model_fun &lt;- function(data, ...){
  glm(y ~ x1 + x2, data=data, family="binomial")
}

pred_fun &lt;- function(model, data, ...){
  predict(model, newdata=data, type="response")
}

# CV Corrected = Apparent - CV Optimism
# CV Average = average score in held out fold
crossval(data=dat, outcome="y", model_fun=model_fun, pred_fun=pred_fun, k=10)

</code></pre>

<hr>
<h2 id='dcurve_stability'>Plot decision curve stability across bootstrap replicates</h2><span id='topic+dcurve_stability'></span>

<h3>Description</h3>

<p>A decision curve (in)stability plot shows decision curves for bootstrap
models evaluated on original outcome. A stable model should produce
curves that differ minimally from the 'apparent' curve.
See Riley and Collins (2023).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcurve_stability(
  x,
  thresholds = seq(0, 0.99, by = 0.01),
  xlim,
  ylim,
  xlab,
  ylab,
  col,
  subset,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dcurve_stability_+3A_x">x</code></td>
<td>
<p>an object produced by <code><a href="#topic+validate">validate</a></code> with method = &quot;boot_*&quot; (or <code><a href="#topic+boot_optimism">boot_optimism</a></code> with method=&quot;boot&quot;)</p>
</td></tr>
<tr><td><code id="dcurve_stability_+3A_thresholds">thresholds</code></td>
<td>
<p>points at which to evaluate the decision curves (see <code>dcurves::dca</code>)</p>
</td></tr>
<tr><td><code id="dcurve_stability_+3A_xlim">xlim</code></td>
<td>
<p>x limits (default = range of thresholds)</p>
</td></tr>
<tr><td><code id="dcurve_stability_+3A_ylim">ylim</code></td>
<td>
<p>y limits (default = range of net benefit)</p>
</td></tr>
<tr><td><code id="dcurve_stability_+3A_xlab">xlab</code></td>
<td>
<p>a title for the x axis</p>
</td></tr>
<tr><td><code id="dcurve_stability_+3A_ylab">ylab</code></td>
<td>
<p>a title for the y axis</p>
</td></tr>
<tr><td><code id="dcurve_stability_+3A_col">col</code></td>
<td>
<p>color of points (default = grDevices::grey(.5, .5))</p>
</td></tr>
<tr><td><code id="dcurve_stability_+3A_subset">subset</code></td>
<td>
<p>vector of observations to include (row indices). This can be used to select a random subset of observations.</p>
</td></tr>
<tr><td><code id="dcurve_stability_+3A_plot">plot</code></td>
<td>
<p>if FALSE just returns curves (see value)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plots decision curve (in)stability.
Invisibly returns a list containing data for each curve. These are returned from <code>dcurves::dca</code>.
The first element of this list is the apparent curve (original model on original outcome).
</p>


<h3>References</h3>

<p>Riley, R. D., &amp; Collins, G. S. (2023). Stability of clinical prediction models developed using statistical or machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(456)
# simulate data with two predictors that interact
dat &lt;- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3)
mean(dat$y)
dat$LP &lt;- NULL # remove linear predictor

# fit a (misspecified) logistic regression model
m1 &lt;- glm(y ~ ., data=dat, family="binomial")

# internal validation of m1 via bootstrap optimism with 10 resamples
# B = 10 for example but should be &gt;= 200 in practice
m1_iv &lt;- validate(m1, method="boot_optimism", B=10)

dcurve_stability(m1_iv)

</code></pre>

<hr>
<h2 id='get_stability'>Get stability from internal_validate or internal_boot object</h2><span id='topic+get_stability'></span>

<h3>Description</h3>

<p>Get stability from internal_validate or internal_boot object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_stability(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_stability_+3A_x">x</code></td>
<td>
<p>a internal_validate (see <code><a href="#topic+validate">validate</a></code>) or internal_boot
(see <code><a href="#topic+boot_optimism">boot_optimism</a></code>) object. The former should have been
created with method = &quot;boot_optimism&quot; or &quot;boot_simple&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the stability matrix (an n x B+1 matrix.
Each column contains predictions for p observations. First column contains
predictions from original model. Other columns are from bootstrap models) and
the original binary outcome (y). Unsuccessful resamples are omitted.
</p>

<hr>
<h2 id='mape_stability'>Mean absolute predictor error (MAPE) stability plot</h2><span id='topic+mape_stability'></span>

<h3>Description</h3>

<p>A MAPE (in)stability plot shows mean absolute predictor error (average absolute difference between original
estimated risk and risk from B bootstrap models) as a function of apparent
estimated risk (prediction from original/development model). See Riley and Collins (2023).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mape_stability(x, xlim, ylim, xlab, ylab, pch, cex, col, subset, plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mape_stability_+3A_x">x</code></td>
<td>
<p>an object produced by <code><a href="#topic+validate">validate</a></code> with method = &quot;boot_*&quot; (or <code><a href="#topic+boot_optimism">boot_optimism</a></code> with method=&quot;boot&quot;)</p>
</td></tr>
<tr><td><code id="mape_stability_+3A_xlim">xlim</code></td>
<td>
<p>x limits (default = range of estimated risks)</p>
</td></tr>
<tr><td><code id="mape_stability_+3A_ylim">ylim</code></td>
<td>
<p>y limits (default = c(0, maximum mape))</p>
</td></tr>
<tr><td><code id="mape_stability_+3A_xlab">xlab</code></td>
<td>
<p>a title for the x axis</p>
</td></tr>
<tr><td><code id="mape_stability_+3A_ylab">ylab</code></td>
<td>
<p>a title for the y axis</p>
</td></tr>
<tr><td><code id="mape_stability_+3A_pch">pch</code></td>
<td>
<p>plotting character (default = 16)</p>
</td></tr>
<tr><td><code id="mape_stability_+3A_cex">cex</code></td>
<td>
<p>controls point size (default = 1)</p>
</td></tr>
<tr><td><code id="mape_stability_+3A_col">col</code></td>
<td>
<p>color of points (default = grDevices::grey(.5, .5))</p>
</td></tr>
<tr><td><code id="mape_stability_+3A_subset">subset</code></td>
<td>
<p>vector of observations to include (row indices). This can be used to select a random subset of observations.</p>
</td></tr>
<tr><td><code id="mape_stability_+3A_plot">plot</code></td>
<td>
<p>if FALSE just returns MAPE values (see value)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plots calibration (in)stability.
Invisibly returns a list containing individual and average MAPE.
</p>


<h3>References</h3>

<p>Riley, R. D., &amp; Collins, G. S. (2023). Stability of clinical prediction models developed using statistical or machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(456)
# simulate data with two predictors that interact
dat &lt;- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3)
mean(dat$y)
dat$LP &lt;- NULL # remove linear predictor

# fit a (misspecified) logistic regression model
m1 &lt;- glm(y ~ ., data=dat, family="binomial")

# internal validation of m1 via bootstrap optimism with 10 resamples
# B = 10 for example but should be &gt;= 200 in practice
m1_iv &lt;- validate(m1, method="boot_optimism", B=10)

mape_stability(m1_iv)

</code></pre>

<hr>
<h2 id='prediction_stability'>Plot prediction stability across bootstrap replicates</h2><span id='topic+prediction_stability'></span>

<h3>Description</h3>

<p>A prediction (in)stability plot shows estimated risk probabilities from
models developed on resampled data evaluated on the original development data
as a function of the 'apparent' prediction (prediction from original/development
model evaluated on original data). A stable model should produce points that
exhibit minimal dispersion. See Riley and Collins (2023).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prediction_stability(
  x,
  bounds = 0.95,
  smooth_bounds = FALSE,
  xlab,
  ylab,
  pch,
  cex,
  col,
  lty,
  span,
  subset,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prediction_stability_+3A_x">x</code></td>
<td>
<p>an object produced by <code><a href="#topic+validate">validate</a></code> with method = &quot;boot_*&quot; (or <code><a href="#topic+boot_optimism">boot_optimism</a></code> with method=&quot;boot&quot;)</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_bounds">bounds</code></td>
<td>
<p>width of the 'stability interval' (percentiles of the bootstrap model predictions). NULL = do not add bounds to plot.</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_smooth_bounds">smooth_bounds</code></td>
<td>
<p>if TRUE, use <code>loess</code> to smooth the bounds (default = FALSE)</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_xlab">xlab</code></td>
<td>
<p>a title for the x axis</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_ylab">ylab</code></td>
<td>
<p>a title for the y axis</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_pch">pch</code></td>
<td>
<p>plotting character (default = 16)</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_cex">cex</code></td>
<td>
<p>controls point size (default = 0.4)</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_col">col</code></td>
<td>
<p>color of points (default = grDevices::grey(.5, .5))</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_lty">lty</code></td>
<td>
<p>line type for bounds (default = 2)</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_span">span</code></td>
<td>
<p>controls the degree of smoothing (see <code>loess</code>; default = 0.75)</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_subset">subset</code></td>
<td>
<p>vector of observations to include (row indices). If dataset is large plotting N points for B bootstrap resamples is demanding. This can be used to select a random subset of observations.</p>
</td></tr>
<tr><td><code id="prediction_stability_+3A_plot">plot</code></td>
<td>
<p>if FALSE just returns stability matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plots prediction (in)stability. The stability bounds are not smoothed.
Invisibly returns stability matrix (where column 1 are original predictions)
that can be used for creating plots with other packages/software.
</p>


<h3>References</h3>

<p>Riley, R. D., &amp; Collins, G. S. (2023). Stability of clinical prediction models developed using statistical or machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(456)
# simulate data with two predictors that interact
dat &lt;- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3)
mean(dat$y)
dat$LP &lt;- NULL # remove linear predictor

# fit a (misspecified) logistic regression model
m1 &lt;- glm(y ~ ., data=dat, family="binomial")

# internal validation of m1 via bootstrap optimism with 10 resamples
# B = 10 for example but should be &gt;= 200 in practice
m1_iv &lt;- validate(m1, method="boot_optimism", B=10)

prediction_stability(m1_iv)

</code></pre>

<hr>
<h2 id='print.internal_boot'>Print a internal_boot object</h2><span id='topic+print.internal_boot'></span>

<h3>Description</h3>

<p>Print a internal_boot object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'internal_boot'
print(x, digits = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.internal_boot_+3A_x">x</code></td>
<td>
<p>an object created with <code>boot_optimism</code></p>
</td></tr>
<tr><td><code id="print.internal_boot_+3A_digits">digits</code></td>
<td>
<p>number of digits to print (default = 2)</p>
</td></tr>
<tr><td><code id="print.internal_boot_+3A_...">...</code></td>
<td>
<p>additional arguments to print</p>
</td></tr>
</table>


<h3>Value</h3>

<p>invisibly returns <code>x</code> and prints estimates to console
</p>

<hr>
<h2 id='print.internal_cv'>Print a internal_cv object</h2><span id='topic+print.internal_cv'></span>

<h3>Description</h3>

<p>Print a internal_cv object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'internal_cv'
print(x, digits = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.internal_cv_+3A_x">x</code></td>
<td>
<p>an object created with <code>crossval</code></p>
</td></tr>
<tr><td><code id="print.internal_cv_+3A_digits">digits</code></td>
<td>
<p>number of digits to print (default = 2)</p>
</td></tr>
<tr><td><code id="print.internal_cv_+3A_...">...</code></td>
<td>
<p>additional arguments to print</p>
</td></tr>
</table>


<h3>Value</h3>

<p>invisibly returns <code>x</code> and prints estimates to console
</p>

<hr>
<h2 id='print.internal_validate'>print a internal_validate object</h2><span id='topic+print.internal_validate'></span>

<h3>Description</h3>

<p>print a internal_validate object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'internal_validate'
print(x, digits = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.internal_validate_+3A_x">x</code></td>
<td>
<p>a <code>internal_validate</code> object</p>
</td></tr>
<tr><td><code id="print.internal_validate_+3A_digits">digits</code></td>
<td>
<p>number of digits to print</p>
</td></tr>
<tr><td><code id="print.internal_validate_+3A_...">...</code></td>
<td>
<p>optional arguments passed to print</p>
</td></tr>
</table>


<h3>Value</h3>

<p>prints a summary
</p>

<hr>
<h2 id='print.internal_validatesummary'>Print summary of internal_validate object</h2><span id='topic+print.internal_validatesummary'></span>

<h3>Description</h3>

<p>Print summary of internal_validate object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'internal_validatesummary'
print(x, digits = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.internal_validatesummary_+3A_x">x</code></td>
<td>
<p>a <code>internal_validatesummary</code> object</p>
</td></tr>
<tr><td><code id="print.internal_validatesummary_+3A_digits">digits</code></td>
<td>
<p>number of digits to print</p>
</td></tr>
<tr><td><code id="print.internal_validatesummary_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>invisible(x) - prints a summary
</p>

<hr>
<h2 id='score_binary'>Score predictions for binary events</h2><span id='topic+score_binary'></span>

<h3>Description</h3>

<p>Calculate scores summarizing discrimination/calibration of predictions
against observed binary events. If score_fun is not defined when calling
<code><a href="#topic+validate">validate</a></code> this function is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score_binary(y, p, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="score_binary_+3A_y">y</code></td>
<td>
<p>vector containing a binary outcome</p>
</td></tr>
<tr><td><code id="score_binary_+3A_p">p</code></td>
<td>
<p>vector of predictions</p>
</td></tr>
<tr><td><code id="score_binary_+3A_...">...</code></td>
<td>
<p>additional arguments. This function only supports calib_args as
an optional argument. calib_args should contain arguments for pmcalibration::pmcalibration.
If a calibration plot (apparent vs bias corrected calibration curves via <code><a href="#topic+cal_plot">cal_plot</a></code>)
is desired the argument 'eval' should be provided. This should be the points at which to evaluate
the calibration curve on each boot resample or crossvalidation fold. A good option would be
calib_args = list(eval = seq(min(p), max(p), length.out=100)); where p are predictions from the
original model evaluated on the original data. Dots can be used to supply additional arguments to
user-defined functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following measures are returned in a named vector.
</p>

<dl>
<dt>C</dt><dd><p>the c-statistic (aka area under the ROC curve). Probability that randomly selected
observation with y = 1 with have higher p compared to randomly selected y = 0.</p>
</dd>
<dt>Brier</dt><dd><p>mean squared error - mean((y - p)^2)</p>
</dd>
<dt>Intercept</dt><dd><p>Intercept from a logistic calibration model: glm(y ~ 1 + offset(qlogis(p)), family=&quot;binomial&quot;)</p>
</dd>
<dt>Slope</dt><dd><p>Slope from a logistic calibration model: glm(y ~ 1 + qlogis(p), family=&quot;binomial&quot;)</p>
</dd>
<dt>Eavg</dt><dd><p>average absolute difference between p and calibration curve
(aka integrated calibration index or ICI).</p>
</dd>
<dt>E50</dt><dd><p>median absolute difference between p and calibration curve</p>
</dd>
<dt>E90</dt><dd><p>90th percentile absolute difference between p and calibration curve</p>
</dd>
<dt>Emax</dt><dd><p>maximum absolute difference between p and calibration curve</p>
</dd>
<dt>ECI</dt><dd><p>average squared difference between p and calibration curve. Estimated
calibration index (Van Hoorde et al. 2015)</p>
</dd>
<dt>cal_plot</dt><dd><p>if eval is specified (via calib_args), values for
plotting apparent and bias-corrected calibration curves are returned (see <code><a href="#topic+cal_plot">cal_plot</a></code>).
By default these are omitted from the summary printed (see <code><a href="#topic+summary.internal_validate">summary.internal_validate</a></code>).</p>
</dd>
</dl>

<p>Logistic calibration and other calibration metrics from non-linear calibration curves
assessing 'moderate-calibration' (Eavg, E50, E90, Emax, ECI; see references) are calculated
via the <code>pmcalibration</code> package. The default settings can be modified by passing
calib_args to <code><a href="#topic+validate">validate</a></code> call. calib_args should be a named list corresponding to
arguments to <code>pmcalibration::pmcalibration</code>.
</p>


<h3>Value</h3>

<p>a named vector of scores (see Details)
</p>


<h3>References</h3>

<p>Austin PC, Steyerberg EW. (2019) The Integrated Calibration Index (ICI) and related metrics for quantifying the calibration of logistic regression models. <em>Statistics in Medicine</em>. 38, pp. 1–15. https://doi.org/10.1002/sim.8281
</p>
<p>Van Hoorde, K., Van Huffel, S., Timmerman, D., Bourne, T., Van Calster, B. (2015). A spline-based tool to assess and visualize the calibration of multiclass risk predictions. <em>Journal of Biomedical Informatics</em>, 54, pp. 283-93
</p>
<p>Van Calster, B., Nieboer, D., Vergouwe, Y., De Cock, B., Pencina M., Steyerberg E.W. (2016). A calibration hierarchy for risk models was defined: from utopia to empirical data. <em>Journal of Clinical Epidemiology</em>, 74, pp. 167-176
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p &lt;- runif(100)
y &lt;- rbinom(length(p), 1, p)
score_binary(y = y, p = p)
</code></pre>

<hr>
<h2 id='summary.internal_validate'>Summarize a internal_validate object</h2><span id='topic+summary.internal_validate'></span>

<h3>Description</h3>

<p>Summarize a internal_validate object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'internal_validate'
summary(object, ignore_scores = "^cal_plot", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.internal_validate_+3A_object">object</code></td>
<td>
<p>created by call to <code><a href="#topic+validate">validate</a></code></p>
</td></tr>
<tr><td><code id="summary.internal_validate_+3A_ignore_scores">ignore_scores</code></td>
<td>
<p>a string used to identify scores to omit from summary.
<code><a href="#topic+score_binary">score_binary</a></code> produces scores with prefix 'cal_plot' when a calibration plot
is desired (see <code><a href="#topic+cal_plot">cal_plot</a></code>) and these are ignored by default.</p>
</td></tr>
<tr><td><code id="summary.internal_validate_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with 4 columns (apparent score, optimism, bias-corrected score, number of successful resamples/folds)
and one row per score. Not all methods produce an optimism estimate so this row may be all NA. If confidence intervals
have been added for all measures via <code><a href="#topic+confint.internal_validate">confint.internal_validate</a></code>, two additional columns containing lower and upper bounds for
bias-corrected performance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(pminternal)
set.seed(456)
# simulate data with two predictors that interact
dat &lt;- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3)
mean(dat$y)
dat$LP &lt;- NULL # remove linear predictor

# fit a (misspecified) logistic regression model
m1 &lt;- glm(y ~ ., data=dat, family="binomial")

# internal validation of m1 via bootstrap optimism with 10 resamples
# B = 10 for example but should be &gt;= 200 in practice
m1_iv &lt;- validate(m1, method="boot_optimism", B=10)
summary(m1_iv)

</code></pre>

<hr>
<h2 id='validate'>Get bias-corrected performance measures via bootstrapping or cross-validation</h2><span id='topic+validate'></span>

<h3>Description</h3>

<p>Performs internal validation of a prediction model development procedure via bootstrapping
or cross-validation. Many model types are supported via the <code>insight</code> and <code>marginaleffects</code>
packages or users can supply user-defined functions that implement the model development
procedure and retrieve predictions. Bias-corrected scores and estimates of optimism (where applicable)
are provided. See <code><a href="#topic+confint.internal_validate">confint.internal_validate</a></code> for calculation of confidence intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate(
  fit,
  method = c("boot_optimism", "boot_simple", ".632", "cv_optimism", "cv_average", "none"),
  data,
  outcome,
  model_fun,
  pred_fun,
  score_fun,
  B,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_+3A_fit">fit</code></td>
<td>
<p>a model object. If fit is given the <code>insight</code> package is
used to extract data, outcome, and original model call. Therefore, it is important
that fit be supported by <code>insight</code> and implements the entire model development
process (see Harrell 2015). A fit given after selection of variables by some method will
not give accurate bias-correction. Model predictions are obtained via
<code>marginaleffects::get_predict</code> with type = &quot;response&quot; so fit should be
compatible with this function. If fit is provided the arguments data, outcome,
model_fun, and pred_fun are all ignored.</p>
</td></tr>
<tr><td><code id="validate_+3A_method">method</code></td>
<td>
<p>bias-correction method. Valid options are &quot;boot_optimism&quot;, &quot;boot_simple&quot;,
&quot;.632&quot;, &quot;cv_optimism&quot;, &quot;cv_average&quot;, or &quot;none&quot; (return apparent performance). See details.</p>
</td></tr>
<tr><td><code id="validate_+3A_data">data</code></td>
<td>
<p>a data.frame containing data used to fit development model</p>
</td></tr>
<tr><td><code id="validate_+3A_outcome">outcome</code></td>
<td>
<p>character denoting the column name of the outcome in data</p>
</td></tr>
<tr><td><code id="validate_+3A_model_fun">model_fun</code></td>
<td>
<p>for models that cannot be supplied via fit this should be a function
that takes one named argument: 'data' (function should include ... among arguments).
This function should implement the entire model development
procedure (hyperparameter tuning, variable selection, imputation etc) and return an object
that can be used by pred_fun. Additional arguments can be supplied by ...</p>
</td></tr>
<tr><td><code id="validate_+3A_pred_fun">pred_fun</code></td>
<td>
<p>for models that cannot be supplied via fit this should be a function
that takes two named arguments: 'model' and 'data' (function should include ... among arguments).
'model' is an object returned by model_fun.
The function should return a vector of predicted risk probabilities of the same length as the number
of rows in data. Additional arguments can be supplied by ...</p>
</td></tr>
<tr><td><code id="validate_+3A_score_fun">score_fun</code></td>
<td>
<p>function used to produce performance measures from predicted risks
and observed binary outcome. Should take two named arguments: 'y' and 'p' (function should include ... among arguments).
This function should return a named vector of scores. If unspecified <code><a href="#topic+score_binary">score_binary</a></code>
is used and this should be good for most purposes.</p>
</td></tr>
<tr><td><code id="validate_+3A_b">B</code></td>
<td>
<p>number of bootstrap replicates or crossvalidation folds. If unspecified B is set to
200 for method = &quot;boot_*&quot;/&quot;.632&quot;, or is set to 10 for method = &quot;cv_*&quot;.</p>
</td></tr>
<tr><td><code id="validate_+3A_...">...</code></td>
<td>
<p>additional arguments for user-defined functions. Arguments for
producing calibration curves can be set via 'calib_args' which should be
a named list (see <code><a href="#topic+cal_plot">cal_plot</a></code> and <code><a href="#topic+score_binary">score_binary</a></code>).
For method = &quot;boot_optimism&quot;, &quot;boot_simple&quot;, or &quot;.632&quot; users can specify a
<code>cores</code> argument (e.g., <code>cores = 4</code>) to run bootstrap samples in parallel.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internal validation can provide bias-corrected estimates of performance (e.g., C-statistic/AUC, calibration intercept/slope)
for a model development procedure (i.e., expected performance if the same procedure were applied
to another sample of the same size from the same population; see references). There are several approaches to producing
bias-corrected estimates (see below). It is important that the <code>fit</code> or <code>model_fun</code> provided implement
the entire model development procedure, including any hyperparameter tuning and/or variable selection.
</p>
<p>Note that <code><a href="#topic+validate">validate</a></code> does very little to check for missing values in predictors/features. If <code>fit</code> is
supplied <code>insight::get_data</code> will extract the data used to fit the model and usually
this will result in complete cases being used. User-defined model and predict functions can
be specified to handle missing values among predictor variables. Currently any user supplied data will
have rows with missing outcome values removed.
</p>
<p><b>method</b>
Different options for the method argument are described below:
</p>

<dl>
<dt>boot_optimism</dt><dd><p> (default) estimates optimism for each score and subtracts from apparent score (score calculated
with the original/development model evaluated on the original sample). A new model is fit using the same procedure
using each bootstrap resample. Scores are calculated when applying the boot model to the boot sample (<code class="reqn">S_{boot}</code>)
and the original sample (<code class="reqn">S_{orig}</code>) and the difference gives an estimate of optimism for a given resample (<code class="reqn">S_{boot} - S_{orig}</code>).
The average optimism across the B resamples is subtracted from the apparent score to produce the bias corrected score.</p>
</dd>
<dt>boot_simple</dt><dd><p>implements the simple bootstrap. B bootstrap models are fit and evaluated on the original data.
The average score across the B replicates is the bias-corrected score.</p>
</dd>
<dt>.632</dt><dd><p>implements Harrell's adaption of Efron's .632 estimator for binary outcomes
(see rms::predab.resample and rms::validate). In this case the estimate of optimism is
<code class="reqn">0.632 \times (S_{app} - mean(S_{omit} \times w))</code> where <code class="reqn">S_{app}</code> is the apparent performance
score and <code class="reqn">S_{omit}</code> is the score estimated using the bootstrap model evaluated on the out-of-sample
observations and <code class="reqn">w</code> weights for the proportion of observations omitted (see Harrell 2015, p. 115).</p>
</dd>
<dt>cv_optimism</dt><dd><p>estimate optimism via B-fold crossvalidation. Optimism is the average of the difference
in performance measure between predictions made on the training vs test (held out fold) data. This is the approach
implemented in <code>rms::validate</code> with method=&quot;crossvalidation&quot;.</p>
</dd>
<dt>cv_average</dt><dd><p>bias corrected scores are the average of scores calculated by assessing the model developed on each
fold evaluated on the test/held out data. This approach is described and compared to &quot;boot_optimism&quot; and &quot;.632&quot; in
Steyerberg et al. (2001).</p>
</dd></dl>

<p><b>Calibration curves</b>
To make calibration curves and calculate the associated estimates (ICI, ECI, etc - see <code><a href="#topic+score_binary">score_binary</a></code>)
<code>validate</code> uses the default arguments in <code><a href="#topic+cal_defaults">cal_defaults</a></code>. These arguments are passed to the <code>pmcalibration</code> package
(see <code>?pmcalibration::pmcalibration</code> for options).
</p>
<p>If a calibration plot (apparent vs bias corrected calibration curves via <code><a href="#topic+cal_plot">cal_plot</a></code>)
is desired, the argument 'eval' should be provided. This should be the points at which to evaluate
the calibration curve on each boot resample or crossvalidation fold. A good option would be
<code>calib_args = list(eval = seq(min(p), max(p), length.out=100))</code>; where p are predictions from the
original model evaluated on the original data.
</p>
<p><b>Number of resamples/folds is less than requested</b>
If the <code>model_fun</code> produces an error or if <code>score_binary</code> is supplied with constant predictions
or outcomes (e.g. all(y == 0)) the returned scores will all be NA. These will be omitted from the calculation
of optimism or other bias-corrected estimates (cv_average, boot_simple) and the number of successful resamples/folds
will be &lt; B. <code>validate</code> collects error messages and will produce a warning summarizing them. The number of successful
samples is given in the 'n' column in the printed summary of an 'internal_validate' object.
</p>
<p>It is important to understand what is causing the loss of resamples/folds. Some potential sources (which will need to be added to) are that
for rare events the resamples/folds may be resulting in samples that have zero outcomes. For 'cv_*' this will especially
be the case if B (n folds) is set high. There may be problems with factor/binary predictor variables with rare levels, which could be dealt with
by specifying a <code>model_fun</code> that omits variables for the model formula if only one level is present. The issue may be related to the construction
of calibration curves and may be addressed by more carefully selecting settings (see section above).
</p>


<h3>Value</h3>

<p>an object of class internal_validate containing apparent and bias-corrected
estimates of performance scores. If method = &quot;boot_*&quot; it also contains results pertaining
to stability of predictions across bootstrapped models (see Riley and Collins, 2023).
</p>


<h3>References</h3>

<p>Steyerberg, E. W., Harrell Jr, F. E., Borsboom, G. J., Eijkemans, M. J. C., Vergouwe, Y., &amp; Habbema, J. D. F. (2001). Internal validation of predictive models: efficiency of some procedures for logistic regression analysis. Journal of clinical epidemiology, 54(8), 774-781.
</p>
<p>Harrell Jr F. E. (2015). Regression Modeling Strategies: with applications to linear models, logistic and ordinal regression, and survival analysis. New York: Springer Science, LLC.
</p>
<p>Efron (1983). “Estimating the error rate of a prediction rule: improvement on cross-validation”. Journal of the American Statistical Association, 78(382):316-331
</p>
<p>Van Calster, B., Steyerberg, E. W., Wynants, L., and van Smeden, M. (2023). There is no such thing as a validated prediction model. BMC medicine, 21(1), 70.
</p>
<p>Riley, R. D., &amp; Collins, G. S. (2023). Stability of clinical prediction models developed using statistical or machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(pminternal)
set.seed(456)
# simulate data with two predictors that interact
dat &lt;- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3)
mean(dat$y)
dat$LP &lt;- NULL # remove linear predictor

# fit a (misspecified) logistic regression model
m1 &lt;- glm(y ~ ., data=dat, family="binomial")

# internal validation of m1 via bootstrap optimism with 10 resamples
# B = 10 for example but should be &gt;= 200 in practice
m1_iv &lt;- validate(m1, method="boot_optimism", B=10)
m1_iv

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
