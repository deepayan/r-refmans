<!DOCTYPE html><html lang="en"><head><title>Help for package dfoptim</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dfoptim}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dfoptim'>
<p>Derivative-Free Optimization</p></a></li>
<li><a href='#hjk'>
<p>Hooke-Jeeves derivative-free minimization algorithm</p></a></li>
<li><a href='#mads'>
<p>Mesh Adaptive Direct Searches (MADS) algorithm for derivative-free and black-box optimization</p></a></li>
<li><a href='#nmk'>
<p>Nelder-Mead optimziation algorithm for derivative-free optimization</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Derivative-Free Optimization</td>
</tr>
<tr>
<td>Description:</td>
<td>Derivative-Free optimization algorithms. These algorithms do not require gradient information. More importantly, they can be used to solve non-smooth optimization problems.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.1)</td>
</tr>
<tr>
<td>Version:</td>
<td>2023.1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-21</td>
</tr>
<tr>
<td>Author:</td>
<td>Ravi Varadhan[aut, cre], Johns Hopkins University, Hans W. Borchers[aut],
        ABB Corporate Research, and Vincent Bechard[aut], HEC Montreal (Montreal University)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ravi Varadhan &lt;ravi.varadhan@jhu.edu&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-22 21:47:10 UTC; rvaradh1</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-23 17:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='dfoptim'>
Derivative-Free Optimization
</h2><span id='topic+dfoptim-package'></span>

<h3>Description</h3>

<p>Derivative-Free optimization algorithms. These algorithms do not require gradient information.  
More importantly, they can be used to solve non-smooth optimization problems. They can also handle box constraints on parameters.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> dfoptim</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 2023.1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-08-21</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2 or greater</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Derivative-Free optimization algorithms. These algorithms do not require gradient information.  
More importantly, they can be used to solve non-smooth optimization problems. 
These algorithms were translated from the Matlab code of Prof. C.T. Kelley, given in his book &quot;Iterative methods for optimization&quot;.
However, there are some non-trivial modifications of the algorithm. <br />
</p>
<p>Currently, the Nelder-Mead and Hooke-Jeeves algorithms is implemented.  In future, more derivative-free algorithms may be added.
</p>


<h3>Author(s)</h3>

<p>Ravi Varadhan, Johns Hopkins University   <br />
URL:  http://www.jhsph.edu/agingandhealth/People/Faculty_personal_pages/Varadhan.html  <br />
Hans W. Borchers, ABB Corporate Research <br />
Maintainer:  Ravi Varadhan &lt;ravi.varadhan@jhu.edu&gt;
</p>


<h3>References</h3>

<p>C.T. Kelley (1999), Iterative Methods for Optimization, SIAM.
</p>

<hr>
<h2 id='hjk'>
Hooke-Jeeves derivative-free minimization algorithm
</h2><span id='topic+hjk'></span><span id='topic+hjkb'></span>

<h3>Description</h3>

<p>An implementation of the Hooke-Jeeves algorithm for derivative-free
optimization.  A bounded and an unbounded version are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hjk(par, fn, control = list(), ...)

hjkb(par, fn, lower = -Inf, upper = Inf, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hjk_+3A_par">par</code></td>
<td>
<p>Starting vector of parameter values.    The initial vector may lie on the boundary. If <code>lower[i]=upper[i]</code>
for some <code>i</code>, the <code>i</code>-th component of the solution vector will
simply be kept fixed.</p>
</td></tr>
<tr><td><code id="hjk_+3A_fn">fn</code></td>
<td>
<p>Nonlinear objective function that is to be optimized. 
A scalar function that takes a real vector as argument and 
returns a scalar that is the value of the function at that point.</p>
</td></tr>
<tr><td><code id="hjk_+3A_lower">lower</code>, <code id="hjk_+3A_upper">upper</code></td>
<td>
<p>Lower and upper bounds on the parameters.
A vector of the same length as the parameters.
If a single value is specified, it is assumed that the 
same bound applies to all parameters. The
starting parameter values must lie within the bounds.</p>
</td></tr>
<tr><td><code id="hjk_+3A_control">control</code></td>
<td>
<p>A list of control parameters.
See <b>Details</b> for more information.</p>
</td></tr>
<tr><td><code id="hjk_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Argument <code>control</code> is a list specifing changes to default values of
algorithm control parameters.
Note that parameter names may be abbreviated as long as they are unique.
</p>
<p>The list items are as follows:
</p>

<dl>
<dt><code>tol</code></dt><dd><p>Convergence tolerance. Iteration is terminated when the 
step length of the main loop becomes smaller than <code>tol</code>. This does
<em>not</em> imply that the optimum is found with the same accuracy.
Default is 1.e-06.</p>
</dd>
<dt><code>maxfeval</code></dt><dd><p>Maximum number of objective function evaluations 
allowed. Default is Inf, that is no restriction at all.</p>
</dd>
<dt><code>maximize</code></dt><dd><p>A logical indicating whether the objective function 
is to be maximized (TRUE) or minimized (FALSE). Default is FALSE.</p>
</dd>
<dt><code>target</code></dt><dd><p>A real number restricting the absolute function value. 
The procedure stops if this value is exceeded.
Default is Inf, that is no restriction.</p>
</dd>
<dt><code>info</code></dt><dd><p>A logical variable indicating whether the step number, 
number of function calls, best function value, and the first component of
the solution vector will be printed to the console. Default is FALSE.</p>
</dd>
</dl>

<p>If the minimization process threatens to go into an infinite loop, set
either <code>maxfeval</code> or <code>target</code>.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table role = "presentation">
<tr><td><code>par</code></td>
<td>
<p>Best estimate of the parameter vector found by the algorithm.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>value of the objective function at termination.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>indicates convergence (<code>=0</code>) or not (<code>=1</code>).</p>
</td></tr>
<tr><td><code>feval</code></td>
<td>
<p>number of times the objective <code>fn</code> was evaluated.</p>
</td></tr>
<tr><td><code>niter</code></td>
<td>
<p>number of iterations in the main loop.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This algorithm is based on the Matlab code of Prof. C. T. Kelley, given
in his book &ldquo;Iterative methods for optimization&quot;.
It is implemented here with the permission of Prof. Kelley.
</p>
<p>This version does not (yet) implement a cache for storing function values
that have already been computed as searching the cache makes it slower.
</p>


<h3>Author(s)</h3>

<p>Hans W Borchers  &lt;hwborchers@googlemail.com&gt;
</p>


<h3>References</h3>

<p>C.T. Kelley (1999), Iterative Methods for Optimization, SIAM.
</p>
<p>Quarteroni, Sacco, and Saleri (2007), Numerical Mathematics, Springer.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>, <code><a href="#topic+nmk">nmk</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##  Hooke-Jeeves solves high-dim. Rosenbrock function
  rosenbrock &lt;- function(x){
    n &lt;- length(x)
    sum (100*(x[1:(n-1)]^2 - x[2:n])^2 + (x[1:(n-1)] - 1)^2)
  }
par0 &lt;- rep(0, 10)
hjk(par0, rosenbrock)

hjkb(c(0, 0, 0), rosenbrock, upper = 0.5)
# $par
# [1] 0.50000000 0.25742722 0.06626892


##  Hooke-Jeeves does not work well on non-smooth functions
  nsf &lt;- function(x) {
	f1 &lt;- x[1]^2 + x[2]^2
	f2 &lt;- x[1]^2 + x[2]^2 + 10 * (-4*x[1] - x[2] + 4)
	f3 &lt;- x[1]^2 + x[2]^2 + 10 * (-x[1] - 2*x[2] + 6)
	max(f1, f2, f3)
  }
par0 &lt;- c(1, 1)                                 # true min 7.2 at (1.2, 2.4)
hjk(par0, nsf) # fmin=8 at xmin=(2,2)
</code></pre>

<hr>
<h2 id='mads'>
Mesh Adaptive Direct Searches (MADS) algorithm for derivative-free and black-box optimization 
</h2><span id='topic+mads'></span>

<h3>Description</h3>

<p>An implementation of the Mesh Adaptive Direct Searches (MADS) algorithm for derivative-free and black-box optimization. It uses a series of variable size meshes to search the space and to converge to (local) minima with mathematical proof of convergence. It is usable on unbounded and bounded unconstrained problems. The objective function can return &ldquo;NA&rdquo; if out-of-bound or violating constraints (strict barrier approach for constraints), or a penalty can be added to the objective function.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mads(par, fn, lower=-Inf, upper=Inf, scale=1, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mads_+3A_par">par</code></td>
<td>
<p>A starting vector of parameter values. Must be feasible, i.e. lie strictly between lower and upper bounds.</p>
</td></tr>
<tr><td><code id="mads_+3A_fn">fn</code></td>
<td>

<p>Noisy, non-differentiable, non-convex, piecewise or nonlinear objective function that is to be optimized. It takes a real vector as argument and returns a scalar or &ldquo;NA&rdquo; that is the value of the function at that point (see details).</p>
</td></tr>
<tr><td><code id="mads_+3A_lower">lower</code></td>
<td>
<p>Lower bounds on the parameters.  A vector of the same length as the parameters.  If a single value is specified, it is assumed that the same lower bound applies to all parameters. If all lower bounds are -Inf and all upper bounds are Inf, then the problem is treated as unbounded.</p>
</td></tr> 
<tr><td><code id="mads_+3A_upper">upper</code></td>
<td>
<p>Upper bounds on the parameters.  A vector of the same length as the parameters.  If a single value is specified, it is assumed that the same upper bound applies to all parameters. If all lower bounds are -Inf and all upper bounds are Inf, then the problem is treated as unbounded.</p>
</td></tr>
<tr><td><code id="mads_+3A_scale">scale</code></td>
<td>
<p>Optional scaling, default is 1.  A vector of the same length as the parameters. If a single value is specified, it is assumed that the same scale factor applies to all parameters.  This scale factor can be customized for each parameter allowing non-proportional moves in the space (normally used for unbounded problems). </p>
</td></tr>  
<tr><td><code id="mads_+3A_control">control</code></td>
<td>
<p>A list of control parameters.  See *Details* for more information.  </p>
</td></tr>
<tr><td><code id="mads_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>fn</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Argument <code>control</code> is a list specifing any changes to default values of algorithm control parameters for the outer loop.  The list items are as follows:
</p>
<p><code>tol</code> Convergence tolerance.  Iteration is terminated when the absolute difference in function value between successive iteration is below <code>tol</code>.  Default is 1.e-06.
</p>
<p><code>maxfeval</code>: Maximum number of objective function evaluations allowed.  Default is 10000).
</p>
<p><code>trace</code> A logical variable indicating whether information is printed on the console during execution.  Default is TRUE.
</p>
<p><code>maximize</code> A logical variable indicating whether the objective function should be maximized.  Default is FALSE (hence default is minimization).
</p>
<p><code>pollStyle</code> A string variable indicating density of the poll set, or, number of vectors in the positive basis. Choices are: &ldquo;lite&rdquo; (n+1 points) or &ldquo;full&rdquo; (2n points).  Default is &ldquo;lite&rdquo;.
</p>
<p><code>deltaInit</code> A numerical value specifying the initial mesh size, between &ldquo;tol&rdquo; and 1 (mesh size is limited to 1).  Default is 0.01.
</p>
<p><code>expand</code> A numerical value &gt;1 specifying the expansion (is success) and contraction (if no success) factor of the mesh at the end of an iteration. Default is 4.
</p>
<p><code>lineSearch</code> A integer value indicating the maximum of search steps to consider. Line search is performed at the end of a successful poll set evaluation, along the line going from last to new &ldquo;best&rdquo; solution. Stepsize will be automatically increased according to the Fibonacci series. Default is 20. Set to -1 to disable the feature.
</p>
<p><code>seed</code> Seed value for the internal pseudo random numbers generator. Default is 1138.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table role = "presentation">
<tr><td><code>par</code></td>
<td>
<p>Best estimate of the parameter vector found by the algorithm.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective function at termination.</p>
</td></tr>
<tr><td><code>feval</code></td>
<td>
<p>The number of times the objective <code>fn</code> was evaluated.
</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>Final mesh size, should be &lt;tol if successfule convergence. If feval reached maxfeval, then the algorithm did not converge.
</p>
</td></tr>
<tr><td><code>iterlog</code></td>
<td>
<p>A dataframe used to log properties of the &ldquo;best&rdquo; solution at the end of each iteration. 
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This algorithm is based on the Lower Triangular method described in the reference.
</p>


<h3>Author(s)</h3>

<p>Vincent Bechard &lt;vincent.bechard@hec.ca&gt;, HEC Montreal (Montreal University)
URL:https://www.linkedin.com/in/vincentbechard 
</p>


<h3>References</h3>

<p>C. Audet and J. E. Dennis, Jr. Mesh adaptive direct search algorithms for constrained optimization. SIAM Journal on Optimization, 17(1): 188-217, 2006.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>, <code><a href="#topic+hjk">hjk</a></code>, <code><a href="#topic+nmk">nmk</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> rosbkext &lt;- function(x){
# Extended Rosenbrock function
 n &lt;- length(x)
 sum (100*(x[1:(n-1)]^2 - x[2:n])^2 + (x[1:(n-1)] - 1)^2)
 }

np &lt;- 10
p0 &lt;- rnorm(np)
ans1 &lt;- mads(fn=rosbkext, par=p0, lower=-10, upper=10, scale=1, control=list(trace=FALSE))

### A non-smooth problem from Hock &amp; Schittkowski #78
hs78 &lt;- function(x){
  f &lt;- rep(NA, 3)
  f[1] &lt;- sum(x^2) - 10
  f[2] &lt;- x[2]*x[3] - 5*x[4]*x[5]
  f[3] &lt;- x[1]^3 + x[2]^3 + 1
  F &lt;- prod(x) + 10*sum(abs(f))
  return(F)
}

p0 &lt;- c(-2,1.5,2,-1,-1)
ans2 &lt;- mads(p0, hs78, control=list(trace=FALSE)) #minimum value around -2.81


</code></pre>

<hr>
<h2 id='nmk'>
Nelder-Mead optimziation algorithm for derivative-free optimization 
</h2><span id='topic+nmk'></span><span id='topic+nmkb'></span>

<h3>Description</h3>

<p>An implementation of the Nelder-Mead algorithm for derivative-free optimization.  This allows bounds to be placed on parameters. Bounds are enforced by means of a parameter transformation.</p>


<h3>Usage</h3>

<pre><code class='language-R'>nmk(par, fn, control = list(), ...)

nmkb(par, fn, lower=-Inf, upper=Inf, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nmk_+3A_par">par</code></td>
<td>
<p>A starting vector of parameter values. Must be feasible, i.e. lie strictly between lower and upper bounds.</p>
</td></tr>
<tr><td><code id="nmk_+3A_fn">fn</code></td>
<td>

<p>Nonlinear objective function that is to be optimized. 
A scalar function that takes a real vector as argument and 
returns a scalar that is the value of the function at that point 
(see details).</p>
</td></tr>
<tr><td><code id="nmk_+3A_lower">lower</code></td>
<td>
<p>Lower bounds on the parameters.  A vector of the same length as the parameters.  If a single value is specified, it is assumed that the same lower bound applies to all parameters.</p>
</td></tr> 
<tr><td><code id="nmk_+3A_upper">upper</code></td>
<td>
<p>Upper bounds on the parameters.  A vector of the same length as the parameters.  If a single value is specified, it is assumed that the same upper bound applies to all parameters.</p>
</td></tr> 
<tr><td><code id="nmk_+3A_control">control</code></td>
<td>
<p>A list of control parameters.  See *Details* for more information.
</p>
</td></tr>
<tr><td><code id="nmk_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>fn</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Argument <code>control</code> is a list specifing any changes to default values of algorithm control parameters for the outer loop.  Note that the names of these must be specified completely.  Partial matching will not work.  The list items are as follows:
</p>
<p><code>tol</code> Convergence tolerance.  Iteration is terminated when the absolute difference in function value between successive iteration is below <code>tol</code>.  Default is 1.e-06.
</p>
<p><code>maxfeval</code>: Maximum number of objective function evaluations allowed.  Default is min(5000, max(1500, 20*length(par)^2)).
</p>
<p><code>regsimp</code> A logical variable indicating whether the starting parameter configuration is a regular simplex.  Default is TRUE.
</p>
<p><code>maximize</code> A logical variable indicating whether the objective function should be maximized.  Default is FALSE.
</p>
<p><code>restarts.max</code> Maximum number of times the algorithm should be restarted before declaring failure. Default is 3.
</p>
<p><code>trace</code> A logical variable indicating whether the starting parameter configuration is a regular simplex.  Default is FALSE.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table role = "presentation">
<tr><td><code>par</code></td>
<td>
<p>Best estimate of the parameter vector found by the algorithm.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective function at termination.</p>
</td></tr>
<tr><td><code>feval</code></td>
<td>
<p>The number of times the objective <code>fn</code> was evaluated.
</p>
</td></tr>
<tr><td><code>restarts</code></td>
<td>
<p>The number of times the algorithm had to be restarted when it stagnated.
</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer code indicating type of convergence.  <code>0</code> indicates successful convergence. Positive integer codes indicate failure to converge.    
</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>Text message indicating the type of convergence or failure.  
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This algorithm is based on the Matlab code of Prof. C.T. Kelley, given in his book &quot;Iterative methods for optimization&quot;.
It is implemented here with the permission of Prof. Kelley and SIAM.  However, there are some non-trivial modifications of the algorithm.
</p>


<h3>Author(s)</h3>

<p>Ravi Varadhan &lt;rvaradhan@jhmi.edu&gt;, Johns Hopkins University
URL:http://www.jhsph.edu/agingandhealth/People/Faculty_personal_pages/Varadhan.html
</p>


<h3>References</h3>

<p>C.T. Kelley (1999), Iterative Methods for Optimization, SIAM.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>, <code><a href="#topic+hjk">hjk</a></code>, <code><a href="#topic+mads">mads</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> rosbkext &lt;- function(x){
# Extended Rosenbrock function
 n &lt;- length(x)
 sum (100*(x[1:(n-1)]^2 - x[2:n])^2 + (x[1:(n-1)] - 1)^2)
 }

np &lt;- 10
set.seed(123)

p0 &lt;- rnorm(np)
xm1 &lt;- nmk(fn=rosbkext, par=p0) # maximum `fevals' is not sufficient to find correct minimum
xm1b &lt;- nmkb(fn=rosbkext, par=p0, lower=-2, upper=2)

### A non-smooth problem
hald &lt;- function(x) {
#Hald J &amp; Madsen K (1981), Combined LP and quasi-Newton methods 
#for minimax optimization, Mathematical Programming, 20, p.42-62.
	i &lt;- 1:21
	t &lt;- -1 + (i - 1)/10
	f &lt;- (x[1] + x[2] * t) / ( 1 + x[3]*t + x[4]*t^2 + x[5]*t^3) - exp(t)
	max(abs(f))
	}

p0 &lt;- runif(5)
xm2 &lt;- nmk(fn=hald, par=p0)
xm2b &lt;- nmkb(fn=hald, par=p0, lower=c(0,0,0,0,-2), upper=4)

## Another non-smooth functions
  nsf &lt;- function(x) {
	f1 &lt;- x[1]^2 + x[2]^2
	f2 &lt;- x[1]^2 + x[2]^2 + 10 * (-4*x[1] - x[2] + 4)
	f3 &lt;- x[1]^2 + x[2]^2 + 10 * (-x[1] - 2*x[2] + 6)
	max(f1, f2, f3)
  }
par0 &lt;- c(1, 1)                                 # true min 7.2 at (1.2, 2.4)
nmk(par0, nsf) # fmin=8 at xmin=(2,2)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
