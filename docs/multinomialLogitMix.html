<!DOCTYPE html><html><head><title>Help for package multinomialLogitMix</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {multinomialLogitMix}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#multinomialLogitMix-package'>
<p>Clustering Multinomial Count Data under the Presence of Covariates</p></a></li>
<li><a href='#dealWithLabelSwitching'>
<p>Post-process the generated MCMC sample in order to undo possible label switching.</p></a></li>
<li><a href='#expected_complete_LL'>
<p>Expected complete LL</p></a></li>
<li><a href='#gibbs_mala_sampler'>
<p>The core of the Hybrid Gibbs/MALA MCMC sampler for the multinomial logit mixture.</p></a></li>
<li><a href='#gibbs_mala_sampler_ppt'>
<p>Prior parallel tempering scheme of hybrid Gibbs/MALA MCMC samplers for the multinomial logit mixture.</p></a></li>
<li><a href='#log_dirichlet_pdf'>
<p>Log-density function of the Dirichlet distribution</p></a></li>
<li><a href='#mala_proposal'>
<p>Proposal mechanism of the MALA step.</p></a></li>
<li><a href='#mix_mnm_logistic'>
<p>EM algorithm</p></a></li>
<li><a href='#mixLoglikelihood_GLM'>
<p>Log-likelihood of the multinomial logit.</p></a></li>
<li><a href='#multinomial_logistic_EM'>
<p>Part of the EM algorithm for multinomial logit mixture</p></a></li>
<li><a href='#multinomialLogitMix'>
<p>Main function</p></a></li>
<li><a href='#myDirichlet'>
<p>Simulate from the Dirichlet distribution</p></a></li>
<li><a href='#newton_raphson_mstep'>
<p>M-step of the EM algorithm</p></a></li>
<li><a href='#shakeEM_GLM'>
<p>Shake-small EM</p></a></li>
<li><a href='#simulate_multinomial_data'>
<p>Synthetic data generator</p></a></li>
<li><a href='#splitEM_GLM'>
<p>Split-small EM scheme.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Clustering Multinomial Count Data under the Presence of
Covariates</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-07-13</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Panagiotis Papastamoulis &lt;papapast@yahoo.gr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Methods for model-based clustering of multinomial counts under the presence of covariates using mixtures of multinomial logit models, as implemented in Papastamoulis (2023) &lt;<a href="https://doi.org/10.1007%2Fs11634-023-00547-5">doi:10.1007/s11634-023-00547-5</a>&gt;. These models are estimated under  a frequentist as well as a Bayesian setup using the Expectation-Maximization algorithm and Markov chain Monte Carlo sampling (MCMC), respectively. The (unknown) number of clusters is selected according to the Integrated Completed Likelihood criterion (for the frequentist model), and estimating the number of non-empty components using overfitting mixture models after imposing suitable sparse prior assumptions on the mixing proportions (in the Bayesian case), see Rousseau and Mengersen (2011) &lt;<a href="https://doi.org/10.1111%2Fj.1467-9868.2011.00781.x">doi:10.1111/j.1467-9868.2011.00781.x</a>&gt;. In the latter case, various MCMC chains run in parallel and are allowed to switch states. The final MCMC output is suitably post-processed in order to undo label switching using the Equivalence Classes Representatives (ECR) algorithm, as described in Papastamoulis (2016) &lt;<a href="https://doi.org/10.18637%2Fjss.v069.c01">doi:10.18637/jss.v069.c01</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.8.3), MASS, doParallel, foreach, label.switching,
ggplot2, coda, matrixStats, mvtnorm, RColorBrewer</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-07-14 07:55:32 UTC; panos</td>
</tr>
<tr>
<td>Author:</td>
<td>Panagiotis Papastamoulis
    <a href="https://orcid.org/0000-0001-9468-7613"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-07-17 05:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='multinomialLogitMix-package'>
Clustering Multinomial Count Data under the Presence of Covariates
</h2><span id='topic+multinomialLogitMix-package'></span>

<h3>Description</h3>

<p>Methods for model-based clustering of multinomial counts under the presence of covariates using mixtures of multinomial logit models, as implemented in Papastamoulis (2023) &lt;DOI:10.1007/s11634-023-00547-5&gt;. These models are estimated under  a frequentist as well as a Bayesian setup using the Expectation-Maximization algorithm and Markov chain Monte Carlo sampling (MCMC), respectively. The (unknown) number of clusters is selected according to the Integrated Completed Likelihood criterion (for the frequentist model), and estimating the number of non-empty components using overfitting mixture models after imposing suitable sparse prior assumptions on the mixing proportions (in the Bayesian case), see Rousseau and Mengersen (2011) &lt;DOI:10.1111/j.1467-9868.2011.00781.x&gt;. In the latter case, various MCMC chains run in parallel and are allowed to switch states. The final MCMC output is suitably post-processed in order to undo label switching using the Equivalence Classes Representatives (ECR) algorithm, as described in Papastamoulis (2016) &lt;DOI:10.18637/jss.v069.c01&gt;. 
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> multinomialLogitMix</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Clustering Multinomial Count Data under the Presence of Covariates</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-07-13</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> 
    c(person(given = "Panagiotis",
             family = "Papastamoulis",
             email = "papapast@yahoo.gr",
             role = c( "aut", "cre"),
             comment = c(ORCID = "0000-0001-9468-7613")))</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Panagiotis Papastamoulis &lt;papapast@yahoo.gr&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Methods for model-based clustering of multinomial counts under the presence of covariates using mixtures of multinomial logit models, as implemented in Papastamoulis (2023) &lt;DOI:10.1007/s11634-023-00547-5&gt;. These models are estimated under  a frequentist as well as a Bayesian setup using the Expectation-Maximization algorithm and Markov chain Monte Carlo sampling (MCMC), respectively. The (unknown) number of clusters is selected according to the Integrated Completed Likelihood criterion (for the frequentist model), and estimating the number of non-empty components using overfitting mixture models after imposing suitable sparse prior assumptions on the mixing proportions (in the Bayesian case), see Rousseau and Mengersen (2011) &lt;DOI:10.1111/j.1467-9868.2011.00781.x&gt;. In the latter case, various MCMC chains run in parallel and are allowed to switch states. The final MCMC output is suitably post-processed in order to undo label switching using the Equivalence Classes Representatives (ECR) algorithm, as described in Papastamoulis (2016) &lt;DOI:10.18637/jss.v069.c01&gt;. </td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> Rcpp (&gt;= 1.0.8.3), MASS, doParallel, foreach, label.switching, ggplot2, coda, matrixStats, mvtnorm, RColorBrewer</td>
</tr>
<tr>
 <td style="text-align: left;">
LinkingTo: </td><td style="text-align: left;"> Rcpp, RcppArmadillo</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Panagiotis Papastamoulis [aut, cre]
    (&lt;https://orcid.org/0000-0001-9468-7613&gt;)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
dealWithLabelSwitching
                        Post-process the generated MCMC sample in order
                        to undo possible label switching.
expected_complete_LL    Expected complete LL
gibbs_mala_sampler      The core of the Hybrid Gibbs/MALA MCMC sampler
                        for the multinomial logit mixture.
gibbs_mala_sampler_ppt
                        Prior parallel tempering scheme of hybrid
                        Gibbs/MALA MCMC samplers for the multinomial
                        logit mixture.
log_dirichlet_pdf       Log-density function of the Dirichlet
                        distribution
mala_proposal           Proposal mechanism of the MALA step.
mixLoglikelihood_GLM    Log-likelihood of the multinomial logit.
mix_mnm_logistic        EM algorithm
multinomialLogitMix     Main function
multinomialLogitMix-package
                        Clustering Multinomial Count Data under the
                        Presence of Covariates
multinomial_logistic_EM
                        Part of the EM algorithm for multinomial logit
                        mixture
myDirichlet             Simulate from the Dirichlet distribution
newton_raphson_mstep    M-step of the EM algorithm
shakeEM_GLM             Shake-small EM
simulate_multinomial_data
                        Synthetic data generator
splitEM_GLM             Split-small EM scheme.
</pre>
<p>See the main function of the package: <code><a href="#topic+multinomialLogitMix">multinomialLogitMix</a></code>, which wraps automatically calls to the MCMC sampler <code><a href="#topic+gibbs_mala_sampler_ppt">gibbs_mala_sampler_ppt</a></code> and the EM algorithm <code><a href="#topic+mix_mnm_logistic">mix_mnm_logistic</a></code>. 
</p>


<h3>Author(s)</h3>

<p>NA
</p>
<p>Maintainer: Panagiotis Papastamoulis &lt;papapast@yahoo.gr&gt;
</p>


<h3>References</h3>

<p>Papastamoulis, P. Model based clustering of multinomial count data. Advances in Data Analysis and Classification (2023). https://doi.org/10.1007/s11634-023-00547-5
</p>
<p>Papastamoulis, P. and Iliopoulos, G. (2010). An Artificial Allocations Based Solution to the Label Switching Problem in Bayesian Analysis of Mixtures of Distributions. Journal of Computational and Graphical Statistics, 19(2), 313-331. http://www.jstor.org/stable/25703571
</p>
<p>Papastamoulis, P. (2016). label.switching: An R Package for Dealing with the Label Switching Problem in MCMC Outputs. Journal of Statistical Software, Code Snippets, 69(1), 1-24. https://doi.org/10.18637/jss.v069.c01
</p>
<p>Rousseau, J. and Mengersen, K. (2011), Asymptotic behaviour of the posterior distribution in overfitted mixture models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73: 689-710. https://doi.org/10.1111/j.1467-9868.2011.00781.x
</p>


<h3>See Also</h3>

<p><code><a href="#topic+multinomialLogitMix">multinomialLogitMix</a></code>, <code><a href="#topic+gibbs_mala_sampler_ppt">gibbs_mala_sampler_ppt</a></code>,<code><a href="#topic+mix_mnm_logistic">mix_mnm_logistic</a></code>
</p>

<hr>
<h2 id='dealWithLabelSwitching'>
Post-process the generated MCMC sample in order to undo possible label switching. 
</h2><span id='topic+dealWithLabelSwitching'></span>

<h3>Description</h3>

<p>This function implements the Equivalence Classes Representatives (ECR) algorithm from the label.switching package in order to undo the label switching phenomenon.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dealWithLabelSwitching(gs, burn, thin = 10, zPivot = NULL, returnRaw = FALSE, maxM = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dealWithLabelSwitching_+3A_gs">gs</code></td>
<td>

<p>An object generated by the main function of the package. 
</p>
</td></tr>
<tr><td><code id="dealWithLabelSwitching_+3A_burn">burn</code></td>
<td>

<p>Number of draws that will be discarder as burn-in.
</p>
</td></tr>
<tr><td><code id="dealWithLabelSwitching_+3A_thin">thin</code></td>
<td>

<p>Thinning of the MCMC sample.
</p>
</td></tr>
<tr><td><code id="dealWithLabelSwitching_+3A_zpivot">zPivot</code></td>
<td>

<p>Optional vector of allocations that will be used as the pivot of the ECR algorithm. If this is not supplied, the pivot will be selected as the allocation vector that corresponds to the iteration that maximized the log-likelihood of the model.
</p>
</td></tr>
<tr><td><code id="dealWithLabelSwitching_+3A_returnraw">returnRaw</code></td>
<td>

<p>Boolean. If true, the function will also return the raw output. 
</p>
</td></tr>
<tr><td><code id="dealWithLabelSwitching_+3A_maxm">maxM</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Papastamoulis (2016).
</p>


<h3>Value</h3>

<table>
<tr><td><code>cluster</code></td>
<td>
<p>Single best clustering of the data, according to the Maximum A Posteriori rule.</p>
</td></tr>
<tr><td><code>nClusters_posterior</code></td>
<td>
<p>Estimated posterior distribution of the number of clusters.</p>
</td></tr>
<tr><td><code>mcmc</code></td>
<td>
<p>Post-processed mcmc output.</p>
</td></tr>
<tr><td><code>posteriorProbabilities</code></td>
<td>
<p>Estimated posterior membership probabilities.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Papastamoulis, P. (2016). <code>label.switching</code>: An R Package for Dealing with the Label Switching Problem in MCMC Outputs. Journal of Statistical Software, 69(1), 1-24.
</p>

<hr>
<h2 id='expected_complete_LL'>
Expected complete LL
</h2><span id='topic+expected_complete_LL'></span>

<h3>Description</h3>

<p>This function is not used at the moment. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expected_complete_LL(y, X, b, w, pr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expected_complete_LL_+3A_y">y</code></td>
<td>

<p>count data.
</p>
</td></tr>
<tr><td><code id="expected_complete_LL_+3A_x">X</code></td>
<td>

<p>design matrix.
</p>
</td></tr>
<tr><td><code id="expected_complete_LL_+3A_b">b</code></td>
<td>

<p>Logit coefficients.
</p>
</td></tr>
<tr><td><code id="expected_complete_LL_+3A_w">w</code></td>
<td>

<p>mixing proportions.
</p>
</td></tr>
<tr><td><code id="expected_complete_LL_+3A_pr">pr</code></td>
<td>

<p>mixing proportions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Complete log-likelihood of the model.</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>

<hr>
<h2 id='gibbs_mala_sampler'>
The core of the Hybrid Gibbs/MALA MCMC sampler for the multinomial logit mixture.
</h2><span id='topic+gibbs_mala_sampler'></span>

<h3>Description</h3>

<p>This function implements Gibbs sampling to update the mixing proportions and latent allocations variables of the mixture model. The coefficients of the logit model are updated according to Metropolis-Hastings type move, based on a Metropolis adjusted Langevin (MALA) proposal. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbs_mala_sampler(y, X, tau = 3e-05, nu2, K, mcmc_iter = 100, 
	alpha_prior = NULL, start_values = "EM", em_iter = 10, 
	thin = 10, verbose = FALSE, checkAR = NULL, 
	probsSave = FALSE, ar_low = 0.4, ar_up = 0.6)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gibbs_mala_sampler_+3A_y">y</code></td>
<td>

<p>matrix of counts.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_x">X</code></td>
<td>

<p>design matrix (including constant term). 
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_tau">tau</code></td>
<td>

<p>the variance of the normal prior distribution of the logit coefficients.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_nu2">nu2</code></td>
<td>

<p>scale of the MALA proposal (positive).
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_k">K</code></td>
<td>

<p>number of components of the (overfitting) mixture model.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_mcmc_iter">mcmc_iter</code></td>
<td>

<p>Number of MCMC iterations.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_alpha_prior">alpha_prior</code></td>
<td>

<p>Parameter of the Dirichlet prior distribution for the mixing proportions.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_start_values">start_values</code></td>
<td>

<p>Optional list of starting values. Random initialization is used if this is not provided.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_em_iter">em_iter</code></td>
<td>

<p>Maximum number of iterations if an EM initialization is enabled.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_thin">thin</code></td>
<td>

<p>optional thinning of the generated MCMC output.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_verbose">verbose</code></td>
<td>

<p>Boolean.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_checkar">checkAR</code></td>
<td>

<p>Number of iterations to adjust the scale of the proposal in MALA mechanism during the initial warm-up phase of the sampler.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_probssave">probsSave</code></td>
<td>

<p>Optional.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_ar_low">ar_low</code></td>
<td>

<p>Lowest threshold for the acceptance rate of the MALA proposal (optional) .
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_+3A_ar_up">ar_up</code></td>
<td>

<p>Highest threshold for the acceptance rate of the MALA proposal (optional). 
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>nClusters</code></td>
<td>
<p>sampled values of the number of clusters (non-empty mixture components).</p>
</td></tr>
<tr><td><code>allocations</code></td>
<td>
<p>sampled values of the latent allocation variables.</p>
</td></tr>
<tr><td><code>logLikelihood</code></td>
<td>
<p>Log-likelihood values per MCMC iteration.</p>
</td></tr>
<tr><td><code>mixing_proportions</code></td>
<td>
<p>sampled values of mixing proportions.</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>sapled values of the coefficients of the multinomial logit.</p>
</td></tr>
<tr><td><code>complete_logLikelihood</code></td>
<td>
<p>Complete log-likelihood values per MCMC iteration.</p>
</td></tr>
<tr><td><code>class_probs</code></td>
<td>
<p>Classification probabilities per iteration (optional).</p>
</td></tr>
<tr><td><code>AR</code></td>
<td>
<p>Acceptance rate of the MALA proposal.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is used inside the prior tempering scheme, which is the main function. 
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gibbs_mala_sampler_ppt">gibbs_mala_sampler_ppt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#	Generate synthetic data
	K &lt;- 2
	p &lt;- 2
	D &lt;- 2
	n &lt;- 2
	set.seed(116)
	simData &lt;- simulate_multinomial_data(K = K, p = p, D = D, n = n, size = 20, prob = 0.025)   


	gs &lt;- gibbs_mala_sampler(y = simData$count_data, X = simData$design_matrix, 
		tau = 0.00035, nu2 = 100, K = 2, mcmc_iter = 3, 
		alpha_prior = rep(1,K), start_values = "RANDOM", 
		thin = 1, verbose = FALSE, checkAR = 100)

</code></pre>

<hr>
<h2 id='gibbs_mala_sampler_ppt'>
Prior parallel tempering scheme of hybrid Gibbs/MALA MCMC samplers for the multinomial logit mixture.
</h2><span id='topic+gibbs_mala_sampler_ppt'></span>

<h3>Description</h3>

<p>The main MCMC scheme of the package. Multiple chains are run in parallel and swaps between are proposed. Each chain uses different parameters on the Dirichlet prior of the mixing proportion. The smaller concentration parameter should correspond to the first chain, which is the one that used for inference. Subsequent chains should have larger values of concentration parameter for the Dirichlet prior. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbs_mala_sampler_ppt(y, X, tau = 3e-05, nu2, K, 
	mcmc_cycles = 100, iter_per_cycle = 10, dirPriorAlphas, 
	start_values = "EM", em_iter = 10, nChains = 4, nCores = 4, 
	warm_up = 100, checkAR = 50, probsSave = FALSE, 
	showGraph = 50, ar_low = 0.4, ar_up = 0.6, withRandom = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_y">y</code></td>
<td>

<p>matrix of counts.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_x">X</code></td>
<td>

<p>design matrix (including constant term). 
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_tau">tau</code></td>
<td>

<p>the variance of the normal prior distribution of the logit coefficients.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_nu2">nu2</code></td>
<td>

<p>scale of the MALA proposal (positive).
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_k">K</code></td>
<td>

<p>number of components of the (overfitting) mixture model.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_mcmc_cycles">mcmc_cycles</code></td>
<td>

<p>Number of MCMC cycles. At the end of each cycle, a swap between chains is attempted.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_iter_per_cycle">iter_per_cycle</code></td>
<td>

<p>Number of iterations per cycle. 
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_dirprioralphas">dirPriorAlphas</code></td>
<td>

<p>Vector of concentration parameters for the Dirichlet priors in increasing order.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_start_values">start_values</code></td>
<td>

<p>Optional list of start values. Randomly generated values are used if this is not provided. 
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_em_iter">em_iter</code></td>
<td>

<p>Maximum number of iterations if an EM initialization is enabled.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_nchains">nChains</code></td>
<td>

<p>Total number of parallel chains. 
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_ncores">nCores</code></td>
<td>

<p>Total number of CPU cores for parallel processing of the <code>nChains</code>.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_warm_up">warm_up</code></td>
<td>

<p>Initial warm-up period of the sampler, in order to adaptively tune the scale of the MALA proposal (optional). 
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_checkar">checkAR</code></td>
<td>

<p>Number of iterations to adjust the scale of the proposal in MALA mechanism during the initial warm-up phase of the sampler.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_probssave">probsSave</code></td>
<td>

<p>Optional.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_showgraph">showGraph</code></td>
<td>

<p>Optional.
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_ar_low">ar_low</code></td>
<td>

<p>Lowest threshold for the acceptance rate of the MALA proposal (optional) .
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_ar_up">ar_up</code></td>
<td>

<p>Highest threshold for the acceptance rate of the MALA proposal (optional). 
</p>
</td></tr>
<tr><td><code id="gibbs_mala_sampler_ppt_+3A_withrandom">withRandom</code></td>
<td>

<p>Logical. If TRUE (default) then a random permutation is applied to the supplied starting values. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the paper for details.
</p>


<h3>Value</h3>

<table>
<tr><td><code>nClusters</code></td>
<td>
<p>sampled values of the number of clusters (non-empty mixture components).</p>
</td></tr>
<tr><td><code>allocations</code></td>
<td>
<p>sampled values of the latent allocation variables.</p>
</td></tr>
<tr><td><code>logLikelihood</code></td>
<td>
<p>Log-likelihood values per MCMC iteration.</p>
</td></tr>
<tr><td><code>mixing_proportions</code></td>
<td>
<p>sampled values of mixing proportions.</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>sapled values of the coefficients of the multinomial logit.</p>
</td></tr>
<tr><td><code>complete_logLikelihood</code></td>
<td>
<p>Complete log-likelihood values per MCMC iteration.</p>
</td></tr>
<tr><td><code>class_probs</code></td>
<td>
<p>Classification probabilities per iteration (optional).</p>
</td></tr>
<tr><td><code>AR</code></td>
<td>
<p>Acceptance rate of the MALA proposal.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The output of the MCMC sampler is not identifiable, due to possible label switching. In order to draw meaningful inferences, the output should be post-processed by <code><a href="#topic+dealWithLabelSwitching">dealWithLabelSwitching</a></code>.</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Papastamoulis, P (2022). Model-based clustering of multinomial count data. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#	Generate synthetic data

	K &lt;- 2
	p &lt;- 2
	D &lt;- 3
	n &lt;- 2
	set.seed(116)
	simData &lt;- simulate_multinomial_data(K = K, p = p, D = D, n = n, size = 20, prob = 0.025)   



# apply mcmc sampler based on random starting values 

Kmax = 2
nChains = 2
dirPriorAlphas  = c(1, 1 + 5*exp((seq(2, 14, length = nChains - 1)))/100)/(200)
nCores &lt;- 2
mcmc_cycles &lt;- 2
iter_per_cycle = 2
warm_up &lt;- 2

mcmc_random1 &lt;-  gibbs_mala_sampler_ppt( y = simData$count_data, X = simData$design_matrix, 
		tau = 0.00035, nu2 = 100,  K = Kmax, dirPriorAlphas = dirPriorAlphas,
		mcmc_cycles = mcmc_cycles, iter_per_cycle = iter_per_cycle, 
		start_values = 'RANDOM', 
		nChains = nChains, nCores = nCores, warm_up = warm_up, showGraph = 1000, 
		checkAR = 1000)

#sampled values for the number of clusters (non-empty mixture components) per chain (columns)
mcmc_random1$nClusters
</code></pre>

<hr>
<h2 id='log_dirichlet_pdf'>
Log-density function of the Dirichlet distribution
</h2><span id='topic+log_dirichlet_pdf'></span>

<h3>Description</h3>

<p>Log-density function of the Dirichlet distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_dirichlet_pdf(alpha, weights)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_dirichlet_pdf_+3A_alpha">alpha</code></td>
<td>

<p>Parameter vector
</p>
</td></tr>
<tr><td><code id="log_dirichlet_pdf_+3A_weights">weights</code></td>
<td>

<p>Vector of weights.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Log-density of the <code class="reqn">D(\alpha_1,\ldots,\alpha_k)</code> evaluated at <code class="reqn">w_1,\ldots,w_k</code>.
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>

<hr>
<h2 id='mala_proposal'>
Proposal mechanism of the MALA step.
</h2><span id='topic+mala_proposal'></span><span id='topic+mala_proposal_cpp'></span><span id='topic+log_mix_prior_derivative'></span><span id='topic+log_prior_mix'></span>

<h3>Description</h3>

<p>Only the mala_proposal_cpp function is used in the package - which is written as an RCPP function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mala_proposal(y, X, b, z, tau, A = FALSE, pr, nu2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mala_proposal_+3A_y">y</code></td>
<td>

<p>count data
</p>
</td></tr>
<tr><td><code id="mala_proposal_+3A_x">X</code></td>
<td>

<p>design matrix
</p>
</td></tr>
<tr><td><code id="mala_proposal_+3A_b">b</code></td>
<td>

<p>coefficients (array
</p>
</td></tr>
<tr><td><code id="mala_proposal_+3A_z">z</code></td>
<td>

<p>allocation vector
</p>
</td></tr>
<tr><td><code id="mala_proposal_+3A_tau">tau</code></td>
<td>

<p>prior variance
</p>
</td></tr>
<tr><td><code id="mala_proposal_+3A_a">A</code></td>
<td>

<p>A
</p>
</td></tr>
<tr><td><code id="mala_proposal_+3A_pr">pr</code></td>
<td>

<p>mixing proportions
</p>
</td></tr>
<tr><td><code id="mala_proposal_+3A_nu2">nu2</code></td>
<td>

<p>parameter nu2
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>theta</code></td>
<td>
<p>theta values</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>coeeficients</p>
</td></tr>
<tr><td><code>acceptance</code></td>
<td>
<p>log-likelihood.</p>
</td></tr>
<tr><td><code>gradient</code></td>
<td>
<p>log-likelihood.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>

<hr>
<h2 id='mix_mnm_logistic'>
EM algorithm
</h2><span id='topic+mix_mnm_logistic'></span>

<h3>Description</h3>

<p>Estimation of the multinomial logit mixture using the EM algorithm. The algorithm exploits a careful initialization procedure (Papastamoulis et al., 2016) combined with a ridge-stabilized implementation of the Newton-Raphson
method (Goldfeld et al., 1966) in the M-step.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mix_mnm_logistic(y, X, Kmax = 10, maxIter = 100, emthreshold = 1e-08, 
	maxNR = 5, nCores, tsplit = 8, msplit = 5, split = TRUE, 
	shake = TRUE, random = TRUE, criterion = "ICL", 
	plotting = FALSE, R0 = 0.1, method = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mix_mnm_logistic_+3A_y">y</code></td>
<td>

<p>matrix of counts
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_x">X</code></td>
<td>

<p>design matrix (including constant term).
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_kmax">Kmax</code></td>
<td>

<p>Maximum number of mixture components. 
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_maxiter">maxIter</code></td>
<td>

<p>Maximum number of iterations. 
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_emthreshold">emthreshold</code></td>
<td>

<p>Minimum loglikelihood difference between successive iterations in order to terminate. 
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_maxnr">maxNR</code></td>
<td>

<p>maximum number of Newton Raphson iterations
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_ncores">nCores</code></td>
<td>

<p>number of cores for parallel computations. 
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_tsplit">tsplit</code></td>
<td>

<p>positive integer denoting the number of different runs for each call of the splitting small EM used by split-small EM initialization procedure.
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_msplit">msplit</code></td>
<td>

<p>positive integer denoting the number of different runs for each call of the splitting small EM.
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_split">split</code></td>
<td>

<p>Boolean indicating if the split initialization should be enabled in the small-EM scheme.
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_shake">shake</code></td>
<td>

<p>Boolean indicating if the shake initialization should be enabled in the small-EM scheme.
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_random">random</code></td>
<td>

<p>Boolean indicating if random initializations should be enabled in the small-EM scheme.
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_criterion">criterion</code></td>
<td>

<p>set to &quot;ICL&quot; to select the number of clusters according to the ICL criterion. 
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_plotting">plotting</code></td>
<td>

<p>Boolean for displaying intermediate graphical output. 
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_r0">R0</code></td>
<td>

<p>controls the step size of the update: smaller values result to larger step
sizes. See description in paper.
</p>
</td></tr>
<tr><td><code id="mix_mnm_logistic_+3A_method">method</code></td>
<td>

<p>this should be set to 5. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>estimated_K</code></td>
<td>
<p>selected value of the number of clusters.</p>
</td></tr>
<tr><td><code>all_runs</code></td>
<td>
<p>detailed output per run.</p>
</td></tr>
<tr><td><code>BIC_values</code></td>
<td>
<p>values of bayesian information criterion.</p>
</td></tr>
<tr><td><code>ICL_BIC_values</code></td>
<td>
<p>values of ICL-BIC.</p>
</td></tr>
<tr><td><code>estimated_clustering</code></td>
<td>
<p>Single best-clustering of the data, according to the MAP rule.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Papastamoulis P (2022). Model-based clustering of multinomial count data.  arXiv:2207.13984 [stat.ME]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#	Generate synthetic data

	K &lt;- 2
	p &lt;- 2
	D &lt;- 3
	n &lt;- 2
	set.seed(116)
	simData &lt;- simulate_multinomial_data(K = K, p = p, D = D, n = n, size = 20, prob = 0.025)   

	
	SplitShakeSmallEM &lt;- mix_mnm_logistic(y = simData$count_data, 
		X = simData$design_matrix, Kmax = 2, maxIter = 1, 
		emthreshold = 1e-8, maxNR = 1, nCores = 2, tsplit = 1, 
		msplit = 2, split = TRUE, R0 = 0.1, method = 5, 
		plotting = FALSE)
	#selected number of clusters
	SplitShakeSmallEM$estimated_K
	#estimated single best-clustering, according to MAP rule
	SplitShakeSmallEM$estimated_clustering
	# detailed output for all parameters of the selected number of clusters
	SplitShakeSmallEM$all_runs[[SplitShakeSmallEM$estimated_K]]
	
</code></pre>

<hr>
<h2 id='mixLoglikelihood_GLM'>
Log-likelihood of the multinomial logit. 
</h2><span id='topic+mixLoglikelihood_GLM'></span>

<h3>Description</h3>

<p>Log-likelihood of the multinomial logit. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixLoglikelihood_GLM(y, theta, pi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixLoglikelihood_GLM_+3A_y">y</code></td>
<td>

<p>matrix of counts
</p>
</td></tr>
<tr><td><code id="mixLoglikelihood_GLM_+3A_theta">theta</code></td>
<td>

<p>a three-dimensional array containing the multinomial probabilities per cluster, for each observation. 
</p>
</td></tr>
<tr><td><code id="mixLoglikelihood_GLM_+3A_pi">pi</code></td>
<td>

<p>a numeric vector of length K (the number of mixture components) containing the mixing proportions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Log-likelihood value.
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>

<hr>
<h2 id='multinomial_logistic_EM'>
Part of the EM algorithm for multinomial logit mixture 
</h2><span id='topic+multinomial_logistic_EM'></span>

<h3>Description</h3>

<p>Part of the EM algorithm for multinomial logit mixture 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinomial_logistic_EM(y, x, K, w_start, b_start, 
	maxIter = 1000, emthreshold = 1e-08, maxNR = 5, 
	nCores = NULL, verbose = FALSE, R0, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multinomial_logistic_EM_+3A_y">y</code></td>
<td>

<p>y
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_x">x</code></td>
<td>

<p>X
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_k">K</code></td>
<td>

<p>K
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_w_start">w_start</code></td>
<td>

<p>w
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_b_start">b_start</code></td>
<td>

<p>b
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_maxiter">maxIter</code></td>
<td>

<p>max
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_emthreshold">emthreshold</code></td>
<td>

<p>em
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_maxnr">maxNR</code></td>
<td>

<p>maxnr
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_ncores">nCores</code></td>
<td>

<p>nc
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_verbose">verbose</code></td>
<td>

<p>verb
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_r0">R0</code></td>
<td>

<p>or
</p>
</td></tr>
<tr><td><code id="multinomial_logistic_EM_+3A_method">method</code></td>
<td>

<p>method
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>value
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>

<hr>
<h2 id='multinomialLogitMix'>
Main function
</h2><span id='topic+multinomialLogitMix'></span>

<h3>Description</h3>

<p>The main function of the package. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinomialLogitMix(response, design_matrix, method, 
	Kmax = 10, mcmc_parameters = NULL, em_parameters = NULL, 
	nCores, splitSmallEM = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multinomialLogitMix_+3A_response">response</code></td>
<td>

<p>matrix of counts.
</p>
</td></tr>
<tr><td><code id="multinomialLogitMix_+3A_design_matrix">design_matrix</code></td>
<td>

<p>design matrix (including constant term). 
</p>
</td></tr>
<tr><td><code id="multinomialLogitMix_+3A_method">method</code></td>
<td>

<p>character with two possible values: &quot;EM&quot; or &quot;MCMC&quot; indicating the desired method in order to estimate the model. 
</p>
</td></tr>
<tr><td><code id="multinomialLogitMix_+3A_kmax">Kmax</code></td>
<td>

<p>number of components of the (overfitting) mixture model.
</p>
</td></tr>
<tr><td><code id="multinomialLogitMix_+3A_ncores">nCores</code></td>
<td>

<p>Total number of CPU cores for parallel processing.
</p>
</td></tr>
<tr><td><code id="multinomialLogitMix_+3A_mcmc_parameters">mcmc_parameters</code></td>
<td>
<p>List with the parameter set-up of the MCMC sampler. See details for changing the defaults.</p>
</td></tr>
<tr><td><code id="multinomialLogitMix_+3A_em_parameters">em_parameters</code></td>
<td>
<p>List with the parameter set-up of the EM algorithm. See details  for changing the defaults.</p>
</td></tr>
<tr><td><code id="multinomialLogitMix_+3A_splitsmallem">splitSmallEM</code></td>
<td>
<p>Boolean value, indicating whether the split-small EM scheme should be used to initialize the <code>method</code>. Default: true (suggested).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The details of the parameter setup of the EM algorithm and MCMC sampler. The following specification correspond to the minimal default settings. Larger values of <code>tsplit</code> will result to better performance. 
</p>
<p>em_parameters &lt;- list(maxIter = 100, emthreshold = 1e-08, 
maxNR = 10, tsplit = 16, msplit = 10, split = TRUE, 
R0 = 0.1, plotting = TRUE)
</p>
<p>mcmc_parameters &lt;- list(tau = 0.00035, nu2 = 100, mcmc_cycles = 2600, 
iter_per_cycle = 20, nChains = 8, dirPriorAlphas = c(1, 
1 + 5 * exp((seq(2, 14, length = nChains - 1)))/100)/(200), 
warm_up = 48000, checkAR = 500, probsSave = FALSE, 
showGraph = 100, ar_low = 0.15, ar_up = 0.25, burn = 100, 
thin = 1, withRandom = TRUE)
</p>


<h3>Value</h3>

<table>
<tr><td><code>EM</code></td>
<td>
<p>List with the results of the EM algorithm.</p>
</td></tr>
<tr><td><code>MCMC_raw</code></td>
<td>
<p>List with the raw output of the MCMC sampler - not identifiable MCMC output.</p>
</td></tr>
<tr><td><code>MCMC_post_processed</code></td>
<td>
<p>Post-processed MCMC, used for the inference.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Papastamoulis, P. Model based clustering of multinomial count data. Advances in Data Analysis and Classification (2023). https://doi.org/10.1007/s11634-023-00547-5
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#	Generate synthetic data

	K &lt;- 2	#number of clusters
	p &lt;- 2	#number of covariates (constant incl)
	D &lt;- 5	#number of categories
	n &lt;- 20 #generated number of observations
	set.seed(1)
	simData &lt;- simulate_multinomial_data(K = K, p = p, D = D, n = n, size = 20, prob = 0.025)   


	# EM parameters
em_parameters &lt;- list(maxIter = 100, emthreshold = 1e-08, 
    maxNR = 10, tsplit = 16, msplit = 10, split = TRUE, 
    R0 = 0.1, plotting = TRUE)

	#  MCMC parameters - just for illustration
	#	typically, set `mcmc_cycles` and `warm_up`to a larger values
	#	such as` mcmc_cycles = 2500` or more 
	#	and `warm_up = 40000` or more.
	nChains &lt;- 2 #(set this to a larger value, such as 8 or more)
	mcmc_parameters &lt;- list(tau = 0.00035, nu2 = 100, mcmc_cycles = 260, 
	    iter_per_cycle = 20, nChains = nChains, dirPriorAlphas = c(1, 
		1 + 5 * exp((seq(2, 14, length = nChains - 1)))/100)/(200), 
	    warm_up = 4800, checkAR = 500, probsSave = FALSE, 
	    showGraph = 100, ar_low = 0.15, ar_up = 0.25, burn = 100, 
	    thin = 1, withRandom = TRUE)

	# run EM with split-small-EM initialization, and then use the output to 
	#	initialize MCMC algorithm for an overfitting mixture with 
	#	Kmax = 5 components (max number of clusters - usually this is 
	#	set to a larger value, e.g. 10 or 20).
	#	Note: 
	#		1. the MCMC output is based on the non-empty components
	#		2. the EM algorithm clustering corresponds to the selected 
	#			number of clusters according to ICL.
	#		3. `nCores` should by adjusted according to your available cores.
	
	mlm &lt;- multinomialLogitMix(response = simData$count_data, 
		design_matrix = simData$design_matrix, method = "MCMC", 
             Kmax = 5, nCores = 2, splitSmallEM = TRUE, 
             mcmc_parameters = mcmc_parameters, em_parameters = em_parameters)
	# retrieve clustering according to EM
	mlm$EM$estimated_clustering
	# retrieve clustering according to MCMC
	mlm$MCMC_post_processed$cluster	
	
</code></pre>

<hr>
<h2 id='myDirichlet'>
Simulate from the Dirichlet distribution
</h2><span id='topic+myDirichlet'></span>

<h3>Description</h3>

<p>Generate a random draw from the Dirichlet distribution <code class="reqn">D(\alpha_1,\ldots,\alpha_k)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>myDirichlet(alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="myDirichlet_+3A_alpha">alpha</code></td>
<td>

<p>Parameter vector
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Simulated vector
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>

<hr>
<h2 id='newton_raphson_mstep'>
M-step of the EM algorithm
</h2><span id='topic+newton_raphson_mstep'></span>

<h3>Description</h3>

<p>Implements the maximization step of the EM algorithm based on a ridge-stabilized version of the Newton-Raphson algorithm, see Goldfeld et al. (1966). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>newton_raphson_mstep(y, X, b, w, maxNR = 5, R0 = 0.1, method = 5, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="newton_raphson_mstep_+3A_y">y</code></td>
<td>

<p>count data matrix
</p>
</td></tr>
<tr><td><code id="newton_raphson_mstep_+3A_x">X</code></td>
<td>

<p>design matrix (including const).
</p>
</td></tr>
<tr><td><code id="newton_raphson_mstep_+3A_b">b</code></td>
<td>

<p>coefficients of the multinomial logit mixture
</p>
</td></tr>
<tr><td><code id="newton_raphson_mstep_+3A_w">w</code></td>
<td>

<p>mixing proportions
</p>
</td></tr>
<tr><td><code id="newton_raphson_mstep_+3A_maxnr">maxNR</code></td>
<td>

<p>threshold
</p>
</td></tr>
<tr><td><code id="newton_raphson_mstep_+3A_r0">R0</code></td>
<td>

<p>inital value for the parameter that controls the step-size of the update. 
</p>
</td></tr>
<tr><td><code id="newton_raphson_mstep_+3A_method">method</code></td>
<td>

<p>set to 5. Always.
</p>
</td></tr>
<tr><td><code id="newton_raphson_mstep_+3A_verbose">verbose</code></td>
<td>

<p>Boolean.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>b</code></td>
<td>
<p>coefficients</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>theta values</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>
<p>log-likelihood.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Goldfeld, S. M., Quandt, R. E., and Trotter, H. F. (1966). Maximization by quadratic hill-climbing. Econometrica: Journal of the Econometric Society, 541-551.
</p>

<hr>
<h2 id='shakeEM_GLM'>
Shake-small EM
</h2><span id='topic+shakeEM_GLM'></span>

<h3>Description</h3>

<p>Assume that there are
at least two clusters in the fitted model. We randomly select 2 of them and propose to randomly re-allocate the assigned observations within those 2 clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shakeEM_GLM(y, x, K, equalModel, tsplit = 10, maxIter = 20, 
	emthreshold = 1e-08, maxNR = 5, nCores, 
	split = TRUE, R0, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shakeEM_GLM_+3A_y">y</code></td>
<td>

<p>y
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_x">x</code></td>
<td>

<p>X
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_k">K</code></td>
<td>

<p>K
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_equalmodel">equalModel</code></td>
<td>

<p>eq
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_tsplit">tsplit</code></td>
<td>

<p>tsplit
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_maxiter">maxIter</code></td>
<td>

<p>maxiter
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_emthreshold">emthreshold</code></td>
<td>

<p>em
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_maxnr">maxNR</code></td>
<td>

<p>max
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_ncores">nCores</code></td>
<td>

<p>nc
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_split">split</code></td>
<td>

<p>spl
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_r0">R0</code></td>
<td>

<p>ro
</p>
</td></tr>
<tr><td><code id="shakeEM_GLM_+3A_method">method</code></td>
<td>

<p>met
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>valu</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis</p>

<hr>
<h2 id='simulate_multinomial_data'>
Synthetic data generator
</h2><span id='topic+simulate_multinomial_data'></span>

<h3>Description</h3>

<p>This function simulates data from mixture of multinomial logistic regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_multinomial_data(K, p, D, n, size = 20, prob = 0.025, betaTrue = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_multinomial_data_+3A_k">K</code></td>
<td>

<p>Number of clusters.
</p>
</td></tr>
<tr><td><code id="simulate_multinomial_data_+3A_p">p</code></td>
<td>

<p>Number of covariates, including constant.
</p>
</td></tr>
<tr><td><code id="simulate_multinomial_data_+3A_d">D</code></td>
<td>
<p>Number of multinomial categories.</p>
</td></tr>
<tr><td><code id="simulate_multinomial_data_+3A_n">n</code></td>
<td>
<p>Number of data points to simulate.</p>
</td></tr>
<tr><td><code id="simulate_multinomial_data_+3A_size">size</code></td>
<td>
<p>Negative Binomial parameter (number of successes). Default: 20.</p>
</td></tr>
<tr><td><code id="simulate_multinomial_data_+3A_prob">prob</code></td>
<td>
<p>Negative Binomial parameter (probability of success). Default: 0.025.</p>
</td></tr>
<tr><td><code id="simulate_multinomial_data_+3A_betatrue">betaTrue</code></td>
<td>
<p>An array which contains the true values of the logit coefficients per cluster. Default: randomly generated values.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>count_data</code></td>
<td>
<p>matrix of data counts.</p>
</td></tr>
<tr><td><code>design_matrix</code></td>
<td>
<p>design matrix.</p>
</td></tr>
<tr><td><code>clustering</code></td>
<td>
<p>Ground-truth partition of the data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>

<hr>
<h2 id='splitEM_GLM'>
Split-small EM scheme.
</h2><span id='topic+splitEM_GLM'></span>

<h3>Description</h3>

<p>Split two randomly selected clusters based on a model with one component smaller than the current one. This procedure is repeated within a small-EM scheme. The best split is chose to initialize the model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitEM_GLM(y, x, K, smallerModel, tsplit = 10, maxIter = 20, 
	emthreshold = 1e-08, maxNR = 5, nCores, 
	split = TRUE, R0, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="splitEM_GLM_+3A_y">y</code></td>
<td>

<p>y
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_x">x</code></td>
<td>

<p>x
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_k">K</code></td>
<td>

<p>k
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_smallermodel">smallerModel</code></td>
<td>

<p>smla
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_tsplit">tsplit</code></td>
<td>

<p>tsp
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_maxiter">maxIter</code></td>
<td>

<p>max
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_emthreshold">emthreshold</code></td>
<td>

<p>thr
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_maxnr">maxNR</code></td>
<td>

<p>maxn
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_ncores">nCores</code></td>
<td>

<p>nc
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_split">split</code></td>
<td>

<p>spi
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_r0">R0</code></td>
<td>

<p>ro
</p>
</td></tr>
<tr><td><code id="splitEM_GLM_+3A_method">method</code></td>
<td>

<p>meth
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>val
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Papastamoulis, P., Martin-Magniette, M. L., and Maugis-Rabusseau, C. (2016). On the estimation of mixtures of Poisson regression models with large number of components. Computational Statistics &amp; Data Analysis, 93, 97-106.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
