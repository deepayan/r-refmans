<!DOCTYPE html><html><head><title>Help for package irlba</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {irlba}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#irlba'><p>Find a few approximate singular values and corresponding</p>
singular vectors of a matrix.</a></li>
<li><a href='#partial_eigen'><p>Find a few approximate largest eigenvalues and corresponding eigenvectors of a symmetric matrix.</p></a></li>
<li><a href='#prcomp_irlba'><p>Principal Components Analysis</p></a></li>
<li><a href='#ssvd'><p>Sparse regularized low-rank matrix approximation.</p></a></li>
<li><a href='#summary.irlba_prcomp'><p>Summary method for truncated pca objects computed by <code>prcomp_irlba</code>.</p></a></li>
<li><a href='#svdr'><p>Find a few approximate largest singular values and corresponding</p>
singular vectors of a matrix.</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast Truncated Singular Value Decomposition and Principal
Components Analysis for Large Dense and Sparse Matrices</td>
</tr>
<tr>
<td>Version:</td>
<td>2.3.5.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-12-05</td>
</tr>
<tr>
<td>Description:</td>
<td>Fast and memory efficient methods for truncated singular value
    decomposition and principal components analysis of large sparse and dense
    matrices.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.2), Matrix</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Matrix</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, methods</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bwlewis/irlba/issues">https://github.com/bwlewis/irlba/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>5.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-10-03 15:17:05 UTC; ripley</td>
</tr>
<tr>
<td>Author:</td>
<td>Jim Baglama [aut, cph],
  Lothar Reichel [aut, cph],
  B. W. Lewis [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>B. W. Lewis &lt;blewis@illposed.net&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-10-03 18:38:49 UTC</td>
</tr>
</table>
<hr>
<h2 id='irlba'>Find a few approximate singular values and corresponding
singular vectors of a matrix.</h2><span id='topic+irlba'></span>

<h3>Description</h3>

<p>The augmented implicitly restarted Lanczos bidiagonalization algorithm
(IRLBA) finds a few approximate largest (or, optionally, smallest) singular
values and corresponding
singular vectors of a sparse or dense matrix using a method of Baglama and
Reichel.  It is a fast and memory-efficient way to compute a partial SVD.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>irlba(A, nv = 5, nu = nv, maxit = 1000, work = nv + 7, reorth = TRUE,
  tol = 1e-05, v = NULL, right_only = FALSE, verbose = FALSE,
  scale = NULL, center = NULL, shift = NULL, mult = NULL,
  fastpath = TRUE, svtol = tol, smallest = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="irlba_+3A_a">A</code></td>
<td>
<p>numeric real- or complex-valued matrix or real-valued sparse matrix.</p>
</td></tr>
<tr><td><code id="irlba_+3A_nv">nv</code></td>
<td>
<p>number of right singular vectors to estimate.</p>
</td></tr>
<tr><td><code id="irlba_+3A_nu">nu</code></td>
<td>
<p>number of left singular vectors to estimate (defaults to <code>nv</code>).</p>
</td></tr>
<tr><td><code id="irlba_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="irlba_+3A_work">work</code></td>
<td>
<p>working subspace dimension, larger values can speed convergence at the cost of more memory use.</p>
</td></tr>
<tr><td><code id="irlba_+3A_reorth">reorth</code></td>
<td>
<p>if <code>TRUE</code>, apply full reorthogonalization to both SVD bases, otherwise
only apply reorthogonalization to the right SVD basis vectors; the latter case is cheaper per
iteration but, overall, may require more iterations for convergence. Automatically <code>TRUE</code>
when <code>fastpath=TRUE</code> (see below).</p>
</td></tr>
<tr><td><code id="irlba_+3A_tol">tol</code></td>
<td>
<p>convergence is determined when <code class="reqn">\|A^TU - VS\| &lt; tol\|A\|</code>,
and when the maximum relative change in estimated singular values from one iteration to the
next is less than <code>svtol = tol</code> (see <code>svtol</code> below),
where the spectral norm ||A|| is approximated by the
largest estimated singular value, and U, V, S are the matrices corresponding
to the estimated left and right singular vectors, and diagonal matrix of
estimated singular values, respectively.</p>
</td></tr>
<tr><td><code id="irlba_+3A_v">v</code></td>
<td>
<p>optional starting vector or output from a previous run of <code>irlba</code> used
to restart the algorithm from where it left off (see the notes).</p>
</td></tr>
<tr><td><code id="irlba_+3A_right_only">right_only</code></td>
<td>
<p>logical value indicating return only the right singular vectors
(<code>TRUE</code>) or both sets of vectors (<code>FALSE</code>). The right_only option can be
cheaper to compute and use much less memory when <code>nrow(A) &gt;&gt; ncol(A)</code> but note
that obtained solutions typically lose accuracy due to lack of re-orthogonalization in the
algorithm and that <code>right_only = TRUE</code> sets <code>fastpath = FALSE</code> (only use this option
for really large problems that run out of memory and when <code>nrow(A) &gt;&gt; ncol(A)</code>).
Consider increasing the <code>work</code> option to improve accuracy with <code>right_only=TRUE</code>.</p>
</td></tr>
<tr><td><code id="irlba_+3A_verbose">verbose</code></td>
<td>
<p>logical value that when <code>TRUE</code> prints status messages during the computation.</p>
</td></tr>
<tr><td><code id="irlba_+3A_scale">scale</code></td>
<td>
<p>optional column scaling vector whose values divide each column of <code>A</code>;
must be as long as the number of columns of <code>A</code> (see notes).</p>
</td></tr>
<tr><td><code id="irlba_+3A_center">center</code></td>
<td>
<p>optional column centering vector whose values are subtracted from each
column of <code>A</code>; must be as long as the number of columns of <code>A</code> and may
not be used together with the deflation options below (see notes).</p>
</td></tr>
<tr><td><code id="irlba_+3A_shift">shift</code></td>
<td>
<p>optional shift value (square matrices only, see notes).</p>
</td></tr>
<tr><td><code id="irlba_+3A_mult">mult</code></td>
<td>
<p>DEPRECATED optional custom matrix multiplication function (default is <code>%*%</code>, see notes).</p>
</td></tr>
<tr><td><code id="irlba_+3A_fastpath">fastpath</code></td>
<td>
<p>try a fast C algorithm implementation if possible; set <code>fastpath=FALSE</code> to use the
reference R implementation. See the notes for more details.</p>
</td></tr>
<tr><td><code id="irlba_+3A_svtol">svtol</code></td>
<td>
<p>additional stopping tolerance on maximum allowed absolute relative change across each
estimated singular value between iterations.
The default value of this parameter is to set it to <code>tol</code>. You can set <code>svtol=Inf</code> to
effectively disable this stopping criterion. Setting <code>svtol=Inf</code> allows the method to
terminate on the first Lanczos iteration if it finds an invariant subspace, but with less certainty
that the converged subspace is the desired one. (It may, for instance, miss some of the largest
singular values in difficult problems.)</p>
</td></tr>
<tr><td><code id="irlba_+3A_smallest">smallest</code></td>
<td>
<p>set <code>smallest=TRUE</code> to estimate the smallest singular values and associated
singular vectors. WARNING: this option is somewhat experimental, and may produce poor
estimates for ill-conditioned matrices.</p>
</td></tr>
<tr><td><code id="irlba_+3A_...">...</code></td>
<td>
<p>optional additional arguments used to support experimental and deprecated features.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with entries:
</p>

<dl>
<dt>d:</dt><dd><p> max(nu, nv) approximate singular values</p>
</dd>
<dt>u:</dt><dd><p> nu approximate left singular vectors (only when right_only=FALSE)</p>
</dd>
<dt>v:</dt><dd><p> nv approximate right singular vectors</p>
</dd>
<dt>iter:</dt><dd><p> The number of Lanczos iterations carried out</p>
</dd>
<dt>mprod:</dt><dd><p> The total number of matrix vector products carried out</p>
</dd>
</dl>



<h3>Note</h3>

<p>The syntax of <code>irlba</code> partially follows <code>svd</code>, with an important
exception. The usual R <code>svd</code> function always returns a complete set of
singular values, even if the number of singular vectors <code>nu</code> or <code>nv</code>
is set less than the maximum. The <code>irlba</code> function returns a number of
estimated singular values equal to the maximum of the number of specified
singular vectors <code>nu</code> and <code>nv</code>.
</p>
<p>Use the optional <code>scale</code> parameter to implicitly scale each column of
the matrix <code>A</code> by the values in the <code>scale</code> vector, computing the
truncated SVD of the column-scaled <code>sweep(A, 2, scale, FUN=`/`)</code>, or
equivalently, <code>A %*% diag(1 / scale)</code>, without explicitly forming the
scaled matrix. <code>scale</code> must be a non-zero vector of length equal
to the number of columns of <code>A</code>.
</p>
<p>Use the optional <code>center</code> parameter to implicitly subtract the values
in the <code>center</code> vector from each column of <code>A</code>, computing the
truncated SVD of <code>sweep(A, 2, center, FUN=`-`)</code>,
without explicitly forming the centered matrix. <code>center</code>
must be a vector of length equal to the number of columns of <code>A</code>.
This option may be used to efficiently compute principal components without
explicitly forming the centered matrix (which can, importantly, preserve
sparsity in the matrix). See the examples.
</p>
<p>The optional <code>shift</code> scalar valued argument applies only to square matrices; use it
to estimate the partial svd of <code>A + diag(shift, nrow(A), nrow(A))</code>
(without explicitly forming the shifted matrix).
</p>
<p>(Deprecated) Specify an optional alternative matrix multiplication operator in the
<code>mult</code> parameter. <code>mult</code> must be a function of two arguments,
and must handle both cases where one argument is a vector and the other
a matrix. This option is deprecated and will be removed in a future version.
The new preferred method simply uses R itself to define a custom matrix class
with your user-defined matrix multiplication operator. See the examples.
</p>
<p>Use the <code>v</code> option to supply a starting vector for the iterative
method. A random vector is used by default (precede with <code>set.seed()</code>
for reproducibility). Optionally set <code>v</code> to
the output of a previous run of <code>irlba</code> to restart the method, adding
additional singular values/vectors without recomputing the solution
subspace. See the examples.
</p>
<p>The function may generate the following warnings:
</p>

<ul>
<li><p>&quot;did not converge&ndash;results might be invalid!; try increasing work or maxit&quot;
means that the algorithm didn't
converge &ndash; this is potentially a serious problem and the returned results may not be valid. <code>irlba</code>
reports a warning here instead of an error so that you can inspect whatever is returned. If this
happens, carefully heed the warning and inspect the result. You may also try setting <code>fastpath=FALSE</code>.
</p>
</li>
<li><p>&quot;You're computing a large percentage of total singular values, standard svd might work better!&quot;
<code>irlba</code> is designed to efficiently compute a few of the largest singular values and associated
singular vectors of a matrix. The standard <code>svd</code> function will be more efficient for computing
large numbers of singular values than <code>irlba</code>.
</p>
</li>
<li><p>&quot;convergence criterion below machine epsilon&quot; means that the product of <code>tol</code> and the
largest estimated singular value is really small and the normal convergence criterion is only
met up to round off error.
</p>
</li></ul>

<p>The function might return an error for several reasons including a situation when the starting
vector <code>v</code> is near the null space of the matrix. In that case, try a different <code>v</code>.
</p>
<p>The <code>fastpath=TRUE</code> option only supports real-valued matrices and sparse matrices
of type <code>dgCMatrix</code> (for now). Other problems fall back to the reference
R implementation.
</p>


<h3>References</h3>

<p>Baglama, James, and Lothar Reichel. &quot;Augmented implicitly restarted Lanczos bidiagonalization methods.&quot; SIAM Journal on Scientific Computing 27.1 (2005): 19-42.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+svd">svd</a></code>, <code><a href="stats.html#topic+prcomp">prcomp</a></code>, <code><a href="#topic+partial_eigen">partial_eigen</a></code>, <code><a href="#topic+svdr">svdr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)

A &lt;- matrix(runif(400), nrow=20)
S &lt;- irlba(A, 3)
S$d

# Compare with svd
svd(A)$d[1:3]

# Restart the algorithm to compute more singular values
# (starting with an existing solution S)
S1 &lt;- irlba(A, 5, v=S)

# Estimate smallest singular values
irlba(A, 3, smallest=TRUE)$d

#Compare with
tail(svd(A)$d, 3)

# Principal components (see also prcomp_irlba)
P &lt;- irlba(A, nv=1, center=colMeans(A))

# Compare with prcomp and prcomp_irlba (might vary up to sign)
cbind(P$v,
      prcomp(A)$rotation[, 1],
      prcomp_irlba(A)$rotation[, 1])

# A custom matrix multiplication function that scales the columns of A
# (cf the scale option). This function scales the columns of A to unit norm.
col_scale &lt;- sqrt(apply(A, 2, crossprod))
setClass("scaled_matrix", contains="matrix", slots=c(scale="numeric"))
setMethod("%*%", signature(x="scaled_matrix", y="numeric"),
   function(x ,y) x@.Data %*% (y / x@scale))
setMethod("%*%", signature(x="numeric", y="scaled_matrix"),
   function(x ,y) (x %*% y@.Data) / y@scale)
a &lt;- new("scaled_matrix", A, scale=col_scale)
irlba(a, 3)$d

# Compare with:
svd(sweep(A, 2, col_scale, FUN=`/`))$d[1:3]


</code></pre>

<hr>
<h2 id='partial_eigen'>Find a few approximate largest eigenvalues and corresponding eigenvectors of a symmetric matrix.</h2><span id='topic+partial_eigen'></span>

<h3>Description</h3>

<p>Use <code>partial_eigen</code> to estimate a subset of the largest (most positive)
eigenvalues and corresponding eigenvectors of a symmetric dense or sparse
real-valued matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial_eigen(x, n = 5, symmetric = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial_eigen_+3A_x">x</code></td>
<td>
<p>numeric real-valued dense or sparse matrix.</p>
</td></tr>
<tr><td><code id="partial_eigen_+3A_n">n</code></td>
<td>
<p>number of largest eigenvalues and corresponding eigenvectors to compute.</p>
</td></tr>
<tr><td><code id="partial_eigen_+3A_symmetric">symmetric</code></td>
<td>
<p><code>TRUE</code> indicates <code>x</code> is a symmetric matrix (the default);
specify <code>symmetric=FALSE</code> to compute the largest eigenvalues and corresponding
eigenvectors of <code>t(x) %*% x</code> instead.</p>
</td></tr>
<tr><td><code id="partial_eigen_+3A_...">...</code></td>
<td>
<p>optional additional parameters passed to the <code>irlba</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with entries:
</p>

<ul>
<li><p>values n approximate largest eigenvalues
</p>
</li>
<li><p>vectors n approximate corresponding eigenvectors
</p>
</li></ul>



<h3>Note</h3>

<p>Specify <code>symmetric=FALSE</code> to compute the largest <code>n</code> eigenvalues
and corresponding eigenvectors of the symmetric matrix cross-product
<code>t(x) %*% x</code>.
</p>
<p>This function uses the <code>irlba</code> function under the hood. See <code>?irlba</code>
for description of additional options, especially the <code>tol</code> parameter.
</p>
<p>See the RSpectra package https://cran.r-project.org/package=RSpectra for more comprehensive
partial eigenvalue decomposition.
</p>


<h3>References</h3>

<p>Augmented Implicitly Restarted Lanczos Bidiagonalization Methods, J. Baglama and L. Reichel, SIAM J. Sci. Comput. 2005.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+eigen">eigen</a></code>, <code><a href="#topic+irlba">irlba</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
# Construct a symmetric matrix with some positive and negative eigenvalues:
V &lt;- qr.Q(qr(matrix(runif(100), nrow=10)))
x &lt;- V %*% diag(c(10, -9, 8, -7, 6, -5, 4, -3, 2, -1)) %*% t(V)
partial_eigen(x, 3)$values

# Compare with eigen
eigen(x)$values[1:3]

# Use symmetric=FALSE to compute the eigenvalues of t(x) %*% x for general
# matrices x:
x &lt;- matrix(rnorm(100), 10)
partial_eigen(x, 3, symmetric=FALSE)$values
eigen(crossprod(x))$values

</code></pre>

<hr>
<h2 id='prcomp_irlba'>Principal Components Analysis</h2><span id='topic+prcomp_irlba'></span>

<h3>Description</h3>

<p>Efficient computation of a truncated principal components analysis of a given data matrix
using an implicitly restarted Lanczos method from the <code><a href="#topic+irlba">irlba</a></code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prcomp_irlba(x, n = 3, retx = TRUE, center = TRUE, scale. = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prcomp_irlba_+3A_x">x</code></td>
<td>
<p>a numeric or complex matrix (or data frame) which provides
the data for the principal components analysis.</p>
</td></tr>
<tr><td><code id="prcomp_irlba_+3A_n">n</code></td>
<td>
<p>integer number of principal component vectors to return, must be less than
<code>min(dim(x))</code>.</p>
</td></tr>
<tr><td><code id="prcomp_irlba_+3A_retx">retx</code></td>
<td>
<p>a logical value indicating whether the rotated variables should be returned.</p>
</td></tr>
<tr><td><code id="prcomp_irlba_+3A_center">center</code></td>
<td>
<p>a logical value indicating whether the variables should be
shifted to be zero centered. Alternately, a centering vector of length
equal the number of columns of <code>x</code> can be supplied.</p>
</td></tr>
<tr><td><code id="prcomp_irlba_+3A_scale.">scale.</code></td>
<td>
<p>a logical value indicating whether the variables should be
scaled to have unit variance before the analysis takes place.
The default is <code>FALSE</code> for consistency with S, but scaling is often advisable.
Alternatively, a vector of length equal the number of columns of <code>x</code> can be supplied.
</p>
<p>The value of <code>scale</code> determines how column scaling is performed
(after centering).  If <code>scale</code> is a numeric vector with length
equal to the number of columns of <code>x</code>, then each column of <code>x</code> is
divided by the corresponding value from <code>scale</code>.  If <code>scale</code> is
<code>TRUE</code> then scaling is done by dividing the (centered) columns of
<code>x</code> by their standard deviations if <code>center=TRUE</code>, and the
root mean square otherwise.  If <code>scale</code> is <code>FALSE</code>, no scaling is done.
See <code><a href="base.html#topic+scale">scale</a></code> for more details.</p>
</td></tr>
<tr><td><code id="prcomp_irlba_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="#topic+irlba">irlba</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class &quot;prcomp&quot; containing the following components:
</p>

<ul>
<li><p>sdev the standard deviations of the principal components (i.e.,
the square roots of the eigenvalues of the
covariance/correlation matrix, though the calculation is
actually done with the singular values of the data matrix).
</p>
</li>
<li><p>rotation the matrix of variable loadings (i.e., a matrix whose columns
contain the eigenvectors).
</p>
</li>
<li> <p>x if <code>retx</code> is <code>TRUE</code> the value of the rotated data (the centred
(and scaled if requested) data multiplied by the <code>rotation</code>
matrix) is returned.  Hence, <code>cov(x)</code> is the diagonal matrix
<code>diag(sdev^2)</code>.
</p>
</li>
<li><p>center, scale the centering and scaling used, or <code>FALSE</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>The signs of the columns of the rotation matrix are arbitrary, and
so may differ between different programs for PCA, and even between
different builds of R.
</p>
<p>NOTE DIFFERENCES WITH THE DEFAULT <code><a href="stats.html#topic+prcomp">prcomp</a></code> FUNCTION!
The <code>tol</code> truncation argument found in <code>prcomp</code> is not supported.
In place of the truncation tolerance in the original function, the
<code>prcomp_irlba</code>  function has the argument <code>n</code> explicitly giving the
number of principal components to return. A warning is generated if the
argument <code>tol</code> is used, which is interpreted differently between
the two functions.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+prcomp">prcomp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x  &lt;- matrix(rnorm(200), nrow=20)
p1 &lt;- prcomp_irlba(x, n=3)
summary(p1)

# Compare with
p2 &lt;- prcomp(x, tol=0.7)
summary(p2)


</code></pre>

<hr>
<h2 id='ssvd'>Sparse regularized low-rank matrix approximation.</h2><span id='topic+ssvd'></span>

<h3>Description</h3>

<p>Estimate an <code class="reqn">{\ell}1</code>-penalized
singular value or principal components decomposition (SVD or PCA) that introduces sparsity in the
right singular vectors based on the fast and memory-efficient
sPCA-rSVD algorithm of Haipeng Shen and Jianhua Huang.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ssvd(x, k = 1, n = 2, maxit = 500, tol = 0.001, center = FALSE,
  scale. = FALSE, alpha = 0, tsvd = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ssvd_+3A_x">x</code></td>
<td>
<p>A numeric real- or complex-valued matrix or real-valued sparse matrix.</p>
</td></tr>
<tr><td><code id="ssvd_+3A_k">k</code></td>
<td>
<p>Matrix rank of the computed decomposition (see the Details section below).</p>
</td></tr>
<tr><td><code id="ssvd_+3A_n">n</code></td>
<td>
<p>Number of nonzero components in the right singular vectors. If <code>k &gt; 1</code>,
then a single value of <code>n</code> specifies the number of nonzero components
in each regularized right singular vector. Or, specify a vector of length
<code>k</code> indicating the number of desired nonzero components in each
returned vector. See the examples.</p>
</td></tr>
<tr><td><code id="ssvd_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of soft-thresholding iterations.</p>
</td></tr>
<tr><td><code id="ssvd_+3A_tol">tol</code></td>
<td>
<p>Convergence is determined when <code class="reqn">\|U_j - U_{j-1}\|_F &lt; tol</code>, where <code class="reqn">U_j</code> is the matrix of estimated left regularized singular vectors at iteration <code class="reqn">j</code>.</p>
</td></tr>
<tr><td><code id="ssvd_+3A_center">center</code></td>
<td>
<p>a logical value indicating whether the variables should be
shifted to be zero centered. Alternately, a centering vector of length
equal the number of columns of <code>x</code> can be supplied. Use <code>center=TRUE</code>
to perform a regularized sparse PCA.</p>
</td></tr>
<tr><td><code id="ssvd_+3A_scale.">scale.</code></td>
<td>
<p>a logical value indicating whether the variables should be
scaled to have unit variance before the analysis takes place.
Alternatively, a vector of length equal the number of columns of <code>x</code> can be supplied.
</p>
<p>The value of <code>scale</code> determines how column scaling is performed
(after centering).  If <code>scale</code> is a numeric vector with length
equal to the number of columns of <code>x</code>, then each column of <code>x</code> is
divided by the corresponding value from <code>scale</code>.  If <code>scale</code> is
<code>TRUE</code> then scaling is done by dividing the (centered) columns of
<code>x</code> by their standard deviations if <code>center=TRUE</code>, and the
root mean square otherwise.  If <code>scale</code> is <code>FALSE</code>, no scaling is done.
See <code><a href="base.html#topic+scale">scale</a></code> for more details.</p>
</td></tr>
<tr><td><code id="ssvd_+3A_alpha">alpha</code></td>
<td>
<p>Optional  scalar regularization parameter between zero and one (see Details below).</p>
</td></tr>
<tr><td><code id="ssvd_+3A_tsvd">tsvd</code></td>
<td>
<p>Optional initial rank-k truncated SVD or PCA (skips computation if supplied).</p>
</td></tr>
<tr><td><code id="ssvd_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="#topic+irlba">irlba</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>ssvd</code> function implements a version of an algorithm by
Shen and Huang that computes a penalized SVD or PCA that introduces
sparsity in the right singular vectors by solving a penalized least squares problem.
The algorithm in the rank 1 case finds vectors <code class="reqn">u, w</code> that minimize
</p>
<p style="text-align: center;"><code class="reqn">\|x - u w^T\|_F^2 + \lambda \|w\|_1</code>
</p>

<p>such that <code class="reqn">\|u\| = 1</code>,
and then sets <code class="reqn">v = w / \|w\|</code> and
<code class="reqn">d = u^T x v</code>;
see the referenced paper for details. The penalty <code class="reqn">\lambda</code> is
implicitly determined from the specified desired number of nonzero values <code>n</code>.
Higher rank output is determined similarly
but using a sequence of <code class="reqn">\lambda</code> values determined to maintain the desired number
of nonzero elements in each column of <code>v</code> specified by <code>n</code>.
Unlike standard SVD or PCA, the columns of the returned <code>v</code> when <code>k &gt; 1</code> may not be orthogonal.
</p>


<h3>Value</h3>

<p>A list containing the following components:
</p>

<ul>
<li><p>u regularized left singular vectors with orthonormal columns
</p>
</li>
<li><p>d regularized upper-triangluar projection matrix so that <code>x %*% v == u %*% d</code>
</p>
</li>
<li><p>v regularized, sparse right singular vectors with columns of unit norm
</p>
</li>
<li><p>center, scale the centering and scaling used, if any
</p>
</li>
<li><p>lambda the per-column regularization parameter found to obtain the desired sparsity
</p>
</li>
<li><p>iter number of soft thresholding iterations
</p>
</li>
<li><p>n value of input parameter <code>n</code>
</p>
</li>
<li><p>alpha value of input parameter <code>alpha</code>
</p>
</li></ul>



<h3>Note</h3>

<p>Our <code>ssvd</code> implementation of the Shen-Huang method makes the following choices:
</p>

<ol>
<li><p>The l1 penalty is the only available penalty function. Other penalties may appear in the future.
</p>
</li>
<li><p>Given a desired number of nonzero elements in <code>v</code>, value(s) for the <code class="reqn">\lambda</code>
penalty are determined to achieve the sparsity goal subject to the parameter <code>alpha</code>.
</p>
</li>
<li><p>An experimental block implementation is used for results with rank greater than 1 (when <code>k &gt; 1</code>)
instead of the deflation method described in the reference.
</p>
</li>
<li><p>The choice of a penalty lambda associated with a given number of desired nonzero
components is not unique. The <code>alpha</code> parameter, a scalar between zero and one,
selects any possible value of lambda that produces the desired number of
nonzero entries. The default <code>alpha = 0</code> selects a penalized solution with
largest corresponding value of <code>d</code> in the 1-d case. Think of <code>alpha</code> as
fine-tuning of the penalty.
</p>
</li>
<li><p>Our method returns an upper-triangular matrix <code>d</code> when <code>k &gt; 1</code> so
that <code>x %*% v == u %*% d</code>. Non-zero
elements above the diagonal result from non-orthogonality of the <code>v</code> matrix,
providing a simple interpretation of cumulative information, or explained variance
in the PCA case, via the singular value decomposition of <code>d %*% t(v)</code>.
</p>
</li></ol>

<p>What if you have no idea for values of the argument <code>n</code> (the desired sparsity)?
The reference describes a cross-validation and an ad-hoc approach; neither of which are
in the package yet. Both are prohibitively computationally expensive for matrices with a huge
number of columns. A future version of this package will include a revised approach to
automatically selecting a reasonable sparsity constraint.
</p>
<p>Compare with the similar but more general functions <code>SPC</code> and <code>PMD</code> in the <code>PMA</code> package
by Daniela M. Witten, Robert Tibshirani, Sam Gross, and Balasubramanian Narasimhan.
The <code>PMD</code> function can compute low-rank regularized matrix decompositions with sparsity penalties
on both the <code>u</code> and <code>v</code> vectors. The <code>ssvd</code> function is
similar to the PMD(*, L1) method invocation of <code>PMD</code> or alternatively the <code>SPC</code> function.
Although less general than <code>PMD</code>(*),
the <code>ssvd</code> function can be faster and more memory efficient for the
basic sparse PCA problem.
See <a href="https://bwlewis.github.io/irlba/ssvd.html">https://bwlewis.github.io/irlba/ssvd.html</a> for more information.
</p>
<p>(* Note that the s4vd package by Martin Sill and Sebastian Kaiser, <a href="https://cran.r-project.org/package=s4vd">https://cran.r-project.org/package=s4vd</a>,
includes a fast optimized version of a closely related algorithm by Shen, Huang, and Marron, that penalizes
both <code>u</code> and <code>v</code>.)
</p>


<h3>References</h3>


<ul>
<li><p>Shen, Haipeng, and Jianhua Z. Huang. &quot;Sparse principal component analysis via regularized low rank matrix approximation.&quot; Journal of multivariate analysis 99.6 (2008): 1015-1034.
</p>
</li>
<li><p>Witten, Tibshirani and Hastie (2009) A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis. _Biostatistics_ 10(3): 515-534.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
u &lt;- matrix(rnorm(200), ncol=1)
v &lt;- matrix(c(runif(50, min=0.1), rep(0,250)), ncol=1)
u &lt;- u / drop(sqrt(crossprod(u)))
v &lt;- v / drop(sqrt(crossprod(v)))
x &lt;- u %*% t(v) + 0.001 * matrix(rnorm(200*300), ncol=300)
s &lt;- ssvd(x, n=50)
table(actual=v[, 1] != 0, estimated=s$v[, 1] != 0)
oldpar &lt;- par(mfrow=c(2, 1))
plot(u, cex=2, main="u (black circles), Estimated u (blue discs)")
points(s$u, pch=19, col=4)
plot(v, cex=2, main="v (black circles), Estimated v (blue discs)")
points(s$v, pch=19, col=4)

# Let's consider a trivial rank-2 example (k=2) with noise. Like the
# last example, we know the exact number of nonzero elements in each
# solution vector of the noise-free matrix. Note the application of
# different sparsity constraints on each column of the estimated v.
# Also, the decomposition is unique only up to sign, which we adjust
# for below.
set.seed(1)
u &lt;- qr.Q(qr(matrix(rnorm(400), ncol=2)))
v &lt;- matrix(0, ncol=2, nrow=300)
v[sample(300, 15), 1] &lt;- runif(15, min=0.1)
v[sample(300, 50), 2] &lt;- runif(50, min=0.1)
v &lt;- qr.Q(qr(v))
x &lt;- u %*% (c(2, 1) * t(v)) + .001 * matrix(rnorm(200 * 300), 200)
s &lt;- ssvd(x, k=2, n=colSums(v != 0))

# Compare actual and estimated vectors (adjusting for sign):
s$u &lt;- sign(u) * abs(s$u)
s$v &lt;- sign(v) * abs(s$v)
table(actual=v[, 1] != 0, estimated=s$v[, 1] != 0)
table(actual=v[, 2] != 0, estimated=s$v[, 2] != 0)
plot(v[, 1], cex=2, main="True v1 (black circles), Estimated v1 (blue discs)")
points(s$v[, 1], pch=19, col=4)
plot(v[, 2], cex=2, main="True v2 (black circles), Estimated v2 (blue discs)")
points(s$v[, 2], pch=19, col=4)
par(oldpar)

</code></pre>

<hr>
<h2 id='summary.irlba_prcomp'>Summary method for truncated pca objects computed by <code>prcomp_irlba</code>.</h2><span id='topic+summary.irlba_prcomp'></span>

<h3>Description</h3>

<p>Summary method for truncated pca objects computed by <code>prcomp_irlba</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'irlba_prcomp'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.irlba_prcomp_+3A_object">object</code></td>
<td>
<p>An object returned by <code>prcomp_irlba</code>.</p>
</td></tr>
<tr><td><code id="summary.irlba_prcomp_+3A_...">...</code></td>
<td>
<p>Optional arguments passed to <code>summary</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='svdr'>Find a few approximate largest singular values and corresponding
singular vectors of a matrix.</h2><span id='topic+svdr'></span>

<h3>Description</h3>

<p>The randomized method for truncated SVD by P. G. Martinsson and colleagues
finds a few approximate largest singular values and corresponding
singular vectors of a sparse or dense matrix. It is a fast and
memory-efficient way to compute a partial SVD, similar in performance
for many problems to <code><a href="#topic+irlba">irlba</a></code>. The <code>svdr</code> method
is a block method and may produce more accurate estimations with
less work for problems with clustered large singular values (see
the examples). In other problems, <code>irlba</code> may exhibit faster
convergence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svdr(x, k, tol = 1e-05, it = 100L, extra = min(10L, dim(x) - k),
  center = NULL, Q = NULL, return.Q = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svdr_+3A_x">x</code></td>
<td>
<p>numeric real- or complex-valued matrix or real-valued sparse matrix.</p>
</td></tr>
<tr><td><code id="svdr_+3A_k">k</code></td>
<td>
<p>dimension of subspace to estimate (number of approximate singular values to compute).</p>
</td></tr>
<tr><td><code id="svdr_+3A_tol">tol</code></td>
<td>
<p>stop iteration when the largest absolute relative change in estimated singular
values from one iteration to the next falls below this value.</p>
</td></tr>
<tr><td><code id="svdr_+3A_it">it</code></td>
<td>
<p>maximum number of algorithm iterations.</p>
</td></tr>
<tr><td><code id="svdr_+3A_extra">extra</code></td>
<td>
<p>number of extra vectors of dimension <code>ncol(x)</code>, larger values generally improve accuracy (with increased
computational cost).</p>
</td></tr>
<tr><td><code id="svdr_+3A_center">center</code></td>
<td>
<p>optional column centering vector whose values are implicitly subtracted from each
column of <code>A</code> without explicitly forming the centered matrix (preserving sparsity).
Optionally specify <code>center=TRUE</code> as shorthand for <code>center=colMeans(x)</code>.
Use for efficient principal components computation.</p>
</td></tr>
<tr><td><code id="svdr_+3A_q">Q</code></td>
<td>
<p>optional initial random matrix, defaults to a matrix of size <code>ncol(x)</code> by <code>k + extra</code> with
entries sampled from a normal random distribution.</p>
</td></tr>
<tr><td><code id="svdr_+3A_return.q">return.Q</code></td>
<td>
<p>if <code>TRUE</code> return the <code>Q</code> matrix for restarting (see examples).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Also see an alternate implementation (<code>rsvd</code>) of this method by N. Benjamin Erichson
in the https://cran.r-project.org/package=rsvd package.
</p>


<h3>Value</h3>

<p>Returns a list with entries:
</p>

<dl>
<dt>d:</dt><dd><p> k approximate singular values</p>
</dd>
<dt>u:</dt><dd><p> k approximate left singular vectors</p>
</dd>
<dt>v:</dt><dd><p> k approximate right singular vectors</p>
</dd>
<dt>mprod:</dt><dd><p> total number of matrix products carried out</p>
</dd>
<dt>Q:</dt><dd><p> optional subspace matrix (when <code>return.Q=TRUE</code>)</p>
</dd>
</dl>



<h3>References</h3>

<p>Finding structure with randomness: Stochastic algorithms for constructing
approximate matrix decompositions N. Halko, P. G. Martinsson, J. Tropp. Sep. 2009.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+irlba">irlba</a></code>, <code><a href="base.html#topic+svd">svd</a></code>, <code>rsvd</code> in the rsvd package
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)

A &lt;- matrix(runif(400), nrow=20)
svdr(A, 3)$d

# Compare with svd
svd(A)$d[1:3]

# Compare with irlba
irlba(A, 3)$d

## Not run: 
# A problem with clustered large singular values where svdr out-performs irlba.
tprolate &lt;- function(n, w=0.25)
{
  a &lt;- rep(0, n)
  a[1] &lt;- 2 * w
  a[2:n] &lt;- sin( 2 * pi * w * (1:(n-1)) ) / ( pi * (1:(n-1)) )
  toeplitz(a)
}

x &lt;- tprolate(512)
set.seed(1)
tL &lt;- system.time(L &lt;- irlba(x, 20))
tR &lt;- system.time(R &lt;- svdr(x, 20))
S &lt;- svd(x)
plot(S$d)
data.frame(time=c(tL[3], tR[3]),
           error=sqrt(c(crossprod(L$d - S$d[1:20]), crossprod(R$d - S$d[1:20]))),
           row.names=c("IRLBA", "Randomized SVD"))

# But, here is a similar problem with clustered singular values where svdr
# doesn't out-perform irlba as easily...clusters of singular values are,
# in general, very hard to deal with!
# (This example based on https://github.com/bwlewis/irlba/issues/16.)
set.seed(1)
s &lt;- svd(matrix(rnorm(200 * 200), 200))
x &lt;- s$u %*% (c(exp(-(1:100)^0.3) * 1e-12 + 1, rep(0.5, 100)) * t(s$v))
tL &lt;- system.time(L &lt;- irlba(x, 5))
tR &lt;- system.time(R &lt;- svdr(x, 5))
S &lt;- svd(x)
plot(S$d)
data.frame(time=c(tL[3], tR[3]),
           error=sqrt(c(crossprod(L$d - S$d[1:5]), crossprod(R$d - S$d[1:5]))),
           row.names=c("IRLBA", "Randomized SVD"))

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
