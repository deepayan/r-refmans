<!DOCTYPE html><html><head><title>Help for package tfdeploy</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tfdeploy}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#load_savedmodel'><p>Load a SavedModel</p></a></li>
<li><a href='#predict_savedmodel'><p>Predict using a SavedModel</p></a></li>
<li><a href='#predict_savedmodel.export_prediction'><p>Predict using an Exported SavedModel</p></a></li>
<li><a href='#predict_savedmodel.graph_prediction'><p>Predict using a Loaded SavedModel</p></a></li>
<li><a href='#predict_savedmodel.webapi_prediction'><p>Predict using a Web API</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#serve_savedmodel'><p>Serve a SavedModel</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Deploy 'TensorFlow' Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Falbel &lt;daniel@rstudio.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools to deploy 'TensorFlow' <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a> models across 
  multiple services. Currently, it provides a local server for testing 'cloudml' 
  compatible services.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>httpuv, httr, jsonlite, magrittr, reticulate, swagger,
tensorflow</td>
</tr>
<tr>
<td>Suggests:</td>
<td>cloudml, knitr, pixels, processx, testthat, yaml, stringr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-06-13 18:26:35 UTC; dfalbel</td>
</tr>
<tr>
<td>Author:</td>
<td>Javier Luraschi [aut, ctb],
  Daniel Falbel [cre, ctb],
  RStudio [cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-06-14 16:30:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='load_savedmodel'>Load a SavedModel</h2><span id='topic+load_savedmodel'></span>

<h3>Description</h3>

<p>Loads a SavedModel using the given TensorFlow session and
returns the model's graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_savedmodel(sess = NULL, model_dir = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_savedmodel_+3A_sess">sess</code></td>
<td>
<p>The TensorFlow session. <code>NULL</code> if using Eager execution.</p>
</td></tr>
<tr><td><code id="load_savedmodel_+3A_model_dir">model_dir</code></td>
<td>
<p>The path to the exported model, as a string. Defaults to
a &quot;savedmodel&quot; path or the latest training run.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Loading a model improves performance over multiple <code>predict_savedmodel()</code>
calls.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+export_savedmodel">export_savedmodel()</a></code>, <code><a href="#topic+predict_savedmodel">predict_savedmodel()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# start session
sess &lt;- tensorflow::tf$Session()

# preload an existing model into a TensorFlow session
graph &lt;- tfdeploy::load_savedmodel(
  sess,
  system.file("models/tensorflow-mnist", package = "tfdeploy")
)

# perform prediction based on a pre-loaded model
tfdeploy::predict_savedmodel(
  list(rep(9, 784)),
  graph
)

# close session
sess$close()

## End(Not run)

</code></pre>

<hr>
<h2 id='predict_savedmodel'>Predict using a SavedModel</h2><span id='topic+predict_savedmodel'></span>

<h3>Description</h3>

<p>Runs a prediction over a saved model file, web API or graph object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_savedmodel(instances, model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_savedmodel_+3A_instances">instances</code></td>
<td>
<p>A list of prediction instances to be passed as input tensors
to the service. Even for single predictions, a list with one entry is expected.</p>
</td></tr>
<tr><td><code id="predict_savedmodel_+3A_model">model</code></td>
<td>
<p>The model as a local path, a REST url or graph object.
</p>
<p>A local path can be exported using <code>export_savedmodel()</code>, a REST URL
can be created using <code>serve_savedmodel()</code> and a graph object loaded using
<code>load_savedmodel()</code>.
</p>
<p>A <code>type</code> parameter can be specified to explicitly choose the type model
performing the prediction. Valid values are <code>export</code>, <code>webapi</code> and
<code>graph</code>.</p>
</td></tr>
<tr><td><code id="predict_savedmodel_+3A_...">...</code></td>
<td>
<p>See <code><a href="#topic+predict_savedmodel.export_prediction">predict_savedmodel.export_prediction()</a></code>,
<code><a href="#topic+predict_savedmodel.graph_prediction">predict_savedmodel.graph_prediction()</a></code>,
<code><a href="#topic+predict_savedmodel.webapi_prediction">predict_savedmodel.webapi_prediction()</a></code> for additional options.
</p>
<p>#' @section Implementations:
</p>

<ul>
<li> <p><code><a href="#topic+predict_savedmodel.export_prediction">predict_savedmodel.export_prediction()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+predict_savedmodel.graph_prediction">predict_savedmodel.graph_prediction()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+predict_savedmodel.webapi_prediction">predict_savedmodel.webapi_prediction()</a></code>]
</p>
</li></ul>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+export_savedmodel">export_savedmodel()</a></code>, <code><a href="#topic+serve_savedmodel">serve_savedmodel()</a></code>, <code><a href="#topic+load_savedmodel">load_savedmodel()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# perform prediction based on an existing model
tfdeploy::predict_savedmodel(
  list(rep(9, 784)),
  system.file("models/tensorflow-mnist", package = "tfdeploy")
)

## End(Not run)

</code></pre>

<hr>
<h2 id='predict_savedmodel.export_prediction'>Predict using an Exported SavedModel</h2><span id='topic+predict_savedmodel.export_prediction'></span>

<h3>Description</h3>

<p>Performs a prediction using a locally exported SavedModel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'export_prediction'
predict_savedmodel(instances, model,
  signature_name = "serving_default", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_savedmodel.export_prediction_+3A_instances">instances</code></td>
<td>
<p>A list of prediction instances to be passed as input tensors
to the service. Even for single predictions, a list with one entry is expected.</p>
</td></tr>
<tr><td><code id="predict_savedmodel.export_prediction_+3A_model">model</code></td>
<td>
<p>The model as a local path, a REST url or graph object.
</p>
<p>A local path can be exported using <code>export_savedmodel()</code>, a REST URL
can be created using <code>serve_savedmodel()</code> and a graph object loaded using
<code>load_savedmodel()</code>.
</p>
<p>A <code>type</code> parameter can be specified to explicitly choose the type model
performing the prediction. Valid values are <code>export</code>, <code>webapi</code> and
<code>graph</code>.</p>
</td></tr>
<tr><td><code id="predict_savedmodel.export_prediction_+3A_signature_name">signature_name</code></td>
<td>
<p>The named entry point to use in the model for prediction.</p>
</td></tr>
<tr><td><code id="predict_savedmodel.export_prediction_+3A_...">...</code></td>
<td>
<p>See <code><a href="#topic+predict_savedmodel.export_prediction">predict_savedmodel.export_prediction()</a></code>,
<code><a href="#topic+predict_savedmodel.graph_prediction">predict_savedmodel.graph_prediction()</a></code>,
<code><a href="#topic+predict_savedmodel.webapi_prediction">predict_savedmodel.webapi_prediction()</a></code> for additional options.
</p>
<p>#' @section Implementations:
</p>

<ul>
<li> <p><code><a href="#topic+predict_savedmodel.export_prediction">predict_savedmodel.export_prediction()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+predict_savedmodel.graph_prediction">predict_savedmodel.graph_prediction()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+predict_savedmodel.webapi_prediction">predict_savedmodel.webapi_prediction()</a></code>]
</p>
</li></ul>
</td></tr>
</table>

<hr>
<h2 id='predict_savedmodel.graph_prediction'>Predict using a Loaded SavedModel</h2><span id='topic+predict_savedmodel.graph_prediction'></span>

<h3>Description</h3>

<p>Performs a prediction using a SavedModel model already loaded using
<code>load_savedmodel()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'graph_prediction'
predict_savedmodel(instances, model, sess,
  signature_name = "serving_default", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_savedmodel.graph_prediction_+3A_instances">instances</code></td>
<td>
<p>A list of prediction instances to be passed as input tensors
to the service. Even for single predictions, a list with one entry is expected.</p>
</td></tr>
<tr><td><code id="predict_savedmodel.graph_prediction_+3A_model">model</code></td>
<td>
<p>The model as a local path, a REST url or graph object.
</p>
<p>A local path can be exported using <code>export_savedmodel()</code>, a REST URL
can be created using <code>serve_savedmodel()</code> and a graph object loaded using
<code>load_savedmodel()</code>.
</p>
<p>A <code>type</code> parameter can be specified to explicitly choose the type model
performing the prediction. Valid values are <code>export</code>, <code>webapi</code> and
<code>graph</code>.</p>
</td></tr>
<tr><td><code id="predict_savedmodel.graph_prediction_+3A_sess">sess</code></td>
<td>
<p>The active TensorFlow session.</p>
</td></tr>
<tr><td><code id="predict_savedmodel.graph_prediction_+3A_signature_name">signature_name</code></td>
<td>
<p>The named entry point to use in the model for prediction.</p>
</td></tr>
<tr><td><code id="predict_savedmodel.graph_prediction_+3A_...">...</code></td>
<td>
<p>See <code><a href="#topic+predict_savedmodel.export_prediction">predict_savedmodel.export_prediction()</a></code>,
<code><a href="#topic+predict_savedmodel.graph_prediction">predict_savedmodel.graph_prediction()</a></code>,
<code><a href="#topic+predict_savedmodel.webapi_prediction">predict_savedmodel.webapi_prediction()</a></code> for additional options.
</p>
<p>#' @section Implementations:
</p>

<ul>
<li> <p><code><a href="#topic+predict_savedmodel.export_prediction">predict_savedmodel.export_prediction()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+predict_savedmodel.graph_prediction">predict_savedmodel.graph_prediction()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+predict_savedmodel.webapi_prediction">predict_savedmodel.webapi_prediction()</a></code>]
</p>
</li></ul>
</td></tr>
</table>

<hr>
<h2 id='predict_savedmodel.webapi_prediction'>Predict using a Web API</h2><span id='topic+predict_savedmodel.webapi_prediction'></span>

<h3>Description</h3>

<p>Performs a prediction using a Web API providing a SavedModel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'webapi_prediction'
predict_savedmodel(instances, model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_savedmodel.webapi_prediction_+3A_instances">instances</code></td>
<td>
<p>A list of prediction instances to be passed as input tensors
to the service. Even for single predictions, a list with one entry is expected.</p>
</td></tr>
<tr><td><code id="predict_savedmodel.webapi_prediction_+3A_model">model</code></td>
<td>
<p>The model as a local path, a REST url or graph object.
</p>
<p>A local path can be exported using <code>export_savedmodel()</code>, a REST URL
can be created using <code>serve_savedmodel()</code> and a graph object loaded using
<code>load_savedmodel()</code>.
</p>
<p>A <code>type</code> parameter can be specified to explicitly choose the type model
performing the prediction. Valid values are <code>export</code>, <code>webapi</code> and
<code>graph</code>.</p>
</td></tr>
<tr><td><code id="predict_savedmodel.webapi_prediction_+3A_...">...</code></td>
<td>
<p>See <code><a href="#topic+predict_savedmodel.export_prediction">predict_savedmodel.export_prediction()</a></code>,
<code><a href="#topic+predict_savedmodel.graph_prediction">predict_savedmodel.graph_prediction()</a></code>,
<code><a href="#topic+predict_savedmodel.webapi_prediction">predict_savedmodel.webapi_prediction()</a></code> for additional options.
</p>
<p>#' @section Implementations:
</p>

<ul>
<li> <p><code><a href="#topic+predict_savedmodel.export_prediction">predict_savedmodel.export_prediction()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+predict_savedmodel.graph_prediction">predict_savedmodel.graph_prediction()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+predict_savedmodel.webapi_prediction">predict_savedmodel.webapi_prediction()</a></code>]
</p>
</li></ul>
</td></tr>
</table>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic++25+3E+25'></span><span id='topic+export_savedmodel'></span><span id='topic+view_savedmodel'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>magrittr</dt><dd><p><code><a href="magrittr.html#topic++25+3E+25">%&gt;%</a></code></p>
</dd>
<dt>tensorflow</dt><dd><p><code><a href="tensorflow.html#topic+export_savedmodel">export_savedmodel</a></code>, <code><a href="tensorflow.html#topic+view_savedmodel">view_savedmodel</a></code></p>
</dd>
</dl>

<hr>
<h2 id='serve_savedmodel'>Serve a SavedModel</h2><span id='topic+serve_savedmodel'></span>

<h3>Description</h3>

<p>Serve a TensorFlow SavedModel as a local web api.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>serve_savedmodel(model_dir, host = "127.0.0.1", port = 8089,
  daemonized = FALSE, browse = !daemonized)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="serve_savedmodel_+3A_model_dir">model_dir</code></td>
<td>
<p>The path to the exported model, as a string.</p>
</td></tr>
<tr><td><code id="serve_savedmodel_+3A_host">host</code></td>
<td>
<p>Address to use to serve model, as a string.</p>
</td></tr>
<tr><td><code id="serve_savedmodel_+3A_port">port</code></td>
<td>
<p>Port to use to serve model, as numeric.</p>
</td></tr>
<tr><td><code id="serve_savedmodel_+3A_daemonized">daemonized</code></td>
<td>
<p>Makes 'httpuv' server daemonized so R interactive sessions
are not blocked to handle requests. To terminate a daemonized server, call
'httpuv::stopDaemonizedServer()' with the handle returned from this call.</p>
</td></tr>
<tr><td><code id="serve_savedmodel_+3A_browse">browse</code></td>
<td>
<p>Launch browser with serving landing page?</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+export_savedmodel">export_savedmodel()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# serve an existing model over a web interface
tfdeploy::serve_savedmodel(
  system.file("models/tensorflow-mnist", package = "tfdeploy")
)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
