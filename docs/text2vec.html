<!DOCTYPE html><html lang="en"><head><title>Help for package text2vec</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {text2vec}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#text2vec'><p>text2vec</p></a></li>
<li><a href='#as.lda_c'><p>Converts document-term matrix sparse matrix to 'lda_c' format</p></a></li>
<li><a href='#BNS'><p>BNS</p></a></li>
<li><a href='#check_analogy_accuracy'><p>Checks accuracy of word embeddings on the analogy task</p></a></li>
<li><a href='#coherence'><p>Coherence metrics for topic models</p></a></li>
<li><a href='#Collocations'><p>Collocations model.</p></a></li>
<li><a href='#combine_vocabularies'><p>Combines multiple vocabularies into one</p></a></li>
<li><a href='#create_dtm'><p>Document-term matrix construction</p></a></li>
<li><a href='#create_tcm'><p>Term-co-occurence matrix construction</p></a></li>
<li><a href='#create_vocabulary'><p>Creates a vocabulary of unique terms</p></a></li>
<li><a href='#distances'><p>Pairwise Distance Matrix Computation</p></a></li>
<li><a href='#GloVe'><p>re-export rsparse::GloVe</p></a></li>
<li><a href='#ifiles'><p>Creates iterator over text files from the disk</p></a></li>
<li><a href='#itoken'><p>Iterators (and parallel iterators) over input objects</p></a></li>
<li><a href='#jsPCA_robust'><p>(numerically robust) Dimension reduction via Jensen-Shannon Divergence &amp; Principal Components</p></a></li>
<li><a href='#LatentDirichletAllocation'><p>Creates Latent Dirichlet Allocation model.</p></a></li>
<li><a href='#LatentSemanticAnalysis'><p>Latent Semantic Analysis model</p></a></li>
<li><a href='#movie_review'><p>IMDB movie reviews</p></a></li>
<li><a href='#normalize'><p>Matrix normalization</p></a></li>
<li><a href='#perplexity'><p>Perplexity of a topic model</p></a></li>
<li><a href='#prepare_analogy_questions'><p>Prepares list of analogy questions</p></a></li>
<li><a href='#print.text2vec_vocabulary'><p>Printing Vocabulary</p></a></li>
<li><a href='#prune_vocabulary'><p>Prune vocabulary</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#RelaxedWordMoversDistance'><p>Creates Relaxed Word Movers Distance (RWMD) model</p></a></li>
<li><a href='#similarities'><p>Pairwise Similarity Matrix Computation</p></a></li>
<li><a href='#split_into'><p>Split a vector for parallel processing</p></a></li>
<li><a href='#TfIdf'><p>TfIdf</p></a></li>
<li><a href='#tokenizers'><p>Simple tokenization functions for string splitting</p></a></li>
<li><a href='#vectorizers'><p>Vocabulary and hash vectorizers</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.4</td>
</tr>
<tr>
<td>Title:</td>
<td>Modern Text Mining Framework for R</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> | file LICENSE [expanded from: GPL (&ge; 2) | file LICENSE]</td>
</tr>
<tr>
<td>Description:</td>
<td>Fast and memory-friendly tools for text vectorization, topic
    modeling (LDA, LSA), word embeddings (GloVe), similarities. This package
    provides a source-agnostic streaming API, which allows researchers to perform
    analysis of collections of documents which are larger than available RAM. All
    core functions are parallelized to benefit from multicore machines.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dmitriy Selivanov &lt;selivanov.dmitriy@gmail.com&gt;</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0), methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix (&ge; 1.5-2), Rcpp (&ge; 1.0.3), R6 (&ge; 2.3.0),
data.table(&ge; 1.9.6), rsparse (&ge; 0.3.3.4), stringi (&ge; 1.1.5),
mlapi (&ge; 0.1.0), lgr (&ge; 0.2), digest (&ge; 0.6.8)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, digest (&ge; 0.6.8)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>magrittr, udpipe (&ge; 0.6), glmnet, testthat, covr, knitr,
rmarkdown, proxy</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://text2vec.org">http://text2vec.org</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/dselivanov/text2vec/issues">https://github.com/dselivanov/text2vec/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-09 02:33:27 UTC; dselivanov</td>
</tr>
<tr>
<td>Author:</td>
<td>Dmitriy Selivanov [aut, cre, cph],
  Manuel Bickel [aut, cph] (Coherence measures for topic models),
  Qing Wang [aut, cph] (Author of the WaprLDA C++ code)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-09 07:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='text2vec'>text2vec</h2><span id='topic+text2vec'></span><span id='topic+text2vec-package'></span>

<h3>Description</h3>

<p>Fast vectorization, topic modeling, distances and GloVe word embeddings in R.
</p>


<h3>Details</h3>

<p>To learn more about text2vec visit project website: <a href="http://text2vec.org">http://text2vec.org</a>
Or start with the vignettes:
<code>browseVignettes(package = "text2vec")</code>
</p>

<hr>
<h2 id='as.lda_c'>Converts document-term matrix sparse matrix to 'lda_c' format</h2><span id='topic+as.lda_c'></span>

<h3>Description</h3>

<p>Converts 'dgCMatrix' (or coercible to 'dgCMatrix') to 'lda_c' format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.lda_c(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.lda_c_+3A_x">X</code></td>
<td>
<p>Document-Term matrix</p>
</td></tr>
</table>

<hr>
<h2 id='BNS'>BNS</h2><span id='topic+BNS'></span>

<h3>Description</h3>

<p>Creates BNS (bi-normal separation) model.
Defined as: Q(true positive rate) - Q(false positive rate), where Q is a quantile function of normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BNS
</code></pre>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object.</p>


<h3>Details</h3>

<p>Bi-Normal Separation
</p>


<h3>Fields</h3>


<dl>
<dt><code>bns_stat</code></dt><dd><p><code>data.table</code> with computed BNS statistic.
Useful for feature selection.</p>
</dd>
</dl>


<h3>Usage</h3>

<p>For usage details see <b>Methods, Arguments and Examples</b> sections.
</p>
<pre>
bns = BNS$new(treshold = 0.0005)
bns$fit_transform(x, y)
bns$transform(x)
</pre>


<h3>Methods</h3>


<dl>
<dt><code>$new(treshold = 0.0005)</code></dt><dd><p>Creates bns model</p>
</dd>
<dt><code>$fit_transform(x, y)</code></dt><dd><p>fit model to an input sparse matrix (preferably in &quot;dgCMatrix&quot;
format) and then transforms it.</p>
</dd>
<dt><code>$transform(x)</code></dt><dd><p>transform new data <code>x</code> using bns from train data</p>
</dd>
</dl>



<h3>Arguments</h3>


<dl>
<dt>bns</dt><dd><p>A <code>BNS</code> object</p>
</dd>
<dt>x</dt><dd><p>An input document term matrix. Preferably in <code>dgCMatrix</code> format</p>
</dd>
<dt>y</dt><dd><p>Binary target variable coercible to logical.</p>
</dd>
<dt>treshold</dt><dd><p>Clipping treshold to avoid infinities in quantile function.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("movie_review")
N = 1000
it = itoken(head(movie_review$review, N), preprocessor = tolower, tokenizer = word_tokenizer)
vocab = create_vocabulary(it)
dtm = create_dtm(it, vocab_vectorizer(vocab))
model_bns = BNS$new()
dtm_bns = model_bns$fit_transform(dtm, head(movie_review$sentiment, N))

## End(Not run)
</code></pre>

<hr>
<h2 id='check_analogy_accuracy'>Checks accuracy of word embeddings on the analogy task</h2><span id='topic+check_analogy_accuracy'></span>

<h3>Description</h3>

<p>This function checks how well the GloVe word embeddings do on
the analogy task. For full examples see <a href="#topic+GloVe">GloVe</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_analogy_accuracy(questions_list, m_word_vectors)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_analogy_accuracy_+3A_questions_list">questions_list</code></td>
<td>
<p><code>list</code> of questions. Each element of
<code>questions_list</code> is a <code>integer matrix</code> with four columns. It
represents a set of questions related to a particular category. Each
element of matrix is an index of a row in <code>m_word_vectors</code>. See output
of <a href="#topic+prepare_analogy_questions">prepare_analogy_questions</a> for details</p>
</td></tr>
<tr><td><code id="check_analogy_accuracy_+3A_m_word_vectors">m_word_vectors</code></td>
<td>
<p>word vectors <code>numeric matrix</code>. Each row should
represent a word.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+prepare_analogy_questions">prepare_analogy_questions</a>, <a href="#topic+GloVe">GloVe</a>
</p>

<hr>
<h2 id='coherence'>Coherence metrics for topic models</h2><span id='topic+coherence'></span>

<h3>Description</h3>

<p>Given a topic model with topics represented as ordered term lists, the coherence may be used to assess the quality of individual topics.
This function is an implementation of several of the numerous possible metrics for such kind of assessments.
Coherence calculation is sensitive to the content of the reference <code>tcm</code> that is used for evaluation
and that may be created with different parameter settings. Please refer to the details section (or reference section) for information
on typical combinations of metric and type of <code>tcm</code>. For more general information on measuring coherence
a starting point is given in the reference section.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coherence(x, tcm, metrics = c("mean_logratio", "mean_pmi", "mean_npmi",
  "mean_difference", "mean_npmi_cosim", "mean_npmi_cosim2"),
  smooth = 1e-12, n_doc_tcm = -1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="coherence_+3A_x">x</code></td>
<td>
<p>A <code>character matrix</code> with the top terms per topic (each column represents one topic),
e.g., as created by <code>get_top_words()</code>.
Terms of <code>x</code> have to be ranked per topic starting with rank 1 in row 1.</p>
</td></tr>
<tr><td><code id="coherence_+3A_tcm">tcm</code></td>
<td>
<p>The term co-occurrence matrix, e.g, a <code>Matrix::sparseMatrix</code> or <code>base::matrix</code>,
serving as the reference to calculate coherence metrics.
Please note that a memory efficient version of the <code>tcm</code> is assumed as input
with all entries in the lower triangle (excluding diagonal) set to zero (see, e.g., <code>create_tcm</code>).
Please also note that some efforts during any pre-processing steps might be skipped since the <code>tcm</code>
is internally reduced to the top word space, i.e., all unique terms of <code>x</code>.</p>
</td></tr>
<tr><td><code id="coherence_+3A_metrics">metrics</code></td>
<td>
<p>Character vector specifying the metrics to be calculated. Currently the following metrics are implemented:
<code>c("mean_logratio", "mean_pmi", "mean_npmi", "mean_difference", "mean_npmi_cosim", "mean_npmi_cosim2")</code>.
Please refer to the details section for more information on the metrics.</p>
</td></tr>
<tr><td><code id="coherence_+3A_smooth">smooth</code></td>
<td>
<p>Numeric smoothing constant to avoid logarithm of zero. By default, set to <code>1e-12</code>.</p>
</td></tr>
<tr><td><code id="coherence_+3A_n_doc_tcm">n_doc_tcm</code></td>
<td>
<p>The <code>integer</code> number of documents or text windows that was used to create the <code>tcm</code>.
<code>n_doc_tcm</code> is used to calculate term probabilities from term counts as required for several metrics.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The currently implemented coherence <code>metrics</code> are described below including a description of the
content type of the <code>tcm</code> that showed good performance in combination with a specific metric.  <br />
For details on how to create <code>tcm</code> see the example section.  <br />
For details on performance of metrics see the resources in the reference section
that served for definition of standard settings for individual metrics.  <br />
Note that depending on the use case, still, different settings than the standard settings for creation of <code>tcm</code> may be reasonable.  <br />
Note that for all currently implemented metrics the <code>tcm</code> is reduced to the top word space on basis of the terms in <code>x</code>.
</p>
<p>Considering the use case of finding the optimum number of topics among several models with different metrics,
calculating the mean score over all topics and normalizing this mean coherence scores from different metrics
might be considered for direct comparison.
</p>
<p>Each metric usually opts for a different optimum number of topics. From initial experience it may be assumed
that logratio, pmi and nmpi usually opt for smaller numbers, whereas the other metrics rather tend to propose higher numbers.
</p>
<p>Implemented metrics:
</p>

<ul>
<li><p> &quot;mean_logratio&quot;  <br />
The logarithmic ratio is calculated as <br />
<code>log(smooth + tcm[x,y]) - log(tcm[y,y])</code>,  <br />
where x and y are term index pairs from a &quot;preceding&quot; term index combination.  <br />
Given the indices c(1,2,3), combinations are <code>list(c(2,1), c(3,1), c(3,2))</code>.  <br />
<br />
The <code>tcm</code> should represent the boolean term co-occurrence (internally the actual counts are used)
in the original documents and, therefore, is an intrinsic metric in the standard use case.  <br />
<br />
This metric is similar to the UMass metric, however, with a smaller smoothing constant by default
and using the mean for aggregation instead of the sum.  <br />
</p>
</li>
<li><p> &quot;mean_pmi&quot; <br />
The pointwise mutual information is calculated as  <br />
<code>log2((tcm[x,y]/n_doc_tcm) + smooth) - log2(tcm[x,x]/n_doc_tcm) - log2(tcm[y,y]/n_doc_tcm)</code>,  <br />
where x and y are term index pairs from an arbitrary term index combination  <br />
that subsets the lower or upper triangle of <code>tcm</code>, e.g. &quot;preceding&quot;.  <br />
<br />
The <code>tcm</code> should represent term co-occurrences within a boolean sliding window of size <code>10</code> (internally probabilities are used)
in an external reference corpus and, therefore, is an extrinsic metric in the standard use case.  <br />
<br />
This metric is similar to the UCI metric, however, with a smaller smoothing constant by default
and using the mean for aggregation instead of the sum.
</p>
</li>
<li><p> &quot;mean_npmi&quot; <br />
Similar (in terms of all parameter settings, etc.) to &quot;mean_pmi&quot; metric
but using the normalized pmi instead, which is calculated as <br />
<code>(log2((tcm[x,y]/n_doc_tcm) + smooth) - log2(tcm[x,x]/n_doc_tcm) - log2(tcm[y,y]/n_doc_tcm)) / -log2((tcm[x,y]/n_doc_tcm) + smooth)</code>,  <br />
<br />
This metric may perform better than the simpler pmi metric.
</p>
</li>
<li><p> &quot;mean_difference&quot; <br />
The difference is calculated as  <br />
<code>tcm[x,y]/tcm[x,x] - (tcm[y,y]/n_tcm_windows)</code>,  <br />
where x and y are term index pairs from a &quot;preceding&quot; term index combination.  <br />
Given the indices c(1,2,3), combinations are <code>list(c(1,2), c(1,3), c(2,3))</code>.  <br />
<br />
The <code>tcm</code> should represent the boolean term co-occurrence (internally probabilities are used)
in the original documents and, therefore, is an intrinsic metric in the standard use case.
</p>
</li>
<li><p> &quot;mean_npmi_cosim&quot; <br />
First, the npmi of an individual top word with each of the top words is calculated as in &quot;mean_npmi&quot;. <br />
This result in a vector of npmi values for each top word. <br />
On this basis, the cosine similarity between each pair of vectors is calculated. <br />
<br />
The <code>tcm</code> should represent term co-occurrences within a boolean sliding window of size <code>5</code> (internally probabilities are used)
in an external reference corpus and, therefore, is an extrinsic metric in the standard use case.  <br />
</p>
</li>
<li><p> &quot;mean_npmi_cosim2&quot; <br />
First, a vector of npmi values for each top word is calculated as in &quot;mean_npmi_cosim&quot;. <br />
On this basis, the cosine similarity between each vector and the sum of all vectors is calculated
(instead of the similarity between each pair). <br />
<br />
The <code>tcm</code> should represent term co-occurrences within a boolean sliding window of size <code>110</code> (internally probabilities are used)
in an external reference corpus and, therefore, is an extrinsic metric in the standard use case.  <br />
</p>
</li></ul>



<h3>Value</h3>

<p>A <code>numeric matrix</code> with the coherence scores of the specified <code>metrics</code> per topic.
</p>


<h3>References</h3>

<p>Below mentioned paper is the main theoretical basis for this code. <br />
Currently only a selection of metrics stated in this paper is included in this R implementation. <br />
Authors: Roeder, Michael; Both, Andreas; Hinneburg, Alexander (2015) <br />
Title: Exploring the Space of Topic Coherence Measures. <br />
In: Xueqi Cheng, Hang Li, Evgeniy Gabrilovich und Jie Tang (Eds.): <br />
Proceedings of the Eighth ACM International Conference on Web Search and Data Mining - WSDM '15. <br />
the Eighth ACM International Conference. Shanghai, China, 02.02.2015 - 06.02.2015. <br />
New York, USA: ACM Press, p. 399-408. <br />
https://dl.acm.org/citation.cfm?id=2685324 <br />
This paper has been implemented by above listed authors as the Java program &quot;palmetto&quot;. <br />
See https://github.com/dice-group/Palmetto or http://aksw.org/Projects/Palmetto.html.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(data.table)
library(text2vec)
library(Matrix)
data("movie_review")
N = 500
tokens = word_tokenizer(tolower(movie_review$review[1:N]))
it = itoken(tokens, progressbar = FALSE)
v = create_vocabulary(it)
v = prune_vocabulary(v, term_count_min = 5, doc_proportion_max = 0.2)
dtm = create_dtm(it, vocab_vectorizer(v))

n_topics = 10
lda_model = LDA$new(n_topics)
fitted = lda_model$fit_transform(dtm, n_iter = 20)
tw = lda_model$get_top_words(n = 10, lambda = 1)

# for demonstration purposes create intrinsic TCM from original documents
# scores might not make sense for metrics that are designed for extrinsic TCM
tcm = crossprod(sign(dtm))

# check coherence
logger = lgr::get_logger('text2vec')
logger$set_threshold('debug')
res = coherence(tw, tcm, n_doc_tcm = N)
res

# example how to create TCM for extrinsic measures from an external corpus
external_reference_corpus = tolower(movie_review$review[501:1000])
tokens_ext = word_tokenizer(external_reference_corpus)
iterator_ext = itoken(tokens_ext, progressbar = FALSE)
v_ext = create_vocabulary(iterator_ext)
# for reasons of efficiency vocabulary may be reduced to the terms matched in the original corpus
v_ext= v_ext[v_ext$term %in% v$term, ]
# external vocabulary may be pruned depending on the use case
v_ext = prune_vocabulary(v_ext, term_count_min = 5, doc_proportion_max = 0.2)
vectorizer_ext = vocab_vectorizer(v_ext)

# for demonstration purposes a boolean co-occurrence within sliding window of size 10 is used
# 10 represents sentence co-occurrence, a size of 110 would, e.g., be paragraph co-occurrence
window_size = 5

tcm_ext = create_tcm(iterator_ext, vectorizer_ext
                      ,skip_grams_window = window_size
                      ,weights = rep(1, window_size)
                      ,binary_cooccurence = TRUE
                     )
#add marginal probabilities in diagonal (by default only upper triangle of tcm is created)
diag(tcm_ext) = attributes(tcm_ext)$word_count

# get number of sliding windows that serve as virtual documents, i.e. n_doc_tcm argument
n_skip_gram_windows = sum(sapply(tokens_ext, function(x) {length(x)}))

## End(Not run)
</code></pre>

<hr>
<h2 id='Collocations'>Collocations model.</h2><span id='topic+Collocations'></span>

<h3>Description</h3>

<p>Creates Collocations model which can be used for phrase extraction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Collocations
</code></pre>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object.</p>


<h3>Fields</h3>


<dl>
<dt><code>collocation_stat</code></dt><dd><p><code>data.table</code> with collocations(phrases) statistics.
Useful for filtering non-relevant phrases</p>
</dd>
</dl>


<h3>Usage</h3>

<p>For usage details see <b>Methods, Arguments and Examples</b> sections.
</p>
<pre>
model = Collocations$new(vocabulary = NULL, collocation_count_min = 50, pmi_min = 5, gensim_min = 0,
                         lfmd_min = -Inf, llr_min = 0, sep = "_")
model$partial_fit(it, ...)
model$fit(it, n_iter = 1, ...)
model$transform(it)
model$prune(pmi_min = 5, gensim_min = 0, lfmd_min = -Inf, llr_min = 0)
model$collocation_stat
</pre>


<h3>Methods</h3>


<dl>
<dt><code>$new(vocabulary = NULL, collocation_count_min = 50, sep = "_")</code></dt><dd><p>Constructor for Collocations model.
For description of arguments see <b>Arguments</b> section.</p>
</dd>
<dt><code>$fit(it, n_iter = 1, ...)</code></dt><dd><p>fit Collocations model to input iterator <code>it</code>.
Iterating over input iterator <code>it</code> <code>n_iter</code> times, so hierarchically can learn multi-word phrases.
Invisibly returns <code>collocation_stat</code>.</p>
</dd>
<dt><code>$partial_fit(it, ...)</code></dt><dd><p>iterates once over data and learns collocations. Invisibly returns <code>collocation_stat</code>.
Workhorse for <code>$fit()</code></p>
</dd>
<dt><code>$transform(it)</code></dt><dd><p>transforms input iterator using learned collocations model.
Result of the transformation is new <code>itoken</code> or <code>itoken_parallel</code> iterator which will
produce tokens with phrases collapsed into single token.</p>
</dd>
<dt><code>$prune(pmi_min = 5, gensim_min = 0, lfmd_min = -Inf, llr_min = 0)</code></dt><dd>
<p>filter out non-relevant phrases with low score. User can do it directly by modifying <code>collocation_stat</code> object.</p>
</dd>
</dl>



<h3>Arguments</h3>


<dl>
<dt>model</dt><dd><p>A <code>Collocation</code> model object</p>
</dd>
<dt>n_iter</dt><dd><p>number of iteration over data</p>
</dd>
<dt>pmi_min, gensim_min, lfmd_min, llr_min</dt><dd><p>minimal scores of the corresponding
statistics in order to collapse tokens into collocation:
</p>

<ul>
<li><p> pointwise mutual information
</p>
</li>
<li><p> &quot;gensim&quot; scores - https://radimrehurek.com/gensim/models/phrases.html adapted from word2vec paper
</p>
</li>
<li><p> log-frequency biased mutual dependency
</p>
</li>
<li><p> Dunning's  logarithm of the ratio between the likelihoods of the hypotheses of dependence and independence
</p>
</li></ul>

<p>See <a href="https://aclanthology.org/I05-1050/">https://aclanthology.org/I05-1050/</a> for details.
Also see data in <code>model$collocation_stat</code> for better intuition</p>
</dd>
<dt>it</dt><dd><p>An input <code>itoken</code> or <code>itoken_parallel</code> iterator</p>
</dd>
<dt>vocabulary</dt><dd><p><code>text2vec_vocabulary</code> - if provided will look for collocations consisted of only from vocabulary</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>library(text2vec)
data("movie_review")

preprocessor = function(x) {
  gsub("[^[:alnum:]\\s]", replacement = " ", tolower(x))
}
sample_ind = 1:100
tokens = word_tokenizer(preprocessor(movie_review$review[sample_ind]))
it = itoken(tokens, ids = movie_review$id[sample_ind])
system.time(v &lt;- create_vocabulary(it))
v = prune_vocabulary(v, term_count_min = 5)

model = Collocations$new(collocation_count_min = 5, pmi_min = 5)
model$fit(it, n_iter = 2)
model$collocation_stat

it2 = model$transform(it)
v2 = create_vocabulary(it2)
v2 = prune_vocabulary(v2, term_count_min = 5)
# check what phrases model has learned
setdiff(v2$term, v$term)
# [1] "main_character"  "jeroen_krabb"    "boogey_man"      "in_order"
# [5] "couldn_t"        "much_more"       "my_favorite"     "worst_film"
# [9] "have_seen"       "characters_are"  "i_mean"          "better_than"
# [13] "don_t_care"      "more_than"       "look_at"         "they_re"
# [17] "each_other"      "must_be"         "sexual_scenes"   "have_been"
# [21] "there_are_some"  "you_re"          "would_have"      "i_loved"
# [25] "special_effects" "hit_man"         "those_who"       "people_who"
# [29] "i_am"            "there_are"       "could_have_been" "we_re"
# [33] "so_bad"          "should_be"       "at_least"        "can_t"
# [37] "i_thought"       "isn_t"           "i_ve"            "if_you"
# [41] "didn_t"          "doesn_t"         "i_m"             "don_t"

# and same way we can create document-term matrix which contains
# words and phrases!
dtm = create_dtm(it2, vocab_vectorizer(v2))
# check that dtm contains phrases
which(colnames(dtm) == "jeroen_krabb")
</code></pre>

<hr>
<h2 id='combine_vocabularies'>Combines multiple vocabularies into one</h2><span id='topic+combine_vocabularies'></span>

<h3>Description</h3>

<p>Combines multiple vocabularies into one
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine_vocabularies(..., combine_stopwords = function(x)
  unique(unlist(lapply(x, attr, which = "stopwords"), use.names = FALSE)),
  combine_ngram = function(x) attr(x[[1]], "ngram"),
  combine_sep_ngram = function(x) attr(x[[1]], "sep_ngram"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="combine_vocabularies_+3A_...">...</code></td>
<td>
<p>vocabulary objects created with <a href="#topic+create_vocabulary">create_vocabulary</a>.</p>
</td></tr>
<tr><td><code id="combine_vocabularies_+3A_combine_stopwords">combine_stopwords</code></td>
<td>
<p>function to combine stopwords from input vocabularies.
By default we take a union of all stopwords.</p>
</td></tr>
<tr><td><code id="combine_vocabularies_+3A_combine_ngram">combine_ngram</code></td>
<td>
<p>function to combine lower and upper boundary for n-grams
from input vocabularies. Usually these values should be the same, so we take this parameter
from first vocabulary.</p>
</td></tr>
<tr><td><code id="combine_vocabularies_+3A_combine_sep_ngram">combine_sep_ngram</code></td>
<td>
<p>function to combine stopwords from input vocabularies.
Usually these values should be the same, so we take this parameter
from first vocabulary.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>text2vec_vocabulary</code> see details in <a href="#topic+create_vocabulary">create_vocabulary</a>.
</p>

<hr>
<h2 id='create_dtm'>Document-term matrix construction</h2><span id='topic+create_dtm'></span><span id='topic+create_dtm.itoken'></span><span id='topic+create_dtm.itoken_parallel'></span>

<h3>Description</h3>

<p>This is a high-level function for creating a document-term
matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_dtm(it, vectorizer, type = c("dgCMatrix", "dgTMatrix",
  "dgRMatrix", "CsparseMatrix", "TsparseMatrix", "RsparseMatrix"), ...)

## S3 method for class 'itoken'
create_dtm(it, vectorizer, type = c("dgCMatrix",
  "dgTMatrix", "dgRMatrix", "CsparseMatrix", "TsparseMatrix",
  "RsparseMatrix"), ...)

## S3 method for class 'itoken_parallel'
create_dtm(it, vectorizer,
  type = c("dgCMatrix", "dgTMatrix", "dgRMatrix", "CsparseMatrix",
  "TsparseMatrix", "RsparseMatrix"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_dtm_+3A_it">it</code></td>
<td>
<p><a href="#topic+itoken">itoken</a> iterator or <code>list</code> of <code>itoken</code> iterators.</p>
</td></tr>
<tr><td><code id="create_dtm_+3A_vectorizer">vectorizer</code></td>
<td>
<p><code>function</code> vectorizer function; see
<a href="#topic+vectorizers">vectorizers</a>.</p>
</td></tr>
<tr><td><code id="create_dtm_+3A_type">type</code></td>
<td>
<p><code>character</code>, one of <code>c("CsparseMatrix", "TsparseMatrix")</code>.</p>
</td></tr>
<tr><td><code id="create_dtm_+3A_...">...</code></td>
<td>
<p>placeholder for additional arguments (not used at the moment).
over <code>it</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a parallel backend is registered and first argument is a list of <code>itoken</code>,
iterators, function will construct the DTM in multiple threads.
User should keep in mind that he or she should split the data itself and provide a list of
<a href="#topic+itoken">itoken</a> iterators. Each element of <code>it</code> will be handled in separate
thread and combined at the end of processing.
</p>


<h3>Value</h3>

<p>A document-term matrix
</p>


<h3>See Also</h3>

<p><a href="#topic+itoken">itoken</a> <a href="#topic+vectorizers">vectorizers</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("movie_review")
N = 1000
it = itoken(movie_review$review[1:N], preprocess_function = tolower,
             tokenizer = word_tokenizer)
v = create_vocabulary(it)
#remove very common and uncommon words
pruned_vocab = prune_vocabulary(v, term_count_min = 10,
 doc_proportion_max = 0.5, doc_proportion_min = 0.001)
vectorizer = vocab_vectorizer(v)
it = itoken(movie_review$review[1:N], preprocess_function = tolower,
             tokenizer = word_tokenizer)
dtm = create_dtm(it, vectorizer)
# get tf-idf matrix from bag-of-words matrix
dtm_tfidf = transformer_tfidf(dtm)

## Example of parallel mode
it = token_parallel(movie_review$review[1:N], tolower, word_tokenizer, movie_review$id[1:N])
vectorizer = hash_vectorizer()
dtm = create_dtm(it, vectorizer, type = 'TsparseMatrix')

## End(Not run)
</code></pre>

<hr>
<h2 id='create_tcm'>Term-co-occurence matrix construction</h2><span id='topic+create_tcm'></span><span id='topic+create_tcm.itoken'></span><span id='topic+create_tcm.itoken_parallel'></span>

<h3>Description</h3>

<p>This is a function for constructing a
term-co-occurrence matrix(TCM). TCM matrix usually used with <a href="#topic+GloVe">GloVe</a> word embedding model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_tcm(it, vectorizer, skip_grams_window = 5L,
  skip_grams_window_context = c("symmetric", "right", "left"),
  weights = 1/seq_len(skip_grams_window), binary_cooccurence = FALSE,
  ...)

## S3 method for class 'itoken'
create_tcm(it, vectorizer, skip_grams_window = 5L,
  skip_grams_window_context = c("symmetric", "right", "left"),
  weights = 1/seq_len(skip_grams_window), binary_cooccurence = FALSE,
  ...)

## S3 method for class 'itoken_parallel'
create_tcm(it, vectorizer,
  skip_grams_window = 5L, skip_grams_window_context = c("symmetric",
  "right", "left"), weights = 1/seq_len(skip_grams_window),
  binary_cooccurence = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_tcm_+3A_it">it</code></td>
<td>
<p><code>list</code> of iterators over tokens from <a href="#topic+itoken">itoken</a>.
Each element is a list of tokens, that is, tokenized and normalized
strings.</p>
</td></tr>
<tr><td><code id="create_tcm_+3A_vectorizer">vectorizer</code></td>
<td>
<p><code>function</code> vectorizer function. See
<a href="#topic+vectorizers">vectorizers</a>.</p>
</td></tr>
<tr><td><code id="create_tcm_+3A_skip_grams_window">skip_grams_window</code></td>
<td>
<p><code>integer</code> window for term-co-occurence matrix
construction. <code>skip_grams_window</code> should be &gt; 0 if you plan to use
<code>vectorizer</code> in <a href="#topic+create_tcm">create_tcm</a> function.
Value of <code>0L</code> means to not construct the TCM.</p>
</td></tr>
<tr><td><code id="create_tcm_+3A_skip_grams_window_context">skip_grams_window_context</code></td>
<td>
<p>one of <code>c("symmetric", "right", "left")</code> -
which context words to use when count co-occurence statistics.</p>
</td></tr>
<tr><td><code id="create_tcm_+3A_weights">weights</code></td>
<td>
<p>weights for context/distant words during co-occurence statistics calculation.
By default we are setting <code>weight = 1 / distance_from_current_word</code>.
Should have length equal to skip_grams_window.</p>
</td></tr>
<tr><td><code id="create_tcm_+3A_binary_cooccurence">binary_cooccurence</code></td>
<td>
<p><code>FALSE</code> by default. If set to <code>TRUE</code> then function only counts first
appearence of the context word and remaining occurrence are ignored. Useful when creating TCM for evaluation
of coherence of topic models.
<code>"symmetric"</code> by default - take into account <code>skip_grams_window</code> left and right.</p>
</td></tr>
<tr><td><code id="create_tcm_+3A_...">...</code></td>
<td>
<p>placeholder for additional arguments (not used at the moment).
<code>it</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a parallel backend is registered, it will construct the TCM in multiple threads.
The user should keep in mind that he/she should split data and provide a list
of <a href="#topic+itoken">itoken</a> iterators. Each element of <code>it</code> will be handled
in a separate thread combined at the end of processing.
</p>


<h3>Value</h3>

<p><code>TsparseMatrix</code> TCM matrix
</p>


<h3>See Also</h3>

<p><a href="#topic+itoken">itoken</a> <a href="#topic+create_dtm">create_dtm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("movie_review")

# single thread

tokens = word_tokenizer(tolower(movie_review$review))
it = itoken(tokens)
v = create_vocabulary(jobs)
vectorizer = vocab_vectorizer(v)
tcm = create_tcm(itoken(tokens), vectorizer, skip_grams_window = 3L)

# parallel version

# set to number of cores on your machine
it = token_parallel(movie_review$review[1:N], tolower, word_tokenizer, movie_review$id[1:N])
v = create_vocabulary(jobs)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = 'TsparseMatrix')
tcm = create_tcm(jobs, vectorizer, skip_grams_window = 3L, skip_grams_window_context = "symmetric")

## End(Not run)
</code></pre>

<hr>
<h2 id='create_vocabulary'>Creates a vocabulary of unique terms</h2><span id='topic+create_vocabulary'></span><span id='topic+vocabulary'></span><span id='topic+create_vocabulary.character'></span><span id='topic+create_vocabulary.itoken'></span><span id='topic+create_vocabulary.itoken_parallel'></span>

<h3>Description</h3>

<p>This function collects unique terms and corresponding statistics.
See the below for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_vocabulary(it, ngram = c(ngram_min = 1L, ngram_max = 1L),
  stopwords = character(0), sep_ngram = "_", window_size = 0L, ...)

vocabulary(it, ngram = c(ngram_min = 1L, ngram_max = 1L),
  stopwords = character(0), sep_ngram = "_", window_size = 0L, ...)

## S3 method for class 'character'
create_vocabulary(it, ngram = c(ngram_min = 1L,
  ngram_max = 1L), stopwords = character(0), sep_ngram = "_",
  window_size = 0L, ...)

## S3 method for class 'itoken'
create_vocabulary(it, ngram = c(ngram_min = 1L,
  ngram_max = 1L), stopwords = character(0), sep_ngram = "_",
  window_size = 0L, ...)

## S3 method for class 'itoken_parallel'
create_vocabulary(it, ngram = c(ngram_min = 1L,
  ngram_max = 1L), stopwords = character(0), sep_ngram = "_",
  window_size = 0L, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_vocabulary_+3A_it">it</code></td>
<td>
<p>iterator over a <code>list</code> of <code>character</code> vectors,
which are the documents from which the user wants to construct a vocabulary.
See <a href="#topic+itoken">itoken</a>.
Alternatively, a <code>character</code> vector of user-defined vocabulary terms
(which will be used &quot;as is&quot;).</p>
</td></tr>
<tr><td><code id="create_vocabulary_+3A_ngram">ngram</code></td>
<td>
<p><code>integer</code> vector. The lower and upper boundary of the range
of n-values for different n-grams to be extracted. All values of <code>n</code>
such that ngram_min &lt;= n &lt;= ngram_max will be used.</p>
</td></tr>
<tr><td><code id="create_vocabulary_+3A_stopwords">stopwords</code></td>
<td>
<p><code>character</code> vector of stopwords to filter out. <b>NOTE</b> that
stopwords will be used &quot;as is&quot;. This means that if preprocessing function in <a href="#topic+itoken">itoken</a> does some
text modification (like stemming), then this preprocessing need to be applied to stopwords before passing them here.
See <a href="https://github.com/dselivanov/text2vec/issues/228">https://github.com/dselivanov/text2vec/issues/228</a> for example.</p>
</td></tr>
<tr><td><code id="create_vocabulary_+3A_sep_ngram">sep_ngram</code></td>
<td>
<p><code>character</code> a character string to concatenate words in ngrams</p>
</td></tr>
<tr><td><code id="create_vocabulary_+3A_window_size">window_size</code></td>
<td>
<p><code>integer</code> (0 by default). If <code>window_size &gt; 0</code> than vocabulary will
be created from pseudo-documents which are obtained by virtually splitting each documents into
chunks of the length <code>window_size</code> by going with sliding window through them.
This is useful for creating special statistics which are used for coherence estimation in topic models.</p>
</td></tr>
<tr><td><code id="create_vocabulary_+3A_...">...</code></td>
<td>
<p>placeholder for additional arguments (not used at the moment).</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>text2vec_vocabulary</code> object, which is actually a <code>data.frame</code>
with following columns:
</p>
<table role = "presentation">
<tr><td><code>term</code></td>
<td>
 <p><code>character</code> vector of unique terms</p>
</td></tr>
<tr><td><code>term_count</code></td>
<td>
 <p><code>integer</code> vector of term counts across all
documents</p>
</td></tr> <tr><td><code>doc_count</code></td>
<td>
 <p><code>integer</code> vector of document
counts that contain corresponding term</p>
</td></tr>
</table>
<p>Also it contains metainformation in attributes:
<code>ngram</code>: <code>integer</code> vector, the lower and upper boundary of the
range of n-gram-values.
<code>document_count</code>: <code>integer</code> number of documents vocabulary was
built.
<code>stopwords</code>: <code>character</code> vector of stopwords
<code>sep_ngram</code>: <code>character</code> separator for ngrams
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>character</code>: creates <code>text2vec_vocabulary</code> from predefined
character vector. Terms will be inserted <b>as is</b>, without any checks
(ngrams number, ngram delimiters, etc.).
</p>
</li>
<li> <p><code>itoken</code>: collects unique terms and corresponding statistics from object.
</p>
</li>
<li> <p><code>itoken_parallel</code>: collects unique terms and corresponding
statistics from iterator.
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>data("movie_review")
txt = movie_review[['review']][1:100]
it = itoken(txt, tolower, word_tokenizer, n_chunks = 10)
vocab = create_vocabulary(it)
pruned_vocab = prune_vocabulary(vocab, term_count_min = 10, doc_proportion_max = 0.8,
doc_proportion_min = 0.001, vocab_term_max = 20000)
</code></pre>

<hr>
<h2 id='distances'>Pairwise Distance Matrix Computation</h2><span id='topic+distances'></span><span id='topic+dist2'></span><span id='topic+pdist2'></span>

<h3>Description</h3>

<p><code>dist2</code> calculates pairwise distances/similarities between the
rows of two data matrices. <b>Note</b> that some methods work only on sparse matrices and
others work only on dense matrices.
</p>
<p><code>pdist2</code> calculates &quot;parallel&quot; distances between the rows of two data matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist2(x, y = NULL, method = c("cosine", "euclidean", "jaccard"),
  norm = c("l2", "l1", "none"))

pdist2(x, y, method = c("cosine", "euclidean", "jaccard"),
  norm = c("l2", "l1", "none"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="distances_+3A_x">x</code></td>
<td>
<p>first matrix.</p>
</td></tr>
<tr><td><code id="distances_+3A_y">y</code></td>
<td>
<p>second matrix. For <code>dist2</code> <code>y = NULL</code> set by default.
This means that we will assume <code>y = x</code> and calculate distances/similarities between all rows of the <code>x</code>.</p>
</td></tr>
<tr><td><code id="distances_+3A_method">method</code></td>
<td>
<p>usually <code>character</code> or instance of <code>tet2vec_distance</code> class.
The distances/similarity measure to be used. One of <code>c("cosine", "euclidean", "jaccard")</code> or <a href="#topic+RWMD">RWMD</a>.
<code>RWMD</code> works only on bag-of-words matrices.
<b>In case of <code>"cosine"</code> distance max distance will be 1 - (-1) = 2</b></p>
</td></tr>
<tr><td><code id="distances_+3A_norm">norm</code></td>
<td>
<p><code>character = c("l2", "l1", "none")</code> - how to scale input matrices.
If they already scaled - use <code>"none"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the distance matrix computed by using the specified method.
Similar to <a href="stats.html#topic+dist">dist</a> function, but works with two matrices.
</p>
<p><code>pdist2</code> takes two matrices and return a single vector.
giving the ‘parallel’ distances of the vectors.
</p>


<h3>Value</h3>

<p><code>dist2</code> returns <code>matrix</code> of distances/similarities between each row of
matrix <code>x</code> and each row of matrix <code>y</code>.
</p>
<p><code>pdist2</code> returns <code>vector</code> of &quot;parallel&quot; distances between rows
of <code>x</code> and <code>y</code>.
</p>

<hr>
<h2 id='GloVe'>re-export rsparse::GloVe</h2><span id='topic+GloVe'></span><span id='topic+GlobalVectors'></span>

<h3>Description</h3>

<p>re-export rsparse::GloVe
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GlobalVectors
</code></pre>


<h3>Format</h3>

<p>An object of class <code>R6ClassGenerator</code> of length 25.</p>

<hr>
<h2 id='ifiles'>Creates iterator over text files from the disk</h2><span id='topic+ifiles'></span><span id='topic+idir'></span><span id='topic+ifiles_parallel'></span>

<h3>Description</h3>

<p>The result of this function usually used in an <a href="#topic+itoken">itoken</a> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ifiles(file_paths, reader = readLines)

idir(path, reader = readLines)

ifiles_parallel(file_paths, reader = readLines, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ifiles_+3A_file_paths">file_paths</code></td>
<td>
<p><code>character</code> paths of input files</p>
</td></tr>
<tr><td><code id="ifiles_+3A_reader">reader</code></td>
<td>
<p><code>function</code> which will perform reading of text
files from disk, which should take a path as its first argument. <code>reader()</code> function should
return <b>named character vector: elements of vector = documents,
names of the elements = document ids which will be used in DTM construction</b>.
If user doesn't provide named character vector, document ids will be generated as
file_name + line_number (assuming that each line is a document).</p>
</td></tr>
<tr><td><code id="ifiles_+3A_path">path</code></td>
<td>
<p><code>character</code> path of directory. All files in the directory will be read.</p>
</td></tr>
<tr><td><code id="ifiles_+3A_...">...</code></td>
<td>
<p>other arguments (not used at the moment)</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+itoken">itoken</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
current_dir_files = list.files(path = ".", full.names = TRUE)
files_iterator = ifiles(current_dir_files)
parallel_files_iterator = ifiles_parallel(current_dir_files, n_chunks = 4)
it = itoken_parallel(parallel_files_iterator)
dtm = create_dtm(it, hash_vectorizer(2**16), type = 'TsparseMatrix')

## End(Not run)
dir_files_iterator = idir(path = ".")
</code></pre>

<hr>
<h2 id='itoken'>Iterators (and parallel iterators) over input objects</h2><span id='topic+itoken'></span><span id='topic+itoken.character'></span><span id='topic+itoken.list'></span><span id='topic+itoken.iterator'></span><span id='topic+itoken_parallel'></span><span id='topic+itoken_parallel.character'></span><span id='topic+itoken_parallel.iterator'></span><span id='topic+itoken_parallel.list'></span>

<h3>Description</h3>

<p>This family of function creates iterators over input objects
in order to create vocabularies, or DTM and TCM matrices.
iterators usually used in following functions : <a href="#topic+create_vocabulary">create_vocabulary</a>,
<a href="#topic+create_dtm">create_dtm</a>, <a href="#topic+vectorizers">vectorizers</a>,
<a href="#topic+create_tcm">create_tcm</a>. See them for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>itoken(iterable, ...)

## S3 method for class 'character'
itoken(iterable, preprocessor = identity,
  tokenizer = space_tokenizer, n_chunks = 10,
  progressbar = interactive(), ids = NULL, ...)

## S3 method for class 'list'
itoken(iterable, n_chunks = 10,
  progressbar = interactive(), ids = names(iterable), ...)

## S3 method for class 'iterator'
itoken(iterable, preprocessor = identity,
  tokenizer = space_tokenizer, progressbar = interactive(), ...)

itoken_parallel(iterable, ...)

## S3 method for class 'character'
itoken_parallel(iterable, preprocessor = identity,
  tokenizer = space_tokenizer, n_chunks = 10, ids = NULL, ...)

## S3 method for class 'iterator'
itoken_parallel(iterable, preprocessor = identity,
  tokenizer = space_tokenizer, n_chunks = 1L, ...)

## S3 method for class 'list'
itoken_parallel(iterable, n_chunks = 10, ids = NULL,
  ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="itoken_+3A_iterable">iterable</code></td>
<td>
<p>an object from which to generate an iterator</p>
</td></tr>
<tr><td><code id="itoken_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods</p>
</td></tr>
<tr><td><code id="itoken_+3A_preprocessor">preprocessor</code></td>
<td>
<p><code>function</code> which takes chunk of
<code>character</code> vectors and does all pre-processing.
Usually <code>preprocessor</code> should return a
<code>character</code> vector of preprocessed/cleaned documents. See &quot;Details&quot;
section.</p>
</td></tr>
<tr><td><code id="itoken_+3A_tokenizer">tokenizer</code></td>
<td>
<p><code>function</code> which takes a <code>character</code> vector from
<code>preprocessor</code>, split it into tokens and returns a <code>list</code>
of <code>character</code> vectors. If you need to perform stemming -
call stemmer inside tokenizer. See examples section.</p>
</td></tr>
<tr><td><code id="itoken_+3A_n_chunks">n_chunks</code></td>
<td>
<p><code>integer</code>, the number of pieces that object should
be divided into. Then each chunk is processed independently (and in case <code>itoken_parallel</code>
<b>in parallel if some parallel backend is registered</b>).
Usually there is tradeoff: larger number of chunks means lower memory footprint, but slower (if
<code>preprocessor, tokenizer</code> functions are efficiently vectorized). And small number
of chunks means larger memory footprint but faster execution (again if user
supplied <code>preprocessor, tokenizer</code> functions are efficiently vectorized).</p>
</td></tr>
<tr><td><code id="itoken_+3A_progressbar">progressbar</code></td>
<td>
<p><code>logical</code> indicates whether to show progress bar.</p>
</td></tr>
<tr><td><code id="itoken_+3A_ids">ids</code></td>
<td>
<p><code>vector</code> of document ids. If <code>ids</code> is not provided,
<code>names(iterable)</code> will be used. If <code>names(iterable) == NULL</code>,
incremental ids will be assigned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>S3 methods for creating an itoken iterator from list of tokens
</p>

<ul>
<li><p><code>list</code>: all elements of the input list should be
character vectors containing tokens
</p>
</li>
<li><p><code>character</code>: raw text
source: the user must provide a tokenizer function
</p>
</li>
<li><p><code>ifiles</code>: from files, a user must provide a function to read in the file
(to <a href="#topic+ifiles">ifiles</a>) and a function to tokenize it (to <a href="#topic+itoken">itoken</a>)
</p>
</li>
<li><p><code>idir</code>: from a directory, the user must provide a function to
read in the files (to <a href="#topic+idir">idir</a>) and a function to tokenize it (to <a href="#topic+itoken">itoken</a>)
</p>
</li>
<li><p><code>ifiles_parallel</code>: from files in parallel
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+ifiles">ifiles</a>, <a href="#topic+idir">idir</a>, <a href="#topic+create_vocabulary">create_vocabulary</a>,
<a href="#topic+create_dtm">create_dtm</a>, <a href="#topic+vectorizers">vectorizers</a>,
<a href="#topic+create_tcm">create_tcm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("movie_review")
txt = movie_review$review[1:100]
ids = movie_review$id[1:100]
it = itoken(txt, tolower, word_tokenizer, n_chunks = 10)
it = itoken(txt, tolower, word_tokenizer, n_chunks = 10, ids = ids)
# Example of stemming tokenizer
# stem_tokenizer =function(x) {
#   lapply(word_tokenizer(x), SnowballC::wordStem, language="en")
# }
it = itoken_parallel(movie_review$review[1:100], n_chunks = 4)
system.time(dtm &lt;- create_dtm(it, hash_vectorizer(2**16), type = 'TsparseMatrix'))
</code></pre>

<hr>
<h2 id='jsPCA_robust'>(numerically robust) Dimension reduction via Jensen-Shannon Divergence &amp; Principal Components</h2><span id='topic+jsPCA_robust'></span>

<h3>Description</h3>

<p>This function is largely a copy of the repsective function in
https://github.com/cpsievert/LDAvis/blob/master/R/createJSON.R, however,
with a fix to avoid log(0) proposed by Maren-Eckhoff in
https://github.com/cpsievert/LDAvis/issues/56
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jsPCA_robust(phi)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jsPCA_robust_+3A_phi">phi</code></td>
<td>
<p>matrix, with each row containing the distribution over terms
for a topic, with as many rows as there are topics in the model, and as
many columns as there are terms in the vocabulary.</p>
</td></tr>
</table>

<hr>
<h2 id='LatentDirichletAllocation'>Creates Latent Dirichlet Allocation model.</h2><span id='topic+LatentDirichletAllocation'></span><span id='topic+LDA'></span>

<h3>Description</h3>

<p>Creates Latent Dirichlet Allocation model.
At the moment only 'WarpLDA' is implemented.
WarpLDA, an LDA sampler which achieves both the best O(1) time
complexity per token and the best O(K) scope of random access.
Our empirical results in a wide range of testing conditions demonstrate that
WarpLDA is consistently 5-15x faster than the state-of-the-art Metropolis-Hastings
based LightLDA, and is comparable or faster than the sparsity aware F+LDA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LatentDirichletAllocation

LDA
</code></pre>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object.</p>


<h3>Fields</h3>


<dl>
<dt><code>topic_word_distribution</code></dt><dd><p>distribution of words for each topic. Available after model fitting with
<code>model$fit_transform()</code> method.</p>
</dd>
<dt><code>components</code></dt><dd><p>unnormalized word counts for each topic-word entry. Available after model fitting with
<code>model$fit_transform()</code> method.</p>
</dd>
</dl>


<h3>Usage</h3>

<p>For usage details see <b>Methods, Arguments and Examples</b> sections.
</p>
<pre>
lda = LDA$new(n_topics = 10L, doc_topic_prior = 50 / n_topics, topic_word_prior = 1 / n_topics)
lda$fit_transform(x, n_iter = 1000, convergence_tol = 1e-3, n_check_convergence = 10, progressbar = interactive())
lda$transform(x, n_iter = 1000, convergence_tol = 1e-3, n_check_convergence = 5, progressbar = FALSE)
lda$get_top_words(n = 10, topic_number = 1L:private$n_topics, lambda = 1)
</pre>


<h3>Methods</h3>


<dl>
<dt><code>$new(n_topics,
              doc_topic_prior = 50 / n_topics, # alpha
              topic_word_prior = 1 / n_topics, # beta
              method = "WarpLDA")</code></dt><dd><p>Constructor for LDA model.
For description of arguments see <b>Arguments</b> section.</p>
</dd>
<dt><code>$fit_transform(x, n_iter, convergence_tol = -1,
               n_check_convergence = 0, progressbar = interactive())</code></dt><dd><p>fit LDA model to input matrix
<code>x</code> and transforms input documents to topic space.
Result is a matrix where each row represents corresponding document.
Values in a row form distribution over topics.</p>
</dd>
<dt><code>$transform(x, n_iter, convergence_tol = -1,
               n_check_convergence = 0, progressbar = FALSE)</code></dt><dd><p> transforms new documents into topic space.
Result is a matrix where each row is a distribution of a documents over latent topic space.</p>
</dd>
<dt><code>$get_top_words(n = 10, topic_number = 1L:private$n_topics, lambda = 1)</code></dt><dd><p>returns &quot;top words&quot;
for a given topic (or several topics). Words for each topic can be
sorted by probability of chance to observe word in a given topic (<code>lambda = 1</code>) and by
&quot;relevance&quot; which also takes into account frequency of word in corpus (<code>lambda &lt; 1</code>).
From our experience in most cases setting <code> 0.2 &lt; lambda &lt; 0.4</code> works well.
See <a href="http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf">http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf</a> for details.</p>
</dd>
<dt><code>$plot(lambda.step = 0.1, reorder.topics = FALSE, ...)</code></dt><dd><p>plot LDA model using <a href="https://cran.r-project.org/package=LDAvis">https://cran.r-project.org/package=LDAvis</a> package.
<code>...</code> will be passed to <code>LDAvis::createJSON</code> and <code>LDAvis::serVis</code> functions</p>
</dd>
</dl>



<h3>Arguments</h3>


<dl>
<dt>lda</dt><dd><p>A <code>LDA</code> object</p>
</dd>
<dt>x</dt><dd><p>An input document-term matrix (should have column names = terms).
<b>CSR <code>RsparseMatrix</code> used internally</b>,
other formats will be tried to convert to CSR via <code>as()</code> function call.</p>
</dd>
<dt>n_topics</dt><dd><p><code>integer</code> desired number of latent topics. Also knows as <b>K</b></p>
</dd>
<dt>doc_topic_prior</dt><dd><p><code>numeric</code> prior for document-topic multinomial distribution.
Also knows as <b>alpha</b></p>
</dd>
<dt>topic_word_prior</dt><dd><p><code>numeric</code> prior for topic-word multinomial distribution.
Also knows as <b>eta</b></p>
</dd>
<dt>n_iter</dt><dd><p><code>integer</code> number of sampling iterations while fitting model</p>
</dd>
<dt>n_iter_inference</dt><dd><p><code>integer</code> number iterations used when sampling from converged model for inference.
In other words number of samples from distribution after burn-in.</p>
</dd>
<dt>n_check_convergence</dt><dd><p> defines how often calculate score to check convergence </p>
</dd>
<dt>convergence_tol</dt><dd><p><code>numeric = -1</code> defines early stopping strategy. We stop fitting
when one of two following conditions will be satisfied: (a) we have used
all iterations, or (b) <code>score_previous_check / score_current &lt; 1 + convergence_tol</code></p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(text2vec)
data("movie_review")
N = 500
tokens = word_tokenizer(tolower(movie_review$review[1:N]))
it = itoken(tokens, ids = movie_review$id[1:N])
v = create_vocabulary(it)
v = prune_vocabulary(v, term_count_min = 5, doc_proportion_max = 0.2)
dtm = create_dtm(it, vocab_vectorizer(v))
lda_model = LDA$new(n_topics = 10)
doc_topic_distr = lda_model$fit_transform(dtm, n_iter = 20)
# run LDAvis visualisation if needed (make sure LDAvis package installed)
# lda_model$plot()

## End(Not run)
</code></pre>

<hr>
<h2 id='LatentSemanticAnalysis'>Latent Semantic Analysis model</h2><span id='topic+LatentSemanticAnalysis'></span><span id='topic+LSA'></span>

<h3>Description</h3>

<p>Creates LSA(Latent semantic analysis) model.
See <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">https://en.wikipedia.org/wiki/Latent_semantic_analysis</a> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LatentSemanticAnalysis

LSA
</code></pre>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object.</p>


<h3>Usage</h3>

<p>For usage details see <b>Methods, Arguments and Examples</b> sections.
</p>
<pre>
lsa = LatentSemanticAnalysis$new(n_topics)
lsa$fit_transform(x, ...)
lsa$transform(x, ...)
lsa$components
</pre>


<h3>Methods</h3>


<dl>
<dt><code>$new(n_topics)</code></dt><dd><p>create LSA model with <code>n_topics</code> latent topics</p>
</dd>
<dt><code>$fit_transform(x, ...)</code></dt><dd><p>fit model to an input sparse matrix (preferably in <code>dgCMatrix</code>
format) and then transform <code>x</code> to latent space</p>
</dd>
<dt><code>$transform(x, ...)</code></dt><dd><p>transform new data <code>x</code> to latent space</p>
</dd>
</dl>



<h3>Arguments</h3>


<dl>
<dt>lsa</dt><dd><p>A <code>LSA</code> object.</p>
</dd>
<dt>x</dt><dd><p>An input document-term matrix. Preferably in <code>dgCMatrix</code> format</p>
</dd>
<dt>n_topics</dt><dd><p><code>integer</code> desired number of latent topics.</p>
</dd>
<dt>...</dt><dd><p>Arguments to internal functions. Notably useful for <code>fit_transform()</code> -
these arguments will be passed to <code>rsparse::soft_svd</code></p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data("movie_review")
N = 100
tokens = word_tokenizer(tolower(movie_review$review[1:N]))
dtm = create_dtm(itoken(tokens), hash_vectorizer(2**10))
n_topics = 5
lsa_1 = LatentSemanticAnalysis$new(n_topics)
d1 = lsa_1$fit_transform(dtm)
# the same, but wrapped with S3 methods
d2 = fit_transform(dtm, lsa_1)

</code></pre>

<hr>
<h2 id='movie_review'>IMDB movie reviews</h2><span id='topic+movie_review'></span>

<h3>Description</h3>

<p>The labeled dataset consists of 5000 IMDB movie reviews, specially selected
for sentiment analysis. The sentiment of the reviews is binary, meaning an
IMDB rating &lt; 5 results in a sentiment score of 0, and a rating &gt;=7 has a
sentiment score of 1. No individual movie has more than 30 reviews. Important
note: we removed non ASCII symbols from the original dataset to satisfy CRAN
policy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("movie_review")
</code></pre>


<h3>Format</h3>

<p>A data frame with 5000 rows and 3 variables: </p>

<dl>
<dt>id</dt><dd><p>Unique ID of each review</p>
</dd> <dt>sentiment</dt><dd><p>Sentiment of the
review; 1 for positive reviews and 0 for negative reviews</p>
</dd>
<dt>review</dt><dd><p>Text of the review (UTF-8) </p>
</dd></dl>


<h3>Source</h3>

<p><a href="http://ai.stanford.edu/~amaas/data/sentiment/">http://ai.stanford.edu/~amaas/data/sentiment/</a>
</p>

<hr>
<h2 id='normalize'>Matrix normalization</h2><span id='topic+normalize'></span>

<h3>Description</h3>

<p>normalize matrix rows using given norm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize(m, norm = c("l1", "l2", "none"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normalize_+3A_m">m</code></td>
<td>
<p><code>matrix</code> (sparse or dense).</p>
</td></tr>
<tr><td><code id="normalize_+3A_norm">norm</code></td>
<td>
<p><code>character</code> the method used to normalize term vectors</p>
</td></tr>
</table>


<h3>Value</h3>

<p>normalized matrix
</p>


<h3>See Also</h3>

<p><a href="#topic+create_dtm">create_dtm</a>
</p>

<hr>
<h2 id='perplexity'>Perplexity of a topic model</h2><span id='topic+perplexity'></span>

<h3>Description</h3>

<p>Given document-term matrix, topic-word distribution, document-topic
distribution calculates perplexity
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perplexity(X, topic_word_distribution, doc_topic_distribution)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="perplexity_+3A_x">X</code></td>
<td>
<p>sparse document-term matrix which contains terms counts. Internally <code>Matrix::RsparseMatrix</code> is used.
If <code>!inherits(X, 'RsparseMatrix')</code> function will try to coerce <code>X</code> to <code>RsparseMatrix</code>
via <code>as()</code> call.</p>
</td></tr>
<tr><td><code id="perplexity_+3A_topic_word_distribution">topic_word_distribution</code></td>
<td>
<p>dense matrix for topic-word distribution. Number of rows = <code>n_topics</code>,
number of columns = <code>vocabulary_size</code>. Sum of elements in each row should be equal to 1 -
each row is a distribution of words over topic.</p>
</td></tr>
<tr><td><code id="perplexity_+3A_doc_topic_distribution">doc_topic_distribution</code></td>
<td>
<p>dense matrix for document-topic distribution. Number of rows = <code>n_documents</code>,
number of columns = <code>n_topics</code>. Sum of elements in each row should be equal to 1 -
each row is a distribution of topics over document.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(text2vec)
data("movie_review")
n_iter = 10
train_ind = 1:200
ids = movie_review$id[train_ind]
txt = tolower(movie_review[['review']][train_ind])
names(txt) = ids
tokens = word_tokenizer(txt)
it = itoken(tokens, progressbar = FALSE, ids = ids)
vocab = create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5, doc_proportion_min = 0.02)
dtm = create_dtm(it, vectorizer = vocab_vectorizer(vocab))
n_topic = 10
model = LDA$new(n_topic, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr  =
  model$fit_transform(dtm, n_iter = n_iter, n_check_convergence = 1,
                      convergence_tol = -1, progressbar = FALSE)
topic_word_distr_10 = model$topic_word_distribution
perplexity(dtm, topic_word_distr_10, doc_topic_distr)

## End(Not run)
</code></pre>

<hr>
<h2 id='prepare_analogy_questions'>Prepares list of analogy questions</h2><span id='topic+prepare_analogy_questions'></span>

<h3>Description</h3>

<p>This function prepares a list of questions from a
<code>questions-words.txt</code> format. For full examples see <a href="#topic+GloVe">GloVe</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_analogy_questions(questions_file_path, vocab_terms)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_analogy_questions_+3A_questions_file_path">questions_file_path</code></td>
<td>
<p><code>character</code> path to questions file.</p>
</td></tr>
<tr><td><code id="prepare_analogy_questions_+3A_vocab_terms">vocab_terms</code></td>
<td>
<p><code>character</code> words which we have in the
vocabulary and word embeddings matrix.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+check_analogy_accuracy">check_analogy_accuracy</a>, <a href="#topic+GloVe">GloVe</a>
</p>

<hr>
<h2 id='print.text2vec_vocabulary'>Printing Vocabulary</h2><span id='topic+print.text2vec_vocabulary'></span>

<h3>Description</h3>

<p>Print a vocabulary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'text2vec_vocabulary'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.text2vec_vocabulary_+3A_x">x</code></td>
<td>
<p>vocabulary</p>
</td></tr>
<tr><td><code id="print.text2vec_vocabulary_+3A_...">...</code></td>
<td>
<p>optional arguments to print methods.</p>
</td></tr>
</table>

<hr>
<h2 id='prune_vocabulary'>Prune vocabulary</h2><span id='topic+prune_vocabulary'></span>

<h3>Description</h3>

<p>This function filters the input vocabulary and throws out very
frequent and very infrequent terms. See examples in for the
<a href="#topic+vocabulary">vocabulary</a> function. The parameter <code>vocab_term_max</code> can
also be used to limit the absolute size of the vocabulary to only the most
frequently used terms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prune_vocabulary(vocabulary, term_count_min = 1L, term_count_max = Inf,
  doc_proportion_min = 0, doc_proportion_max = 1, doc_count_min = 1L,
  doc_count_max = Inf, vocab_term_max = Inf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prune_vocabulary_+3A_vocabulary">vocabulary</code></td>
<td>
<p>a vocabulary from the <a href="#topic+vocabulary">vocabulary</a> function.</p>
</td></tr>
<tr><td><code id="prune_vocabulary_+3A_term_count_min">term_count_min</code></td>
<td>
<p>minimum number of occurences over all documents.</p>
</td></tr>
<tr><td><code id="prune_vocabulary_+3A_term_count_max">term_count_max</code></td>
<td>
<p>maximum number of occurences over all documents.</p>
</td></tr>
<tr><td><code id="prune_vocabulary_+3A_doc_proportion_min">doc_proportion_min</code></td>
<td>
<p>minimum proportion of documents which should contain term.</p>
</td></tr>
<tr><td><code id="prune_vocabulary_+3A_doc_proportion_max">doc_proportion_max</code></td>
<td>
<p>maximum proportion of documents which should contain term.</p>
</td></tr>
<tr><td><code id="prune_vocabulary_+3A_doc_count_min">doc_count_min</code></td>
<td>
<p>term will be kept number of documents contain this term is larger than this value</p>
</td></tr>
<tr><td><code id="prune_vocabulary_+3A_doc_count_max">doc_count_max</code></td>
<td>
<p>term will be kept number of documents contain this term is smaller than this value</p>
</td></tr>
<tr><td><code id="prune_vocabulary_+3A_vocab_term_max">vocab_term_max</code></td>
<td>
<p>maximum number of terms in vocabulary.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+vocabulary">vocabulary</a>
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+fit'></span><span id='topic+fit_transform'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>mlapi</dt><dd><p><code><a href="mlapi.html#topic+fit">fit</a></code>, <code><a href="mlapi.html#topic+fit_transform">fit_transform</a></code></p>
</dd>
</dl>

<hr>
<h2 id='RelaxedWordMoversDistance'>Creates Relaxed Word Movers Distance (RWMD) model</h2><span id='topic+RelaxedWordMoversDistance'></span><span id='topic+RWMD'></span>

<h3>Description</h3>

<p>RWMD model can be used to query the &quot;relaxed word movers distance&quot; from a document to a
collection of documents. RWMD tries to measure distance between query document and collection of documents by
calculating how hard is to transform words from query document into words from each document in collection.
For more detail see following article: <a href="http://mkusner.github.io/publications/WMD.pdf">http://mkusner.github.io/publications/WMD.pdf</a>.
However in contrast to the article above we calculate &quot;easiness&quot; of the convertion of one word into another
by using <b>cosine</b> similarity (but not a euclidean distance).
Also here in text2vec we've implemented effiient RWMD using the tricks from the
Linear-Complexity Relaxed Word Mover's Distance with GPU Acceleration article https://arxiv.org/abs/1711.07227
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RelaxedWordMoversDistance

RWMD
</code></pre>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object.</p>


<h3>Usage</h3>

<p>For usage details see <b>Methods, Arguments and Examples</b> sections.
</p>
<pre>
rwmd = RelaxedWordMoversDistance$new(x, embeddings)
rwmd$sim2(x)
</pre>


<h3>Methods</h3>


<dl>
<dt><code>$new(x, embeddings)</code></dt><dd><p>Constructor for RWMD model.
<code>x</code> - docuent-term matrix which represents collection of
documents against which you want to perform queries. <code>embeddings</code> -
matrix of word embeddings which will be used to calculate similarities
between words (each row represents a word vector).</p>
</dd>
<dt><code>$sim(x)</code></dt><dd><p>calculates similarity from a collection of documents
to collection query documents <code>x</code>.
<code>x</code> here is a document-term matrix which represents the set of query documents</p>
</dd>
<dt><code>$dist(x)</code></dt><dd><p>calculates distance from a collection of documents
to collection query documents <code>x</code>
<code>x</code> here is a document-term matrix which represents the set of query documents</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(text2vec)
library(rsparse)
data("movie_review")
tokens = word_tokenizer(tolower(movie_review$review))
v = create_vocabulary(itoken(tokens))
v = prune_vocabulary(v, term_count_min = 5, doc_proportion_max = 0.5)
it = itoken(tokens)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer)
tcm = create_tcm(it, vectorizer, skip_grams_window = 5)
glove_model = GloVe$new(rank = 50, x_max = 10)
wv = glove_model$fit_transform(tcm, n_iter = 5)
# get average of main and context vectors as proposed in GloVe paper
wv = wv + t(glove_model$components)
rwmd_model = RelaxedWordMoversDistance$new(dtm, wv)
rwms = rwmd_model$sim2(dtm[1:10, ])
head(sort(rwms[1, ], decreasing = T))

## End(Not run)
</code></pre>

<hr>
<h2 id='similarities'>Pairwise Similarity Matrix Computation</h2><span id='topic+similarities'></span><span id='topic+sim2'></span><span id='topic+psim2'></span>

<h3>Description</h3>

<p><code>sim2</code> calculates pairwise similarities between the
rows of two data matrices. <b>Note</b> that some methods work only on sparse matrices and
others work only on dense matrices.
</p>
<p><code>psim2</code> calculates &quot;parallel&quot; similarities between the rows of two data matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim2(x, y = NULL, method = c("cosine", "jaccard"), norm = c("l2",
  "none"))

psim2(x, y, method = c("cosine", "jaccard"), norm = c("l2", "none"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="similarities_+3A_x">x</code></td>
<td>
<p>first matrix.</p>
</td></tr>
<tr><td><code id="similarities_+3A_y">y</code></td>
<td>
<p>second matrix. For <code>sim2</code> <code>y = NULL</code> set by default.
This means that we will assume <code>y = x</code> and calculate similarities between all rows of the <code>x</code>.</p>
</td></tr>
<tr><td><code id="similarities_+3A_method">method</code></td>
<td>
<p><code>character</code>, the similarity measure to be used. One of <code>c("cosine", "jaccard")</code>.</p>
</td></tr>
<tr><td><code id="similarities_+3A_norm">norm</code></td>
<td>
<p><code>character = c("l2", "none")</code> - how to scale input matrices. If they already scaled - use <code>"none"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the similarity matrix using given method.
</p>
<p><code>psim2</code> takes two matrices and return a single vector.
giving the ‘parallel’ similarities of the vectors.
</p>


<h3>Value</h3>

<p><code>sim2</code> returns <code>matrix</code> of similarities between each row of
matrix <code>x</code> and each row of matrix <code>y</code>.
</p>
<p><code>psim2</code> returns <code>vector</code> of &quot;parallel&quot; similarities between rows of <code>x</code> and <code>y</code>.
</p>

<hr>
<h2 id='split_into'>Split a vector for parallel processing</h2><span id='topic+split_into'></span>

<h3>Description</h3>

<p>This function splits a vector into <code>n</code> parts of roughly
equal size. These splits can be used for parallel processing. In general,
<code>n</code> should be equal to the number of jobs you want to run, which
should be the number of cores you want to use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_into(vec, n)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="split_into_+3A_vec">vec</code></td>
<td>
<p>input vector</p>
</td></tr>
<tr><td><code id="split_into_+3A_n">n</code></td>
<td>
<p><code>integer</code> desired number of chunks</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>list</code> with <code>n</code> elements, each of roughly equal length
</p>

<hr>
<h2 id='TfIdf'>TfIdf</h2><span id='topic+TfIdf'></span>

<h3>Description</h3>

<p>Creates TfIdf(Latent semantic analysis) model.
&quot;smooth&quot; IDF (default) is defined as follows: <code>idf = log(1 + (# documents in the corpus) / (# documents where the term appears) )</code>
&quot;non-smooth&quot; IDF is defined as follows: <code>idf = log((# documents in the corpus) / (# documents where the term appears) )</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TfIdf
</code></pre>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object.</p>


<h3>Details</h3>

<p>Term Frequency Inverse Document Frequency
</p>


<h3>Usage</h3>

<p>For usage details see <b>Methods, Arguments and Examples</b> sections.
</p>
<pre>
tfidf = TfIdf$new(smooth_idf = TRUE, norm = c('l1', 'l2', 'none'), sublinear_tf = FALSE)
tfidf$fit_transform(x)
tfidf$transform(x)
</pre>


<h3>Methods</h3>


<dl>
<dt><code>$new(smooth_idf = TRUE, norm = c("l1", "l2", "none"), sublinear_tf = FALSE)</code></dt><dd><p>Creates tf-idf model</p>
</dd>
<dt><code>$fit_transform(x)</code></dt><dd><p>fit model to an input sparse matrix (preferably in &quot;dgCMatrix&quot;
format) and then transforms it.</p>
</dd>
<dt><code>$transform(x)</code></dt><dd><p>transform new data <code>x</code> using tf-idf from train data</p>
</dd>
</dl>



<h3>Arguments</h3>


<dl>
<dt>tfidf</dt><dd><p>A <code>TfIdf</code> object</p>
</dd>
<dt>x</dt><dd><p>An input term-co-occurence matrix. Preferably in <code>dgCMatrix</code> format</p>
</dd>
<dt>smooth_idf</dt><dd><p><code>TRUE</code> smooth IDF weights by adding one to document
frequencies, as if an extra document was seen containing every term in the
collection exactly once.</p>
</dd>
<dt>norm</dt><dd><p><code>c("l1", "l2", "none")</code> Type of normalization to apply to term vectors.
<code>"l1"</code> by default, i.e., scale by the number of words in the document. </p>
</dd>
<dt>sublinear_tf</dt><dd><p><code>FALSE</code> Apply sublinear term-frequency scaling, i.e.,
replace the term frequency with <code>1 + log(TF)</code></p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data("movie_review")
N = 100
tokens = word_tokenizer(tolower(movie_review$review[1:N]))
dtm = create_dtm(itoken(tokens), hash_vectorizer())
model_tfidf = TfIdf$new()
dtm_tfidf = model_tfidf$fit_transform(dtm)
</code></pre>

<hr>
<h2 id='tokenizers'>Simple tokenization functions for string splitting</h2><span id='topic+tokenizers'></span><span id='topic+word_tokenizer'></span><span id='topic+char_tokenizer'></span><span id='topic+space_tokenizer'></span><span id='topic+postag_lemma_tokenizer'></span>

<h3>Description</h3>

<p>Few simple tokenization functions. For more comprehensive list see <code>tokenizers</code> package:
<a href="https://cran.r-project.org/package=tokenizers">https://cran.r-project.org/package=tokenizers</a>.
Also check <code>stringi::stri_split_*</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>word_tokenizer(strings, ...)

char_tokenizer(strings, ...)

space_tokenizer(strings, sep = " ", xptr = FALSE, ...)

postag_lemma_tokenizer(strings, udpipe_model, tagger = "default",
  tokenizer = "tokenizer", pos_keep = character(0),
  pos_remove = c("PUNCT", "DET", "ADP", "SYM", "PART", "SCONJ", "CCONJ",
  "AUX", "X", "INTJ"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tokenizers_+3A_strings">strings</code></td>
<td>
<p><code>character</code> vector</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_...">...</code></td>
<td>
<p>other parameters (usually not used - see source code for details).</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_sep">sep</code></td>
<td>
<p><code>character</code>, <code>nchar(sep)</code> = 1 - split strings by this character.</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_xptr">xptr</code></td>
<td>
<p><code>logical</code> tokenize at C++ level - could speed-up by 15-50%.</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_udpipe_model">udpipe_model</code></td>
<td>
<p>- udpipe model, can be loaded with <code>?udpipe::udpipe_load_model</code></p>
</td></tr>
<tr><td><code id="tokenizers_+3A_tagger">tagger</code></td>
<td>
<p><code>"default"</code> - tagger parameter as per <code>?udpipe::udpipe_annotate</code> docs.</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_tokenizer">tokenizer</code></td>
<td>
<p><code>"tokenizer"</code> - tokenizer parameter as per <code>?udpipe::udpipe_annotate</code> docs.</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_pos_keep">pos_keep</code></td>
<td>
<p><code>character(0)</code> specifies which tokens to keep. <code>character(0)</code> means to keep all of them</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_pos_remove">pos_remove</code></td>
<td>
<p><code>c("PUNCT", "DET", "ADP", "SYM", "PART", "SCONJ", "CCONJ", "AUX", "X", "INTJ")</code> - which tokens to remove.
<code>character(0)</code> is equal to not remove any.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>list</code> of <code>character</code> vectors. Each element of list contains vector of tokens.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>doc = c("first  second", "bla, bla, blaa")
# split by words
word_tokenizer(doc)
#faster, but far less general - perform split by a fixed single whitespace symbol.
space_tokenizer(doc, " ")
</code></pre>

<hr>
<h2 id='vectorizers'>Vocabulary and hash vectorizers</h2><span id='topic+vectorizers'></span><span id='topic+vocab_vectorizer'></span><span id='topic+hash_vectorizer'></span>

<h3>Description</h3>

<p>This function creates an object (closure) which defines on how to
transform list of tokens into vector space - i.e. how to map words to indices.
It supposed to be used only as argument to <a href="#topic+create_dtm">create_dtm</a>, <a href="#topic+create_tcm">create_tcm</a>,
<a href="#topic+create_vocabulary">create_vocabulary</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vocab_vectorizer(vocabulary)

hash_vectorizer(hash_size = 2^18, ngram = c(1L, 1L),
  signed_hash = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vectorizers_+3A_vocabulary">vocabulary</code></td>
<td>
<p><code>text2vec_vocabulary</code> object, see <a href="#topic+create_vocabulary">create_vocabulary</a>.</p>
</td></tr>
<tr><td><code id="vectorizers_+3A_hash_size">hash_size</code></td>
<td>
<p><code>integer</code> The number of of hash-buckets for the feature
hashing trick. The number must be greater than 0, and preferably it will be
a power of 2.</p>
</td></tr>
<tr><td><code id="vectorizers_+3A_ngram">ngram</code></td>
<td>
<p><code>integer</code> vector. The lower and upper boundary of the range
of n-values for different n-grams to be extracted. All values of <code>n</code>
such that ngram_min &lt;= n &lt;= ngram_max will be used.</p>
</td></tr>
<tr><td><code id="vectorizers_+3A_signed_hash">signed_hash</code></td>
<td>
<p><code>logical</code>,  indicating whether to use a signed
hash-function to reduce collisions when hashing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vectorizer <code>object</code> (closure).
</p>


<h3>See Also</h3>

<p><a href="#topic+create_dtm">create_dtm</a> <a href="#topic+create_tcm">create_tcm</a> <a href="#topic+create_vocabulary">create_vocabulary</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("movie_review")
N = 100
vectorizer = hash_vectorizer(2 ^ 18, c(1L, 2L))
it = itoken(movie_review$review[1:N], preprocess_function = tolower,
             tokenizer = word_tokenizer, n_chunks = 10)
hash_dtm = create_dtm(it, vectorizer)

it = itoken(movie_review$review[1:N], preprocess_function = tolower,
             tokenizer = word_tokenizer, n_chunks = 10)
v = create_vocabulary(it, c(1L, 1L) )

vectorizer = vocab_vectorizer(v)

it = itoken(movie_review$review[1:N], preprocess_function = tolower,
             tokenizer = word_tokenizer, n_chunks = 10)

dtm = create_dtm(it, vectorizer)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
