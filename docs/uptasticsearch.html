<!DOCTYPE html><html><head><title>Help for package uptasticsearch</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {uptasticsearch}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#chomp_aggs'><p>Aggs query to data.table</p></a></li>
<li><a href='#chomp_hits'><p>Hits to data.tables</p></a></li>
<li><a href='#doc_shared'><p>NULL Object For Common Documentation</p></a></li>
<li><a href='#es_search'><p>Execute an ES query and get a data.table</p></a></li>
<li><a href='#get_fields'><p>Get the names and data types of the indexed fields in an index</p></a></li>
<li><a href='#parse_date_time'><p>Parse date-times from Elasticsearch records</p></a></li>
<li><a href='#unpack_nested_data'><p>Unpack a nested data.table</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Get Data Frame Representations of 'Elasticsearch' Results</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>James Lamb &lt;jaylamb20@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>
    'Elasticsearch' is an open-source, distributed, document-based datastore
    (<a href="https://www.elastic.co/products/elasticsearch">https://www.elastic.co/products/elasticsearch</a>).
    It provides an 'HTTP' 'API' for querying the database and extracting datasets, but that
    'API' was not designed for common data science workflows like pulling large batches of
    records and normalizing those documents into a data frame that can be used as a training
    dataset for statistical models. 'uptasticsearch' provides an interface for 'Elasticsearch'
    that is explicitly designed to make these data science workflows easy and fun.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>assertthat (&ge; 0.2.0), data.table, futile.logger, httr,
jsonlite, purrr, stringr, utils, uuid</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/BSD-3-Clause">BSD_3_clause</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/uptake/uptasticsearch">https://github.com/uptake/uptasticsearch</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/uptake/uptasticsearch/issues">https://github.com/uptake/uptasticsearch/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-09-11 17:29:59 UTC; jlamb</td>
</tr>
<tr>
<td>Author:</td>
<td>James Lamb [aut, cre],
  Nick Paras [aut],
  Austin Dickey [aut],
  Michael Frasco [ctb],
  Weiwen Gu [ctb],
  Will Dearden [ctb],
  Uptake Technologies Inc. [cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-09-11 18:30:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='chomp_aggs'>Aggs query to data.table</h2><span id='topic+chomp_aggs'></span>

<h3>Description</h3>

<p>Given some raw JSON from an aggs query in Elasticsearch, parse the
aggregations into a data.table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chomp_aggs(aggs_json = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chomp_aggs_+3A_aggs_json">aggs_json</code></td>
<td>
<p>A character vector. If its length is greater than 1, its elements will be pasted
together. This can contain a JSON returned from an <code>aggs</code> query in Elasticsearch, or
a filepath or URL pointing at one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table representation of the result or NULL if the aggregation result is empty.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A sample raw result from an aggs query combining date_histogram and extended_stats:
result &lt;- '{"aggregations":{"dateTime":{"buckets":[{"key_as_string":"2016-12-01T00:00:00.000Z",
"key":1480550400000,"doc_count":123,"num_potatoes":{"count":120,"min":0,"max":40,"avg":15,
"sum":1800,"sum_of_squares":28000,"variance":225,"std_deviation":15,"std_deviation_bounds":{
"upper":26,"lower":13}}},{"key_as_string":"2017-01-01T00:00:00.000Z","key":1483228800000,
"doc_count":134,"num_potatoes":{"count":131,"min":0,"max":39,"avg":16,"sum":2096,
"sum_of_squares":34000,"variance":225,"std_deviation":15,"std_deviation_bounds":{"upper":26,
"lower":13}}}]}}}'

# Parse into a data.table
aggDT &lt;- chomp_aggs(aggs_json = result)
print(aggDT)
</code></pre>

<hr>
<h2 id='chomp_hits'>Hits to data.tables</h2><span id='topic+chomp_hits'></span>

<h3>Description</h3>

<p>A function for converting Elasticsearch docs into R data.tables. It
uses <code><a href="jsonlite.html#topic+fromJSON">fromJSON</a></code> with <code>flatten = TRUE</code> to convert a
JSON into an R data.frame, and formats it into a data.table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chomp_hits(hits_json = NULL, keep_nested_data_cols = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chomp_hits_+3A_hits_json">hits_json</code></td>
<td>
<p>A character vector. If its length is greater than 1, its elements will be pasted
together. This can contain a JSON returned from a <code>search</code> query in
Elasticsearch, or a filepath or URL pointing at one.</p>
</td></tr>
<tr><td><code id="chomp_hits_+3A_keep_nested_data_cols">keep_nested_data_cols</code></td>
<td>
<p>a boolean (default TRUE); whether to keep columns that are nested
arrays in the original JSON. A warning will be given if these
columns are deleted.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># A sample raw result from a hits query:
result &lt;- '[{"_source":{"timestamp":"2017-01-01","cust_name":"Austin","details":{
"cust_class":"big_spender","location":"chicago","pastPurchases":[{"film":"The Notebook",
"pmt_amount":6.25},{"film":"The Town","pmt_amount":8.00},{"film":"Zootopia","pmt_amount":7.50,
"matinee":true}]}}},{"_source":{"timestamp":"2017-02-02","cust_name":"James","details":{
"cust_class":"peasant","location":"chicago","pastPurchases":[{"film":"Minions",
"pmt_amount":6.25,"matinee":true},{"film":"Rogue One","pmt_amount":10.25},{"film":"Bridesmaids",
"pmt_amount":8.75},{"film":"Bridesmaids","pmt_amount":6.25,"matinee":true}]}}},{"_source":{
"timestamp":"2017-03-03","cust_name":"Nick","details":{"cust_class":"critic","location":"cannes",
"pastPurchases":[{"film":"Aala Kaf Ifrit","pmt_amount":0,"matinee":true},{
"film":"Dopo la guerra (Apres la Guerre)","pmt_amount":0,"matinee":true},{
"film":"Avengers: Infinity War","pmt_amount":12.75}]}}}]'

# Chomp into a data.table
sampleChompedDT &lt;- chomp_hits(hits_json = result, keep_nested_data_cols = TRUE)
print(sampleChompedDT)

# (Note: use es_search() to get here in one step)

# Unpack by details.pastPurchases
unpackedDT &lt;- unpack_nested_data(chomped_df = sampleChompedDT
                                 , col_to_unpack = "details.pastPurchases")
print(unpackedDT)
</code></pre>

<hr>
<h2 id='doc_shared'>NULL Object For Common Documentation</h2><span id='topic+doc_shared'></span>

<h3>Description</h3>

<p>This is a NULL object with documentation so that later functions can call
inheritParams
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="doc_shared_+3A_es_host">es_host</code></td>
<td>
<p>A string identifying an Elasticsearch host. This should be of the form 
<code>[transfer_protocol][hostname]:[port]</code>. For example, <code>'http://myindex.thing.com:9200'</code>.</p>
</td></tr>
<tr><td><code id="doc_shared_+3A_es_index">es_index</code></td>
<td>
<p>The name of an Elasticsearch index to be queried. Note that passing
<code>NULL</code> is not supported. Technically, not passing an index
to Elasticsearch is legal and results in searching over all indexes.
To be sure that this very expensive query is not executed by accident,
uptasticsearch forbids this. If you want to execute a query over
all indexes in the cluster, set this argument to <code>"_all"</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='es_search'>Execute an ES query and get a data.table</h2><span id='topic+es_search'></span>

<h3>Description</h3>

<p>Given a query and some optional parameters, <code>es_search</code> gets results
from HTTP requests to Elasticsearch and returns a data.table
representation of those results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>es_search(es_host, es_index, size = 10000, query_body = "{}",
  scroll = "5m", max_hits = Inf,
  n_cores = ceiling(parallel::detectCores()/2),
  break_on_duplicates = TRUE, ignore_scroll_restriction = FALSE,
  intermediates_dir = getwd())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="es_search_+3A_es_host">es_host</code></td>
<td>
<p>A string identifying an Elasticsearch host. This should be of the form 
<code>[transfer_protocol][hostname]:[port]</code>. For example, <code>'http://myindex.thing.com:9200'</code>.</p>
</td></tr>
<tr><td><code id="es_search_+3A_es_index">es_index</code></td>
<td>
<p>The name of an Elasticsearch index to be queried. Note that passing
<code>NULL</code> is not supported. Technically, not passing an index
to Elasticsearch is legal and results in searching over all indexes.
To be sure that this very expensive query is not executed by accident,
uptasticsearch forbids this. If you want to execute a query over
all indexes in the cluster, set this argument to <code>"_all"</code>.</p>
</td></tr>
<tr><td><code id="es_search_+3A_size">size</code></td>
<td>
<p>Number of records per page of results. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-from-size.html">Elasticsearch docs</a> for more.
Note that this will be reset to 0 if you submit a <code>query_body</code> with
an &quot;aggs&quot; request in it. Also see <code>max_hits</code>.</p>
</td></tr>
<tr><td><code id="es_search_+3A_query_body">query_body</code></td>
<td>
<p>String with a valid Elasticsearch query. Default is an empty query.</p>
</td></tr>
<tr><td><code id="es_search_+3A_scroll">scroll</code></td>
<td>
<p>How long should the scroll context be held open? This should be a
duration string like &quot;1m&quot; (for one minute) or &quot;15s&quot; (for 15 seconds).
The scroll context will be refreshed every time you ask Elasticsearch
for another record, so this parameter should just be the amount of
time you expect to pass between requests. See the
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html">Elasticsearch scroll/pagination docs</a>
for more information.</p>
</td></tr>
<tr><td><code id="es_search_+3A_max_hits">max_hits</code></td>
<td>
<p>Integer. If specified, <code>es_search</code> will stop pulling data as soon
as it has pulled this many hits. Default is <code>Inf</code>, meaning that
all possible hits will be pulled.</p>
</td></tr>
<tr><td><code id="es_search_+3A_n_cores">n_cores</code></td>
<td>
<p>Number of cores to distribute fetching and processing over.</p>
</td></tr>
<tr><td><code id="es_search_+3A_break_on_duplicates">break_on_duplicates</code></td>
<td>
<p>Boolean, defaults to TRUE. <code>es_search</code> uses the size of the final object it returns
to check whether or not some data were lost during the processing.
If you have duplicates in the source data, you will have to set this flag to
FALSE and just trust that no data have been lost. Sorry :( .</p>
</td></tr>
<tr><td><code id="es_search_+3A_ignore_scroll_restriction">ignore_scroll_restriction</code></td>
<td>
<p>There is a cost associated with keeping an
Elasticsearch scroll context open. By default,
this function does not allow arguments to <code>scroll</code>
which exceed one hour. This is done to prevent
costly mistakes made by novice Elasticsearch users.
If you understand the cost of keeping the context
open for a long time and would like to pass a <code>scroll</code>
value longer than an hour, set <code>ignore_scroll_restriction</code>
to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="es_search_+3A_intermediates_dir">intermediates_dir</code></td>
<td>
<p>When scrolling over search results, this function writes
intermediate results to disk. By default, 'es_search' will create a temporary
directory in whatever working directory the function is called from. If you
want to change this behavior, provide a path here. 'es_search' will create
and write to a temporary directory under whatever path you provide.</p>
</td></tr>
</table>


<h3>References</h3>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/search-request-scroll.html">ES 6 scrolling strategy</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

###=== Example 1: Get low-scoring food survey results ===###

query_body &lt;- '{"query":{"filtered":{"filter":{"bool":{"must":[
               {"exists":{"field":"customer_comments"}},
               {"terms":{"overall_satisfaction":["very low","low"]}}]}}},
               "query":{"match_phrase":{"customer_comments":"food"}}}}'

# Execute the query, parse into a data.table
commentDT &lt;- es_search(es_host = 'http://mydb.mycompany.com:9200'
                       , es_index = "survey_results"
                       , query_body = query_body
                       , scroll = "1m"
                       , n_cores = 4)

###=== Example 2: Time series agg features ===###

# Create query that will give you daily summary stats for revenue
query_body &lt;- '{"query":{"filtered":{"filter":{"bool":{"must":[
               {"exists":{"field":"pmt_amount"}}]}}}},
               "aggs":{"timestamp":{"date_histogram":{"field":"timestamp","interval":"day"},
               "aggs":{"revenue":{"extended_stats":{"field":"pmt_amount"}}}}},"size":0}'

# Execute the query and get the result
resultDT &lt;- es_search(es_host = "http://es.custdb.mycompany.com:9200"
                      , es_index = 'ticket_sales'
                      , query_body = query_body)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_fields'>Get the names and data types of the indexed fields in an index</h2><span id='topic+get_fields'></span>

<h3>Description</h3>

<p>For a given Elasticsearch index, return the mapping from field name
to data type for all indexed fields.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_fields(es_host, es_indices = "_all")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_fields_+3A_es_host">es_host</code></td>
<td>
<p>A string identifying an Elasticsearch host. This should be of the form 
<code>[transfer_protocol][hostname]:[port]</code>. For example, <code>'http://myindex.thing.com:9200'</code>.</p>
</td></tr>
<tr><td><code id="get_fields_+3A_es_indices">es_indices</code></td>
<td>
<p>A character vector that contains the names of indices for
which to get mappings. Default is <code>'_all'</code>, which means
get the mapping for all indices. Names of indices can be
treated as regular expressions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table containing four columns: index, type, field, and data_type
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# get the mapping for all indexed fields in the ticket_sales and customers indices
mappingDT &lt;- get_fields(es_host = "http://es.custdb.mycompany.com:9200"
                              , es_indices = c("ticket_sales", "customers"))

## End(Not run)
</code></pre>

<hr>
<h2 id='parse_date_time'>Parse date-times from Elasticsearch records</h2><span id='topic+parse_date_time'></span>

<h3>Description</h3>

<p>Given a data.table with date-time strings,
this function converts those dates-times to type POSIXct with the appropriate
time zone. Assumption is that dates are of the form &quot;2016-07-25T22:15:19Z&quot;
where T is just a separator and the last letter is a military timezone.
</p>
<p>This is a side-effect-free function: it returns a new data.table and the
input data.table is unmodified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_date_time(input_df, date_cols, assume_tz = "UTC")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parse_date_time_+3A_input_df">input_df</code></td>
<td>
<p>a data.table with one or more date-time columns you want to convert</p>
</td></tr>
<tr><td><code id="parse_date_time_+3A_date_cols">date_cols</code></td>
<td>
<p>Character vector of column names to convert. Columns should have
string dates of the form &quot;2016-07-25T22:15:19Z&quot;.</p>
</td></tr>
<tr><td><code id="parse_date_time_+3A_assume_tz">assume_tz</code></td>
<td>
<p>Timezone to convert to if parsing fails. Default is UTC</p>
</td></tr>
</table>


<h3>References</h3>

<p><a href="https://www.timeanddate.com/time/zones/military">https://www.timeanddate.com/time/zones/military</a>
</p>
<p><a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones">https://en.wikipedia.org/wiki/List_of_tz_database_time_zones</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Sample es_search(), chomp_hits(), or chomp_aggs() output:
someDT &lt;- data.table::data.table(id = 1:5
                                 , company = c("Apple", "Apple", "Banana", "Banana", "Cucumber")
                                 , timestamp = c("2015-03-14T09:26:53B", "2015-03-14T09:26:54B"
                                                 , "2031-06-28T08:53:07Z", "2031-06-28T08:53:08Z"
                                                 , "2000-01-01"))

# Note that the date field is character right now
str(someDT)

# Let's fix that!
someDT &lt;- parse_date_time(input_df = someDT
                          , date_cols = "timestamp"
                          , assume_tz = "UTC")
str(someDT)
</code></pre>

<hr>
<h2 id='unpack_nested_data'>Unpack a nested data.table</h2><span id='topic+unpack_nested_data'></span>

<h3>Description</h3>

<p>After calling a <code>chomp_*</code> function or <code>es_search</code>, if
you had a nested array in the JSON, its corresponding column in the
resulting data.table is a data.frame itself (or a list of vectors). This
function expands that nested column out, adding its data to the original
data.table, and duplicating metadata down the rows as necessary.
</p>
<p>This is a side-effect-free function: it returns a new data.table and the
input data.table is unmodified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unpack_nested_data(chomped_df, col_to_unpack)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unpack_nested_data_+3A_chomped_df">chomped_df</code></td>
<td>
<p>a data.table</p>
</td></tr>
<tr><td><code id="unpack_nested_data_+3A_col_to_unpack">col_to_unpack</code></td>
<td>
<p>a character vector of length one: the column name to
unpack</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># A sample raw result from a hits query:
result &lt;- '[{"_source":{"timestamp":"2017-01-01","cust_name":"Austin","details":{
"cust_class":"big_spender","location":"chicago","pastPurchases":[{"film":"The Notebook",
"pmt_amount":6.25},{"film":"The Town","pmt_amount":8.00},{"film":"Zootopia","pmt_amount":7.50,
"matinee":true}]}}},{"_source":{"timestamp":"2017-02-02","cust_name":"James","details":{
"cust_class":"peasant","location":"chicago","pastPurchases":[{"film":"Minions",
"pmt_amount":6.25,"matinee":true},{"film":"Rogue One","pmt_amount":10.25},{"film":"Bridesmaids",
"pmt_amount":8.75},{"film":"Bridesmaids","pmt_amount":6.25,"matinee":true}]}}},{"_source":{
"timestamp":"2017-03-03","cust_name":"Nick","details":{"cust_class":"critic","location":"cannes",
"pastPurchases":[{"film":"Aala Kaf Ifrit","pmt_amount":0,"matinee":true},{
"film":"Dopo la guerra (Apres la Guerre)","pmt_amount":0,"matinee":true},{
"film":"Avengers: Infinity War","pmt_amount":12.75}]}}}]'

# Chomp into a data.table
sampleChompedDT &lt;- chomp_hits(hits_json = result, keep_nested_data_cols = TRUE)
print(sampleChompedDT)

# (Note: use es_search() to get here in one step)

# Unpack by details.pastPurchases
unpackedDT &lt;- unpack_nested_data(chomped_df = sampleChompedDT
                                 , col_to_unpack = "details.pastPurchases")
print(unpackedDT)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
