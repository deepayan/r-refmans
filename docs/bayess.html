<!DOCTYPE html><html><head><title>Help for package bayess</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bayess}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ardipper'><p>Accept-reject algorithm for the open population capture-recapture model</p></a></li>
<li><a href='#ARllog'>
<p>log-likelihood associated with an AR(p) model defined either through</p>
its natural coefficients or through the roots of the associated lag-polynomial</a></li>
<li><a href='#ARmh'>
<p>Metropolis&ndash;Hastings evaluation of the posterior associated with an AR(p) model</p></a></li>
<li><a href='#bank'>
<p>bank dataset (Chapter 4)</p></a></li>
<li><a href='#BayesReg'>
<p>Bayesian linear regression output</p></a></li>
<li><a href='#caterpillar'>
<p>Pine processionary caterpillar dataset</p></a></li>
<li><a href='#datha'>
<p>Non-standardised Licence dataset</p></a></li>
<li><a href='#Dnadataset'>
<p>DNA sequence of an HIV genome</p></a></li>
<li><a href='#eurodip'><p>European Dipper dataset</p></a></li>
<li><a href='#Eurostoxx50'>
<p>Eurostoxx50 exerpt dataset</p></a></li>
<li><a href='#gibbs'>
<p>Gibbs sampler and Chib's evidence approximation</p>
for a generic univariate mixture of normal distributions</a></li>
<li><a href='#gibbscap1'>
<p>Gibbs sampler for the two-stage open population capture-recapture</p>
model</a></li>
<li><a href='#gibbscap2'>
<p>Gibbs sampling for the Arnason-Schwarz capture-recapture model</p></a></li>
<li><a href='#gibbsmean'>
<p>Gibbs sampler on a mixture posterior distribution with unknown means</p></a></li>
<li><a href='#gibbsnorm'>
<p>Gibbs sampler for a generic mixture posterior distribution</p></a></li>
<li><a href='#hmflatlogit'>
<p>Metropolis-Hastings for the logit model under a flat prior</p></a></li>
<li><a href='#hmflatloglin'>
<p>Metropolis-Hastings for the log-linear model under a flat prior</p></a></li>
<li><a href='#hmflatprobit'>
<p>Metropolis-Hastings for the probit model under a flat prior</p></a></li>
<li><a href='#hmhmm'>
<p>Estimation of a hidden Markov model with 2 hidden and 4 observed states</p></a></li>
<li><a href='#hmmeantemp'>
<p>Metropolis-Hastings with tempering steps for the mean mixture posterior model</p></a></li>
<li><a href='#hmnoinflogit'>
<p>Metropolis-Hastings for the logit model under a noninformative prior</p></a></li>
<li><a href='#hmnoinfloglin'>
<p>Metropolis-Hastings for the log-linear model under a noninformative prior</p></a></li>
<li><a href='#hmnoinfprobit'>
<p>Metropolis-Hastings for the probit model under a noninformative prior</p></a></li>
<li><a href='#isinghm'>
<p>Metropolis-Hastings for the Ising model</p></a></li>
<li><a href='#isingibbs'>
<p>Gibbs sampler for the Ising model</p></a></li>
<li><a href='#Laichedata'>
<p>Laiche dataset</p></a></li>
<li><a href='#logitll'>
<p>Log-likelihood of the logit model</p></a></li>
<li><a href='#logitnoinflpost'>
<p>Log of the posterior distribution for the probit model under a noninformative prior</p></a></li>
<li><a href='#loglinll'>
<p>Log of the likelihood of the log-linear model</p></a></li>
<li><a href='#loglinnoinflpost'>
<p>Log of the posterior density for the log-linear model under a noninformative prior</p></a></li>
<li><a href='#MAllog'>
<p>log-likelihood associated with an MA(p) model</p></a></li>
<li><a href='#MAmh'>
<p>Metropolis&ndash;Hastings evaluation of the posterior associated with an MA(p) model</p></a></li>
<li><a href='#Menteith'>
<p>Grey-level image of the Lake of Menteith</p></a></li>
<li><a href='#ModChoBayesReg'>
<p>Bayesian model choice procedure for the linear model</p></a></li>
<li><a href='#normaldata'>
<p>Normal dataset</p></a></li>
<li><a href='#pbino'>
<p>Posterior expectation for the binomial capture-recapture model</p></a></li>
<li><a href='#pcapture'>
<p>Posterior probabilities for the multiple stage capture-recapture model</p></a></li>
<li><a href='#pdarroch'>
<p>Posterior probabilities for the Darroch model</p></a></li>
<li><a href='#plotmix'>
<p>Graphical representation of a normal mixture log-likelihood</p></a></li>
<li><a href='#pottsgibbs'>
<p>Gibbs sampler for the Potts model</p></a></li>
<li><a href='#pottshm'>
<p>Metropolis-Hastings sampler for a Potts model with <code>ncol</code> classes</p></a></li>
<li><a href='#probet'>
<p>Coverage of the interval <code class="reqn">(a,b)</code> by the Beta cdf</p></a></li>
<li><a href='#probitll'>
<p>Log-likelihood of the probit model</p></a></li>
<li><a href='#probitnoinflpost'>
<p>Log of the posterior density for the probit model under a non-informative model</p></a></li>
<li><a href='#rdirichlet'>
<p>Random generator for the Dirichlet distribution</p></a></li>
<li><a href='#reconstruct'>
<p>Image reconstruction for the Potts model with six classes</p></a></li>
<li><a href='#solbeta'>
<p>Recursive resolution of beta prior calibration</p></a></li>
<li><a href='#sumising'>
<p>Approximation by path sampling of the normalising constant</p>
for the Ising model</a></li>
<li><a href='#thresh'>
<p>Bound for the accept-reject algorithm in Chapter 5</p></a></li>
<li><a href='#truncnorm'>
<p>Random simulator for the truncated normal distribution</p></a></li>
<li><a href='#xneig4'>
<p>Number of neighbours with the same colour</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Bayesian Essentials with R</td>
</tr>
<tr>
<td>Version:</td>
<td>1.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-04</td>
</tr>
<tr>
<td>Depends:</td>
<td>stats, mnormt, gplots, combinat</td>
</tr>
<tr>
<td>Author:</td>
<td>Jean-Michel Marin [aut, cre],
        Christian P. Robert [aut]  </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jean-Michel Marin &lt;jean-michel.marin@umontpellier.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Allows the reenactment of the R programs used in 
        the book Bayesian Essentials with R without further programming. 
        R code being available as well, they can be modified by the user
        to conduct one's own simulations. 
	    Marin J.-M. and Robert C. P. (2014) &lt;<a href="https://doi.org/10.1007%2F978-1-4614-8687-9">doi:10.1007/978-1-4614-8687-9</a>&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.r-project.org">https://www.r-project.org</a>, <a href="https://github.com/jmm34/bayess">https://github.com/jmm34/bayess</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-04 10:30:19 UTC; marin</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-06 14:30:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='ardipper'>Accept-reject algorithm for the open population capture-recapture model</h2><span id='topic+ardipper'></span>

<h3>Description</h3>

<p>This function is associated with Chapter 5 on capture-recapture model.
It simulates samples from the non-standard 
distribution on <code class="reqn">r_1</code>, the number of individuals vanishing
between the first and second experiments, as expressed in (5.4)
in the book, conditional on <code class="reqn">r_2</code>, the number of individuals vanishing
between the second and third experiments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ardipper(nsimu, n1, c2, c3, r2, q1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ardipper_+3A_nsimu">nsimu</code></td>
<td>
<p>number of simulations</p>
</td></tr>
<tr><td><code id="ardipper_+3A_n1">n1</code></td>
<td>
<p>first capture sample size</p>
</td></tr>
<tr><td><code id="ardipper_+3A_c2">c2</code></td>
<td>
<p>number of individuals recaptured during the second experiment</p>
</td></tr>
<tr><td><code id="ardipper_+3A_c3">c3</code></td>
<td>
<p>number of individuals recaptured during the third experiment</p>
</td></tr>
<tr><td><code id="ardipper_+3A_r2">r2</code></td>
<td>
<p>number of individuals vanishing between the second and third experiments</p>
</td></tr>
<tr><td><code id="ardipper_+3A_q1">q1</code></td>
<td>
<p>probability of disappearing from the population</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sample of <code>nsimu</code> integers
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ardipper(10,11,3,1,0,.1)
</code></pre>

<hr>
<h2 id='ARllog'>
log-likelihood associated with an AR(p) model defined either through
its natural coefficients or through the roots of the associated lag-polynomial
</h2><span id='topic+ARllog'></span>

<h3>Description</h3>

<p>This function is related to Chapter 6 on dynamical models.
It returns the numerical value of the log-likelihood associated with a time
series and an AR(p) model, along with the natural coefficients <code class="reqn">psi</code> of the AR(p) model
if it is defined via the roots <code>lr</code> and <code>lc</code> of the associated lag-polynomial.
The function thus uses either the natural parameterisation of the AR(p) model
</p>
<p style="text-align: center;"><code class="reqn">x_t - \mu + \sum_{i=1}^p \psi_i (x_{t-i}-\mu) = \varepsilon_t</code>
</p>

<p>or the parameterisation via the lag-polynomial roots
</p>
<p style="text-align: center;"><code class="reqn">\prod_{i=1}^p (1-\lambda_i B) x_t = \varepsilon_t</code>
</p>

<p>where <code class="reqn">B^j x_t = x_{t-j}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ARllog(p,dat,pr, pc, lr, lc, mu, sig2, compsi = TRUE, pepsi = c(1, rep(0, p)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ARllog_+3A_p">p</code></td>
<td>
<p>order of the AR<code class="reqn">(p)</code> model</p>
</td></tr>
<tr><td><code id="ARllog_+3A_dat">dat</code></td>
<td>
<p>time series modelled by the AR<code class="reqn">(p)</code> model</p>
</td></tr>
<tr><td><code id="ARllog_+3A_pr">pr</code></td>
<td>
<p>number of real roots</p>
</td></tr>
<tr><td><code id="ARllog_+3A_pc">pc</code></td>
<td>
<p>number of non-conjugate complex roots</p>
</td></tr>
<tr><td><code id="ARllog_+3A_lr">lr</code></td>
<td>
<p>real roots</p>
</td></tr>
<tr><td><code id="ARllog_+3A_lc">lc</code></td>
<td>
<p>complex roots, stored as real part for odd indices and imaginary part for even indices</p>
</td></tr>
<tr><td><code id="ARllog_+3A_mu">mu</code></td>
<td>
<p>drift coefficient <code class="reqn">\mu</code> such that <code class="reqn">(x_t-\mu)_t</code> is a standard AR<code class="reqn">(p)</code> series</p>
</td></tr>
<tr><td><code id="ARllog_+3A_sig2">sig2</code></td>
<td>
<p>variance of the Gaussian white noise <code class="reqn">(\varepsilon_t)_t</code></p>
</td></tr>
<tr><td><code id="ARllog_+3A_compsi">compsi</code></td>
<td>
<p>boolean variable indicating whether the coefficients <code class="reqn">\psi_i</code> need to be retrievedfrom the roots of the lag-polynomial, i.e. if the model is defined by <code>pepsi</code> (when <code>compsi</code> is <code>FALSE</code>) or by <code>lr</code> and <code>lc</code> (when <code>compsi</code> is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="ARllog_+3A_pepsi">pepsi</code></td>
<td>
<p>potential p+1 coefficients <code class="reqn">\psi_i</code> if <code>compsi</code> is <code>FALSE</code>, with 1 asthe compulsory first value</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>ll</code></td>
<td>
<p>value of the log-likelihood</p>
</td></tr>
<tr><td><code>ps</code></td>
<td>
<p>vector of the <code class="reqn">\psi_i</code>'s</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+MAllog">MAllog</a></code>,<code><a href="#topic+ARmh">ARmh</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ARllog(p=3,dat=faithful[,1],pr=3,pc=0,
lr=c(-.1,.5,.2),lc=0,mu=0,sig2=var(faithful[,1]),compsi=FALSE,pepsi=c(1,rep(.1,3)))
</code></pre>

<hr>
<h2 id='ARmh'>
Metropolis&ndash;Hastings evaluation of the posterior associated with an AR(p) model
</h2><span id='topic+ARmh'></span>

<h3>Description</h3>

<p>This function is associated with Chapter 6 on dynamic models. It
implements a Metropolis&ndash;Hastings algorithm on the coefficients
of the AR(p) model resorting to a simulation of the real and complex roots of
the model. It includes jumps between adjacent numbers of real and complex roots,
as well as random modifications for a given number of real and complex roots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ARmh(x, p = 1, W = 10^3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ARmh_+3A_x">x</code></td>
<td>

<p>time series to be modelled as an AR(p) model
</p>
</td></tr>
<tr><td><code id="ARmh_+3A_p">p</code></td>
<td>

<p>order of the AR(p) model
</p>
</td></tr>
<tr><td><code id="ARmh_+3A_w">W</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Even though <em>Bayesian Essentials with R</em> does not cover the reversible jump MCMC
techniques due to Green (1995), which allows to explore spaces of different dimensions
at once, this function relies on a simple form of reversible jump MCMC when moving from
one number of complex roots to the next.
</p>


<h3>Value</h3>



<table>
<tr><td><code>psis</code></td>
<td>
<p>matrix of simulated <code class="reqn">\psi_i</code>'s</p>
</td></tr>
<tr><td><code>mus</code></td>
<td>
<p>vector of simulated <code class="reqn">\mu</code>'s</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>vector of simulated <code class="reqn">\sigma^2</code>'s</p>
</td></tr>
<tr><td><code>llik</code></td>
<td>
<p>vector of corresponding likelihood values (useful to check for convergence)</p>
</td></tr>
<tr><td><code>pcomp</code></td>
<td>
<p>vector of simulated numbers of complex roots</p>
</td></tr>
</table>


<h3>References</h3>

<p>Green, P.J. (1995) Reversible jump MCMC computaton and Bayesian model choice.
<em>Biometrika</em> <b>82</b>, 711&ndash;732.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ARllog">ARllog</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Eurostoxx50)
x=Eurostoxx50[, 4]
resAR5=ARmh(x=x,p=5,W=50)
plot(resAR5$mus,type="l",col="steelblue4",xlab="Iterations",ylab=expression(mu))
</code></pre>

<hr>
<h2 id='bank'>
bank dataset (Chapter 4)
</h2><span id='topic+bank'></span>

<h3>Description</h3>

<p>The bank dataset we analyze in the first part
of Chapter 3 comes from Flury and Riedwyl (1988) and
is made of four measurements on 100 genuine Swiss banknotes and 100 counterfeit ones.
The response variable <code class="reqn">y</code> is thus the status of the banknote, where 0 stands 
for genuine and 1 stands for counterfeit, while the explanatory factors are bill
measurements.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bank)</code></pre>


<h3>Format</h3>

<p>A data frame with 200 observations on the following 5 variables.
</p>

<dl>
<dt><code>x1</code></dt><dd><p>length of the bill (in mm)</p>
</dd>
<dt><code>x2</code></dt><dd><p>width of the left edge (in mm)</p>
</dd>
<dt><code>x3</code></dt><dd><p>width of the right edge (in mm)</p>
</dd>
<dt><code>x4</code></dt><dd><p>bottom margin width (in mm)</p>
</dd>
<dt><code>y</code></dt><dd><p>response variable</p>
</dd>
</dl>



<h3>Source</h3>

<p>Flury, B. and Riedwyl, H. (1988) <em>Multivariate Statistics. A Practical Approach</em>, Chapman and Hall,
London-New York.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)
summary(bank)
</code></pre>

<hr>
<h2 id='BayesReg'>
Bayesian linear regression output
</h2><span id='topic+BayesReg'></span>

<h3>Description</h3>

<p>This function contains the R code for the implementation of Zellner's <code class="reqn">G</code>-prior analysis of
the regression model as described in Chapter 3. The purpose of <code>BayesRef</code>
is dual: first, this R function shows how easily automated
this approach can be. Second, it also illustrates how it is possible to get exactly the same 
type of output as the standard R function <code>summary(lm(y~X))</code>. In particular,
it calculates the Bayes factors for variable selection, more precisely single variable exclusion.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BayesReg(y, X, g = length(y), betatilde = rep(0, dim(X)[2]), prt = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BayesReg_+3A_y">y</code></td>
<td>

<p>response variable
</p>
</td></tr>
<tr><td><code id="BayesReg_+3A_x">X</code></td>
<td>

<p>matrix of regressors
</p>
</td></tr>
<tr><td><code id="BayesReg_+3A_g">g</code></td>
<td>

<p>constant g for the <code class="reqn">G</code>-prior
</p>
</td></tr>
<tr><td><code id="BayesReg_+3A_betatilde">betatilde</code></td>
<td>

<p>prior mean on <code class="reqn">\beta</code>
</p>
</td></tr>
<tr><td><code id="BayesReg_+3A_prt">prt</code></td>
<td>

<p>boolean variable for printing out the standard output
</p>
</td></tr>
</table>


<h3>Value</h3>



<table>
<tr><td><code>postmeancoeff</code></td>
<td>
<p>posterior mean of the regression coefficients</p>
</td></tr>
<tr><td><code>postsqrtcoeff</code></td>
<td>
<p>posterior standard deviation of the regression coefficients</p>
</td></tr>
<tr><td><code>log10bf</code></td>
<td>
<p>log-Bayes factors against the full model</p>
</td></tr>
<tr><td><code>postmeansigma2</code></td>
<td>
<p>posterior mean of the variance of the model</p>
</td></tr>
<tr><td><code>postvarsigma2</code></td>
<td>
<p>posterior variance of the variance of the model</p>
</td></tr>

</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(faithful)
BayesReg(faithful[,1],faithful[,2])
</code></pre>

<hr>
<h2 id='caterpillar'>
Pine processionary caterpillar dataset
</h2><span id='topic+caterpillar'></span>

<h3>Description</h3>

<p>The <code>caterpillar</code> dataset is extracted from a
1973 study on pine processionary caterpillars.
The response variable is the
log transform of the number of nests per unit.
There are <code class="reqn">p=8</code> potential explanatory variables
and <code class="reqn">n=33</code> areas.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(caterpillar)</code></pre>


<h3>Format</h3>

<p>A data frame with 33 observations on the following 9 variables.
</p>

<dl>
<dt><code>x1</code></dt><dd><p>altitude (in meters)</p>
</dd>
<dt><code>x2</code></dt><dd><p>slope (in degrees)</p>
</dd>
<dt><code>x3</code></dt><dd><p>number of pine trees in the area</p>
</dd>
<dt><code>x4</code></dt><dd><p>height (in meters) of the tree sampled at the center of the area</p>
</dd>
<dt><code>x5</code></dt><dd><p>orientation of the area (from 1 if southbound to 2 otherwise)</p>
</dd>
<dt><code>x6</code></dt><dd><p>height (in meters) of the dominant tree</p>
</dd>
<dt><code>x7</code></dt><dd><p>number of vegetation strata</p>
</dd>
<dt><code>x8</code></dt><dd><p>mix settlement index (from 1 if not mixed to 2 if mixed)</p>
</dd>
<dt><code>y</code></dt><dd><p>logarithmic transform of the average number of nests of caterpillars per tree</p>
</dd>
</dl>



<h3>Details</h3>

<p>This dataset is used in Chapter 3 on linear regression. It assesses the
influence of some forest settlement characteristics on the development of
caterpillar colonies.  It was first published and studied in
Tomassone et al. (1993).  The response variable is the
logarithmic transform of the average number of nests of caterpillars per tree
in an area of 500 square meters (which corresponds to the last column in
<code>caterpillar</code>).  There are <code class="reqn">p=8</code> potential explanatory variables
defined on <code class="reqn">n=33</code> areas.
</p>


<h3>Source</h3>

<p>Tomassone, R., Dervin, C., and Masson, J.P. (1993)
<em>Biometrie: modelisation de phenomenes biologiques</em>.
Dunod, Paris.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(caterpillar)
summary(caterpillar)
</code></pre>

<hr>
<h2 id='datha'>
Non-standardised Licence dataset
</h2><span id='topic+datha'></span>

<h3>Description</h3>

<p>The dataset used in Chapter 6 is derived from an image of a license plate, called <code>license</code>
and not provided in the package.  The actual histogram of the grey levels is
concentrated on 256 values because of the poor resolution of the image, but
we transformed the original data as <code>datha.txt</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(datha)</code></pre>


<h3>Format</h3>

<p>A data frame with 2625 observations on the following variable.
</p>

<dl>
<dt><code>x</code></dt><dd><p>Grey levels</p>
</dd>
</dl>



<h3>Details</h3>

<p><code>datha.txt</code> was produced by the following <span class="rlang"><b>R</b></span> code:
</p>
<pre>
&gt; license=jitter(license,10)
&gt; datha=log((license-min(license)+.01)/
+ (max(license)+.01-license))
&gt; write.table(datha,"datha.txt",row.names=FALSE,col.names=FALSE)
</pre>
<p>where <code>jitter</code> is used to randomize the dataset and avoid repetitions
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(datha)
datha=as.matrix(datha)
range(datha)
</code></pre>

<hr>
<h2 id='Dnadataset'>
DNA sequence of an HIV genome
</h2><span id='topic+Dnadataset'></span>

<h3>Description</h3>

<p><code>Dnadataset</code> is a base sequence corresponding to a complete HIV
(which stands for Human Immunodeficiency Virus)
genome where A, C, G, and T have been recoded as 1,2,3,4.
It is modelled as a hidden Markov chain and is used in Chapter 7.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Dnadataset)</code></pre>


<h3>Format</h3>

<p>A data frame with 9718 rows and two columns, the first one corresponding
to the row number and the second one to the amino-acid value coded from 1 to 4.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Dnadataset)
summary(Dnadataset)
</code></pre>

<hr>
<h2 id='eurodip'>European Dipper dataset</h2><span id='topic+eurodip'></span>

<h3>Description</h3>

<p>This capture-recapture dataset on the <em>European dipper</em> bird
covers 7 years (1981-1987 inclusive) of observations of captures
within one of three zones. It is used in Chapter 5.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(eurodip)</code></pre>


<h3>Format</h3>

<p>A data frame with 294 observations on the following 7 variables.
</p>

<dl>
<dt><code>t1</code></dt><dd><p>non-capture/location on year 1981</p>
</dd>
<dt><code>t2</code></dt><dd><p>non-capture/location on year 1982</p>
</dd>
<dt><code>t3</code></dt><dd><p>non-capture/location on year 1983</p>
</dd>
<dt><code>t4</code></dt><dd><p>non-capture/location on year 1984</p>
</dd>
<dt><code>t5</code></dt><dd><p>non-capture/location on year 1985</p>
</dd>
<dt><code>t6</code></dt><dd><p>non-capture/location on year 1986</p>
</dd>
<dt><code>t7</code></dt><dd><p>non-capture/location on year 1987</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data consists of markings and recaptures of breeding adults 
each year during the breeding period from early March to early June. Birds
were at least one year old when initially banded. In <code>eurodip</code>,  
each row gof seven digits corresponds to a capture-recapture story for a given dipper,
0 indicating an absence of capture that year and, in the case of a capture,
1, 2, or 3 representing the zone where the dipper is captured. This dataset corresponds to
three geographical zones covering 200 square kilometers in eastern France.
It was kindly provided to us by J.D. Lebreton.
</p>


<h3>References</h3>

<p>Lebreton, J.-D., K. P. Burnham, J. Clobert, and D. R. Anderson. (1992)
Modeling survival and testing biological hypotheses using marked animals:
case studies and recent advances. <em>Ecol. Monogr.</em> <b>62</b>, 67-118.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(eurodip)
summary(eurodip)
</code></pre>

<hr>
<h2 id='Eurostoxx50'>
Eurostoxx50 exerpt dataset
</h2><span id='topic+Eurostoxx50'></span>

<h3>Description</h3>

<p>This dataset is a collection of four time series connected with the stock market.
Those are the stock values of the four companies ABN Amro, Aegon,
Ahold Kon., and Air Liquide, observed from January 1, 1998, to November 9, 2003.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Eurostoxx50)</code></pre>


<h3>Format</h3>

<p>A data frame with 1486 observations on the following 5 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>six-digit date</p>
</dd>
<dt><code>Abn</code></dt><dd><p>value of the ABN Amro stock</p>
</dd>
<dt><code>Aeg</code></dt><dd><p>value of the Aegon stock</p>
</dd>
<dt><code>Aho</code></dt><dd><p>value of the Ahold Kon. stock</p>
</dd> 
<dt><code>AL</code></dt><dd><p>value of the Air Liquide stock</p>
</dd>
</dl>



<h3>Details</h3>

<p>Those four companies are the first stocks (in alphabetical order) to appear in the financial index
Eurostoxx50.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Eurostoxx50)
summary(Eurostoxx50)
</code></pre>

<hr>
<h2 id='gibbs'>
Gibbs sampler and Chib's evidence approximation 
for a generic univariate mixture of normal distributions
</h2><span id='topic+gibbs'></span>

<h3>Description</h3>

<p>This function implements a regular Gibbs sampling algorithm on the
posterior distribution associated with a mixture of normal distributions, 
taking advantage of the missing data structure. It then runs an averaging 
of the simulations over all permutations of the component indices in order
to avoid incomplete label switching and to validate Chib's representation
of the evidence. This function reproduces <code>gibbsnorm</code> as its first stage,
<em>however it may be much slower because of its second stage</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbs(niter, datha, mix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gibbs_+3A_niter">niter</code></td>
<td>

<p>number of Gibbs iterations
</p>
</td></tr>
<tr><td><code id="gibbs_+3A_datha">datha</code></td>
<td>

<p>sample vector
</p>
</td></tr>
<tr><td><code id="gibbs_+3A_mix">mix</code></td>
<td>

<p>list made of <code>k</code>, number of components, <code>p</code>, <code>mu</code>,
and <code>sig</code>, starting values of the paramerers, all of size <code>k</code> (see
example below)
</p>
</td></tr>
</table>


<h3>Value</h3>



<table>
<tr><td><code>k</code></td>
<td>
<p>number of components in the mixture (superfluous as it is invariant over the execution of the
R code)</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>matrix of the Gibbs samples on the <code class="reqn">\mu_i</code> parameters</p>
</td></tr>
<tr><td><code>sig</code></td>
<td>
<p>matrix of the Gibbs samples on the <code class="reqn">\sigma_i</code> parameters</p>
</td></tr>
<tr><td><code>prog</code></td>
<td>
<p>matrix of the Gibbs samples on the mixture weights</p>
</td></tr>
<tr><td><code>lolik</code></td>
<td>
<p>vector of the observed log-likelihoods along iterations</p>
</td></tr>
<tr><td><code>chibdeno</code></td>
<td>
<p>denominator of Chib's approximation to the evidence (see example below)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Chib, S. (1995) Marginal likelihood from the Gibbs output. 
<em>J. American Statist. Associ.</em> <b>90</b>, 1313-1321.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gibbsnorm">gibbsnorm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>faithdata=faithful[,1]
mu=rnorm(3,mean=mean(faithdata),sd=sd(faithdata)/10)
sig=1/rgamma(3,shape=10,scale=var(faithdata))
mix=list(k=3,p=rdirichlet(par=rep(1,3)),mu=mu,sig=sig)
resim3=gibbs(100,faithdata,mix)
lulu=order(resim3$lolik)[100]
lnum1=resim3$lolik[lulu]
lnum2=sum(dnorm(resim3$mu[lulu,],mean=mean(faithdata),sd=resim3$sig[lulu,],log=TRUE)+
dgamma(resim3$sig[lulu,],10,var(faithdata),log=TRUE)-2*log(resim3$sig[lulu,]))+
sum((rep(0.5,mix$k)-1)*log(resim3$p[lulu,]))+
lgamma(sum(rep(0.5,mix$k)))-sum(lgamma(rep(0.5,mix$k)))
lchibapprox3=lnum1+lnum2-log(resim3$deno)
</code></pre>

<hr>
<h2 id='gibbscap1'>
Gibbs sampler for the two-stage open population capture-recapture
model
</h2><span id='topic+gibbscap1'></span>

<h3>Description</h3>

<p>This function implements a regular Gibbs sampler associated with Chapter 5 for a two-stage
capture recapture model with open populations, accounting for the possibility
that some individuals vanish between two successive capture experiments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbscap1(nsimu, n1, c2, c3, N0 = n1/runif(1), r10, r20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gibbscap1_+3A_nsimu">nsimu</code></td>
<td>

<p>number of simulated values in the sample
</p>
</td></tr>
<tr><td><code id="gibbscap1_+3A_n1">n1</code></td>
<td>

<p>first capture population size
</p>
</td></tr>
<tr><td><code id="gibbscap1_+3A_c2">c2</code></td>
<td>

<p>number of individuals recaptured during the second experiment
</p>
</td></tr>
<tr><td><code id="gibbscap1_+3A_c3">c3</code></td>
<td>

<p>number of individuals recaptured during the third experiment
</p>
</td></tr>
<tr><td><code id="gibbscap1_+3A_n0">N0</code></td>
<td>

<p>starting value for the population size
</p>
</td></tr>
<tr><td><code id="gibbscap1_+3A_r10">r10</code></td>
<td>

<p>starting value for the number of individuals who vanished between the first and second experiments
</p>
</td></tr>
<tr><td><code id="gibbscap1_+3A_r20">r20</code></td>
<td>

<p>starting value for the number of individuals who vanished between the second and third experiments
</p>
</td></tr>
</table>


<h3>Value</h3>



<table>
<tr><td><code>N</code></td>
<td>
<p>Gibbs sample of the simulated population size</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>Gibbs sample of the probability of capture</p>
</td></tr>
<tr><td><code>q</code></td>
<td>
<p>Gibbs sample of the probability of leaving the population</p>
</td></tr>
<tr><td><code>r1</code></td>
<td>
<p>Gibbs sample of the number of individuals who vanished between the first and second experiments</p>
</td></tr>
<tr><td><code>r2</code></td>
<td>
<p>Gibbs sample of the number of individuals who vanished between the second and third experiments</p>
</td></tr>

</table>


<h3>Examples</h3>

<pre><code class='language-R'>res=gibbscap1(100,32,21,15,200,10,5)
plot(res$p,type="l",col="steelblue3",xlab="iterations",ylab="p")
</code></pre>

<hr>
<h2 id='gibbscap2'>
Gibbs sampling for the Arnason-Schwarz capture-recapture model
</h2><span id='topic+gibbscap2'></span>

<h3>Description</h3>

<p>In the Arnason-Schwarz capture-recapture model (see Chapter 5), individual histories
are observed and missing steps can be inferred upon. For the dataset <code>eurodip</code>,
the moves between regions can be reconstituted. This is the first instance
of a hidden Markov model met in the book.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbscap2(nsimu, z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gibbscap2_+3A_nsimu">nsimu</code></td>
<td>

<p>numbed of simulation steps in the Gibbs sampler
</p>
</td></tr>
<tr><td><code id="gibbscap2_+3A_z">z</code></td>
<td>

<p>data, capture history of each individual, with <code>0</code> coding non-capture
</p>
</td></tr>
</table>


<h3>Value</h3>



<table>
<tr><td><code>p</code></td>
<td>
<p>Gibbs sample of capture probabilities across time</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>
<p>Gibbs sample of survival probabilities across time and locations</p>
</td></tr>
<tr><td><code>psi</code></td>
<td>
<p>Gibbs sample of interstata movement probabilities across time and locations</p>
</td></tr>
<tr><td><code>late</code></td>
<td>
<p>Gibbs averages of completed histories</p>
</td></tr>

</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(eurodip)
res=gibbscap2(10,eurodip[1:100,])
plot(res$p,type="l",col="steelblue3",xlab="iterations",ylab="p")
</code></pre>

<hr>
<h2 id='gibbsmean'>
Gibbs sampler on a mixture posterior distribution with unknown means
</h2><span id='topic+gibbsmean'></span>

<h3>Description</h3>

<p>This function implements a Gibbs sampler for a toy mixture problem (Chapter 6) with
two Gaussian components and only the means unknown, so that
likelihood and posterior surfaces can be drawn.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbsmean(p, datha, niter = 10^4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gibbsmean_+3A_p">p</code></td>
<td>

<p>first component weight
</p>
</td></tr>
<tr><td><code id="gibbsmean_+3A_datha">datha</code></td>
<td>

<p>dataset to be modelled as a mixture
</p>
</td></tr>
<tr><td><code id="gibbsmean_+3A_niter">niter</code></td>
<td>

<p>number of Gibbs iterations
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Sample of <code class="reqn">\mu</code>'s as a matrix of size <code>niter</code> x 2
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotmix">plotmix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat=plotmix(plottin=FALSE)$sample
simu=gibbsmean(0.7,dat,niter=100)
plot(simu,pch=19,cex=.5,col="sienna",xlab=expression(mu[1]),ylab=expression(mu[2]))
</code></pre>

<hr>
<h2 id='gibbsnorm'>
Gibbs sampler for a generic mixture posterior distribution
</h2><span id='topic+gibbsnorm'></span>

<h3>Description</h3>

<p>This function implements the generic Gibbs sampler of Diebolt and Robert (1994)
for producing a sample from the posterior distribution associated
with a univariate mixture of <code class="reqn">k</code> normal components with all <code class="reqn">3k-1</code> parameters
unknown.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbsnorm(niter, dat, mix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gibbsnorm_+3A_niter">niter</code></td>
<td>

<p>number of iterations in the Gibbs sampler
</p>
</td></tr>
<tr><td><code id="gibbsnorm_+3A_dat">dat</code></td>
<td>

<p>mixture sample
</p>
</td></tr>
<tr><td><code id="gibbsnorm_+3A_mix">mix</code></td>
<td>

<p>list defined as <code>mix=list(k=k,p=p,mu=mu,sig=sig)</code>,
where <code>k</code> is an integer and the remaining entries are
vectors of length <code>k</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Under conjugate priors on the means (normal distributions), variances (inverse gamma
distributions), and weights (Dirichlet distribution), the full conditional distributions
given the latent variables are directly available and can be used in a straightforward 
Gibbs sampler. This function is only the first step of the function <code>gibbs</code>, but it
may be much faster as it avoids the computation of the evidence via Chib's approach.

</p>


<h3>Value</h3>



<table>
<tr><td><code>k</code></td>
<td>
<p>number of components (superfluous)</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Gibbs sample of all mean parameters</p>
</td></tr>
<tr><td><code>sig</code></td>
<td>
<p>Gibbs sample of all variance parameters</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>Gibbs sample of all weight parameters</p>
</td></tr>
<tr><td><code>lopost</code></td>
<td>
<p>sequence of log-likelihood values along Gibbs iterations</p>
</td></tr>

</table>


<h3>References</h3>

<p>Chib, S. (1995) Marginal likelihood from the Gibbs output. 
<em>J. American Statist. Associ.</em> <b>90</b>, 1313-1321.
</p>
<p>Diebolt, J. and Robert, C.P. (1992) Estimation of finite mixture distributions by Bayesian sampling.
<em>J. Royal Statist. Society</em> <b>56</b>, 363-375.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rdirichlet">rdirichlet</a></code>, <code><a href="#topic+gibbs">gibbs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(datha)
datha=as.matrix(datha)
mix=list(k=3,mu=mean(datha),sig=var(datha))
res=gibbsnorm(10,datha,mix)
plot(res$p[,1],type="l",col="steelblue3",xlab="iterations",ylab="p")

</code></pre>

<hr>
<h2 id='hmflatlogit'>
Metropolis-Hastings for the logit model under a flat prior
</h2><span id='topic+hmflatlogit'></span>

<h3>Description</h3>

<p>Under the assumption that the posterior distribution is well-defined,
this Metropolis-Hastings algorithm produces a sample from the
posterior distribution on the logit model coefficient <code class="reqn">\beta</code>
under a flat prior. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmflatlogit(niter, y, X, scale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hmflatlogit_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="hmflatlogit_+3A_y">y</code></td>
<td>

<p>binary response variable
</p>
</td></tr>
<tr><td><code id="hmflatlogit_+3A_x">X</code></td>
<td>

<p>matrix of covariates with the same number of rows as <code>y</code>
</p>
</td></tr>
<tr><td><code id="hmflatlogit_+3A_scale">scale</code></td>
<td>

<p>scale of the Metropolis-Hastings random walk
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>The function produces a sample of <code class="reqn">\beta</code>'s as a matrix of size <code>niter</code> x <code>p</code>,
where <code>p</code> is the number of covariates.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hmflatprobit">hmflatprobit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)
bank=as.matrix(bank)
y=bank[,5]
X=bank[,1:4]
flatlogit=hmflatlogit(1000,y,X,1)
par(mfrow=c(1,3),mar=1+c(1.5,1.5,1.5,1.5))
plot(flatlogit[,1],type="l",xlab="Iterations",ylab=expression(beta[1]))
hist(flatlogit[101:1000,1],nclass=50,prob=TRUE,main="",xlab=expression(beta[1]))
acf(flatlogit[101:1000,1],lag=10,main="",ylab="Autocorrelation",ci=FALSE)
</code></pre>

<hr>
<h2 id='hmflatloglin'>
Metropolis-Hastings for the log-linear model under a flat prior
</h2><span id='topic+hmflatloglin'></span>

<h3>Description</h3>

<p>This version of <code>hmflatlogit</code> operates on the log-linear model, assuming
that the posterior associated with the flat prior and the data
is well-defined. The proposal is based on a random walk
Metropolis-Hastings step.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmflatloglin(niter, y, X, scale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hmflatloglin_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="hmflatloglin_+3A_y">y</code></td>
<td>

<p>binary response variable
</p>
</td></tr>
<tr><td><code id="hmflatloglin_+3A_x">X</code></td>
<td>

<p>matrix of covariates with the same number of rows as <code>y</code>
</p>
</td></tr>
<tr><td><code id="hmflatloglin_+3A_scale">scale</code></td>
<td>

<p>scale of the Metropolis-Hastings random walk
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>The function produces a sample of <code class="reqn">\beta</code>'s as a matrix of size <code>niter</code> x <code>p</code>,
where <code>p</code> is the number of covariates.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hmflatlogit">hmflatlogit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>airqual=na.omit(airquality)
ozone=cut(airqual$Ozone,c(min(airqual$Ozone),median(airqual$Ozone),max(airqual$Ozone)),
include.lowest=TRUE)
month=as.factor(airqual$Month)
tempe=cut(airqual$Temp,c(min(airqual$Temp),median(airqual$Temp),max(airqual$Temp)),
include.lowest=TRUE)
counts=table(ozone,tempe,month)
counts=as.vector(counts)
ozo=gl(2,1,20)
temp=gl(2,2,20)
mon=gl(5,4,20)
x1=rep(1,20)
lulu=rep(0,20)
x2=x3=x4=x5=x6=x7=x8=x9=lulu
x2[ozo==2]=x3[temp==2]=x4[mon==2]=x5[mon==3]=x6[mon==4]=1
x7[mon==5]=x8[ozo==2 &amp; temp==2]=x9[ozo==2 &amp; mon==2]=1
x10=x11=x12=x13=x14=x15=x16=lulu
x10[ozo==2 &amp; mon==3]=x11[ozo==2 &amp; mon==4]=x12[ozo==2 &amp; mon==5]=1
x13[temp==2 &amp; mon==2]=x14[temp==2 &amp; mon==3]=x15[temp==2 &amp; mon==4]=1
x16[temp==2 &amp; mon==5]=1
X=cbind(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16)
flatloglin=hmflatloglin(1000,counts,X,0.5)
par(mfrow=c(4,4),mar=1+c(1.5,1.5,1.5,1.5),cex=0.8)
for (i in 1:16) plot(flatloglin[,i],type="l",ylab="",xlab="Iterations")
</code></pre>

<hr>
<h2 id='hmflatprobit'>
Metropolis-Hastings for the probit model under a flat prior
</h2><span id='topic+hmflatprobit'></span>

<h3>Description</h3>

<p>This random walk Metropolis-Hastings algorithm takes advantage of the
availability of the maximum likelihood estimator (available via the <code>glm</code>
function) to center and scale the random walk in an efficient manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmflatprobit(niter, y, X, scale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hmflatprobit_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="hmflatprobit_+3A_y">y</code></td>
<td>

<p>binary response variable
</p>
</td></tr>
<tr><td><code id="hmflatprobit_+3A_x">X</code></td>
<td>

<p>covariates
</p>
</td></tr>
<tr><td><code id="hmflatprobit_+3A_scale">scale</code></td>
<td>

<p>scale of the random walk
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>The function produces a sample of <code class="reqn">\beta</code>'s of size <code>niter</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hmflatlogit">hmflatlogit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)
bank=as.matrix(bank)
y=bank[,5]
X=bank[,1:4]
flatprobit=hmflatprobit(1000,y,X,1)
mean(flatprobit[101:1000,1])
</code></pre>

<hr>
<h2 id='hmhmm'>
Estimation of a hidden Markov model with 2 hidden and 4 observed states
</h2><span id='topic+hmhmm'></span><span id='topic+likej'></span>

<h3>Description</h3>

<p>This function implements a Metropolis within Gibbs algorithm that produces
a sample on the parameters <code class="reqn">p_{ij}</code> and <code class="reqn">q^i_j</code> of the hidden Markov
model (Chapter 7). It includes a function <code>likej</code> that computes the likelihood of
the times series using a forward-backward algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmhmm(M = 100, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hmhmm_+3A_m">M</code></td>
<td>

<p>Number of Gibbs iterations
</p>
</td></tr>
<tr><td><code id="hmhmm_+3A_y">y</code></td>
<td>

<p>times series to be modelled by a hidden Markov model
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Metropolis-within-Gibbs step involves Dirichlet proposals with
a random choice of the scale between 1 and 1e5.
</p>


<h3>Value</h3>



<table>
<tr><td><code>BigR</code></td>
<td>
<p>matrix of the iterated values returned by the MCMC algorithm containing
<code class="reqn">p_{11}</code> and <code class="reqn">p_{22}</code>, transition probabilities, and 
<code class="reqn">q^1</code> and <code class="reqn">q^2</code>, vector of probabilities for both latent states</p>
</td></tr>
<tr><td><code>olike</code></td>
<td>
<p>sequence of the log-likelihoods produced by the MCMC sequence</p>
</td></tr>

</table>


<h3>Examples</h3>

<pre><code class='language-R'>res=hmhmm(M=500,y=sample(1:4,10,rep=TRUE))
plot(res$olike,type="l",main="log-likelihood",xlab="iterations",ylab="")
</code></pre>

<hr>
<h2 id='hmmeantemp'>
Metropolis-Hastings with tempering steps for the mean mixture posterior model
</h2><span id='topic+hmmeantemp'></span>

<h3>Description</h3>

<p>This function provides another toy illustration of the capabilities
of a tempered random walk Metropolis-Hastings algorithm applied
to the posterior distribution associated with a two-component normal mixture with only its means
unknown (Chapter 7). It shows how a decrease in the temperature leads to a proper exploration of
the target density surface, despite the existence of two well-separated modes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmmeantemp(dat, niter, var = 1, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hmmeantemp_+3A_dat">dat</code></td>
<td>
<p>set to be modelled as a mixture</p>
</td></tr>
<tr><td><code id="hmmeantemp_+3A_niter">niter</code></td>
<td>
<p>number of iterations</p>
</td></tr>
<tr><td><code id="hmmeantemp_+3A_var">var</code></td>
<td>
<p>variance of the random walk</p>
</td></tr>
<tr><td><code id="hmmeantemp_+3A_alpha">alpha</code></td>
<td>
<p>temperature, expressed as power of the likelihood</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code class="reqn">\alpha=1</code> the function operates (and can be used) as a regular Metropolis-Hastings algorithm.
</p>


<h3>Value</h3>

<p>sample of <code class="reqn">\mu_i</code>'s as a matrix of size <code>niter</code> x 2
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat=plotmix(plot=FALSE)$sample
simu=hmmeantemp(dat,1000)
plot(simu,pch=19,cex=.5,col="sienna",xlab=expression(mu[1]),ylab=expression(mu[2]))
</code></pre>

<hr>
<h2 id='hmnoinflogit'>
Metropolis-Hastings for the logit model under a noninformative prior
</h2><span id='topic+hmnoinflogit'></span>

<h3>Description</h3>

<p>This function runs a Metropolis-Hastings algorithm that produces a sample from the
posterior distribution for the logit model (Chapter 4) coefficient <code class="reqn">\beta</code>
associated with a noninformative prior defined in the book.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmnoinflogit(niter, y, X, scale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hmnoinflogit_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="hmnoinflogit_+3A_y">y</code></td>
<td>

<p>binary response variable
</p>
</td></tr>
<tr><td><code id="hmnoinflogit_+3A_x">X</code></td>
<td>

<p>matrix of covariates with the same number of rows as <code>y</code>
</p>
</td></tr>
<tr><td><code id="hmnoinflogit_+3A_scale">scale</code></td>
<td>

<p>scale of the random walk
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>sample of <code class="reqn">\beta</code>'s as a matrix of size <code>niter</code> x p, where p is the number
of covariates
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hmnoinfprobit">hmnoinfprobit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)
bank=as.matrix(bank)
y=bank[,5]
X=bank[,1:4]
noinflogit=hmnoinflogit(1000,y,X,1)
par(mfrow=c(1,3),mar=1+c(1.5,1.5,1.5,1.5))
plot(noinflogit[,1],type="l",xlab="Iterations",ylab=expression(beta[1]))
hist(noinflogit[101:1000,1],nclass=50,prob=TRUE,main="",xlab=expression(beta[1]))
acf(noinflogit[101:1000,1],lag=10,main="",ylab="Autocorrelation",ci=FALSE)
</code></pre>

<hr>
<h2 id='hmnoinfloglin'>
Metropolis-Hastings for the log-linear model under a noninformative prior
</h2><span id='topic+hmnoinfloglin'></span>

<h3>Description</h3>

<p>This function is a version of <code>hmnoinflogit</code> for the log-linear model, using a non-informative
prior defined in Chapter 4 and a proposal based on a random walk Metropolis-Hastings step.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmnoinfloglin(niter, y, X, scale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hmnoinfloglin_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="hmnoinfloglin_+3A_y">y</code></td>
<td>

<p>binary response variable
</p>
</td></tr>
<tr><td><code id="hmnoinfloglin_+3A_x">X</code></td>
<td>

<p>matrix of covariates with the same number of rows as <code>y</code>
</p>
</td></tr>
<tr><td><code id="hmnoinfloglin_+3A_scale">scale</code></td>
<td>

<p>scale of the random walk

</p>
</td></tr>
</table>


<h3>Value</h3>


<p>The function produces a sample of <code class="reqn">\beta</code>'s as a matrix of size <code>niter</code> x <code>p</code>,
where <code>p</code> is the number of covariates.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hmflatloglin">hmflatloglin</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>airqual=na.omit(airquality)
ozone=cut(airqual$Ozone,c(min(airqual$Ozone),median(airqual$Ozone),max(airqual$Ozone)),
include.lowest=TRUE)
month=as.factor(airqual$Month)
tempe=cut(airqual$Temp,c(min(airqual$Temp),median(airqual$Temp),max(airqual$Temp)),
include.lowest=TRUE)
counts=table(ozone,tempe,month)
counts=as.vector(counts)
ozo=gl(2,1,20)
temp=gl(2,2,20)
mon=gl(5,4,20)
x1=rep(1,20)
lulu=rep(0,20)
x2=x3=x4=x5=x6=x7=x8=x9=lulu
x2[ozo==2]=x3[temp==2]=x4[mon==2]=x5[mon==3]=1
x6[mon==4]=x7[mon==5]=x8[ozo==2 &amp; temp==2]=x9[ozo==2 &amp; mon==2]=1
x10=x11=x12=x13=x14=x15=x16=lulu
x10[ozo==2 &amp; mon==3]=x11[ozo==2 &amp; mon==4]=x12[ozo==2 &amp; mon==5]=x13[temp==2 &amp; mon==2]=1
x14[temp==2 &amp; mon==3]=x15[temp==2 &amp; mon==4]=x16[temp==2 &amp; mon==5]=1
X=cbind(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16)
noinloglin=hmnoinfloglin(1000,counts,X,0.5)
par(mfrow=c(4,4),mar=1+c(1.5,1.5,1.5,1.5),cex=0.8)
for (i in 1:16) plot(noinloglin[,i],type="l",ylab="",xlab="Iterations")
</code></pre>

<hr>
<h2 id='hmnoinfprobit'>
Metropolis-Hastings for the probit model under a noninformative prior
</h2><span id='topic+hmnoinfprobit'></span>

<h3>Description</h3>

<p>This function runs a Metropolis-Hastings algorithm that produces a sample from the
posterior distribution for the probit model coefficient <code class="reqn">\beta</code>
associated with a noninformative prior defined in Chapter 4.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hmnoinfprobit(niter, y, X, scale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hmnoinfprobit_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="hmnoinfprobit_+3A_y">y</code></td>
<td>

<p>binary response variable
</p>
</td></tr>
<tr><td><code id="hmnoinfprobit_+3A_x">X</code></td>
<td>

<p>matrix of covariates with the same number of rows as <code>y</code>
</p>
</td></tr>
<tr><td><code id="hmnoinfprobit_+3A_scale">scale</code></td>
<td>

<p>scale of the random walk

</p>
</td></tr>
</table>


<h3>Value</h3>


<p>The function produces a sample of <code class="reqn">\beta</code>'s as a matrix of size <code>niter</code> x <code>p</code>,
where <code>p</code> is the number of covariates.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hmnoinflogit">hmnoinflogit</a></code>, <code><a href="#topic+hmflatprobit">hmflatprobit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)
bank=as.matrix(bank)
y=bank[,5]
X=bank[,1:4]
noinfprobit=hmflatprobit(1000,y,X,1)
par(mfrow=c(1,3),mar=1+c(1.5,1.5,1.5,1.5))
plot(noinfprobit[,1],type="l",xlab="Iterations",ylab=expression(beta[1]))
hist(noinfprobit[101:1000,1],nclass=50,prob=TRUE,main="",xlab=expression(beta[1]))
acf(noinfprobit[101:1000,1],lag=10,main="",ylab="Autocorrelation",ci=FALSE)
</code></pre>

<hr>
<h2 id='isinghm'>
Metropolis-Hastings for the Ising model
</h2><span id='topic+isinghm'></span>

<h3>Description</h3>

<p>This is the Metropolis-Hastings version of the original Gibbs algorithm
on the Ising model (Chapter 8). Its basic step only proposes changes of values at selected
pixels, avoiding the inefficient updates that do not modify the current value
of <code>x</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isinghm(niter, n, m=n,beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isinghm_+3A_niter">niter</code></td>
<td>

<p>number of iterations of the algorithm
</p>
</td></tr>
<tr><td><code id="isinghm_+3A_n">n</code></td>
<td>

<p>number of rows in the grid
</p>
</td></tr>
<tr><td><code id="isinghm_+3A_m">m</code></td>
<td>

<p>number of columns in the grid
</p>
</td></tr>
<tr><td><code id="isinghm_+3A_beta">beta</code></td>
<td>

<p>Ising parameter
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code>, a realisation from the Ising distribution as a <code>n</code> x <code>m</code> matrix
of 0's and 1's
</p>


<h3>See Also</h3>

<p><code><a href="#topic+isingibbs">isingibbs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prepa=runif(1,0,2)
prop=isinghm(10,24,24,prepa)
image(1:24,1:24,prop)
</code></pre>

<hr>
<h2 id='isingibbs'>
Gibbs sampler for the Ising model
</h2><span id='topic+isingibbs'></span>

<h3>Description</h3>

<p>This is the original Geman and Geman (1984) Gibbs sampler
on the Ising model that gave its name to the method. It simulates
an <code class="reqn">n\times m</code> grid from the Ising distribution.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>isingibbs(niter, n, m=n, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isingibbs_+3A_niter">niter</code></td>
<td>

<p>number of iterations of the algorithm
</p>
</td></tr>
<tr><td><code id="isingibbs_+3A_n">n</code></td>
<td>

<p>number of rows in the grid
</p>
</td></tr>
<tr><td><code id="isingibbs_+3A_m">m</code></td>
<td>

<p>number of columns in the grid
</p>
</td></tr>
<tr><td><code id="isingibbs_+3A_beta">beta</code></td>
<td>

<p>Ising parameter
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code>, a realisation from the Ising distribution
as a matrix of size <code>n</code> x <code>m</code>
</p>


<h3>References</h3>

<p>Geman, S. and Geman, D. (1984)
Stochastic relaxation, Gibbs distributions and the Bayesian restoration of
images. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em>, <b>6</b>, 721&ndash;741.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+isinghm">isinghm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>image(1:20,1:20,isingibbs(10,20,20,beta=0.3))
</code></pre>

<hr>
<h2 id='Laichedata'>
Laiche dataset
</h2><span id='topic+Laichedata'></span>

<h3>Description</h3>

<p>This dataset depicts the presence of plants (tufted sedges) in a part of a wetland. It is
25x25 matrix of zeroes and ones, used in Chapter 8.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Laichedata)</code></pre>


<h3>Format</h3>

<p>A data frame corresponding to a 25x25 matrix of zeroes and ones.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Laichedata)
image(as.matrix(Laichedata))
</code></pre>

<hr>
<h2 id='logitll'>
Log-likelihood of the logit model
</h2><span id='topic+logitll'></span>

<h3>Description</h3>

<p>Direct computation of the logarithm of the likelihood
of a standard logit model (Chapter 4) </p>
<p style="text-align: center;"><code class="reqn">P(y=1|X,\beta)=
\{1+\exp(-\beta^{T}X)\}^{-1}.</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>logitll(beta, y, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logitll_+3A_beta">beta</code></td>
<td>

<p>coefficient of the logit model
</p>
</td></tr>
<tr><td><code id="logitll_+3A_y">y</code></td>
<td>

<p>vector of binary response variables
</p>
</td></tr>
<tr><td><code id="logitll_+3A_x">X</code></td>
<td>

<p>covariate matrix
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>returns the logarithm of the logit likelihood for the data <code>y</code>, 
covariate matrix <code>X</code> and parameter vector <code>beta</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+probitll">probitll</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)
y=bank[,5]
X=as.matrix(bank[,-5])
logitll(runif(4),y,X)
</code></pre>

<hr>
<h2 id='logitnoinflpost'>
Log of the posterior distribution for the probit model under a noninformative prior
</h2><span id='topic+logitnoinflpost'></span>

<h3>Description</h3>

<p>This function computes the logarithm of the posterior density associated
with a logit model and the noninformative prior used in Chapter 4.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logitnoinflpost(beta, y, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logitnoinflpost_+3A_beta">beta</code></td>
<td>

<p>parameter of the logit model
</p>
</td></tr>
<tr><td><code id="logitnoinflpost_+3A_y">y</code></td>
<td>

<p>binary response variable
</p>
</td></tr>
<tr><td><code id="logitnoinflpost_+3A_x">X</code></td>
<td>

<p>covariate matrix
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>returns the logarithm of the logit likelihood for the data <code>y</code>,
covariate matrix <code>X</code> and parameter <code>beta</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+probitnoinflpost">probitnoinflpost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)
y=bank[,5]
X=as.matrix(bank[,-5])
logitnoinflpost(runif(4),y,X)
</code></pre>

<hr>
<h2 id='loglinll'>
Log of the likelihood of the log-linear model
</h2><span id='topic+loglinll'></span>

<h3>Description</h3>

<p>This function provides a direct computation of the logarithm of the likelihood
of a standard log-linear model, as defined in Chapter 4.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loglinll(beta, y, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loglinll_+3A_beta">beta</code></td>
<td>

<p>coefficient of the logit model
</p>
</td></tr>
<tr><td><code id="loglinll_+3A_y">y</code></td>
<td>

<p>vector of binary response variables
</p>
</td></tr>
<tr><td><code id="loglinll_+3A_x">X</code></td>
<td>

<p>covariate matrix
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>returns the logarithmic value of the logit likelihood for the data <code>y</code>,
covariate matrix <code>X</code> and parameter vector <code>beta</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X=matrix(rnorm(20*3),ncol=3)
beta=c(3,-2,1)
y=rpois(20,exp(X%*%beta))
loglinll(beta, y, X)
</code></pre>

<hr>
<h2 id='loglinnoinflpost'>
Log of the posterior density for the log-linear model under a noninformative prior
</h2><span id='topic+loglinnoinflpost'></span>

<h3>Description</h3>

<p>This function computes the logarithm of the posterior density associated
with a log-linear model and the noninformative prior used in Chapter 4.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loglinnoinflpost(beta, y, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loglinnoinflpost_+3A_beta">beta</code></td>
<td>

<p>parameter of the log-linear model
</p>
</td></tr>
<tr><td><code id="loglinnoinflpost_+3A_y">y</code></td>
<td>

<p>binary response variable
</p>
</td></tr>
<tr><td><code id="loglinnoinflpost_+3A_x">X</code></td>
<td>

<p>covariate matrix
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function does not test for coherence between the lengths of <code>y</code>,
<code>X</code> and <code>beta</code>, hence may return an error message in case of
incoherence.
</p>


<h3>Value</h3>


<p>returns the logarithm of the logit posterior density for the data <code>y</code>,
covariate matrix <code>X</code> and parameter vector <code>beta</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X=matrix(rnorm(20*3),ncol=3)
beta=c(3,-2,1)
y=rpois(20,exp(X%*%beta))
loglinnoinflpost(beta, y, X) 
</code></pre>

<hr>
<h2 id='MAllog'>
log-likelihood associated with an MA(p) model
</h2><span id='topic+MAllog'></span>

<h3>Description</h3>

<p>This function returns the numerical value of the log-likelihood associated with a time
series and an MA(p) model in Chapter 7. It either uses the natural parameterisation of the MA(p)
model </p>
<p style="text-align: center;"><code class="reqn">x_t-\mu = \varepsilon_t - \sum_{j=1}^p \psi_{j} \varepsilon_{t-j}</code>
</p>

<p>or the parameterisation via the lag-polynomial roots
</p>
<p style="text-align: center;"><code class="reqn">x_t - \mu = \prod_{i=1}^p (1-\lambda_i B) \varepsilon_t</code>
</p>

<p>where <code class="reqn">B^j \varepsilon_t = \varepsilon_{t-j}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MAllog(p,dat,pr,pc,lr,lc,mu,sig2,compsi=T,pepsi=rep(0,p),eps=rnorm(p))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MAllog_+3A_p">p</code></td>
<td>

<p>order of the MA model
</p>
</td></tr>
<tr><td><code id="MAllog_+3A_dat">dat</code></td>
<td>

<p>time series modelled by the MA(p) model
</p>
</td></tr>
<tr><td><code id="MAllog_+3A_pr">pr</code></td>
<td>

<p>number of real roots in the lag polynomial
</p>
</td></tr>
<tr><td><code id="MAllog_+3A_pc">pc</code></td>
<td>

<p>number of complex roots in the lag polynomial, necessarily even
</p>
</td></tr>
<tr><td><code id="MAllog_+3A_lr">lr</code></td>
<td>

<p>real roots
</p>
</td></tr>
<tr><td><code id="MAllog_+3A_lc">lc</code></td>
<td>

<p>complex roots, stored as real part for odd indices and 
imaginary part for even indices. (<code>lc</code> is either 0 when
<code>pc=0</code> or a vector of even length when <code>pc&gt;0</code>.)
</p>
</td></tr>
<tr><td><code id="MAllog_+3A_mu">mu</code></td>
<td>

<p>drift parameter <code class="reqn">\mu</code> such that <code class="reqn">(X_t-\mu)_t</code> is a standard MA<code class="reqn">(p)</code> series
</p>
</td></tr>
<tr><td><code id="MAllog_+3A_sig2">sig2</code></td>
<td>

<p>variance of the Gaussian white noise <code class="reqn">(\varepsilon_t)_t</code>
</p>
</td></tr>
<tr><td><code id="MAllog_+3A_compsi">compsi</code></td>
<td>

<p>boolean variable indicating whether the coefficients <code class="reqn">\psi_i</code> need to be retrieved
from the roots of the lag-polynomial (if <code>TRUE</code>) or not (if <code>FALSE</code>)
</p>
</td></tr>
<tr><td><code id="MAllog_+3A_pepsi">pepsi</code></td>
<td>

<p>potential coefficients <code class="reqn">\psi_i</code>, computed by the function if <code>compsi</code> is <code>TRUE</code>
</p>
</td></tr>
<tr><td><code id="MAllog_+3A_eps">eps</code></td>
<td>

<p>white noise terms <code class="reqn">(\varepsilon_t)_{t\le 0}</code> with negative indices 
</p>
</td></tr>
</table>


<h3>Value</h3>



<table>
<tr><td><code>ll</code></td>
<td>
<p>value of the log-likelihood</p>
</td></tr>
<tr><td><code>ps</code></td>
<td>
<p>vector of the <code class="reqn">\psi_i</code>'s, similar to the entry if <code>compsi</code> is <code>FALSE</code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ARllog">ARllog</a></code>, <code><a href="#topic+MAmh">MAmh</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>MAllog(p=3,dat=faithful[,1],pr=3,pc=0,lr=rep(.1,3),lc=0,
mu=0,sig2=var(faithful[,1]),compsi=FALSE,pepsi=rep(.1,3),eps=rnorm(3))
</code></pre>

<hr>
<h2 id='MAmh'>
Metropolis&ndash;Hastings evaluation of the posterior associated with an MA(p) model
</h2><span id='topic+MAmh'></span>

<h3>Description</h3>

<p>This function implements a Metropolis&ndash;Hastings algorithm on the coefficients
of the MA(p) model, involving the simulation of the real and complex roots of
the model. The algorithm includes jumps between adjacent numbers of real and complex roots,
as well as random modifications for a given number of real and complex roots. It is thus
a special case of a <em>reversible jump MCMC</em> algorithm (Green, 1995).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MAmh(x, p = 1, W = 10^3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MAmh_+3A_x">x</code></td>
<td>

<p>time series to be modelled as an MA(p) model
</p>
</td></tr>
<tr><td><code id="MAmh_+3A_p">p</code></td>
<td>

<p>order of the MA(p) model
</p>
</td></tr>
<tr><td><code id="MAmh_+3A_w">W</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
</table>


<h3>Value</h3>



<table>
<tr><td><code>psis</code></td>
<td>
<p>matrix of simulated <code class="reqn">\psi_i</code>'s</p>
</td></tr>
<tr><td><code>mus</code></td>
<td>
<p>vector of simulated <code class="reqn">\mu</code>'s</p>
</td></tr>
<tr><td><code>sigs</code></td>
<td>
<p>vector of simulated <code class="reqn">\sigma^2</code>'s</p>
</td></tr>
<tr><td><code>llik</code></td>
<td>
<p>vector of corresponding log-likelihood values (useful to check for convergence)</p>
</td></tr>
<tr><td><code>pcomp</code></td>
<td>
<p>vector of simulated numbers of complex roots</p>
</td></tr>
</table>


<h3>References</h3>

<p>Green, P.J. (1995) Reversible jump MCMC computaton and Bayesian model choice.
<em>Biometrika</em> <b>82</b>, 711&ndash;732.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MAllog">MAllog</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Eurostoxx50)
x=Eurostoxx50[1:350, 5]
resMA5=MAmh(x=x,p=5,W=50)
plot(resMA5$mus,type="l",col="steelblue4",xlab="Iterations",ylab=expression(mu))
</code></pre>

<hr>
<h2 id='Menteith'>
Grey-level image of the Lake of Menteith
</h2><span id='topic+Menteith'></span>

<h3>Description</h3>

<p>This dataset is a 100x100 pixel satellite image of the lake of Menteith,
near Stirling, Scotland. The purpose of analyzing this satellite dataset is to
classify all pixels into one of six states in order to detect some
homogeneous regions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Menteith)</code></pre>


<h3>Format</h3>

<p>data frame of a 100 x 100 image with 106 grey levels
</p>


<h3>See Also</h3>

<p><code><a href="#topic+reconstruct">reconstruct</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Menteith)
image(1:100,1:100,as.matrix(Menteith),col=gray(256:1/256),xlab="",ylab="")
</code></pre>

<hr>
<h2 id='ModChoBayesReg'>
Bayesian model choice procedure for the linear model
</h2><span id='topic+ModChoBayesReg'></span>

<h3>Description</h3>

<p>This function computes the posterior probabilities of all
(for less than 15 covariates) or the most probable (for more than 15 covariates)
submodels obtained by eliminating some covariates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ModChoBayesReg(y, X, g = length(y), betatilde = rep(0, dim(X)[2]), 
niter = 1e+05, prt = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ModChoBayesReg_+3A_y">y</code></td>
<td>

<p>response variable
</p>
</td></tr>
<tr><td><code id="ModChoBayesReg_+3A_x">X</code></td>
<td>

<p>covariate matrix
</p>
</td></tr>
<tr><td><code id="ModChoBayesReg_+3A_g">g</code></td>
<td>

<p>constant in the <code class="reqn">g</code> prior
</p>
</td></tr>
<tr><td><code id="ModChoBayesReg_+3A_betatilde">betatilde</code></td>
<td>

<p>prior expectation of the regression coefficient <code class="reqn">\beta</code>
</p>
</td></tr>
<tr><td><code id="ModChoBayesReg_+3A_niter">niter</code></td>
<td>

<p>number of Gibbs iterations in the case there are more than 15 covariates
</p>
</td></tr>
<tr><td><code id="ModChoBayesReg_+3A_prt">prt</code></td>
<td>

<p>boolean variable for printing the standard output
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When using a conjugate prior for the linear model such as the <code class="reqn">G</code> prior,
the marginal likelihood and hence the evidence are available in closed form. If the number
of explanatory variables is less than 15, the exact
derivation of the posterior probabilities for all submodels can be undertaken.
Indeed, <code class="reqn">2^{15}=32768</code> means that the problem remains tractable. 
When the number of explanatory variables gets larger, a random exploration of the collection
of submodels becomes necessary, as explained in the book (Chapter 3). The proposal to change
one variable indicator is made at random and accepting this move follows from a Metropolis&ndash;Hastings
step.
</p>


<h3>Value</h3>


<table>
<tr><td><code>top10models</code></td>
<td>
<p>models with the ten largest posterior probabilities</p>
</td></tr>
<tr><td><code>postprobtop10</code></td>
<td>
<p>posterior probabilities of those ten most likely models</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(caterpillar)
y=log(caterpillar$y)
X=as.matrix(caterpillar[,1:8])
res2=ModChoBayesReg(y,X)
</code></pre>

<hr>
<h2 id='normaldata'>
Normal dataset
</h2><span id='topic+normaldata'></span>

<h3>Description</h3>

<p>This dataset is used as &quot;the&quot; normal dataset in Chapter 2. 
It is linked with the famous Michelson-Morley experiment that
opened the way to Einstein's relativity theory in 1887. It corresponds
to the more precise experiment of Illingworth in 1927. The datapoints
are measurment of differences in the speeds of
two light beams travelling the same distance in two orthogonal directions.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(normaldata)</code></pre>


<h3>Format</h3>

<p>A data frame with 64 observations on the following 2 variables.
</p>

<dl>
<dt><code>x1</code></dt><dd><p>index of the experiment</p>
</dd>
<dt><code>x2</code></dt><dd><p>averaged fringe displacement in the experiment</p>
</dd>
</dl>



<h3>Details</h3>

<p>The 64 data points in this dataset are associated with session numbers, corresponding to two different times of
the day, and they represent the averaged fringe displacement due to orientation taken over ten measurements 
made by Illingworth, who assumed a normal error model. 
</p>


<h3>See Also</h3>

<p><cite><a href="datasets.html#topic+morley">morley</a></cite>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(normaldata)
shift=matrix(normaldata,ncol=2,byrow=TRUE)[,2]
hist(shift[[1]],nclass=10,col="steelblue",prob=TRUE,main="")
</code></pre>

<hr>
<h2 id='pbino'>
Posterior expectation for the binomial capture-recapture model
</h2><span id='topic+pbino'></span>

<h3>Description</h3>

<p>This function provides an estimation of the number of dippers by a posterior expectation,
based on a uniform prior and the <code>eurodip</code> dataset, as described in Chapter 5.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pbino(nplus)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pbino_+3A_nplus">nplus</code></td>
<td>

<p>number of different dippers captured
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>returns a numerical value that estimates the number of dippers
in the population
</p>


<h3>See Also</h3>

<p><code><a href="#topic+eurodip">eurodip</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(eurodip)
year81=eurodip[,1]
nplus=sum(year81&gt;0)
sum((1:400)*pbino(nplus))
</code></pre>

<hr>
<h2 id='pcapture'>
Posterior probabilities for the multiple stage capture-recapture model
</h2><span id='topic+pcapture'></span>

<h3>Description</h3>

<p>This function computes the posterior expectation of the population size for a 
multiple stage capture-recapture experiment (Chapter 5) under a uniform prior on 
the range (0,400).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcapture(T, nplus, nc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcapture_+3A_t">T</code></td>
<td>

<p>number of experiments
</p>
</td></tr>
<tr><td><code id="pcapture_+3A_nplus">nplus</code></td>
<td>

<p>total number of captured animals
</p>
</td></tr>
<tr><td><code id="pcapture_+3A_nc">nc</code></td>
<td>

<p>total number of captures
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This analysis is based on the restrictive assumption that all dippers captured in the
second year were already present in the population during the first year.
</p>


<h3>Value</h3>


<p>numerical value of the posterior expectation
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pdarroch">pdarroch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sum((1:400)*pcapture(2,70,81))
</code></pre>

<hr>
<h2 id='pdarroch'>
Posterior probabilities for the Darroch model
</h2><span id='topic+pdarroch'></span>

<h3>Description</h3>

<p>This function computes the posterior expectation of the population size for a
two-stage Darroch capture-recapture experiment (Chapter 5) under a uniform prior on 
the range (0,400).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pdarroch(n1, n2, m2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pdarroch_+3A_n1">n1</code></td>
<td>

<p>size of the first capture experiment
</p>
</td></tr>
<tr><td><code id="pdarroch_+3A_n2">n2</code></td>
<td>

<p>size of the second capture experiment
</p>
</td></tr>
<tr><td><code id="pdarroch_+3A_m2">m2</code></td>
<td>

<p>number of recaptured individuals
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This model can be seen as a conditional version of the two-stage model when
conditioning on both sample sizes <code class="reqn">n_1</code> and <code class="reqn">n_2</code>.
</p>


<h3>Value</h3>


<p>numerical value of the posterior expectation
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcapture">pcapture</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>for (i in 6:16) print(round(sum(pdarroch(22,43,i)*1:400)))
</code></pre>

<hr>
<h2 id='plotmix'>
Graphical representation of a normal mixture log-likelihood
</h2><span id='topic+plotmix'></span>

<h3>Description</h3>

<p>This function gives an <code>image</code> representation of the log-likelihood
surface of a mixture (Chapter 6) of two normal densities with means <code class="reqn">\mu_1</code>
and <code class="reqn">\mu_2</code> unknown. It first generates the random sample associated 
with the distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotmix(mu1 = 2.5, mu2 = 0, p = 0.7, n = 500, plottin = TRUE, nl = 50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotmix_+3A_mu1">mu1</code></td>
<td>

<p>first mean
</p>
</td></tr>
<tr><td><code id="plotmix_+3A_mu2">mu2</code></td>
<td>

<p>second mean
</p>
</td></tr>
<tr><td><code id="plotmix_+3A_p">p</code></td>
<td>

<p>weight of the first component
</p>
</td></tr>
<tr><td><code id="plotmix_+3A_n">n</code></td>
<td>

<p>number of observations
</p>
</td></tr>
<tr><td><code id="plotmix_+3A_plottin">plottin</code></td>
<td>

<p>boolean variable to plot the surface (or not)
</p>
</td></tr>
<tr><td><code id="plotmix_+3A_nl">nl</code></td>
<td>

<p>number of contours
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In this case, the parameters are identifiable: <code class="reqn">\mu_1</code>
and <code class="reqn">\mu_2</code> cannot be confused when <code class="reqn">p</code> is not 0.5.
Nonetheless, the log-likelihood surface in this figure often
exhibits two modes, one being close to the true value of the parameters
used to simulate the dataset and one corresponding to a reflected separation of 
the dataset into two homogeneous groups.
</p>


<h3>Value</h3>

<table>
<tr><td><code>sample</code></td>
<td>
<p>the simulated sample</p>
</td></tr>
<tr><td><code>like</code></td>
<td>
<p>the discretised representation of the log-likelihood surface</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><cite><a href="#topic+gibbsmean">gibbsmean</a></cite>,
<cite><a href="#topic+hmmeantemp">hmmeantemp</a></cite>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>resumix=plotmix()
</code></pre>

<hr>
<h2 id='pottsgibbs'>
Gibbs sampler for the Potts model
</h2><span id='topic+pottsgibbs'></span>

<h3>Description</h3>

<p>This function produces one simulation of a square <code>numb</code> by <code>numb</code> grid
from a Potts distribution with four colours and a four neighbour
structure, relying on <code>niter</code> iterations of a standard Gibbs sampler.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pottsgibbs(niter, numb, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pottsgibbs_+3A_niter">niter</code></td>
<td>

<p>number of Gibbs iterations
</p>
</td></tr>
<tr><td><code id="pottsgibbs_+3A_numb">numb</code></td>
<td>

<p>size of the square grid
</p>
</td></tr>
<tr><td><code id="pottsgibbs_+3A_beta">beta</code></td>
<td>

<p>parameter of the Potts model
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>returns a random realisation from the Potts model
</p>


<h3>References</h3>

<p>Geman, S. and Geman, D. (1984)
Stochastic relaxation, Gibbs distributions and the Bayesian restoration of
images. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em>, <b>6</b>, 721&ndash;741.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pottshm">pottshm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ex=pottsgibbs(100,15,.4)
image(ex)
</code></pre>

<hr>
<h2 id='pottshm'>
Metropolis-Hastings sampler for a Potts model with <code>ncol</code> classes
</h2><span id='topic+pottshm'></span>

<h3>Description</h3>

<p>This function returns a simulation of a <code>n</code> by <code>m</code> grid
from a Potts distribution with <code>ncol</code> colours and a four neighbour
structure, using a Metropolis-Hastings step that avoids proposing
a value identical to the current state of the Markov chain.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pottshm(ncol=2,niter=10^4,n,m=n,beta=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pottshm_+3A_ncol">ncol</code></td>
<td>

<p>number of colors
</p>
</td></tr>
<tr><td><code id="pottshm_+3A_niter">niter</code></td>
<td>

<p>number of Metropolis-Hastings iterations
</p>
</td></tr>
<tr><td><code id="pottshm_+3A_n">n</code></td>
<td>

<p>number of rows in the image
</p>
</td></tr>
<tr><td><code id="pottshm_+3A_m">m</code></td>
<td>

<p>number of columns in the image
</p>
</td></tr>
<tr><td><code id="pottshm_+3A_beta">beta</code></td>
<td>

<p>parameter of the Potts model
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a random realisation from the Potts model
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pottsgibbs">pottsgibbs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ex=pottshm(niter=50,n=15,beta=.4)
hist(ex,prob=TRUE,col="steelblue",main="pottshm()")
</code></pre>

<hr>
<h2 id='probet'>
Coverage of the interval <code class="reqn">(a,b)</code> by the Beta cdf
</h2><span id='topic+probet'></span>

<h3>Description</h3>

<p>This function computes the coverage of the interval <code class="reqn">(a,b)</code> by the Beta 
<code class="reqn">\mathrm{B}(\alpha,\alpha (1-c)/c)</code> distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>probet(a, b, c, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="probet_+3A_a">a</code></td>
<td>

<p>lower bound of the prior 95%~confidence interval
</p>
</td></tr>
<tr><td><code id="probet_+3A_b">b</code></td>
<td>

<p>upper bound of the prior 95%~confidence interval
</p>
</td></tr>
<tr><td><code id="probet_+3A_c">c</code></td>
<td>

<p>mean parameter of the prior distribution
</p>
</td></tr>
<tr><td><code id="probet_+3A_alpha">alpha</code></td>
<td>

<p>scale parameter of the prior distribution
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numerical value between 0 and 1 corresponding to the coverage
</p>


<h3>See Also</h3>

<p><code><a href="#topic+solbeta">solbeta</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>probet(.1,.5,.3,2)
</code></pre>

<hr>
<h2 id='probitll'>
Log-likelihood of the probit model
</h2><span id='topic+probitll'></span>

<h3>Description</h3>

<p>This function implements a direct computation of the logarithm of the likelihood
of a standard probit model </p>
<p style="text-align: center;"><code class="reqn">P(y=1|X,\beta)=
\Phi(\beta^{T}X).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>probitll(beta, y, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="probitll_+3A_beta">beta</code></td>
<td>

<p>coefficient of the probit model
</p>
</td></tr>
<tr><td><code id="probitll_+3A_y">y</code></td>
<td>

<p>vector of binary response variables
</p>
</td></tr>
<tr><td><code id="probitll_+3A_x">X</code></td>
<td>

<p>covariate matrix
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>returns the logarithm of the probit likelihood for the data <code>y</code>,
covariate matrix <code>X</code> and parameter vector <code>beta</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logitll">logitll</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)
y=bank[,5]
X=as.matrix(bank[,-5])
probitll(runif(4),y,X)
</code></pre>

<hr>
<h2 id='probitnoinflpost'>
Log of the posterior density for the probit model under a non-informative model
</h2><span id='topic+probitnoinflpost'></span>

<h3>Description</h3>

<p>This function computes the logarithm of the posterior density associated
with a probit model and the non-informative prior used in Chapter 4.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>probitnoinflpost(beta, y, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="probitnoinflpost_+3A_beta">beta</code></td>
<td>

<p>parameter of the probit model
</p>
</td></tr>
<tr><td><code id="probitnoinflpost_+3A_y">y</code></td>
<td>

<p>binary response variable
</p>
</td></tr>
<tr><td><code id="probitnoinflpost_+3A_x">X</code></td>
<td>

<p>covariate matrix
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>returns the logarithm of the posterior density associated with a
logit model for the data <code>y</code>,
covariate matrix <code>X</code> and parameter <code>beta</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logitnoinflpost">logitnoinflpost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)
y=bank[,5]
X=as.matrix(bank[,-5])
probitnoinflpost(runif(4),y,X)
</code></pre>

<hr>
<h2 id='rdirichlet'>
Random generator for the Dirichlet distribution
</h2><span id='topic+rdirichlet'></span>

<h3>Description</h3>

<p>This function simulates a sample from
a Dirichlet distribution on the <code class="reqn">k</code> dimensional simplex with
arbitrary parameters.
The simulation is based on a renormalised vector of gamma variates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rdirichlet(n = 1, par = rep(1, 2))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rdirichlet_+3A_n">n</code></td>
<td>

<p>number of simulations
</p>
</td></tr>
<tr><td><code id="rdirichlet_+3A_par">par</code></td>
<td>

<p>parameters of the Dirichlet distribution, whose length determines
the value of <code>k</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Surprisingly, there is no default Dirichlet distribution generator in the R base
packages like <code>MASS</code> or <code>stats</code>. This function can be used in full
generality, apart from the book (Chapter 6).
</p>


<h3>Value</h3>

<p>returns a <code class="reqn">(n,k)</code> matrix of Dirichlet simulations
</p>


<h3>Examples</h3>

<pre><code class='language-R'>apply(rdirichlet(10,rep(3,5)),2,mean)
</code></pre>

<hr>
<h2 id='reconstruct'>
Image reconstruction for the Potts model with six classes
</h2><span id='topic+reconstruct'></span>

<h3>Description</h3>

<p>This function adresses the reconstruction of an image distributed from
a Potts model based on a noisy version of this image. The purpose of image segmentation 
(Chapter 8) is to cluster pixels into homogeneous classes without supervision or
preliminary definition of those classes, based only on the spatial
coherence of the structure. The underlying algorithm is an hybrid Gibbs sampler.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reconstruct(niter, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reconstruct_+3A_niter">niter</code></td>
<td>

<p>number of Gibbs iterations
</p>
</td></tr>
<tr><td><code id="reconstruct_+3A_y">y</code></td>
<td>

<p>blurred image defined as a matrix
</p>
</td></tr>



</table>


<h3>Details</h3>

<p>Using a Potts model on the true image, and uniform priors on
the genuine parameters of the model, the hybrid Gibbs sampler generates
the image pixels and the other parameters one at a time,
the <em>hybrid</em> stage being due to the Potts model parameter, since
it implies using a numerical integration via <code>integrate</code>. 
The code includes (or rather excludes!) the numerical integration via the vector <code>dali</code>,
which contains the values of the integration over a 21 point grid, since
this numerical integration is extremely time-consuming.

</p>


<h3>Value</h3>


<table>
<tr><td><code>beta</code></td>
<td>
<p>MCMC chain for the parameter <code class="reqn">\beta</code> of the Potts model</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>MCMC chain for the mean parameter of the blurring model</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>MCMC chain for the variance parameter of the blurring model</p>
</td></tr>
<tr><td><code>xcum</code></td>
<td>
<p>frequencies of simulated colours at every pixel of the image</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+Menteith">Menteith</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: data(Menteith)
lm3=as.matrix(Menteith)
#warning, this step is a bit lengthy
titus=reconstruct(20,lm3)
#allocation function
affect=function(u) order(u)[6]
#
aff=apply(titus$xcum,1,affect)
aff=t(matrix(aff,100,100))
par(mfrow=c(2,1))
image(1:100,1:100,lm3,col=gray(256:1/256),xlab="",ylab="")
image(1:100,1:100,aff,col=gray(6:1/6),xlab="",ylab="")

## End(Not run)</code></pre>

<hr>
<h2 id='solbeta'>
Recursive resolution of beta prior calibration
</h2><span id='topic+solbeta'></span>

<h3>Description</h3>


<p>In the capture-recapture experiment of Chapter 5, the prior information
is represented by a prior expectation and prior confidence intervals. This
function derives the corresponding beta <code class="reqn">B(\alpha,\beta)</code>
prior distribution by a divide-and-conquer scheme.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solbeta(a, b, c, prec = 10^(-3))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solbeta_+3A_a">a</code></td>
<td>


<p>lower bound of the prior 95%~confidence interval
</p>
</td></tr>
<tr><td><code id="solbeta_+3A_b">b</code></td>
<td>


<p>upper bound of the prior 95%~confidence interval
</p>
</td></tr>
<tr><td><code id="solbeta_+3A_c">c</code></td>
<td>


<p>mean of the prior distribution
</p>
</td></tr>
<tr><td><code id="solbeta_+3A_prec">prec</code></td>
<td>


<p>maximal precision on the beta coefficient <code class="reqn">\alpha</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Since the mean <code class="reqn">\mu</code> of the beta distribution is known, there is a single free parameter
<code class="reqn">\alpha</code> to determine, since <code class="reqn">\beta=\alpha(1-\mu)/\mu</code>. The function <code>solbeta</code> searches for
the corresponding value of <code class="reqn">\alpha</code>, starting with a precision of <code class="reqn">1</code> and stopping
at the requested precision <code>prec</code>.

</p>


<h3>Value</h3>



<table>
<tr><td><code>alpha</code></td>
<td>
<p>first coefficient of the beta distribution</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>second coefficient of the beta distribution</p>
</td></tr>

</table>


<h3>See Also</h3>


<p><code><a href="#topic+probet">probet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>solbeta(.1,.5,.3,10^(-4))
</code></pre>

<hr>
<h2 id='sumising'>
Approximation by path sampling of the normalising constant 
for the Ising model
</h2><span id='topic+sumising'></span>

<h3>Description</h3>

<p>This function implements a path sampling approximation of the 
normalising constant of an Ising model with a four neighbour relation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sumising(niter = 10^3, numb, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sumising_+3A_niter">niter</code></td>
<td>

<p>number of iterations
</p>
</td></tr>
<tr><td><code id="sumising_+3A_numb">numb</code></td>
<td>

<p>size of the square grid for the Ising model
</p>
</td></tr>
<tr><td><code id="sumising_+3A_beta">beta</code></td>
<td>

<p>Ising model parameter
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>returns a vector of 21 values for <code class="reqn">Z(\beta)</code> corresponding to a regular sequence
of <code class="reqn">\beta</code>'s between 0 and 2
</p>


<h3>See Also</h3>

<p><code><a href="#topic+isingibbs">isingibbs</a></code>,<code><a href="#topic+isinghm">isinghm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Z=seq(0,2,length=21)
for (i in 1:21)
  Z[i]=sumising(5,numb=24,beta=Z[i])
lrcst=approxfun(seq(0,2,length=21),Z)
plot(seq(0,2,length=21),Z,xlab="",ylab="")
curve(lrcst,0,2,add=TRUE)
</code></pre>

<hr>
<h2 id='thresh'>
Bound for the accept-reject algorithm in Chapter 5
</h2><span id='topic+thresh'></span>

<h3>Description</h3>

<p>This function is used in <code>ardipper</code> to determine the bound for
the accept-reject algorithm simulating the non-standard conditional distribution
of <code class="reqn">r_1</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thresh(k, n1, c2, c3, r2, q1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="thresh_+3A_k">k</code></td>
<td>

<p>current proposal for the number of individuals vanishing
between the first and second experiments
</p>
</td></tr>
<tr><td><code id="thresh_+3A_n1">n1</code></td>
<td>

<p>first capture population size
</p>
</td></tr>
<tr><td><code id="thresh_+3A_c2">c2</code></td>
<td>

<p>number of individuals recaptured during the second experiment
</p>
</td></tr>
<tr><td><code id="thresh_+3A_c3">c3</code></td>
<td>

<p>number of individuals recaptured during the third experiment
</p>
</td></tr>
<tr><td><code id="thresh_+3A_r2">r2</code></td>
<td>

<p>number of individuals vanishing
between the second and third experiments
</p>
</td></tr>
<tr><td><code id="thresh_+3A_q1">q1</code></td>
<td>

<p>probability of disappearing from the population
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This upper bound is equal to
</p>
<p style="text-align: center;"><code class="reqn">\frac{{n_1-c_2 \choose k} {n_1-k \choose c_3+r_2}}{{\bar r\choose k}}</code>
</p>



<h3>Value</h3>


<p>numerical value of the upper bound, to be compared with the uniform random draw
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ardipper">ardipper</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: if (runif(1) &lt; thresh(y,n1,c2,c3,r2,q1))
</code></pre>

<hr>
<h2 id='truncnorm'>
Random simulator for the truncated normal distribution
</h2><span id='topic+truncnorm'></span>

<h3>Description</h3>

<p>This is a plain random generator for a normal variate
<code class="reqn">\mathcal{N}(\mu,\tau^2)</code> truncated
to <code class="reqn">(a,b)</code>, using the inverse cdf <code>qnorm</code>. It may thus
be imprecise for extreme values of the bounds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>truncnorm(n, mu, tau2, a, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="truncnorm_+3A_n">n</code></td>
<td>

<p>number of simulated variates
</p>
</td></tr>
<tr><td><code id="truncnorm_+3A_mu">mu</code></td>
<td>

<p>mean of the original normal
</p>
</td></tr>
<tr><td><code id="truncnorm_+3A_tau2">tau2</code></td>
<td>

<p>variance of the original normal
</p>
</td></tr>
<tr><td><code id="truncnorm_+3A_a">a</code></td>
<td>

<p>lower bound
</p>
</td></tr>
<tr><td><code id="truncnorm_+3A_b">b</code></td>
<td>

<p>upper bound
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>a sample of real numbers over <code class="reqn">(a,b)</code> with size <code>n</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+reconstruct">reconstruct</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x=truncnorm(10^3,1,2,3,4)
hist(x,nclass=123,col="wheat",prob=TRUE)
</code></pre>

<hr>
<h2 id='xneig4'>
Number of neighbours with the same colour
</h2><span id='topic+xneig4'></span>

<h3>Description</h3>

<p>This is a basis function used in simulation algorithms on the Ising
and Potts models. It counts how many of the four neighbours
of <code class="reqn">x_{a,b}</code> are of the same colour as this
pixel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xneig4(x, a, b, col)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xneig4_+3A_x">x</code></td>
<td>

<p>grid of coloured pixels
</p>
</td></tr>
<tr><td><code id="xneig4_+3A_a">a</code></td>
<td>

<p>row index
</p>
</td></tr>
<tr><td><code id="xneig4_+3A_b">b</code></td>
<td>

<p>column index
</p>
</td></tr>
<tr><td><code id="xneig4_+3A_col">col</code></td>
<td>

<p>current or proposed colour
</p>
</td></tr>
</table>


<h3>Value</h3>


<p>integer between 0 and 4 giving the number of neighbours with the same colour
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pottsgibbs">pottsgibbs</a></code>,
<code><a href="#topic+sumising">sumising</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Laichedata)
xneig4(Laichedata,2,3,1)
xneig4(Laichedata,2,3,0)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
