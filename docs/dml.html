<!DOCTYPE html><html><head><title>Help for package dml</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dml}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dca'><p>Discriminative Component Analysis</p></a></li>
<li><a href='#GdmDiag'><p>Global Distance Metric Learning</p></a></li>
<li><a href='#GdmFull'><p>Global Distance Metric Learning</p></a></li>
<li><a href='#rca'><p>Relevant Component Analysis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-08-26</td>
</tr>
<tr>
<td>Title:</td>
<td>Distance Metric Learning in R</td>
</tr>
<tr>
<td>Description:</td>
<td>The state-of-the-art algorithms for distance metric learning, including global and local methods such as Relevant Component Analysis, Discriminative Component Analysis, Local Fisher Discriminant Analysis, etc. These distance metric learning methods are widely applied in feature extraction, dimensionality reduction, clustering, classification, information retrieval, and computer vision problems.</td>
</tr>
<tr>
<td>Depends:</td>
<td>MASS</td>
</tr>
<tr>
<td>Imports:</td>
<td>lfda</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>Author:</td>
<td>Yuan Tang &lt;terrytangyuan@gmail.com&gt;, Gao Tao &lt;joegaotao@gmail.com&gt;, Xiao Nan &lt;road2stat@gmail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yuan Tang &lt;terrytangyuan@gmail.com&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/terrytangyuan/dml">https://github.com/terrytangyuan/dml</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/terrytangyuan/dml/issues">https://github.com/terrytangyuan/dml/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2015-08-28 15:32:47 UTC; Yuan</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2015-08-29 13:14:59</td>
</tr>
</table>
<hr>
<h2 id='dca'>Discriminative Component Analysis</h2><span id='topic+dca'></span>

<h3>Description</h3>

<p>Performs discriminative component analysis on the given data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dca(data, chunks, neglinks, useD = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dca_+3A_data">data</code></td>
<td>
<p><code>n * d</code> data matrix. <code>n</code> is the number of data points,
<code>d</code> is the dimension of the data.
Each data point is a row in the matrix.</p>
</td></tr>
<tr><td><code id="dca_+3A_chunks">chunks</code></td>
<td>
<p>length <code>n</code> vector describing the chunklets:
<code>-1</code> in the <code>i</code> th place means point <code>i</code>
doesn't belong to any chunklet;
integer <code>j</code> in place <code>i</code> means point <code>i</code>
belongs to chunklet j.
The chunklets indexes should be 1:(number of chunklets).</p>
</td></tr>
<tr><td><code id="dca_+3A_neglinks">neglinks</code></td>
<td>
<p><code>s * s</code> symmetric matrix describing the negative relationship
between all the <code>s</code> chunklets.
For the element <code class="reqn">neglinks_{ij}</code>:
<code class="reqn">neglinks_{ij} = 1</code> means chunklet <code>i</code> and chunklet j
have negative constraint(s);
<code class="reqn">neglinks_{ij} = 0</code> means chunklet <code>i</code> and chunklet j
don't have negative constraints
or we don't have information about that.</p>
</td></tr>
<tr><td><code id="dca_+3A_used">useD</code></td>
<td>
<p>Integer. Optional. When not given, DCA is done in the
original dimension and B is full rank. When useD is given,
DCA is preceded by constraints based LDA which reduces the
dimension to useD. B in this case is of rank useD.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Put DCA function details here.
</p>


<h3>Value</h3>

<p>list of the DCA results:
</p>
<table>
<tr><td><code>B</code></td>
<td>
<p>DCA suggested Mahalanobis matrix</p>
</td></tr>
<tr><td><code>DCA</code></td>
<td>
<p>DCA suggested transformation of the data.
The dimension is (original data dimension) * (useD)</p>
</td></tr>
<tr><td><code>newData</code></td>
<td>
<p>DCA transformed data</p>
</td></tr>
</table>
<p>For every two original data points (x1, x2) in newData (y1, y2):
</p>
<p><code class="reqn">(x2 - x1)' * B * (x2 - x1) = || (x2 - x1) * A ||^2 = || y2 - y1 ||^2</code>
</p>


<h3>Note</h3>

<p>Put some note here.
</p>


<h3>Author(s)</h3>

<p>Xiao Nan &lt;<a href="http://www.road2stat.com">http://www.road2stat.com</a>&gt;
</p>


<h3>References</h3>

<p>Steven C.H. Hoi, W. Liu, M.R. Lyu and W.Y. Ma (2006).
Learning Distance Metrics with Contextual Constraints for Image Retrieval.
<em>Proceedings IEEE Conference on Computer Vision and Pattern Recognition
(CVPR2006)</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(123)
require(MASS)  # generate synthetic Gaussian data
k = 100        # sample size of each class
n = 3          # specify how many class
N = k * n      # total sample number
x1 = mvrnorm(k, mu = c(-10, 6), matrix(c(10, 4, 4, 10), ncol = 2))
x2 = mvrnorm(k, mu = c(0, 0), matrix(c(10, 4, 4, 10), ncol = 2))
x3 = mvrnorm(k, mu = c(10, -6), matrix(c(10, 4, 4, 10), ncol = 2))
data = as.data.frame(rbind(x1, x2, x3))
# The fully labeled data set with 3 classes
plot(data$V1, data$V2, bg = c("#E41A1C", "#377EB8", "#4DAF4A")[gl(n, k)],
     pch = c(rep(22, k), rep(21, k), rep(25, k)))
Sys.sleep(3)
# Same data unlabeled; clearly the classes' structure is less evident
plot(x$V1, x$V2)
Sys.sleep(3)

chunk1 = sample(1:100, 5)
chunk2 = sample(setdiff(1:100, chunk1), 5)
chunk3 = sample(101:200, 5)
chunk4 = sample(setdiff(101:200, chunk3), 5)
chunk5 = sample(201:300, 5)
chks = list(chunk1, chunk2, chunk3, chunk4, chunk5)
chunks = rep(-1, 300)
# positive samples in the chunks
for (i in 1:5) {
  for (j in chks[[i]]) {
    chunks[j] = i
  }
}

# define the negative constrains between chunks
neglinks = matrix(c(
		0, 0, 1, 1, 1,
		0, 0, 1, 1, 1,
		1, 1, 0, 0, 0,
		1, 1, 0, 0, 1,
		1, 1, 1, 1, 0),
		ncol = 5, byrow = TRUE)

dcaData = dca(data = data, chunks = chunks, neglinks = neglinks)$newData
# plot DCA transformed data
plot(dcaData[, 1], dcaData[, 2], bg = c("#E41A1C", "#377EB8", "#4DAF4A")[gl(n, k)],
     pch = c(rep(22, k), rep(21, k), rep(25, k)),
     xlim = c(-15, 15), ylim = c(-15, 15))

## End(Not run)
</code></pre>

<hr>
<h2 id='GdmDiag'>Global Distance Metric Learning</h2><span id='topic+GdmDiag'></span>

<h3>Description</h3>

<p>Performs Global Distance Metric Learning (GDM) on the given data, learning a diagonal matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GdmDiag(data, simi, dism, C0 = 1, threshold = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GdmDiag_+3A_data">data</code></td>
<td>
<p><code>n * d</code> data matrix. <code>n</code> is the number of data points,
<code>d</code> is the dimension of the data.
Each data point is a row in the matrix.</p>
</td></tr>
<tr><td><code id="GdmDiag_+3A_simi">simi</code></td>
<td>
<p><code>n * 2</code> matrix describing the similar constrains.
Each row of matrix is serial number of a similar pair in the original data.
For example, pair(1, 3) represents the first observation is similar the 3th observation in the original data.</p>
</td></tr>
<tr><td><code id="GdmDiag_+3A_dism">dism</code></td>
<td>
<p><code>n * 2</code> matrix describing the dissimilar constrains as <code>simi</code>.
Each row of matrix is serial number of a dissimilar pair in the original data.</p>
</td></tr>
<tr><td><code id="GdmDiag_+3A_c0">C0</code></td>
<td>
<p>numeric, the bound of similar constrains.</p>
</td></tr>
<tr><td><code id="GdmDiag_+3A_threshold">threshold</code></td>
<td>
<p>numeric, the threshold of stoping the learning iteration.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Put GdmDiag function details here.
</p>


<h3>Value</h3>

<p>list of the GdmDiag results:
</p>
<table>
<tr><td><code>newData</code></td>
<td>
<p>GdmDiag transformed data</p>
</td></tr>
<tr><td><code>diagonalA</code></td>
<td>
<p>suggested Mahalanobis matrix</p>
</td></tr>
<tr><td><code>dmlA</code></td>
<td>
<p>matrix to transform data, square root of diagonalA </p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>the precision of obtained distance metric by Newton-Raphson optimization </p>
</td></tr>
</table>
<p>For every two original data points (x1, x2) in newData (y1, y2):
</p>
<p><code class="reqn">(x2 - x1)' * A * (x2 - x1) = || (x2 - x1) * B ||^2 = || y2 - y1 ||^2</code>
</p>


<h3>Note</h3>

<p>Be sure to check whether the dimension of original data and constrains' format are valid for the function.
</p>


<h3>Author(s)</h3>

<p>Gao Tao &lt;<a href="http://www.gaotao.name">http://www.gaotao.name</a>&gt;
</p>


<h3>References</h3>

<p>Steven C.H. Hoi, W. Liu, M.R. Lyu and W.Y. Ma (2003).
Distance metric learning, with application to clustering with side-information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(602)
library(MASS)
library(scatterplot3d)

# generate simulated Gaussian data
k = 100
m &lt;- matrix(c(1, 0.5, 1, 0.5, 2, -1, 1, -1, 3), nrow =3, byrow = T)
x1 &lt;- mvrnorm(k, mu = c(1, 1, 1), Sigma = m)
x2 &lt;- mvrnorm(k, mu = c(-1, 0, 0), Sigma = m)
data &lt;- rbind(x1, x2)

# define similar constrains
simi &lt;- rbind(t(combn(1:k, 2)), t(combn((k+1):(2*k), 2)))

temp &lt;-  as.data.frame(t(simi))
tol &lt;- as.data.frame(combn(1:(2*k), 2))

# define disimilar constrains
dism &lt;- t(as.matrix(tol[!tol %in% simi]))

# transform data using GdmDiag
result &lt;- GdmDiag(data, simi, dism)
newData &lt;- result$newData
# plot original data
color &lt;- gl(2, k, labels = c("red", "blue"))
par(mfrow = c(2, 1), mar = rep(0, 4) + 0.1)
scatterplot3d(data, color = color, cex.symbols = 0.6,
			  xlim = range(data[, 1], newData[, 1]),
			  ylim = range(data[, 2], newData[, 2]),
			  zlim = range(data[, 3], newData[, 3]),
			  main = "Original Data")
# plot GdmDiag transformed data
scatterplot3d(newData, color = color, cex.symbols = 0.6,
			  xlim = range(data[, 1], newData[, 1]),
			  ylim = range(data[, 2], newData[, 2]),
			  zlim = range(data[, 3], newData[, 3]),
			  main = "Transformed Data")

## End(Not run)
</code></pre>

<hr>
<h2 id='GdmFull'>Global Distance Metric Learning</h2><span id='topic+GdmFull'></span>

<h3>Description</h3>

<p>Performs Global Distance Metric Learning (GDM) on the given data, learning a full matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GdmFull(data, simi, dism, maxiter = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GdmFull_+3A_data">data</code></td>
<td>
<p><code>n * d</code> data matrix. <code>n</code> is the number of data points,
<code>d</code> is the dimension of the data.
Each data point is a row in the matrix.</p>
</td></tr>
<tr><td><code id="GdmFull_+3A_simi">simi</code></td>
<td>
<p><code>n * 2</code> matrix describing the similar constrains.
Each row of matrix is serial number of a similar pair in the original data.
For example, pair(1, 3) represents the first observation is similar the 3th observation in the original data.</p>
</td></tr>
<tr><td><code id="GdmFull_+3A_dism">dism</code></td>
<td>
<p><code>n * 2</code> matrix describing the dissimilar constrains as <code>simi</code>.
Each row of matrix is serial number of a dissimilar pair in the original data.</p>
</td></tr>
<tr><td><code id="GdmFull_+3A_maxiter">maxiter</code></td>
<td>
<p>numeric, the number of iteration.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Put GdmFull function details here.
</p>


<h3>Value</h3>

<p>list of the GdmDiag results:
</p>
<table>
<tr><td><code>newData</code></td>
<td>
<p>GdmDiag transformed data</p>
</td></tr>
<tr><td><code>fullA</code></td>
<td>
<p>suggested Mahalanobis matrix</p>
</td></tr>
<tr><td><code>dmlA</code></td>
<td>
<p>matrix to transform data, square root of diagonalA </p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>whether the iteration-projection optimization is converged or not</p>
</td></tr>
</table>
<p>For every two original data points (x1, x2) in newData (y1, y2):
</p>
<p><code class="reqn">(x2 - x1)' * A * (x2 - x1) = || (x2 - x1) * B ||^2 = || y2 - y1 ||^2</code>
</p>


<h3>Note</h3>

<p>Be sure to check whether the dimension of original data and constrains' format are valid for the function.
</p>


<h3>Author(s)</h3>

<p>Gao Tao &lt;<a href="http://www.gaotao.name">http://www.gaotao.name</a>&gt;
</p>


<h3>References</h3>

<p>Steven C.H. Hoi, W. Liu, M.R. Lyu and W.Y. Ma (2003).
Distance metric learning, with application to clustering with side-information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(123)
library(MASS)
library(scatterplot3d)

# generate simulated Gaussian data
k = 100
m &lt;- matrix(c(1, 0.5, 1, 0.5, 2, -1, 1, -1, 3), nrow =3, byrow = T)
x1 &lt;- mvrnorm(k, mu = c(1, 1, 1), Sigma = m)
x2 &lt;- mvrnorm(k, mu = c(-1, 0, 0), Sigma = m)
data &lt;- rbind(x1, x2)

# define similar constrains
simi &lt;- rbind(t(combn(1:k, 2)), t(combn((k+1):(2*k), 2)))

temp &lt;-  as.data.frame(t(simi))
tol &lt;- as.data.frame(combn(1:(2*k), 2))

# define disimilar constrains
dism &lt;- t(as.matrix(tol[!tol %in% simi]))

# transform data using GdmFull
result &lt;- GdmFull(data, simi, dism)
newData &lt;- result$newData
# plot original data
color &lt;- gl(2, k, labels = c("red", "blue"))
par(mfrow = c(2, 1), mar = rep(0, 4) + 0.1)
scatterplot3d(data, color = color, cex.symbols = 0.6,
			  xlim = range(data[, 1], newData[, 1]),
			  ylim = range(data[, 2], newData[, 2]),
			  zlim = range(data[, 3], newData[, 3]),
			  main = "Original Data")
# plot GdmFull transformed data
scatterplot3d(newData, color = color, cex.symbols = 0.6,
			  xlim = range(data[, 1], newData[, 1]),
			  ylim = range(data[, 2], newData[, 2]),
			  zlim = range(data[, 3], newData[, 3]),
			  main = "Transformed Data")

## End(Not run)
</code></pre>

<hr>
<h2 id='rca'>Relevant Component Analysis</h2><span id='topic+rca'></span>

<h3>Description</h3>

<p>Performs relevant component analysis on the given data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rca(x, chunks)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rca_+3A_x">x</code></td>
<td>
<p>matrix or data frame of original data.
Each row is a feature vector of a data instance.</p>
</td></tr>
<tr><td><code id="rca_+3A_chunks">chunks</code></td>
<td>
<p>list of <code>k</code> numerical vectors.
Each vector represents a chunklet, the elements
in the vectors indicate where the samples locate
in <code>x</code>. See examples for more information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The RCA function takes a data set and a set of positive constraints
as arguments and returns a linear transformation of the data space
into better representation, alternatively, a Mahalanobis metric
over the data space.
</p>
<p>Relevant component analysis consists of three steps:
</p>
<ol>
<li><p> locate the test point
</p>
</li>
<li><p> compute the distances between the test points
</p>
</li>
<li><p> find <code class="reqn">k</code> shortest distances and the bla</p>
</li></ol>

<p>The new representation is known to be optimal in an information
theoretic sense under a constraint of keeping equivalent data
points close to each other.
</p>


<h3>Value</h3>

<p>list of the RCA results:
</p>
<table>
<tr><td><code>B</code></td>
<td>
<p>The RCA suggested Mahalanobis matrix.
Distances between data points x1, x2 should be
computed by (x2 - x1)' * B * (x2 - x1)</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>The RCA suggested transformation of the data.
The data should be transformed by A * data</p>
</td></tr>
<tr><td><code>newX</code></td>
<td>
<p>The data after the RCA transformation (A).
newData = A * data</p>
</td></tr>
</table>
<p>The three returned argument are just different forms of the same output.
If one is interested in a Mahalanobis metric over the original data space,
the first argument is all she/he needs. If a transformation into another
space (where one can use the Euclidean metric) is preferred, the second
returned argument is sufficient. Using A and B is equivalent in the
following sense:
</p>
<p>if y1 = A * x1, y2 = A * y2  then
(x2 - x1)' * B * (x2 - x1) = (y2 - y1)' * (y2 - y1)
</p>


<h3>Note</h3>

<p>Note that any different sets of instances (chunklets),
e.g. 1, 3, 7 and 4, 6, might belong to the
same class and might belong to different classes.
</p>


<h3>Author(s)</h3>

<p>Xiao Nan &lt;<a href="http://www.road2stat.com">http://www.road2stat.com</a>&gt;
</p>


<h3>References</h3>

<p>Aharon Bar-Hillel, Tomer Hertz, Noam Shental, and Daphna Weinshall (2003).
Learning Distance Functions using Equivalence Relations.
<em>Proceedings of 20th International Conference on
Machine Learning (ICML2003)</em>.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+dca">dca</a></code> for exploiting negative constrains.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(1234)
require(MASS)  # generate synthetic Gaussian data
k = 100        # sample size of each class
n = 3          # specify how many class
N = k * n      # total sample number
x1 = mvrnorm(k, mu = c(-10, 6), matrix(c(10, 4, 4, 10), ncol = 2))
x2 = mvrnorm(k, mu = c(0, 0), matrix(c(10, 4, 4, 10), ncol = 2))
x3 = mvrnorm(k, mu = c(10, -6), matrix(c(10, 4, 4, 10), ncol = 2))
x = as.data.frame(rbind(x1, x2, x3))
x$V3 = gl(n, k)

# The fully labeled data set with 3 classes
plot(x$V1, x$V2, bg = c("#E41A1C", "#377EB8", "#4DAF4A")[x$V3],
     pch = c(rep(22, k), rep(21, k), rep(25, k)))
Sys.sleep(3)

# Same data unlabeled; clearly the classes' structure is less evident
plot(x$V1, x$V2)
Sys.sleep(3)

chunk1 = sample(1:100, 5)
chunk2 = sample(setdiff(1:100, chunk1), 5)
chunk3 = sample(101:200, 5)
chunk4 = sample(setdiff(101:200, chunk3), 5)
chunk5 = sample(201:300, 5)
chks = x[c(chunk1, chunk2, chunk3, chunk4, chunk5), ]
chunks = list(chunk1, chunk2, chunk3, chunk4, chunk5)

# The chunklets provided to the RCA algorithm
plot(chks$V1, chks$V2, col = rep(c("#E41A1C", "#377EB8",
     "#4DAF4A", "#984EA3", "#FF7F00"), each = 5),
     pch = rep(0:4, each = 5), ylim = c(-15, 15))
Sys.sleep(3)

# Whitening transformation applied to the  chunklets
chkTransformed = as.matrix(chks[ , 1:2]) %*% rca(x[ , 1:2], chunks)$A

plot(chkTransformed[ , 1], chkTransformed[ , 2], col = rep(c(
     "#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00"), each = 5),
     pch = rep(0:4, each = 5), ylim = c(-15, 15))
Sys.sleep(3)

# The origin data after applying the RCA transformation
plot(rca(x[ , 1:2], chunks)$newX[, 1], rca(x[ , 1:2], chunks)$newX[, 2],
         bg = c("#E41A1C", "#377EB8", "#4DAF4A")[gl(n, k)],
         pch = c(rep(22, k), rep(21, k), rep(25, k)))

# The RCA suggested transformation of the data, dimensionality reduced
rca(x[ , 1:2], chunks)$A

# The RCA suggested Mahalanobis matrix
rca(x[ , 1:2], chunks)$B

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
