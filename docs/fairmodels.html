<!DOCTYPE html><html><head><title>Help for package fairmodels</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fairmodels}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adult'><p>Adult dataset</p></a></li>
<li><a href='#adult_test'><p>Adult test dataset</p></a></li>
<li><a href='#all_cutoffs'><p>All cutoffs</p></a></li>
<li><a href='#calculate_group_fairness_metrics'><p>Calculate fairness metrics in groups</p></a></li>
<li><a href='#ceteris_paribus_cutoff'><p>Ceteris paribus cutoff</p></a></li>
<li><a href='#choose_metric'><p>Choose metric</p></a></li>
<li><a href='#compas'><p>Modified COMPAS dataset</p></a></li>
<li><a href='#confusion_matrix'><p>Confusion matrix</p></a></li>
<li><a href='#disparate_impact_remover'><p>Disparate impact remover</p></a></li>
<li><a href='#expand_fairness_object'><p>Expand Fairness Object</p></a></li>
<li><a href='#fairness_check'><p>Fairness check</p></a></li>
<li><a href='#fairness_check_regression'><p>Fairness check regression</p></a></li>
<li><a href='#fairness_heatmap'><p>Fairness heatmap</p></a></li>
<li><a href='#fairness_pca'><p>Fairness PCA</p></a></li>
<li><a href='#fairness_radar'><p>Fairness radar</p></a></li>
<li><a href='#german'><p>Modified German Credit data dataset</p></a></li>
<li><a href='#group_matrices'><p>Group confusion matrices</p></a></li>
<li><a href='#group_metric'><p>Group metric</p></a></li>
<li><a href='#group_model_performance'><p>Group model performance</p></a></li>
<li><a href='#metric_scores'><p>Metric scores</p></a></li>
<li><a href='#performance_and_fairness'><p>Performance and fairness</p></a></li>
<li><a href='#plot_density'><p>Plot fairness object</p></a></li>
<li><a href='#plot_fairmodels'><p>Plot fairmodels</p></a></li>
<li><a href='#plot.all_cutoffs'><p>Plot all cutoffs</p></a></li>
<li><a href='#plot.ceteris_paribus_cutoff'><p>Ceteris paribus cutoff plot</p></a></li>
<li><a href='#plot.chosen_metric'><p>Plot chosen metric</p></a></li>
<li><a href='#plot.fairness_heatmap'><p>Plot Heatmap</p></a></li>
<li><a href='#plot.fairness_object'><p>Plot fairness object</p></a></li>
<li><a href='#plot.fairness_pca'><p>Plot fairness PCA</p></a></li>
<li><a href='#plot.fairness_radar'><p>Plot fairness radar</p></a></li>
<li><a href='#plot.fairness_regression_object'><p>Plot fairness regression object</p></a></li>
<li><a href='#plot.group_metric'><p>Plot group metric</p></a></li>
<li><a href='#plot.metric_scores'><p>Plot metric scores</p></a></li>
<li><a href='#plot.performance_and_fairness'><p>Plot fairness and performance</p></a></li>
<li><a href='#plot.stacked_metrics'><p>Plot stacked Metrics</p></a></li>
<li><a href='#pre_process_data'><p>Pre-process data</p></a></li>
<li><a href='#print.all_cutoffs'><p>Print all cutoffs</p></a></li>
<li><a href='#print.ceteris_paribus_cutoff'><p>Print ceteris paribus cutoff</p></a></li>
<li><a href='#print.chosen_metric'><p>Print chosen metric</p></a></li>
<li><a href='#print.fairness_heatmap'><p>Print fairness heatmap</p></a></li>
<li><a href='#print.fairness_object'><p>Print Fairness Object</p></a></li>
<li><a href='#print.fairness_pca'><p>Print fairness PCA</p></a></li>
<li><a href='#print.fairness_radar'><p>Print fairness radar</p></a></li>
<li><a href='#print.fairness_regression_object'><p>Print Fairness Regression Object</p></a></li>
<li><a href='#print.group_metric'><p>Print group metric</p></a></li>
<li><a href='#print.metric_scores'><p>Print metric scores data</p></a></li>
<li><a href='#print.performance_and_fairness'><p>Print performance and fairness</p></a></li>
<li><a href='#print.stacked_metrics'><p>Print stacked metrics</p></a></li>
<li><a href='#regression_metrics'><p>Regression metrics</p></a></li>
<li><a href='#resample'><p>Resample</p></a></li>
<li><a href='#reweight'><p>Reweight</p></a></li>
<li><a href='#roc_pivot'><p>Reject Option based Classification pivot</p></a></li>
<li><a href='#stack_metrics'><p>Stack metrics</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Flexible Tool for Bias Detection, Visualization, and Mitigation</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Measure fairness metrics in one place for many models. Check how big is model's bias towards different races, sex, nationalities etc. Use measures such as Statistical Parity, Equal odds to detect the discrimination against unprivileged groups. Visualize the bias using heatmap, radar plot, biplot, bar chart (and more!). There are various pre-processing and post-processing bias mitigation algorithms implemented. Package also supports calculating fairness metrics for regression models. Find more details in (Wiśniewski, Biecek (2021)) &lt;<a href="https://doi.org/10.48550/arXiv.2104.00507">doi:10.48550/arXiv.2104.00507</a>&gt;.  </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>Imports:</td>
<td>DALEX, ggplot2, scales, stats, patchwork,</td>
</tr>
<tr>
<td>Suggests:</td>
<td>ranger, gbm, knitr, rmarkdown, covr, testthat, spelling,
ggdendro, ggrepel,</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1.9001</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://fairmodels.drwhy.ai/">https://fairmodels.drwhy.ai/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ModelOriented/fairmodels/issues">https://github.com/ModelOriented/fairmodels/issues</a></td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-08-23 19:31:51 UTC; jakwi</td>
</tr>
<tr>
<td>Author:</td>
<td>Jakub Wiśniewski [aut, cre],
  Przemysław Biecek <a href="https://orcid.org/0000-0001-8423-1823"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jakub Wiśniewski &lt;jakwisn@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-08-23 19:50:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='adult'>Adult dataset</h2><span id='topic+adult'></span>

<h3>Description</h3>

<p><code>adult</code> dataset consists of many columns containing various information about relationship, hours worked per week, workclass etc... and about
salary, whether more than 50K a year or not. Lot's of possible protected attributes such as sex, race age. Some columns contain
level &quot;unknown&quot; and these values are not removed and removing them depends on user as they might contain some information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(adult)
</code></pre>


<h3>Format</h3>

<p>A data frame with 32561 rows and 15 variables:
</p>

<dl>
<dt>salary</dt><dd><p>factor, &lt;=50K/&gt;50K whether a person salary exceeds 50K a year or not</p>
</dd>
<dt>age</dt><dd><p>integer, age of person</p>
</dd>
<dt>workclass</dt><dd><p>factor, field of work</p>
</dd>
<dt>fnlwgt</dt><dd><p>numeric</p>
</dd>
<dt>education</dt><dd><p>factor, completed education degree</p>
</dd>
<dt>education_num</dt><dd><p>numeric, education number in converted from education factor, the bigger the better</p>
</dd>
<dt>marital_status</dt><dd><p>factor</p>
</dd>
<dt>occupation</dt><dd><p>factor, where this person works</p>
</dd>
<dt>relationship</dt><dd><p>factor, relationship information</p>
</dd>
<dt>race</dt><dd><p>factor, ethnicity of a person</p>
</dd>
<dt>sex</dt><dd><p>factor, gender of a person</p>
</dd>
<dt>capital_gain</dt><dd><p>numeric</p>
</dd>
<dt>capital_loss</dt><dd><p>numeric</p>
</dd>
<dt>hours_per_week</dt><dd><p>numeric, how many hours per week does this person work</p>
</dd>
<dt>native_country</dt><dd><p>factor, in which country was this person born</p>
</dd>
</dl>



<h3>Source</h3>

<p>Data from UCL <a href="https://archive.ics.uci.edu/ml/datasets/adult">https://archive.ics.uci.edu/ml/datasets/adult</a>
</p>

<hr>
<h2 id='adult_test'>Adult test dataset</h2><span id='topic+adult_test'></span>

<h3>Description</h3>

<p><code>adult_test</code> dataset consists of many columns containing various information about relationship, hours worked per week, workclass etc... and about
salary, whether more than 50K a year or not. Lot's of possible protected attributes such as sex, race age. Some columns contain
level &quot;unknown&quot; and these values are not removed and removing them depends on user as they might contain some information.
Data is designed for testing and ready to go.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(adult_test)
</code></pre>


<h3>Format</h3>

<p>A data frame with 16281 rows and 15 variables:
</p>

<dl>
<dt>salary</dt><dd><p>factor, &lt;=50K/&gt;50K whether a person salary exceeds 50K a year or not</p>
</dd>
<dt>age</dt><dd><p>integer, age of person</p>
</dd>
<dt>workclass</dt><dd><p>factor, field of work</p>
</dd>
<dt>fnlwgt</dt><dd><p>numeric</p>
</dd>
<dt>education</dt><dd><p>factor, completed education degree</p>
</dd>
<dt>education_num</dt><dd><p>numeric, education number in converted from education factor, the bigger the better</p>
</dd>
<dt>marital_status</dt><dd><p>factor</p>
</dd>
<dt>occupation</dt><dd><p>factor, where this person works</p>
</dd>
<dt>relationship</dt><dd><p>factor, relationship information</p>
</dd>
<dt>race</dt><dd><p>factor, ethnicity of a person</p>
</dd>
<dt>sex</dt><dd><p>factor, gender of a person</p>
</dd>
<dt>capital_gain</dt><dd><p>numeric</p>
</dd>
<dt>capital_loss</dt><dd><p>numeric</p>
</dd>
<dt>hours_per_week</dt><dd><p>numeric, how many hours per week does this person work</p>
</dd>
<dt>native_country</dt><dd><p>factor, in which country was this person born</p>
</dd>
</dl>



<h3>Source</h3>

<p>Data from UCL <a href="https://archive.ics.uci.edu/ml/datasets/adult">https://archive.ics.uci.edu/ml/datasets/adult</a>
</p>

<hr>
<h2 id='all_cutoffs'>All cutoffs</h2><span id='topic+all_cutoffs'></span>

<h3>Description</h3>

<p>Create <code>all_cutoffs</code> object and see how with the change of cutoffs parity loss of fairness metrics changes. Value of cutoff changes equally for all subgroups.
User can pick which fairness metrics to create the object with via fairness_metrics vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>all_cutoffs(
  x,
  grid_points = 101,
  fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="all_cutoffs_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="all_cutoffs_+3A_grid_points">grid_points</code></td>
<td>
<p>numeric, grid for cutoffs to test. Number of points between 0 and 1 spread evenly</p>
</td></tr>
<tr><td><code id="all_cutoffs_+3A_fairness_metrics">fairness_metrics</code></td>
<td>
<p>character, name of parity_loss metric or vector of multiple metrics names. Full names can be found in <code>fairness_check</code> documentation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>all_cutoffs</code> object, <code>data.frame</code> containing information about label, metric and parity_loss at particular cutoff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

ac &lt;- all_cutoffs(fobject)
plot(ac)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 100,
  seed = 1
)


explainer_rf &lt;- DALEX::explain(rf_model,
  data = german[, -1],
  y = y_numeric
)

fobject &lt;- fairness_check(explainer_rf, fobject)

ac &lt;- all_cutoffs(fobject)

plot(ac)


</code></pre>

<hr>
<h2 id='calculate_group_fairness_metrics'>Calculate fairness metrics in groups</h2><span id='topic+calculate_group_fairness_metrics'></span>

<h3>Description</h3>

<p>Create <code>data.frame</code> from <code>group_matrices</code> object containing metric scores for each subgroup.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_group_fairness_metrics(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calculate_group_fairness_metrics_+3A_x">x</code></td>
<td>
<p>object of class <code>group_matrices</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>group_metric_matrix</code> object
It's a <code>data.frame</code> with metrics as row names and scores for those metrics for each subgroup in columns
</p>

<hr>
<h2 id='ceteris_paribus_cutoff'>Ceteris paribus cutoff</h2><span id='topic+ceteris_paribus_cutoff'></span>

<h3>Description</h3>

<p>Ceteris paribus cutoff is way to check how will parity loss behave if only cutoff for one subgroup was changed.
By using parameter <code>new_cutoffs</code> parity loss for metrics with new cutoffs will be calculated. Note that cutoff for subgroup (passed as parameter) will
change no matter <code>new_cutoff</code>'s value at that position. When parameter <code>cumulated</code> is set to true, all metrics will be summed and facets will
collapse to one plot with different models on it. Sometimes due to the fact that some metric might contain NA for all cutoff values, cumulated plot might be present without
this model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ceteris_paribus_cutoff(
  x,
  subgroup,
  new_cutoffs = NULL,
  fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP"),
  grid_points = 101,
  cumulated = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ceteris_paribus_cutoff_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="ceteris_paribus_cutoff_+3A_subgroup">subgroup</code></td>
<td>
<p>character, name of subgroup (level in protected variable)</p>
</td></tr>
<tr><td><code id="ceteris_paribus_cutoff_+3A_new_cutoffs">new_cutoffs</code></td>
<td>
<p>list of cutoffs with names matching those of subgroups. Each value should represent cutoff for particular subgroup.
Position corresponding to subgroups in levels will be changed. Default is NULL</p>
</td></tr>
<tr><td><code id="ceteris_paribus_cutoff_+3A_fairness_metrics">fairness_metrics</code></td>
<td>
<p>character, name of parity_loss metric or vector of multiple metrics, for full metric names check <code>fairness_check</code> documentation.</p>
</td></tr>
<tr><td><code id="ceteris_paribus_cutoff_+3A_grid_points">grid_points</code></td>
<td>
<p>numeric, grid for cutoffs to test. Number of points between 0 and 1 spread evenly.</p>
</td></tr>
<tr><td><code id="ceteris_paribus_cutoff_+3A_cumulated">cumulated</code></td>
<td>
<p>logical, if <code>TRUE</code>  facets will collapse to one plot and parity loss for each model will be summed. Default <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ceteris_paribus_cutoff</code> <code>data.frame</code> containing information about label, metric and parity_loss at particular cutoff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("compas")

# positive outcome - not being recidivist
two_yr_recidivism &lt;- factor(compas$Two_yr_Recidivism, levels = c(1, 0))
y_numeric &lt;- as.numeric(two_yr_recidivism) - 1
compas$Two_yr_Recidivism &lt;- two_yr_recidivism


lm_model &lt;- glm(Two_yr_Recidivism ~ .,
  data = compas,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = compas[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = compas$Ethnicity,
  privileged = "Caucasian"
)

cpc &lt;- ceteris_paribus_cutoff(fobject, "African_American")
plot(cpc)

rf_model &lt;- ranger::ranger(Two_yr_Recidivism ~ .,
  data = compas,
  probability = TRUE,
  num.trees = 200
)

explainer_rf &lt;- DALEX::explain(rf_model, data = compas[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = compas$Ethnicity,
  privileged = "Caucasian"
)

cpc &lt;- ceteris_paribus_cutoff(fobject, "African_American")
plot(cpc)


</code></pre>

<hr>
<h2 id='choose_metric'>Choose metric</h2><span id='topic+choose_metric'></span>

<h3>Description</h3>

<p>Extracts metrics from <code>metric_data</code> from fairness object.
It allows to visualize and compare parity loss of chosen metric values across all models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>choose_metric(x, fairness_metric = "FPR")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="choose_metric_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="choose_metric_+3A_fairness_metric">fairness_metric</code></td>
<td>
<p><code>char</code>, single name of metric, one of metrics:
</p>

<ul>
<li><p> TPR - parity loss of True Positive Rate (Sensitivity, Recall, Equal Odds)
</p>
</li>
<li><p> TNR - parity loss of True Negative Rate (Specificity)
</p>
</li>
<li><p> PPV - parity loss of Positive Predictive Value (Precision)
</p>
</li>
<li><p> NPV - parity loss of Negative Predictive Value
</p>
</li>
<li><p> FNR - parity loss of False Negative Rate
</p>
</li>
<li><p> FPR - parity loss of False Positive Rate
</p>
</li>
<li><p> FDR - parity loss of False Discovery Rate
</p>
</li>
<li><p> FOR - parity loss of False Omission Rate
</p>
</li>
<li><p> TS  - parity loss of Threat Score
</p>
</li>
<li><p> ACC - parity loss of Accuracy
</p>
</li>
<li><p> STP - parity loss of Statistical Parity
</p>
</li>
<li><p> F1  - parity loss of F1 Score
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p><code>chosen_metric</code> object
It is a list with following fields:
</p>

<ul>
<li><p>parity_loss_metric_data <code>data.frame</code> with columns: parity_loss_metric and label
</p>
</li>
<li><p>metric chosen metric
</p>
</li>
<li><p>label character, vector of model labels
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)


cm &lt;- choose_metric(fobject, "TPR")
plot(cm)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)


explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

cm &lt;- choose_metric(fobject, "TPR")
plot(cm)


</code></pre>

<hr>
<h2 id='compas'>Modified COMPAS dataset</h2><span id='topic+compas'></span>

<h3>Description</h3>

<p><code>compas</code> dataset. From ProPublica: across the nation, judges, probation and parole officers are increasingly using algorithms to assess a criminal defendant’s likelihood to re-offend.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(compas)
</code></pre>


<h3>Format</h3>

<p>A data frame with 6172 rows and 7 variables:
</p>


<h3>Details</h3>


<dl>
<dt>Two_yr_Recidivism</dt><dd><p>factor, 1/0 for future recidivism or no recidivism. Models should predict this values</p>
</dd>
<dt>Number_of_Priors</dt><dd><p>numeric, number of priors</p>
</dd>
<dt>Age_Above_FourtyFive</dt><dd><p>factor, 1/0 for age above 45 years or not</p>
</dd>
<dt>Age_Below_TwentyFive</dt><dd><p>factor, 1/0 for age below 25 years or not</p>
</dd>
<dt>Misdemeanor</dt><dd><p>factor, 1/0 for having recorded misdemeanor(s) or not</p>
</dd>
<dt>Ethnicity</dt><dd><p>factor, Caucasian, African American, Asian, Hispanic, Native American or Other</p>
</dd>
<dt>Sex</dt><dd><p>factor, female/male for gender</p>
</dd>
</dl>



<h3>Source</h3>

<p>The original source of data is <a href="https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis">https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis</a>.
Modified data used here comes from <a href="https://www.kaggle.com/danofer/compass/">https://www.kaggle.com/danofer/compass/</a> (probublicaCompassRecidivism_data_fairml.csv)
</p>

<hr>
<h2 id='confusion_matrix'>Confusion matrix</h2><span id='topic+confusion_matrix'></span>

<h3>Description</h3>

<p>Calculates confusion matrix for given cutoff
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confusion_matrix(probs, observed, cutoff)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confusion_matrix_+3A_probs">probs</code></td>
<td>
<p>numeric, vector with probabilities given by model</p>
</td></tr>
<tr><td><code id="confusion_matrix_+3A_observed">observed</code></td>
<td>
<p>numeric, vector with actual values from outcome, either 0 or 1</p>
</td></tr>
<tr><td><code id="confusion_matrix_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric, single value denoting cutoff/threshold</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class <code>confussion_matrix</code>
It is a list with following fields:
</p>

<ul>
<li><p>tpnumber of True Positives
</p>
</li>
<li><p>fpnumber of False Positives
</p>
</li>
<li><p>tnnumber of True Negatives
</p>
</li>
<li><p>fnnumber of False Negatives
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
probs &lt;- rnorm(20, 0.4, 0.1)
observed &lt;- round(runif(20))

confusion_matrix(probs, observed, 0.5)
</code></pre>

<hr>
<h2 id='disparate_impact_remover'>Disparate impact remover</h2><span id='topic+disparate_impact_remover'></span>

<h3>Description</h3>

<p>Disparate impact remover is a pre-processing bias mitigation method. It removes bias hidden in numeric columns in data. It changes distribution of ordinal features of data with regard to
earth mover distance. It works best if among subgroups there is similar number of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>disparate_impact_remover(data, protected, features_to_transform, lambda = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="disparate_impact_remover_+3A_data">data</code></td>
<td>
<p><code>data.frame</code>, data to be transformed</p>
</td></tr>
<tr><td><code id="disparate_impact_remover_+3A_protected">protected</code></td>
<td>
<p>factor, vector containing sensitive information such as gender, race etc... If vector is character it will transform it to factor.</p>
</td></tr>
<tr><td><code id="disparate_impact_remover_+3A_features_to_transform">features_to_transform</code></td>
<td>
<p>character, vector of column names to be transformed. Columns must have numerical, ordinal values</p>
</td></tr>
<tr><td><code id="disparate_impact_remover_+3A_lambda">lambda</code></td>
<td>
<p>numeric, amount of repair desired. Value from 0 to 1, where 0 will return almost unchanged dataset and 1 fully repaired dataset</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is implementation of geometric method which preserves ranks unlike combinatorial repair. <code>lambda</code> close to 1 denotes that distributions will be very close to each other
and <code>lambda</code> close to 0 means that densities will barely change. Note that although <code>lambda</code> equal 0 should mean that original data will be returned, it usually changes distributions slightly due to
pigeonholing. The number of pigeonholes is fixed and equal to min101, unique(a), where a is vector with values for subgroup. So if some subgroup is not numerous and
the distribution is discrete with small number of variables then there will be small number of pigeonholes. It will affect data significantly.
</p>


<h3>Value</h3>

<p>repaired data (<code>data.frame</code> object)
</p>


<h3>References</h3>

<p>This method was implemented based on Feldman, Friedler, Moeller, Scheidegger, Venkatasubramanian 2015 <a href="https://arxiv.org/pdf/1412.3756.pdf">https://arxiv.org/pdf/1412.3756.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("ggplot2")

set.seed(1)
# custom data frame with kind and score
custom_data &lt;- data.frame(
  kind = as.factor(c(rep("second", 500), rep("first", 500))),
  score = c(rnorm(500, 400, 40), rnorm(500, 600, 100))
)

ggplot(custom_data, aes(score, fill = kind)) +
  geom_density(alpha = 0.5)

fixed_data &lt;- disparate_impact_remover(
  data = custom_data,
  protected = custom_data$kind,
  features_to_transform = "score",
  lambda = 0.8
)

ggplot(fixed_data, aes(score, fill = kind)) +
  geom_density(alpha = 0.5)

# lambda 1 gives identical distribution, lambda 0 (almost) original distributions

fixed_data_unchanged &lt;- disparate_impact_remover(
  data = custom_data,
  protected = custom_data$kind,
  features_to_transform = "score",
  lambda = 0
)

ggplot(fixed_data_unchanged, aes(score, fill = kind)) +
  geom_density(alpha = 0.5)


fixed_data_fully_changed &lt;- disparate_impact_remover(
  data = custom_data,
  protected = custom_data$kind,
  features_to_transform = "score",
  lambda = 1
)

ggplot(fixed_data_fully_changed, aes(score, fill = kind)) +
  geom_density(alpha = 0.5) +
  facet_wrap(kind ~ ., nrow = 2)
</code></pre>

<hr>
<h2 id='expand_fairness_object'>Expand Fairness Object</h2><span id='topic+expand_fairness_object'></span>

<h3>Description</h3>

<p>Unfold fairness object to 3 columns (metrics, label, score) to construct better base for visualization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expand_fairness_object(
  x,
  scale = FALSE,
  drop_metrics_with_na = FALSE,
  fairness_metrics = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expand_fairness_object_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="expand_fairness_object_+3A_scale">scale</code></td>
<td>
<p>logical, if <code>TRUE</code> standardized.</p>
</td></tr>
<tr><td><code id="expand_fairness_object_+3A_drop_metrics_with_na">drop_metrics_with_na</code></td>
<td>
<p>logical, if <code>TRUE</code> metrics with NA will be omitted</p>
</td></tr>
<tr><td><code id="expand_fairness_object_+3A_fairness_metrics">fairness_metrics</code></td>
<td>
<p>character, vector of fairness metrics names indicating from which expand.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class <code>expand_fairness_object</code>. It is a <code>data.frame</code> with scores for each metric and model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)
expand_fairness_object(fobject, drop_metrics_with_na = TRUE)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)


explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

expand_fairness_object(fobject, drop_metrics_with_na = TRUE)


</code></pre>

<hr>
<h2 id='fairness_check'>Fairness check</h2><span id='topic+fairness_check'></span>

<h3>Description</h3>

<p>Fairness check creates <code>fairness_object</code> which measures different fairness metrics and wraps data, explainers and parameters in useful object. This is fundamental object in this package.
It enables to visualize fairness metrics and models in many ways and compare models on both fairness and performance level. Fairness check acts as merger and wrapper for explainers and fairness objects.
While other fairness objects values are not changed, fairness check assigns cutoffs and labels to provided explainers so same explainers with changed labels/cutoffs might be gradually added to fairness object.
Users through print and plot methods may quickly check values of most popular fairness metrics. More on that topic in details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fairness_check(
  x,
  ...,
  protected = NULL,
  privileged = NULL,
  cutoff = NULL,
  label = NULL,
  epsilon = 0.8,
  verbose = TRUE,
  colorize = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fairness_check_+3A_x">x</code></td>
<td>
<p>object created with <code><a href="DALEX.html#topic+explain">explain</a></code> or of class <code>fairness_object</code>.
It can be multiple fairness_objects, multiple explainers, or combination on both, as long as
they predict the same data. If at least one fairness_object is provided there is no need to
pass protected and privileged parameters. Explainers must be binary classification type.</p>
</td></tr>
<tr><td><code id="fairness_check_+3A_...">...</code></td>
<td>
<p>possibly more objects created with <code><a href="DALEX.html#topic+explain">explain</a></code> and/or objects of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="fairness_check_+3A_protected">protected</code></td>
<td>
<p>factor, protected variable (also called sensitive attribute), containing privileged and unprivileged groups</p>
</td></tr>
<tr><td><code id="fairness_check_+3A_privileged">privileged</code></td>
<td>
<p>factor/character, one value of <code>protected</code>, in regard to what subgroup parity loss is calculated</p>
</td></tr>
<tr><td><code id="fairness_check_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric, vector of cutoffs (thresholds) for each value of protected variable, affecting only explainers.</p>
</td></tr>
<tr><td><code id="fairness_check_+3A_label">label</code></td>
<td>
<p>character, vector of labels to be assigned for explainers, default is explainer label.</p>
</td></tr>
<tr><td><code id="fairness_check_+3A_epsilon">epsilon</code></td>
<td>
<p>numeric, boundary for fairness checking, lowest acceptable ratio of metrics between unprivileged and privileged subgroups. Default value is 0.8. More on the idea behind epsilon in details section.</p>
</td></tr>
<tr><td><code id="fairness_check_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to print information about creation of fairness object</p>
</td></tr>
<tr><td><code id="fairness_check_+3A_colorize">colorize</code></td>
<td>
<p>logical, whether to print information in color</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fairness check
</p>
<p>Metrics used are made for each subgroup, then base metric score is subtracted leaving loss of particular metric.
If absolute loss of metrics ratio is not within acceptable boundaries than such metric is marked as &quot;not passed&quot;. It means that values of metrics should be within (epsilon, 1/epsilon) boundary.
The default ratio is set to 0.8 which adhere to US 80
score achieved in metrics by privileged subgroup. For example if TPR_unprivileged/TPR_privileged is less than 0.8 then such ratio is sign of discrimination. On the other hand if
TPR_privileged/TPR_unprivileged is more than 1.25 (1/0.8) than there is discrimination towards privileged group.
Epsilon value can be adjusted to user's needs. It should be interpreted as the lowest ratio of metrics allowed.  There are some metrics that might be derived from existing metrics (For example Equalized Odds - equal TPR and FPR for all subgroups).
That means passing 5 metrics in fairness check asserts that model is even more fair. In <code>fairness_check</code> models must always predict positive result. Not adhering to this rule
may lead to misinterpretation of the plot. More on metrics and their equivalents:
<a href="https://fairware.cs.umass.edu/papers/Verma.pdf">https://fairware.cs.umass.edu/papers/Verma.pdf</a>
<a href="https://en.wikipedia.org/wiki/Fairness_(machine_learning)">https://en.wikipedia.org/wiki/Fairness_(machine_learning)</a>
</p>
<p>Parity loss - visualization tool
</p>
<p>Parity loss is computed as follows:
M_parity_loss = sum(abs(log(metric/metric_privileged)))
</p>
<p>where:
</p>
<p>M - some metric mentioned above
</p>
<p>metric - vector of metric scores from each subgroup
metric_privileged - value of metric vector for privileged subgroup
</p>
<p>base_metric - scalar, value of metric for base subgroup
</p>


<h3>Value</h3>

<p>An object of class <code>fairness_object</code> which is a list with elements:
</p>

<ul>
<li><p> parity_loss_metric_data - data.frame containing parity loss for various fairness metrics. Created with following metrics:
</p>

<ul>
<li><p>TPR - True Positive Rate (Sensitivity, Recall)
</p>
</li>
<li><p>TNR - True Negative Rate (Specificity)
</p>
</li>
<li><p>PPV - Positive Predictive Value (Precision)
</p>
</li>
<li><p>NPV - Negative Predictive Value
</p>
</li>
<li><p>FNR - False Negative Rate
</p>
</li>
<li><p>FPR - False Positive Rate
</p>
</li>
<li><p>FDR - False Discovery Rate
</p>
</li>
<li><p>FOR - False Omission Rate
</p>
</li>
<li><p>TS - Threat Score
</p>
</li>
<li><p>STP - Statistical Parity
</p>
</li>
<li><p>ACC - Accuracy
</p>
</li>
<li><p>F1 - F1 Score
</p>
</li></ul>

</li>
<li><p>groups_data - metrics across levels in protected variable
</p>
</li>
<li><p>groups_confusion_matrices - confusion matrices for each subgroup
</p>
</li>
<li><p>explainers - list of <code>DALEX</code> explainers used to create object
</p>
</li>
<li><p>cutoffs - list of cutoffs for each explainer and subgroup
</p>
</li>
<li><p>fairness_check_data - <code>data.frame</code> used for for plotting <code>fairness_object</code>
</p>
</li>
<li><p>... - other parameters passed to function
</p>
</li></ul>



<h3>References</h3>

<p>Zafar,Valera, Rodriguez, Gummadi (2017)  <a href="https://arxiv.org/pdf/1610.08452.pdf">https://arxiv.org/pdf/1610.08452.pdf</a>
</p>
<p>Hardt, Price, Srebro (2016) <a href="https://arxiv.org/pdf/1610.02413.pdf">https://arxiv.org/pdf/1610.02413.pdf</a>
</p>
<p>Verma, Rubin (2018) <a href="https://fairware.cs.umass.edu/papers/Verma.pdf">https://fairware.cs.umass.edu/papers/Verma.pdf</a>
</p>
<p>Barocas, Hardt, Narayanan (2019) <a href="https://fairmlbook.org/">https://fairmlbook.org/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)
plot(fobject)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  max.depth = 3,
  num.trees = 100,
  seed = 1
)


explainer_rf &lt;- DALEX::explain(rf_model,
  data = german[, -1],
  y = y_numeric
)

fobject &lt;- fairness_check(explainer_rf, fobject)

plot(fobject)

# custom print
plot(fobject, fairness_metrics = c("ACC", "TPR"))


</code></pre>

<hr>
<h2 id='fairness_check_regression'>Fairness check regression</h2><span id='topic+fairness_check_regression'></span>

<h3>Description</h3>

<p>This is an experimental approach. Please have it in mind when using it.
Fairness_check_regression enables to check fairness in regression models. It uses so-called probabilistic classification to approximate fairness measures.
The metrics in use are independence, separation, and sufficiency. The intuition behind this method is that the closer to 1 the metrics are the better.
When all metrics are close to 1 then it means that from the perspective of a predictive model there are no meaningful differences between subgroups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fairness_check_regression(
  x,
  ...,
  protected = NULL,
  privileged = NULL,
  label = NULL,
  epsilon = NULL,
  verbose = TRUE,
  colorize = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fairness_check_regression_+3A_x">x</code></td>
<td>
<p>object created with <code><a href="DALEX.html#topic+explain">explain</a></code> or of class <code>fairness_regression_object</code>.
It can be multiple fairness_objects, multiple explainers, or combination on both, as long as
they predict the same data. If at least one fairness_object is provided there is no need to
pass protected and privileged parameters. Explainers must be of type regression</p>
</td></tr>
<tr><td><code id="fairness_check_regression_+3A_...">...</code></td>
<td>
<p>possibly more objects created with <code><a href="DALEX.html#topic+explain">explain</a></code> and/or objects of class <code>fairness_regression_object</code></p>
</td></tr>
<tr><td><code id="fairness_check_regression_+3A_protected">protected</code></td>
<td>
<p>factor, protected variable (also called sensitive attribute), containing privileged and unprivileged groups</p>
</td></tr>
<tr><td><code id="fairness_check_regression_+3A_privileged">privileged</code></td>
<td>
<p>factor/character, one value of <code>protected</code>, denoting subgroup suspected of the most privilege</p>
</td></tr>
<tr><td><code id="fairness_check_regression_+3A_label">label</code></td>
<td>
<p>character, vector of labels to be assigned for explainers, default is explainer label.</p>
</td></tr>
<tr><td><code id="fairness_check_regression_+3A_epsilon">epsilon</code></td>
<td>
<p>numeric, boundary for fairness checking, lowest/maximal acceptable metric values for unprivileged. Default value is 0.8.</p>
</td></tr>
<tr><td><code id="fairness_check_regression_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to print information about creation of fairness object</p>
</td></tr>
<tr><td><code id="fairness_check_regression_+3A_colorize">colorize</code></td>
<td>
<p>logical, whether to print information in color</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sometimes during metric calculation faze approximation algorithms (logistic regression models) might not coverage properly. This might
indicate that the membership to subgroups has strong predictive power.
</p>


<h3>References</h3>

<p>Steinberg, Daniel &amp; Reid, Alistair &amp; O'Callaghan, Simon. (2020). Fairness Measures for Regression via Probabilistic Classification. - <a href="https://arxiv.org/pdf/2001.06089.pdf">https://arxiv.org/pdf/2001.06089.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(123)
data &lt;- data.frame(
  x = c(rnorm(500, 500, 100), rnorm(500, 400, 200)),
  pop = c(rep("A", 500), rep("B", 500))
)

data$y &lt;- rnorm(length(data$x), 1.5 * data$x, 100)

# create model
model &lt;- lm(y ~ ., data = data)

# create explainer
exp &lt;- DALEX::explain(model, data = data, y = data$y)

# create fobject
fobject &lt;- fairness_check_regression(exp, protected = data$pop, privileged = "A")

# results

fobject
plot(fobject)


model_ranger &lt;- ranger::ranger(y ~ ., data = data, seed = 123)
exp2 &lt;- DALEX::explain(model_ranger, data = data, y = data$y)

fobject &lt;- fairness_check_regression(exp2, fobject)

# results
fobject

plot(fobject)


</code></pre>

<hr>
<h2 id='fairness_heatmap'>Fairness heatmap</h2><span id='topic+fairness_heatmap'></span>

<h3>Description</h3>

<p>Create <code>fairness_heatmap</code> object to compare both models and metrics.
If parameter <code>scale</code> is set to <code>TRUE</code> metrics will be scaled to median = 0 and sd = 1.
If NA's appear heatmap will still plot, but with gray area where NA's were.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fairness_heatmap(x, scale = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fairness_heatmap_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="fairness_heatmap_+3A_scale">scale</code></td>
<td>
<p>logical, if codeTRUE metrics will be scaled to mean 0 and sd 1. Default <code>FALSE</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>fairness_heatmap</code> object.
</p>
<p>It is a list with following fields:
</p>

<ul>
<li><p>heatmap_data - <code>data.frame</code> with information about score for model and parity loss metric
</p>
</li>
<li><p>matrix_model - matrix used in dendogram plots
</p>
</li>
<li><p>scale - logical parameter passed to <code>fairness_heatmap</code>
</p>
</li>
<li><p>label - character, vector of model labels
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

# same explainers with different cutoffs for female
fobject &lt;- fairness_check(explainer_lm, explainer_rf, fobject,
  protected = german$Sex,
  privileged = "male",
  cutoff = list(female = 0.4),
  label = c("lm_2", "rf_2")
)


fh &lt;- fairness_heatmap(fobject)

plot(fh)
</code></pre>

<hr>
<h2 id='fairness_pca'>Fairness PCA</h2><span id='topic+fairness_pca'></span>

<h3>Description</h3>

<p>Calculate PC for metric_matrix to see similarities between models and metrics. If <code>omit_models_with_NA</code> is set to <code>TRUE</code> models with NA will be omitted as opposed
to default behavior, when metrics are omitted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fairness_pca(x, omit_models_with_NA = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fairness_pca_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness object</code></p>
</td></tr>
<tr><td><code id="fairness_pca_+3A_omit_models_with_na">omit_models_with_NA</code></td>
<td>
<p>logical, if <code>TRUE</code> omits rows in <code>metric_matrix</code>, else omits columns (default)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>fairness_pca</code> object
It is list containing following fields:
</p>

<ul>
<li><p>pc_1_2 - amount of data variance explained with each component
</p>
</li>
<li><p>rotation - rotation from <code>stats::prcomp</code>
</p>
</li>
<li><p>x - x from <code>stats::prcomp</code>
</p>
</li>
<li><p>sdev - sdev from <code>stats::prcomp</code>
</p>
</li>
<li><p>label - model labels
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

# same explainers with different cutoffs for female
fobject &lt;- fairness_check(explainer_lm, explainer_rf, fobject,
  protected = german$Sex,
  privileged = "male",
  cutoff = list(female = 0.4),
  label = c("lm_2", "rf_2")
)

fpca &lt;- fairness_pca(fobject)

plot(fpca)
</code></pre>

<hr>
<h2 id='fairness_radar'>Fairness radar</h2><span id='topic+fairness_radar'></span>

<h3>Description</h3>

<p>Make <code>fairness_radar</code> object with chosen <code>fairness_metrics</code>. Note that there must be at least three metrics that does not contain NA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fairness_radar(x, fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fairness_radar_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="fairness_radar_+3A_fairness_metrics">fairness_metrics</code></td>
<td>
<p>character, vector of metric names, at least 3 metrics without NA needed. Full names of metrics can be found in <code>fairness_check</code> documentation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>fairness_radar</code> object.
It is a list containing:
</p>

<ul>
<li><p>radar_data - <code>data.frame</code> containing scores for each model and parity loss metric
</p>
</li>
<li><p>label - model labels
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

fradar &lt;- fairness_radar(fobject, fairness_metrics = c(
  "ACC", "STP", "TNR",
  "TPR", "PPV"
))

plot(fradar)


rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)


explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)


fradar &lt;- fairness_radar(fobject, fairness_metrics = c(
  "ACC",
  "STP",
  "TNR",
  "TPR",
  "PPV"
))

plot(fradar)


</code></pre>

<hr>
<h2 id='german'>Modified German Credit data dataset</h2><span id='topic+german'></span>

<h3>Description</h3>

<p><code>german</code> dataset. Data contains information about people and their credit risks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(german)
</code></pre>


<h3>Format</h3>

<p>A data frame with 1000 rows and 10 variables:
</p>

<dl>
<dt>Risk</dt><dd><p>factor, good/bad risk connected with giving the credit. Models should predict this values</p>
</dd>
<dt>Sex</dt><dd><p>factor, male/female , considered to be protected group</p>
</dd>
<dt>Job</dt><dd><p>numeric, job titles converted to integers where 0- unemployed/unskilled, 3- management/ self-employed/highly qualified employee/ officer</p>
</dd>
<dt>Housing</dt><dd><p>factor, rent/own/free where this person lives</p>
</dd>
<dt>Saving.accounts</dt><dd><p>factor, little/moderate/quite rich/rich/not_known, where not_known indicates NA</p>
</dd>
<dt>Checking.account</dt><dd><p>factor, little/moderate/rich/not_known, where not_known indicates NA</p>
</dd>
<dt>Credit.amount</dt><dd><p>numeric, amount of money in credit</p>
</dd>
<dt>Duration</dt><dd><p>numeric, duration of credit</p>
</dd>
<dt>Purpose</dt><dd><p>factor, purpose of credit</p>
</dd>
<dt>Age</dt><dd><p>numeric, age of person that applied for credit</p>
</dd>
</dl>



<h3>Source</h3>

<p>Data from kaggle <a href="https://www.kaggle.com/kabure/german-credit-data-with-risk/">https://www.kaggle.com/kabure/german-credit-data-with-risk/</a>. The original source is UCL <a href="https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)">https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)</a>.
</p>

<hr>
<h2 id='group_matrices'>Group confusion matrices</h2><span id='topic+group_matrices'></span>

<h3>Description</h3>

<p>Calculates confusion matrices for each subgroup
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group_matrices(protected, probs, preds, cutoff)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="group_matrices_+3A_protected">protected</code></td>
<td>
<p>vector containing protected variable</p>
</td></tr>
<tr><td><code id="group_matrices_+3A_probs">probs</code></td>
<td>
<p><code>character</code> name of column with probabilities</p>
</td></tr>
<tr><td><code id="group_matrices_+3A_preds">preds</code></td>
<td>
<p>numeric, vector with predictions</p>
</td></tr>
<tr><td><code id="group_matrices_+3A_cutoff">cutoff</code></td>
<td>
<p><code>numeric</code> cutoff for probabilities, default = 0.5</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>group_matrices</code> object
It is a list with values:
</p>

<p>For each subgroup:
</p>
<ul>
<li><p> subgroup
</p>

<ul>
<li><p>tp - number of true positives
</p>
</li>
<li><p>fp - number of false positives
</p>
</li>
<li><p>tn - number of true negatives
</p>
</li>
<li><p>fn - number of false negatives
</p>
</li></ul>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data("compas")

glm_compas &lt;- glm(Two_yr_Recidivism ~ ., data = compas, family = binomial(link = "logit"))
y_prob &lt;- glm_compas$fitted.values

y_numeric &lt;- as.numeric(compas$Two_yr_Recidivism) - 1

gm &lt;- group_matrices(compas$Ethnicity,
  y_prob,
  y_numeric,
  cutoff = list(
    Asian = 0.45,
    African_American = 0.5,
    Other = 0.5,
    Hispanic = 0.5,
    Caucasian = 0.4,
    Native_American = 0.5
  )
)

gm
</code></pre>

<hr>
<h2 id='group_metric'>Group metric</h2><span id='topic+group_metric'></span>

<h3>Description</h3>

<p>Group metric enables to extract data from metrics generated for each subgroup (values in protected variable)
The closer metric values are to each other, the less bias particular model has. If <code>parity_loss</code> parameter is set to <code>TRUE</code>, distance between
privileged and unprivileged subgroups will be measured. When plotted shows both fairness metric and chosen performance metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group_metric(
  x,
  fairness_metric = NULL,
  performance_metric = NULL,
  parity_loss = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="group_metric_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="group_metric_+3A_fairness_metric">fairness_metric</code></td>
<td>
<p>character, fairness metric name, if <code>NULL</code> the default metric will be used which is TPR.</p>
</td></tr>
<tr><td><code id="group_metric_+3A_performance_metric">performance_metric</code></td>
<td>
<p>character, performance metric name</p>
</td></tr>
<tr><td><code id="group_metric_+3A_parity_loss">parity_loss</code></td>
<td>
<p>logical, if <code>TRUE</code> parity loss will supersede basic metric</p>
</td></tr>
<tr><td><code id="group_metric_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to print information about metrics on console or not. Default <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Available metrics:
</p>
<p>Fairness metrics (Full names explained in <code>fairness_check</code> documentation):
</p>

<ul>
<li><p> TPR
</p>
</li>
<li><p> TNR
</p>
</li>
<li><p> PPV
</p>
</li>
<li><p> NPV
</p>
</li>
<li><p> FNR
</p>
</li>
<li><p> FPR
</p>
</li>
<li><p> FDR
</p>
</li>
<li><p> FOR
</p>
</li>
<li><p> TS
</p>
</li>
<li><p> ACC
</p>
</li>
<li><p> STP
</p>
</li>
<li><p> F1
</p>
</li></ul>

<p>Performance metrics
</p>

<ul>
<li><p> recall
</p>
</li>
<li><p> precision
</p>
</li>
<li><p> accuracy
</p>
</li>
<li><p> f1
</p>
</li>
<li><p> auc
</p>
</li></ul>



<h3>Value</h3>

<p><code>group_metric</code> object.
It is a list with following items:
</p>

<ul>
<li><p>group_metric_data - <code>data.frame</code> containing fairness metric scores for each model
</p>
</li>
<li><p>performance_data - <code>data.frame</code> containing performance metric scores for each model
</p>
</li>
<li><p>fairness_metric - name of fairness metric
</p>
</li>
<li><p>performance_metric - name of performance metric
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)


explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

gm &lt;- group_metric(fobject, "TPR", "f1", parity_loss = TRUE)
plot(gm)


rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)

explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

gm &lt;- group_metric(fobject, "TPR", "f1", parity_loss = TRUE)

plot(gm)


</code></pre>

<hr>
<h2 id='group_model_performance'>Group model performance</h2><span id='topic+group_model_performance'></span>

<h3>Description</h3>

<p>Special method for model performance evaluation. Counts number of tp, tn, fp, fn for each subgroup (and therefore potentially distinct cutoff), sums afterwards.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group_model_performance(x, protected, cutoff, performance_metric)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="group_model_performance_+3A_x">x</code></td>
<td>
<p>object created with <code><a href="DALEX.html#topic+explain">explain</a></code></p>
</td></tr>
<tr><td><code id="group_model_performance_+3A_protected">protected</code></td>
<td>
<p>factor, vector with levels as subgroups</p>
</td></tr>
<tr><td><code id="group_model_performance_+3A_cutoff">cutoff</code></td>
<td>
<p>vector of thresholds for each subgroup</p>
</td></tr>
<tr><td><code id="group_model_performance_+3A_performance_metric">performance_metric</code></td>
<td>
<p>name of performance metric</p>
</td></tr>
</table>


<h3>Value</h3>

<p>score in performance metric between 0 and 1
</p>

<hr>
<h2 id='metric_scores'>Metric scores</h2><span id='topic+metric_scores'></span>

<h3>Description</h3>

<p>Creates <code>metric_scores</code> object to facilitate visualization. Check how the metric scores differ among models, what is this score, and how it changes
for example after applying bias mitigation technique. The vertical black lines
denote the scores for privileged subgroup. It is best to use only few metrics (using <code>fairness_metrics</code> parameter)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metric_scores(x, fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metric_scores_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="metric_scores_+3A_fairness_metrics">fairness_metrics</code></td>
<td>
<p>character, vector with fairness metric names. Default metrics are ones in <code>fairness_check</code> plot, full names can be found in <code>fairness_check</code> documentation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>metric_scores</code> object.
It is a list containing:
</p>

<ul>
<li><p>metric_scores_data - <code>data.frame</code> with information about score in particular subgroup, metric, and model
</p>
</li>
<li><p>privileged - name of privileged subgroup
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)


explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

ms &lt;- metric_scores(fobject, fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP"))
plot(ms)


rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)

explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

ms &lt;- metric_scores(fobject, fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP"))
plot(ms)


</code></pre>

<hr>
<h2 id='performance_and_fairness'>Performance and fairness</h2><span id='topic+performance_and_fairness'></span>

<h3>Description</h3>

<p>Measure performance in both fairness metric and
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_and_fairness(x, fairness_metric = NULL, performance_metric = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_and_fairness_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="performance_and_fairness_+3A_fairness_metric">fairness_metric</code></td>
<td>
<p>fairness metric, one of metrics in fairness_objects parity_loss_metric_data  (ACC, TPR, PPV, ...) Full list in <code>fairness_check</code> documentation.</p>
</td></tr>
<tr><td><code id="performance_and_fairness_+3A_performance_metric">performance_metric</code></td>
<td>
<p>performance metric, one of</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Creates <code>perfomance_and_fairness</code> object. Measure model performance and model fairness metric at the same time. Choose best model according to both metrics. When plotted y axis is inversed to accentuate
that models in top right corner are the best according to both metrics.
</p>


<h3>Value</h3>

<p><code>performance_and_fairness</code> object.
It is list containing:
</p>

<ul>
<li><p>paf_data - performance and fairness <code>data.frame</code> containing fairness and performance metric scores for each model
</p>
</li>
<li><p>fairness_metric - chosen fairness metric name
</p>
</li>
<li><p>performance_metric - chosen performance_metric name
</p>
</li>
<li><p>label - model labels
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)


explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

paf &lt;- performance_and_fairness(fobject)
plot(paf)


rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)

explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

# same explainers with different cutoffs for female
fobject &lt;- fairness_check(explainer_lm, explainer_rf, fobject,
  protected = german$Sex,
  privileged = "male",
  cutoff = list(female = 0.4),
  label = c("lm_2", "rf_2")
)

paf &lt;- performance_and_fairness(fobject)

plot(paf)


</code></pre>

<hr>
<h2 id='plot_density'>Plot fairness object</h2><span id='topic+plot_density'></span>

<h3>Description</h3>

<p>Plot distribution for models output probabilities. See how being in particular subgroup affects models decision.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_density(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_density_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="plot_density_+3A_...">...</code></td>
<td>
<p>other plot parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("compas")

glm_compas &lt;- glm(Two_yr_Recidivism ~ ., data = compas, family = binomial(link = "logit"))

y_numeric &lt;- as.numeric(compas$Two_yr_Recidivism) - 1

explainer_glm &lt;- DALEX::explain(glm_compas, data = compas, y = y_numeric)

fobject &lt;- fairness_check(explainer_glm,
  protected = compas$Ethnicity,
  privileged = "Caucasian"
)

plot_density(fobject)
</code></pre>

<hr>
<h2 id='plot_fairmodels'>Plot fairmodels</h2><span id='topic+plot_fairmodels'></span><span id='topic+plot_fairmodels.explainer'></span><span id='topic+plot_fairmodels.fairness_object'></span><span id='topic+plot_fairmodels.default'></span>

<h3>Description</h3>

<p>Easier access to all plots in fairmodels. Provide plot type (that matches to function name), pass additional parameters and plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_fairmodels(x, type, ...)

## S3 method for class 'explainer'
plot_fairmodels(x, type = "fairness_check", ..., protected, privileged)

## S3 method for class 'fairness_object'
plot_fairmodels(x, type = "fairness_check", ...)

## Default S3 method:
plot_fairmodels(x, type = "fairness_check", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_fairmodels_+3A_x">x</code></td>
<td>
<p>object created with <code>fairness_check</code> or with <code><a href="DALEX.html#topic+explain">explain</a></code></p>
</td></tr>
<tr><td><code id="plot_fairmodels_+3A_type">type</code></td>
<td>
<p>character, type of plot. Should match function name in fairmodels. Default is fairness_check.</p>
</td></tr>
<tr><td><code id="plot_fairmodels_+3A_...">...</code></td>
<td>
<p>other parameters passed to fairmodels functions.</p>
</td></tr>
<tr><td><code id="plot_fairmodels_+3A_protected">protected</code></td>
<td>
<p>factor, vector containing sensitive attributes such as gender, race, etc...</p>
</td></tr>
<tr><td><code id="plot_fairmodels_+3A_privileged">privileged</code></td>
<td>
<p>character/factor, level in factor denoting privileged subgroup</p>
</td></tr>
</table>


<h3>Details</h3>

<p>types (function names) available:
</p>

<ul>
<li><p> fairness_check
</p>
</li>
<li><p> stack_metrics
</p>
</li>
<li><p> fairness_heatmap
</p>
</li>
<li><p> fairness_pca
</p>
</li>
<li><p> fairness_radar
</p>
</li>
<li><p> group_metric
</p>
</li>
<li><p> choose_metric
</p>
</li>
<li><p> metric_scores
</p>
</li>
<li><p> performance_and_fairness
</p>
</li>
<li><p> all_cutoffs
</p>
</li>
<li><p> ceteris_paribus_cutoff
</p>
</li></ul>



<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)
explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

# works with explainer when protected and privileged are passed
plot_fairmodels(explainer_lm,
  type = "fairness_radar",
  protected = german$Sex,
  privileged = "male"
)

# or with fairness_object
fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

plot_fairmodels(fobject, type = "fairness_radar")
</code></pre>

<hr>
<h2 id='plot.all_cutoffs'>Plot all cutoffs</h2><span id='topic+plot.all_cutoffs'></span>

<h3>Description</h3>

<p>All cutoffs plot allows to check how parity loss of chosen metrics is affected by the change of cutoff. Values of cutoff
are the same for all subgroups (levels of protected variable) no matter what cutoff values were in <code>fairness_object</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'all_cutoffs'
plot(x, ..., label = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.all_cutoffs_+3A_x">x</code></td>
<td>
<p><code>all_cutoffs</code> object</p>
</td></tr>
<tr><td><code id="plot.all_cutoffs_+3A_...">...</code></td>
<td>
<p>other plot parameters</p>
</td></tr>
<tr><td><code id="plot.all_cutoffs_+3A_label">label</code></td>
<td>
<p>character, label of model to plot. Default NULL. If default prints all models.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

ac &lt;- all_cutoffs(fobject)
plot(ac)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 100,
  seed = 1
)


explainer_rf &lt;- DALEX::explain(rf_model,
  data = german[, -1],
  y = y_numeric
)

fobject &lt;- fairness_check(explainer_rf, fobject)

ac &lt;- all_cutoffs(fobject)
plot(ac)


</code></pre>

<hr>
<h2 id='plot.ceteris_paribus_cutoff'>Ceteris paribus cutoff plot</h2><span id='topic+plot.ceteris_paribus_cutoff'></span>

<h3>Description</h3>

<p>Ceteris paribus cutoff is way to check how will parity loss behave if we changed only cutoff in one subgroup.
It plots object of class ceteris_paribus_cutoff. It might have two types - default and cumulated. Cumulated sums metrics and plots
it all in one plot. When default one is used all chosen metrics will be plotted for each model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ceteris_paribus_cutoff'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ceteris_paribus_cutoff_+3A_x">x</code></td>
<td>
<p>ceteris_paribus_cutoff object</p>
</td></tr>
<tr><td><code id="plot.ceteris_paribus_cutoff_+3A_...">...</code></td>
<td>
<p>other plot parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("compas")

# positive outcome - not being recidivist
two_yr_recidivism &lt;- factor(compas$Two_yr_Recidivism, levels = c(1, 0))
y_numeric &lt;- as.numeric(two_yr_recidivism) - 1
compas$Two_yr_Recidivism &lt;- two_yr_recidivism


lm_model &lt;- glm(Two_yr_Recidivism ~ .,
  data = compas,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = compas[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = compas$Ethnicity,
  privileged = "Caucasian"
)

cpc &lt;- ceteris_paribus_cutoff(fobject, "African_American")
plot(cpc)

rf_model &lt;- ranger::ranger(Two_yr_Recidivism ~ .,
  data = compas,
  probability = TRUE,
  num.trees = 200
)

explainer_rf &lt;- DALEX::explain(rf_model, data = compas[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = compas$Ethnicity,
  privileged = "Caucasian"
)

cpc &lt;- ceteris_paribus_cutoff(fobject, "African_American")
plot(cpc)


</code></pre>

<hr>
<h2 id='plot.chosen_metric'>Plot chosen metric</h2><span id='topic+plot.chosen_metric'></span>

<h3>Description</h3>

<p>Choose metric from parity loss metrics and plot it for every model.
The one with the least parity loss is more fair in terms of this particular metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'chosen_metric'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.chosen_metric_+3A_x">x</code></td>
<td>
<p>object of class <code>chosen_metric</code></p>
</td></tr>
<tr><td><code id="plot.chosen_metric_+3A_...">...</code></td>
<td>
<p>other objects of class <code>chosen_metric</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)


cm &lt;- choose_metric(fobject, "TPR")
plot(cm)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)


explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

cm &lt;- choose_metric(fobject, "TPR")
plot(cm)


</code></pre>

<hr>
<h2 id='plot.fairness_heatmap'>Plot Heatmap</h2><span id='topic+plot.fairness_heatmap'></span>

<h3>Description</h3>

<p>Heatmap shows all parity loss metrics across all models while displaying similarity between variables (in form of dendograms). All metrics are visible. Some have identical values
as it should be in terms of their parity loss (eg. TPR parity loss == FNR parity loss, because TPR = 1 - FNR ).
NA's in metrics are gray.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fairness_heatmap'
plot(
  x,
  ...,
  midpoint = NULL,
  title = NULL,
  subtitle = NULL,
  text = TRUE,
  text_size = 3,
  flip_axis = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fairness_heatmap_+3A_x">x</code></td>
<td>
<p><code>fairness_heatmap</code></p>
</td></tr>
<tr><td><code id="plot.fairness_heatmap_+3A_...">...</code></td>
<td>
<p>other <code>fairness_heatmap</code> objects</p>
</td></tr>
<tr><td><code id="plot.fairness_heatmap_+3A_midpoint">midpoint</code></td>
<td>
<p>numeric, midpoint on gradient scale</p>
</td></tr>
<tr><td><code id="plot.fairness_heatmap_+3A_title">title</code></td>
<td>
<p>character, title of the plot</p>
</td></tr>
<tr><td><code id="plot.fairness_heatmap_+3A_subtitle">subtitle</code></td>
<td>
<p>character, subtitle of the plot</p>
</td></tr>
<tr><td><code id="plot.fairness_heatmap_+3A_text">text</code></td>
<td>
<p>logical, default <code>TRUE</code> means it shows values on tiles</p>
</td></tr>
<tr><td><code id="plot.fairness_heatmap_+3A_text_size">text_size</code></td>
<td>
<p>numeric, size of text</p>
</td></tr>
<tr><td><code id="plot.fairness_heatmap_+3A_flip_axis">flip_axis</code></td>
<td>
<p>logical, whether to change axis with metrics on axis with models</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of <code>ggplot2</code> objects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1,
  seed = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

# same explainers with different cutoffs for female
fobject &lt;- fairness_check(explainer_lm, explainer_rf, fobject,
  protected = german$Sex,
  privileged = "male",
  cutoff = list(female = 0.4),
  label = c("lm_2", "rf_2")
)


fh &lt;- fairness_heatmap(fobject)

plot(fh)
</code></pre>

<hr>
<h2 id='plot.fairness_object'>Plot fairness object</h2><span id='topic+plot.fairness_object'></span>

<h3>Description</h3>

<p>Plot fairness check enables to look how big differences are between base subgroup (privileged) and unprivileged ones.
If bar plot reaches red zone it means that for this subgroup fairness goal is not satisfied. Multiple subgroups and models can be plotted.
Red and green zone boundary can be moved through epsilon parameter, that needs to be passed through <code>fairness_check</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fairness_object'
plot(x, ..., fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fairness_object_+3A_x">x</code></td>
<td>
<p><code>fairness_object</code> object</p>
</td></tr>
<tr><td><code id="plot.fairness_object_+3A_...">...</code></td>
<td>
<p>other plot parameters</p>
</td></tr>
<tr><td><code id="plot.fairness_object_+3A_fairness_metrics">fairness_metrics</code></td>
<td>
<p>character, vector of metrics. Subset of fairness metrics to be used.
The full set is defined as c(&quot;ACC&quot;, &quot;TPR&quot;, &quot;PPV&quot;, &quot;FPR&quot;, &quot;STP&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)
plot(fobject)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  max.depth = 3,
  num.trees = 100,
  seed = 1
)


explainer_rf &lt;- DALEX::explain(rf_model,
  data = german[, -1],
  y = y_numeric
)

fobject &lt;- fairness_check(explainer_rf, fobject)

plot(fobject)

# custom print
plot(fobject, fairness_metrics = c("ACC", "TPR"))


</code></pre>

<hr>
<h2 id='plot.fairness_pca'>Plot fairness PCA</h2><span id='topic+plot.fairness_pca'></span>

<h3>Description</h3>

<p>Plot pca calculated on fairness_object metrics. Similar models and metrics should be close to each other. Plot doesn't work on multiple <code>fairness_pca</code> objects.
Unlike in other plots here other <code>fairness_pca</code> objects cannot be added.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fairness_pca'
plot(x, scale = 0.5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fairness_pca_+3A_x">x</code></td>
<td>
<p><code>fairness_pca</code> object</p>
</td></tr>
<tr><td><code id="plot.fairness_pca_+3A_scale">scale</code></td>
<td>
<p>scaling loadings plot, from 0 to 1</p>
</td></tr>
<tr><td><code id="plot.fairness_pca_+3A_...">...</code></td>
<td>
<p>other plot parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

# same explainers with different cutoffs for female
fobject &lt;- fairness_check(explainer_lm, explainer_rf, fobject,
  protected = german$Sex,
  privileged = "male",
  cutoff = list(female = 0.4),
  label = c("lm_2", "rf_2")
)

fpca &lt;- fairness_pca(fobject)

plot(fpca)
</code></pre>

<hr>
<h2 id='plot.fairness_radar'>Plot fairness radar</h2><span id='topic+plot.fairness_radar'></span>

<h3>Description</h3>

<p>Makes radar plot showing different fairness metrics that allow to compare models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fairness_radar'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fairness_radar_+3A_x">x</code></td>
<td>
<p><code>fairness_radar</code> object</p>
</td></tr>
<tr><td><code id="plot.fairness_radar_+3A_...">...</code></td>
<td>
<p>other plot parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>References</h3>

<p>code based on ModelOriented auditor package, thanks agosiewska!  <a href="https://modeloriented.github.io/auditor/">https://modeloriented.github.io/auditor/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

fradar &lt;- fairness_radar(fobject, fairness_metrics = c(
  "ACC", "STP", "TNR",
  "TPR", "PPV"
))

plot(fradar)


rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)


explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)


fradar &lt;- fairness_radar(fobject, fairness_metrics = c(
  "ACC", "STP", "TNR",
  "TPR", "PPV"
))

plot(fradar)


</code></pre>

<hr>
<h2 id='plot.fairness_regression_object'>Plot fairness regression object</h2><span id='topic+plot.fairness_regression_object'></span>

<h3>Description</h3>

<p>Please note that this is experimental approach. Plot fairness check regression enables to look how big differences are between base subgroup (privileged) and unprivileged ones.
If bar plot reaches red zone it means that for this subgroup fairness goal is not satisfied. Multiple subgroups and models can be plotted.
Red and green zone boundary can be moved through epsilon parameter, that needs to be passed through <code>fairness_check</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fairness_regression_object'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fairness_regression_object_+3A_x">x</code></td>
<td>
<p><code>fairness_regression_object</code> object</p>
</td></tr>
<tr><td><code id="plot.fairness_regression_object_+3A_...">...</code></td>
<td>
<p>other plot parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(123)
data &lt;- data.frame(
  x = c(rnorm(500, 500, 100), rnorm(500, 400, 200)),
  pop = c(rep("A", 500), rep("B", 500))
)

data$y &lt;- rnorm(length(data$x), 1.5 * data$x, 100)

# create model
model &lt;- lm(y ~ ., data = data)

# create explainer
exp &lt;- DALEX::explain(model, data = data, y = data$y)

# create fobject
fobject &lt;- fairness_check_regression(exp, protected = data$pop, privileged = "A")

# results

fobject
plot(fobject)


model_ranger &lt;- ranger::ranger(y ~ ., data = data, seed = 123)
exp2 &lt;- DALEX::explain(model_ranger, data = data, y = data$y)

fobject &lt;- fairness_check_regression(exp2, fobject)

# results
fobject

plot(fobject)


</code></pre>

<hr>
<h2 id='plot.group_metric'>Plot group metric</h2><span id='topic+plot.group_metric'></span>

<h3>Description</h3>

<p>Plot chosen metric in group. Notice how models are treating different subgroups.
Compare models both in fairness metrics and in performance. Parity loss can be enabled when creating <code>group_metric</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'group_metric'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.group_metric_+3A_x">x</code></td>
<td>
<p>object of class group_metric</p>
</td></tr>
<tr><td><code id="plot.group_metric_+3A_...">...</code></td>
<td>
<p>other group_metric objects and other parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of <code>ggplot2</code> objects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)


explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

gm &lt;- group_metric(fobject, "TPR", "f1", parity_loss = TRUE)
plot(gm)


rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)

explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

gm &lt;- group_metric(fobject, "TPR", "f1", parity_loss = TRUE)

plot(gm)


</code></pre>

<hr>
<h2 id='plot.metric_scores'>Plot metric scores</h2><span id='topic+plot.metric_scores'></span>

<h3>Description</h3>

<p>Plot metric scores
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'metric_scores'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.metric_scores_+3A_x">x</code></td>
<td>
<p><code>metric_scores</code> object</p>
</td></tr>
<tr><td><code id="plot.metric_scores_+3A_...">...</code></td>
<td>
<p>other plot parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)


explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

ms &lt;- metric_scores(fobject, fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP"))
plot(ms)


rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)

explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

ms &lt;- metric_scores(fobject, fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP"))
plot(ms)


</code></pre>

<hr>
<h2 id='plot.performance_and_fairness'>Plot fairness and performance</h2><span id='topic+plot.performance_and_fairness'></span>

<h3>Description</h3>

<p>visualize fairness and model metric at the same time. Note that fairness metric parity scale is reversed so that the best models are in top right corner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'performance_and_fairness'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.performance_and_fairness_+3A_x">x</code></td>
<td>
<p><code>performance_and_fairness</code> object</p>
</td></tr>
<tr><td><code id="plot.performance_and_fairness_+3A_...">...</code></td>
<td>
<p>other plot parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)


explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

paf &lt;- performance_and_fairness(fobject)
plot(paf)


rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)

explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

# same explainers with different cutoffs for female
fobject &lt;- fairness_check(explainer_lm, explainer_rf, fobject,
  protected = german$Sex,
  privileged = "male",
  cutoff = list(female = 0.4),
  label = c("lm_2", "rf_2")
)

paf &lt;- performance_and_fairness(fobject)

plot(paf)


</code></pre>

<hr>
<h2 id='plot.stacked_metrics'>Plot stacked Metrics</h2><span id='topic+plot.stacked_metrics'></span>

<h3>Description</h3>

<p>Stacked metrics is like plot for <code>chosen_metric</code> but with all unique metrics stacked on top of each other.
Metrics containing NA's will be dropped to enable fair comparison.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stacked_metrics'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.stacked_metrics_+3A_x">x</code></td>
<td>
<p><code>stacked_metrics</code> object</p>
</td></tr>
<tr><td><code id="plot.stacked_metrics_+3A_...">...</code></td>
<td>
<p>other plot parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)


explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

sm &lt;- stack_metrics(fobject)
plot(sm)


rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)

explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

sm &lt;- stack_metrics(fobject)
plot(sm)


</code></pre>

<hr>
<h2 id='pre_process_data'>Pre-process data</h2><span id='topic+pre_process_data'></span>

<h3>Description</h3>

<p>Function aggregates all pre-processing algorithms for bias mitigation. User passes unified arguments and specifies type to receive transformed <code>data.frame</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pre_process_data(data, protected, y, type = "resample_uniform", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pre_process_data_+3A_data">data</code></td>
<td>
<p><code>data.frame</code></p>
</td></tr>
<tr><td><code id="pre_process_data_+3A_protected">protected</code></td>
<td>
<p>factor, protected attribute (sensitive variable) containing information about gender, race etc...</p>
</td></tr>
<tr><td><code id="pre_process_data_+3A_y">y</code></td>
<td>
<p>numeric, numeric values of predicted variable. 1 should denote favorable outcome.</p>
</td></tr>
<tr><td><code id="pre_process_data_+3A_type">type</code></td>
<td>
<p>character, type of pre-processing algorithm to be used, one of:
</p>

<ul>
<li><p>resample_uniform
</p>
</li>
<li><p>resample_preferential
</p>
</li>
<li><p>reweight
</p>
</li>
<li><p>disparate_impact_remover
</p>
</li></ul>
</td></tr>
<tr><td><code id="pre_process_data_+3A_...">...</code></td>
<td>
<p>other parameters passed to pre-processing algorithms</p>
</td></tr>
</table>


<h3>Value</h3>

<p>modified data (<code>data.frame</code>). In case of type = 'reweight' data has feature '_weights_' containing weights that need to be passed to model.
In other cases data is ready to be passed as training data to a model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("german")

pre_process_data(german,
  german$Sex,
  as.numeric(german$Risk) - 1,
  type = "disparate_impact_remover",
  features_to_transform = "Age"
)
</code></pre>

<hr>
<h2 id='print.all_cutoffs'>Print all cutoffs</h2><span id='topic+print.all_cutoffs'></span>

<h3>Description</h3>

<p>Print all cutoffs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'all_cutoffs'
print(x, ..., label = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.all_cutoffs_+3A_x">x</code></td>
<td>
<p><code>all_cuttofs</code> object</p>
</td></tr>
<tr><td><code id="print.all_cutoffs_+3A_...">...</code></td>
<td>
<p>other print parameters</p>
</td></tr>
<tr><td><code id="print.all_cutoffs_+3A_label">label</code></td>
<td>
<p>character, label of model to plot. Default NULL. If default prints all models.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

ac &lt;- all_cutoffs(fobject,
  fairness_metrics = c(
    "TPR",
    "FPR"
  )
)
print(ac)
</code></pre>

<hr>
<h2 id='print.ceteris_paribus_cutoff'>Print ceteris paribus cutoff</h2><span id='topic+print.ceteris_paribus_cutoff'></span>

<h3>Description</h3>

<p>Print ceteris paribus cutoff
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ceteris_paribus_cutoff'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ceteris_paribus_cutoff_+3A_x">x</code></td>
<td>
<p><code>ceteris_paribus_cutoff</code> object</p>
</td></tr>
<tr><td><code id="print.ceteris_paribus_cutoff_+3A_...">...</code></td>
<td>
<p>other print parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

german &lt;- german[1:500, ]
y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

ceteris_paribus_cutoff(fobject, "female")
</code></pre>

<hr>
<h2 id='print.chosen_metric'>Print chosen metric</h2><span id='topic+print.chosen_metric'></span>

<h3>Description</h3>

<p>Choose metric from parity loss metrics and plot it for every model.
The one with the least parity loss is more fair in terms of this particular metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'chosen_metric'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.chosen_metric_+3A_x">x</code></td>
<td>
<p><code>chosen_metric</code> object</p>
</td></tr>
<tr><td><code id="print.chosen_metric_+3A_...">...</code></td>
<td>
<p>other print parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

cm &lt;- choose_metric(fobject, "TPR")
print(cm)
</code></pre>

<hr>
<h2 id='print.fairness_heatmap'>Print fairness heatmap</h2><span id='topic+print.fairness_heatmap'></span>

<h3>Description</h3>

<p>Print fairness heatmap
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fairness_heatmap'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fairness_heatmap_+3A_x">x</code></td>
<td>
<p><code>fairness_heatmap</code> object</p>
</td></tr>
<tr><td><code id="print.fairness_heatmap_+3A_...">...</code></td>
<td>
<p>other print parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

# same explainers with different cutoffs for female
fobject &lt;- fairness_check(explainer_lm, explainer_rf, fobject,
  protected = german$Sex,
  privileged = "male",
  cutoff = list(female = 0.4),
  label = c("lm_2", "rf_2")
)


fh &lt;- fairness_heatmap(fobject)
print(fh)
</code></pre>

<hr>
<h2 id='print.fairness_object'>Print Fairness Object</h2><span id='topic+print.fairness_object'></span>

<h3>Description</h3>

<p>Print Fairness Object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fairness_object'
print(
  x,
  ...,
  colorize = TRUE,
  fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP"),
  fair_level = NULL,
  border_width = 1,
  loss_aggregating_function = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fairness_object_+3A_x">x</code></td>
<td>
<p><code>fairness_object</code> object</p>
</td></tr>
<tr><td><code id="print.fairness_object_+3A_...">...</code></td>
<td>
<p>other parameters</p>
</td></tr>
<tr><td><code id="print.fairness_object_+3A_colorize">colorize</code></td>
<td>
<p>logical, whether information about metrics should be in color or not</p>
</td></tr>
<tr><td><code id="print.fairness_object_+3A_fairness_metrics">fairness_metrics</code></td>
<td>
<p>character, vector of metrics. Subset of fairness metrics to be used.
The full set is defined as c(&quot;ACC&quot;, &quot;TPR&quot;, &quot;PPV&quot;, &quot;FPR&quot;, &quot;STP&quot;).</p>
</td></tr>
<tr><td><code id="print.fairness_object_+3A_fair_level">fair_level</code></td>
<td>
<p>numerical, amount of fairness metrics that need do be passed in
order to call a model fair. Default is 5.</p>
</td></tr>
<tr><td><code id="print.fairness_object_+3A_border_width">border_width</code></td>
<td>
<p>numerical, width of border between fair and unfair models.
If <code>border_width</code> is 1 and model passes one metric less than the <code>fair_level</code> it will be
printed with yellow. If <code>border_width</code> is 0 information will be printed in either red or green.</p>
</td></tr>
<tr><td><code id="print.fairness_object_+3A_loss_aggregating_function">loss_aggregating_function</code></td>
<td>
<p>function, loss aggregating function that may be provided. It takes
metric scores as vector and aggregates them to one value. The default is 'Total loss' that
measures the total sum of distances to 1. It may be interpreted as sum of bar heights in
fairness_check.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  max.depth = 3,
  num.trees = 100,
  seed = 1,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

explainer_rf &lt;- DALEX::explain(rf_model,
  data = german[, -1],
  y = y_numeric
)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

print(fobject)

# custom print
print(fobject,
  fairness_metrics = c("ACC", "TPR"), # amount of metrics to be printed
  border_width = 0, # in our case 2/2 will be printed in green and 1/2 in red
  loss_aggregating_function = function(x) sum(abs(x)) + 10
) # custom loss function - takes vector
</code></pre>

<hr>
<h2 id='print.fairness_pca'>Print fairness PCA</h2><span id='topic+print.fairness_pca'></span>

<h3>Description</h3>

<p>Print principal components after using pca on fairness object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fairness_pca'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fairness_pca_+3A_x">x</code></td>
<td>
<p><code>fairness_pca</code> object</p>
</td></tr>
<tr><td><code id="print.fairness_pca_+3A_...">...</code></td>
<td>
<p>other print parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

# same explainers with different cutoffs for female
fobject &lt;- fairness_check(explainer_lm, explainer_rf, fobject,
  protected = german$Sex,
  privileged = "male",
  cutoff = list(female = 0.4),
  label = c("lm_2", "rf_2")
)

fpca &lt;- fairness_pca(fobject)

print(fpca)
</code></pre>

<hr>
<h2 id='print.fairness_radar'>Print fairness radar</h2><span id='topic+print.fairness_radar'></span>

<h3>Description</h3>

<p>Print fairness radar
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fairness_radar'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fairness_radar_+3A_x">x</code></td>
<td>
<p><code>fairness_radar</code> object</p>
</td></tr>
<tr><td><code id="print.fairness_radar_+3A_...">...</code></td>
<td>
<p>other print parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)


fradar &lt;- fairness_radar(fobject)

print(fradar)
</code></pre>

<hr>
<h2 id='print.fairness_regression_object'>Print Fairness Regression Object</h2><span id='topic+print.fairness_regression_object'></span>

<h3>Description</h3>

<p>Print Fairness Regression Object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fairness_regression_object'
print(x, ..., colorize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fairness_regression_object_+3A_x">x</code></td>
<td>
<p><code>fairness_regression_object</code> object</p>
</td></tr>
<tr><td><code id="print.fairness_regression_object_+3A_...">...</code></td>
<td>
<p>other parameters</p>
</td></tr>
<tr><td><code id="print.fairness_regression_object_+3A_colorize">colorize</code></td>
<td>
<p>logical, whether information about metrics should be in color or not</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(123)
data &lt;- data.frame(
  x = c(rnorm(500, 500, 100), rnorm(500, 400, 200)),
  pop = c(rep("A", 500), rep("B", 500))
)

data$y &lt;- rnorm(length(data$x), 1.5 * data$x, 100)

# create model
model &lt;- lm(y ~ ., data = data)

# create explainer
exp &lt;- DALEX::explain(model, data = data, y = data$y)

# create fobject
fobject &lt;- fairness_check_regression(exp, protected = data$pop, privileged = "A")

# results

fobject


model_ranger &lt;- ranger::ranger(y ~ ., data = data, seed = 123)
exp2 &lt;- DALEX::explain(model_ranger, data = data, y = data$y)

fobject &lt;- fairness_check_regression(exp2, fobject)

# results
fobject


</code></pre>

<hr>
<h2 id='print.group_metric'>Print group metric</h2><span id='topic+print.group_metric'></span>

<h3>Description</h3>

<p>Print group metric
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'group_metric'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.group_metric_+3A_x">x</code></td>
<td>
<p><code>group_metric</code> object</p>
</td></tr>
<tr><td><code id="print.group_metric_+3A_...">...</code></td>
<td>
<p>other print parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

gm &lt;- group_metric(fobject, "TPR", "f1", parity_loss = TRUE)

print(gm)
</code></pre>

<hr>
<h2 id='print.metric_scores'>Print metric scores data</h2><span id='topic+print.metric_scores'></span>

<h3>Description</h3>

<p>Print metric scores data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'metric_scores'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.metric_scores_+3A_x">x</code></td>
<td>
<p><code>metric_scores</code> object</p>
</td></tr>
<tr><td><code id="print.metric_scores_+3A_...">...</code></td>
<td>
<p>other print parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

ms &lt;- metric_scores(fobject, fairness_metrics = c("TPR", "STP", "ACC"))
ms
</code></pre>

<hr>
<h2 id='print.performance_and_fairness'>Print performance and fairness</h2><span id='topic+print.performance_and_fairness'></span>

<h3>Description</h3>

<p>Print performance and fairness
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'performance_and_fairness'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.performance_and_fairness_+3A_x">x</code></td>
<td>
<p><code>performance_and_fairness</code> object</p>
</td></tr>
<tr><td><code id="print.performance_and_fairness_+3A_...">...</code></td>
<td>
<p>other print parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

# same explainers with different cutoffs for female
fobject &lt;- fairness_check(explainer_lm, explainer_rf, fobject,
  protected = german$Sex,
  privileged = "male",
  cutoff = list(female = 0.4),
  label = c("lm_2", "rf_2")
)

paf &lt;- performance_and_fairness(fobject)

paf
</code></pre>

<hr>
<h2 id='print.stacked_metrics'>Print stacked metrics</h2><span id='topic+print.stacked_metrics'></span>

<h3>Description</h3>

<p>Stack metrics sums parity loss metrics for all models. Higher value of stacked metrics means the model is less fair (has higher bias)
for subgroups from protected vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stacked_metrics'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.stacked_metrics_+3A_x">x</code></td>
<td>
<p><code>stacked_metrics</code> object</p>
</td></tr>
<tr><td><code id="print.stacked_metrics_+3A_...">...</code></td>
<td>
<p>other print parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)

rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200,
  num.threads = 1
)

explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)
explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm, explainer_rf,
  protected = german$Sex,
  privileged = "male"
)

sm &lt;- stack_metrics(fobject)
print(sm)
</code></pre>

<hr>
<h2 id='regression_metrics'>Regression metrics</h2><span id='topic+regression_metrics'></span>

<h3>Description</h3>

<p>Regression metrics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression_metrics(explainer, protected, privileged)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regression_metrics_+3A_explainer">explainer</code></td>
<td>
<p>object created with <code><a href="DALEX.html#topic+explain">explain</a></code></p>
</td></tr>
<tr><td><code id="regression_metrics_+3A_protected">protected</code></td>
<td>
<p>factor, protected variable (also called sensitive attribute), containing privileged and unprivileged groups</p>
</td></tr>
<tr><td><code id="regression_metrics_+3A_privileged">privileged</code></td>
<td>
<p>factor/character, one value of <code>protected</code>, denoting subgroup suspected of the most privilege</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data.frame</code>
</p>

<hr>
<h2 id='resample'>Resample</h2><span id='topic+resample'></span>

<h3>Description</h3>

<p>Method of bias mitigation. Similarly to <code>reweight</code> this method computes desired number of observations if the protected variable is independent
from y and on this basis decides if this subgroup with certain class (+ or -) should be more or less numerous. Than performs oversampling or undersampling depending on the case.
If type of sampling is set to 'preferential' and probs are provided than instead of uniform sampling preferential sampling will be performed. Preferential sampling depending on the case
will sample observations close to border or far from border.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resample(protected, y, type = "uniform", probs = NULL, cutoff = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="resample_+3A_protected">protected</code></td>
<td>
<p>factor, protected variables with subgroups as levels (sensitive attributes)</p>
</td></tr>
<tr><td><code id="resample_+3A_y">y</code></td>
<td>
<p>numeric, vector with classes 0 and 1, where 1 means favorable class.</p>
</td></tr>
<tr><td><code id="resample_+3A_type">type</code></td>
<td>
<p>character, either (default) 'uniform' or 'preferential'</p>
</td></tr>
<tr><td><code id="resample_+3A_probs">probs</code></td>
<td>
<p>numeric, vector with probabilities for preferential sampling</p>
</td></tr>
<tr><td><code id="resample_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric, threshold for probabilities</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of indexes
</p>


<h3>References</h3>

<p>This method was implemented based on Kamiran, Calders 2011 <a href="https://link.springer.com/content/pdf/10.1007/s10115-011-0463-8.pdf">https://link.springer.com/content/pdf/10.1007/s10115-011-0463-8.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("german")

data &lt;- german

data$Age &lt;- as.factor(ifelse(data$Age &lt;= 25, "young", "old"))
y_numeric &lt;- as.numeric(data$Risk) - 1

rf &lt;- ranger::ranger(Risk ~ .,
  data = data,
  probability = TRUE,
  num.trees = 50,
  num.threads = 1,
  seed = 123
)

u_indexes &lt;- resample(data$Age, y = y_numeric)

rf_u &lt;- ranger::ranger(Risk ~ .,
  data = data[u_indexes, ],
  probability = TRUE,
  num.trees = 50,
  num.threads = 1,
  seed = 123
)

explainer_rf &lt;- DALEX::explain(rf,
  data = data[, -1],
  y = y_numeric,
  label = "not_sampled"
)

explainer_rf_u &lt;- DALEX::explain(rf_u, data = data[, -1], y = y_numeric, label = "sampled_uniform")

fobject &lt;- fairness_check(explainer_rf, explainer_rf_u,
  protected = data$Age,
  privileged = "old"
)

fobject
plot(fobject)

p_indexes &lt;- resample(data$Age, y = y_numeric, type = "preferential", probs = explainer_rf$y_hat)
rf_p &lt;- ranger::ranger(Risk ~ .,
  data = data[p_indexes, ],
  probability = TRUE,
  num.trees = 50,
  num.threads = 1,
  seed = 123
)

explainer_rf_p &lt;- DALEX::explain(rf_p,
  data = data[, -1], y = y_numeric,
  label = "sampled_preferential"
)

fobject &lt;- fairness_check(explainer_rf, explainer_rf_u, explainer_rf_p,
  protected = data$Age,
  privileged = "old"
)

fobject
plot(fobject)


</code></pre>

<hr>
<h2 id='reweight'>Reweight</h2><span id='topic+reweight'></span>

<h3>Description</h3>

<p>Function returns weights for model training. The purpose of this weights is to mitigate bias in statistical parity.
In fact this could potentially worsen the overall performance in other fairness metrics. This affects also model's performance metrics (accuracy).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reweight(protected, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reweight_+3A_protected">protected</code></td>
<td>
<p>factor, protected variables with subgroups as levels (sensitive attributes)</p>
</td></tr>
<tr><td><code id="reweight_+3A_y">y</code></td>
<td>
<p>numeric, vector with classes 0 and 1, where 1 means favorable class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method produces weights for each subgroup for each class. Firstly assumes that protected variable and class are independent and
calculates expected probability of this certain event (that subgroup == a and class = c). Than it calculates the actual probability of this event based
on empirical data. Finally the weight is quotient of those probabilities
</p>


<h3>Value</h3>

<p>numeric, vector of weights
</p>


<h3>References</h3>

<p>This method was implemented based on Kamiran, Calders 2011 <a href="https://link.springer.com/content/pdf/10.1007/s10115-011-0463-8.pdf">https://link.springer.com/content/pdf/10.1007/s10115-011-0463-8.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("german")

data &lt;- german
data$Age &lt;- as.factor(ifelse(data$Age &lt;= 25, "young", "old"))
data$Risk &lt;- as.numeric(data$Risk) - 1

# training 2 models
weights &lt;- reweight(protected = data$Age, y = data$Risk)

gbm_model &lt;- gbm::gbm(Risk ~ ., data = data)
gbm_model_weighted &lt;- gbm::gbm(Risk ~ ., data = data, weights = weights)

gbm_explainer &lt;- DALEX::explain(gbm_model, data = data[, -1], y = data$Risk)
gbm_weighted_explainer &lt;- DALEX::explain(gbm_model_weighted, data = data[, -1], y = data$Risk)

fobject &lt;- fairness_check(gbm_explainer, gbm_weighted_explainer,
  protected = data$Age,
  privileged = "old",
  label = c("original", "weighted")
)
# fairness check
fobject
plot(fobject)

# radar
plot(fairness_radar(fobject))
</code></pre>

<hr>
<h2 id='roc_pivot'>Reject Option based Classification pivot</h2><span id='topic+roc_pivot'></span>

<h3>Description</h3>

<p>Reject Option based Classifier is post-processing bias mitigation method. Method changes labels of favorable, privileged and close to cutoff observations to unfavorable
and the opposite for unprivileged observations (changing unfavorable and close to cutoff observations to favorable, more in details).
By this potentially wrongfully labeled observations are assigned different labels.
Note that in y in DALEX explainer 1 should indicate favorable outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_pivot(explainer, protected, privileged, cutoff = 0.5, theta = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_pivot_+3A_explainer">explainer</code></td>
<td>
<p>created with <code><a href="DALEX.html#topic+explain">explain</a></code></p>
</td></tr>
<tr><td><code id="roc_pivot_+3A_protected">protected</code></td>
<td>
<p>factor, protected variables with subgroups as levels (sensitive attributes)</p>
</td></tr>
<tr><td><code id="roc_pivot_+3A_privileged">privileged</code></td>
<td>
<p>factor/character, level in protected denoting privileged subgroup</p>
</td></tr>
<tr><td><code id="roc_pivot_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric, threshold for all subgroups</p>
</td></tr>
<tr><td><code id="roc_pivot_+3A_theta">theta</code></td>
<td>
<p>numeric, variable specifies maximal euclidean distance to cutoff resulting ing label switch</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method implemented implemented based on article (Kamiran, Karim, Zhang 2012). In original implementation labels should be switched. Due to specific DALEX methods
probabilities (y_hat) are assigned value in equal distance but other side of cutoff. The method changes explainers y_hat values in two cases.
<br />
1. When unprivileged subgroup is within (cutoff - theta, cutoff)<br />
2. When privileged subgroup is within (cutoff, cutoff + theta)<br />
</p>


<h3>Value</h3>

<p>DALEX <code>explainer</code> with changed y_hat. This explainer should be used ONLY by fairmodels as it contains unchanged
predict function (changed predictions (y_hat) can possibly be invisible by DALEX functions and methods).
</p>


<h3>References</h3>

<p>Kamiran, Karim, Zhang 2012 <a href="https://ieeexplore.ieee.org/document/6413831/">https://ieeexplore.ieee.org/document/6413831/</a> ROC method
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")
data &lt;- german
data$Age &lt;- as.factor(ifelse(data$Age &lt;= 25, "young", "old"))
y_numeric &lt;- as.numeric(data$Risk) - 1

lr_model &lt;- stats::glm(Risk ~ ., data = data, family = binomial())
lr_explainer &lt;- DALEX::explain(lr_model, data = data[, -1], y = y_numeric)

fobject &lt;- fairness_check(lr_explainer,
  protected = data$Age,
  privileged = "old"
)
plot(fobject)

lr_explainer_fixed &lt;- roc_pivot(lr_explainer,
  protected = data$Age,
  privileged = "old"
)

fobject2 &lt;- fairness_check(lr_explainer_fixed, fobject,
  protected = data$Age,
  privileged = "old",
  label = "lr_fixed"
)
fobject2
plot(fobject2)
</code></pre>

<hr>
<h2 id='stack_metrics'>Stack metrics</h2><span id='topic+stack_metrics'></span>

<h3>Description</h3>

<p>Stack metrics sums parity loss metrics for all models. Higher value of stacked metrics means the model is less fair (has higher bias)
for subgroups from protected vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stack_metrics(x, fairness_metrics = c("ACC", "TPR", "PPV", "FPR", "STP"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stack_metrics_+3A_x">x</code></td>
<td>
<p>object of class <code>fairness_object</code></p>
</td></tr>
<tr><td><code id="stack_metrics_+3A_fairness_metrics">fairness_metrics</code></td>
<td>
<p>character, vector of fairness parity_loss metric names to include in plot. Full names are provided in <code>fairess_check</code> documentation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>stacked_metrics</code> object. It contains <code>data.frame</code> with information about score for each metric and model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("german")

y_numeric &lt;- as.numeric(german$Risk) - 1

lm_model &lt;- glm(Risk ~ .,
  data = german,
  family = binomial(link = "logit")
)


explainer_lm &lt;- DALEX::explain(lm_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_lm,
  protected = german$Sex,
  privileged = "male"
)

sm &lt;- stack_metrics(fobject)
plot(sm)


rf_model &lt;- ranger::ranger(Risk ~ .,
  data = german,
  probability = TRUE,
  num.trees = 200
)

explainer_rf &lt;- DALEX::explain(rf_model, data = german[, -1], y = y_numeric)

fobject &lt;- fairness_check(explainer_rf, fobject)

sm &lt;- stack_metrics(fobject)
plot(sm)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
