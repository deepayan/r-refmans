<!DOCTYPE html><html lang="en"><head><title>Help for package statConfR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {statConfR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#estimateMetaI'><p>Estimate Measures of Metacognition from Information Theory</p></a></li>
<li><a href='#fitConf'><p>Fit a static confidence model to data</p></a></li>
<li><a href='#fitConfModels'><p>Fit several static confidence models to multiple participants</p></a></li>
<li><a href='#fitMetaDprime'><p>title Compute measures of metacognitive sensitivity (meta-d') and metacognitive efficiency(meta-d'/d') for data from one or several subjects</p></a></li>
<li><a href='#MaskOri'><p>Data of 16 participants in a masked orientation discrimination experiment (Hellmann et al., 2023, Exp. 1)</p></a></li>
<li><a href='#plotConfModelFit'><p>Plot the prediction of fitted parameters of one model of confidence over the corresponding  data</p></a></li>
<li><a href='#simConf'><p>Simulate data  according to a static model of confidence</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Models of Decision Confidence and Measures of Metacognition</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-02-11</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Manuel Rausch &lt;manuel.rausch@hochschule-rhein-waal.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides fitting functions and other tools for decision confidence 
    and metacognition researchers, including meta-d'/d', often considered to be 
    the gold standard to measure metacognitive efficiency, and information-theoretic measures of metacognition. 
    Also allows to fit several static models of decision making and confidence.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ManuelRausch/StatConfR">https://github.com/ManuelRausch/StatConfR</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ManuelRausch/StatConfR/issues">https://github.com/ManuelRausch/StatConfR/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>parallel, plyr, stats, utils, ggplot2, Rmisc</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-13 17:10:01 UTC</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-13 13:50:54 UTC; PPA714</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Author:</td>
<td>Manuel Rausch <a href="https://orcid.org/0000-0002-5805-5544"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Sascha Meyen <a href="https://orcid.org/0000-0001-6928-4126"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Sebastian Hellmann
    <a href="https://orcid.org/0000-0002-3621-6343"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut]</td>
</tr>
</table>
<hr>
<h2 id='estimateMetaI'>Estimate Measures of Metacognition from Information Theory</h2><span id='topic+estimateMetaI'></span>

<h3>Description</h3>

<p><code>estimateMetaI</code> estimates meta-<code class="reqn">I</code>, an information-theoretic
measure of metacognitive sensitivity proposed by Dayan (2023), as well as
similar derived measures, including meta-<code class="reqn">I_{1}^{r}</code> and Meta-<code class="reqn">I_{2}^{r}</code>.
These are different normalizations of meta-<code class="reqn">I</code>:
</p>

<ul>
<li><p> Meta-<code class="reqn">I_{1}^{r}</code> normalizes by the meta-<code class="reqn">I</code> that would be
expected from an underlying normal distribution with the same
sensitivity.
</p>
</li>
<li><p> Meta-<code class="reqn">I_{1}^{r\prime}</code> is a variant of meta-<code class="reqn">I_{1}^{r}</code> not discussed by Dayan
(2023) which normalizes by the meta-<code class="reqn">I</code> that would be expected from
an underlying normal distribution with the same accuracy (this is
similar to the sensitivity approach but without considering variable
thresholds).
</p>
</li>
<li><p> Meta-<code class="reqn">I_{2}^{r}</code> normalizes by the maximum amount of meta-<code class="reqn">I</code>
which would be reached if all uncertainty about the stimulus was removed.
</p>
</li>
<li> <p><code class="reqn">RMI</code> normalizes meta-<code class="reqn">I</code> by the range of its possible
values and therefore scales between 0 and 1. RMI is a novel measure not discussed by Dayan (2023).
</p>
</li></ul>

<p>All measures can be calculated with a bias-reduced variant for which the
observed frequencies are taken as underlying probability distribution to
estimate the sampling bias. The estimated bias is then subtracted from the
initial measures. This approach uses Monte-Carlo simulations and is
therefore not deterministic (values can vary from one evaluation of the
function to the next). However, this is a simple way to reduce the bias
inherent in these measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimateMetaI(data, bias_reduction = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estimateMetaI_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> where each row is one trial, containing following
variables:
</p>

<ul>
<li> <p><code>participant</code> (some group ID, most often a participant identifier;
the meta-I measures are estimated for each subset of <code>data</code>
determined by the different values of this column),
</p>
</li>
<li> <p><code>stimulus</code> (stimulus category in a binary choice task,
should be a factor with two levels, otherwise it will be transformed to
a factor with a warning),
</p>
</li>
<li> <p><code>rating</code> (discrete confidence judgments, should be a factor with levels
ordered from lowest confidence to highest confidence;
otherwise will be transformed to factor with a warning),
</p>
</li>
<li> <p><code>correct</code> (encoding whether the response was correct; should  be 0 for
incorrect responses and 1 for correct responses)
</p>
</li></ul>
</td></tr>
<tr><td><code id="estimateMetaI_+3A_bias_reduction">bias_reduction</code></td>
<td>
<p><code>logical</code>. Whether to apply the bias reduction or
not. If runtime is too long, consider setting this to FALSE
(default: TRUE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that a classifier (possibly a human being performing a discrimination task)
or an algorithmic classifier in a classification application,
makes a binary prediction <code class="reqn">R</code> about a true state of the world <code class="reqn">S</code> and gives a confidence rating <code class="reqn">C</code>.
Meta-<code class="reqn">I</code> is defined as the mutual information between the confidence and
accuracy and is calculated as the transmitted information minus the
minimal information given the accuracy,
</p>
<p style="text-align: center;"><code class="reqn">meta-I = I(S; R, C) - I(S; R).</code>
</p>

<p>This is equivalent to Dayan's formulation where meta-I is the information
that confidence transmits about the correctness of a response,
</p>
<p style="text-align: center;"><code class="reqn">meta-I = I(S = R; C).</code>
</p>

<p>Meta-<code class="reqn">I</code> is expressed in bits, i.e. the log base is 2).
The other measures are different normalizations of meta-<code class="reqn">I</code> and are unitless.
It should be noted that Dayan (2023) pointed out that a liberal or
conservative use of the confidence levels will affected the mutual
information and thus influence meta-I.
</p>


<h3>Value</h3>

<p>a <code>data.frame</code> with one row for each subject and the following
columns:
</p>

<ul>
<li> <p><code>participant</code> is the participant ID,
</p>
</li>
<li> <p><code>meta_I</code> is the estimated meta-<code class="reqn">I</code> value (expressed in bits, i.e. log base is 2),
</p>
</li>
<li> <p><code>meta_Ir1</code> is meta-<code class="reqn">I_{1}^{r}</code>,
</p>
</li>
<li> <p><code>meta_Ir1_acc</code> is meta-<code class="reqn">I_{1}^{r\prime}</code>,
</p>
</li>
<li> <p><code>meta_Ir2</code> is meta-<code class="reqn">I_{2}^{r}</code>, and
</p>
</li>
<li> <p><code>RMI</code> is RMI.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Sascha Meyen, <a href="mailto:saschameyen@gmail.com">saschameyen@gmail.com</a>
</p>


<h3>References</h3>

<p>Dayan, P. (2023). Metacognitive Information Theory.
Open Mind, 7, 392â€“411. doi:10.1162/opmi_a_00091
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 1. Select two subjects from the masked orientation discrimination experiment
data &lt;- subset(MaskOri, participant %in% c(1:2))
head(data)

# 2. Calculate meta-I measures with bias reduction (this may take 10 s per subject)

metaIMeasures &lt;- estimateMetaI(data)


# 3. Calculate meta-I measures for all participants without bias reduction (much faster)
metaIMeasures &lt;- estimateMetaI(MaskOri, bias_reduction = FALSE)
metaIMeasures
</code></pre>

<hr>
<h2 id='fitConf'>Fit a static confidence model to data</h2><span id='topic+fitConf'></span>

<h3>Description</h3>

<p>The <code>fitConf</code> function fits the parameters of one static model of decision confidence,
provided by the <code>model</code> argument, to binary choices and confidence judgments.
See Details for the mathematical specification of the implemented models and
their parameters.
Parameters are fitted using a maximum likelihood estimation method with a
initial grid search to find promising starting values for the optimization.
In addition, several measures of model fit (negative log-likelihood, BIC, AIC, and AICc)
are computed, which can be used for a quantitative model evaluation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitConf(data, model = "SDT", nInits = 5, nRestart = 4)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fitConf_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> where each row is one trial, containing following
variables:
</p>

<ul>
<li> <p><code>diffCond</code> (optional; different levels of discriminability,
should be a factor with levels ordered from hardest to easiest),
</p>
</li>
<li> <p><code>rating</code> (discrete confidence judgments, should be a factor with levels ordered from lowest confidence to highest confidence;
otherwise will be transformed to factor with a warning),
</p>
</li>
<li> <p><code>stimulus</code> (stimulus category in a binary choice task,
should be a factor with two levels, otherwise it will be transformed to
a factor with a warning),
</p>
</li>
<li> <p><code>correct</code> (encoding whether the response was correct; should  be 0 for incorrect responses and 1 for correct responses)
</p>
</li></ul>
</td></tr>
<tr><td><code id="fitConf_+3A_model">model</code></td>
<td>
<p><code>character</code> of length 1. The generative model that should be
fitted. Models implemented so far: 'WEV', 'SDT', 'GN', 'PDA', 'IG',
'ITGc', 'ITGcm', 'logN', and 'logWEV'.</p>
</td></tr>
<tr><td><code id="fitConf_+3A_ninits">nInits</code></td>
<td>
<p><code>integer</code>. Number of starting values used for maximum likelihood optimization.
Defaults to 5.</p>
</td></tr>
<tr><td><code id="fitConf_+3A_nrestart">nRestart</code></td>
<td>
<p><code>integer</code>. Number of times the optimization algorithm is restarted.
Defaults to 4.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The fitting routine first performs a coarse grid search to find promising
starting values for the maximum likelihood optimization procedure. Then the best <code>nInits</code>
parameter sets found by the grid search are used as the initial values for separate
runs of the Nelder-Mead algorithm implemented in <code><a href="stats.html#topic+optim">optim</a></code>.
Each run is restarted <code>nRestart</code> times.
</p>


<h4>Mathematical description of models</h4>

<p>The computational models are all based on signal detection theory (Green &amp; Swets, 1966). It is assumed
that participants select a binary discrimination response <code class="reqn">R</code> about a stimulus <code class="reqn">S</code>.
Both <code class="reqn">S</code> and <code class="reqn">R</code> can be either -1 or 1.
<code class="reqn">R</code> is considered correct if <code class="reqn">S=R</code>.
In addition, we assume that there are <code class="reqn">K</code> different levels of stimulus discriminability
in the experiment, i.e. a physical variable that makes the discrimination task easier or harder.
For each level of discriminability, the function fits a different discrimination
sensitivity parameter <code class="reqn">d_k</code>. If there is more than one sensitivity parameter,
we assume that the sensitivity parameters are ordered such as <code class="reqn">0 &lt; d_1 &lt;  ... &lt; d_K</code>.
The models assume that the stimulus generates normally distributed sensory evidence <code class="reqn">x</code> with mean <code class="reqn">S\times d_k/2</code>
and variance of 1. The sensory evidence <code class="reqn">x</code> is compared to a decision
criterion <code class="reqn">c</code> to generate a discrimination response
<code class="reqn">R</code>, which is 1, if <code class="reqn">x</code> exceeds <code class="reqn">c</code> and -1 else.
To generate confidence, it is assumed that the confidence variable <code class="reqn">y</code> is compared to another
set of criteria <code class="reqn">\theta_{R,i}, i = 1, ..., L-1</code>, depending on the
discrimination response <code class="reqn">R</code> to produce a <code class="reqn">L</code>-step discrete confidence response.
The number of thresholds will be inferred from the number of steps in the
<code>rating</code> column of <code>data</code>. Thus, the parameters shared between all models are:
</p>

<ul>
<li><p> sensitivity parameters <code class="reqn">d_1</code>,...,<code class="reqn">d_K</code> (<code class="reqn">K</code>: number of difficulty levels)
</p>
</li>
<li><p> decision criterion <code class="reqn">c</code>
</p>
</li>
<li><p> confidence criterion <code class="reqn">\theta_{-1,1}</code>,<code class="reqn">\theta_{-1,2}</code>,
..., <code class="reqn">\theta_{-1,L-1}</code>, <code class="reqn">\theta_{1,1}</code>,  <code class="reqn">\theta_{1,2}</code>,...,
<code class="reqn">\theta_{1,L-1}</code> (<code class="reqn">L</code>: number of confidence categories available for confidence ratings)
</p>
</li></ul>

<p>How the confidence variable <code class="reqn">y</code> is computed varies across the different models.
The following models have been implemented so far:
</p>


<h5><strong>Signal detection rating model (SDT)</strong></h5>

<p>According to SDT, the same sample of sensory
evidence is used to generate response and confidence, i.e.,
<code class="reqn">y=x</code> and the confidence criteria span from the left and
right side of the decision criterion <code class="reqn">c</code> (Green &amp; Swets, 1966).
</p>



<h5><strong>Gaussian noise model (GN)</strong></h5>

<p>According to the model, <code class="reqn">y</code> is subject to
additive noise and assumed to be normally distributed around the decision
evidence value <code class="reqn">x</code> with a standard deviation <code class="reqn">\sigma</code> (Maniscalco &amp; Lau, 2016).
The parameter  <code class="reqn">\sigma</code> is a free parameter.
</p>



<h5><strong>Weighted evidence and visibility model (WEV)</strong></h5>

<p>WEV assumes that the observer combines evidence about decision-relevant features
of the stimulus with the strength of evidence about choice-irrelevant features
to generate confidence (Rausch et al., 2018). Here, we use the version of the WEV model
used by Rausch et al. (2023), which assumes that <code class="reqn">y</code> is normally
distributed with a mean of <code class="reqn">(1-w)\times x+w \times d_k\times R</code> and standard deviation <code class="reqn">\sigma</code>.
The parameter <code class="reqn">\sigma</code> quantifies the amount of unsystematic variability
contributing to confidence judgments but not to the discrimination judgments.
The parameter <code class="reqn">w</code> represents the weight that is put on the choice-irrelevant
features in the confidence judgment. <code class="reqn">w</code> and <code class="reqn">\sigma</code> are fitted in
addition to the set of shared parameters.
</p>



<h5><strong>Post-decisional accumulation model (PDA)</strong></h5>

<p>PDA represents the idea of on-going information accumulation after the
discrimination choice (Rausch et al., 2018). The parameter <code class="reqn">b</code> indicates the amount of additional
accumulation. The confidence variable is normally distributed with mean
<code class="reqn">x+S\times d_k\times b</code> and variance <code class="reqn">b</code>.
For this model the parameter <code class="reqn">b</code> is fitted in
addition to the set of shared parameters.
</p>



<h5><strong>Independent Gaussian model (IG)</strong></h5>

<p>According to IG, <code class="reqn">y</code> is sampled independently
from <code class="reqn">x</code> (Rausch &amp; Zehetleitner, 2017). <code class="reqn">y</code> is normally distributed with a mean of <code class="reqn">a\times d_k</code> and variance
of 1 (again as it would scale with <code class="reqn">m</code>). The free parameter <code class="reqn">m</code>
represents the amount of information available for confidence judgment
relative to amount of evidence available for the discrimination decision and can
be smaller as well as greater than 1.
</p>



<h5><strong>Independent truncated Gaussian model: HMetad-Version (ITGc)</strong></h5>

<p>According to the version of ITG consistent
with the HMetad-method (Fleming, 2017; see Rausch et al., 2023), <code class="reqn">y</code> is sampled independently
from <code class="reqn">x</code> from a truncated Gaussian distribution with a location parameter
of <code class="reqn">S\times d_k \times m/2</code> and a scale parameter of 1. The Gaussian distribution of <code class="reqn">y</code>
is truncated in a way that it is impossible to sample evidence that contradicts
the original decision: If <code class="reqn">R = -1</code>, the distribution is truncated to the
right of <code class="reqn">c</code>. If <code class="reqn">R = 1</code>, the distribution is truncated to the left
of <code class="reqn">c</code>. The additional parameter <code class="reqn">m</code> represents metacognitive efficiency,
i.e., the amount of information available for confidence judgments relative to
amount of evidence available for discrimination decisions and  can be smaller
as well as greater than 1.
</p>



<h5><strong>Independent truncated Gaussian model: Meta-d'-Version (ITGcm)</strong></h5>

<p>According to the version of the ITG consistent
with the original meta-d' method (Maniscalco &amp; Lau, 2012, 2014; see Rausch et al., 2023),
<code class="reqn">y</code> is sampled independently from <code class="reqn">x</code> from a truncated Gaussian distribution with a location parameter
of <code class="reqn">S\times d_k \times m/2</code> and a scale parameter
of 1. If <code class="reqn">R = -1</code>, the distribution is truncated to the right of <code class="reqn">m\times c</code>.
If <code class="reqn">R = 1</code>, the distribution is truncated to the left of  <code class="reqn">m\times c</code>.
The additional parameter <code class="reqn">m</code> represents metacognitive efficiency, i.e.,
the amount of information available for confidence judgments relative to
amount of evidence available for the discrimination decision and  can be smaller
as well as greater than 1.
</p>



<h5><strong>Logistic noise model (logN)</strong></h5>

<p>According to logN, the same sample
of sensory evidence is used to generate response and confidence, i.e.,
<code class="reqn">y=x</code> just as in SDT (Shekhar &amp; Rahnev, 2021). However, according to logN, the confidence criteria
are not assumed to be constant, but instead they are affected by noise drawn from
a lognormal distribution. In each trial, <code class="reqn">\theta_{-1,i}</code> is given
by <code class="reqn">c -  \epsilon_i</code>. Likewise,  <code class="reqn">\theta_{1,i}</code> is given by
<code class="reqn">c + \epsilon_i</code>. <code class="reqn">\epsilon_i</code> is drawn from a lognormal distribution with
the location parameter
<code class="reqn">\mu_{R,i}=log(|\overline{\theta}_{R,i}- c|) - 0.5 \times \sigma^{2}</code> and
scale parameter <code class="reqn">\sigma</code>. <code class="reqn">\sigma</code> is a free parameter designed to
quantify metacognitive ability. It is assumed that the criterion noise is perfectly
correlated across confidence criteria, ensuring that the confidence criteria
are always perfectly ordered. Because <code class="reqn">\theta_{-1,1}</code>, ..., <code class="reqn">\theta_{-1,L-1}</code>,
<code class="reqn">\theta_{1,1}</code>, ..., <code class="reqn">\theta_{1,L-1}</code> change from trial to trial, they are not estimated
as free parameters. Instead, we estimate the means of the confidence criteria, i.e., <code class="reqn">\overline{\theta}_{-1,1}, ...,
\overline{\theta}_{-1,L-1}, \overline{\theta}_{1,1}, ...  \overline{\theta}_{1,L-1}</code>,
as free parameters.
</p>



<h5><strong>Logistic weighted evidence and visibility model (logWEV)</strong></h5>

<p>logWEV is a combination of logN and WEV proposed by Shekhar and Rahnev (2023).
Conceptually, logWEV assumes that the observer combines evidence about decision-relevant features
of the stimulus with the strength of evidence about choice-irrelevant features (Rausch et al., 2018).
The model also assumes that noise affecting the confidence decision variable is lognormal
in accordance with Shekhar and Rahnev (2021).
According to logWEV, the confidence decision variable <code class="reqn">y</code> is equal to
<code class="reqn">y^*\times R</code>. <code class="reqn">y^*</code> is sampled from a lognormal distribution with a location parameter
of <code class="reqn">(1-w)\times x\times R + w \times d_k</code> and a scale parameter of <code class="reqn">\sigma</code>.
The parameter <code class="reqn">\sigma</code> quantifies the amount of unsystematic variability
contributing to confidence judgments but not to the discrimination judgments.
The parameter <code class="reqn">w</code> represents the weight that is put on the choice-irrelevant
features in the confidence judgment. <code class="reqn">w</code> and <code class="reqn">\sigma</code> are fitted in
addition to the set of shared parameters.
</p>




<h3>Value</h3>

<p>Gives data frame with one row and one column for each of the fitted parameters of the
selected model as well as additional information about the fit
(<code>negLogLik</code> (negative log-likelihood of the final set of parameters),
<code>k</code> (number of parameters), <code>N</code> (number of data rows),
<code>AIC</code> (Akaike Information Criterion; Akaike, 1974),
<code>BIC</code> (Bayes information criterion; Schwarz, 1978), and
<code>AICc</code> (AIC corrected for small samples; Burnham &amp; Anderson, 2002))
</p>


<h3>Author(s)</h3>

<p>Sebastian Hellmann, <a href="mailto:sebastian.hellmann@tum.de">sebastian.hellmann@tum.de</a><br />
Manuel Rausch, <a href="mailto:manuel.rausch@hochschule-rhein-waal.de">manuel.rausch@hochschule-rhein-waal.de</a>
</p>


<h3>References</h3>

<p>Akaike, H. (1974). A New Look at the Statistical Model Identification. IEEE Transactions on Automatic Control, AC-19(6), 716â€“723.doi: 10.1007/978-1-4612-1694-0_16<br />
</p>
<p>Burnham, K. P., &amp; Anderson, D. R. (2002). Model selection and multimodel inference: A practical information-theoretic approach. Springer.<br />
</p>
<p>Fleming, S. M. (2017). HMeta-d: Hierarchical Bayesian estimation of metacognitive efficiency from confidence ratings. Neuroscience of Consciousness, 1, 1â€“14. doi: 10.1093/nc/nix007<br />
</p>
<p>Green, D. M., &amp; Swets, J. A. (1966). Signal detection theory and psychophysics. Wiley.<br />
</p>
<p>Maniscalco, B., &amp; Lau, H. (2012). A signal detection theoretic method for estimating metacognitive sensitivity from confidence ratings. Consciousness and Cognition, 21(1), 422â€“430.<br />
</p>
<p>Maniscalco, B., &amp; Lau, H. C. (2014). Signal Detection Theory Analysis of Type 1 and Type 2 Data: Meta-dâ€™, Response- Specific Meta-dâ€™, and the Unequal Variance SDT Model. In S. M. Fleming &amp; C. D. Frith (Eds.), The Cognitive Neuroscience of Metacognition (pp. 25â€“66). Springer. doi: 10.1007/978-3-642-45190-4_3<br />
</p>
<p>Maniscalco, B., &amp; Lau, H. (2016). The signal processing architecture underlying subjective reports of sensory awareness. Neuroscience of Consciousness, 1, 1â€“17. doi: 10.1093/nc/niw002<br />
</p>
<p>Rausch, M., Hellmann, S., &amp; Zehetleitner, M. (2018). Confidence in masked orientation judgments is informed by both evidence and visibility. Attention, Perception, and Psychophysics, 80(1), 134â€“154. doi: 10.3758/s13414-017-1431-5<br />
</p>
<p>Rausch, M., Hellmann, S., &amp; Zehetleitner, M. (2023). Measures of metacognitive efficiency across cognitive models of decision confidence. Psychological Methods. doi: 10.31234/osf.io/kdz34<br />
</p>
<p>Rausch, M., &amp; Zehetleitner, M. (2017). Should metacognition be measured by logistic regression? Consciousness and Cognition, 49, 291â€“312. doi: 10.1016/j.concog.2017.02.007<br />
</p>
<p>Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6(2), 461â€“464. doi: 10.1214/aos/1176344136<br />
</p>
<p>Shekhar, M., &amp; Rahnev, D. (2021). The Nature of Metacognitive Inefficiency in Perceptual Decision Making. Psychological Review, 128(1), 45â€“70. doi: 10.1037/rev0000249<br />
</p>
<p>Shekhar, M., &amp; Rahnev, D. (2023). How Do Humans Give Confidence? A Comprehensive Comparison of Process Models of Perceptual Metacognition. Journal of Experimental Psychology: General. doi:10.1037/xge0001524<br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 1. Select one subject from the masked orientation discrimination experiment
data &lt;- subset(MaskOri, participant == 1)
head(data)

# 2. Use fitting function

  # Fitting takes some time (about 10 minutes on an 2.8GHz processor) to run:
  FitFirstSbjWEV &lt;- fitConf(data, model="WEV")

</code></pre>

<hr>
<h2 id='fitConfModels'>Fit several static confidence models to multiple participants</h2><span id='topic+fitConfModels'></span>

<h3>Description</h3>

<p>The <code>fitConfModels</code> function fits the parameters of several computational models of decision
confidence, in binary choice tasks,  specified in the <code>model</code> argument, to
different subsets of one data frame, indicated by different values in the column
<code>participant</code> of the <code>data</code> argument.
<code>fitConfModels</code> is a wrapper of the function <code><a href="#topic+fitConf">fitConf</a></code> and calls
<code><a href="#topic+fitConf">fitConf</a></code> for every possible combination
of model in the <code>models</code> argument and sub-data frame of <code>data</code> for each value
in the <code>participant</code> column.
See Details for more information about the parameters.
Parameters are fitted using a maximum likelihood estimation method with a
initial grid search to find promising starting values for the optimization.
In addition, several measures of model fit (negative log-likelihood, BIC, AIC, and AICc)
are computed, which can be used for a quantitative model evaluation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitConfModels(data, models = "all", nInits = 5, nRestart = 4,
  .parallel = FALSE, n.cores = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fitConfModels_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> where each row is one trial, containing following
variables:
</p>

<ul>
<li> <p><code>diffCond</code> (optional; different levels of discriminability,
should be a factor with levels ordered from hardest to easiest),
</p>
</li>
<li> <p><code>rating</code> (discrete confidence judgments, should be a factor with levels
ordered from lowest confidence to highest confidence;
otherwise will be transformed to factor with a warning),
</p>
</li>
<li> <p><code>stimulus</code> (stimulus category in a binary choice task,
should be a factor with two levels, otherwise it will be transformed to
a factor with a warning),
</p>
</li>
<li> <p><code>correct</code> (encoding whether the response was correct; should  be 0 for
incorrect responses and 1 for correct responses)
</p>
</li>
<li> <p><code>participant</code> (some group ID, most often a participant identifier;
the models given in the second argument are fitted to each subset of <code>data</code>
determined by the different values of this column)
</p>
</li></ul>
</td></tr>
<tr><td><code id="fitConfModels_+3A_models">models</code></td>
<td>
<p><code>character</code>. The different computational models that should be
fitted. Models implemented so far: 'WEV', 'SDT', 'GN', 'PDA', 'IG', 'ITGc',
'ITGcm', 'logN', and 'logWEV'. Alternatively, if <code>model="all"</code> (default),
all implemented models will be fit.</p>
</td></tr>
<tr><td><code id="fitConfModels_+3A_ninits">nInits</code></td>
<td>
<p><code>integer</code>. Number of initial values used for maximum likelihood optimization.
Defaults to 5.</p>
</td></tr>
<tr><td><code id="fitConfModels_+3A_nrestart">nRestart</code></td>
<td>
<p><code>integer</code>. Number of times the optimization is restarted.
Defaults to 4.</p>
</td></tr>
<tr><td><code id="fitConfModels_+3A_.parallel">.parallel</code></td>
<td>
<p><code>logical</code>. Whether to parallelize the fitting over models and participant
(default: FALSE)</p>
</td></tr>
<tr><td><code id="fitConfModels_+3A_n.cores">n.cores</code></td>
<td>
<p><code>integer</code>. Number of cores used for parallelization. If NULL (default), the available
number of cores -1 will be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The provided <code>data</code> argument is split into subsets according to the values of
the <code>participant</code> column. Then for each subset and each model in the <code>models</code>
argument, the parameters of the respective model are fitted to the data subset.
</p>
<p>The fitting routine first performs a coarse grid search to find promising
starting values for the maximum likelihood optimization procedure. Then the best <code>nInits</code>
parameter sets found by the grid search are used as the initial values for separate
runs of the Nelder-Mead algorithm implemented in <code><a href="stats.html#topic+optim">optim</a></code>.
Each run is restarted <code>nRestart</code> times.
</p>


<h4>Mathematical description of models</h4>

<p>The computational models are all based on signal detection theory (Green &amp; Swets, 1966). It is assumed
that participants select a binary discrimination response <code class="reqn">R</code> about a stimulus <code class="reqn">S</code>.
Both <code class="reqn">S</code> and <code class="reqn">R</code> can be either -1 or 1.
<code class="reqn">R</code> is considered correct if <code class="reqn">S=R</code>.
In addition, we assume that there are <code class="reqn">K</code> different levels of stimulus discriminability
in the experiment, i.e. a physical variable that makes the discrimination task easier or harder.
For each level of discriminability, the function fits a different discrimination
sensitivity parameter <code class="reqn">d_k</code>. If there is more than one sensitivity parameter,
we assume that the sensitivity parameters are ordered such as <code class="reqn">0 &lt; d_1 &lt; d_2 &lt; ... &lt; d_K</code>.
The models assume that the stimulus generates normally distributed sensory evidence <code class="reqn">x</code> with mean <code class="reqn">S\times d_k/2</code>
and variance of 1. The sensory evidence <code class="reqn">x</code> is compared to a decision
criterion <code class="reqn">c</code> to generate a discrimination response
<code class="reqn">R</code>, which is 1, if <code class="reqn">x</code> exceeds <code class="reqn">c</code> and -1 else.
To generate confidence, it is assumed that the confidence variable <code class="reqn">y</code> is compared to another
set of criteria <code class="reqn">\theta_{R,i}, i=1,2,...,L-1</code>, depending on the
discrimination response <code class="reqn">R</code> to produce a <code class="reqn">L</code>-step discrete confidence response.
The number of thresholds will be inferred from the number of steps in the
<code>rating</code> column of <code>data</code>.
Thus, the parameters shared between all models are:
</p>

<ul>
<li><p> sensitivity parameters <code class="reqn">d_1</code>,...,<code class="reqn">d_K</code> (<code class="reqn">K</code>: number of difficulty levels)
</p>
</li>
<li><p> decision criterion <code class="reqn">c</code>
</p>
</li>
<li><p> confidence criterion <code class="reqn">\theta_{-1,1}</code>,<code class="reqn">\theta_{-1,2}</code>,
..., <code class="reqn">\theta_{-1,L-1}</code>, <code class="reqn">\theta_{1,1}</code>,  <code class="reqn">\theta_{1,2}</code>,...,
<code class="reqn">\theta_{1,L-1}</code> (<code class="reqn">L</code>: number of confidence categories available for confidence ratings)
</p>
</li></ul>

<p>How the confidence variable <code class="reqn">y</code> is computed varies across the different models.
The following models have been implemented so far:
</p>


<h5><strong>Signal detection rating model (SDT)</strong></h5>

<p>According to SDT, the same sample of sensory
evidence is used to generate response and confidence, i.e.,
<code class="reqn">y=x</code> and the confidence criteria span from the left and
right side of the decision criterion <code class="reqn">c</code>(Green &amp; Swets, 1966).
</p>



<h5><strong>Gaussian noise model (GN)</strong></h5>

<p>According to the model, <code class="reqn">y</code> is subject to
additive noise and assumed to be normally distributed around the decision
evidence value <code class="reqn">x</code> with a standard deviation <code class="reqn">\sigma</code>(Maniscalco &amp; Lau, 2016).
<code class="reqn">\sigma</code> is an additional free parameter.
</p>



<h5><strong>Weighted evidence and visibility model (WEV)</strong></h5>

<p>WEV assumes that the observer combines evidence about decision-relevant features
of the stimulus with the strength of evidence about choice-irrelevant features
to generate confidence (Rausch et al., 2018). Thus, the WEV model assumes that <code class="reqn">y</code> is normally
distributed with a mean of <code class="reqn">(1-w)\times x+w \times d_k\times R</code> and standard deviation <code class="reqn">\sigma</code>.
The standard deviation quantifies the amount of unsystematic variability
contributing to confidence judgments but not to the discrimination judgments.
The parameter <code class="reqn">w</code> represents the weight that is put on the choice-irrelevant
features in the confidence judgment. <code class="reqn">w</code> and <code class="reqn">\sigma</code> are fitted in
addition to the set of shared parameters.
</p>



<h5><strong>Post-decisional accumulation model (PDA)</strong></h5>

<p>PDA represents the idea of on-going information accumulation after the
discrimination choice (Rausch et al., 2018). The parameter <code class="reqn">a</code> indicates the amount of additional
accumulation. The confidence variable is normally distributed with mean
<code class="reqn">x+S\times d_k\times a</code> and variance <code class="reqn">a</code>.
For this model the parameter <code class="reqn">a</code> is fitted in addition to the shared
parameters.
</p>



<h5><strong>Independent Gaussian model (IG)</strong></h5>

<p>According to IG, <code class="reqn">y</code> is sampled independently
from <code class="reqn">x</code> (Rausch &amp; Zehetleitner, 2017). <code class="reqn">y</code> is normally distributed with a mean of <code class="reqn">a\times d_k</code> and variance
of 1 (again as it would scale with <code class="reqn">m</code>). The additional parameter <code class="reqn">m</code>
represents the amount of information available for confidence judgment
relative to amount of evidence available for the discrimination decision and can
be smaller as well as greater than 1.
</p>



<h5><strong>Independent truncated Gaussian model: HMetad-Version (ITGc)</strong></h5>

<p>According to the version of ITG consistent
with the HMetad-method (Fleming, 2017; see Rausch et al., 2023), <code class="reqn">y</code> is sampled independently
from <code class="reqn">x</code> from a truncated Gaussian distribution with a location parameter
of <code class="reqn">S\times d_k \times m/2</code> and a scale parameter of 1. The Gaussian distribution of <code class="reqn">y</code>
is truncated in a way that it is impossible to sample evidence that contradicts
the original decision: If <code class="reqn">R = -1</code>, the distribution is truncated to the
right of <code class="reqn">c</code>. If <code class="reqn">R = 1</code>, the distribution is truncated to the left
of <code class="reqn">c</code>. The additional parameter <code class="reqn">m</code> represents metacognitive efficiency,
i.e., the amount of information available for confidence judgments relative to
amount of evidence available for discrimination decisions and  can be smaller
as well as greater than 1.
</p>



<h5><strong>Independent truncated Gaussian model: Meta-d'-Version (ITGcm)</strong></h5>

<p>According to the version of the ITG consistent
with the original meta-d' method (Maniscalco &amp; Lau, 2012, 2014; see Rausch et al., 2023),
<code class="reqn">y</code> is sampled independently from <code class="reqn">x</code> from a truncated Gaussian distribution with a location parameter
of <code class="reqn">S\times d_k \times m/2</code> and a scale parameter
of 1. If <code class="reqn">R = -1</code>, the distribution is truncated to the right of <code class="reqn">m\times c</code>.
If <code class="reqn">R = 1</code>, the distribution is truncated to the left of  <code class="reqn">m\times c</code>.
The additional parameter <code class="reqn">m</code> represents metacognitive efficiency, i.e.,
the amount of information available for confidence judgments relative to
amount of evidence available for the discrimination decision and  can be smaller
as well as greater than 1.
</p>



<h5><strong>Logistic noise model (logN)</strong></h5>

<p>According to logN, the same sample
of sensory evidence is used to generate response and confidence, i.e.,
<code class="reqn">y=x</code> just as in SDT (Shekhar &amp; Rahnev, 2021). However, according to logN, the confidence criteria
are not assumed to be constant, but instead they are affected by noise drawn from
a lognormal distribution. In each trial, <code class="reqn">\theta_{-1,i}</code> is given
by <code class="reqn">c -  \epsilon_i</code>. Likewise,  <code class="reqn">\theta_{1,i}</code> is given by
<code class="reqn">c + \epsilon_i</code>. <code class="reqn">\epsilon_i</code> is drawn from a lognormal distribution with
the location parameter
<code class="reqn">\mu_{R,i}=log(|\overline{\theta}_{R,i}- c|) - 0.5 \times \sigma^{2}</code> and
scale parameter <code class="reqn">\sigma</code>. <code class="reqn">\sigma</code> is a free parameter designed to
quantify metacognitive ability. It is assumed that the criterion noise is perfectly
correlated across confidence criteria, ensuring that the confidence criteria
are always perfectly ordered. Because <code class="reqn">\theta_{-1,1}</code>, ..., <code class="reqn">\theta_{-1,L-1}</code>,
<code class="reqn">\theta_{1,1}</code>, ..., <code class="reqn">\theta_{1,L-1}</code> change from trial to trial, they are not estimated
as free parameters. Instead, we estimate the means of the confidence criteria, i.e., <code class="reqn">\overline{\theta}_{-1,1}, ...,
\overline{\theta}_{-1,L-1}, \overline{\theta}_{1,1}, ...  \overline{\theta}_{1,L-1}</code>,
as free parameters.
</p>



<h5><strong>Logistic weighted evidence and visibility model (logWEV)</strong></h5>

<p>logWEV is a combination of logN and WEV proposed by Shekhar and Rahnev (2023).
Conceptually, logWEV assumes that the observer combines evidence about decision-relevant features
of the stimulus with the strength of evidence about choice-irrelevant features (Rausch et al., 2018).
The model also assumes that noise affecting the confidence decision variable is lognormal
in accordance with Shekhar and Rahnev (2021).
According to logWEV, the confidence decision variable is <code class="reqn">y</code> is equal to
<code class="reqn">y^*\times R</code>. <code class="reqn">y^*</code> is sampled from a lognormal distribution with a location parameter
of <code class="reqn">(1-w)\times x\times R + w \times d_k</code> and a scale parameter of <code class="reqn">\sigma</code>.
The parameter <code class="reqn">\sigma</code> quantifies the amount of unsystematic variability
contributing to confidence judgments but not to the discrimination judgments.
The parameter <code class="reqn">w</code> represents the weight that is put on the choice-irrelevant
features in the confidence judgment. <code class="reqn">w</code> and <code class="reqn">\sigma</code> are fitted in
addition to the set of shared parameters.
</p>




<h3>Value</h3>

<p>Gives <code>data.frame</code> with one row for each combination of model and
participant. There are different columns for the model, the participant ID, and one
one column for each estimated model parameter (parameters
not present in a specific model are filled with NAs).
Additional information  about the fit is provided in additional columns:
</p>

<ul>
<li> <p><code>negLogLik</code> (negative log-likelihood of the best-fitting set of parameters),
</p>
</li>
<li> <p><code>k</code> (number of parameters),
</p>
</li>
<li> <p><code>N</code> (number of trials),
</p>
</li>
<li> <p><code>AIC</code> (Akaike Information Criterion; Akaike, 1974),
</p>
</li>
<li> <p><code>BIC</code> (Bayes information criterion; Schwarz, 1978),
</p>
</li>
<li> <p><code>AICc</code> (AIC corrected for small samples; Burnham &amp; Anderson, 2002)
If length(models) &gt; 1 or models == &quot;all&quot;, there will be three additional columns:
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Sebastian Hellmann, <a href="mailto:sebastian.hellmann@tum.de">sebastian.hellmann@tum.de</a><br />
Manuel Rausch, <a href="mailto:manuel.rausch@hochschule-rhein-waal.de">manuel.rausch@hochschule-rhein-waal.de</a>
</p>


<h3>References</h3>

<p>Akaike, H. (1974). A New Look at the Statistical Model Identification. IEEE Transactions on Automatic Control, AC-19(6), 716â€“723.doi: 10.1007/978-1-4612-1694-0_16<br />
</p>
<p>Burnham, K. P., &amp; Anderson, D. R. (2002). Model selection and multimodel inference: A practical information-theoretic approach. Springer.<br />
</p>
<p>Fleming, S. M. (2017). HMeta-d: Hierarchical Bayesian estimation of metacognitive efficiency from confidence ratings. Neuroscience of Consciousness, 1, 1â€“14. doi: 10.1093/nc/nix007<br />
</p>
<p>Green, D. M., &amp; Swets, J. A. (1966). Signal detection theory and psychophysics. Wiley.<br />
</p>
<p>Maniscalco, B., &amp; Lau, H. (2012). A signal detection theoretic method for estimating metacognitive sensitivity from confidence ratings. Consciousness and Cognition, 21(1), 422â€“430.<br />
</p>
<p>Maniscalco, B., &amp; Lau, H. C. (2014). Signal Detection Theory Analysis of Type 1 and Type 2 Data: Meta-dâ€™, Response- Specific Meta-dâ€™, and the Unequal Variance SDT Model. In S. M. Fleming &amp; C. D. Frith (Eds.), The Cognitive Neuroscience of Metacognition (pp. 25â€“66). Springer. doi: 10.1007/978-3-642-45190-4_3<br />
</p>
<p>Maniscalco, B., &amp; Lau, H. (2016). The signal processing architecture underlying subjective reports of sensory awareness. Neuroscience of Consciousness, 1, 1â€“17. doi: 10.1093/nc/niw002<br />
</p>
<p>Rausch, M., Hellmann, S., &amp; Zehetleitner, M. (2018). Confidence in masked orientation judgments is informed by both evidence and visibility. Attention, Perception, and Psychophysics, 80(1), 134â€“154. doi: 10.3758/s13414-017-1431-5<br />
</p>
<p>Rausch, M., Hellmann, S., &amp; Zehetleitner, M. (2023). Measures of metacognitive efficiency across cognitive models of decision confidence. Psychological Methods. doi: 10.31234/osf.io/kdz34<br />
</p>
<p>Rausch, M., &amp; Zehetleitner, M. (2017). Should metacognition be measured by logistic regression? Consciousness and Cognition, 49, 291â€“312. doi: 10.1016/j.concog.2017.02.007<br />
</p>
<p>Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6(2), 461â€“464. doi: 10.1214/aos/1176344136<br />
</p>
<p>Shekhar, M., &amp; Rahnev, D. (2021). The Nature of Metacognitive Inefficiency in Perceptual Decision Making. Psychological Review, 128(1), 45â€“70. doi: 10.1037/rev0000249<br />
</p>
<p>Shekhar, M., &amp; Rahnev, D. (2023). How Do Humans Give Confidence? A Comprehensive Comparison of Process Models of Perceptual Metacognition. Journal of Experimental Psychology: General. doi:10.1037/xge0001524<br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 1. Select two subjects from the masked orientation discrimination experiment
data &lt;- subset(MaskOri, participant %in% c(1:2))
head(data)

# 2. Fit some models to each subject of the masked orientation discrimination experiment

  # Fitting several models to several subjects takes quite some time
  # (about 10 minutes per model fit per participant on a 2.8GHz processor
  # with the default values of nInits and nRestart).
  # If you want to fit more than just two subjects,
  # we strongly recommend setting .parallel=TRUE
  Fits &lt;- fitConfModels(data, models = c("SDT", "ITGc"), .parallel = FALSE)

</code></pre>

<hr>
<h2 id='fitMetaDprime'>title Compute measures of metacognitive sensitivity (meta-d') and metacognitive efficiency(meta-d'/d') for data from one or several subjects</h2><span id='topic+fitMetaDprime'></span>

<h3>Description</h3>

<p>This function computes the measures for metacognitive sensitivity, meta-d',
and metacognitive efficiency, meta-d'/d' (Maniscalco and Lau, 2012, 2014;
Fleming, 2017) to data from binary choice tasks with discrete confidence
judgments. Meta-d' and meta-d'/d' are computed using a maximum likelihood
method for each subset of the <code>data</code> argument indicated by different values
in the column <code>participant</code>, which can represent different subjects as well
as experimental conditions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitMetaDprime(data, model = "ML", nInits = 5, nRestart = 3,
  .parallel = FALSE, n.cores = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fitMetaDprime_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> where each row is one trial, containing following
variables:
</p>

<ul>
<li> <p><code>rating</code> (discrete confidence judgments, should be given as factor;
otherwise will be transformed to factor with a warning),
</p>
</li>
<li> <p><code>stimulus</code> (stimulus category in a binary choice task,
should be a factor with two levels, otherwise it will be transformed to
a factor with a warning),
</p>
</li>
<li> <p><code>correct</code> (encoding whether the response was correct; should  be 0 for incorrect responses and 1 for correct responses)
</p>
</li>
<li> <p><code>participant</code> (giving the subject ID; the models given in the second argument are fitted for each
subject individually.
</p>
</li></ul>
</td></tr>
<tr><td><code id="fitMetaDprime_+3A_model">model</code></td>
<td>
<p><code>character</code> of length 1. Either &quot;ML&quot; to use the original model
specification by Maniscalco and Lau (2012,  2014) or &quot;F&quot; to use the model
specification by Fleming (2017)'s HmetaD method.  Defaults to &quot;ML&quot;</p>
</td></tr>
<tr><td><code id="fitMetaDprime_+3A_ninits">nInits</code></td>
<td>
<p><code>integer</code>. Number of initial values used for maximum likelihood optimization.
Defaults to 5.</p>
</td></tr>
<tr><td><code id="fitMetaDprime_+3A_nrestart">nRestart</code></td>
<td>
<p><code>integer</code>. Number of times the optimization is restarted.
Defaults to 3.</p>
</td></tr>
<tr><td><code id="fitMetaDprime_+3A_.parallel">.parallel</code></td>
<td>
<p><code>logical</code>. Whether to parallelize the fitting over models and participant
(default: FALSE)</p>
</td></tr>
<tr><td><code id="fitMetaDprime_+3A_n.cores">n.cores</code></td>
<td>
<p><code>integer</code>. Number of cores used for parallelization. If NULL (default), the available
number of cores -1 will be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes meta-d' and meta-d'/d' either using the
hypothetical signal detection model assumed by Maniscalco and Lau (2012, 2014)
or the one assumed by Fleming (2014).
</p>
<p>The conceptual idea of meta-d' is to quantify metacognition in terms of sensitivity
in a hypothetical signal detection rating model describing the primary task,
under the assumption that participants had perfect access to the sensory evidence
and were perfectly consistent in placing their confidence criteria (Maniscalco &amp; Lau, 2012, 2014).
Using a signal detection model describing the primary task to quantify metacognition allows
a direct comparison between metacognitive accuracy and discrimination performance
because both are measured on the same scale. Meta-d' can be compared against
the estimate of the distance between the two stimulus distributions
estimated from discrimination responses, which is referred to as d':
If meta-d' equals d', it means that metacognitive accuracy is exactly
as good as expected from discrimination performance.
Ifmeta-d' is lower than d', it means that metacognitive accuracy is suboptimal.
It can be shown that the implicit model of confidence underlying the meta-d'/d'
method is identical to the independent truncated Gaussian model.
</p>
<p>The provided <code>data</code> argument is split into subsets according to the values of
the <code>participant</code> column. Then for each subset, the parameters of the
hypothetical signal detection model determined by the <code>model</code> argument
are fitted to the data subset.
</p>
<p>The fitting routine first performs a coarse grid search to find promising
starting values for the maximum likelihood optimization procedure. Then the best <code>nInits</code>
parameter sets found by the grid search are used as the initial values for separate
runs of the Nelder-Mead algorithm implemented in <code><a href="stats.html#topic+optim">optim</a></code>.
Each run is restarted <code>nRestart</code> times. Warning: meta-d'/d'
is only guaranteed to be unbiased from discrimination sensitivity, discrimination
bias, and confidence criteria if the data is generated according to the
independent truncated Gaussian model (see Rausch et al., 2023).
</p>


<h3>Value</h3>

<p>Gives data frame with one row for each participant and following columns:
</p>

<ul>
<li> <p><code>model</code> gives the model used for the computation of meta-d' (see <code>model</code> argument)
</p>
</li>
<li> <p><code>participant</code> is the participant ID for the respecitve row
</p>
</li>
<li> <p><code>dprime</code> is the discrimination sensitivity index d, calculated using a standard SDT formula
</p>
</li>
<li> <p><code>c</code> is the discrimination bias c, calculated using a standard SDT formula
</p>
</li>
<li> <p><code>metaD</code> is meta-d', discrimination sensitivity estimated from confidence judgments conditioned on the response
</p>
</li>
<li> <p><code>Ratio</code> is meta-d'/d', a quantity usually referred to as metacognitive efficiency.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Manuel Rausch, <a href="mailto:manuel.rausch@hochschule-rhein-waal.de">manuel.rausch@hochschule-rhein-waal.de</a>
</p>


<h3>References</h3>

<p>Fleming, S. M. (2017). HMeta-d: Hierarchical Bayesian estimation of metacognitive efficiency from confidence ratings. Neuroscience of Consciousness, 1, 1â€“14. doi: 10.1093/nc/nix007
</p>
<p>Maniscalco, B., &amp; Lau, H. (2012). A signal detection theoretic method for estimating metacognitive sensitivity from confidence ratings. Consciousness and Cognition, 21(1), 422â€“430.
</p>
<p>Maniscalco, B., &amp; Lau, H. C. (2014). Signal Detection Theory Analysis of Type 1 and Type 2 Data: Meta-dâ€™, Response- Specific Meta-dâ€™, and the Unequal Variance SDT Model. In S. M. Fleming &amp; C. D. Frith (Eds.), The Cognitive Neuroscience of Metacognition (pp. 25â€“66). Springer. doi: 10.1007/978-3-642-45190-4_3
</p>
<p>Rausch, M., Hellmann, S., &amp; Zehetleitner, M. (2023). Measures of metacognitive efficiency across cognitive models of decision confidence. Psychological Methods. doi: 10.31234/osf.io/kdz34
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 1. Select two subject from the masked orientation discrimination experiment
data &lt;- subset(MaskOri, participant %in% c(1:2))
head(data)

# 2. Fit meta-d/d for each subject in data
MetaDs &lt;- fitMetaDprime(data, model="F", .parallel = FALSE)
</code></pre>

<hr>
<h2 id='MaskOri'>Data of 16 participants in a masked orientation discrimination experiment (Hellmann et al., 2023, Exp. 1)</h2><span id='topic+MaskOri'></span>

<h3>Description</h3>

<p>In each trial, participants were shown a sinusoidal grating oriented either horizontally or vertically,
followed by a mask after varying stimulus-onset-asynchronies.
Participants were instructed to report the orientation and their degree of confidence as accurately as possible
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(MaskOri)
</code></pre>


<h3>Format</h3>

<p>A data.frame with 25920 rows representing different trials and 5 variables:
</p>

<dl>
<dt>participant</dt><dd><p>integer values as unique participant identifier</p>
</dd>
<dt>stimulus</dt><dd><p>orientation of the grating (90: vertical, 0: horizontal)</p>
</dd>
<dt>response </dt><dd><p> participants' orientation judgment about the grating (90: vertical, 0: horizontal)</p>
</dd>
<dt>correct</dt><dd><p>0-1 column indicating whether the discrimination response was correct (1) or not (0)</p>
</dd>
<dt>rating</dt><dd><p>0-4 confidence rating on a continous scale binned into five categories</p>
</dd>
<dt>diffCond</dt><dd><p>stimulus-onset-asynchrony in ms (i.e. time between stimulus and mask onset)</p>
</dd>
<dt>trialNo</dt><dd><p>Enumeration of trials per participant</p>
</dd>
</dl>



<h3>References</h3>

<p>Hellmann, S., Zehetleitner, M., &amp; Rausch, M. (2023). Simultaneous modeling of choice, confidence, and response time in visual perception. Psychological Review. 130(6), 1521â€“1543. doi:10.1037/rev0000411
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MaskOri)
summary(MaskOri)
</code></pre>

<hr>
<h2 id='plotConfModelFit'>Plot the prediction of fitted parameters of one model of confidence over the corresponding  data</h2><span id='topic+plotConfModelFit'></span>

<h3>Description</h3>

<p>The <code>plotConfModelFit</code> function plots the predicted distribution of discrimination responses
and confidence ratings created from a <code>data.frame</code> of parameters obtaind from <code><a href="#topic+fitConfModels">fitConfModels</a></code>
and overlays the predicted distribution over the data to which the model parameters were fitted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotConfModelFit(data, fitted_pars, model = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotConfModelFit_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> where each row is one trial, containing following
variables:
</p>

<ul>
<li> <p><code>diffCond</code> (optional; different levels of discriminability,
should be a factor with levels ordered from hardest to easiest),
</p>
</li>
<li> <p><code>rating</code> (discrete confidence judgments, should be a factor with levels
ordered from lowest confidence to highest confidence;
otherwise will be transformed to factor with a warning),
</p>
</li>
<li> <p><code>stimulus</code> (stimulus category in a binary choice task,
should be a factor with two levels, otherwise it will be transformed to
a factor with a warning),
</p>
</li>
<li> <p><code>correct</code> (encoding whether the response was correct; should  be 0 for
incorrect responses and 1 for correct responses)
</p>
</li>
<li> <p><code>participant</code> (some group ID, most often a participant identifier;
the models given in the second argument are fitted to each subset of <code>data</code>
determined by the different values of this column)
</p>
</li></ul>
</td></tr>
<tr><td><code id="plotConfModelFit_+3A_fitted_pars">fitted_pars</code></td>
<td>
<p>a <code>data.frame</code> with one row for each participant and model parameters in different columns.
fitted_pars also may contain a column called <code>model</code> specifying the model to be visualized.
If there is no model column in data or if there are multiple models in fitted_pars,
it is necessary to specify the model argument.</p>
</td></tr>
<tr><td><code id="plotConfModelFit_+3A_model">model</code></td>
<td>
<p><code>character</code>. See <code><a href="#topic+fitConfModels">fitConfModels</a></code> for all available models</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ggplot</code> object with empirically observed distribution of responses and confidence ratings
as bars on the x-axis as a function of discriminability (in the rows) and stimulus
(in the columns). Superimposed on the empirical data,
the plot also shows the prediction of one selected model as dots.
</p>


<h3>Author(s)</h3>

<p>Manuel Rausch, <a href="mailto:manuel.rausch@hochschule-rhein-waal.de">manuel.rausch@hochschule-rhein-waal.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 1. Fit some models to each subject of the masked orientation discrimination experiment
  # Normally, the fits should be created using the function fitConfModels
  # Fits &lt;- fitConfModels(data, models = "WEV", .parallel = TRUE)
  # Here, we create the dataframe manually because fitting models takes about
  # 10 minutes per model fit per participant on a 2.8GHz processor.
  pars &lt;- data.frame(participant = 1:16,
  d_1 = c(0.20, 0.05, 0.41, 0.03, 0.00, 0.01, 0.11, 0.03, 0.19, 0.08, 0.00,
  0.24, 0.00, 0.00, 0.25, 0.01),
  d_2 = c(0.61, 0.19, 0.86, 0.18, 0.17, 0.39, 0.69, 0.14, 0.45, 0.30, 0.00,
  0.27, 0.00, 0.05, 0.57, 0.23),
  d_3 = c(1.08, 1.04, 2.71, 2.27, 1.50, 1.21, 1.83, 0.80, 1.06, 0.68, 0.29,
  0.83, 0.77, 2.19, 1.93, 0.54),
  d_4 = c(3.47, 4.14, 6.92, 4.79, 3.72, 3.24, 4.55, 2.51, 3.78, 2.40, 1.95,
  2.55, 4.59, 4.27, 4.08, 1.80),
  d_5 = c(4.08, 5.29, 7.99, 5.31, 4.53, 4.66, 6.21, 4.67, 5.85, 3.39, 3.39,
  4.42, 6.48, 5.35, 5.28, 2.87),
  c = c(-0.30, -0.15, -1.37, 0.17, -0.12, -0.19, -0.12, 0.41, -0.27, 0.00,
  -0.19, -0.21, -0.91, -0.26, -0.20, 0.10),
  theta_minus.4 = c(-2.07, -2.04, -2.76, -2.32, -2.21, -2.33, -2.27, -2.29,
  -2.69, -3.80, -2.83, -1.74, -2.58, -3.09, -2.20, -1.57),
  theta_minus.3 = c(-1.25, -1.95, -1.92, -2.07, -1.62, -1.68, -2.04, -2.02,
  -1.84, -3.37, -1.89, -1.44, -2.31, -2.08, -1.53, -1.46),
  theta_minus.2 = c(-0.42, -1.40, -0.37, -1.96, -1.45, -1.27, -1.98, -1.66,
  -1.11, -2.69, -1.60, -1.25, -2.21, -1.68, -1.08, -1.17),
  theta_minus.1 = c(0.13, -0.90,  0.93, -1.71, -1.25, -0.59, -1.40, -1.00,
  -0.34, -1.65, -1.21, -0.76, -1.99, -0.92, -0.28, -0.99),
  theta_plus.1 = c(-0.62, 0.82, -2.77, 2.01, 1.39, 0.60, 1.51, 0.90, 0.18,
  1.62, 0.99,0.88, 1.67, 0.92, 0.18,  0.88),
  theta_plus.2 = c(0.15, 1.45, -1.13,2.17, 1.61, 1.24, 1.99, 1.55, 0.96, 2.44,
  1.53, 1.66, 2.00, 1.51, 1.08, 1.05),
  theta_plus.3 = c(1.40, 2.24, 0.77, 2.32, 1.80, 1.58, 2.19, 2.19, 1.54, 3.17,
  1.86, 1.85, 2.16, 2.09, 1.47, 1.70),
  theta_plus.4 = c(2.19, 2.40, 1.75, 2.58, 2.53, 2.24, 2.59, 2.55, 2.58, 3.85,
  2.87, 2.15, 2.51, 3.31, 2.27, 1.79),
  sigma = c(1.01, 0.64, 1.33, 0.39, 0.30, 0.75, 0.75, 1.07, 0.65, 0.29, 0.31,
  0.78, 0.39, 0.42, 0.69, 0.52),
  w = c(0.54, 0.50, 0.38, 0.38, 0.36, 0.44, 0.48, 0.48, 0.52, 0.46, 0.53, 0.48,
   0.29, 0.45, 0.51, 0.63))

# 2. Plot the predicted probabilities based on model and fitted parameters
  # against the observed relative frequencies.

  PlotFitWEV &lt;- plotConfModelFit(MaskOri, pars, model="WEV")
  PlotFitWEV

</code></pre>

<hr>
<h2 id='simConf'>Simulate data  according to a static model of confidence</h2><span id='topic+simConf'></span>

<h3>Description</h3>

<p>This function generates a data frame with random trials generated according to
the computational model of decision confidence specified in the <code>model</code> argument
with given parameters.
Simulations can be used to visualize and test qualitative model predictions
(e.g. using previously fitted parameters returned by <code><a href="#topic+fitConf">fitConf</a></code>).
See <code><a href="#topic+fitConf">fitConf</a></code> for a full mathematical description of all models
and their parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simConf(model = "SDT", paramDf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simConf_+3A_model">model</code></td>
<td>
<p><code>character</code> of length 1. The generative model that should be
used for simulation. Models implemented so far: 'WEV', 'SDT', 'GN', 'PDA',
'IG', 'ITGc', 'ITGcm', 'logN', and 'logWEV'.</p>
</td></tr>
<tr><td><code id="simConf_+3A_paramdf">paramDf</code></td>
<td>
<p>a <code>data.frame</code>providing the number of generared trials and
the parameters of the chosen model. <code>paramDf</code> should contain following columns
(which parameters are needed depends on the specific model):
</p>

<ul>
<li> <p><code>N</code> (the number of trials be simulated),
</p>
</li>
<li> <p><code>participant</code> (optional, the participant ID of each parameter set. Should be unique to each row),
</p>
</li>
<li> <p><code>d_1</code>, <code>d_2</code>, ... (sensitivity parameters. The number of sensitivity parameters determines the number of levels of discriminability),
</p>
</li>
<li> <p><code>c</code> (discrimination bias),
</p>
</li>
<li> <p><code>theta_minus.1</code>, <code>theta_minus.2</code>, ... (confidence criteria associated with the response R = -1. The function simulates one more confidence category than there are confidence criteria),
</p>
</li>
<li> <p><code>theta_plus.1</code>, <code>theta_plus.2</code>, ... (confidence criteria associated with the response R = 1. The function simulates one more confidence category than there are confidence criteria),
</p>
</li>
<li> <p><code>w</code> (only for models WEV and logWEV: the visibility weighting parameter, bounded between 0 and 1),
</p>
</li>
<li> <p><code>sigma</code> (only for models WEV, GN, logN, and logWEV: confidence noise, bounded between 0 and Inf),
</p>
</li>
<li> <p><code>m</code> (only for IG, ITGm, and ITGcm: metacognitive efficiency parameter, bounded between 0 and Inf),
</p>
</li>
<li> <p><code>b</code> (only for PDA: postdecisional accumulation parameter, bounded between 0 and Inf),
</p>
</li>
<li> <p><code>M_theta_minus.1</code>, <code>M_theta_minus.2</code>, ... (only for logN: Mean confidence criteria associated with the response R = -1),
</p>
</li>
<li> <p><code>M_theta_plus.1</code>, <code>M_theta_plus.2</code>,... (only for logN: Mean confidence criteria associated with the response R = 1).
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>The function generates about <code>N</code> trials per row with the provided parameters
in the data frame. The output includes a column <code>participant</code> indicating the
row ID of the simulated data. The values of the <code>participant</code> column may be
controlled by the user, by including a <code>participant</code> column in the input
<code>paramDf</code>. Note that the values of this column have to be unique! If no
<code>participant</code> column is present in the input, the row numbers will be used
as row IDs.
</p>
<p>The number of simulated trials for each row of parameters may slightly
deviate from the provided <code>N</code>.
Precisely, if there are K levels of sensitivity (i.e. there are columns
d1, d2, ..., dK), the function simulates <code>round(N/2/K)</code> trials per stimulus
identity (2 levels) and level of sensitivity (K levels).
</p>
<p>Simulation is performed following the generative process structure of the models.
See <code>fitConf</code> for a detailed description of the different models.
</p>


<h3>Value</h3>

<p>a dataframe with about <code>nrow(paramDf)*N</code> rows (see Details),
and the following columns:
</p>

<ul>
<li> <p><code>participant</code> giving the row ID of the simulation (see Details)
</p>
</li>
<li> <p><code>stimulus</code> giving the category of the stimulus (-1 or 1)
</p>
</li>
<li><p> only, if more than 1 sensitivity parameter (<code>d1</code>,<code>d2</code>,...) is provided:
<code>diffCond</code> representing the difficulty condition (values correspond to
the levels of the sensitivity parameters, i.e. diffCond=1 represents
simulated trials with sensitivity <code>d1</code>)
</p>
</li>
<li> <p><code>response</code> giving the response category (-1 or 1, corresponding to the stimulus categories)
</p>
</li>
<li> <p><code>rating</code> giving the discrete confidence rating (integer, number of
categories depends on the number of confidence criteria provided in the parameters)
</p>
</li>
<li> <p><code>correct</code> giving the accuracy of the response (0 incorrect, 1 correct)
</p>
</li>
<li> <p><code>ratings</code> same as <code>rating</code> but as a factor
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Manuel Rausch, <a href="mailto:manuel.rausch@hochschule-rhein-waal.de">manuel.rausch@hochschule-rhein-waal.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 1. define some parameters
paramDf &lt;- data.frame(d_1 = 0, d_2 = 2, d_3 = 4,c = .0,
theta_minus.2 = -2, theta_minus.1 = -1, theta_plus.1 = 1, theta_plus.2 = 2,
sigma = 1/2, w = 0.5, N = 500)
# 2. Simulate dataset
SimulatedData &lt;- simConf(model = "WEV", paramDf)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
