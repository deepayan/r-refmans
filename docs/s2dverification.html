<!DOCTYPE html><html><head><title>Help for package s2dverification</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {s2dverification}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#s2dverification'><p>Set of Common Tools for Forecast Verification</p></a></li>
<li><a href='#.LoadDataFile'>
<p>Load Data From File Into Environment</p></a></li>
<li><a href='#ACC'><p>Computes Anomaly Correlation Coefficient</p></a></li>
<li><a href='#Alpha'><p>Estimates AutoCorrelation At Lag 1 following Guemas et al, BAMS, 2013b</p></a></li>
<li><a href='#AnimateMap'><p>Animate Maps of Forecast/Observed Values or Scores Over Forecast Time</p></a></li>
<li><a href='#Ano'><p>Computes Forecast or Observed Anomalies</p></a></li>
<li><a href='#Ano_CrossValid'><p>Computes Anomalies In Cross-Validation Mode</p></a></li>
<li><a href='#ArrayToNetCDF'><p>Save multidimensional R arrays into NetCDF files</p></a></li>
<li><a href='#BrierScore'><p>Compute Brier Score And Its Decomposition And Brier Skill Score</p></a></li>
<li><a href='#CDORemap'><p>Interpolates arrays with longitude and latitude dimensions using CDO</p></a></li>
<li><a href='#Clim'><p>Computes Bias Corrected Climatologies</p></a></li>
<li><a href='#clim.palette'><p>Generate Climate Color Palettes</p></a></li>
<li><a href='#Cluster'><p>K-means Clustering</p></a></li>
<li><a href='#ColorBar'><p>Draws a Color Bar</p></a></li>
<li><a href='#Composite'><p>Computes composites</p></a></li>
<li><a href='#ConfigApplyMatchingEntries'><p>Apply Matching Entries To Dataset Name And Variable Name To Find Related Info</p></a></li>
<li><a href='#ConfigEditDefinition'><p>Add Modify Or Remove Variable Definitions In Configuration</p></a></li>
<li><a href='#ConfigEditEntry'><p>Add, Remove Or Edit Entries In The Configuration</p></a></li>
<li><a href='#ConfigFileOpen'><p>Functions To Create Open And Save Configuration File</p></a></li>
<li><a href='#ConfigShowSimilarEntries'><p>Find Similar Entries In Tables Of Datasets</p></a></li>
<li><a href='#ConfigShowTable'><p>Show Configuration Tables And Definitions</p></a></li>
<li><a href='#Consist_Trend'><p>Computes Trends Using Only Model Data For Which Observations Are Available</p></a></li>
<li><a href='#Corr'><p>Computes the correlation coefficient between an array of forecasts and their corresponding observations</p></a></li>
<li><a href='#Enlarge'><p>Extends The Number Of Dimensions of A Matrix</p></a></li>
<li><a href='#Eno'><p>Computes Effective Sample Size With Classical Method</p></a></li>
<li><a href='#EnoNew'><p>Computes Effective Sample Size Following Guemas et al, BAMS, 2013b</p></a></li>
<li><a href='#EOF'><p>Area-Weighted Empirical Orthogonal Function Analysis Using SVD</p></a></li>
<li><a href='#Filter'><p>Filter Frequency Peaks From An Array</p></a></li>
<li><a href='#FitAcfCoef'><p>Fits an AR1 AutoCorrelation Function Using the Cardano Formula</p></a></li>
<li><a href='#FitAutocor'><p>Fits an AR1 Autocorrelation Function Using Dichotomy</p></a></li>
<li><a href='#GenSeries'><p>Generates An AR1 Time Series</p></a></li>
<li><a href='#Histo2Hindcast'><p>Chunks Long Simulations For Comparison With Hindcasts</p></a></li>
<li><a href='#IniListDims'><p>Creates A List Of Integer Ranges</p></a></li>
<li><a href='#InsertDim'><p>Adds A Dimension To An Array</p></a></li>
<li><a href='#LeapYear'><p>Checks Whether A Year Is Leap Year</p></a></li>
<li><a href='#Load'><p>Loads Experimental And Observational Data</p></a></li>
<li><a href='#Mean1Dim'><p>Averages An Array Along A Dimension</p></a></li>
<li><a href='#MeanListDim'><p>Averages An Array Along Multiple Dimensions</p></a></li>
<li><a href='#NAO'><p>Computes the North Atlantic Oscillation (NAO) Index</p></a></li>
<li><a href='#Plot2VarsVsLTime'><p>Plot Two Scores With Confidence Intervals In A Common Plot</p></a></li>
<li><a href='#PlotACC'><p>Plot Plumes/Timeseries Of Anomaly Correlation Coefficients</p></a></li>
<li><a href='#PlotAno'><p>Plot Raw Or Smoothed Anomalies</p></a></li>
<li><a href='#PlotBoxWhisker'><p>Box-And-Whisker Plot of Time Series with Ensemble Distribution</p></a></li>
<li><a href='#PlotClim'><p>Plots Climatologies</p></a></li>
<li><a href='#PlotEquiMap'><p>Maps A Two-Dimensional Variable On A Cylindrical Equidistant Projection</p></a></li>
<li><a href='#PlotLayout'><p>Arrange and Fill Multi-Pannel Layouts With Optional Colour Bar</p></a></li>
<li><a href='#PlotMatrix'><p>Function to convert any numerical table to a grid of coloured squares.</p></a></li>
<li><a href='#PlotSection'><p>Plots A Vertical Section</p></a></li>
<li><a href='#PlotStereoMap'><p>Maps A Two-Dimensional Variable On A Polar Stereographic Projection</p></a></li>
<li><a href='#PlotVsLTime'><p>Plots A Score Along The Forecast Time With Its Confidence Interval</p></a></li>
<li><a href='#ProbBins'><p>Computes Probabilistic Information of a Forecast Relative to a Threshold or a Quantile</p></a></li>
<li><a href='#ProjectField'><p>Project Anomalies onto Modes of Variability</p></a></li>
<li><a href='#RatioRMS'><p>Computes the Ratio Between The RMSE of Two Experiments</p></a></li>
<li><a href='#RatioSDRMS'><p>Computes the ratio between the ensemble spread and RMSE</p></a></li>
<li><a href='#Regression'><p>Computes The Regression Of An Array On Another Along A Dimension</p></a></li>
<li><a href='#RMS'><p>Computes Root Mean Square Error</p></a></li>
<li><a href='#RMSSS'><p>Computes Root Mean Square Skill Score</p></a></li>
<li><a href='#sampleDepthData'><p>Sample of Experimental Data for Forecast Verification In Function Of</p>
Latitudes And Depths</a></li>
<li><a href='#sampleMap'><p>Sample Of Observational And Experimental Data For Forecast Verification In Function Of Longitudes And Latitudes</p></a></li>
<li><a href='#sampleTimeSeries'><p>Sample Of Observational And Experimental Data For Forecast Verification As Area Averages</p></a></li>
<li><a href='#Season'><p>Computes Seasonal Means</p></a></li>
<li><a href='#SelIndices'><p>Slices A Matrix Along A Dimension</p></a></li>
<li><a href='#Smoothing'><p>Smoothes An Array Along A Dimension</p></a></li>
<li><a href='#Spectrum'><p>Estimates Frequency Spectrum</p></a></li>
<li><a href='#Spread'><p>Computes InterQuartile Range, Maximum-Minimum, Standard Deviation and</p>
Median Absolute Deviation of the Ensemble Members</a></li>
<li><a href='#StatSeasAtlHurr'><p>Compute estimate of seasonal mean of Atlantic hurricane activity</p></a></li>
<li><a href='#Subset'><p>Subset a Data Array</p></a></li>
<li><a href='#SVD'><p>Single Value Decomposition (Maximum Covariance Analysis)</p></a></li>
<li><a href='#ToyModel'><p>Synthetic forecast generator imitating seasonal to decadal forecasts. The</p>
components of a forecast: (1) predictabiltiy (2) forecast error (3)
non-stationarity and (4) ensemble generation. The forecast can be computed
for real observations or observations generated artifically.</a></li>
<li><a href='#Trend'><p>Computes the Trend of the Ensemble Mean</p></a></li>
<li><a href='#UltimateBrier'><p>Computes Brier Scores</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Set of Common Tools for Forecast Verification</td>
</tr>
<tr>
<td>Version:</td>
<td>2.10.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Set of tools to verify forecasts through the computation of typical
    prediction scores against one or more observational datasets or reanalyses (a
    reanalysis being a physical extrapolation of observations that relies on the
    equations from a model, not a pure observational dataset). Intended for seasonal
    to decadal climate forecasts although can be useful to verify other kinds of
    forecasts. The package can be helpful in climate sciences for other purposes
    than forecasting. To find more details, see the review paper Manubens, N.et al. (2018)
    &lt;<a href="https://doi.org/10.1016%2Fj.envsoft.2018.01.018">doi:10.1016/j.envsoft.2018.01.018</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>maps, methods, R (&ge; 2.14.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>abind, bigmemory, GEOmap, geomapdata, mapproj, NbClust, ncdf4,
parallel, plyr, SpecsVerification (&ge; 0.5.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>easyVerification, testthat</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://earth.bsc.es/gitlab/es/s2dverification/-/wikis/home">https://earth.bsc.es/gitlab/es/s2dverification/-/wikis/home</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://earth.bsc.es/gitlab/es/s2dverification/-/issues">https://earth.bsc.es/gitlab/es/s2dverification/-/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>cdo</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-04-20 07:24:38 UTC; aho</td>
</tr>
<tr>
<td>Author:</td>
<td>BSC-CNS [aut, cph],
  Virginie Guemas [aut],
  Nicolau Manubens [aut],
  An-Chi Ho [ctb, cre],
  Nuria Perez-Zanon [aut],
  Javier Garcia-Serrano [aut],
  Neven Fuckar [aut],
  Louis-Philippe Caron [aut],
  Omar Bellprat [aut],
  Luis Rodrigues [aut],
  Veronica Torralba [aut],
  Alasdair Hunter [aut],
  Chloe Prodhomme [aut],
  Martin Menegoz [aut],
  Domingo Manubens [ctb],
  Constantin Ardilouze [ctb],
  Lauriane Batte [ctb],
  Fabian Lienert [ctb],
  Julia Giner [ctb],
  Jean-Philippe Baudouin [ctb],
  Nube Gonzalez [ctb],
  Ludovic Auger [ctb],
  Nicola Cortesi [ctb],
  Eleftheria Exarchou [ctb],
  Ruben Cruz [ctb],
  Isabel Andreu-Burillo [ctb],
  Ramiro Saurral [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>An-Chi Ho &lt;an.ho@bsc.es&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-04-20 08:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='s2dverification'>Set of Common Tools for Forecast Verification</h2><span id='topic+s2dverification'></span><span id='topic+s2dverification-package'></span>

<h3>Description</h3>

<p>Set of tools to verify forecasts through the computation of typical  
prediction scores against one or more observational datasets or reanalyses  
(a reanalysis being a physical extrapolation of observations that relies on  
the equations from a model, not a pure observational dataset). Intended for  
seasonal to decadal climate forecasts although can be useful to verify other  
kinds of forecasts. The package can be helpful in climate sciences for other  
purposes than forecasting.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;"> 
Package: </td><td style="text-align: left;"> s2dverification</td>
</tr>
<tr>
 <td style="text-align: left;"> 
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;"> 
Version: </td><td style="text-align: left;"> 2.9.0</td>
</tr>
<tr>
 <td style="text-align: left;"> 
Date: </td><td style="text-align: left;"> 2020-10-30</td>
</tr>
<tr>
 <td style="text-align: left;"> 
License: </td><td style="text-align: left;"> LGPLv3</td>
</tr>
<tr>
 <td style="text-align: left;"> 
 </td>
</tr>

</table>
 
<p>Check an overview of the package functionalities and its modules at  
<a href="https://earth.bsc.es/gitlab/es/s2dverification/-/wikis/home">https://earth.bsc.es/gitlab/es/s2dverification/-/wikis/home</a>. 
For more information load the package and check the help for each function  
or the documentation attached to the package.
</p>


<h3>Author(s)</h3>

<p>Nicolau Manubens
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://earth.bsc.es/gitlab/es/s2dverification/-/wikis/home">https://earth.bsc.es/gitlab/es/s2dverification/-/wikis/home</a>
</p>
</li>
<li><p> Report bugs at <a href="https://earth.bsc.es/gitlab/es/s2dverification/-/issues">https://earth.bsc.es/gitlab/es/s2dverification/-/issues</a>
</p>
</li></ul>


<hr>
<h2 id='.LoadDataFile'>
Load Data From File Into Environment
</h2><span id='topic+.LoadDataFile'></span>

<h3>Description</h3>

<p>This function receives a 'work piece', a named list which contains information on a data file to be loaded.<br />
It can be run in 'explore' mode or in 'load' mode.<br />
When running in 'explore' mode, the metadata of the file is read and the sizes of the dimensions in the file are returned in a named list:
</p>

<ul>
<li><p>'member': Number of members
</p>
</li>
<li><p>'time': Number of lead-times
</p>
</li>
<li><p>'lon': Longitudes in the file
</p>
</li>
<li><p>'lat': Latitudes in the file
</p>
</li></ul>

<p>When running in 'load' mode it loads and performs any requested computations in additional parameters in the 'work piece', such as interpolating, slicing, ..., and finally stores it in a shared memory matrix pointed by the parameter 'out_pointer' in the 'work piece'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.LoadDataFile(work_piece, explore_dims = FALSE, silent = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".LoadDataFile_+3A_work_piece">work_piece</code></td>
<td>

<p>Named list with information on the file to load or explore and additional parameters.<br />
The needed variables in the work piece are:
</p>

<ul>
<li><p>In explore mode:
</p>

<ul>
<li><p>dataset_type: 'exp'/'obs'
</p>
</li>
<li><p>filename: full path to the data file
</p>
</li>
<li><p>dimnames: named list with names 'lon', 'lat' and 'member' and with values the associated actual dimension names inside the NetCDF files.
</p>
</li>
<li><p>namevar: name of the variable in the nc file
</p>
</li>
<li><p>grid: common grid into which interpolate or NULL if no interpolation
</p>
</li>
<li><p>remap: interpolation method ('remapbil'/'remapdis'/'remapcon'/'remapbic')
</p>
</li>
<li><p>lon_limits: c(lon_min, lon_max)
</p>
</li>
<li><p>lat_limits: c(lat_min, lat_max)
</p>
</li>
<li><p>is_file_per_member: TRUE/FALSE
</p>
</li>
<li><p>is_file_per_dataset: TRUE/FALSE
</p>
</li>
<li><p>is_single_dataset: TRUE/FALSE
</p>
</li></ul>


</li>
<li><p>In load mode:
</p>

<ul>
<li><p>dataset_type: 'exp'/'obs'
</p>
</li>
<li><p>filename: full path to the data file
</p>
</li>
<li><p>dimnames: named list with names 'lon', 'lat' and 'member' and with values the associated actual dimension names inside the NetCDF files.
</p>
</li>
<li><p>namevar: name of the variable in the nc file
</p>
</li>
<li><p>is_2d_var: TRUE/FALSE
</p>
</li>
<li><p>grid: common grid into which interpolate or NULL if no interpolation
</p>
</li>
<li><p>remap: interpolation method ('remapbil'/'remapdis'/'remapcon'/'remapbic')
</p>
</li>
<li><p>lon_limits: c(lon_min, lon_max)
</p>
</li>
<li><p>lat_limits: c(lat_min, lat_max)
</p>
</li>
<li><p>is_file_per_dataset: TRUE/FALSE
</p>
</li>
<li><p>startdates: in the case that filename points to a whole dataset file, the list of starting dates to load must be specified. c('sdate1', 'sdate2', ...)
</p>
</li>
<li><p>out_pointer: big.matrix descriptor pointing to the array (transformed into a matrix) where to keep the data
</p>
</li>
<li><p>dims: named list with dimension sizes of the original array into which the data is kept. Names must be c(['lon', ]['lat', ]'time', 'member', 'sdates', 'dataset').
</p>
</li>
<li><p>indices: vector of initial positions corresponding to each dimension in 'dims' where to store data in the original array. First two indices ('lon', 'lat') can be missing.
</p>
</li>
<li><p>nmember: number of members expected to be loaded
</p>
</li>
<li><p>mask: complete (untrimmed + interpolated if needed) mask to activate/deactivate data points, with dimensions c('lon', 'lat'), see <code>?Load</code> for more details.
</p>
</li>
<li><p>leadtimes: vector of time indices to be loaded from the file
</p>
</li>
<li><p>var_limits: c(var_min, var_max)
</p>
</li>
<li><p>is_single_dataset: TRUE/FALSE (whether the user only asked for data of one single dataset. If so, then data won't be re-interpolated when the first longitude of its grid is != 0. Otherwise it must be re-interpolated to ensure all data will be properly aligned over longitudes.
</p>
</li></ul>


</li></ul>

</td></tr>
<tr><td><code id=".LoadDataFile_+3A_explore_dims">explore_dims</code></td>
<td>

<p>Run in dimension explore mode (TRUE) or in load and calculation mode (FALSE).<br />
Takes by default the value FALSE (calculation mode).
</p>
</td></tr>
<tr><td><code id=".LoadDataFile_+3A_silent">silent</code></td>
<td>

<p>Parameter to allow (FALSE) or deactivate (TRUE) printing of explanatory messages.<br />
When deactivated any warning messages will still be displayed.<br />
Takes by default the value FALSE (verbose mode).
</p>
</td></tr>
<tr><td><code id=".LoadDataFile_+3A_remapcells">remapcells</code></td>
<td>

<p>Width in number of cells of the surrounding area of the requested subset to be taken into account for the interpolation. See parameter <code>remapcells</code> of <code>Load()</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>When called in 'explore' mode, a named list is returned with:
</p>

<ul>
<li><p>dims
List with the found lengths for members, leadtimes and the latitudes and longitudes already trimmed and reordered if needed. The names are 'member', 'time', 'lon', 'lat'. If it is a file from a file-per-member dataset, the number of files that match the filename replacing the $MEMBER_NUMBER$ part by an asterisk is returned (which is the supposed number of members). There are known issues with this method of detection. See documentation on parameter 'nmember' and 'nmemberobs' in Load() function. When the specified file path is a URL, the returned number of members is NULL.<br />

</p>
</li>
<li><p>is_2d_var
Boolean indicating whether the found variable is 2-dimensional (TRUE) or a global mean (FALSE).

</p>
</li>
<li><p>grid
Character string with the name of the grid of the file, following the cdo grid naming conventions.

</p>
</li>
<li><p>var_long_name
A character string with the variable long name. If not available, the short name is returned.

</p>
</li>
<li><p>units
A character string with the units of the variable.

</p>
</li></ul>

<p>When called in 'calculation' mode, the found file path or URL is returned if the file was found and NULL is returned otherwise.  
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2015-01  (N. Manubens)  -  First version
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## Not run: 
data &lt;- s2dverification:::.LoadDataFile(list(dataset_type = 'exp', 
        filename = system.file('sample_data/model/experiment/monthly_mean', 
                               'tos_3hourly/tos_19901101.nc', 
                               package = 's2dverification'),
        namevar = 'tos', lon_limits = c(-12, 40), 
        lat_limits = c(27, 48), is_file_per_member = TRUE, 
        dimnames = list(lon = 'longitude', lat = 'latitude', 
        member = 'ensemble')), explore_dims = TRUE, silent = FALSE)
  
## End(Not run)
</code></pre>

<hr>
<h2 id='ACC'>Computes Anomaly Correlation Coefficient</h2><span id='topic+ACC'></span>

<h3>Description</h3>

<p>Calculates the Anomaly Correlation Coefficient for the ensemble mean of 
each model and the corresponding references for each startdate and each 
leadtime.<br />
The domain of interest can be specified by providing the list 
of longitudes/latitudes (lon/lat) of the grid together with the corners 
of the domain:<br />
lonlatbox = c(lonmin, lonmax, latmin, latmax).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ACC(
  var_exp,
  var_obs,
  lon = NULL,
  lat = NULL,
  lonlatbox = NULL,
  conf = TRUE,
  conftype = "parametric",
  siglev = 0.95
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ACC_+3A_var_exp">var_exp</code></td>
<td>
<p>Array of experimental anomalies with dimensions: 
c(nexp, nsdates, nltimes, nlat, nlon) or 
c(nexp, nsdates, nmembers, nltimes, nlat, nlon).</p>
</td></tr>
<tr><td><code id="ACC_+3A_var_obs">var_obs</code></td>
<td>
<p>Array of observational anomalies, same dimensions as var_exp 
except along the first dimension and the second if it corresponds to the 
member dimension.</p>
</td></tr>
<tr><td><code id="ACC_+3A_lon">lon</code></td>
<td>
<p>Array of longitudes of the var_exp/var_obs grids, optional.</p>
</td></tr>
<tr><td><code id="ACC_+3A_lat">lat</code></td>
<td>
<p>Array of latitudes of the var_exp/var_obs grids, optional.</p>
</td></tr>
<tr><td><code id="ACC_+3A_lonlatbox">lonlatbox</code></td>
<td>
<p>Domain to select: c(lonmin, lonmax, latmin, latmax), 
optional.</p>
</td></tr>
<tr><td><code id="ACC_+3A_conf">conf</code></td>
<td>
<p>TRUE/FALSE: confidence intervals and significance level 
provided or not.</p>
</td></tr>
<tr><td><code id="ACC_+3A_conftype">conftype</code></td>
<td>
<p>&quot;Parametric&quot; provides a confidence interval for the ACC 
computed by a Fisher transformation and a significance level for the ACC 
from a one-sided student-T distribution. &quot;Bootstrap&quot; provides a confidence 
interval for the ACC and MACC computed from bootstrapping on the members 
with 100 drawings with replacement. To guarantee the statistical robustness 
of the result, make sure that your experiments/oservations/startdates/
leadtimes always have the same number of members.</p>
</td></tr>
<tr><td><code id="ACC_+3A_siglev">siglev</code></td>
<td>
<p>The confidence level for the computed confidence intervals.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>ACC</code></td>
<td>

<p>If <code>conf = TRUE</code>, array with dimensions: 
c(nexp, nobs, nsdates, nleadtimes, 4) 
The fifth dimension of length 4 corresponds to the lower limit of the 
<code>siglev</code>% confidence interval, the ACC, the upper limit of the 
<code>siglev</code>% confidence interval and the <code>siglev</code>% significance 
level. If <code>conf = FALSE</code>, the array of the Anomaly Correlation 
Coefficient has dimensions c(nexp, nobs, nsdates, nleadtimes).
</p>
</td></tr>
<tr><td><code>MACC</code></td>
<td>

<p>The array of the Mean Anomaly Correlation Coefficient with dimensions 
c(nexp, nobs, nleadtimes).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1 - 2013-08 (V. Guemas) - Original code<br />
1.0 - 2013-09 (N. Manubens) - Formatting to CRAN<br />
1.1 - 2013-09 (C. Prodhomme) - optimization<br />
1.2 - 2014-08 (V. Guemas) - Bug-fixes: handling of NA &amp; selection of domain + Simplification of code<br />
1.3.0 - 2014-08 (V. Guemas) - Boostrapping over members<br />
1.3.1 - 2014-09 (C. Prodhomme) - Add comments and minor style changes<br />
1.3.2 - 2015-02 (N. Manubens) - Fixed ACC documentation and examples
</p>


<h3>References</h3>

<p>Joliffe and Stephenson (2012). Forecast Verification: A 
Practitioner's Guide in Atmospheric Science. Wiley-Blackwell.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See ?Load for explanations on the first part of this example.
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path, 
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates, 
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
sampleData$mod &lt;- Season(sampleData$mod, 4, 11, 12, 2)
sampleData$obs &lt;- Season(sampleData$obs, 4, 11, 12, 2)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
acc &lt;- ACC(Mean1Dim(ano_exp, 2), Mean1Dim(ano_obs, 2))
 
PlotACC(acc$ACC, startDates)
 
</code></pre>

<hr>
<h2 id='Alpha'>Estimates AutoCorrelation At Lag 1 following Guemas et al, BAMS, 2013b</h2><span id='topic+Alpha'></span>

<h3>Description</h3>

<p>This function, relying on the <code>FitAcfCoef()</code> function, estimates the 
autocorrelation at lag 1 of the xdata array following the method described 
in Guemas V., Auger L., Doblas-Reyes F., JAMC, 2013. After applying a linear 
detrending and/or a filtering of any frequency peak if requested, the sample 
autocorrelation is estimated.<br />
Then the theoretical autocorrelation of an AR1 is fitted to the sample 
autocorrelation using the Cardano's formula (see <code>FitAcfCoef()</code>) to 
obtain the autocorrelation at lag 1. This method assumes xdata is an AR1 
process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Alpha(xdata, detrend = FALSE, filter = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Alpha_+3A_xdata">xdata</code></td>
<td>
<p>Timeseries from which the autocorrelation at lag 1 is requested.</p>
</td></tr>
<tr><td><code id="Alpha_+3A_detrend">detrend</code></td>
<td>
<p>TRUE applies a linear detrending to xdata prior to the 
estimation of the autocorrelation at lag 1.</p>
</td></tr>
<tr><td><code id="Alpha_+3A_filter">filter</code></td>
<td>
<p>TRUE applies a filtering of any frequency peak prior to the 
estimation of the autocorrelation at lag 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Autocorrelation at lag 1.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1 - 2012-06 (V. Guemas) - Original code<br />
1.0 - 2013-09 (N. Manubens) - Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
alpha &lt;- Alpha(sampleData$mod[1, 1, , 1])
print(alpha)

</code></pre>

<hr>
<h2 id='AnimateMap'>Animate Maps of Forecast/Observed Values or Scores Over Forecast Time</h2><span id='topic+AnimateMap'></span>

<h3>Description</h3>

<p>Create animations of maps in an equi-rectangular or stereographic 
projection, showing the anomalies, the climatologies, the mean InterQuartile 
Range, Maximum-Mininum, Standard Deviation, Median Absolute Deviation, 
the trends, the RMSE, the correlation or the RMSSS, between modelled and 
observed data along the forecast time (lead-time) for all input experiments 
and input observational datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AnimateMap(
  var,
  lon,
  lat,
  toptitle = rep("", 11),
  sizetit = 1,
  units = "",
  monini = 1,
  freq = 12,
  msk95lev = FALSE,
  brks = NULL,
  cols = NULL,
  filled.continents = FALSE,
  lonmin = 0,
  lonmax = 360,
  latmin = -90,
  latmax = 90,
  intlon = 20,
  intlat = 30,
  drawleg = TRUE,
  subsampleg = 1,
  colNA = "white",
  equi = TRUE,
  fileout = c("output1_animvsltime.gif", "output2_animvsltime.gif",
    "output3_animvsltime.gif"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AnimateMap_+3A_var">var</code></td>
<td>
<p>Matrix of dimensions (nltime, nlat, nlon) or 
(nexp/nmod, nltime, nlat, nlon) or (nexp/nmod, 3/4, nltime, nlat, nlon) or 
(nexp/nmod, nobs, 3/4, nltime, nlat, nlon).</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_lon">lon</code></td>
<td>
<p>Vector containing longtitudes (degrees).</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_lat">lat</code></td>
<td>
<p>Vector containing latitudes (degrees).</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_toptitle">toptitle</code></td>
<td>
<p>c(&rdquo;,&rdquo;, ...) array of main title for each animation, 
optional. If RMS, RMSSS, correlations: first exp with successive obs, then 
second exp with successive obs, etc ...</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_sizetit">sizetit</code></td>
<td>
<p>Multiplicative factor to increase title size, optional.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_units">units</code></td>
<td>
<p>Units, optional.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_monini">monini</code></td>
<td>
<p>Starting month between 1 and 12. Default = 1.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_freq">freq</code></td>
<td>
<p>1 = yearly, 12 = monthly, 4 = seasonal ...</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_msk95lev">msk95lev</code></td>
<td>
<p>TRUE/FALSE grid points with dots if 95% significance level 
reached. Default = FALSE.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_brks">brks</code></td>
<td>
<p>Limits of colour levels, optional. For example: 
seq(min(var), max(var), (max(var) - min(var)) / 10).</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_cols">cols</code></td>
<td>
<p>Vector of colours of length(brks) - 1, optional.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_filled.continents">filled.continents</code></td>
<td>
<p>Continents filled in grey (TRUE) or represented by 
a black line (FALSE). Default = TRUE. Filling unavailable if crossing 
Greenwich and equi = TRUE. Filling unavailable if square = FALSE and 
equi = TRUE.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_lonmin">lonmin</code></td>
<td>
<p>Westward limit of the domain to plot (&gt; 0 or &lt; 0). 
Default : 0 degrees.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_lonmax">lonmax</code></td>
<td>
<p>Eastward limit of the domain to plot (&gt; 0 or &lt; 0). 
lonmax &gt; lonmin. Default : 360 degrees.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_latmin">latmin</code></td>
<td>
<p>Southward limit of the domain to plot. Default : -90 degrees.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_latmax">latmax</code></td>
<td>
<p>Northward limit of the domain to plot. Default : 90 degrees.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_intlon">intlon</code></td>
<td>
<p>Interval between longitude ticks on x-axis. 
Default = 20 degrees.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_intlat">intlat</code></td>
<td>
<p>Interval between latitude ticks on y-axis for equi = TRUE or 
between latitude circles for equi = FALSE. Default = 30 degrees.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_drawleg">drawleg</code></td>
<td>
<p>Draw a colorbar. Can be FALSE only if square = FALSE or 
equi = FALSE. Default = TRUE.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_subsampleg">subsampleg</code></td>
<td>
<p>Supsampling factor of the interval between ticks on 
colorbar. Default = 1 = every colour level.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_colna">colNA</code></td>
<td>
<p>Color used to represent NA. Default = 'white'.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_equi">equi</code></td>
<td>
<p>TRUE/FALSE == cylindrical equidistant/stereographic projection. 
Default: TRUE.</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_fileout">fileout</code></td>
<td>
<p>c(&rdquo;, &rdquo;, ...) array of output file name for each animation.
If RMS, RMSSS, correlations : first exp with successive obs, then second 
exp with successive obs, etc ...</p>
</td></tr>
<tr><td><code id="AnimateMap_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the following 
graphical parameters:<br />
adj ann ask bty cex cex.axis cex.lab cex.main cex.sub
cin col.axis col.lab col.main col.sub cra crt csi cxy err family fg fig 
font font.axis font.lab font.main font.sub las lheight ljoin lmitre lty 
lwd mai mar mex mfcol mfrow mfg mgp mkh oma omd omi page pch plt pty smo 
srt tck tcl usr xaxp xaxs xaxt xlog xpd yaxp yaxs yaxt ylbias ylog. <br />
For more information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Examples of input:
</p>

<ol>
<li>
<p>Outputs from clim (exp, obs, memb = FALSE):
(nmod, nltime, nlat, nlon)
or (nobs, nltime, nlat, nlon) 

</p>
</li>
<li>
<p>Model output from load/ano/smoothing:
(nmod, nmemb, sdate, nltime, nlat, nlon)
then passed through spread(var, posdim = 2, narm = TRUE)
&amp; mean1dim(var, posdim = 3, narm = TRUE)
or through trend(mean1dim(var, 2), posTR = 2):
(nmod, 3, nltime, nlat, nlon)
animates average along start dates of IQR/MaxMin/SD/MAD across members 
or trends of the ensemble-mean computed accross the start dates.

</p>
</li>
<li>
<p>model and observed output from load/ano/smoothing:
(nmod, nmemb, sdate, nltime, nlat, nlon)
&amp; (nobs, nmemb, sdate, nltime, nlat, nlon)
then averaged along members mean1dim(var_exp/var_obs, posdim = 2):
(nmod, sdate, nltime, nlat, nlon)
(nobs, sdate, nltime, nlat, nlon)
then passed through corr(exp, obs, posloop = 1, poscor = 2)
or RMS(exp, obs, posloop = 1, posRMS = 2):
(nmod, nobs, 3, nltime, nlat, nlon)
animates correlations or RMS between each exp &amp; each obs against leadtime.

</p>
</li></ol>



<h3>Author(s)</h3>

<p>History:<br />
1.0 - 2012-04 (V. Guemas) - Original code<br />
1.1 - 2014-04 (N. Manubens) - Formatting to CRAN<br />
1.2 - 2015-05 (V. Guemas) - Use of PlotEquiMap and PlotStereoMap
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See ?Load for explanations on the first part of this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path,
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  output = 'lonlat', latmin = 27, latmax = 48, 
                  lonmin = -12, lonmax = 40)
 
## End(Not run)
 
clim &lt;- Clim(sampleData$mod, sampleData$obs, memb = FALSE)
 ## Not run: 
AnimateMap(clim$clim_exp, sampleData$lon, sampleData$lat,
 toptitle = "climatology of decadal prediction", sizetit = 1, 
 units = "degree", brks = seq(270, 300, 3), monini = 11, freq = 12, 
 msk95lev = FALSE, filled.continents = TRUE, intlon = 10, intlat = 10,
 fileout = 'clim_dec.gif')
 
## End(Not run)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
leadtimes_dimension &lt;- 4
initial_month &lt;- 11
mean_start_month &lt;- 1
mean_stop_month &lt;- 12
season_means_mod &lt;- Season(ano_exp, leadtimes_dimension, initial_month,
                          mean_start_month, mean_stop_month)
season_means_obs &lt;- Season(ano_obs, leadtimes_dimension, initial_month,
                          mean_start_month, mean_stop_month)
 ## Not run: 
AnimateMap(Mean1Dim(season_means_mod, 2)[1, 1, , , ], sampleData$lon,
 sampleData$lat, toptitle = "Annual anomalies 1985 of decadal prediction",
 sizetit = 1, units = "degree", monini = 1, freq = 1, msk95lev = FALSE,
 brks = seq(-0.5, 0.5, 0.1), intlon = 10, intlat = 10,
 filled.continents = TRUE, fileout = 'annual_means_dec.gif')
 
## End(Not run)
dim_to_mean &lt;- 2  # Mean along members
rms &lt;- RMS(Mean1Dim(season_means_mod, dim_to_mean),
          Mean1Dim(season_means_obs, dim_to_mean))
 
AnimateMap(rms, sampleData$lon, sampleData$lat, toptitle =
 "RMSE decadal prediction", sizetit = 1, units = "degree",
 monini = 1, freq = 1, msk95lev = FALSE, brks = seq(0, 0.8, 0.08),
 intlon = 10, intlat = 10, filled.continents = TRUE,
 fileout = 'rmse_dec.gif')
 
</code></pre>

<hr>
<h2 id='Ano'>Computes Forecast or Observed Anomalies</h2><span id='topic+Ano'></span>

<h3>Description</h3>

<p>This function computes anomalies from any experimental or observational 
matrix output from <code>Load()</code> and their climatologies output from 
<code>Clim()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Ano(var, clim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ano_+3A_var">var</code></td>
<td>
<p>Model or observational data:<br /> 
c(nmod/nexp/nobs, nmemb/nparam, nsdates, nltime) up to <br />
c(nmod/nexp/nobs, nmemb/nparam, nsdates, nltime, nlevel, nlat, nlon)<br /></p>
</td></tr>
<tr><td><code id="Ano_+3A_clim">clim</code></td>
<td>
<p>Climatologies from clim: c(nmod/nexp/nobs, nltime) <br />
up to c(nmod/nexp/nobs, nltime, nlevel, nlat, nlon) or <br />
c(nmod/nexp/nobs, nmemb/nparam, nltime) up to <br />
c(nmod/nexp/nobs, nmemb/nparam, nltime, nlevel, nlat, nlon) or <br />
c(nmod/nexp/nobs, nmemb/nparam, nsdates, nltime) up to <br />
c(nmod/nexp/nobs, nmemb/nparam, nsdates, nltime, nlevel, nlat, nlon) <br />
depending on the options provided to <code>Clim()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Array with same dimensions as 'var'.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1 - 2012-03 (V. Guemas) - Original code<br />
1.0 - 2013-09 (N. Manubens) - Formatting to R CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
runmean_nb_months &lt;- 12
dim_to_smooth &lt;- 4  # Smooth along lead-times
smooth_ano_exp &lt;- Smoothing(ano_exp, runmean_nb_months, dim_to_smooth)
smooth_ano_obs &lt;- Smoothing(ano_obs, runmean_nb_months, dim_to_smooth)
 
PlotAno(smooth_ano_exp, smooth_ano_obs, startDates, 
       toptitle = paste('smoothed anomalies'), ytitle = c('K', 'K', 'K'), 
       legends = 'ERSST', biglab = FALSE, fileout = 'tos_ano.eps')
 
</code></pre>

<hr>
<h2 id='Ano_CrossValid'>Computes Anomalies In Cross-Validation Mode</h2><span id='topic+Ano_CrossValid'></span>

<h3>Description</h3>

<p>Computes the anomalies from the arrays of the experimental and observational 
data output from <code>load()</code> by subtracting the climatologies computed 
with a cross-validation technique and a per-pair method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Ano_CrossValid(var_exp, var_obs, memb = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ano_CrossValid_+3A_var_exp">var_exp</code></td>
<td>
<p>Model data:<br />
c(nmod/nexp, nmemb/nparam, nsdates, nltime) up to <br />
c(nmod/nexp, nmemb/nparam, nsdates, nltime, nlevel, nlat, nlon).</p>
</td></tr>
<tr><td><code id="Ano_CrossValid_+3A_var_obs">var_obs</code></td>
<td>
<p>Observational data: <br />
c(nobs, nmemb, nsdates, nltime) up to <br />
c(nobs, nmemb, nsdates, nltime, nlevel, nlat, nlon).</p>
</td></tr>
<tr><td><code id="Ano_CrossValid_+3A_memb">memb</code></td>
<td>
<p>TRUE/FALSE (1 climatology for each member/1 climatology 
averaging all the members). Default = TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>$ano_exp</code></td>
<td>
<p>Matrix with same dimensions as var_exp</p>
</td></tr>
<tr><td><code>$ano_obs</code></td>
<td>
<p>Matrix with same dimensions as var_obs</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1 - 2011-12 (V. Guemas) - Original code<br />
1.0 - 2013-09 (N. Manubens) - Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
anomalies &lt;- Ano_CrossValid(sampleData$mod, sampleData$obs)
 
PlotAno(anomalies$ano_exp, anomalies$ano_obs, startDates, 
       toptitle = paste('anomalies'), ytitle = c('K', 'K', 'K'), 
       legends = 'ERSST', biglab = FALSE, fileout = 'tos_ano_crossvalid.eps')
 
</code></pre>

<hr>
<h2 id='ArrayToNetCDF'>Save multidimensional R arrays into NetCDF files</h2><span id='topic+ArrayToNetCDF'></span>

<h3>Description</h3>

<p>This function takes as input one or a list of multidimensional R arrays and 
stores them in a NetCDF file, using the <code>ncdf4</code> package. The full path 
and name of the resulting file must be specified. Metadata can be attached 
to the arrays and propagated into the NetCDF file in 3 possible ways:
</p>

<ul>
<li>
<p>Via the list names if a list of arrays is provided: Each name in 
the input list, corresponding to one multidimensional array, will be 
interpreted as the name of the variable it contains.<br />
E.g:<br />
<code>ArrayToNetCDF(arrays = list(temperature = array(1:9, c(3, 3))),</code><br />
<code>                       file_path = 'example.nc')</code>

</p>
</li>
<li>
<p>Via the dimension names of each provided array: The dimension names 
of each of the provided arrays will be interpreted as names for the 
dimensions of the NetCDF files. Read further for special dimension names 
that will trigger special behaviours, such as 'time' and 'var'.<br />
E.g:<br />
<code>temperature &lt;- array(rnorm(100 * 50 * 10), dim = c(100, 50, 10))</code><br />
<code>names(dim(temperature)) &lt;- c('longitude', 'latitude', 'time')</code><br />
<code>ArrayToNetCDF(list(temperature = temperature), file_path = 'example.nc')</code>

</p>
</li>
<li>
<p>Via the attribute 'variables' of each provided array: The arrays can 
be provided with metadata in an attribute named 'variables', which is 
expected to be a named list of named lists, where the names of the 
container list are the names of the variables present in the provided 
array, and where each sub-list contains metadata for each of the variables.
The attribute names and values supported in the sub-lists must follow the 
same format the package <code>ncdf4</code> uses to represent the NetCDF 
file headers.<br />
E.g:<br />
<code>a &lt;- array(1:400, dim = c(5, 10, 4, 2))</code><br />
<code>metadata &lt;- list(tos = list(addOffset = 100,</code><br />
<code>                 scaleFact = 10,</code><br />
<code>                 dim = list(list(name = 'time',</code><br />
<code>                                 unlim = FALSE))))</code><br />
<code>attr(a, 'variables') &lt;- metadata</code><br />
<code>names(dim(a)) &lt;- c('lat', 'lon', 'time', 'var')</code><br />
<code>ArrayToNetCDF(a, 'tmp.nc')</code>

</p>
</li></ul>

<p>The special dimension names are 'var'/'variable' and 'time'.<br />
If a dimension is named 'var' or 'variable', <code>ArrayToNetCDF</code> will 
interpret each array entry along such dimension corresponds to a separate 
new variable, hence will create a new variable inside the NetCDF file and 
will use it to store all the data in the provided array for the 
corresponding entry along the 'var'/'variable' dimension.<br />
If a dimension is named 'time', by default it will be interpreted and built 
as an unlimited dimension. The 'time' dimension must be the last dimension 
of the array (the right-most). If a 'var'/'variable' dimension is present, 
the 'time' dimension can be also placed on its left (i.e. the one before the 
last dimension). The default behaviour of creating the 'time' as unlimited 
dimension can be disabled by setting manually the attribute 
<code>unlim = FALSE</code>, as shown in the previous example.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ArrayToNetCDF(arrays, file_path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ArrayToNetCDF_+3A_arrays">arrays</code></td>
<td>
<p>One or a list of multidimensional data arrays. The list can be 
provided with names, which will be interpreted as variable names. The 
arrays can be provided with dimension names. The arrays can be provided 
with metadata in the attribute 'variables' (read section Description for 
details).</p>
</td></tr>
<tr><td><code id="ArrayToNetCDF_+3A_file_path">file_path</code></td>
<td>
<p>Path and name of the NetCDF file to be created.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns NULL.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.0 - 2017-01 (N. Manubens) - Original code.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## Not run: 
# Minimal use case
ArrayToNetCDF(array(1:9, c(3, 3)), 'tmp.nc')

# Works with arrays of any number of dimensions
ArrayToNetCDF(array(1:27, c(3, 3, 3)), 'tmp.nc')

# Arrays can also be provided in [named] lists
ArrayToNetCDF(list(tos = array(1:27, c(3, 3, 3))), 'tmp.nc')

# Or with dimension names
# 'var' dimension name will generate multiple variables in the 
# resulting NetCDF file
a &lt;- array(1:27, dim = c(3, 3, 3))
names(dim(a)) &lt;- c('lon', 'lat', 'var')
ArrayToNetCDF(a, 'tmp.nc')

# 'variable' as dimension name will do the same
a &lt;- array(1:27, dim = c(3, 3, 3))
names(dim(a)) &lt;- c('lon', 'lat', 'variable')
ArrayToNetCDF(a, 'tmp.nc')

# The 'time' dimension will be built as unlimited dimension, by default
a &lt;- array(1:1600, dim = c(10, 20, 4, 2))
names(dim(a)) &lt;- c('lat', 'lon', 'time', 'var')
ArrayToNetCDF(a, 'tmp.nc')

# Putting the 'time' dimension in a position which is not the last, or the one
# right before 'var'/'variable' will crash. Unlimited dimension must be in the
# last position
a &lt;- array(1:1600, dim = c(10, 20, 4, 2))
names(dim(a)) &lt;- c('time', 'lat', 'lon', 'var')
ArrayToNetCDF(a, 'tmp.nc')
a &lt;- array(1:1600, dim = c(10, 20, 4, 2))
names(dim(a)) &lt;- c('lat', 'time', 'lon', 'var')
ArrayToNetCDF(a, 'tmp.nc')

# The dimension 'var'/'variable' can be in any position and can have any length
a &lt;- array(1:1600, dim = c(10, 20, 4, 2))
names(dim(a)) &lt;- c('lat', 'var', 'lon', 'time')
ArrayToNetCDF(a, 'tmp.nc')

# Multiple arrays can be provided in a list
a &lt;- array(1:400, dim = c(5, 10, 4, 2))
names(dim(a)) &lt;- c('lat', 'lon', 'time', 'var')
ArrayToNetCDF(list(a, a), 'tmp.nc')

# If no dimension names are given to an array, new names will be automatically
# generated
a &lt;- array(1:400, dim = c(5, 10, 4, 2))
b &lt;- array(1:400, dim = c(5, 11, 4, 2))
names(dim(a)) &lt;- c('lat', 'lon', 'time', 'var')
ArrayToNetCDF(list(a, b), 'tmp.nc')

# If two arrays use a same dimension but their lengths differ, the function 
# will crash
a &lt;- array(1:400, dim = c(5, 10, 4, 2))
b &lt;- array(1:400, dim = c(5, 11, 4, 2))
names(dim(a)) &lt;- c('lat', 'lon', 'time', 'var')
names(dim(b)) &lt;- c('lat', 'lon', 'time', 'var')
ArrayToNetCDF(list(a, b), 'tmp.nc')

# Metadata can be provided for each variable in each array, via the
# attribute 'variables'. In this example the metadata is empty.
a &lt;- array(1:400, dim = c(5, 10, 4, 2))
metadata &lt;- list(
             tos = list(),
             tas = list()
           )
attr(a, 'variables') &lt;- metadata
names(dim(a)) &lt;- c('lat', 'lon', 'time', 'var')
ArrayToNetCDF(a, 'tmp.nc')

# Variable names can be manually specified
a &lt;- array(1:400, dim = c(5, 10, 4, 2))
metadata &lt;- list(
             tos = list(name = 'name1'),
             tas = list(name = 'name2')
           )
attr(a, 'variables') &lt;- metadata
names(dim(a)) &lt;- c('lat', 'lon', 'time', 'var')
ArrayToNetCDF(a, 'tmp.nc')

# Units can be specified
a &lt;- array(1:400, dim = c(5, 10, 4, 2))
metadata &lt;- list(
             tos = list(units = 'K'),
             tas = list(units = 'K')
           )
attr(a, 'variables') &lt;- metadata
names(dim(a)) &lt;- c('lat', 'lon', 'time', 'var')
ArrayToNetCDF(a, 'tmp.nc')

# addOffset and scaleFactor can be specified
a &lt;- array(1:400, dim = c(5, 10, 4, 2))
metadata &lt;- list(
             tos = list(addOffset = 100,
                        scaleFact = 10),
             tas = list(addOffset = 100,
                        scaleFact = 10)
           )
attr(a, 'variables') &lt;- metadata
names(dim(a)) &lt;- c('lat', 'lon', 'time', 'var')
ArrayToNetCDF(a, 'tmp.nc')

# Unlimited dimensions can be manually created
a &lt;- array(1:400, dim = c(5, 10, 4, 2))
metadata &lt;- list(
             tos = list(addOffset = 100,
                        scaleFact = 10,
                        dim = list(list(name = 'unlimited',
                                        unlim = TRUE))),
             tas = list(addOffset = 100,
                        scaleFact = 10,
                        dim = list(list(name = 'unlimited',
                                        unlim = TRUE)))
           )
attr(a, 'variables') &lt;- metadata
names(dim(a)) &lt;- c('lat', 'lon', 'unlimited', 'var')
ArrayToNetCDF(a, 'tmp.nc')

# A 'time' dimension can be built without it necessarily being unlimited
a &lt;- array(1:400, dim = c(5, 10, 4, 2))
metadata &lt;- list(
             tos = list(addOffset = 100,
                        scaleFact = 10,
                        dim = list(list(name = 'time',
                                        unlim = FALSE))),
             tas = list(addOffset = 100,
                        scaleFact = 10,
                        dim = list(list(name = 'time',
                                        unlim = FALSE)))
           )
attr(a, 'variables') &lt;- metadata
names(dim(a)) &lt;- c('lat', 'lon', 'time', 'var')
ArrayToNetCDF(a, 'tmp.nc')

# Multiple arrays with data for multiple variables can be saved into a 
# NetCDF file at once.
tos &lt;- array(1:400, dim = c(5, 10, 4))
metadata &lt;- list(tos = list(units = 'K'))
attr(tos, 'variables') &lt;- metadata
names(dim(tos)) &lt;- c('lat', 'lon', 'time')
lon &lt;- seq(0, 360 - 360 / 10, length.out = 10)
dim(lon) &lt;- length(lon)
metadata &lt;- list(lon = list(units = 'degrees_east'))
attr(lon, 'variables') &lt;- metadata
names(dim(lon)) &lt;- 'lon'
lat &lt;- seq(-90, 90, length.out = 5)
dim(lat) &lt;- length(lat)
metadata &lt;- list(lat = list(units = 'degrees_north'))
attr(lat, 'variables') &lt;- metadata
names(dim(lat)) &lt;- 'lat'
ArrayToNetCDF(list(tos, lon, lat), 'tmp.nc')

## End(Not run)
</code></pre>

<hr>
<h2 id='BrierScore'>Compute Brier Score And Its Decomposition And Brier Skill Score</h2><span id='topic+BrierScore'></span><span id='topic+.BrierScore'></span>

<h3>Description</h3>

<p>Computes the Brier score (BS) and the components of its standard 
decomposition as well with the two within-bin components described in 
Stephenson et al., (2008). It also returns the bias-corrected decomposition 
of the BS (Ferro and Fricker, 2012). BSS having the climatology as the 
reference forecast. <br /><br />
.BrierScore provides the same functionality, but taking a matrix of ensemble 
members (exp) as input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BrierScore(obs, pred, thresholds = seq(0, 1, 0.1))

.BrierScore(exp, obs, thresholds = seq(0, 1, 0.1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BrierScore_+3A_obs">obs</code></td>
<td>
<p>Vector of binary observations (1 or 0).</p>
</td></tr>
<tr><td><code id="BrierScore_+3A_pred">pred</code></td>
<td>
<p>Vector of probablistic predictions with values in the range [0,1].</p>
</td></tr>
<tr><td><code id="BrierScore_+3A_thresholds">thresholds</code></td>
<td>
<p>Values used to bin the forecasts. By default the bins are 
[0,0.1), [0.1, 0.2), ... [0.9, 1].</p>
</td></tr>
<tr><td><code id="BrierScore_+3A_exp">exp</code></td>
<td>
<p>Matrix of predictions with values in the range [0,1] for the 
.BrierScore function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Both BrierScore and .Brier score provide the same outputs:
</p>
 
<ul>
<li><p>$relstandard reliability
</p>
</li>
<li><p>$resstandard resolution
</p>
</li>
<li><p>$uncstandard uncertainty  
</p>
</li>
<li><p>$bsBrier score
</p>
</li>
<li><p>$bs_check_resrel-res+unc
</p>
</li>
<li><p>$bss_resres-rel/unc
</p>
</li>
<li><p>$gresgeneralized resolution
</p>
</li>
<li><p>$bs_check_gresrel-gres+unc
</p>
</li>
<li><p>$bss_gresgres-rel/unc
</p>
</li>
<li><p>$rel_bias_correctedbias-corrected rel
</p>
</li>
<li><p>$gres_bias_correctedbias-corrected gres
</p>
</li>
<li><p>$unc_bias_correctedbias-corrected unc
</p>
</li>
<li><p>$bss_bias_correctedgres_bias_corrected-rel_bias_corrected/unc_bias_corrected
</p>
</li>
<li><p>$nknumber of forecast in each bin
</p>
</li>
<li><p>$fkbaraverage probability of each bin
</p>
</li>
<li><p>$okbarrelative frequency that the observed event occurred
</p>
</li>
<li><p>$binsbins used
</p>
</li>
<li><p>$predvalues with which the forecasts are verified
</p>
</li>
<li><p>$obsprobability forecasts of the event
</p>
</li></ul>



<h3>Author(s)</h3>

<p>History:<br />
0.1 - 2012-04 (L. Rodrigues) - Original code<br />
0.2 - 2017-02 (A. Hunter) - Adapted to veriApply()
</p>


<h3>References</h3>

<p>Wilks (2006) Statistical Methods in the Atmospheric Sciences.<br />
Stephenson et al. (2008). Two extra components in the Brier score decomposition. 
Weather and Forecasting, 23: 752-757.<br />
Ferro and Fricker (2012). A bias-corrected decomposition of the BS. 
Quarterly Journal of the Royal Meteorological Society, DOI: 10.1002/qj.1924.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Minimalist examples with BrierScore
a &lt;- runif(10)
b &lt;- round(a)
x &lt;- BrierScore(b, a)
x$bs - x$bs_check_res
x$bs - x$bs_check_gres
x$rel_bias_corrected - x$gres_bias_corrected + x$unc_bias_corrected
 ## Not run: 
a &lt;- runif(10)
b &lt;- cbind(round(a),round(a)) # matrix containing 2 identical ensemble members...
x2 &lt;- BrierScore(a, b)
 
## End(Not run)

# Example of BrierScore using UltimateBrier
# See ?UltimateBrier for more information
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
bs &lt;- UltimateBrier(ano_exp, ano_obs, thr = c(1/3, 2/3))

 ## Not run: 
# Example of .BrierScore with veriApply
require(easyVerification)
BrierScore2 &lt;- s2dverification:::.BrierScore
bins_ano_exp &lt;- ProbBins(ano_exp, thr = c(1/3, 2/3), posdates = 3, posdim = 2)
bins_ano_obs &lt;- ProbBins(ano_obs, thr = c(1/3, 2/3), posdates = 3, posdim = 2)
bs2 &lt;- veriApply("BrierScore2", bins_ano_exp, Mean1Dim(bins_ano_ob,s 3), 
                tdim = 2, ensdim = 3)
 
## End(Not run)
</code></pre>

<hr>
<h2 id='CDORemap'>Interpolates arrays with longitude and latitude dimensions using CDO</h2><span id='topic+CDORemap'></span>

<h3>Description</h3>

<p>This function takes as inputs a multidimensional array (optional), a vector 
or matrix of longitudes, a vector or matrix of latitudes, a destination grid 
specification, and the name of a method to be used to interpolate (one of 
those available in the 'remap' utility in CDO). The interpolated array is 
returned (if provided) together with the new longitudes and latitudes.<br /><br /> 
<code>CDORemap()</code> permutes by default the dimensions of the input array (if 
needed), splits it in chunks (CDO can work with data arrays of up to 4 
dimensions), generates a file with the data of each chunk, interpolates it 
with CDO, reads it back into R and merges it into a result array. If no 
input array is provided, the longitude and latitude vectors will be 
transformed only. If the array is already on the desired destination grid, 
no transformation is performed (this behvaiour works only for lonlat and 
gaussian grids). <br /><br />
Any metadata attached to the input data array, longitudes or latitudes will 
be preserved or accordingly modified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CDORemap(
  data_array = NULL,
  lons,
  lats,
  grid,
  method,
  avoid_writes = TRUE,
  crop = TRUE,
  force_remap = FALSE,
  write_dir = tempdir()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CDORemap_+3A_data_array">data_array</code></td>
<td>
<p>Multidimensional numeric array to be interpolated. If 
provided, it must have at least a longitude and a latitude dimensions, 
identified by the array dimension names. The names for these dimensions 
must be one of the recognized by s2dverification (can be checked with 
<code>s2dverification:::.KnownLonNames()</code> and 
<code>s2dverification:::.KnownLatNames()</code>).</p>
</td></tr>
<tr><td><code id="CDORemap_+3A_lons">lons</code></td>
<td>
<p>Numeric vector or array of longitudes of the centers of the grid 
cells. Its size must match the size of the longitude/latitude dimensions 
of the input array.</p>
</td></tr>
<tr><td><code id="CDORemap_+3A_lats">lats</code></td>
<td>
<p>Numeric vector or array of latitudes of the centers of the grid 
cells. Its size must match the size of the longitude/latitude dimensions 
of the input array.</p>
</td></tr>
<tr><td><code id="CDORemap_+3A_grid">grid</code></td>
<td>
<p>Character string specifying either a name of a target grid 
(recognized by CDO; e.g.: 'r256x128', 't106grid') or a path to another 
NetCDF file which to read the target grid from (a single grid must be 
defined in such file).</p>
</td></tr>
<tr><td><code id="CDORemap_+3A_method">method</code></td>
<td>
<p>Character string specifying an interpolation method 
(recognized by CDO; e.g.: 'con', 'bil', 'bic', 'dis'). The following 
long names are also supported: 'conservative', 'bilinear', 'bicubic' and 
'distance-weighted'.</p>
</td></tr>
<tr><td><code id="CDORemap_+3A_avoid_writes">avoid_writes</code></td>
<td>
<p>The step of permutation is needed when the input array 
has more than 3 dimensions and none of the longitude or latitude dimensions
in the right-most position (CDO would not accept it without permuting 
previously). This step, executed by default when needed, can be avoided 
for the price of writing more intermediate files (whis usually is 
unconvenient) by setting the parameter <code>avoid_writes = TRUE</code>.</p>
</td></tr>
<tr><td><code id="CDORemap_+3A_crop">crop</code></td>
<td>
<p>Whether to crop the data after interpolation with 
'cdo sellonlatbox' (TRUE) or to extend interpolated data to the whole 
world as CDO does by default (FALSE). If <code>crop = TRUE</code> then the 
longitude and latitude borders which to crop at are taken as the limits of 
the cells at the borders ('lons' and 'lats' are perceived as cell centers), 
i.e. the resulting array will contain data that covers the same area as 
the input array. This is equivalent to specifying <code>crop = 'preserve'</code>, 
i.e. preserving area. If <code>crop = 'tight'</code> then the borders which to 
crop at are taken as the minimum and maximum cell centers in 'lons' and 
'lats', i.e. the area covered by the resulting array may be smaller if 
interpolating from a coarse grid to a fine grid. The parameter 'crop' also 
accepts a numeric vector of custom borders which to crop at: 
c(western border, eastern border, southern border, northern border).</p>
</td></tr>
<tr><td><code id="CDORemap_+3A_force_remap">force_remap</code></td>
<td>
<p>Whether to force remapping, even if the input data array 
is already on the target grid.</p>
</td></tr>
<tr><td><code id="CDORemap_+3A_write_dir">write_dir</code></td>
<td>
<p>Path to the directory where to create the intermediate 
files for CDO to work. By default, the R session temporary directory is 
used (<code>tempdir()</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>'data_array'</code></td>
<td>
<p>The interpolated data array (if an input array 
is provided at all, NULL otherwise).</p>
</td></tr>
<tr><td><code>'lons'</code></td>
<td>
<p>The longitudes of the data on the destination grid.</p>
</td></tr>
<tr><td><code>'lats'</code></td>
<td>
<p>The latitudes of the data on the destination grid.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.0 - 2017-01 (N. Manubens) - Original code.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## Not run: 
# Interpolating only vectors of longitudes and latitudes
lon &lt;- seq(0, 360 - 360/50, length.out = 50)
lat &lt;- seq(-90, 90, length.out = 25)
tas2 &lt;- CDORemap(NULL, lon, lat, 't170grid', 'bil', TRUE)

# Minimal array interpolation
tas &lt;- array(1:50, dim = c(25, 50))
names(dim(tas)) &lt;- c('lat', 'lon')
lon &lt;- seq(0, 360 - 360/50, length.out = 50)
lat &lt;- seq(-90, 90, length.out = 25)
tas2 &lt;- CDORemap(tas, lon, lat, 't170grid', 'bil', TRUE)

# Metadata can be attached to the inputs. It will be preserved and 
# accordignly modified.
tas &lt;- array(1:50, dim = c(25, 50))
names(dim(tas)) &lt;- c('lat', 'lon')
lon &lt;- seq(0, 360 - 360/50, length.out = 50)
metadata &lt;- list(lon = list(units = 'degrees_east'))
attr(lon, 'variables') &lt;- metadata
lat &lt;- seq(-90, 90, length.out = 25)
metadata &lt;- list(lat = list(units = 'degrees_north'))
attr(lat, 'variables') &lt;- metadata
metadata &lt;- list(tas = list(dim = list(lat = list(len = 25,
                                                 vals = lat),
                                      lon = list(len = 50,
                                                 vals = lon)
                                     )))
attr(tas, 'variables') &lt;- metadata
tas2 &lt;- CDORemap(tas, lon, lat, 't170grid', 'bil', TRUE)

# Arrays of any number of dimensions in any order can be provided.
num_lats &lt;- 25
num_lons &lt;- 50
tas &lt;- array(1:(10*num_lats*10*num_lons*10), 
            dim = c(10, num_lats, 10, num_lons, 10))
names(dim(tas)) &lt;- c('a', 'lat', 'b', 'lon', 'c')
lon &lt;- seq(0, 360 - 360/num_lons, length.out = num_lons)
metadata &lt;- list(lon = list(units = 'degrees_east'))
attr(lon, 'variables') &lt;- metadata
lat &lt;- seq(-90, 90, length.out = num_lats)
metadata &lt;- list(lat = list(units = 'degrees_north'))
attr(lat, 'variables') &lt;- metadata
metadata &lt;- list(tas = list(dim = list(a = list(),
                                      lat = list(len = num_lats,
                                                 vals = lat),
                                      b = list(),
                                      lon = list(len = num_lons,
                                                 vals = lon),
                                      c = list()
                                     )))
attr(tas, 'variables') &lt;- metadata
tas2 &lt;- CDORemap(tas, lon, lat, 't17grid', 'bil', TRUE)
# The step of permutation can be avoided but more intermediate file writes
# will be performed.
tas2 &lt;- CDORemap(tas, lon, lat, 't17grid', 'bil', FALSE)

# If the provided array has the longitude or latitude dimension in the 
# right-most position, the same number of file writes will be performed,
# even if avoid_wrties = FALSE.
num_lats &lt;- 25
num_lons &lt;- 50
tas &lt;- array(1:(10*num_lats*10*num_lons*10), 
            dim = c(10, num_lats, 10, num_lons))
names(dim(tas)) &lt;- c('a', 'lat', 'b', 'lon')
lon &lt;- seq(0, 360 - 360/num_lons, length.out = num_lons)
metadata &lt;- list(lon = list(units = 'degrees_east'))
attr(lon, 'variables') &lt;- metadata
lat &lt;- seq(-90, 90, length.out = num_lats)
metadata &lt;- list(lat = list(units = 'degrees_north'))
attr(lat, 'variables') &lt;- metadata
metadata &lt;- list(tas = list(dim = list(a = list(),
                                      lat = list(len = num_lats,
                                                 vals = lat),
                                      b = list(),
                                      lon = list(len = num_lons,
                                                 vals = lon)
                                     )))
attr(tas, 'variables') &lt;- metadata
tas2 &lt;- CDORemap(tas, lon, lat, 't17grid', 'bil', TRUE)
tas2 &lt;- CDORemap(tas, lon, lat, 't17grid', 'bil', FALSE)

# An example of an interpolation from and onto a rectangular regular grid
num_lats &lt;- 25
num_lons &lt;- 50
tas &lt;- array(1:(1*num_lats*num_lons), dim = c(num_lats, num_lons))
names(dim(tas)) &lt;- c('y', 'x')
lon &lt;- array(seq(0, 360 - 360/num_lons, length.out = num_lons), 
            dim = c(num_lons, num_lats))
metadata &lt;- list(lon = list(units = 'degrees_east'))
names(dim(lon)) &lt;- c('x', 'y')
attr(lon, 'variables') &lt;- metadata
lat &lt;- t(array(seq(-90, 90, length.out = num_lats), 
              dim = c(num_lats, num_lons)))
metadata &lt;- list(lat = list(units = 'degrees_north'))
names(dim(lat)) &lt;- c('x', 'y')
attr(lat, 'variables') &lt;- metadata
tas2 &lt;- CDORemap(tas, lon, lat, 'r100x50', 'bil')

# An example of an interpolation from an irregular grid onto a gaussian grid
num_lats &lt;- 25
num_lons &lt;- 50
tas &lt;- array(1:(10*num_lats*10*num_lons*10), 
            dim = c(10, num_lats, 10, num_lons))
names(dim(tas)) &lt;- c('a', 'j', 'b', 'i')
lon &lt;- array(seq(0, 360 - 360/num_lons, length.out = num_lons), 
            dim = c(num_lons, num_lats))
metadata &lt;- list(lon = list(units = 'degrees_east'))
names(dim(lon)) &lt;- c('i', 'j')
attr(lon, 'variables') &lt;- metadata
lat &lt;- t(array(seq(-90, 90, length.out = num_lats), 
        dim = c(num_lats, num_lons)))
metadata &lt;- list(lat = list(units = 'degrees_north'))
names(dim(lat)) &lt;- c('i', 'j')
attr(lat, 'variables') &lt;- metadata
tas2 &lt;- CDORemap(tas, lon, lat, 't17grid', 'bil')

# Again, the dimensions can be in any order
num_lats &lt;- 25
num_lons &lt;- 50
tas &lt;- array(1:(10*num_lats*10*num_lons), 
            dim = c(10, num_lats, 10, num_lons))
names(dim(tas)) &lt;- c('a', 'j', 'b', 'i')
lon &lt;- array(seq(0, 360 - 360/num_lons, length.out = num_lons), 
            dim = c(num_lons, num_lats))
names(dim(lon)) &lt;- c('i', 'j')
lat &lt;- t(array(seq(-90, 90, length.out = num_lats), 
              dim = c(num_lats, num_lons)))
names(dim(lat)) &lt;- c('i', 'j')
tas2 &lt;- CDORemap(tas, lon, lat, 't17grid', 'bil')
tas2 &lt;- CDORemap(tas, lon, lat, 't17grid', 'bil', FALSE)
# It is ossible to specify an external NetCDF file as target grid reference
tas2 &lt;- CDORemap(tas, lon, lat, 'external_file.nc', 'bil')

## End(Not run)
</code></pre>

<hr>
<h2 id='Clim'>Computes Bias Corrected Climatologies</h2><span id='topic+Clim'></span>

<h3>Description</h3>

<p>This function computes only per-pair climatologies from the experimental 
and observational matrices output from <code>Load()</code>.
To compute plain climatologies from only experimental or observational 
data from <code>Load()</code>, the following code can be used:<br /> 
<code>clim &lt;- array(apply(obs_data, c(1, 4, 5, 6), mean),</code><br />
<code>              dim = dim(obs_datta)[-c(2, 3)])</code><br />
The function <code>Clim()</code> computes per-pair climatologies using one of the 
following methods:
</p>

<ol>
<li><p>per-pair method (Garcia-Serrano and Doblas-Reyes, CD, 2012)
</p>
</li>
<li><p>Kharin method (Karin et al, GRL, 2012)
</p>
</li>
<li><p>Fuckar method (Fuckar et al, GRL, 2014)
</p>
</li></ol>

<p><code>Clim()</code> computes climatologies using the startdates covered by the 
whole experiments/observational data sets. The startdates not available for 
all the data (model and obs) are excluded when computing the climatologies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Clim(var_exp, var_obs, memb = TRUE, kharin = FALSE, NDV = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Clim_+3A_var_exp">var_exp</code></td>
<td>
<p>Model data: c(nmod/nexp, nmemb/nparam, nsdates, nltime) up to 
c(nmod/nexp, nmemb/nparam, nsdates, nltime, nlevel, nlat, nlon).</p>
</td></tr>
<tr><td><code id="Clim_+3A_var_obs">var_obs</code></td>
<td>
<p>Observational data: c(nobs, nmemb, nsdates, nltime) up to 
c(nobs, nmemb, nsdates, nltime, nlevel, nlat, nlon).</p>
</td></tr>
<tr><td><code id="Clim_+3A_memb">memb</code></td>
<td>
<p>TRUE/FALSE (1 climatology for each member). Default = TRUE.</p>
</td></tr>
<tr><td><code id="Clim_+3A_kharin">kharin</code></td>
<td>
<p>TRUE/FALSE (if Kharin method is applied or not). 
Default = FALSE.</p>
</td></tr>
<tr><td><code id="Clim_+3A_ndv">NDV</code></td>
<td>
<p>TRUE/FALSE (if Fuckar method is applied or not). Default = FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>clim_exp</code></td>
<td>
<p>Array with same dimensions as var_exp except the third 
(starting dates) and, depending on the parameters, the second (members), 
which disappear.</p>
</td></tr>
<tr><td><code>clim_obs</code></td>
<td>
<p>Array with same dimensions as var_obs except the third 
(starting dates) and, depending on the parameters, the second (members), 
which disappear.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.9 - 2011-03 (V. Guemas) - Original code<br />
1.0 - 2013-09 (N. Manubens) - Formatting to R CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
 
PlotClim(clim$clim_exp, clim$clim_obs, 
        toptitle = paste('sea surface temperature climatologies'), 
        ytitle = 'K', monini = 11, listexp = c('CMIP5 IC3'), 
        listobs = c('ERSST'), biglab = FALSE, fileout = 'tos_clim.eps')
 
</code></pre>

<hr>
<h2 id='clim.palette'>Generate Climate Color Palettes</h2><span id='topic+clim.palette'></span><span id='topic+clim.colors'></span>

<h3>Description</h3>

<p>Generates a colorblind friendly color palette with color ranges useful in 
climate temperature variable plotting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clim.palette(palette = "bluered")

clim.colors(n, palette = "bluered")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clim.palette_+3A_palette">palette</code></td>
<td>
<p>Which type of palette to generate: from blue through white 
to red ('bluered'), from red through white to blue ('redblue'), from 
yellow through orange to red ('yellowred'), or from red through orange 
to red ('redyellow').</p>
</td></tr>
<tr><td><code id="clim.palette_+3A_n">n</code></td>
<td>
<p>Number of colors to generate.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.0  -  2016-01  (N. Manubens)  -  Original code.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lims &lt;- seq(-1, 1, length.out = 21)

ColorBar(lims, color_fun = clim.palette('redyellow'))

cols &lt;- clim.colors(20)
ColorBar(lims, cols)

</code></pre>

<hr>
<h2 id='Cluster'>K-means Clustering</h2><span id='topic+Cluster'></span>

<h3>Description</h3>

<p>This function computes cluster centers and their time series of occurrences, 
with the K-means clustering method using Euclidean distance, of an array of 
input data with any number of dimensions, one of them (the 'posdates'th) 
corresponding to time. By default the first dimension is expected to 
correspond to time. Specifically, it partitions the array along time axis in 
K groups or clusters in which each space vector/array belongs to (i.e., is a 
member of) the cluster with the nearest center or centroid. This function 
relies on the NbClust package (Charrad et al., 2014 JSS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cluster(var, weights, nclusters = NULL, index = "sdindex", posdates = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cluster_+3A_var">var</code></td>
<td>
<p>An array with any number of dimensions, one of them (the 
'posdates'th) corresponding to time with either area-averages over a 
series of domains or the grid points for any sptial grid structure (x), 
(y), (z), (x,y), (x,y,z), (y,z), ...</p>
</td></tr>
<tr><td><code id="Cluster_+3A_weights">weights</code></td>
<td>
<p>A vector/array of multiplicative weights based on the areas 
covering each domain/region or grid-cell of var; the dimensions of weights 
vector must be equal to the dimensions of 'var' without the 
'posdates'th dimension.</p>
</td></tr>
<tr><td><code id="Cluster_+3A_nclusters">nclusters</code></td>
<td>
<p>This is positive integer K that must be bigger than 1. 
K is the number of clusters to be computed, or K initial cluster centers 
to be used in the method. Default is NULL and then user has to specify 
which index from NbClust and the associated criteria for selecting the 
optimal number of clusters will be used for K-means clustering of var.</p>
</td></tr>
<tr><td><code id="Cluster_+3A_index">index</code></td>
<td>
<p>A validity index from NbClust package that can be used to 
determine optimal K if K is not specified as positive integer bigger than 
1 or initial/seed cluster centers in nclusters. 'sdindex' is deafult 
(Halkidi et al. 2001, JIIS). Other indices also available in NBClust are 
&quot;kl&quot;, &quot;ch&quot;, &quot;hartigan&quot;, &quot;ccc&quot;, &quot;scott&quot;, &quot;marriot&quot;, &quot;trcovw&quot;, &quot;tracew&quot;, 
&quot;friedman&quot;, &quot;rubin&quot;, &quot;cindex&quot;, &quot;db&quot;, &quot;silhouette&quot;, &quot;duda&quot;, &quot;pseudot2&quot;, 
&quot;beale&quot;, &quot;ratkowsky&quot;, &quot;ball&quot;, &quot;ptbiserial&quot;, &quot;gap&quot;, &quot;frey&quot;, &quot;mcclain&quot;, 
&quot;gamma&quot;, &quot;gplus&quot;, &quot;tau&quot;, &quot;dunn&quot;, &quot;hubert&quot;, &quot;sdindex&quot;, and &quot;sdbw&quot;. 
One can also use all of them with the option 'alllong' or almost all indices 
except gap, gamma, gplus and tau with 'all', when the optimal number of 
clusters K is detremined by the majority rule (the maximum of histogram of 
the results of all indices with finite solutions). Use of some indices on 
a big and/or unstructured dataset can be computationally intense and/or 
could lead to numerical singularity.</p>
</td></tr>
<tr><td><code id="Cluster_+3A_posdates">posdates</code></td>
<td>
<p>The index of the dimension that corresponds to time in the 
provided array in the parameter 'var', the first by default.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>cluster</code></td>
<td>

<p>A vector (time series) of integers indicating the occurrence 
of a cluster, i.e., when 'certain data member in time is allocated to a 
specific cluster (e.g., 2 1 3 1 1 1 ..).
</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>

<p>A matrix of cluster centres or centroids (e.g. 
[1:K, 1:spatial degrees of freedom]).
</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>

<p>The total sum of squares.
</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>

<p>A vector of within-cluster sum of squares, one component 
per cluster.
</p>
</td></tr>
<tr><td><code>tot.withinss</code></td>
<td>

<p>Total within-cluster sum of squares, 
i.e., sum(withinss).
</p>
</td></tr>
<tr><td><code>betweenss</code></td>
<td>

<p>The between-cluster sum of squares, i.e. totss-tot.withinss.
</p>
</td></tr>
<tr><td><code>size</code></td>
<td>

<p>The number of points in each cluster.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
1.0 # 2014-10 (N.S. Fuckar) - Original code
</p>


<h3>References</h3>

<p>Wilks, 2011, Statistical Methods in the Atmospheric Sciences, 3rd ed., Elsevire, pp 676.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generating synthetic data
a1 &lt;- array(dim = c(200, 4))
mean1 &lt;- 0
sd1 &lt;- 0.3 

c0 &lt;- seq(1, 200)
c1 &lt;- sort(sample(x = 1:200, size = sample(x = 50:150, size = 1), replace = FALSE))
x1 &lt;- c(1, 1, 1, 1)
for (i1 in c1) {
 a1[i1, ] &lt;- x1 + rnorm(4, mean = mean1, sd = sd1)
}

c1p5 &lt;- c0[!(c0 %in% c1)]
c2 &lt;- c1p5[seq(1, length(c1p5), 2)] 
x2 &lt;- c(2, 2, 4, 4)
for (i2 in c2) {
 a1[i2, ] &lt;- x2 + rnorm(4, mean = mean1, sd = sd1)
}

c3 &lt;- c1p5[seq(2, length(c1p5), 2)]
x3 &lt;- c(3, 3, 1, 1)
for (i3 in c3) {
 a1[i3, ] &lt;- x3 + rnorm(4, mean = mean1, sd = sd1)
}

# Computing the clusters
res1 &lt;- Cluster(var = a1, weights = array(1, dim = dim(a1)[2]), nclusters = 3)
print(res1$cluster)
print(res1$centers)

res2 &lt;- Cluster(var = a1, weights = array(1, dim = dim(a1)[2]))
print(res2$cluster)
print(res2$centers)
</code></pre>

<hr>
<h2 id='ColorBar'>Draws a Color Bar</h2><span id='topic+ColorBar'></span>

<h3>Description</h3>

<p>Generates a color bar to use as colouring function for map plots and 
optionally draws it (horizontally or vertically) to be added to map 
multipanels or plots. It is possible to draw triangles at the ends of the 
colour bar to represent values that go beyond the range of interest. A 
number of options is provided to adjust the colours and the position and 
size of the components. The drawn colour bar spans a whole figure region 
and is compatible with figure layouts.<br /><br /> 
The generated colour bar consists of a set of breaks that define the 
length(brks) - 1 intervals to classify each of the values in each of the 
grid cells of a two-dimensional field. The corresponding grid cell of a 
given value of the field will be coloured in function of the interval it 
belongs to.<br /><br />
The only mandatory parameters are 'var_limits' or 'brks' (in its second 
format, see below).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ColorBar(
  brks = NULL,
  cols = NULL,
  vertical = TRUE,
  subsampleg = NULL,
  bar_limits = NULL,
  var_limits = NULL,
  triangle_ends = NULL,
  col_inf = NULL,
  col_sup = NULL,
  color_fun = clim.palette(),
  plot = TRUE,
  draw_ticks = TRUE,
  draw_separators = FALSE,
  triangle_ends_scale = 1,
  extra_labels = NULL,
  title = NULL,
  title_scale = 1,
  label_scale = 1,
  tick_scale = 1,
  extra_margin = rep(0, 4),
  label_digits = 4,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ColorBar_+3A_brks">brks</code></td>
<td>
<p>Can be provided in two formats:
</p>

<ul>
<li><p>A single value with the number of breaks to be generated 
automatically, between the minimum and maximum specified in 'var_limits' 
(both inclusive). Hence the parameter 'var_limits' is mandatory if 'brks' 
is provided with this format. If 'bar_limits' is additionally provided, 
values only between 'bar_limits' will be generated. The higher the value 
of 'brks', the smoother the plot will look.
</p>
</li>
<li><p>A vector with the actual values of the desired breaks. Values will 
be reordered by force to ascending order. If provided in this format, no 
other parameters are required to generate/plot the colour bar.
</p>
</li></ul>

<p>This parameter is optional if 'var_limits' is specified. If 'brks' not 
specified but 'cols' is specified, it will take as value length(cols) + 1. 
If 'cols' is not specified either, 'brks' will take 21 as value.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_cols">cols</code></td>
<td>
<p>Vector of length(brks) - 1 valid colour identifiers, for each 
interval defined by the breaks. This parameter is optional and will be 
filled in with a vector of length(brks) - 1 colours generated with the 
function provided in 'color_fun' (<code>clim.colors</code> by default).<br /> 'cols' 
can have one additional colour at the beginning and/or at the end with the 
aim to colour field values beyond the range of interest represented in the 
colour bar. If any of these extra colours is provided, parameter 
'triangle_ends' becomes mandatory in order to disambiguate which of the 
ends the colours have been provided for.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_vertical">vertical</code></td>
<td>
<p>TRUE/FALSE for vertical/horizontal colour bar 
(disregarded if plot = FALSE).</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_subsampleg">subsampleg</code></td>
<td>
<p>The first of each subsampleg breaks will be ticked on the 
colorbar. Takes by default an approximation of a value that yields a 
readable tick arrangement (extreme breaks always ticked). If set to 0 or 
lower, no labels are drawn. See the code of the function for details or 
use 'extra_labels' for customized tick arrangements.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_bar_limits">bar_limits</code></td>
<td>
<p>Vector of two numeric values with the extremes of the 
range of values represented in the colour bar. If 'var_limits' go beyond 
this interval, the drawing of triangle extremes is triggered at the 
corresponding sides, painted in 'col_inf' and 'col_sup'. Either of them 
can be set as NA and will then take as value the corresponding extreme in 
'var_limits' (hence a triangle end won't be triggered for these sides). 
Takes as default the extremes of 'brks' if available, else the same values 
as 'var_limits'.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_var_limits">var_limits</code></td>
<td>
<p>Vector of two numeric values with the minimum and maximum 
values of the field to represent. These are used to know whether to draw 
triangle ends at the extremes of the colour bar and what colour to fill 
them in with. If not specified, take the same value as the extremes of 
'brks'. Hence the parameter 'brks' is mandatory if 'var_limits' is not 
specified.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_triangle_ends">triangle_ends</code></td>
<td>
<p>Vector of two logical elements, indicating whether to 
force the drawing of triangle ends at each of the extremes of the colour 
bar. This choice is automatically made from the provided 'brks', 
'bar_limits', 'var_limits', 'col_inf' and 'col_sup', but the behaviour 
can be manually forced to draw or not to draw the triangle ends with this 
parameter. If 'cols' is provided, 'col_inf' and 'col_sup' will take 
priority over 'triangle_ends' when deciding whether to draw the triangle 
ends or not.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_col_inf">col_inf</code></td>
<td>
<p>Colour to fill the inferior triangle end with. Useful if 
specifying colours manually with parameter 'cols', to specify the colour 
and to trigger the drawing of the lower extreme triangle, or if 'cols' is 
not specified, to replace the colour automatically generated by ColorBar().</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_col_sup">col_sup</code></td>
<td>
<p>Colour to fill the superior triangle end with. Useful if 
specifying colours manually with parameter 'cols', to specify the colour 
and to trigger the drawing of the upper extreme triangle, or if 'cols' is 
not specified, to replace the colour automatically generated by ColorBar().</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_color_fun">color_fun</code></td>
<td>
<p>Function to generate the colours of the color bar. Must 
take an integer and must return as many colours. The returned colour vector 
can have the attribute 'na_color', with a colour to draw NA values. This 
parameter is set by default to clim.palette().</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_plot">plot</code></td>
<td>
<p>Logical value indicating whether to only compute its breaks and 
colours (FALSE) or to also draw it on the current device (TRUE).</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_draw_ticks">draw_ticks</code></td>
<td>
<p>Whether to draw ticks for the labels along the colour bar 
(TRUE) or not (FALSE). TRUE by default. Disregarded if 'plot = FALSE'.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_draw_separators">draw_separators</code></td>
<td>
<p>Whether to draw black lines in the borders of each of 
the colour rectancles of the colour bar (TRUE) or not (FALSE). FALSE by 
default. Disregarded if 'plot = FALSE'.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_triangle_ends_scale">triangle_ends_scale</code></td>
<td>
<p>Scale factor for the drawn triangle ends of the 
colour bar, if drawn at all. Takes 1 by default (rectangle triangle 
proportional to the thickness of the colour bar). Disregarded if 
'plot = FALSE'.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_extra_labels">extra_labels</code></td>
<td>
<p>Numeric vector of extra labels to draw along axis of 
the colour bar. The number of provided decimals will be conserved. 
Disregarded if 'plot = FALSE'.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_title">title</code></td>
<td>
<p>Title to draw on top of the colour bar, most commonly with the 
units of the represented field in the neighbour figures. Empty by default.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_title_scale">title_scale</code></td>
<td>
<p>Scale factor for the 'title' of the colour bar. 
Takes 1 by default.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_label_scale">label_scale</code></td>
<td>
<p>Scale factor for the labels of the colour bar. 
Takes 1 by default.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_tick_scale">tick_scale</code></td>
<td>
<p>Scale factor for the length of the ticks of the labels 
along the colour bar. Takes 1 by default.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_extra_margin">extra_margin</code></td>
<td>
<p>Extra margins to be added around the colour bar, 
in the format c(y1, x1, y2, x2). The units are margin lines. Takes 
rep(0, 4) by default.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_label_digits">label_digits</code></td>
<td>
<p>Number of significant digits to be displayed in the 
labels of the colour bar, usually to avoid too many decimal digits 
overflowing the figure region. This does not have effect over the labels 
provided in 'extra_labels'. Takes 4 by default.</p>
</td></tr>
<tr><td><code id="ColorBar_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the following 
graphical parameters:<br /> adj ann ask bg bty cex.lab cex.main cex.sub cin 
col.axis col.lab col.main col.sub cra crt csi cxy err family fg fig fin 
font font.axis font.lab font.main font.sub lend lheight ljoin lmitre lty 
lwd mai mex mfcol mfrow mfg mkh oma omd omi page pch pin plt pty smo srt 
tck tcl usr xaxp xaxs xaxt xlog xpd yaxp yaxs yaxt ylbias ylog.<br /> For more 
information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>brks</code></td>
<td>

<p>Breaks used for splitting the range in intervals.
</p>
</td></tr>
<tr><td><code>cols</code></td>
<td>

<p>Colours generated for each of the length(brks) - 1 intervals. 
Always of length length(brks) - 1.
</p>
</td></tr>
<tr><td><code>col_inf</code></td>
<td>

<p>Colour used to draw the lower triangle end in the colour 
bar (NULL if not drawn at all).
</p>
</td></tr>
<tr><td><code>col_sup</code></td>
<td>

<p>Colour used to draw the upper triangle end in the colour 
bar (NULL if not drawn at all).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1 - 2012-04  (V. Guemas) - Original code<br />
0.2 - 2013-04  (I. Andreu-Burillo) - Vert option<br />
1.0 - 2013-09  (N. Manubens) - Formatting to CRAN<br />
1.1 - 2013-09  (C. Prodhomme) - Add cex option<br />
1.2 - 2016-08  (N. Manubens) - New ColorBar<br />
(V. Torralba)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cols &lt;- c("dodgerblue4", "dodgerblue1", "forestgreen", "yellowgreen", "white",
         "white", "yellow", "orange", "red", "saddlebrown")
lims &lt;- seq(-1, 1, 0.2)
ColorBar(lims, cols)
</code></pre>

<hr>
<h2 id='Composite'>Computes composites</h2><span id='topic+Composite'></span>

<h3>Description</h3>

<p>Composites a 3-d field var(x, y, time) according to the indices of 
mode/cluster occurrences in time and computes the pvalues (t-test). x and y 
are typically lon and lat, but function can accept other 2-d fields such as 
lat and depth, lon and depth, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Composite(var, occ, lag = 0, eno = FALSE, K = NULL, fileout = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Composite_+3A_var">var</code></td>
<td>
<p>3-dimensional array (x, y, time) containing the variable to 
composite.</p>
</td></tr>
<tr><td><code id="Composite_+3A_occ">occ</code></td>
<td>
<p>1-dimensional array for the occurrence time series of 
mode(s)/cluster(s).
(*1) When one wants to composite all modes, e.g., all K = 3 clusters then 
for example occurrences could look like: 1 1 2 3 2 3 1 3 3 2 3 2 2 3 2.
(*2) Otherwise for compositing only the 2nd mode or cluster of the above 
example occurrences should look like 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1.</p>
</td></tr>
<tr><td><code id="Composite_+3A_lag">lag</code></td>
<td>
<p>Lag time step (an integer), e.g., for lag = 2 composite will 
use +2 occurrences (i.e., shifted 2 time steps forward). Default is lag = 0.</p>
</td></tr>
<tr><td><code id="Composite_+3A_eno">eno</code></td>
<td>
<p>For using the effective sample size (TRUE) or the total sample 
size (FALSE that is the default) for the number of degrees of freedom.</p>
</td></tr>
<tr><td><code id="Composite_+3A_k">K</code></td>
<td>
<p>numeric value indicating the maximum number of composites. By default (NULL), it takes the maximum value provided in occ.</p>
</td></tr>
<tr><td><code id="Composite_+3A_fileout">fileout</code></td>
<td>
<p>Name of the .sav output file (NULL is the default).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>$composite</code></td>
<td>
 
<p>3-d array (x, y, k) containing the composites k=1,..,K for case (*1)
or only k=1 for any specific cluster, i.e., case (*2).
</p>
</td></tr>
<tr><td><code>$pvalue</code></td>
<td>

<p>3-d array (x, y, k) containing the pvalue of the 
composites obtained through a t-test that accounts for the serial
dependence of the data with the same structure as Composite.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:
0.1  #  2014-08  (N.S. Fuckar) # Original code
</p>


<h3>Examples</h3>

<pre><code class='language-R'>blank &lt;- array(0, dim=c(20, 10, 30))

x1 &lt;- blank
t1 &lt;- blank
f1 &lt;- blank

for (i in 1:20) {
 x1[i,,] &lt;- i
}

for (i in 1:30) {
 t1[,,i] &lt;- i
}

# This is 2D propagating sin wave example, where we use (x,y,t) structure of 
# f1 wave field. Compositing (like using stroboscopicc light) at different time 
# steps can lead to modification or cancelation of wave pattern.

for (i in 1:20) {
 for (j in 1:30) {
   f1[i,,j] &lt;- 3*sin(2*pi*x1[i,,j]/5. - 2*pi*t1[i,,j]/6.)
 }
}

occ1 &lt;- rep(0, 30)
occ1[c(2, 5, 8, 11, 14, 17, 20, 23)] &lt;- 1

filled.contour(Composite(var=f1, occ=occ1)$composite[,,1])

occ2 &lt;- rep(0, 30)
occ2[c(3, 9, 15, 21)] &lt;- 1

filled.contour(Composite(var=f1, occ=occ2)$composite[,,1])

# Example with one missing composite (#3) in occ:
data &lt;- 1 : (4 * 5 * 6)
dim(data) &lt;- c(lon = 4, lat = 5, case = 6)
occ &lt;- c(1, 1, 2, 2, 3, 3) 
res &lt;- Composite(data, occ, K = 4)

</code></pre>

<hr>
<h2 id='ConfigApplyMatchingEntries'>Apply Matching Entries To Dataset Name And Variable Name To Find Related Info</h2><span id='topic+ConfigApplyMatchingEntries'></span>

<h3>Description</h3>

<p>Given a pair of dataset name and variable name, this function determines 
applies all the matching entries found in the corresponding configuration 
table to work out the dataset main path, file path, actual name of variable 
inside NetCDF files, ...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ConfigApplyMatchingEntries(
  configuration,
  var,
  exp = NULL,
  obs = NULL,
  show_entries = FALSE,
  show_result = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ConfigApplyMatchingEntries_+3A_configuration">configuration</code></td>
<td>
<p>Configuration object obtained from ConfigFileOpen() 
or ConfigFileCreate().</p>
</td></tr>
<tr><td><code id="ConfigApplyMatchingEntries_+3A_var">var</code></td>
<td>
<p>Name of the variable to load. Will be interpreted as a string, 
regular expressions do not apply here.
Examples: 'tas' or 'tasmax_q90'.</p>
</td></tr>
<tr><td><code id="ConfigApplyMatchingEntries_+3A_exp">exp</code></td>
<td>
<p>Set of experimental dataset identifiers. Will be interpreted as 
a strings, regular expressions do not apply here. Can be NULL (not to 
check in experimental dataset tables), and takes by default NULL.
Examples: c('EnsEcmwfSeas', 'EnsUkmoSeas'), c('i00k').</p>
</td></tr>
<tr><td><code id="ConfigApplyMatchingEntries_+3A_obs">obs</code></td>
<td>
<p>Set of observational dataset identifiers. Will be interpreted as 
a strings, regular expressions do not apply here. Can be NULL (not to 
check in observational dataset tables), and takes by default NULL.
Examples: c('GLORYS', 'ERAint'), c('NCEP').</p>
</td></tr>
<tr><td><code id="ConfigApplyMatchingEntries_+3A_show_entries">show_entries</code></td>
<td>
<p>Flag to stipulate whether to show the found matching 
entries for all datasets and variable name.</p>
</td></tr>
<tr><td><code id="ConfigApplyMatchingEntries_+3A_show_result">show_result</code></td>
<td>
<p>Flag to stipulate whether to show the result of applying 
all the matching entries (dataset main path, file path, ...).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the information resulting of applying the matching 
entries is returned.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1 - 2015-05 (N. Manubens) - First version<br />
1.0 - 2015-11 (N. Manubens) - Removed grid column and storage types
</p>


<h3>See Also</h3>

<p>ConfigApplyMatchingEntries, ConfigEditDefinition, 
ConfigEditEntry, ConfigFileOpen, ConfigShowSimilarEntries, 
ConfigShowTable
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create an empty configuration file
config_file &lt;- paste0(tempdir(), "/example.conf")
s2dverification:::ConfigFileCreate(config_file, confirm = FALSE)
# Open it into a configuration object
configuration &lt;- ConfigFileOpen(config_file)
# Add an entry at the bottom of 4th level of file-per-startdate experiments 
# table which will associate the experiment "ExampleExperiment2" and variable 
# "ExampleVariable" to some information about its location.
configuration &lt;- ConfigAddEntry(configuration, "experiments", 
                "last", "ExampleExperiment2", "ExampleVariable", 
                "/path/to/ExampleExperiment2/", 
                "ExampleVariable/ExampleVariable_$START_DATE$.nc")
# Edit entry to generalize for any variable. Changing variable needs .
configuration &lt;- ConfigEditEntry(configuration, "experiments", 1, 
                var_name = ".*", 
                file_path = "$VAR_NAME$/$VAR_NAME$_$START_DATE$.nc")
# Now apply matching entries for variable and experiment name and show the 
# result
match_info &lt;- ConfigApplyMatchingEntries(configuration, 'tas', 
             exp = c('ExampleExperiment2'), show_result = TRUE)
</code></pre>

<hr>
<h2 id='ConfigEditDefinition'>Add Modify Or Remove Variable Definitions In Configuration</h2><span id='topic+ConfigEditDefinition'></span><span id='topic+ConfigRemoveDefinition'></span>

<h3>Description</h3>

<p>These functions help in adding, modifying or removing variable definitions 
in a configuration object obtained with <code><a href="#topic+ConfigFileOpen">ConfigFileOpen</a></code> or 
<code><a href="#topic+ConfigFileCreate">ConfigFileCreate</a></code>. ConfigEditDefinition() will add the 
definition if not existing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ConfigEditDefinition(configuration, name, value, confirm = TRUE)

ConfigRemoveDefinition(configuration, name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ConfigEditDefinition_+3A_configuration">configuration</code></td>
<td>
<p>Configuration object obtained wit ConfigFileOpen() or 
ConfigFileCreate().</p>
</td></tr>
<tr><td><code id="ConfigEditDefinition_+3A_name">name</code></td>
<td>
<p>Name of the variable to add/modify/remove.</p>
</td></tr>
<tr><td><code id="ConfigEditDefinition_+3A_value">value</code></td>
<td>
<p>Value to associate to the variable.</p>
</td></tr>
<tr><td><code id="ConfigEditDefinition_+3A_confirm">confirm</code></td>
<td>
<p>Flag to stipulate whether to ask for confirmation if the 
variable is being modified. Takes by default TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A modified configuration object is returned.
</p>


<h3>Author(s)</h3>

<p>History:
0.1 - 2015-05 (N. Manubens) - First version
</p>


<h3>See Also</h3>

<p>[ConfigApplyMatchingEntries()], [ConfigEditDefinition()], 
[ConfigEditEntry()], [ConfigFileOpen()], [ConfigShowSimilarEntries()], 
[ConfigShowTable()].
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create an empty configuration file
config_file &lt;- paste0(tempdir(), "/example.conf")
ConfigFileCreate(config_file, confirm = FALSE)
# Open it into a configuration object
configuration &lt;- ConfigFileOpen(config_file)
# Add an entry at the bottom of 4th level of file-per-startdate experiments 
# table which will associate the experiment "ExampleExperiment2" and variable 
# "ExampleVariable" to some information about its location.
configuration &lt;- ConfigAddEntry(configuration, "experiments", 
                "last", "ExampleExperiment2", "ExampleVariable", 
                "/path/to/ExampleExperiment2/", 
                "ExampleVariable/ExampleVariable_$START_DATE$.nc")
# Edit entry to generalize for any variable. Changing variable needs .
configuration &lt;- ConfigEditEntry(configuration, "experiments", 1, 
                var_name = ".*", 
                file_path = "$VAR_NAME$/$VAR_NAME$_$START_DATE$.nc")
# Now apply matching entries for variable and experiment name and show the 
# result
match_info &lt;- ConfigApplyMatchingEntries(configuration, 'tas', 
             exp = c('ExampleExperiment2'), show_result = TRUE)

</code></pre>

<hr>
<h2 id='ConfigEditEntry'>Add, Remove Or Edit Entries In The Configuration</h2><span id='topic+ConfigEditEntry'></span><span id='topic+ConfigAddEntry'></span><span id='topic+ConfigRemoveEntry'></span>

<h3>Description</h3>

<p>ConfigAddEntry(), ConfigEditEntry() and ConfigRemoveEntry() are functions 
to manage entries in a configuration object created with ConfigFileOpen().<br />
Before adding an entry, make sure the defaults don't do already what you 
want (ConfigShowDefinitions(), ConfigShowTable()).<br />
Before adding an entry, make sure it doesn't override and spoil what other 
entries do (ConfigShowTable(), ConfigFileOpen()).<br />
Before adding an entry, make sure there aren't other entries that already 
do what you want (ConfigShowSimilarEntries()).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ConfigEditEntry(
  configuration,
  dataset_type,
  position,
  dataset_name = NULL,
  var_name = NULL,
  main_path = NULL,
  file_path = NULL,
  nc_var_name = NULL,
  suffix = NULL,
  varmin = NULL,
  varmax = NULL
)

ConfigAddEntry(
  configuration,
  dataset_type,
  position = "last",
  dataset_name = ".*",
  var_name = ".*",
  main_path = "*",
  file_path = "*",
  nc_var_name = "*",
  suffix = "*",
  varmin = "*",
  varmax = "*"
)

ConfigRemoveEntry(
  configuration,
  dataset_type,
  dataset_name = NULL,
  var_name = NULL,
  position = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ConfigEditEntry_+3A_configuration">configuration</code></td>
<td>
<p>Configuration object obtained via ConfigFileOpen() 
or ConfigFileCreate() that will be modified accordingly.</p>
</td></tr>
<tr><td><code id="ConfigEditEntry_+3A_dataset_type">dataset_type</code></td>
<td>
<p>Whether to modify a table of experimental datasets or 
a table of observational datasets. Can take values 'experiments' or 
'observations' respectively.</p>
</td></tr>
<tr><td><code id="ConfigEditEntry_+3A_position">position</code></td>
<td>
<p>'position' tells the index in the table of the entry to 
edit or remove. Use ConfigShowTable() to see the index of the entry.
In ConfigAddEntry() it can also take the value &quot;last&quot; (default), that will 
put the entry at the end of the corresponding level, or &quot;first&quot; at the 
beginning. See ?ConfigFileOpen for more information.
If 'dataset_name' and 'var_name' are specified this argument is ignored in 
ConfigRemoveEntry().</p>
</td></tr>
<tr><td><code id="ConfigEditEntry_+3A_dataset_name">dataset_name</code>, <code id="ConfigEditEntry_+3A_var_name">var_name</code>, <code id="ConfigEditEntry_+3A_main_path">main_path</code>, <code id="ConfigEditEntry_+3A_file_path">file_path</code>, <code id="ConfigEditEntry_+3A_nc_var_name">nc_var_name</code>, <code id="ConfigEditEntry_+3A_suffix">suffix</code>, <code id="ConfigEditEntry_+3A_varmin">varmin</code>, <code id="ConfigEditEntry_+3A_varmax">varmax</code></td>
<td>
<p>These parameters tell the dataset name, variable name, main path, ..., of 
the entry to add, edit or remove.<br /> 'dataset_name' and 'var_name' can take 
as a value a POSIX 1003.2 regular expression (see ?ConfigFileOpen).<br />
Other parameters can take as a value a shell globbing expression 
(see ?ConfigFileOpen).<br />
'dataset_name' and 'var_name' take by default the regular expression '.*' 
(match any dataset and variable name), and the others take by default '*' 
(associate to the pair 'dataset_name' and 'var_name' all the defined 
default values. In this case '*' has a special behaviour, it won't be 
used as a shell globbing expression. See ?ConfigFileOpen and 
?ConfigShowDefinitions).<br />
'var_min' and 'var_max' must be a character string.<br />
To define these values, you can use defined variables via $VARIABLE_NAME$ 
or other entry attributes via $ATTRIBUTE_NAME$. See ?ConfigFileOpen for 
more information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an accordingly modified configuration object. 
To apply the changes in the configuration file it must be saved using 
ConfigFileSave().
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1 - 2015-05 (N. Manubens) - First version<br />
1.0  - 2015-11 (N. Manubens) - Removed grid column and storage formats
</p>


<h3>See Also</h3>

<p>ConfigApplyMatchingEntries, ConfigEditDefinition, ConfigEditEntry, 
ConfigFileOpen, ConfigShowSimilarEntries, ConfigShowTable
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create an empty configuration file
config_file &lt;- paste0(tempdir(), "/example.conf")
ConfigFileCreate(config_file, confirm = FALSE)
# Open it into a configuration object
configuration &lt;- ConfigFileOpen(config_file)
# Add an entry at the bottom of 4th level of file-per-startdate experiments 
# table which will associate the experiment "ExampleExperiment" and variable 
# "ExampleVariable" to some information about its location.
configuration &lt;- ConfigAddEntry(configuration, "experiments", 
                "last", "ExampleExperiment", "ExampleVariable", 
                "/path/to/ExampleExperiment/", 
                "ExampleVariable/ExampleVariable_$START_DATE$.nc")
# Add another entry
configuration &lt;- ConfigAddEntry(configuration, "experiments",
                "last", "ExampleExperiment2", "ExampleVariable", 
                "/path/to/ExampleExperiment2/",
                "ExampleVariable/ExampleVariable_$START_DATE$.nc")
# Edit second entry to generalize for any variable. Changing variable needs .
configuration &lt;- ConfigEditEntry(configuration, "experiments", 2, 
                var_name = ".*",
                file_path = "$VAR_NAME$/$VAR_NAME$_$START_DATE$.nc")
# Remove first entry
configuration &lt;- ConfigRemoveEntry(configuration, "experiments",
                "ExampleExperiment", "ExampleVariable")
# Show results
ConfigShowTable(configuration, "experiments")
# Save the configuration
ConfigFileSave(configuration, config_file, confirm = FALSE)
</code></pre>

<hr>
<h2 id='ConfigFileOpen'>Functions To Create Open And Save Configuration File</h2><span id='topic+ConfigFileOpen'></span><span id='topic+ConfigFileCreate'></span><span id='topic+ConfigFileSave'></span>

<h3>Description</h3>

<p>These functions help in creating, opening and saving configuration files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ConfigFileOpen(file_path, silent = FALSE, stop = FALSE)

ConfigFileCreate(file_path, confirm = TRUE)

ConfigFileSave(configuration, file_path, confirm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ConfigFileOpen_+3A_file_path">file_path</code></td>
<td>
<p>Path to the configuration file to create/open/save.</p>
</td></tr>
<tr><td><code id="ConfigFileOpen_+3A_silent">silent</code></td>
<td>
<p>Flag to activate or deactivate verbose mode.
Defaults to FALSE (verbose mode on).</p>
</td></tr>
<tr><td><code id="ConfigFileOpen_+3A_stop">stop</code></td>
<td>
<p>TRUE/FALSE whether to raise an error if not all the mandatory 
default variables are defined in the configuration file.</p>
</td></tr>
<tr><td><code id="ConfigFileOpen_+3A_confirm">confirm</code></td>
<td>
<p>Flag to stipulate whether to ask for confirmation when 
saving a configuration file that already exists.<br />
Defaults to TRUE (confirmation asked).</p>
</td></tr>
<tr><td><code id="ConfigFileOpen_+3A_configuration">configuration</code></td>
<td>
<p>Configuration object to save in a file.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>ConfigFileOpen() loads all the data contained in the configuration file 
specified as parameter 'file_path'.
Returns a configuration object with the variables needed for the 
configuration file mechanism to work.
This function is called from inside the Load() function to load the 
configuration file specified in 'configfile'.<br /><br />
ConfigFileCreate() creates an empty configuration file and saves it to 
the specified path. It may be opened later with ConfigFileOpen() to be edited.
Some default values are set when creating a file with this function, you 
can check these with ConfigShowDefinitions().<br /><br />
ConfigFileSave() saves a configuration object into a file, which may then 
be used from Load().<br /><br />
Two examples of configuration files can be found inside the 'inst/config/' 
folder in the package:
</p>

<ul>
<li><p>BSC.conf: configuration file used at BSC-CNS. Contains location 
data on several datasets and variables.
</p>
</li>
<li><p>template.conf: very simple configuration file intended to be used as 
pattern when starting from scratch.
</p>
</li></ul>

<p>How the configuration file works:<br />
~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br />
It contains one list and two tables.<br />
Each of these have a header that starts with '!!'. These are key lines and 
should not be removed or reordered.<br />
Lines starting with '#' and blank lines will be ignored.
The list should contains variable definitions and default value definitions.<br />
The first table contains information about experiments.<br />
The third table contains information about observations.<br />
Each table entry is a list of comma-separated elements.<br />
The two first are part of a key that is associated to a value formed by the 
other elements.<br />
The key elements are a dataset identifier and a variable name.<br />
The value elements are the dataset main path, dataset file path, the 
variable name inside the .nc file, a default suffix (explained below) and a 
minimum and maximum vaues beyond which loaded data is deactivated.<br />
Given a dataset name and a variable name, a full path is obtained 
concatenating the main path and the file path.<br />
Also the nc variable name, the suffixes and the limit values are obtained.<br />
Any of the elements in the keys can contain regular expressions[1] that will 
cause matching for sets of dataset names or variable names.<br />
The dataset path and file path can contain shell globbing expressions[2] 
that will cause matching for sets of paths when fetching the file in the 
full path.<br />
The full path can point to an OPeNDAP URL.<br />
Any of the elements in the value can contain variables that will be replaced 
to an associated string.<br />
Variables can be defined only in the list at the top of the file. <br />
The pattern of a variable definition is<br />
VARIABLE_NAME = VARIABLE_VALUE<br />
and can be accessed from within the table values or from within the variable 
values as<br />
$VARIABLE_NAME$<br />
For example:<br />
FILE_NAME = tos.nc<br />
!!table of experiments<br />
ecmwf, tos, /path/to/dataset/, $FILE_NAME$<br />
There are some reserved variables that will offer information about the 
store frequency, the current startdate Load() is fetching, etc:<br />
$VAR_NAME$, $START_DATE$, $STORE_FREQ$, $MEMBER_NUMBER$<br />
for experiments only: $EXP_NAME$<br />
for observations only: $OBS_NAME$, $YEAR$, $MONTH$, $DAY$<br />
Additionally, from an element in an entry value you can access the other 
elements of the entry as:<br />
$EXP_MAIN_PATH$, $EXP_FILE_PATH$, <br />$VAR_NAME$, $SUFFIX$, $VAR_MIN$, $VAR_MAX$<br />
<br />
The variable $SUFFIX$ is useful because it can be used to take part in the 
main or file path. For example: '/path/to$SUFFIX$/dataset/'.<br />
It will be replaced by the value in the column that corresponds to the 
suffix unless the user specifies a different suffix via the parameter 
'suffixexp' or 'suffixobs'.<br />
This way the user is able to load two variables with the same name in the 
same dataset but with slight modifications, with a suffix anywhere in the 
path to the data that advices of this slight modification.<br /><br />
The entries in a table will be grouped in 4 levels of specificity:
</p>

<ol>
<li>
<p>General entries:<br />
- the key dataset name and variable name are both a regular expression 
matching any sequence of characters (.*) that will cause matching for any 
pair of dataset and variable names<br />
Example:  .*, .*, /dataset/main/path/, file/path, nc_var_name, suffix, 
var_min, var_max

</p>
</li>
<li>
<p>Dataset entries:<br />
- the key variable name matches any sequence of characters<br />
Example:  ecmwf, .*, /dataset/main/path/, file/path, nc_var_name, 
suffix, var_min, var_max

</p>
</li>
<li>
<p>Variable entries:<br />
- the key dataset name matches any sequence of characters<br />
Example:  .*, tos, /dataset/main/path/, file/path, nc_var_name, 
suffix, var_min, var_max

</p>
</li>
<li>
<p>Specific entries:<br />
- both key values are specified<br />
Example:  ecmwf, tos, /dataset/main/path/, file/path, nc_var_name, 
suffix, var_min, var_max

</p>
</li></ol>

<p>Given a pair of dataset name and variable name for which we want to know the 
full path, all the rules that match will be applied from more general to 
more specific.<br />
If there is more than one entry per group that match a given key pair, 
these will be applied in the order of appearance in the configuration file 
(top to bottom).<br /><br />
An asterisk (*) in any value element will be interpreted as 'leave it as is 
or take the default value if yet not defined'.<br />
The default values are defined in the following reserved variables:<br />
$DEFAULT_EXP_MAIN_PATH$, $DEFAULT_EXP_FILE_PATH$, $DEFAULT_NC_VAR_NAME$, 
$DEFAULT_OBS_MAIN_PATH$, $DEFAULT_OBS_FILE_PATH$, $DEFAULT_SUFFIX$, 
$DEFAULT_VAR_MIN$, $DEFAULT_VAR_MAX$, <br />
$DEFAULT_DIM_NAME_LATITUDES$, $DEFAULT_DIM_NAME_LONGITUDES$, <br />
$DEFAULT_DIM_NAME_MEMBERS$<br /><br />
Trailing asterisks in an entry are not mandatory. For example<br />
ecmwf, .*, /dataset/main/path/, *, *, *, *, *<br />
will have the same effect as<br />
ecmwf, .*, /dataset/main/path/ <br /><br />
A double quote only (&quot;) in any key or value element will be interpreted as 
'fill in with the same value as the entry above'.
</p>


<h3>Value</h3>

<p>ConfigFileOpen() returns a configuration object with all the information for 
the configuration file mechanism to work.<br />
ConfigFileSave() returns TRUE if the file has been saved and FALSE otherwise.<br />
ConfigFileCreate() returns nothing.
</p>


<h3>Author(s)</h3>

<p>History:
0.1 - 2015-05 (N. Manubens) - First version
1.0 - 2015-11 (N. Manubens) - Removed grid column and storage formats
</p>


<h3>References</h3>

<p>[1] <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html">https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html</a><br />
[2] <a href="https://tldp.org/LDP/abs/html/globbingref.html">https://tldp.org/LDP/abs/html/globbingref.html</a>
</p>


<h3>See Also</h3>

<p>ConfigApplyMatchingEntries, ConfigEditDefinition, 
ConfigEditEntry, ConfigFileOpen, ConfigShowSimilarEntries, ConfigShowTable
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create an empty configuration file
config_file &lt;- paste0(tempdir(), "/example.conf")
ConfigFileCreate(config_file, confirm = FALSE)
# Open it into a configuration object
configuration &lt;- ConfigFileOpen(config_file)
# Add an entry at the bottom of 4th level of file-per-startdate experiments 
# table which will associate the experiment "ExampleExperiment2" and variable 
# "ExampleVariable" to some information about its location.
configuration &lt;- ConfigAddEntry(configuration, "experiments", 
                "last", "ExampleExperiment2", "ExampleVariable", 
                "/path/to/ExampleExperiment2/", 
                "ExampleVariable/ExampleVariable_$START_DATE$.nc")
# Edit entry to generalize for any variable. Changing variable needs .
configuration &lt;- ConfigEditEntry(configuration, "experiments", 1, 
                var_name = ".*", 
                file_path = "$VAR_NAME$/$VAR_NAME$_$START_DATE$.nc")
# Now apply matching entries for variable and experiment name and show the 
# result
match_info &lt;- ConfigApplyMatchingEntries(configuration, 'tas', 
             exp = c('ExampleExperiment2'), show_result = TRUE)
# Finally save the configuration file.
ConfigFileSave(configuration, config_file, confirm = FALSE)

</code></pre>

<hr>
<h2 id='ConfigShowSimilarEntries'>Find Similar Entries In Tables Of Datasets</h2><span id='topic+ConfigShowSimilarEntries'></span>

<h3>Description</h3>

<p>These functions help in finding similar entries in tables of supported 
datasets by comparing all entries with some given information.<br />
This is useful when dealing with complex configuration files and not sure 
if already support certain variables or datasets.<br />
At least one field must be provided in ConfigShowSimilarEntries(). 
Other fields can be unspecified and won't be taken into account. If more 
than one field is provided, sameness is avreaged over all provided fields 
and entries are sorted from higher average to lower.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ConfigShowSimilarEntries(
  configuration,
  dataset_name = NULL,
  var_name = NULL,
  main_path = NULL,
  file_path = NULL,
  nc_var_name = NULL,
  suffix = NULL,
  varmin = NULL,
  varmax = NULL,
  n_results = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ConfigShowSimilarEntries_+3A_configuration">configuration</code></td>
<td>
<p>Configuration object obtained either from 
ConfigFileCreate() or ConfigFileOpen().</p>
</td></tr>
<tr><td><code id="ConfigShowSimilarEntries_+3A_dataset_name">dataset_name</code></td>
<td>
<p>Optional dataset name to look for similars of.</p>
</td></tr>
<tr><td><code id="ConfigShowSimilarEntries_+3A_var_name">var_name</code></td>
<td>
<p>Optional variable name to look for similars of.</p>
</td></tr>
<tr><td><code id="ConfigShowSimilarEntries_+3A_main_path">main_path</code></td>
<td>
<p>Optional main path to look for similars of.</p>
</td></tr>
<tr><td><code id="ConfigShowSimilarEntries_+3A_file_path">file_path</code></td>
<td>
<p>Optional file path to look for similars of.</p>
</td></tr>
<tr><td><code id="ConfigShowSimilarEntries_+3A_nc_var_name">nc_var_name</code></td>
<td>
<p>Optional variable name inside NetCDF file to look for similars of.</p>
</td></tr>
<tr><td><code id="ConfigShowSimilarEntries_+3A_suffix">suffix</code></td>
<td>
<p>Optional suffix to look for similars of.</p>
</td></tr>
<tr><td><code id="ConfigShowSimilarEntries_+3A_varmin">varmin</code></td>
<td>
<p>Optional variable minimum to look for similars of.</p>
</td></tr>
<tr><td><code id="ConfigShowSimilarEntries_+3A_varmax">varmax</code></td>
<td>
<p>Optional variable maximum to look for similars of.</p>
</td></tr>
<tr><td><code id="ConfigShowSimilarEntries_+3A_n_results">n_results</code></td>
<td>
<p>Top 'n_results' alike results will be shown only. Defaults 
to 10 in ConfigShowSimilarEntries() and to 5 in ConfigShowSimilarVars().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sameness is calculated with string distances as specified by Simon White 
in [1].
</p>


<h3>Value</h3>

<p>These functions return information about the found matches.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1 - 2015-05 (N. Manubens) - First version<br />
1.0 - 2015-11 (N. Manubens) - Removed grid column and storage formats
</p>


<h3>References</h3>

<p>[1] Simon White, string seamness: 
<a href="http://www.catalysoft.com/articles/StrikeAMatch.html">http://www.catalysoft.com/articles/StrikeAMatch.html</a>
</p>


<h3>See Also</h3>

<p>ConfigApplyMatchingEntries, ConfigEditDefinition, 
ConfigEditEntry, ConfigFileOpen, ConfigShowSimilarEntries, ConfigShowTable
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create an empty configuration file
config_file &lt;- paste0(tempdir(), "/example.conf")
ConfigFileCreate(config_file, confirm = FALSE)
# Open it into a configuration object
configuration &lt;- ConfigFileOpen(config_file)
# Add an entry at the bottom of 4th level of file-per-startdate experiments 
# table which will associate the experiment "ExampleExperiment2" and variable 
# "ExampleVariable" to some information about its location.
configuration &lt;- ConfigAddEntry(configuration, "experiments", "last", 
                "ExampleExperiment2", "ExampleVariable", 
                "/path/to/ExampleExperiment2/", 
                "ExampleVariable/ExampleVariable_$START_DATE$.nc")
# Edit entry to generalize for any variable. Changing variable needs .
configuration &lt;- ConfigEditEntry(configuration, "experiments", 1, 
                var_name = "Var.*", 
                file_path = "$VAR_NAME$/$VAR_NAME$_$START_DATE$.nc")
# Look for similar entries
ConfigShowSimilarEntries(configuration, dataset_name = "Exper", 
                        var_name = "Vari")

</code></pre>

<hr>
<h2 id='ConfigShowTable'>Show Configuration Tables And Definitions</h2><span id='topic+ConfigShowTable'></span><span id='topic+ConfigShowDefinitions'></span>

<h3>Description</h3>

<p>These functions show the tables of supported datasets and definitions in a 
configuration object obtained via ConfigFileCreate() or ConfigFileOpen().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ConfigShowTable(configuration, dataset_type, line_numbers = NULL)

ConfigShowDefinitions(configuration)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ConfigShowTable_+3A_configuration">configuration</code></td>
<td>
<p>Configuration object obtained from ConfigFileCreate() 
or ConfigFileOpen().</p>
</td></tr>
<tr><td><code id="ConfigShowTable_+3A_dataset_type">dataset_type</code></td>
<td>
<p>In ConfigShowTable(), 'dataset_type' tells whether the 
table to show is of experimental datasets or of observational datasets.
Can take values 'experiments' or 'observations'.</p>
</td></tr>
<tr><td><code id="ConfigShowTable_+3A_line_numbers">line_numbers</code></td>
<td>
<p>'line_numbers' is an optional vector of numbers as long 
as the number of entries in the specified table. Intended for internal use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>These functions return nothing.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2015-05  (N. Manubens)  -  First version<br />
1.0  -  2015-11  (N. Manubens)  -  Removed grid column and storage formats
</p>


<h3>See Also</h3>

<p>[ConfigApplyMatchingEntries()], [ConfigEditDefinition()], 
[ConfigEditEntry()], [ConfigFileOpen()], [ConfigShowSimilarEntries()], 
[ConfigShowTable()].
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create an empty configuration file
config_file &lt;- paste0(tempdir(), "/example.conf")
ConfigFileCreate(config_file, confirm = FALSE)
# Open it into a configuration object
configuration &lt;- ConfigFileOpen(config_file)
# Add an entry at the bottom of 4th level of file-per-startdate experiments 
# table which will associate the experiment "ExampleExperiment2" and variable 
# "ExampleVariable" to some information about its location.
configuration &lt;- ConfigAddEntry(configuration, "experiments", "last", 
                "ExampleExperiment2", "ExampleVariable", 
                "/path/to/ExampleExperiment2/", 
                "ExampleVariable/ExampleVariable_$START_DATE$.nc")
# Edit entry to generalize for any variable. Changing variable needs .
configuration &lt;- ConfigEditEntry(configuration, "experiments", 1, 
                var_name = ".*", 
                file_path = "$VAR_NAME$/$VAR_NAME$_$START_DATE$.nc")
# Show tables, lists and definitions
ConfigShowTable(configuration, 'experiments')
ConfigShowDefinitions(configuration)

</code></pre>

<hr>
<h2 id='Consist_Trend'>Computes Trends Using Only Model Data For Which Observations Are Available</h2><span id='topic+Consist_Trend'></span>

<h3>Description</h3>

<p>Computes the trend coefficients for a time series by least square fitting, 
together with the associated error interval for both the observational and 
model data.<br />
Provides also the detrended observational and modeled data.<br />
By default, the trend is computed along the second dimension of the input 
array, which is expected to be the start date dimension. For arrays 
containing multiple model members, the user will first have to calculate 
the ensemble average using <code>Mean1Dim()</code> or elsewhise (see the example).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Consist_Trend(var_exp, var_obs, interval = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Consist_Trend_+3A_var_exp">var_exp</code></td>
<td>
<p>Ensemble mean of model hindcasts with dimensions:<br />
c(nmod/nexp, nsdates, nltime) up to<br />
c(nmod/nexp, nsdates, nltime, nlevel, nlat, nlon)</p>
</td></tr>
<tr><td><code id="Consist_Trend_+3A_var_obs">var_obs</code></td>
<td>
<p>Ensemble mean of observational data with dimensions:<br />
c(nobs, nsdates, nltime) up to<br />
c(nobs, nsdates, nltime, nlevel, nlat, nlon)<br />
Dimensions 2 to 6 should be the same as var_exp.</p>
</td></tr>
<tr><td><code id="Consist_Trend_+3A_interval">interval</code></td>
<td>
<p>Number of months/years between 2 start dates. Default = 1. 
The trends will be provided respectively in field unit per month or per year.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>$trend</code></td>
<td>

<p>Trend coefficients of model and observational data with dimensions:<br />
c(nmod/nexp + nobs, 3, nltime) up to<br />
c(nmod/nexp + nobs, 3, nltime, nlevel, nlat, nlon)<br />
The length 3 dimension corresponds to the lower limit of the 95% 
confidence interval, the slope of the trends and the upper limit of the 
95% confidence interval.
</p>
</td></tr>
<tr><td><code>$detrendedmod</code></td>
<td>

<p>Same dimensions as var_exp with linearly detrended values of var_exp 
along the second = start date dimension.
</p>
</td></tr>
<tr><td><code>$detrendedobs</code></td>
<td>

<p>Same dimensions as var_exp with linearly detrended values of var_obs 
along the second = start date dimension.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-11  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#'# Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
runmean_months &lt;- 12
dim_to_smooth &lt;- 4  # Smooth along lead-times
smooth_ano_exp &lt;- Smoothing(ano_exp, runmean_months, dim_to_smooth)
smooth_ano_obs &lt;- Smoothing(ano_obs, runmean_months, dim_to_smooth)
dim_to_mean &lt;- 2  # Mean along members
years_between_startdates &lt;- 5
trend &lt;- Consist_Trend(Mean1Dim(smooth_ano_exp, dim_to_mean), 
                      Mean1Dim(smooth_ano_obs, dim_to_mean), 
                      years_between_startdates)

 
PlotVsLTime(trend$trend, toptitle = "trend", ytitle = "K/(5 years)", 
           monini = 11, limits = c(-0.8, 0.8), listexp = c('CMIP5 IC3'), 
           listobs = c('ERSST'), biglab = FALSE, hlines = c(0), 
           fileout = 'tos_consist_trend.eps')
PlotAno(InsertDim(trend$detrendedmod,2,1), InsertDim(trend$detrendedobs,2,1), 
       startDates, "Detrended tos anomalies", ytitle = 'K', 
       legends = 'ERSST', biglab = FALSE, fileout = 'tos_detrended_ano.eps')
 

</code></pre>

<hr>
<h2 id='Corr'>Computes the correlation coefficient between an array of forecasts and their corresponding observations</h2><span id='topic+Corr'></span><span id='topic+.Corr'></span>

<h3>Description</h3>

<p>Calculates the correlation coefficient (Pearson, Kendall or Spearman) for 
an array of forecasts and observations. The input should be an array with 
dimensions c(no. of datasets, no. of start dates, no. of forecast times, 
no. of lons, no. of lats.), where the longitude and latitude dimensions are 
optional. The correlations are computed along the poscor dimension which 
should correspond to the startdate dimension. If compROW is given, the 
correlations are computed only if rows along the compROW dimension are 
complete between limits[1] and limits[2], i.e. there are no NAs between 
limits[1] and limits[2]. This option can be activated if the user wishes to 
account only for the forecasts for which observations are available at all 
leadtimes. <br /> 
Default: limits[1] = 1 and limits[2] = length(compROW dimension).<br /> 
The confidence interval is computed by a Fisher transformation.<br /> 
The significance level relies on a one-sided student-T distribution.<br /> 
We can modifiy the treshold of the test modifying siglev (default value=0.95).<br /><br /> 
.Corr calculates the correlation between the ensemble mean and the 
observations, using an N by M matrix (exp) of forecasts and a vector of 
observations (obs) as input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Corr(
  var_exp,
  var_obs,
  posloop = 1,
  poscor = 2,
  compROW = NULL,
  limits = NULL,
  siglev = 0.95,
  method = "pearson",
  conf = TRUE,
  pval = TRUE
)

.Corr(exp, obs, siglev = 0.95, method = "pearson", conf = TRUE, pval = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Corr_+3A_var_exp">var_exp</code></td>
<td>
<p>Array of experimental data.</p>
</td></tr>
<tr><td><code id="Corr_+3A_var_obs">var_obs</code></td>
<td>
<p>Array of observational data, same dimensions as var_exp 
except along posloop dimension, where the length can be nobs instead of nexp.</p>
</td></tr>
<tr><td><code id="Corr_+3A_posloop">posloop</code></td>
<td>
<p>Dimension nobs and nexp.</p>
</td></tr>
<tr><td><code id="Corr_+3A_poscor">poscor</code></td>
<td>
<p>Dimension along which correlation are to be computed (the 
dimension of the start dates).</p>
</td></tr>
<tr><td><code id="Corr_+3A_comprow">compROW</code></td>
<td>
<p>Data taken into account only if (compROW)th row is complete.
Default = NULL.</p>
</td></tr>
<tr><td><code id="Corr_+3A_limits">limits</code></td>
<td>
<p>Complete between limits[1] &amp; limits[2]. Default = NULL.</p>
</td></tr>
<tr><td><code id="Corr_+3A_siglev">siglev</code></td>
<td>
<p>Significance level. Default = 0.95.</p>
</td></tr>
<tr><td><code id="Corr_+3A_method">method</code></td>
<td>
<p>Type of correlation: 'pearson', 'spearman' or 'kendall'. 
Default='pearson'</p>
</td></tr>
<tr><td><code id="Corr_+3A_conf">conf</code></td>
<td>
<p>Whether to compute confidence intervals (default = 'TRUE') or 
not (FALSE).</p>
</td></tr>
<tr><td><code id="Corr_+3A_pval">pval</code></td>
<td>
<p>Whether to compute statistical significance p-value (default = 'TRUE') 
or not (FALSE).</p>
</td></tr>
<tr><td><code id="Corr_+3A_exp">exp</code></td>
<td>
<p>N by M matrix of N forecasts from M ensemble members.</p>
</td></tr>
<tr><td><code id="Corr_+3A_obs">obs</code></td>
<td>
<p>Vector of the corresponding observations of length N.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Corr: Array with dimensions :<br /> 
c(# of datasets along posloop in var_exp, # of datasets along posloop in 
var_obs, 4, all other dimensions of var_exp &amp; var_obs except poscor).<br /> 
The third dimension, of length 4 maximum, contains to the lower limit of 
the 95% confidence interval, the correlation, the upper limit of the 95% 
confidence interval and the 95% significance level given by a one-sided 
T-test. If the p-value is disabled via <code>pval = FALSE</code>, this dimension 
will be of length 3. If the confidence intervals are disabled via 
<code>conf = FALSE</code>, this dimension will be of length 2. If both are 
disabled, this will be of length 2. <br /><br />
.Corr:
</p>

<ul>
<li><p>$corrThe correlation statistic.
</p>
</li>
<li><p>$p_valCorresponds to the p values for the <code>siglev</code>% 
(only present if <code>pval = TRUE</code>) for the correlation. 
</p>
</li>
<li><p>$conf_lowCorresponds to the upper limit of the <code>siglev</code>% 
(only present if <code>conf = TRUE</code>) for the correlation. 
</p>
</li>
<li><p>$conf_highCorresponds to the lower limit of the <code>siglev</code>% 
(only present if <code>conf = TRUE</code>) for the correlation.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-04  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN<br />
1.1  -  2014-10  (M. Menegoz)  -  Adding siglev argument<br />
1.2  -  2015-03  (L.P. Caron)  - Adding method argument<br />
1.3  -  2017-02  (A. Hunter)  -  Adapted to veriApply()
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example: 
example(Load) 
clim &lt;- Clim(sampleData$mod, sampleData$obs) 
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp) 
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs) 
runmean_months &lt;- 12 
dim_to_smooth &lt;- 4  
# Smooth along lead-times   
smooth_ano_exp &lt;- Smoothing(ano_exp, runmean_months, dim_to_smooth) 
smooth_ano_obs &lt;- Smoothing(ano_obs, runmean_months, dim_to_smooth) 
dim_to_mean &lt;- 2  # Mean along members 
required_complete_row &lt;- 3  # Discard start dates which contain any NA lead-times 
leadtimes_per_startdate &lt;- 60 
corr &lt;- Corr(Mean1Dim(smooth_ano_exp, dim_to_mean),              
            Mean1Dim(smooth_ano_obs, dim_to_mean),              
            compROW = required_complete_row,              
            limits = c(ceiling((runmean_months + 1) / 2),                         
            leadtimes_per_startdate - floor(runmean_months / 2))) 
 
PlotVsLTime(corr, toptitle = "correlations", ytitle = "correlation",             
           monini = 11, limits = c(-1, 2), listexp = c('CMIP5 IC3'),
           listobs = c('ERSST'), biglab = FALSE, hlines = c(-1, 0, 1),
           fileout = 'tos_cor.eps')
 

# The following example uses veriApply combined with .Corr instead of Corr
 ## Not run: 
require(easyVerification)  
Corr2 &lt;- s2dverification:::.Corr
corr2 &lt;- veriApply("Corr2", 
                  smooth_ano_exp, 
                  # see ?veriApply for how to use the 'parallel' option
                  Mean1Dim(smooth_ano_obs, dim_to_mean), 
                  tdim = 3, ensdim = 2)
 
## End(Not run)
</code></pre>

<hr>
<h2 id='Enlarge'>Extends The Number Of Dimensions of A Matrix</h2><span id='topic+Enlarge'></span>

<h3>Description</h3>

<p>Extends the number of dimensions of var to numdims (the added dimensions 
have length 1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Enlarge(var, numdims)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Enlarge_+3A_var">var</code></td>
<td>
<p>Matrix to be extended.</p>
</td></tr>
<tr><td><code id="Enlarge_+3A_numdims">numdims</code></td>
<td>
<p>Output number of dimensions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Output number of dimensions.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-03  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN<br />
1.1  -  2015-03  (N. Manubens)  -  Improved<br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- array(1, c(2, 2, 3))
print(dim(Enlarge(data, 5)))
</code></pre>

<hr>
<h2 id='Eno'>Computes Effective Sample Size With Classical Method</h2><span id='topic+Eno'></span>

<h3>Description</h3>

<p>Computes the effective number of independent values along the posdim 
dimension of a matrix.<br />
This effective number of independent observations can be used in 
statistical/inference tests.<br />
Based on eno function from Caio Coelho from rclim.txt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Eno(obs, posdim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Eno_+3A_obs">obs</code></td>
<td>
<p>Matrix of any number of dimensions up to 10.</p>
</td></tr>
<tr><td><code id="Eno_+3A_posdim">posdim</code></td>
<td>
<p>Dimension along which to compute the effective sample size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Same dimensions as var but without the posdim dimension.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-05  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
exp &lt;- list(
        name = 'experiment',
        path = file.path(data_path, 'model/$EXP_NAME$/monthly_mean',
                         '$VAR_NAME$_3hourly/$VAR_NAME$_$START_DATES$.nc')
      )
obs &lt;- list(
        name = 'observation',
        path = file.path(data_path, 'observation/$OBS_NAME$/monthly_mean',
                         '$VAR_NAME$/$VAR_NAME$_$YEAR$$MONTH$.nc')
      )
# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(exp), list(obs), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
sampleData$mod &lt;- Season(sampleData$mod, 4, 11, 1, 12)
eno &lt;- Eno(sampleData$mod[1, 1, , 1, , ], 1)
PlotEquiMap(eno, sampleData$lon, sampleData$lat)

</code></pre>

<hr>
<h2 id='EnoNew'>Computes Effective Sample Size Following Guemas et al, BAMS, 2013b</h2><span id='topic+EnoNew'></span>

<h3>Description</h3>

<p>This function computes the effective number of independent values in the 
xdata array following the method described in 
Guemas V., Auger L., Doblas-Reyes F., JAMC, 2013. <code>EnoNew</code> provides 
similar functionality to <code>Eno</code> but with the added options to remove 
the linear trend or filter the frequency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EnoNew(xdata, detrend = FALSE, filter = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EnoNew_+3A_xdata">xdata</code></td>
<td>
<p>A numeric vector.</p>
</td></tr>
<tr><td><code id="EnoNew_+3A_detrend">detrend</code></td>
<td>
<p>Should the linear trend be removed from the data prior to 
the estimation of the equivalent number of independent values.</p>
</td></tr>
<tr><td><code id="EnoNew_+3A_filter">filter</code></td>
<td>
<p>Should a filtering of the frequency peaks be applied prior 
to the estimation of the equivalent number of independant data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-06  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>References</h3>

<p>Guemas V, Auger L, Doblas-Reyes FJ, Rust H, Ribes A, 2014, Dependencies in 
Statistical Hypothesis Tests for Climate Time Series. Bulletin of the 
American Meteorological Society, 95 (11), 1666-1667.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
exp &lt;- list(
        name = 'experiment',
        path = file.path(data_path, 'model/$EXP_NAME$/monthly_mean',
                         '$VAR_NAME$_3hourly/$VAR_NAME$_$START_DATES$.nc')
      )
obs &lt;- list(
        name = 'observation',
        path = file.path(data_path, 'observation/$OBS_NAME$/monthly_mean',
                         '$VAR_NAME$/$VAR_NAME$_$YEAR$$MONTH$.nc')
      )
# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(exp), list(obs), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
eno &lt;- EnoNew(sampleData$mod[1, 1, , 1, 2, 3])
print(eno)

</code></pre>

<hr>
<h2 id='EOF'>Area-Weighted Empirical Orthogonal Function Analysis Using SVD</h2><span id='topic+EOF'></span>

<h3>Description</h3>

<p>Performs an area-weighted EOF analysis using SVD based on a covariance matrix 
by default, based on the correlation matrix if <code>corr</code> argument is set to 
<code>TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EOF(ano, lon, lat, neofs = 15, corr = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EOF_+3A_ano">ano</code></td>
<td>
<p>Array of anomalies with dimensions (number of timesteps, 
number of latitudes, number of longitudes). NAs could exist but it should 
be consistent along time_dim. That is, if one grid point has NAs, all the 
time steps at this point should be NAs.</p>
</td></tr>
<tr><td><code id="EOF_+3A_lon">lon</code></td>
<td>
<p>Vector of longitudes of <code>ano</code>.</p>
</td></tr>
<tr><td><code id="EOF_+3A_lat">lat</code></td>
<td>
<p>Vector of latitudes of <code>ano</code>.</p>
</td></tr>
<tr><td><code id="EOF_+3A_neofs">neofs</code></td>
<td>
<p>Number of modes to be kept. Default = 15.</p>
</td></tr>
<tr><td><code id="EOF_+3A_corr">corr</code></td>
<td>
<p>Whether to base on a correlation matrix (<code>TRUE</code>) or on a 
covariance matrix (default, <code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>EOFs</code></td>
<td>

<p>An array of EOF patterns normalized to 1 (unitless) with dimensions 
(number of modes, number of latitudes, number of longitues). 
Multiplying <code>EOFs</code> by <code>PCs</code> gives the original reconstructed field.
</p>
</td></tr>
<tr><td><code>PCs</code></td>
<td>

<p>An array of pincipal components with the units of the original field to 
the power of 2, with dimensions (number of time steps, number of modes). 
<code>PCs</code> contains already the percentage of explained variance so, 
to reconstruct the original field it's only needed to multiply <code>EOFs</code> 
by <code>PCs</code>.
</p>
</td></tr> 
<tr><td><code>var</code></td>
<td>

<p>Percentage (
mode (number of modes).
</p>
</td></tr>
<tr><td><code>mask</code></td>
<td>

<p>Mask with dimensions (number of latitudes, number of longitudes).
</p>
</td></tr>
<tr><td><code>wght</code></td>
<td>

<p>Weights with dimensions (number of latitudes, number of longitudes).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-10  (F. Lienert)  -  Original
code, inspired by R. Benestad's EOF() in R package clim.pact.<br />
0.2  -  2014-03  (Lauriane Batte)  -  Bug-fixes:<br />
1- Reversion of latitudes in the weights<br />
2- Correlation matrix was used instead of covariance<br />
3- Double use of the weights<br />
0.3  -  2014-03  (Virginie Guemas)  -  Bug-fixes:<br />
1- Weight computation - division by sum of cos(lat)<br />
2- Shuffling of EOFs in EOF.2 intermediate vector<br />
3- Crash when neofs = 1 sorted out<br />
4- Crash when neofs &gt; nt sorted out<br />
0.4  -  2014-03  (Lauriane Batte)  -  Fixes:<br />
1- BIG cleanup of code and clarification<br />
2- Reduction of the number of transpositions and associated bug-fixes<br />
4- Remove of the obsolete LINPACK options<br />
0.5  -  2014-04  (Virginie Guemas)  - Fixes:<br />
1- Bug-fix in dimensions handling EOF composition restitutes now the
original field in all cases<br />
2- Simplification of the convention transpose<br />
3- Options to use the correlation matrix rather than the 
covariance matrix<br />
4- Security checks<br />
5- New normalization of PCs so that PC*EOF only reconstruct the 
original file<br />
6- Weights = sqrt(cos(lat)) for ano so that covariance matrice 
weighted by cos(lat)<br />
7- Division of EOF by weights so that the reconstruction is simply 
EOF * PC<br />
1.0  -  2016-03  (N. Manubens)  -  Formatting to R CRAN
</p>


<h3>See Also</h3>

<p>ProjectField, NAO, PlotBoxWhisker
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path, 
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path, 
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
# This example computes the EOFs along forecast horizons and plots the one that 
# explains the greatest amount of variability. The example data is very low 
# resolution so it does not make a lot of sense.
ano &lt;- Ano_CrossValid(sampleData$mod, sampleData$obs)
eof &lt;- EOF(Mean1Dim(ano$ano_exp, 2)[1, , 1, , ], sampleData$lon, sampleData$lat)
PlotEquiMap(eof$EOFs[1, , ], sampleData$lon, sampleData$lat)

</code></pre>

<hr>
<h2 id='Filter'>Filter Frequency Peaks From An Array</h2><span id='topic+Filter'></span>

<h3>Description</h3>

<p>This function filters out the selected frequency from a time series.<br /> 
The filtering is performed by dichotomy, seeking for a frequency around 
the parameter <code>freq</code> and the phase that maximizes the signal to subtract 
from the time series.<br />
The maximization of the signal to subtract relies on a minimization of the 
mean square differences between the time series (xdata) and the cosine of 
the specified frequency and phase.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Filter(xdata, freq)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Filter_+3A_xdata">xdata</code></td>
<td>
<p>Array to be filtered.</p>
</td></tr>
<tr><td><code id="Filter_+3A_freq">freq</code></td>
<td>
<p>Frequency to filter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Filtered Array.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-02  (V. Guemas)  -  Original code<br />
1.0  -  2012-02  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
ensmod &lt;- Mean1Dim(sampleData$mod, 2)
for (jstartdate in 1:3) {
 spectrum &lt;- Spectrum(ensmod[1, jstartdate, ])
 for (jlen in 1:dim(spectrum)[1]) {
   if (spectrum[jlen, 2] &gt; spectrum[jlen, 4]) {
     ensmod[1, jstartdate, ] &lt;- Filter(ensmod[1, jstartdate, ], 
                                       spectrum[jlen, 1])
   }
 }
}
 
PlotAno(InsertDim(ensmod, 2, 1), sdates = startDates, fileout =
       'filtered_ensemble_mean.eps') 
 

</code></pre>

<hr>
<h2 id='FitAcfCoef'>Fits an AR1 AutoCorrelation Function Using the Cardano Formula</h2><span id='topic+FitAcfCoef'></span>

<h3>Description</h3>

<p>This function finds the minimum point of the fourth order polynom 
(a - x)2 + 0.25(b - x2)2 written to fit the two autoregression coefficients 
a and b.<br />
A consequence of the Cardano formula is that, provided a and b are in [0 1], 
the problem is well posed, delta &gt; 0 and there is only one minimum.<br /><br />
This function is called in Alpha() to minimize the mean square differences 
between the theoretical autocorrelation function of an AR1 and the first 
guess of the estimated autocorrelation function estacf, using only the 
first two lags.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FitAcfCoef(a, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FitAcfCoef_+3A_a">a</code></td>
<td>
<p>Coefficient a : first estimate of the autocorrelation at lag 1.</p>
</td></tr>
<tr><td><code id="FitAcfCoef_+3A_b">b</code></td>
<td>
<p>Coefficient b : first estimate of the autocorrelation at lag 2.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Best estimate of the autocorrelation at lag 1.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-06  (L. Auger)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>series &lt;- GenSeries(1000, 0.35, 2, 1)
estacf &lt;- acf(series[951:1000], plot = FALSE)$acf
alpha &lt;- FitAcfCoef(max(estacf[2], 0), max(estacf[3], 0))
print(alpha)

</code></pre>

<hr>
<h2 id='FitAutocor'>Fits an AR1 Autocorrelation Function Using Dichotomy</h2><span id='topic+FitAutocor'></span>

<h3>Description</h3>

<p>This function fits the theoretical autocorrelation function of an AR1 to 
the first guess of the estimated autocorrelation function estacf containing 
any number of lags. The fitting relies on a dichotomial minimisation of the 
mean square differences between both autocorrelation functions. It returns 
the autocorrelation at lag 1 of the fitted AR1 process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FitAutocor(estacf, window = c(-1, 1), prec = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FitAutocor_+3A_estacf">estacf</code></td>
<td>
<p>First guess for the autocorrelation function.</p>
</td></tr>
<tr><td><code id="FitAutocor_+3A_window">window</code></td>
<td>
<p>Interval in which the autocorrelation at lag 1 should be found.</p>
</td></tr>
<tr><td><code id="FitAutocor_+3A_prec">prec</code></td>
<td>
<p>Precision to which the autocorrelation function at lag 1 is to be estimated.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Best estimate of the autocorrelation at lag 1.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-02  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>series &lt;- GenSeries(1000, 0.35, 2, 1)
estacf &lt;- acf(series[951:1000], plot = FALSE)$acf
alpha &lt;-  FitAutocor(estacf, c(-1, 1), 0.01)
print(alpha)
</code></pre>

<hr>
<h2 id='GenSeries'>Generates An AR1 Time Series</h2><span id='topic+GenSeries'></span>

<h3>Description</h3>

<p>This function generates AR1 processes containing n data points, where alpha 
is the autocorrelation at lag 1, and the mean and standard deviation are 
specified by the mean and std arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GenSeries(n, alpha, mean, std)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GenSeries_+3A_n">n</code></td>
<td>
<p>Length of the timeseries to be generated.</p>
</td></tr>
<tr><td><code id="GenSeries_+3A_alpha">alpha</code></td>
<td>
<p>Autocorrelation at lag 1.</p>
</td></tr>
<tr><td><code id="GenSeries_+3A_mean">mean</code></td>
<td>
<p>Mean of the data.</p>
</td></tr>
<tr><td><code id="GenSeries_+3A_std">std</code></td>
<td>
<p>Standard deviation of the data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>AR1 timeseries.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-04  (L. Auger)  -  Original code<br />
1.0  -  2012-04  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>series &lt;- GenSeries(1000, 0.35, 2, 1)
plot(series, type = 'l')

</code></pre>

<hr>
<h2 id='Histo2Hindcast'>Chunks Long Simulations For Comparison With Hindcasts</h2><span id='topic+Histo2Hindcast'></span>

<h3>Description</h3>

<p>This function reorganizes a long run (historical typically) with only one 
start date into chunks corresponding to a set of start dates. The expected 
input structure is the one output from <code>Load()</code> with 4 to 7 dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Histo2Hindcast(varin, sdatesin, sdatesout, nleadtimesout)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Histo2Hindcast_+3A_varin">varin</code></td>
<td>
<p>Array of model or observational data with dimensions:<br />
c(nmod/nexp/nobs, nmemb/nparam, nsdates, nltimes) up to<br />
c(nmod/nexp/nobs, nmemb/nparam, nsdates, nltimes, nlevel, nlat, nlon)</p>
</td></tr>
<tr><td><code id="Histo2Hindcast_+3A_sdatesin">sdatesin</code></td>
<td>
<p>Start date of the input matrix 'YYYYMMDD'.</p>
</td></tr>
<tr><td><code id="Histo2Hindcast_+3A_sdatesout">sdatesout</code></td>
<td>
<p>List of start dates of the output matrix 
c('YYYYMMDD', 'YYYYMMDD', ...).</p>
</td></tr>
<tr><td><code id="Histo2Hindcast_+3A_nleadtimesout">nleadtimesout</code></td>
<td>
<p>Number of leadtimes in the output matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An array with the same number of dimensions as varin, the same 
dimensions 1 and 2 and potentially the same dimensions 5 to 7. Dimensions 
3 and 4 are set by the arguments sdatesout and nleadtimesout.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-11  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
exp &lt;- list(
        name = 'experiment',
        path = file.path(data_path, 'model/$EXP_NAME$/monthly_mean',
                         '$VAR_NAME$_3hourly/$VAR_NAME$_$START_DATES$.nc')
      )
obs &lt;- list(
        name = 'observation',
        path = file.path(data_path, 'observation/$OBS_NAME$/monthly_mean',
                         '$VAR_NAME$/$VAR_NAME$_$YEAR$$MONTH$.nc')
      )
# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(exp), list(obs), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 


start_dates_out &lt;- c('19901101', '19911101', '19921101', '19931101', '19941101')
leadtimes_per_startdate &lt;- 12
experimental_data &lt;- Histo2Hindcast(sampleData$mod, startDates[1], 
                                   start_dates_out, leadtimes_per_startdate)
observational_data &lt;- Histo2Hindcast(sampleData$obs, startDates[1], 
                                    start_dates_out, leadtimes_per_startdate)
 
PlotAno(experimental_data, observational_data, start_dates_out, 
       toptitle = paste('anomalies reorganized into shorter chunks'), 
       ytitle = 'K', fileout='tos_histo2hindcast.eps')
 

</code></pre>

<hr>
<h2 id='IniListDims'>Creates A List Of Integer Ranges</h2><span id='topic+IniListDims'></span>

<h3>Description</h3>

<p>This function generates a list of arrays containing integers greater than or 
equal to 1. This list of arrays is used in other functions as a list of 
indices of the elements of the matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IniListDims(dims, lenlist)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IniListDims_+3A_dims">dims</code></td>
<td>
<p>The dimensions of a matrix for which we need the possible 
indices for each dimension. For exemple, if the dimensions sent are 
c(3,2,5), the following list of arrays will be generated:<br />
list(c(1:3), c(1:2), c(1:5)).</p>
</td></tr>
<tr><td><code id="IniListDims_+3A_lenlist">lenlist</code></td>
<td>
<p>'lenlist' is the length of the list because the list will 
be complemented above length(dims) by arrays of length 1.<br />
For example, if lenlist is set to 7, the previous list of arrays will be 
extended to:<br />
list(c(1:3), c(1:2), c(1:5), 1, 1, 1, 1).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with lenlist elements, each with arrays with integers from 1 
to the numbers in dims array and with only 1 for the dimensions above 
length(dims).
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-04  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN<br />
1.1  -  2015-03  (N. Manubens)  -  Improved
</p>


<h3>Examples</h3>

<pre><code class='language-R'>indices &lt;- IniListDims(c(2, 2, 4, 3), 6)
print(indices)
</code></pre>

<hr>
<h2 id='InsertDim'>Adds A Dimension To An Array</h2><span id='topic+InsertDim'></span>

<h3>Description</h3>

<p>Inserts an extra dimension into an array at position 'posdim' with length 
'lendim' and which correspond to 'lendim' repetitions of the 'var' array.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>InsertDim(var, posdim, lendim)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="InsertDim_+3A_var">var</code></td>
<td>
<p>Matrix to which a dimension should be added.</p>
</td></tr>
<tr><td><code id="InsertDim_+3A_posdim">posdim</code></td>
<td>
<p>Position of the new dimension.</p>
</td></tr>
<tr><td><code id="InsertDim_+3A_lendim">lendim</code></td>
<td>
<p>Length of the new dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix with the added dimension.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-03  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN<br />
1.1  -  2015-03  (N. Manubens)  -  Improvements
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- array(rnorm(15), dim = c(3, 1, 5, 1))
print(dim(a))
print(dim(a[, , , ]))
print(dim(InsertDim(InsertDim(a[, , , ], 2, 1), 4, 1)))

</code></pre>

<hr>
<h2 id='LeapYear'>Checks Whether A Year Is Leap Year</h2><span id='topic+LeapYear'></span>

<h3>Description</h3>

<p>This function tells whether a year is a leap year or not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LeapYear(year)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LeapYear_+3A_year">year</code></td>
<td>
<p>A numeric value indicating the year in the Gregorian calendar.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Boolean telling whether the year is a leap year or not.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-03  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>print(LeapYear(1990))
print(LeapYear(1991))
print(LeapYear(1992))
print(LeapYear(1993))
</code></pre>

<hr>
<h2 id='Load'>Loads Experimental And Observational Data</h2><span id='topic+Load'></span>

<h3>Description</h3>

<p>This function loads monthly or daily data from a set of specified 
experimental datasets together with data that date-corresponds from a set 
of specified observational datasets. See parameters 'storefreq', 
'sampleperiod', 'exp' and 'obs'.<br /><br />
A set of starting dates is specified through the parameter 'sdates'. Data of 
each starting date is loaded for each model.
<code>Load()</code> arranges the data in two arrays with a similar format both 
with the following dimensions:
</p>

<ol>
<li><p>The number of experimental datasets determined by the user through 
the argument 'exp' (for the experimental data array) or the number of 
observational datasets available for validation (for the observational 
array) determined as well by the user through the argument 'obs'.
</p>
</li>
<li><p>The greatest number of members across all experiments (in the 
experimental data array) or across all observational datasets (in the 
observational data array).
</p>
</li>
<li><p>The number of starting dates determined by the user through the 
'sdates' argument.
</p>
</li>
<li><p>The greatest number of lead-times.
</p>
</li>
<li><p>The number of latitudes of the selected zone.
</p>
</li>
<li><p>The number of longitudes of the selected zone.
</p>
</li></ol>

<p>Dimensions 5 and 6 are optional and their presence depends on the type of 
the specified variable (global mean or 2-dimensional) and on the selected 
output type (area averaged time series, latitude averaged time series, 
longitude averaged time series or 2-dimensional time series).<br />
In the case of loading an area average the dimensions of the arrays will be 
only the first 4.<br /><br />
Only a specified variable is loaded from each experiment at each starting 
date. See parameter 'var'.<br />
Afterwards, observational data that matches every starting date and lead-time 
of every experimental dataset is fetched in the file system (so, if two 
predictions at two different start dates overlap, some observational values 
will be loaded and kept in memory more than once).<br />
If no data is found in the file system for an experimental or observational 
array point it is filled with an NA value.<br /><br />
If the specified output is 2-dimensional or latitude- or longitude-averaged 
time series all the data is interpolated into a common grid. If the 
specified output type is area averaged time series the data is averaged on 
the individual grid of each dataset but can also be averaged after 
interpolating into a common grid. See parameters 'grid' and 'method'.<br />
Once the two arrays are filled by calling this function, other functions in 
the s2dverification package that receive as inputs data formatted in this 
data structure can be executed (e.g: <code>Clim()</code> to compute climatologies, 
<code>Ano()</code> to compute anomalies, ...).<br /><br />
Load() has many additional parameters to disable values and trim dimensions 
of selected variable, even masks can be applied to 2-dimensional variables. 
See parameters 'nmember', 'nmemberobs', 'nleadtime', 'leadtimemin', 
'leadtimemax', 'sampleperiod', 'lonmin', 'lonmax', 'latmin', 'latmax', 
'maskmod', 'maskobs', 'varmin', 'varmax'.<br /><br />
The parameters 'exp' and 'obs' can take various forms. The most direct form 
is a list of lists, where each sub-list has the component 'path' associated 
to a character string with a pattern of the path to the files of a dataset 
to be loaded. These patterns can contain wildcards and tags that will be 
replaced automatically by <code>Load()</code> with the specified starting dates, 
member numbers, variable name, etc.<br />
See parameter 'exp' or 'obs' for details.<br /><br />
Only NetCDF files are supported. OPeNDAP URLs to NetCDF files are also 
supported.<br />
<code>Load()</code> can load 2-dimensional or global mean variables in any of the 
following formats:
</p>

<ul>
<li><p>experiments:
</p>

<ul>
<li><p>file per ensemble per starting date 
(YYYY, MM and DD somewhere in the path)
</p>
</li>
<li><p>file per member per starting date 
(YYYY, MM, DD and MemberNumber somewhere in the path. Ensemble 
experiments with different numbers of members can be loaded in 
a single <code>Load()</code> call.)
</p>
</li></ul>

<p>(YYYY, MM and DD specify the starting dates of the predictions)

</p>
</li>
<li><p>observations:
</p>

<ul>
<li><p>file per ensemble per month 
(YYYY and MM somewhere in the path)
</p>
</li>
<li><p>file per member per month 
(YYYY, MM and MemberNumber somewhere in the path, obs with different 
numbers of members supported)
</p>
</li>
<li><p>file per dataset (No constraints in the path but the time axes 
in the file have to be properly defined)
</p>
</li></ul>

<p>(YYYY and MM correspond to the actual month data in the file)

</p>
</li></ul>

<p>In all the formats the data can be stored in a daily or monthly frequency, 
or a multiple of these (see parameters 'storefreq' and 'sampleperiod').<br />
All the data files must contain the target variable defined over time and 
potentially over members, latitude and longitude dimensions in any order, 
time being the record dimension.<br />
In the case of a two-dimensional variable, the variables longitude and 
latitude must be defined inside the data file too and must have the same 
names as the dimension for longitudes and latitudes respectively.<br />
The names of these dimensions (and longitude and latitude variables) and the 
name for the members dimension are expected to be 'longitude', 'latitude' 
and 'ensemble' respectively. However, these names can be adjusted with the 
parameter 'dimnames' or can be configured in the configuration file (read 
below in parameters 'exp', 'obs' or see <code>?ConfigFileOpen</code> 
for more information.<br />
All the data files are expected to have numeric values representable with 
32 bits. Be aware when choosing the fill values or infinite values in the 
datasets to load.<br /><br />
The Load() function returns a named list following a structure similar to 
the used in the package 'downscaleR'.<br />
The components are the following:
</p>

<ul>
<li><p>'mod' is the array that contains the experimental data. It has the 
attribute 'dimensions' associated to a vector of strings with the labels 
of each dimension of the array, in order.
</p>
</li>
<li><p>'obs' is the array that contains the observational data. It has 
the attribute 'dimensions' associated to a vector of strings with the 
labels of each dimension of the array, in order.
</p>
</li>
<li><p>'obs' is the array that contains the observational data.
</p>
</li>
<li><p>'lat' and 'lon' are the latitudes and longitudes of the grid into 
which the data is interpolated (0 if the loaded variable is a global 
mean or the output is an area average).<br />
Both have the attribute 'cdo_grid_des' associated with a character
string with the name of the common grid of the data, following the CDO 
naming conventions for grids.<br />
The attribute 'projection' is kept for compatibility with 'downscaleR'.

</p>
</li>
<li><p>'Variable' has the following components:
</p>

<ul>
<li><p>'varName', with the short name of the loaded variable as 
specified in the parameter 'var'.
</p>
</li>
<li><p>'level', with information on the pressure level of the variable. 
Is kept to NULL by now.
</p>
</li></ul>

<p>And the following attributes:
</p>

<ul>
<li><p>'is_standard', kept for compatibility with 'downscaleR', 
tells if a dataset has been homogenized to standards with 
'downscaleR' catalogs.
</p>
</li>
<li><p>'units', a character string with the units of measure of the 
variable, as found in the source files.
</p>
</li>
<li><p>'longname', a character string with the long name of the 
variable, as found in the source files.
</p>
</li>
<li><p>'daily_agg_cellfun', 'monthly_agg_cellfun', 'verification_time', 
kept for compatibility with 'downscaleR'.
</p>
</li></ul>


</li>
<li><p>'Datasets' has the following components:
</p>

<ul>
<li><p>'exp', a named list where the names are the identifying 
character strings of each experiment in 'exp', each associated to a 
list with the following components:
</p>

<ul>
<li><p>'members', a list with the names of the members of the 
dataset.
</p>
</li>
<li><p>'source', a path or URL to the source of the dataset.
</p>
</li></ul>


</li>
<li><p>'obs', similar to 'exp' but for observational datasets.
</p>
</li></ul>


</li>
<li><p>'Dates', with the follwing components:
</p>

<ul>
<li><p>'start', an array of dimensions (sdate, time) with the POSIX 
initial date of each forecast time of each starting date. 
</p>
</li>
<li><p>'end', an array of dimensions (sdate, time) with the POSIX 
final date of each forecast time of each starting date.
</p>
</li></ul>


</li>
<li><p>'InitializationDates', a vector of starting dates as specified in 
'sdates', in POSIX format.
</p>
</li>
<li><p>'when', a time stamp of the date the <code>Load()</code> call to obtain 
the data was issued.
</p>
</li>
<li><p>'source_files', a vector of character strings with complete paths 
to all the found files involved in the <code>Load()</code> call.
</p>
</li>
<li><p>'not_found_files', a vector of character strings with complete 
paths to not found files involved in the <code>Load()</code> call.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>Load(
  var,
  exp = NULL,
  obs = NULL,
  sdates,
  nmember = NULL,
  nmemberobs = NULL,
  nleadtime = NULL,
  leadtimemin = 1,
  leadtimemax = NULL,
  storefreq = "monthly",
  sampleperiod = 1,
  lonmin = 0,
  lonmax = 360,
  latmin = -90,
  latmax = 90,
  output = "areave",
  method = "conservative",
  grid = NULL,
  maskmod = vector("list", 15),
  maskobs = vector("list", 15),
  configfile = NULL,
  varmin = NULL,
  varmax = NULL,
  silent = FALSE,
  nprocs = NULL,
  dimnames = NULL,
  remapcells = 2,
  path_glob_permissive = "partial"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Load_+3A_var">var</code></td>
<td>
<p>Short name of the variable to load. It should coincide with the 
variable name inside the data files.<br />
E.g.: <code>var = 'tos'</code>, <code>var = 'tas'</code>, <code>var = 'prlr'</code>.<br />
In some cases, though, the path to the files contains twice or more times 
the short name of the variable but the actual name of the variable inside 
the data files is different. In these cases it may be convenient to provide 
<code>var</code> with the name that appears in the file paths (see details on 
parameters <code>exp</code> and <code>obs</code>).</p>
</td></tr>
<tr><td><code id="Load_+3A_exp">exp</code></td>
<td>
<p>Parameter to specify which experimental datasets to load data 
from.<br />
It can take two formats: a list of lists or a vector of character strings. 
Each format will trigger a different mechanism of locating the requested 
datasets.<br />
The first format is adequate when loading data you'll only load once or 
occasionally. The second format is targeted to avoid providing repeatedly 
the information on a certain dataset but is more complex to use.<br /><br />
IMPORTANT: Place first the experiment with the largest number of members 
and, if possible, with the largest number of leadtimes. If not possible, 
the arguments 'nmember' and/or 'nleadtime' should be filled to not miss 
any member or leadtime.<br />
If 'exp' is not specified or set to NULL, observational data is loaded for 
each start-date as far as 'leadtimemax'. If 'leadtimemax' is not provided, 
<code>Load()</code> will retrieve data of a period of time as long as the time 
period between the first specified start date and the current date.<br /><br />
List of lists:<br />
A list of lists where each sub-list contains information on the location 
and format of the data files of the dataset to load.<br />
Each sub-list can have the following components:
</p>

<ul>
<li><p>'name': A character string to identify the dataset. Optional.
</p>
</li>
<li><p>'path': A character string with the pattern of the path to the 
files of the dataset. This pattern can be built up making use of some 
special tags that <code>Load()</code> will replace with the appropriate 
values to find the dataset files. The allowed tags are $START_DATE$, 
$YEAR$, $MONTH$, $DAY$, $MEMBER_NUMBER$, $STORE_FREQ$, $VAR_NAME$, 
$EXP_NAME$ (only for experimental datasets), $OBS_NAME$ (only for 
observational datasets) and $SUFFIX$<br />
Example: /path/to/$EXP_NAME$/postprocessed/$VAR_NAME$/<br />
$VAR_NAME$_$START_DATE$.nc<br />
If 'path' is not specified and 'name' is specified, the dataset 
information will be fetched with the same mechanism as when using 
the vector of character strings (read below).

</p>
</li>
<li><p>'nc_var_name': Character string with the actual variable name 
to look for inside the dataset files. Optional. Takes, by default, 
the same value as the parameter 'var'.

</p>
</li>
<li><p>'suffix': Wildcard character string that can be used to build 
the 'path' of the dataset. It can be accessed with the tag $SUFFIX$. 
Optional. Takes &rdquo; by default.

</p>
</li>
<li><p>'var_min': Important: Character string. Minimum value beyond 
which read values will be deactivated to NA. Optional. No deactivation 
is performed by default.

</p>
</li>
<li><p>'var_max': Important: Character string. Maximum value beyond 
which read values will be deactivated to NA. Optional. No deactivation 
is performed by default.

</p>
</li></ul>

<p>The tag $START_DATES$ will be replaced with all the starting dates 
specified in 'sdates'. $YEAR$, $MONTH$ and $DAY$ will take a value for each 
iteration over 'sdates', simply these are the same as $START_DATE$ but 
split in parts.<br />
$MEMBER_NUMBER$ will be replaced by a character string with each member 
number, from 1 to the value specified in the parameter 'nmember' (in 
experimental datasets) or in 'nmemberobs' (in observational datasets). It 
will range from '01' to 'N' or '0N' if N &lt; 10.<br />
$STORE_FREQ$ will take the value specified in the parameter 'storefreq' 
('monthly' or 'daily').<br />
$VAR_NAME$ will take the value specified in the parameter 'var'.<br />
$EXP_NAME$ will take the value specified in each component of the parameter 
'exp' in the sub-component 'name'.<br />
$OBS_NAME$ will take the value specified in each component of the parameter 
'obs' in the sub-component 'obs.<br />
$SUFFIX$ will take the value specified in each component of the parameters 
'exp' and 'obs' in the sub-component 'suffix'.<br />
Example:
</p>
<pre>
list(
  list(
    name = 'experimentA',
    path = file.path('/path/to/$DATASET_NAME$/$STORE_FREQ$',
                     '$VAR_NAME$$SUFFIX$',
                     '$VAR_NAME$_$START_DATE$.nc'),
    nc_var_name = '$VAR_NAME$',
    suffix = '_3hourly',
    var_min = '-1e19',
    var_max = '1e19'
  )
)
</pre>
<p>This will make <code>Load()</code> look for, for instance, the following paths, 
if 'sdates' is c('19901101', '19951101', '20001101'):<br />
/path/to/experimentA/monthly_mean/tas_3hourly/tas_19901101.nc<br />
/path/to/experimentA/monthly_mean/tas_3hourly/tas_19951101.nc<br />
/path/to/experimentA/monthly_mean/tas_3hourly/tas_20001101.nc<br /><br />
Vector of character strings:
To avoid specifying constantly the same information to load the same 
datasets, a vector with only the names of the datasets to load can be 
specified.<br />
<code>Load()</code> will then look for the information in a configuration file 
whose path must be specified in the parameter 'configfile'.<br />
Check <code>?ConfigFileCreate</code>, <code>ConfigFileOpen</code>, 
<code>ConfigEditEntry</code> &amp; co. to learn how to create a new configuration 
file and how to add the information there.<br />
Example: c('experimentA', 'experimentB')</p>
</td></tr>
<tr><td><code id="Load_+3A_obs">obs</code></td>
<td>
<p>Argument with the same format as parameter 'exp'. See details on 
parameter 'exp'.<br />
If 'obs' is not specified or set to NULL, no observational data is loaded.<br /></p>
</td></tr>
<tr><td><code id="Load_+3A_sdates">sdates</code></td>
<td>
<p>Vector of starting dates of the experimental runs to be loaded 
following the pattern 'YYYYMMDD'.<br />
This argument is mandatory.<br />
E.g. c('19601101', '19651101', '19701101')</p>
</td></tr>
<tr><td><code id="Load_+3A_nmember">nmember</code></td>
<td>
<p>Vector with the numbers of members to load from the specified 
experimental datasets in 'exp'.<br />
If not specified, the automatically detected number of members of the 
first experimental dataset is detected and replied to all the experimental 
datasets.<br />
If a single value is specified it is replied to all the experimental 
datasets.<br />
Data for each member is fetched in the file system. If not found is 
filled with NA values.<br />
An NA value in the 'nmember' list is interpreted as &quot;fetch as many members 
of each experimental dataset as the number of members of the first 
experimental dataset&quot;.<br />
Note: It is recommended to specify the number of members of the first 
experimental dataset if it is stored in file per member format because 
there are known issues in the automatic detection of members if the path 
to the dataset in the configuration file contains Shell Globbing wildcards 
such as '*'.<br />
E.g., c(4, 9)</p>
</td></tr>
<tr><td><code id="Load_+3A_nmemberobs">nmemberobs</code></td>
<td>
<p>Vector with the numbers of members to load from the 
specified observational datasets in 'obs'.<br />
If not specified, the automatically detected number of members of the 
first observational dataset is detected and replied to all the 
observational datasets.<br />
If a single value is specified it is replied to all the observational 
datasets.<br />
Data for each member is fetched in the file system. If not found is 
filled with NA values.<br />
An NA value in the 'nmemberobs' list is interpreted as &quot;fetch as many 
members of each observational dataset as the number of members of the 
first observational dataset&quot;.<br />
Note: It is recommended to specify the number of members of the first 
observational dataset if it is stored in file per member format because 
there are known issues in the automatic detection of members if the path 
to the dataset in the configuration file contains Shell Globbing wildcards 
such as '*'.<br />
E.g., c(1, 5)</p>
</td></tr>
<tr><td><code id="Load_+3A_nleadtime">nleadtime</code></td>
<td>
<p>Deprecated. See parameter 'leadtimemax'.</p>
</td></tr>
<tr><td><code id="Load_+3A_leadtimemin">leadtimemin</code></td>
<td>
<p>Only lead-times higher or equal to 'leadtimemin' are 
loaded. Takes by default value 1.</p>
</td></tr>
<tr><td><code id="Load_+3A_leadtimemax">leadtimemax</code></td>
<td>
<p>Only lead-times lower or equal to 'leadtimemax' are loaded. 
Takes by default the number of lead-times of the first experimental 
dataset in 'exp'.<br />
If 'exp' is NULL this argument won't have any effect 
(see <code>?Load</code> description).</p>
</td></tr>
<tr><td><code id="Load_+3A_storefreq">storefreq</code></td>
<td>
<p>Frequency at which the data to be loaded is stored in the 
file system. Can take values 'monthly' or 'daily'.<br />
By default it takes 'monthly'.<br />
Note: Data stored in other frequencies with a period which is divisible by 
a month can be loaded with a proper use of 'storefreq' and 'sampleperiod' 
parameters. It can also be loaded if the period is divisible by a day and 
the observational datasets are stored in a file per dataset format or 
'obs' is empty.</p>
</td></tr>
<tr><td><code id="Load_+3A_sampleperiod">sampleperiod</code></td>
<td>
<p>To load only a subset between 'leadtimemin' and 
'leadtimemax' with the period of subsampling 'sampleperiod'.<br />
Takes by default value 1 (all lead-times are loaded).<br />
See 'storefreq' for more information.</p>
</td></tr>
<tr><td><code id="Load_+3A_lonmin">lonmin</code></td>
<td>
<p>If a 2-dimensional variable is loaded, values at longitudes 
lower than 'lonmin' aren't loaded.<br />
Must take a value in the range [-360, 360] (if negative longitudes are 
found in the data files these are translated to this range).<br />
It is set to 0 if not specified.<br />
If 'lonmin' &gt; 'lonmax', data across Greenwich is loaded.</p>
</td></tr>
<tr><td><code id="Load_+3A_lonmax">lonmax</code></td>
<td>
<p>If a 2-dimensional variable is loaded, values at longitudes 
higher than 'lonmax' aren't loaded.<br />
Must take a value in the range [-360, 360] (if negative longitudes are 
found in the data files these are translated to this range).<br />
It is set to 360 if not specified.<br />
If 'lonmin' &gt; 'lonmax', data across Greenwich is loaded.</p>
</td></tr>
<tr><td><code id="Load_+3A_latmin">latmin</code></td>
<td>
<p>If a 2-dimensional variable is loaded, values at latitudes 
lower than 'latmin' aren't loaded.<br />
Must take a value in the range [-90, 90].<br />
It is set to -90 if not specified.</p>
</td></tr>
<tr><td><code id="Load_+3A_latmax">latmax</code></td>
<td>
<p>If a 2-dimensional variable is loaded, values at latitudes 
higher than 'latmax' aren't loaded.<br />
Must take a value in the range [-90, 90].<br />
It is set to 90 if not specified.</p>
</td></tr>
<tr><td><code id="Load_+3A_output">output</code></td>
<td>
<p>This parameter determines the format in which the data is 
arranged in the output arrays.<br />
Can take values 'areave', 'lon', 'lat', 'lonlat'.<br />
</p>

<ul>
<li><p>'areave': Time series of area-averaged variables over the specified domain.
</p>
</li>
<li><p>'lon': Time series of meridional averages as a function of longitudes.
</p>
</li>
<li><p>'lat': Time series of zonal averages as a function of latitudes.
</p>
</li>
<li><p>'lonlat': Time series of 2d fields.
</p>
</li></ul>

<p>Takes by default the value 'areave'. If the variable specified in 'var' is 
a global mean, this parameter is forced to 'areave'.<br />
All the loaded data is interpolated into the grid of the first experimental 
dataset except if 'areave' is selected. In that case the area averages are 
computed on each dataset original grid. A common grid different than the 
first experiment's can be specified through the parameter 'grid'. If 'grid' 
is specified when selecting 'areave' output type, all the loaded data is 
interpolated into the specified grid before calculating the area averages.</p>
</td></tr>
<tr><td><code id="Load_+3A_method">method</code></td>
<td>
<p>This parameter determines the interpolation method to be used 
when regridding data (see 'output'). Can take values 'bilinear', 'bicubic', 
'conservative', 'distance-weighted'.<br />
See <code>remapcells</code> for advanced adjustments.<br />
Takes by default the value 'conservative'.</p>
</td></tr>
<tr><td><code id="Load_+3A_grid">grid</code></td>
<td>
<p>A common grid can be specified through the parameter 'grid' when 
loading 2-dimensional data. Data is then interpolated onto this grid 
whichever 'output' type is specified. If the selected output type is 
'areave' and a 'grid' is specified, the area averages are calculated after 
interpolating to the specified grid.<br />
If not specified and the selected output type is 'lon', 'lat' or 'lonlat', 
this parameter takes as default value the grid of the first experimental 
dataset, which is read automatically from the source files.<br />
The grid must be supported by 'cdo' tools. Now only supported: rNXxNY 
or tTRgrid.<br />
Both rNXxNY and tRESgrid yield rectangular regular grids. rNXxNY yields 
grids that are evenly spaced in longitudes and latitudes (in degrees). 
tRESgrid refers to a grid generated with series of spherical harmonics 
truncated at the RESth harmonic. However these spectral grids are usually 
associated to a gaussian grid, the latitudes of which are spaced with a 
Gaussian quadrature (not evenly spaced in degrees). The pattern tRESgrid 
will yield a gaussian grid.<br />
E.g., 'r96x72' 
Advanced: If the output type is 'lon', 'lat' or 'lonlat' and no common 
grid is specified, the grid of the first experimental or observational 
dataset is detected and all data is then interpolated onto this grid. 
If the first experimental or observational dataset's data is found shifted 
along the longitudes (i.e., there's no value at the longitude 0 but at a 
longitude close to it), the data is re-interpolated to suppress the shift. 
This has to be done in order to make sure all the data from all the 
datasets is properly aligned along longitudes, as there's no option so far 
in <code>Load</code> to specify grids starting at longitudes other than 0. 
This issue doesn't affect when loading in 'areave' mode without a common 
grid, the data is not re-interpolated in that case.</p>
</td></tr>
<tr><td><code id="Load_+3A_maskmod">maskmod</code></td>
<td>
<p>List of masks to be applied to the data of each experimental 
dataset respectively, if a 2-dimensional variable is specified in 'var'.<br />
Each mask can be defined in 2 formats:<br />
a) a matrix with dimensions c(longitudes, latitudes).<br />
b) a list with the components 'path' and, optionally, 'nc_var_name'.<br />
In the format a), the matrix must have the same size as the common grid 
or with the same size as the grid of the corresponding experimental dataset 
if 'areave' output type is specified and no common 'grid' is specified.<br />
In the format b), the component 'path' must be a character string with the 
path to a NetCDF mask file, also in the common grid or in the grid of the 
corresponding dataset if 'areave' output type is specified and no common 
'grid' is specified. If the mask file contains only a single variable, 
there's no need to specify the component 'nc_var_name'. Otherwise it must 
be a character string with the name of the variable inside the mask file 
that contains the mask values. This variable must be defined only over 2 
dimensions with length greater or equal to 1.<br />
Whichever the mask format, a value of 1 at a point of the mask keeps the 
original value at that point whereas a value of 0 disables it (replaces 
by a NA value).<br />
By default all values are kept (all ones).<br />
The longitudes and latitudes in the matrix must be in the same order as in 
the common grid or as in the original grid of the corresponding dataset 
when loading in 'areave' mode. You can find out the order of the longitudes 
and latitudes of a file with 'cdo griddes'.<br />
Note that in a common CDO grid defined with the patterns 't&lt;RES&gt;grid' or 
'r&lt;NX&gt;x&lt;NY&gt;' the latitudes and latitudes are ordered, by definition, from 
-90 to 90 and from 0 to 360, respectively.<br />
If you are loading maps ('lonlat', 'lon' or 'lat' output types) all the 
data will be interpolated onto the common 'grid'. If you want to specify 
a mask, you will have to provide it already interpolated onto the common 
grid (you may use 'cdo' libraries for this purpose). It is not usual to 
apply different masks on experimental datasets on the same grid, so all 
the experiment masks are expected to be the same.<br />
Warning: When loading maps, any masks defined for the observational data 
will be ignored to make sure the same mask is applied to the experimental 
and observational data.<br />
Warning: list() compulsory even if loading 1 experimental dataset only!<br />
E.g., list(array(1, dim = c(num_lons, num_lats)))</p>
</td></tr>
<tr><td><code id="Load_+3A_maskobs">maskobs</code></td>
<td>
<p>See help on parameter 'maskmod'.</p>
</td></tr>
<tr><td><code id="Load_+3A_configfile">configfile</code></td>
<td>
<p>Path to the s2dverification configuration file from which 
to retrieve information on location in file system (and other) of datasets.<br />
If not specified, the configuration file used at BSC-ES will be used 
(it is included in the package).<br />
Check the BSC's configuration file or a template of configuration file in 
the folder 'inst/config' in the package.<br />
Check further information on the configuration file mechanism in 
<code>ConfigFileOpen()</code>.</p>
</td></tr>
<tr><td><code id="Load_+3A_varmin">varmin</code></td>
<td>
<p>Loaded experimental and observational data values smaller 
than 'varmin' will be disabled (replaced by NA values).<br />
By default no deactivation is performed.</p>
</td></tr>
<tr><td><code id="Load_+3A_varmax">varmax</code></td>
<td>
<p>Loaded experimental and observational data values greater 
than 'varmax' will be disabled (replaced by NA values).<br />
By default no deactivation is performed.</p>
</td></tr>
<tr><td><code id="Load_+3A_silent">silent</code></td>
<td>
<p>Parameter to show (FALSE) or hide (TRUE) information messages.<br />
Warnings will be displayed even if 'silent' is set to TRUE.<br />
Takes by default the value 'FALSE'.</p>
</td></tr>
<tr><td><code id="Load_+3A_nprocs">nprocs</code></td>
<td>
<p>Number of parallel processes created to perform the fetch 
and computation of data.<br />
These processes will use shared memory in the processor in which Load() 
is launched.<br />
By default the number of logical cores in the machine will be detected 
and as many processes as logical cores there are will be created.<br />
A value of 1 won't create parallel processes.<br />
When running in multiple processes, if an error occurs in any of the 
processes, a crash message appears in the R session of the original 
process but no detail is given about the error. A value of 1 will display 
all error messages in the original and only R session.<br />
Note: the parallel process create other blocking processes each time they 
need to compute an interpolation via 'cdo'.</p>
</td></tr>
<tr><td><code id="Load_+3A_dimnames">dimnames</code></td>
<td>
<p>Named list where the name of each element is a generic 
name of the expected dimensions inside the NetCDF files. These generic 
names are 'lon', 'lat' and 'member'. 'time' is not needed because it's 
detected automatically by discard.<br />
The value associated to each name is the actual dimension name in the 
NetCDF file.<br />
The variables in the file that contain the longitudes and latitudes of 
the data (if the data is a 2-dimensional variable) must have the same 
name as the longitude and latitude dimensions.<br />
By default, these names are 'longitude', 'latitude' and 'ensemble. If any 
of those is defined in the 'dimnames' parameter, it takes priority and 
overwrites the default value.
E.g., list(lon = 'x', lat = 'y')
In that example, the dimension 'member' will take the default value 'ensemble'.</p>
</td></tr>
<tr><td><code id="Load_+3A_remapcells">remapcells</code></td>
<td>
<p>When loading a 2-dimensional variable, spatial subsets can 
be requested via <code>lonmin</code>, <code>lonmax</code>, <code>latmin</code> and 
<code>latmax</code>. When <code>Load()</code> obtains the subset it is then 
interpolated if needed with the method specified in <code>method</code>.<br />
The result of this interpolation can vary if the values surrounding the 
spatial subset are not present. To better control this process, the width 
in number of grid cells of the surrounding area to be taken into account 
can be specified with <code>remapcells</code>. A value of 0 will take into 
account no additional cells but will generate less traffic between the 
storage and the R processes that load data.<br />
A value beyond the limits in the data files will be automatically runcated 
to the actual limit.<br />
The default value is 2.</p>
</td></tr>
<tr><td><code id="Load_+3A_path_glob_permissive">path_glob_permissive</code></td>
<td>
<p>In some cases, when specifying a path pattern 
(either in the parameters 'exp'/'obs' or in a configuration file) one can 
specify path patterns that contain shell globbing expressions. Too much 
freedom in putting globbing expressions in the path patterns can be 
dangerous and make <code>Load()</code> find a file in the file system for a 
start date for a dataset that really does not belong to that dataset. 
For example, if the file system contains two directories for two different 
experiments that share a part of their path and the path pattern contains 
globbing expressions:
/experiments/model1/expA/monthly_mean/tos/tos_19901101.nc
/experiments/model2/expA/monthly_mean/tos/tos_19951101.nc
And the path pattern is used as in the example right below to load data of 
only the experiment 'expA' of the model 'model1' for the starting dates 
'19901101' and '19951101', <code>Load()</code> will undesiredly yield data for 
both starting dates, even if in fact there is data only for the 
first one:<br />
<code>
expA &lt;- list(path = file.path('/experiments/*/expA/monthly_mean/$VAR_NAME$',
                              '$VAR_NAME$_$START_DATE$.nc')
data &lt;- Load('tos', list(expA), NULL, c('19901101', '19951101'))
    </code>
To avoid these situations, the parameter <code>path_glob_permissive</code> is 
set by default to <code>'partial'</code>, which forces <code>Load()</code> to replace 
all the globbing expressions of a path pattern of a data set by fixed 
values taken from the path of the first found file for each data set, up 
to the folder right before the final files (globbing expressions in the 
file name will not be replaced, only those in the path to the file). 
Replacement of globbing expressions in the file name can also be triggered 
by setting <code>path_glob_permissive</code> to <code>FALSE</code> or <code>'no'</code>. If 
needed to keep all globbing expressions, <code>path_glob_permissive</code> can 
be set to <code>TRUE</code> or <code>'yes'</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The two output matrices have between 2 and 6 dimensions:<br />
</p>

<ol>
<li><p>Number of experimental/observational datasets.
</p>
</li>
<li><p>Number of members.
</p>
</li>
<li><p>Number of startdates.
</p>
</li>
<li><p>Number of leadtimes.
</p>
</li>
<li><p>Number of latitudes (optional).
</p>
</li>
<li><p>Number of longitudes (optional).
</p>
</li></ol>

<p>but the two matrices have the same number of dimensions and only the first 
two dimensions can have different lengths depending on the input arguments.    
For a detailed explanation of the process, read the documentation attached 
to the package or check the comments in the code.
</p>


<h3>Value</h3>

<p><code>Load()</code> returns a named list following a structure similar to the 
used in the package 'downscaleR'.<br />
The components are the following:
</p>

<ul>
<li>
<p>'mod' is the array that contains the experimental data. It has the 
attribute 'dimensions' associated to a vector of strings with the 
labels of each dimension of the array, in order. The order of the 
latitudes is always forced to be from 90 to -90 whereas the order of 
the longitudes is kept as in the original files (if possible). The 
longitude values provided in <code>lon</code> lower than 0 are added 360 
(but still kept in the original order). In some cases, however, if 
multiple data sets are loaded in longitude-latitude mode, the 
longitudes (and also the data arrays in <code>mod</code> and <code>obs</code>) are 
re-ordered afterwards by <code>Load()</code> to range from 0 to 360; a 
warning is given in such cases. The longitude and latitude of the 
center of the grid cell that corresponds to the value [j, i] in 'mod' 
(along the dimensions latitude and longitude, respectively) can be 
found in the outputs <code>lon</code>[i] and <code>lat</code>[j]

</p>
</li>
<li><p>'obs' is the array that contains the observational data. The 
same documentation of parameter 'mod' applies to this parameter.
</p>
</li>
<li><p>'lat' and 'lon' are the latitudes and longitudes of the centers of 
the cells of the grid the data is interpolated into (0 if the loaded 
variable is a global mean or the output is an area average).<br />
Both have the attribute 'cdo_grid_des' associated with a character 
string with the name of the common grid of the data, following the CDO 
naming conventions for grids.<br />
'lon' has the attributes 'first_lon' and 'last_lon', with the first 
and last longitude values found in the region defined by 'lonmin' and 
'lonmax'. 'lat' has also the equivalent attributes 'first_lat' and 
'last_lat'.<br />
'lon' has also the attribute 'data_across_gw' which tells whether the 
requested region via 'lonmin', 'lonmax', 'latmin', 'latmax' goes across 
the Greenwich meridian. As explained in the documentation of the 
parameter 'mod', the loaded data array is kept in the same order as in 
the original files when possible: this means that, in some cases, even 
if the data goes across the Greenwich, the data array may not go 
across the Greenwich. The attribute 'array_across_gw' tells whether 
the array actually goes across the Greenwich. E.g: The longitudes in 
the data files are defined to be from 0 to 360. The requested 
longitudes are from -80 to 40. The original order is kept, hence the 
longitudes in the array will be ordered as follows: 
0, ..., 40, 280, ..., 360. In that case, 'data_across_gw' will be TRUE 
and 'array_across_gw' will be FALSE.<br />
The attribute 'projection' is kept for compatibility with 'downscaleR'.

</p>
</li>
<li><p>'Variable' has the following components:
</p>

<ul>
<li><p>'varName', with the short name of the loaded variable as 
specified in the parameter 'var'.

</p>
</li>
<li><p>'level', with information on the pressure level of the 
variable. Is kept to NULL by now.

</p>
</li></ul>

<p>And the following attributes:
</p>

<ul>
<li><p>'is_standard', kept for compatibility with 'downscaleR', 
tells if a dataset has been homogenized to standards with 
'downscaleR' catalogs.

</p>
</li>
<li><p>'units', a character string with the units of measure of the 
variable, as found in the source files.

</p>
</li>
<li><p>'longname', a character string with the long name of the 
variable, as found in the source files.

</p>
</li>
<li><p>'daily_agg_cellfun', 'monthly_agg_cellfun', 
'verification_time', kept for compatibility with 'downscaleR'.

</p>
</li></ul>


</li>
<li><p>'Datasets' has the following components:
</p>

<ul>
<li><p>'exp', a named list where the names are the identifying 
character strings of each experiment in 'exp', each associated to 
a list with the following components:
</p>

<ul>
<li><p>'members', a list with the names of the members of the dataset.
</p>
</li>
<li><p>'source', a path or URL to the source of the dataset.
</p>
</li></ul>


</li>
<li><p>'obs', similar to 'exp' but for observational datasets.
</p>
</li></ul>


</li>
<li><p>'Dates', with the follwing components:
</p>

<ul>
<li><p>'start', an array of dimensions (sdate, time) with the POSIX 
initial date of each forecast time of each starting date.
 
</p>
</li>
<li><p>'end', an array of dimensions (sdate, time) with the POSIX 
final date of each forecast time of each starting date.

</p>
</li></ul>


</li>
<li><p>'InitializationDates', a vector of starting dates as specified in 
'sdates', in POSIX format.

</p>
</li>
<li><p>'when', a time stamp of the date the <code>Load()</code> call to obtain 
the data was issued.

</p>
</li>
<li><p>'source_files', a vector of character strings with complete paths 
to all the found files involved in the <code>Load()</code> call.

</p>
</li>
<li><p>'not_found_files', a vector of character strings with complete 
paths to not found files involved in the <code>Load()</code> call.

</p>
</li></ul>



<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-03  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN<br />
1.2  -  2015-02  (N. Manubens)  -  Generalisation + parallelisation<br />
1.3  -  2015-07  (N. Manubens)  -  Improvements related to configuration file mechanism<br />
1.4  -  2016-01  (N. Manubens)  -  Added subsetting capabilities
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Let's assume we want to perform verification with data of a variable
# called 'tos' from a model called 'model' and observed data coming from 
# an observational dataset called 'observation'.
#
# The model was run in the context of an experiment named 'experiment'. 
# It simulated from 1st November in 1985, 1990, 1995, 2000 and 2005 for a 
# period of 5 years time from each starting date. 5 different sets of 
# initial conditions were used so an ensemble of 5 members was generated 
# for each starting date.
# The model generated values for the variables 'tos' and 'tas' in a 
# 3-hourly frequency but, after some initial post-processing, it was 
# averaged over every month.
# The resulting monthly average series were stored in a file for each 
# starting date for each variable with the data of the 5 ensemble members.
# The resulting directory tree was the following:
#   model
#    |--&gt; experiment
#          |--&gt; monthly_mean
#                |--&gt; tos_3hourly
#                |     |--&gt; tos_19851101.nc
#                |     |--&gt; tos_19901101.nc
#                |               .
#                |               .
#                |     |--&gt; tos_20051101.nc 
#                |--&gt; tas_3hourly
#                      |--&gt; tas_19851101.nc
#                      |--&gt; tas_19901101.nc
#                                .
#                                .
#                      |--&gt; tas_20051101.nc
# 
# The observation recorded values of 'tos' and 'tas' at each day of the 
# month over that period but was also averaged over months and stored in 
# a file per month. The directory tree was the following:
#   observation
#    |--&gt; monthly_mean
#          |--&gt; tos
#          |     |--&gt; tos_198511.nc
#          |     |--&gt; tos_198512.nc
#          |     |--&gt; tos_198601.nc
#          |               .
#          |               .
#          |     |--&gt; tos_201010.nc
#          |--&gt; tas
#                |--&gt; tas_198511.nc
#                |--&gt; tas_198512.nc
#                |--&gt; tas_198601.nc
#                          .
#                          .
#                |--&gt; tas_201010.nc
#
# The model data is stored in a file-per-startdate fashion and the
# observational data is stored in a file-per-month, and both are stored in 
# a monthly frequency. The file format is NetCDF.
# Hence all the data is supported by Load() (see details and other supported 
# conventions in ?Load) but first we need to configure it properly.
#
# These data files are included in the package (in the 'sample_data' folder),
# only for the variable 'tos'. They have been interpolated to a very low 
# resolution grid so as to make it on CRAN.
# The original grid names (following CDO conventions) for experimental and 
# observational data were 't106grid' and 'r180x89' respectively. The final
# resolutions are 'r20x10' and 'r16x8' respectively. 
# The experimental data comes from the decadal climate prediction experiment 
# run at IC3 in the context of the CMIP5 project. Its name within IC3 local 
# database is 'i00k'. 
# The observational dataset used for verification is the 'ERSST' 
# observational dataset.
#
# The next two examples are equivalent and show how to load the variable 
# 'tos' from these sample datasets, the first providing lists of lists to 
# the parameters 'exp' and 'obs' (see documentation on these parameters) and 
# the second providing vectors of character strings, hence using a 
# configuration file.
#
# The code is not run because it dispatches system calls to 'cdo' which is 
# not allowed in the examples as per CRAN policies. You can run it on your 
# system though. 
# Instead, the code in 'dontshow' is run, which loads the equivalent
# already processed data in R.
#
# Example 1: Providing lists of lists to 'exp' and 'obs':
#
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
exp &lt;- list(
        name = 'experiment',
        path = file.path(data_path, 'model/$EXP_NAME$/monthly_mean',
                         '$VAR_NAME$_3hourly/$VAR_NAME$_$START_DATES$.nc')
      )
obs &lt;- list(
        name = 'observation',
        path = file.path(data_path, 'observation/$OBS_NAME$/monthly_mean',
                         '$VAR_NAME$/$VAR_NAME$_$YEAR$$MONTH$.nc')
      )
# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(exp), list(obs), startDates,
                  output = 'areave', latmin = 27, latmax = 48, 
                  lonmin = -12, lonmax = 40)
 
## End(Not run)
#
# Example 2: Providing vectors of character strings to 'exp' and 'obs'
#            and using a configuration file.
#
# The configuration file 'sample.conf' that we will create in the example 
# has the proper entries to load these (see ?LoadConfigFile for details on 
# writing a configuration file). 
#
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path, 
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  output = 'areave', latmin = 27, latmax = 48, 
                  lonmin = -12, lonmax = 40)
#
# Example 2: providing character strings in 'exp' and 'obs', and providing
# a configuration file.
# The configuration file 'sample.conf' that we will create in the example 
# has the proper entries to load these (see ?LoadConfigFile for details on 
# writing a configuration file). 
#
configfile &lt;- paste0(tempdir(), '/sample.conf')
ConfigFileCreate(configfile, confirm = FALSE)
c &lt;- ConfigFileOpen(configfile)
c &lt;- ConfigEditDefinition(c, 'DEFAULT_VAR_MIN', '-1e19', confirm = FALSE)
c &lt;- ConfigEditDefinition(c, 'DEFAULT_VAR_MAX', '1e19', confirm = FALSE)
data_path &lt;- system.file('sample_data', package = 's2dverification')
exp_data_path &lt;- paste0(data_path, '/model/$EXP_NAME$/')
obs_data_path &lt;- paste0(data_path, '/$OBS_NAME$/')
c &lt;- ConfigAddEntry(c, 'experiments', dataset_name = 'experiment', 
    var_name = 'tos', main_path = exp_data_path,
    file_path = '$STORE_FREQ$_mean/$VAR_NAME$_3hourly/$VAR_NAME$_$START_DATE$.nc')
c &lt;- ConfigAddEntry(c, 'observations', dataset_name = 'observation', 
    var_name = 'tos', main_path = obs_data_path,
    file_path = '$STORE_FREQ$_mean/$VAR_NAME$/$VAR_NAME$_$YEAR$$MONTH$.nc')
ConfigFileSave(c, configfile, confirm = FALSE)

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', c('experiment'), c('observation'), startDates, 
                  output = 'areave', latmin = 27, latmax = 48, 
                  lonmin = -12, lonmax = 40, configfile = configfile)
 
## End(Not run)
  
</code></pre>

<hr>
<h2 id='Mean1Dim'>Averages An Array Along A Dimension</h2><span id='topic+Mean1Dim'></span>

<h3>Description</h3>

<p>Averages the array along the posdim dimension along the user specified 
dimension. The user can specify a subset of the dimension to take the mean 
along.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Mean1Dim(var, posdim, narm = TRUE, limits = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Mean1Dim_+3A_var">var</code></td>
<td>
<p>Matrix to average.</p>
</td></tr>
<tr><td><code id="Mean1Dim_+3A_posdim">posdim</code></td>
<td>
<p>Dimension to average along.</p>
</td></tr>
<tr><td><code id="Mean1Dim_+3A_narm">narm</code></td>
<td>
<p>Ignore NA (TRUE) values or not (FALSE).</p>
</td></tr>
<tr><td><code id="Mean1Dim_+3A_limits">limits</code></td>
<td>
<p>Limits to average between. Default is to take the mean along 
the entire dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Array with one dimension less than the input array, containing 
the average along the posdim dimension.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-04  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- array(rnorm(24), dim = c(2, 3, 4))
print(a)
print(Mean1Dim(a, 2))
</code></pre>

<hr>
<h2 id='MeanListDim'>Averages An Array Along Multiple Dimensions</h2><span id='topic+MeanListDim'></span>

<h3>Description</h3>

<p>Averages an array along a set of dimensions given by the argument dims.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MeanListDim(var, dims, narm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MeanListDim_+3A_var">var</code></td>
<td>
<p>Input array.</p>
</td></tr>
<tr><td><code id="MeanListDim_+3A_dims">dims</code></td>
<td>
<p>List of dimensions to average along.</p>
</td></tr>
<tr><td><code id="MeanListDim_+3A_narm">narm</code></td>
<td>
<p>Ignore NA (TRUE) values or not (FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The averaged array, with the dimensions specified in <code>dims</code> 
removed.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-04  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN<br />
1.1  -  2015-03  (N. Manubens)  -  Improved memory usage
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- array(rnorm(24), dim = c(2, 3, 4))
print(a)
print(Mean1Dim(a, 2))
print(MeanListDim(a, c(2, 3)))
</code></pre>

<hr>
<h2 id='NAO'>Computes the North Atlantic Oscillation (NAO) Index</h2><span id='topic+NAO'></span>

<h3>Description</h3>

<p>Compute the North Atlantic Oscillation (NAO) index based on the leading EOF 
of the sea level pressure (SLP) anomalies over the north Atlantic region 
(20N-80N, 80W-40E). The PCs are obtained by projecting the forecast and 
observed anomalies onto the observed EOF pattern (Pobs) or the forecast 
anomalies onto the EOF pattern of the other years of the forecast (Pmod). 
By default (ftime_average = 2:4) NAO() computes the NAO index for 1-month 
lead seasonal forecasts that can be plotted with BoxPlot(). Returns 
cross-validated PCs of the NAO index for forecast (ano_exp) and observations 
(ano_obs) based on the leading EOF pattern.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NAO(
  ano_exp = NULL,
  ano_obs = NULL,
  lon,
  lat,
  ftime_average = 2:4,
  obsproj = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NAO_+3A_ano_exp">ano_exp</code></td>
<td>
<p>Array of North Atlantic SLP (20N-80N, 80W-40E) forecast 
anomalies from <code>Ano()</code> or <code>Ano_CrossValid()</code> with dimensions 
(n. of experimental data sets, n. of ensemble members, n. of start dates, 
n. of forecast time steps, n. of latitudes, n. of longitudes). If only 
NAO of observational data needs to be computed, this parameter can be left 
to NULL (default).</p>
</td></tr>
<tr><td><code id="NAO_+3A_ano_obs">ano_obs</code></td>
<td>
<p>Array of North Atlantic SLP (20N-80N, 80W-40E) observed 
anomalies from <code>Ano()</code> or <code>Ano_CrossValid()</code> with dimensions 
(n. of observational data sets, n. of obs. ensemble members, 
n. of start dates, n. of forecast time steps, n. of latitudes, 
n. of longitudes). If only NAO of experimental data needs to be computed, 
this parameter can be left to NULL (default).</p>
</td></tr>
<tr><td><code id="NAO_+3A_lon">lon</code></td>
<td>
<p>Vector with the longitudes of <code>ano_exp</code> and <code>ano_obs</code>.</p>
</td></tr>
<tr><td><code id="NAO_+3A_lat">lat</code></td>
<td>
<p>Vector with the latitudes of <code>ano_exp</code> and <code>ano_obs</code>.</p>
</td></tr>
<tr><td><code id="NAO_+3A_ftime_average">ftime_average</code></td>
<td>
<p>A vector with the forecast time steps to average across 
defining the target period. Takes by default 2:4, i.e. from 2nd to 4th 
forecast time steps.</p>
</td></tr>
<tr><td><code id="NAO_+3A_obsproj">obsproj</code></td>
<td>
<p><code>obsproj = TRUE</code> will compute the NAO index by 
projecting the forecast anomalies onto the leading EOF of observational 
reference.<br />
<code>obsproj = FALSE</code> will compute the NAO by first computing the leading 
EOF of the forecast anomalies (in cross-validation mode, i.e. leaving the 
year you are evaluating out), and then projecting forecast anomalies onto 
this EOF.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>NAO_exp</code></td>
<td>

<p>Array of forecast NAO index in verification format (ensemble members, 
start dates).
</p>
</td></tr>
<tr><td><code>NAO_obs</code></td>
<td>

<p>Array of observed NAO index in verification format (1, number of start
dates).
</p>
</td></tr>
<tr><td><code>EOFs_obs</code></td>
<td>

<p>EOFs of the observational references.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2013-08  (F. Lienert)  -  Original code<br />
0.2  -  2014-03  (V. Guemas)  -  Removing the 
rotation<br />
0.3  -  2014-05  (L. Batte)  -  Changes to
simplify function and add Pobs and Pmod options for NAO projection 
calculations<br />
0.4  -  2015-03  (L. Batte)  -  Polarity 
check and correction is wrong. Switched to have a negative NAO index when the
anomaly pattern corresponds to NAO-.
1.0  -  2016-03  (N. Manubens)  -  
Formatted to CRAN
</p>


<h3>References</h3>

<p>Doblas-Reyes, F.J., Pavan, V. and Stephenson, D. (2003). The skill of 
multi-model seasonal forecasts of the wintertime North Atlantic Oscillation. 
Climate Dynamics, 21, 501-514. DOI: 10.1007/s00382-003-0350-4
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path,
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 20, latmax = 90, lonmin = -80, lonmax = 40)
 
## End(Not run)
 

# Now ready to compute the EOFs and project on, for example, the first 
# variability mode.
ano &lt;- Ano_CrossValid(sampleData$mod, sampleData$obs)
# Note that computing the NAO over the region for which there is available 
# example data is not the full NAO area: NAO() will raise a warning.
nao &lt;- NAO(ano$ano_exp, ano$ano_obs, sampleData$lon, sampleData$lat)
# Finally plot the NAO index
 
PlotBoxWhisker(nao$NAO_exp, nao$NAO_obs, "NAO index, DJF", "NAO index (PC1) TOS",
       monini = 12, yearini = 1985, freq = 1, "Exp. A", "Obs. X")
 

</code></pre>

<hr>
<h2 id='Plot2VarsVsLTime'>Plot Two Scores With Confidence Intervals In A Common Plot</h2><span id='topic+Plot2VarsVsLTime'></span>

<h3>Description</h3>

<p>Plots two input variables having the same dimensions in a common plot.<br />
One plot for all experiments.<br />
Input variables should have dimensions (nexp/nmod, nltime).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Plot2VarsVsLTime(
  var1,
  var2,
  toptitle = "",
  ytitle = "",
  monini = 1,
  freq = 12,
  nticks = NULL,
  limits = NULL,
  listexp = c("exp1", "exp2", "exp3"),
  listvars = c("var1", "var2"),
  biglab = FALSE,
  hlines = NULL,
  leg = TRUE,
  siglev = FALSE,
  sizetit = 1,
  show_conf = TRUE,
  fileout = "output_plot2varsvsltime.eps",
  width = 8,
  height = 5,
  size_units = "in",
  res = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Plot2VarsVsLTime_+3A_var1">var1</code></td>
<td>
<p>Matrix of dimensions (nexp/nmod, nltime).</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_var2">var2</code></td>
<td>
<p>Matrix of dimensions (nexp/nmod, nltime).</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_toptitle">toptitle</code></td>
<td>
<p>Main title, optional.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_ytitle">ytitle</code></td>
<td>
<p>Title of Y-axis, optional.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_monini">monini</code></td>
<td>
<p>Starting month between 1 and 12. Default = 1.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_freq">freq</code></td>
<td>
<p>1 = yearly, 12 = monthly, 4 = seasonal, ... Default = 12.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_nticks">nticks</code></td>
<td>
<p>Number of ticks and labels on the x-axis, optional.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_limits">limits</code></td>
<td>
<p>c(lower limit, upper limit): limits of the Y-axis, optional.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_listexp">listexp</code></td>
<td>
<p>List of experiment names, up to three, optional.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_listvars">listvars</code></td>
<td>
<p>List of names of input variables, optional.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_biglab">biglab</code></td>
<td>
<p>TRUE/FALSE for presentation/paper plot. Default = FALSE.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_hlines">hlines</code></td>
<td>
<p>c(a, b, ...) Add horizontal black lines at Y-positions a, b, 
...<br />
Default: NULL.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_leg">leg</code></td>
<td>
<p>TRUE/FALSE if legend should be added or not to the plot. 
Default = TRUE.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_siglev">siglev</code></td>
<td>
<p>TRUE/FALSE if significance level should replace confidence 
interval.<br /> 
Default = FALSE.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_sizetit">sizetit</code></td>
<td>
<p>Multiplicative factor to change title size, optional.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_show_conf">show_conf</code></td>
<td>
<p>TRUE/FALSE to show/not confidence intervals for input 
variables.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_fileout">fileout</code></td>
<td>
<p>Name of output file. Extensions allowed: eps/ps, jpeg, png, 
pdf, bmp and tiff. <br />
Default = 'output_plot2varsvsltime.eps'</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_width">width</code></td>
<td>
<p>File width, in the units specified in the parameter size_units 
(inches by default). Takes 8 by default.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_height">height</code></td>
<td>
<p>File height, in the units specified in the parameter 
size_units (inches by default). Takes 5 by default.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot 
in. Inches ('in') by default. See ?Devices and the creator function of the 
corresponding device.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See 
?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="Plot2VarsVsLTime_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the following
graphical parameters:<br />  
adj ann ask bg bty cex.sub cin col.axis col.lab col.main col.sub cra crt 
csi cxy err family fg fig font font.axis font.lab font.main font.sub lend 
lheight ljoin lmitre mar mex mfcol mfrow mfg mkh oma omd omi page pch plt 
smo srt tck tcl usr xaxp xaxs xaxt xlog xpd yaxp yaxs yaxt ylbias ylog <br />
For more information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Examples of input:<br />
&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;<br /><br />  
RMSE error for a number of experiments and along lead-time: (nexp, nltime)
</p>


<h3>Author(s)</h3>

<p>History:<br />
1.0  -  2013-03  (I. Andreu-Burillo) 
- Original code
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
runmean_months &lt;- 12
dim_to_smooth &lt;- 4  # Smooth along lead-times
smooth_ano_exp &lt;- Smoothing(ano_exp, runmean_months, dim_to_smooth)
smooth_ano_obs &lt;- Smoothing(ano_obs, runmean_months, dim_to_smooth)
dim_to_mean &lt;- 2  # Mean along members
required_complete_row &lt;- 3  # Discard start dates that contain NA along lead-times
leadtimes_per_startdate &lt;- 60
rms &lt;- RMS(Mean1Dim(smooth_ano_exp, dim_to_mean), 
          Mean1Dim(smooth_ano_obs, dim_to_mean), 
          compROW = required_complete_row, 
          limits = c(ceiling((runmean_months + 1) / 2), 
                     leadtimes_per_startdate - floor(runmean_months / 2)))
smooth_ano_exp_m_sub &lt;- smooth_ano_exp - InsertDim(Mean1Dim(smooth_ano_exp, 2, 
                       narm = TRUE), 2, dim(smooth_ano_exp)[2])
spread &lt;- Spread(smooth_ano_exp_m_sub, c(2, 3))  
 
Plot2VarsVsLTime(InsertDim(rms[, , , ], 1, 1), spread$sd, 
                toptitle = 'RMSE and spread', monini = 11, freq = 12, 
                listexp = c('CMIP5 IC3'), listvar = c('RMSE', 'spread'),
                fileout = 'plot2vars.eps')
 

</code></pre>

<hr>
<h2 id='PlotACC'>Plot Plumes/Timeseries Of Anomaly Correlation Coefficients</h2><span id='topic+PlotACC'></span>

<h3>Description</h3>

<p>Plots plumes/timeseries of ACC from an array with dimensions 
(output from <code>ACC()</code>): <br />
c(nexp, nobs, nsdates, nltime, 4)<br />
where the fourth dimension is of length 4 and contains the lower limit of 
the 95% confidence interval, the ACC, the upper limit of the 95% 
confidence interval and the 95% significance level given by a one-sided 
T-test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotACC(
  ACC,
  sdates,
  toptitle = "",
  sizetit = 1,
  ytitle = "",
  limits = NULL,
  legends = NULL,
  freq = 12,
  biglab = FALSE,
  fill = FALSE,
  linezero = FALSE,
  points = TRUE,
  vlines = NULL,
  fileout = "output_PlotACC.eps",
  width = 8,
  height = 5,
  size_units = "in",
  res = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotACC_+3A_acc">ACC</code></td>
<td>
<p>ACC matrix with with dimensions:<br />
c(nexp, nobs, nsdates, nltime, 4)<br />
with the fourth dimension of length 4 containing the lower limit of the 
95% confidence interval, the ACC, the upper limit of the 95% confidence 
interval and the 95% significance level.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_sdates">sdates</code></td>
<td>
<p>List of startdates: c('YYYYMMDD','YYYYMMDD').</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_toptitle">toptitle</code></td>
<td>
<p>Main title, optional.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_sizetit">sizetit</code></td>
<td>
<p>Multiplicative factor to scale title size, optional.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_ytitle">ytitle</code></td>
<td>
<p>Title of Y-axis for each experiment: c(&rdquo;,&rdquo;), optional.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_limits">limits</code></td>
<td>
<p>c(lower limit, upper limit): limits of the Y-axis, optional.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_legends">legends</code></td>
<td>
<p>List of flags (characters) to be written in the legend, 
optional.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_freq">freq</code></td>
<td>
<p>1 = yearly, 12 = monthly, 4 = seasonal, ... Default: 12.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_biglab">biglab</code></td>
<td>
<p>TRUE/FALSE for presentation/paper plot, Default = FALSE.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_fill">fill</code></td>
<td>
<p>TRUE/FALSE if filled confidence interval. Default = FALSE.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_linezero">linezero</code></td>
<td>
<p>TRUE/FALSE if a line at y=0 should be added. Default = FALSE.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_points">points</code></td>
<td>
<p>TRUE/FALSE if points instead of lines. Default = TRUE.<br />
Must be TRUE if only 1 leadtime.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_vlines">vlines</code></td>
<td>
<p>List of x location where to add vertical black lines, optional.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_fileout">fileout</code></td>
<td>
<p>Name of output file. Extensions allowed: eps/ps, jpeg, png, 
pdf, bmp and tiff. <br />
Default = 'output_PlotACC.eps'</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_width">width</code></td>
<td>
<p>File width, in the units specified in the parameter size_units 
(inches by default). Takes 8 by default.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_height">height</code></td>
<td>
<p>File height, in the units specified in the parameter 
size_units (inches by default). Takes 5 by default.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot 
in. Inches ('in') by default. See ?Devices and the creator function of the 
corresponding device.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See 
?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotACC_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the following
graphical parameters:<br />
adj ann ask bg bty cex.sub cin col.axis col.lab col.main col.sub cra crt 
csi cxy err family fg fig fin font font.axis font.lab font.main font.sub 
lend lheight ljoin lmitre mar mex mfcol mfrow mfg mkh oma omd omi page 
plt smo srt tck tcl usr xaxp xaxs xaxt xlog xpd yaxp yaxs yaxt ylbias ylog<br />
For more information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2013-08  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path,
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
sampleData$mod &lt;- Season(sampleData$mod, 4, 11, 12, 2)
sampleData$obs &lt;- Season(sampleData$obs, 4, 11, 12, 2)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
acc &lt;- ACC(Mean1Dim(sampleData$mod, 2), 
          Mean1Dim(sampleData$obs, 2))
 
PlotACC(acc$ACC, startDates, toptitle = "Anomaly Correlation Coefficient")

 
</code></pre>

<hr>
<h2 id='PlotAno'>Plot Raw Or Smoothed Anomalies</h2><span id='topic+PlotAno'></span>

<h3>Description</h3>

<p>Plots timeseries of raw or smoothed anomalies of any variable output from 
<code>Load()</code> or <code>Ano()</code> or or <code>Ano_CrossValid()</code> or 
<code>Smoothing()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotAno(
  exp_ano,
  obs_ano = NULL,
  sdates,
  toptitle = rep("", 15),
  ytitle = rep("", 15),
  limits = NULL,
  legends = NULL,
  freq = 12,
  biglab = FALSE,
  fill = TRUE,
  memb = TRUE,
  ensmean = TRUE,
  linezero = FALSE,
  points = FALSE,
  vlines = NULL,
  sizetit = 1,
  fileout = paste0("output", 1:5, "_plotano.eps"),
  width = 8,
  height = 5,
  size_units = "in",
  res = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotAno_+3A_exp_ano">exp_ano</code></td>
<td>
<p>Array containing the experimental data:<br />
c(nmod/nexp, nmemb/nparam, nsdates, nltime).</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_obs_ano">obs_ano</code></td>
<td>
<p>Optional matrix containing the observational data:<br />
c(nobs, nmemb, nsdates, nltime)</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_sdates">sdates</code></td>
<td>
<p>List of starting dates: c('YYYYMMDD','YYYYMMDD').</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_toptitle">toptitle</code></td>
<td>
<p>Main title for each experiment: c(&rdquo;,&rdquo;), optional.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_ytitle">ytitle</code></td>
<td>
<p>Title of Y-axis for each experiment: c(&rdquo;,&rdquo;), optional.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_limits">limits</code></td>
<td>
<p>c(lower limit, upper limit): limits of the Y-axis, optional.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_legends">legends</code></td>
<td>
<p>List of observational dataset names, optional.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_freq">freq</code></td>
<td>
<p>1 = yearly, 12 = monthly, 4 = seasonal, ... Default: 12.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_biglab">biglab</code></td>
<td>
<p>TRUE/FALSE for presentation/paper plot. Default = FALSE.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_fill">fill</code></td>
<td>
<p>TRUE/FALSE if the spread between members should be filled. 
Default = TRUE.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_memb">memb</code></td>
<td>
<p>TRUE/FALSE if all members/only the ensemble-mean should be 
plotted.<br />
Default = TRUE.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_ensmean">ensmean</code></td>
<td>
<p>TRUE/FALSE if the ensemble-mean should be plotted. 
Default = TRUE.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_linezero">linezero</code></td>
<td>
<p>TRUE/FALSE if a line at y=0 should be added. 
Default = FALSE.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_points">points</code></td>
<td>
<p>TRUE/FALSE if points instead of lines should be shown. 
Default = FALSE.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_vlines">vlines</code></td>
<td>
<p>List of x location where to add vertical black lines, optional.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_sizetit">sizetit</code></td>
<td>
<p>Multiplicative factor to scale title size, optional.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_fileout">fileout</code></td>
<td>
<p>Name of the output file for each experiment: c(&rdquo;,&rdquo;). 
Extensions allowed: eps/ps, jpeg, png, pdf, bmp and tiff. If filenames 
with different extensions are passed, it will be considered only the first 
one and it will be extended to the rest. <br />
Default = c('output1_plotano.eps', 'output2_plotano.eps', 
'output3_plotano.eps', 'output4_plotano.eps', 
'output5_plotano.eps')</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_width">width</code></td>
<td>
<p>File width, in the units specified in the parameter size_units 
(inches by default). Takes 8 by default.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_height">height</code></td>
<td>
<p>File height, in the units specified in the parameter 
size_units (inches by default). Takes 5 by default.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot 
in. Inches ('in') by default. See ?Devices and the creator function of the 
corresponding device.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See 
?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotAno_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the following
graphical parameters:<br />  
adj ann ask bg bty cex.sub cin col.axis col.lab col.main col.sub cra crt 
csi cxy err family fg fig font font.axis font.lab font.main font.sub lend 
lheight ljoin lmitre mar mex mfcol mfrow mfg mkh oma omd omi page plt smo 
srt tck tcl usr xaxp xaxs xaxt xlog xpd yaxp yaxs yaxt ylbias ylog <br />
For more information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-03  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
runmean_nb_months &lt;- 12
dim_to_smooth &lt;- 4  # Smooth along lead-times
smooth_ano_exp &lt;- Smoothing(ano_exp, runmean_nb_months, dim_to_smooth)
smooth_ano_obs &lt;- Smoothing(ano_obs, runmean_nb_months, dim_to_smooth)
 
PlotAno(smooth_ano_exp, smooth_ano_obs, startDates, 
       toptitle = paste('smoothed anomalies'), ytitle = c('K', 'K', 'K'), 
       legends = 'ERSST', biglab = FALSE, fileout = 'tos_ano.eps')
 

</code></pre>

<hr>
<h2 id='PlotBoxWhisker'>Box-And-Whisker Plot of Time Series with Ensemble Distribution</h2><span id='topic+PlotBoxWhisker'></span>

<h3>Description</h3>

<p>Produce time series of box-and-whisker plot showing the distribution of the 
members of a forecast vs. the observed evolution. The correlation between
forecast and observational data is calculated and displayed. Only works for 
n-monthly to n-yearly time series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotBoxWhisker(
  exp,
  obs,
  toptitle = "",
  ytitle = "",
  monini = 1,
  yearini = 0,
  freq = 1,
  expname = "exp 1",
  obsname = "obs 1",
  drawleg = TRUE,
  fileout = "output_PlotBoxWhisker.ps",
  width = 8,
  height = 5,
  size_units = "in",
  res = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotBoxWhisker_+3A_exp">exp</code></td>
<td>
<p>Forecast array of multi-member time series, e.g., the NAO index 
of one experiment. The expected dimensions are 
c(members, start dates/forecast horizons). A vector with only the time 
dimension can also be provided. Only monthly or lower frequency time 
series are supported. See parameter freq.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_obs">obs</code></td>
<td>
<p>Observational vector or array of time series, e.g., the NAO index 
of the observations that correspond the forecast data in <code>exp</code>.
The expected dimensions are c(start dates/forecast horizons) or 
c(1, start dates/forecast horizons). Only monthly or lower frequency time 
series are supported. See parameter freq.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_toptitle">toptitle</code></td>
<td>
<p>Character string to be drawn as figure title.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_ytitle">ytitle</code></td>
<td>
<p>Character string to be drawn as y-axis title.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_monini">monini</code></td>
<td>
<p>Number of the month of the first time step, from 1 to 12.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_yearini">yearini</code></td>
<td>
<p>Year of the first time step.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_freq">freq</code></td>
<td>
<p>Frequency of the provided time series: 1 = yearly, 12 = monthly,</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_expname">expname</code></td>
<td>
<p>Experimental dataset name.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_obsname">obsname</code></td>
<td>
<p>Name of the observational reference dataset.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_drawleg">drawleg</code></td>
<td>
<p>TRUE/FALSE: whether to draw the legend or not.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_fileout">fileout</code></td>
<td>
<p>Name of output file. Extensions allowed: eps/ps, jpeg, png, 
pdf, bmp and tiff. <br />
Default = 'output_PlotBox.ps'.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_width">width</code></td>
<td>
<p>File width, in the units specified in the parameter size_units 
(inches by default). Takes 8 by default.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_height">height</code></td>
<td>
<p>File height, in the units specified in the parameter 
size_units (inches by default). Takes 5 by default.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot 
in. Inches ('in') by default. See ?Devices and the creator function of the 
corresponding device.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See 
?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotBoxWhisker_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the following
graphical parameters:<br />
ann ask bg cex.lab cex.sub cin col.axis col.lab col.main col.sub cra crt 
csi cxy err family fg fig font font.axis font.lab font.main font.sub lend 
lheight ljoin lmitre mex mfcol mfrow mfg mkh oma omd omi page pin plt pty 
smo srt tck tcl usr xaxp xaxs xaxt xlog xpd yaxp yaxs yaxt ylbias ylog <br />
For more information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Generates a file at the path specified via <code>fileout</code>.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2013-09  (F. Lienert)  -  Original code<br />
0.2  -  2015-03  (L. Batte)  -  Removed all<br />
normalization for sake of clarity.
1.0  -  2016-03  (N. Manubens)  -  Formatting to R CRAN
</p>


<h3>See Also</h3>

<p>EOF, ProjectField, NAO
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path,
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
# Now ready to compute the EOFs and project on, for example, the first 
# variability mode.
ano &lt;- Ano_CrossValid(sampleData$mod, sampleData$obs)
nao &lt;- NAO(ano$ano_exp, ano$ano_obs, sampleData$lon, sampleData$lat)
# Finally plot the nao index
 
PlotBoxWhisker(nao$NAO_exp, nao$NAO_obs, "NAO index, DJF", "NAO index (PC1) TOS", 
              monini = 12, yearini = 1985, freq = 1, "Exp. A", "Obs. X")
 

</code></pre>

<hr>
<h2 id='PlotClim'>Plots Climatologies</h2><span id='topic+PlotClim'></span>

<h3>Description</h3>

<p>Plots climatologies as a function of the forecast time for any index output 
from <code>Clim()</code> and organized in matrix with dimensions:<br />
c(nmod/nexp, nmemb/nparam, nltime) or c(nmod/nexp, nltime) for the 
experiment data<br />
c(nobs, nmemb, nltime) or c(nobs, nltime) for the observational data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotClim(
  exp_clim,
  obs_clim = NULL,
  toptitle = "",
  ytitle = "",
  monini = 1,
  freq = 12,
  limits = NULL,
  listexp = c("exp1", "exp2", "exp3"),
  listobs = c("obs1", "obs2", "obs3"),
  biglab = FALSE,
  leg = TRUE,
  sizetit = 1,
  fileout = "output_plotclim.eps",
  width = 8,
  height = 5,
  size_units = "in",
  res = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotClim_+3A_exp_clim">exp_clim</code></td>
<td>
<p>Matrix containing the experimental data with dimensions:<br />
c(nmod/nexp, nmemb/nparam, nltime) or c(nmod/nexp, nltime)</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_obs_clim">obs_clim</code></td>
<td>
<p>Matrix containing the observational data (optional) with 
dimensions:<br />
c(nobs, nmemb, nltime) or c(nobs, nltime)</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_toptitle">toptitle</code></td>
<td>
<p>Main title, optional.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_ytitle">ytitle</code></td>
<td>
<p>Title of Y-axis, optional.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_monini">monini</code></td>
<td>
<p>Starting month between 1 and 12. Default = 1.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_freq">freq</code></td>
<td>
<p>1 = yearly, 12 = monthly, 4 = seasonal, ... Default = 12.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_limits">limits</code></td>
<td>
<p>c(lower limit, upper limit): limits of the Y-axis, optional.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_listexp">listexp</code></td>
<td>
<p>List of experiment names, optional.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_listobs">listobs</code></td>
<td>
<p>List of observational dataset names, optional.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_biglab">biglab</code></td>
<td>
<p>TRUE/FALSE for presentation/paper plot. Default = FALSE.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_leg">leg</code></td>
<td>
<p>TRUE/FALSE to plot the legend or not.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_sizetit">sizetit</code></td>
<td>
<p>Multiplicative factor to scale title size, optional.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_fileout">fileout</code></td>
<td>
<p>Name of output file. Extensions allowed: eps/ps, jpeg, png, 
pdf, bmp and tiff. <br />
Default = 'output_plotclim.eps'.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_width">width</code></td>
<td>
<p>File width, in the units specified in the parameter size_units 
(inches by default). Takes 8 by default.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_height">height</code></td>
<td>
<p>File height, in the units specified in the parameter 
size_units (inches by default). Takes 5 by default.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot 
in. Inches ('in') by default. See ?Devices and the creator function of the 
corresponding device.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See 
?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotClim_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the following
graphical parameters:<br />
adj ann ask bg bty cex.sub cin col.axis col.lab col.main col.sub cra crt 
csi cxy err family fg fig font font.axis font.lab font.main font.sub lend 
lheight ljoin lmitre mar mex mfcol mfrow mfg mkh oma omd omi page pch plt 
smo srt tck usr xaxp xaxs xaxt xlog xpd yaxp yaxs yaxt ylbias ylog <br />
For more information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-03  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
 
PlotClim(clim$clim_exp, clim$clim_obs, toptitle = paste('climatologies'), 
        ytitle = 'K', monini = 11, listexp = c('CMIP5 IC3'), 
        listobs = c('ERSST'), biglab = FALSE, fileout = 'tos_clim.eps')
 

</code></pre>

<hr>
<h2 id='PlotEquiMap'>Maps A Two-Dimensional Variable On A Cylindrical Equidistant Projection</h2><span id='topic+PlotEquiMap'></span>

<h3>Description</h3>

<p>Map longitude-latitude array (on a regular rectangular or gaussian grid) 
on a cylindrical equidistant latitude and longitude projection with coloured 
grid cells. Only the region for which data has been provided is displayed. 
A colour bar (legend) can be plotted and adjusted. It is possible to draw 
superimposed arrows, dots, symbols, contour lines and boxes. A number of 
options is provided to adjust the position, size and colour of the 
components. This plot function is compatible with figure layouts if colour 
bar is disabled.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotEquiMap(
  var,
  lon,
  lat,
  varu = NULL,
  varv = NULL,
  toptitle = NULL,
  sizetit = NULL,
  units = NULL,
  brks = NULL,
  cols = NULL,
  bar_limits = NULL,
  triangle_ends = NULL,
  col_inf = NULL,
  col_sup = NULL,
  colNA = NULL,
  color_fun = clim.palette(),
  square = TRUE,
  filled.continents = NULL,
  coast_color = NULL,
  coast_width = 1,
  lake_color = NULL,
  contours = NULL,
  brks2 = NULL,
  contour_lwd = 0.5,
  contour_color = "black",
  contour_lty = 1,
  contour_draw_label = TRUE,
  contour_label_scale = 1,
  dots = NULL,
  dot_symbol = 4,
  dot_size = 1,
  arr_subsamp = floor(length(lon)/30),
  arr_scale = 1,
  arr_ref_len = 15,
  arr_units = "m/s",
  arr_scale_shaft = 1,
  arr_scale_shaft_angle = 1,
  axelab = TRUE,
  labW = FALSE,
  lab_dist_x = NULL,
  lab_dist_y = NULL,
  intylat = 20,
  intxlon = 20,
  axes_tick_scale = 1,
  axes_label_scale = 1,
  drawleg = TRUE,
  subsampleg = NULL,
  bar_extra_labels = NULL,
  draw_bar_ticks = TRUE,
  draw_separators = FALSE,
  triangle_ends_scale = 1,
  bar_label_digits = 4,
  bar_label_scale = 1,
  units_scale = 1,
  bar_tick_scale = 1,
  bar_extra_margin = rep(0, 4),
  boxlim = NULL,
  boxcol = "purple2",
  boxlwd = 5,
  margin_scale = rep(1, 4),
  title_scale = 1,
  numbfig = NULL,
  fileout = NULL,
  width = 8,
  height = 5,
  size_units = "in",
  res = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotEquiMap_+3A_var">var</code></td>
<td>
<p>Array with the values at each cell of a grid on a regular 
rectangular or gaussian grid. The array is expected to have two 
dimensions: c(latitude, longitude). Longitudes can be in ascending or 
descending order and latitudes in any order. It can contain NA values 
(coloured with 'colNA'). Arrays with dimensions c(longitude, latitude) 
will also be accepted but 'lon' and 'lat' will be used to disambiguate so 
this alternative is not appropriate for square arrays.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_lon">lon</code></td>
<td>
<p>Numeric vector of longitude locations of the cell centers of the 
grid of 'var', in ascending or descending order (same as 'var'). Expected 
to be regularly spaced, within either of the ranges [-180, 180] or 
[0, 360]. Data for two adjacent regions split by the limits of the 
longitude range can also be provided, e.g. <code>lon = c(0:50, 300:360)</code> 
('var' must be provided consitently).</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_lat">lat</code></td>
<td>
<p>Numeric vector of latitude locations of the cell centers of the 
grid of 'var', in any order (same as 'var'). Expected to be from a regular 
rectangular or gaussian grid, within the range [-90, 90].</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_varu">varu</code></td>
<td>
<p>Array of the zonal component of wind/current/other field with 
the same dimensions as 'var'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_varv">varv</code></td>
<td>
<p>Array of the meridional component of wind/current/other field 
with the same dimensions as 'var'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_toptitle">toptitle</code></td>
<td>
<p>Top title of the figure, scalable with parameter 
'title_scale'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_sizetit">sizetit</code></td>
<td>
<p>Scale factor for the figure top title provided in parameter 
'toptitle'. Deprecated. Use 'title_scale' instead.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_units">units</code></td>
<td>
<p>Title at the top of the colour bar, most commonly the units of 
the variable provided in parameter 'var'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_brks">brks</code>, <code id="PlotEquiMap_+3A_cols">cols</code>, <code id="PlotEquiMap_+3A_bar_limits">bar_limits</code>, <code id="PlotEquiMap_+3A_triangle_ends">triangle_ends</code></td>
<td>
<p>Usually only providing 'brks' is 
enough to generate the desired colour bar. These parameters allow to 
define n breaks that define n - 1 intervals to classify each of the values 
in 'var'. The corresponding grid cell of a given value in 'var' will be 
coloured in function of the interval it belongs to. These parameters are 
sent to <code>ColorBar()</code> to generate the breaks and colours. Additional 
colours for values beyond the limits of the colour bar are also generated 
and applied to the plot if 'bar_limits' or 'brks' and 'triangle_ends' are 
properly provided to do so. See ?ColorBar for a full explanation.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_col_inf">col_inf</code>, <code id="PlotEquiMap_+3A_col_sup">col_sup</code>, <code id="PlotEquiMap_+3A_colna">colNA</code></td>
<td>
<p>Colour identifiers to colour the values in 
'var' that go beyond the extremes of the colour bar and to colour NA 
values, respectively. 'colNA' takes attr(cols, 'na_color') if available by 
default, where cols is the parameter 'cols' if provided or the vector of 
colors returned by 'color_fun'. If not available, it takes 'pink' by 
default. 'col_inf' and 'col_sup' will take the value of 'colNA' if not 
specified. See ?ColorBar for a full explanation on 'col_inf' and 'col_sup'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_color_fun">color_fun</code>, <code id="PlotEquiMap_+3A_subsampleg">subsampleg</code>, <code id="PlotEquiMap_+3A_bar_extra_labels">bar_extra_labels</code>, <code id="PlotEquiMap_+3A_draw_bar_ticks">draw_bar_ticks</code>, <code id="PlotEquiMap_+3A_draw_separators">draw_separators</code>, <code id="PlotEquiMap_+3A_triangle_ends_scale">triangle_ends_scale</code>, <code id="PlotEquiMap_+3A_bar_label_digits">bar_label_digits</code>, <code id="PlotEquiMap_+3A_bar_label_scale">bar_label_scale</code>, <code id="PlotEquiMap_+3A_units_scale">units_scale</code>, <code id="PlotEquiMap_+3A_bar_tick_scale">bar_tick_scale</code>, <code id="PlotEquiMap_+3A_bar_extra_margin">bar_extra_margin</code></td>
<td>
<p>Set of parameters to control the visual 
aspect of the drawn colour bar. See ?ColorBar for a full explanation.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_square">square</code></td>
<td>
<p>Logical value to choose either to draw a coloured square for 
each grid cell in 'var' (TRUE; default) or to draw contour lines and fill 
the spaces in between with colours (FALSE). In the latter case, 
'filled.continents' will take the value FALSE if not specified.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_filled.continents">filled.continents</code></td>
<td>
<p>Colour to fill in drawn projected continents. 
Takes the value gray(0.5) by default or, if 'square = FALSE', takes the 
value FALSE. If set to FALSE, continents are not filled in.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_coast_color">coast_color</code></td>
<td>
<p>Colour of the coast line of the drawn projected continents.
Takes the value gray(0.5) by default.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_coast_width">coast_width</code></td>
<td>
<p>Line width of the coast line of the drawn projected 
continents. Takes the value 1 by default.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_lake_color">lake_color</code></td>
<td>
<p>Colour of the lake or other water body inside continents.
It is only functional when 'filled.continents = TRUE'. The default value is
'white'. For now, it is only functional if longitude range is [0, 360].</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_contours">contours</code></td>
<td>
<p>Array of same dimensions as 'var' to be added to the plot 
and displayed with contours. Parameter 'brks2' is required to define the 
magnitude breaks for each contour curve. Disregarded if 'square = FALSE'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_brks2">brks2</code></td>
<td>
<p>Vector of magnitude breaks where to draw contour curves for the 
array provided in 'contours' or if 'square = FALSE'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_contour_lwd">contour_lwd</code></td>
<td>
<p>Line width of the contour curves provided via 'contours' 
and 'brks2', or if 'square = FALSE'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_contour_color">contour_color</code></td>
<td>
<p>Line color of the contour curves provided via 'contours' 
and 'brks2', or if 'square = FALSE'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_contour_lty">contour_lty</code></td>
<td>
<p>Line type of the contour curves. Takes 1 (solid) by 
default. See help on 'lty' in par() for other accepted values.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_contour_draw_label">contour_draw_label</code></td>
<td>
<p>A logical value indicating whether to draw the 
contour labels or not. The default value is TRUE.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_contour_label_scale">contour_label_scale</code></td>
<td>
<p>Scale factor for the superimposed labels when 
drawing contour levels.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_dots">dots</code></td>
<td>
<p>Array of same dimensions as 'var' or with dimensions 
c(n, dim(var)), where n is the number of dot/symbol layers to add to the 
plot. A value of TRUE at a grid cell will draw a dot/symbol on the 
corresponding square of the plot. By default all layers provided in 'dots' 
are plotted with dots, but a symbol can be specified for each of the 
layers via the parameter 'dot_symbol'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_dot_symbol">dot_symbol</code></td>
<td>
<p>Single character/number or vector of characters/numbers 
that correspond to each of the symbol layers specified in parameter 'dots'. 
If a single value is specified, it will be applied to all the layers in 
'dots'. Takes 15 (centered square) by default. See 'pch' in par() for 
additional accepted options.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_dot_size">dot_size</code></td>
<td>
<p>Scale factor for the dots/symbols to be plotted, specified 
in 'dots'. If a single value is specified, it will be applied to all 
layers in 'dots'. Takes 1 by default.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_arr_subsamp">arr_subsamp</code></td>
<td>
<p>Subsampling factor to select a subset of arrows in 
'varu' and 'varv' to be drawn. Only one out of arr_subsamp arrows will 
be drawn. Takes 1 by default.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_arr_scale">arr_scale</code></td>
<td>
<p>Scale factor for drawn arrows from 'varu' and 'varv'. 
Takes 1 by default.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_arr_ref_len">arr_ref_len</code></td>
<td>
<p>Length of the refence arrow to be drawn as legend at the 
bottom of the figure (in same units as 'varu' and 'varv', only affects the
legend for the wind or variable in these arrays). Defaults to 15.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_arr_units">arr_units</code></td>
<td>
<p>Units of 'varu' and 'varv', to be drawn in the legend. 
Takes 'm/s' by default.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_arr_scale_shaft">arr_scale_shaft</code></td>
<td>
<p>Parameter for the scale of the shaft of the arrows 
(which also depend on the number of figures and the arr_scale parameter). 
Defaults to 1.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_arr_scale_shaft_angle">arr_scale_shaft_angle</code></td>
<td>
<p>Parameter for the scale of the angle of the 
shaft of the arrows (which also depend on the number of figure and the 
arr_scale parameter). Defaults to 1.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_axelab">axelab</code></td>
<td>
<p>Whether to draw longitude and latitude axes or not. 
TRUE by default.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_labw">labW</code></td>
<td>
<p>Whether to label the longitude axis with a 'W' instead of minus 
for negative values. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_lab_dist_x">lab_dist_x</code></td>
<td>
<p>A numeric of the distance of the longitude labels to the 
box borders. The default value is NULL and is automatically adjusted by 
the function.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_lab_dist_y">lab_dist_y</code></td>
<td>
<p>A numeric of the distance of the latitude labels to the 
box borders. The default value is NULL and is automatically adjusted by 
the function.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_intylat">intylat</code></td>
<td>
<p>Interval between latitude ticks on y-axis, in degrees. 
Defaults to 20.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_intxlon">intxlon</code></td>
<td>
<p>Interval between latitude ticks on x-axis, in degrees. 
Defaults to 20.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_axes_tick_scale">axes_tick_scale</code></td>
<td>
<p>Scale factor for the tick lines along the longitude 
and latitude axes.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_axes_label_scale">axes_label_scale</code></td>
<td>
<p>Scale factor for the labels along the longitude 
and latitude axes.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_drawleg">drawleg</code></td>
<td>
<p>Whether to plot a color bar (legend, key) or not. Defaults to 
TRUE. It is not possible to plot the colour bar if 'add = TRUE'. Use 
ColorBar() and the return values of PlotEquiMap() instead.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_boxlim">boxlim</code></td>
<td>
<p>Limits of a box to be added to the plot, in degrees: 
c(x1, y1, x2, y2). A list with multiple box specifications can also be 
provided.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_boxcol">boxcol</code></td>
<td>
<p>Colour of the box lines. A vector with a colour for each of 
the boxes is also accepted. Defaults to 'purple2'.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_boxlwd">boxlwd</code></td>
<td>
<p>Line width of the box lines. A vector with a line width for 
each of the boxes is also accepted. Defaults to 5.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_margin_scale">margin_scale</code></td>
<td>
<p>Scale factor for the margins around the map plot, with 
the format c(y1, x1, y2, x2). Defaults to rep(1, 4). If drawleg = TRUE, 
then margin_scale[1] is subtracted 1 unit.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_title_scale">title_scale</code></td>
<td>
<p>Scale factor for the figure top title. Defaults to 1.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_numbfig">numbfig</code></td>
<td>
<p>Number of figures in the layout the plot will be put into. 
A higher numbfig will result in narrower margins and smaller labels, 
axe labels, ticks, thinner lines, ... Defaults to 1.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_fileout">fileout</code></td>
<td>
<p>File where to save the plot. If not specified (default) a 
graphics device will pop up. Extensions allowed: eps/ps, jpeg, png, pdf, 
bmp and tiff.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_width">width</code></td>
<td>
<p>File width, in the units specified in the parameter size_units 
(inches by default). Takes 8 by default.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_height">height</code></td>
<td>
<p>File height, in the units specified in the parameter 
size_units (inches by default). Takes 5 by default.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot 
in. Inches ('in') by default. See ?Devices and the creator function of 
the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See 
?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotEquiMap_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the following 
graphical parameters:<br />
adj ann ask bg bty cex.sub cin col.axis col.lab col.main col.sub cra crt 
csi cxy err family fg font font.axis font.lab font.main font.sub lend 
lheight ljoin lmitre mex mfcol mfrow mfg mkh omd omi page pch pin plt 
pty smo srt tcl usr xaxp xaxs xaxt xlog xpd yaxp yaxs yaxt ylbias ylog <br />
For more information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>brks</code></td>
<td>

<p>Breaks used for colouring the map (and legend if drawleg = TRUE).
</p>
</td></tr>
<tr><td><code>cols</code></td>
<td>

<p>Colours used for colouring the map (and legend if drawleg = TRUE). Always 
of length length(brks) - 1.
</p>
</td></tr>
<tr><td><code>col_inf</code></td>
<td>

<p>Colour used to draw the lower triangle end in the colour bar (NULL if not 
drawn at all).
</p>
</td></tr>
<tr><td><code>col_sup</code></td>
<td>

<p>Colour used to draw the upper triangle end in the colour bar (NULL if not 
drawn at all).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-11  (V. Guemas)  -  Original code<br />
0.2  -  2013-04  (R. Saurral) - LabW<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN<br />
1.1  -  2013-09  (C. Prodhomme)  - add winds<br />
1.2  -  2016-08  (N. Manubens)  -  Refactored and added features,
and adapted to new ColorBar.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path,
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
PlotEquiMap(sampleData$mod[1, 1, 1, 1, , ], sampleData$lon, sampleData$lat, 
           toptitle = 'Predicted sea surface temperature for Nov 1960 from 1st Nov',
           sizetit = 0.5)
</code></pre>

<hr>
<h2 id='PlotLayout'>Arrange and Fill Multi-Pannel Layouts With Optional Colour Bar</h2><span id='topic+PlotLayout'></span>

<h3>Description</h3>

<p>This function takes an array or list of arrays and loops over each of them 
to plot all the sub-arrays they contain on an automatically generated 
multi-pannel layout. A different plot function (not necessarily from 
s2dverification) can be applied over each of the provided arrays. The input 
dimensions of each of the functions have to be specified, either with the 
names or the indices of the corresponding input dimensions. It is possible 
to draw a common colour bar at any of the sides of the multi-pannel for all 
the s2dverification plots that use a colour bar. Common plotting arguments 
for all the arrays in 'var' can be specified via the '...' parameter, and 
specific plotting arguments for each array can be fully adjusted via 
'special_args'. It is possible to draw titles for each of the figures, 
layout rows, layout columns and for the whole figure. A number of parameters 
is provided in order to adjust the position, size and colour of the 
components. Blank cells can be forced to appear and later be filled in 
manually with customized plots.<br />
This function pops up a blank new device and fills it in, so it cannot be 
nested in complex layouts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotLayout(
  fun,
  plot_dims,
  var,
  ...,
  special_args = NULL,
  nrow = NULL,
  ncol = NULL,
  toptitle = NULL,
  row_titles = NULL,
  col_titles = NULL,
  bar_scale = 1,
  title_scale = 1,
  title_margin_scale = 1,
  title_left_shift_scale = 1,
  subtitle_scale = 1,
  subtitle_margin_scale = 1,
  brks = NULL,
  cols = NULL,
  drawleg = "S",
  titles = NULL,
  subsampleg = NULL,
  bar_limits = NULL,
  triangle_ends = NULL,
  col_inf = NULL,
  col_sup = NULL,
  color_fun = clim.colors,
  draw_bar_ticks = TRUE,
  draw_separators = FALSE,
  triangle_ends_scale = 1,
  bar_extra_labels = NULL,
  units = NULL,
  units_scale = 1,
  bar_label_scale = 1,
  bar_tick_scale = 1,
  bar_extra_margin = rep(0, 4),
  bar_left_shift_scale = 1,
  bar_label_digits = 4,
  extra_margin = rep(0, 4),
  fileout = NULL,
  width = NULL,
  height = NULL,
  size_units = "in",
  res = 100,
  close_device = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotLayout_+3A_fun">fun</code></td>
<td>
<p>Plot function (or name of the function) to be called on the 
arrays provided in 'var'. If multiple arrays are provided in 'var', a 
vector of as many function names (character strings!) can be provided in 
'fun', one for each array in 'var'.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_plot_dims">plot_dims</code></td>
<td>
<p>Numeric or character string vector with identifiers of the 
input plot dimensions of the plot function specified in 'fun'. If 
character labels are provided, names(dim(var)) or attr('dimensions', var) 
will be checked to locate the dimensions. As many plots as 
prod(dim(var)[-plot_dims]) will be generated. If multiple arrays are 
provided in 'var', 'plot_dims' can be sent a list with a vector of plot 
dimensions for each. If a single vector is provided, it will be used for 
all the arrays in 'var'.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_var">var</code></td>
<td>
<p>Multi-dimensional array with at least the dimensions expected by 
the specified plot function in 'fun'. The dimensions reqired by the 
function must be specified in 'plot_dims'. The dimensions can be 
disordered and will be reordered automatically. Dimensions can optionally 
be labelled in order to refer to them with names in 'plot_dims'. All the 
available plottable sub-arrays will be automatically plotted and arranged 
in consecutive cells of an automatically arranged layout. A list of 
multiple (super-)arrays can be specified. The process will be repeated for 
each of them, by default applying the same plot function to all of them 
or, if properly specified in 'fun', a different plot function will be 
applied to each of them. NAs can be passed to the list: a NA will yield a 
blank cell in the layout, which can be populated after 
(see .SwitchToFigure).</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_...">...</code></td>
<td>
<p>Parameters to be sent to the plotting function 'fun'. If 
multiple arrays are provided in 'var' and multiple functions are provided 
in 'fun', the parameters provided through ... will be sent to all the 
plot functions, as common parameters. To specify concrete arguments for 
each of the plot functions see parameter 'special_args'.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_special_args">special_args</code></td>
<td>
<p>List of sub-lists, each sub-list having specific extra 
arguments for each of the plot functions provided in 'fun'. If you want to 
fix a different value for each plot in the layout you can do so by 
a) splitting your array into a list of sub-arrays (each with the data for 
one plot) and providing it as parameter 'var', 
b) providing a list of named sub-lists in 'special_args', where the names 
of each sub-list match the names of the parameters to be adjusted, and 
each value in a sub-list contains the value of the corresponding parameter.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_nrow">nrow</code></td>
<td>
<p>Numeric value to force the number of rows in the automatically 
generated layout. If higher than the required, this will yield blank cells 
in the layout (which can then be populated). If lower than the required 
the function will stop. By default it is configured to arrange the layout 
in a shape as square as possible. Blank cells can be manually populated 
after with customized plots (see SwitchTofigure).</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_ncol">ncol</code></td>
<td>
<p>Numeric value to force the number of columns in the 
automatically generated layout. If higher than the required, this will 
yield blank cells in the layout (which can then be populated). If lower 
than the required the function will stop. By default it is configured to 
arrange the layout in a shape as square as possible. Blank cells can be 
manually populated after with customized plots (see SwitchTofigure).</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_toptitle">toptitle</code></td>
<td>
<p>Topt title for the multi-pannel. Blank by default.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_row_titles">row_titles</code></td>
<td>
<p>Character string vector with titles for each of the rows 
in the layout. Blank by default.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_col_titles">col_titles</code></td>
<td>
<p>Character string vector with titles for each of the 
columns in the layout. Blank by default.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_bar_scale">bar_scale</code></td>
<td>
<p>Scale factor for the common colour bar. Takes 1 by default.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_title_scale">title_scale</code></td>
<td>
<p>Scale factor for the multi-pannel title. Takes 1 by 
default.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_title_margin_scale">title_margin_scale</code></td>
<td>
<p>Scale factor for the margins surrounding the top 
title. Takes 1 by default.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_title_left_shift_scale">title_left_shift_scale</code></td>
<td>
<p>When plotting row titles, a shift is added 
to the horizontal positioning of the top title in order to center it to 
the region of the figures (without taking row titles into account). This 
shift can be reduced. A value of 0 will remove the shift completely, 
centering the title to the total width of the device. This parameter will 
be disregarded if no 'row_titles' are provided.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_subtitle_scale">subtitle_scale</code></td>
<td>
<p>Scale factor for the row titles and column titles 
(specified in 'row_titles' and 'col_titles'). Takes 1 by default.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_subtitle_margin_scale">subtitle_margin_scale</code></td>
<td>
<p>Scale factor for the margins surrounding the 
subtitles. Takes 1 by default.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_brks">brks</code>, <code id="PlotLayout_+3A_cols">cols</code>, <code id="PlotLayout_+3A_bar_limits">bar_limits</code>, <code id="PlotLayout_+3A_triangle_ends">triangle_ends</code></td>
<td>
<p>Usually only providing 'brks' is 
enough to generate the desired colour bar. These parameters allow to 
define n breaks that define n - 1 intervals to classify each of the values 
in 'var'. The corresponding grid cell of a given value in 'var' will be 
coloured in function of the interval it belongs to. These parameters are 
sent to <code>ColorBar()</code> to generate the breaks and colours. Additional 
colours for values beyond the limits of the colour bar are also generated 
and applied to the plot if 'bar_limits' or 'brks' and 'triangle_ends' are 
properly provided to do so. See ?ColorBar for a full explanation.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_drawleg">drawleg</code></td>
<td>
<p>Where to draw the common colour bar. Can take values TRUE, 
FALSE or:<br />
'up', 'u', 'U', 'top', 't', 'T', 'north', 'n', 'N'<br />
'down', 'd', 'D', 'bottom', 'b', 'B', 'south', 's', 'S' (default)<br />
'right', 'r', 'R', 'east', 'e', 'E'<br />
'left', 'l', 'L', 'west', 'w', 'W'</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_titles">titles</code></td>
<td>
<p>Character string vector with titles for each of the figures in 
the multi-pannel, from top-left to bottom-right. Blank by default.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_col_inf">col_inf</code>, <code id="PlotLayout_+3A_col_sup">col_sup</code></td>
<td>
<p>Colour identifiers to colour the values in 'var' that 
go beyond the extremes of the colour bar and to colour NA values, 
respectively. 'colNA' takes 'white' by default. 'col_inf' and 'col_sup' 
will take the value of 'colNA' if not specified. See ?ColorBar for a full 
explanation on 'col_inf' and 'col_sup'.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_color_fun">color_fun</code>, <code id="PlotLayout_+3A_subsampleg">subsampleg</code>, <code id="PlotLayout_+3A_bar_extra_labels">bar_extra_labels</code>, <code id="PlotLayout_+3A_draw_bar_ticks">draw_bar_ticks</code>, <code id="PlotLayout_+3A_draw_separators">draw_separators</code>, <code id="PlotLayout_+3A_triangle_ends_scale">triangle_ends_scale</code>, <code id="PlotLayout_+3A_bar_label_digits">bar_label_digits</code>, <code id="PlotLayout_+3A_bar_label_scale">bar_label_scale</code>, <code id="PlotLayout_+3A_units_scale">units_scale</code>, <code id="PlotLayout_+3A_bar_tick_scale">bar_tick_scale</code>, <code id="PlotLayout_+3A_bar_extra_margin">bar_extra_margin</code></td>
<td>
<p>Set of parameters to control the visual aspect of the drawn colour bar. See ?ColorBar for a full explanation.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_units">units</code></td>
<td>
<p>Title at the top of the colour bar, most commonly the units of 
the variable provided in parameter 'var'.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_bar_left_shift_scale">bar_left_shift_scale</code></td>
<td>
<p>When plotting row titles, a shift is added to 
the horizontal positioning of the colour bar in order to center it to the 
region of the figures (without taking row titles into account). This shift 
can be reduced. A value of 0 will remove the shift completely, centering 
the colour bar to the total width of the device. This parameter will be 
disregarded if no 'row_titles' are provided.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_extra_margin">extra_margin</code></td>
<td>
<p>Extra margins to be added around the layout, in the 
format c(y1, x1, y2, x2). The units are margin lines. Takes rep(0, 4) 
by default.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_fileout">fileout</code></td>
<td>
<p>File where to save the plot. If not specified (default) a 
graphics device will pop up. Extensions allowed: eps/ps, jpeg, png, pdf, 
bmp and tiff.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_width">width</code></td>
<td>
<p>Width in inches of the multi-pannel. 7 by default, or 11 if 
'fielout' has been specified.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_height">height</code></td>
<td>
<p>Height in inches of the multi-pannel. 7 by default, or 11 if 
'fileout' has been specified.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot 
in. Inches ('in') by default. See ?Devices and the creator function of 
the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See 
?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotLayout_+3A_close_device">close_device</code></td>
<td>
<p>Whether to close the graphics device after plotting 
the layout and a 'fileout' has been specified. This is useful to avoid 
closing the device when saving the layout into a file and willing to add 
extra elements or figures. Takes TRUE by default. Disregarded if no 
'fileout' has been specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>brks</code></td>
<td>

<p>Breaks used for colouring the map (and legend if drawleg = TRUE).
</p>
</td></tr>
<tr><td><code>cols</code></td>
<td>

<p>Colours used for colouring the map (and legend if drawleg = TRUE). 
Always of length length(brks) - 1.
</p>
</td></tr>
<tr><td><code>col_inf</code></td>
<td>

<p>Colour used to draw the lower triangle end in the colour bar 
(NULL if not drawn at all).
</p>
</td></tr>
<tr><td><code>col_sup</code></td>
<td>

<p>Colour used to draw the upper triangle end in the colour bar 
(NULL if not drawn at all).
</p>
</td></tr>
<tr><td><code>layout_matrix</code></td>
<td>

<p>Underlying matrix of the layout. Useful to later set any of the layout 
cells as current figure to add plot elements. See .SwitchToFigure.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2016-08  (N. Manubens)  -  Original code
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path,
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
PlotLayout(PlotEquiMap, c('lat', 'lon'), sampleData$mod[1, , 1, 1, , ], 
          sampleData$lon, sampleData$lat,
          toptitle = 'Predicted tos for Nov 1960 from 1st Nov',
          titles = paste('Member', 1:15))

</code></pre>

<hr>
<h2 id='PlotMatrix'>Function to convert any numerical table to a grid of coloured squares.</h2><span id='topic+PlotMatrix'></span>

<h3>Description</h3>

<p>This function converts a numerical data matrix into a coloured 
grid. It is useful for a slide or article to present tabular results as 
colors instead of numbers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotMatrix(
  var,
  brks = NULL,
  cols = NULL,
  toptitle = NULL,
  title.color = "royalblue4",
  xtitle = NULL,
  ytitle = NULL,
  xlabels = NULL,
  xvert = FALSE,
  ylabels = NULL,
  line = 3,
  figure.width = 1,
  legend = TRUE,
  legend.width = 0.15,
  xlab_dist = NULL,
  ylab_dist = NULL,
  fileout = NULL,
  size_units = "px",
  res = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotMatrix_+3A_var">var</code></td>
<td>
<p>A numerical matrix containing the values to be displayed in a 
colored image.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_brks">brks</code></td>
<td>
<p>A vector of the color bar intervals. The length must be one more 
than the parameter 'cols'. Use ColorBar() to generate default values.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_cols">cols</code></td>
<td>
<p>A vector of valid color identifiers for color bar. The length
must be one less than the parameter 'brks'. Use ColorBar() to generate 
default values.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_toptitle">toptitle</code></td>
<td>
<p>A string of the title of the grid. Set NULL as default.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_title.color">title.color</code></td>
<td>
<p>A string of valid color identifier to decide the title 
color. Set &quot;royalblue4&quot; as default.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_xtitle">xtitle</code></td>
<td>
<p>A string of title of the x-axis. Set NULL as default.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_ytitle">ytitle</code></td>
<td>
<p>A string of title of the y-axis. Set NULL as default.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_xlabels">xlabels</code></td>
<td>
<p>A vector of labels of the x-axis. The length must be 
length of the column of parameter 'var'. Set the sequence from 1 to the 
length of the column of parameter 'var' as default.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_xvert">xvert</code></td>
<td>
<p>A logical value to decide whether to place x-axis labels 
vertically. Set FALSE as default, which keeps the labels horizontally.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_ylabels">ylabels</code></td>
<td>
<p>A vector of labels of the y-axis The length must be 
length of the row of parameter 'var'. Set the sequence from 1 to the 
length of the row of parameter 'var' as default.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_line">line</code></td>
<td>
<p>An integer specifying the distance between the title of the 
x-axis and the x-axis. Set 3 as default. Adjust if the x-axis labels 
are long.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_figure.width">figure.width</code></td>
<td>
<p>A positive number as a ratio adjusting the width of the 
grids. Set 1 as default.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_legend">legend</code></td>
<td>
<p>A logical value to decide to draw the grid color legend or not. 
Set TRUE as default.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_legend.width">legend.width</code></td>
<td>
<p>A number between 0 and 0.5 to adjust the legend width.
Set 0.15 as default.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_xlab_dist">xlab_dist</code></td>
<td>
<p>A number specifying the distance between the x labels and 
the x axis. If not specified, it equals to -1 - (nrow(var) / 10 - 1).</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_ylab_dist">ylab_dist</code></td>
<td>
<p>A number specifying the distance between the y labels and 
the y axis. If not specified, it equals to 0.5 - ncol(var) / 10.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_fileout">fileout</code></td>
<td>
<p>A string of full directory path and file name indicating where 
to save the plot. If not specified (default), a graphics device will pop up.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_size_units">size_units</code></td>
<td>
<p>A string indicating the units of the size of the device 
(file or window) to plot in. Set 'px' as default. See ?Devices and the 
creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_res">res</code></td>
<td>
<p>A positive number indicating resolution of the device (file or window) 
to plot in. See ?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotMatrix_+3A_...">...</code></td>
<td>
<p>The additional parameters to be passed to function ColorBar() in 
s2dverification for color legend creation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A figure in popup window by default, or saved to the specified path.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example with random data
PlotMatrix(var = matrix(rnorm(n = 120, mean = 0.3), 10, 12),
           cols = c('white','#fef0d9','#fdd49e','#fdbb84','#fc8d59',
                      '#e34a33','#b30000', '#7f0000'),
           brks = c(-1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 1),
           toptitle = "Mean Absolute Error", 
           xtitle = "Forecast time (month)", ytitle = "Start date",
           xlabels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", 
                       "Aug", "Sep", "Oct", "Nov", "Dec"))
</code></pre>

<hr>
<h2 id='PlotSection'>Plots A Vertical Section</h2><span id='topic+PlotSection'></span>

<h3>Description</h3>

<p>Plot a (longitude,depth) or (latitude,depth) section.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotSection(
  var,
  horiz,
  depth,
  toptitle = "",
  sizetit = 1,
  units = "",
  brks = NULL,
  cols = NULL,
  axelab = TRUE,
  intydep = 200,
  intxhoriz = 20,
  drawleg = TRUE,
  fileout = NULL,
  width = 8,
  height = 5,
  size_units = "in",
  res = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotSection_+3A_var">var</code></td>
<td>
<p>Matrix to plot with (longitude/latitude, depth) dimensions.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_horiz">horiz</code></td>
<td>
<p>Array of longitudes or latitudes.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_depth">depth</code></td>
<td>
<p>Array of depths.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_toptitle">toptitle</code></td>
<td>
<p>Title, optional.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_sizetit">sizetit</code></td>
<td>
<p>Multiplicative factor to increase title size, optional.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_units">units</code></td>
<td>
<p>Units, optional.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_brks">brks</code></td>
<td>
<p>Colour levels, optional.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_cols">cols</code></td>
<td>
<p>List of colours, optional.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_axelab">axelab</code></td>
<td>
<p>TRUE/FALSE, label the axis. Default = TRUE.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_intydep">intydep</code></td>
<td>
<p>Interval between depth ticks on y-axis. Default: 200m.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_intxhoriz">intxhoriz</code></td>
<td>
<p>Interval between longitude/latitude ticks on x-axis.<br />
Default: 20deg.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_drawleg">drawleg</code></td>
<td>
<p>Draw colorbar. Default: TRUE.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_fileout">fileout</code></td>
<td>
<p>Name of output file. Extensions allowed: eps/ps, jpeg, png, 
pdf, bmp and tiff. <br />
Default = NULL</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_width">width</code></td>
<td>
<p>File width, in the units specified in the parameter size_units 
(inches by default). Takes 8 by default.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_height">height</code></td>
<td>
<p>File height, in the units specified in the parameter 
size_units (inches by default). Takes 5 by default.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot 
in. Inches ('in') by default. See ?Devices and the creator function of the 
corresponding device.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See 
?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotSection_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the following
graphical parameters:<br />
adj ann ask bg bty cex.lab cex.sub cin col.axis col.lab col.main col.sub 
cra crt csi cxy err family fg fig fin font font.axis font.lab font.main 
font.sub lend lheight ljoin lmitre lty lwd mex mfcol mfrow mfg mkh oma omd 
omi page pch pin plt pty smo srt tcl usr xaxp xaxs xaxt xlog xpd yaxp yaxs 
yaxt ylbias ylog <br />
For more information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-09  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sampleData &lt;- s2dverification::sampleDepthData
PlotSection(sampleData$mod[1, 1, 1, 1, , ], sampleData$lat, sampleData$depth,
           toptitle = 'temperature 1995-11 member 0')
</code></pre>

<hr>
<h2 id='PlotStereoMap'>Maps A Two-Dimensional Variable On A Polar Stereographic Projection</h2><span id='topic+PlotStereoMap'></span>

<h3>Description</h3>

<p>Map longitude-latitude array (on a regular rectangular or gaussian grid) on 
a polar stereographic world projection with coloured grid cells. Only the 
region within a specified latitude interval is displayed. A colour bar 
(legend) can be plotted and adjusted. It is possible to draw superimposed 
dots, symbols, boxes, contours, and arrows. A number of options is provided to
adjust the position, size and colour of the components. This plot function is 
compatible with figure layouts if colour bar is disabled.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotStereoMap(
  var,
  lon,
  lat,
  varu = NULL,
  varv = NULL,
  latlims = c(60, 90),
  toptitle = NULL,
  sizetit = NULL,
  units = NULL,
  brks = NULL,
  cols = NULL,
  bar_limits = NULL,
  triangle_ends = NULL,
  col_inf = NULL,
  col_sup = NULL,
  colNA = NULL,
  color_fun = clim.palette(),
  filled.continents = FALSE,
  coast_color = NULL,
  coast_width = 1,
  contours = NULL,
  brks2 = NULL,
  contour_lwd = 0.5,
  contour_color = "black",
  contour_lty = 1,
  contour_label_draw = TRUE,
  contour_label_scale = 0.6,
  dots = NULL,
  dot_symbol = 4,
  dot_size = 0.8,
  intlat = 10,
  arr_subsamp = floor(length(lon)/30),
  arr_scale = 1,
  arr_ref_len = 15,
  arr_units = "m/s",
  arr_scale_shaft = 1,
  arr_scale_shaft_angle = 1,
  drawleg = TRUE,
  subsampleg = NULL,
  bar_extra_labels = NULL,
  draw_bar_ticks = TRUE,
  draw_separators = FALSE,
  triangle_ends_scale = 1,
  bar_label_digits = 4,
  bar_label_scale = 1,
  units_scale = 1,
  bar_tick_scale = 1,
  bar_extra_margin = rep(0, 4),
  boxlim = NULL,
  boxcol = "purple2",
  boxlwd = 5,
  margin_scale = rep(1, 4),
  title_scale = 1,
  numbfig = NULL,
  fileout = NULL,
  width = 6,
  height = 5,
  size_units = "in",
  res = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotStereoMap_+3A_var">var</code></td>
<td>
<p>Array with the values at each cell of a grid on a regular 
rectangular or gaussian grid. The array is expected to have two dimensions: 
c(latitude, longitude). Longitudes can be in ascending or descending order 
and latitudes in any order. It can contain NA values (coloured with 
'colNA'). Arrays with dimensions c(longitude, latitude) will also be 
accepted but 'lon' and 'lat' will be used to disambiguate so this 
alternative is not appropriate for square arrays.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_lon">lon</code></td>
<td>
<p>Numeric vector of longitude locations of the cell centers of the 
grid of 'var', in ascending or descending order (same as 'var'). Expected 
to be regularly spaced, within either of the ranges [-180, 180] or 
[0, 360]. Data for two adjacent regions split by the limits of the 
longitude range can also be provided, e.g. <code>lon = c(0:50, 300:360)</code> 
('var' must be provided consitently).</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_lat">lat</code></td>
<td>
<p>Numeric vector of latitude locations of the cell centers of the 
grid of 'var', in any order (same as 'var'). Expected to be from a regular 
rectangular or gaussian grid, within the range [-90, 90].</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_varu">varu</code></td>
<td>
<p>Array of the zonal component of wind/current/other field with 
the same dimensions as 'var'.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_varv">varv</code></td>
<td>
<p>Array of the meridional component of wind/current/other field 
with the same dimensions as 'var'.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_latlims">latlims</code></td>
<td>
<p>Latitudinal limits of the figure.<br />
Example : c(60, 90) for the North Pole<br />
c(-90,-60) for the South Pole</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_toptitle">toptitle</code></td>
<td>
<p>Top title of the figure, scalable with parameter 
'title_scale'.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_sizetit">sizetit</code></td>
<td>
<p>Scale factor for the figure top title provided in parameter 
'toptitle'. Deprecated. Use 'title_scale' instead.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_units">units</code></td>
<td>
<p>Title at the top of the colour bar, most commonly the units of 
the variable provided in parameter 'var'.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_brks">brks</code>, <code id="PlotStereoMap_+3A_cols">cols</code>, <code id="PlotStereoMap_+3A_bar_limits">bar_limits</code>, <code id="PlotStereoMap_+3A_triangle_ends">triangle_ends</code></td>
<td>
<p>Usually only providing 'brks' is 
enough to generate the desired colour bar. These parameters allow to 
define n breaks that define n - 1 intervals to classify each of the values 
in 'var'. The corresponding grid cell of a given value in 'var' will be 
coloured in function of the interval it belongs to. These parameters are 
sent to <code>ColorBar()</code> to generate the breaks and colours. Additional 
colours for values beyond the limits of the colour bar are also generated 
and applied to the plot if 'bar_limits' or 'brks' and 'triangle_ends' are 
properly provided to do so. See ?ColorBar for a full explanation.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_col_inf">col_inf</code>, <code id="PlotStereoMap_+3A_col_sup">col_sup</code>, <code id="PlotStereoMap_+3A_colna">colNA</code></td>
<td>
<p>Colour identifiers to colour the values in 
'var' that go beyond the extremes of the colour bar and to colour NA 
values, respectively. 'colNA' takes attr(cols, 'na_color') if available by 
default, where cols is the parameter 'cols' if provided or the vector of 
colors returned by 'color_fun'. If not available, it takes 'pink' by 
default. 'col_inf' and 'col_sup' will take the value of 'colNA' if not 
specified. See ?ColorBar for a full explanation on 'col_inf' and 'col_sup'.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_color_fun">color_fun</code>, <code id="PlotStereoMap_+3A_subsampleg">subsampleg</code>, <code id="PlotStereoMap_+3A_bar_extra_labels">bar_extra_labels</code>, <code id="PlotStereoMap_+3A_draw_bar_ticks">draw_bar_ticks</code>, <code id="PlotStereoMap_+3A_draw_separators">draw_separators</code>, <code id="PlotStereoMap_+3A_triangle_ends_scale">triangle_ends_scale</code>, <code id="PlotStereoMap_+3A_bar_label_digits">bar_label_digits</code>, <code id="PlotStereoMap_+3A_bar_label_scale">bar_label_scale</code>, <code id="PlotStereoMap_+3A_units_scale">units_scale</code>, <code id="PlotStereoMap_+3A_bar_tick_scale">bar_tick_scale</code>, <code id="PlotStereoMap_+3A_bar_extra_margin">bar_extra_margin</code></td>
<td>
<p>Set of parameters to control the visual 
aspect of the drawn colour bar. See ?ColorBar for a full explanation.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_filled.continents">filled.continents</code></td>
<td>
<p>Colour to fill in drawn projected continents. Takes 
the value gray(0.5) by default. If set to FALSE, continents are not 
filled in.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_coast_color">coast_color</code></td>
<td>
<p>Colour of the coast line of the drawn projected 
continents. Takes the value gray(0.5) by default.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_coast_width">coast_width</code></td>
<td>
<p>Line width of the coast line of the drawn projected 
continents. Takes the value 1 by default.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_contours">contours</code></td>
<td>
<p>Array of same dimensions as 'var' to be added to the plot 
and displayed with contours. Parameter 'brks2' is required to define the 
magnitude breaks for each contour curve.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_brks2">brks2</code></td>
<td>
<p>A numeric value or vector of magnitude breaks where to draw 
contour curves for the array provided in 'contours'. If it is a number, it
represents the number of breaks (n) that defines (n - 1) intervals to 
classify 'contours'.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_contour_lwd">contour_lwd</code></td>
<td>
<p>Line width of the contour curves provided via 'contours' 
and 'brks2'. The default value is 0.5.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_contour_color">contour_color</code></td>
<td>
<p>Line color of the contour curves provided via 'contours' 
and 'brks2'.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_contour_lty">contour_lty</code></td>
<td>
<p>Line type of the contour curves. Takes 1 (solid) by 
default. See help on 'lty' in par() for other accepted values.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_contour_label_draw">contour_label_draw</code></td>
<td>
<p>A logical value indicating whether to draw the 
contour labels (TRUE) or not (FALSE) when 'contours' is used. The default
value is TRUE.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_contour_label_scale">contour_label_scale</code></td>
<td>
<p>Scale factor for the superimposed labels when 
drawing contour levels. The default value is 0.6.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_dots">dots</code></td>
<td>
<p>Array of same dimensions as 'var' or with dimensions 
c(n, dim(var)), where n is the number of dot/symbol layers to add to the 
plot. A value of TRUE at a grid cell will draw a dot/symbol on the 
corresponding square of the plot. By default all layers provided in 'dots' 
are plotted with dots, but a symbol can be specified for each of the 
layers via the parameter 'dot_symbol'.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_dot_symbol">dot_symbol</code></td>
<td>
<p>Single character/number or vector of characters/numbers 
that correspond to each of the symbol layers specified in parameter 'dots'. 
If a single value is specified, it will be applied to all the layers in 
'dots'. Takes 15 (centered square) by default. See 'pch' in par() for 
additional accepted options.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_dot_size">dot_size</code></td>
<td>
<p>Scale factor for the dots/symbols to be plotted, specified 
in 'dots'. If a single value is specified, it will be applied to all 
layers in 'dots'. Takes 1 by default.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_intlat">intlat</code></td>
<td>
<p>Interval between latitude lines (circles), in degrees. 
Defaults to 10.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_arr_subsamp">arr_subsamp</code></td>
<td>
<p>A number as subsampling factor to select a subset of arrows 
in 'varu' and 'varv' to be drawn. Only one out of arr_subsamp arrows will 
be drawn. The default value is 1.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_arr_scale">arr_scale</code></td>
<td>
<p>A number as scale factor for drawn arrows from 'varu' and 
'varv'. The default value is 1.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_arr_ref_len">arr_ref_len</code></td>
<td>
<p>A number of the length of the refence arrow to be drawn as
legend at the bottom of the figure (in same units as 'varu' and 'varv', only
affects the legend for the wind or variable in these arrays). The default 
value is 15.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_arr_units">arr_units</code></td>
<td>
<p>Units of 'varu' and 'varv', to be drawn in the legend. 
Takes 'm/s' by default.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_arr_scale_shaft">arr_scale_shaft</code></td>
<td>
<p>A number for the scale of the shaft of the arrows 
(which also depend on the number of figures and the arr_scale parameter). 
The default value is 1.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_arr_scale_shaft_angle">arr_scale_shaft_angle</code></td>
<td>
<p>A number for the scale of the angle of the 
shaft of the arrows (which also depend on the number of figure and the 
arr_scale parameter). The default value is 1.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_drawleg">drawleg</code></td>
<td>
<p>Whether to plot a color bar (legend, key) or not. 
Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_boxlim">boxlim</code></td>
<td>
<p>Limits of a box to be added to the plot, in degrees: 
c(x1, y1, x2, y2). A list with multiple box specifications can also 
be provided.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_boxcol">boxcol</code></td>
<td>
<p>Colour of the box lines. A vector with a colour for each of 
the boxes is also accepted. Defaults to 'purple2'.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_boxlwd">boxlwd</code></td>
<td>
<p>Line width of the box lines. A vector with a line width for 
each of the boxes is also accepted. Defaults to 5.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_margin_scale">margin_scale</code></td>
<td>
<p>Scale factor for the margins to be added to the plot, 
with the format c(y1, x1, y2, x2). Defaults to rep(1, 4). If drawleg = TRUE, 
margin_scale[1] is subtracted 1 unit.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_title_scale">title_scale</code></td>
<td>
<p>Scale factor for the figure top title. Defaults to 1.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_numbfig">numbfig</code></td>
<td>
<p>Number of figures in the layout the plot will be put into. 
A higher numbfig will result in narrower margins and smaller labels, 
axe labels, ticks, thinner lines, ... Defaults to 1.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_fileout">fileout</code></td>
<td>
<p>File where to save the plot. If not specified (default) a 
graphics device will pop up. Extensions allowed: eps/ps, jpeg, png, pdf, 
bmp and tiff.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_width">width</code></td>
<td>
<p>File width, in the units specified in the parameter size_units 
(inches by default). Takes 8 by default.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_height">height</code></td>
<td>
<p>File height, in the units specified in the parameter 
size_units (inches by default). Takes 5 by default.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot 
in. Inches ('in') by default. See ?Devices and the creator function of 
the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See 
?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotStereoMap_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the 
following graphical parameters:<br />
adj ann ask bg bty cex.sub cin col.axis col.lab col.main col.sub cra crt 
csi cxy err family fg font font.axis font.lab font.main font.sub lend 
lheight ljoin lmitre mex mfcol mfrow mfg mkh omd omi page pch pin plt pty 
smo srt tcl usr xaxp xaxs xaxt xlog xpd yaxp yaxs yaxt ylbias ylog <br />
For more information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>brks</code></td>
<td>

<p>Breaks used for colouring the map (and legend if drawleg = TRUE).
</p>
</td></tr>
<tr><td><code>cols</code></td>
<td>

<p>Colours used for colouring the map (and legend if drawleg = TRUE). Always 
of length length(brks) - 1.
</p>
</td></tr>
<tr><td><code>col_inf</code></td>
<td>

<p>Colour used to draw the lower triangle end in the colour bar (NULL if not 
drawn at all).
</p>
</td></tr>
<tr><td><code>col_sup</code></td>
<td>

<p>Colour used to draw the upper triangle end in the colour bar (NULL if not 
drawn at all).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
1.0  -  2014-07  (V. Guemas)  -  Original code<br />
1.1  -  2015-12  (C. Ardilouze)  - Box(es) drawing<br />
1.2  -  2016-08  (N. Manubens)  - Refacotred the function and 
merged in Jean-Philippe circle
border and Constantin boxes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- matrix(rnorm(100 * 50), 100, 50)
x &lt;- seq(from = 0, to = 360, length.out = 100)
y &lt;- seq(from = -90, to = 90, length.out = 50)
PlotStereoMap(data, x, y, latlims = c(60, 90), brks = 50,
             toptitle = "This is the title")
</code></pre>

<hr>
<h2 id='PlotVsLTime'>Plots A Score Along The Forecast Time With Its Confidence Interval</h2><span id='topic+PlotVsLTime'></span>

<h3>Description</h3>

<p>Plots The Correlation (<code>Corr()</code>) or the Root Mean Square Error 
(<code>RMS()</code>) between the forecasted values and their observational 
counterpart or the slopes of their trends (<code>Trend()</code>) or the 
InterQuartile Range, Maximum-Mininum, Standard Deviation or Median Absolute 
Deviation of the Ensemble Members (<code>Spread()</code>), or the ratio between 
the Ensemble Spread and the RMSE of the Ensemble Mean (<code>RatioSDRMS()</code>) 
along the forecast time for all the input experiments on the same figure 
with their confidence intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotVsLTime(
  var,
  toptitle = "",
  ytitle = "",
  monini = 1,
  freq = 12,
  nticks = NULL,
  limits = NULL,
  listexp = c("exp1", "exp2", "exp3"),
  listobs = c("obs1", "obs2", "obs3"),
  biglab = FALSE,
  hlines = NULL,
  leg = TRUE,
  siglev = FALSE,
  sizetit = 1,
  show_conf = TRUE,
  fileout = "output_plotvsltime.eps",
  width = 8,
  height = 5,
  size_units = "in",
  res = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotVsLTime_+3A_var">var</code></td>
<td>
<p>Matrix containing any Prediction Score with dimensions:<br />
(nexp/nmod, 3/4 ,nltime)<br />
or (nexp/nmod, nobs, 3/4 ,nltime).</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_toptitle">toptitle</code></td>
<td>
<p>Main title, optional.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_ytitle">ytitle</code></td>
<td>
<p>Title of Y-axis, optional.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_monini">monini</code></td>
<td>
<p>Starting month between 1 and 12. Default = 1.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_freq">freq</code></td>
<td>
<p>1 = yearly, 12 = monthly, 4 = seasonal, ... Default = 12.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_nticks">nticks</code></td>
<td>
<p>Number of ticks and labels on the x-axis, optional.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_limits">limits</code></td>
<td>
<p>c(lower limit, upper limit): limits of the Y-axis, optional.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_listexp">listexp</code></td>
<td>
<p>List of experiment names, optional.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_listobs">listobs</code></td>
<td>
<p>List of observation names, optional.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_biglab">biglab</code></td>
<td>
<p>TRUE/FALSE for presentation/paper plot. Default = FALSE.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_hlines">hlines</code></td>
<td>
<p>c(a,b, ..) Add horizontal black lines at Y-positions a,b, ...<br />
Default = NULL.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_leg">leg</code></td>
<td>
<p>TRUE/FALSE if legend should be added or not to the plot. 
Default = TRUE.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_siglev">siglev</code></td>
<td>
<p>TRUE/FALSE if significance level should replace confidence 
interval.<br />
Default = FALSE.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_sizetit">sizetit</code></td>
<td>
<p>Multiplicative factor to change title size, optional.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_show_conf">show_conf</code></td>
<td>
<p>TRUE/FALSE to show/not confidence intervals for input 
variables.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_fileout">fileout</code></td>
<td>
<p>Name of output file. Extensions allowed: eps/ps, jpeg, png, 
pdf, bmp and tiff.<br />
Default = 'output_plotvsltime.eps'</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_width">width</code></td>
<td>
<p>File width, in the units specified in the parameter size_units 
(inches by default). Takes 8 by default.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_height">height</code></td>
<td>
<p>File height, in the units specified in the parameter 
size_units (inches by default). Takes 5 by default.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_size_units">size_units</code></td>
<td>
<p>Units of the size of the device (file or window) to plot 
in. Inches ('in') by default. See ?Devices and the creator function of the 
corresponding device.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_res">res</code></td>
<td>
<p>Resolution of the device (file or window) to plot in. See 
?Devices and the creator function of the corresponding device.</p>
</td></tr>
<tr><td><code id="PlotVsLTime_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the method. Only accepts the following
graphical parameters:<br />
adj ann ask bg bty cex.sub cin col.axis col.lab col.main col.sub cra crt 
csi cxy err family fg fig font font.axis font.lab font.main font.sub 
lheight ljoin lmitre mar mex mfcol mfrow mfg mkh oma omd omi page pch plt 
smo srt tck tcl usr xaxp xaxs xaxt xlog xpd yaxp yaxs yaxt ylbias ylog <br />
For more information about the parameters see 'par'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Examples of input:<br />
Model and observed output from <code>Load()</code> then <code>Clim()</code> then 
<code>Ano()</code> then <code>Smoothing()</code>:<br />
(nmod, nmemb, nsdate, nltime) and (nobs, nmemb, nsdate, nltime)<br />
then averaged over the members<br />
<code>Mean1Dim(var_exp/var_obs, posdim = 2)</code>:<br />
(nmod, nsdate, nltime) and (nobs, nsdate, nltime)<br />
then passed through<br />
<code>Corr(exp, obs, posloop = 1, poscor = 2)</code> or<br />
<code>RMS(exp, obs, posloop = 1, posRMS = 2)</code>:<br />
(nmod, nobs, 3, nltime)<br />
would plot the correlations or RMS between each exp &amp; each obs as a function 
of the forecast time.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-03  (V. Guemas)  -  Original code<br />
0.2  -  2013-03  (I. Andreu-Burillo)  -  Introduced parameter sizetit<br />
0.3  -  2013-10  (I. Andreu-Burillo)  -  Introduced parameter show_conf<br />
1.0  -  2013-11  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
runmean_months &lt;- 12
dim_to_smooth &lt;- 4  # Smooth along lead-times
smooth_ano_exp &lt;- Smoothing(ano_exp, runmean_months, dim_to_smooth)
smooth_ano_obs &lt;- Smoothing(ano_obs, runmean_months, dim_to_smooth)
dim_to_mean &lt;- 2  # Mean along members
required_complete_row &lt;- 3  # Discard startdates for which there are NA leadtimes
leadtimes_per_startdate &lt;- 60
corr &lt;- Corr(Mean1Dim(smooth_ano_exp, dim_to_mean), 
            Mean1Dim(smooth_ano_obs, dim_to_mean), 
            compROW = required_complete_row, 
            limits = c(ceiling((runmean_months + 1) / 2), 
                       leadtimes_per_startdate - floor(runmean_months / 2)))
 
PlotVsLTime(corr, toptitle = "correlations", ytitle = "correlation", 
           monini = 11, limits = c(-1, 2), listexp = c('CMIP5 IC3'), 
           listobs = c('ERSST'), biglab = FALSE, hlines = c(-1, 0, 1), 
           fileout = 'tos_cor.eps')
 

</code></pre>

<hr>
<h2 id='ProbBins'>Computes Probabilistic Information of a Forecast Relative to a Threshold or a Quantile</h2><span id='topic+ProbBins'></span>

<h3>Description</h3>

<p>Compute probabilistic bins of a set of forecast years ('fcyr') relative to 
the forecast climatology over the whole period of anomalies, optionally excluding 
the selected forecast years ('fcyr') or the forecast year for which the 
probabilistic bins are being computed (see 'compPeriod').
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ProbBins(
  ano,
  fcyr = "all",
  thr,
  quantile = TRUE,
  posdates = 3,
  posdim = 2,
  compPeriod = "Full period"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ProbBins_+3A_ano">ano</code></td>
<td>
<p>Array of anomalies from Ano().<br />
Must be of dimension (nexp/nobs, nmemb, nsdates, nleadtime, nlat,  nlon)</p>
</td></tr>
<tr><td><code id="ProbBins_+3A_fcyr">fcyr</code></td>
<td>
<p>Indices of the forecast years of the anomalies which to compute 
the probabilistic bins for, or 'all' to compute the bins for all the 
years.<br />
E.g., c(1:5), c(1, 4), 4 or 'all'.</p>
</td></tr>
<tr><td><code id="ProbBins_+3A_thr">thr</code></td>
<td>
<p>Values used as thresholds to bin the anomalies.</p>
</td></tr>
<tr><td><code id="ProbBins_+3A_quantile">quantile</code></td>
<td>
<p>If quantile is TRUE (default), the threshold ('thr') 
are quantiles.<br />
If quantile is FALSE the thresholds ('thr') introduced are the absolute 
thresholds of the bins.</p>
</td></tr>
<tr><td><code id="ProbBins_+3A_posdates">posdates</code></td>
<td>
<p>Position of the dimension in <code>ano</code> that corresponds to 
the start dates (default = 3).</p>
</td></tr>
<tr><td><code id="ProbBins_+3A_posdim">posdim</code></td>
<td>
<p>Position of the dimension in <code>ano</code> which will be combined 
with 'posdates' to compute the quantiles (default = 2, ensemble members).</p>
</td></tr>
<tr><td><code id="ProbBins_+3A_compperiod">compPeriod</code></td>
<td>
<p>Three options: 
&quot;Full period&quot;/&quot;Without fcyr&quot;/&quot;Cross-validation&quot; (The probabilities are 
computed with the terciles based on ano/ano with all 'fcyr's 
removed/cross-validation). The default is &quot;Full period&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Array with probabilistic information and dimensions:<br />
c(length('thr') + 1, length(fcyr), nmemb/nparam, nmod/nexp/nobs, 
nltime, nlat, nlon)<br />
The values along the first dimension take values 0 or 1 depending on which 
of the 'thr'+1 cathegories the forecast/observation at the corresponding 
grid point, time step, member and starting date belongs to.
</p>


<h3>Author(s)</h3>

<p>History:<br />
1.0  -  2013  (F.Lienert)  -  Original code<br />
2.0  -  2014-03  (N. Gonzalez and V. Torralba)  -  Debugging
2.1  -  2017-02  (V. Torralba and N. Manubens)  -  Fix bug with cross-validation
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path,
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  output = 'lonlat', latmin = 27, latmax = 48, 
                  lonmin = -12, lonmax = 40)
 
## End(Not run)
 
clim &lt;- Clim(sampleMap$mod, sampleMap$obs)
ano_exp &lt;- Ano(sampleMap$mod, clim$clim_exp)
PB &lt;- ProbBins(ano_exp, fcyr = 3, thr = c(1/3, 2/3), quantile = TRUE, posdates = 3,
              posdim = 2)

</code></pre>

<hr>
<h2 id='ProjectField'>Project Anomalies onto Modes of Variability</h2><span id='topic+ProjectField'></span>

<h3>Description</h3>

<p>Project anomalies onto modes of variability to get the temporal evolution of 
the EOF mode selected. Returns principal components (PCs) by 
area-weighted projection onto EOF pattern (from <code>EOF()</code>). 
Able to handle NAs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ProjectField(ano, eof, mode = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ProjectField_+3A_ano">ano</code></td>
<td>
<p>Array of forecast or observational reference anomalies from 
<code>Ano()</code> or <code>Ano_CrossValid</code> with dimensions (number of forecast 
systems, ensemble members, start dates, forecast horizons, latitudes, 
longitudes).</p>
</td></tr>
<tr><td><code id="ProjectField_+3A_eof">eof</code></td>
<td>
<p>R object with EOFs from <code>EOF</code>.</p>
</td></tr>
<tr><td><code id="ProjectField_+3A_mode">mode</code></td>
<td>
<p>Variability mode number in the provided EOF object which to 
project onto.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Array of principal components in verification format (number of 
forecast systems, ensemble members, start dates, forecast horizons).
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-03  (F. Lienert)  -  Original code<br />
0.2  -  2014-03  (Lauriane Batte)  -  Bug-fixes:<br />
1- Extra weighting of the anomalies before projection.<br />
2- Reversion of the anomalies along latitudes.<br />
3- Extra-normalisation not necessary.<br />
0.3  -  2014-03  (Virginie Guemas)  -  Bug-fixes:<br />
1- Another extra-normalisation.<br />
2- 15 lines to compute the em reduced to 1.
0.4  -  2014-03  (Lauriane Batte)  -  Normalization<br />
by std before returning PCs to be coherent with EOF().<br />
0.5  -  2014-04  (Virginie Guemas)  - Fixes:<br />
1- Removal of lon, lat, ncpu and neofs argument unused<br />
2- Security checks ano and eof consistency<br />
3- Removal of the mask which is already contained in the EOFs<br />
4- Removal of the PC normalization since we have chosen in<br />
<code>EOF()</code> to normalize the EOFs and multiply the PCs by the normalization<br />
factor and the eigenvalue so that the restitution of the original field is <br />
done simply by PC * EOFs<br />
5 - The new convention in <code>EOF()</code> is to divide by the weights<br />
so that the reconstruction of the original field rather than the weighted<br />
field is obtained by PC * EOFs. The EOFs need therefore to be multiplied <br />
back by the weights before projection so that EOF * t(EOF) = 1<br />
6 - Since W *X = PC * EOF if EOF is multiplied back by the weights,<br />
PC = W * X * t(EOF) and X the input field to be projected (X) needs to be<br />
multiplied by W. Getting input dimensions.
1.0  -  2016-03  (N. Manubens)  -  Formatting to R CRAN<br />
(J.-P. Baudouin)  -  Example code and testing
</p>


<h3>See Also</h3>

<p>EOF, NAO, PlotBoxWhisker
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path, 
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path, 
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
# Now ready to compute the EOFs and project.
ano &lt;- Ano_CrossValid(sampleData$mod, sampleData$obs)

# Compute the EOF of the observation.
eof &lt;- EOF(ano$ano_obs[1, 1, , 1, , ], sampleData$lon, sampleData$lat)
# check the first mode represent the NAO
PlotEquiMap(eof$EOFs[1, , ], sampleData$lon, sampleData$lat, filled.continents = FALSE)

mode1_exp &lt;- ProjectField(ano$ano_exp, eof, 1)
mode1_obs &lt;- ProjectField(ano$ano_obs, eof, 1)

# Plot the forecast and the observation of the first mode
# for the last year of forecast
plot(mode1_obs[1, 1, dim(sampleData$mod)[3], ], type = "l", ylim = c(-1, 1), lwd = 2)
for (i in 1:dim(sampleData$mod)[2]) {
 par(new = TRUE)
 plot(mode1_exp[1, i, dim(sampleData$mod)[3], ], type = "l", col = rainbow(10)[i], 
      ylim = c(-15000, 15000))
}

</code></pre>

<hr>
<h2 id='RatioRMS'>Computes the Ratio Between The RMSE of Two Experiments</h2><span id='topic+RatioRMS'></span><span id='topic+.RatioRMS'></span>

<h3>Description</h3>

<p>Calculates the ratio of the RMSE for two forecasts of the same observations.<br />
The ratio RMSE(ens, obs) / RMSE(ens.ref, obs) is output.<br />
The p-value is provided by a two-sided Fischer test.<br /><br />
.RatioRMS provides the same functionality but taking two matrices of 
ensemble members (ens and ens.ref) as input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RatioRMS(var_exp1, var_exp2, var_obs, posRMS = 1, pval = TRUE)

.RatioRMS(exp, exp_ref, obs, pval = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RatioRMS_+3A_var_exp1">var_exp1</code></td>
<td>
<p>Array of experimental data 1.</p>
</td></tr>
<tr><td><code id="RatioRMS_+3A_var_exp2">var_exp2</code></td>
<td>
<p>Array of experimental data 2.</p>
</td></tr>
<tr><td><code id="RatioRMS_+3A_var_obs">var_obs</code></td>
<td>
<p>Array of observations.</p>
</td></tr>
<tr><td><code id="RatioRMS_+3A_posrms">posRMS</code></td>
<td>
<p>Dimension along which the RMSE are to be computed = the 
position of the start dates.</p>
</td></tr>
<tr><td><code id="RatioRMS_+3A_pval">pval</code></td>
<td>
<p>Whether to compute the p-value of Ho : RMSE1/RMSE2 = 1 or not. 
TRUE by default.</p>
</td></tr>
<tr><td><code id="RatioRMS_+3A_exp">exp</code></td>
<td>
<p>Matrix of experimental data 1.</p>
</td></tr>
<tr><td><code id="RatioRMS_+3A_exp_ref">exp_ref</code></td>
<td>
<p>Matrix of experimental data 2.</p>
</td></tr>
<tr><td><code id="RatioRMS_+3A_obs">obs</code></td>
<td>
<p>Vector of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>RatioRMS:<br />
Matrix with the same dimensions as var_exp1/var_exp2/var_obs except along 
posRMS where the dimension has length 2 if 'pval = TRUE', or 1 otherwise. 
The dimension of length 2 corresponds to the ratio between the RMSE 
(RMSE1/RMSE2) and the p-value of the two-sided Fisher test with Ho: 
RMSE1/RMSE2 = 1.<br /><br />
.RatioRMS:<br />
</p>

<ul>
<li><p>ratiormsThe ratio of the RMSE of the two experimental datasets
</p>
</li>
<li><p>p_valThe p-value
</p>
</li></ul>



<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-11  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN<br />
1.1  -  2017-02  (A. Hunter)  -  Adapted to veriApply()
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path,
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  output = 'lonlat', latmin = 27, latmax = 48, 
                  lonmin = -12, lonmax = 40)
 
## End(Not run)
 
# Compute DJF seasonal means and anomalies.
leadtimes_dimension &lt;- 4
initial_month &lt;- 11
mean_start_month &lt;- 12
mean_stop_month &lt;- 2                                
sampleData$mod &lt;- Season(sampleData$mod, leadtimes_dimension, initial_month, 
                        mean_start_month, mean_stop_month)
sampleData$obs &lt;- Season(sampleData$obs, leadtimes_dimension, initial_month, 
                        mean_start_month, mean_stop_month)                              
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
# Generate two experiments with 2 and 1 members from the only experiment 
# available in the sample data. Take only data values for a single forecast
# time step.
ano_exp_1 &lt;- Subset(ano_exp, 'member', c(1, 2))
ano_exp_2 &lt;- Subset(ano_exp, 'member', c(3))
ano_exp_1 &lt;- Subset(ano_exp_1, c('dataset', 'ftime'), list(1, 1), drop = 'selected')
ano_exp_2 &lt;- Subset(ano_exp_2, c('dataset', 'ftime'), list(1, 1), drop = 'selected')
ano_obs &lt;- Subset(ano_obs, c('dataset', 'ftime'), list(1, 1), drop = 'selected')
# Compute ensemble mean and provide as inputs to RatioRMS.
rrms &lt;- RatioRMS(Mean1Dim(ano_exp_1, 1), 
                Mean1Dim(ano_exp_2, 1), 
                Mean1Dim(ano_obs, 1))
# Plot the RatioRMS for the first forecast time step.
PlotEquiMap(rrms[1, , ], sampleData$lon, sampleData$lat, 
           toptitle = 'Ratio RMSE')

# The following example uses veriApply combined with .RatioRMS instead of RatioRMS
 ## Not run: 
require(easyVerification)
# The name of the function has to end in 'ss' in order for veriApply() to 
# detect it as a skill score.
RatioRMSss &lt;- s2dverification:::.RatioRMS
rrms2 &lt;- veriApply("RatioRMSss", ano_exp_1,
                  # see ?veriApply for how to use the 'parallel' option
                  Mean1Dim(ano_obs, 1),
                  ano_exp_2,
                  tdim = 2, ensdim = 1)
 
## End(Not run)
</code></pre>

<hr>
<h2 id='RatioSDRMS'>Computes the ratio between the ensemble spread and RMSE</h2><span id='topic+RatioSDRMS'></span><span id='topic+.RatioSDRMS'></span>

<h3>Description</h3>

<p>Arrays var_exp &amp; var_obs should have dimensions between<br />
c(nmod/nexp, nmemb/nparam, nsdates, nltime)<br />
and<br />
c(nmod/nexp, nmemb/nparam, nsdates, nltime, nlevel, nlat, nlon)<br />
The ratio between the standard deviation of the members around the ensemble 
mean in var_exp and the RMSE between var_exp and var_obs is output for each 
experiment and each observational dataset.<br />
The p-value is provided by a one-sided Fischer test.<br /><br />
.RatioSDRMS provides the same functionality but taking a matrix of ensemble 
members as input (exp).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RatioSDRMS(var_exp, var_obs, pval = TRUE)

.RatioSDRMS(exp, obs, pval = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RatioSDRMS_+3A_var_exp">var_exp</code></td>
<td>
<p>Model data:<br />
c(nmod/nexp, nmemb/nparam, nsdates, nltime) up to<br />
c(nmod/nexp, nmemb/nparam, nsdates, nltime, nlevel, nlat, nlon)</p>
</td></tr>
<tr><td><code id="RatioSDRMS_+3A_var_obs">var_obs</code></td>
<td>
<p>Observational data:<br />
c(nobs, nmemb, nsdates, nltime) up to<br />
c(nobs, nmemb, nsdates, nltime, nlevel, nlat, nlon)</p>
</td></tr>
<tr><td><code id="RatioSDRMS_+3A_pval">pval</code></td>
<td>
<p>Whether to compute the p-value of Ho : SD/RMSE = 1 or not.</p>
</td></tr>
<tr><td><code id="RatioSDRMS_+3A_exp">exp</code></td>
<td>
<p>N by M matrix of N forecasts from M ensemble members.</p>
</td></tr>
<tr><td><code id="RatioSDRMS_+3A_obs">obs</code></td>
<td>
<p>Vector of the corresponding observations of length N.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>RatioSDRMS: Array with dimensions c(nexp/nmod, nobs, 1 or 2, nltime) 
up to c(nexp/nmod, nobs, 1 or 2, nltime, nlevel, nlat, nlon).<br />
The 3rd dimension corresponds to the ratio (SD/RMSE) and the p.value 
(only present if <code>pval = TRUE</code>) of the one-sided Fisher test with 
Ho: SD/RMSE = 1.<br /><br />
.RatioSDRMS:
</p>

<ul>
<li><p>$ratio
The ratio of the ensemble spread and RMSE, 

</p>
</li>
<li><p>$p_val
Corresponds to the p values of the ratio (only present if 
<code>pval = TRUE</code>).

</p>
</li></ul>



<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-12  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN<br />
1.1  -  2017-02  (A. Hunter)  -  Adapted to veriApply()
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
rsdrms &lt;- RatioSDRMS(sampleData$mod, sampleData$obs)
# Reorder the data in order to plot it with PlotVsLTime
rsdrms_plot &lt;- array(dim = c(dim(rsdrms)[1:2], 4, dim(rsdrms)[4]))
rsdrms_plot[, , 2, ] &lt;- rsdrms[, , 1, ]
rsdrms_plot[, , 4, ] &lt;- rsdrms[, , 2, ]
 
PlotVsLTime(rsdrms_plot, toptitle = "Ratio ensemble spread / RMSE", ytitle = "", 
           monini = 11, limits = c(-1, 1.3), listexp = c('CMIP5 IC3'), 
           listobs = c('ERSST'), biglab = FALSE, siglev = TRUE, 
           fileout = 'tos_rsdrms.eps')
 

# The following example uses veriApply combined with .RatioSDRMS instead of RatioSDRMS
 ## Not run: 
require(easyVerification)  
RatioSDRMS2 &lt;- s2dverification:::.RatioSDRMS
rsdrms2 &lt;- veriApply("RatioSDRMS2",
                    sampleData$mod,
                    # see ?veriApply for how to use the 'parallel' option
                    Mean1Dim(sampleData$obs, 2),
                    tdim = 3, ensdim = 2)
 
## End(Not run)
</code></pre>

<hr>
<h2 id='Regression'>Computes The Regression Of An Array On Another Along A Dimension</h2><span id='topic+Regression'></span>

<h3>Description</h3>

<p>Computes the regression of the input matrice vary on the input matrice varx 
along the posREG dimension by least square fitting. Provides the slope of 
the regression, the associated confidence interval, and the intercept.<br />
Provides also the vary data filtered out from the regression onto varx.<br />
The confidence interval relies on a student-T distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Regression(vary, varx, posREG = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Regression_+3A_vary">vary</code></td>
<td>
<p>Array of any number of dimensions up to 10.</p>
</td></tr>
<tr><td><code id="Regression_+3A_varx">varx</code></td>
<td>
<p>Array of any number of dimensions up to 10. 
Same dimensions as vary.</p>
</td></tr>
<tr><td><code id="Regression_+3A_posreg">posREG</code></td>
<td>
<p>Position along which to compute the regression.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>$regression</code></td>
<td>

<p>Array with same dimensions as varx and vary except along posREG 
dimension which is replaced by a length 4 dimension, corresponding 
to the lower limit of the 95% confidence interval, the slope, 
the upper limit of the 95% confidence interval and the intercept.
</p>
</td></tr>
<tr><td><code>$filtered</code></td>
<td>

<p>Same dimensions as vary filtered out from the regression onto varx 
along the posREG dimension.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2013-05  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path,
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  output = 'lonlat', latmin = 27, latmax = 48, 
                  lonmin = -12, lonmax = 40)
 
## End(Not run)
 
sampleData$mod &lt;- Season(sampleData$mod, 4, 11, 12, 2)
sampleData$obs &lt;- Season(sampleData$obs, 4, 11, 12, 2)
reg &lt;- Regression(Mean1Dim(sampleData$mod, 2),
                 Mean1Dim(sampleData$obs, 2), 2)
PlotEquiMap(reg$regression[1, 2, 1, , ], sampleData$lon, sampleData$lat, 
           toptitle='Regression of the prediction on the observations', 
           sizetit = 0.5)

</code></pre>

<hr>
<h2 id='RMS'>Computes Root Mean Square Error</h2><span id='topic+RMS'></span><span id='topic+.RMS'></span>

<h3>Description</h3>

<p>Computes the root mean square error for an array of forecasts, var_exp and 
an array of observations, var_obs, which should have the same dimensions 
except along the posloop dimension where the lengths can be different, with 
the number of experiments/models for var_exp (nexp) and the number of 
obserational datasets for var_obs (nobs).<br />
The RMSE is computed along the posRMS dimension which should correspond to 
the startdate dimension.<br />
If compROW is given, the RMSE is computed only if rows along the compROW 
dimension are complete between limits[1] and limits[2], i.e. there are no 
NAs between limits[1] and limits[2]. This option can be activated if the 
user wishes to account only for the forecasts for which observations are 
available at all leadtimes.<br />
Default: limits[1] = 1 and limits[2] = length(compROW dimension).<br />
The confidence interval relies on a chi2 distribution.<br /><br />
.RMS provides the same functionality but taking a matrix of ensemble 
members as input (exp).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RMS(
  var_exp,
  var_obs,
  posloop = 1,
  posRMS = 2,
  compROW = NULL,
  limits = NULL,
  siglev = 0.95,
  conf = TRUE
)

.RMS(exp, obs, siglev = 0.95, conf = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RMS_+3A_var_exp">var_exp</code></td>
<td>
<p>Matrix of experimental data.</p>
</td></tr>
<tr><td><code id="RMS_+3A_var_obs">var_obs</code></td>
<td>
<p>Matrix of observational data, same dimensions as var_exp 
except along posloop dimension, where the length can be nobs instead of 
nexp.</p>
</td></tr>
<tr><td><code id="RMS_+3A_posloop">posloop</code></td>
<td>
<p>Dimension nobs and nexp.</p>
</td></tr>
<tr><td><code id="RMS_+3A_posrms">posRMS</code></td>
<td>
<p>Dimension along which RMSE are to be computed (the dimension 
of the start dates).</p>
</td></tr>
<tr><td><code id="RMS_+3A_comprow">compROW</code></td>
<td>
<p>Data taken into account only if (compROW)th row is complete.<br />
Default = NULL.</p>
</td></tr>
<tr><td><code id="RMS_+3A_limits">limits</code></td>
<td>
<p>Complete between limits[1] &amp; limits[2]. Default = NULL.</p>
</td></tr>
<tr><td><code id="RMS_+3A_siglev">siglev</code></td>
<td>
<p>Confidence level of the computed confidence interval. 0.95 
by default.</p>
</td></tr>
<tr><td><code id="RMS_+3A_conf">conf</code></td>
<td>
<p>Whether to compute confidence interval or not. TRUE by default.</p>
</td></tr>
<tr><td><code id="RMS_+3A_exp">exp</code></td>
<td>
<p>N by M matrix of N forecasts from M ensemble members.</p>
</td></tr>
<tr><td><code id="RMS_+3A_obs">obs</code></td>
<td>
<p>Vector of the corresponding observations of length N.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>RMS: Array with dimensions:<br />
c(length(posloop) in var_exp, length(posloop) in var_obs, 1 or 3, all 
other dimensions of var_exp &amp; var_obs except posRMS).<br />
The 3rd dimension corresponds to the lower limit of the  95% confidence 
interval (only present if <code>conf = TRUE</code>), the RMSE, and the upper 
limit of the 95% confidence interval (only present if 
<code>conf = TRUE</code>).<br /><br />
.RMS: 
</p>
<table>
<tr><td><code>$rms</code></td>
<td>

<p>The root mean square error, 
</p>
</td></tr>
<tr><td><code>$conf_low</code></td>
<td>

<p>Corresponding to the lower limit of the <code>siglev</code>% confidence interval 
(only present if <code>conf = TRUE</code>) for the rms.
</p>
</td></tr> 
<tr><td><code>$conf_high</code></td>
<td>

<p>Corresponding to the upper limit of the <code>siglev</code>% confidence interval 
(only present if <code>conf = TRUE</code>) for the rms.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-05  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN<br />
1.1  -  2017-02  (A. Hunter)  -  Adapted to veriApply()
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
runmean_months &lt;- 12
dim_to_smooth &lt;- 4  # Smooth along lead-times
smooth_ano_exp &lt;- Smoothing(ano_exp, runmean_months, dim_to_smooth)
smooth_ano_obs &lt;- Smoothing(ano_obs, runmean_months, dim_to_smooth)
dim_to_mean &lt;- 2  # Mean along members
# Discard start-dates for which some leadtimes are missing
required_complete_row &lt;- 3
leadtimes_per_startdate &lt;- 60
rms &lt;- RMS(Mean1Dim(smooth_ano_exp, dim_to_mean), 
          Mean1Dim(smooth_ano_obs, dim_to_mean), 
          compROW = required_complete_row, 
          limits = c(ceiling((runmean_months + 1) / 2), 
                     leadtimes_per_startdate - floor(runmean_months / 2)))
 
PlotVsLTime(rms, toptitle = "Root Mean Square Error", ytitle = "K", 
           monini = 11, limits = NULL, listexp = c('CMIP5 IC3'), 
           listobs = c('ERSST'), biglab = FALSE, hlines = c(0), 
           fileout = 'tos_rms.eps')
 
# The following example uses veriApply combined with .RMS instead of RMS
 ## Not run: 
require(easyVerification)
RMS2 &lt;- s2dverification:::.RMS
rms2 &lt;- veriApply("RMS2",
                 smooth_ano_exp,
                 # see ?veriApply for how to use the 'parallel' option
                 Mean1Dim(smooth_ano_obs, dim_to_mean),
                 tdim = 3, ensdim = 2)
 
## End(Not run)

</code></pre>

<hr>
<h2 id='RMSSS'>Computes Root Mean Square Skill Score</h2><span id='topic+RMSSS'></span><span id='topic+.RMSSS'></span>

<h3>Description</h3>

<p>Computes the root mean square error skill score between an array of 
forecasts, var_exp and an array of observations, var_obs, which should 
have the same dimensions except along posloop where the lengths can be 
different, with the number of experiments/models for var_exp (nexp) and 
the number of obserational datasets for var_obs (nobs).<br />
RMSSS computes the Root Mean Square Skill Score of each jexp in 1:nexp 
against each jobs in 1:nobs which gives nexp x nobs RMSSS for each other 
grid point of the matrix (each latitude/longitude/level/leadtime).<br />
The RMSSS are computed along the posRMS dimension which should correspond 
to the startdate dimension.<br />
The p-value is optionally provided by a one-sided Fisher test.<br /><br />
.RMSSS provides the same functionality but taking a matrix of ensemble 
members as input (exp).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RMSSS(var_exp, var_obs, posloop = 1, posRMS = 2, pval = TRUE)

.RMSSS(exp, obs, pval = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RMSSS_+3A_var_exp">var_exp</code></td>
<td>
<p>Array of experimental data.</p>
</td></tr>
<tr><td><code id="RMSSS_+3A_var_obs">var_obs</code></td>
<td>
<p>Array of observational data, same dimensions as var_exp 
except along posloop dimension, where the length can be nobs instead of 
nexp.</p>
</td></tr>
<tr><td><code id="RMSSS_+3A_posloop">posloop</code></td>
<td>
<p>Dimension nobs and nexp.</p>
</td></tr>
<tr><td><code id="RMSSS_+3A_posrms">posRMS</code></td>
<td>
<p>Dimension along which the RMSE are to be computed (the 
dimension of the start dates).</p>
</td></tr>
<tr><td><code id="RMSSS_+3A_pval">pval</code></td>
<td>
<p>Whether to compute or not the p-value of the test Ho : 
RMSSS = 0. TRUE by default.</p>
</td></tr>
<tr><td><code id="RMSSS_+3A_exp">exp</code></td>
<td>
<p>N by M matrix of N forecasts from M ensemble members.</p>
</td></tr>
<tr><td><code id="RMSSS_+3A_obs">obs</code></td>
<td>
<p>Vector of the corresponding observations of length N.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>RMSSS: Array with dimensions:<br />
c(length(posloop) in var_exp, length(posloop) in var_obs, 1 or 2, 
all other dimensions of var_exp &amp; var_obs except posRMS).<br />
The 3rd dimension corresponds to the RMSSS and, if <code>pval = TRUE</code>, 
the p-value of the one-sided Fisher test with Ho: RMSSS = 0.<br /><br />
.RMSSS:
</p>

<ul>
<li><p>$rmsss
The RMSSS.

</p>
</li>
<li><p>$p_val
Corresponds to the p values (only present if <code>pval = TRUE</code>) for 
the RMSSS.

</p>
</li></ul>



<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-04  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN<br />
1.1  -  2017-02  (A. Hunter)  -  Adapted to veriApply()
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
rmsss &lt;- RMSSS(Mean1Dim(ano_exp, 2), Mean1Dim(ano_obs, 2))
rmsss_plot &lt;- array(dim = c(dim(rmsss)[1:2], 4, dim(rmsss)[4]))
rmsss_plot[, , 2, ] &lt;- rmsss[, , 1, ]
rmsss_plot[, , 4, ] &lt;- rmsss[, , 2, ]
 
PlotVsLTime(rmsss_plot, toptitle = "Root Mean Square Skill Score", ytitle = "", 
           monini = 11, limits = c(-1, 1.3), listexp = c('CMIP5 IC3'), 
           listobs = c('ERSST'), biglab = FALSE, hlines = c(-1, 0, 1), 
           fileout = 'tos_rmsss.eps')
 
# The following example uses veriApply combined with .RMSSS instead of RMSSS
 ## Not run: 
require(easyVerification)
RMSSS2 &lt;- s2dverification:::.RMSSS
rmsss2 &lt;- veriApply("RMSSS2", ano_exp,
                   # see ?veriApply for how to use the 'parallel' option
                   Mean1Dim(ano_obs, 2),
                   tdim = 3, ensdim = 2)
 
## End(Not run)
</code></pre>

<hr>
<h2 id='sampleDepthData'>Sample of Experimental Data for Forecast Verification In Function Of 
Latitudes And Depths</h2><span id='topic+sampleDepthData'></span>

<h3>Description</h3>

<p>This data set provides data in function of latitudes and depths for the 
variable 'tos', i.e. sea surface temperature, from the decadal climate 
prediction experiment run at IC3 in the context of the CMIP5 project.<br /> 
Its name within IC3 local database is 'i00k'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sampleDepthData)
</code></pre>


<h3>Format</h3>

<p>The data set provides with a variable named 'sampleDepthData'.<br /><br />
</p>
<p>sampleDepthData$exp is an array that contains the experimental data and the 
dimension meanings and values are:<br />
c(# of experimental datasets, # of members, # of starting dates, 
# of lead-times, # of depths, # of latitudes)<br />
c(1, 5, 3, 60, 7, 21)<br /><br />
</p>
<p>sampleDepthData$obs should be an array that contained the observational data 
but in this sample is not defined (NULL).<br /><br />
</p>
<p>sampleDepthData$depths is an array with the 7 longitudes covered by the data.<br /><br />
</p>
<p>sampleDepthData$lat is an array with the 21 latitudes covered by the data.<br /><br /></p>


<h3>Author(s)</h3>

<p>Nicolau Manubens
</p>

<hr>
<h2 id='sampleMap'>Sample Of Observational And Experimental Data For Forecast Verification In Function Of Longitudes And Latitudes</h2><span id='topic+sampleMap'></span>

<h3>Description</h3>

<p>This data set provides data in function of longitudes and latitudes for the variable 'tos', i.e. sea surface temperature, over the mediterranean zone from the sample experimental and observational datasets attached to the package. See examples on how to use Load() for details.<br /><br />
The data is provided through a variable named 'sampleMap' and is structured as expected from the 'Load()' function in the 's2dverification' package if was called as follows:<br /><br />
</p>
<pre>
data_path &lt;- system.file('sample_data', package = 's2dverification')
exp &lt;- list(
        name = 'experiment',
        path = file.path(data_path, 'model/$EXP_NAME$/monthly_mean',
                         '$VAR_NAME$_3hourly/$VAR_NAME$_$START_DATES$.nc')
      )
obs &lt;- list(
        name = 'observation',
        path = file.path(data_path, 'observation/$OBS_NAME$/monthly_mean',
                         '$VAR_NAME$/$VAR_NAME$_$YEAR$$MONTH$.nc')
      )
# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(exp), list(obs), startDates,
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 </pre>
<p>Check the documentation on 'Load()' in the package 's2dverification' for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sampleMap)
</code></pre>


<h3>Format</h3>

<p>The data set provides with a variable named 'sampleMap'.<br /><br />
</p>
<p>sampleMap$mod is an array that contains the experimental data and the dimension meanings and values are:<br />
c(# of experimental datasets, # of members, # of starting dates, # of lead-times, # of latitudes, # of longitudes)<br />
c(1, 3, 5, 60, 2, 3)<br /><br />
</p>
<p>sampleMap$obs is an array that contains the observational data and the dimension meanings and values are:<br />
c(# of observational datasets, # of members, # of starting dates, # of lead-times, # of latitudes, # of longitudes)<br />
c(1, 1, 5, 60, 2, 3)<br /><br />
</p>
<p>sampleMap$lat is an array with the 2 latitudes covered by the data (see examples on Load() for details on why such low resolution).<br /><br />
</p>
<p>sampleMap$lon is an array with the 3 longitudes covered by the data (see examples on Load() for details on why such low resolution).</p>


<h3>Author(s)</h3>

<p>Nicolau Manubens
</p>

<hr>
<h2 id='sampleTimeSeries'>Sample Of Observational And Experimental Data For Forecast Verification As Area Averages</h2><span id='topic+sampleTimeSeries'></span>

<h3>Description</h3>

<p>This data set provides area averaged data for the variable 'tos', i.e. sea 
surface temperature, over the mediterranean zone from the example datasets 
attached to the package. See examples on Load() for more details.<br /><br />
The data is provided through a variable named 'sampleTimeSeries' and is 
structured as expected from the 'Load()' function in the 's2dverification' 
package if was called as follows:<br /><br />
</p>
<pre>
data_path &lt;- system.file('sample_data', package = 's2dverification')
exp &lt;- list(
        name = 'experiment',
        path = file.path(data_path, 'model/$EXP_NAME$/monthly_mean',
                         '$VAR_NAME$_3hourly/$VAR_NAME$_$START_DATES$.nc')
      )
obs &lt;- list(
        name = 'observation',
        path = file.path(data_path, 'observation/$OBS_NAME$/monthly_mean',
                         '$VAR_NAME$/$VAR_NAME$_$YEAR$$MONTH$.nc')
      )
# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(exp), list(obs), startDates,
                  output = 'areave', latmin = 27, latmax = 48, lonmin = -12,
                  lonmax = 40)
 </pre>
<p>Check the documentation on 'Load()' in the package 's2dverification' for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sampleTimeSeries)
</code></pre>


<h3>Format</h3>

<p>The data set provides with a variable named 'sampleTimeSeries'.<br /><br />
</p>
<p>sampleTimeSeries$mod is an array that contains the experimental data and the dimension meanings and values are:<br />
c(# of experimental datasets, # of members, # of starting dates, # of lead-times)<br />
c(1, 3, 5, 60)<br /><br />
</p>
<p>sampleTimeSeries$obs is an array that contains the observational data and the dimension meanings and values are:<br />
c(# of observational datasets, # of members, # of starting dates, # of lead-times)<br />
c(1, 1, 5, 60)<br /><br />
</p>
<p>sampleTimeSeries$lat is an array with the 2 latitudes covered by the data that was area averaged to calculate the time series (see examples on Load() for details on why such low resolution).<br /><br />
</p>
<p>sampleTimeSeries$lon is an array with the 3 longitudes covered by the data that was area averaged to calculate the time series (see examples on Load() for details on why such low resolution).</p>


<h3>Author(s)</h3>

<p>Nicolau Manubens
</p>

<hr>
<h2 id='Season'>Computes Seasonal Means</h2><span id='topic+Season'></span>

<h3>Description</h3>

<p>Computes seasonal means on timeseries organized in a array of any number of 
dimensions up to 10 dimensions where the time dimension is one of those 10 
dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Season(var, posdim = 4, monini, moninf, monsup)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Season_+3A_var">var</code></td>
<td>
<p>Array containing the timeseries along one of its dimensions.</p>
</td></tr>
<tr><td><code id="Season_+3A_posdim">posdim</code></td>
<td>
<p>Dimension along which to compute seasonal means = Time 
dimension.</p>
</td></tr>
<tr><td><code id="Season_+3A_monini">monini</code></td>
<td>
<p>an integer indicating the first month of the time series: 1 to 
12.</p>
</td></tr>
<tr><td><code id="Season_+3A_moninf">moninf</code></td>
<td>
<p>an integer indicating the month when to start the seasonal 
means: 1 to 12.</p>
</td></tr>
<tr><td><code id="Season_+3A_monsup">monsup</code></td>
<td>
<p>an integer indicating the month when to stop the seasonal 
means: 1 to 12.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Array with the same dimensions as var except along the posdim 
dimension whose length corresponds to the number of seasons. Partial 
seasons are not accounted for.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-03  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
leadtimes_dimension &lt;- 4
initial_month &lt;- 11
mean_start_month &lt;- 12
mean_stop_month &lt;- 2
season_means_mod &lt;- Season(sampleData$mod, leadtimes_dimension, initial_month,
                          mean_start_month, mean_stop_month)
season_means_obs &lt;- Season(sampleData$obs, leadtimes_dimension, initial_month,
                          mean_start_month, mean_stop_month)
 
PlotAno(season_means_mod, season_means_obs, startDates, 
       toptitle = paste('winter (DJF) temperatures'), ytitle = c('K'), 
       legends = 'ERSST', biglab = FALSE, fileout = 'tos_season_means.eps')
 
</code></pre>

<hr>
<h2 id='SelIndices'>Slices A Matrix Along A Dimension</h2><span id='topic+SelIndices'></span>

<h3>Description</h3>

<p>This function selects a subset of ensemble members from an array containing 
any number of dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SelIndices(var, posdim, limits)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SelIndices_+3A_var">var</code></td>
<td>
<p>An array with any number of dimensions.</p>
</td></tr>
<tr><td><code id="SelIndices_+3A_posdim">posdim</code></td>
<td>
<p>The dimension along which the ensemble subset should be 
selected.</p>
</td></tr>
<tr><td><code id="SelIndices_+3A_limits">limits</code></td>
<td>
<p>The lower and upper limits for the selection of ensemble 
members along the posdim dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The subsetted array.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-04  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- array(rnorm(24), dim = c(2, 3, 4, 1))
print(a)
print(a[, , 2:3, ])
print(dim(a[, , 2:3, ]))
print(SelIndices(a, 3, c(2, 3)))
print(dim(SelIndices(a, 3, c(2, 3))))

</code></pre>

<hr>
<h2 id='Smoothing'>Smoothes An Array Along A Dimension</h2><span id='topic+Smoothing'></span>

<h3>Description</h3>

<p>Smoothes an array of any number of dimensions along one of its dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Smoothing(var, runmeanlen = 12, numdimt = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Smoothing_+3A_var">var</code></td>
<td>
<p>Array to be smoothed along one of its dimension (typically the 
forecast time dimension).</p>
</td></tr>
<tr><td><code id="Smoothing_+3A_runmeanlen">runmeanlen</code></td>
<td>
<p>Running mean length in number of sampling units 
(typically months).</p>
</td></tr>
<tr><td><code id="Smoothing_+3A_numdimt">numdimt</code></td>
<td>
<p>Dimension to smooth.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Array with same the dimensions as 'var' but smoothed along the 
'numdimt'-th dimension.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-03  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to R CRAN<br />
1.1  -  2015-05  (N. Manubens)  -  Adding
security checks, fixing computation in cases where runmeanlen is odd and 
making it able to work on arrays of any number of dimensions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
runmean_months &lt;- 12
dim_to_smooth &lt;- 4  # Smooth along lead-times
smooth_ano_exp &lt;- Smoothing(ano_exp, runmean_months, dim_to_smooth)
smooth_ano_obs &lt;- Smoothing(ano_obs, runmean_months, dim_to_smooth)
 
PlotAno(smooth_ano_exp, smooth_ano_obs, startDates, 
       toptitle = "Smoothed Mediterranean mean SST", ytitle = "K",
       fileout = "tos_smoothed_ano.eps")
 
</code></pre>

<hr>
<h2 id='Spectrum'>Estimates Frequency Spectrum</h2><span id='topic+Spectrum'></span>

<h3>Description</h3>

<p>This function estimates the frequency spectrum of the xdata array together 
with its 95% and 99% significance level. The output is provided as an 
array with dimensions c(number of frequencies, 4). The column contains the 
frequency values, the power, the 95% significance level and the 99% one.<br />
The spectrum estimation relies on a R built-in function and the significance 
levels are estimated by a Monte-Carlo method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Spectrum(xdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Spectrum_+3A_xdata">xdata</code></td>
<td>
<p>Array of which the frequency spectrum is required.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Frequency spectrum with dimensions c(number of frequencies, 4). The 
column contains the frequency values, the power, the 95% significance 
level and the 99% one.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2012-02  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)

ensmod &lt;- Mean1Dim(sampleData$mod, 2)
for (jstartdate in 1:3) {
 spectrum &lt;- Spectrum(ensmod[1, jstartdate, ])
 for (jlen in 1:dim(spectrum)[1]) {
   if (spectrum[jlen, 2] &gt; spectrum[jlen, 4]) {
     ensmod[1, jstartdate, ] &lt;- Filter(ensmod[1, jstartdate, ],  
                                       spectrum[jlen, 1])
   }
 }
}
 
PlotAno(InsertDim(ensmod, 2, 1), sdates = startDates, fileout =
       'filtered_ensemble_mean.eps')
 

</code></pre>

<hr>
<h2 id='Spread'>Computes InterQuartile Range, Maximum-Minimum, Standard Deviation and 
Median Absolute Deviation of the Ensemble Members</h2><span id='topic+Spread'></span>

<h3>Description</h3>

<p>Computes the InterQuartile Range, the Maximum minus Mininum, the Standard 
Deviation and the Median Absolute Deviation along the list of dimensions 
provided by the posdim argument (typically along the ensemble member and 
start date dimension).<br />
The confidence interval is optionally computed by bootstrapping.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Spread(var, posdim = 2, narm = TRUE, siglev = 0.95, conf = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Spread_+3A_var">var</code></td>
<td>
<p>Matrix of any number of dimensions up to 10.</p>
</td></tr>
<tr><td><code id="Spread_+3A_posdim">posdim</code></td>
<td>
<p>List of dimensions along which to compute IQR/MaxMin/SD/MAD.</p>
</td></tr>
<tr><td><code id="Spread_+3A_narm">narm</code></td>
<td>
<p>TRUE/FALSE if NA removed/kept for computation. Default = TRUE.</p>
</td></tr>
<tr><td><code id="Spread_+3A_siglev">siglev</code></td>
<td>
<p>Confidence level of the computed confidence interval. 
0.95 by default.</p>
</td></tr>
<tr><td><code id="Spread_+3A_conf">conf</code></td>
<td>
<p>Whether to compute the confidence intervals or not. 
TRUE by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Example:<br />
&mdash;&mdash;&ndash;<br />
To compute IQR, Max-Min, SD &amp; MAD accross the members and start dates of 
var output from <code>Load()</code> or <code>Ano()</code> or <code>Ano_CrossValid()</code>, 
call:<br />
spread(var, posdim = c(2, 3), narm = TRUE)
</p>


<h3>Value</h3>

<p>Matrix with the same dimensions as var except along the first posdim 
dimension which is replaced by a length 1 or 3 dimension, corresponding to 
the lower limit of the <code>siglev</code>% confidence interval 
(only present if <code>conf = TRUE</code>), the spread, and the upper limit of 
the <code>siglev</code>% confidence interval (only present if <code>conf = TRUE</code>) 
for each experiment/leadtime/latitude/longitude.
</p>
<table>
<tr><td><code>$iqr</code></td>
<td>

<p>InterQuartile Range.
</p>
</td></tr>
<tr><td><code>$maxmin</code></td>
<td>

<p>Maximum - Minimum.
</p>
</td></tr>
<tr><td><code>$sd</code></td>
<td>

<p>Standard Deviation.
</p>
</td></tr>
<tr><td><code>$mad</code></td>
<td>

<p>Median Absolute Deviation.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-03  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
runmean_months &lt;- 12
dim_to_smooth &lt;- 4  # Smooth along lead-times
smooth_ano_exp &lt;- Smoothing(ano_exp, runmean_months, dim_to_smooth)
smooth_ano_exp_m_sub &lt;- smooth_ano_exp - InsertDim(Mean1Dim(smooth_ano_exp, 2, 
                       narm = TRUE), 2, dim(smooth_ano_exp)[2])
spread &lt;- Spread(smooth_ano_exp_m_sub, c(2, 3))
 
PlotVsLTime(spread$iqr, 
           toptitle = "Inter-Quartile Range between ensemble members",
           ytitle = "K", monini = 11, limits = NULL, 
           listexp = c('CMIP5 IC3'), listobs = c('ERSST'), biglab = FALSE, 
           hlines = c(0), fileout = 'tos_iqr.eps')
PlotVsLTime(spread$maxmin, toptitle = "Maximum minus minimum of the members", 
           ytitle = "K", monini = 11, limits = NULL, 
           listexp = c('CMIP5 IC3'), listobs = c('ERSST'), biglab = FALSE, 
           hlines = c(0), fileout = 'tos_maxmin.eps')
PlotVsLTime(spread$sd, toptitle = "Standard deviation of the members", 
           ytitle = "K", monini = 11, limits = NULL, 
           listexp = c('CMIP5 IC3'), listobs = c('ERSST'), biglab = FALSE, 
           hlines = c(0), fileout = 'tos_sd.eps')
PlotVsLTime(spread$mad, toptitle = "Median Absolute Deviation of the members",
           ytitle = "K", monini = 11, limits = NULL, 
           listexp = c('CMIP5 IC3'), listobs = c('ERSST'), biglab = FALSE, 
           hlines = c(0), fileout = 'tos_mad.eps')
 

</code></pre>

<hr>
<h2 id='StatSeasAtlHurr'>Compute estimate of seasonal mean of Atlantic hurricane activity</h2><span id='topic+StatSeasAtlHurr'></span>

<h3>Description</h3>

<p>Compute one of G. Villarini's statistically downscaled measure of mean 
Atlantic hurricane activity and its variance. The hurricane activity is 
estimated using seasonal averages of sea surface temperature anomalies over 
the tropical Atlantic (bounded by 10N-25N and 80W-20W) and the tropics at 
large (bounded by 30N-30S). The anomalies are for the JJASON season.<br />
The estimated seasonal average is either 1) number of hurricanes, 2) number 
of tropical cyclones with lifetime &gt;=48h or 3) power dissipation index 
(PDI; in 10^11 m^3 s^-2).<br />
The statistical models used in this function are described in<br />
</p>


<h3>Usage</h3>

<pre><code class='language-R'>StatSeasAtlHurr(atlano = NULL, tropano = NULL, hrvar = "HR")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="StatSeasAtlHurr_+3A_atlano">atlano</code></td>
<td>
<p>Array of Atlantic sea surface temperature anomalies. 
Must have the same dimension as tropano.</p>
</td></tr>
<tr><td><code id="StatSeasAtlHurr_+3A_tropano">tropano</code></td>
<td>
<p>Array of tropical sea surface temperature anomalies. 
Must have the same dimension as atlano.</p>
</td></tr>
<tr><td><code id="StatSeasAtlHurr_+3A_hrvar">hrvar</code></td>
<td>
<p>The seasonal average to be estimated. The options are either<br />
&quot;HR&quot; (hurricanes) <br />
&quot;TC&quot; (tropical cyclones with lifetime &gt;=48h) <br />
&quot;PDI&quot; (power dissipation index) <br /></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list composed of two matrices:<br />
</p>

<ol>
<li>
<p>A matrix (mean) with the seasonal average values of the desired quantity.<br /> 

</p>
</li>
<li>
<p>A matrix (var) of the variance of that quantity.<br />

</p>
</li></ol>

<p>The dimensions of the two matrices are the same as the dimensions of 
atlano/tropano.
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2015-11  (Louis-Philippe Caron)  -  Original code
</p>


<h3>References</h3>

<p>Villarini et al. (2010) Mon Wea Rev, 138, 2681-2705.<br />
Villarini et al. (2012) Mon Wea Rev, 140, 44-65.<br />
Villarini et al. (2012) J Clim, 25, 625-637.<br />
An example of how the function can be used in hurricane forecast studies 
is given in<br />
Caron, L.-P. et al. (2014) Multi-year prediction skill of Atlantic hurricane 
activity in CMIP5 decadal hindcasts. Climate Dynamics, 42, 2675-2690. 
doi:10.1007/s00382-013-1773-1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Let AtlAno represents 5 different 5-year forecasts of seasonally averaged 
# Atlantic sea surface temperature anomalies.
AtlAno &lt;- matrix(c(-0.31, -0.36, 0.26, -0.16, -0.16, 
                  -0.06, -0.22, -0.31, -0.36, -0.39, 
                   0.20, -0.14, 0.12, 0.22, 0.02,
                  -0.28, 0.26, -0.10, 0.18, 0.33, 
                   0.45, 0.46, 0.04, 0.12, 0.21), 
                   nrow = 5, ncol = 5)
# Let TropAno represents 5 corresponding 5-year forecasts of seasonally averaged 
# tropical sea surface temperature anomalies.
TropAno &lt;- matrix(c(-0.22, -.13, 0.07, -0.16, -0.15,
                    0.00,  -0.03, -0.22, -0.13, -0.10,
                    0.07, -0.07, 0.17, 0.10, -0.15,
                   -0.01, 0.08, 0.07, 0.17, 0.13,
                    0.16, 0.15, -0.09, 0.03, 0.27),
                    nrow = 5, ncol = 5)
# The seasonal average of hurricanes for each of the five forecasted years, 
# for each forecast, would then be given by
hr_count &lt;- StatSeasAtlHurr(atlano = AtlAno, 
                           tropano = TropAno, 
                           hrvar = 'HR')
print(hr_count$mean)

</code></pre>

<hr>
<h2 id='Subset'>Subset a Data Array</h2><span id='topic+Subset'></span>

<h3>Description</h3>

<p>This function allows to subset (i.e. slice, take a chunk of) an array, in a 
similar way as done in the function <code>take()</code> in the package plyr. There
are two main inprovements:<br /><br />
The input array can have dimension names, either 
in <code>names(dim(x))</code> or in the attribute 'dimensions', and the dimensions 
to subset along can be specified via the parameter <code>along</code> either with 
integer indices or either by their name.<br /><br />
There are additional ways to adjust which dimensions are dropped in the 
resulting array: either to drop all, to drop none, to drop only the ones that 
have been sliced or to drop only the ones that have not been sliced.<br /><br />
If an array is provided without dimension names, dimension names taken from 
the parameter <code>dim_names</code> will be added to the array.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Subset(x, along, indices, drop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Subset_+3A_x">x</code></td>
<td>
<p>A multidimensional array to be sliced. It can have dimension names 
either in <code>names(dim(x))</code> or either in the attribute 'dimensions'.</p>
</td></tr>
<tr><td><code id="Subset_+3A_along">along</code></td>
<td>
<p>Vector with references to the dimensions to take the subset 
from: either integers or dimension names.</p>
</td></tr>
<tr><td><code id="Subset_+3A_indices">indices</code></td>
<td>
<p>List of indices to take from each dimension specified in 
'along'. If a single dimension is specified in 'along' the indices can be 
directly provided as a single integer or as a vector.</p>
</td></tr>
<tr><td><code id="Subset_+3A_drop">drop</code></td>
<td>
<p>Whether to drop all the dimensions of length 1 in the resulting 
array, none, only those that are specified in 'along', or only those that 
are not specified in 'along'. The possible values are, respectively: 'all' 
or TRUE, 'none' or FALSE, 'selected', and 'non-selected'.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>subset &lt;- Subset(sampleMap$mod, c('dataset', 'sdate', 'ftime'), 
                list(1, 1, 1), drop = 'selected')
PlotLayout(PlotEquiMap, c('lat', 'lon'), subset, 
          sampleMap$lon, sampleMap$lat, 
          titles = paste('Member', 1:3))

</code></pre>

<hr>
<h2 id='SVD'>Single Value Decomposition (Maximum Covariance Analysis)</h2><span id='topic+SVD'></span>

<h3>Description</h3>

<p>Computes a Maximum Covariance Analysis (MCA) between vary and varx, both
of dimensions c(n. of time steps, n. of latitudes, n. of longitudes), each
over a region of interest, e.g.: prlr over Europe and tos over North Atlantic.
The input fields are latitude-weighted by default (can be adjustable via 
<code>weight</code>).<br />
Returns a vector of squared covariance fraction (SCFs) explained by 
each pair of covariability modes, a vector of correlation coefficient 
(RUVs) between expansion coefficients (ECs) that measures their linear 
relationship, and a set of regression (MCAs) associated with the 
covariability modes (ECs). Note that MCAs are 'homogeneous' patterns obtained 
as regression/correlation between each field (predictor, predictand) 
and its expansion coefficient.<br />
The MCA is computed by default with the covariance matrix. It can be computed
with the correlation matrix by setting <code>corr = TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SVD(
  vary,
  varx,
  laty = NULL,
  latx = NULL,
  nmodes = 15,
  corr = FALSE,
  weight = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SVD_+3A_vary">vary</code></td>
<td>
<p>Array containing the anomalies field for the predictor. The 
expected dimensions are c(n. of time steps, n. of latitudes, n. of 
longitudes).</p>
</td></tr>
<tr><td><code id="SVD_+3A_varx">varx</code></td>
<td>
<p>Array containing the anomalies field for the predictand. The 
expected dimensions are c(n. of time steps, n. of latitudes, n. of 
longitudes).</p>
</td></tr>
<tr><td><code id="SVD_+3A_laty">laty</code></td>
<td>
<p>Vector of latitudes of the array <code>vary</code>. Only required if 
<code>weight = TRUE</code>.</p>
</td></tr>
<tr><td><code id="SVD_+3A_latx">latx</code></td>
<td>
<p>Vector of latitudes of the array <code>varx</code>. Only required if 
<code>weight = TRUE</code>.</p>
</td></tr>
<tr><td><code id="SVD_+3A_nmodes">nmodes</code></td>
<td>
<p>Number of ECs/MCAs/modes retained and provided in the outputs.</p>
</td></tr>
<tr><td><code id="SVD_+3A_corr">corr</code></td>
<td>
<p>Whether to compute the MCA over a covariance matrix (FALSE) or 
a correlation matrix (TRUE).</p>
</td></tr>
<tr><td><code id="SVD_+3A_weight">weight</code></td>
<td>
<p>Whether to apply latitude weights on the input fields or not. 
TRUE by default.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>$SC</code></td>
<td>

<p>Vector of squared covariance (n. of modes).
</p>
</td></tr>
<tr><td><code>$SCFs</code></td>
<td>

<p>Vector of squared covariance fractions (n. of modes).
</p>
</td></tr>
<tr><td><code>$RUVs</code></td>
<td>

<p>Vector of correlations between expansion coefficients (n. of modes).
</p>
</td></tr>
<tr><td><code>$ECs_U</code></td>
<td>

<p>Array of expansion coefficients of predictor field (n. of time steps, 
n. of modes).
</p>
</td></tr>
<tr><td><code>$MCAs_U</code></td>
<td>

<p>Array of covariability patterns of predictor field (c(dim), n. of modes).
</p>
</td></tr>
<tr><td><code>$ECs_V</code></td>
<td>

<p>Array of expansion coefficients of predictand field (n. of time steps, 
n. of modes).
</p>
</td></tr>
<tr><td><code>$MCAs_V</code></td>
<td>

<p>Array of covariability patterns of predictand field (c(dim), n. of modes).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2010-09  (J.-G. Serrano)  -  Original code<br />
1.0  -  2016-04  (N. Manubens)  -  Formatting to R CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See examples on Load() to understand the first lines in this example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path, 
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates, 
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
# This example computes the ECs and MCAs along forecast horizons and plots the 
# one that explains the greatest amount of variability. The example data is 
# very low resolution so it does not make a lot of sense.
ano &lt;- Ano_CrossValid(sampleData$mod, sampleData$obs)
mca &lt;- SVD(Mean1Dim(ano$ano_exp, 2)[1, , 1, , ], 
          Mean1Dim(ano$ano_obs, 2)[1, , 1, , ], 
          sampleData$lat, sampleData$lat)
PlotEquiMap(mca$MCAs_U[1, , ], sampleData$lon, sampleData$lat)
plot(mca$ECs_U[1, ])
PlotEquiMap(mca$MCAs_V[1, , ], sampleData$lon, sampleData$lat)
plot(mca$ECs_V[1, ])

</code></pre>

<hr>
<h2 id='ToyModel'>Synthetic forecast generator imitating seasonal to decadal forecasts. The
components of a forecast: (1) predictabiltiy (2) forecast error (3) 
non-stationarity and (4) ensemble generation. The forecast can be computed 
for real observations or observations generated artifically.</h2><span id='topic+ToyModel'></span>

<h3>Description</h3>

<p>The toymodel is based on the model presented in Weigel et al. (2008) QJRS
with an extension to consider non-stationary distributions prescribing a
linear trend. The toymodel allows to generate an aritifical forecast
based on obsevations provided by the input (from Load) or artificially
generated observations based on the input parameters (sig, trend). 
The forecast can be specfied for any number of start-dates, lead-time and 
ensemble members. It imitates components of a forecast: (1) predictabiltiy
(2) forecast error (3) non-stationarity and (4) ensemble generation. 
The forecast can be computed for real observations or observations generated
artifically.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ToyModel(
  alpha = 0.1,
  beta = 0.4,
  gamma = 1,
  sig = 1,
  trend = 0,
  nstartd = 30,
  nleadt = 4,
  nmemb = 10,
  obsini = NULL,
  fxerr = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ToyModel_+3A_alpha">alpha</code></td>
<td>
<p>Predicabiltiy of the forecast on the observed residuals 
Must be a scalar 0 &lt; alpha &lt; 1.</p>
</td></tr>
<tr><td><code id="ToyModel_+3A_beta">beta</code></td>
<td>
<p>Standard deviation of forecast error 
Must be a scalar 0 &lt; beta &lt; 1.</p>
</td></tr>
<tr><td><code id="ToyModel_+3A_gamma">gamma</code></td>
<td>
<p>Factor on the linear trend to sample model uncertainty. Can be 
a scalar or a vector of scalars -inf &lt; gammay &lt; inf.
Defining a scalar results in multiple forecast, corresponding to different 
models with different trends.</p>
</td></tr>
<tr><td><code id="ToyModel_+3A_sig">sig</code></td>
<td>
<p>Standard deviation of the residual variability of the forecast.
If observations are provided 'sig' is computed from the observations.</p>
</td></tr>
<tr><td><code id="ToyModel_+3A_trend">trend</code></td>
<td>
<p>Linear trend of the forecast. The same trend is used for each 
lead-time. If observations are provided the 'trend' is computed from the 
observations, with potentially different trends for each lead-time. The 
trend has no unit and needs to be defined according to the time vector 
[1,2,3,... nstartd].</p>
</td></tr>
<tr><td><code id="ToyModel_+3A_nstartd">nstartd</code></td>
<td>
<p>Number of start-dates of the forecast. 
If observations are provided the 'nstartd' is computed from the observations.</p>
</td></tr>
<tr><td><code id="ToyModel_+3A_nleadt">nleadt</code></td>
<td>
<p>Number of lead-times of the forecats.
If observations are provided the 'nleadt' is computed from the observations.</p>
</td></tr>
<tr><td><code id="ToyModel_+3A_nmemb">nmemb</code></td>
<td>
<p>Number of members of the forecasts.</p>
</td></tr>
<tr><td><code id="ToyModel_+3A_obsini">obsini</code></td>
<td>
<p>Observations that can be used in the synthetic forecast coming 
from Load (anomalies are expected). If no observations are provided 
artifical observations are generated based on Gaussian variaiblity with 
standard deviation from 'sig' and linear trend from 'trend'.</p>
</td></tr>
<tr><td><code id="ToyModel_+3A_fxerr">fxerr</code></td>
<td>
<p>Provides a fixed error of the forecast instead of generating 
one from the level of beta. This allows to perform pair of forecasts with 
the same conditional error as required for instance in an attribution context.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of forecast with $mod including the forecast and $obs the 
observations. The dimensions correspond to 
c(length(gamma), nmemb, nstartd, nleadt)
</p>


<h3>Author(s)</h3>

<p>History:<br />
1.0  -  2014-08  (O.Bellprat)  -  Original code
1.1  -  2016-02  (O.Bellprat)  -  Include security check for parameters
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: Generate forecast with artifical observations
# Seasonal prediction example
a &lt;- 0.1
b &lt;- 0.3
g &lt;- 1
sig &lt;- 1
t &lt;- 0.02
ntd &lt;- 30
nlt &lt;- 4
nm &lt;- 10
toyforecast &lt;- ToyModel(alpha = a, beta = b, gamma = g, sig = sig, trend = t, 
                       nstartd = ntd, nleadt = nlt, nmemb = nm)

# Example 2: Generate forecast from loaded observations
# Decadal prediction example
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path,
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates,
                  output = 'areave', latmin = 27, latmax = 48,
                  lonmin = -12, lonmax = 40)
 
## End(Not run)
 

a &lt;- 0.1
b &lt;- 0.3
g &lt;- 1
nm &lt;- 10

toyforecast &lt;- ToyModel(alpha = a, beta = b, gamma = g, nmemb = nm, 
                       obsini = sampleData$obs, nstartd = 5, nleadt = 60)
 
PlotAno(toyforecast$mod, toyforecast$obs, startDates, 
       toptitle = c("Synthetic decadal temperature prediction"), 
       fileout = "ex_toymodel.eps")
 

</code></pre>

<hr>
<h2 id='Trend'>Computes the Trend of the Ensemble Mean</h2><span id='topic+Trend'></span><span id='topic+.Trend'></span>

<h3>Description</h3>

<p>Computes the trend along the forecast time of the ensemble mean by least 
square fitting, and the associated error interval.<br />
Trend() also provides the time series of the detrended ensemble mean 
forecasts.<br />
The confidence interval relies on a student-T distribution.<br /><br />
.Trend provides the same functionality but taking a matrix ensemble members 
as input (exp).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Trend(var, posTR = 2, interval = 1, siglev = 0.95, conf = TRUE)

.Trend(exp, interval = 1, siglev = 0.95, conf = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Trend_+3A_var">var</code></td>
<td>
<p>An array of any number of dimensions up to 10.</p>
</td></tr>
<tr><td><code id="Trend_+3A_postr">posTR</code></td>
<td>
<p>An integer indicating the position along which to compute the 
trend.</p>
</td></tr>
<tr><td><code id="Trend_+3A_interval">interval</code></td>
<td>
<p>A number of months/years between 2 points along posTR 
dimension. Set 1 as default.</p>
</td></tr>
<tr><td><code id="Trend_+3A_siglev">siglev</code></td>
<td>
<p>A numeric value indicating the confidence level for the 
computation of confidence interval. Set 0.95 as default.</p>
</td></tr>
<tr><td><code id="Trend_+3A_conf">conf</code></td>
<td>
<p>A logical value indicating whether to compute the confidence 
levels or not. Set TRUE as default.</p>
</td></tr>
<tr><td><code id="Trend_+3A_exp">exp</code></td>
<td>
<p>An M by N matrix representing M forecasts from N ensemble members.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>$trend</code></td>
<td>

<p>The intercept and slope coefficients for the least squares fitting of the 
trend. 
An array with same dimensions as parameter 'var' except along the posTR 
dimension, which is replaced by a length 4 (or length 2 if conf = FALSE) 
dimension, corresponding to the lower limit of the confidence interval 
(only present if conf = TRUE), the slope, the upper limit of the confidence 
interval (only present if conf = TRUE), and the intercept.
</p>
</td></tr>
<tr><td><code>$detrended</code></td>
<td>

<p>Same dimensions as var with linearly detrended var along the posTR 
dimension.
</p>
</td></tr>
</table>
<p>Only in .Trend:
</p>
<table>
<tr><td><code>$conf.int</code></td>
<td>

<p>Corresponding to the limits of the <code>siglev</code>% confidence interval 
(only present if <code>conf = TRUE</code>) for the slope coefficient.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2011-05  (V. Guemas)  -  Original code<br />
1.0  -  2013-09  (N. Manubens)  -  Formatting to CRAN<br />
2.0  -  2017-02  (A. Hunter)  -  Adapt to veriApply()
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load sample data as in Load() example:
example(Load)
months_between_startdates &lt;- 60
trend &lt;- Trend(sampleData$obs, 3, months_between_startdates)
 
PlotVsLTime(trend$trend, toptitle = "trend", ytitle = "K / (5 year)",
           monini = 11, limits = c(-1,1), listexp = c('CMIP5 IC3'),
           listobs = c('ERSST'), biglab = FALSE, hlines = 0,
           fileout = 'tos_obs_trend.eps')
PlotAno(trend$detrended, NULL, startDates, 
       toptitle = 'detrended anomalies (along the startdates)', ytitle = 'K',
       legends = 'ERSST', biglab = FALSE, fileout = 'tos_detrended_obs.eps')
 

</code></pre>

<hr>
<h2 id='UltimateBrier'>Computes Brier Scores</h2><span id='topic+UltimateBrier'></span>

<h3>Description</h3>

<p>Interface to compute probabilistic scores (Brier Score, Brier Skill Score) 
from data obtained from s2dverification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>UltimateBrier(
  ano_exp,
  ano_obs,
  posdatasets = 1,
  posmemb = 2,
  posdates = 3,
  quantile = TRUE,
  thr = c(5/100, 95/100),
  type = "BS",
  decomposition = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="UltimateBrier_+3A_ano_exp">ano_exp</code></td>
<td>
<p>Array of forecast anomalies, as provided by <code>Ano()</code>. 
Dimensions c(n. of experimental datasets, n. of members, n. of start dates, 
n. of forecast time steps, n. of latitudes, n. of longitudes). Dimensions 
in other orders are also supported. See parameters <code>posdatasets</code>, 
<code>posmemb</code> and <code>posdates</code>.</p>
</td></tr>
<tr><td><code id="UltimateBrier_+3A_ano_obs">ano_obs</code></td>
<td>
<p>Array of observational reference anomalies, as provided by 
<code>Ano()</code>. Dimensions c(n. of observational reference datasets, 
n. of members, n. of start dates, n. of forecast time steps, 
n. of latitudes, n. of longitudes). Dimensions in other orders are also 
supported. See parameters <code>posdatasets</code>, <code>posmemb</code> and 
<code>posdates</code>.</p>
</td></tr>
<tr><td><code id="UltimateBrier_+3A_posdatasets">posdatasets</code></td>
<td>
<p>Expected position of dimension corresponding to the 
different evaluated datasets in input data (ano_exp and ano_obs). 
By default 1.</p>
</td></tr>
<tr><td><code id="UltimateBrier_+3A_posmemb">posmemb</code></td>
<td>
<p>Expected position of dimension corresponding to members in 
input data (ano_exp and ano_obs). By default 2.</p>
</td></tr>
<tr><td><code id="UltimateBrier_+3A_posdates">posdates</code></td>
<td>
<p>Expected position of dimension corresponding to starting 
dates in input data (ano_exp and ano_obs). By default 3.</p>
</td></tr>
<tr><td><code id="UltimateBrier_+3A_quantile">quantile</code></td>
<td>
<p>Flag to stipulate whether a quantile (TRUE) or a threshold 
(FALSE) is used to estimate the forecast and observed probabilities. 
Takes TRUE by default.</p>
</td></tr>
<tr><td><code id="UltimateBrier_+3A_thr">thr</code></td>
<td>
<p>Values to be used as quantiles if 'quantile' is TRUE or as 
thresholds if 'quantile' is FALSE. Takes by default <code>c(0.05, 0.95)</code> 
if 'quantile' is TRUE.</p>
</td></tr>
<tr><td><code id="UltimateBrier_+3A_type">type</code></td>
<td>
<p>Type of score desired. Can take the following values:
</p>

<ul>
<li><p>'BS': Simple Brier Score.
</p>
</li>
<li><p>'FairEnsembleBS': Corrected Brier Score computed across ensemble 
members.
</p>
</li>
<li><p>'FairStartDatesBS': Corrected Brier Score computed across starting 
dates.
</p>
</li>
<li><p>'BSS': Simple Brier Skill Score.
</p>
</li>
<li><p>'FairEnsembleBSS': Corrected Brier Skill Score computed across 
ensemble members.
</p>
</li>
<li><p>'FairStartDatesBSS': Corrected Brier Skill Score computed across 
starting dates.
</p>
</li></ul>
</td></tr>
<tr><td><code id="UltimateBrier_+3A_decomposition">decomposition</code></td>
<td>
<p>Flag to determine whether the decomposition of the 
Brier Score into its components should be provided (TRUE) or not (FALSE). 
Takes TRUE by default. The decomposition will be computed only if 'type' 
is 'BS' or 'FairStartDatesBS'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If 'type' is 'FairEnsembleBS', 'BSS', 'FairEnsemblesBSS' or 
'FairStartDatesBSS' or 'decomposition' is FALSE and 'type' is 'BS' or 
'FairStartDatesBS', the Brier Score or Brier Skill Score will be returned 
respectively.
If 'decomposition' is TRUE and 'type' is 'BS' or 'FairStartDatesBS' the 
returned value is a named list with the following entries:
</p>

<ul>
<li><p>'BS': Brier Score.
</p>
</li>
<li><p>'REL': Reliability component.
</p>
</li>
<li><p>'UNC': Uncertainty component.
</p>
</li>
<li><p>'RES': Resolution component.
</p>
</li></ul>

<p>The dimensions of each of these arrays will be c(n. of experimental datasets, 
n. of observational reference datasets, n. of bins, the rest of input 
dimensions except for the ones pointed by 'posmemb' and 'posdates').
</p>


<h3>Author(s)</h3>

<p>History:<br />
0.1  -  2015-05 (V. Guemas,<br />
C. Prodhomme,<br />
O. Bellprat,<br />
V. Torralba,<br />
N. Manubens)  -  First version
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See ?Load for an explanation on the first part of this example.
 ## Not run: 
data_path &lt;- system.file('sample_data', package = 's2dverification')
expA &lt;- list(name = 'experiment', path = file.path(data_path, 
            'model/$EXP_NAME$/$STORE_FREQ$_mean/$VAR_NAME$_3hourly',
            '$VAR_NAME$_$START_DATE$.nc'))
obsX &lt;- list(name = 'observation', path = file.path(data_path,
            '$OBS_NAME$/$STORE_FREQ$_mean/$VAR_NAME$',
            '$VAR_NAME$_$YEAR$$MONTH$.nc'))

# Now we are ready to use Load().
startDates &lt;- c('19851101', '19901101', '19951101', '20001101', '20051101')
sampleData &lt;- Load('tos', list(expA), list(obsX), startDates, 
                  leadtimemin = 1, leadtimemax = 4, output = 'lonlat',
                  latmin = 27, latmax = 48, lonmin = -12, lonmax = 40)
 
## End(Not run)
 
sampleData$mod &lt;- Season(sampleData$mod, 4, 11, 12, 2)
sampleData$obs &lt;- Season(sampleData$obs, 4, 11, 12, 2)
clim &lt;- Clim(sampleData$mod, sampleData$obs)
ano_exp &lt;- Ano(sampleData$mod, clim$clim_exp)
ano_obs &lt;- Ano(sampleData$obs, clim$clim_obs)
bs &lt;- UltimateBrier(ano_exp, ano_obs)
bss &lt;- UltimateBrier(ano_exp, ano_obs, type = 'BSS')
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
