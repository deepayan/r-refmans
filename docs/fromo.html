<!DOCTYPE html><html><head><title>Help for package fromo</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fromo}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#fromo-package'><p>Fast Robust Moments.</p></a></li>
<li><a href='#+25-+25'><p>unconcatenate centsums objects.</p></a></li>
<li><a href='#+25-+25+2Ccentcosums+2Ccentcosums-method'><p>unconcatenate centcosums objects.</p></a></li>
<li><a href='#accessor'><p>Accessor methods.</p></a></li>
<li><a href='#as.centcosums'><p>Coerce to a centcosums object.</p></a></li>
<li><a href='#as.centsums'><p>Coerce to a centsums object.</p></a></li>
<li><a href='#c.centcosums'><p>concatenate centcosums objects.</p></a></li>
<li><a href='#c.centsums'><p>concatenate centsums objects.</p></a></li>
<li><a href='#cent_cosums'><p>Multivariate centered sums; join and unjoined.</p></a></li>
<li><a href='#cent_sums'><p>Centered sums; join and unjoined.</p></a></li>
<li><a href='#cent2raw'><p>Convert between different types of moments, raw, central, standardized.</p></a></li>
<li><a href='#centcosums-accessor'><p>Accessor methods.</p></a></li>
<li><a href='#centcosums-class'><p>centcosums Class.</p></a></li>
<li><a href='#centsums-class'><p>centsums Class.</p></a></li>
<li><a href='#fromo-NEWS'><p>News for package 'fromo':</p></a></li>
<li><a href='#running_apx_quantiles'><p>Compute approximate quantiles over a sliding window</p></a></li>
<li><a href='#running_centered'><p>Compare data to moments computed over a sliding window.</p></a></li>
<li><a href='#running_sd3'><p>Compute first K moments over a sliding window</p></a></li>
<li><a href='#running_sum'><p>Compute sums or means over a sliding window.</p></a></li>
<li><a href='#sd3'><p>Compute first K moments</p></a></li>
<li><a href='#show'><p>Show a centsums object.</p></a></li>
<li><a href='#t_running_apx_quantiles'><p>Compute approximate quantiles over a sliding time window</p></a></li>
<li><a href='#t_running_centered'><p>Compare data to moments computed over a time sliding window.</p></a></li>
<li><a href='#t_running_sd3'><p>Compute first K moments over a sliding time-based window</p></a></li>
<li><a href='#t_running_sum'><p>Compute sums or means over a sliding time window.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Steven E. Pav &lt;shabbychef@gmail.com&gt;</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-01-29</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a></td>
</tr>
<tr>
<td>Title:</td>
<td>Fast Robust Moments</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/shabbychef/fromo/issues">https://github.com/shabbychef/fromo/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Fast, numerically robust computation of weighted moments via 'Rcpp'. 
   Supports computation on vectors and matrices, and Monoidal append of moments. 
   Moments and cumulants over running fixed length windows can be computed, 
   as well as over time-based windows.
   Moment computations are via a generalization of Welford's method, as described
   by Bennett et. (2009) &lt;<a href="https://doi.org/10.1109%2FCLUSTR.2009.5289161">doi:10.1109/CLUSTR.2009.5289161</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.3), methods</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, testthat, moments, PDQutils, e1071, microbenchmark</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/shabbychef/fromo">https://github.com/shabbychef/fromo</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Collate:</td>
<td>'fromo.r' 'RcppExports.R' 'zzz_centsums.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-01-30 06:37:08 UTC; root</td>
</tr>
<tr>
<td>Author:</td>
<td>Steven E. Pav <a href="https://orcid.org/0000-0002-4197-6195"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-01-30 07:10:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='fromo-package'>Fast Robust Moments.</h2><span id='topic+fromo-package'></span>

<h3>Description</h3>

<p>Fast, numerically robust moments computations, along with computation of
cumulants, running means, etc.
</p>


<h3>Robust Moments</h3>

<p>Welford described a method for 'robust' one-pass computation of the
standard deviation. By 'robust', we mean robust to round-off caused
by a large shift in the mean. This method was generalized by Terriberry,
and Bennett <em>et. al.</em> to the case of higher-order moments. 
This package provides those algorithms for computing moments.
</p>
<p>Generally we should find that the stock implementations of <code>sd</code>,
<code>skewness</code> and so on are <em>already</em> robust and likely using
these algorithms under the hood. This package was written for a few
reasons:
</p>

<ol>
<li><p> As an exercise to learn Rcpp.
</p>
</li>
<li><p> Often I found I needed the first <code class="reqn">k</code> moments. For example,
when computing the Z-score, the standard deviation and mean must be
computed separately, which is inefficient. Similarly Merten's correction
for the standard error of the Sharpe ratio uses the first four moments.
These are all computed as a side effect of computation of the kurtosis,
but discarded by the standard methods.
</p>
</li></ol>



<h3>Legal Mumbo Jumbo</h3>

<p>fromo is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.
</p>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>
<p>This package was developed as an exercise in learning Rcpp.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>

<hr>
<h2 id='+25-+25'>unconcatenate centsums objects.</h2><span id='topic++25-+25'></span><span id='topic++25-+25+2Ccentsums+2Ccentsums-method'></span>

<h3>Description</h3>

<p>Unconcatenate centsums objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>x %-% y

## S4 method for signature 'centsums,centsums'
x %-% y
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25-+2B25_+3A_x">x</code></td>
<td>
<p>a <code>centsums</code> objects</p>
</td></tr>
<tr><td><code id="+2B25-+2B25_+3A_y">y</code></td>
<td>
<p>a <code>centsums</code> objects</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>unjoin_cent_sums
</p>

<hr>
<h2 id='+25-+25+2Ccentcosums+2Ccentcosums-method'>unconcatenate centcosums objects.</h2><span id='topic++25-+25+2Ccentcosums+2Ccentcosums-method'></span>

<h3>Description</h3>

<p>Unconcatenate centcosums objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'centcosums,centcosums'
x %-% y
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25-+2B25+2B2Ccentcosums+2B2Ccentcosums-method_+3A_x">x</code></td>
<td>
<p>a <code>centcosums</code> objects</p>
</td></tr>
<tr><td><code id="+2B25-+2B25+2B2Ccentcosums+2B2Ccentcosums-method_+3A_y">y</code></td>
<td>
<p>a <code>centcosums</code> objects</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>unjoin_cent_cosums
</p>

<hr>
<h2 id='accessor'>Accessor methods.</h2><span id='topic+accessor'></span><span id='topic+sums'></span><span id='topic+sums+2Ccentsums-method'></span><span id='topic+moments'></span><span id='topic+moments+2Ccentsums-method'></span>

<h3>Description</h3>

<p>Access slot data from a <code>centsums</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sums(x)

## S4 method for signature 'centsums'
sums(x)

moments(x, type = c("central", "raw", "standardized"))

## S4 method for signature 'centsums'
moments(x, type = c("central", "raw", "standardized"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="accessor_+3A_x">x</code></td>
<td>
<p>a <code>centsums</code> object.</p>
</td></tr>
<tr><td><code id="accessor_+3A_type">type</code></td>
<td>
<p>the type of moment to compute.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>

<hr>
<h2 id='as.centcosums'>Coerce to a centcosums object.</h2><span id='topic+as.centcosums'></span><span id='topic+as.centcosums.default'></span><span id='topic+as.centcosums'></span>

<h3>Description</h3>

<p>Convert data to a <code>centcosums</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.centcosums(x, order=2, na.omit=TRUE)

## Default S3 method:
as.centcosums(x, order = 2, na.omit = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.centcosums_+3A_x">x</code></td>
<td>
<p>a matrix.</p>
</td></tr>
<tr><td><code id="as.centcosums_+3A_order">order</code></td>
<td>
<p>the order, defaulting to <code>2</code>.</p>
</td></tr>
<tr><td><code id="as.centcosums_+3A_na.omit">na.omit</code></td>
<td>
<p>whether to remove rows with <code>NA</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the raw cosums on data, and stuffs the results into a 
<code>centcosums</code> object.
</p>


<h3>Value</h3>

<p>A centcosums object.
</p>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
x &lt;- matrix(rnorm(100*3),ncol=3)
cs &lt;- as.centcosums(x, order=2)
</code></pre>

<hr>
<h2 id='as.centsums'>Coerce to a centsums object.</h2><span id='topic+as.centsums'></span><span id='topic+as.centsums.default'></span><span id='topic+as.centsums'></span>

<h3>Description</h3>

<p>Convert data to a <code>centsums</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.centsums(x, order=3, na.rm=TRUE)

## Default S3 method:
as.centsums(x, order = 3, na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.centsums_+3A_x">x</code></td>
<td>
<p>a numeric, array, or matrix.</p>
</td></tr>
<tr><td><code id="as.centsums_+3A_order">order</code></td>
<td>
<p>the order, defaulting to <code>length(sums)+1</code>.</p>
</td></tr>
<tr><td><code id="as.centsums_+3A_na.rm">na.rm</code></td>
<td>
<p>whether to remove <code>NA</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the raw sums on data, and stuffs the results into a 
<code>centsums</code> object.
</p>


<h3>Value</h3>

<p>A centsums object.
</p>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
x &lt;- rnorm(1000)
cs &lt;- as.centsums(x, order=5)
</code></pre>

<hr>
<h2 id='c.centcosums'>concatenate centcosums objects.</h2><span id='topic+c.centcosums'></span>

<h3>Description</h3>

<p>Concatenate centcosums objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>\method{c}{centcosums}(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="c.centcosums_+3A_...">...</code></td>
<td>
<p><code>centcosums</code> objects</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>join_cent_cosums
</p>

<hr>
<h2 id='c.centsums'>concatenate centsums objects.</h2><span id='topic+c.centsums'></span>

<h3>Description</h3>

<p>Concatenate centsums objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>\method{c}{centsums}(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="c.centsums_+3A_...">...</code></td>
<td>
<p><code>centsums</code> objects</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>join_cent_sums
</p>

<hr>
<h2 id='cent_cosums'>Multivariate centered sums; join and unjoined.</h2><span id='topic+cent_cosums'></span><span id='topic+cent_comoments'></span><span id='topic+join_cent_cosums'></span><span id='topic+unjoin_cent_cosums'></span>

<h3>Description</h3>

<p>Compute, join, or unjoin multivariate centered (co-) sums.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cent_cosums(v, max_order = 2L, na_omit = FALSE)

cent_comoments(v, max_order = 2L, used_df = 0L, na_omit = FALSE)

join_cent_cosums(ret1, ret2)

unjoin_cent_cosums(ret3, ret2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cent_cosums_+3A_v">v</code></td>
<td>
<p>an <code class="reqn">m</code> by <code class="reqn">n</code> matrix, each row an independent observation of some
<code class="reqn">n</code> variate variable.</p>
</td></tr>
<tr><td><code id="cent_cosums_+3A_max_order">max_order</code></td>
<td>
<p>the maximum order of cosum to compute. For now this can only be
2; in the future higher order cosums should be possible.</p>
</td></tr>
<tr><td><code id="cent_cosums_+3A_na_omit">na_omit</code></td>
<td>
<p>a boolean; if <code>TRUE</code>, then only rows of <code>v</code> with complete
observations will be used.</p>
</td></tr>
<tr><td><code id="cent_cosums_+3A_used_df">used_df</code></td>
<td>
<p>the number of degrees of freedom consumed, used in the denominator
of the centered moments computation. These are subtracted from the number of
observations.</p>
</td></tr>
<tr><td><code id="cent_cosums_+3A_ret1">ret1</code></td>
<td>
<p>a multdimensional array as output by <code><a href="#topic+cent_cosums">cent_cosums</a></code>.</p>
</td></tr>
<tr><td><code id="cent_cosums_+3A_ret2">ret2</code></td>
<td>
<p>a multdimensional array as output by <code><a href="#topic+cent_cosums">cent_cosums</a></code>.</p>
</td></tr>
<tr><td><code id="cent_cosums_+3A_ret3">ret3</code></td>
<td>
<p>a multdimensional array as output by <code><a href="#topic+cent_cosums">cent_cosums</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a multidimensional arry of dimension <code>max_order</code>, each side of length
<code class="reqn">1+n</code>. For the case currently implemented where <code>max_order</code> must be 2, the
output is a symmetric matrix, where the element in the <code>1,1</code> position is the count of 
complete) rows of <code>v</code>, the <code>2:(n+1),1</code> column is the mean, and the
<code>2:(n+1),2:(n+1)</code> is the co <em>sums</em> matrix, which is the covariance up to scaling
by the count. <code>cent_comoments</code> performs this normalization for you.
</p>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>


<h3>See Also</h3>

<p>cent_sums
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 set.seed(1234)
 x1 &lt;- matrix(rnorm(1e3*5,mean=1),ncol=5)
 x2 &lt;- matrix(rnorm(1e3*5,mean=1),ncol=5)
 max_ord &lt;- 2L
 rs1 &lt;- cent_cosums(x1,max_ord)
 rs2 &lt;- cent_cosums(x2,max_ord)
 rs3 &lt;- cent_cosums(rbind(x1,x2),max_ord)
 rs3alt &lt;- join_cent_cosums(rs1,rs2)
 stopifnot(max(abs(rs3 - rs3alt)) &lt; 1e-7)
 rs1alt &lt;- unjoin_cent_cosums(rs3,rs2)
 rs2alt &lt;- unjoin_cent_cosums(rs3,rs1)
 stopifnot(max(abs(rs1 - rs1alt)) &lt; 1e-7)
 stopifnot(max(abs(rs2 - rs2alt)) &lt; 1e-7)

</code></pre>

<hr>
<h2 id='cent_sums'>Centered sums; join and unjoined.</h2><span id='topic+cent_sums'></span><span id='topic+join_cent_sums'></span><span id='topic+unjoin_cent_sums'></span>

<h3>Description</h3>

<p>Compute, join, or unjoin centered sums.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cent_sums(v, max_order = 5L, na_rm = FALSE, wts = NULL,
  check_wts = FALSE, normalize_wts = TRUE)

join_cent_sums(ret1, ret2)

unjoin_cent_sums(ret3, ret2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cent_sums_+3A_v">v</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="cent_sums_+3A_max_order">max_order</code></td>
<td>
<p>the maximum order of the centered moment to be computed.</p>
</td></tr>
<tr><td><code id="cent_sums_+3A_na_rm">na_rm</code></td>
<td>
<p>whether to remove NA, false by default.</p>
</td></tr>
<tr><td><code id="cent_sums_+3A_wts">wts</code></td>
<td>
<p>an optional vector of weights. Weights are &lsquo;replication&rsquo;
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding <code>v</code> value. If <code>NULL</code>, corresponds to
equal unit weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial, with the exception that methods which check for minimum df will,
in the weighted case, check against the sum of weights. For this reason,
weights less than 1 could cause <code>NA</code> to be returned unexpectedly due
to the minimum condition. When weights are <code>NA</code>, the same rules for checking <code>v</code>
are applied. That is, the observation will not contribute to the moment
if the weight is <code>NA</code> when <code>na_rm</code> is true. When there is no
checking, an <code>NA</code> value will cause the output to be <code>NA</code>.</p>
</td></tr>
<tr><td><code id="cent_sums_+3A_check_wts">check_wts</code></td>
<td>
<p>a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.</p>
</td></tr>
<tr><td><code id="cent_sums_+3A_normalize_wts">normalize_wts</code></td>
<td>
<p>a boolean for whether the weights should be
renormalized to have a mean value of 1. This mean is computed over elements
which contribute to the moments, so if <code>na_rm</code> is set, that means non-NA
elements of <code>wts</code> that correspond to non-NA elements of the data
vector.</p>
</td></tr>
<tr><td><code id="cent_sums_+3A_ret1">ret1</code></td>
<td>
<p>an <code class="reqn">ord+1</code> vector as output by <code><a href="#topic+cent_sums">cent_sums</a></code> consisting of
the count, the mean, then the k through ordth centered sum of some observations.</p>
</td></tr>
<tr><td><code id="cent_sums_+3A_ret2">ret2</code></td>
<td>
<p>an <code class="reqn">ord+1</code> vector as output by <code><a href="#topic+cent_sums">cent_sums</a></code> consisting of
the count, the mean, then the k through ordth centered sum of some observations.</p>
</td></tr>
<tr><td><code id="cent_sums_+3A_ret3">ret3</code></td>
<td>
<p>an <code class="reqn">ord+1</code> vector as output by <code><a href="#topic+cent_sums">cent_sums</a></code> consisting of
the count, the mean, then the k through ordth centered sum of some observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector the same size as the input consisting of the adjusted version of the input.
When there are not sufficient (non-nan) elements for the computation, <code>NaN</code> are returned.
</p>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 set.seed(1234)
 x1 &lt;- rnorm(1e3,mean=1)
 x2 &lt;- rnorm(1e3,mean=1)
 max_ord &lt;- 6L
 rs1 &lt;- cent_sums(x1,max_ord)
 rs2 &lt;- cent_sums(x2,max_ord)
 rs3 &lt;- cent_sums(c(x1,x2),max_ord)
 rs3alt &lt;- join_cent_sums(rs1,rs2)
 stopifnot(max(abs(rs3 - rs3alt)) &lt; 1e-7)
 rs1alt &lt;- unjoin_cent_sums(rs3,rs2)
 rs2alt &lt;- unjoin_cent_sums(rs3,rs1)
 stopifnot(max(abs(rs1 - rs1alt)) &lt; 1e-7)
 stopifnot(max(abs(rs2 - rs2alt)) &lt; 1e-7)

</code></pre>

<hr>
<h2 id='cent2raw'>Convert between different types of moments, raw, central, standardized.</h2><span id='topic+cent2raw'></span>

<h3>Description</h3>

<p>Given raw or central or standardized moments, convert to another type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cent2raw(input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cent2raw_+3A_input">input</code></td>
<td>
<p>a vector of the count, then the mean, then the <code>2</code> through <code>k</code>
raw or central moments.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>

<hr>
<h2 id='centcosums-accessor'>Accessor methods.</h2><span id='topic+centcosums-accessor'></span><span id='topic+cosums'></span><span id='topic+cosums+2Ccentcosums-method'></span><span id='topic+sums+2Ccentcosums-method'></span><span id='topic+comoments'></span><span id='topic+comoments+2Ccentcosums-method'></span>

<h3>Description</h3>

<p>Access slot data from a <code>centcosums</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cosums(x)

## S4 method for signature 'centcosums'
cosums(x)

comoments(x, type = c("central", "raw"))

## S4 method for signature 'centcosums'
comoments(x, type = c("central", "raw"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="centcosums-accessor_+3A_x">x</code></td>
<td>
<p>a <code>centcosums</code> object.</p>
</td></tr>
<tr><td><code id="centcosums-accessor_+3A_type">type</code></td>
<td>
<p>the type of moment to compute.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>

<hr>
<h2 id='centcosums-class'>centcosums Class.</h2><span id='topic+centcosums-class'></span><span id='topic+initialize+2Ccentcosums-method'></span><span id='topic+initialize+2Ccentcosums-class'></span><span id='topic+centcosums'></span>

<h3>Description</h3>

<p>An S4 class to store (centered) cosums of data, and to support operations on 
the same.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'centcosums'
initialize(.Object, cosums, order = NA_real_)

centcosums(cosums, order = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="centcosums-class_+3A_.object">.Object</code></td>
<td>
<p>a <code>centcosums</code> object, or proto-object.</p>
</td></tr>
<tr><td><code id="centcosums-class_+3A_cosums">cosums</code></td>
<td>
<p>the output of <code><a href="#topic+cent_cosums">cent_cosums</a></code>, say.</p>
</td></tr>
<tr><td><code id="centcosums-class_+3A_order">order</code></td>
<td>
<p>the order, defaulting to <code>2</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A <code>centcosums</code> object contains a multidimensional array (now only
2-diemnsional), as output by <code>cent_cosums</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>centcosums</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>cosums</code></dt><dd><p>a multidimensional array of the cosums.</p>
</dd>
<dt><code>order</code></dt><dd><p>the maximum order. ignored for now.</p>
</dd>
</dl>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>


<h3>See Also</h3>

<p>cent_cosums
</p>


<h3>Examples</h3>

<pre><code class='language-R'>obj &lt;- new("centcosums",cosums=cent_cosums(matrix(rnorm(100*3),ncol=3),max_order=2),order=2)

</code></pre>

<hr>
<h2 id='centsums-class'>centsums Class.</h2><span id='topic+centsums-class'></span><span id='topic+initialize+2Ccentsums-method'></span><span id='topic+initialize+2Ccentsums-class'></span><span id='topic+centsums'></span>

<h3>Description</h3>

<p>An S4 class to store (centered) sums of data, and to support operations on 
the same.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'centsums'
initialize(.Object, sums, order = NA_real_)

centsums(sums, order = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="centsums-class_+3A_.object">.Object</code></td>
<td>
<p>a <code>centsums</code> object, or proto-object.</p>
</td></tr>
<tr><td><code id="centsums-class_+3A_sums">sums</code></td>
<td>
<p>a numeric vector.</p>
</td></tr>
<tr><td><code id="centsums-class_+3A_order">order</code></td>
<td>
<p>the order, defaulting to <code>length(sums)+1</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A <code>centsums</code> object contains a vector value of the data count,
the mean, and the <code class="reqn">k</code>th centered sum, for <code class="reqn">k</code> up to some
maximum order.
</p>


<h3>Value</h3>

<p>An object of class <code>centsums</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>sums</code></dt><dd><p>a numeric vector of the sums.</p>
</dd>
<dt><code>order</code></dt><dd><p>the maximum order.</p>
</dd>
</dl>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>obj &lt;- new("centsums",sums=c(1000,1.234,0.235),order=2)

</code></pre>

<hr>
<h2 id='fromo-NEWS'>News for package 'fromo':</h2><span id='topic+fromo-NEWS'></span>

<h3>Description</h3>

<p>News for package 'fromo'
</p>





<h3><a href="https://github.com/shabbychef/fromo"></a> Version 0.2.1 (2019-01-29) </h3>


<ul>
<li><p> fix memory leak for case where the mean only need be computed via a Welford object.
</p>
</li></ul>



<h3><a href="https://github.com/shabbychef/fromo"></a> Version 0.2.0 (2019-01-12) </h3>


<ul>
<li><p> add <code>std_cumulants</code>
</p>
</li>
<li><p> add <code><a href="#topic+running_sum">running_sum</a></code>, <code><a href="#topic+running_mean">running_mean</a></code>.
</p>
</li>
<li><p> Kahan compensated summation for these.
</p>
</li>
<li><p> Welford object under the hood.
</p>
</li>
<li><p> add weighted moments computation.
</p>
</li>
<li><p> add time-based running window computations.
</p>
</li>
<li><p> some speedups for obviously fast cases: no checking of NA, etc.
</p>
</li>
<li><p> move github figures to location CRAN understands.
</p>
</li></ul>



<h3><a href="https://github.com/shabbychef/fromo"></a> Version 0.1.3 (2016-04-04) </h3>


<ul>
<li><p> submit to CRAN
</p>
</li></ul>



<h3><a href="https://github.com/shabbychef/fromo"></a> Initial Version 0.1.0 (2016-03-25) </h3>


<ul>
<li><p> start work
</p>
</li></ul>


<hr>
<h2 id='running_apx_quantiles'>Compute approximate quantiles over a sliding window</h2><span id='topic+running_apx_quantiles'></span><span id='topic+running_apx_median'></span>

<h3>Description</h3>

<p>Computes cumulants up to some given order, then employs the Cornish-Fisher approximation
to compute approximate quantiles using a Gaussian basis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>running_apx_quantiles(v, p, window = NULL, wts = NULL, max_order = 5L,
  na_rm = FALSE, min_df = 0L, used_df = 0, restart_period = 100L,
  check_wts = FALSE, normalize_wts = TRUE)

running_apx_median(v, window = NULL, wts = NULL, max_order = 5L,
  na_rm = FALSE, min_df = 0L, used_df = 0, restart_period = 100L,
  check_wts = FALSE, normalize_wts = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="running_apx_quantiles_+3A_v">v</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="running_apx_quantiles_+3A_p">p</code></td>
<td>
<p>the probability points at which to compute the quantiles. Should be in the range (0,1).</p>
</td></tr>
<tr><td><code id="running_apx_quantiles_+3A_window">window</code></td>
<td>
<p>the window size. if given as finite integer or double, passed through.
If <code>NULL</code>, <code>NA_integer_</code>, <code>NA_real_</code> or <code>Inf</code> are given, equivalent
to an infinite window size. If negative, an error will be thrown.</p>
</td></tr>
<tr><td><code id="running_apx_quantiles_+3A_wts">wts</code></td>
<td>
<p>an optional vector of weights. Weights are &lsquo;replication&rsquo;
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding <code>v</code> value. If <code>NULL</code>, corresponds to
equal unit weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial, with the exception that methods which check for minimum df will,
in the weighted case, check against the sum of weights. For this reason,
weights less than 1 could cause <code>NA</code> to be returned unexpectedly due
to the minimum condition. When weights are <code>NA</code>, the same rules for checking <code>v</code>
are applied. That is, the observation will not contribute to the moment
if the weight is <code>NA</code> when <code>na_rm</code> is true. When there is no
checking, an <code>NA</code> value will cause the output to be <code>NA</code>.</p>
</td></tr>
<tr><td><code id="running_apx_quantiles_+3A_max_order">max_order</code></td>
<td>
<p>the maximum order of the centered moment to be computed.</p>
</td></tr>
<tr><td><code id="running_apx_quantiles_+3A_na_rm">na_rm</code></td>
<td>
<p>whether to remove NA, false by default.</p>
</td></tr>
<tr><td><code id="running_apx_quantiles_+3A_min_df">min_df</code></td>
<td>
<p>the minimum df to return a value, otherwise <code>NaN</code> is returned.
This can be used to prevent moments from being computed on too few observations.
Defaults to zero, meaning no restriction.</p>
</td></tr>
<tr><td><code id="running_apx_quantiles_+3A_used_df">used_df</code></td>
<td>
<p>the number of degrees of freedom consumed, used in the denominator
of the centered moments computation. These are subtracted from the number of
observations.</p>
</td></tr>
<tr><td><code id="running_apx_quantiles_+3A_restart_period">restart_period</code></td>
<td>
<p>the recompute period. because subtraction of elements can cause
loss of precision, the computation of moments is restarted periodically based on 
this parameter. Larger values mean fewer restarts and faster, though less accurate
results.</p>
</td></tr>
<tr><td><code id="running_apx_quantiles_+3A_check_wts">check_wts</code></td>
<td>
<p>a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.</p>
</td></tr>
<tr><td><code id="running_apx_quantiles_+3A_normalize_wts">normalize_wts</code></td>
<td>
<p>a boolean for whether the weights should be
renormalized to have a mean value of 1. This mean is computed over elements
which contribute to the moments, so if <code>na_rm</code> is set, that means non-NA
elements of <code>wts</code> that correspond to non-NA elements of the data
vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the cumulants, then approximates quantiles using AS269 of Lee &amp; Lin.
</p>


<h3>Value</h3>

<p>A matrix, with one row for each element of <code>x</code>, and one column for each element of <code>q</code>.
</p>


<h3>Note</h3>

<p>The current implementation is not as space-efficient as it could be, as it first computes
the cumulants for each row, then performs the Cornish-Fisher approximation on a row-by-row
basis. In the future, this computation may be moved earlier into the pipeline to be more
space efficient. File an issue if the memory footprint is an issue for you.
</p>
<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>
<p>Note that when weights are given, they are treated as replication weights.
This can have subtle effects on computations which require minimum
degrees of freedom, since the sum of weights will be compared to
that minimum, not the number of data points. Weight values
(much) less than 1 can cause computations to return <code>NA</code>
somewhat unexpectedly due to this condition, while values greater
than one might cause the computation to spuriously return a value
with little precision.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+t_running_apx_quantiles">t_running_apx_quantiles</a></code>, <code><a href="#topic+running_cumulants">running_cumulants</a></code>, <code>PDQutils::qapx_cf</code>, <code>PDQutils::AS269</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(1e5)
xq &lt;- running_apx_quantiles(x,c(0.1,0.25,0.5,0.75,0.9))
xm &lt;- running_apx_median(x)

</code></pre>

<hr>
<h2 id='running_centered'>Compare data to moments computed over a sliding window.</h2><span id='topic+running_centered'></span><span id='topic+running_scaled'></span><span id='topic+running_zscored'></span><span id='topic+running_sharpe'></span><span id='topic+running_tstat'></span>

<h3>Description</h3>

<p>Computes moments over a sliding window, then adjusts the data accordingly, centering, or scaling,
or z-scoring, and so on.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>running_centered(v, window = NULL, wts = NULL, na_rm = FALSE,
  min_df = 0L, used_df = 1, lookahead = 0L, restart_period = 100L,
  check_wts = FALSE, normalize_wts = FALSE)

running_scaled(v, window = NULL, wts = NULL, na_rm = FALSE, min_df = 0L,
  used_df = 1, lookahead = 0L, restart_period = 100L, check_wts = FALSE,
  normalize_wts = TRUE)

running_zscored(v, window = NULL, wts = NULL, na_rm = FALSE,
  min_df = 0L, used_df = 1, lookahead = 0L, restart_period = 100L,
  check_wts = FALSE, normalize_wts = TRUE)

running_sharpe(v, window = NULL, wts = NULL, na_rm = FALSE,
  compute_se = FALSE, min_df = 0L, used_df = 1, restart_period = 100L,
  check_wts = FALSE, normalize_wts = TRUE)

running_tstat(v, window = NULL, wts = NULL, na_rm = FALSE, min_df = 0L,
  used_df = 1, restart_period = 100L, check_wts = FALSE,
  normalize_wts = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="running_centered_+3A_v">v</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="running_centered_+3A_window">window</code></td>
<td>
<p>the window size. if given as finite integer or double, passed through.
If <code>NULL</code>, <code>NA_integer_</code>, <code>NA_real_</code> or <code>Inf</code> are given, equivalent
to an infinite window size. If negative, an error will be thrown.</p>
</td></tr>
<tr><td><code id="running_centered_+3A_wts">wts</code></td>
<td>
<p>an optional vector of weights. Weights are &lsquo;replication&rsquo;
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding <code>v</code> value. If <code>NULL</code>, corresponds to
equal unit weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial, with the exception that methods which check for minimum df will,
in the weighted case, check against the sum of weights. For this reason,
weights less than 1 could cause <code>NA</code> to be returned unexpectedly due
to the minimum condition. When weights are <code>NA</code>, the same rules for checking <code>v</code>
are applied. That is, the observation will not contribute to the moment
if the weight is <code>NA</code> when <code>na_rm</code> is true. When there is no
checking, an <code>NA</code> value will cause the output to be <code>NA</code>.</p>
</td></tr>
<tr><td><code id="running_centered_+3A_na_rm">na_rm</code></td>
<td>
<p>whether to remove NA, false by default.</p>
</td></tr>
<tr><td><code id="running_centered_+3A_min_df">min_df</code></td>
<td>
<p>the minimum df to return a value, otherwise <code>NaN</code> is returned.
This can be used to prevent <em>e.g.</em> Z-scores from being computed on only 3
observations. Defaults to zero, meaning no restriction, which can result in 
infinite Z-scores during the burn-in period.</p>
</td></tr>
<tr><td><code id="running_centered_+3A_used_df">used_df</code></td>
<td>
<p>the number of degrees of freedom consumed, used in the denominator
of the centered moments computation. These are subtracted from the number of
observations.</p>
</td></tr>
<tr><td><code id="running_centered_+3A_lookahead">lookahead</code></td>
<td>
<p>for some of the operations, the value is compared to 
mean and standard deviation possibly using 'future' or 'past' information
by means of a non-zero lookahead. Positive values mean data are taken from
the future.</p>
</td></tr>
<tr><td><code id="running_centered_+3A_restart_period">restart_period</code></td>
<td>
<p>the recompute period. because subtraction of elements can cause
loss of precision, the computation of moments is restarted periodically based on 
this parameter. Larger values mean fewer restarts and faster, though less accurate
results.</p>
</td></tr>
<tr><td><code id="running_centered_+3A_check_wts">check_wts</code></td>
<td>
<p>a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.</p>
</td></tr>
<tr><td><code id="running_centered_+3A_normalize_wts">normalize_wts</code></td>
<td>
<p>a boolean for whether the weights should be
renormalized to have a mean value of 1. This mean is computed over elements
which contribute to the moments, so if <code>na_rm</code> is set, that means non-NA
elements of <code>wts</code> that correspond to non-NA elements of the data
vector.</p>
</td></tr>
<tr><td><code id="running_centered_+3A_compute_se">compute_se</code></td>
<td>
<p>for <code>running_sharpe</code>, return an extra column of the
standard error, as computed by Mertens' correction.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given the length <code class="reqn">n</code> vector <code class="reqn">x</code>, for
a given index <code class="reqn">i</code>, define <code class="reqn">x^{(i)}</code>
as the vector of 
<code class="reqn">x_{i-window+1},x_{i-window+2},...,x_{i}</code>,
where we do not run over the 'edge' of the vector. In code, this is essentially
<code>x[(max(1,i-window+1)):i]</code>. Then define <code class="reqn">\mu_i</code>, <code class="reqn">\sigma_i</code>
and <code class="reqn">n_i</code> as, respectively, the sample mean, standard deviation and number of
non-NA elements in <code class="reqn">x^{(i)}</code>. 
</p>
<p>We compute output vector <code class="reqn">m</code> the same size as <code class="reqn">x</code>. 
For the 'centered' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = x_i - \mu_i</code>.
For the 'scaled' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = x_i / \sigma_i</code>.
For the 'z-scored' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = (x_i - \mu_i) / \sigma_i</code>.
For the 't-scored' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = \sqrt{n_i} \mu_i / \sigma_i</code>.
</p>
<p>We also allow a 'lookahead' for some of these operations.
If positive, the moments are computed using data from larger indices;
if negative, from smaller indices. Letting <code class="reqn">j = i + lookahead</code>:
For the 'centered' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = x_i - \mu_j</code>.
For the 'scaled' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = x_i / \sigma_j</code>.
For the 'z-scored' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = (x_i - \mu_j) / \sigma_j</code>.
</p>


<h3>Value</h3>

<p>a vector the same size as the input consisting of the adjusted version of the input.
When there are not sufficient (non-nan) elements for the computation, <code>NaN</code> are returned.
</p>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>
<p>Note that when weights are given, they are treated as replication weights.
This can have subtle effects on computations which require minimum
degrees of freedom, since the sum of weights will be compared to
that minimum, not the number of data points. Weight values
(much) less than 1 can cause computations to return <code>NA</code>
somewhat unexpectedly due to this condition, while values greater
than one might cause the computation to spuriously return a value
with little precision.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+t_running_centered">t_running_centered</a></code>, <code><a href="base.html#topic+scale">scale</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (require(moments)) {
    set.seed(123)
    x &lt;- rnorm(5e1)
    window &lt;- 10L
    rm1 &lt;- t(sapply(seq_len(length(x)),function(iii) { 
                  xrang &lt;- x[max(1,iii-window+1):iii]
                  c(sd(xrang),mean(xrang),length(xrang)) },
                  simplify=TRUE))
    rcent &lt;- running_centered(x,window=window)
    rscal &lt;- running_scaled(x,window=window)
    rzsco &lt;- running_zscored(x,window=window)
    rshrp &lt;- running_sharpe(x,window=window)
    rtsco &lt;- running_tstat(x,window=window)
    rsrse &lt;- running_sharpe(x,window=window,compute_se=TRUE)
    stopifnot(max(abs(rcent - (x - rm1[,2])),na.rm=TRUE) &lt; 1e-12)
    stopifnot(max(abs(rscal - (x / rm1[,1])),na.rm=TRUE) &lt; 1e-12)
    stopifnot(max(abs(rzsco - ((x - rm1[,2]) / rm1[,1])),na.rm=TRUE) &lt; 1e-12)
    stopifnot(max(abs(rshrp - (rm1[,2] / rm1[,1])),na.rm=TRUE) &lt; 1e-12)
    stopifnot(max(abs(rtsco - ((sqrt(rm1[,3]) * rm1[,2]) / rm1[,1])),na.rm=TRUE) &lt; 1e-12)
    stopifnot(max(abs(rsrse[,1] - rshrp),na.rm=TRUE) &lt; 1e-12)

    rm2 &lt;- t(sapply(seq_len(length(x)),function(iii) { 
                  xrang &lt;- x[max(1,iii-window+1):iii]
                  c(kurtosis(xrang)-3.0,skewness(xrang)) },
                  simplify=TRUE))
    mertens_se &lt;- sqrt((1 + ((2 + rm2[,1])/4) * rshrp^2 - rm2[,2]*rshrp) / rm1[,3])
    stopifnot(max(abs(rsrse[,2] - mertens_se),na.rm=TRUE) &lt; 1e-12)
}

</code></pre>

<hr>
<h2 id='running_sd3'>Compute first K moments over a sliding window</h2><span id='topic+running_sd3'></span><span id='topic+running_skew4'></span><span id='topic+running_kurt5'></span><span id='topic+running_sd'></span><span id='topic+running_skew'></span><span id='topic+running_kurt'></span><span id='topic+running_cent_moments'></span><span id='topic+running_std_moments'></span><span id='topic+running_cumulants'></span>

<h3>Description</h3>

<p>Compute the (standardized) 2nd through kth moments, the mean, and the number of elements over
an infinite or finite sliding window, returning a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>running_sd3(v, window = NULL, wts = NULL, na_rm = FALSE, min_df = 0L,
  used_df = 1, restart_period = 100L, check_wts = FALSE,
  normalize_wts = TRUE)

running_skew4(v, window = NULL, wts = NULL, na_rm = FALSE, min_df = 0L,
  used_df = 1, restart_period = 100L, check_wts = FALSE,
  normalize_wts = TRUE)

running_kurt5(v, window = NULL, wts = NULL, na_rm = FALSE, min_df = 0L,
  used_df = 1, restart_period = 100L, check_wts = FALSE,
  normalize_wts = TRUE)

running_sd(v, window = NULL, wts = NULL, na_rm = FALSE, min_df = 0L,
  used_df = 1, restart_period = 100L, check_wts = FALSE,
  normalize_wts = TRUE)

running_skew(v, window = NULL, wts = NULL, na_rm = FALSE, min_df = 0L,
  used_df = 1, restart_period = 100L, check_wts = FALSE,
  normalize_wts = TRUE)

running_kurt(v, window = NULL, wts = NULL, na_rm = FALSE, min_df = 0L,
  used_df = 1, restart_period = 100L, check_wts = FALSE,
  normalize_wts = TRUE)

running_cent_moments(v, window = NULL, wts = NULL, max_order = 5L,
  na_rm = FALSE, max_order_only = FALSE, min_df = 0L, used_df = 0,
  restart_period = 100L, check_wts = FALSE, normalize_wts = TRUE)

running_std_moments(v, window = NULL, wts = NULL, max_order = 5L,
  na_rm = FALSE, min_df = 0L, used_df = 0, restart_period = 100L,
  check_wts = FALSE, normalize_wts = TRUE)

running_cumulants(v, window = NULL, wts = NULL, max_order = 5L,
  na_rm = FALSE, min_df = 0L, used_df = 0, restart_period = 100L,
  check_wts = FALSE, normalize_wts = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="running_sd3_+3A_v">v</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="running_sd3_+3A_window">window</code></td>
<td>
<p>the window size. if given as finite integer or double, passed through.
If <code>NULL</code>, <code>NA_integer_</code>, <code>NA_real_</code> or <code>Inf</code> are given, equivalent
to an infinite window size. If negative, an error will be thrown.</p>
</td></tr>
<tr><td><code id="running_sd3_+3A_wts">wts</code></td>
<td>
<p>an optional vector of weights. Weights are &lsquo;replication&rsquo;
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding <code>v</code> value. If <code>NULL</code>, corresponds to
equal unit weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial, with the exception that methods which check for minimum df will,
in the weighted case, check against the sum of weights. For this reason,
weights less than 1 could cause <code>NA</code> to be returned unexpectedly due
to the minimum condition. When weights are <code>NA</code>, the same rules for checking <code>v</code>
are applied. That is, the observation will not contribute to the moment
if the weight is <code>NA</code> when <code>na_rm</code> is true. When there is no
checking, an <code>NA</code> value will cause the output to be <code>NA</code>.</p>
</td></tr>
<tr><td><code id="running_sd3_+3A_na_rm">na_rm</code></td>
<td>
<p>whether to remove NA, false by default.</p>
</td></tr>
<tr><td><code id="running_sd3_+3A_min_df">min_df</code></td>
<td>
<p>the minimum df to return a value, otherwise <code>NaN</code> is returned.
This can be used to prevent moments from being computed on too few observations.
Defaults to zero, meaning no restriction.</p>
</td></tr>
<tr><td><code id="running_sd3_+3A_used_df">used_df</code></td>
<td>
<p>the number of degrees of freedom consumed, used in the denominator
of the centered moments computation. These are subtracted from the number of
observations.</p>
</td></tr>
<tr><td><code id="running_sd3_+3A_restart_period">restart_period</code></td>
<td>
<p>the recompute period. because subtraction of elements can cause
loss of precision, the computation of moments is restarted periodically based on 
this parameter. Larger values mean fewer restarts and faster, though less accurate
results.</p>
</td></tr>
<tr><td><code id="running_sd3_+3A_check_wts">check_wts</code></td>
<td>
<p>a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.</p>
</td></tr>
<tr><td><code id="running_sd3_+3A_normalize_wts">normalize_wts</code></td>
<td>
<p>a boolean for whether the weights should be
renormalized to have a mean value of 1. This mean is computed over elements
which contribute to the moments, so if <code>na_rm</code> is set, that means non-NA
elements of <code>wts</code> that correspond to non-NA elements of the data
vector.</p>
</td></tr>
<tr><td><code id="running_sd3_+3A_max_order">max_order</code></td>
<td>
<p>the maximum order of the centered moment to be computed.</p>
</td></tr>
<tr><td><code id="running_sd3_+3A_max_order_only">max_order_only</code></td>
<td>
<p>for <code>running_cent_moments</code>, if this flag is set, only compute
the maximum order centered moment, and return in a vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the number of elements, the mean, and the 2nd through kth
centered (and typically standardized) moments, for <code class="reqn">k=2,3,4</code>. These
are computed via the numerically robust one-pass method of Bennett <em>et. al.</em>
</p>
<p>Given the length <code class="reqn">n</code> vector <code class="reqn">x</code>, we output matrix <code class="reqn">M</code> where
<code class="reqn">M_{i,j}</code> is the <code class="reqn">order - j + 1</code> moment (<em>i.e.</em>
excess kurtosis, skewness, standard deviation, mean or number of elements)
of <code class="reqn">x_{i-window+1},x_{i-window+2},...,x_{i}</code>.
Barring <code>NA</code> or <code>NaN</code>, this is over a window of size <code>window</code>.
During the 'burn-in' phase, we take fewer elements.
</p>


<h3>Value</h3>

<p>Typically a matrix, where the first columns are the kth, k-1th through 2nd standardized, 
centered moments, then a column of the mean, then a column of the number of (non-nan) elements in the input,
with the following exceptions:
</p>

<dl>
<dt>running_cent_moments</dt><dd><p>Computes arbitrary order centered moments. When <code>max_order_only</code> is set,
only a column of the maximum order centered moment is returned.</p>
</dd>
<dt>running_std_moments</dt><dd><p>Computes arbitrary order standardized moments, then the standard deviation, the mean,
and the count. There is not yet an option for <code>max_order_only</code>, but probably should be.</p>
</dd>
<dt>running_cumulants</dt><dd><p>Computes arbitrary order cumulants, and returns the kth, k-1th, through the second 
(which is the variance) cumulant, then the mean, and the count.</p>
</dd>
</dl>



<h3>Note</h3>

<p>the kurtosis is <em>excess kurtosis</em>, with a 3 subtracted, and should be nearly zero
for Gaussian input.
</p>
<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>
<p>Note that when weights are given, they are treated as replication weights.
This can have subtle effects on computations which require minimum
degrees of freedom, since the sum of weights will be compared to
that minimum, not the number of data points. Weight values
(much) less than 1 can cause computations to return <code>NA</code>
somewhat unexpectedly due to this condition, while values greater
than one might cause the computation to spuriously return a value
with little precision.
</p>
<p>As this code may add and remove observations, numerical imprecision
may result in negative estimates of squared quantities, like the
second or fourth moments.  We do not currently correct for this
issue, although it may be somewhat mitigated by setting a smaller
<code>restart_period</code>. In the future we will add a check for
this case. Post an issue if you experience this bug.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(1e5)
xs3 &lt;- running_sd3(x,10)
xs4 &lt;- running_skew4(x,10)

if (require(moments)) {
    set.seed(123)
    x &lt;- rnorm(5e1)
    window &lt;- 10L
    kt5 &lt;- running_kurt5(x,window=window)
    rm1 &lt;- t(sapply(seq_len(length(x)),function(iii) { 
                xrang &lt;- x[max(1,iii-window+1):iii]
                c(moments::kurtosis(xrang)-3.0,moments::skewness(xrang),
                sd(xrang),mean(xrang),length(xrang)) },
             simplify=TRUE))
    stopifnot(max(abs(kt5 - rm1),na.rm=TRUE) &lt; 1e-12)
}

xc6 &lt;- running_cent_moments(x,window=100L,max_order=6L)

</code></pre>

<hr>
<h2 id='running_sum'>Compute sums or means over a sliding window.</h2><span id='topic+running_sum'></span><span id='topic+running_mean'></span>

<h3>Description</h3>

<p>Compute the mean or sum over 
an infinite or finite sliding window, returning a vector the same size as the input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>running_sum(v, window = NULL, wts = NULL, na_rm = FALSE,
  restart_period = 10000L, check_wts = FALSE)

running_mean(v, window = NULL, wts = NULL, na_rm = FALSE, min_df = 0L,
  restart_period = 10000L, check_wts = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="running_sum_+3A_v">v</code></td>
<td>
<p>a vector.</p>
</td></tr>
<tr><td><code id="running_sum_+3A_window">window</code></td>
<td>
<p>the window size. if given as finite integer or double, passed through.
If <code>NULL</code>, <code>NA_integer_</code>, <code>NA_real_</code> or <code>Inf</code> are given, equivalent
to an infinite window size. If negative, an error will be thrown.</p>
</td></tr>
<tr><td><code id="running_sum_+3A_wts">wts</code></td>
<td>
<p>an optional vector of weights. Weights are &lsquo;replication&rsquo;
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding <code>v</code> value. If <code>NULL</code>, corresponds to
equal unit weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial, with the exception that methods which check for minimum df will,
in the weighted case, check against the sum of weights. For this reason,
weights less than 1 could cause <code>NA</code> to be returned unexpectedly due
to the minimum condition. When weights are <code>NA</code>, the same rules for checking <code>v</code>
are applied. That is, the observation will not contribute to the moment
if the weight is <code>NA</code> when <code>na_rm</code> is true. When there is no
checking, an <code>NA</code> value will cause the output to be <code>NA</code>.</p>
</td></tr>
<tr><td><code id="running_sum_+3A_na_rm">na_rm</code></td>
<td>
<p>whether to remove NA, false by default.</p>
</td></tr>
<tr><td><code id="running_sum_+3A_restart_period">restart_period</code></td>
<td>
<p>the recompute period. because subtraction of elements can cause
loss of precision, the computation of moments is restarted periodically based on 
this parameter. Larger values mean fewer restarts and faster, though potentially less 
accurate results. Unlike in the computation of even order moments, loss of precision
is unlikely to be disastrous, so the default value is rather large.</p>
</td></tr>
<tr><td><code id="running_sum_+3A_check_wts">check_wts</code></td>
<td>
<p>a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.</p>
</td></tr>
<tr><td><code id="running_sum_+3A_min_df">min_df</code></td>
<td>
<p>the minimum df to return a value, otherwise <code>NaN</code> is returned,
only for the means computation.
This can be used to prevent moments from being computed on too few observations.
Defaults to zero, meaning no restriction.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the mean or sum of the elements, using a Kahan's Compensated Summation Algorithm,
a numerically robust one-pass method.
</p>
<p>Given the length <code class="reqn">n</code> vector <code class="reqn">x</code>, we output matrix <code class="reqn">M</code> where
<code class="reqn">M_{i,1}</code> is the sum or mean 
of <code class="reqn">x_{i-window+1},x_{i-window+2},...,x_{i}</code>.
Barring <code>NA</code> or <code>NaN</code>, this is over a window of size <code>window</code>.
During the 'burn-in' phase, we take fewer elements. If fewer than <code>min_df</code> for
<code>running_mean</code>, returns <code>NA</code>.
</p>


<h3>Value</h3>

<p>A vector the same size as the input.
</p>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>
<p>Kahan, W. &quot;Further remarks on reducing truncation errors,&quot;
Communications of the ACM, 8 (1), 1965.
<a href="https://doi.org/10.1145/363707.363723">https://doi.org/10.1145/363707.363723</a>
</p>
<p>Wikipedia contributors &quot;Kahan summation algorithm,&quot; 
Wikipedia, The Free Encyclopedia, 
<a href="https://en.wikipedia.org/w/index.php?title=Kahan_summation_algorithm&amp;oldid=777164752">https://en.wikipedia.org/w/index.php?title=Kahan_summation_algorithm&amp;oldid=777164752</a>
(accessed May 31, 2017).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(1e5)
xs &lt;- running_sum(x,10)
xm &lt;- running_mean(x,100)

</code></pre>

<hr>
<h2 id='sd3'>Compute first K moments</h2><span id='topic+sd3'></span><span id='topic+skew4'></span><span id='topic+kurt5'></span><span id='topic+cent_moments'></span><span id='topic+std_moments'></span><span id='topic+cent_cumulants'></span><span id='topic+std_cumulants'></span>

<h3>Description</h3>

<p>Compute the (standardized) 2nd through kth moments, the mean, and the number of elements.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sd3(v, na_rm = FALSE, wts = NULL, sg_df = 1, check_wts = FALSE,
  normalize_wts = TRUE)

skew4(v, na_rm = FALSE, wts = NULL, sg_df = 1, check_wts = FALSE,
  normalize_wts = TRUE)

kurt5(v, na_rm = FALSE, wts = NULL, sg_df = 1, check_wts = FALSE,
  normalize_wts = TRUE)

cent_moments(v, max_order = 5L, used_df = 0L, na_rm = FALSE, wts = NULL,
  check_wts = FALSE, normalize_wts = TRUE)

std_moments(v, max_order = 5L, used_df = 0L, na_rm = FALSE, wts = NULL,
  check_wts = FALSE, normalize_wts = TRUE)

cent_cumulants(v, max_order = 5L, used_df = 0L, na_rm = FALSE,
  wts = NULL, check_wts = FALSE, normalize_wts = TRUE)

std_cumulants(v, max_order = 5L, used_df = 0L, na_rm = FALSE,
  wts = NULL, check_wts = FALSE, normalize_wts = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sd3_+3A_v">v</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="sd3_+3A_na_rm">na_rm</code></td>
<td>
<p>whether to remove NA, false by default.</p>
</td></tr>
<tr><td><code id="sd3_+3A_wts">wts</code></td>
<td>
<p>an optional vector of weights. Weights are &lsquo;replication&rsquo;
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding <code>v</code> value. If <code>NULL</code>, corresponds to
equal unit weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial, with the exception that methods which check for minimum df will,
in the weighted case, check against the sum of weights. For this reason,
weights less than 1 could cause <code>NA</code> to be returned unexpectedly due
to the minimum condition. When weights are <code>NA</code>, the same rules for checking <code>v</code>
are applied. That is, the observation will not contribute to the moment
if the weight is <code>NA</code> when <code>na_rm</code> is true. When there is no
checking, an <code>NA</code> value will cause the output to be <code>NA</code>.</p>
</td></tr>
<tr><td><code id="sd3_+3A_sg_df">sg_df</code></td>
<td>
<p>the number of degrees of freedom consumed in the computation of
the variance or standard deviation. This defaults to 1 to match the 
&lsquo;Bessel correction&rsquo;.</p>
</td></tr>
<tr><td><code id="sd3_+3A_check_wts">check_wts</code></td>
<td>
<p>a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.</p>
</td></tr>
<tr><td><code id="sd3_+3A_normalize_wts">normalize_wts</code></td>
<td>
<p>a boolean for whether the weights should be
renormalized to have a mean value of 1. This mean is computed over elements
which contribute to the moments, so if <code>na_rm</code> is set, that means non-NA
elements of <code>wts</code> that correspond to non-NA elements of the data
vector.</p>
</td></tr>
<tr><td><code id="sd3_+3A_max_order">max_order</code></td>
<td>
<p>the maximum order of the centered moment to be computed.</p>
</td></tr>
<tr><td><code id="sd3_+3A_used_df">used_df</code></td>
<td>
<p>the number of degrees of freedom consumed, used in the denominator
of the centered moments computation. These are subtracted from the number of
observations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the number of elements, the mean, and the 2nd through kth
centered standardized moment, for <code class="reqn">k=2,3,4</code>. These
are computed via the numerically robust one-pass method of Bennett <em>et. al.</em>
In general they will <em>not</em> match exactly with the 'standard'
implementations, due to differences in roundoff.
</p>
<p>These methods are reasonably fast, on par with the 'standard' implementations.
However, they will usually be faster than calling the various standard implementations
more than once.
</p>
<p>Moments are computed as follows, given some values <code class="reqn">x_i</code> and optional weights <code class="reqn">w_i</code>,
defaulting to 1, the weighted mean is computed as
</p>
<p style="text-align: center;"><code class="reqn">\mu = \frac{\sum_i x_i w_i}{\sum w_i}.</code>
</p>

<p>The weighted kth central sum is computed as
</p>
<p style="text-align: center;"><code class="reqn">\mu = \sum_i \left(x_i - \mu\right)^k w_i.</code>
</p>

<p>Let <code class="reqn">n = \sum_i w_i</code> be the sum of weights (or number of observations in the unweighted case).
Then the weighted kth central moment is computed as that weighted sum divided by the
adjusted sum weights:
</p>
<p style="text-align: center;"><code class="reqn">\mu_k = \frac{\sum_i \left(x_i - \mu\right)^k w_i}{n - \nu},</code>
</p>

<p>where <code class="reqn">\nu</code> is the &lsquo;used df&rsquo;, provided by the user to adjust the denominator.
(Typical values are 0 or 1.)
The weighted kth standardized moment is the central moment divided by the second central moment
to the <code class="reqn">k/2</code> power:
</p>
<p style="text-align: center;"><code class="reqn">\tilde{\mu}_k = \frac{\mu_k}{\mu_2^{k/2}}.</code>
</p>

<p>The (centered) rth cumulant, for <code class="reqn">r \ge 2</code> is then computed using the formula of Willink, namely
</p>
<p style="text-align: center;"><code class="reqn">\kappa_r = \mu_r - \sum_{j=0}^{r - 2} {r - 1 \choose j} \mu_j \kappa {r-j}.</code>
</p>

<p>The standardized rth cumulant is the rth centered cumulant divided by <code class="reqn">\mu_2^{r/2}</code>.
</p>


<h3>Value</h3>

<p>a vector, filled out as follows:
</p>

<dl>
<dt>sd3</dt><dd><p>A vector of the (sample) standard devation, mean, and number of elements (or the total weight when <code>wts</code>
are given).</p>
</dd>
<dt>skew4</dt><dd><p>A vector of the (sample) skewness, standard devation, mean, and number of elements (or the total weight when 
<code>wts</code> are given).</p>
</dd>
<dt>kurt5</dt><dd><p>A vector of the (sample) excess kurtosis, skewness, standard devation, mean, and number of elements (or the
total weight when <code>wts</code> are given).</p>
</dd>
<dt>cent_moments</dt><dd><p>A vector of the (sample) <code class="reqn">k</code>th centered moment, then <code class="reqn">k-1</code>th centered moment, ..., 
then the <em>variance</em>, the mean, and number of elements (total weight when <code>wts</code> are given).</p>
</dd>
<dt>std_moments</dt><dd><p>A vector of the (sample) <code class="reqn">k</code>th standardized (and centered) moment, then 
<code class="reqn">k-1</code>th, ..., then standard devation, mean, and number of elements (total weight).</p>
</dd>
<dt>cent_cumulants</dt><dd><p>A vector of the (sample) <code class="reqn">k</code>th (centered, but this is redundant) cumulant, then the <code class="reqn">k-1</code>th, ...,
then the <em>variance</em> (which is the second cumulant), then <em>the mean</em>, then the number of elements (total weight).</p>
</dd>
<dt>std_cumulants</dt><dd><p>A vector of the (sample) <code class="reqn">k</code>th standardized (and centered, but this is redundant) cumulant, then the <code class="reqn">k-1</code>th, ...,
down to the third, then <em>the variance</em>, <em>the mean</em>, then the number of elements (total weight).</p>
</dd>
</dl>



<h3>Note</h3>

<p>The first centered (and standardized) moment is often defined to be identically 0. Instead <code>cent_moments</code>
and <code>std_moments</code> returns the mean. 
Similarly, the second standardized moments defined to be identically 1; <code>std_moments</code> instead returns the standard
deviation. The reason is that a user can always decide to ignore the results and fill in a 0 or 1 as they need, but 
could not efficiently compute the mean and standard deviation from scratch if we discard it.
The antepenultimate element of the output of <code>std_cumulants</code> is not a one, even though that &lsquo;should&rsquo; be
the standardized second cumulant.
</p>
<p>The antepenultimate element of the output of <code>cent_moments</code>, <code>cent_cumulants</code> and <code>std_cumulants</code> is the <em>variance</em>,
not the standard deviation. All other code return the standard deviation in that place.
</p>
<p>The kurtosis is <em>excess kurtosis</em>, with a 3 subtracted, and should be nearly zero
for Gaussian input.
</p>
<p>The term 'centered cumulants' is redundant. The intent was to avoid possible collision with existing code named 'cumulants'.
</p>
<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>
<p>Note that when weights are given, they are treated as replication weights.
This can have subtle effects on computations which require minimum
degrees of freedom, since the sum of weights will be compared to
that minimum, not the number of data points. Weight values
(much) less than 1 can cause computations to return <code>NA</code>
somewhat unexpectedly due to this condition, while values greater
than one might cause the computation to spuriously return a value
with little precision.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>
<p>Willink, R.  &quot;Relationships Between Central Moments and Cumulants, with Formulae for the Central Moments of Gamma Distributions.&quot;
Communications in Statistics - Theory and Methods, 32 no 4 (2003): 701-704.
<a href="https://doi.org/10.1081/STA-120018823">https://doi.org/10.1081/STA-120018823</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(1e5)
sd3(x)[1] - sd(x)
skew4(x)[4] - length(x)
skew4(x)[3] - mean(x)
skew4(x)[2] - sd(x)
if (require(moments)) {
  skew4(x)[1] - skewness(x)
}


# check 'robustness'; only the mean should change:
kurt5(x + 1e12) - kurt5(x)
# check speed
if (require(microbenchmark) &amp;&amp; require(moments)) {
  dumbk &lt;- function(x) { c(kurtosis(x) - 3.0,skewness(x),sd(x),mean(x),length(x)) }
  set.seed(1234)
  x &lt;- rnorm(1e6)
  print(kurt5(x) - dumbk(x))
  microbenchmark(dumbk(x),kurt5(x),times=10L)
}
y &lt;- std_moments(x,6)
cml &lt;- cent_cumulants(x,6)
std &lt;- std_cumulants(x,6)

# check that skew matches moments::skewness
if (require(moments)) {
    set.seed(1234)
    x &lt;- rnorm(1000)
    resu &lt;- fromo::skew4(x)

    msku &lt;- moments::skewness(x)
    stopifnot(abs(msku - resu[1]) &lt; 1e-14)
}

# check skew vs e1071 skewness, which has a different denominator
if (require(e1071)) {
    set.seed(1234)
    x &lt;- rnorm(1000)
    resu &lt;- fromo::skew4(x)

    esku &lt;- e1071::skewness(x,type=3)
    nobs &lt;- resu[4]
    stopifnot(abs(esku - resu[1] * ((nobs-1)/nobs)^(3/2)) &lt; 1e-14)

    # similarly:
    resu &lt;- fromo::std_moments(x,max_order=3,used_df=0)
    stopifnot(abs(esku - resu[1] * ((nobs-1)/nobs)^(3/2)) &lt; 1e-14)
}

</code></pre>

<hr>
<h2 id='show'>Show a centsums object.</h2><span id='topic+show'></span><span id='topic+show+2Ccentsums-method'></span>

<h3>Description</h3>

<p>Displays the centsums object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show(object)

## S4 method for signature 'centsums'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show_+3A_object">object</code></td>
<td>
<p>a <code>centsums</code> object.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
x &lt;- rnorm(1000)
obj &lt;- as.centsums(x, order=5)
obj
</code></pre>

<hr>
<h2 id='t_running_apx_quantiles'>Compute approximate quantiles over a sliding time window</h2><span id='topic+t_running_apx_quantiles'></span><span id='topic+t_running_apx_median'></span>

<h3>Description</h3>

<p>Computes cumulants up to some given order, then employs the Cornish-Fisher approximation
to compute approximate quantiles using a Gaussian basis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_running_apx_quantiles(v, p, time = NULL, time_deltas = NULL,
  window = NULL, wts = NULL, lb_time = NULL, max_order = 5L,
  na_rm = FALSE, min_df = 0L, used_df = 0, restart_period = 100L,
  variable_win = FALSE, wts_as_delta = TRUE, check_wts = FALSE,
  normalize_wts = TRUE)

t_running_apx_median(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, max_order = 5L, na_rm = FALSE,
  min_df = 0L, used_df = 0, restart_period = 100L, variable_win = FALSE,
  wts_as_delta = TRUE, check_wts = FALSE, normalize_wts = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_running_apx_quantiles_+3A_v">v</code></td>
<td>
<p>a vector of data.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_p">p</code></td>
<td>
<p>the probability points at which to compute the quantiles. Should be in the range (0,1).</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_time">time</code></td>
<td>
<p>an optional vector of the timestamps of <code>v</code>. If given, must be
the same length as <code>v</code>. If not given, we try to infer it by summing the
<code>time_deltas</code>.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_time_deltas">time_deltas</code></td>
<td>
<p>an optional vector of the deltas of timestamps. If given, must be
the same length as <code>v</code>. If not given, and <code>wts</code> are given and <code>wts_as_delta</code> is true,
we take the <code>wts</code> as the time deltas.  The deltas must be positive. We sum them to arrive
at the times.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_window">window</code></td>
<td>
<p>the window size, in time units. if given as finite integer or double, passed through.
If <code>NULL</code>, <code>NA_integer_</code>, <code>NA_real_</code> or <code>Inf</code> are given, 
and <code>variable_win</code> is true, then we infer the window from the lookback times: the
first window is infinite, but the remaining is the deltas between lookback times.
If <code>variable_win</code> is false, then these undefined values are equivalent to an
infinite window.
If negative, an error will be thrown.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_wts">wts</code></td>
<td>
<p>an optional vector of weights. Weights are &lsquo;replication&rsquo;
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding <code>v</code> value. If <code>NULL</code>, corresponds to
equal unit weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial, with the exception that methods which check for minimum df will,
in the weighted case, check against the sum of weights. For this reason,
weights less than 1 could cause <code>NA</code> to be returned unexpectedly due
to the minimum condition. When weights are <code>NA</code>, the same rules for checking <code>v</code>
are applied. That is, the observation will not contribute to the moment
if the weight is <code>NA</code> when <code>na_rm</code> is true. When there is no
checking, an <code>NA</code> value will cause the output to be <code>NA</code>.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_lb_time">lb_time</code></td>
<td>
<p>a vector of the times from which lookback will be performed. The output should
be the same size as this vector. If not given, defaults to <code>time</code>.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_max_order">max_order</code></td>
<td>
<p>the maximum order of the centered moment to be computed.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_na_rm">na_rm</code></td>
<td>
<p>whether to remove NA, false by default.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_min_df">min_df</code></td>
<td>
<p>the minimum df to return a value, otherwise <code>NaN</code> is returned.
This can be used to prevent moments from being computed on too few observations.
Defaults to zero, meaning no restriction.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_used_df">used_df</code></td>
<td>
<p>the number of degrees of freedom consumed, used in the denominator
of the centered moments computation. These are subtracted from the number of
observations.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_restart_period">restart_period</code></td>
<td>
<p>the recompute period. because subtraction of elements can cause
loss of precision, the computation of moments is restarted periodically based on 
this parameter. Larger values mean fewer restarts and faster, though less accurate
results.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_variable_win">variable_win</code></td>
<td>
<p>if true, and the <code>window</code> is not a concrete number,
the computation window becomes the time between lookback times.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_wts_as_delta">wts_as_delta</code></td>
<td>
<p>if true and the <code>time</code> and <code>time_deltas</code> are not
given, but <code>wts</code> are given, we take <code>wts</code> as the <code>time_deltas</code>.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_check_wts">check_wts</code></td>
<td>
<p>a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.</p>
</td></tr>
<tr><td><code id="t_running_apx_quantiles_+3A_normalize_wts">normalize_wts</code></td>
<td>
<p>a boolean for whether the weights should be
renormalized to have a mean value of 1. This mean is computed over elements
which contribute to the moments, so if <code>na_rm</code> is set, that means non-NA
elements of <code>wts</code> that correspond to non-NA elements of the data
vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the cumulants, then approximates quantiles using AS269 of Lee &amp; Lin.
</p>


<h3>Value</h3>

<p>A matrix, with one row for each element of <code>x</code>, and one column for each element of <code>q</code>.
</p>


<h3>Time Windowing </h3>

<p>This function supports time (or other counter) based running computation. 
Here the input are the data <code class="reqn">x_i</code>, and optional weights vectors, <code class="reqn">w_i</code>, defaulting to 1,
and a vector of time indices, <code class="reqn">t_i</code> of the same length as <code class="reqn">x</code>. The
times must be non-decreasing:
</p>
<p style="text-align: center;"><code class="reqn">t_1 \le t_2 \le \ldots</code>
</p>

<p>It is assumed that <code class="reqn">t_0 = -\infty</code>.
The window, <code class="reqn">W</code> is now a time-based window. 
An optional set of <em>lookback times</em> are also given, <code class="reqn">b_j</code>, which
may have different length than the <code class="reqn">x</code> and <code class="reqn">w</code>. 
The output will correspond to the lookback times, and should be the same
length. The <code class="reqn">j</code>th output is computed over indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">b_j - W &lt; t_i \le b_j.</code>
</p>

<p>For comparison functions (like Z-score, rescaling, centering), which compare
values of <code class="reqn">x_i</code> to local moments, the lookbacks may not be given, but
a lookahead <code class="reqn">L</code> is admitted. In this case, the <code class="reqn">j</code>th output is computed over
indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">t_j - W + L &lt; t_i \le t_j + L.</code>
</p>

<p>If the times are not given, &lsquo;deltas&rsquo; may be given instead. If
<code class="reqn">\delta_i</code> are the deltas, then we compute the times as
</p>
<p style="text-align: center;"><code class="reqn">t_i = \sum_{1 \le j \le i} \delta_j.</code>
</p>

<p>The deltas must be the same length as <code class="reqn">x</code>.  
If times and deltas are not given, but weights are given and the &lsquo;weights as deltas&rsquo;
flag is set true, then the weights are used as the deltas.
</p>
<p>Some times it makes sense to have the computational window be the space
between lookback times. That is, the <code class="reqn">j</code>th output is to be
computed over indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">b_{j-1} - W &lt; t_i \le b_j.</code>
</p>

<p>This can be achieved by setting the &lsquo;variable window&rsquo; flag true
and setting the window to null. This will not make much sense if
the lookback times are equal to the times, since each moment computation
is over a set of a single index, and most moments are underdefined.
</p>


<h3>Note</h3>

<p>The current implementation is not as space-efficient as it could be, as it first computes
the cumulants for each row, then performs the Cornish-Fisher approximation on a row-by-row
basis. In the future, this computation may be moved earlier into the pipeline to be more
space efficient. File an issue if the memory footprint is an issue for you.
</p>
<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>
<p>Note that when weights are given, they are treated as replication weights.
This can have subtle effects on computations which require minimum
degrees of freedom, since the sum of weights will be compared to
that minimum, not the number of data points. Weight values
(much) less than 1 can cause computations to return <code>NA</code>
somewhat unexpectedly due to this condition, while values greater
than one might cause the computation to spuriously return a value
with little precision.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+running_apx_quantiles">running_apx_quantiles</a></code>, <code><a href="#topic+t_running_cumulants">t_running_cumulants</a></code>, <code>PDQutils::qapx_cf</code>, <code>PDQutils::AS269</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(1e5)
xq &lt;- t_running_apx_quantiles(x,c(0.1,0.25,0.5,0.75,0.9),
       time=seq_along(x),window=200,lb_time=c(100,200,400))

xq &lt;- t_running_apx_median(x,time=seq_along(x),window=200,lb_time=c(100,200,400))
xq &lt;- t_running_apx_median(x,time=cumsum(runif(length(x),min=0.5,max=1.5)),
      window=200,lb_time=c(100,200,400))

# weighted median?
wts &lt;- runif(length(x),min=1,max=5)
xq &lt;- t_running_apx_median(x,wts=wts,wts_as_delta=TRUE,window=1000,lb_time=seq(1000,10000,by=1000))

# these should give the same answer:
xr &lt;- running_apx_median(x,window=200);
xt &lt;- t_running_apx_median(x,time=seq_along(x),window=199.99)

</code></pre>

<hr>
<h2 id='t_running_centered'>Compare data to moments computed over a time sliding window.</h2><span id='topic+t_running_centered'></span><span id='topic+t_running_scaled'></span><span id='topic+t_running_zscored'></span><span id='topic+t_running_sharpe'></span><span id='topic+t_running_tstat'></span>

<h3>Description</h3>

<p>Computes moments over a sliding window, then adjusts the data accordingly, centering, or scaling,
or z-scoring, and so on.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_running_centered(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, na_rm = FALSE, min_df = 0L, used_df = 1, lookahead = 0,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_scaled(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, na_rm = FALSE, min_df = 0L, used_df = 1, lookahead = 0,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_zscored(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, na_rm = FALSE, min_df = 0L, used_df = 1, lookahead = 0,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_sharpe(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, compute_se = FALSE,
  min_df = 0L, used_df = 1, restart_period = 100L, variable_win = FALSE,
  wts_as_delta = TRUE, check_wts = FALSE, normalize_wts = TRUE)

t_running_tstat(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, compute_se = FALSE,
  min_df = 0L, used_df = 1, restart_period = 100L, variable_win = FALSE,
  wts_as_delta = TRUE, check_wts = FALSE, normalize_wts = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_running_centered_+3A_v">v</code></td>
<td>
<p>a vector of data.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_time">time</code></td>
<td>
<p>an optional vector of the timestamps of <code>v</code>. If given, must be
the same length as <code>v</code>. If not given, we try to infer it by summing the
<code>time_deltas</code>.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_time_deltas">time_deltas</code></td>
<td>
<p>an optional vector of the deltas of timestamps. If given, must be
the same length as <code>v</code>. If not given, and <code>wts</code> are given and <code>wts_as_delta</code> is true,
we take the <code>wts</code> as the time deltas.  The deltas must be positive. We sum them to arrive
at the times.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_window">window</code></td>
<td>
<p>the window size, in time units. if given as finite integer or double, passed through.
If <code>NULL</code>, <code>NA_integer_</code>, <code>NA_real_</code> or <code>Inf</code> are given, 
and <code>variable_win</code> is true, then we infer the window from the lookback times: the
first window is infinite, but the remaining is the deltas between lookback times.
If <code>variable_win</code> is false, then these undefined values are equivalent to an
infinite window.
If negative, an error will be thrown.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_wts">wts</code></td>
<td>
<p>an optional vector of weights. Weights are &lsquo;replication&rsquo;
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding <code>v</code> value. If <code>NULL</code>, corresponds to
equal unit weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial, with the exception that methods which check for minimum df will,
in the weighted case, check against the sum of weights. For this reason,
weights less than 1 could cause <code>NA</code> to be returned unexpectedly due
to the minimum condition. When weights are <code>NA</code>, the same rules for checking <code>v</code>
are applied. That is, the observation will not contribute to the moment
if the weight is <code>NA</code> when <code>na_rm</code> is true. When there is no
checking, an <code>NA</code> value will cause the output to be <code>NA</code>.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_na_rm">na_rm</code></td>
<td>
<p>whether to remove NA, false by default.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_min_df">min_df</code></td>
<td>
<p>the minimum df to return a value, otherwise <code>NaN</code> is returned.
This can be used to prevent <em>e.g.</em> Z-scores from being computed on only 3
observations. Defaults to zero, meaning no restriction, which can result in 
infinite Z-scores during the burn-in period.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_used_df">used_df</code></td>
<td>
<p>the number of degrees of freedom consumed, used in the denominator
of the centered moments computation. These are subtracted from the number of
observations.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_lookahead">lookahead</code></td>
<td>
<p>for some of the operations, the value is compared to 
mean and standard deviation possibly using 'future' or 'past' information
by means of a non-zero lookahead. Positive values mean data are taken from
the future. This is in time units, and so should be a real.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_restart_period">restart_period</code></td>
<td>
<p>the recompute period. because subtraction of elements can cause
loss of precision, the computation of moments is restarted periodically based on 
this parameter. Larger values mean fewer restarts and faster, though less accurate
results.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_variable_win">variable_win</code></td>
<td>
<p>if true, and the <code>window</code> is not a concrete number,
the computation window becomes the time between lookback times.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_wts_as_delta">wts_as_delta</code></td>
<td>
<p>if true and the <code>time</code> and <code>time_deltas</code> are not
given, but <code>wts</code> are given, we take <code>wts</code> as the <code>time_deltas</code>.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_check_wts">check_wts</code></td>
<td>
<p>a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_normalize_wts">normalize_wts</code></td>
<td>
<p>a boolean for whether the weights should be
renormalized to have a mean value of 1. This mean is computed over elements
which contribute to the moments, so if <code>na_rm</code> is set, that means non-NA
elements of <code>wts</code> that correspond to non-NA elements of the data
vector.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_lb_time">lb_time</code></td>
<td>
<p>a vector of the times from which lookback will be performed. The output should
be the same size as this vector. If not given, defaults to <code>time</code>.</p>
</td></tr>
<tr><td><code id="t_running_centered_+3A_compute_se">compute_se</code></td>
<td>
<p>for <code>running_sharpe</code>, return an extra column of the
standard error, as computed by Mertens' correction.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given the length <code class="reqn">n</code> vector <code class="reqn">x</code>, for
a given index <code class="reqn">i</code>, define <code class="reqn">x^{(i)}</code>
as the elements of <code class="reqn">x</code> defined by the sliding time window (see the section
on time windowing).
Then define <code class="reqn">\mu_i</code>, <code class="reqn">\sigma_i</code>
and <code class="reqn">n_i</code> as, respectively, the sample mean, standard deviation and number of
non-NA elements in <code class="reqn">x^{(i)}</code>. 
</p>
<p>We compute output vector <code class="reqn">m</code> the same size as <code class="reqn">x</code>. 
For the 'centered' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = x_i - \mu_i</code>.
For the 'scaled' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = x_i / \sigma_i</code>.
For the 'z-scored' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = (x_i - \mu_i) / \sigma_i</code>.
For the 't-scored' version of <code class="reqn">x</code>, we have <code class="reqn">m_i = \sqrt{n_i} \mu_i / \sigma_i</code>.
</p>
<p>We also allow a 'lookahead' for some of these operations.
If positive, the moments are computed using data from larger indices;
if negative, from smaller indices.
</p>


<h3>Value</h3>

<p>a vector the same size as the input consisting of the adjusted version of the input.
When there are not sufficient (non-nan) elements for the computation, <code>NaN</code> are returned.
</p>


<h3>Time Windowing </h3>

<p>This function supports time (or other counter) based running computation. 
Here the input are the data <code class="reqn">x_i</code>, and optional weights vectors, <code class="reqn">w_i</code>, defaulting to 1,
and a vector of time indices, <code class="reqn">t_i</code> of the same length as <code class="reqn">x</code>. The
times must be non-decreasing:
</p>
<p style="text-align: center;"><code class="reqn">t_1 \le t_2 \le \ldots</code>
</p>

<p>It is assumed that <code class="reqn">t_0 = -\infty</code>.
The window, <code class="reqn">W</code> is now a time-based window. 
An optional set of <em>lookback times</em> are also given, <code class="reqn">b_j</code>, which
may have different length than the <code class="reqn">x</code> and <code class="reqn">w</code>. 
The output will correspond to the lookback times, and should be the same
length. The <code class="reqn">j</code>th output is computed over indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">b_j - W &lt; t_i \le b_j.</code>
</p>

<p>For comparison functions (like Z-score, rescaling, centering), which compare
values of <code class="reqn">x_i</code> to local moments, the lookbacks may not be given, but
a lookahead <code class="reqn">L</code> is admitted. In this case, the <code class="reqn">j</code>th output is computed over
indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">t_j - W + L &lt; t_i \le t_j + L.</code>
</p>

<p>If the times are not given, &lsquo;deltas&rsquo; may be given instead. If
<code class="reqn">\delta_i</code> are the deltas, then we compute the times as
</p>
<p style="text-align: center;"><code class="reqn">t_i = \sum_{1 \le j \le i} \delta_j.</code>
</p>

<p>The deltas must be the same length as <code class="reqn">x</code>.  
If times and deltas are not given, but weights are given and the &lsquo;weights as deltas&rsquo;
flag is set true, then the weights are used as the deltas.
</p>
<p>Some times it makes sense to have the computational window be the space
between lookback times. That is, the <code class="reqn">j</code>th output is to be
computed over indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">b_{j-1} - W &lt; t_i \le b_j.</code>
</p>

<p>This can be achieved by setting the &lsquo;variable window&rsquo; flag true
and setting the window to null. This will not make much sense if
the lookback times are equal to the times, since each moment computation
is over a set of a single index, and most moments are underdefined.
</p>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>
<p>Note that when weights are given, they are treated as replication weights.
This can have subtle effects on computations which require minimum
degrees of freedom, since the sum of weights will be compared to
that minimum, not the number of data points. Weight values
(much) less than 1 can cause computations to return <code>NA</code>
somewhat unexpectedly due to this condition, while values greater
than one might cause the computation to spuriously return a value
with little precision.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+running_centered">running_centered</a></code>, <code><a href="base.html#topic+scale">scale</a></code>
</p>

<hr>
<h2 id='t_running_sd3'>Compute first K moments over a sliding time-based window</h2><span id='topic+t_running_sd3'></span><span id='topic+t_running_skew4'></span><span id='topic+t_running_kurt5'></span><span id='topic+t_running_sd'></span><span id='topic+t_running_skew'></span><span id='topic+t_running_kurt'></span><span id='topic+t_running_cent_moments'></span><span id='topic+t_running_std_moments'></span><span id='topic+t_running_cumulants'></span>

<h3>Description</h3>

<p>Compute the (standardized) 2nd through kth moments, the mean, and the number of elements over
an infinite or finite sliding time based window, returning a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_running_sd3(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, min_df = 0L, used_df = 1,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_skew4(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, min_df = 0L, used_df = 1,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_kurt5(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, min_df = 0L, used_df = 1,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_sd(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, min_df = 0L, used_df = 1,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_skew(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, min_df = 0L, used_df = 1,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_kurt(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, min_df = 0L, used_df = 1,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_cent_moments(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, max_order = 5L, na_rm = FALSE,
  max_order_only = FALSE, min_df = 0L, used_df = 0,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_std_moments(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, max_order = 5L, na_rm = FALSE,
  min_df = 0L, used_df = 0, restart_period = 100L, variable_win = FALSE,
  wts_as_delta = TRUE, check_wts = FALSE, normalize_wts = TRUE)

t_running_cumulants(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, max_order = 5L, na_rm = FALSE,
  min_df = 0L, used_df = 0, restart_period = 100L, variable_win = FALSE,
  wts_as_delta = TRUE, check_wts = FALSE, normalize_wts = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_running_sd3_+3A_v">v</code></td>
<td>
<p>a vector of data.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_time">time</code></td>
<td>
<p>an optional vector of the timestamps of <code>v</code>. If given, must be
the same length as <code>v</code>. If not given, we try to infer it by summing the
<code>time_deltas</code>.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_time_deltas">time_deltas</code></td>
<td>
<p>an optional vector of the deltas of timestamps. If given, must be
the same length as <code>v</code>. If not given, and <code>wts</code> are given and <code>wts_as_delta</code> is true,
we take the <code>wts</code> as the time deltas.  The deltas must be positive. We sum them to arrive
at the times.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_window">window</code></td>
<td>
<p>the window size, in time units. if given as finite integer or double, passed through.
If <code>NULL</code>, <code>NA_integer_</code>, <code>NA_real_</code> or <code>Inf</code> are given, 
and <code>variable_win</code> is true, then we infer the window from the lookback times: the
first window is infinite, but the remaining is the deltas between lookback times.
If <code>variable_win</code> is false, then these undefined values are equivalent to an
infinite window.
If negative, an error will be thrown.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_wts">wts</code></td>
<td>
<p>an optional vector of weights. Weights are &lsquo;replication&rsquo;
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding <code>v</code> value. If <code>NULL</code>, corresponds to
equal unit weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial, with the exception that methods which check for minimum df will,
in the weighted case, check against the sum of weights. For this reason,
weights less than 1 could cause <code>NA</code> to be returned unexpectedly due
to the minimum condition. When weights are <code>NA</code>, the same rules for checking <code>v</code>
are applied. That is, the observation will not contribute to the moment
if the weight is <code>NA</code> when <code>na_rm</code> is true. When there is no
checking, an <code>NA</code> value will cause the output to be <code>NA</code>.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_lb_time">lb_time</code></td>
<td>
<p>a vector of the times from which lookback will be performed. The output should
be the same size as this vector. If not given, defaults to <code>time</code>.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_na_rm">na_rm</code></td>
<td>
<p>whether to remove NA, false by default.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_min_df">min_df</code></td>
<td>
<p>the minimum df to return a value, otherwise <code>NaN</code> is returned.
This can be used to prevent moments from being computed on too few observations.
Defaults to zero, meaning no restriction.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_used_df">used_df</code></td>
<td>
<p>the number of degrees of freedom consumed, used in the denominator
of the centered moments computation. These are subtracted from the number of
observations.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_restart_period">restart_period</code></td>
<td>
<p>the recompute period. because subtraction of elements can cause
loss of precision, the computation of moments is restarted periodically based on 
this parameter. Larger values mean fewer restarts and faster, though less accurate
results.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_variable_win">variable_win</code></td>
<td>
<p>if true, and the <code>window</code> is not a concrete number,
the computation window becomes the time between lookback times.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_wts_as_delta">wts_as_delta</code></td>
<td>
<p>if true and the <code>time</code> and <code>time_deltas</code> are not
given, but <code>wts</code> are given, we take <code>wts</code> as the <code>time_deltas</code>.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_check_wts">check_wts</code></td>
<td>
<p>a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_normalize_wts">normalize_wts</code></td>
<td>
<p>a boolean for whether the weights should be
renormalized to have a mean value of 1. This mean is computed over elements
which contribute to the moments, so if <code>na_rm</code> is set, that means non-NA
elements of <code>wts</code> that correspond to non-NA elements of the data
vector.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_max_order">max_order</code></td>
<td>
<p>the maximum order of the centered moment to be computed.</p>
</td></tr>
<tr><td><code id="t_running_sd3_+3A_max_order_only">max_order_only</code></td>
<td>
<p>for <code>running_cent_moments</code>, if this flag is set, only compute
the maximum order centered moment, and return in a vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the number of elements, the mean, and the 2nd through kth
centered (and typically standardized) moments, for <code class="reqn">k=2,3,4</code>. These
are computed via the numerically robust one-pass method of Bennett <em>et. al.</em>
</p>
<p>Given the length <code class="reqn">n</code> vector <code class="reqn">x</code>, we output matrix <code class="reqn">M</code> where
<code class="reqn">M_{i,j}</code> is the <code class="reqn">order - j + 1</code> moment (<em>i.e.</em>
excess kurtosis, skewness, standard deviation, mean or number of elements)
of some elements <code class="reqn">x_i</code> defined by the sliding time window.
Barring <code>NA</code> or <code>NaN</code>, this is over a window of time width <code>window</code>.
</p>


<h3>Value</h3>

<p>Typically a matrix, where the first columns are the kth, k-1th through 2nd standardized, 
centered moments, then a column of the mean, then a column of the number of (non-nan) elements in the input,
with the following exceptions:
</p>

<dl>
<dt>t_running_cent_moments</dt><dd><p>Computes arbitrary order centered moments. When <code>max_order_only</code> is set,
only a column of the maximum order centered moment is returned.</p>
</dd>
<dt>t_running_std_moments</dt><dd><p>Computes arbitrary order standardized moments, then the standard deviation, the mean,
and the count. There is not yet an option for <code>max_order_only</code>, but probably should be.</p>
</dd>
<dt>t_running_cumulants</dt><dd><p>Computes arbitrary order cumulants, and returns the kth, k-1th, through the second 
(which is the variance) cumulant, then the mean, and the count.</p>
</dd>
</dl>



<h3>Time Windowing </h3>

<p>This function supports time (or other counter) based running computation. 
Here the input are the data <code class="reqn">x_i</code>, and optional weights vectors, <code class="reqn">w_i</code>, defaulting to 1,
and a vector of time indices, <code class="reqn">t_i</code> of the same length as <code class="reqn">x</code>. The
times must be non-decreasing:
</p>
<p style="text-align: center;"><code class="reqn">t_1 \le t_2 \le \ldots</code>
</p>

<p>It is assumed that <code class="reqn">t_0 = -\infty</code>.
The window, <code class="reqn">W</code> is now a time-based window. 
An optional set of <em>lookback times</em> are also given, <code class="reqn">b_j</code>, which
may have different length than the <code class="reqn">x</code> and <code class="reqn">w</code>. 
The output will correspond to the lookback times, and should be the same
length. The <code class="reqn">j</code>th output is computed over indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">b_j - W &lt; t_i \le b_j.</code>
</p>

<p>For comparison functions (like Z-score, rescaling, centering), which compare
values of <code class="reqn">x_i</code> to local moments, the lookbacks may not be given, but
a lookahead <code class="reqn">L</code> is admitted. In this case, the <code class="reqn">j</code>th output is computed over
indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">t_j - W + L &lt; t_i \le t_j + L.</code>
</p>

<p>If the times are not given, &lsquo;deltas&rsquo; may be given instead. If
<code class="reqn">\delta_i</code> are the deltas, then we compute the times as
</p>
<p style="text-align: center;"><code class="reqn">t_i = \sum_{1 \le j \le i} \delta_j.</code>
</p>

<p>The deltas must be the same length as <code class="reqn">x</code>.  
If times and deltas are not given, but weights are given and the &lsquo;weights as deltas&rsquo;
flag is set true, then the weights are used as the deltas.
</p>
<p>Some times it makes sense to have the computational window be the space
between lookback times. That is, the <code class="reqn">j</code>th output is to be
computed over indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">b_{j-1} - W &lt; t_i \le b_j.</code>
</p>

<p>This can be achieved by setting the &lsquo;variable window&rsquo; flag true
and setting the window to null. This will not make much sense if
the lookback times are equal to the times, since each moment computation
is over a set of a single index, and most moments are underdefined.
</p>


<h3>Note</h3>

<p>the kurtosis is <em>excess kurtosis</em>, with a 3 subtracted, and should be nearly zero
for Gaussian input.
</p>
<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>
<p>Note that when weights are given, they are treated as replication weights.
This can have subtle effects on computations which require minimum
degrees of freedom, since the sum of weights will be compared to
that minimum, not the number of data points. Weight values
(much) less than 1 can cause computations to return <code>NA</code>
somewhat unexpectedly due to this condition, while values greater
than one might cause the computation to spuriously return a value
with little precision.
</p>
<p>As this code may add and remove observations, numerical imprecision
may result in negative estimates of squared quantities, like the
second or fourth moments.  We do not currently correct for this
issue, although it may be somewhat mitigated by setting a smaller
<code>restart_period</code>. In the future we will add a check for
this case. Post an issue if you experience this bug.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+running_sd3">running_sd3</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(1e5)
xs3 &lt;- t_running_sd3(x,time=seq_along(x),window=10)
xs4 &lt;- t_running_skew4(x,time=seq_along(x),window=10)
# but what if you only cared about some middle values?
xs4 &lt;- t_running_skew4(x,time=seq_along(x),lb_time=(length(x) / 2) + 0:10,window=20)

</code></pre>

<hr>
<h2 id='t_running_sum'>Compute sums or means over a sliding time window.</h2><span id='topic+t_running_sum'></span><span id='topic+t_running_mean'></span>

<h3>Description</h3>

<p>Compute the mean or sum over 
an infinite or finite sliding time window, returning a vector the same size as the lookback
times.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t_running_sum(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, min_df = 0L,
  restart_period = 10000L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE)

t_running_mean(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, min_df = 0L,
  restart_period = 10000L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t_running_sum_+3A_v">v</code></td>
<td>
<p>a vector.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_time">time</code></td>
<td>
<p>an optional vector of the timestamps of <code>v</code>. If given, must be
the same length as <code>v</code>. If not given, we try to infer it by summing the
<code>time_deltas</code>.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_time_deltas">time_deltas</code></td>
<td>
<p>an optional vector of the deltas of timestamps. If given, must be
the same length as <code>v</code>. If not given, and <code>wts</code> are given and <code>wts_as_delta</code> is true,
we take the <code>wts</code> as the time deltas.  The deltas must be positive. We sum them to arrive
at the times.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_window">window</code></td>
<td>
<p>the window size, in time units. if given as finite integer or double, passed through.
If <code>NULL</code>, <code>NA_integer_</code>, <code>NA_real_</code> or <code>Inf</code> are given, 
and <code>variable_win</code> is true, then we infer the window from the lookback times: the
first window is infinite, but the remaining is the deltas between lookback times.
If <code>variable_win</code> is false, then these undefined values are equivalent to an
infinite window.
If negative, an error will be thrown.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_wts">wts</code></td>
<td>
<p>an optional vector of weights. Weights are &lsquo;replication&rsquo;
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding <code>v</code> value. If <code>NULL</code>, corresponds to
equal unit weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial, with the exception that methods which check for minimum df will,
in the weighted case, check against the sum of weights. For this reason,
weights less than 1 could cause <code>NA</code> to be returned unexpectedly due
to the minimum condition. When weights are <code>NA</code>, the same rules for checking <code>v</code>
are applied. That is, the observation will not contribute to the moment
if the weight is <code>NA</code> when <code>na_rm</code> is true. When there is no
checking, an <code>NA</code> value will cause the output to be <code>NA</code>.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_lb_time">lb_time</code></td>
<td>
<p>a vector of the times from which lookback will be performed. The output should
be the same size as this vector. If not given, defaults to <code>time</code>.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_na_rm">na_rm</code></td>
<td>
<p>whether to remove NA, false by default.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_min_df">min_df</code></td>
<td>
<p>the minimum df to return a value, otherwise <code>NaN</code> is returned,
only for the means computation.
This can be used to prevent moments from being computed on too few observations.
Defaults to zero, meaning no restriction.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_restart_period">restart_period</code></td>
<td>
<p>the recompute period. because subtraction of elements can cause
loss of precision, the computation of moments is restarted periodically based on 
this parameter. Larger values mean fewer restarts and faster, though potentially less 
accurate results. Unlike in the computation of even order moments, loss of precision
is unlikely to be disastrous, so the default value is rather large.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_variable_win">variable_win</code></td>
<td>
<p>if true, and the <code>window</code> is not a concrete number,
the computation window becomes the time between lookback times.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_wts_as_delta">wts_as_delta</code></td>
<td>
<p>if true and the <code>time</code> and <code>time_deltas</code> are not
given, but <code>wts</code> are given, we take <code>wts</code> as the <code>time_deltas</code>.</p>
</td></tr>
<tr><td><code id="t_running_sum_+3A_check_wts">check_wts</code></td>
<td>
<p>a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the mean or sum of the elements, using a Kahan's Compensated Summation Algorithm,
a numerically robust one-pass method.
</p>
<p>Given the length <code class="reqn">n</code> vector <code class="reqn">x</code>, we output matrix <code class="reqn">M</code> where
<code class="reqn">M_{i,1}</code> is the sum or mean 
of some elements <code class="reqn">x_i</code> defined by the sliding time window.
Barring <code>NA</code> or <code>NaN</code>, this is over a window of time width <code>window</code>.
</p>


<h3>Value</h3>

<p>A vector the same size as the lookback times.
</p>


<h3>Time Windowing </h3>

<p>This function supports time (or other counter) based running computation. 
Here the input are the data <code class="reqn">x_i</code>, and optional weights vectors, <code class="reqn">w_i</code>, defaulting to 1,
and a vector of time indices, <code class="reqn">t_i</code> of the same length as <code class="reqn">x</code>. The
times must be non-decreasing:
</p>
<p style="text-align: center;"><code class="reqn">t_1 \le t_2 \le \ldots</code>
</p>

<p>It is assumed that <code class="reqn">t_0 = -\infty</code>.
The window, <code class="reqn">W</code> is now a time-based window. 
An optional set of <em>lookback times</em> are also given, <code class="reqn">b_j</code>, which
may have different length than the <code class="reqn">x</code> and <code class="reqn">w</code>. 
The output will correspond to the lookback times, and should be the same
length. The <code class="reqn">j</code>th output is computed over indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">b_j - W &lt; t_i \le b_j.</code>
</p>

<p>For comparison functions (like Z-score, rescaling, centering), which compare
values of <code class="reqn">x_i</code> to local moments, the lookbacks may not be given, but
a lookahead <code class="reqn">L</code> is admitted. In this case, the <code class="reqn">j</code>th output is computed over
indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">t_j - W + L &lt; t_i \le t_j + L.</code>
</p>

<p>If the times are not given, &lsquo;deltas&rsquo; may be given instead. If
<code class="reqn">\delta_i</code> are the deltas, then we compute the times as
</p>
<p style="text-align: center;"><code class="reqn">t_i = \sum_{1 \le j \le i} \delta_j.</code>
</p>

<p>The deltas must be the same length as <code class="reqn">x</code>.  
If times and deltas are not given, but weights are given and the &lsquo;weights as deltas&rsquo;
flag is set true, then the weights are used as the deltas.
</p>
<p>Some times it makes sense to have the computational window be the space
between lookback times. That is, the <code class="reqn">j</code>th output is to be
computed over indices <code class="reqn">i</code> such that
</p>
<p style="text-align: center;"><code class="reqn">b_{j-1} - W &lt; t_i \le b_j.</code>
</p>

<p>This can be achieved by setting the &lsquo;variable window&rsquo; flag true
and setting the window to null. This will not make much sense if
the lookback times are equal to the times, since each moment computation
is over a set of a single index, and most moments are underdefined.
</p>


<h3>Note</h3>

<p>The moment computations provided by fromo are 
numerically robust, but will often <em>not</em> provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
</p>
<p>Note that when weights are given, they are treated as replication weights.
This can have subtle effects on computations which require minimum
degrees of freedom, since the sum of weights will be compared to
that minimum, not the number of data points. Weight values
(much) less than 1 can cause computations to return <code>NA</code>
somewhat unexpectedly due to this condition, while values greater
than one might cause the computation to spuriously return a value
with little precision.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Terriberry, T. &quot;Computing Higher-Order Moments Online.&quot;
<a href="http://people.xiph.org/~tterribe/notes/homs.html">http://people.xiph.org/~tterribe/notes/homs.html</a>
</p>
<p>J. Bennett, et. al., &quot;Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms,&quot; Proceedings of IEEE
International Conference on Cluster Computing, 2009.
<a href="https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265">https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265</a>
</p>
<p>Cook, J. D. &quot;Accurately computing running variance.&quot;
<a href="http://www.johndcook.com/standard_deviation.html">http://www.johndcook.com/standard_deviation.html</a>
</p>
<p>Cook, J. D. &quot;Comparing three methods of computing 
standard deviation.&quot;
<a href="http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation">http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation</a>
</p>
<p>Kahan, W. &quot;Further remarks on reducing truncation errors,&quot;
Communications of the ACM, 8 (1), 1965.
<a href="https://doi.org/10.1145/363707.363723">https://doi.org/10.1145/363707.363723</a>
</p>
<p>Wikipedia contributors &quot;Kahan summation algorithm,&quot; 
Wikipedia, The Free Encyclopedia, 
<a href="https://en.wikipedia.org/w/index.php?title=Kahan_summation_algorithm&amp;oldid=777164752">https://en.wikipedia.org/w/index.php?title=Kahan_summation_algorithm&amp;oldid=777164752</a>
(accessed May 31, 2017).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(1e5)
xs &lt;- t_running_sum(x,time=seq_along(x),window=10)
xm &lt;- t_running_mean(x,time=cumsum(runif(length(x))),window=7.3)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
