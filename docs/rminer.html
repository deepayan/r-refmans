<!DOCTYPE html><html lang="en"><head><title>Help for package rminer</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rminer}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CasesSeries'>
<p>Create a training set (data.frame) from a time series using a sliding window.</p></a></li>
<li><a href='#crossvaldata'>
<p>Computes k-fold cross validation for rminer models.</p></a></li>
<li><a href='#delevels'>
<p>Reduce, replace or transform levels of a data.frame or factor variable (useful for preprocessing datasets).</p></a></li>
<li><a href='#fit'>
<p>Fit a supervised data mining model (classification or regression) model</p></a></li>
<li><a href='#holdout'>
<p>Computes indexes for holdout data split into training and test sets.</p></a></li>
<li><a href='#Importance'>
<p>Measure input importance (including sensitivity analysis) given a supervised data mining model.</p></a></li>
<li><a href='#imputation'><p>Missing data imputation (e.g. substitution by value or hotdeck method).</p></a></li>
<li><a href='#lforecast'>
<p>Compute long term forecasts.</p></a></li>
<li><a href='#mgraph'>
<p>Mining graph function</p></a></li>
<li><a href='#mining'>
<p>Powerful function that trains and tests a particular fit model under several runs and a given validation method</p></a></li>
<li><a href='#mmetric'>
<p>Compute classification or regression error metrics.</p></a></li>
<li><a href='#mparheuristic'>
<p>Function that returns a list of searching (hyper)parameters for a particular model (classification or regression) or for a multiple list of models (automl or ensembles).</p></a></li>
<li><a href='#predict.fit'><p>predict method for fit objects (rminer)</p></a></li>
<li><a href='#rminer-internal'><p>Internal rminer Functions</p></a></li>
<li><a href='#sa_fri1'>
<p>Synthetic regression and classification datasets for measuring input importance of supervised learning models</p></a></li>
<li><a href='#savemining'>
<p>Load/save into a file the result of a fit (model) or mining functions.</p></a></li>
<li><a href='#sin1reg'><p>sin1 regression dataset</p></a></li>
<li><a href='#vecplot'>
<p>VEC plot function (to use in conjunction with Importance function).</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.4.8</td>
</tr>
<tr>
<td>Title:</td>
<td>Data Mining Classification and Regression Methods</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-10-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Paulo Cortez [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Paulo Cortez &lt;pcortez@dsi.uminho.pt&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Facilitates the use of data mining algorithms in classification and regression (including time series forecasting) tasks by presenting a short and coherent set of functions. Versions: 1.4.8 improved help, several warning and error code fixes (more stable version, all examples run correctly); 1.4.7 improved Importance function and examples, minor error fixes; 1.4.6 / 1.4.5 / 1.4.4 new automated machine learning (AutoML) and ensembles, via improved fit(), mining() and mparheuristic() functions, and new categorical preprocessing, via improved delevels() function; 1.4.3 new metrics (e.g., macro precision, explained variance), new "lssvm" model and improved mparheuristic() function; 1.4.2 new "NMAE" metric, "xgboost" and "cv.glmnet" models (16 classification and 18 regression models); 1.4.1 new tutorial and more robust version; 1.4 - new classification and regression models, with a total of 14 classification and 15 regression methods, including: Decision Trees, Neural Networks, Support Vector Machines, Random Forests, Bagging and Boosting; 1.3 and 1.3.1 - new classification and regression metrics; 1.2 - new input importance methods via improved Importance() function; 1.0 - first version.</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, plotrix, lattice, nnet, kknn, pls, MASS, mda, rpart,
randomForest, adabag, party, Cubist, kernlab, e1071, glmnet,
xgboost</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>Yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://cran.r-project.org/package=rminer">https://cran.r-project.org/package=rminer</a>
<a href="http://www3.dsi.uminho.pt/pcortez/rminer.html">http://www3.dsi.uminho.pt/pcortez/rminer.html</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-10-28 15:58:18 UTC; root</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-10-29 08:40:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='CasesSeries'>
Create a training set (data.frame) from a time series using a sliding window.
</h2><span id='topic+CasesSeries'></span>

<h3>Description</h3>

<p>Create a training set (data.frame) from a time series using a sliding window.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CasesSeries(t, W, start = 1, end = length(t))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CasesSeries_+3A_t">t</code></td>
<td>
<p>a time series (numeric vector).</p>
</td></tr>
<tr><td><code id="CasesSeries_+3A_w">W</code></td>
<td>
<p>a sliding window (with time lags, numeric vector).</p>
</td></tr>
<tr><td><code id="CasesSeries_+3A_start">start</code></td>
<td>
<p>starting period.</p>
</td></tr>
<tr><td><code id="CasesSeries_+3A_end">end</code></td>
<td>
<p>ending period.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Check reference for details.
</p>


<h3>Value</h3>

<p>Returns a data.frame, where <code>y</code> is the output target and the inputs are the time lags.
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>


<ul>
<li><p> To check for more details:<br />
P. Cortez.<br />
Sensitivity Analysis for Time Lag Selection to Forecast Seasonal Time Series using Neural Networks and Support Vector Machines.<br />
In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN 2010), pp. 3694-3701, Barcelona, Spain, July, 2010. 
IEEE Computer Society, ISBN: 978-1-4244-6917-8 (DVD edition).<br />
<a href="https://doi.org/10.1109/IJCNN.2010.5596890">doi:10.1109/IJCNN.2010.5596890</a><br />
</p>
</li>
<li><p> This tutorial shows additional code examples:<br />
P. Cortez.<br />
A tutorial on using the rminer R package for data mining tasks.<br />
Teaching Report, Department of Information Systems, ALGORITMI Research Centre, Engineering School, University of Minho, Guimaraes, 
Portugal, July 2015.<br />
<a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a><br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code>, <code><a href="#topic+lforecast">lforecast</a></code>, <code><a href="#topic+predict.fit">predict.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>t=1:20
d=CasesSeries(1:10,c(1,3,4))
print(d)
d=CasesSeries(1:10,c(1,2,3))
print(d)
</code></pre>

<hr>
<h2 id='crossvaldata'>
Computes k-fold cross validation for rminer models.
</h2><span id='topic+crossvaldata'></span>

<h3>Description</h3>

<p>Computes k-fold cross validation for rminer models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crossvaldata(x, data, theta.fit, theta.predict, ngroup = 10, 
             mode = "stratified", seed = NULL, model, task, feature = "none",
             ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="crossvaldata_+3A_x">x</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="crossvaldata_+3A_data">data</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="crossvaldata_+3A_theta.fit">theta.fit</code></td>
<td>
<p>fitting function</p>
</td></tr>
<tr><td><code id="crossvaldata_+3A_theta.predict">theta.predict</code></td>
<td>
<p>prediction function</p>
</td></tr>
<tr><td><code id="crossvaldata_+3A_ngroup">ngroup</code></td>
<td>
<p>number of folds</p>
</td></tr>
<tr><td><code id="crossvaldata_+3A_mode">mode</code></td>
<td>
<p>Possibilities are: &quot;stratified&quot;, &quot;random&quot; or &quot;order&quot; (see <code><a href="#topic+holdout">holdout</a></code> for details).</p>
</td></tr>
<tr><td><code id="crossvaldata_+3A_seed">seed</code></td>
<td>
<p>if <code>NULL</code> then no seed is used and the current R randomness is assumed; else a fixed seed is adopted to generate local random sample sequences, returning always the same result for the same seed (local means that it does not affect the state of other random number generations called after this function, see <code><a href="#topic+holdout">holdout</a></code> example).</p>
</td></tr>
<tr><td><code id="crossvaldata_+3A_model">model</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="crossvaldata_+3A_task">task</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="crossvaldata_+3A_feature">feature</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="crossvaldata_+3A_...">...</code></td>
<td>
<p>Additional parameters sent to <code>theta.fit</code> or <code>theta.predic</code> (e.g. <code>search</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Standard k-fold cross-validation adopted for rminer models.
By default, for classification tasks (&quot;class&quot; or &quot;prob&quot;) a stratified sampling is used 
(the class distributions are identical for each fold), unless <code>mode</code> is set to <code>random</code> or <code>order</code> 
(see <code><a href="#topic+holdout">holdout</a></code> for details).
</p>


<h3>Value</h3>

<p>Returns a list with:
</p>

<ul>
<li><p> $cv.fit &ndash; all predictions (factor if <code>task="class"</code>, matrix if <code>task="prob"</code> or numeric if <code>task="reg"</code>);
</p>
</li>
<li><p> $model &ndash; vector list with the model for each fold. 
</p>
</li>
<li><p> $mpar &ndash; vector list with the mpar for each fold;
</p>
</li>
<li><p> $attributes &ndash; the selected attributes for each fold if a feature selection algorithm was adopted;
</p>
</li>
<li><p> $ngroup &ndash; the number of folds;
</p>
</li>
<li><p> $leave.out &ndash; the computed size for each fold (=<code>nrow(data)/ngroup</code>);
</p>
</li>
<li><p> $groups &ndash; vector list with the indexes of each group;
</p>
</li>
<li><p> $call &ndash; the call of this function;
</p>
</li></ul>



<h3>Note</h3>

<p>A better control (e.g. use of several Runs) is achieved using the simpler <code><a href="#topic+mining">mining</a></code> function.</p>


<h3>Author(s)</h3>

<p>This function was adapted by Paulo Cortez from the <code>crossval</code> function of the bootstrap library (S original by R. Tibshirani and R port by F. Leisch).</p>


<h3>References</h3>

<p>Check the <code><a href="bootstrap.html#topic+crossval">crossval</a></code> function.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+holdout">holdout</a></code>, <code><a href="#topic+fit">fit</a></code>, <code><a href="#topic+mining">mining</a></code> and <code><a href="#topic+predict.fit">predict.fit</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>### dontrun is used when the execution of the example requires some computational effort.
## Not run: 
 data(iris)
 # 3-fold cross validation using fit and predict
 # the control argument is sent to rpart function 
 # rpart.control() is from the rpart package
 M=crossvaldata(Species~.,iris,fit,predict,ngroup=3,seed=12345,model="rpart",
                task="prob", control = rpart::rpart.control(cp=0.05))
 print("cross validation object:")
 print(M)
 C=mmetric(iris$Species,M$cv.fit,metric="CONF")
 print("confusion matrix:")
 print(C)

## End(Not run)
</code></pre>

<hr>
<h2 id='delevels'>
Reduce, replace or transform levels of a data.frame or factor variable (useful for preprocessing datasets).
</h2><span id='topic+delevels'></span>

<h3>Description</h3>

<p>Reduce, replace or transform levels of a data.frame or factor variable (useful for preprocessing datasets).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delevels(x, levels, label = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delevels_+3A_x">x</code></td>
<td>
<p><code><a href="base.html#topic+factor">factor</a></code> with several levels or a <code><a href="base.html#topic+data.frame">data.frame</a></code>. If a <code>data.frame</code>, then all factor attributes are transformed.</p>
</td></tr>
<tr><td><code id="delevels_+3A_levels">levels</code></td>
<td>
<p>character vector with several options: 
</p>

<ul>
<li> <p><code>idf</code> &ndash; factor is transformed into a numeric vector using IDF transform. 
</p>
</li>
<li> <p><code>pcp</code> or <code>c("pcp",perc)</code> &ndash; factor is transformed using PCP transform. If perc is not provided, the default 0.1 value is used.
</p>
</li>
<li><p> any other values &ndash; all level values are merged into a single factor level according to <code>label</code>.
</p>
</li></ul>

<p>Another possibility is to define a vector list, with <code>levels[[i]]</code> values for each <code>factor</code> of the <code>data.frame</code> (see example).
</p>
</td></tr>
<tr><td><code id="delevels_+3A_label">label</code></td>
<td>
<p>the new label used for all <code>levels</code> examples (if <code>NULL</code> then <code>"_OTHER"</code> is assumed).</p>
</td></tr>
</table>


<h3>Details</h3>

<p> The Inverse Document Frequency (IDF) uses f(x)= log(n/f_x), where n is the length of x and f_x is the frequency of x.<br />
The Percentage Categorical Pruned (PCP) merges all least frequent levels (summing up to perc percent) into a single level.<br />
When other values are used for <code>levels</code>, this function replaces all <code>levels</code> values with the single <code>label</code> value.
</p>


<h3>Value</h3>

<p>Returns a transformed factor or data.frame.
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>


<ul>
<li><p> PCP transform:<br />
L.M. Matos, P. Cortez, R. Mendes, A. Moreau.<br />
Using Deep Learning for Mobile Marketing User Conversion Prediction. 
In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN 2019), 
paper N-19327, Budapest, Hungary, July, 2019 (8 pages), IEEE, ISBN 978-1-7281-2009-6.<br />
<a href="https://doi.org/10.1109/IJCNN.2019.8851888">doi:10.1109/IJCNN.2019.8851888</a><br />
<a href="http://hdl.handle.net/1822/62771">http://hdl.handle.net/1822/62771</a><br />
</p>
</li>
<li><p> IDF transform:<br />
L.M. Matos, P. Cortez, R. Mendes and A. Moreau.<br />
A Comparison of Data-Driven Approaches for Mobile Marketing User Conversion Prediction.
In Proceedings of 9th IEEE International Conference on Intelligent Systems (IS 2018), pp. 140-146,
Funchal, Madeira, Portugal, September, 2018, IEEE, ISBN 978-1-5386-7097-2.<br />
<a href="https://ieeexplore.ieee.org/document/8710472">https://ieeexplore.ieee.org/document/8710472</a><br />
<a href="http://hdl.handle.net/1822/61586">http://hdl.handle.net/1822/61586</a><br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code> and <code><a href="#topic+imputation">imputation</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### simples examples:
f=factor(c("A","A","B","B","C","D","E"))
print(table(f))
# replace "A" with "a":
f1=delevels(f,"A","a")
print(table(f1))
# merge c("C","D","E") into "CDE":
f2=delevels(f,c("C","D","E"),"CDE")
print(table(f2))
# merge c("B","C","D","E") into _OTHER:
f3=delevels(f,c("B","C","D","E"))
print(table(f3))

## Not run: 
# larger factor:
x=factor(c(1,rep(2,2),rep(3,3),rep(4,4),rep(5,5),rep(10,10),rep(100,100)))
print(table(x))
# IDF: frequent values are close to zero and
# infrequent ones are more close to each other:
x1=delevels(x,"idf")
print(table(x1))
# PCP: infrequent values are merged
x2=delevels(x,c("pcp",0.1)) # around 10
print(table(x2))

# example with a data.frame:
y=factor(c(rep("a",100),rep("b",20),rep("c",5)))
z=1:125 # numeric
d=data.frame(x=x,y=y,z=z,x2=x)
print(summary(d))

# IDF:
d1=delevels(d,"idf")
print(summary(d1))
# PCP:
d2=delevels(d,"pcp")
print(summary(d2))
# delevels:
L=vector("list",ncol(d)) # one per attribute
L[[1]]=c("1","2","3","4","5")
L[[2]]=c("b","c")
L[[4]]=c("1","2","3") # different on purpose
d3=delevels(d,levels=L,label="other")
print(summary(d3))

## End(Not run) # end dontrun 

</code></pre>

<hr>
<h2 id='fit'>
Fit a supervised data mining model (classification or regression) model
</h2><span id='topic+fit'></span><span id='topic+model-class'></span>

<h3>Description</h3>

<p>Fit a supervised data mining model (classification or regression) model. Wrapper function that allows to fit distinct data mining (16 classification
and 18 regression) methods under the same coherent function structure. 
Also, it tunes the hyperparameters of the models (e.g., <code>kknn</code>, <code>mlpe</code> and <code>ksvm</code>) and performs some feature selection methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit(x, data = NULL, model = "default", task = "default", 
    search = "heuristic", mpar = NULL, feature = "none", 
    scale = "default", transform = "none", 
    created = NULL, fdebug = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit_+3A_x">x</code></td>
<td>
<p>a symbolic description (formula) of the model to be fit.<br /> 
If <code>data=NULL</code> it is assumed that <code>x</code> contains a formula expression with known variables (see first example below).</p>
</td></tr>
<tr><td><code id="fit_+3A_data">data</code></td>
<td>
<p>an optional data frame (columns denote attributes, rows show examples) containing the training data, when using a formula.</p>
</td></tr>
<tr><td><code id="fit_+3A_model">model</code></td>
<td>
<p>Typically this should be a character object with the model type name (data mining method, as explained in valid character options).<br />
<br />
First usage: individual fit. Valid character options are the typical R base learning functions (individual models), namely one of: 
</p>

<ul>
<li> <p><code>naive</code> most common class (classification) or mean output value (regression)
</p>
</li>
<li> <p><code>ctree</code> &ndash; conditional inference tree (classification and regression, uses <code><a href="party.html#topic+ctree">ctree</a></code>)
</p>
</li>
<li> <p><code>cv.glmnet</code> &ndash; generalized linear model (GLM) with lasso or elasticnet regularization (classification and regression, uses <code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code>; note: cross-validation is used to automatically set the lambda parameter that is needed to compute the predictions)
</p>
</li>
<li> <p><code>rpart</code> or <code>dt</code> &ndash; decision tree (classification and regression, uses <code><a href="rpart.html#topic+rpart">rpart</a></code>)
</p>
</li>
<li> <p><code>kknn</code> or <code>knn</code> &ndash; k-nearest neighbor (classification and regression, uses <code><a href="kknn.html#topic+kknn">kknn</a></code>)
</p>
</li>
<li> <p><code>ksvm</code> or <code>svm</code> &ndash; support vector machine (classification and regression, uses <code><a href="kernlab.html#topic+ksvm">ksvm</a></code>)
</p>
</li>
<li> <p><code>lssvm</code>  &ndash; least squares support vector machine (pure classification only, uses <code><a href="kernlab.html#topic+lssvm">lssvm</a></code>)
</p>
</li>
<li> <p><code>mlp</code> &ndash; multilayer perceptron with one hidden layer (classification and regression, uses <code><a href="nnet.html#topic+nnet">nnet</a></code> (in this version, for both <code>mlp</code> and <code>mlpe</code>, the maximum number of weights was increased and fixed to <code>MaxNWts=10000</code>))
</p>
</li>
<li> <p><code>mlpe</code> &ndash; multilayer perceptron ensemble (classification and regression, uses <code><a href="nnet.html#topic+nnet">nnet</a></code>)
</p>
</li>
<li> <p><code>randomForest</code> or <code>randomforest</code> &ndash; random forest algorithm (classification and regression, uses <code><a href="randomForest.html#topic+randomForest">randomForest</a></code>)
</p>
</li>
<li> <p><code>xgboost</code> &ndash; eXtreme Gradient Boosting (Tree) (classification and regression, uses <code><a href="xgboost.html#topic+xgboost">xgboost</a></code>; note: <code>nrounds</code> parameter is set by default to 2)
</p>
</li>
<li> <p><code>bagging</code> &ndash; bagging from Breiman, 1996 (classification, uses <code><a href="adabag.html#topic+bagging">bagging</a></code>)
</p>
</li>
<li> <p><code>boosting</code> &ndash; adaboost.M1 method from Freund and Schapire, 1996 (classification, uses <code><a href="adabag.html#topic+boosting">boosting</a></code>)
</p>
</li>
<li> <p><code>lda</code> &ndash; linear discriminant analysis (classification, uses <code><a href="MASS.html#topic+lda">lda</a></code>)
</p>
</li>
<li> <p><code>multinom</code> or <code>lr</code> &ndash; logistic regression (classification, uses <code><a href="nnet.html#topic+multinom">multinom</a></code>)
</p>
</li>
<li> <p><code>naiveBayes</code> or <code>naivebayes</code> &ndash; naive bayes (classification, uses <code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code>)
</p>
</li>
<li> <p><code>qda</code> &ndash; quadratic discriminant analysis (classification, uses <code><a href="MASS.html#topic+qda">qda</a></code>)
</p>
</li>
<li> <p><code>cubist</code> &ndash; M5 rule-based model (regression, uses <code><a href="Cubist.html#topic+cubist">cubist</a></code>)
</p>
</li>
<li> <p><code>lm</code> &ndash; standard multiple/linear regression (uses <code><a href="stats.html#topic+lm">lm</a></code>)
</p>
</li>
<li> <p><code>mr</code> &ndash; multiple regression (regression, equivalent to <code><a href="stats.html#topic+lm">lm</a></code> but uses <code><a href="nnet.html#topic+nnet">nnet</a></code> with zero hidden nodes and linear output function)
</p>
</li>
<li> <p><code>mars</code> &ndash; multivariate adaptive regression splines (regression, uses <code><a href="mda.html#topic+mars">mars</a></code>)
</p>
</li>
<li> <p><code>pcr</code> &ndash; principal component regression (regression, uses <code><a href="pls.html#topic+pcr">pcr</a></code>)
</p>
</li>
<li> <p><code>plsr</code> &ndash; partial least squares regression (regression, uses <code><a href="pls.html#topic+plsr">plsr</a></code>)
</p>
</li>
<li> <p><code>cppls</code> &ndash; canonical powered partial least squares (regression, uses <code><a href="pls.html#topic+cppls">cppls</a></code>)
</p>
</li>
<li> <p><code>rvm</code> &ndash; relevance vector machine (regression, uses <code><a href="kernlab.html#topic+rvm">rvm</a></code>)
</p>
</li></ul>

<p>Second usage: multiple models. <code>model</code> can be used to perform Automated Machine Learning (AutoML) or ensembles of several individual models:
</p>

<ul>
<li> <p><code>auto</code> &ndash; first, the best model is automatically set by searching all models defined in <code>search</code> and selecting the one with the best &ldquo;validation&rdquo; metric on a validation set (depending on the method defined in <code>search</code>); then, the selected best model is fit to all training data. When <code>auto</code> is used, a ranked leaderboard of the models (and their selected hyperparameters) is returned as a new <code>$LB</code> field of the <code>@mpar</code> returned slot (e.g., try: <code>print(M@mpar$LB)</code>, where <code>M</code> is an object returned by <code>fit</code>).
</p>
</li>
<li> <p><code>AE</code>, <code>WE</code> or <code>SE</code> &ndash; all individual models are first fit to the data; then an ensemble is built by: <code>AE</code> &ndash; Average Ensemble, majority (if <code>task=="class"</code>) or average of the predictions; <code>WE</code>) &ndash; Weighted Ensemble, similar to <code>AE</code> but each prediction is weighted according to the validation metric 
(for  <code>task=="class"</code> it is equal to <code>AE</code>); <code>SE</code> &ndash; Stacking Ensemble, applies a second-level GLM to weight the individual predictions. For any ensemble, when an individual model produces an error then it is excluded from the ensemble. After excluding invalid models, if there is just a single model then such model is returned (and no ensemble is produced).
</p>
</li></ul>

<p>Third usage: <code>model</code> can be a <code><a href="base.html#topic+list">list</a></code> with 2 possibilities of fields A) and B).<br />
A) if you have your one fit function, then you can embed it using:
</p>

<ul>
<li> <p><code>$fit</code> &ndash; a fit function that accepts the arguments <code>x</code>, <code>data</code> and <code>...</code>, the goal is to accept here any R classification or regression model, mainly for its use within the <code><a href="#topic+mining">mining</a></code> or <code><a href="#topic+Importance">Importance</a></code> functions, or to use a hyperparameter search (via <code>search</code>).
</p>
</li>
<li> <p><code>$predict</code> &ndash; a predict function that accepts the arguments <code>object</code>, <code>newdata</code>, this function should behave as any rminer prediction, i.e., return: a factor when <code>task=="class"</code>; a matrix with <em>Probabilities x Instances</em> when <code>task=="prob"</code>; and a vector when <code>task=="reg"</code>.
</p>
</li>
<li> <p><code>$name</code> &ndash; optional field with the name of the method.
</p>
</li></ul>

<p>B) automatically produced by some ensemble methods, for the sake of documentation the fields for the ensembles (&quot;AE&quot;, &quot;WE&quot; or &quot;SE&quot;) are listed here:
</p>

<ul>
<li> <p><code>$m</code> &ndash; a vector character with the fit object model names. 
</p>
</li>
<li> <p><code>$f</code> &ndash; a vector list with several fit objects.
</p>
</li>
<li> <p><code>$w</code> &ndash; a vector with the &quot;weighting&quot; of the individual models.
</p>
</li></ul>

<p>Note: current rminer version emphasizes the use of native fitting functions from their respective packages, since these functions contain several specific hyperparameters that can now be searched or set using the <code>search</code> or <code>...</code> arguments. For compatibility with previous rminer versions, older <code>model</code> options are kept.
</p>
</td></tr>
<tr><td><code id="fit_+3A_task">task</code></td>
<td>
<p>data mining task. Valid options are:
</p>

<ul>
<li> <p><code>prob</code> (or <code>p</code>) &ndash; classification with output probabilities (i.e. the sum of all outputs equals 1).
</p>
</li>
<li> <p><code>class</code> (or <code>c</code>) &ndash; classification with discrete outputs (<code><a href="base.html#topic+factor">factor</a></code>) 
</p>
</li>
<li> <p><code>reg</code> (or <code>r</code>) &ndash; regression (numeric output) 
</p>
</li>
<li> <p><code>default</code> tries to guess the best task (<code>prob</code> or <code>reg</code>) given the <code>model</code> and output variable type (if factor then <code>prob</code> else <code>reg</code>)
</p>
</li></ul>

</td></tr>
<tr><td><code id="fit_+3A_search">search</code></td>
<td>
<p>used to tune hyperparameter(s) of the model, such as: <code>kknn</code> &ndash; number of neighbors (k); 
<code>mlp</code> or <code>mlpe</code> &ndash; number of hidden nodes (<em>size</em>) or <em>decay</em>;
<code>ksvm</code> &ndash; gaussian kernel parameter (<em>sigma</em>);
<code>randomForest</code> &ndash; <code>mtry</code> parameter).<br /> 
This is a very flexible argument that can be used under several options: simpler use, complex tuning of an individual model or multiple models. The simpler use is kept for compatibility issues but it is advised to define this argument via the easier <code><a href="#topic+mparheuristic">mparheuristic</a></code> function.<br />
Valid options for a simpler <b>character type</b> <code>search</code> use:
</p>

<ul>
<li> <p><code>heuristic</code> &ndash; simple heuristic, one search parameter (e.g., <em>size</em>=inputs/2 for <code>mlp</code> or 
<em>size</em>=10 if classification and inputs/2&gt;10, <em>sigma</em> is set using <code>kpar="automatic"</code> and <code>kernel="rbfdot"</code> 
of <code><a href="kernlab.html#topic+ksvm">ksvm</a></code>). Important Note: instead of the &quot;heuristic&quot; options, it is advisable to use the explicit <code><a href="#topic+mparheuristic">mparheuristic</a></code> function
that is designed for a wider option of models (all &quot;heuristic&quot; options were kept due to compatibility issues and work 
only for: <code>kknn</code>; <code>mlp</code> or <code>mlpe</code>; <code>ksvm</code>, with <code>kernel="rbfdot"</code>; and <code>randomForest</code>). 
</p>
</li>
<li> <p><code>heuristic5</code> &ndash; heuristic with a 5 range grid-search (e.g., <code>seq(1,9,2)</code> for <code>kknn</code>, <code>seq(0,8,2)</code> for <code>mlp</code> or <code>mlpe</code>, 
<code>2^seq(-15,3,4)</code> for <code>ksvm</code>, <code>1:5</code> for <code>randomRorest</code>)
</p>
</li>
<li> <p><code>heuristic10</code> &ndash; heuristic with a 10 range grid-search (e.g., <code>seq(1,10,1)</code> for <code>kknn</code>, <code>seq(0,9,1)</code> for <code>mlp</code> or <code>mlpe</code>, 
<code>2^seq(-15,3,2)</code> for <code>ksvm</code>, <code>1:10</code> for <code>randomRorest</code>)
</p>
</li>
<li> <p><code>UD</code>, <code>UD1</code> or <code>UD2</code> &ndash; uniform design 2-Level with 13 (<code>UD</code> or <code>UD2</code>) or 21 (<code>UD1</code>) searches (only works for <code>ksvm</code> and <code>kernel="rbfdot"</code>).
</p>
</li></ul>
 
<p>Another simpler use of the <code>search</code> argument is a <b>numeric type</b>:
</p>

<ul>
<li><p> a-<code><a href="base.html#topic+vector">vector</a></code> &ndash; numeric vector with all hyperparameter values that will be searched within an internal grid-search (the number of searches is <code>length(search)</code> when <code>convex=0</code>)
</p>
</li></ul>

<p>A more complex but advised use of <code>search</code> is to use a <code><a href="base.html#topic+list">list</a></code>. Non expert users should create this list via the <code><a href="#topic+mparheuristic">mparheuristic</a></code> function, which is very easy to use. Nevertheless, the fields of the list for a single fit (individual model) are shown here:
</p>

<ul>
<li> <p><code>$smethod</code> &ndash; type of search method. Valid options are:
</p>

<ul>
<li> <p><code>none</code> &ndash; no search is executed, one single fit is performed.
</p>
</li>
<li> <p><code>matrix</code> &ndash; matrix search (tests only n searches, all search parameters are of size n).
</p>
</li>
<li> <p><code>grid</code> &ndash; normal grid search (tests all combinations of search parameters).
</p>
</li>
<li> <p><code>2L</code> - nested 2-Level grid search. First level range is set by <code>$search</code> and then the 2nd level performs a fine tuning, with <code>length($search)</code> searches around (original range/2) best value in first level (2nd level is only performed on numeric searches).
</p>
</li>
<li> <p><code>UD</code>, <code>UD1</code> or <code>UD2</code> &ndash; uniform design 2-Level with 13 (<code>UD</code> or <code>UD2</code>) or 21 (<code>UD1</code>) searches (note: only works for <code>model="ksvm"</code> and <code>kernel="rbfdot"</code>). Under this option,
<code>$search</code> should contain the first level ranges, such as <code>c(-15,3,-5,15)</code> for classification (<em>gamma</em> min and max, <em>C</em> min and max, 
after which a <code>2^</code> transform is applied) or <code>c(-8,0,-1,6,-8,-1)</code> for regression (last two values are <em>epsilon</em> min and max, after which a <code>2^</code> transform is applied).
</p>
</li></ul>

</li>
<li> <p><code>$search</code> &ndash; a-<code><a href="base.html#topic+list">list</a></code> with all hyperparameter values to be searched or character with previous described options 
(e.g., &quot;heuristic&quot;, &quot;heuristic5&quot;, &quot;UD&quot;). If a character, then <code>$smethod</code> equal to <code>"none"</code> or <code>"grid"</code> or <code>"UD"</code> is automatically assumed.
</p>
</li>
<li> <p><code>$convex</code> &ndash; number that defines how many searches are performed after a local minimum/maximum is found (if &gt;0, the search can be stopped without testing all grid-search values) 
</p>
</li>
<li> <p><code>$method</code> &ndash; type of internal (validation) estimation method used during the search (see <code>method</code> argument of <code><a href="#topic+mining">mining</a></code> for details)
</p>
</li>
<li> <p><code>$metric</code> &ndash; used to compute a metric value during internal estimation. Can be a single character such as <code>"SAD"</code> or a list with all the arguments used by the <code>mmetric</code> function except <code>y</code> and <code>x</code>, such as:<br />
<code>search$metric=list(metric="AUC",TC=3,D=0.7)</code>. See <code><a href="#topic+mmetric">mmetric</a></code> for more details.
</p>
</li></ul>

<p>A more sophisticated definition of <code>search</code> involves the tuning of several models (used by the <code>model=</code> <code>auto</code>, <code>AE</code>, <code>WE</code> or <code>SE</code>). Again, this sophisticated definition should be automatically set using the <code><a href="#topic+mparheuristic">mparheuristic</a></code> function. The list of fields for the multiple tuning mode are:
</p>

<ul>
<li> <p><code>$models</code> - a vector character with LM individual <code>model</code> values. This field can also include ensembles (<code>"AE"</code>, <code>"WE"</code>, <code>"SE"</code>) provided they appear at the end of this vector. They will work if more than one valid individual model is included. 
</p>
</li>
<li> <p><code>$ls</code> - a vector list with LM search values (for each individual model, the values are the same as in individual search <code>$search</code> field). 
</p>
</li>
<li> <p><code>$smethod</code> - must have the <code>auto</code> value.
</p>
</li>
<li> <p><code>$smethod</code> - must have the <code>auto</code> value.
</p>
</li>
<li> <p><code>$method</code> - internal (validation) estimation method (equal to the individual search <code>$method</code> field).
</p>
</li>
<li> <p><code>$metric</code> - internal (validation) estimation metric (equal to the individual search <code>$metric</code> field).
</p>
</li>
<li> <p><code>$convex</code> - equal to the individual search <code>$convex</code> field.
</p>
</li></ul>

<p>Note: the <code>mpar</code> argument only appears due to compatibility issues. If used, then the <code>mpar</code> values are automatically fed into search. However, a direct use of the 
<code>search</code> argument is advised instead of <code>mpar</code>, since <code>search</code> is more flexible and powerful.
</p>
</td></tr>
<tr><td><code id="fit_+3A_mpar">mpar</code></td>
<td>
<p>(important note: this argument only is kept in this version due to compatibility with previous rminer versions. Instead of <code>mpar</code>, you should use the more flexible and powerful <code>search</code> argument.)<br />
<br />
vector with extra default (fixed) model parameters (used for modeling, search and feature selection) with:	
</p>
 
<ul>
<li><p> c(<em>vmethod</em>,<em>vpar</em>,<code>metric</code>) &ndash; generic use of mpar (including most models);
</p>
</li>
<li><p> c(<em>C</em>,<em>epsilon</em>,<em>vmethod</em>,<em>vpar</em>,<code>metric</code>) &ndash; if <code>ksvm</code> and C and epsilon are explicitly set;
</p>
</li>
<li><p> c(<em>nr</em>,<em>maxit</em>,<em>vmethod</em>,<em>vpar</em>,<code>metric</code>) &ndash; if <code>mlp</code> or <code>mlpe</code> and nr and maxit are explicitly set; 
</p>
</li></ul>

<p><em>C</em> and <em>epsilon</em> are default values for <code>svm</code> (if any of these is <code>=NA</code> then heuristics are used to set the value).<br />
<em>nr</em> is the number of <code>mlp</code> runs or <code>mlpe</code> individual models, while <em>maxit</em> is the maximum number of epochs (if any of these is <code>=NA</code> then heuristics are used to set the value).<br />
For help on <em>vmethod</em> and <em>vpar</em> see <code><a href="#topic+mining">mining</a></code>.<br />
<code>metric</code> is the internal error function (e.g., used by search to select the best model), valid options are explained in <code><a href="#topic+mmetric">mmetric</a></code>. 
When <code>mpar=NULL</code> then default values are used. If there are <code>NA</code> values (e.g., <code>mpar=c(NA,NA)</code>) then default values are used.
</p>
</td></tr>
<tr><td><code id="fit_+3A_feature">feature</code></td>
<td>
<p>feature selection and sensitivity analysis control. Valid <code>fit</code> function options are:
</p>
 
<ul>
<li> <p><code>none</code> &ndash; no feature selection;
</p>
</li>
<li><p> a <em>fmethod</em> character value, such as <code>sabs</code> (see below);
</p>
</li>
<li><p> a-<code><a href="base.html#topic+vector">vector</a></code> &ndash; vector with c(<em>fmethod</em>,<em>deletions</em>,<em>Runs</em>,<em>vmethod</em>,<em>vpar</em>,<em>defaultsearch</em>)
</p>
</li>
<li><p> a-<code><a href="base.html#topic+vector">vector</a></code> &ndash; vector with c(<em>fmethod</em>,<em>deletions</em>,<em>Runs</em>,<em>vmethod</em>,<em>vpar</em>)
</p>
</li></ul>

<p><em>fmethod</em> sets the type. Valid options are: 
</p>
 
<ul>
<li> <p><code>sbs</code> &ndash; standard backward selection;
</p>
</li>
<li> <p><code>sabs</code> &ndash; sensitivity analysis backward selection (faster);
</p>
</li>
<li> <p><code>sabsv</code> &ndash; equal to <code>sabs</code> but uses variance for sensitivity importance measure;
</p>
</li>
<li> <p><code>sabsr</code> &ndash; equal to <code>sabs</code> but uses range for sensitivity importance measure;
</p>
</li>
<li> <p><code>sabsg</code> &ndash; equal to <code>sabs</code> (uses gradient for sensitivity importance measure);
</p>
</li></ul>

<p><em>deletions</em> is the maximum number of feature deletions (if -1 not used).<br />
<em>Runs</em> is the number of runs for each feature set evaluation (e.g., 1).<br />
For help on <em>vmethod</em> and <em>vpar</em> see <code><a href="#topic+mining">mining</a></code>.<br />
<em>defaultsearch</em> is one hyperparameter used during the feature selection search, after selecting the best feature set then <code>search</code> is used (faster). 
If not defined, then <code>search</code> is used during feature selection (may be slow).<br />
When feature is a vector then default values are used to fill missing values or <code>NA</code> values.
Note: feature selection capabilities are expected to be enhanced in next rminer versions.
</p>
</td></tr>
<tr><td><code id="fit_+3A_scale">scale</code></td>
<td>
<p>if data needs to be scaled (i.e. for <code>mlp</code> or <code>mlpe</code>). Valid options are:
</p>
 
<ul>
<li> <p><code>default</code> &ndash; uses scaling when needed (i.e. for <code>mlp</code> or <code>mlpe</code>)
</p>
</li>
<li> <p><code>none</code> &ndash; no scaling; 
</p>
</li>
<li> <p><code>inputs</code> &ndash; standardizes (0 mean, 1 st. deviation) input attributes; 
</p>
</li>
<li> <p><code>all</code> &ndash; standardizes (0 mean, 1 st. deviation) input and output attributes;
</p>
</li></ul>

<p>If needed, the <code>predict</code> function of rminer performs the inverse scaling.
</p>
</td></tr>
<tr><td><code id="fit_+3A_transform">transform</code></td>
<td>
<p>if the output data needs to be transformed (e.g., <code>log</code> transform). Valid options are:
</p>
 
<ul>
<li> <p><code>none</code> &ndash; no transform; 
</p>
</li>
<li> <p><code>log</code> &ndash; y=(log(y+1)) (the inverse function is applied in the <code>predict</code> function);
</p>
</li>
<li> <p><code>positive</code> &ndash; all predictions are positive (negative values are turned into zero);
</p>
</li>
<li> <p><code>logpositive</code> &ndash; both <code>log</code> and <code>logpositive</code>; 
</p>
</li></ul>

</td></tr>
<tr><td><code id="fit_+3A_created">created</code></td>
<td>
<p>time stamp for the model. By default, the system time is used. Else, you can specify another time.</p>
</td></tr>
<tr><td><code id="fit_+3A_fdebug">fdebug</code></td>
<td>
<p>if TRUE show some search details.</p>
</td></tr>
<tr><td><code id="fit_+3A_...">...</code></td>
<td>
<p> additional and specific parameters send to each fit function model (e.g., <code>dt</code>, <code>rpart</code>, <code>randomforest</code>, <code>kernlab</code>). A few examples:<br />
&ndash; the <code><a href="rpart.html#topic+rpart">rpart</a></code> function is used for decision trees, thus you can have:<br />
<code>control=rpart.control(cp=.05)</code> (see <code><a href="#topic+crossvaldata">crossvaldata</a></code> example).<br />
&ndash; the <code><a href="kernlab.html#topic+ksvm">ksvm</a></code> function is used for support vector machines, thus you can change the kernel type: <code>kernel="polydot"</code> (see examples below).<br />
Important note: if you use package functions and get an error, then try to explicitly define the package. For instance, you might
need to use <code>fit(</code><em>several-arguments</em><code>,control=Cubist::cubistControl())</code> instead of<br />
<code>fit(</code><em>several-arguments</em><code>,control=cubistControl())</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fits a classification or regression model given a data.frame (see [Cortez, 2010] for more details).
The <code>...</code> optional arguments should be used to fix values used by specific <code>model</code> functions (see examples).
Notes: <br />
- if there is an error in the fit, then a warning is issued (see example).<br /> 
- the new <code>search</code> argument is very flexible and allows a powerful design of supervised learning models.<br />
- the <code>search</code> correct use is very dependent on the R learning base functions. For example, if you are tuning <code>model="rpart"</code> then read
carefully the help of function <code><a href="rpart.html#topic+rpart">rpart</a></code>.<br />
- <code>mpar</code> argument is only kept due to compatibility issues and should be avoided; instead, use the more flexible <code>search</code>.<br />
<br />
Details about some models: 
</p>

<ul>
<li><p> Neural Network: <code>mlp</code> trains <em>nr</em> multilayer perceptrons (with <em>maxit</em> epochs, <em>size</em> hidden nodes 
and <em>decay</em> value according to the <code><a href="nnet.html#topic+nnet">nnet</a></code> function) and selects the best network according to minimum penalized error (<code>$value</code>). <code>mlpe</code> uses an ensemble
of <em>nr</em> networks and the final prediction is given by the average of all outputs. To tune <code>mlp</code> or <code>mlpe</code> you can use the <code>search</code> parameter, which performs a grid
search for <em>size</em> or <em>decay</em>.
</p>
</li>
<li><p> Support Vector Machine: <code>svm</code> adopts by default the gaussian (rbfdot) kernel. For classification tasks, you can use <code>search</code> to tune <em>sigma</em> (gaussian kernel parameter) and <em>C</em> (complexity parameter). For regression, the epsilon insensitive function is adopted and there is an additional hyperparameter <em>epsilon</em>.
</p>
</li>
<li><p> Other methods: Random Forest &ndash; if needed, you can tune several parameters, including the default <code>mtry</code> parameter adopted by <code>search</code> heuristics; k-nearest neighbor &ndash; <code>search</code> by default tunes <em>k</em>. The remaining models can also be tunned but a full definition of <code>search</code> is
required (e.g., with <code>$smethod</code>, <code>$search</code> and other fields); please check <code><a href="#topic+mparheuristic">mparheuristic</a></code> function for further tuning examples (e.g., <code>rpart</code>).
</p>
</li></ul>



<h3>Value</h3>

<p>Returns a model object. You can check all model elements with <code>str(M)</code>, where <code>M</code> is a model object. The slots are:
</p>
 
<ul>
<li> <p><code>@formula</code> &ndash; the <code>x</code>;
</p>
</li>
<li> <p><code>@model</code> &ndash; the <code>model</code>;
</p>
</li>
<li> <p><code>@task</code> &ndash; the <code>task</code>;
</p>
</li>
<li> <p><code>@mpar</code> &ndash; data.frame with the best model parameters (interpretation depends on <code>model</code>);
</p>
</li>
<li> <p><code>@attributes</code> &ndash; the attributes used by the model;
</p>
</li>
<li> <p><code>@scale</code> &ndash; the <code>scale</code>;
</p>
</li>
<li> <p><code>@transform</code> &ndash; the <code>transform</code>;
</p>
</li>
<li> <p><code>@created</code> &ndash; the date when the model was created;
</p>
</li>
<li> <p><code>@time</code> &ndash; computation effort to fit the model;
</p>
</li>
<li> <p><code>@object</code> &ndash; the R object model (e.g., <code>rpart</code>, <code>nnet</code>, ...);
</p>
</li>
<li> <p><code>@outindex</code> &ndash; the output index (of @attributes);
</p>
</li>
<li> <p><code>@levels</code> &ndash; if <code>task=="prob"||task=="class"</code> stores the output levels; 
</p>
</li>
<li> <p><code>@error</code> &ndash; similarly to <code>mining</code> this is the &quot;validation&quot; error for some <code>search</code> options; 
</p>
</li></ul>



<h3>Note</h3>

<p>See also <a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a> and <a href="http://www3.dsi.uminho.pt/pcortez/rminer.html">http://www3.dsi.uminho.pt/pcortez/rminer.html</a>
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="https://pcortez.dsi.uminho.pt">https://pcortez.dsi.uminho.pt</a>
</p>


<h3>References</h3>


<ul>
<li><p> To check for more details about rminer and for citation purposes:<br />
P. Cortez.<br />
Data Mining with Neural Networks and Support Vector Machines Using the R/rminer Tool.<br />
In P. Perner (Ed.), Advances in Data Mining - Applications and Theoretical Aspects 10th Industrial Conference on Data Mining (ICDM 2010), Lecture Notes in Artificial Intelligence 6171, pp. 572-583, Berlin, Germany, July, 2010. Springer. ISBN: 978-3-642-14399-1.<br />
@Springer: <a href="https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44">https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44</a><br />
<a href="http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf">http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf</a><br />
</p>
</li>
<li><p> This tutorial shows additional code examples:<br />
P. Cortez.<br />
A tutorial on using the rminer R package for data mining tasks.<br />
Teaching Report, Department of Information Systems, ALGORITMI Research Centre, Engineering School, University of Minho, Guimaraes, 
Portugal, July 2015.<br />
<a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a><br />
</p>
</li>
<li><p> For the grid search and other optimization methods:<br />
P. Cortez.<br />
Modern Optimization with R.<br />
Use R! series, Springer, 2nd edition, July 2021, ISBN 978-3-030-72818-2.<br />
<a href="https://link.springer.com/book/10.1007/978-3-030-72819-9">https://link.springer.com/book/10.1007/978-3-030-72819-9</a><br />
</p>
</li>
<li><p> The automl is inspired in this work:<br />
L. Ferreira, A. Pilastri, C. Martins, P. Santos, P. Cortez.<br />
An Automated and Distributed Machine Learning Framework for Telecommunications Risk Management.
In J. van den Herik et al. (Eds.), 
Proceedings of 12th International Conference on Agents and Artificial Intelligence &ndash; ICAART 2020, Volume 2, pp. 99-107,
Valletta, Malta, February, 2020, SCITEPRESS, ISBN 978-989-758-395-7.<br />
@INSTICC: <a href="http://hdl.handle.net/1822/66818">http://hdl.handle.net/1822/66818</a><br />
</p>
</li>
<li><p> For the sabs feature selection:<br />
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.<br />
Modeling wine preferences by data mining from physicochemical properties.<br />
In Decision Support Systems, Elsevier, 47(4):547-553, 2009.<br />
<a href="https://doi.org/10.1016/j.dss.2009.05.016">doi:10.1016/j.dss.2009.05.016</a><br />
</p>
</li>
<li><p> For the uniform design details:<br />
C.M. Huang, Y.J. Lee, D.K.J. Lin and S.Y. Huang.<br />
Model selection for support vector machines via uniform design,<br />
In Computational Statistics &amp; Data Analysis, 52(1):335-346, 2007.<br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+mparheuristic">mparheuristic</a></code>,<code><a href="#topic+mining">mining</a></code>, <code><a href="#topic+predict.fit">predict.fit</a></code>, <code><a href="#topic+mgraph">mgraph</a></code>, <code><a href="#topic+mmetric">mmetric</a></code>, <code><a href="#topic+savemining">savemining</a></code>, <code><a href="#topic+CasesSeries">CasesSeries</a></code>, <code><a href="#topic+lforecast">lforecast</a></code>,
<code><a href="#topic+holdout">holdout</a></code> and <code><a href="#topic+Importance">Importance</a></code>. Check all rminer functions using: <code>help(package=rminer)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### dontrun is used when the execution of the example requires some computational effort.

### simple regression (with a formula) example.
x1=rnorm(200,100,20); x2=rnorm(200,100,20)
y=0.7*sin(x1/(25*pi))+0.3*sin(x2/(25*pi))
M=fit(y~x1+x2,model="mlpe")
new1=rnorm(100,100,20); new2=rnorm(100,100,20)
ynew=0.7*sin(new1/(25*pi))+0.3*sin(new2/(25*pi))
P=predict(M,data.frame(x1=new1,x2=new2,y=rep(NA,100)))
print(mmetric(ynew,P,"MAE"))

### simple classification example.
## Not run: 
data(iris)
M=fit(Species~.,iris,model="rpart")
plot(M@object); text(M@object) # show model
P=predict(M,iris)
print(mmetric(iris$Species,P,"CONF"))
print(mmetric(iris$Species,P,"ALL"))
mgraph(iris$Species,P,graph="ROC",TC=2,main="versicolor ROC",
baseline=TRUE,leg="Versicolor",Grid=10)

M2=fit(Species~.,iris,model="ctree")
plot(M2@object) # show model
P2=predict(M2,iris)
print(mmetric(iris$Species,P2,"CONF"))

# ctree with different setup:
# (ctree_control is from the party package)
M3=fit(Species~.,iris,model="ctree",controls = party::ctree_control(testtype="MonteCarlo"))
plot(M3@object) # show model

## End(Not run)

### simple binary classification example with cv.glmnet and xgboost
## Not run: 
data(sa_ssin_2)
H=holdout(sa_ssin_2$y,ratio=2/3)
# cv.glmnet:
M=fit(y~.,sa_ssin_2[H$tr,],model="cv.glmnet",task="cla") # pure classes
P=predict(M,sa_ssin_2[H$ts,])
cat("1st prediction, class:",as.character(P[1]),"\n")
cat("Confusion matrix:\n")
print(mmetric(sa_ssin_2[H$ts,]$y,P,"CONF")$conf)

M2=fit(y~.,sa_ssin_2[H$tr,],model="cv.glmnet") # probabilities
P2=predict(M2,sa_ssin_2[H$ts,])
L=M2@levels
cat("1st prediction, prob:",L[1],"=",P2[1,1],",",L[2],"=",P2[1,2],"\n")
cat("Confusion matrix:\n")
print(mmetric(sa_ssin_2[H$ts,]$y,P2,"CONF")$conf)
cat("AUC of ROC curve:\n")
print(mmetric(sa_ssin_2[H$ts,]$y,P2,"AUC"))

M3=fit(y~.,sa_ssin_2[H$tr,],model="cv.glmnet",nfolds=3) # use 3 folds instead of 10
plot(M3@object) # show cv.glmnet object
P3=predict(M3,sa_ssin_2[H$ts,])

# xgboost:
M4=fit(y~.,sa_ssin_2[H$tr,],model="xgboost",verbose=1) # nrounds=2, show rounds:
P4=predict(M4,sa_ssin_2[H$ts,])
print(mmetric(sa_ssin_2[H$ts,]$y,P4,"AUC"))
M5=fit(y~.,sa_ssin_2[H$tr,],model="xgboost",nrounds=3,verbose=1) # nrounds=3, show rounds:
P5=predict(M5,sa_ssin_2[H$ts,])
print(mmetric(sa_ssin_2[H$ts,]$y,P5,"AUC"))

## End(Not run)

### classification example with discrete classes, probabilities and holdout
## Not run: 
data(iris)
H=holdout(iris$Species,ratio=2/3)
M=fit(Species~.,iris[H$tr,],model="ksvm",task="class")
M1=fit(Species~.,iris[H$tr,],model="lssvm") # default task="class" is assumed
M2=fit(Species~.,iris[H$tr,],model="ksvm",task="prob")
P=predict(M,iris[H$ts,]) # classes
P1=predict(M1,iris[H$ts,]) # classes
P2=predict(M2,iris[H$ts,]) # probabilities
print(mmetric(iris$Species[H$ts],P,"CONF"))
print(mmetric(iris$Species[H$ts],P1,"CONF"))
print(mmetric(iris$Species[H$ts],P2,"CONF"))
print(mmetric(iris$Species[H$ts],P,"CONF",TC=1))
print(mmetric(iris$Species[H$ts],P2,"CONF",TC=1))
print(mmetric(iris$Species[H$ts],P2,"AUC"))

### exploration of some rminer classification models:
models=c("lda","naiveBayes","kknn","randomForest","cv.glmnet","xgboost")
for(m in models)
 { cat("model:",m,"\n") 
   M=fit(Species~.,iris[H$tr,],model=m)
   P=predict(M,iris[H$ts,])
   print(mmetric(iris$Species[H$ts],P,"AUC")[[1]])
 }

## End(Not run)

### classification example with hyperparameter selection 
###    note: for regression, similar code can be used
### SVM 
## Not run: 
data(iris)
# large list of SVM configurations:
# SVM with kpar="automatic" sigma rbfdot kernel estimation and default C=1:
#  note: each execution can lead to different M@mpar due to sigest stochastic nature:
M=fit(Species~.,iris,model="ksvm")
print(M@mpar) # model hyperparameters/arguments
# same thing, explicit use of mparheuristic:
M=fit(Species~.,iris,model="ksvm",search=list(search=mparheuristic("ksvm")))
print(M@mpar) # model hyperparameters

# SVM with C=3, sigma=2^-7
M=fit(Species~.,iris,model="ksvm",C=3,kpar=list(sigma=2^-7))
print(M@mpar)
# SVM with different kernels:
M=fit(Species~.,iris,model="ksvm",kernel="polydot",kpar="automatic") 
print(M@mpar)
# fit already has a scale argument, thus the only way to fix scale of "tanhdot"
# is to use the special search argument with the "none" method:
s=list(smethod="none",search=list(scale=2,offset=2))
M=fit(Species~.,iris,model="ksvm",kernel="tanhdot",search=s) 
print(M@mpar)
# heuristic: 10 grid search values for sigma, rbfdot kernel (fdebug is used only for more verbose):
s=list(search=mparheuristic("ksvm",10)) # advised "heuristic10" usage
M=fit(Species~.,iris,model="ksvm",search=s,fdebug=TRUE)
print(M@mpar)
# same thing, uses older search="heuristic10"
M=fit(Species~.,iris,model="ksvm",search="heuristic10",fdebug=TRUE)
print(M@mpar)
# identical search under a different and explicit code:
s=list(search=2^seq(-15,3,2))
M=fit(Species~.,iris,model="ksvm",search=2^seq(-15,3,2),fdebug=TRUE)
print(M@mpar)

# uniform design "UD" for sigma and C, rbfdot kernel, two level of grid searches, 
# under exponential (2^x) search scale:
M=fit(Species~.,iris,model="ksvm",search="UD",fdebug=TRUE)
print(M@mpar)
M=fit(Species~.,iris,model="ksvm",search="UD1",fdebug=TRUE)
print(M@mpar)
# now the more powerful search argument is used for modeling SVM:
# grid 3 x 3 search:
s=list(smethod="grid",search=list(sigma=2^c(-15,-5,3),C=2^c(-5,0,15)),convex=0,
            metric="AUC",method=c("kfold",3,12345))
print(s)
M=fit(Species~.,iris,model="ksvm",search=s,fdebug=TRUE)
print(M@mpar)
# identical search with different argument smethod="matrix" 
s$smethod="matrix"
s$search=list(sigma=rep(2^c(-15,-5,3),times=3),C=rep(2^c(-5,0,15),each=3))
print(s)
M=fit(Species~.,iris,model="ksvm",search=s,fdebug=TRUE)
print(M@mpar)
# search for best kernel (only works for kpar="automatic"):
s=list(smethod="grid",search=list(kernel=c("rbfdot","laplacedot","polydot","vanilladot")),
       convex=0,metric="AUC",method=c("kfold",3,12345))
print(s)
M=fit(Species~.,iris,model="ksvm",search=s,fdebug=TRUE)
print(M@mpar)
# search for best parameters of "rbfdot" or "laplacedot" (which use same kpar):
s$search=list(kernel=c("rbfdot","laplacedot"),sigma=2^seq(-15,3,5))
print(s)
M=fit(Species~.,iris,model="ksvm",search=s,fdebug=TRUE)
print(M@mpar)

### randomForest
# search for mtry and ntree
s=list(smethod="grid",search=list(mtry=c(1,2,3),ntree=c(100,200,500)),
            convex=0,metric="AUC",method=c("kfold",3,12345))
print(s)
M=fit(Species~.,iris,model="randomForest",search=s,fdebug=TRUE)
print(M@mpar)

### rpart
# simpler way to tune cp in 0.01 to 0.9 (10 searches):
s=list(search=mparheuristic("rpart",n=10,lower=0.01,upper=0.9),method=c("kfold",3,12345))
M=fit(Species~.,iris,model="rpart",search=s,fdebug=TRUE)
print(M@mpar)

# same thing but with more lines of code
# note: this code can be adapted to tune other rpart parameters,
#       while mparheuristic only tunes cp
# a vector list needs to be used for the search$search parameter
lcp=vector("list",10) # 10 grid values for the complexity cp
names(lcp)=rep("cp",10) # same cp name 
scp=seq(0.01,0.9,length.out=10) # 10 values from 0.01 to 0.18
for(i in 1:10) lcp[[i]]=scp[i] # cycle needed due to [[]] notation
s=list(smethod="grid",search=list(control=lcp),
            convex=0,metric="AUC",method=c("kfold",3,12345))
M=fit(Species~.,iris,model="rpart",search=s,fdebug=TRUE)
print(M@mpar)

### ctree 
# simpler way to tune mincriterion in 0.1 to 0.98 (9 searches):
mint=c("kfold",3,123) # internal validation method
s=list(search=mparheuristic("ctree",n=8,lower=0.1,upper=0.99),method=mint)
M=fit(Species~.,iris,model="ctree",search=s,fdebug=TRUE)
print(M@mpar)
# same thing but with more lines of code
# note: this code can be adapted to tune other ctree parameters,
#       while mparheuristic only tunes mincriterion
# a vector list needs to be used for the search$search parameter
lmc=vector("list",9) # 9 grid values for the mincriterion
smc=seq(0.1,0.99,length.out=9)
for(i in 1:9) lmc[[i]]=party::ctree_control(mincriterion=smc[i]) 
s=list(smethod="grid",search=list(controls=lmc),method=mint,convex=0)
M=fit(Species~.,iris,model="ctree",search=s,fdebug=TRUE)
print(M@mpar)

### some MLP fitting examples:
# simplest use:
M=fit(Species~.,iris,model="mlpe")  
print(M@mpar)
# same thing, with explicit use of mparheuristic:
M=fit(Species~.,iris,model="mlpe",search=list(search=mparheuristic("mlpe")))
print(M@mpar) # hidden nodes and number of ensemble mlps
# setting some nnet parameters:
M=fit(Species~.,iris,model="mlpe",size=3,decay=0.1,maxit=100,rang=0.9) 
print(M@mpar) # mlpe hyperparameters
# MLP, 5 grid search fdebug is only used to put some verbose in the console:
s=list(search=mparheuristic("mlpe",n=5)) # 5 searches for size
print(s) # show search
M=fit(Species~.,iris,model="mlpe",search=s,fdebug=TRUE)
print(M@mpar)
# previous searches used a random holdout (seed=NULL), now a fixed seed (123) is used:
s=list(smethod="grid",search=mparheuristic("mlpe",n=5),convex=0,metric="AUC",
            method=c("holdout",2/3,123))
print(s)
M=fit(Species~.,iris,model="mlpe",search=s,fdebug=TRUE)
print(M@mpar)
# faster and greedy grid search:
s$convex=1;s$search=list(size=0:9)
print(s)
M=fit(Species~.,iris,model="mlpe",search=s,fdebug=TRUE)
print(M@mpar)
# 2 level grid with total of 5 searches 
#  note of caution: some "2L" ranges may lead to non integer (e.g., 1.3) values at
#  the 2nd level search. And some R functions crash if non integer values are used for
#  integer parameters.
s$smethod="2L";s$convex=0;s$search=list(size=c(4,8,12))
print(s)
M=fit(Species~.,iris,model="mlpe",search=s,fdebug=TRUE)
print(M@mpar)

# testing of all 17 rminer classification methods:
model=c("naive","ctree","cv.glmnet","rpart","kknn","ksvm","lssvm","mlp","mlpe",
 "randomForest","xgboost","bagging","boosting","lda","multinom","naiveBayes","qda")
inputs=ncol(iris)-1
ho=holdout(iris$Species,2/3,seed=123) # 2/3 for training and 1/3 for testing
Y=iris[ho$ts,ncol(iris)]
for(i in 1:length(model))
 {
  cat("i:",i,"model:",model[i],"\n")
  search=list(search=mparheuristic(model[i])) # rminer default values
  M=fit(Species~.,data=iris[ho$tr,],model=model[i],search=search,fdebug=TRUE)
  P=predict(M,iris[ho$ts,])
  cat("predicted ACC:",round(mmetric(Y,P,metric="ACC"),1),"\n")
 }


## End(Not run)

### example of an error (warning) generated using fit:
## Not run: 
data(iris)
# size needs to be a positive integer, thus 0.1 leads to an error:
M=fit(Species~.,iris,model="mlp",size=0.1)  
print(M@object)

## End(Not run)

### exploration of some rminer regression models:
## Not run: 
data(sa_ssin)
H=holdout(sa_ssin$y,ratio=2/3,seed=12345)
models=c("lm","mr","ctree","mars","cubist","cv.glmnet","xgboost","rvm")
for(m in models)
 { cat("model:",m,"\n") 
   M=fit(y~.,sa_ssin[H$tr,],model=m)
   P=predict(M,sa_ssin[H$ts,])
   print(mmetric(sa_ssin$y[H$ts],P,"MAE"))
 }

## End(Not run)

# testing of all 18 rminer regression methods:
## Not run: 
model=c("naive","ctree","cv.glmnet","rpart","kknn","ksvm","mlp","mlpe",
 "randomForest","xgboost","cubist","lm","mr","mars","pcr","plsr","cppls","rvm")
# note: in this example, default values are considered for the hyperparameters.
# better results can be achieved by tuning hyperparameters via improved usage
# of the search argument (via mparheuristic function or written code)
data(iris)
ir2=iris[,1:4] # predict iris "Petal.Width"
names(ir2)[ncol(ir2)]="y" # change output name
inputs=ncol(ir2)-1
ho=holdout(ir2$y,2/3,seed=123) # 2/3 for training and 1/3 for testing
Y=ir2[ho$ts,ncol(ir2)]
for(i in 1:length(model))
 {
  cat("i:",i,"model:",model[i],"\n")
  search=list(search=mparheuristic(model[i])) # rminer default values
  M=fit(y~.,data=ir2[ho$tr,],model=model[i],search=search,fdebug=TRUE)
  P=predict(M,ir2[ho$ts,])
  cat("predicted MAE:",round(mmetric(Y,P,metric="MAE"),1),"\n")
 }

## End(Not run)

### regression example with hyperparameter selection:
## Not run: 
data(sa_ssin)
# some SVM experiments:
# default SVM:
M=fit(y~.,data=sa_ssin,model="svm")
print(M@mpar)
# SVM with (Cherkassy and Ma, 2004) heuristics to set C and epsilon:
M=fit(y~.,data=sa_ssin,model="svm",C=NA,epsilon=NA)
print(M@mpar)
# SVM with Uniform Design set sigma, C and epsilon:
M=fit(y~.,data=sa_ssin,model="ksvm",search="UD",fdebug=TRUE)
print(M@mpar)

# sensitivity analysis feature selection
M=fit(y~.,data=sa_ssin,model="ksvm",search=list(search=mparheuristic("ksvm",n=5)),feature="sabs") 
print(M@mpar)
print(M@attributes) # selected attributes (1, 2 and 3 are the relevant inputs)

# example that shows how transform works:
M=fit(y~.,data=sa_ssin,model="mr") # linear regression
P=predict(M,data.frame(x1=-1000,x2=0,x3=0,x4=0,y=NA)) # P should be negative
print(P)
M=fit(y~.,data=sa_ssin,model="mr",transform="positive")
P=predict(M,data.frame(x1=-1000,x2=0,x3=0,x4=0,y=NA)) # P is not negative
print(P)

## End(Not run)

### pure classification example with a generic R (not rminer default) model ###
## Not run: 
### nnet is adopted here but virtually ANY fitting function/package could be used:

# since the default nnet prediction is to provide probabilities, there is
# a need to create this "wrapping" function:
predictprob=function(object,newdata)
{ predict(object,newdata,type="class") }
# list with a fit and predict function:
# nnet::nnet (package::function)
model=list(fit=nnet::nnet,predict=predictprob,name="nnet")
data(iris)
# note that size is not a fit parameter and it is sent directly to nnet:
M=fit(Species~.,iris,model=model,size=3,task="class") 
P=predict(M,iris)
print(P)

## End(Not run) 

### multiple models: automl and ensembles 
## Not run: 
data(iris)
d=iris
names(d)[ncol(d)]="y" # change output name
inputs=ncol(d)-1
metric="AUC"

# consult the help of mparheuristic for more automl and ensemble examples:
#
# automatic machine learining (automl) with 5 distinct models and "SE" ensemble.
# the single models are tuned with 10 internal hyperparameter searches, 
# except ksvm that uses 13 searches via "UD".
# fit performs an internal validation 
sm=mparheuristic(model="automl3",n=NA,task="prob", inputs= inputs )
method=c("kfold",3,123)
search=list(search=sm,smethod="auto",method=method,metric=metric,convex=0)
M=fit(y~.,data=d,model="auto",search=search,fdebug=TRUE)
P=predict(M,d)
# show leaderboard:
cat("&gt; leaderboard models:",M@mpar$LB$model,"\n")
cat("&gt;  validation values:",round(M@mpar$LB$eval,4),"\n")
cat("best model is:",M@model,"\n")
cat(metric,"=",round(mmetric(d$y,P,metric=metric),2),"\n")


# average ensemble of 5 distinct models
# the single models are tuned with 1 (heuristic) hyperparameter search 
sm2=mparheuristic(model="automl",n=NA,task="prob", inputs= inputs )
method=c("kfold",3,123)
search2=list(search=sm2,smethod="auto",method=method,metric=metric,convex=0)
M2=fit(y~.,data=d,model="AE",search=search2,fdebug=TRUE)
P2=predict(M,d)
cat("best model is:",M2@model,"\n")
cat(metric,"=",round(mmetric(d$y,P2,metric=metric),2),"\n")

# example with an invalid model exclusion: 
# in this case, randomForest produces an error and warning
# thus it is excluded from the leaderboard
sm=mparheuristic(model="automl3",n=NA,task="prob", inputs= inputs )
method=c("holdout",2/3,123)
search=list(search=sm,smethod="auto",method=method,metric=metric,convex=0)
d2=d
#
d2[,2]=as.factor(1:150) # force randomForest error
M=fit(y~.,data=d2,model="auto",search=search,fdebug=TRUE)
P=predict(M,d2)
# show leaderboard:
cat("&gt; leaderboard models:",M@mpar$LB$model,"\n")
cat("&gt;  validation values:",round(M@mpar$LB$eval,4),"\n")
cat("best model is:",M@model,"\n")
cat(metric,"=",round(mmetric(d$y,P,metric=metric),2),"\n")


## End(Not run)

</code></pre>

<hr>
<h2 id='holdout'>
Computes indexes for holdout data split into training and test sets.
</h2><span id='topic+holdout'></span>

<h3>Description</h3>

<p>Computes indexes for holdout data split into training and test sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>holdout(y, ratio = 2/3, internalsplit = FALSE, mode = "stratified", iter = 1, 
               seed = NULL, window=10, increment=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="holdout_+3A_y">y</code></td>
<td>
<p>desired target: numeric vector; or factor &ndash; then a stratified holdout is applied (i.e. the proportions of the classes are the same for each set).</p>
</td></tr>
<tr><td><code id="holdout_+3A_ratio">ratio</code></td>
<td>
<p>split ratio (in percentage &ndash; sets the training set size; or in total number of examples &ndash; sets the test set size).</p>
</td></tr>
<tr><td><code id="holdout_+3A_internalsplit">internalsplit</code></td>
<td>
<p>if <code>TRUE</code> then the training data is further split into training and validation sets. The same <code>ratio</code> parameter is used for the internal split.</p>
</td></tr>
<tr><td><code id="holdout_+3A_mode">mode</code></td>
<td>
<p>sampling mode. Options are:
</p>

<ul>
<li> <p><code>stratified</code> &ndash; stratified randomized holdout if <code>y</code> is a factor; else it behaves as standard randomized holdout;
</p>
</li>
<li> <p><code>random</code> &ndash; standard randomized holdout;
</p>
</li>
<li> <p><code>order</code> &ndash; static mode, where the first examples are used for training and the later ones for testing (useful for time series data);
</p>
</li>
<li> <p><code>rolling</code> &ndash; rolling window, also known as sliding window (e.g. useful for stock market prediction), similar to <code>order</code> except that <code>window</code> is the window size, <code>iter</code> is the rolling iteration and <code>increment</code> is the number of samples slided at each iteration. In each iteration, the training set size is fixed to <code>window</code>, while the test set size is equal to <code>ratio</code> except for the last iteration
(where it may be smaller).
</p>
</li>
<li> <p><code>incremental</code> &ndash; incremental retraining mode, also known as growing windows, similar to <code>order</code> except that <code>window</code> is the initial window size, <code>iter</code> is the incremental iteration and <code>increment</code> is the number of samples added at each iteration. In each iteration, the training set size grows (+increment), while the test set size is equal to <code>ratio</code> except
for the last iteration (where it may be smaller).
</p>
</li></ul>

</td></tr>
<tr><td><code id="holdout_+3A_iter">iter</code></td>
<td>
<p>iteration of the incremental retraining mode (only used when <code>mode="rolling"</code> or <code>"incremental"</code>, typically <code>iter</code> is set within a cycle, see the example below).</p>
</td></tr>
<tr><td><code id="holdout_+3A_seed">seed</code></td>
<td>
<p>if <code>NULL</code> then no seed is used and the current R randomness is assumed; else a fixed seed is adopted to generate local random sample sequences, returning always the same result for the same seed (local means that it does not affect the state of other random number generations called after this function, see example).</p>
</td></tr>
<tr><td><code id="holdout_+3A_window">window</code></td>
<td>
<p>training window size (if <code>mode="rolling"</code>) or initial training window size (if <code>mode="incremental"</code>).</p>
</td></tr>
<tr><td><code id="holdout_+3A_increment">increment</code></td>
<td>
<p>number of samples added to the training window at each iteration (if <code>mode="incremental"</code> or <code>mode="rolling"</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes indexes for holdout data split into training and test sets.
</p>


<h3>Value</h3>

<p>A list with the components:
</p>

<ul>
<li><p> $tr &ndash; numeric vector with the training examples indexes;
</p>
</li>
<li><p> $ts &ndash; numeric vector with the test examples indexes;
</p>
</li>
<li><p> $itr &ndash; numeric vector with the internal training examples indexes;
</p>
</li>
<li><p> $val &ndash; numeric vector with the internal validation examples indexes;
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>

<p>See <code><a href="#topic+fit">fit</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code>, <code><a href="#topic+predict.fit">predict.fit</a></code>, <code><a href="#topic+mining">mining</a></code>, <code><a href="#topic+mgraph">mgraph</a></code>, <code><a href="#topic+mmetric">mmetric</a></code>, <code><a href="#topic+savemining">savemining</a></code>, <code><a href="#topic+Importance">Importance</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### simple examples:
# preserves order, last two elements go into test set
H=holdout(1:10,ratio=2,internal=TRUE,mode="order")
print(H)
# no seed or NULL returns different splits:
H=holdout(1:10,ratio=2/3,mode="random")
print(H)
H=holdout(1:10,ratio=2/3,mode="random",seed=NULL)
print(H)
# same seed returns identical split:
H=holdout(1:10,ratio=2/3,mode="random",seed=12345)
print(H)
H=holdout(1:10,ratio=2/3,mode="random",seed=12345)
print(H)

### classification example
## Not run: 
data(iris)
# random stratified holdout
H=holdout(iris$Species,ratio=2/3,mode="stratified") 
print(table(iris[H$tr,]$Species))
print(table(iris[H$ts,]$Species))
M=fit(Species~.,iris[H$tr,],model="rpart") # training data only
P=predict(M,iris[H$ts,]) # test data
print(mmetric(iris$Species[H$ts],P,"CONF"))

## End(Not run)

### regression example with incremental and rolling window holdout:
## Not run: 
ts=c(1,4,7,2,5,8,3,6,9,4,7,10,5,8,11,6,9)
d=CasesSeries(ts,c(1,2,3))
print(d) # with 14 examples
# incremental holdout example (growing window)
for(b in 1:4) # iterations
  {
   H=holdout(d$y,ratio=4,mode="incremental",iter=b,window=5,increment=2)
   M=fit(y~.,d[H$tr,],model="mlpe",search=2)
   P=predict(M,d[H$ts,])
   cat("batch :",b,"TR from:",H$tr[1],"to:",H$tr[length(H$tr)],"size:",length(H$tr),
       "TS from:",H$ts[1],"to:",H$ts[length(H$ts)],"size:",length(H$ts),
       "mae:",mmetric(d$y[H$ts],P,"MAE"),"\n")
  }
# rolling holdout example (sliding window)
for(b in 1:4) # iterations
  {
   H=holdout(d$y,ratio=4,mode="rolling",iter=b,window=5,increment=2)
   M=fit(y~.,d[H$tr,],model="mlpe",search=2)
   P=predict(M,d[H$ts,])
   cat("batch :",b,"TR from:",H$tr[1],"to:",H$tr[length(H$tr)],"size:",length(H$tr),
       "TS from:",H$ts[1],"to:",H$ts[length(H$ts)],"size:",length(H$ts),
       "mae:",mmetric(d$y[H$ts],P,"MAE"),"\n")
  }

## End(Not run)

### local seed simple example
## Not run: 
# seed is defined, same sequence for N1 and N2:
# s2 generation sequence is not affected by the holdout call
set.seed(1); s1=sample(1:10,3)
set.seed(1);
N1=holdout(1:10,seed=123) # local seed
N2=holdout(1:10,seed=123) # local seed
print(N1$tr)
print(N2$tr)
s2=sample(1:10,3)
cat("s1:",s1,"\n") 
cat("s2:",s2,"\n") # s2 is equal to s1

## End(Not run)

</code></pre>

<hr>
<h2 id='Importance'>
Measure input importance (including sensitivity analysis) given a supervised data mining model.
</h2><span id='topic+Importance'></span>

<h3>Description</h3>

<p>Measure input importance (including sensitivity analysis) given a supervised data mining model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Importance(M, data, RealL = 7, method = "1D-SA", measure = "AAD", 
           sampling = "regular", baseline = "mean", responses = TRUE, 
           outindex = NULL, task = "default", PRED = NULL, 
           interactions = NULL, Aggregation = -1, LRandom = -1,
           MRandom = "discrete", Lfactor = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Importance_+3A_m">M</code></td>
<td>
<p>fitted model, typically is the object returned by <code><a href="#topic+fit">fit</a></code>. 
Can also be any fitted model (i.e. not from rminer), provided that the predict function PRED is defined (see examples for details).</p>
</td></tr>
<tr><td><code id="Importance_+3A_data">data</code></td>
<td>
<p>training data (the same data.frame that was used to fit the model, currently only used to add data histogram to VEC curve).</p>
</td></tr>
<tr><td><code id="Importance_+3A_reall">RealL</code></td>
<td>
<p>the number of sensitivity analysis levels (e.g. 7). Note: you need to use <code>RealL</code>&gt;=2.</p>
</td></tr>
<tr><td><code id="Importance_+3A_method">method</code></td>
<td>
<p>input importance method. Options are:
</p>

<ul>
<li><p> 1D-SA &ndash; 1 dimensional sensitivity analysis, very fast, sets interactions to NULL.
</p>
</li>
<li><p> sens or SA &ndash; sensitivity analysis. There are some extra variants:
sensa &ndash; equal to <code>sens</code> but also sets <code>measure="AAD"</code>; sensv &ndash; sets <code>measure="variance"</code>; sensg &ndash; sets <code>measure="gradient"</code>; sensr &ndash; sets <code>measure="range"</code>. if interactions is not null, then GSA is assumed, else 1D-SA is assumed.
</p>
</li>
<li><p> DSA &ndash; Data-based SA (good option if input interactions need to be detected). 
</p>
</li>
<li><p> MSA &ndash; Monte-Carlo SA.
</p>
</li>
<li><p> CSA &ndash; Cluster-based SA.
</p>
</li>
<li><p> GSA &ndash; Global SA (very slow method, particularly if the number of inputs is large, should be avoided).
</p>
</li>
<li><p> randomForest &ndash; uses method of Leo Breiman (type=1), only makes sense when M is a randomRorest.
</p>
</li></ul>

</td></tr>
<tr><td><code id="Importance_+3A_measure">measure</code></td>
<td>
<p>sensitivity analysis measure (used to measure input importance). Options are:
</p>

<ul>
<li><p> AAD &ndash; average absolute deviation from the median.
</p>
</li>
<li><p> gradient &ndash; average absolute gradient (y_i+1-y_i) of the responses.
</p>
</li>
<li><p> variance &ndash; variance of the responses. 
</p>
</li>
<li><p> range &ndash; maximum - minimum of the responses. 
</p>
</li></ul>

</td></tr>
<tr><td><code id="Importance_+3A_sampling">sampling</code></td>
<td>
<p>for numeric inputs, the sampling scan function. Options are:
</p>

<ul>
<li><p>  regular &ndash; regular sequence (uniform distribution), do not change this value, kept here only due to compatibility issues.
</p>
</li></ul>

</td></tr>
<tr><td><code id="Importance_+3A_baseline">baseline</code></td>
<td>
<p>baseline vector used during the sensitivity analysis. Options are:
</p>

<ul>
<li><p> mean &ndash; uses a vector with the mean values of each attribute from <code>data</code>.
</p>
</li>
<li><p> median &ndash; uses a vector with the median values of each attribute from <code>data</code>.
</p>
</li>
<li><p> a data.frame with the baseline example (should have the same attribute names as <code>data</code>). 
</p>
</li></ul>

</td></tr>
<tr><td><code id="Importance_+3A_responses">responses</code></td>
<td>
<p>if <code>TRUE</code> then all sensitivity analysis responses are stored and returned.</p>
</td></tr>
<tr><td><code id="Importance_+3A_outindex">outindex</code></td>
<td>
<p>the output index (column) of <code>data</code> if <code>M</code> is not a model object (returned by fit).</p>
</td></tr>
<tr><td><code id="Importance_+3A_task">task</code></td>
<td>
<p>the <code>task</code> as defined in <code><a href="#topic+fit">fit</a></code> if <code>M</code> is not a model object (returned by fit).</p>
</td></tr>
<tr><td><code id="Importance_+3A_pred">PRED</code></td>
<td>
<p>the prediction function of <code>M</code>, if <code>M</code> is not a model object (returned by fit). 
Note: this function should behave like the rminer <code><a href="#topic+predict-methods">predict-methods</a></code>, i.e. return a numeric vector in case of regression;
a matrix of examples (rows) vs probabilities (columns) (<code>task="prob"</code>) or a factor (<code>task="class"</code>) in case of classification.
</p>
</td></tr>
<tr><td><code id="Importance_+3A_interactions">interactions</code></td>
<td>
<p>numeric vector with the attributes (columns) used by Ith-D sensitivity analysis (2-D or higher, &quot;GSA&quot; method):
</p>

<ul>
<li><p> if <code>NULL</code> then only a 1-D sensitivity analysis is performed.
</p>
</li>
<li><p> if <code>length(interactions)==1</code> then a &quot;special&quot; 2-D sensitivity analysis is performed using the index of interactions versus
all remaining inputs. Note: the $sresponses[[interactions]] will be empty (in <code><a href="#topic+vecplot">vecplot</a></code> do not use <code>xval</code> <code>=interactions</code>).
</p>
</li>
<li><p> if <code>length(interactions)&gt;1</code> then a full Ith-D sensitivity analysis is performed, where I=length(interactions).
Note: Computational effort can highly increase if I is too large, i.e. O(RealL^I). Also, you need to preprocess the 
returned list (e.g. using <code>avg_imp</code>) to use the <code><a href="#topic+vecplot">vecplot</a></code> function (see the examples).
</p>
</li></ul>

</td></tr>
<tr><td><code id="Importance_+3A_aggregation">Aggregation</code></td>
<td>
<p>numeric value that sets the number of multi-metric aggregation function (used only for &quot;DSA&quot;, &quot;&quot;). Options are:
</p>

<ul>
<li><p> -1 &ndash; the default value that should work in most cases (if regression, sets Aggregation=3, else if classification then sets Aggregation=1).
</p>
</li>
<li><p> 1 &ndash; value that should work for classification (only use the average of all sensitivity values).
</p>
</li>
<li><p> 3 &ndash; value that should work for regression (use 3 metrics, the minimum, average and maximum of all sensitivity values).
</p>
</li></ul>

</td></tr>
<tr><td><code id="Importance_+3A_lrandom">LRandom</code></td>
<td>
<p>number of samples used by DSA and MSA methods. The default value is -1, which means: use a number equal to training set size. If a different value is used (1&lt;= value &lt;= number of training samples), then LRandom samples are randomly selected.</p>
</td></tr>
<tr><td><code id="Importance_+3A_mrandom">MRandom</code></td>
<td>
<p>sampling type used by MSA: &quot;discrete&quot; (default discrete uniform distribution) or &quot;continuous&quot; (from continuous uniform distribution).</p>
</td></tr>
<tr><td><code id="Importance_+3A_lfactor">Lfactor</code></td>
<td>
<p>sets the maximum number of sensitivity levels for discrete inputs. if FALSE then a maximum of up to RealL levels are used (most frequent ones), else (TRUE) then all levels of the input are used in the SA analysis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides several algorithms for measuring input importance of supervised data mining models and the average effect of a given input (or pair of inputs) in the model.
A particular emphasis is given on sensitivity analysis (SA), which is a simple method that measures the effects on the output of a given model
when the inputs are varied through their range of values. Check the references for more details.
</p>


<h3>Value</h3>

<p>A <code>list</code> with the components:
</p>

<ul>
<li><p> $value &ndash; numeric vector with the computed sensitivity analysis measure for each attribute.
</p>
</li>
<li><p> $imp &ndash; numeric vector with the relative importance for each attribute (only makes sense for 1-D analysis).
</p>
</li>
<li><p> $sresponses &ndash; vector list as described in the Value documentation of <code><a href="#topic+mining">mining</a></code>.
</p>
</li>
<li><p> $data &ndash; if DSA or MSA, store the used data samples, needed for visualizations made by vecplot.
</p>
</li>
<li><p> $method &ndash; SA method
</p>
</li>
<li><p> $measure &ndash; SA measure
</p>
</li>
<li><p> $agg &ndash; Aggregation value 
</p>
</li>
<li><p> $nclasses &ndash; if task=&quot;prob&quot; or &quot;class&quot;, the number of output classes, else nclasses=1 
</p>
</li>
<li><p> $inputs &ndash; indexes of the input attributes
</p>
</li>
<li><p> $Llevels &ndash; sensitivity levels used for each attribute (NA means output attribute)
</p>
</li>
<li><p> $interactions &ndash; which attributes were interacted when method=GSA. 
</p>
</li></ul>



<h3>Note</h3>

<p>See also <a href="http://www3.dsi.uminho.pt/pcortez/rminer.html">http://www3.dsi.uminho.pt/pcortez/rminer.html</a>
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>


<ul>
<li><p> To cite the Importance function, sensitivity analysis methods or synthetic datasets, please use:<br />
P. Cortez and M.J. Embrechts.<br />
Using Sensitivity Analysis and Visualization Techniques to Open Black Box Data Mining Models.<br />
In Information Sciences, Elsevier, 225:1-17, March 2013.<br />
<a href="https://doi.org/10.1016/j.ins.2012.10.039">doi:10.1016/j.ins.2012.10.039</a><br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+vecplot">vecplot</a></code>, <code><a href="#topic+fit">fit</a></code>, <code><a href="#topic+mining">mining</a></code>, <code><a href="#topic+mgraph">mgraph</a></code>, <code><a href="#topic+mmetric">mmetric</a></code>, <code><a href="#topic+savemining">savemining</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
### dontrun is used when the execution of the example requires some computational effort.

### 1st example, regression, 1-D sensitivity analysis
## Not run: 
data(sa_ssin) # x1 should account for 55
M=fit(y~.,sa_ssin,model="ksvm")
I=Importance(M,sa_ssin,method="1D-SA") # 1-D SA, AAD
print(round(I$imp,digits=2))

L=list(runs=1,sen=t(I$imp),sresponses=I$sresponses)
mgraph(L,graph="IMP",leg=names(sa_ssin),col="gray",Grid=10)
mgraph(L,graph="VEC",xval=1,Grid=10,data=sa_ssin,
   main="VEC curve for x1 influence on y") # or:
vecplot(I,xval=1,Grid=10,data=sa_ssin,datacol="gray",
   main="VEC curve for x1 influence on y") # same graph
vecplot(I,xval=c(1,2,3),pch=c(1,2,3),Grid=10,
leg=list(pos="bottomright",leg=c("x1","x2","x3"))) # all x1, x2 and x3 VEC curves

## End(Not run)

### 2nd example, regression, DSA sensitivity analysis:
## Not run: 
I2=Importance(M,sa_ssin,method="DSA")
print(I2)
# influence of x1 and x2 over y
vecplot(I2,graph="VEC",xval=1) # VEC curve
vecplot(I2,graph="VECB",xval=1) # VEC curve with boxplots
vecplot(I2,graph="VEC3",xval=c(1,2)) # VEC surface
vecplot(I2,graph="VECC",xval=c(1,2)) # VEC contour

## End(Not run)

### 3th example, classification (pure class labels, task="cla"), DSA:
## Not run: 
data(sa_int2_3c) # pair (x1,x2) is more relevant than x3, all x1,x2,x3 affect y, 
                 # x4 has a null effect.
M2=fit(y~.,sa_int2_3c,model="mlpe",task="class")
I4=Importance(M2,sa_int2_3c,method="DSA")
# VEC curve (should present a kind of "saw" shape curve) for class B (TC=2):
vecplot(I4,graph="VEC",xval=2,cex=1.2,TC=2,
 main="VEC curve for x2 influence on y (class B)",xlab="x2")
# same VEC curve but with boxplots:
vecplot(I4,graph="VECB",xval=2,cex=1.2,TC=2,
 main="VEC curve with box plots for x2 influence on y (class B)",xlab="x2")

## End(Not run)

### 4th example, regression, DSA and GSA:
## Not run: 
data(sa_psin)
# same model from Table 1 of the reference:
M3=fit(y~.,sa_psin,model="ksvm",search=2^-2,C=2^6.87,epsilon=2^-8)
# in this case: Aggregation should be -1 (default), 1 (class) or 3 (reg), see ref. paper.
I5=Importance(M3,sa_psin,method="DSA",Aggregation=3)
print("Input importances:")
print(round(I5$imp,digits=2)) # INS 2013 similar results

# 2D analysis (check reference for more details), RealL=L=7:
# need to aggregate results into a matrix of SA measure by using the agg_matrix_imp function.
# important notes: 
# - agg_matrix_imp only works for the methods "DSA", "MSA" and "GSA".
# - reliable agg_matrix_imp results for "DSA" or "MSA" only for a 
#   a large LRandom value (e.g., LRandom=1000) or when LRandom=-1 (all training samples)
cm=agg_matrix_imp(I5)
print("show Table 8 DSA results (from the reference):")
print(round(cm$m1,digits=2))
print(round(cm$m2,digits=2))
# internal rminer function:
# show most relevant (darker) input pairs, in this case (x1,x2) &gt; (x1,x3) &gt; (x2,x3)
# to build a nice plot, a fixed threshold=c(0.05,0.05) is used. note that
# in the paper and for real data, we use threshold=0.1, 
# which means threshold=rep(max(cm$m1,cm$m2)*threshold,2)
fcm=cmatrixplot(cm,threshold=c(0.05,0.05)) 
# 2D analysis using pair AT=c(x1,x2') (check reference for more details), RealL=7:
# nice 3D VEC surface plot:
vecplot(I5,xval=c(1,2),graph="VEC3",xlab="x1",ylab="x2",zoom=1.1,
 main="VEC surface of (x1,x2') influence on y")
# same influence but know shown using VEC contour:
par(mar=c(4.0,4.0,1.0,0.3)) # change the graph window space size
vecplot(I5,xval=c(1,2),graph="VECC",xlab="x1",ylab="x2",
 main="VEC surface of (x1,x2') influence on y")
# slower GSA:
I6=Importance(M3,sa_psin,method="GSA",interactions=1:4)
print("Input importances:")
print(round(I6$imp,digits=2)) # INS 2013 similar results
cm2=agg_matrix_imp(I6)
# compare cm2 with cm1, almost identical:
print(round(cm2$m1,digits=2))
print(round(cm2$m2,digits=2))
fcm2=cmatrixplot(cm2,threshold=0.1) 

## End(Not run)

### 5th example, classification, 1D_SA, DSA, MSA and GSA:
## Not run: 
data(sa_ssin_n2p)
# same model from Table 1 of the reference:
M4=fit(y~.,sa_ssin_n2p,model="ksvm",kpar=list(sigma=2^-8.25),C=2^10)

I7=Importance(M4,sa_ssin_n2p,method="1D-SA")
print("1D-SA Input importances:")
print(round(I7$imp,digits=2)) # INS 2013 similar results (Table 6)

I8=Importance(M4,sa_ssin_n2p,method="GSA",interactions=1:4)
print("GSA Input importances:")
print(round(I8$imp,digits=2)) # INS 2013 similar results (Table 6)

I9=Importance(M4,sa_ssin_n2p,method="DSA",LRandom=1000)
print("DSA Ns=1000 Input importances:")
print(round(I9$imp,digits=2)) # INS 2013 similar results (Table 6)

I10=Importance(M4,sa_ssin_n2p,method="DSA",LRandom=10)
print("DSA Ns=10 Input importances:")
print(round(I10$imp,digits=2)) # INS 2013 similar results (Table 6)

I11=Importance(M4,sa_ssin_n2p,method="MSA",LRandom=10)
print("MSA Ns=10 Input importances:")
print(round(I11$imp,digits=2)) # INS 2013 similar results (Table 6)

# 2D analysis:
cm3=agg_matrix_imp(I8)
fcm3=cmatrixplot(cm3,threshold=c(0.05,0.05)) 
cm4=agg_matrix_imp(I9)
fcm4=cmatrixplot(cm4,threshold=c(0.05,0.05)) 

## End(Not run)

### If you want to use Importance over your own model (different than rminer ones):
# 1st example, regression, uses the theoretical sin1reg function: x1=70% and x2=30%
data(sin1reg)
mypred=function(M,data)
{ return (M[1]*sin(pi*data[,1]/M[3])+M[2]*sin(pi*data[,2]/M[3])) }
M=c(0.7,0.3,2000)
# 4 is the column index of y
I=Importance(M,sin1reg,method="sens",measure="AAD",PRED=mypred,outindex=4) 
print(I$imp) # x1=72.3% and x2=27.7%
L=list(runs=1,sen=t(I$imp),sresponses=I$sresponses)
mgraph(L,graph="IMP",leg=names(sin1reg),col="gray",Grid=10)
mgraph(L,graph="VEC",xval=1,Grid=10) # equal to:
par(mar=c(2.0,2.0,1.0,0.3)) # change the graph window space size
vecplot(I,graph="VEC",xval=1,Grid=10,main="VEC curve for x1 influence on y:")

### 2nd example, 3-class classification for iris and lda model:
## Not run: 
data(iris)
library(MASS)
predlda=function(M,data) # the PRED function
{ return (predict(M,data)$posterior) }
LDA=lda(Species ~ .,iris, prior = c(1,1,1)/3)
# 4 is the column index of Species
I=Importance(LDA,iris,method="1D-SA",PRED=predlda,outindex=4)
vecplot(I,graph="VEC",xval=1,Grid=10,TC=1,
main="1-D VEC for Sepal.Lenght (x-axis) influence in setosa (prob.)")

## End(Not run)

### 3rd example, binary classification for setosa iris and lda model:
## Not run: 
data(iris)
library(MASS)
iris2=iris;iris2$Species=factor(iris$Species=="setosa")
predlda2=function(M,data) # the PRED function
{ return (predict(M,data)$class) }
LDA2=lda(Species ~ .,iris2)
I=Importance(LDA2,iris2,method="1D-SA",PRED=predlda2,outindex=4)
vecplot(I,graph="VEC",xval=1,
main="1-D VEC for Sepal.Lenght (x-axis) influence in setosa (class)",Grid=10)

## End(Not run)

### Example with discrete inputs 
## Not run: 
data(iris)
ir1=iris
ir1[,1]=cut(ir1[,1],breaks=4)
ir1[,2]=cut(ir1[,2],breaks=4)
M=fit(Species~.,ir1,model="mlpe")
I=Importance(M,ir1,method="DSA")
# discrete example:
vecplot(I,graph="VEC",xval=1,TC=1,main="class: setosa (discrete x1)",data=ir1)
# continuous example:
vecplot(I,graph="VEC",xval=3,TC=1,main="class: setosa (cont. x1)",data=ir1)

## End(Not run)

</code></pre>

<hr>
<h2 id='imputation'>Missing data imputation (e.g. substitution by value or hotdeck method).</h2><span id='topic+imputation'></span>

<h3>Description</h3>

<p>Missing data imputation (e.g. substitution by value or hotdeck method).  </p>


<h3>Usage</h3>

<pre><code class='language-R'>imputation(imethod = "value", D, Attribute = NULL, Missing = NA, Value = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="imputation_+3A_imethod">imethod</code></td>
<td>
<p>imputation method type:
</p>

<ul>
<li> <p><code>value</code> &ndash; substitutes missing data by <code>Value</code> (with single element or several elements); 
</p>
</li>
<li> <p><code>hotdeck</code> &ndash; searches first the most similar example (i.e. using a k-nearest neighbor method &ndash; <code>knn</code>) in the dataset 
and replaces the missing data by the value found in such example;  
</p>
</li></ul>

</td></tr>
<tr><td><code id="imputation_+3A_d">D</code></td>
<td>
<p>dataset with missing data (data.frame)</p>
</td></tr>
<tr><td><code id="imputation_+3A_attribute">Attribute</code></td>
<td>
<p>if <code>NULL</code> then all attributes (data columns) with missing data are replaced. Else, <code>Attribute</code> is the attribute number (numeric) or name (character).</p>
</td></tr>
<tr><td><code id="imputation_+3A_missing">Missing</code></td>
<td>
<p>missing data symbol</p>
</td></tr>
<tr><td><code id="imputation_+3A_value">Value</code></td>
<td>
<p>the substitution value (if <code>imethod=value</code>) or number of neighbors (<em>k</em> of <code>knn</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Check the references.</p>


<h3>Value</h3>

<p>A data.frame without missing data.
</p>


<h3>Note</h3>

<p>See also <a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a> and <a href="http://www3.dsi.uminho.pt/pcortez/rminer.html">http://www3.dsi.uminho.pt/pcortez/rminer.html</a>
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>


<ul>
<li><p> M. Brown and J. Kros.<br />
Data mining and the impact of missing data.<br />
In Industrial Management &amp; Data Systems, 103(8):611-621, 2003.<br />
</p>
</li>
<li><p> This tutorial shows additional code examples:<br />
P. Cortez.<br />
A tutorial on using the rminer R package for data mining tasks.<br />
Teaching Report, Department of Information Systems, ALGORITMI Research Centre, Engineering School, University of Minho, Guimaraes, 
Portugal, July 2015.<br />
<a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a><br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code> and <code><a href="#topic+delevels">delevels</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d=matrix(ncol=5,nrow=5)
d[1,]=c(5,4,3,2,1)
d[2,]=c(4,3,4,3,4)
d[3,]=c(1,1,1,1,1)
d[4,]=c(4,NA,3,4,4)
d[5,]=c(5,NA,NA,2,1)
d=data.frame(d); d[,3]=factor(d[,3])
print(d)
print(imputation("value",d,3,Value="3"))
print(imputation("value",d,2,Value=median(na.omit(d[,2]))))
print(imputation("value",d,2,Value=c(1,2)))
print(imputation("hotdeck",d,"X2",Value=1))
print(imputation("hotdeck",d,Value=1))

## Not run: 
# hotdeck 1-nearest neighbor substitution on a real dataset:
require(kknn)
d=read.table(
   file="http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data",
   sep=",",na.strings="?",stringsAsFactors=TRUE)
print(summary(d))
d2=imputation("hotdeck",d,Value=1)
print(summary(d2))
par(mfrow=c(2,1))
hist(d$V26)
hist(d2$V26)
par(mfrow=c(1,1)) # reset mfrow

## End(Not run)

</code></pre>

<hr>
<h2 id='lforecast'>
Compute long term forecasts.
</h2><span id='topic+lforecast'></span>

<h3>Description</h3>

<p>Performs multi-step forecasts by iteratively using 1-ahead predictions as inputs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lforecast(M, data, start, horizon)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lforecast_+3A_m">M</code></td>
<td>
<p>fitted model, the object returned by <code><a href="#topic+fit">fit</a></code>.</p>
</td></tr>
<tr><td><code id="lforecast_+3A_data">data</code></td>
<td>
<p>training data, typically built using <code><a href="#topic+CasesSeries">CasesSeries</a></code>.</p>
</td></tr>
<tr><td><code id="lforecast_+3A_start">start</code></td>
<td>
<p>starting period (when out-of-samples start).</p>
</td></tr>
<tr><td><code id="lforecast_+3A_horizon">horizon</code></td>
<td>
<p>number of multi-step predictions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Check the reference for details.
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the multi-step predictions.
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>


<ul>
<li><p> This tutorial shows additional code examples:<br />
P. Cortez.<br />
A tutorial on using the rminer R package for data mining tasks.<br />
Teaching Report, Department of Information Systems, ALGORITMI Research Centre, Engineering School, University of Minho, Guimaraes, 
Portugal, July 2015.<br />
<a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a><br />
</p>
</li>
<li><p> To check for more details:<br />
P. Cortez.<br />
Sensitivity Analysis for Time Lag Selection to Forecast Seasonal Time Series using Neural Networks and Support Vector Machines.<br />
In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN 2010), pp. 3694-3701, Barcelona, Spain, July, 2010. 
IEEE Computer Society, ISBN: 978-1-4244-6917-8 (DVD edition).<br />
<a href="https://doi.org/10.1109/IJCNN.2010.5596890">doi:10.1109/IJCNN.2010.5596890</a><br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code>, <code><a href="#topic+CasesSeries">CasesSeries</a></code>, <code><a href="#topic+predict.fit">predict.fit</a></code>, <code><a href="#topic+mgraph">mgraph</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ts=c(1,4,7,2,5,8,3,6,9,4,7,10,5,8,11,6,9)
d=CasesSeries(ts,c(1,2,3))
M=fit(y~.,d[1:7,],model="mlpe",search=2)
P1=predict(M,d[8:14,]) # single-step predictions
P2=lforecast(M,d,8,7) # multi-step predictions, horizon=7
print(mmetric(d$y[8:14],P1,"MAE"))
print(mmetric(d$y[8:14],P2,"MAE"))
L=vector("list",2); pred=vector("list",1);test=vector("list",1)
pred[[1]]=P1; test[[1]]=d$y[8:14]; L[[1]]=list(pred=pred,test=test,runs=1)
pred[[1]]=P2; test[[1]]=d$y[8:14]; L[[2]]=list(pred=pred,test=test,runs=1)
mgraph(L,graph="REG",Grid=10,leg=c("y","P1","P2"),col=c("black","cyan","blue"))
mgraph(L,graph="RSC",Grid=10,leg=c("P1","P2"),col=c("cyan","blue"))
</code></pre>

<hr>
<h2 id='mgraph'>
Mining graph function
</h2><span id='topic+mgraph'></span>

<h3>Description</h3>

<p>Plots a graph given a <code><a href="#topic+mining">mining</a></code> list, list of several mining lists or given the pair y - target and x - predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mgraph(y, x = NULL, graph, leg = NULL, xval = -1, PDF = "", PTS = -1, 
       size = c(5, 5), sort = TRUE, ranges = NULL, data = NULL,
       digits = NULL, TC = -1, intbar = TRUE, lty = 1, col = "black",
       main = "", metric = "MAE", baseline = FALSE, Grid = 0, 
       axis = NULL, cex = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mgraph_+3A_y">y</code></td>
<td>
<p>if there are predictions (<code>!is.null(x)</code>), <code>y</code> should be a numeric vector or factor with the target desired responses (or output values).<br />
Else, <code>y</code> should be a list returned by the  <code><a href="#topic+mining">mining</a></code> function or a vector list with several mining lists.
</p>
</td></tr>
<tr><td><code id="mgraph_+3A_x">x</code></td>
<td>
<p>the predictions (should be a numeric vector if <code>task="reg"</code>, matrix if <code>task="prob"</code> or factor if <code>task="class"</code> (use if <code>y</code> is not a list).</p>
</td></tr>
<tr><td><code id="mgraph_+3A_graph">graph</code></td>
<td>
<p>type of graph. Options are:
</p>

<ul>
<li> <p><code>ROC</code> &ndash; ROC curve (classification);
</p>
</li>
<li> <p><code>LIFT</code> &ndash; LIFT accumulative curve (classification);
</p>
</li>
<li> <p><code>IMP</code> &ndash; relative input importance barplot;
</p>
</li>
<li> <p><code>REC</code> &ndash; REC curve (regression);
</p>
</li>
<li> <p><code>VEC</code> &ndash; variable effect curve;
</p>
</li>
<li> <p><code>RSC</code> &ndash; regression scatter plot;
</p>
</li>
<li> <p><code>REP</code> &ndash; regression error plot;
</p>
</li>
<li> <p><code>REG</code> &ndash; regression plot;
</p>
</li>
<li> <p><code>DLC</code> &ndash; distance line comparison (for comparing errors in one line);
</p>
</li></ul>

</td></tr>
<tr><td><code id="mgraph_+3A_leg">leg</code></td>
<td>
<p>legend of graph:
</p>

<ul>
<li><p> if <code>NULL</code> &ndash; not used;
</p>
</li>
<li><p> if -1 and <code>graph="ROC" or "LIFT"</code>  &ndash; the target class name is used;
</p>
</li>
<li><p> if -1 and <code>graph="REG"</code>  &ndash; <code>leg=c("Target","Predictions")</code>;
</p>
</li>
<li><p> if -1 and <code>graph="RSC"</code>  &ndash; <code>leg=c("Predictions")</code>;
</p>
</li>
<li><p> if vector with &quot;character&quot; type (text) &ndash; the text of the legend;
</p>
</li>
<li><p> if is list &ndash; <code>$leg</code> = vector with the text of the legend and <code>$pos</code> is the position of the legend (e.g. &quot;top&quot; or c(4,5));
</p>
</li></ul>

</td></tr>
<tr><td><code id="mgraph_+3A_xval">xval</code></td>
<td>
<p>auxiliary value, used by some graphs:
</p>

<ul>
<li> <p><code>VEC</code> &ndash; if -1 means perform several 1-D sensitivity analysis VEC curves, one for each attribute, if &gt;0 means the attribute index (e.g. 1).
</p>
</li>
<li> <p><code>ROC</code> or <code>LIFT</code> or <code>REC</code> &ndash; if -1 then <code>xval=1</code>. For these graphs, <code>xval</code> is the maximum x-axis value.
</p>
</li>
<li> <p><code>IMP</code> &ndash; <code>xval</code> is the x-axis value for the legend of the attributes.
</p>
</li>
<li> <p><code>REG</code> &ndash; <code>xval</code> is the set of plotted examples (e.g. 1:5), if -1 then all examples are used.
</p>
</li>
<li> <p><code>DLC</code> &ndash; <code>xval</code> is the <code>val</code> of the <code><a href="#topic+mmetric">mmetric</a></code> function.
</p>
</li></ul>

</td></tr>
<tr><td><code id="mgraph_+3A_pdf">PDF</code></td>
<td>
<p>if <code>""</code> then the graph is plotted on the screen, else the graph is saved into a pdf file with the name set in this argument.</p>
</td></tr>
<tr><td><code id="mgraph_+3A_pts">PTS</code></td>
<td>
<p>number of points in each line plot. If -1 then <code>PTS=11</code> (for <code>ROC</code>, <code>REC</code> or <code>LIFT</code>) or <code>PTS=6</code> (<code>VEC</code>).</p>
</td></tr>
<tr><td><code id="mgraph_+3A_size">size</code></td>
<td>
<p>size of the graph, c(width,height), in inches.</p>
</td></tr>
<tr><td><code id="mgraph_+3A_sort">sort</code></td>
<td>
<p>if TRUE then sorts the data (works only for some graphs, e.g. <code>VEC</code>, <code>IMP</code>, <code>REP</code>).</p>
</td></tr>
<tr><td><code id="mgraph_+3A_ranges">ranges</code></td>
<td>
<p>matrix with the attribute minimum and maximum ranges (only used by <code>VEC</code>). </p>
</td></tr>
<tr><td><code id="mgraph_+3A_data">data</code></td>
<td>
<p>the training data, for plotting histograms and getting the minimum and maximum attribute ranges if not defined in ranges (only used by <code>VEC</code>).</p>
</td></tr>
<tr><td><code id="mgraph_+3A_digits">digits</code></td>
<td>
<p>the number of digits for the axis, can also be defined as c(x-axis digits,y-axis digits) (only used by <code>VEC</code>).</p>
</td></tr>
<tr><td><code id="mgraph_+3A_tc">TC</code></td>
<td>
<p>target class (for multi-class classification class) from 1 to <em>Nc</em>, where <em>Nc</em> is the number of classes. If multi-class and TC==-1 then TC is set to the index of the last class.</p>
</td></tr>
<tr><td><code id="mgraph_+3A_intbar">intbar</code></td>
<td>
<p>if 95% confidence interval bars (according to t-student distribution) should be plotted as whiskers.</p>
</td></tr>
<tr><td><code id="mgraph_+3A_lty">lty</code></td>
<td>
<p>the same <code>lty</code> argument of the <code><a href="graphics.html#topic+par">par</a></code> function.</p>
</td></tr>
<tr><td><code id="mgraph_+3A_col">col</code></td>
<td>
<p>color, as defined in the <code><a href="graphics.html#topic+par">par</a></code> function.</p>
</td></tr>
<tr><td><code id="mgraph_+3A_main">main</code></td>
<td>
<p>the title of the graph, as defined in the <code><a href="base.html#topic+plot">plot</a></code> function.</p>
</td></tr>
<tr><td><code id="mgraph_+3A_metric">metric</code></td>
<td>
<p>the error metric, as defined in <code><a href="#topic+mmetric">mmetric</a></code> (used by <code>DLC</code>).</p>
</td></tr>
<tr><td><code id="mgraph_+3A_baseline">baseline</code></td>
<td>
<p>if the baseline should be plotted (used by <code>ROC</code> and <code>LIFT</code>).</p>
</td></tr>
<tr><td><code id="mgraph_+3A_grid">Grid</code></td>
<td>
<p>if &gt;1 then there are GRID light gray squared grid lines in the plot.</p>
</td></tr>
<tr><td><code id="mgraph_+3A_axis">axis</code></td>
<td>
<p>Currently only used by <code>IMP</code>: numeric vector with the axis numbers (1 &ndash; bottom, 3 &ndash; top). If <code>NULL</code> then <code>axis=c(1,3)</code>.</p>
</td></tr>
<tr><td><code id="mgraph_+3A_cex">cex</code></td>
<td>
<p>label font size</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots a graph given a <code><a href="#topic+mining">mining</a></code> list, list of several mining lists or given the pair y - target and x - predictions.
</p>


<h3>Value</h3>

<p>A graph (in screen or pdf file).
</p>


<h3>Note</h3>

<p>See also <a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a> and <a href="http://www3.dsi.uminho.pt/pcortez/rminer.html">http://www3.dsi.uminho.pt/pcortez/rminer.html</a></p>


<h3>Author(s)</h3>

<p> Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a> </p>


<h3>References</h3>


<ul>
<li><p> To check for more details about rminer and for citation purposes:<br />
P. Cortez.<br />
Data Mining with Neural Networks and Support Vector Machines Using the R/rminer Tool.<br />
In P. Perner (Ed.), Advances in Data Mining - Applications and Theoretical Aspects 10th Industrial Conference on Data Mining (ICDM 2010), Lecture Notes in Artificial Intelligence 6171, pp. 572-583, Berlin, Germany, July, 2010. Springer. ISBN: 978-3-642-14399-1.<br />
@Springer: <a href="https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44">https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44</a><br />
<a href="http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf">http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf</a><br />
</p>
</li>
<li><p> This tutorial shows additional code examples:<br />
P. Cortez.<br />
A tutorial on using the rminer R package for data mining tasks.<br />
Teaching Report, Department of Information Systems, ALGORITMI Research Centre, Engineering School, University of Minho, Guimaraes, 
Portugal, July 2015.<br />
<a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a><br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code>, <code><a href="#topic+predict.fit">predict.fit</a></code>, <code><a href="#topic+mining">mining</a></code>, <code><a href="#topic+mmetric">mmetric</a></code>, <code><a href="#topic+savemining">savemining</a></code> and <code><a href="#topic+Importance">Importance</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### regression
y=c(1,5,10,11,7,3,2,1);x=rnorm(length(y),0,1.0)+y
mgraph(y,x,graph="RSC",Grid=10,col=c("blue"))
mgraph(y,x,graph="REG",Grid=10,lty=1,col=c("black","blue"),
       leg=list(pos="topleft",leg=c("target","predictions")))
mgraph(y,x,graph="REP",Grid=10)
mgraph(y,x,graph="REP",Grid=10,sort=FALSE)
x2=rnorm(length(y),0,1.2)+y;x3=rnorm(length(y),0,1.4)+y;
L=vector("list",3); pred=vector("list",1); test=vector("list",1);
pred[[1]]=y; test[[1]]=x; L[[1]]=list(pred=pred,test=test,runs=1)
test[[1]]=x2; L[[2]]=list(pred=pred,test=test,runs=1)
test[[1]]=x3; L[[3]]=list(pred=pred,test=test,runs=1)
# distance line comparison graph:
mgraph(L,graph="DLC",metric="MAE",leg=c("x1","x2","x3"),main="MAE errors")

# new REC multi-curve single graph with NAREC (normalized Area of REC) values
# for maximum tolerance of val=0.5 (other val values can be used)
e1=mmetric(y,x,metric="NAREC",val=5)
e2=mmetric(y,x2,metric="NAREC",val=5)
e3=mmetric(y,x3,metric="NAREC",val=5)
l1=paste("x1, NAREC=",round(e1,digits=2))
l2=paste("x2, NAREC=",round(e2,digits=2))
l3=paste("x3, NAREC=",round(e3,digits=2))
mgraph(L,graph="REC",leg=list(pos="bottom",leg=c(l1,l2,l3)),main="REC curves")

### regression example with mining
## Not run: 
data(sin1reg)
M1=mining(y~.,sin1reg[,c(1,2,4)],model="mr",Runs=5)
M2=mining(y~.,sin1reg[,c(1,2,4)],model="mlpe",nr=3,maxit=50,size=4,Runs=5,feature="simp")
L=vector("list",2); L[[1]]=M2; L[[2]]=M1
mgraph(L,graph="REC",xval=0.1,leg=c("mlpe","mr"),main="REC curve")
mgraph(L,graph="DLC",metric="TOLERANCE",xval=0.01,
       leg=c("mlpe","mr"),main="DLC: TOLERANCE plot")
mgraph(M2,graph="IMP",xval=0.01,leg=c("x1","x2"),
       main="sin1reg Input importance",axis=1)
mgraph(M2,graph="VEC",xval=1,main="sin1reg 1-D VEC curve for x1")
mgraph(M2,graph="VEC",xval=1,
       main="sin1reg 1-D VEC curve and histogram for x1",data=sin1reg)

## End(Not run)

### classification example
## Not run: 
data(iris)
M1=mining(Species~.,iris,model="rpart",Runs=5) # decision tree (DT)
M2=mining(Species~.,iris,model="ksvm",Runs=5) # support vector machine (SVM)
L=vector("list",2); L[[1]]=M2; L[[2]]=M1
mgraph(M1,graph="ROC",TC=3,leg=-1,baseline=TRUE,Grid=10,main="ROC")
mgraph(M1,graph="ROC",TC=3,leg=-1,baseline=TRUE,Grid=10,main="ROC",intbar=FALSE)
mgraph(L,graph="ROC",TC=3,leg=c("SVM","DT"),baseline=TRUE,Grid=10,
       main="ROC for virginica")
mgraph(L,graph="LIFT",TC=3,leg=list(pos=c(0.4,0.2),leg=c("SVM","DT")),
       baseline=TRUE,Grid=10,main="LIFT for virginica")

## End(Not run)

</code></pre>

<hr>
<h2 id='mining'>
Powerful function that trains and tests a particular fit model under several runs and a given validation method
</h2><span id='topic+mining'></span><span id='topic+centralpar'></span>

<h3>Description</h3>

<p>Powerful function that trains and tests a particular fit model under several runs and a given validation method.
Since there can be a huge number of models, the fitted models are not stored. Yet, several useful statistics (e.g. predictions) are returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mining(x, data = NULL, Runs = 1, method = NULL, model = "default", 
       task = "default", search = "heuristic", mpar = NULL,
       feature="none", scale = "default", transform = "none", 
       debug = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mining_+3A_x">x</code></td>
<td>
<p>a symbolic description (formula) of the model to be fit. If <code>x</code> contains the data, then <code>data=NULL</code> (similar to x in <code><a href="kernlab.html#topic+ksvm">ksvm</a></code>, kernlab package).</p>
</td></tr>
<tr><td><code id="mining_+3A_data">data</code></td>
<td>
<p>an optional data frame (columns denote attributes, rows show examples) containing the training data, when using a formula.</p>
</td></tr>
<tr><td><code id="mining_+3A_runs">Runs</code></td>
<td>
<p>number of runs used (e.g. 1, 5, 10, 20, 30)</p>
</td></tr>
<tr><td><code id="mining_+3A_method">method</code></td>
<td>
<p>a vector with c(<em>vmethod</em>,<em>vpar</em>,<em>seed</em>) or c(<em>vmethod</em>,<em>vpar</em>,<em>window</em>,<em>increment</em>), 
where <em>vmethod</em> is:
</p>

<ul>
<li> <p><code>all</code> &ndash; all <em>NROW</em> examples are used as both training and test sets (no <em>vpar</em> or <em>seed</em> is needed).
</p>
</li>
<li> <p><code>holdout</code> &ndash; standard holdout method. If <em>vpar</em>&lt;1 then <em>NROW</em>*vpar random samples are used for training and the remaining rows are used for testing. Else, then <em>NROW</em>*vpar random samples are used for testing and the remaining are used for training. For classification tasks (<code>prob</code> or <code>class</code>) a stratified sampling is assumed (equal to <code>mode="stratified"</code> in <code><a href="#topic+holdout">holdout</a></code>).
</p>
</li>
<li> <p><code>holdoutrandom</code> &ndash; similar to <code>holdout</code> except that assumes always a random sampling (not stratified).
</p>
</li>
<li> <p><code>holdoutorder</code> &ndash; similar to <code>holdout</code> except that instead of a random sampling, the first rows (until the split) are used for training and the remaining ones for testing (equal to <code>mode="order"</code> in <code><a href="#topic+holdout">holdout</a></code>).
</p>
</li>
<li> <p><code>holdoutinc</code> &ndash; incremental holdout retraining (e.g. used for stock market data). Here, <em>vpar</em> is the test size,
<em>window</em> is the initial window size and <em>increment</em> is the number of samples added at each iteration. 
Note: argument <code>Runs</code> is automatically set when this option is used. See also <code><a href="#topic+holdout">holdout</a></code>.
</p>
</li>
<li> <p><code>holdoutrol</code> &ndash; rolling holdout retraining (e.g. used for stock market data). Here, <em>vpar</em> is the test size,
<em>window</em> is the window size and <em>increment</em> is the number of samples added at each iteration. 
Note: argument <code>Runs</code> is automatically set when this option is used. See also <code><a href="#topic+holdout">holdout</a></code>.
</p>
</li>
<li> <p><code>kfold</code> &ndash; K-fold cross-validation method, where <em>vpar</em> is the number of folds. For classification tasks (<code>prob</code> or <code>class</code>) a stratified split is assumed (equal to <code>mode="stratified"</code> in <code><a href="#topic+crossvaldata">crossvaldata</a></code>).
</p>
</li>
<li> <p><code>kfoldrandom</code> &ndash; similar to <code>kfold</code> except that assumes always a random sampling (not stratified).
</p>
</li>
<li> <p><code>kfoldorder</code> &ndash; similar to <code>kfold</code> except that instead of a random sampling, the order of the rows is used to build the folds.
</p>
</li></ul>

<p><em>vpar</em> &ndash; number used by <em>vmethod</em> (optional, if not defined 2/3 for <code>holdout</code> and 10 for <code>kfold</code> is assumed);<br />
and <em>seed</em> (optional, if not defined then <code>NA</code> is assumed) is:
</p>

<ul>
<li> <p><code>NA</code> &ndash; random seed is adopted (default R method for generating random numbers);
</p>
</li>
<li><p> a vector of size <code>Runs</code> with fixed seed numbers for each Run;
</p>
</li>
<li><p> a number &ndash; <code>set.seed</code>(<em>number</em>) is applied then a vector of seeds (of size Runs) is generated.
</p>
</li></ul>

</td></tr>
<tr><td><code id="mining_+3A_model">model</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="mining_+3A_task">task</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="mining_+3A_search">search</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="mining_+3A_mpar">mpar</code></td>
<td>
<p>Only kept for compatibility with previous <code>rminer</code> versions, as you should use <code>search</code> instead of <code>mpar</code>. See <code><a href="#topic+fit">fit</a></code> for details.<br />
</p>
</td></tr>
<tr><td><code id="mining_+3A_feature">feature</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for more details about <code>feature="none"</code>, <code>"sabs"</code> or <code>"sbs"</code> options.<br />
For the <code>mining</code> function, additional options are <code>feature=</code><em>fmethod</em>, where <em>fmethod</em> can be one of:
</p>
 
<ul>
<li> <p><code>sens</code> or <code>sensg</code> &ndash; compute the 1-D sensitivity analysis input importances (<code>$sen</code>), gradient measure. 
</p>
</li>
<li> <p><code>sensv</code> &ndash; compute the 1-D sensitivity analysis input importances (<code>$sen</code>), variance measure.
</p>
</li>
<li> <p><code>sensr</code> &ndash; compute the 1-D sensitivity analysis input importances (<code>$sen</code>), range measure.
</p>
</li>
<li> <p><code>simp</code>, <code>simpg</code> or <code>s</code> &ndash; equal to <code>sensg</code> but also computes the 1-D sensitivity responses (<code>$sresponses</code>, 
useful for <code>graph="VEC"</code>).
</p>
</li>
<li> <p><code>simpv</code> &ndash; equal to <code>sensv</code> but also computes the 1-D sensitivity responses (useful for <code>graph="VEC"</code>).
</p>
</li>
<li> <p><code>simpr</code> &ndash; equal to <code>sensr</code> but also computes the 1-D sensitivity responses (useful for <code>graph="VEC"</code>).
</p>
</li></ul>

</td></tr>
<tr><td><code id="mining_+3A_scale">scale</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="mining_+3A_transform">transform</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
<tr><td><code id="mining_+3A_debug">debug</code></td>
<td>
<p>If TRUE shows some information about each run.</p>
</td></tr>
<tr><td><code id="mining_+3A_...">...</code></td>
<td>
<p>See <code><a href="#topic+fit">fit</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Powerful function that trains and tests a particular fit model under several runs and a given validation method
(see [Cortez, 2010] for more details).<br />
Several <code>Runs</code> are performed. In each run, the same validation method is adopted (e.g. <code>holdout</code>) and
several relevant statistics are stored. Note: this function can require some computational effort, specially if
a large dataset and/or a high number of <code>Runs</code> is adopted.
</p>


<h3>Value</h3>

<p>A <code>list</code> with the components:
</p>

<ul>
<li><p> $object &ndash; fitted object values of the last run (used by multiple model fitting: &quot;auto&quot; mode). For &quot;holdout&quot;, it is equal to a <code>fit</code> object, while for &quot;kfold&quot; it is a list.
</p>
</li>
<li><p> $time &ndash; vector with time elapsed for each run.
</p>
</li>
<li><p> $test &ndash; vector list, where each element contains the test (target) results for each run.
</p>
</li>
<li><p> $pred &ndash; vector list, where each element contains the predicted results for each test set and each run.
</p>
</li>
<li><p> $error &ndash; vector with a (validation) measure (often it is a error value) according to <code>search$metric</code> for each run (valid options are explained in <code><a href="#topic+mmetric">mmetric</a></code>).
</p>
</li>
<li><p> $mpar &ndash; vector list, where each element contains the fit model mpar parameters (for each run).
</p>
</li>
<li><p> $model &ndash; the <code>model</code>.
</p>
</li>
<li><p> $task &ndash; the <code>task</code>.
</p>
</li>
<li><p> $method &ndash; the external validation <code>method</code>.
</p>
</li>
<li><p> $sen &ndash; a matrix with the 1-D sensitivity analysis input importances. The number of rows is <code>Runs</code> times <em>vpar</em>, if <code>kfold</code>, else is <code>Runs</code>.
</p>
</li>
<li><p> $sresponses &ndash; a vector list with a size equal to the number of attributes (useful for <code>graph="VEC"</code>). 
Each element contains a list with the 1-D sensitivity analysis input responses
(<code>n</code> &ndash; name of the attribute; <code>l</code> &ndash; number of levels; <code>x</code> &ndash; attribute values; <code>y</code> &ndash; 1-D sensitivity responses.<br />
Important note: sresponses (and &quot;VEC&quot; graphs) are only available if <code>feature="sabs"</code> or <code>"simp"</code> related (see <code>feature</code>).
</p>
</li>
<li><p> $runs &ndash; the <code>Runs</code>.
</p>
</li>
<li><p> $attributes &ndash; vector list with all attributes (features) selected in each run (and fold if <code>kfold</code>) if a feature selection algorithm is used.
</p>
</li>
<li><p> $feature &ndash; the <code>feature</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>See also <a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a> and <a href="http://www3.dsi.uminho.pt/pcortez/rminer.html">http://www3.dsi.uminho.pt/pcortez/rminer.html</a>
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>


<ul>
<li><p> To check for more details about rminer and for citation purposes:<br />
P. Cortez.<br />
Data Mining with Neural Networks and Support Vector Machines Using the R/rminer Tool.<br />
In P. Perner (Ed.), Advances in Data Mining - Applications and Theoretical Aspects 10th Industrial Conference on Data Mining (ICDM 2010), Lecture Notes in Artificial Intelligence 6171, pp. 572-583, Berlin, Germany, July, 2010. Springer. ISBN: 978-3-642-14399-1.<br />
@Springer: <a href="https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44">https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44</a><br />
<a href="http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf">http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf</a><br />
</p>
</li>
<li><p> This tutorial shows additional code examples:<br />
P. Cortez.<br />
A tutorial on using the rminer R package for data mining tasks.<br />
Teaching Report, Department of Information Systems, ALGORITMI Research Centre, Engineering School, University of Minho, Guimaraes, 
Portugal, July 2015.<br />
<a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a><br />
</p>
</li>
<li><p> For the grid search and other optimization methods:<br />
P. Cortez.<br />
Modern Optimization with R.<br />
Use R! series, Springer, 2nd edition, July 2021, ISBN 978-3-030-72818-2.<br />
<a href="https://link.springer.com/book/10.1007/978-3-030-72819-9">https://link.springer.com/book/10.1007/978-3-030-72819-9</a><br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code>, <code><a href="#topic+predict.fit">predict.fit</a></code>, <code><a href="#topic+mparheuristic">mparheuristic</a></code>, <code><a href="#topic+mgraph">mgraph</a></code>, <code><a href="#topic+mmetric">mmetric</a></code>, <code><a href="#topic+savemining">savemining</a></code>, <code><a href="#topic+holdout">holdout</a></code> and <code><a href="#topic+Importance">Importance</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### dontrun is used when the execution of the example requires some computational effort.

### simple regression example
set.seed(123); x1=rnorm(200,100,20); x2=rnorm(200,100,20)
y=0.7*sin(x1/(25*pi))+0.3*sin(x2/(25*pi))
# mining with an ensemble of neural networks, each fixed with size=2 hidden nodes
# assumes a default holdout (random split) with 2/3 for training and 1/3 for testing:
M=mining(y~x1+x2,Runs=2,model="mlpe",search=2)
print(M)
print(mmetric(M,metric="MAE"))

### more regression examples:
## Not run: 
# simple nonlinear regression task; x3 is a random variable and does not influence y:
data(sin1reg)
# 5 runs of an external holdout with 2/3 for training and 1/3 for testing, fixed seed 12345
# feature selection: sabs method
# model selection: 5 searches for size, internal 2-fold cross validation fixed seed 123
#                  with optimization for minimum MAE metric 
M=mining(y~.,data=sin1reg,Runs=5,method=c("holdout",2/3,12345),model="mlpe",
         search=list(search=mparheuristic("mlpe",n=5),method=c("kfold",2,123),metric="MAE"),
         feature="sabs")
print(mmetric(M,metric="MAE"))
print(M$mpar)
print("median hidden nodes (size) and number of MLPs (nr):")
print(centralpar(M$mpar))
print("attributes used by the model in each run:")
print(M$attributes)
mgraph(M,graph="RSC",Grid=10,main="sin1 MLPE scatter plot")
mgraph(M,graph="REP",Grid=10,main="sin1 MLPE scatter plot",sort=FALSE)
mgraph(M,graph="REC",Grid=10,main="sin1 MLPE REC")
mgraph(M,graph="IMP",Grid=10,main="input importances",xval=0.1,leg=names(sin1reg))
# average influence of x1 on the model:
mgraph(M,graph="VEC",Grid=10,main="x1 VEC curve",xval=1,leg=names(sin1reg)[1])

## End(Not run)

### regression example with holdout rolling windows:
## Not run: 
# simple nonlinear regression task; x3 is a random variable and does not influence y:
data(sin1reg)
# rolling with 20 test samples, training window size of 300 and increment of 50 in each run:
# note that Runs argument is automatically set to 14 in this example:
M=mining(y~.,data=sin1reg,method=c("holdoutrol",20,300,50),
         model="mlpe",debug=TRUE)

## End(Not run)

### regression example with all rminer models:
## Not run: 
# simple nonlinear regression task; x3 is a random variable and does not influence y:
data(sin1reg)
models=c("naive","ctree","rpart","kknn","mlp","mlpe","ksvm","randomForest","mr","mars",
         "cubist","pcr","plsr","cppls","rvm")
for(model in models)
{ 
 M=mining(y~.,data=sin1reg,method=c("holdout",2/3,12345),model=model)
 cat("model:",model,"MAE:",round(mmetric(M,metric="MAE")$MAE,digits=3),"\n")
}

## End(Not run)

### classification example (task="prob")
## Not run: 
data(iris)
# 10 runs of a 3-fold cross validation with fixed seed 123 for generating the 3-fold runs
M=mining(Species~.,iris,Runs=10,method=c("kfold",3,123),model="rpart")
print(mmetric(M,metric="CONF"))
print(mmetric(M,metric="AUC"))
print(meanint(mmetric(M,metric="AUC")))
mgraph(M,graph="ROC",TC=2,baseline=TRUE,Grid=10,leg="Versicolor",
       main="versicolor ROC")
mgraph(M,graph="LIFT",TC=2,baseline=TRUE,Grid=10,leg="Versicolor",
       main="Versicolor ROC")
M2=mining(Species~.,iris,Runs=10,method=c("kfold",3,123),model="ksvm")
L=vector("list",2)
L[[1]]=M;L[[2]]=M2
mgraph(L,graph="ROC",TC=2,baseline=TRUE,Grid=10,leg=c("DT","SVM"),main="ROC")

## End(Not run)

### other classification examples
## Not run: 
### 1st example:
data(iris)
# 2 runs of an external 2-fold validation, random seed
# model selection: SVM model with rbfdot kernel, automatic search for sigma,
#                  internal 3-fold validation, random seed, minimum "AUC" is assumed
# feature selection: none, "s" is used only to store input importance values
M=mining(Species~.,data=iris,Runs=2,method=c("kfold",2,NA),model="ksvm",
         search=list(search=mparheuristic("ksvm"),method=c("kfold",3)),feature="s")

print(mmetric(M,metric="AUC",TC=2))
mgraph(M,graph="ROC",TC=2,baseline=TRUE,Grid=10,leg="SVM",main="ROC",intbar=FALSE)
mgraph(M,graph="IMP",TC=2,Grid=10,main="input importances",xval=0.1,
leg=names(iris),axis=1)
mgraph(M,graph="VEC",TC=2,Grid=10,main="Petal.Width VEC curve",
data=iris,xval=4)
### 2nd example, ordered kfold, k-nearest neigbor:
M=mining(Species~.,iris,Runs=1,method=c("kfoldo",3),model="knn")
# confusion matrix:
print(mmetric(M,metric="CONF"))

### 3rd example, use of all rminer models: 
models=c("naive","ctree","rpart","kknn","mlp","mlpe","ksvm","randomForest","bagging",
         "boosting","lda","multinom","naiveBayes","qda")
for(model in models)
{ 
 M=mining(Species~.,iris,Runs=1,method=c("kfold",3,123),model=model)
 cat("model:",model,"ACC:",round(mmetric(M,metric="ACC")$ACC,digits=1),"\n")
}

## End(Not run)

### multiple models: automl or ensembles 
## Not run: 

data(iris)
d=iris
names(d)[ncol(d)]="y" # change output name
inputs=ncol(d)-1
metric="AUC"

# simple automl (1 search per individual model),
# internal holdout and external holdout:
sm=mparheuristic(model="automl",n=NA,task="prob",inputs=inputs)
mode="auto"

imethod=c("holdout",4/5,123) # internal validation method
emethod=c("holdout",2/3,567) # external validation method

search=list(search=sm,smethod=mode,method=imethod,metric=metric,convex=0)
M=mining(y~.,data=d,model="auto",search=search,method=emethod,fdebug=TRUE)
# 1 single model was selected:
cat("best",emethod[1],"selected model:",M$object@model,"\n")
cat(metric,"=",round(as.numeric(mmetric(M,metric=metric)),2),"\n")

# simple automl (1 search per individual model),
# internal kfold and external kfold: 
imethod=c("kfold",3,123) # internal validation method
emethod=c("kfold",5,567) # external validation method
search=list(search=sm,smethod=mode,method=imethod,metric=metric,convex=0)
M=mining(y~.,data=d,model="auto",search=search,method=emethod,fdebug=TRUE)
# kfold models were selected:
kfolds=as.numeric(emethod[2])
models=vector(length=kfolds)
for(i in 1:kfolds) models[i]=M$object$model[[i]]
cat("best",emethod[1],"selected models:",models,"\n")
cat(metric,"=",round(as.numeric(mmetric(M,metric=metric)),2),"\n")

# example with weighted ensemble:
M=mining(y~.,data=d,model="WE",search=search,method=emethod,fdebug=TRUE)
for(i in 1:kfolds) models[i]=M$object$model[[i]]
cat("best",emethod[1],"selected models:",models,"\n")
cat(metric,"=",round(as.numeric(mmetric(M,metric=metric)),2),"\n")


## End(Not run)


### for more fitting examples check the help of function fit: help(fit,package="rminer")
</code></pre>

<hr>
<h2 id='mmetric'>
Compute classification or regression error metrics.
</h2><span id='topic+mmetric'></span><span id='topic+metrics'></span>

<h3>Description</h3>

<p>Compute classification or regression error metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmetric(y, x = NULL, metric, D = 0.5, TC = -1, val = NULL, aggregate = "no")</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mmetric_+3A_y">y</code></td>
<td>
<p>if there are predictions (<code>!is.null(x)</code>), <code>y</code> should be a numeric vector or factor with the target desired responses (or output values).<br />
Else, <code>y</code> should be a list returned by the  <code><a href="#topic+mining">mining</a></code> function.
</p>
</td></tr>
<tr><td><code id="mmetric_+3A_x">x</code></td>
<td>
<p>the predictions (should be a numeric vector if <code>task="reg"</code>, matrix if <code>task="prob"</code> or factor if <code>task="class"</code> (used if <code>y</code> is not a list).</p>
</td></tr>
<tr><td><code id="mmetric_+3A_metric">metric</code></td>
<td>
<p>a R function or a character.<br />
Note: if a R function, then it should be set to provide lower values for better models if the intention is to be used
within the <code>search</code> argument of <code><a href="#topic+fit">fit</a></code> and <code><a href="#topic+mining">mining</a></code> (i.e., &quot;&lt;&quot; meaning).<br />
Valid character options are (&quot;&gt;&quot; means &quot;better&quot; if higher value; &quot;&lt;&quot; means &quot;better&quot; if lower value):
</p>

<ul>
<li> <p><code>ALL</code> &ndash; returns all classification or regression metrics (context dependent, multi-metric). 
</p>
</li>
<li><p> if vector &ndash; returns all metrics included in the vector, vector elements can be any of the options below (multi-metric).
</p>
</li>
<li> <p><code>CONF</code> &ndash; confusion matrix (classification, matrix). 
</p>
</li>
<li> <p><code>ACC</code> &ndash; classification accuracy rate, equal to micro averaged F1 score (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>macroACC</code> &ndash; macro average ACC score, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>weightedACC</code> &ndash; weighted average ACC score, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>CE</code> &ndash; classification error or misclassification error rate (classification, &quot;&lt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>MAEO</code> &ndash; mean absolute error for ordinal classification (classification, &quot;&lt;&quot;, [0-Inf[). 
</p>
</li>
<li> <p><code>MSEO</code> &ndash; mean squared error for ordinal classification (classification, &quot;&lt;&quot;, [0-Inf[). 
</p>
</li>
<li> <p><code>KENDALL</code> &ndash; Kendalls's coefficient for ordinal classification or (mean if) ranking (classification, &quot;&gt;&quot;, [-1;1]). Note: if ranking, <code>y</code> is a matrix and mean metric is computed.
</p>
</li>
<li> <p><code>SPEARMAN</code> &ndash; Mean Spearman's rho coefficient for ranking (classification, &quot;&gt;&quot;, [-1;1]). Note: if ranking, <code>y</code> is a matrix and mean metric is computed.
</p>
</li>
<li> <p><code>BER</code> &ndash; balanced error rate (classification, &quot;&lt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>KAPPA</code> &ndash; kappa index (classification, &quot;&lt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>CRAMERV</code> &ndash; Cramer's V  (classification, &quot;&gt;&quot;, [0,1.0]). 
</p>
</li>
<li> <p><code>ACCLASS</code> &ndash; classification accuracy rate per class (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>BAL_ACC</code> &ndash; balanced accuracy rate per class (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>TPR</code> &ndash; true positive rate, sensitivity or recall (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>macroTPR</code> &ndash; macro average TPR score, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>weightedTPR</code> &ndash; weighted average TPR score, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>TNR</code> &ndash; true negative rate or specificity (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>macroTNR</code> &ndash; macro average TNR score, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>weightedTNR</code> &ndash; weighted average TNR score, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>microTNR</code> &ndash; micro average TNR score, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>PRECISION</code> &ndash; precision (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>macroPRECISION</code> &ndash; macro average precision, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>weightedPRECISION</code> &ndash; weighted average precision, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>F1</code> &ndash; F1 score (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>macroF1</code> &ndash; macro average F1 score, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>weightedF1</code> &ndash; weighted average F1 score, for multiclass tasks (classification, &quot;&gt;&quot;, [0-%100]). 
</p>
</li>
<li> <p><code>MCC</code> &ndash; Matthews correlation coefficient (classification, &quot;&gt;&quot;, [-1,1]). 
</p>
</li>
<li> <p><code>BRIER</code> &ndash; overall Brier score (classification &quot;prob&quot;, &quot;&lt;&quot;, [0,1.0]).
</p>
</li>
<li> <p><code>BRIERCLASS</code> &ndash; Brier score per class (classification &quot;prob&quot;, &quot;&lt;&quot;, [0,1.0]).
</p>
</li>
<li> <p><code>ROC</code> &ndash;  Receiver Operating Characteristic curve (classification &quot;prob&quot;, list with several components).
</p>
</li>
<li> <p><code>AUC</code> &ndash; overall area under the curve (of ROC curve, classification &quot;prob&quot;, &quot;&gt;&quot;, domain values: [0,1.0]). 
</p>
</li>
<li> <p><code>AUCCLASS</code> &ndash; area under the curve per class (of ROC curve, classification &quot;prob&quot;, &quot;&gt;&quot;, domain values: [0,1.0]). 
</p>
</li>
<li> <p><code>NAUC</code> &ndash; normalized AUC (given a fixed <code>val=</code>FPR, classification &quot;prob&quot;, &quot;&gt;&quot;, [0,1.0]). 
</p>
</li>
<li> <p><code>TPRATFPR</code> &ndash; the TPR (given a fixed <code>val=</code>FPR, classification &quot;prob&quot;, &quot;&gt;&quot;, [0,1.0]).
</p>
</li>
<li> <p><code>LIFT</code> &ndash; accumulative percent of responses captured (LIFT accumulative curve, classification &quot;prob&quot;, list with several components).
</p>
</li>
<li> <p><code>ALIFT</code> &ndash; area of the accumulative percent of responses captured (LIFT accumulative curve, classification &quot;prob&quot;, &quot;&gt;&quot;, [0,1.0]).
</p>
</li>
<li> <p><code>NALIFT</code> &ndash; normalized ALIFT (given a fixed <code>val=</code>percentage of examples, classification &quot;prob&quot;, &quot;&gt;&quot;, [0,1.0]).
</p>
</li>
<li> <p><code>ALIFTATPERC</code> &ndash; ALIFT value (given a fixed <code>val=</code>percentage of examples, classification &quot;prob&quot;, &quot;&gt;&quot;, [0,1.0]).
</p>
</li>
<li> <p><code>SAE</code> &ndash; sum absolute error/deviation (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>MAE</code> &ndash; mean absolute error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>MdAE</code> &ndash; median absolute error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>GMAE</code> &ndash; geometric mean absolute error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>MaxAE</code> &ndash; maximum absolute error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>NMAE</code> &ndash; normalized mean absolute error (regression, &quot;&lt;&quot;, [0%,Inf[). Note: by default, this metric assumes the range of <code>y</code> as the denominator of <code>NMAE</code>; a different range can be set by setting the optional <code>val</code> argument (see example).
</p>
</li>
<li> <p><code>RAE</code> &ndash; relative absolute error (regression, &quot;&lt;&quot;, [0%,Inf[).
</p>
</li>
<li> <p><code>SSE</code> &ndash; sum squared error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>MSE</code> &ndash; mean squared error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>MdSE</code> &ndash; median squared error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>RMSE</code> &ndash; root mean squared error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>GMSE</code> &ndash; geometric mean squared error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>HRMSE</code> &ndash; Heteroscedasticity consistent root mean squared error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>RSE</code> &ndash; relative squared error (regression, &quot;&lt;&quot;, [0%,Inf[).
</p>
</li>
<li> <p><code>RRSE</code> &ndash; root relative squared error (regression, &quot;&lt;&quot;, [0%,Inf[).
</p>
</li>
<li> <p><code>ME</code> &ndash; mean error (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>SMinkowski3</code> &ndash; sum of Minkowski loss function (q=3, heavier penalty for large errors when compared with SSE, regression, &quot;&lt;&quot;, [0%,Inf[).
</p>
</li>
<li> <p><code>MMinkowski3</code> &ndash; mean of Minkowski loss function (q=3, heavier penalty for large errors when compared with SSE, regression, &quot;&lt;&quot;, [0%,Inf[).
</p>
</li>
<li> <p><code>MdMinkowski3</code> &ndash; median of Minkowski loss function (q=3, heavier penalty for large errors when compared with SSE, regression, &quot;&lt;&quot;, [0%,Inf[).
</p>
</li>
<li> <p><code>COR</code> &ndash; Pearson correlation (regression, &quot;&gt;&quot;, [-1,1]).
</p>
</li>
<li> <p><code>q2</code> &ndash; =1-correlation^2 test error metric, as used by M.J. Embrechts  (regression, &quot;&lt;&quot;, [0,1.0]).
</p>
</li>
<li> <p><code>R2</code> &ndash; coefficient of determination R^2 (regression, &quot;&gt;&quot;, squared pearson correlation coefficient: [0,1]).
</p>
</li>
<li> <p><code>R22</code> &ndash; 2nd variant of coefficient of determination R^2 (regression, &quot;&gt;&quot;, most general definition that however can lead to negative values: ]-Inf,1]. In previous rminer versions, this variant was known as &quot;R2&quot;).
</p>
</li>
<li> <p><code>EV</code> &ndash; explained variance, 1 - var(y-x)/var(y) (regression, &quot;&gt;&quot;, ]-Inf,1]).
</p>
</li>
<li> <p><code>Q2</code> &ndash; R^2/SD test error metric, as used by M.J. Embrechts (regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>REC</code> &ndash;  Regression Error Characteristic curve (regression, list with several components).
</p>
</li>
<li> <p><code>NAREC</code> &ndash; normalized REC area (given a fixed <code>val=</code>tolerance, regression, &quot;&gt;&quot;, [0,1.0]).
</p>
</li>
<li> <p><code>TOLERANCE</code> &ndash; the tolerance (y-axis value) of a REC curve given a fixed <code>val=</code>tolerance value, regression, &quot;&gt;&quot;, [0,1.0]).
</p>
</li>
<li> <p><code>TOLERANCEPERC</code> &ndash; the tolerance (y-axis value) of a REC curve given a percentage <code>val=</code> value (in terms of y range), regression, &quot;&gt;&quot;, [0,1.0]).
</p>
</li>
<li> <p><code>MAPE</code> &ndash;  Mean Absolute Percentage mmetric forecasting metric (regression, &quot;&lt;&quot;, [0%,Inf[).
</p>
</li>
<li> <p><code>MdAPE</code> &ndash; Median Absolute Percentage mmetric forecasting metric (regression, &quot;&lt;&quot;), [0%,Inf[).
</p>
</li>
<li> <p><code>RMSPE</code> &ndash; Root Mean Square Percentage mmetric forecasting metric (regression, &quot;&lt;&quot;, [0%,Inf[).
</p>
</li>
<li> <p><code>RMdSPE</code> &ndash; Root Median Square Percentage mmetric forecasting metric (regression, &quot;&lt;&quot;, [0%,Inf[).
</p>
</li>
<li> <p><code>SMAPE</code> &ndash;  Symmetric Mean Absolute Percentage mmetric forecasting metric (regression, &quot;&lt;&quot;, [0%,200%]).
</p>
</li>
<li> <p><code>SMdAPE</code> &ndash; Symmetric Median Absolute Percentage mmetric forecasting metric  (regression, &quot;&lt;&quot;, [0%,200%]).
</p>
</li>
<li> <p><code>MRAE</code> &ndash; Mean Relative Absolute mmetric forecasting metric (<code>val</code> should contain the last in-sample/training data value (for random walk) or full benchmark time series related with out-of-sample values, regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>MdRAE</code> &ndash; Median Relative Absolute mmetric forecasting metric (<code>val</code> should contain the last in-sample/training data value (for random walk) or full benchmark time series, regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>GMRAE</code> &ndash; Geometric Mean Relative Absoluate mmetric forecasting metric  (<code>val</code> should contain the last in-sample/training data value (for random walk) or full benchmark time series, regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>THEILSU2</code> &ndash; Theils'U2 forecasting metric (<code>val</code> should contain the last in-sample/training data value (for random walk) or full benchmark time series, regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li>
<li> <p><code>MASE</code> &ndash; MASE forecasting metric  (<code>val</code> should contain the time series in-samples or training data, regression, &quot;&lt;&quot;, [0,Inf[).
</p>
</li></ul>

</td></tr>
<tr><td><code id="mmetric_+3A_d">D</code></td>
<td>
<p>decision threshold (for <code>task="prob"</code>, probabilistic classification) within [0,1]. The class is TRUE if <em>prob&gt;D</em>.</p>
</td></tr>
<tr><td><code id="mmetric_+3A_tc">TC</code></td>
<td>
<p>target class index or vector of indexes (for multi-class classification class) from 1 to <em>Nc</em>, where <em>Nc</em> is the number of classes:&lt;cr&gt;
</p>

<ul>
<li><p> if <code>TC==-1</code> (the default value), then it is assumed:
</p>

<ul>
<li><p> if <code>metric</code> is &quot;CONF&quot; &ndash; <code>D</code> is ignored and highest probability class is assumed (if <code>TC&gt;0</code>, the metric is computed for positive <code>TC</code> class and <code>D</code> is used).
</p>
</li>
<li><p> if <code>metric</code> is &quot;ACC&quot;, &quot;CE&quot;, &quot;BER&quot;, &quot;KAPPA&quot;, &quot;CRAMERV&quot;, &quot;BRIER&quot;, or &quot;AUC&quot; &ndash; the global metric (for all classes) is computed (if <code>TC&gt;0</code>, the metric is computed for positive <code>TC</code> class).
</p>
</li>
<li><p> if <code>metric</code> is &quot;ACCLASS&quot;, &quot;TPR&quot;, &quot;TNR&quot;, &quot;Precision&quot;, &quot;F1&quot;, &quot;MCC&quot;, &quot;ROC&quot;, &quot;BRIERCLASS&quot;, &quot;AUCCLASS&quot; &ndash; it returns one result per class (if <code>TC&gt;0</code>, it returns negative (e.g. &quot;TPR1&quot;) and positive (TC, e.g. &quot;TPR2&quot;) result).
</p>
</li>
<li><p> if <code>metric</code> is &quot;NAUC&quot;, &quot;TPRATFPR&quot;, &quot;LIFT&quot;, &quot;ALIFT&quot;, &quot;NALIFT&quot; or &quot;ALIFTATPERC&quot; &ndash; TC is set to the index of the last class.
</p>
</li></ul>

</li></ul>

</td></tr>
<tr><td><code id="mmetric_+3A_val">val</code></td>
<td>
<p>auxiliary value:
</p>

<ul>
<li><p> when two or more metrics need different <code>val</code> values, then <code>val</code> should be a vector list, see example.
</p>
</li>
<li><p> if numeric or vector &ndash; check the <code>metric</code> argument for specific details of each metric val meaning.
</p>
</li></ul>

</td></tr>
<tr><td><code id="mmetric_+3A_aggregate">aggregate</code></td>
<td>
<p>character with type of aggregation performed when y is a <code><a href="#topic+mining">mining</a></code> list. Valid options are:
</p>

<ul>
<li><p> no &ndash; returns all metrics for all <code><a href="#topic+mining">mining</a></code> runs. If <code>metric</code> includes &quot;CONF&quot;, &quot;ROC&quot;, &quot;LIFT&quot; or &quot;REC&quot;, it returns a vector list, else if <code>metric</code> includes a single metric, it returns a vector; else it retuns a data.frame (runs x metrics).
</p>
</li>
<li><p> sum &ndash; sums all run results. 
</p>
</li>
<li><p> mean &ndash; averages all run results.
</p>
</li>
<li><p> note: both &quot;sum&quot; and &quot;mean&quot; only work if only <code>metric=="CONF"</code> is used or if <code>metric</code> does not contain &quot;ROC&quot;, &quot;LIFT&quot; or &quot;REC&quot;.
</p>
</li></ul>

</td></tr>
</table>


<h3>Details</h3>

<p>Compute classification or regression error metrics:
</p>

<ul>
<li> <p><code>mmetric</code> &ndash; compute one or more classification/regression metrics given y and x OR a mining list.
</p>
</li>
<li> <p><code>metrics</code> &ndash; deprecated function, same as <code>mmetric(x,y,metric="ALL")</code>, included here just for compatability purposes but will be removed from the package.
</p>
</li></ul>



<h3>Value</h3>

<p>Returns the computed error metric(s): 
</p>

<ul>
<li><p> one value if only one <code>metric</code> is requested (and <code>y</code> is not a mining list);
</p>
</li>
<li><p> named vector if 2 or more elements are requested in <code>metric</code> (and <code>y</code> is not a mining list); 
</p>
</li>
<li><p> list if there is a &quot;CONF&quot;, &quot;ROC&quot;, &quot;LIFT&quot; or &quot;REC&quot; request on <code>metric</code> (other metrics are stored in field <code>$res</code>, and <code>y</code> is not a mining list).
</p>
</li>
<li><p> if <code>y</code> is a mining list then there can be several runs, thus:
</p>

<ul>
<li><p> a vector list of size <code>y$runs</code> is returned if <code>metric</code> includes &quot;CONF&quot;, &quot;ROC&quot;, &quot;LIFT&quot; or &quot;REC&quot; and <code>aggregate="no"</code>;
</p>
</li>
<li><p> a data.frame is returned if <code>aggregate="no"</code> and  <code>metric</code> does not include &quot;CONF&quot;, &quot;ROC&quot;, &quot;LIFT&quot; or &quot;REC&quot;;
</p>
</li>
<li><p> a table is returned if <code>aggregate="sum" or "mean"</code> and <code>metric="CONF"</code>;
</p>
</li>
<li><p> a vector or numeric value is returned if  <code>aggregate="sum" or "mean"</code>  and <code>metric</code> is not &quot;CONF&quot;.
</p>
</li></ul>

</li></ul>



<h3>Note</h3>

<p>See also <a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a> and <a href="http://www3.dsi.uminho.pt/pcortez/rminer.html">http://www3.dsi.uminho.pt/pcortez/rminer.html</a>
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>


<ul>
<li><p> To check for more details about rminer and for citation purposes:<br />
P. Cortez.<br />
Data Mining with Neural Networks and Support Vector Machines Using the R/rminer Tool.<br />
In P. Perner (Ed.), Advances in Data Mining - Applications and Theoretical Aspects 10th Industrial Conference on Data Mining (ICDM 2010), Lecture Notes in Artificial Intelligence 6171, pp. 572-583, Berlin, Germany, July, 2010. Springer. ISBN: 978-3-642-14399-1.<br />
@Springer: <a href="https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44">https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44</a><br />
<a href="http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf">http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf</a><br />
</p>
</li>
<li><p> This tutorial shows additional code examples:<br />
P. Cortez.<br />
A tutorial on using the rminer R package for data mining tasks.<br />
Teaching Report, Department of Information Systems, ALGORITMI Research Centre, Engineering School, University of Minho, Guimaraes, 
Portugal, July 2015.<br />
<a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a><br />
</p>
</li>
<li><p> About the Brier and Global AUC scores:<br />
A. Silva, P. Cortez, M.F. Santos, L. Gomes and J. Neves.<br />
Rating Organ Failure via Adverse Events using Data Mining in the Intensive Care Unit.<br />
In Artificial Intelligence in Medicine, Elsevier, 43 (3): 179-193, 2008.<br />
<a href="https://doi.org/10.1016/j.artmed.2008.03.010">doi:10.1016/j.artmed.2008.03.010</a><br />
</p>
</li>
<li><p> About the classification and regression metrics:<br />
I. Witten and E. Frank.<br />
Data Mining: Practical machine learning tools and techniques.<br />
Morgan Kaufmann, 2005.
</p>
</li>
<li><p> About the forecasting metrics:<br />
R. Hyndman and A. Koehler<br />
Another look at measures of forecast accuracy.<br />
In International Journal of Forecasting, 22(4):679-688, 2006.<br />
</p>
</li>
<li><p> About the ordinal classification metrics:<br />
J.S. Cardoso and R. Sousa.<br />
Measuring the Performance of Ordinal Classification.<br />
In International Journal of Pattern Recognition and Artificial Intelligence, 25(8):1173-1195, 2011.<br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code>, <code><a href="#topic+predict.fit">predict.fit</a></code>, <code><a href="#topic+mining">mining</a></code>, <code><a href="#topic+mgraph">mgraph</a></code>, <code><a href="#topic+savemining">savemining</a></code> and <code><a href="#topic+Importance">Importance</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### pure binary classification 
y=factor(c("a","a","a","a","b","b","b","b"))
x=factor(c("a","a","b","a","b","a","b","a"))
print(mmetric(y,x,"CONF")$conf)
print(mmetric(y,x,metric=c("ACC","TPR","ACCLASS")))
print(mmetric(y,x,"ALL"))

### probabilities binary classification 
y=factor(c("a","a","a","a","b","b","b","b"))
px=matrix(nrow=8,ncol=2)
px[,1]=c(1.0,0.9,0.8,0.7,0.6,0.5,0.4,0.3)
px[,2]=1-px[,1]
print(px)
print(mmetric(y,px,"CONF")$conf)
print(mmetric(y,px,"CONF",D=0.5,TC=2)$conf)
print(mmetric(y,px,"CONF",D=0.3,TC=2)$conf)
print(mmetric(y,px,metric="ALL",D=0.3,TC=2))
print(mmetric(y,px,metric=c("ACC","AUC","AUCCLASS","BRIER","BRIERCLASS","CE"),D=0.3,TC=2))
# ACC and confusion matrix:
print(mmetric(y,px,metric=c("ACC","CONF"),D=0.3,TC=2))
# ACC and ROC curve:
print(mmetric(y,px,metric=c("ACC","ROC"),D=0.3,TC=2))
# ACC, ROC and LIFT curve:
print(mmetric(y,px,metric=c("ACC","ROC","LIFT"),D=0.3,TC=2))

### pure multi-class classification 
y=c('A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A',
'A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A',
'A','A','B','B','B','B','B','B','B','B','B','B','C','C','C','C','C','C','C','C','C','C',
'C','C','C','C','C','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
'D','D','D','D','D','D','D','D','E','E','E','E','E')
x=c('A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A',
'A','A','A','A','A','A','A','A','A','A','A','A','A','A','E','E','E','E','E','D','D','D',
'D','D','B','B','B','B','B','B','B','B','B','D','C','C','C','C','C','C','C','B','B','B',
'B','B','C','C','C','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
'D','D','D','D','D','D','C','C','E','A','A','B','B')
y=factor(y)
x=factor(x)
print(mmetric(y,x,metric="CONF")$conf) # confusion matrix
print(mmetric(y,x,metric="CONF",TC=-1)$conf) # same thing
print(mmetric(y,x,metric="CONF",TC=1)$conf) # for target class TC=1: "A"
mshow=function(y,x,metric) print(round(mmetric(y,x,metric),digits=0))
mshow(y,x,"ALL")
mshow(y,x,c("ACCLASS","BAL_ACC","KAPPA"))
mshow(y,x,c("PRECISION")) # precision
mshow(y,x,c("TPR")) # recall 
mshow(y,x,c("F1")) # F1 score

# micro (=ACC), macro and weighted average:
mshow(y,x,c("ACC","macroPRECISION","weightedPRECISION")) 
mshow(y,x,c("ACC","macroTPR","weightedTPR")) 
mshow(y,x,c("ACC","macroF1","weightedF1"))
mshow(y,x,c("ACC","macroACC","weightedACC"))

# several metrics in a single returned object:
print(mmetric(y,x,metric=c("CONF","macroF1","weightedF1","ACC")))

### probabilities multi-class 
y=factor(c("a","a","b","b","c","c"))
px=matrix(nrow=6,ncol=3)
px[,1]=c(1.0,0.7,0.5,0.3,0.1,0.7)
px[,2]=c(0.0,0.2,0.4,0.7,0.3,0.2)
px[,3]=1-px[,1]-px[,2]
print(px)
print(mmetric(y,px,metric="ALL",TC=-1,val=0.1))
print(mmetric(y,px,metric=c("AUC","AUCCLASS","NAUC"),TC=-1,val=0.1))
print(mmetric(y,px,metric=c("AUC","NAUC"),TC=3,val=0.1))
print(mmetric(y,px,metric=c("ACC","ACCLASS"),TC=-1))
print(mmetric(y,px,metric=c("CONF"),TC=3,D=0.5)$conf)
print(mmetric(y,px,metric=c("ACCLASS"),TC=3,D=0.5))
print(mmetric(y,px,metric=c("CONF"),TC=3,D=0.7)$conf)
print(mmetric(y,px,metric=c("ACCLASS"),TC=3,D=0.7))

### ordinal multi-class (example in Ricardo Sousa PhD thesis 2012)
y=ordered(c(rep("a",4),rep("b",6),rep("d",3)),levels=c("a","b","c","d"))
x=ordered(c(rep("c",(4+6)),rep("d",3)),levels=c("a","b","c","d"))
print(mmetric(y,x,metric="CONF")$conf)
print(mmetric(y,x,metric=c("CE","MAEO","MSEO","KENDALL")))
# note: only y needs to be ordered
x=factor(c(rep("b",4),rep("a",6),rep("d",3)),levels=c("a","b","c","d"))
print(mmetric(y,x,metric="CONF")$conf)
print(mmetric(y,x,metric=c("CE","MAEO","MSEO","KENDALL")))
print(mmetric(y,x,metric="ALL"))

### ranking (multi-class) 
y=matrix(nrow=1,ncol=12);x=y
# http://www.youtube.com/watch?v=D56dvoVrBBE
y[1,]=1:12
x[1,]=c(2,1,4,3,6,5,8,7,10,9,12,11)
print(mmetric(y,x,metric="KENDALL"))
print(mmetric(y,x,metric="ALL"))

y=matrix(nrow=2,ncol=7);x=y
y[1,]=c(2,6,5,4,3,7,1)
y[2,]=7:1
x[1,]=1:7
x[2,]=1:7
print(mmetric(y,x,metric="ALL"))

### regression examples: y - desired values; x - predictions
y=c(95.01,96.1,97.2,98.0,99.3,99.7);x=95:100
print(mmetric(y,x,"ALL"))
print(mmetric(y,x,"MAE"))
mshow=function(y,x,metric) print(round(mmetric(y,x,metric),digits=2))
mshow(y,x,c("MAE","RMSE","RAE","RSE"))
# getting NMAE:
m=mmetric(y,x,"NMAE")
cat("NMAE:",round(m,digits=2)," (denominator=",diff(range(y)),")\n")
m=mmetric(y,x,"NMAE",val=5) # usage of different range
cat("NMAE:",round(m,digits=2)," (denominator=",5,")\n")
# get REC curve and other measures:
m=mmetric(y,x,c("REC","TOLERANCEPERC","MAE"),val=5)
print(m)

# correlation or similar measures:
mshow(y,x,c("COR","R2","R22","EV")) # ideal is close to 1
mshow(y,x,c("q2","Q2")) # ideal is close to 0
# other measures:
print(mmetric(y,x,c("TOLERANCE","NAREC"),val=0.5)) # if admitted/accepted absolute error is 0.5
print(mmetric(y,x,"TOLERANCEPERC",val=0.05)) # tolerance for a 5% of yrange 
# tolerance for fixed 0.1 value and 5% of yrange:
print(mmetric(y,x,c("TOLERANCE","TOLERANCEPERC"),val=c(0.1,0.05))) 
print(mmetric(y,x,"THEILSU2",val=94.1)) # val = 1-ahead random walk, c(y,94.1), same as below
print(mmetric(y,x,"THEILSU2",val=c(94.1,y[1:5]))) # val = 1-ahead random walk (previous y values)
print(mmetric(y,x,"MASE",val=c(88.1,89.9,93.2,94.1))) # val = in-samples
val=vector("list",length=4)
val[[2]]=0.5;val[[3]]=94.1;val[[4]]=c(88.1,89.9,93.2,94.1)
print(mmetric(y,x,c("MAE","NAREC","THEILSU2","MASE"),val=val))
# user defined error function example:
# myerror = number of samples with absolute error above 0.1% of y: 
myerror=function(y,x){return (sum(abs(y-x)&gt;(0.001*y)))}
print(mmetric(y,x,metric=myerror))
# example that returns a list since "REC" is included:
print(mmetric(y,x,c("MAE","REC","TOLERANCE","EV"),val=1))


### mining, several runs, prob multi-class
## Not run: 
data(iris)
M=mining(Species~.,iris,model="rpart",Runs=2)
R=mmetric(M,metric="CONF",aggregate="no")
print(R[[1]]$conf)
print(R[[2]]$conf)
print(mmetric(M,metric="CONF",aggregate="mean"))
print(mmetric(M,metric="CONF",aggregate="sum"))
print(mmetric(M,metric=c("ACC","ACCLASS"),aggregate="no"))
print(mmetric(M,metric=c("ACC","ACCLASS"),aggregate="mean"))
print(mmetric(M,metric="ALL",aggregate="no"))
print(mmetric(M,metric="ALL",aggregate="mean"))

## End(Not run)

### mining, several runs, regression
## Not run: 
data(sin1reg)
S=sample(1:nrow(sin1reg),40)
M=mining(y~.,data=sin1reg[S,],model="ksvm",search=2^3,Runs=10)
R=mmetric(M,metric="MAE")
print(mmetric(M,metric="MAE",aggregate="mean"))
miR=meanint(R) # mean and t-student confidence intervals
cat("MAE=",round(miR$mean,digits=2),"+-",round(miR$int,digits=2),"\n")
print(mmetric(M,metric=c("MAE","RMSE")))
print(mmetric(M,metric=c("MAE","RMSE"),aggregate="mean"))
R=mmetric(M,metric="REC",aggregate="no")
print(R[[1]]$rec)
print(mmetric(M,metric=c("TOLERANCE","NAREC"),val=0.2))
print(mmetric(M,metric=c("TOLERANCE","NAREC"),val=0.2,aggregate="mean"))

## End(Not run)

</code></pre>

<hr>
<h2 id='mparheuristic'>
Function that returns a list of searching (hyper)parameters for a particular model (classification or regression) or for a multiple list of models (automl or ensembles).
</h2><span id='topic+mparheuristic'></span>

<h3>Description</h3>

<p>Easy to use function that returns a list of searching (hyper)parameters for a particular model (classification or regression) or for a multiple list of models (automl or ensembles). 
The result is to be put in a <code>search</code> argument, used by <code><a href="#topic+fit">fit</a></code> or <code><a href="#topic+mining">mining</a></code> functions. Something
like:<br /> <code>search=list(search=mparheuristic(...),...)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mparheuristic(model, n = NA, lower = NA, upper = NA, by = NA, exponential = NA, 
              kernel = "rbfdot", task = "prob", inputs = NA)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mparheuristic_+3A_model">model</code></td>
<td>
<p>model type name. See <code><a href="#topic+fit">fit</a></code> for the individual model details (e.g., <code>"ksvm"</code>). For multiple models use:
</p>

<ul>
<li> <p><code>automl</code> - 5 individual machine learning algorithms: generalized linear model (GLM, via <code>cv.glmnet</code>), support vector machine (SVM, via <code>ksvm</code>), multilayer perceptron (MLP, via <code>mlpe</code>), random forest (RF, via <code>randomForest</code>) and extreme gradient boosting (XG, via <code>xgboost</code>). The <code>n="heuristic"</code> setting (see below) is assumed for all algorithms, thus just one hyperparameter is tested for each model. This option is thus the fastest automl to run.
</p>
</li>
<li> <p><code>automl2</code> - same 5 individual machine learning algorithms as <code>automl</code>. For each algorithm, a grid search is executed with 10 searches (same as:<br /> <code>n="heuristic10"</code>), except for <code>ksvm</code>, which uses 13 searches of an uniform design (<code>"UD"</code>).
</p>
</li>
<li> <p><code>automl3</code> - same as <code>automl2</code> except that a six extra stacking ensemble (<code>"SE"</code>) model is performed using the 5 best tuned algorithm versions (GLM, SVM, MLP, RF and XG).
</p>
</li>
<li><p> a character vector with several models - see the example section for a demonstration of this option.
</p>
</li></ul>

</td></tr>
<tr><td><code id="mparheuristic_+3A_n">n</code></td>
<td>
<p>number of searches or heuristic or numeric vector (either <code>n</code> or <code>by</code> should be used, <code>n</code> has prevalence over <code>by</code>). By default, the searches are linear for all models except for SVM several <code>rbfdot</code> kernel based models (<code>"ksvm"</code>,<code>"rsvm"</code>,<code>"lssvm"</code>, which can assume <code>2^</code>search-range; please check the result of this function to confirm if the search is linear or <code>2^</code>search-range).<br />
If this argument is 1 then the main hyperparameter of the model is set to the value of <code>lower</code> (if lower is not NA).<br />
If this argument is a numeric vector, then the main hyperparameter of the model is set to this vector.<br />
If this argument is a character type, then it is assumed to be an heuristic. Possible heuristic values are:
</p>

<ul>
<li> <p><code>heuristic</code> - only one model is fit, uses default rminer values, same as <code>mparheuristic(model)</code>.
</p>
</li>
<li> <p><code>heuristic5</code> - 5 hyperparameter searches from lower to upper, only works for the following models: 
<code>ctree</code>, <code>rpart</code>, <code>kknn</code>, <code>ksvm</code>, <code>lssvm</code>, <code>mlp</code>, <code>mlpe</code>, <code>randomForest</code>, <code>multinom</code>, <code>rvm</code>, <code>xgboost</code>.
Notes:
<code>rpart</code> - different <code>cp</code> values (see <code><a href="rpart.html#topic+rpart.control">rpart.control</a></code>); 
<code>ctree</code> - different <code>mincriterion</code> values (see <code><a href="party.html#topic+ctree_control">ctree_control</a></code>); 
<code>randomForest</code> &ndash; upper argument is limited by the number of <code>inputs</code> (<code>mtry</code> is searched); 
<code>ksvm</code>, <code>lssvm</code> or <code>rvm</code> - the optional <code>kernel</code> argument can be used.
</p>
</li>
<li> <p><code>heuristic10</code> - same as <code>heuristic5</code> but with 10 searches from <code>lower</code> to <code>upper</code>.
</p>
</li>
<li> <p><code>UD</code> or <code>UD1</code> - <code>UD</code> or <code>UD1</code> uniform design search (only for <code>ksvm</code> and <code>rbfdof</code> kernel). This option assumes 2 hyperparameters for classification (sigma, C) and 3 hyperparameters (sigma, C, epsilon) for regression, thus <code>task="reg"</code> argument needs to be set when regression is used.
</p>
</li>
<li> <p><code>xgb9</code> - 9 searches (3 for <code>eta</code> and 3 for <code>max_depth</code>, works only when <code>model=xgboost</code>.
</p>
</li>
<li> <p><code>mlp_t</code> - heuristic 33 from Delgado 2014 paper, 10 searches, works only when <code>model=mlp</code> or <code>model=mlpe</code>.
</p>
</li>
<li> <p><code>avNNet_t</code> - heuristic 34 from Delgado 2014 paper, 9 searches, works only when <code>model=mlpe</code>.
</p>
</li>
<li> <p><code>nnet_t</code> - heuristic 36 from Delgado 2014 paper, 25 searches, works only when <code>model=mlp</code> or <code>model=mlpe</code>.
</p>
</li>
<li> <p><code>svm_C</code> - heuristic 48 from Delgado 2014 paper, 130 searches (may take time), works only when <code>model=ksvm</code>.
</p>
</li>
<li> <p><code>svmRadial_t</code> - heuristic 52 from Delgado 2014 paper, 25 searches, works only when <code>model=ksvm</code>.
</p>
</li>
<li> <p><code>svmLinear_t</code> - heuristic 54 from Delgado 2014 paper, 5 searches, works only when <code>model=ksvm</code>.
</p>
</li>
<li> <p><code>svmPoly_t</code> - heuristic 55 from Delgado 2014 paper, 27 searches, works only when <code>model=ksvm</code>.
</p>
</li>
<li> <p><code>lsvmRadial_t</code> - heuristic 56 from Delgado 2014 paper, 10 searches, works only when <code>model=lssvm</code>.
</p>
</li>
<li> <p><code>rpart_t</code> - heuristic 59 from Delgado 2014 paper, 10 searches, works only when <code>model=rpart</code>.
</p>
</li>
<li> <p><code>rpart2_t</code> - heuristic 60 from Delgado 2014 paper, 10 searches, works only when <code>model=rpart</code>.
</p>
</li>
<li> <p><code>ctree_t</code> - heuristic 63 from Delgado 2014 paper, 10 searches, works only when <code>model=ctree</code>.
</p>
</li>
<li> <p><code>ctree2_t</code> - heuristic 64 from Delgado 2014 paper, 10 searches, works only when <code>model=ctree</code>.
</p>
</li>
<li> <p><code>rf_t</code> - heuristic 131 from Delgado 2014 paper, 10 searches, works only when <code>model=randomForest</code>.
</p>
</li>
<li> <p><code>knn_R</code> - heuristic 154 from Delgado 2014 paper, 19 searches, works only when <code>model=kknn</code>.
</p>
</li>
<li> <p><code>knn_t</code> - heuristic 155 from Delgado 2014 paper, 10 searches, works only when <code>model=kknn</code>.
</p>
</li>
<li> <p><code>multinom_t</code> - heuristic 167 from Delgado 2014 paper, 10 searches, works only when <code>model=multinom</code>.
</p>
</li></ul>

</td></tr>
<tr><td><code id="mparheuristic_+3A_lower">lower</code></td>
<td>
<p>lower bound for the (hyper)parameter (if <code>NA</code> a default value is assumed). If <code>n=1</code> and <code>!is.na(lower)</code> then lower is the main hyperparameter value for some models (e.g., &quot;kknn&quot;, &quot;mlpe&quot;, &quot;ksvm&quot;, &quot;xgboost&quot;).
</p>
</td></tr>
<tr><td><code id="mparheuristic_+3A_upper">upper</code></td>
<td>
<p>upper bound for the (hyper)parameter (if <code>NA</code> a default value is assumed).
</p>
</td></tr>
<tr><td><code id="mparheuristic_+3A_by">by</code></td>
<td>
<p>increment in the sequence (if <code>NA</code> a default value is assumed depending on <code>n</code>).
</p>
</td></tr>
<tr><td><code id="mparheuristic_+3A_exponential">exponential</code></td>
<td>
<p>if an exponential scale should be used in the search sequence (the <code>NA</code> is a default value that assumes a linear scale unless <code>model</code> is a support vector machine).
</p>
</td></tr>
<tr><td><code id="mparheuristic_+3A_kernel">kernel</code></td>
<td>
<p>optional kernel type, only used when <code>model="ksvm"</code>, <code>model="rsvm"</code> or <code>model="lssvm"</code>. Currently mapped kernels are <code>"rbfdot"</code> (Gaussian), 
<code>"polydot"</code> (polynomial) and <code>"vanilladot"</code> (linear); see <code><a href="kernlab.html#topic+ksvm">ksvm</a></code> for kernel details.
</p>
</td></tr>
<tr><td><code id="mparheuristic_+3A_task">task</code></td>
<td>
<p>optional task argument, only used for uniform design (<code>UD</code> or <code>UD1</code>) (with <code>"ksvm"</code> and <code>"rbfdot"</code>).
</p>
</td></tr>
<tr><td><code id="mparheuristic_+3A_inputs">inputs</code></td>
<td>
<p>optional inputs argument: the number of inputs, only used by <code>"randomForest"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function facilitates the definition of the <code>search</code> argument used by <code><a href="#topic+fit">fit</a></code> or <code><a href="#topic+mining">mining</a></code> functions.
Using simple heuristics, reasonable (hyper)parameter search values are suggested for several rminer models. For models not
mapped in this function, the function returns <code>NULL</code>, which means that no hyperparameter search is executed (often,
this implies using rminer or R function default values).
</p>
<p>The simple usage of <code>heuristic</code> assumes lower and upper bounds for a (hyper)parameter. If <code>n=1</code>, then rminer or R defaults are assumed.
Else, a search is created using <code>seq(lower,upper,by)</code>, where <code>by</code> was set by the used or computed from <code>n</code>.
For some <code>model="ksvm"</code> setups, <code>2^seq(...)</code> is used for sigma and C, <code>(1/10)^seq(...)</code> is used for scale. 
Please check the resulting object to inspect the obtained final search values.
</p>
<p>This function also allows to easily set multiple model searches, under the: &quot;automl&quot;, &quot;automl2&quot;, &quot;automl3&quot; or vector character options (see below examples).
</p>


<h3>Value</h3>

<p>A list with one ore more (hyper)parameter values to be searched.
</p>


<h3>Note</h3>

<p>See also <a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a> and <a href="http://www3.dsi.uminho.pt/pcortez/rminer.html">http://www3.dsi.uminho.pt/pcortez/rminer.html</a>
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>


<ul>
<li><p> To check for more details about rminer and for citation purposes:<br />
P. Cortez.<br />
Data Mining with Neural Networks and Support Vector Machines Using the R/rminer Tool.<br />
In P. Perner (Ed.), Advances in Data Mining - Applications and Theoretical Aspects 10th Industrial Conference on Data Mining (ICDM 2010), Lecture Notes in Artificial Intelligence 6171, pp. 572-583, Berlin, Germany, July, 2010. Springer. ISBN: 978-3-642-14399-1.<br />
@Springer: <a href="https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44">https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44</a><br />
<a href="http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf">http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf</a><br />
</p>
</li>
<li><p> The automl is inspired in this work:<br />
L. Ferreira, A. Pilastri, C. Martins, P. Santos, P. Cortez.<br />
An Automated and Distributed Machine Learning Framework for Telecommunications Risk Management.
In J. van den Herik et al. (Eds.), 
Proceedings of 12th International Conference on Agents and Artificial Intelligence &ndash; ICAART 2020, Volume 2, pp. 99-107,
Valletta, Malta, February, 2020, SCITEPRESS, ISBN 978-989-758-395-7.<br />
@INSTICC: <a href="https://www.insticc.org/Primoris/Resources/PaperPdf.ashx?idPaper=89528">https://www.insticc.org/Primoris/Resources/PaperPdf.ashx?idPaper=89528</a><br />
</p>
</li>
<li><p> This tutorial shows additional code examples:<br />
P. Cortez.<br />
A tutorial on using the rminer R package for data mining tasks.<br />
Teaching Report, Department of Information Systems, ALGORITMI Research Centre, Engineering School, University of Minho, Guimaraes, 
Portugal, July 2015.<br />
<a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a><br />
</p>
</li>
<li><p> Some lower/upper bounds and heuristics were retrieved from:<br />
M. Fernandez-Delgado, E. Cernadas, S. Barro and D. Amorim.
Do we need hundreds of classifiers to solve real world classification problems?. 
In The Journal of Machine Learning Research, 15(1), 3133-3181, 2014.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code> and <code><a href="#topic+mining">mining</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## "kknn"
s=mparheuristic("kknn",n="heuristic")
print(s) 
s=mparheuristic("kknn",n=1) # same thing
print(s) 
s=mparheuristic("kknn",n="heuristic5")
print(s) 
s=mparheuristic("kknn",n=5) # same thing
print(s)
s=mparheuristic("kknn",lower=5,upper=15,by=2)
print(s)
# exponential scale:
s=mparheuristic("kknn",lower=1,upper=5,by=1,exponential=2)
print(s)

## "mlpe"
s=mparheuristic("mlpe")
print(s) # "NA" means set size with min(inputs/2,10) in fit
s=mparheuristic("mlpe",n="heuristic10")
print(s) 
s=mparheuristic("mlpe",n=10) # same thing
print(s) 
s=mparheuristic("mlpe",n=10,lower=2,upper=20) 
print(s) 

# numeric (single number or vector) usage of n:
s=mparheuristic("mlpe",n=NA) #
print(s) 
s=mparheuristic("mlpe",n=1,lower=NA) # same thing
print(s) 
s=mparheuristic("mlpe",n=1,lower=2) # size=2
print(s) 
s=mparheuristic("mlpe",n=1:9) # size=1:9
print(s) 


## "randomForest", upper should be set to the number of inputs = max mtry
s=mparheuristic("randomForest",n=10,upper=6)
print(s) 

## "ksvm"
s=mparheuristic("ksvm",n=10)
print(s) 
s=mparheuristic("ksvm",n=10,kernel="vanilladot")
print(s) 
s=mparheuristic("ksvm",n=10,kernel="polydot")
print(s) 

## lssvm
s=mparheuristic("lssvm",n=10)
print(s) 

## rvm 
s=mparheuristic("rvm",n=5)
print(s) 
s=mparheuristic("rvm",n=5,kernel="vanilladot")
print(s) 

## "rpart" and "ctree" are special cases (see help(fit,package=rminer) examples):
s=mparheuristic("rpart",n=3) # 3 cp values
print(s) 
s=mparheuristic("ctree",n=3) # 3 mincriterion values
print(s) 

### examples with fit
## Not run: 
### classification
data(iris)
# ksvm and rbfdot:
model="ksvm";kernel="rbfdot"
s=mparheuristic(model,n="heuristic5",kernel=kernel)
print(s) # 5 sigma values
search=list(search=s,method=c("holdout",2/3,123))
# task "prob" is assumed, optimization of "AUC":
M=fit(Species~.,data=iris,model=model,search=search,fdebug=TRUE)
print(M@mpar)

# different lower and upper range:
s=mparheuristic(model,n=5,kernel=kernel,lower=-5,upper=1)
print(s) # from 2^-5 to 2^1 
search=list(search=s,method=c("holdout",2/3,123))
# task "prob" is assumed, optimization of "AUC":
M=fit(Species~.,data=iris,model=model,search=search,fdebug=TRUE)
print(M@mpar)

# different exponential scale: 
s=mparheuristic(model,n=5,kernel=kernel,lower=-4,upper=0,exponential=10)
print(s) # from 10^-5 to 10^1 
search=list(search=s,method=c("holdout",2/3,123))
# task "prob" is assumed, optimization of "AUC":
M=fit(Species~.,data=iris,model=model,search=search,fdebug=TRUE)
print(M@mpar)

# "lssvm" Gaussian model, pure classification and ACC optimization, full iris:
model="lssvm";kernel="rbfdot"
s=mparheuristic("lssvm",n=3,kernel=kernel)
print(s)
search=list(search=s,method=c("holdout",2/3,123))
M=fit(Species~.,data=iris,model=model,search=search,fdebug=TRUE)
print(M@mpar)

# test several heuristic5 searches, full iris:
n="heuristic5";inputs=ncol(iris)-1
model=c("ctree","rpart","kknn","ksvm","lssvm","mlpe","randomForest")
for(i in 1:length(model))
 {
  cat("--- i:",i,"model:",model[i],"\n")
  if(model[i]=="randomForest") s=mparheuristic(model[i],n=n,upper=inputs) 
  else s=mparheuristic(model[i],n=n)
  print(s)
  search=list(search=s,method=c("holdout",2/3,123))
  M=fit(Species~.,data=iris,model=model[i],search=search,fdebug=TRUE)
  print(M@mpar)
 }


# test several Delgado 2014 searches (some cases launch warnings):
model=c("mlp","mlpe","mlp","ksvm","ksvm","ksvm",
        "ksvm","lssvm","rpart","rpart","ctree",
        "ctree","randomForest","kknn","kknn","multinom")
n=c("mlp_t","avNNet_t","nnet_t","svm_C","svmRadial_t","svmLinear_t",
    "svmPoly_t","lsvmRadial_t","rpart_t","rpart2_t","ctree_t",
    "ctree2_t","rf_t","knn_R","knn_t","multinom_t")
inputs=ncol(iris)-1
for(i in 1:length(model))
 {
  cat("--- i:",i,"model:",model[i],"heuristic:",n[i],"\n")
  if(model[i]=="randomForest") s=mparheuristic(model[i],n=n[i],upper=inputs) 
  else s=mparheuristic(model[i],n=n[i])
  print(s)
  search=list(search=s,method=c("holdout",2/3,123))
  M=fit(Species~.,data=iris,model=model[i],search=search,fdebug=TRUE)
  print(M@mpar)
 }

## End(Not run) #dontrun

### regression
## Not run: 
data(sa_ssin)
s=mparheuristic("ksvm",n=3,kernel="polydot")
print(s)
search=list(search=s,metric="MAE",method=c("holdout",2/3,123))
M=fit(y~.,data=sa_ssin,model="ksvm",search=search,fdebug=TRUE)
print(M@mpar)

# regression task, predict iris "Petal.Width":
data(iris)
ir2=iris[,1:4]
names(ir2)[ncol(ir2)]="y" # change output name
n=3;inputs=ncol(ir2)-1 # 3 hyperparameter searches
model=c("ctree","rpart","kknn","ksvm","mlpe","randomForest","rvm")
for(i in 1:length(model))
 {
  cat("--- i:",i,"model:",model[i],"\n")
  if(model[i]=="randomForest") s=mparheuristic(model[i],n=n,upper=inputs)
  else s=mparheuristic(model[i],n=n)
  print(s)
  search=list(search=s,method=c("holdout",2/3,123))
  M=fit(y~.,data=ir2,model=model[i],search=search,fdebug=TRUE)
  print(M@mpar)
 }

## End(Not run) #dontrun

### multiple model examples:
## Not run: 
data(iris)
inputs=ncol(iris)-1; task="prob"

# 5 machine learning (ML) algorithms, 1 heuristic hyperparameter per algorithm:
sm=mparheuristic(model="automl",task=task,inputs=inputs)
print(sm)

# 5 ML with 10/13 hyperparameter searches:
sm=mparheuristic(model="automl2",task=task,inputs=inputs)
# note: mtry only has 4 searches due to the inputs limit:
print(sm)

# regression example:
ir2=iris[,1:4]
inputs=ncol(ir2)-1; task="reg"
sm=mparheuristic(model="automl2",task=task,inputs=inputs)
# note: ksvm contains 3 UD hyperparameters (and not 2) since task="reg": 
print(sm)

# 5 ML and stacking:
inputs=ncol(iris)-1; task="prob"
sm=mparheuristic(model="automl3",task=task,inputs=inputs)
# note: $ls only has 5 elements, one for each individual ML 
print(sm)

# other manual design examples: --------------------------------------

# 5 ML and three ensembles:
# the fit or mining functions will search for the best option
# between any of the 5 ML algorithms and any of the three 
# ensemble approaches:
sm2=mparheuristic(model="automl3",task=task,inputs=inputs)
# note: ensembles need to be at the end of the $models field:
sm2$models=c(sm2$models,"AE","WE") # add AE and WE
sm2$smethod=c(sm2$smethod,rep("grid",2)) # add grid to AE and WE
# note: $ls only has 5 elements, one for each individual ML 
print(sm2)

# 3 ML example:
models=c("cv.glmnet","mlpe","ksvm") # just 3 models
# note: in rminer the default cv.glmnet does not have "hyperparameters"
# since the cv automatically sets lambda 
n=c(NA,10,"UD") # 10 searches for mlpe and 13 for ksvm 
sm3=mparheuristic(model=models,n=n)
# note: $ls only has 5 elements, one for each individual ML 
print(sm3)

# usage in sm2 and sm3 for fit (see mining help for usages in mining):
method=c("holdout",2/3,123)
d=iris
names(d)[ncol(d)]="y" # change output name
s2=list(search=sm2,smethod="auto",method=method,metric="AUC",convex=0)
M2=fit(y~.,data=d,model="auto",search=s2,fdebug=TRUE)

s3=list(search=sm3,smethod="auto",method=method,metric="AUC",convex=0)
M3=fit(y~.,data=d,model="auto",search=s3,fdebug=TRUE)
# -------------------------------------------------------------------

## End(Not run)

</code></pre>

<hr>
<h2 id='predict.fit'>predict method for fit objects (rminer)</h2><span id='topic+predict-methods'></span><span id='topic+predict.fit'></span><span id='topic+predict+2Cmodel-method'></span>

<h3>Description</h3>

<p>predict method for fit objects (rminer)</p>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.fit_+3A_object">object</code></td>
<td>
<p>a model object created by <code><a href="#topic+fit">fit</a></code></p>
</td></tr>
<tr><td><code id="predict.fit_+3A_newdata">newdata</code></td>
<td>
<p>a data frame or matrix containing new data</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns predictions for a fit model. Note: the <code>...</code> optional argument is currently only used by <code>cubist</code> model (see example).
</p>


<h3>Value</h3>

<p>If <code>task</code> is <code>prob</code> returns a matrix, where each column is the class probability.<br />
If <code>task</code> is <code>class</code> returns a factor.<br />
If <code>task</code> is <code>reg</code> returns a numeric vector.<br />
</p>


<h3>Methods</h3>


<dl>
<dt><code>signature(object = "model")</code></dt><dd><p> describe this method here </p>
</dd>
</dl>


<h3>References</h3>


<ul>
<li><p> To check for more details about rminer and for citation purposes:<br />
P. Cortez.<br />
Data Mining with Neural Networks and Support Vector Machines Using the R/rminer Tool.<br />
In P. Perner (Ed.), Advances in Data Mining - Applications and Theoretical Aspects 10th Industrial Conference on Data Mining (ICDM 2010), Lecture Notes in Artificial Intelligence 6171, pp. 572-583, Berlin, Germany, July, 2010. Springer. ISBN: 978-3-642-14399-1.<br />
@Springer: <a href="https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44">https://link.springer.com/chapter/10.1007/978-3-642-14400-4_44</a><br />
<a href="http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf">http://www3.dsi.uminho.pt/pcortez/2010-rminer.pdf</a><br />
</p>
</li>
<li><p> This tutorial shows additional code examples:<br />
P. Cortez.<br />
A tutorial on using the rminer R package for data mining tasks.<br />
Teaching Report, Department of Information Systems, ALGORITMI Research Centre, Engineering School, University of Minho, Guimaraes, 
Portugal, July 2015.<br />
<a href="http://hdl.handle.net/1822/36210">http://hdl.handle.net/1822/36210</a><br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code>, <code><a href="#topic+mining">mining</a></code>, <code><a href="#topic+mgraph">mgraph</a></code>, <code><a href="#topic+mmetric">mmetric</a></code>, <code><a href="#topic+savemining">savemining</a></code>, <code><a href="#topic+CasesSeries">CasesSeries</a></code>, <code><a href="#topic+lforecast">lforecast</a></code> and <code><a href="#topic+Importance">Importance</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### simple classification example with logistic regression
data(iris)
M=fit(Species~.,iris,model="lr")
P=predict(M,iris)
print(mmetric(iris$Species,P,"CONF")) # confusion matrix

### simple regression example
data(sa_ssin)
H=holdout(sa_ssin$y,ratio=0.5,seed=12345)
Y=sa_ssin[H$ts,]$y # desired test set
# fit multiple regression on training data (half of samples)
M=fit(y~.,sa_ssin[H$tr,],model="mr") # multiple regression
P1=predict(M,sa_ssin[H$ts,]) # predictions on test set
print(mmetric(Y,P1,"MAE")) # mean absolute error

### fit cubist model
M=fit(y~.,sa_ssin[H$tr,],model="cubist") #
P2=predict(M,sa_ssin[H$ts,],neighbors=3) #
print(mmetric(Y,P2,"MAE")) # mean absolute error
P3=predict(M,sa_ssin[H$ts,],neighbors=7) #
print(mmetric(Y,P3,"MAE")) # mean absolute error

### check fit for more examples
</code></pre>

<hr>
<h2 id='rminer-internal'>Internal rminer Functions</h2><span id='topic+AAD_responses'></span><span id='topic+addSearch'></span><span id='topic+aggregate_imp'></span><span id='topic+agg_matrix_imp'></span><span id='topic+balanced_responses'></span><span id='topic+cmatrixplot'></span><span id='topic+datalevels'></span><span id='topic+INTERPOLATE'></span><span id='topic+LIFTcurve'></span><span id='topic+MCrandom'></span><span id='topic+mids'></span><span id='topic+RECcurve'></span><span id='topic+ROCcurve'></span><span id='topic+ranges'></span><span id='topic+rmboxplot'></span><span id='topic+rmsample'></span><span id='topic+rmtable'></span><span id='topic+s_measure'></span><span id='topic+TPR_FOR_FPR'></span><span id='topic+addfactor'></span><span id='topic+attcorrelated'></span><span id='topic+avg_imp'></span><span id='topic+avg_imp_1D'></span><span id='topic+bestfit'></span><span id='topic+bssearch'></span><span id='topic+centralaux'></span><span id='topic+conflevel'></span><span id='topic+Conf'></span><span id='topic+crossfolds'></span><span id='topic+curvearea'></span><span id='topic+defaultask'></span><span id='topic+defaultfeature'></span><span id='topic+defaultmodel'></span><span id='topic+defaultmpar'></span><span id='topic+defaultsearch'></span><span id='topic+dforder'></span><span id='topic+dlplot'></span><span id='topic+enlarge'></span><span id='topic+factorize'></span><span id='topic+factor2numeric'></span><span id='topic+factorsample'></span><span id='topic+feature_needed'></span><span id='topic+fenlarge'></span><span id='topic+filter_equal'></span><span id='topic+forplot'></span><span id='topic+getmetric'></span><span id='topic+gradient_responses'></span><span id='topic+mgrid'></span><span id='topic+hotdeck'></span><span id='topic+impvalue'></span><span id='topic+invtransform'></span><span id='topic+isbest'></span><span id='topic+is.mmetric'></span><span id='topic+knn.fit'></span><span id='topic+majorClass'></span><span id='topic+mean_resp'></span><span id='topic+meanint'></span><span id='topic+medianfirst'></span><span id='topic+mhistogram'></span><span id='topic+middleclass'></span><span id='topic+midrangesearch'></span><span id='topic+missingatts'></span><span id='topic+mlp.fit'></span><span id='topic+modelargs'></span><span id='topic+modelplot'></span><span id='topic+mostcommon'></span><span id='topic+mpause'></span><span id='topic+one_of_c'></span><span id='topic+output_index'></span><span id='topic+partialcurve'></span><span id='topic+pathlength_responses'></span><span id='topic+plotH'></span><span id='topic+range_responses'></span><span id='topic+readmethod'></span><span id='topic+readsearch'></span><span id='topic+resp_to_list'></span><span id='topic+scaleinputs'></span><span id='topic+scaleinputs2'></span><span id='topic+svm.fit'></span><span id='topic+transform_needed'></span><span id='topic+trap_area'></span><span id='topic+tsacf'></span><span id='topic+tsplot'></span><span id='topic+twoclassLift'></span><span id='topic+twoclassROC'></span><span id='topic+uniform_design'></span><span id='topic+variance_responses'></span><span id='topic+vaveraging'></span><span id='topic+worst'></span><span id='topic+xmiddle_point'></span><span id='topic+xtransform'></span><span id='topic+yaggregate'></span>

<h3>Description</h3>

<p>Internal rminer functions
</p>


<h3>Details</h3>

<p>These are not to be called by the user (or in some cases are just
waiting for proper documentation to be written :).
</p>

<hr>
<h2 id='sa_fri1'>
Synthetic regression and classification datasets for measuring input importance of supervised learning models
</h2><span id='topic+sa_fri1'></span><span id='topic+sa_ssin'></span><span id='topic+sa_psin'></span><span id='topic+sa_int2'></span><span id='topic+sa_tree'></span><span id='topic+sa_ssin_2'></span><span id='topic+sa_ssin_n2p'></span><span id='topic+sa_int2_3c'></span><span id='topic+sa_int2_8p'></span>

<h3>Description</h3>

<p>5 Synthetic regression (sa_fri1, sa_ssin, sa_psin, sa_int2, sa_tree) and 4 classification (sa_ssin_2, sa_ssin_n2p, sa_int2_3c, sa_int2_8p) datasets for measuring input importance of supervised learning models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sa_fri1)</code></pre>


<h3>Format</h3>

<p>A data frame with 1000 observations on the following variables.
</p>

<dl>
<dt><code>x</code>n</dt><dd><p>input (numeric or factor, depends on the dataset)</p>
</dd>
<dt><code>y</code></dt><dd><p>output target (numeric or factor, depends on the dataset)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Check reference or source for full details
</p>


<h3>Source</h3>

<p>See references</p>


<h3>References</h3>


<ul>
<li><p> To cite the Importance function, sensitivity analysis methods or synthetic datasets, please use:<br />
P. Cortez and M.J. Embrechts.<br />
Using Sensitivity Analysis and Visualization Techniques to Open Black Box Data Mining Models.<br />
In Information Sciences, Elsevier, 225:1-17, March 2013.<br />
<a href="https://doi.org/10.1016/j.ins.2012.10.039">doi:10.1016/j.ins.2012.10.039</a><br />
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(sa_ssin)
print(summary(sa_ssin))
## Not run: plot(sa_ssin$x1,sa_ssin$y)
</code></pre>

<hr>
<h2 id='savemining'>
Load/save into a file the result of a fit (model) or mining functions.
</h2><span id='topic+savemining'></span><span id='topic+savemodel'></span><span id='topic+loadmining'></span><span id='topic+loadmodel'></span>

<h3>Description</h3>

<p>Load/save into a file the result of a <code><a href="#topic+fit">fit</a></code> (model) or <code><a href="#topic+mining">mining</a></code> functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>savemining(mmm_mining, file, ascii = TRUE)</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="savemining_+3A_mmm_mining">mmm_mining</code></td>
<td>
<p>the list object that is returned by the <code><a href="#topic+mining">mining</a></code> function.</p>
</td></tr>
<tr><td><code id="savemining_+3A_file">file</code></td>
<td>
<p>filename that should include an extension</p>
</td></tr>
<tr><td><code id="savemining_+3A_ascii">ascii</code></td>
<td>
<p>if <code>TRUE</code> then ascii format is used to store the file (larger file size), else a binary format is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Very simple functions that do what their names say. Additional usages are:<br />
<code>loadmining(file)</code><br />
<code>savemodel(MM_model,file,ascii=FALSE)</code><br />
<code>loadmodel(file)</code><br />
</p>


<h3>Value</h3>

<p><code>loadmining</code> returns a <code><a href="#topic+mining">mining</a></code> mining list, while <code>loadmodel</code> returns a <code>model</code> object (from <code><a href="#topic+fit">fit</a></code>).
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>

<p>See <code><a href="#topic+fit">fit</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit">fit</a></code>, <code><a href="#topic+predict.fit">predict.fit</a></code>, <code><a href="#topic+mining">mining</a></code>, <code><a href="#topic+mgraph">mgraph</a></code>, <code><a href="#topic+mmetric">mmetric</a></code>, <code><a href="#topic+savemining">savemining</a></code>, <code><a href="#topic+Importance">Importance</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### dontrun is used here to avoid the creation of a new file
### in the CRAN servers. The example should work fine:
## Not run: 
data(iris)
M=fit(Species~.,iris,model="rpart")
tempdirpath=tempdir()
filename=paste(tempdirpath,"/iris.model",sep="")
savemodel(M,filename) # saves to file
M=NULL # cleans M
M=loadmodel(filename) # load from file
print(M)

## End(Not run)
</code></pre>

<hr>
<h2 id='sin1reg'>sin1 regression dataset</h2><span id='topic+sin1reg'></span>

<h3>Description</h3>

<p>Simple synthetic dataset with 1000 points, where y=0.7*sin(pi*x1/2000)+0.3*sin(pi*x2/2000)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sin1reg)</code></pre>


<h3>Format</h3>

<p>The format is:
chr &quot;sin1reg&quot;
</p>


<h3>Details</h3>

<p>Simple synthetic dataset with 1000 points, where y=0.7*sin(pi*x1/2000)+0.3*sin(pi*x2/2000)</p>


<h3>Source</h3>

<p>See references</p>


<h3>References</h3>


<ul>
<li><p> To cite the Importance function, sensitivity analysis methods or synthetic datasets, please use:<br />
P. Cortez and M.J. Embrechts.<br />
Using Sensitivity Analysis and Visualization Techniques to Open Black Box Data Mining Models.<br />
In Information Sciences, Elsevier, 225:1-17, March 2013.<br />
<a href="https://doi.org/10.1016/j.ins.2012.10.039">doi:10.1016/j.ins.2012.10.039</a><br />
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(sin1reg)
print(summary(sin1reg))
</code></pre>

<hr>
<h2 id='vecplot'>
VEC plot function (to use in conjunction with Importance function).
</h2><span id='topic+vecplot'></span>

<h3>Description</h3>

<p>VEC plot function (to use in conjunction with Importance function).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vecplot(I, graph = "VEC", leg = NULL, xval = 1, sort = FALSE, data = NULL, 
digits = c(1, 1), TC = 1, intbar = NULL, lty = 1, pch = 19, col = NULL, 
datacol = NULL, main = "", main2 = "", Grid = 0, 
xlab = "", ylab = "", zlab = "", 
levels = NULL, levels2 = NULL, showlevels = FALSE, 
screen = list(z = 40, x = -60), zoom = 1, cex = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vecplot_+3A_i">I</code></td>
<td>
<p>the output list of the <code><a href="#topic+Importance">Importance</a></code> function.
</p>
</td></tr>
<tr><td><code id="vecplot_+3A_graph">graph</code></td>
<td>
<p>type of VEC graph:
</p>

<ul>
<li> <p><code>VEC</code> &ndash; 1-D VEC curve; 
</p>
</li>
<li> <p><code>VECB</code> &ndash; 1-D VEC curve with box plots (only valid for SA methods: &quot;DSA&quot;, &quot;MSA&quot;);
</p>
</li>
<li> <p><code>VEC3</code> &ndash; 2-D VEC surface;
</p>
</li>
<li> <p><code>VECC</code> &ndash; 2-D VEC contour; 
</p>
</li></ul>

</td></tr>
<tr><td><code id="vecplot_+3A_leg">leg</code></td>
<td>
<p>see <code><a href="#topic+mgraph">mgraph</a></code></p>
</td></tr>
<tr><td><code id="vecplot_+3A_xval">xval</code></td>
<td>
<p>the attribute input index (e.g. 1), only used if <code>graph="VEC"</code> or (<code>graph="VEC3" or "VECC"</code> 
and <code>length(interactions)=1</code>, see <code><a href="#topic+Importance">Importance</a></code>). if a vector, then several VEC curves are plotted (in this case, x-axis is scaled).</p>
</td></tr>
<tr><td><code id="vecplot_+3A_sort">sort</code></td>
<td>
<p>if factor inputs are sorted:
</p>

<ul>
<li> <p><code>increasing</code> &ndash; sorts the first attribute (if factor) according to the response values, increasing order; 
</p>
</li>
<li> <p><code>decreasing</code> &ndash; similar to <code>increasing</code> but uses reverse order; 
</p>
</li>
<li> <p><code>TRUE</code> &ndash; similar to <code>increasing</code>; 
</p>
</li>
<li> <p><code>increasing2</code> &ndash; sorts the second attribute (for <code>graph="VEC3" or "VECC"</code>, if factor, according to the response values), increasing order; 
</p>
</li>
<li> <p><code>decreasing2</code> &ndash; similar to <code>increasing2</code> but uses reverse order; 
</p>
</li>
<li> <p><code>FALSE</code> &ndash; no sort is used; 
</p>
</li></ul>

</td></tr>
<tr><td><code id="vecplot_+3A_data">data</code></td>
<td>
<p>see <code><a href="#topic+mgraph">mgraph</a></code></p>
</td></tr>
<tr><td><code id="vecplot_+3A_digits">digits</code></td>
<td>
<p>see <code><a href="#topic+mgraph">mgraph</a></code></p>
</td></tr>
<tr><td><code id="vecplot_+3A_tc">TC</code></td>
<td>
<p>see <code><a href="#topic+mgraph">mgraph</a></code></p>
</td></tr>
<tr><td><code id="vecplot_+3A_intbar">intbar</code></td>
<td>
<p>see <code><a href="#topic+mgraph">mgraph</a></code></p>
</td></tr>
<tr><td><code id="vecplot_+3A_lty">lty</code></td>
<td>
<p>see <code><a href="#topic+mgraph">mgraph</a></code></p>
</td></tr>
<tr><td><code id="vecplot_+3A_pch">pch</code></td>
<td>
<p>point type for the <code>graph="VEC"</code> curve, can be a vector if there are several VEC curve plots</p>
</td></tr>
<tr><td><code id="vecplot_+3A_col">col</code></td>
<td>
<p>color (e.g. &quot;black&quot;, &quot;grayrange&quot;, &quot;white&quot;)</p>
</td></tr>
<tr><td><code id="vecplot_+3A_datacol">datacol</code></td>
<td>
<p>color of the data histogram for <code>graph="VEC"</code></p>
</td></tr>
<tr><td><code id="vecplot_+3A_main">main</code></td>
<td>
<p>see <code><a href="#topic+mgraph">mgraph</a></code></p>
</td></tr>
<tr><td><code id="vecplot_+3A_main2">main2</code></td>
<td>
<p>key title for <code>graph="VECC"</code></p>
</td></tr>
<tr><td><code id="vecplot_+3A_grid">Grid</code></td>
<td>
<p>see <code><a href="#topic+mgraph">mgraph</a></code></p>
</td></tr>
<tr><td><code id="vecplot_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="vecplot_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label</p>
</td></tr>
<tr><td><code id="vecplot_+3A_zlab">zlab</code></td>
<td>
<p>z-axis label</p>
</td></tr>
<tr><td><code id="vecplot_+3A_levels">levels</code></td>
<td>
<p>if x1 is factor you can choose the order of the levels to this argument</p>
</td></tr>
<tr><td><code id="vecplot_+3A_levels2">levels2</code></td>
<td>
<p>if x2 is factor you can choose the order of the levels to this argument</p>
</td></tr>
<tr><td><code id="vecplot_+3A_showlevels">showlevels</code></td>
<td>
<p>if you want to show the factor levels in x1 or x2 axis in <code>graph="VEC3"</code>:
</p>

<ul>
<li> <p><code>FALSE</code> or <code>TRUE</code> &ndash; do not (do) show the levels in x1, x2 and z axis for factor variables;
</p>
</li>
<li><p> vector with 3 logical values &ndash; if you want to show the levels in each of the x1, x2 or z axis for factor variables (e.g. <code>c(FALSE,FALSE,TRUE)</code> only shows for z-axis).
</p>
</li></ul>

</td></tr>
<tr><td><code id="vecplot_+3A_screen">screen</code></td>
<td>
<p>select the perspective angle of the <code>VEC3</code> graph:
</p>

<ul>
<li> <p><code>x</code> &ndash; assumes <code>list(z=0,x=-90,y=0)</code>;
</p>
</li>
<li> <p><code>X</code> &ndash; assumes <code>list(x=-75)</code>;
</p>
</li>
<li> <p><code>y</code> &ndash; assumes <code>list(z=0,x=-90,y=-90)</code>;
</p>
</li>
<li> <p><code>Y</code> &ndash; assumes <code>list(z=10,x=-90,y=-90)</code>;
</p>
</li>
<li> <p><code>z</code> &ndash; assumes <code>list(z=0,x=0,y=0)</code>;
</p>
</li>
<li> <p><code>xy</code> &ndash; assumes <code>list(z=10,x=-90,y=-45)</code>;
</p>
</li>
<li><p> else you need to specify a list with z, x an y angles, see <code><a href="lattice.html#topic+wireframe">wireframe</a></code>
</p>
</li></ul>

</td></tr>
<tr><td><code id="vecplot_+3A_zoom">zoom</code></td>
<td>
<p>zoom of the wireframe (<code>graph="VEC3"</code>)</p>
</td></tr>
<tr><td><code id="vecplot_+3A_cex">cex</code></td>
<td>
<p>label font size</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For examples and references check: <code><a href="#topic+Importance">Importance</a></code>
</p>


<h3>Value</h3>

<p>A VEC curve/surface/contour plot.
</p>


<h3>Author(s)</h3>

<p>Paulo Cortez <a href="http://www3.dsi.uminho.pt/pcortez/">http://www3.dsi.uminho.pt/pcortez/</a>
</p>


<h3>References</h3>


<ul>
<li><p> To cite the Importance function or sensitivity analysis method, please use:<br />
<br />
P. Cortez and M.J. Embrechts.<br />
Using Sensitivity Analysis and Visualization Techniques to Open Black Box Data Mining Models.<br />
In Information Sciences, Elsevier, 225:1-17, March 2013.<br />
<br />
<a href="https://doi.org/10.1016/j.ins.2012.10.039">doi:10.1016/j.ins.2012.10.039</a><br />
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+Importance">Importance</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
