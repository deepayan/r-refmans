<!DOCTYPE html><html><head><title>Help for package dbscan</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dbscan}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dbscan-package'><p>dbscan: Density-Based Spatial Clustering of Applications with Noise (DBSCAN)</p>
and Related Algorithms</a></li>
<li><a href='#comps'><p>Find Connected Components in a Nearest-neighbor Graph</p></a></li>
<li><a href='#dbscan'><p>Density-based Spatial Clustering of Applications with Noise (DBSCAN)</p></a></li>
<li><a href='#dendrogram'><p>Coersions to Dendrogram</p></a></li>
<li><a href='#DS3'><p>DS3: Spatial data with arbitrary shapes</p></a></li>
<li><a href='#extractFOSC'><p>Framework for the Optimal Extraction of Clusters from Hierarchies</p></a></li>
<li><a href='#frNN'><p>Find the Fixed Radius Nearest Neighbors</p></a></li>
<li><a href='#glosh'><p>Global-Local Outlier Score from Hierarchies</p></a></li>
<li><a href='#hdbscan'><p>Hierarchical DBSCAN (HDBSCAN)</p></a></li>
<li><a href='#hullplot'><p>Plot Convex Hulls of Clusters</p></a></li>
<li><a href='#jpclust'><p>Jarvis-Patrick Clustering</p></a></li>
<li><a href='#kNN'><p>Find the k Nearest Neighbors</p></a></li>
<li><a href='#kNNdist'><p>Calculate and Plot k-Nearest Neighbor Distances</p></a></li>
<li><a href='#lof'><p>Local Outlier Factor Score</p></a></li>
<li><a href='#moons'><p>Moons Data</p></a></li>
<li><a href='#NN'><p>NN &mdash; Nearest Neighbors Superclass</p></a></li>
<li><a href='#optics'><p>Ordering Points to Identify the Clustering Structure (OPTICS)</p></a></li>
<li><a href='#pointdensity'><p>Calculate Local Density at Each Data Point</p></a></li>
<li><a href='#reachability'><p>Reachability Distances</p></a></li>
<li><a href='#sNN'><p>Find Shared Nearest Neighbors</p></a></li>
<li><a href='#sNNclust'><p>Shared Nearest Neighbor Clustering</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.1-12</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-28</td>
</tr>
<tr>
<td>Title:</td>
<td>Density-Based Spatial Clustering of Applications with Noise
(DBSCAN) and Related Algorithms</td>
</tr>
<tr>
<td>Description:</td>
<td>A fast reimplementation of several density-based algorithms of
    the DBSCAN family. Includes the clustering algorithms DBSCAN (density-based 
    spatial clustering of applications with noise) and HDBSCAN (hierarchical 
    DBSCAN), the ordering algorithm OPTICS (ordering points to identify the 
    clustering structure), shared nearest neighbor clustering, and the outlier 
    detection algorithms LOF (local outlier factor) and GLOSH (global-local 
    outlier score from hierarchies). The implementations use the kd-tree data 
    structure (from library ANN) for faster k-nearest neighbor search. An R 
    interface to fast kNN and fixed-radius NN search is also provided. 
    Hahsler, Piekenbrock and Doran (2019) &lt;<a href="https://doi.org/10.18637%2Fjss.v091.i01">doi:10.18637/jss.v091.i01</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.0), graphics, stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>fpc, microbenchmark, testthat, dendextend, igraph, knitr,
rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mhahsler/dbscan">https://github.com/mhahsler/dbscan</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mhahsler/dbscan/issues">https://github.com/mhahsler/dbscan/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Copyright:</td>
<td>ANN library is copyright by University of Maryland, Sunil
Arya and David Mount. All other code is copyright by Michael
Hahsler and Matthew Piekenbrock.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-28 14:45:10 UTC; hahsler</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Hahsler [aut, cre, cph],
  Matthew Piekenbrock [aut, cph],
  Sunil Arya [ctb, cph],
  David Mount [ctb, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Hahsler &lt;mhahsler@lyle.smu.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-28 17:10:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='dbscan-package'>dbscan: Density-Based Spatial Clustering of Applications with Noise (DBSCAN)
and Related Algorithms</h2><span id='topic+dbscan-package'></span>

<h3>Description</h3>

<p>A fast reimplementation of several density-based algorithms of
the DBSCAN family. Includes the clustering algorithms DBSCAN (density-based
spatial clustering of applications with noise) and HDBSCAN (hierarchical
DBSCAN), the ordering algorithm OPTICS (ordering points to identify the
clustering structure), shared nearest neighbor clustering, and the outlier
detection algorithms LOF (local outlier factor) and GLOSH (global-local
outlier score from hierarchies). The implementations use the kd-tree data
structure (from library ANN) for faster k-nearest neighbor search. An R
interface to fast kNN and fixed-radius NN search is also provided.
Hahsler, Piekenbrock and Doran (2019) .
</p>


<h3>Key functions</h3>


<ul>
<li><p> Clustering: <code><a href="#topic+dbscan">dbscan()</a></code>, <code><a href="#topic+hdbscan">hdbscan()</a></code>, <code><a href="#topic+optics">optics()</a></code>, <code><a href="#topic+jpclust">jpclust()</a></code>, <code><a href="#topic+sNNclust">sNNclust()</a></code>
</p>
</li>
<li><p> Outliers: <code><a href="#topic+lof">lof()</a></code>, <code><a href="#topic+glosh">glosh()</a></code>, <code><a href="#topic+pointdensity">pointdensity()</a></code>
</p>
</li>
<li><p> Nearest Neighbors: <code><a href="#topic+kNN">kNN()</a></code>, <code><a href="#topic+frNN">frNN()</a></code>, <code><a href="#topic+sNN">sNN()</a></code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michael Hahsler and Matthew Piekenbrock
</p>


<h3>References</h3>

<p>Hahsler M, Piekenbrock M, Doran D (2019). dbscan: Fast Density-Based Clustering with R. Journal of Statistical Software, 91(1), 1-30. <a href="https://doi.org/10.18637/jss.v091.i01">doi:10.18637/jss.v091.i01</a>
</p>

<hr>
<h2 id='comps'>Find Connected Components in a Nearest-neighbor Graph</h2><span id='topic+comps'></span><span id='topic+components'></span><span id='topic+comps.dist'></span><span id='topic+comps.kNN'></span><span id='topic+comps.sNN'></span><span id='topic+comps.frNN'></span>

<h3>Description</h3>

<p>Generic function and methods to find connected components in nearest neighbor graphs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comps(x, ...)

## S3 method for class 'dist'
comps(x, eps, ...)

## S3 method for class 'kNN'
comps(x, mutual = FALSE, ...)

## S3 method for class 'sNN'
comps(x, ...)

## S3 method for class 'frNN'
comps(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="comps_+3A_x">x</code></td>
<td>
<p>the <a href="#topic+NN">NN</a> object representing the graph or a <a href="stats.html#topic+dist">dist</a> object</p>
</td></tr>
<tr><td><code id="comps_+3A_...">...</code></td>
<td>
<p>further arguments are currently unused.</p>
</td></tr>
<tr><td><code id="comps_+3A_eps">eps</code></td>
<td>
<p>threshold on the distance</p>
</td></tr>
<tr><td><code id="comps_+3A_mutual">mutual</code></td>
<td>
<p>for a pair of points, do both have to be in each other's neighborhood?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that for kNN graphs, one point may be in the kNN of the other but nor vice versa.
<code>mutual = TRUE</code> requires that both points are in each other's kNN.
</p>


<h3>Value</h3>

<p>a integer vector with component assignments.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other NN functions: 
<code><a href="#topic+NN">NN</a></code>,
<code><a href="#topic+frNN">frNN</a>()</code>,
<code><a href="#topic+kNNdist">kNNdist</a>()</code>,
<code><a href="#topic+kNN">kNN</a>()</code>,
<code><a href="#topic+sNN">sNN</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(665544)
n &lt;- 100
x &lt;- cbind(
  x=runif(10, 0, 5) + rnorm(n, sd = 0.4),
  y=runif(10, 0, 5) + rnorm(n, sd = 0.4)
  )
plot(x, pch = 16)

# Connected components on a graph where each pair of points
# with a distance less or equal to eps are connected
d &lt;- dist(x)
components &lt;- comps(d, eps = .8)
plot(x, col = components, pch = 16)

# Connected components in a fixed radius nearest neighbor graph
# Gives the same result as the threshold on the distances above
frnn &lt;- frNN(x, eps = .8)
components &lt;- comps(frnn)
plot(frnn, data = x, col = components)

# Connected components on a k nearest neighbors graph
knn &lt;- kNN(x, 3)
components &lt;- comps(knn, mutual = FALSE)
plot(knn, data = x, col = components)

components &lt;- comps(knn, mutual = TRUE)
plot(knn, data = x, col = components)

# Connected components in a shared nearest neighbor graph
snn &lt;- sNN(x, k = 10, kt = 5)
components &lt;- comps(snn)
plot(snn, data = x, col = components)
</code></pre>

<hr>
<h2 id='dbscan'>Density-based Spatial Clustering of Applications with Noise (DBSCAN)</h2><span id='topic+dbscan'></span><span id='topic+DBSCAN'></span><span id='topic+print.dbscan_fast'></span><span id='topic+is.corepoint'></span><span id='topic+predict.dbscan_fast'></span>

<h3>Description</h3>

<p>Fast reimplementation of the DBSCAN (Density-based spatial clustering of
applications with noise) clustering algorithm using a kd-tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dbscan(x, eps, minPts = 5, weights = NULL, borderPoints = TRUE, ...)

is.corepoint(x, eps, minPts = 5, ...)

## S3 method for class 'dbscan_fast'
predict(object, newdata, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dbscan_+3A_x">x</code></td>
<td>
<p>a data matrix, a data.frame, a <a href="stats.html#topic+dist">dist</a> object or a <a href="#topic+frNN">frNN</a> object with
fixed-radius nearest neighbors.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_eps">eps</code></td>
<td>
<p>size (radius) of the epsilon neighborhood. Can be omitted if
<code>x</code> is a frNN object.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_minpts">minPts</code></td>
<td>
<p>number of minimum points required in the eps neighborhood for
core points (including the point itself).</p>
</td></tr>
<tr><td><code id="dbscan_+3A_weights">weights</code></td>
<td>
<p>numeric; weights for the data points. Only needed to perform
weighted clustering.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_borderpoints">borderPoints</code></td>
<td>
<p>logical; should border points be assigned to clusters.
The default is <code>TRUE</code> for regular DBSCAN. If <code>FALSE</code> then border
points are considered noise (see DBSCAN* in Campello et al, 2013).</p>
</td></tr>
<tr><td><code id="dbscan_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on to the fixed-radius nearest
neighbor search algorithm. See <code><a href="#topic+frNN">frNN()</a></code> for details on how to
control the search strategy.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_object">object</code></td>
<td>
<p>clustering object.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_newdata">newdata</code></td>
<td>
<p>new data points for which the cluster membership should be
predicted.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_data">data</code></td>
<td>
<p>the data set used to create the clustering object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The
implementation is significantly faster and can work with larger data sets
than <code><a href="fpc.html#topic+dbscan">fpc::dbscan()</a></code> in <span class="pkg">fpc</span>. Use <code>dbscan::dbscan()</code> (with specifying the package) to
call this implementation when you also load package <span class="pkg">fpc</span>.
</p>
<p><strong>The algorithm</strong>
</p>
<p>This implementation of DBSCAN follows the original
algorithm as described by Ester et al (1996). DBSCAN performs the following steps:
</p>

<ol>
<li><p> Estimate the density
around each data point by counting the number of points in a user-specified
eps-neighborhood and applies a used-specified minPts thresholds to identify
core, border and noise points.
</p>
</li>
<li><p> Core points are joined into
a cluster if they are density-reachable (i.e., there is a chain of core
points where one falls inside the eps-neighborhood of the next).
</p>
</li>
<li><p> Border points are assigned to clusters. The algorithm needs parameters
<code>eps</code> (the radius of the epsilon neighborhood) and <code>minPts</code> (the
density threshold).
</p>
</li></ol>

<p>Border points are arbitrarily assigned to clusters in the original
algorithm. DBSCAN* (see Campello et al 2013) treats all border points as
noise points. This is implemented with <code>borderPoints = FALSE</code>.
</p>
<p><strong>Specifying the data</strong>
</p>
<p>If <code>x</code> is a matrix or a data.frame, then fast fixed-radius nearest
neighbor computation using a kd-tree is performed using Euclidean distance.
See <code><a href="#topic+frNN">frNN()</a></code> for more information on the parameters related to
nearest neighbor search. <strong>Note</strong> that only numerical values are allowed in <code>x</code>.
</p>
<p>Any precomputed distance matrix (dist object) can be specified as <code>x</code>.
You may run into memory issues since distance matrices are large.
</p>
<p>A precomputed frNN object can be supplied as <code>x</code>. In this case
<code>eps</code> does not need to be specified. This option us useful for large
data sets, where a sparse distance matrix is available. See
<code><a href="#topic+frNN">frNN()</a></code> how to create frNN objects.
</p>
<p><strong>Setting parameters for DBSCAN</strong>
</p>
<p>The parameters <code>minPts</code> and <code>eps</code> depend on each other and
changing one typically requires changing the other one as well. The original
DBSCAN paper suggests to start by setting <code>minPts</code> to the
dimensionality of the data plus one or higher. <code>minPts</code> defines the
minimum density around a core point (i.e., the minimum density for non-noise
areas). Increase the parameter to suppress more noise in the data and
require more points to form a cluster. A suitable neighborhood size
parameter <code>eps</code> given a fixed value for <code>minPts</code> can be found
visually by inspecting the <code><a href="#topic+kNNdistplot">kNNdistplot()</a></code> of the data using
<code>k = minPts - 1</code> (<code>minPts</code> includes the point itself, while the
k-nearest neighbors distance does not). The k-nearest neighbor distance plot
sorts all data points by their k-nearest neighbor distance. A sudden
increase of the kNN distance (a knee) indicates that the points to the right
are most likely outliers. Choose <code>eps</code> for DBSCAN where the knee is.
</p>
<p><strong>Predict cluster memberships</strong>
</p>
<p><code><a href="stats.html#topic+predict">predict()</a></code> can be used to predict cluster memberships for new data
points. A point is considered a member of a cluster if it is within the eps
neighborhood of a core point of the cluster. Points
which cannot be assigned to a cluster will be reported as
noise points (i.e., cluster ID 0).
<strong>Important note:</strong> <code>predict()</code> currently can only use Euclidean distance to determine
the neighborhood of core points. If <code>dbscan()</code> was called using distances other than Euclidean,
then the neighborhood calculation will not be correct and only approximated by Euclidean
distances. If the data contain factor columns (e.g., using Gower's distance), then
the factors in <code>data</code> and <code>query</code> first need to be converted to numeric to use the
Euclidean approximation.
</p>


<h3>Value</h3>

<p><code>dbscan()</code> returns an object of class <code>dbscan_fast</code> with the following components:
</p>
<table>
<tr><td><code>eps</code></td>
<td>
<p> value of the <code>eps</code> parameter.</p>
</td></tr>
<tr><td><code>minPts</code></td>
<td>
<p> value of the <code>minPts</code> parameter.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>A integer vector with cluster assignments. Zero indicates noise points.</p>
</td></tr>
</table>
<p><code>is.corepoint()</code> returns a logical vector indicating for each data point if it is a
core point.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Hahsler M, Piekenbrock M, Doran D (2019). dbscan: Fast
Density-Based Clustering with R.  <em>Journal of Statistical Software,</em>
91(1), 1-30.
<a href="https://doi.org/10.18637/jss.v091.i01">doi:10.18637/jss.v091.i01</a>
</p>
<p>Martin Ester, Hans-Peter Kriegel, Joerg Sander, Xiaowei Xu (1996). A
Density-Based Algorithm for Discovering Clusters in Large Spatial Databases
with Noise. Institute for Computer Science, University of Munich.
<em>Proceedings of 2nd International Conference on Knowledge Discovery and
Data Mining (KDD-96),</em> 226-231.
<a href="https://dl.acm.org/doi/10.5555/3001460.3001507">https://dl.acm.org/doi/10.5555/3001460.3001507</a>
</p>
<p>Campello, R. J. G. B.; Moulavi, D.; Sander, J. (2013). Density-Based
Clustering Based on Hierarchical Density Estimates. Proceedings of the
17th Pacific-Asia Conference on Knowledge Discovery in Databases, PAKDD
2013, <em>Lecture Notes in Computer Science</em> 7819, p. 160.
<a href="https://doi.org/10.1007/978-3-642-37456-2_14">doi:10.1007/978-3-642-37456-2_14</a>
</p>


<h3>See Also</h3>

<p>Other clustering functions: 
<code><a href="#topic+extractFOSC">extractFOSC</a>()</code>,
<code><a href="#topic+hdbscan">hdbscan</a>()</code>,
<code><a href="#topic+jpclust">jpclust</a>()</code>,
<code><a href="#topic+optics">optics</a>()</code>,
<code><a href="#topic+sNNclust">sNNclust</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example 1: use dbscan on the iris data set
data(iris)
iris &lt;- as.matrix(iris[, 1:4])

## Find suitable DBSCAN parameters:
## 1. We use minPts = dim + 1 = 5 for iris. A larger value can also be used.
## 2. We inspect the k-NN distance plot for k = minPts - 1 = 4
kNNdistplot(iris, minPts = 5)

## Noise seems to start around a 4-NN distance of .7
abline(h=.7, col = "red", lty = 2)

## Cluster with the chosen parameters
res &lt;- dbscan(iris, eps = .7, minPts = 5)
res

pairs(iris, col = res$cluster + 1L)

## Use a precomputed frNN object
fr &lt;- frNN(iris, eps = .7)
dbscan(fr, minPts = 5)

## Example 2: use data from fpc
set.seed(665544)
n &lt;- 100
x &lt;- cbind(
  x = runif(10, 0, 10) + rnorm(n, sd = 0.2),
  y = runif(10, 0, 10) + rnorm(n, sd = 0.2)
  )

res &lt;- dbscan(x, eps = .3, minPts = 3)
res

## plot clusters and add noise (cluster 0) as crosses.
plot(x, col = res$cluster)
points(x[res$cluster == 0, ], pch = 3, col = "grey")

hullplot(x, res)

## Predict cluster membership for new data points
## (Note: 0 means it is predicted as noise)
newdata &lt;- x[1:5,] + rnorm(10, 0, .3)
hullplot(x, res)
points(newdata, pch = 3 , col = "red", lwd = 3)
text(newdata, pos = 1)

pred_label &lt;- predict(res, newdata, data = x)
pred_label
points(newdata, col = pred_label + 1L,  cex = 2, lwd = 2)

## Compare speed against fpc version (if microbenchmark is installed)
## Note: we use dbscan::dbscan to make sure that we do now run the
## implementation in fpc.
## Not run: 
if (requireNamespace("fpc", quietly = TRUE) &amp;&amp;
    requireNamespace("microbenchmark", quietly = TRUE)) {
  t_dbscan &lt;- microbenchmark::microbenchmark(
    dbscan::dbscan(x, .3, 3), times = 10, unit = "ms")
  t_dbscan_linear &lt;- microbenchmark::microbenchmark(
    dbscan::dbscan(x, .3, 3, search = "linear"), times = 10, unit = "ms")
  t_dbscan_dist &lt;- microbenchmark::microbenchmark(
    dbscan::dbscan(x, .3, 3, search = "dist"), times = 10, unit = "ms")
  t_fpc &lt;- microbenchmark::microbenchmark(
    fpc::dbscan(x, .3, 3), times = 10, unit = "ms")

  r &lt;- rbind(t_fpc, t_dbscan_dist, t_dbscan_linear, t_dbscan)
  r

  boxplot(r,
    names = c('fpc', 'dbscan (dist)', 'dbscan (linear)', 'dbscan (kdtree)'),
    main = "Runtime comparison in ms")

  ## speedup of the kd-tree-based version compared to the fpc implementation
  median(t_fpc$time) / median(t_dbscan$time)
}
## End(Not run)

## Example 3: manually create a frNN object for dbscan (dbscan only needs ids and eps)
nn &lt;- structure(list(ids = list(c(2,3), c(1,3), c(1,2,3), c(3,5), c(4,5)), eps = 1),
  class =  c("NN", "frNN"))
nn
dbscan(nn, minPts = 2)

</code></pre>

<hr>
<h2 id='dendrogram'>Coersions to Dendrogram</h2><span id='topic+dendrogram'></span><span id='topic+as.dendrogram'></span><span id='topic+as.dendrogram.default'></span><span id='topic+as.dendrogram.hclust'></span><span id='topic+as.dendrogram.hdbscan'></span><span id='topic+as.dendrogram.reachability'></span>

<h3>Description</h3>

<p>Provides a new generic function to coerce objects to dendrograms with
<code><a href="stats.html#topic+dendrogram">stats::as.dendrogram()</a></code> as the default. Additional methods for
<a href="stats.html#topic+hclust">hclust</a>, <a href="#topic+hdbscan">hdbscan</a> and <a href="#topic+reachability">reachability</a> objects are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.dendrogram(object, ...)

## Default S3 method:
as.dendrogram(object, ...)

## S3 method for class 'hclust'
as.dendrogram(object, ...)

## S3 method for class 'hdbscan'
as.dendrogram(object, ...)

## S3 method for class 'reachability'
as.dendrogram(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dendrogram_+3A_object">object</code></td>
<td>
<p>the object</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_...">...</code></td>
<td>
<p>further arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Coersion methods for
<a href="stats.html#topic+hclust">hclust</a>, <a href="#topic+hdbscan">hdbscan</a> and <a href="#topic+reachability">reachability</a> objects to <a href="#topic+dendrogram">dendrogram</a> are provided.
</p>
<p>The coercion from <code>hclust</code> is a faster C++ reimplementation of the coercion in
package <code>stats</code>. The original implementation can be called
using <code><a href="stats.html#topic+dendrogram">stats::as.dendrogram()</a></code>.
</p>
<p>The coersion from <a href="#topic+hdbscan">hdbscan</a> builds the non-simplified HDBSCAN hierarchy as a
dendrogram object.
</p>

<hr>
<h2 id='DS3'>DS3: Spatial data with arbitrary shapes</h2><span id='topic+DS3'></span>

<h3>Description</h3>

<p>Contains 8000 2-d points, with 6 &quot;natural&quot; looking shapes, all of which have
an sinusoid-like shape that intersects with each cluster.
The data set was originally used as a benchmark data set for the Chameleon clustering
algorithm (Karypis, Han and Kumar, 1999) to
illustrate the a data set containing arbitrarily shaped
spatial data surrounded by both noise and artifacts.
</p>


<h3>Format</h3>

<p>A data.frame with 8000 observations on the following 2 columns:
</p>

<dl>
<dt>X</dt><dd><p>a numeric vector</p>
</dd>
<dt>Y</dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Source</h3>

<p>Obtained from <a href="http://cs.joensuu.fi/sipu/datasets/">http://cs.joensuu.fi/sipu/datasets/</a>
</p>


<h3>References</h3>

<p>Karypis, George, Eui-Hong Han, and Vipin Kumar (1999).
Chameleon: Hierarchical clustering using dynamic modeling. <em>Computer</em>
32(8): 68-75.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(DS3)
plot(DS3, pch = 20, cex = 0.25)
</code></pre>

<hr>
<h2 id='extractFOSC'>Framework for the Optimal Extraction of Clusters from Hierarchies</h2><span id='topic+extractFOSC'></span>

<h3>Description</h3>

<p>Generic reimplementation of the <em>Framework for Optimal Selection of Clusters</em>
(FOSC; Campello et al, 2013) to extract clusterings from hierarchical clustering (i.e.,
<a href="stats.html#topic+hclust">hclust</a> objects).
Can be parameterized to perform unsupervised
cluster extraction through a stability-based measure, or semisupervised
cluster extraction through either a constraint-based extraction (with a
stability-based tiebreaker) or a mixed (weighted) constraint and
stability-based objective extraction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractFOSC(
  x,
  constraints,
  alpha = 0,
  minPts = 2L,
  prune_unstable = FALSE,
  validate_constraints = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extractFOSC_+3A_x">x</code></td>
<td>
<p>a valid <a href="stats.html#topic+hclust">hclust</a> object created via <code><a href="stats.html#topic+hclust">hclust()</a></code> or <code><a href="#topic+hdbscan">hdbscan()</a></code>.</p>
</td></tr>
<tr><td><code id="extractFOSC_+3A_constraints">constraints</code></td>
<td>
<p>Either a list or matrix of pairwise constraints. If
missing, an unsupervised measure of stability is used to make local cuts and
extract the optimal clusters. See details.</p>
</td></tr>
<tr><td><code id="extractFOSC_+3A_alpha">alpha</code></td>
<td>
<p>numeric; weight between <code class="reqn">[0, 1]</code> for mixed-objective
semi-supervised extraction. Defaults to 0.</p>
</td></tr>
<tr><td><code id="extractFOSC_+3A_minpts">minPts</code></td>
<td>
<p>numeric; Defaults to 2. Only needed if class-less noise is a
valid label in the model.</p>
</td></tr>
<tr><td><code id="extractFOSC_+3A_prune_unstable">prune_unstable</code></td>
<td>
<p>logical; should significantly unstable subtrees be
pruned? The default is <code>FALSE</code> for the original optimal extraction
framework (see Campello et al, 2013). See details for what <code>TRUE</code>
implies.</p>
</td></tr>
<tr><td><code id="extractFOSC_+3A_validate_constraints">validate_constraints</code></td>
<td>
<p>logical; should constraints be checked for
validity? See details for what are considered valid constraints.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Campello et al (2013) suggested a <em>Framework for Optimal Selection of
Clusters</em> (FOSC) as a framework to make local (non-horizontal) cuts to any
cluster tree hierarchy. This function implements the original extraction
algorithms as described by the framework for hclust objects. Traditional
cluster extraction methods from hierarchical representations (such as
<a href="stats.html#topic+hclust">hclust</a> objects) generally rely on global parameters or cutting values
which are used to partition a cluster hierarchy into a set of disjoint, flat
clusters. This is implemented in R in function <code><a href="stats.html#topic+cutree">cutree()</a></code>.
Although such methods are widespread, using global parameter
settings are inherently limited in that they cannot capture patterns within
the cluster hierarchy at varying <em>local</em> levels of granularity.
</p>
<p>Rather than partitioning a hierarchy based on the number of the cluster one
expects to find (<code class="reqn">k</code>) or based on some linkage distance threshold
(<code class="reqn">H</code>), the FOSC proposes that the optimal clusters may exist at varying
distance thresholds in the hierarchy. To enable this idea, FOSC requires one
parameter (minPts) that represents <em>the minimum number of points that
constitute a valid cluster.</em> The first step of the FOSC algorithm is to
traverse the given cluster hierarchy divisively, recording new clusters at
each split if both branches represent more than or equal to minPts. Branches
that contain less than minPts points at one or both branches inherit the
parent clusters identity. Note that using FOSC, due to the constraint that
minPts must be greater than or equal to 2, it is possible that the optimal
cluster solution chosen makes local cuts that render parent branches of
sizes less than minPts as noise, which are denoted as 0 in the final
solution.
</p>
<p>Traversing the original cluster tree using minPts creates a new, simplified
cluster tree that is then post-processed recursively to extract clusters
that maximize for each cluster <code class="reqn">C_i</code> the cost function
</p>
<p style="text-align: center;"><code class="reqn">\max_{\delta_2, \dots, \delta_k} J = \sum\limits_{i=2}^{k} \delta_i
S(C_i)</code>
</p>
<p> where
<code class="reqn">S(C_i)</code> is the stability-based measure as </p>
<p style="text-align: center;"><code class="reqn"> S(C_i) =
\sum_{x_j \in C_i}(\frac{1}{h_{min} (x_j, C_i)} - \frac{1}{h_{max} (C_i)})
</code>
</p>

<p><code class="reqn">\delta_i</code> represents an indicator function, which constrains
the solution space such that clusters must be disjoint (cannot assign more
than 1 label to each cluster). The measure <code class="reqn">S(C_i)</code> used by FOSC
is an unsupervised validation measure based on the assumption that, if you
vary the linkage/distance threshold across all possible values, more
prominent clusters that survive over many threshold variations should be
considered as stronger candidates of the optimal solution. For this reason,
using this measure to detect clusters is referred to as an unsupervised,
<em>stability-based</em> extraction approach. In some cases it may be useful
to enact <em>instance-level</em> constraints that ensure the solution space
conforms to linkage expectations known <em>a priori</em>. This general idea of
using preliminary expectations to augment the clustering solution will be
referred to as <em>semisupervised clustering</em>. If constraints are given in
the call to <code>extractFOSC()</code>, the following alternative objective function
is maximized:
</p>
<p style="text-align: center;"><code class="reqn">J = \frac{1}{2n_c}\sum\limits_{j=1}^n \gamma (x_j)</code>
</p>

<p><code class="reqn">n_c</code> is the total number of constraints given and
<code class="reqn">\gamma(x_j)</code> represents the number of constraints involving
object <code class="reqn">x_j</code> that are satisfied. In the case of ties (such as
solutions where no constraints were given), the unsupervised solution is
used as a tiebreaker. See Campello et al (2013) for more details.
</p>
<p>As a third option, if one wishes to prioritize the degree at which the
unsupervised and semisupervised solutions contribute to the overall optimal
solution, the parameter <code class="reqn">\alpha</code> can be set to enable the extraction of
clusters that maximize the <code>mixed</code> objective function
</p>
<p style="text-align: center;"><code class="reqn">J = \alpha S(C_i) + (1 - \alpha) \gamma(C_i))</code>
</p>

<p>FOSC expects the pairwise constraints to be passed as either 1) an
<code class="reqn">n(n-1)/2</code> vector of integers representing the constraints, where 1
represents should-link, -1 represents should-not-link, and 0 represents no
preference using the unsupervised solution (see below for examples).
Alternatively, if only a few constraints are needed, a named list
representing the (symmetric) adjacency list can be used, where the names
correspond to indices of the points in the original data, and the values
correspond to integer vectors of constraints (positive indices for
should-link, negative indices for should-not-link). Again, see the examples
section for a demonstration of this.
</p>
<p>The parameters to the input function correspond to the concepts discussed
above. The <code>minPts</code> parameter to represent the minimum cluster size to
extract. The optional <code>constraints</code> parameter contains the pairwise,
instance-level constraints of the data. The optional <code>alpha</code> parameters
controls whether the mixed objective function is used (if <code>alpha</code> is
greater than 0). If the <code>validate_constraints</code> parameter is set to
true, the constraints are checked (and fixed) for symmetry (if point A has a
should-link constraint with point B, point B should also have the same
constraint). Asymmetric constraints are not supported.
</p>
<p>Unstable branch pruning was not discussed by Campello et al (2013), however
in some data sets it may be the case that specific subbranches scores are
significantly greater than sibling and parent branches, and thus sibling
branches should be considered as noise if their scores are cumulatively
lower than the parents. This can happen in extremely nonhomogeneous data
sets, where there exists locally very stable branches surrounded by unstable
branches that contain more than <code>minPts</code> points.
<code>prune_unstable = TRUE</code> will remove the unstable branches.
</p>


<h3>Value</h3>

<p>A list with the elements:
</p>
<table>
<tr><td><code>cluster</code></td>
<td>
<p>A integer vector with cluster assignments. Zero
indicates noise points (if any).</p>
</td></tr>
<tr><td><code>hc</code></td>
<td>
<p>The original <a href="stats.html#topic+hclust">hclust</a> object with additional list elements
<code>"stability"</code>, <code>"constraint"</code>, and <code>"total"</code>
for the <code class="reqn">n - 1</code> cluster-wide objective scores from the extraction.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matt Piekenbrock
</p>


<h3>References</h3>

<p>Campello, Ricardo JGB, Davoud Moulavi, Arthur Zimek, and Joerg
Sander (2013). A framework for semi-supervised and unsupervised optimal
extraction of clusters from hierarchies. <em>Data Mining and Knowledge
Discovery</em> 27(3): 344-371.
<a href="https://doi.org/10.1007/s10618-013-0311-4">doi:10.1007/s10618-013-0311-4</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+hclust">hclust()</a></code>, <code><a href="#topic+hdbscan">hdbscan()</a></code>, <code><a href="stats.html#topic+cutree">stats::cutree()</a></code>
</p>
<p>Other clustering functions: 
<code><a href="#topic+dbscan">dbscan</a>()</code>,
<code><a href="#topic+hdbscan">hdbscan</a>()</code>,
<code><a href="#topic+jpclust">jpclust</a>()</code>,
<code><a href="#topic+optics">optics</a>()</code>,
<code><a href="#topic+sNNclust">sNNclust</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("moons")

## Regular HDBSCAN using stability-based extraction (unsupervised)
cl &lt;- hdbscan(moons, minPts = 5)
cl$cluster

## Constraint-based extraction from the HDBSCAN hierarchy
## (w/ stability-based tiebreaker (semisupervised))
cl_con &lt;- extractFOSC(cl$hc, minPts = 5,
  constraints = list("12" = c(49, -47)))
cl_con$cluster

## Alternative formulation: Constraint-based extraction from the HDBSCAN hierarchy
## (w/ stability-based tiebreaker (semisupervised)) using distance thresholds
dist_moons &lt;- dist(moons)
cl_con2 &lt;- extractFOSC(cl$hc, minPts = 5,
  constraints = ifelse(dist_moons &lt; 0.1, 1L,
                ifelse(dist_moons &gt; 1, -1L, 0L)))

cl_con2$cluster # same as the second example
</code></pre>

<hr>
<h2 id='frNN'>Find the Fixed Radius Nearest Neighbors</h2><span id='topic+frNN'></span><span id='topic+frnn'></span><span id='topic+print.frnn'></span><span id='topic+sort.frNN'></span><span id='topic+adjacencylist.frNN'></span><span id='topic+print.frNN'></span>

<h3>Description</h3>

<p>This function uses a kd-tree to find the fixed radius nearest neighbors
(including distances) fast.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>frNN(
  x,
  eps,
  query = NULL,
  sort = TRUE,
  search = "kdtree",
  bucketSize = 10,
  splitRule = "suggest",
  approx = 0
)

## S3 method for class 'frNN'
sort(x, decreasing = FALSE, ...)

## S3 method for class 'frNN'
adjacencylist(x, ...)

## S3 method for class 'frNN'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="frNN_+3A_x">x</code></td>
<td>
<p>a data matrix, a dist object or a frNN object.</p>
</td></tr>
<tr><td><code id="frNN_+3A_eps">eps</code></td>
<td>
<p>neighbors radius.</p>
</td></tr>
<tr><td><code id="frNN_+3A_query">query</code></td>
<td>
<p>a data matrix with the points to query. If query is not
specified, the NN for all the points in <code>x</code> is returned. If query is
specified then <code>x</code> needs to be a data matrix.</p>
</td></tr>
<tr><td><code id="frNN_+3A_sort">sort</code></td>
<td>
<p>sort the neighbors by distance? This is expensive and can be
done later using <code>sort()</code>.</p>
</td></tr>
<tr><td><code id="frNN_+3A_search">search</code></td>
<td>
<p>nearest neighbor search strategy (one of <code>"kdtree"</code>, <code>"linear"</code> or
<code>"dist"</code>).</p>
</td></tr>
<tr><td><code id="frNN_+3A_bucketsize">bucketSize</code></td>
<td>
<p>max size of the kd-tree leafs.</p>
</td></tr>
<tr><td><code id="frNN_+3A_splitrule">splitRule</code></td>
<td>
<p>rule to split the kd-tree. One of <code>"STD"</code>, <code>"MIDPT"</code>, <code>"FAIR"</code>,
<code>"SL_MIDPT"</code>, <code>"SL_FAIR"</code> or <code>"SUGGEST"</code> (SL stands for sliding). <code>"SUGGEST"</code> uses
ANNs best guess.</p>
</td></tr>
<tr><td><code id="frNN_+3A_approx">approx</code></td>
<td>
<p>use approximate nearest neighbors. All NN up to a distance of
a factor of <code>1 + approx</code> eps may be used. Some actual NN may be omitted
leading to spurious clusters and noise points.  However, the algorithm will
enjoy a significant speedup.</p>
</td></tr>
<tr><td><code id="frNN_+3A_decreasing">decreasing</code></td>
<td>
<p>sort in decreasing order?</p>
</td></tr>
<tr><td><code id="frNN_+3A_...">...</code></td>
<td>
<p>further arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> is specified as a data matrix, then Euclidean distances an fast
nearest neighbor lookup using a kd-tree are used.
</p>
<p>To create a frNN object from scratch, you need to supply at least the
elements <code>id</code> with a list of integer vectors with the nearest neighbor
ids for each point and <code>eps</code> (see below).
</p>
<p><strong>Self-matches:</strong> Self-matches are not returned!
</p>


<h3>Value</h3>

<p><code>frNN()</code> returns an object of class <a href="#topic+frNN">frNN</a> (subclass of
<a href="#topic+NN">NN</a>) containing a list with the following components:
</p>
<table>
<tr><td><code>id</code></td>
<td>
<p>a list of
integer vectors. Each vector contains the ids of the fixed radius nearest
neighbors. </p>
</td></tr>
<tr><td><code>dist</code></td>
<td>
<p>a list with distances (same structure as
<code>id</code>). </p>
</td></tr>
<tr><td><code>eps</code></td>
<td>
<p> neighborhood radius <code>eps</code> that was used. </p>
</td></tr>
</table>
<p><code>adjacencylist()</code> returns a list with one entry per data point in <code>x</code>. Each entry
contains the id of the nearest neighbors.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>David M. Mount and Sunil Arya (2010). ANN: A Library for
Approximate Nearest Neighbor Searching,
<a href="http://www.cs.umd.edu/~mount/ANN/">http://www.cs.umd.edu/~mount/ANN/</a>.
</p>


<h3>See Also</h3>

<p>Other NN functions: 
<code><a href="#topic+NN">NN</a></code>,
<code><a href="#topic+comps">comps</a>()</code>,
<code><a href="#topic+kNNdist">kNNdist</a>()</code>,
<code><a href="#topic+kNN">kNN</a>()</code>,
<code><a href="#topic+sNN">sNN</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- iris[, -5]

# Example 1: Find fixed radius nearest neighbors for each point
nn &lt;- frNN(x, eps = .5)

# Number of neighbors
hist(sapply(adjacencylist(nn), length),
  xlab = "k", main="Number of Neighbors",
  sub = paste("Neighborhood size eps =", nn$eps))

# Explore neighbors of point i = 10
i &lt;- 10
nn$id[[i]]
nn$dist[[i]]
plot(x, col = ifelse(1:nrow(iris) %in% nn$id[[i]], "red", "black"))

# get an adjacency list
head(adjacencylist(nn))

# plot the fixed radius neighbors (and then reduced to a radius of .3)
plot(nn, x)
plot(frNN(nn, eps = .3), x)

## Example 2: find fixed-radius NN for query points
q &lt;- x[c(1,100),]
nn &lt;- frNN(x, eps = .5, query = q)

plot(nn, x, col = "grey")
points(q, pch = 3, lwd = 2)
</code></pre>

<hr>
<h2 id='glosh'>Global-Local Outlier Score from Hierarchies</h2><span id='topic+glosh'></span><span id='topic+GLOSH'></span>

<h3>Description</h3>

<p>Calculate the Global-Local Outlier Score from Hierarchies (GLOSH) score for
each data point using a kd-tree to speed up kNN search.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glosh(x, k = 4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glosh_+3A_x">x</code></td>
<td>
<p>an <a href="stats.html#topic+hclust">hclust</a> object, data matrix, or <a href="stats.html#topic+dist">dist</a> object.</p>
</td></tr>
<tr><td><code id="glosh_+3A_k">k</code></td>
<td>
<p>size of the neighborhood.</p>
</td></tr>
<tr><td><code id="glosh_+3A_...">...</code></td>
<td>
<p>further arguments are passed on to <code><a href="#topic+kNN">kNN()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>GLOSH compares the density of a point to densities of any points associated
within current and child clusters (if any). Points that have a substantially
lower density than the density mode (cluster) they most associate with are
considered outliers. GLOSH is computed from a hierarchy a clusters.
</p>
<p>Specifically, consider a point <em>x</em> and a density or distance threshold
<em>lambda</em>. GLOSH is calculated by taking 1 minus the ratio of how long
any of the child clusters of the cluster <em>x</em> belongs to &quot;survives&quot;
changes in <em>lambda</em> to the highest <em>lambda</em> threshold of x, above
which x becomes a noise point.
</p>
<p>Scores close to 1 indicate outliers. For more details on the motivation for
this calculation, see Campello et al (2015).
</p>


<h3>Value</h3>

<p>A numeric vector of length equal to the size of the original data
set containing GLOSH values for all data points.
</p>


<h3>Author(s)</h3>

<p>Matt Piekenbrock
</p>


<h3>References</h3>

<p>Campello, Ricardo JGB, Davoud Moulavi, Arthur Zimek, and Joerg
Sander. Hierarchical density estimates for data clustering, visualization,
and outlier detection. <em>ACM Transactions on Knowledge Discovery from Data
(TKDD)</em> 10, no. 1 (2015).
<a href="https://doi.org/10.1145/2733381">doi:10.1145/2733381</a>
</p>


<h3>See Also</h3>

<p>Other Outlier Detection Functions: 
<code><a href="#topic+kNNdist">kNNdist</a>()</code>,
<code><a href="#topic+lof">lof</a>()</code>,
<code><a href="#topic+pointdensity">pointdensity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(665544)
n &lt;- 100
x &lt;- cbind(
  x=runif(10, 0, 5) + rnorm(n, sd = 0.4),
  y=runif(10, 0, 5) + rnorm(n, sd = 0.4)
  )

### calculate GLOSH score
glosh &lt;- glosh(x, k = 3)

### distribution of outlier scores
summary(glosh)
hist(glosh, breaks = 10)

### simple function to plot point size is proportional to GLOSH score
plot_glosh &lt;- function(x, glosh){
  plot(x, pch = ".", main = "GLOSH (k = 3)")
  points(x, cex = glosh*3, pch = 1, col = "red")
  text(x[glosh &gt; 0.80, ], labels = round(glosh, 3)[glosh &gt; 0.80], pos = 3)
}
plot_glosh(x, glosh)

### GLOSH with any hierarchy
x_dist &lt;- dist(x)
x_sl &lt;- hclust(x_dist, method = "single")
x_upgma &lt;- hclust(x_dist, method = "average")
x_ward &lt;- hclust(x_dist, method = "ward.D2")

## Compare what different linkage criterion consider as outliers
glosh_sl &lt;- glosh(x_sl, k = 3)
plot_glosh(x, glosh_sl)

glosh_upgma &lt;- glosh(x_upgma, k = 3)
plot_glosh(x, glosh_upgma)

glosh_ward &lt;- glosh(x_ward, k = 3)
plot_glosh(x, glosh_ward)

## GLOSH is automatically computed with HDBSCAN
all(hdbscan(x, minPts = 3)$outlier_scores == glosh(x, k = 3))
</code></pre>

<hr>
<h2 id='hdbscan'>Hierarchical DBSCAN (HDBSCAN)</h2><span id='topic+hdbscan'></span><span id='topic+HDBSCAN'></span><span id='topic+print.hdbscan'></span><span id='topic+plot.hdbscan'></span><span id='topic+coredist'></span><span id='topic+mrdist'></span><span id='topic+predict.hdbscan'></span>

<h3>Description</h3>

<p>Fast C++ implementation of the HDBSCAN (Hierarchical DBSCAN) and its related
algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hdbscan(
  x,
  minPts,
  gen_hdbscan_tree = FALSE,
  gen_simplified_tree = FALSE,
  verbose = FALSE
)

## S3 method for class 'hdbscan'
print(x, ...)

## S3 method for class 'hdbscan'
plot(
  x,
  scale = "suggest",
  gradient = c("yellow", "red"),
  show_flat = FALSE,
  ...
)

coredist(x, minPts)

mrdist(x, minPts, coredist = NULL)

## S3 method for class 'hdbscan'
predict(object, newdata, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hdbscan_+3A_x">x</code></td>
<td>
<p>a data matrix (Euclidean distances are used) or a <a href="stats.html#topic+dist">dist</a> object
calculated with an arbitrary distance metric.</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_minpts">minPts</code></td>
<td>
<p>integer; Minimum size of clusters. See details.</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_gen_hdbscan_tree">gen_hdbscan_tree</code></td>
<td>
<p>logical; should the robust single linkage tree be
explicitly computed (see cluster tree in Chaudhuri et al, 2010).</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_gen_simplified_tree">gen_simplified_tree</code></td>
<td>
<p>logical; should the simplified hierarchy be
explicitly computed (see Campello et al, 2013).</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_verbose">verbose</code></td>
<td>
<p>report progress.</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on.</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_scale">scale</code></td>
<td>
<p>integer; used to scale condensed tree based on the graphics
device. Lower scale results in wider trees.</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_gradient">gradient</code></td>
<td>
<p>character vector; the colors to build the condensed tree
coloring with.</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_show_flat">show_flat</code></td>
<td>
<p>logical; whether to draw boxes indicating the most stable
clusters.</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_coredist">coredist</code></td>
<td>
<p>numeric vector with precomputed core distances (optional).</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_object">object</code></td>
<td>
<p>clustering object.</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_newdata">newdata</code></td>
<td>
<p>new data points for which the cluster membership should be
predicted.</p>
</td></tr>
<tr><td><code id="hdbscan_+3A_data">data</code></td>
<td>
<p>the data set used to create the clustering object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This fast implementation of HDBSCAN (Campello et al., 2013) computes the
hierarchical cluster tree representing density estimates along with the
stability-based flat cluster extraction. HDBSCAN essentially computes the
hierarchy of all DBSCAN* clusterings, and
then uses a stability-based extraction method to find optimal cuts in the
hierarchy, thus producing a flat solution.
</p>
<p>HDBSCAN performs the following steps:
</p>

<ol>
<li><p> Compute mutual reachability distance mrd between points
(based on distances and core distances).
</p>
</li>
<li><p> Use mdr as a distance measure to construct a minimum spanning tree.
</p>
</li>
<li><p> Prune the tree using stability.
</p>
</li>
<li><p> Extract the clusters.
</p>
</li></ol>

<p>Additional, related algorithms including the &quot;Global-Local Outlier Score
from Hierarchies&quot; (GLOSH; see section 6 of Campello et al., 2015)
is available in function <code><a href="#topic+glosh">glosh()</a></code>
and the ability to cluster based on instance-level constraints (see
section 5.3 of Campello et al. 2015) are supported. The algorithms only need
the parameter <code>minPts</code>.
</p>
<p>Note that <code>minPts</code> not only acts as a minimum cluster size to detect,
but also as a &quot;smoothing&quot; factor of the density estimates implicitly
computed from HDBSCAN.
</p>
<p><code>coredist()</code>: The core distance is defined for each point as
the distance to the <code>MinPts</code>'s neighbor. It is a density estimate.
</p>
<p><code>mrdist()</code>: The mutual reachability distance is defined between two points as
<code>mrd(a, b) = max(coredist(a), coredist(b), dist(a, b))</code>. This distance metric is used by
HDBSCAN. It has the effect of increasing distances in low density areas.
</p>
<p><code>predict()</code> assigns each new data point to the same cluster as the nearest point
if it is not more than that points core distance away. Otherwise the new point
is classified as a noise point (i.e., cluster ID 0).
</p>


<h3>Value</h3>

<p><code>hdbscan()</code> returns object of class <code>hdbscan</code> with the following components:
</p>
<table>
<tr><td><code>cluster</code></td>
<td>
<p>A integer vector with cluster assignments. Zero indicates
noise points.</p>
</td></tr>
<tr><td><code>minPts</code></td>
<td>
<p> value of the <code>minPts</code> parameter.</p>
</td></tr>
<tr><td><code>cluster_scores</code></td>
<td>
<p>The sum of the stability scores for each salient
(flat) cluster. Corresponds to cluster IDs given the in <code>"cluster"</code> element.
</p>
</td></tr>
<tr><td><code>membership_prob</code></td>
<td>
<p>The probability or individual stability of a
point within its clusters. Between 0 and 1.</p>
</td></tr>
<tr><td><code>outlier_scores</code></td>
<td>
<p>The GLOSH outlier score of each point. </p>
</td></tr>
<tr><td><code>hc</code></td>
<td>
<p>An <a href="stats.html#topic+hclust">hclust</a> object of the HDBSCAN hierarchy. </p>
</td></tr>
</table>
<p><code>coredist()</code> returns a vector with the core distance for each data point.
</p>
<p><code>mrdist()</code> returns a <a href="stats.html#topic+dist">dist</a> object containing pairwise mutual reachability distances.
</p>


<h3>Author(s)</h3>

<p>Matt Piekenbrock
</p>


<h3>References</h3>

<p>Campello RJGB, Moulavi D, Sander J (2013). Density-Based Clustering Based on
Hierarchical Density Estimates. Proceedings of the 17th Pacific-Asia
Conference on Knowledge Discovery in Databases, PAKDD 2013, <em>Lecture Notes
in Computer Science</em> 7819, p. 160.
<a href="https://doi.org/10.1007/978-3-642-37456-2_14">doi:10.1007/978-3-642-37456-2_14</a>
</p>
<p>Campello RJGB, Moulavi D, Zimek A, Sander J (2015). Hierarchical density
estimates for data clustering, visualization, and outlier detection.
<em>ACM Transactions on Knowledge Discovery from Data (TKDD),</em> 10(5):1-51.
<a href="https://doi.org/10.1145/2733381">doi:10.1145/2733381</a>
</p>


<h3>See Also</h3>

<p>Other clustering functions: 
<code><a href="#topic+dbscan">dbscan</a>()</code>,
<code><a href="#topic+extractFOSC">extractFOSC</a>()</code>,
<code><a href="#topic+jpclust">jpclust</a>()</code>,
<code><a href="#topic+optics">optics</a>()</code>,
<code><a href="#topic+sNNclust">sNNclust</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## cluster the moons data set with HDBSCAN
data(moons)

res &lt;- hdbscan(moons, minPts = 5)
res

plot(res)
plot(moons, col = res$cluster + 1L)

## cluster the moons data set with HDBSCAN using Manhattan distances
res &lt;- hdbscan(dist(moons, method = "manhattan"), minPts = 5)
plot(res)
plot(moons, col = res$cluster + 1L)

## DS3 from Chameleon
data("DS3")

res &lt;- hdbscan(DS3, minPts = 50)
res

## Plot the simplified tree, highlight the most stable clusters
plot(res, show_flat = TRUE)

## Plot the actual clusters (noise has cluster id 0 and is shown in black)
plot(DS3, col = res$cluster + 1L, cex = .5)
</code></pre>

<hr>
<h2 id='hullplot'>Plot Convex Hulls of Clusters</h2><span id='topic+hullplot'></span>

<h3>Description</h3>

<p>This function produces a two-dimensional scatter plot with added convex
hulls for clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hullplot(
  x,
  cl,
  col = NULL,
  cex = 0.5,
  hull_lwd = 1,
  hull_lty = 1,
  solid = TRUE,
  alpha = 0.2,
  main = "Convex Cluster Hulls",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hullplot_+3A_x">x</code></td>
<td>
<p>a data matrix. If more than 2 columns are provided, then the data
is plotted using the first two principal components.</p>
</td></tr>
<tr><td><code id="hullplot_+3A_cl">cl</code></td>
<td>
<p>a clustering. Either a numeric cluster assignment vector or a
clustering object (a list with an element named <code>cluster</code>).</p>
</td></tr>
<tr><td><code id="hullplot_+3A_col">col</code></td>
<td>
<p>colors used for clusters. Defaults to the standard palette.  The
first color (default is black) is used for noise/unassigned points (cluster
id 0).</p>
</td></tr>
<tr><td><code id="hullplot_+3A_cex">cex</code></td>
<td>
<p>expansion factor for symbols.</p>
</td></tr>
<tr><td><code id="hullplot_+3A_hull_lwd">hull_lwd</code>, <code id="hullplot_+3A_hull_lty">hull_lty</code></td>
<td>
<p>line width and line type used for the convex hull.</p>
</td></tr>
<tr><td><code id="hullplot_+3A_solid">solid</code>, <code id="hullplot_+3A_alpha">alpha</code></td>
<td>
<p>draw filled polygons instead of just lines for the convex
hulls? alpha controls the level of alpha shading.</p>
</td></tr>
<tr><td><code id="hullplot_+3A_main">main</code></td>
<td>
<p>main title.</p>
</td></tr>
<tr><td><code id="hullplot_+3A_...">...</code></td>
<td>
<p>additional arguments passed on to plot.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2)
n &lt;- 400

x &lt;- cbind(
  x = runif(4, 0, 1) + rnorm(n, sd = 0.1),
  y = runif(4, 0, 1) + rnorm(n, sd = 0.1)
  )
cl &lt;- rep(1:4, time = 100)

### original data with true clustering
hullplot(x, cl, main = "True clusters")
### use differnt symbols
hullplot(x, cl, main = "True clusters", pch = cl)
### just the hulls
hullplot(x, cl, main = "True clusters", pch = NA)
### a version suitable for b/w printing)
hullplot(x, cl, main = "True clusters", solid = FALSE, col = "black", pch = cl)


### run some clustering algorithms and plot the resutls
db &lt;- dbscan(x, eps = .07, minPts = 10)
hullplot(x, db, main = "DBSCAN")

op &lt;- optics(x, eps = 10, minPts = 10)
opDBSCAN &lt;- extractDBSCAN(op, eps_cl = .07)
hullplot(x, opDBSCAN, main = "OPTICS")

opXi &lt;- extractXi(op, xi = 0.05)
hullplot(x, opXi, main = "OPTICSXi")

# Extract minimal 'flat' clusters only
opXi &lt;- extractXi(op, xi = 0.05, minimum = TRUE)
hullplot(x, opXi, main = "OPTICSXi")

km &lt;- kmeans(x, centers = 4)
hullplot(x, km, main = "k-means")

hc &lt;- cutree(hclust(dist(x)), k = 4)
hullplot(x, hc, main = "Hierarchical Clustering")
</code></pre>

<hr>
<h2 id='jpclust'>Jarvis-Patrick Clustering</h2><span id='topic+jpclust'></span><span id='topic+print.general_clustering'></span>

<h3>Description</h3>

<p>Fast C++ implementation of the Jarvis-Patrick clustering which first builds
a shared nearest neighbor graph (k nearest neighbor sparsification) and then
places two points in the same cluster if they are in each other's nearest
neighbor list and they share at least kt nearest neighbors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jpclust(x, k, kt, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jpclust_+3A_x">x</code></td>
<td>
<p>a data matrix/data.frame (Euclidean distance is used), a
precomputed <a href="stats.html#topic+dist">dist</a> object or a kNN object created with <code><a href="#topic+kNN">kNN()</a></code>.</p>
</td></tr>
<tr><td><code id="jpclust_+3A_k">k</code></td>
<td>
<p>Neighborhood size for nearest neighbor sparsification. If <code>x</code>
is a kNN object then <code>k</code> may be missing.</p>
</td></tr>
<tr><td><code id="jpclust_+3A_kt">kt</code></td>
<td>
<p>threshold on the number of shared nearest neighbors (including the
points themselves) to form clusters. Range: <code class="reqn">[1, k]</code></p>
</td></tr>
<tr><td><code id="jpclust_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on to the k nearest neighbor
search algorithm. See <code><a href="#topic+kNN">kNN()</a></code> for details on how to control the
search strategy.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Following the original paper, the shared nearest neighbor list is
constructed as the k neighbors plus the point itself (as neighbor zero).
Therefore, the threshold <code>kt</code> needs to be in the range <code class="reqn">[1, k]</code>.
</p>
<p>Fast nearest neighbors search with <code><a href="#topic+kNN">kNN()</a></code> is only used if <code>x</code> is
a matrix. In this case Euclidean distance is used.
</p>


<h3>Value</h3>

<p>A object of class <code>general_clustering</code> with the following
components:
</p>
<table>
<tr><td><code>cluster</code></td>
<td>
<p>A integer vector with cluster assignments. Zero
indicates noise points.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p> name of used clustering algorithm.</p>
</td></tr>
<tr><td><code>param</code></td>
<td>
<p> list of used clustering parameters. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>R. A. Jarvis and E. A. Patrick. 1973. Clustering Using a
Similarity Measure Based on Shared Near Neighbors. <em>IEEE Trans. Comput.
22,</em> 11 (November 1973), 1025-1034.
<a href="https://doi.org/10.1109/T-C.1973.223640">doi:10.1109/T-C.1973.223640</a>
</p>


<h3>See Also</h3>

<p>Other clustering functions: 
<code><a href="#topic+dbscan">dbscan</a>()</code>,
<code><a href="#topic+extractFOSC">extractFOSC</a>()</code>,
<code><a href="#topic+hdbscan">hdbscan</a>()</code>,
<code><a href="#topic+optics">optics</a>()</code>,
<code><a href="#topic+sNNclust">sNNclust</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("DS3")

# use a shared neighborhood of 20 points and require 12 shared neighbors
cl &lt;- jpclust(DS3, k = 20, kt = 12)
cl

plot(DS3, col = cl$cluster+1L, cex = .5)
# Note: JP clustering does not consider noise and thus,
# the sine wave points chain clusters together.

# use a precomputed kNN object instead of the original data.
nn &lt;- kNN(DS3, k = 30)
nn

cl &lt;- jpclust(nn, k = 20, kt = 12)
cl

# cluster with noise removed (use low pointdensity to identify noise)
d &lt;- pointdensity(DS3, eps = 25)
hist(d, breaks = 20)
DS3_noiseless &lt;- DS3[d &gt; 110,]

cl &lt;- jpclust(DS3_noiseless, k = 20, kt = 10)
cl

plot(DS3_noiseless, col = cl$cluster+1L, cex = .5)
</code></pre>

<hr>
<h2 id='kNN'>Find the k Nearest Neighbors</h2><span id='topic+kNN'></span><span id='topic+knn'></span><span id='topic+sort.kNN'></span><span id='topic+adjacencylist.kNN'></span><span id='topic+print.kNN'></span>

<h3>Description</h3>

<p>This function uses a kd-tree to find all k nearest neighbors in a data
matrix (including distances) fast.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kNN(
  x,
  k,
  query = NULL,
  sort = TRUE,
  search = "kdtree",
  bucketSize = 10,
  splitRule = "suggest",
  approx = 0
)

## S3 method for class 'kNN'
sort(x, decreasing = FALSE, ...)

## S3 method for class 'kNN'
adjacencylist(x, ...)

## S3 method for class 'kNN'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kNN_+3A_x">x</code></td>
<td>
<p>a data matrix, a <a href="stats.html#topic+dist">dist</a> object or a <a href="#topic+kNN">kNN</a> object.</p>
</td></tr>
<tr><td><code id="kNN_+3A_k">k</code></td>
<td>
<p>number of neighbors to find.</p>
</td></tr>
<tr><td><code id="kNN_+3A_query">query</code></td>
<td>
<p>a data matrix with the points to query. If query is not
specified, the NN for all the points in <code>x</code> is returned. If query is
specified then <code>x</code> needs to be a data matrix.</p>
</td></tr>
<tr><td><code id="kNN_+3A_sort">sort</code></td>
<td>
<p>sort the neighbors by distance? Note that some search methods
already sort the results. Sorting is expensive and <code>sort = FALSE</code> may
be much faster for some search methods. kNN objects can be sorted using
<code>sort()</code>.</p>
</td></tr>
<tr><td><code id="kNN_+3A_search">search</code></td>
<td>
<p>nearest neighbor search strategy (one of <code>"kdtree"</code>, <code>"linear"</code> or
<code>"dist"</code>).</p>
</td></tr>
<tr><td><code id="kNN_+3A_bucketsize">bucketSize</code></td>
<td>
<p>max size of the kd-tree leafs.</p>
</td></tr>
<tr><td><code id="kNN_+3A_splitrule">splitRule</code></td>
<td>
<p>rule to split the kd-tree. One of <code>"STD"</code>, <code>"MIDPT"</code>, <code>"FAIR"</code>,
<code>"SL_MIDPT"</code>, <code>"SL_FAIR"</code> or <code>"SUGGEST"</code> (SL stands for sliding). <code>"SUGGEST"</code> uses
ANNs best guess.</p>
</td></tr>
<tr><td><code id="kNN_+3A_approx">approx</code></td>
<td>
<p>use approximate nearest neighbors. All NN up to a distance of
a factor of <code>1 + approx</code> eps may be used. Some actual NN may be omitted
leading to spurious clusters and noise points.  However, the algorithm will
enjoy a significant speedup.</p>
</td></tr>
<tr><td><code id="kNN_+3A_decreasing">decreasing</code></td>
<td>
<p>sort in decreasing order?</p>
</td></tr>
<tr><td><code id="kNN_+3A_...">...</code></td>
<td>
<p>further arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Ties:</strong> If the kth and the (k+1)th nearest neighbor are tied, then the
neighbor found first is returned and the other one is ignored.
</p>
<p><strong>Self-matches:</strong> If no query is specified, then self-matches are
removed.
</p>
<p>Details on the search parameters:
</p>

<ul>
<li> <p><code>search</code> controls if
a kd-tree or linear search (both implemented in the ANN library; see Mount
and Arya, 2010). Note, that these implementations cannot handle NAs.
<code>search = "dist"</code> precomputes Euclidean distances first using R. NAs are
handled, but the resulting distance matrix cannot contain NAs. To use other
distance measures, a precomputed distance matrix can be provided as <code>x</code>
(<code>search</code> is ignored).
</p>
</li>
<li> <p><code>bucketSize</code> and <code>splitRule</code> influence how the kd-tree is
built. <code>approx</code> uses the approximate nearest neighbor search
implemented in ANN. All nearest neighbors up to a distance of
<code>eps / (1 + approx)</code> will be considered and all with a distance
greater than <code>eps</code> will not be considered. The other points might be
considered. Note that this results in some actual nearest neighbors being
omitted leading to spurious clusters and noise points. However, the
algorithm will enjoy a significant speedup. For more details see Mount and
Arya (2010).
</p>
</li></ul>



<h3>Value</h3>

<p>An object of class <code>kNN</code> (subclass of <a href="#topic+NN">NN</a>) containing a
list with the following components:
</p>
<table>
<tr><td><code>dist</code></td>
<td>
<p>a matrix with distances. </p>
</td></tr>
<tr><td><code>id</code></td>
<td>
<p>a matrix with <code>ids</code>. </p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number <code>k</code> used. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>David M. Mount and Sunil Arya (2010). ANN: A Library for
Approximate Nearest Neighbor Searching,
<a href="http://www.cs.umd.edu/~mount/ANN/">http://www.cs.umd.edu/~mount/ANN/</a>.
</p>


<h3>See Also</h3>

<p>Other NN functions: 
<code><a href="#topic+NN">NN</a></code>,
<code><a href="#topic+comps">comps</a>()</code>,
<code><a href="#topic+frNN">frNN</a>()</code>,
<code><a href="#topic+kNNdist">kNNdist</a>()</code>,
<code><a href="#topic+sNN">sNN</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- iris[, -5]

# Example 1: finding kNN for all points in a data matrix (using a kd-tree)
nn &lt;- kNN(x, k = 5)
nn

# explore neighborhood of point 10
i &lt;- 10
nn$id[i,]
plot(x, col = ifelse(1:nrow(iris) %in% nn$id[i,], "red", "black"))

# visualize the 5 nearest neighbors
plot(nn, x)

# visualize a reduced 2-NN graph
plot(kNN(nn, k = 2), x)

# Example 2: find kNN for query points
q &lt;- x[c(1,100),]
nn &lt;- kNN(x, k = 10, query = q)

plot(nn, x, col = "grey")
points(q, pch = 3, lwd = 2)

# Example 3: find kNN using distances
d &lt;- dist(x, method = "manhattan")
nn &lt;- kNN(d, k = 1)
plot(nn, x)
</code></pre>

<hr>
<h2 id='kNNdist'>Calculate and Plot k-Nearest Neighbor Distances</h2><span id='topic+kNNdist'></span><span id='topic+kNNdistplot'></span>

<h3>Description</h3>

<p>Fast calculation of the k-nearest neighbor distances for a dataset
represented as a matrix of points. The kNN distance is defined as the
distance from a point to its k nearest neighbor. The kNN distance plot
displays the kNN distance of all points sorted from smallest to largest. The
plot can be used to help find suitable parameter values for <code>dbscan()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kNNdist(x, k, all = FALSE, ...)

kNNdistplot(x, k, minPts, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kNNdist_+3A_x">x</code></td>
<td>
<p>the data set as a matrix of points (Euclidean distance is used) or
a precalculated <a href="stats.html#topic+dist">dist</a> object.</p>
</td></tr>
<tr><td><code id="kNNdist_+3A_k">k</code></td>
<td>
<p>number of nearest neighbors used for the distance calculation.</p>
</td></tr>
<tr><td><code id="kNNdist_+3A_all">all</code></td>
<td>
<p>should a matrix with the distances to all k nearest neighbors be
returned?</p>
</td></tr>
<tr><td><code id="kNNdist_+3A_...">...</code></td>
<td>
<p>further arguments (e.g., kd-tree related parameters) are passed
on to <code><a href="#topic+kNN">kNN()</a></code>.</p>
</td></tr>
<tr><td><code id="kNNdist_+3A_minpts">minPts</code></td>
<td>
<p>to use a k-NN plot to determine a suitable <code>eps</code> value for <code><a href="#topic+dbscan">dbscan()</a></code>,
<code>minPts</code> used in dbscan can be specified and will set <code>k = minPts - 1</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>kNNdist()</code> returns a numeric vector with the distance to its k
nearest neighbor. If <code>all = TRUE</code> then a matrix with k columns
containing the distances to all 1st, 2nd, ..., kth nearest neighbors is
returned instead.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other Outlier Detection Functions: 
<code><a href="#topic+glosh">glosh</a>()</code>,
<code><a href="#topic+lof">lof</a>()</code>,
<code><a href="#topic+pointdensity">pointdensity</a>()</code>
</p>
<p>Other NN functions: 
<code><a href="#topic+NN">NN</a></code>,
<code><a href="#topic+comps">comps</a>()</code>,
<code><a href="#topic+frNN">frNN</a>()</code>,
<code><a href="#topic+kNN">kNN</a>()</code>,
<code><a href="#topic+sNN">sNN</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
iris &lt;- as.matrix(iris[, 1:4])

## Find the 4-NN distance for each observation (see ?kNN
## for different search strategies)
kNNdist(iris, k = 4)

## Get a matrix with distances to the 1st, 2nd, ..., 4th NN.
kNNdist(iris, k = 4, all = TRUE)

## Produce a k-NN distance plot to determine a suitable eps for
## DBSCAN with MinPts = 5. Use k = 4 (= MinPts -1).
## The knee is visible around a distance of .7
kNNdistplot(iris, k = 4)

cl &lt;- dbscan(iris, eps = .7, minPts = 5)
pairs(iris, col = cl$cluster + 1L)
## Note: black points are noise points
</code></pre>

<hr>
<h2 id='lof'>Local Outlier Factor Score</h2><span id='topic+lof'></span><span id='topic+LOF'></span>

<h3>Description</h3>

<p>Calculate the Local Outlier Factor (LOF) score for each data point using a
kd-tree to speed up kNN search.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lof(x, minPts = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lof_+3A_x">x</code></td>
<td>
<p>a data matrix or a <a href="stats.html#topic+dist">dist</a> object.</p>
</td></tr>
<tr><td><code id="lof_+3A_minpts">minPts</code></td>
<td>
<p>number of nearest neighbors used in defining the local
neighborhood of a point (includes the point itself).</p>
</td></tr>
<tr><td><code id="lof_+3A_...">...</code></td>
<td>
<p>further arguments are passed on to <code><a href="#topic+kNN">kNN()</a></code>.
Note: <code>sort</code> cannot be specified here since <code>lof()</code>
uses always <code>sort = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>LOF compares the local readability density (lrd) of an point to the lrd of
its neighbors. A LOF score of approximately 1 indicates that the lrd around
the point is comparable to the lrd of its neighbors and that the point is
not an outlier. Points that have a substantially lower lrd than their
neighbors are considered outliers and produce scores significantly larger
than 1.
</p>
<p>If a data matrix is specified, then Euclidean distances and fast nearest
neighbor search using a kd-tree is used.
</p>
<p><strong>Note on duplicate points:</strong> If there are more than <code>minPts</code>
duplicates of a point in the data, then LOF the local readability distance
will be 0 resulting in an undefined LOF score of 0/0. We set LOF in this
case to 1 since there is already enough density from the points in the same
location to make them not outliers. The original paper by Breunig et al
(2000) assumes that the points are real duplicates and suggests to remove
the duplicates before computing LOF. If duplicate points are removed first,
then this LOF implementation in <span class="pkg">dbscan</span> behaves like the one described
by Breunig et al.
</p>


<h3>Value</h3>

<p>A numeric vector of length <code>ncol(x)</code> containing LOF values for
all data points.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Breunig, M., Kriegel, H., Ng, R., and Sander, J. (2000). LOF:
identifying density-based local outliers. In <em>ACM Int. Conf. on
Management of Data,</em> pages 93-104.
<a href="https://doi.org/10.1145/335191.335388">doi:10.1145/335191.335388</a>
</p>


<h3>See Also</h3>

<p>Other Outlier Detection Functions: 
<code><a href="#topic+glosh">glosh</a>()</code>,
<code><a href="#topic+kNNdist">kNNdist</a>()</code>,
<code><a href="#topic+pointdensity">pointdensity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(665544)
n &lt;- 100
x &lt;- cbind(
  x=runif(10, 0, 5) + rnorm(n, sd = 0.4),
  y=runif(10, 0, 5) + rnorm(n, sd = 0.4)
  )

### calculate LOF score with a neighborhood of 3 points
lof &lt;- lof(x, minPts = 3)

### distribution of outlier factors
summary(lof)
hist(lof, breaks = 10, main = "LOF (minPts = 3)")

### plot sorted lof. Looks like outliers start arounf a LOF of 2.
plot(sort(lof), type = "l",  main = "LOF (minPts = 3)",
  xlab = "Points sorted by LOF", ylab = "LOF")

### point size is proportional to LOF and mark points with a LOF &gt; 2
plot(x, pch = ".", main = "LOF (minPts = 3)", asp = 1)
points(x, cex = (lof - 1) * 2, pch = 1, col = "red")
text(x[lof &gt; 2,], labels = round(lof, 1)[lof &gt; 2], pos = 3)
</code></pre>

<hr>
<h2 id='moons'>Moons Data</h2><span id='topic+moons'></span>

<h3>Description</h3>

<p>Contains 100 2-d points, half of which are contained in two moons or
&quot;blobs&quot;&quot; (25 points each blob), and the other half in asymmetric facing
crescent shapes. The three shapes are all linearly separable.
</p>


<h3>Format</h3>

<p>A data frame with 100 observations on the following 2 variables.
</p>

<dl>
<dt>X</dt><dd><p>a numeric vector</p>
</dd>
<dt>Y</dt><dd><p>a numeric vector</p>
</dd> </dl>



<h3>Details</h3>

<p>This data was generated with the following Python commands using the
SciKit-Learn library:
</p>
<p><code style="white-space: pre;">&#8288;&gt; import sklearn.datasets as data&#8288;</code>
</p>
<p><code style="white-space: pre;">&#8288;&gt; moons = data.make_moons(n_samples=50, noise=0.05)&#8288;</code>
</p>
<p><code style="white-space: pre;">&#8288;&gt; blobs = data.make_blobs(n_samples=50, centers=[(-0.75,2.25), (1.0, 2.0)], cluster_std=0.25)&#8288;</code>
</p>
<p><code style="white-space: pre;">&#8288;&gt; test_data = np.vstack([moons, blobs])&#8288;</code>
</p>


<h3>Source</h3>

<p>See the HDBSCAN notebook from github documentation:
<a href="http://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html">http://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html</a>
</p>


<h3>References</h3>

<p>Pedregosa, Fabian, Gael Varoquaux, Alexandre Gramfort,
Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel et al.
Scikit-learn: Machine learning in Python. <em>Journal of Machine Learning
Research</em> 12, no. Oct (2011): 2825-2830.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(moons)
plot(moons, pch=20)
</code></pre>

<hr>
<h2 id='NN'>NN &mdash; Nearest Neighbors Superclass</h2><span id='topic+NN'></span><span id='topic+adjacencylist'></span><span id='topic+adjacencylist.NN'></span><span id='topic+sort.NN'></span><span id='topic+plot.NN'></span>

<h3>Description</h3>

<p>NN is an abstract S3 superclass for the classes of the objects returned
by <code><a href="#topic+kNN">kNN()</a></code>, <code><a href="#topic+frNN">frNN()</a></code> and <code><a href="#topic+sNN">sNN()</a></code>. Methods for sorting, plotting and getting an
adjacency list are defined.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjacencylist(x, ...)

## S3 method for class 'NN'
adjacencylist(x, ...)

## S3 method for class 'NN'
sort(x, decreasing = FALSE, ...)

## S3 method for class 'NN'
plot(x, data, main = NULL, pch = 16, col = NULL, linecol = "gray", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NN_+3A_x">x</code></td>
<td>
<p>a <code>NN</code> object</p>
</td></tr>
<tr><td><code id="NN_+3A_...">...</code></td>
<td>
<p>further parameters past on to <code><a href="graphics.html#topic+plot">plot()</a></code>.</p>
</td></tr>
<tr><td><code id="NN_+3A_decreasing">decreasing</code></td>
<td>
<p>sort in decreasing order?</p>
</td></tr>
<tr><td><code id="NN_+3A_data">data</code></td>
<td>
<p>that was used to create <code>x</code></p>
</td></tr>
<tr><td><code id="NN_+3A_main">main</code></td>
<td>
<p>title</p>
</td></tr>
<tr><td><code id="NN_+3A_pch">pch</code></td>
<td>
<p>plotting character.</p>
</td></tr>
<tr><td><code id="NN_+3A_col">col</code></td>
<td>
<p>color used for the data points (nodes).</p>
</td></tr>
<tr><td><code id="NN_+3A_linecol">linecol</code></td>
<td>
<p>color used for edges.</p>
</td></tr>
</table>


<h3>Subclasses</h3>

<p><a href="#topic+kNN">kNN</a>, <a href="#topic+frNN">frNN</a> and <a href="#topic+sNN">sNN</a>
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other NN functions: 
<code><a href="#topic+comps">comps</a>()</code>,
<code><a href="#topic+frNN">frNN</a>()</code>,
<code><a href="#topic+kNNdist">kNNdist</a>()</code>,
<code><a href="#topic+kNN">kNN</a>()</code>,
<code><a href="#topic+sNN">sNN</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- iris[, -5]

# finding kNN directly in data (using a kd-tree)
nn &lt;- kNN(x, k=5)
nn

# plot the kNN where NN are shown as line conecting points.
plot(nn, x)

# show the first few elements of the adjacency list
head(adjacencylist(nn))

## Not run: 
# create a graph and find connected components (if igraph is installed)
library("igraph")
g &lt;- graph_from_adj_list(adjacencylist(nn))
comp &lt;- components(g)
plot(x, col = comp$membership)

# detect clusters (communities) with the label propagation algorithm
cl &lt;- membership(cluster_label_prop(g))
plot(x, col = cl)

## End(Not run)
</code></pre>

<hr>
<h2 id='optics'>Ordering Points to Identify the Clustering Structure (OPTICS)</h2><span id='topic+optics'></span><span id='topic+OPTICS'></span><span id='topic+print.optics'></span><span id='topic+plot.optics'></span><span id='topic+as.reachability.optics'></span><span id='topic+as.dendrogram.optics'></span><span id='topic+extractDBSCAN'></span><span id='topic+extractXi'></span><span id='topic+predict.optics'></span>

<h3>Description</h3>

<p>Implementation of the OPTICS (Ordering points to identify the clustering
structure) point ordering algorithm using a kd-tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optics(x, eps = NULL, minPts = 5, ...)

## S3 method for class 'optics'
print(x, ...)

## S3 method for class 'optics'
plot(x, cluster = TRUE, predecessor = FALSE, ...)

## S3 method for class 'optics'
as.reachability(object, ...)

## S3 method for class 'optics'
as.dendrogram(object, ...)

extractDBSCAN(object, eps_cl)

extractXi(object, xi, minimum = FALSE, correctPredecessors = TRUE)

## S3 method for class 'optics'
predict(object, newdata, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optics_+3A_x">x</code></td>
<td>
<p>a data matrix or a <a href="stats.html#topic+dist">dist</a> object.</p>
</td></tr>
<tr><td><code id="optics_+3A_eps">eps</code></td>
<td>
<p>upper limit of the size of the epsilon neighborhood. Limiting the
neighborhood size improves performance and has no or very little impact on
the ordering as long as it is not set too low. If not specified, the largest
minPts-distance in the data set is used which gives the same result as
infinity.</p>
</td></tr>
<tr><td><code id="optics_+3A_minpts">minPts</code></td>
<td>
<p>the parameter is used to identify dense neighborhoods and the
reachability distance is calculated as the distance to the minPts nearest
neighbor. Controls the smoothness of the reachability distribution. Default
is 5 points.</p>
</td></tr>
<tr><td><code id="optics_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on to fixed-radius nearest
neighbor search algorithm. See <code><a href="#topic+frNN">frNN()</a></code> for details on how to
control the search strategy.</p>
</td></tr>
<tr><td><code id="optics_+3A_cluster">cluster</code>, <code id="optics_+3A_predecessor">predecessor</code></td>
<td>
<p>plot clusters and predecessors.</p>
</td></tr>
<tr><td><code id="optics_+3A_object">object</code></td>
<td>
<p>clustering object.</p>
</td></tr>
<tr><td><code id="optics_+3A_eps_cl">eps_cl</code></td>
<td>
<p>Threshold to identify clusters (<code>eps_cl &lt;= eps</code>).</p>
</td></tr>
<tr><td><code id="optics_+3A_xi">xi</code></td>
<td>
<p>Steepness threshold to identify clusters hierarchically using the
Xi method.</p>
</td></tr>
<tr><td><code id="optics_+3A_minimum">minimum</code></td>
<td>
<p>logical, representing whether or not to extract the minimal
(non-overlapping) clusters in the Xi clustering algorithm.</p>
</td></tr>
<tr><td><code id="optics_+3A_correctpredecessors">correctPredecessors</code></td>
<td>
<p>logical, correct a common artifact by pruning
the steep up area for points that have predecessors not in the
cluster&ndash;found by the ELKI framework, see details below.</p>
</td></tr>
<tr><td><code id="optics_+3A_newdata">newdata</code></td>
<td>
<p>new data points for which the cluster membership should be
predicted.</p>
</td></tr>
<tr><td><code id="optics_+3A_data">data</code></td>
<td>
<p>the data set used to create the clustering object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>The algorithm</strong>
</p>
<p>This implementation of OPTICS implements the original
algorithm as described by Ankerst et al (1999). OPTICS is an ordering
algorithm with methods to extract a clustering from the ordering.
While using similar concepts as DBSCAN, for OPTICS <code>eps</code>
is only an upper limit for the neighborhood size used to reduce
computational complexity. Note that <code>minPts</code> in OPTICS has a different
effect then in DBSCAN. It is used to define dense neighborhoods, but since
<code>eps</code> is typically set rather high, this does not effect the ordering
much. However, it is also used to calculate the reachability distance and
larger values will make the reachability distance plot smoother.
</p>
<p>OPTICS linearly orders the data points such that points which are spatially
closest become neighbors in the ordering. The closest analog to this
ordering is dendrogram in single-link hierarchical clustering. The algorithm
also calculates the reachability distance for each point.
<code>plot()</code> (see <a href="#topic+reachability_plot">reachability_plot</a>)
produces a reachability plot which shows each points reachability distance
between two consecutive points
where the points are sorted by OPTICS. Valleys represent clusters (the
deeper the valley, the more dense the cluster) and high points indicate
points between clusters.
</p>
<p><strong>Specifying the data</strong>
</p>
<p>If <code>x</code> is specified as a data matrix, then Euclidean distances and fast
nearest neighbor lookup using a kd-tree are used. See <code><a href="#topic+kNN">kNN()</a></code> for
details on the parameters for the kd-tree.
</p>
<p><strong>Extracting a clustering</strong>
</p>
<p>Several methods to extract a clustering from the order returned by OPTICS are
implemented:
</p>

<ul>
<li> <p><code>extractDBSCAN()</code> extracts a clustering from an OPTICS ordering that is
similar to what DBSCAN would produce with an eps set to <code>eps_cl</code> (see
Ankerst et al, 1999). The only difference to a DBSCAN clustering is that
OPTICS is not able to assign some border points and reports them instead as
noise.
</p>
</li>
<li> <p><code>extractXi()</code> extract clusters hierarchically specified in Ankerst et al
(1999) based on the steepness of the reachability plot. One interpretation
of the <code>xi</code> parameter is that it classifies clusters by change in
relative cluster density. The used algorithm was originally contributed by
the ELKI framework and is explained in Schubert et al (2018), but contains a
set of fixes.
</p>
</li></ul>

<p><strong>Predict cluster memberships</strong>
</p>
<p><code>predict()</code> requires an extracted DBSCAN clustering with <code>extractDBSCAN()</code> and then
uses predict for <code>dbscan()</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>optics</code> with components:
</p>
<table>
<tr><td><code>eps</code></td>
<td>
<p> value of <code>eps</code> parameter. </p>
</td></tr>
<tr><td><code>minPts</code></td>
<td>
<p> value of <code>minPts</code> parameter. </p>
</td></tr>
<tr><td><code>order</code></td>
<td>
<p> optics order for the data points in <code>x</code>. </p>
</td></tr>
<tr><td><code>reachdist</code></td>
<td>
 <p><a href="#topic+reachability">reachability</a> distance for each data point in <code>x</code>. </p>
</td></tr>
<tr><td><code>coredist</code></td>
<td>
<p> core distance for each data point in <code>x</code>. </p>
</td></tr>
</table>
<p>For <code>extractDBSCAN()</code>, in addition the following
components are available:
</p>
<table>
<tr><td><code>eps_cl</code></td>
<td>
<p> the value of the <code>eps_cl</code> parameter. </p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p> assigned cluster labels in the order of the data points in <code>x</code>. </p>
</td></tr>
</table>
<p>For <code>extractXi()</code>, in addition the following components
are available:
</p>
<table>
<tr><td><code>xi</code></td>
<td>
<p> Steepness threshold<code>x</code>. </p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p> assigned cluster labels in the order of the data points in <code>x</code>.</p>
</td></tr>
<tr><td><code>clusters_xi</code></td>
<td>
<p> data.frame containing the start and end of each cluster
found in the OPTICS ordering. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Hahsler and Matthew Piekenbrock
</p>


<h3>References</h3>

<p>Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, Joerg
Sander (1999). OPTICS: Ordering Points To Identify the Clustering Structure.
<em>ACM SIGMOD international conference on Management of data.</em> ACM Press. pp.
<a href="https://doi.org/10.1145/304181.304187">doi:10.1145/304181.304187</a>
</p>
<p>Hahsler M, Piekenbrock M, Doran D (2019). dbscan: Fast Density-Based
Clustering with R.  <em>Journal of Statistical Software</em>, 91(1), 1-30.
<a href="https://doi.org/10.18637/jss.v091.i01">doi:10.18637/jss.v091.i01</a>
</p>
<p>Erich Schubert, Michael Gertz (2018). Improving the Cluster Structure
Extracted from OPTICS Plots. In <em>Lernen, Wissen, Daten, Analysen (LWDA 2018),</em>
pp. 318-329.
</p>


<h3>See Also</h3>

<p>Density <a href="#topic+reachability">reachability</a>.
</p>
<p>Other clustering functions: 
<code><a href="#topic+dbscan">dbscan</a>()</code>,
<code><a href="#topic+extractFOSC">extractFOSC</a>()</code>,
<code><a href="#topic+hdbscan">hdbscan</a>()</code>,
<code><a href="#topic+jpclust">jpclust</a>()</code>,
<code><a href="#topic+sNNclust">sNNclust</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2)
n &lt;- 400

x &lt;- cbind(
  x = runif(4, 0, 1) + rnorm(n, sd = 0.1),
  y = runif(4, 0, 1) + rnorm(n, sd = 0.1)
  )

plot(x, col=rep(1:4, time = 100))

### run OPTICS (Note: we use the default eps calculation)
res &lt;- optics(x, minPts = 10)
res

### get order
res$order

### plot produces a reachability plot
plot(res)

### plot the order of points in the reachability plot
plot(x, col = "grey")
polygon(x[res$order, ])

### extract a DBSCAN clustering by cutting the reachability plot at eps_cl
res &lt;- extractDBSCAN(res, eps_cl = .065)
res

plot(res)  ## black is noise
hullplot(x, res)

### re-cut at a higher eps threshold
res &lt;- extractDBSCAN(res, eps_cl = .07)
res
plot(res)
hullplot(x, res)

### extract hierarchical clustering of varying density using the Xi method
res &lt;- extractXi(res, xi = 0.01)
res

plot(res)
hullplot(x, res)

# Xi cluster structure
res$clusters_xi

### use OPTICS on a precomputed distance matrix
d &lt;- dist(x)
res &lt;- optics(d, minPts = 10)
plot(res)
</code></pre>

<hr>
<h2 id='pointdensity'>Calculate Local Density at Each Data Point</h2><span id='topic+pointdensity'></span><span id='topic+density'></span>

<h3>Description</h3>

<p>Calculate the local density at each data point as either the number of
points in the eps-neighborhood (as used in <code>dbscan()</code>) or perform kernel density
estimation (KDE) using a uniform kernel. The function uses a kd-tree for fast
fixed-radius nearest neighbor search.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pointdensity(
  x,
  eps,
  type = "frequency",
  search = "kdtree",
  bucketSize = 10,
  splitRule = "suggest",
  approx = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pointdensity_+3A_x">x</code></td>
<td>
<p>a data matrix.</p>
</td></tr>
<tr><td><code id="pointdensity_+3A_eps">eps</code></td>
<td>
<p>radius of the eps-neighborhood, i.e., bandwidth of the uniform
kernel).</p>
</td></tr>
<tr><td><code id="pointdensity_+3A_type">type</code></td>
<td>
<p><code>"frequency"</code> or <code>"density"</code>. should the raw count of
points inside the eps-neighborhood or the kde be returned.</p>
</td></tr>
<tr><td><code id="pointdensity_+3A_search">search</code>, <code id="pointdensity_+3A_bucketsize">bucketSize</code>, <code id="pointdensity_+3A_splitrule">splitRule</code>, <code id="pointdensity_+3A_approx">approx</code></td>
<td>
<p>algorithmic parameters for
<code><a href="#topic+frNN">frNN()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dbscan()</code> estimates the density around a point as the number of points in the
eps-neighborhood of the point (including the query point itself).
Kernel density estimation (KDE) using a uniform kernel, which is just this point
count in the eps-neighborhood divided by <code class="reqn">(2\,eps\,n)</code>, where
<code class="reqn">n</code> is the number of points in <code>x</code>.
</p>
<p>Points with low local density often indicate noise (see e.g., Wishart (1969)
and Hartigan (1975)).
</p>


<h3>Value</h3>

<p>A vector of the same length as data points (rows) in <code>x</code> with
the count or density values for each data point.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Wishart, D. (1969), Mode Analysis: A Generalization of Nearest
Neighbor which Reduces Chaining Effects, in <em>Numerical Taxonomy,</em> Ed., A.J.
Cole, Academic Press, 282-311.
</p>
<p>John A. Hartigan (1975), <em>Clustering Algorithms,</em> John Wiley &amp; Sons, Inc.,
New York, NY, USA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+frNN">frNN()</a></code>, <code><a href="stats.html#topic+density">stats::density()</a></code>.
</p>
<p>Other Outlier Detection Functions: 
<code><a href="#topic+glosh">glosh</a>()</code>,
<code><a href="#topic+kNNdist">kNNdist</a>()</code>,
<code><a href="#topic+lof">lof</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(665544)
n &lt;- 100
x &lt;- cbind(
  x=runif(10, 0, 5) + rnorm(n, sd = 0.4),
  y=runif(10, 0, 5) + rnorm(n, sd = 0.4)
  )
plot(x)

### calculate density
d &lt;- pointdensity(x, eps = .5, type = "density")

### density distribution
summary(d)
hist(d, breaks = 10)

### plot with point size is proportional to Density
plot(x, pch = 19, main = "Density (eps = .5)", cex = d*5)

### Wishart (1969) single link clustering after removing low-density noise
# 1. remove noise with low density
f &lt;- pointdensity(x, eps = .5, type = "frequency")
x_nonoise &lt;- x[f &gt;= 5,]

# 2. use single-linkage on the non-noise points
hc &lt;- hclust(dist(x_nonoise), method = "single")
plot(x, pch = 19, cex = .5)
points(x_nonoise, pch = 19, col= cutree(hc, k = 4) + 1L)
</code></pre>

<hr>
<h2 id='reachability'>Reachability Distances</h2><span id='topic+reachability'></span><span id='topic+reachability_plot'></span><span id='topic+print.reachability'></span><span id='topic+plot.reachability'></span><span id='topic+as.reachability'></span><span id='topic+as.reachability.dendrogram'></span>

<h3>Description</h3>

<p>Reachability distances can be plotted to show the hierarchical relationships between data points.
The idea was originally introduced by Ankerst et al (1999) for <a href="#topic+OPTICS">OPTICS</a>. Later,
Sanders et al (2003) showed that the visualization is useful for other hierarchical
structures and introduced an algorithm to convert <a href="#topic+dendrogram">dendrogram</a> representation to
reachability plots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'reachability'
print(x, ...)

## S3 method for class 'reachability'
plot(
  x,
  order_labels = FALSE,
  xlab = "Order",
  ylab = "Reachability dist.",
  main = "Reachability Plot",
  ...
)

as.reachability(object, ...)

## S3 method for class 'dendrogram'
as.reachability(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reachability_+3A_x">x</code></td>
<td>
<p>object of class <code>reachability</code>.</p>
</td></tr>
<tr><td><code id="reachability_+3A_...">...</code></td>
<td>
<p>graphical parameters are passed on to <code>plot()</code>,
or arguments for other methods.</p>
</td></tr>
<tr><td><code id="reachability_+3A_order_labels">order_labels</code></td>
<td>
<p>whether to plot text labels for each points reachability
distance.</p>
</td></tr>
<tr><td><code id="reachability_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label.</p>
</td></tr>
<tr><td><code id="reachability_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label.</p>
</td></tr>
<tr><td><code id="reachability_+3A_main">main</code></td>
<td>
<p>Title of the plot.</p>
</td></tr>
<tr><td><code id="reachability_+3A_object">object</code></td>
<td>
<p>any object that can be coerced to class
<code>reachability</code>, such as an object of class <a href="#topic+optics">optics</a> or <a href="stats.html#topic+dendrogram">stats::dendrogram</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A reachability plot displays the points as vertical bars, were the height is the
reachability distance between two consecutive points.
The central idea behind reachability plots is that the ordering in which
points are plotted identifies underlying hierarchical density
representation as mountains and valleys of high and low reachability distance.
The original ordering algorithm OPTICS as described by Ankerst et al (1999)
introduced the notion of reachability plots.
</p>
<p>OPTICS linearly orders the data points such that points
which are spatially closest become neighbors in the ordering. Valleys
represent clusters, which can be represented hierarchically. Although the
ordering is crucial to the structure of the reachability plot, its important
to note that OPTICS, like DBSCAN, is not entirely deterministic and, just
like the dendrogram, isomorphisms may exist
</p>
<p>Reachability plots were shown to essentially convey the same information as
the more traditional dendrogram structure by Sanders et al (2003). An dendrograms
can be converted into reachability plots.
</p>
<p>Different hierarchical representations, such as dendrograms or reachability
plots, may be preferable depending on the context. In smaller datasets,
cluster memberships may be more easily identifiable through a dendrogram
representation, particularly is the user is already familiar with tree-like
representations. For larger datasets however, a reachability plot may be
preferred for visualizing macro-level density relationships.
</p>
<p>A variety of cluster extraction methods have been proposed using
reachability plots. Because both cluster extraction depend directly on the
ordering OPTICS produces, they are part of the <code><a href="#topic+optics">optics()</a></code> interface.
Nonetheless, reachability plots can be created directly from other types of
linkage trees, and vice versa.
</p>
<p><em>Note:</em> The reachability distance for the first point is by definition not defined
(it has no preceeding point).
Also, the reachability distances can be undefined when a point does not have enough
neighbors in the epsilon neighborhood. We represent these undefined cases as <code>Inf</code>
and represent them in the plot as a dashed line.
</p>


<h3>Value</h3>

<p>An object of class <code>reachability</code> with components:
</p>
<table>
<tr><td><code>order</code></td>
<td>
<p>order to use for the data points in <code>x</code>. </p>
</td></tr>
<tr><td><code>reachdist</code></td>
<td>
<p>reachability distance for each data point in <code>x</code>. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthew Piekenbrock
</p>


<h3>References</h3>

<p>Ankerst, M., M. M. Breunig, H.-P. Kriegel, J. Sander (1999).
OPTICS: Ordering Points To Identify the Clustering Structure. <em>ACM
SIGMOD international conference on Management of data.</em> ACM Press. pp.
49&ndash;60.
</p>
<p>Sander, J., X. Qin, Z. Lu, N. Niu, and A. Kovarsky (2003). Automatic
extraction of clusters from hierarchical clustering representations.
<em>Pacific-Asia Conference on Knowledge Discovery and Data Mining.</em>
Springer Berlin Heidelberg.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+optics">optics()</a></code>, <code><a href="#topic+as.dendrogram">as.dendrogram()</a></code>, and <code><a href="stats.html#topic+hclust">stats::hclust()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2)
n &lt;- 20

x &lt;- cbind(
  x = runif(4, 0, 1) + rnorm(n, sd = 0.1),
  y = runif(4, 0, 1) + rnorm(n, sd = 0.1)
)

plot(x, xlim = range(x), ylim = c(min(x) - sd(x), max(x) + sd(x)), pch = 20)
text(x = x, labels = 1:nrow(x), pos = 3)

### run OPTICS
res &lt;- optics(x, eps = 10,  minPts = 2)
res

### plot produces a reachability plot.
plot(res)

### Manually extract reachability components from OPTICS
reach &lt;- as.reachability(res)
reach

### plot still produces a reachability plot; points ids
### (rows in the original data) can be displayed with order_labels = TRUE
plot(reach, order_labels = TRUE)

### Reachability objects can be directly converted to dendrograms
dend &lt;- as.dendrogram(reach)
dend
plot(dend)

### A dendrogram can be converted back into a reachability object
plot(as.reachability(dend))
</code></pre>

<hr>
<h2 id='sNN'>Find Shared Nearest Neighbors</h2><span id='topic+sNN'></span><span id='topic+snn'></span><span id='topic+sort.sNN'></span><span id='topic+print.sNN'></span>

<h3>Description</h3>

<p>Calculates the number of shared nearest neighbors, the shared nearest
neighbor similarity and creates a shared nearest neighbors graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sNN(
  x,
  k,
  kt = NULL,
  jp = FALSE,
  sort = TRUE,
  search = "kdtree",
  bucketSize = 10,
  splitRule = "suggest",
  approx = 0
)

## S3 method for class 'sNN'
sort(x, decreasing = TRUE, ...)

## S3 method for class 'sNN'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sNN_+3A_x">x</code></td>
<td>
<p>a data matrix, a <a href="stats.html#topic+dist">dist</a> object or a <a href="#topic+kNN">kNN</a> object.</p>
</td></tr>
<tr><td><code id="sNN_+3A_k">k</code></td>
<td>
<p>number of neighbors to consider to calculate the shared nearest
neighbors.</p>
</td></tr>
<tr><td><code id="sNN_+3A_kt">kt</code></td>
<td>
<p>minimum threshold on the number of shared nearest neighbors to
build the shared nearest neighbor graph. Edges are only preserved if
<code>kt</code> or more neighbors are shared.</p>
</td></tr>
<tr><td><code id="sNN_+3A_jp">jp</code></td>
<td>
<p>use the definition by Javis and Patrick (1973), where shared
neighbors are only counted between points that are in each other's
neighborhood, otherwise 0 is returned. If <code>FALSE</code>, then the number of shared
neighbors is returned, even if the points are not neighbors.</p>
</td></tr>
<tr><td><code id="sNN_+3A_sort">sort</code></td>
<td>
<p>sort by the number of shared nearest neighbors? Note that this
is expensive and <code>sort = FALSE</code> is much faster. sNN objects can be
sorted using <code>sort()</code>.</p>
</td></tr>
<tr><td><code id="sNN_+3A_search">search</code></td>
<td>
<p>nearest neighbor search strategy (one of <code>"kdtree"</code>, <code>"linear"</code> or
<code>"dist"</code>).</p>
</td></tr>
<tr><td><code id="sNN_+3A_bucketsize">bucketSize</code></td>
<td>
<p>max size of the kd-tree leafs.</p>
</td></tr>
<tr><td><code id="sNN_+3A_splitrule">splitRule</code></td>
<td>
<p>rule to split the kd-tree. One of <code>"STD"</code>, <code>"MIDPT"</code>, <code>"FAIR"</code>,
<code>"SL_MIDPT"</code>, <code>"SL_FAIR"</code> or <code>"SUGGEST"</code> (SL stands for sliding). <code>"SUGGEST"</code> uses
ANNs best guess.</p>
</td></tr>
<tr><td><code id="sNN_+3A_approx">approx</code></td>
<td>
<p>use approximate nearest neighbors. All NN up to a distance of
a factor of <code style="white-space: pre;">&#8288;(1 + approx) eps&#8288;</code> may be used. Some actual NN may be omitted
leading to spurious clusters and noise points.  However, the algorithm will
enjoy a significant speedup.</p>
</td></tr>
<tr><td><code id="sNN_+3A_decreasing">decreasing</code></td>
<td>
<p>logical; sort in decreasing order?</p>
</td></tr>
<tr><td><code id="sNN_+3A_...">...</code></td>
<td>
<p>additional parameters are passed on.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of shared nearest neighbors is the intersection of the kNN
neighborhood of two points. Note: that each point is considered to be part
of its own kNN neighborhood. The range for the shared nearest neighbors is
<code class="reqn">[0, k]</code>.
</p>
<p>Javis and Patrick (1973) use the shared nearest neighbor graph for
clustering. They only count shared neighbors between points that are in each
other's kNN neighborhood.
</p>


<h3>Value</h3>

<p>An object of class <code>sNN</code> (subclass of <a href="#topic+kNN">kNN</a> and <a href="#topic+NN">NN</a>) containing a list
with the following components:
</p>
<table>
<tr><td><code>id</code></td>
<td>
<p>a matrix with ids. </p>
</td></tr>
<tr><td><code>dist</code></td>
<td>
<p>a matrix with the distances. </p>
</td></tr>
<tr><td><code>shared</code></td>
<td>
<p>a matrix with the number of shared nearest neighbors. </p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of <code>k</code> used. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>R. A. Jarvis and E. A. Patrick. 1973. Clustering Using a
Similarity Measure Based on Shared Near Neighbors. <em>IEEE Trans. Comput.</em>
22, 11 (November 1973), 1025-1034.
<a href="https://doi.org/10.1109/T-C.1973.223640">doi:10.1109/T-C.1973.223640</a>
</p>


<h3>See Also</h3>

<p>Other NN functions: 
<code><a href="#topic+NN">NN</a></code>,
<code><a href="#topic+comps">comps</a>()</code>,
<code><a href="#topic+frNN">frNN</a>()</code>,
<code><a href="#topic+kNNdist">kNNdist</a>()</code>,
<code><a href="#topic+kNN">kNN</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- iris[, -5]

# finding kNN and add the number of shared nearest neighbors.
k &lt;- 5
nn &lt;- sNN(x, k = k)
nn

# shared nearest neighbor distribution
table(as.vector(nn$shared))

# explore neighborhood of point 10
i &lt;- 10
nn$shared[i,]

plot(nn, x)

# apply a threshold to create a sNN graph with edges
# if more than 3 neighbors are shared.
nn_3 &lt;- sNN(nn, kt = 3)
plot(nn_3, x)

# get an adjacency list for the shared nearest neighbor graph
adjacencylist(nn_3)
</code></pre>

<hr>
<h2 id='sNNclust'>Shared Nearest Neighbor Clustering</h2><span id='topic+sNNclust'></span><span id='topic+snnclust'></span>

<h3>Description</h3>

<p>Implements the shared nearest neighbor clustering algorithm by Ertoz,
Steinbach and Kumar (2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sNNclust(x, k, eps, minPts, borderPoints = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sNNclust_+3A_x">x</code></td>
<td>
<p>a data matrix/data.frame (Euclidean distance is used), a
precomputed <a href="stats.html#topic+dist">dist</a> object or a kNN object created with <code><a href="#topic+kNN">kNN()</a></code>.</p>
</td></tr>
<tr><td><code id="sNNclust_+3A_k">k</code></td>
<td>
<p>Neighborhood size for nearest neighbor sparsification to create the
shared NN graph.</p>
</td></tr>
<tr><td><code id="sNNclust_+3A_eps">eps</code></td>
<td>
<p>Two objects are only reachable from each other if they share at
least <code>eps</code> nearest neighbors. Note: this is different from the <code>eps</code> in DBSCAN!</p>
</td></tr>
<tr><td><code id="sNNclust_+3A_minpts">minPts</code></td>
<td>
<p>minimum number of points that share at least <code>eps</code>
nearest neighbors for a point to be considered a core points.</p>
</td></tr>
<tr><td><code id="sNNclust_+3A_borderpoints">borderPoints</code></td>
<td>
<p>should border points be assigned to clusters like in
<a href="#topic+DBSCAN">DBSCAN</a>?</p>
</td></tr>
<tr><td><code id="sNNclust_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on to the k nearest neighbor
search algorithm. See <code><a href="#topic+kNN">kNN()</a></code> for details on how to control the
search strategy.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Algorithm:</strong>
</p>

<ol>
<li><p> Constructs a shared nearest neighbor graph for a given k. The edge
weights are the number of shared k nearest neighbors (in the range of
<code class="reqn">[0, k]</code>).
</p>
</li>
<li><p> Find each points SNN density, i.e., the number of points which have a
similarity of <code>eps</code> or greater.
</p>
</li>
<li><p> Find the core points, i.e., all points that have an SNN density greater
than <code>MinPts</code>.
</p>
</li>
<li><p> Form clusters from the core points and assign border points (i.e.,
non-core points which share at least <code>eps</code> neighbors with a core point).
</p>
</li></ol>

<p>Note that steps 2-4 are equivalent to the DBSCAN algorithm (see <code><a href="#topic+dbscan">dbscan()</a></code>)
and that <code>eps</code> has a different meaning than for DBSCAN. Here it is
a threshold on the number of shared neighbors (see <code><a href="#topic+sNN">sNN()</a></code>)
which defines a similarity.
</p>


<h3>Value</h3>

<p>A object of class <code>general_clustering</code> with the following
components:
</p>
<table>
<tr><td><code>cluster</code></td>
<td>
<p>A integer vector with cluster assignments. Zero
indicates noise points.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p> name of used clustering algorithm.</p>
</td></tr>
<tr><td><code>param</code></td>
<td>
<p> list of used clustering parameters. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Levent Ertoz, Michael Steinbach, Vipin Kumar, Finding Clusters
of Different Sizes, Shapes, and Densities in Noisy, High Dimensional Data,
<em>SIAM International Conference on Data Mining,</em> 2003, 47-59.
<a href="https://doi.org/10.1137/1.9781611972733.5">doi:10.1137/1.9781611972733.5</a>
</p>


<h3>See Also</h3>

<p>Other clustering functions: 
<code><a href="#topic+dbscan">dbscan</a>()</code>,
<code><a href="#topic+extractFOSC">extractFOSC</a>()</code>,
<code><a href="#topic+hdbscan">hdbscan</a>()</code>,
<code><a href="#topic+jpclust">jpclust</a>()</code>,
<code><a href="#topic+optics">optics</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("DS3")

# Out of k = 20 NN 7 (eps) have to be shared to create a link in the sNN graph.
# A point needs a least 16 (minPts) links in the sNN graph to be a core point.
# Noise points have cluster id 0 and are shown in black.
cl &lt;- sNNclust(DS3, k = 20, eps = 7, minPts = 16)
plot(DS3, col = cl$cluster + 1L, cex = .5)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
