<!DOCTYPE html><html><head><title>Help for package AdaptGauss</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {AdaptGauss}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#AdaptGauss-package'>
<p>Gaussian Mixture Models (GMM)</p></a></li>
<li><a href='#AdaptGauss'><p>Adapt Gaussian Mixture Model (GMM)</p></a></li>
<li><a href='#Bayes4Mixtures'><p>Posterioris of Bayes Theorem</p></a></li>
<li><a href='#BayesClassification'><p>BayesClassification</p></a></li>
<li><a href='#BayesDecisionBoundaries'><p>Decision Boundaries calculated through Bayes Theorem</p></a></li>
<li><a href='#BayesFor2GMM'><p>Posterioris of Bayes Theorem for a two group GMM</p></a></li>
<li><a href='#CDFMixtures'><p>cumulative distribution of mixture model</p></a></li>
<li><a href='#Chi2testMixtures'><p>Pearson's chi-squared goodness of fit test</p></a></li>
<li><a href='#ClassifyByDecisionBoundaries'><p>Classify Data according to decision Boundaries</p></a></li>
<li><a href='#EMGauss'><p>EM Algorithm for GMM</p></a></li>
<li><a href='#GMMplot_ggplot2'>
<p>Plots the Gaussian Mixture Model (GMM) withing ggplot2</p></a></li>
<li><a href='#InformationCriteria4GMM'><p>Information Criteria For GMM</p></a></li>
<li><a href='#Intersect2Mixtures'><p>Intersect of two Gaussians</p></a></li>
<li><a href='#KStestMixtures'><p>Kolmogorov-Smirnov test</p></a></li>
<li><a href='#LikelihoodRatio4Mixtures'><p>Likelihood Ratio for Gaussian Mixtures</p></a></li>
<li><a href='#LKWFahrzeitSeehafen2010'>
<p>Truck driving time seaport 2010</p></a></li>
<li><a href='#LogLikelihood4Mixtures'><p>LogLikelihood for Gaussian Mixture Models</p></a></li>
<li><a href='#Pdf4Mixtures'><p>Calculates pdf for GMM</p></a></li>
<li><a href='#PlotMixtures'><p>Shows GMM</p></a></li>
<li><a href='#PlotMixturesAndBoundaries'><p>Shows GMM with Boundaries</p></a></li>
<li><a href='#QQplotGMM'><p>Quantile Quantile Plot of Data</p></a></li>
<li><a href='#RandomLogGMM'><p>Random Number Generator for Log or Gaussian Mixture Model</p></a></li>
<li><a href='#Symlognpdf'>
<p>computes a special case of log normal distribution density</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Gaussian Mixture Models (GMM)</td>
</tr>
<tr>
<td>Version:</td>
<td>1.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-02</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Thrun &lt;m.thrun@gmx.net&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Multimodal distributions can be modelled as a mixture of components. The model is derived using the Pareto Density Estimation (PDE) for an estimation of the pdf. PDE has been designed in particular to identify groups/classes in a dataset. Precise limits for the classes can be calculated using the theorem of Bayes. Verification of the model is possible by QQ plot, Chi-squared test and Kolmogorov-Smirnov test. The package is based on the publication of Ultsch, A., Thrun, M.C., Hansen-Goos, O., Lotsch, J. (2015)  &lt;<a href="https://doi.org/10.3390%2Fijms161025897">doi:10.3390/ijms161025897</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, shiny, pracma, methods, DataVisualizations, plotly</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mclust, grid, foreach, dqrng, parallelDist, knitr (&ge; 1.12),
rmarkdown (&ge; 0.9), reshape2, ggplot2</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.deepbionics.org">https://www.deepbionics.org</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/Mthrun/AdaptGauss/issues">https://github.com/Mthrun/AdaptGauss/issues</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-02 15:13:20 UTC; mct_l</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Thrun <a href="https://orcid.org/0000-0001-9542-5543"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Onno Hansen-Goos [aut, rev],
  Rabea Griese [ctr, ctb],
  Catharina Lippmann [ctr],
  Florian Lerch [ctb, rev],
  Quirin Stier [ctb, rev],
  Jorn Lotsch [dtc, rev, fnd, ctb],
  Luca Brinkmann [ctb, rev],
  Alfred Ultsch [aut, cph, ths]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-02 15:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='AdaptGauss-package'>
Gaussian Mixture Models (GMM)
</h2><span id='topic+AdaptGauss-package'></span><span id='topic+MultiModal'></span>

<h3>Description</h3>

<p>Multimodal distributions can be modelled as a mixture of components. The model is derived using the Pareto Density Estimation (PDE) for an estimation of the pdf. PDE has been designed in particular to identify groups/classes in a dataset. Precise limits for the classes can be calculated using the theorem of Bayes. Verification of the model is possible by QQ plot, Chi-squared test and Kolmogorov-Smirnov test. The package is based on the publication of Ultsch, A., Thrun, M.C., Hansen-Goos, O., Lotsch, J. (2015)  &lt;DOI:10.3390/ijms161025897&gt;.
</p>


<h3>Details</h3>

<p>Multimodal distributions can be modelled as a mixture of components. The model is derived using the Pareto Density Estimation (PDE) for an estimation of the pdf [Ultsch 2005]. PDE has been designed in particular to identify groups/classes in a dataset. The expectation maximization algorithm estimates a Gaussian mixture model of density states [Bishop 2006] and the limits between the different states are defined by Bayes decision boundaries [Duda 2001]. The model can be verified with Chi-squared test, Kolmogorov-Smirnov test and QQ plot.
</p>
<p>The correct number of modes may be found with AIC or BIC.
</p>

<p>Index:  This package was not yet installed at build time.<br />
</p>


<h3>Author(s)</h3>

<p>Michael Thrun, Onno Hansen-Goos, Rabea Griese, Catharina Lippmann, Florian Lerch, Jorn Lotsch, Alfred Ultsch
Maintainer: Michael Thrun &lt;m.thrun@gmx.net&gt;
</p>


<h3>References</h3>

<p>Ultsch, A., Thrun, M.C., Hansen-Goos, O., Loetsch, J.: Identification of Molecular Fingerprints in Human Heat Pain Thresholds by Use of an Interactive Mixture Model R Toolbox(AdaptGauss), International Journal of Molecular Sciences, doi:10.3390/ijms161025897, 2015.
</p>
<p>Duda, R.O., P.E. Hart, and D.G. Stork, Pattern classification. 2nd. Edition. New York, 2001, p 512 ff
</p>
<p>Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006, p 435 ff
</p>
<p>Ultsch, A.: Pareto density estimation: A density estimation for knowledge discovery, in  Baier, D.; Werrnecke, K. D., (Eds), Innovations in classification, data science, and information systems, Proc Gfkl 2003, pp 91-100, Springer, Berlin, 2005. 
</p>
<p>Thrun M.C., Ultsch, A.: Models of Income Distributions for Knowledge Discovery, European Conference on Data Analysis, DOI 10.13140/RG.2.1.4463.0244, Colchester 2015.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Statistically  significant GMM
## Not run: 
data=c(rnorm(3000,2,1),rnorm(3000,7,3),rnorm(3000,-2,0.5))

gmm=AdaptGauss::AdaptGauss(data,

Means = c(-2, 2, 7),

SDs = c(0.5, 1, 4),

Weights = c(0.3333, 0.3333, 0.3333))

AdaptGauss::Chi2testMixtures(data,

gmm$Means,gmm$SDs,gmm$Weights,PlotIt=T)

AdaptGauss::QQplotGMM(data,gmm$Means,gmm$SDs,gmm$Weights)

## End(Not run)

## Statistically non significant GMM
## Not run: 
data('LKWFahrzeitSeehafen2010')

gmm=AdaptGauss::AdaptGauss(LKWFahrzeitSeehafen2010,

Means = c(52.74, 385.38, 619.46, 162.08),

SDs = c(38.22, 93.21, 57.72, 48.36),

Weights = c(0.2434, 0.5589, 0.1484, 0.0749))

AdaptGauss::Chi2testMixtures(LKWFahrzeitSeehafen2010,

gmm$Means,gmm$SDs,gmm$Weights,PlotIt=T)

AdaptGauss::QQplotGMM(LKWFahrzeitSeehafen2010,gmm$Means,gmm$SDs,gmm$Weights)

## End(Not run)

</code></pre>

<hr>
<h2 id='AdaptGauss'>Adapt Gaussian Mixture Model (GMM)</h2><span id='topic+AdaptGauss'></span>

<h3>Description</h3>

<p>Adapt interactively a Gaussians Mixture Model GMM to the empirical PDF
of the data (generated by DataVisualizations::ParetoDensityEstimation) such that N(Means,SDs)*Weights is a  model for Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AdaptGauss(Data, Means = NaN, SDs = NaN, Weights = NaN,

                   ParetoRadius = NaN, LB = NaN, HB = NaN,
				   
                   ListOfAdaptGauss, fast = T)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AdaptGauss_+3A_data">Data</code></td>
<td>
<p> Data for empirical PDF. Has to be an Array of values. NaNs
and NULLs will be deleted </p>
</td></tr>
<tr><td><code id="AdaptGauss_+3A_means">Means</code></td>
<td>
<p> Optional: Means of gaussians of GMM. </p>
</td></tr>
<tr><td><code id="AdaptGauss_+3A_sds">SDs</code></td>
<td>
<p> Optional: StandardDevations of gaussians of GMM. (Has to be the same length as Means) </p>
</td></tr>
<tr><td><code id="AdaptGauss_+3A_weights">Weights</code></td>
<td>
<p> Optional: Weights of gaussians of GMM. (Has to be the same length as Means) </p>
</td></tr>
<tr><td><code id="AdaptGauss_+3A_paretoradius">ParetoRadius</code></td>
<td>
<p> Optional: Pareto Radius of Pareto Desity Estimation (PDE). </p>
</td></tr>
<tr><td><code id="AdaptGauss_+3A_lb">LB</code></td>
<td>
<p> Optional: Low boundary of estimation. All values below LB will be deleted. Default: min(Data) </p>
</td></tr>
<tr><td><code id="AdaptGauss_+3A_hb">HB</code></td>
<td>
<p> Optional: High boundary of estimation. All values above HB will be deleted. Default: max(Data) </p>
</td></tr>
<tr><td><code id="AdaptGauss_+3A_listofadaptgauss">ListOfAdaptGauss</code></td>
<td>
<p>Optional: If editing of an existing Model is the goal, enables to give the Output of AdaptGaus as the Input of AdaptGauss() instead of setting Means, SDs and Weights separately</p>
</td></tr>
<tr><td><code id="AdaptGauss_+3A_fast">fast</code></td>
<td>
<p>Default=TRUE; FALSE: Using mclust's EM see function <code>densityMclust</code> of that package, TRUE: Naive but faster EM implementation, which may be numerical unstable, because log(gauss) is not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data: maximum length is 10000. If larger, Data will be randomly reduced to 10000 Elements.
MeansIn/DeviationsIn/WeightsIN: If empty, either one or three Gaussian's are generated by kmeans algorithm.
Pareto Radius: If empty: will be generated by DataVisualizations::ParetoDensityEstimation
RMS:  Root Mean Square error is normalized by RMS of Gaussian's with Mean=mean(data) and SD=sd(data), see [Ultsch et.al., 2015] for further details.
</p>


<h3>Value</h3>

<p>List with
</p>
<table>
<tr><td><code>Means</code></td>
<td>
<p>Means of Gaussian's.</p>
</td></tr>
<tr><td><code>SDs</code></td>
<td>
<p>Standard SDs of Gaussian's.</p>
</td></tr>
<tr><td><code>Weights</code></td>
<td>
<p>Weights of Gaussian's.</p>
</td></tr>
<tr><td><code>ParetoRadius</code></td>
<td>
<p>Pareto Radius: Either ParetoRadiusIn, the pareto radius enerated by PretoDensityEstimation(if no Pareto Radius in Input).</p>
</td></tr>
<tr><td><code>RMS</code></td>
<td>
<p>Root Mean Square of Deviation between Gaussian Mixture Model GMM to the empirical PDF. Normalized by RMS of one Gaussian with mean=meanrobust(data) and sdev=stdrobust(data). Further Details in [Ultsch et al 2015]</p>
</td></tr>
<tr><td><code>BayesBoundaries</code></td>
<td>
<p>vector[1:L-1], Bayes decision boundaries</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Onno Hansen-Goos, Michael Thrun</p>


<h3>References</h3>

<p>Ultsch, A., Thrun, M.C., Hansen-Goos, O., Loetsch, J.: Identification of Molecular Fingerprints in Human Heat Pain Thresholds by Use of an Interactive Mixture Model R Toolbox(AdaptGauss), International Journal of Molecular Sciences, doi:10.3390/ijms161025897, 2015.
</p>
<p>Thrun M.C., Ultsch, A.: Models of Income Distributions for Knowledge Discovery, European Conference on Data Analysis, DOI 10.13140/RG.2.1.4463.0244, Colchester 2015.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data1=c(rnorm(1000))
  ## Not run: Vals1=AdaptGauss(data1)
  
  data2=c(rnorm(1000),rnorm(2000)+2,rnorm(1000)*2-1)
  ## Not run: Vals2=AdaptGauss(data2,c(-1,0,2),c(2,1,1),c(0.25,0.25,0.5),0.3,-6,6)
  
 </code></pre>

<hr>
<h2 id='Bayes4Mixtures'>Posterioris of Bayes Theorem</h2><span id='topic+Bayes4Mixtures'></span>

<h3>Description</h3>

<p>Calculates the posterioris of Bayes theorem
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Bayes4Mixtures(Data, Means, SDs, Weights, IsLogDistribution,
 PlotIt, CorrectBorders,Color,xlab,lwd)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bayes4Mixtures_+3A_data">Data</code></td>
<td>
<p>vector (1:N) of data points</p>
</td></tr>
<tr><td><code id="Bayes4Mixtures_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM),L ==  Number of Gaussians</p>
</td></tr>
<tr><td><code id="Bayes4Mixtures_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="Bayes4Mixtures_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="Bayes4Mixtures_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default  vector of zeros of length L</p>
</td></tr>
<tr><td><code id="Bayes4Mixtures_+3A_plotit">PlotIt</code></td>
<td>
<p>Optional, Default: FALSE; TRUE do a Plot </p>
</td></tr>
<tr><td><code id="Bayes4Mixtures_+3A_correctborders">CorrectBorders</code></td>
<td>
<p>Optional, ==TRUE data at right borders of GMM distribution will be  assigned to last gaussian, left border vice versa.
(default ==FALSE) normal Bayes Theorem</p>
</td></tr>
<tr><td><code id="Bayes4Mixtures_+3A_color">Color</code></td>
<td>
<p>Optional, character vector of colors, default rainbow()</p>
</td></tr>
<tr><td><code id="Bayes4Mixtures_+3A_xlab">xlab</code></td>
<td>
<p>Optional, label of x-axis, default 'Data', see intern R documentation</p>
</td></tr>
<tr><td><code id="Bayes4Mixtures_+3A_lwd">lwd</code></td>
<td>
<p>Width of Line, see intern R documentation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See conference presentation for further explanation.
</p>


<h3>Value</h3>

<p>List with
</p>
<table>
<tr><td><code>Posteriors</code></td>
<td>
<p>(1:N,1:L) of Posteriors corresponding to Data</p>
</td></tr>
<tr><td><code>NormalizationFactor</code></td>
<td>
<p>(1:N) denominator of Bayes theorem corresponding to Data</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Catharina Lippmann, Onno Hansen-Goos, Michael Thrun</p>


<h3>References</h3>

<p>Thrun M.C.,Ultsch, A.: Models of Income Distributions for Knowledge Discovery, European Conference on Data Analysis, DOI 10.13140/RG.2.1.4463.0244, Colchester 2015.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesDecisionBoundaries">BayesDecisionBoundaries</a></code>,<code><a href="#topic+AdaptGauss">AdaptGauss</a></code></p>

<hr>
<h2 id='BayesClassification'>BayesClassification</h2><span id='topic+BayesClassification'></span>

<h3>Description</h3>

<p>Bayes Klassifikation den Daten zuordnen
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BayesClassification(Data, Means, SDs, Weights, IsLogDistribution = Means
  * 0, ClassLabels = c(1:length(Means)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BayesClassification_+3A_data">Data</code></td>
<td>
<p>vector of Data</p>
</td></tr>
<tr><td><code id="BayesClassification_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM)</p>
</td></tr>
<tr><td><code id="BayesClassification_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="BayesClassification_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="BayesClassification_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default vector of zeros of length 1:L</p>
</td></tr>
<tr><td><code id="BayesClassification_+3A_classlabels">ClassLabels</code></td>
<td>
<p>Optional numbered class labels that are assigned to the classes. default (1:L), L number of different components of gaussian mixture model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Cls(1:n,1:d) classiffication of Data, such that 1= first component of gaussian mixture model, 2= second component of gaussian mixture model and so on. For Every datapoint a number is returned.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>

<hr>
<h2 id='BayesDecisionBoundaries'>Decision Boundaries calculated through Bayes Theorem</h2><span id='topic+BayesDecisionBoundaries'></span>

<h3>Description</h3>

<p>Function finds the intersections of Gaussians or LogNormals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BayesDecisionBoundaries(Means,SDs,Weights,IsLogDistribution,MinData,MaxData,Ycoor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BayesDecisionBoundaries_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM)</p>
</td></tr>
<tr><td><code id="BayesDecisionBoundaries_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="BayesDecisionBoundaries_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="BayesDecisionBoundaries_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default vector of zeros of length 1:L</p>
</td></tr>
<tr><td><code id="BayesDecisionBoundaries_+3A_mindata">MinData</code></td>
<td>
<p>Optional, Beginning of range, where the Boundaries are searched for, default min(M)</p>
</td></tr>
<tr><td><code id="BayesDecisionBoundaries_+3A_maxdata">MaxData</code></td>
<td>
<p>Optional, End of range, where the Boundaries are searched for, default max(M)</p>
</td></tr>
<tr><td><code id="BayesDecisionBoundaries_+3A_ycoor">Ycoor</code></td>
<td>
<p>Optional, Bool, if TRUE instead of vector of DecisionBoundaries
list of DecisionBoundaries and DBY is returned</p>
</td></tr>
</table>


<h3>Value</h3>

            
<table>
<tr><td><code>DecisionBoundaries</code></td>
<td>
<p>vector[1:L-1], Bayes decision boundaries </p>
</td></tr>
<tr><td><code>DBY</code></td>
<td>
<p>if (Ycoor==TRUE), y values at the cross points of the Gaussians is also returned, that the return is a list of DecisionBoundaries and DBY</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun, Rabea Griese</p>


<h3>References</h3>

<p>Duda, R. O., Hart, P. E., &amp; Stork, D. G. (2001). Pattern classification. 2nd. Edition. New York, p. 512ff</p>


<h3>See Also</h3>

<p><code><a href="#topic+AdaptGauss">AdaptGauss</a></code>,<code><a href="#topic+Intersect2Mixtures">Intersect2Mixtures</a></code>,<code><a href="#topic+Bayes4Mixtures">Bayes4Mixtures</a></code></p>

<hr>
<h2 id='BayesFor2GMM'>Posterioris of Bayes Theorem for a two group GMM</h2><span id='topic+BayesFor2GMM'></span>

<h3>Description</h3>

<p>Calculates the posterioris of Bayes theorem, splits the GMM in two groups beforehand.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BayesFor2GMM(Data, Means, SDs, Weights, IsLogDistribution = Means * 0,
  Ind1 = c(1:floor(length(Means)/2)), Ind2 = c((floor(length(Means)/2)
  + 1):length(Means)), PlotIt = 0, CorrectBorders = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BayesFor2GMM_+3A_data">Data</code></td>
<td>
<p>vector (1:N) of data points</p>
</td></tr>
<tr><td><code id="BayesFor2GMM_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM),L == Number of Gaussians</p>
</td></tr>
<tr><td><code id="BayesFor2GMM_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="BayesFor2GMM_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="BayesFor2GMM_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default vector of zeros of length L</p>
</td></tr>
<tr><td><code id="BayesFor2GMM_+3A_ind1">Ind1</code></td>
<td>
<p>indices from (1:C) such that [M(Ind1),S(Ind1) ,W(Ind1) ]is one mixture, [M(Ind2),S(Ind2) ,W(Ind2) ] the second mixture default Ind1= 1:C/2, Ind2= C/2+1:C</p>
</td></tr>
<tr><td><code id="BayesFor2GMM_+3A_ind2">Ind2</code></td>
<td>
<p>indices from (1:C) such that [M(Ind1),S(Ind1) ,W(Ind1) ]is one mixture, [M(Ind2),S(Ind2) ,W(Ind2) ] the second mixture default Ind1= 1:C/2, Ind2= C/2+1:C</p>
</td></tr>
<tr><td><code id="BayesFor2GMM_+3A_plotit">PlotIt</code></td>
<td>
<p>Optional, Default: FALSE; TRUE do a Plot</p>
</td></tr>
<tr><td><code id="BayesFor2GMM_+3A_correctborders">CorrectBorders</code></td>
<td>
<p>Optional, ==TRUE data at right borders of GMM distribution will be assigned to last gaussian, left border vice versa. (default ==FALSE) normal Bayes Theorem</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See conference presentation for further explanation.
</p>


<h3>Value</h3>

<p>List With 
</p>

<dl>
<dt>Posteriors:</dt><dd><p>(1:N,1:L) of Posteriors corresponding to Data</p>
</dd>
<dt>NormalizationFactor:</dt><dd><p>(1:N) denominator of Bayes theorem corresponding to Data</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alfred Ultsch, Michael Thrun
</p>


<h3>References</h3>

<p>Thrun M.C.,Ultsch, A.: Models of Income Distributions for Knowledge Discovery, European Conference on Data Analysis, DOI 10.13140/RG.2.1.4463.0244, Colchester 2015.
</p>
<p><strong>See Also</strong>
</p>
<p>BayesDecisionBoundaries,AdaptGauss
</p>

<hr>
<h2 id='CDFMixtures'>cumulative distribution of mixture model</h2><span id='topic+CDFMixtures'></span>

<h3>Description</h3>

<p>returns the cdf (cumulative distribution function) of a mixture model of gaussian or log gaussians
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CDFMixtures(Kernels,Means,SDs,Weights,IsLogDistribution)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CDFMixtures_+3A_kernels">Kernels</code></td>
<td>
<p>at these locations N(Means,Sdevs)*Weights is used for cdf calcuation, NOTE: Kernels are usually (but not necessarily) sorted and unique</p>
</td></tr>
<tr><td><code id="CDFMixtures_+3A_means">Means</code></td>
<td>
<p>vector(1:L), Means of Gaussians,  L ==  Number of Gaussians</p>
</td></tr>
<tr><td><code id="CDFMixtures_+3A_sds">SDs</code></td>
<td>
<p>estimated Gaussian Kernels = standard deviations</p>
</td></tr>
<tr><td><code id="CDFMixtures_+3A_weights">Weights</code></td>
<td>
<p>optional, relative number of points in Gaussians (prior probabilities): 
sum(Weights) ==1, default weight is 1/L</p>
</td></tr>
<tr><td><code id="CDFMixtures_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, if IsLogDistribution(i)==1, then mixture is lognormal
default == 0*(1:L)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with
</p>
<table>
<tr><td><code>CDFGaussMixture</code></td>
<td>
<p>(1:N,1),  cdf of Sum of SingleGaussians at Kernels</p>
</td></tr>
<tr><td><code>CDFSingleGaussian</code></td>
<td>
<p>(1:N,1:L)  ,cdf of mixtures at Kernels</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Rabea Griese</p>


<h3>See Also</h3>

<p><code><a href="#topic+Chi2testMixtures">Chi2testMixtures</a></code></p>

<hr>
<h2 id='Chi2testMixtures'>Pearson's chi-squared goodness of fit test</h2><span id='topic+Chi2testMixtures'></span>

<h3>Description</h3>

<p>Chi2testMixtures is goodness of fit test which establishes whether an observed distribution (data) differs from a Gauss Mixture Model (GMM).
Returns a P value of a special case of a chi-square test and visualizes data versus a given GMM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Chi2testMixtures(Data,Means,SDs,Weights,

IsLogDistribution,PlotIt,UpperLimit,VarName,NoRepetitions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Chi2testMixtures_+3A_data">Data</code></td>
<td>
<p>vector of data points (1:n)</p>
</td></tr>
<tr><td><code id="Chi2testMixtures_+3A_means">Means</code></td>
<td>
<p>vector of Means of Gaussians (1:c)</p>
</td></tr>
<tr><td><code id="Chi2testMixtures_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels (1:c)</p>
</td></tr>
<tr><td><code id="Chi2testMixtures_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities) (1:c)</p>
</td></tr>
<tr><td><code id="Chi2testMixtures_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, if IsLogDistribution(i)==1, then mixture is lognormal, default vector of zeros of length 1:L</p>
</td></tr>
<tr><td><code id="Chi2testMixtures_+3A_plotit">PlotIt</code></td>
<td>
<p>Optional, Default: FALSE, do a Plot of the compared cdfs and the KS-test distribution (Diff)</p>
</td></tr>
<tr><td><code id="Chi2testMixtures_+3A_upperlimit">UpperLimit</code></td>
<td>
<p>Optional. test only for Data &lt;= UpperLimit, Default = max(Data) i.e all Data.</p>
</td></tr>
<tr><td><code id="Chi2testMixtures_+3A_varname">VarName</code></td>
<td>
<p>If PlotIt=TRUE, the name of the inspected variable, default 'Data'</p>
</td></tr>
<tr><td><code id="Chi2testMixtures_+3A_norepetitions">NoRepetitions</code></td>
<td>
<p>Optional, scalar, default =1000, Number of Repetitions for monte carlo sampling</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The null hypothesis is that the estimated data distribution does not differ significantly from the GMM.
Let O_i be the observed features and E_i be the expected number E,
than the test statistic is defined with the minimum chi-square estimate T=sum((O_i-E_i)^2/E_i)*1/m, where m the number of data points.
The expected number Ei may be derived for each bin.  If there is a significant difference between the O_i
and the E_i, the Pvalue is small and the null hypothesis can be rejected.
</p>
<p>Further details, see [Thrun &amp; Ultsch, 2015].
</p>


<h3>Value</h3>

<p>List with
</p>
<table>
<tr><td><code>Pvalue</code></td>
<td>
<p>Pvalue of a suiting chi-square , Pvalue ==0 if Pvalue &lt;0.001</p>
</td></tr>
<tr><td><code>BinCenters</code></td>
<td>
<p>bin centers</p>
</td></tr>
<tr><td><code>ObsNrInBin</code></td>
<td>
<p>No. of data in bin</p>
</td></tr>
<tr><td><code>ExpectedNrInBin</code></td>
<td>
<p>No. of data that should be in bin according to GMM</p>
</td></tr>
<tr><td><code>Chi2Value</code></td>
<td>
<p>the  TestStatistic T i.e.: sum((ObsNrInBin(Ind)-ExpectedNrInBin(Ind))^2/ExpectedNrInBin(Ind)) with
Ind = find(ExpectedNrInBin&gt;=10)
The value of <code>Chi2Value</code> is compared to a chi-squared distribution.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The statistic assumption is that the the test statistic follows a chi square distribution.
The number of degrees of freedom is equal to the number of datapoints n-1-3*c
</p>


<h3>Author(s)</h3>

<p>Rabea Griese, Michael Thrun</p>


<h3>References</h3>

<p>Hartung, J., Elpelt, B., and Kloesener, K.H.: Statistik, 8. Aufl. Verlag Oldenburg (1991).
</p>
<p>Thrun, M. C., Ultsch, A.: Models of Income Distributions for Knowledge Discovery, European Conference on Data Analysis, DOI 10.13140/RG.2.1.4463.0244, pp. 28-29, Colchester 2015.
</p>

<hr>
<h2 id='ClassifyByDecisionBoundaries'>Classify Data according to decision Boundaries</h2><span id='topic+ClassifyByDecisionBoundaries'></span>

<h3>Description</h3>

<p>The Decision Boundaries calculated through Bayes Theorem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClassifyByDecisionBoundaries(Data,DecisionBoundaries,ClassLabels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClassifyByDecisionBoundaries_+3A_data">Data</code></td>
<td>
<p>vector of Data</p>
</td></tr>
<tr><td><code id="ClassifyByDecisionBoundaries_+3A_decisionboundaries">DecisionBoundaries</code></td>
<td>
<p>decision boundaries, <code><a href="#topic+BayesDecisionBoundaries">BayesDecisionBoundaries</a></code></p>
</td></tr>
<tr><td><code id="ClassifyByDecisionBoundaries_+3A_classlabels">ClassLabels</code></td>
<td>
<p>Optional numbered class labels that are assigned to the classes. default (1:L), L number of different components of gaussian mixture model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Cls(1:n,1:d)               classiffication of Data, such that 1= first component of gaussian mixture model, 2= second component of gaussian mixture model and so on. For Every datapoint a number is returned.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>Duda, R. O., Hart, P. E., &amp; Stork, D. G. (2001). Pattern classification. 2nd. Edition. New York, p. 512ff</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesDecisionBoundaries">BayesDecisionBoundaries</a></code>, <code><a href="#topic+Bayes4Mixtures">Bayes4Mixtures</a></code></p>

<hr>
<h2 id='EMGauss'>EM Algorithm for GMM</h2><span id='topic+EMGauss'></span>

<h3>Description</h3>

<p>Expectation-Maximization algorithm to calculate optimal Gaussian Mixture Model for given data in one Dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EMGauss(Data, K, Means, SDs,Weights, MaxNumberofIterations,fast)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EMGauss_+3A_data">Data</code></td>
<td>
<p>vector of data points</p>
</td></tr>
<tr><td><code id="EMGauss_+3A_k">K</code></td>
<td>
<p>estimated amount of Gaussian Kernels</p>
</td></tr>
<tr><td><code id="EMGauss_+3A_means">Means</code></td>
<td>
<p>vector(1:L), Means of Gaussians,  L ==  Number of Gaussians</p>
</td></tr>
<tr><td><code id="EMGauss_+3A_sds">SDs</code></td>
<td>
<p>estimated Gaussian Kernels = standard deviations</p>
</td></tr>
<tr><td><code id="EMGauss_+3A_weights">Weights</code></td>
<td>
<p>optional, relative number of points in Gaussians (prior probabilities): 
sum(Weights) ==1, default weight is 1/L</p>
</td></tr>
<tr><td><code id="EMGauss_+3A_maxnumberofiterations">MaxNumberofIterations</code></td>
<td>
<p>Optional, Number of Iterations; default=10</p>
</td></tr>
<tr><td><code id="EMGauss_+3A_fast">fast</code></td>
<td>
<p>Default: FALSE: Using mclust's EM see function <code>densityMclust</code> of that package, TRUE: Naive but faster EM implementation, which may be numerical unstable, because log(gauss) is not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>No adding or removing of Gaussian kernels. Number of Gaussian hast to be set by the length of the vector of Means, SDs and Weights.
This EM is only for univariate data. For multivariate data see package <code>mclust</code>
</p>


<h3>Value</h3>

<p>List with
</p>
<table>
<tr><td><code>Means</code></td>
<td>
<p>means of GMM generated by EM algorithm</p>
</td></tr>
<tr><td><code>SDs</code></td>
<td>
<p>standard deviations of GMM generated by EM algorithm</p>
</td></tr>
<tr><td><code>Weights</code></td>
<td>
<p>prior probabilities of Gaussians</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Onno Hansen-Goos, Michael Thrun, Florian Lerch</p>


<h3>References</h3>

<p>Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006, p 435 ff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+AdaptGauss">AdaptGauss</a></code></p>

<hr>
<h2 id='GMMplot_ggplot2'>
Plots the Gaussian Mixture Model (GMM) withing ggplot2
</h2><span id='topic+GMMplot_ggplot2'></span>

<h3>Description</h3>

<p>PlotMixtures and PlotMixturesAndBoundaries for ggplot2 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GMMplot_ggplot2(Data, Means, SDs, Weights,

BayesBoundaries, SingleGausses = TRUE, Hist = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GMMplot_ggplot2_+3A_data">Data</code></td>
<td>
<p>vector (1:N) of data points</p>
</td></tr>
<tr><td><code id="GMMplot_ggplot2_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM),L ==  Number of Gaussians</p>
</td></tr>
<tr><td><code id="GMMplot_ggplot2_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="GMMplot_ggplot2_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="GMMplot_ggplot2_+3A_bayesboundaries">BayesBoundaries</code></td>
<td>
<p>Optional, x values for baye boundaries, if missing 'BayesDecisionBoundaries' is called</p>
</td></tr>
<tr><td><code id="GMMplot_ggplot2_+3A_singlegausses">SingleGausses</code></td>
<td>
<p>Optional,  SingleGausses=T than components of the mixture in blue will be shown.</p>
</td></tr>
<tr><td><code id="GMMplot_ggplot2_+3A_hist">Hist</code></td>
<td>
<p>Optional, geom_histogram overlayed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot2 object
</p>


<h3>Note</h3>

<p>MT standardized code for CRAN and added dec boundaries and doku
</p>


<h3>Author(s)</h3>

<p>Joern Loetsch, Michael Thrun (ctb)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PlotMixturesAndBoundaries">PlotMixturesAndBoundaries</a></code>, <code><a href="#topic+PlotMixtures">PlotMixtures</a></code>, <code><a href="#topic+BayesDecisionBoundaries">BayesDecisionBoundaries</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data=c(rnorm(1000),rnorm(2000)+2,rnorm(1000)*2-1)

GMMplot_ggplot2(data,c(-1,0,2),c(2,1,1),c(0.25,0.25,0.5),SingleGausses=TRUE)

</code></pre>

<hr>
<h2 id='InformationCriteria4GMM'>Information Criteria For GMM</h2><span id='topic+InformationCriteria4GMM'></span>

<h3>Description</h3>

<p>Calculates the AIC and BIC criteria
</p>


<h3>Usage</h3>

<pre><code class='language-R'>InformationCriteria4GMM(Data, Means, SDs, Weights, IsLogDistribution)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="InformationCriteria4GMM_+3A_data">Data</code></td>
<td>
<p>vector (1:N) of data points</p>
</td></tr>
<tr><td><code id="InformationCriteria4GMM_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM),L ==  Number of Gaussians</p>
</td></tr>
<tr><td><code id="InformationCriteria4GMM_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="InformationCriteria4GMM_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="InformationCriteria4GMM_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default  vector of zeros of length L, LogNormal Modes are at this point only experimental</p>
</td></tr>
</table>


<h3>Details</h3>

<p>AIC =  2*k -2*LogLikelihood, 
k = nr. of model parameter = 3*Nr. of Gaussians
One Gaussian: K=2 (Weight is then not an parameter!)
SMALL SAMPLE CORRECTION: for n= nr of Data and n &lt; 40 * k, AIC is adjusted to AIC=AIC+ (2*k*(k+1))/(n-k-1)
</p>
<p>BIC = k* log(n) - 2*LogLikelihood
</p>
<p>Only for a Gaussian Mixture Model (GMM) verified, for the Log Gaussian, Gaussian, Log Gaussian (LGL) Model only experimental
</p>


<h3>Value</h3>

<p>List with
</p>
<table>
<tr><td><code>K</code></td>
<td>
<p>Number of gaussian mixtures</p>
</td></tr>
<tr><td><code>AIC</code></td>
<td>
<p>Akaike Informations criterium</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p> Bayes Information criterium</p>
</td></tr>
<tr><td><code>LogLikelihood</code></td>
<td>
<p>LogLikelihood of GMM, see <code><a href="#topic+LogLikelihood4Mixtures">LogLikelihood4Mixtures</a></code></p>
</td></tr>
<tr><td><code>PDFmixture</code></td>
<td>
<p>probability density function of GMM, see <code><a href="#topic+Pdf4Mixtures">Pdf4Mixtures</a></code></p>
</td></tr>
<tr><td><code>LogPDFdata</code></td>
<td>
<p>log(PDFmixture)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>Aubert, A. H., Thrun, M. C., Breuer, L., &amp; Ultsch, A.: Knowledge discovery from data structure: hydrology versus biology controlled in-stream nitrate concentration, Scientific reports, Vol. (in revision), pp., 2016.
</p>
<p>Aho, K., Derryberry, D., &amp; Peterson, T.: Model selection for ecologists: the worldviews of AIC and BIC. Ecology, 95(3), pp. 631-636, 2014.
</p>

<hr>
<h2 id='Intersect2Mixtures'>Intersect of two Gaussians</h2><span id='topic+Intersect2Mixtures'></span>

<h3>Description</h3>

<p>Finds the intersect of two gaussians or log gaussians
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Intersect2Mixtures(Mean1,SD1,Weight1,Mean2,SD2,Weight2,IsLogDistribution,MinData,MaxData)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Intersect2Mixtures_+3A_mean1">Mean1</code></td>
<td>
<p>mean of 1.gaussian</p>
</td></tr>
<tr><td><code id="Intersect2Mixtures_+3A_sd1">SD1</code></td>
<td>
<p>standard deviations of 1.gaussian</p>
</td></tr>
<tr><td><code id="Intersect2Mixtures_+3A_weight1">Weight1</code></td>
<td>
<p>weight of 1. guassian</p>
</td></tr>
<tr><td><code id="Intersect2Mixtures_+3A_mean2">Mean2</code></td>
<td>
<p>mean of 2.gaussian</p>
</td></tr>
<tr><td><code id="Intersect2Mixtures_+3A_sd2">SD2</code></td>
<td>
<p>standard deviations of 2.gaussian</p>
</td></tr>
<tr><td><code id="Intersect2Mixtures_+3A_weight2">Weight2</code></td>
<td>
<p>weight of 2. guassian</p>
</td></tr>
<tr><td><code id="Intersect2Mixtures_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default vector of zeros of length 2</p>
</td></tr>
<tr><td><code id="Intersect2Mixtures_+3A_mindata">MinData</code></td>
<td>
<p>Optional, Beginning of range, where the intersect is searched for, default min(Mean1,Mean2)</p>
</td></tr>
<tr><td><code id="Intersect2Mixtures_+3A_maxdata">MaxData</code></td>
<td>
<p>Optional, End of range, where the intersect is searched for, default max(Mean1,Mean2)</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>CutX</code></td>
<td>
<p>x value, where gaussian 1=gaussian2</p>
</td></tr>
<tr><td><code>CutY</code></td>
<td>
<p>y value, where gaussian 1=gaussian2</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun, Rabea Griese</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesDecisionBoundaries">BayesDecisionBoundaries</a></code></p>

<hr>
<h2 id='KStestMixtures'>Kolmogorov-Smirnov test</h2><span id='topic+KStestMixtures'></span>

<h3>Description</h3>

<p>Returns a P value and visualizes for Kolmogorov-Smirnov test of Data versus a given Gauss Mixture Model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KStestMixtures(Data,Means,SDs,Weights,IsLogDistribution,

PlotIt,UpperLimit,NoRepetitions,Silent)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KStestMixtures_+3A_data">Data</code></td>
<td>
<p>vector of data points</p>
</td></tr>
<tr><td><code id="KStestMixtures_+3A_means">Means</code></td>
<td>
<p>vector of Means of Gaussians</p>
</td></tr>
<tr><td><code id="KStestMixtures_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels</p>
</td></tr>
<tr><td><code id="KStestMixtures_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities)</p>
</td></tr>
<tr><td><code id="KStestMixtures_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, if IsLogDistribution(i)==1, then mixture is lognormal, default vector of zeros of length 1:L</p>
</td></tr>
<tr><td><code id="KStestMixtures_+3A_plotit">PlotIt</code></td>
<td>
<p>Optional, Default: FALSE, do a Plot of the compared cdfs and the KS-test distribution (Diff)</p>
</td></tr>
<tr><td><code id="KStestMixtures_+3A_upperlimit">UpperLimit</code></td>
<td>
<p>Optional. test only for Data &lt;= UpperLimit, Default = max(Data) i.e all Data.</p>
</td></tr>
<tr><td><code id="KStestMixtures_+3A_norepetitions">NoRepetitions</code></td>
<td>
<p>Optional, default =1000, scalar, Number of Repetitions for monte carlo sampling</p>
</td></tr>
<tr><td><code id="KStestMixtures_+3A_silent">Silent</code></td>
<td>
<p>Optional, default=TRUE, If FALSE, shows progress of computation by points (On windows systems a progress bar)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The null hypothesis is that the estimated data distribution does not differ significantly from the GMM.
If there is a significant difference, then the Pvalue is small and the null hypothesis is rejected.
</p>


<h3>Value</h3>

<p>List with
</p>
<table>
<tr><td><code>Pvalue</code></td>
<td>
<p>Pvalue of a suiting Kolmogorov-Smirnov test, Pvalue ==0 if Pvalue &lt;0.001</p>
</td></tr>
<tr><td><code>DataKernels</code></td>
<td>
<p>such that plot(DataKernels,DataCDF) gives the cdf(Data)</p>
</td></tr>
<tr><td><code>DataCDF</code></td>
<td>
<p>such that plot(DataKernels,DataCDF) gives the cdf(Data)</p>
</td></tr>
<tr><td><code>CDFGaussMixture</code></td>
<td>
<p>No. of data that should be in bin according to GMM</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun, Alfred Ultsch</p>


<h3>References</h3>

<p>Smirnov, N., Table for Estimating the Goodness of Fit of Empirical Distributions. 1948, (2), 279-281.</p>

<hr>
<h2 id='LikelihoodRatio4Mixtures'>Likelihood Ratio for Gaussian Mixtures</h2><span id='topic+LikelihoodRatio4Mixtures'></span>

<h3>Description</h3>

<p>Computes the likelihood ratio for two Gaussian Mixture Models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LikelihoodRatio4Mixtures(Data,NullMixture,OneMixture,PlotIt,LowerLimit,UpperLimit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LikelihoodRatio4Mixtures_+3A_data">Data</code></td>
<td>
<p> Data points.</p>
</td></tr>
<tr><td><code id="LikelihoodRatio4Mixtures_+3A_nullmixture">NullMixture</code></td>
<td>
<p> A Matrix: cbind(Means0,SDs0,Weights0) or cbind(Means0,SDs0,Weights0,IsLog0). 
The null model; usually with less Gaussians than the OneMixture</p>
</td></tr>
<tr><td><code id="LikelihoodRatio4Mixtures_+3A_onemixture">OneMixture</code></td>
<td>
<p> A Matrix: cbind(Means1,SDs1,Weights1) or  cbind(Means1,SDs1,Weights1,IsLog1). 
The alternative model usually with more Gaussians than the OneMixture.</p>
</td></tr>
<tr><td><code id="LikelihoodRatio4Mixtures_+3A_plotit">PlotIt</code></td>
<td>
<p> Optional: Boolean, if TRUE a Plot of the compared cdf's and the KS-test distribution (Diff) is shown</p>
</td></tr>
<tr><td><code id="LikelihoodRatio4Mixtures_+3A_lowerlimit">LowerLimit</code></td>
<td>
<p> Optional: test only for Data &gt;= LowerLimit, Default = min(Data) i.e all Data.</p>
</td></tr>
<tr><td><code id="LikelihoodRatio4Mixtures_+3A_upperlimit">UpperLimit</code></td>
<td>
<p> Optional: test only for Data &lt;= UpperLimit, Default = max(Data) i.e all Data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with
</p>
<table>
<tr><td><code>Pvalue</code></td>
<td>
<p>the error that we make, if we accept OneMixture as the better Model over the NullMixture</p>
</td></tr>
<tr><td><code>NullLogLikelihood</code></td>
<td>
<p>log likelihood of GMM Null</p>
</td></tr>
<tr><td><code>OneLogLikelihood</code></td>
<td>
<p>log likelihood of GMM One</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alfred Ultsch, Michael Thrun, Catharina Lippmann</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  
  data=c(rnorm(1000),rnorm(2000)+2,rnorm(1000)*2-1)
  ## Not run: Vals=AdaptGauss(data,c(-1,0,2),c(2,1,1),c(0.25,0.25,0.5),0.3,-6,6)
  NullMixture=cbind(Vals$Means,Vals$SDs,Vals$Weights)
  
## End(Not run)
  ## Not run: Vals2=AdaptGauss(data,c(-1,0,2,3),c(2,1,1,1),c(0.25,0.25,0.25,0.25),0.3,-6,6)
  OneMixture=cbind(Vals2$Means,Vals2$SDs,Vals2$Weights)
  
## End(Not run)
  ## Not run: 
  res=LikelihoodRatio4Mixtures(data,NullMixture,OneMixture,T)
  
## End(Not run)
 </code></pre>

<hr>
<h2 id='LKWFahrzeitSeehafen2010'>
Truck driving time seaport 2010
</h2><span id='topic+LKWFahrzeitSeehafen2010'></span>

<h3>Description</h3>

<p>Truck driving time to seaports measured in 2010.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("LKWFahrzeitSeehafen2010")</code></pre>


<h3>Format</h3>

<p>The format is:
num [1:11441] 84.7 13.2 11.5 41.4 52.9 ...
</p>


<h3>References</h3>

<p>Behnisch, M., Ultsch, A.: Knowledge Discovery in Spatial Planning Data - A Concept for Cluster Understanding, in: Helbich, M., Arsanjani, J. J., Leitner, M. (eds.): Computational Approaches for Urban Environments, in: Gatrell, J.D., Jensen, R.R.: Geotechnologies and the Environment Series, Vol, 13, Springer, Berlin, pp. 49-75, 2015.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(LKWFahrzeitSeehafen2010)
## maybe str(LKWFahrzeitSeehafen2010) ; plot(LKWFahrzeitSeehafen2010) ...
</code></pre>

<hr>
<h2 id='LogLikelihood4Mixtures'>LogLikelihood for Gaussian Mixture Models</h2><span id='topic+LogLikelihood4Mixtures'></span>

<h3>Description</h3>

<p>Computes the LogLikelihood for Gaussian Mixture Models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LogLikelihood4Mixtures(Data, Means, SDs, Weights, IsLogDistribution)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LogLikelihood4Mixtures_+3A_data">Data</code></td>
<td>
<p> Data for empirical PDF. Has to be an Array of values. NaNs
and NULLs will be deleted </p>
</td></tr>
<tr><td><code id="LogLikelihood4Mixtures_+3A_means">Means</code></td>
<td>
<p> Optional: Means of gaussians of GMM. </p>
</td></tr>
<tr><td><code id="LogLikelihood4Mixtures_+3A_sds">SDs</code></td>
<td>
<p> Optional: StandardDevations of gaussians of GMM. (Has to be the same length as Means) </p>
</td></tr>
<tr><td><code id="LogLikelihood4Mixtures_+3A_weights">Weights</code></td>
<td>
<p> Optional: Weights of gaussians of GMM. (Has to be the same length as Means) </p>
</td></tr>
<tr><td><code id="LogLikelihood4Mixtures_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default vector of zeros of length 1:L</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with
</p>
<table>
<tr><td><code>LogLikelihood</code></td>
<td>
<p>LogLikelihood = = sum(log(PDFmixture)</p>
</td></tr>
<tr><td><code>LogPDF</code></td>
<td>
<p>=log(PDFmixture)</p>
</td></tr>
<tr><td><code>PDFmixture</code></td>
<td>
<p>die Probability density function for each point</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alfred Ultsch, Catharina Lippmann</p>


<h3>References</h3>

<p>Pattern Recogintion and Machine Learning, C.M. Bishop, 2006, isbn: ISBN-13: 978-0387-31073-2, p. 433 (9.14)
</p>

<hr>
<h2 id='Pdf4Mixtures'>Calculates pdf for GMM</h2><span id='topic+Pdf4Mixtures'></span>

<h3>Description</h3>

<p>Calculate Gaussianthe probability density function for a Mixture Model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Pdf4Mixtures(Data, Means, SDs, Weights,IsLogDistribution,PlotIt)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Pdf4Mixtures_+3A_data">Data</code></td>
<td>
<p>vector (1:N) of data points</p>
</td></tr>
<tr><td><code id="Pdf4Mixtures_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM),L ==  Number of Gaussians</p>
</td></tr>
<tr><td><code id="Pdf4Mixtures_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="Pdf4Mixtures_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="Pdf4Mixtures_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default vector of zeros of length 1:L</p>
</td></tr>
<tr><td><code id="Pdf4Mixtures_+3A_plotit">PlotIt</code></td>
<td>
<p>Optional: =TRUE plot of pdf</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with
</p>
<table>
<tr><td><code>PDF4modes</code></td>
<td>
<p>matrix, where the columns are the gaussians</p>
</td></tr>
<tr><td><code>PDF</code></td>
<td>
<p>matrix, where the columns are the gaussians weighted by Weights</p>
</td></tr>
<tr><td><code>PDFmixture</code></td>
<td>
<p>linear superpositions of PDF -  prior probabilities of Gaussians</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>See Also</h3>

<p><code><a href="#topic+PlotMixtures">PlotMixtures</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data=c(rnorm(1000),rnorm(2000)+2,rnorm(1000)*2-1)
Pdf4Mixtures(data,c(-1,0,2),c(2,1,1),c(0.25,0.25,0.5), PlotIt=TRUE)
</code></pre>

<hr>
<h2 id='PlotMixtures'>Shows GMM</h2><span id='topic+PlotMixtures'></span>

<h3>Description</h3>

<p>Plots Gaussian Mixture Model without Bayes decision boundaries, such that:
</p>
<p>Black is the PDE of Data
</p>
<p>Red is color of the GMM
</p>
<p>Blue is the color of components of the mixture
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotMixtures_+3A_data">Data</code></td>
<td>
<p>vector (1:N) of data points</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM),L ==  Number of Gaussians</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default vector of zeros of length 1:L</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_plotter">Plotter</code></td>
<td>
<p>Optional, plotting package, either native or plotly</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_singlecolor">SingleColor</code></td>
<td>
<p>Optional,Color for line plot of all the single gaussians, default magenta</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_mixturecolor">MixtureColor</code></td>
<td>
<p>Optional,Color of line lot for the mixture default red</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_datacolor">DataColor</code></td>
<td>
<p>Optional,Color of line plot for the data, default black</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_singlegausses">SingleGausses</code></td>
<td>
<p>Optional, If TRUE, single gaussians are shown, default FALSE</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_axes">axes</code></td>
<td>
<p>Optional,Default:TRUE with axis, see argument <code>axis</code> of <code><a href="base.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_xlab">xlab</code></td>
<td>
<p>Optional, see <code><a href="base.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_ylab">ylab</code></td>
<td>
<p>Optional, see <code><a href="base.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_xlim">xlim</code></td>
<td>
<p>Optional, see <code><a href="base.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_ylim">ylim</code></td>
<td>
<p>Optional, see <code><a href="base.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_paretorad">ParetoRad</code></td>
<td>
<p>Optional: Precalculated Pareto Radius to use</p>
</td></tr>
<tr><td><code id="PlotMixtures_+3A_...">...</code></td>
<td>
<p>other plot arguments like xlim = c(1,10)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Example shows that overlapping variances of gaussians will result in inappropriate decision boundaries.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun, Quirin Stier</p>


<h3>See Also</h3>

<p><code><a href="#topic+PlotMixturesAndBoundaries">PlotMixturesAndBoundaries</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data=c(rnorm(1000),rnorm(2000)+2,rnorm(1000)*2-1)

PlotMixtures(data,c(-1,0,2),c(2,1,1),c(0.25,0.25,0.5),SingleColor='blue',SingleGausses=TRUE)

</code></pre>

<hr>
<h2 id='PlotMixturesAndBoundaries'>Shows GMM with Boundaries</h2><span id='topic+PlotMixturesAndBoundaries'></span>

<h3>Description</h3>

<p>Plots Gaussian Mixture Model with Bayes decision boundaries, such that:
</p>
<p>Black is the PDE of Data
</p>
<p>Red is color of the GMM
</p>
<p>Magenta are the Bayes boundaries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotMixturesAndBoundaries(Data, Means, SDs, Weights, 

IsLogDistribution = rep(FALSE, length(Means)), Plotter="native",

SingleColor = "blue", MixtureColor = "red", DataColor = "black",

BoundaryColor = "magenta", xlab, ylab, 
				   
 SingleGausses =TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_data">Data</code></td>
<td>
<p>vector (1:N) of data points</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM),L ==  Number of Gaussians</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default vector of zeros of length 1:L</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_plotter">Plotter</code></td>
<td>
<p>Optional, plotting package, either native or plotly</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_singlecolor">SingleColor</code></td>
<td>
<p>Optional, Color for line plot of all the single gaussians, default magenta</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_mixturecolor">MixtureColor</code></td>
<td>
<p>Optional, Color of line plot for the mixture, default red</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_datacolor">DataColor</code></td>
<td>
<p>Optional, Color of line plot for the data, default black</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_boundarycolor">BoundaryColor</code></td>
<td>
<p>Optional, Color of bayesian boundaries</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_xlab">xlab</code></td>
<td>
<p>Optional, x label, see <code><a href="base.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_ylab">ylab</code></td>
<td>
<p>Optional, y label, ee <code><a href="base.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_singlegausses">SingleGausses</code></td>
<td>
<p>Optional,  SingleGausses=T than components of the mixture in blue will be shown.</p>
</td></tr>
<tr><td><code id="PlotMixturesAndBoundaries_+3A_...">...</code></td>
<td>
<p>Optional, see <code><a href="base.html#topic+plot">plot</a></code> for plot properties and for <code>SingleGausses</code> <code><a href="#topic+PlotMixtures">PlotMixtures</a></code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>See Also</h3>

<p><code><a href="#topic+BayesDecisionBoundaries">BayesDecisionBoundaries</a></code>,<code><a href="#topic+PlotMixtures">PlotMixtures</a></code></p>

<hr>
<h2 id='QQplotGMM'>Quantile Quantile Plot of Data</h2><span id='topic+QQplotGMM'></span>

<h3>Description</h3>

<p>Quantile Quantile plot of data against gaussian distribution mixture model with optional best-fit-line
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QQplotGMM(Data,Means,SDs,Weights,IsLogDistribution,Method,Line,
PlotSymbol,col,xug,xog,LineWidth,PointWidth,PositiveData,Type,NoQuantiles,
ylabel,main,lwd,pch,xlabel,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QQplotGMM_+3A_data">Data</code></td>
<td>
<p>vector (1:N) of data points</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM),L ==  Number of Gaussians</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default Zeros of Length L</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_method">Method</code></td>
<td>
<p>Optional, ==1 default. ==2 for robust calculation</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_line">Line</code></td>
<td>
<p> Optional, Default: TRUE=Regression Line is drawn</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_col">col</code></td>
<td>
<p>Character: color of regression line (only for Method = 2)</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_xug">xug</code></td>
<td>
<p> Optional, lower limit of the interval [xug, xog], in which a line will be interpolated</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_xog">xog</code></td>
<td>
<p> Optional, upper limit of the interval [xug, xog], in which a line will be interpolated</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_plotsymbol">PlotSymbol</code></td>
<td>
<p> Optional, plot symbol. Default is 20. </p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_linewidth">LineWidth</code></td>
<td>
<p> Optional, width of regression line, if Line==TRUE </p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_pointwidth">PointWidth</code></td>
<td>
<p>Optional, width of points</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_positivedata">PositiveData</code></td>
<td>
<p>Optional, Boolean: If true only positive values of GMM sampling are used. Default FALSE</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_type">Type</code></td>
<td>
<p>Optional,Integer: number of method used for computing the quantiles, see <code><a href="stats.html#topic+quantile">quantile</a></code></p>
</td></tr>     
<tr><td><code id="QQplotGMM_+3A_noquantiles">NoQuantiles</code></td>
<td>
<p>Optional, Integer: Number of quantiles to compute (only for Method = 2)</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_ylabel">ylabel</code></td>
<td>
<p>Optional, see <code><a href="base.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_main">main</code></td>
<td>
<p>Optional, see <code><a href="base.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_lwd">lwd</code></td>
<td>
<p>Optional, Integer: graphic parameter - line width option (only for Method = 2)</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_pch">pch</code></td>
<td>
<p>Integer: graphic parameter for points (only for Method = 2)</p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_xlabel">xlabel</code></td>
<td>
<p>Optional, see <code><a href="base.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="QQplotGMM_+3A_...">...</code></td>
<td>
<p>Note: xlab cannot be changed, other parameters see <code><a href="stats.html#topic+qqplot">qqplot</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only verified for a Gaussian Mixture Model, usage of IsLogDistribution for LogNormal Modes is experimental!
</p>


<h3>Value</h3>

   
<p>List with
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>The x coordinates of the points that were plotted</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>The original data vector, i.e., the corresponding y coordinates</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>Michael, J. R. (1983). The stabilized probability plot. Biometrika, 70(1), 11-17.</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+qqplot">qqplot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data=c(rnorm(1000),rnorm(2000)+2,rnorm(1000)*2-1)
QQplotGMM(data,c(-1,0,2),c(2,1,1),c(0.25,0.25,0.5))

</code></pre>

<hr>
<h2 id='RandomLogGMM'>Random Number Generator for Log or Gaussian Mixture Model</h2><span id='topic+RandomLogGMM'></span>

<h3>Description</h3>

<p>Function finds the intersections of Gaussians or LogNormals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RandomLogGMM(Means,SDs,Weights,IsLogDistribution,TotalNoPoints)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RandomLogGMM_+3A_means">Means</code></td>
<td>
<p>vector[1:L] of Means of Gaussians (of GMM)</p>
</td></tr>
<tr><td><code id="RandomLogGMM_+3A_sds">SDs</code></td>
<td>
<p>vector of standard deviations, estimated Gaussian Kernels, has to be the same length as Means</p>
</td></tr>
<tr><td><code id="RandomLogGMM_+3A_weights">Weights</code></td>
<td>
<p>vector of relative number of points in Gaussians (prior probabilities), has to be the same length as Means</p>
</td></tr>
<tr><td><code id="RandomLogGMM_+3A_islogdistribution">IsLogDistribution</code></td>
<td>
<p>Optional, ==1 if distribution(i) is a LogNormal, default vector of Zeros of Length L</p>
</td></tr>
<tr><td><code id="RandomLogGMM_+3A_totalnopoints">TotalNoPoints</code></td>
<td>
<p>Optional, number of point for log or GMM generated</p>
</td></tr>
</table>


<h3>Value</h3>

            
<p>Returns vector of [1:TotalNoPoints] of genrated points for log oder gaussian mixture model
</p>


<h3>Author(s)</h3>

<p>Alfred Ultsch,Michael Thrun, Rabea Griese</p>


<h3>See Also</h3>

<p><code><a href="#topic+QQplotGMM">QQplotGMM</a></code>,<code><a href="#topic+Chi2testMixtures">Chi2testMixtures</a></code></p>

<hr>
<h2 id='Symlognpdf'>
computes a special case of log normal distribution density
</h2><span id='topic+Symlognpdf'></span>

<h3>Description</h3>

<p>Symlognpdf is an internal function for AdaptLGL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Symlognpdf(Data, Mean, SD)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Symlognpdf_+3A_data">Data</code></td>
<td>
<p>vector of data points used for sampling</p>
</td></tr>
<tr><td><code id="Symlognpdf_+3A_mean">Mean</code></td>
<td>
<p>Mean of log Gaussian</p>
</td></tr>
<tr><td><code id="Symlognpdf_+3A_sd">SD</code></td>
<td>
<p>Standard deviation of log Gaussian</p>
</td></tr>
</table>


<h3>Value</h3>

<p>M&gt;0 Log normal distribution density
</p>
<p>M&lt;0 Log normal distribution density mirrored at y axis
</p>


<h3>Note</h3>

<p>not for external usage.
</p>


<h3>See Also</h3>

<p>AdaptLGL
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
