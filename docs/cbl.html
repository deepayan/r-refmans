<!DOCTYPE html><html lang="en"><head><title>Help for package cbl</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cbl}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bipartite'><p>Simulated data</p></a></li>
<li><a href='#cbl'><p>Confounder blanket learner</p></a></li>
<li><a href='#epsilon_fn'><p>Computer the consistency lower bound</p></a></li>
<li><a href='#l0'><p>Feature selection subroutine</p></a></li>
<li><a href='#minD'><p>CPSS upper bound</p></a></li>
<li><a href='#r.TailProbs'><p>CPSS utility functions</p></a></li>
<li><a href='#ss_fn'><p>Infer causal direction using stability selection</p></a></li>
<li><a href='#sub_loop'><p>Complementary pairs subsampling loop</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Causal Discovery under a Confounder Blanket</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Methods for learning causal relationships among a set of
    foreground variables X based on signals from a (potentially much
    larger) set of background variables Z, which are known non-descendants
    of X. The confounder blanket learner (CBL) uses sparse regression
    techniques to simultaneously perform many conditional independence
    tests, with complementary pairs stability selection to guarantee
    finite sample error control. CBL is sound and complete with respect to
    a so-called "lazy oracle", and works with both linear and nonlinear
    systems. For details, see Watson &amp; Silva (2022) &lt;<a href="https://doi.org/10.48550/arXiv.2205.05715">doi:10.48550/arXiv.2205.05715</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/dswatson/cbl">https://github.com/dswatson/cbl</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table, foreach, glmnet, lightgbm</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-12-20 16:55:07 UTC; dswatson</td>
</tr>
<tr>
<td>Author:</td>
<td>David Watson <a href="https://orcid.org/0000-0001-9632-2159"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Watson &lt;david.s.watson11@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-12-20 17:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bipartite'>Simulated data</h2><span id='topic+bipartite'></span>

<h3>Description</h3>

<p>Simulated dataset of <code class="reqn">n=200</code> samples with 2 foreground variables and 10
background variables. The design follows that of Watson &amp; Silva (2022), with
<code class="reqn">Z</code> drawn from a multivariate Gaussian distribution with a Toeplitz
covariance matrix of autocorrelation <code class="reqn">\rho = 0.25</code>. Expected sparsity is
0.5, signal-to-noise ratio is 2, and structural equations are linear. The
ground truth for foreground variables is <code class="reqn">X \rightarrow Y</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bipartite)
</code></pre>


<h3>Format</h3>

<p>A list with two elements: <code>x</code> (foreground variables), and
<code>z</code> (background variables).
</p>


<h3>References</h3>

<p>Watson, D.S. &amp; Silva, R. (2022). Causal discovery under a confounder blanket.
To appear in <em>Proceedings of the 38th Conference on Uncertainty in
Artificial Intelligence</em>. <em>arXiv</em> preprint, 2205.05715.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data
data(bipartite)
x &lt;- bipartite$x
z &lt;- bipartite$z

# Set seed
set.seed(42)

# Run CBL
cbl(x, z)
</code></pre>

<hr>
<h2 id='cbl'>Confounder blanket learner</h2><span id='topic+cbl'></span>

<h3>Description</h3>

<p>This function performs the confounder blanket learner (CBL) algorithm for
causal discovery.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cbl(
  x,
  z,
  s = "lasso",
  B = 50,
  gamma = 0.5,
  maxiter = NULL,
  params = NULL,
  parallel = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cbl_+3A_x">x</code></td>
<td>
<p>Matrix or data frame of foreground variables.</p>
</td></tr>
<tr><td><code id="cbl_+3A_z">z</code></td>
<td>
<p>Matrix or data frame of background variables.</p>
</td></tr>
<tr><td><code id="cbl_+3A_s">s</code></td>
<td>
<p>Feature selection method. Includes native support for sparse linear
regression (<code>s = "lasso"</code>) and gradient boosting (<code>s = "boost"</code>).
Alternatively, a user-supplied function mapping features <code>x</code> and
outcome <code>y</code> to a bit vector indicating which features are selected.
See Examples.</p>
</td></tr>
<tr><td><code id="cbl_+3A_b">B</code></td>
<td>
<p>Number of complementary pairs to draw for stability selection.
Following Shah &amp; Samworth (2013), we recommend leaving this fixed at 50.</p>
</td></tr>
<tr><td><code id="cbl_+3A_gamma">gamma</code></td>
<td>
<p>Omission threshold. If either of two foreground variables is
omitted from the model for the other with frequency <code>gamma</code> or higher,
we infer that they are causally disconnected.</p>
</td></tr>
<tr><td><code id="cbl_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations to loop through if convergence
is elusive.</p>
</td></tr>
<tr><td><code id="cbl_+3A_params">params</code></td>
<td>
<p>Optional list to pass to <code>lgb.train</code> if <code>s = "boost"</code>.
See <code>lightgbm::<a href="lightgbm.html#topic+lgb.train">lgb.train</a></code>.</p>
</td></tr>
<tr><td><code id="cbl_+3A_parallel">parallel</code></td>
<td>
<p>Compute stability selection subroutine in parallel? Must
register backend beforehand, e.g. via <code>doMC</code>.</p>
</td></tr>
<tr><td><code id="cbl_+3A_...">...</code></td>
<td>
<p>Extra parameters to be passed to the feature selection subroutine.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The CBL algorithm (Watson &amp; Silva, 2022) learns a partial order over
foreground variables <code>x</code> via relations of minimal conditional
(in)dependence with respect to a set of background variables <code>z</code>. The
method is sound and complete with respect to a so-called &quot;lazy oracle&quot;, who
only answers independence queries about variable pairs conditioned on the
intersection of their respective non-descendants.
</p>
<p>For computational tractability, CBL performs conditional independence tests
via supervised learning with feature selection. The current implementation
includes support for sparse linear models (<code>s = "lasso"</code>) and gradient
boosting machines (<code>s = "boost"</code>). For statistical inference, CBL uses
complementary pairs stability selection (Shah &amp; Samworth, 2013), which bounds
the probability of errors of commission.
</p>


<h3>Value</h3>

<p>A square, lower triangular ancestrality matrix. Call this matrix <code>m</code>.
If CBL infers that <code class="reqn">X_i \prec X_j</code>, then <code>m[j, i] = 1</code>. If CBL
infers that <code class="reqn">X_i \preceq X_j</code>, then <code>m[j, i] = 0.5</code>. If CBL infers
that <code class="reqn">X_i \sim X_j</code>, then <code>m[j, i] = 0</code>. Otherwise,
<code>m[j, i] = NA</code>.
</p>


<h3>References</h3>

<p>Watson, D.S. &amp; Silva, R. (2022). Causal discovery under a confounder blanket.
To appear in <em>Proceedings of the 38th Conference on Uncertainty in
Artificial Intelligence</em>. <em>arXiv</em> preprint, 2205.05715.
</p>
<p>Shah, R. &amp; Samworth, R. (2013). Variable selection with error control:
Another look at stability selection. <em>J. R. Statist. Soc. B</em>,
<em>75</em>(1):55â€“80, 2013.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data
data(bipartite)
x &lt;- bipartite$x
z &lt;- bipartite$z

# Set seed
set.seed(123)

# Run CBL
cbl(x, z)

# With user-supplied feature selection subroutine
s_new &lt;- function(x, y) {
  # Fit model, extract coefficients
  df &lt;- data.frame(x, y)
  f_full &lt;- lm(y ~ 0 + ., data = df)
  f_reduced &lt;- step(f_full, trace = 0)
  keep &lt;- names(coef(f_reduced))
  # Return bit vector
  out &lt;- ifelse(colnames(x) %in% keep, 1, 0)
  return(out)
}

cbl(x, z, s = s_new)


</code></pre>

<hr>
<h2 id='epsilon_fn'>Computer the consistency lower bound</h2><span id='topic+epsilon_fn'></span>

<h3>Description</h3>

<p>Computer the consistency lower bound
</p>


<h3>Usage</h3>

<pre><code class='language-R'>epsilon_fn(df, B)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="epsilon_fn_+3A_df">df</code></td>
<td>
<p>Table of (de)activation rates.</p>
</td></tr>
<tr><td><code id="epsilon_fn_+3A_b">B</code></td>
<td>
<p>Number of complementary pairs to draw for stability selection.</p>
</td></tr>
</table>

<hr>
<h2 id='l0'>Feature selection subroutine</h2><span id='topic+l0'></span>

<h3>Description</h3>

<p>This function fits a potentially sparse supervised learning model and returns
a bit vector indicating which features were selected.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>l0(x, y, s, params, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="l0_+3A_x">x</code></td>
<td>
<p>Design matrix.</p>
</td></tr>
<tr><td><code id="l0_+3A_y">y</code></td>
<td>
<p>Outcome vector.</p>
</td></tr>
<tr><td><code id="l0_+3A_s">s</code></td>
<td>
<p>Regression method. Current options are <code>"lasso"</code> or
<code>"boost"</code>.</p>
</td></tr>
<tr><td><code id="l0_+3A_params">params</code></td>
<td>
<p>Optional list of parameters to use when <code>s = "boost"</code>.</p>
</td></tr>
<tr><td><code id="l0_+3A_...">...</code></td>
<td>
<p>Extra parameters to be passed to the feature selection subroutine.</p>
</td></tr>
</table>

<hr>
<h2 id='minD'>CPSS upper bound</h2><span id='topic+minD'></span>

<h3>Description</h3>

<p>Compute the min-D factor of Shah &amp; Samworth's Eq. 8 (2013). Code taken
verbatim from Rajen Shah's personal website:
<a href="http://www.statslab.cam.ac.uk/~rds37/papers/r_concave_tail.R">http://www.statslab.cam.ac.uk/~rds37/papers/r_concave_tail.R</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minD(theta, B, r = c(-1/2, -1/4))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="minD_+3A_theta">theta</code></td>
<td>
<p>Low rate threshold.</p>
</td></tr>
<tr><td><code id="minD_+3A_b">B</code></td>
<td>
<p>Number of complementary pairs for stability selection.</p>
</td></tr>
<tr><td><code id="minD_+3A_r">r</code></td>
<td>
<p>Of r-concavity fame.</p>
</td></tr>
</table>

<hr>
<h2 id='r.TailProbs'>CPSS utility functions</h2><span id='topic+r.TailProbs'></span>

<h3>Description</h3>

<p>Compute the tail probability of an r-concave random variable. Code taken
verbatim from Rajen Shah's personal website:
<a href="http://www.statslab.cam.ac.uk/~rds37/papers/r_concave_tail.R">http://www.statslab.cam.ac.uk/~rds37/papers/r_concave_tail.R</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r.TailProbs(eta, B, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="r.TailProbs_+3A_eta">eta</code></td>
<td>
<p>Upper bound on the expectation of the r-concave random variable.</p>
</td></tr>
<tr><td><code id="r.TailProbs_+3A_b">B</code></td>
<td>
<p>Number of complementary pairs for stability selection.</p>
</td></tr>
<tr><td><code id="r.TailProbs_+3A_r">r</code></td>
<td>
<p>Of r-concavity fame.</p>
</td></tr>
</table>

<hr>
<h2 id='ss_fn'>Infer causal direction using stability selection</h2><span id='topic+ss_fn'></span>

<h3>Description</h3>

<p>Infer causal direction using stability selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ss_fn(df, epsilon, order, rule, B)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ss_fn_+3A_df">df</code></td>
<td>
<p>Table of (de)activation rates.</p>
</td></tr>
<tr><td><code id="ss_fn_+3A_epsilon">epsilon</code></td>
<td>
<p>Consistency lower bound, as computed by <code>epsilon_fn</code>.</p>
</td></tr>
<tr><td><code id="ss_fn_+3A_order">order</code></td>
<td>
<p>Causal order of interest, either <code>"ij"</code> or <code>"ji"</code>.</p>
</td></tr>
<tr><td><code id="ss_fn_+3A_rule">rule</code></td>
<td>
<p>Inference rule, either <code>"R1"</code> or <code>"R2"</code>.</p>
</td></tr>
<tr><td><code id="ss_fn_+3A_b">B</code></td>
<td>
<p>Number of complementary pairs to draw for stability selection.</p>
</td></tr>
</table>

<hr>
<h2 id='sub_loop'>Complementary pairs subsampling loop</h2><span id='topic+sub_loop'></span>

<h3>Description</h3>

<p>This function executes one loop of the model quartet for a given pair of
foreground variables and records any disconnections and/or (de)activations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sub_loop(b, i, j, x, z_t, s, params, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sub_loop_+3A_b">b</code></td>
<td>
<p>Subsample index.</p>
</td></tr>
<tr><td><code id="sub_loop_+3A_i">i</code></td>
<td>
<p>First foreground variable index.</p>
</td></tr>
<tr><td><code id="sub_loop_+3A_j">j</code></td>
<td>
<p>Second foreground variable index.</p>
</td></tr>
<tr><td><code id="sub_loop_+3A_x">x</code></td>
<td>
<p>Matrix of foreground variables.</p>
</td></tr>
<tr><td><code id="sub_loop_+3A_z_t">z_t</code></td>
<td>
<p>Intersection of iteration-t known non-descendants for foreground
variables <code>i</code> and <code>j</code>.</p>
</td></tr>
<tr><td><code id="sub_loop_+3A_s">s</code></td>
<td>
<p>Regression method. Current options are <code>"lasso"</code> or
<code>"boost"</code>.</p>
</td></tr>
<tr><td><code id="sub_loop_+3A_params">params</code></td>
<td>
<p>Optional list of parameters to use when <code>s = "boost"</code>.</p>
</td></tr>
<tr><td><code id="sub_loop_+3A_...">...</code></td>
<td>
<p>Extra parameters to be passed to the feature selection subroutine.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
