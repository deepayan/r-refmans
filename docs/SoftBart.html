<!DOCTYPE html><html><head><title>Help for package SoftBart</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SoftBart}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#contr.ltfr'><p>Create a Full Set of Dummy Variables</p></a></li>
<li><a href='#gsoftbart_regression'><p>General SoftBart Regression</p></a></li>
<li><a href='#Hypers'><p>Create a list of hyperparameter values</p></a></li>
<li><a href='#MakeForest'><p>Create an Rcpp_Forest Object</p></a></li>
<li><a href='#Opts'><p>MCMC options for SoftBart</p></a></li>
<li><a href='#partial_dependence_probit'><p>Partial Dependence Function for SoftBART Probit Regression</p></a></li>
<li><a href='#partial_dependence_regression'><p>Partial Dependence Function for SoftBART Regression</p></a></li>
<li><a href='#pdsoftbart'><p>Partial dependence plots for SoftBart</p></a></li>
<li><a href='#posterior_probs'><p>BART Posterior Inclusion Probabilities</p></a></li>
<li><a href='#predict.softbart_probit'><p>Predict for SoftBart Probit Regression</p></a></li>
<li><a href='#predict.softbart_regression'><p>Predict for SoftBart Regression</p></a></li>
<li><a href='#preprocess_df'><p>Preprocess a dataset for use with SoftBart</p></a></li>
<li><a href='#quantile_normalize_bart'><p>Quantile normalization for predictors</p></a></li>
<li><a href='#rmse'><p>Root mean squared error</p></a></li>
<li><a href='#softbart'><p>Fits the SoftBart model</p></a></li>
<li><a href='#softbart_probit'><p>SoftBart Probit Regression</p></a></li>
<li><a href='#softbart_regression'><p>SoftBart Regression</p></a></li>
<li><a href='#vc_softbart_regression'><p>SoftBart Varying Coefficient Regression</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Implements the SoftBart Algorithm</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-10-28</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements the SoftBart model of described by Linero and Yang (2018) &lt;<a href="https://doi.org/10.1111%2Frssb.12293">doi:10.1111/rssb.12293</a>&gt;, with the optional use of a sparsity-inducing prior to allow for variable selection. For usability, the package maintains the same style as the 'BayesTree' package.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.9), glmnet (&ge; 4.0.0), scales (&ge; 1.1.1),
methods, caret, truncnorm, progress, MASS</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-10-29 00:13:31 UTC; antonio</td>
</tr>
<tr>
<td>Author:</td>
<td>Antonio R. Linero [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Antonio R. Linero &lt;antonio.linero@austin.utexas.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-10-29 07:00:17 UTC</td>
</tr>
</table>
<hr>
<h2 id='contr.ltfr'>Create a Full Set of Dummy Variables</h2><span id='topic+contr.ltfr'></span>

<h3>Description</h3>

<p>Used with <code>dummyVars</code> in the <span class="pkg">caret</span> package to create a full set
of dummy variables (i.e. less than full rank parameterization).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contr.ltfr(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="contr.ltfr_+3A_...">...</code></td>
<td>
<p>A list of arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix produced containing full sets of dummy variables.
</p>

<hr>
<h2 id='gsoftbart_regression'>General SoftBart Regression</h2><span id='topic+gsoftbart_regression'></span>

<h3>Description</h3>

<p>Fits the general (Soft) BART (GBART) model, which combines the BART model
with a linear predictor. That is, it fits the semiparametric Gaussian
regression model </p>
<p style="text-align: center;"><code class="reqn">Y = r(X) + Z^\top \beta + \epsilon</code>
</p>
<p> where the function
<code class="reqn">r(x)</code> is modeled using a BART ensemble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gsoftbart_regression(
  formula,
  linear_formula,
  data,
  test_data,
  num_tree = 20,
  k = 2,
  hypers = NULL,
  opts = NULL,
  remove_intercept = TRUE,
  verbose = TRUE,
  warn = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsoftbart_regression_+3A_formula">formula</code></td>
<td>
<p>A model formula with a numeric variable on the left-hand-side and non-linear predictors on the right-hand-side.</p>
</td></tr>
<tr><td><code id="gsoftbart_regression_+3A_linear_formula">linear_formula</code></td>
<td>
<p>A model formula with the linear variables on the right-hand-side (left-hand-side is not used).</p>
</td></tr>
<tr><td><code id="gsoftbart_regression_+3A_data">data</code></td>
<td>
<p>A data frame consisting of the training data.</p>
</td></tr>
<tr><td><code id="gsoftbart_regression_+3A_test_data">test_data</code></td>
<td>
<p>A data frame consisting of the testing data.</p>
</td></tr>
<tr><td><code id="gsoftbart_regression_+3A_num_tree">num_tree</code></td>
<td>
<p>The number of trees used in the ensemble.</p>
</td></tr>
<tr><td><code id="gsoftbart_regression_+3A_k">k</code></td>
<td>
<p>Determines the standard deviation of the leaf node parameters, which is given by <code>3 / k / sqrt(num_tree)</code>.</p>
</td></tr>
<tr><td><code id="gsoftbart_regression_+3A_hypers">hypers</code></td>
<td>
<p>A list of hyperparameters constructed from the <code>Hypers()</code> function (<code>num_tree</code>, <code>k</code>, and <code>sigma_mu</code> are overridden by this function).</p>
</td></tr>
<tr><td><code id="gsoftbart_regression_+3A_opts">opts</code></td>
<td>
<p>A list of options for running the chain constructed from the <code>Opts()</code> function (<code>update_sigma</code> is overridden by this function).</p>
</td></tr>
<tr><td><code id="gsoftbart_regression_+3A_remove_intercept">remove_intercept</code></td>
<td>
<p>If <code>TRUE</code> then any intercept term in the linear formula will be removed, with the overall location of the outcome captured by the nonparametric function.</p>
</td></tr>
<tr><td><code id="gsoftbart_regression_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, progress of the chain will be printed to the console.</p>
</td></tr>
<tr><td><code id="gsoftbart_regression_+3A_warn">warn</code></td>
<td>
<p>If <code>TRUE</code>, remind the user that they probably don't want the linear predictors to be included in the formula for the nonlinear part.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components
</p>

<ul>
<li> <p><code>r_train</code>: samples of the nonparametric function evaluated on the training set.
</p>
</li>
<li> <p><code>r_test</code>: samples of the nonparametric function evaluated on the test set.
</p>
</li>
<li> <p><code>eta_train</code>: samples of the linear predictor on the training set.
</p>
</li>
<li> <p><code>eta_test</code>: samples of the linear predictor on the test set.
</p>
</li>
<li> <p><code>mu_train</code>: samples of the prediction on the training set.
</p>
</li>
<li> <p><code>mu_test</code>: samples of the prediction on the test set.
</p>
</li>
<li> <p><code>beta</code>: samples of the regression coefficients.
</p>
</li>
<li> <p><code>sigma</code>: samples of the error standard deviation.
</p>
</li>
<li> <p><code>sigma_mu</code>: samples of the standard deviation of the leaf node parameters.
</p>
</li>
<li> <p><code>var_counts</code>: a matrix with a column for each nonparametric predictor containing the number of times that predictor is used in the ensemble at each iteration.
</p>
</li>
<li> <p><code>opts</code>: the options used when running the chain.
</p>
</li>
<li> <p><code>formula</code>: the formula specified by the user.
</p>
</li>
<li> <p><code>ecdfs</code>: empirical distribution functions, used by the predict function.
</p>
</li>
<li> <p><code>mu_Y, sd_Y</code>: used with the predict function to transform predictions.
</p>
</li>
<li> <p><code>forest</code>: a forest object for the nonlinear part; see the <code>MakeForest()</code> documentation for more details.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## NOTE: SET NUMBER OF BURN IN AND SAMPLE ITERATIONS HIGHER IN PRACTICE

num_burn &lt;- 10 ## Should be ~ 5000
num_save &lt;- 10 ## Should be ~ 5000

set.seed(1234)
f_fried &lt;- function(x) 10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 +
  10 * x[,4] + 5 * x[,5]

gen_data &lt;- function(n_train, n_test, P, sigma) {
  X &lt;- matrix(runif(n_train * P), nrow = n_train)
  mu &lt;- f_fried(X)
  X_test &lt;- matrix(runif(n_test * P), nrow = n_test)
  mu_test &lt;- f_fried(X_test)
  Y &lt;- mu + sigma * rnorm(n_train)
  Y_test &lt;- mu + sigma * rnorm(n_test)
  
  return(list(X = X, Y = Y, mu = mu, X_test = X_test, Y_test = Y_test,
              mu_test = mu_test))
}

## Simiulate dataset
sim_data &lt;- gen_data(250, 250, 100, 1)

df &lt;- data.frame(X = sim_data$X, Y = sim_data$Y)
df_test &lt;- data.frame(X = sim_data$X_test, Y = sim_data$Y_test)

## Fit the model

opts &lt;- Opts(num_burn = num_burn, num_save = num_save)
fitted_reg &lt;- gsoftbart_regression(Y ~ . - X.4 - X.5, ~ X.4 + X.5, df, df_test, opts = opts)

## Plot results

plot(colMeans(fitted_reg$mu_test), sim_data$mu_test)
abline(a = 0, b = 1)
plot(fitted_reg$beta[,1])
plot(fitted_reg$beta[,2])
</code></pre>

<hr>
<h2 id='Hypers'>Create a list of hyperparameter values</h2><span id='topic+Hypers'></span>

<h3>Description</h3>

<p>Creates a list which holds all the hyperparameters for use with the
model-fitting functions and with the <code>MakeForest</code> functionality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Hypers(
  X,
  Y,
  group = NULL,
  alpha = 1,
  beta = 2,
  gamma = 0.95,
  k = 2,
  sigma_hat = NULL,
  shape = 1,
  width = 0.1,
  num_tree = 20,
  alpha_scale = NULL,
  alpha_shape_1 = 0.5,
  alpha_shape_2 = 1,
  tau_rate = 10,
  num_tree_prob = NULL,
  temperature = 1,
  weights = NULL,
  normalize_Y = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hypers_+3A_x">X</code></td>
<td>
<p>A matrix of training data covariates.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_y">Y</code></td>
<td>
<p>A vector of training data responses.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_group">group</code></td>
<td>
<p>Allows for grouping of covariates with shared splitting proportions, which is useful for categorical dummy variables. For each column of <code>X</code>, <code>group</code> gives the associated group.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_alpha">alpha</code></td>
<td>
<p>Positive constant controlling the sparsity level.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_beta">beta</code></td>
<td>
<p>Parameter penalizing tree depth in the branching process prior.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_gamma">gamma</code></td>
<td>
<p>Parameter penalizing new nodes in the branching process prior.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_k">k</code></td>
<td>
<p>Related to the signal-to-noise ratio, <code>sigma_mu = 0.5 / (sqrt(num_tree) * k)</code>. BART defaults to <code>k = 2</code> after applying the max/min normalization to the outcome.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_sigma_hat">sigma_hat</code></td>
<td>
<p>A prior guess at the conditional variance of <code>Y</code> given <code>X</code>. If not provided, this is estimated empirically by linear regression.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_shape">shape</code></td>
<td>
<p>Shape parameter for gating probabilities.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_width">width</code></td>
<td>
<p>Bandwidth of gating probabilities.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_num_tree">num_tree</code></td>
<td>
<p>Number of trees in the ensemble.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_alpha_scale">alpha_scale</code></td>
<td>
<p>Scale of the prior for <code>alpha</code>; if not provided, defaults to the number of predictors.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_alpha_shape_1">alpha_shape_1</code></td>
<td>
<p>Shape parameter for prior on <code>alpha</code>; if not provided, defaults to 0.5.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_alpha_shape_2">alpha_shape_2</code></td>
<td>
<p>Shape parameter for prior on <code>alpha</code>; if not provided, defaults to 1.0.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_tau_rate">tau_rate</code></td>
<td>
<p>Rate parameter for the bandwidths of the trees with an exponential prior; defaults to 10.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_num_tree_prob">num_tree_prob</code></td>
<td>
<p>Parameter for geometric prior on number of tree.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_temperature">temperature</code></td>
<td>
<p>The temperature applied to the posterior distribution; set to 1 unless you know what you are doing.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_weights">weights</code></td>
<td>
<p>Only used by the function <code>softbart</code>, this is a vector of weights to be used in heteroskedastic regression models, with the variance of an observation given by <code>sigma_sq / weight</code>.</p>
</td></tr>
<tr><td><code id="Hypers_+3A_normalize_y">normalize_Y</code></td>
<td>
<p>Do you want to compute <code>sigma_hat</code> after applying the standard BART max/min normalization to <code class="reqn">(-0.5, 0.5)</code> for the outcome? If <code>FALSE</code>, no normalization is applied. This might be useful for fitting custom models where the outcome is normalized by hand.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list containing the function arguments.
</p>

<hr>
<h2 id='MakeForest'>Create an Rcpp_Forest Object</h2><span id='topic+MakeForest'></span>

<h3>Description</h3>

<p>Make an object of type <code>Rcpp_Fores</code>t, which can be used to embed a soft
BART model into other models. Some examples are given in the package
vignette.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MakeForest(hypers, opts, warn = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MakeForest_+3A_hypers">hypers</code></td>
<td>
<p>A list of hyperparameter values obtained from <code>Hypers()</code> function</p>
</td></tr>
<tr><td><code id="MakeForest_+3A_opts">opts</code></td>
<td>
<p>A list of MCMC chain settings obtained from <code>Opts()</code> function</p>
</td></tr>
<tr><td><code id="MakeForest_+3A_warn">warn</code></td>
<td>
<p>If <code>TRUE</code>, reminds the user to normalize their design matrix when interacting with a forest object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of type <code>Rcpp_Forest</code>. If <code>forest</code> is an
<code>Rcpp_Forest</code> object then it has the following methods.
</p>

<ul>
<li> <p><code>forest$do_gibbs(X, Y, X_test, i)</code> runs <code>i</code> iterations of
the Bayesian backfitting algorithm and predicts on the test set
<code>X_test</code>. The state of forest is also updated.
</p>
</li>
<li> <p><code>forest$do_gibbs_weighted(X, Y, weights X_test, i)</code> runs <code>i</code>
iterations of the Bayesian backfitting algorithm and predicts on the test
set <code>X_test</code>; assumes that <code>Y</code> is heteroskedastic with known weights. The state
of forest is also updated.
</p>
</li>
<li> <p><code>forest$do_predict(X)</code> returns the predictions from a matrix <code>X</code>
of predictors.
</p>
</li>
<li> <p><code>forest$get_counts()</code> returns the number of times each variable
has been used in a splitting rule at the current state of <code>forest</code>.
</p>
</li>
<li> <p><code>forest$get_s()</code> returns the splitting probabilities of the
forest.
</p>
</li>
<li> <p><code>forest$get_sigma()</code> returns the error standard deviation of the
forest.
</p>
</li>
<li> <p><code>forest$get_sigma_mu()</code> returns the standard deviation of the
leaf node parameters.
</p>
</li>
<li> <p><code>forest$get_tree_counts()</code> returns a matrix with a column for
each predictor and a row for each tree that counts the number of times each
predictor is used in each tree at the current state of <code>forest</code>.
</p>
</li>
<li> <p><code>forest$predict_iteration(X, i)</code> returns the predictions from a
matrix <code>X</code> of predictors at iteration <code>i</code>. Requires that <code>opts$cache_trees =
  TRUE</code> in <code>MakeForest(hypers, opts)</code>.
</p>
</li>
<li> <p><code>forest$set_s(s)</code> sets the splitting probabilities of the forest
to <code>s</code>.
</p>
</li>
<li> <p><code>forest$set_sigma(x)</code> sets the error standard deviation of the
forest to <code>x</code>.
</p>
</li>
<li> <p><code>forest$num_gibbs</code> returns the number of iterations in total
that the Gibbs sampler has been run.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
X &lt;- matrix(runif(100 * 10), nrow = 100, ncol = 10)
Y &lt;- rowSums(X) + rnorm(100)
my_forest &lt;- MakeForest(Hypers(X,Y), Opts())
mu_hat &lt;- my_forest$do_gibbs(X,Y,X,200)

</code></pre>

<hr>
<h2 id='Opts'>MCMC options for SoftBart</h2><span id='topic+Opts'></span>

<h3>Description</h3>

<p>Creates a list that provides the parameters for running the Markov chain.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Opts(
  num_burn = 2500,
  num_thin = 1,
  num_save = 2500,
  num_print = 100,
  update_sigma_mu = TRUE,
  update_s = TRUE,
  update_alpha = TRUE,
  update_beta = FALSE,
  update_gamma = FALSE,
  update_tau = TRUE,
  update_tau_mean = FALSE,
  update_sigma = TRUE,
  cache_trees = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Opts_+3A_num_burn">num_burn</code></td>
<td>
<p>Number of warmup iterations for the chain.</p>
</td></tr>
<tr><td><code id="Opts_+3A_num_thin">num_thin</code></td>
<td>
<p>Thinning interval for the chain.</p>
</td></tr>
<tr><td><code id="Opts_+3A_num_save">num_save</code></td>
<td>
<p>The number of samples to collect; in total, <code>num_burn + num_save * num_thin</code> iterations are run.</p>
</td></tr>
<tr><td><code id="Opts_+3A_num_print">num_print</code></td>
<td>
<p>Interval for how often to print the chain's progress.</p>
</td></tr>
<tr><td><code id="Opts_+3A_update_sigma_mu">update_sigma_mu</code></td>
<td>
<p>If <code>TRUE</code>, <code>sigma_mu</code> is  updated, with a half-Cauchy prior on <code>sigma_mu</code> centered at the initial guess.</p>
</td></tr>
<tr><td><code id="Opts_+3A_update_s">update_s</code></td>
<td>
<p>If <code>TRUE</code>, <code>s</code> is updated using the Dirichlet prior <code class="reqn">s \sim D(\alpha / P, \ldots, \alpha / P)</code> where <code class="reqn">P</code> is the number of covariates.</p>
</td></tr>
<tr><td><code id="Opts_+3A_update_alpha">update_alpha</code></td>
<td>
<p>If <code>TRUE</code>, <code>alpha</code> is updated using a scaled beta prime prior.</p>
</td></tr>
<tr><td><code id="Opts_+3A_update_beta">update_beta</code></td>
<td>
<p>If <code>TRUE</code>, <code>beta</code> is updated using a normal prior with mean 0 and variance 4.</p>
</td></tr>
<tr><td><code id="Opts_+3A_update_gamma">update_gamma</code></td>
<td>
<p>If <code>TRUE</code>, gamma is updated using a Uniform(0.5, 1) prior.</p>
</td></tr>
<tr><td><code id="Opts_+3A_update_tau">update_tau</code></td>
<td>
<p>If <code>TRUE</code>, the bandwidth <code>tau</code> is updated for each tree</p>
</td></tr>
<tr><td><code id="Opts_+3A_update_tau_mean">update_tau_mean</code></td>
<td>
<p>If <code>TRUE</code>, the mean of <code>tau</code> is updated</p>
</td></tr>
<tr><td><code id="Opts_+3A_update_sigma">update_sigma</code></td>
<td>
<p>If <code>TRUE</code>, <code>sigma</code> is updated, with a half-Cauchy prior on <code>sigma</code> centered at the initial guess.</p>
</td></tr>
<tr><td><code id="Opts_+3A_cache_trees">cache_trees</code></td>
<td>
<p>If <code>TRUE</code>, we save the trees for each MCMC iteration when using the MakeForest interface</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list containing the function arguments.
</p>

<hr>
<h2 id='partial_dependence_probit'>Partial Dependence Function for SoftBART Probit Regression</h2><span id='topic+partial_dependence_probit'></span>

<h3>Description</h3>

<p>Computes the partial dependence function for a given covariate at a given set of covariate values for the probit model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial_dependence_probit(fit, test_data, var_str, grid)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial_dependence_probit_+3A_fit">fit</code></td>
<td>
<p>A fitted model of type <code>softbart_probit</code>.</p>
</td></tr>
<tr><td><code id="partial_dependence_probit_+3A_test_data">test_data</code></td>
<td>
<p>A data set used to form the baseline distribution of covariates for the partial dependence function.</p>
</td></tr>
<tr><td><code id="partial_dependence_probit_+3A_var_str">var_str</code></td>
<td>
<p>A string giving the variable name of the predictor to compute the partial dependence function for.</p>
</td></tr>
<tr><td><code id="partial_dependence_probit_+3A_grid">grid</code></td>
<td>
<p>The values of the predictor to compute the partial dependence function at.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components:
</p>

<ul>
<li> <p><code>pred_df</code>: a data frame containing columns for a MCMC iteration ID (<code>sample</code>), the value on the grid, and the partial dependence function value.
</p>
</li>
<li> <p><code>p</code>: a matrix containing the same information as <code>pred_df</code>, with the rows corresponding to iterations and columns corresponding to grid values.
</p>
</li>
<li> <p><code>grid</code>: the grid used as input.
</p>
</li></ul>


<hr>
<h2 id='partial_dependence_regression'>Partial Dependence Function for SoftBART Regression</h2><span id='topic+partial_dependence_regression'></span>

<h3>Description</h3>

<p>Computes the partial dependence function for a given covariate at a given set of covariate values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial_dependence_regression(fit, test_data, var_str, grid)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial_dependence_regression_+3A_fit">fit</code></td>
<td>
<p>A fitted model of type <code>softbart_regression</code>.</p>
</td></tr>
<tr><td><code id="partial_dependence_regression_+3A_test_data">test_data</code></td>
<td>
<p>A data set used to form the baseline distribution of covariates for the partial dependence function.</p>
</td></tr>
<tr><td><code id="partial_dependence_regression_+3A_var_str">var_str</code></td>
<td>
<p>A string giving the variable name of the predictor to compute the partial dependence function for.</p>
</td></tr>
<tr><td><code id="partial_dependence_regression_+3A_grid">grid</code></td>
<td>
<p>The values of the predictor to compute the partial dependence function at.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components:
</p>

<ul>
<li> <p><code>pred_df</code>: a data.frame containing columns for a MCMC iteration ID (<code>sample</code>), the value on the grid, and the partial dependence function value.
</p>
</li>
<li> <p><code>mu</code>: a matrix containing the same information as <code>pred_df</code>, with the rows corresponding to iterations and columns corresponding to grid values.
</p>
</li>
<li> <p><code>grid</code>: the grid used as input.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## NOTE: SET NUMBER OF BURN IN AND SAMPLE ITERATIONS HIGHER IN PRACTICE

num_burn &lt;- 10 ## Should be ~ 5000
num_save &lt;- 10 ## Should be ~ 5000

set.seed(1234)
f_fried &lt;- function(x) 10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 + 
  10 * x[,4] + 5 * x[,5]

gen_data &lt;- function(n_train, n_test, P, sigma) {
  X &lt;- matrix(runif(n_train * P), nrow = n_train)
  mu &lt;- f_fried(X)
  X_test &lt;- matrix(runif(n_test * P), nrow = n_test)
  mu_test &lt;- f_fried(X_test)
  Y &lt;- mu + sigma * rnorm(n_train)
  Y_test &lt;- mu + sigma * rnorm(n_test)
  
  return(list(X = X, Y = Y, mu = mu, X_test = X_test, Y_test = Y_test, 
              mu_test = mu_test))
}

## Simiulate dataset
sim_data &lt;- gen_data(250, 250, 10, 1)

df &lt;- data.frame(X = sim_data$X, Y = sim_data$Y)
df_test &lt;- data.frame(X = sim_data$X_test, Y = sim_data$Y_test)

## Fit the model

opts &lt;- Opts(num_burn = num_burn, num_save = num_save)
fitted_reg &lt;- softbart_regression(Y ~ ., df, df_test, opts = opts)

## Compute PDP and plot

grid &lt;- seq(from = 0, to = 1, length = 10)
pdp_x4 &lt;- partial_dependence_regression(fitted_reg, df_test, "X.4", grid)
plot(pdp_x4$grid, colMeans(pdp_x4$mu))
</code></pre>

<hr>
<h2 id='pdsoftbart'>Partial dependence plots for SoftBart</h2><span id='topic+pdsoftbart'></span>

<h3>Description</h3>

<p>Modified version of the <code>pdbart</code> function from the <code>BayesTree</code>
package; largely supplanted by the <code>softbart_regression</code> and
<code>partial_dependence_regression</code> functions. Runs <code>softbart</code> at test
observations constructed so that a plot can be created displaying the effect
of a single variable or pair of variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pdsoftbart(
  X,
  Y,
  xind = NULL,
  levs = NULL,
  levquants = c(0.05, (1:9)/10, 0.95),
  pl = FALSE,
  plquants = c(0.05, 0.95),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pdsoftbart_+3A_x">X</code></td>
<td>
<p>Training data covariates.</p>
</td></tr>
<tr><td><code id="pdsoftbart_+3A_y">Y</code></td>
<td>
<p>Training data response.</p>
</td></tr>
<tr><td><code id="pdsoftbart_+3A_xind">xind</code></td>
<td>
<p>Variables to create the partial dependence plots for.</p>
</td></tr>
<tr><td><code id="pdsoftbart_+3A_levs">levs</code></td>
<td>
<p>List of levels of the covariates to evaluate at.</p>
</td></tr>
<tr><td><code id="pdsoftbart_+3A_levquants">levquants</code></td>
<td>
<p>Used if <code>levs</code> is not supplied; takes <code>levs</code> to be quantiles of associated predictors.</p>
</td></tr>
<tr><td><code id="pdsoftbart_+3A_pl">pl</code></td>
<td>
<p>Create a plot?</p>
</td></tr>
<tr><td><code id="pdsoftbart_+3A_plquants">plquants</code></td>
<td>
<p>Quantiles for the partial dependence plot.</p>
</td></tr>
<tr><td><code id="pdsoftbart_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to softbart or plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with components given below. 
</p>

<ul>
<li> <p><code>fd</code>: A matrix whose <code>(i,j)</code>th value is the <code>i</code>th draw of the partial dependence function for the 
<code>j</code>th level.
</p>
</li>
<li> <p><code>levs</code>: The list of levels used, each component corresponding to a 
variable. If the argument <code>levs</code> was supplied it is unchanged. Otherwise, the 
levels in levs are constructed using the argument <code>levquants</code>.
</p>
</li></ul>


<hr>
<h2 id='posterior_probs'>BART Posterior Inclusion Probabilities</h2><span id='topic+posterior_probs'></span>

<h3>Description</h3>

<p>Computes the posterior inclusion probabilities (PIPs) for the fitted 
SoftBART model, as well as variable importances and the median probability
model (MPM).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posterior_probs(fit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="posterior_probs_+3A_fit">fit</code></td>
<td>
<p>An object of class <code>softbart</code>, <code>softbart_regression</code>, or
<code>softbart_probit</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following:
</p>

<ul>
<li> <p><code>varimp</code>: a vector containing the average number of times a predictor
was used in a splitting rule.
</p>
</li>
<li> <p><code>post_probs</code>: the posterior inclusion probabilities for each predictor.
</p>
</li>
<li> <p><code>median_probability_model</code>: a vector containing the indicies of the
variables included in at least 50 percent of the samples.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## NOTE: SET NUMBER OF BURN IN AND SAMPLE ITERATIONS HIGHER IN PRACTICE

num_burn &lt;- 10 ## Should be ~ 5000
num_save &lt;- 10 ## Should be ~ 5000

set.seed(1234)
f_fried &lt;- function(x) 10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 + 
  10 * x[,4] + 5 * x[,5]

gen_data &lt;- function(n_train, n_test, P, sigma) {
  X &lt;- matrix(runif(n_train * P), nrow = n_train)
  mu &lt;- f_fried(X)
  X_test &lt;- matrix(runif(n_test * P), nrow = n_test)
  mu_test &lt;- f_fried(X_test)
  Y &lt;- mu + sigma * rnorm(n_train)
  Y_test &lt;- mu_test + sigma * rnorm(n_test)
  
  return(list(X = X, Y = Y, mu = mu, X_test = X_test, Y_test = Y_test, mu_test = mu_test))
}

## Simiulate dataset
sim_data &lt;- gen_data(250, 100, 1000, 1)

## Fit the model
fit &lt;- softbart(X = sim_data$X, Y = sim_data$Y, X_test = sim_data$X_test, 
                hypers = Hypers(sim_data$X, sim_data$Y, num_tree = 50, temperature = 1),
                opts = Opts(num_burn = num_burn, num_save = num_save, update_tau = TRUE))
                
## Variable selection

post_probs &lt;- posterior_probs(fit)
plot(post_probs$post_probs)
print(post_probs$median_probability_model)


</code></pre>

<hr>
<h2 id='predict.softbart_probit'>Predict for SoftBart Probit Regression</h2><span id='topic+predict.softbart_probit'></span>

<h3>Description</h3>

<p>Computes predictions from a <code>softbart_probit</code> object for new data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'softbart_probit'
predict(object, newdata, iterations = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.softbart_probit_+3A_object">object</code></td>
<td>
<p>A <code>softbart_probit</code> object obtained as output of the <code>softbart_probit</code> function.</p>
</td></tr>
<tr><td><code id="predict.softbart_probit_+3A_newdata">newdata</code></td>
<td>
<p>A dataset to construct predictions on.</p>
</td></tr>
<tr><td><code id="predict.softbart_probit_+3A_iterations">iterations</code></td>
<td>
<p>The iterations get predictions on; includes all of iterations including burn-in and thinning iterations. Defaults to the saved iterations, running from <code>(num_burn + num_thin):(num_burn + num_thin * num_save)</code>.</p>
</td></tr>
<tr><td><code id="predict.softbart_probit_+3A_...">...</code></td>
<td>
<p>Other arguments passed to predict.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing 
</p>

<ul>
<li> <p><code>mu</code>: samples of the nonparametric function for each observation, where <code>pnorm(mu)</code> is the success probability.
</p>
</li>
<li> <p><code>mu_mean</code>: posterior mean of mu.
</p>
</li>
<li> <p><code>p</code>: samples of the success probability <code>pnorm(mu)</code> for each observation.
</p>
</li>
<li> <p><code>p_mean</code>: posterior mean of <code>p</code>.
</p>
</li></ul>


<hr>
<h2 id='predict.softbart_regression'>Predict for SoftBart Regression</h2><span id='topic+predict.softbart_regression'></span>

<h3>Description</h3>

<p>Computes predictions from a <code>softbart_regression</code> object on new data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'softbart_regression'
predict(object, newdata, iterations = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.softbart_regression_+3A_object">object</code></td>
<td>
<p>A <code>softbart_regression</code> object obtained as output of the <code>softbart_regression()</code> function.</p>
</td></tr>
<tr><td><code id="predict.softbart_regression_+3A_newdata">newdata</code></td>
<td>
<p>A dataset to construct predictions on.</p>
</td></tr>
<tr><td><code id="predict.softbart_regression_+3A_iterations">iterations</code></td>
<td>
<p>The iterations to get predictions on; includes all of iterations including burn-in and thinning iterations. Defaults to the saved iterations, running from <code>(num_burn + num_thin):(num_burn + num_thin * num_save)</code>.</p>
</td></tr>
<tr><td><code id="predict.softbart_regression_+3A_...">...</code></td>
<td>
<p>Other arguments passed to predict.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing 
</p>

<ul>
<li> <p><code>mu</code>: samples of the predicted value for each observation and iteration.
</p>
</li>
<li> <p><code>mu_mean</code>: posterior predicted values for each observation.
</p>
</li></ul>


<hr>
<h2 id='preprocess_df'>Preprocess a dataset for use with SoftBart</h2><span id='topic+preprocess_df'></span>

<h3>Description</h3>

<p>Preprocesses a data frame for use with <code>softbart</code>; not needed with other
model fitting functions, but may also be useful when designing custom methods
with <code>MakeForest</code>. Returns a data matrix X that will work with
categorical predictors, and a vector of group indicators; this is required to
get sensible variable selection for categorical variables, and should be
passed in as the group argument to <code>Hypers</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess_df(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_df_+3A_x">X</code></td>
<td>
<p>A data frame, possibly containing categorical variables stored as
factors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing two elements.
</p>

<ul>
<li> <p><code>X</code>: a matrix consisting of the columns of the input data frame,
with separate columns for the different levels of categorical variables.
</p>
</li>
<li> <p><code>group</code>: a vector of group memberships of the predictors in
<code>X</code>, to be passed as an argument to <code>Hypers</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
preprocess_df(iris)

</code></pre>

<hr>
<h2 id='quantile_normalize_bart'>Quantile normalization for predictors</h2><span id='topic+quantile_normalize_bart'></span>

<h3>Description</h3>

<p>Performs a quantile normalization to each column of the matrix <code>X</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quantile_normalize_bart(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quantile_normalize_bart_+3A_x">X</code></td>
<td>
<p>A design matrix, should not include a column for the intercept.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix <code>X_norm</code> such that each column gives the associated
empirical quantile of each observation for each predictor.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(rgamma(100 * 10, shape = 2), nrow = 100)
X &lt;- quantile_normalize_bart(X)
summary(X)

</code></pre>

<hr>
<h2 id='rmse'>Root mean squared error</h2><span id='topic+rmse'></span>

<h3>Description</h3>

<p>Computes the root mean-squared error between <code>y</code> and <code>yhat</code>, given
by <code>sqrt(mean((y - yhat)^2))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmse(y, yhat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmse_+3A_y">y</code></td>
<td>
<p>the realized outcomes</p>
</td></tr>
<tr><td><code id="rmse_+3A_yhat">yhat</code></td>
<td>
<p>the predicted outcomes</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the root mean-squared error.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
rmse(c(1,1,1), c(1,0,2))
</code></pre>

<hr>
<h2 id='softbart'>Fits the SoftBart model</h2><span id='topic+softbart'></span>

<h3>Description</h3>

<p>Runs the Markov chain for the semiparametric Gaussian model </p>
<p style="text-align: center;"><code class="reqn">Y = r(X) +
\epsilon</code>
</p>
<p> and collects the output, where <code class="reqn">r(x)</code>
is modeled using a soft BART model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softbart(X, Y, X_test, hypers = NULL, opts = Opts(), verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="softbart_+3A_x">X</code></td>
<td>
<p>A matrix of training data covariates.</p>
</td></tr>
<tr><td><code id="softbart_+3A_y">Y</code></td>
<td>
<p>A vector of training data responses.</p>
</td></tr>
<tr><td><code id="softbart_+3A_x_test">X_test</code></td>
<td>
<p>A matrix of test data covariates</p>
</td></tr>
<tr><td><code id="softbart_+3A_hypers">hypers</code></td>
<td>
<p>A ;ist of hyperparameter values obtained from <code>Hypers</code> function</p>
</td></tr>
<tr><td><code id="softbart_+3A_opts">opts</code></td>
<td>
<p>A list of MCMC chain settings obtained from <code>Opts</code> function</p>
</td></tr>
<tr><td><code id="softbart_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, progress of the chain will be printed to the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components:
</p>

<ul>
<li> <p><code>y_hat_train</code>: predicted values for the training data for each iteration of the chain.
</p>
</li>
<li> <p><code>y_hat_test</code>: predicted values for the test data for each iteration of the chain.
</p>
</li>
<li> <p><code>y_hat_train_mean</code>: predicted values for the training data, averaged over iterations.
</p>
</li>
<li> <p><code>y_hat_test_mean</code>: predicted values for the test data, averaged over iterations.
</p>
</li>
<li> <p><code>sigma</code>: posterior samples of the error standard deviations.
</p>
</li>
<li> <p><code>sigma_mu</code>: posterior samples of <code>sigma_mu</code>, the standard deviation of the leaf node parameters.
</p>
</li>
<li> <p><code>s</code>: posterior samples of <code>s</code>.
</p>
</li>
<li> <p><code>alpha</code>: posterior samples of <code>alpha</code>.
</p>
</li>
<li> <p><code>beta</code>: posterior samples of <code>beta</code>.
</p>
</li>
<li> <p><code>gamma</code>: posterior samples of <code>gamma</code>.
</p>
</li>
<li> <p><code>k</code>: posterior samples of <code>k = 0.5 / (sqrt(num_tree) * sigma_mu)</code>
</p>
</li>
<li> <p><code>num_leaves_final</code>: the number of leaves for each tree at the final iteration.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## NOTE: SET NUMBER OF BURN IN AND SAMPLE ITERATIONS HIGHER IN PRACTICE

num_burn &lt;- 10 ## Should be ~ 5000
num_save &lt;- 10 ## Should be ~ 5000

set.seed(1234)
f_fried &lt;- function(x) 10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 + 
  10 * x[,4] + 5 * x[,5]

gen_data &lt;- function(n_train, n_test, P, sigma) {
  X &lt;- matrix(runif(n_train * P), nrow = n_train)
  mu &lt;- f_fried(X)
  X_test &lt;- matrix(runif(n_test * P), nrow = n_test)
  mu_test &lt;- f_fried(X_test)
  Y &lt;- mu + sigma * rnorm(n_train)
  Y_test &lt;- mu_test + sigma * rnorm(n_test)
  
  return(list(X = X, Y = Y, mu = mu, X_test = X_test, Y_test = Y_test, mu_test = mu_test))
}

## Simiulate dataset
sim_data &lt;- gen_data(250, 100, 1000, 1)

## Fit the model
fit &lt;- softbart(X = sim_data$X, Y = sim_data$Y, X_test = sim_data$X_test, 
                hypers = Hypers(sim_data$X, sim_data$Y, num_tree = 50, temperature = 1),
                opts = Opts(num_burn = num_burn, num_save = num_save, update_tau = TRUE))

## Plot the fit (note: interval estimates are not prediction intervals, 
## so they do not cover the predictions at the nominal rate)
plot(fit)

## Look at posterior model inclusion probabilities for each predictor. 

plot(posterior_probs(fit)[["post_probs"]], 
     col = ifelse(posterior_probs(fit)[["post_probs"]] &gt; 0.5, scales::muted("blue"), 
                  scales::muted("green")), 
     pch = 20)


rmse(fit$y_hat_test_mean, sim_data$mu_test)
rmse(fit$y_hat_train_mean, sim_data$mu)

</code></pre>

<hr>
<h2 id='softbart_probit'>SoftBart Probit Regression</h2><span id='topic+softbart_probit'></span>

<h3>Description</h3>

<p>Fits a nonparametric probit regression model with the nonparametric function
modeled using a SoftBart model. Specifically, the model takes <code class="reqn">\Pr(Y = 1
\mid X = x) = \Phi\{a + r(x)\}</code> where
<code class="reqn">a</code> is an offset and <code class="reqn">r(x)</code> is a Soft BART ensemble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softbart_probit(
  formula,
  data,
  test_data,
  num_tree = 20,
  k = 1,
  hypers = NULL,
  opts = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="softbart_probit_+3A_formula">formula</code></td>
<td>
<p>A model formula with a binary factor on the left-hand-side and predictors on the right-hand-side.</p>
</td></tr>
<tr><td><code id="softbart_probit_+3A_data">data</code></td>
<td>
<p>A data frame consisting of the training data.</p>
</td></tr>
<tr><td><code id="softbart_probit_+3A_test_data">test_data</code></td>
<td>
<p>A data frame consisting of the testing data.</p>
</td></tr>
<tr><td><code id="softbart_probit_+3A_num_tree">num_tree</code></td>
<td>
<p>The number of trees in the ensemble to use.</p>
</td></tr>
<tr><td><code id="softbart_probit_+3A_k">k</code></td>
<td>
<p>Determines the standard deviation of the leaf node parameters, which is given by <code>3 / k / sqrt(num_tree)</code>.</p>
</td></tr>
<tr><td><code id="softbart_probit_+3A_hypers">hypers</code></td>
<td>
<p>A list of hyperparameters constructed from the <code>Hypers()</code> function (<code>num_tree</code>, <code>k</code>, and <code>sigma_mu</code> are overridden by this function).</p>
</td></tr>
<tr><td><code id="softbart_probit_+3A_opts">opts</code></td>
<td>
<p>A list of options for running the chain constructed from the <code>Opts()</code> function (<code>update_sigma</code> is overridden by this function).</p>
</td></tr>
<tr><td><code id="softbart_probit_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, progress of the chain will be printed to the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components:
</p>

<ul>
<li> <p><code>sigma_mu</code>: samples of the standard deviation of the leaf node parameters
</p>
</li>
<li> <p><code>var_counts</code>: a matrix with a column for each predictor group containing the number of times each predictor is used in the ensemble at each iteration.
</p>
</li>
<li> <p><code>mu_train</code>: samples of the nonparametric function evaluated on the training set; <code>pnorm(mu_train)</code> gives the success probabilities.
</p>
</li>
<li> <p><code>mu_test</code>: samples of the nonparametric function evaluated on the test set; <code>pnorm(mu_train)</code> gives the success probabilities .
</p>
</li>
<li> <p><code>p_train</code>: samples of probabilities on training set.
</p>
</li>
<li> <p><code>p_test</code>: samples of probabilities on test set.
</p>
</li>
<li> <p><code>mu_train_mean</code>: posterior mean of <code>mu_train</code>.
</p>
</li>
<li> <p><code>mu_test_mean</code>: posterior mean of <code>mu_test</code>.
</p>
</li>
<li> <p><code>p_train_mean</code>: posterior mean of <code>p_train</code>.
</p>
</li>
<li> <p><code>p_test_mean</code>: posterior mean of <code>p_test</code>.
</p>
</li>
<li> <p><code>offset</code>: we fit model of the form (offset + BART), with the offset estimated empirically prior to running the chain.
</p>
</li>
<li> <p><code>pnorm_offset</code>: the <code>pnorm</code> of the offset, which is chosen to match the probability of the second factor level.
</p>
</li>
<li> <p><code>formula</code>: the formula specified by the user.
</p>
</li>
<li> <p><code>ecdfs</code>: empirical distribution functions, used by the <code>predict</code> function.
</p>
</li>
<li> <p><code>opts</code>: the options used when running the chain.
</p>
</li>
<li> <p><code>forest</code>: a forest object; see the <code>MakeForest</code> documentation for more details.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## NOTE: SET NUMBER OF BURN IN AND SAMPLE ITERATIONS HIGHER IN PRACTICE

num_burn &lt;- 10 ## Should be ~ 5000
num_save &lt;- 10 ## Should be ~ 5000

set.seed(1234)
f_fried &lt;- function(x) 10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 + 
  10 * x[,4] + 5 * x[,5]

gen_data &lt;- function(n_train, n_test, P, sigma) {
  X &lt;- matrix(runif(n_train * P), nrow = n_train)
  mu &lt;- (f_fried(X) - 14) / 5
  X_test &lt;- matrix(runif(n_test * P), nrow = n_test)
  mu_test &lt;- (f_fried(X_test) - 14) / 5
  Y &lt;- factor(rbinom(n_train, 1, pnorm(mu)), levels = c(0,1))
  Y_test &lt;- factor(rbinom(n_test, 1, pnorm(mu_test)), levels = c(0,1))
  
  return(list(X = X, Y = Y, mu = mu, X_test = X_test, Y_test = Y_test, 
              mu_test = mu_test))
}

## Simiulate dataset
sim_data &lt;- gen_data(250, 250, 100, 1)

df &lt;- data.frame(X = sim_data$X, Y = sim_data$Y)
df_test &lt;- data.frame(X = sim_data$X_test, Y = sim_data$Y_test)

## Fit the model

opts &lt;- Opts(num_burn = num_burn, num_save = num_save)
fitted_probit &lt;- softbart_probit(Y ~ ., df, df_test, opts = opts)

## Plot results

plot(fitted_probit$mu_test_mean, sim_data$mu_test)
abline(a = 0, b = 1)


</code></pre>

<hr>
<h2 id='softbart_regression'>SoftBart Regression</h2><span id='topic+softbart_regression'></span>

<h3>Description</h3>

<p>Fits a semiparametric regression model with the nonparametric function
modeled using a SoftBart model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softbart_regression(
  formula,
  data,
  test_data,
  num_tree = 20,
  k = 2,
  hypers = NULL,
  opts = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="softbart_regression_+3A_formula">formula</code></td>
<td>
<p>A model formula with a numeric variable on the left-hand-side and predictors on the right-hand-side.</p>
</td></tr>
<tr><td><code id="softbart_regression_+3A_data">data</code></td>
<td>
<p>A data frame consisting of the training data.</p>
</td></tr>
<tr><td><code id="softbart_regression_+3A_test_data">test_data</code></td>
<td>
<p>A data frame consisting of the testing data.</p>
</td></tr>
<tr><td><code id="softbart_regression_+3A_num_tree">num_tree</code></td>
<td>
<p>The number of trees in the ensemble to use.</p>
</td></tr>
<tr><td><code id="softbart_regression_+3A_k">k</code></td>
<td>
<p>Determines the standard deviation of the leaf node parameters, which is given by <code>3 / k / sqrt(num_tree)</code>.</p>
</td></tr>
<tr><td><code id="softbart_regression_+3A_hypers">hypers</code></td>
<td>
<p>A list of hyperparameters constructed from the <code>Hypers()</code> function (<code>num_tree</code>, <code>k</code>, and <code>sigma_mu</code> are overridden by this function).</p>
</td></tr>
<tr><td><code id="softbart_regression_+3A_opts">opts</code></td>
<td>
<p>A list of options for running the chain constructed from the <code>Opts()</code> function (<code>update_sigma</code> is overridden by this function).</p>
</td></tr>
<tr><td><code id="softbart_regression_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, progress of the chain will be printed to the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components:
</p>

<ul>
<li> <p><code>sigma_mu</code>: samples of the standard deviation of the leaf node parameters.
</p>
</li>
<li> <p><code>sigma</code>: samples of the error standard deviation.
</p>
</li>
<li> <p><code>var_counts</code>: a matrix with a column for each predictor group containing the number of times each predictor is used in the ensemble at each iteration.
</p>
</li>
<li> <p><code>mu_train</code>: samples of the nonparametric function evaluated on the training set.
</p>
</li>
<li> <p><code>mu_test</code>: samples of the nonparametric function evaluated on the test set.
</p>
</li>
<li> <p><code>mu_train_mean</code>: posterior mean of <code>mu_train</code>.
</p>
</li>
<li> <p><code>mu_test_mean</code>: posterior mean of <code>mu_test</code>.
</p>
</li>
<li> <p><code>formula</code>: the formula specified by the user.
</p>
</li>
<li> <p><code>ecdfs</code>: empirical distribution functions, used by the <code>predict</code> function.
</p>
</li>
<li> <p><code>opts</code>: the options used when running the chain.
</p>
</li>
<li> <p><code>mu_Y, sd_Y</code>: used with the predict function to transform predictions.
</p>
</li>
<li> <p><code>forest</code>: a forest object; see the <code>MakeForest</code> documentation for more details.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## NOTE: SET NUMBER OF BURN IN AND SAMPLE ITERATIONS HIGHER IN PRACTICE

num_burn &lt;- 10 ## Should be ~ 5000
num_save &lt;- 10 ## Should be ~ 5000

set.seed(1234)
f_fried &lt;- function(x) 10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 + 
  10 * x[,4] + 5 * x[,5]

gen_data &lt;- function(n_train, n_test, P, sigma) {
  X &lt;- matrix(runif(n_train * P), nrow = n_train)
  mu &lt;- f_fried(X)
  X_test &lt;- matrix(runif(n_test * P), nrow = n_test)
  mu_test &lt;- f_fried(X_test)
  Y &lt;- mu + sigma * rnorm(n_train)
  Y_test &lt;- mu + sigma * rnorm(n_test)
  
  return(list(X = X, Y = Y, mu = mu, X_test = X_test, Y_test = Y_test, 
              mu_test = mu_test))
}

## Simiulate dataset
sim_data &lt;- gen_data(250, 250, 100, 1)

df &lt;- data.frame(X = sim_data$X, Y = sim_data$Y)
df_test &lt;- data.frame(X = sim_data$X_test, Y = sim_data$Y_test)

## Fit the model

opts &lt;- Opts(num_burn = num_burn, num_save = num_save)
fitted_reg &lt;- softbart_regression(Y ~ ., df, df_test, opts = opts)

## Plot results

plot(colMeans(fitted_reg$mu_test), sim_data$mu_test)
abline(a = 0, b = 1)
</code></pre>

<hr>
<h2 id='vc_softbart_regression'>SoftBart Varying Coefficient Regression</h2><span id='topic+vc_softbart_regression'></span>

<h3>Description</h3>

<p>Fits a semiparametric varying coefficient regression model with the
nonparametric slope and intercept </p>
<p style="text-align: center;"><code class="reqn">Y = \alpha(X) + Z \beta(X) +
\epsilon</code>
</p>
<p> using a soft BART model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vc_softbart_regression(
  formula,
  linear_var_name,
  data,
  test_data,
  num_tree = 20,
  k = 2,
  hypers_intercept = NULL,
  hypers_slope = NULL,
  opts = NULL,
  verbose = TRUE,
  warn = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vc_softbart_regression_+3A_formula">formula</code></td>
<td>
<p>A model formula with a numeric variable on the left-hand-side and non-linear predictors on the right-hand-side.</p>
</td></tr>
<tr><td><code id="vc_softbart_regression_+3A_linear_var_name">linear_var_name</code></td>
<td>
<p>A string containing the variable in the data that is to be treated linearly.</p>
</td></tr>
<tr><td><code id="vc_softbart_regression_+3A_data">data</code></td>
<td>
<p>A data frame consisting of the training data.</p>
</td></tr>
<tr><td><code id="vc_softbart_regression_+3A_test_data">test_data</code></td>
<td>
<p>A data frame consisting of the testing data.</p>
</td></tr>
<tr><td><code id="vc_softbart_regression_+3A_num_tree">num_tree</code></td>
<td>
<p>The number of trees in the ensemble to use.</p>
</td></tr>
<tr><td><code id="vc_softbart_regression_+3A_k">k</code></td>
<td>
<p>Determines the standard deviation of the leaf node parameters, which
is given by <code>3 / k / sqrt(num_tree)</code> (intercept) and defaults to
<code>1/k/sqrt(num_tree)</code> (slope). This can be modified for the slope by
specifying your own hyperparameter.</p>
</td></tr>
<tr><td><code id="vc_softbart_regression_+3A_hypers_intercept">hypers_intercept</code></td>
<td>
<p>A list of hyperparameters constructed from the <code>Hypers()</code> function (<code>num_tree</code>, <code>k</code>, and <code>sigma_mu</code> are overridden by this function).</p>
</td></tr>
<tr><td><code id="vc_softbart_regression_+3A_hypers_slope">hypers_slope</code></td>
<td>
<p>A list of hyperparameters constructed from the <code>Hypers()</code> function (<code>num_tree</code> is overridden by this function).</p>
</td></tr>
<tr><td><code id="vc_softbart_regression_+3A_opts">opts</code></td>
<td>
<p>A list of options for running the chain constructed from the <code>Opts()</code> function (<code>update_sigma</code> is overridden by this function).</p>
</td></tr>
<tr><td><code id="vc_softbart_regression_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, progress of the chain will be printed to the console.</p>
</td></tr>
<tr><td><code id="vc_softbart_regression_+3A_warn">warn</code></td>
<td>
<p>If <code>TRUE</code>, remind the user that they probably don't want the linear term to be included in the formula for the nonlinear part.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components
</p>

<ul>
<li> <p><code>sigma_mu_alpha</code>: samples of the standard deviation of the leaf node parameters for the intercept.
</p>
</li>
<li> <p><code>sigma_mu_beta</code>: samples of the standard deviation of the leaf node parameters for the slope.
</p>
</li>
<li> <p><code>sigma</code>: samples of the error standard deviation.
</p>
</li>
<li> <p><code>var_counts_alpha</code>: a matrix with a column for each predictor group containing the number of times each predictor is used in the ensemble at each iteration for the intercept.
</p>
</li>
<li> <p><code>var_counts_beta</code>: a matrix with a column for each predictor group containing the number of times each predictor is used in the ensemble at each iteration for the slope.
</p>
</li>
<li> <p><code>alpha_train</code>: samples of the nonparametric intercept evaluated on the training set.
</p>
</li>
<li> <p><code>alpha_test</code>: samples of the nonparametric intercept evaluated on the test set.
</p>
</li>
<li> <p><code>beta_train</code>: samples of the nonparametric slope evaluated on the training set.
</p>
</li>
<li> <p><code>beta_test</code>: samples of the nonparametric slope evaluated on the test set.
</p>
</li>
<li> <p><code>mu_train</code>: samples of the predictions evaluated on the training set.
</p>
</li>
<li> <p><code>mu_test</code>: samples of the predictions evaluated on the test set.
</p>
</li>
<li> <p><code>formula</code>: the formula specified by the user.
</p>
</li>
<li> <p><code>ecdfs</code>: empirical distribution functions, used by the <code>predict</code> function.
</p>
</li>
<li> <p><code>opts</code>: the options used when running the chain.
</p>
</li>
<li> <p><code>mu_Y, sd_Y</code>: used with the <code>predict</code> function to transform predictions.
</p>
</li>
<li> <p><code>alpha_forest</code>: a forest object for the intercept; see the <code>MakeForest</code> documentation for more details.
</p>
</li>
<li> <p><code>beta_forest</code>: a forest object for the slope; see the <code>MakeForest</code> documentation for more details.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## NOTE: SET NUMBER OF BURN IN AND SAMPLE ITERATIONS HIGHER IN PRACTICE

num_burn &lt;- 10 ## Should be ~ 5000
num_save &lt;- 10 ## Should be ~ 5000

set.seed(1234)
f_fried &lt;- function(x) 10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,3] - 0.5)^2 +
  10 * x[,4] + 5 * x[,5]

gen_data &lt;- function(n_train, n_test, P, sigma) {
  X &lt;- matrix(runif(n_train * P), nrow = n_train)
  Z &lt;- rnorm(n_train)
  r &lt;- f_fried(X)
  mu &lt;- Z * r
  X_test &lt;- matrix(runif(n_test * P), nrow = n_test)
  Z_test &lt;- rnorm(n_test)
  r_test &lt;- f_fried(X_test)
  mu_test &lt;- Z_test * r_test
  Y &lt;- mu + sigma * rnorm(n_train)
  Y_test &lt;- mu + sigma * rnorm(n_test)

  return(list(X = X, Y = Y, Z = Z, r = r, mu = mu, X_test = X_test, Y_test =
              Y_test, Z_test = Z_test, r_test = r_test, mu_test = mu_test))
}

## Simiulate dataset
sim_data &lt;- gen_data(250, 250, 100, 1)

df &lt;- data.frame(X = sim_data$X, Y = sim_data$Y, Z = sim_data$Z)
df_test &lt;- data.frame(X = sim_data$X_test, Y = sim_data$Y_test, Z = sim_data$Z_test)

## Fit the model

opts &lt;- Opts(num_burn = num_burn, num_save = num_save)
fitted_vc &lt;- vc_softbart_regression(Y ~ . -Z, "Z", df, df_test, opts = opts)

## Plot results

plot(colMeans(fitted_vc$mu_test), sim_data$mu_test)
abline(a = 0, b = 1)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
