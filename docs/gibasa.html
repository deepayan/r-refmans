<!DOCTYPE html><html><head><title>Help for package gibasa</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gibasa}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#gibasa-package'><p>gibasa: An Alternative 'Rcpp' Wrapper of 'MeCab'</p></a></li>
<li><a href='#as_tokens'><p>Create a list of tokens</p></a></li>
<li><a href='#bind_lr'><p>Bind importance of bigrams</p></a></li>
<li><a href='#bind_tf_idf2'><p>Bind term frequency and inverse document frequency</p></a></li>
<li><a href='#build_sys_dic'><p>Build system dictionary</p></a></li>
<li><a href='#build_user_dic'><p>Build user dictionary</p></a></li>
<li><a href='#collapse_tokens'><p>Collapse sequences of tokens by condition</p></a></li>
<li><a href='#dict_index_sys'><p>Build system dictionary</p></a></li>
<li><a href='#dict_index_user'><p>Build user dictionary</p></a></li>
<li><a href='#dictionary_info'><p>Get dictionary information</p></a></li>
<li><a href='#gbs_tokenize'><p>Tokenize sentences using 'MeCab'</p></a></li>
<li><a href='#get_dict_features'><p>Get dictionary features</p></a></li>
<li><a href='#get_transition_cost'><p>Get transition cost between pos attributes</p></a></li>
<li><a href='#ginga'><p>Whole text of 'Ginga Tetsudo no Yoru' written by Miyazawa Kenji</p>
from Aozora Bunko</a></li>
<li><a href='#is_blank'><p>Check if scalars are blank</p></a></li>
<li><a href='#lex_density'><p>Calculate lexical density</p></a></li>
<li><a href='#mute_tokens'><p>Mute tokens by condition</p></a></li>
<li><a href='#ngram_tokenizer'><p>Ngrams tokenizer</p></a></li>
<li><a href='#pack'><p>Pack a data.frame of tokens</p></a></li>
<li><a href='#posDebugRcpp'><p>Tokenizer for debug use</p></a></li>
<li><a href='#posParallelRcpp'><p>Call tagger inside 'RcppParallel::parallelFor' and return a data.frame.</p></a></li>
<li><a href='#prettify'><p>Prettify tokenized output</p></a></li>
<li><a href='#tokenize'><p>Tokenize sentences using 'MeCab'</p></a></li>
<li><a href='#transition_cost'><p>Get transition cost between pos attributes</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>An Alternative 'Rcpp' Wrapper of 'MeCab'</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Akiru Kato &lt;paithiov909@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A plain 'Rcpp' wrapper of 'MeCab' that can segment Chinese,
    Japanese, and Korean text into tokens. The main goal of this package
    is to provide an alternative to 'tidytext' using morphological
    analysis.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://paithiov909.github.io/gibasa/">https://paithiov909.github.io/gibasa/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/paithiov909/gibasa/issues">https://github.com/paithiov909/gibasa/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, Matrix, purrr, Rcpp, RcppParallel, readr, rlang (&ge;
0.4.11), stringi, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>roxygen2, testthat (&ge; 3.0.0), withr</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppParallel</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>ggdendro, ggpubr, quanteda, quanteda.textmodels,
quanteda.textplots, quanteda.textstats, reactable, stringr,
textmineR, textrecipes, tidymodels, topicmodels, uwot, xgboost,
paithiov909/ldccr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make, MeCab</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-17 03:58:10 UTC; paithiov909</td>
</tr>
<tr>
<td>Author:</td>
<td>Akiru Kato [aut, cre],
  Shogo Ichinose [aut],
  Taku Kudo [aut],
  Jorge Nocedal [ctb],
  Nippon Telegraph and Telephone Corporation [cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-17 04:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='gibasa-package'>gibasa: An Alternative 'Rcpp' Wrapper of 'MeCab'</h2><span id='topic+gibasa'></span><span id='topic+gibasa-package'></span>

<h3>Description</h3>

<p>A plain 'Rcpp' wrapper of 'MeCab' that can segment Chinese, Japanese, and Korean text into tokens. The main goal of this package is to provide an alternative to 'tidytext' using morphological analysis.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Akiru Kato <a href="mailto:paithiov909@gmail.com">paithiov909@gmail.com</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Shogo Ichinose
</p>
</li>
<li><p> Taku Kudo
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Jorge Nocedal [contributor]
</p>
</li>
<li><p> Nippon Telegraph and Telephone Corporation [copyright holder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://paithiov909.github.io/gibasa/">https://paithiov909.github.io/gibasa/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/paithiov909/gibasa/issues">https://github.com/paithiov909/gibasa/issues</a>
</p>
</li></ul>


<hr>
<h2 id='as_tokens'>Create a list of tokens</h2><span id='topic+as_tokens'></span>

<h3>Description</h3>

<p>Create a list of tokens
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_tokens(
  tbl,
  token_field = "token",
  pos_field = get_dict_features()[1],
  nm = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_tokens_+3A_tbl">tbl</code></td>
<td>
<p>A tibble of tokens out of <code>tokenize()</code>.</p>
</td></tr>
<tr><td><code id="as_tokens_+3A_token_field">token_field</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
Column containing tokens.</p>
</td></tr>
<tr><td><code id="as_tokens_+3A_pos_field">pos_field</code></td>
<td>
<p>Column containing features
that will be kept as the names of tokens.
of tokens. If you don't need them, give a <code>NULL</code> for this argument.</p>
</td></tr>
<tr><td><code id="as_tokens_+3A_nm">nm</code></td>
<td>
<p>Names of returned list.
If left with <code>NULL</code>, &quot;doc_id&quot; field of <code>tbl</code> is used instead.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of tokens.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
tokenize(
  data.frame(
    doc_id = seq_along(ginga[5:8]),
    text = ginga[5:8]
  )
) |&gt;
  prettify(col_select = "POS1") |&gt;
  as_tokens()

## End(Not run)
</code></pre>

<hr>
<h2 id='bind_lr'>Bind importance of bigrams</h2><span id='topic+bind_lr'></span>

<h3>Description</h3>

<p>Calculates and binds the importance of bigrams and their synergistic average.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bind_lr(tbl, term = "token", lr_mode = c("n", "dn"), avg_rate = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bind_lr_+3A_tbl">tbl</code></td>
<td>
<p>A tidy text dataset.</p>
</td></tr>
<tr><td><code id="bind_lr_+3A_term">term</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
Column containing terms.</p>
</td></tr>
<tr><td><code id="bind_lr_+3A_lr_mode">lr_mode</code></td>
<td>
<p>Method for computing 'FL' and 'FR' values.
<code>n</code> is equivalent to 'LN' and 'RN', and <code>dn</code> is equivalent to 'LDN' and 'RDN'.</p>
</td></tr>
<tr><td><code id="bind_lr_+3A_avg_rate">avg_rate</code></td>
<td>
<p>Weight of the 'LR' value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The 'LR' value is the synergistic average of bigram importance
that based on the words and their positions (left or right side).
</p>


<h3>Value</h3>

<p>A data.frame.
</p>


<h3>See Also</h3>

<p><a href="https://doi.org/10.5715/jnlp.10.27">doi:10.5715/jnlp.10.27</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- tokenize(
  data.frame(
    doc_id = seq_along(ginga[5:8]),
    text = ginga[5:8]
  )
)
bind_lr(df)

## End(Not run)
</code></pre>

<hr>
<h2 id='bind_tf_idf2'>Bind term frequency and inverse document frequency</h2><span id='topic+bind_tf_idf2'></span>

<h3>Description</h3>

<p>Calculates and binds the term frequency, inverse document frequency,
and TF-IDF of the dataset.
This function experimentally supports 4 types of term frequencies
and 5 types of inverse document frequencies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bind_tf_idf2(
  tbl,
  term = "token",
  document = "doc_id",
  n = "n",
  tf = c("tf", "tf2", "tf3", "itf"),
  idf = c("idf", "idf2", "idf3", "idf4", "df"),
  norm = FALSE,
  rmecab_compat = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bind_tf_idf2_+3A_tbl">tbl</code></td>
<td>
<p>A tidy text dataset.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_term">term</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
Column containing terms.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_document">document</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
Column containing document IDs.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_n">n</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
Column containing document-term counts.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_tf">tf</code></td>
<td>
<p>Method for computing term frequency.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_idf">idf</code></td>
<td>
<p>Method for computing inverse document frequency.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_norm">norm</code></td>
<td>
<p>Logical; If passed as <code>TRUE</code>, TF-IDF values are normalized
being divided with L2 norms.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_rmecab_compat">rmecab_compat</code></td>
<td>
<p>Logical; If passed as <code>TRUE</code>, computes values while
taking care of compatibility with 'RMeCab'.
Note that 'RMeCab' always computes IDF values using term frequency
rather than raw term counts, and thus TF-IDF values may be
doubly affected by term frequency.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Types of term frequency can be switched with <code>tf</code> argument:
</p>

<ul>
<li> <p><code>tf</code> is term frequency (not raw count of terms).
</p>
</li>
<li> <p><code>tf2</code> is logarithmic term frequency of which base is <code>exp(1)</code>.
</p>
</li>
<li> <p><code>tf3</code> is binary-weighted term frequency.
</p>
</li>
<li> <p><code>itf</code> is inverse term frequency. Use with <code>idf="df"</code>.
</p>
</li></ul>

<p>Types of inverse document frequencies can be switched with <code>idf</code> argument:
</p>

<ul>
<li> <p><code>idf</code> is inverse document frequency of which base is 2, with smoothed.
'smoothed' here means just adding 1 to raw values after logarithmizing.
</p>
</li>
<li> <p><code>idf2</code> is global frequency IDF.
</p>
</li>
<li> <p><code>idf3</code> is probabilistic IDF of which base is 2.
</p>
</li>
<li> <p><code>idf4</code> is global entropy, not IDF in actual.
</p>
</li>
<li> <p><code>df</code> is document frequency. Use with <code>tf="itf"</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>A data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- tokenize(
  data.frame(
    doc_id = seq_along(ginga[5:8]),
    text = ginga[5:8]
  )
) |&gt;
  dplyr::group_by(doc_id) |&gt;
  dplyr::count(token) |&gt;
  dplyr::ungroup()
bind_tf_idf2(df)

## End(Not run)
</code></pre>

<hr>
<h2 id='build_sys_dic'>Build system dictionary</h2><span id='topic+build_sys_dic'></span>

<h3>Description</h3>

<p>Builds a UTF-8 system dictionary from source dictionary files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_sys_dic(dic_dir, out_dir, encoding)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_sys_dic_+3A_dic_dir">dic_dir</code></td>
<td>
<p>Directory where the source dictionaries are located.
This argument is passed as '-d' option argument.</p>
</td></tr>
<tr><td><code id="build_sys_dic_+3A_out_dir">out_dir</code></td>
<td>
<p>Directory where the binary dictionary will be written.
This argument is passed as '-o' option argument.</p>
</td></tr>
<tr><td><code id="build_sys_dic_+3A_encoding">encoding</code></td>
<td>
<p>Encoding of input csv files.
This argument is passed as '-f' option argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a wrapper around dictionary compiler of 'MeCab'.
</p>
<p>Running this function will create 4 files:
'char.bin', 'matrix.bin', 'sys.dic', and 'unk.dic' in <code>out_dir</code>.
</p>
<p>To use these compiled dictionary,
you also need create a <code>dicrc</code> file in <code>out_dir</code>.
A <code>dicrc</code> file is included in source dictionaries,
so you can just copy it to <code>out_dir</code>.
</p>


<h3>Value</h3>

<p>A <code>TRUE</code> is invisibly returned if dictionary is successfully built.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (requireNamespace("withr")) {
  # create a sample dictionary in temporary directory
  build_sys_dic(
    dic_dir = system.file("latin", package = "gibasa"),
    out_dir = tempdir(),
    encoding = "utf8"
  )
  # copy the 'dicrc' file
  file.copy(
    system.file("latin/dicrc", package = "gibasa"),
    tempdir()
  )
  # mocking a 'mecabrc' file to temporarily use the dictionary
  withr::with_envvar(
    c(
      "MECABRC" = if (.Platform$OS.type == "windows") {
        "nul"
      } else {
        "/dev/null"
      },
      "RCPP_PARALLEL_BACKEND" = "tinythread"
    ),
    {
      tokenize("katta-wokattauresikatta", sys_dic = tempdir())
    }
  )
}

</code></pre>

<hr>
<h2 id='build_user_dic'>Build user dictionary</h2><span id='topic+build_user_dic'></span>

<h3>Description</h3>

<p>Builds a UTF-8 user dictionary from a csv file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_user_dic(dic_dir, file, csv_file, encoding)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_user_dic_+3A_dic_dir">dic_dir</code></td>
<td>
<p>Directory where the source dictionaries are located.
This argument is passed as '-d' option argument.</p>
</td></tr>
<tr><td><code id="build_user_dic_+3A_file">file</code></td>
<td>
<p>Path to write the user dictionary.
This argument is passed as '-u' option argument.</p>
</td></tr>
<tr><td><code id="build_user_dic_+3A_csv_file">csv_file</code></td>
<td>
<p>Path to an input csv file.</p>
</td></tr>
<tr><td><code id="build_user_dic_+3A_encoding">encoding</code></td>
<td>
<p>Encoding of input csv files.
This argument is passed as '-f' option argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a wrapper around dictionary compiler of 'MeCab'.
</p>
<p>Note that this function does not support auto assignment of word cost field.
So, you can't leave any word costs as empty in your input csv file.
To estimate word costs, use <code>posDebugRcpp()</code> function.
</p>


<h3>Value</h3>

<p>A <code>TRUE</code> is invisibly returned if dictionary is successfully built.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (requireNamespace("withr")) {
  # create a sample dictionary in temporary directory
  build_sys_dic(
    dic_dir = system.file("latin", package = "gibasa"),
    out_dir = tempdir(),
    encoding = "utf8"
  )
  # copy the 'dicrc' file
  file.copy(
    system.file("latin/dicrc", package = "gibasa"),
    tempdir()
  )
  # write a csv file and compile it into a user dictionary
  csv_file &lt;- tempfile(fileext = ".csv")
  writeLines(
    c(
      "qa, 0, 0, 5, \u304f\u3041",
      "qi, 0, 0, 5, \u304f\u3043",
      "qu, 0, 0, 5, \u304f",
      "qe, 0, 0, 5, \u304f\u3047",
      "qo, 0, 0, 5, \u304f\u3049"
    ),
    csv_file
  )
  build_user_dic(
    dic_dir = tempdir(),
    file = (user_dic &lt;- tempfile(fileext = ".dic")),
    csv_file = csv_file,
    encoding = "utf8"
  )
  # mocking a 'mecabrc' file to temporarily use the dictionary
  withr::with_envvar(
    c(
      "MECABRC" = if (.Platform$OS.type == "windows") {
        "nul"
      } else {
        "/dev/null"
      },
      "RCPP_PARALLEL_BACKEND" = "tinythread"
    ),
    {
      tokenize("quensan", sys_dic = tempdir(), user_dic = user_dic)
    }
  )
}

</code></pre>

<hr>
<h2 id='collapse_tokens'>Collapse sequences of tokens by condition</h2><span id='topic+collapse_tokens'></span>

<h3>Description</h3>

<p>Concatenates sequences of tokens in the tidy text dataset,
while grouping them by an expression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>collapse_tokens(tbl, condition, .collapse = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collapse_tokens_+3A_tbl">tbl</code></td>
<td>
<p>A tidy text dataset.</p>
</td></tr>
<tr><td><code id="collapse_tokens_+3A_condition">condition</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
A logical expression.</p>
</td></tr>
<tr><td><code id="collapse_tokens_+3A_.collapse">.collapse</code></td>
<td>
<p>String with which tokens are concatenated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that this function drops all columns except but 'token'
and columns for grouping sequences. So, the returned data.frame
has only 'doc_id', 'sentence_id', 'token_id', and 'token' columns.
</p>


<h3>Value</h3>

<p>A data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- tokenize(
  data.frame(
    doc_id = "odakyu-sen",
    text = "\u5c0f\u7530\u6025\u7dda"
  )
) |&gt;
  prettify(col_select = "POS1")

head(collapse_tokens(
  df,
  POS1 == "\u540d\u8a5e" &amp; stringr::str_detect(token, "^[\\p{Han}]+$")
))

## End(Not run)
</code></pre>

<hr>
<h2 id='dict_index_sys'>Build system dictionary</h2><span id='topic+dict_index_sys'></span>

<h3>Description</h3>

<p>Build system dictionary
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="dict_index_sys_+3A_dic_dir">dic_dir</code></td>
<td>
<p>Directory where the source dictionaries are located.
This argument is passed as '-d' option argument.</p>
</td></tr>
<tr><td><code id="dict_index_sys_+3A_out_dir">out_dir</code></td>
<td>
<p>Directory where the binary dictionary will be written.
This argument is passed as '-o' option argument.</p>
</td></tr>
<tr><td><code id="dict_index_sys_+3A_encoding">encoding</code></td>
<td>
<p>Encoding of input csv files.
This argument is passed as '-f' option argument.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical.
</p>

<hr>
<h2 id='dict_index_user'>Build user dictionary</h2><span id='topic+dict_index_user'></span>

<h3>Description</h3>

<p>Build user dictionary
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="dict_index_user_+3A_dic_dir">dic_dir</code></td>
<td>
<p>Directory where the source dictionaries are located.
This argument is passed as '-d' option argument.</p>
</td></tr>
<tr><td><code id="dict_index_user_+3A_file">file</code></td>
<td>
<p>Path to write the user dictionary.
This argument is passed as '-u' option argument.</p>
</td></tr>
<tr><td><code id="dict_index_user_+3A_csv_file">csv_file</code></td>
<td>
<p>Path to an input csv file.</p>
</td></tr>
<tr><td><code id="dict_index_user_+3A_encoding">encoding</code></td>
<td>
<p>Encoding of input csv files.
This argument is passed as '-f' option argument.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical.
</p>

<hr>
<h2 id='dictionary_info'>Get dictionary information</h2><span id='topic+dictionary_info'></span>

<h3>Description</h3>

<p>Get dictionary information
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="dictionary_info_+3A_sys_dic">sys_dic</code></td>
<td>
<p>String scalar.</p>
</td></tr>
<tr><td><code id="dictionary_info_+3A_user_dic">user_dic</code></td>
<td>
<p>String scalar.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame.
</p>

<hr>
<h2 id='gbs_tokenize'>Tokenize sentences using 'MeCab'</h2><span id='topic+gbs_tokenize'></span>

<h3>Description</h3>

<p>Tokenize sentences using 'MeCab'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbs_tokenize(
  x,
  sys_dic = "",
  user_dic = "",
  split = FALSE,
  partial = FALSE,
  mode = c("parse", "wakati")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbs_tokenize_+3A_x">x</code></td>
<td>
<p>A data.frame like object or a character vector to be tokenized.</p>
</td></tr>
<tr><td><code id="gbs_tokenize_+3A_sys_dic">sys_dic</code></td>
<td>
<p>Character scalar; path to the system dictionary for 'MeCab'.
Note that the system dictionary is expected to be compiled with UTF-8,
not Shift-JIS or other encodings.</p>
</td></tr>
<tr><td><code id="gbs_tokenize_+3A_user_dic">user_dic</code></td>
<td>
<p>Character scalar; path to the user dictionary for 'MeCab'.</p>
</td></tr>
<tr><td><code id="gbs_tokenize_+3A_split">split</code></td>
<td>
<p>Logical. When passed as <code>TRUE</code>, the function internally splits the sentences
into sub-sentences using <code>stringi::stri_split_boundaries(type = "sentence")</code>.</p>
</td></tr>
<tr><td><code id="gbs_tokenize_+3A_partial">partial</code></td>
<td>
<p>Logical. When passed as <code>TRUE</code>, activates partial parsing mode.
To activate this feature, remember that all spaces at the start and end of
the input chunks are already squashed. In particular, trailing spaces
of chunks sometimes cause fatal errors.</p>
</td></tr>
<tr><td><code id="gbs_tokenize_+3A_mode">mode</code></td>
<td>
<p>Character scalar to switch output format.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble or a named list of tokens.
</p>

<hr>
<h2 id='get_dict_features'>Get dictionary features</h2><span id='topic+get_dict_features'></span>

<h3>Description</h3>

<p>Returns names of dictionary features.
Currently supports &quot;unidic17&quot; (2.1.2 src schema), &quot;unidic26&quot; (2.1.2 bin schema),
&quot;unidic29&quot; (schema used in 2.2.0, 2.3.0), &quot;cc-cedict&quot;, &quot;ko-dic&quot; (mecab-ko-dic),
&quot;naist11&quot;, &quot;sudachi&quot;, and &quot;ipa&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_dict_features(
  dict = c("ipa", "unidic17", "unidic26", "unidic29", "cc-cedict", "ko-dic", "naist11",
    "sudachi")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_dict_features_+3A_dict">dict</code></td>
<td>
<p>Character scalar; one of &quot;ipa&quot;, &quot;unidic17&quot;, &quot;unidic26&quot;, &quot;unidic29&quot;,
&quot;cc-cedict&quot;, &quot;ko-dic&quot;, &quot;naist11&quot;, or &quot;sudachi&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>See Also</h3>

<p>See also
<a href="https://github.com/ueda-keisuke/CC-CEDICT-MeCab">'CC-CEDICT-MeCab'</a>
and <a href="https://bitbucket.org/eunjeon/mecab-ko-dic/src/master/">'mecab-ko-dic'</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_dict_features("ipa")
</code></pre>

<hr>
<h2 id='get_transition_cost'>Get transition cost between pos attributes</h2><span id='topic+get_transition_cost'></span>

<h3>Description</h3>

<p>Get transition cost between pos attributes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_transition_cost(rcAttr, lcAttr, sys_dic = "", user_dic = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_transition_cost_+3A_rcattr">rcAttr</code></td>
<td>
<p>Integer.</p>
</td></tr>
<tr><td><code id="get_transition_cost_+3A_lcattr">lcAttr</code></td>
<td>
<p>Integer.</p>
</td></tr>
<tr><td><code id="get_transition_cost_+3A_sys_dic">sys_dic</code></td>
<td>
<p>String.</p>
</td></tr>
<tr><td><code id="get_transition_cost_+3A_user_dic">user_dic</code></td>
<td>
<p>String.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric.
</p>

<hr>
<h2 id='ginga'>Whole text of 'Ginga Tetsudo no Yoru' written by Miyazawa Kenji
from Aozora Bunko</h2><span id='topic+ginga'></span>

<h3>Description</h3>

<p>Whole text of 'Ginga Tetsudo no Yoru' written by Miyazawa Kenji
from Aozora Bunko
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ginga
</code></pre>


<h3>Format</h3>

<p>An object of class <code>character</code> of length 553.
</p>


<h3>Details</h3>

<p>A dataset containing the text of Miyazawa Kenji's novel
&quot;Ginga Tetsudo no Yoru&quot; (English title: &quot;Night on the Galactic Railroad&quot;)
which was published in 1934, the year after Kenji's death.
Copyright of this work has expired since more than 70 years have passed after the author's death.
</p>
<p>The UTF-8 plain text is sourced from <a href="https://www.aozora.gr.jp/cards/000081/card43737.html">https://www.aozora.gr.jp/cards/000081/card43737.html</a> and is cleaned of meta data.
</p>


<h3>Source</h3>

<p><a href="https://www.aozora.gr.jp/cards/000081/files/43737_ruby_19028.zip">https://www.aozora.gr.jp/cards/000081/files/43737_ruby_19028.zip</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>head(ginga)
</code></pre>

<hr>
<h2 id='is_blank'>Check if scalars are blank</h2><span id='topic+is_blank'></span>

<h3>Description</h3>

<p>Check if scalars are blank
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_blank(x, trim = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_blank_+3A_x">x</code></td>
<td>
<p>Object to check its emptiness.</p>
</td></tr>
<tr><td><code id="is_blank_+3A_trim">trim</code></td>
<td>
<p>Logical.</p>
</td></tr>
<tr><td><code id="is_blank_+3A_...">...</code></td>
<td>
<p>Additional arguments for <code>base::sapply()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logicals.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>is_blank(list(c(a = "", b = NA_character_), NULL))
</code></pre>

<hr>
<h2 id='lex_density'>Calculate lexical density</h2><span id='topic+lex_density'></span>

<h3>Description</h3>

<p>The lexical density is the proportion of content words (lexical items)
in documents. This function is a simple helper for calculating
the lexical density of given datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lex_density(vec, contents_words, targets = NULL, negate = c(FALSE, FALSE))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lex_density_+3A_vec">vec</code></td>
<td>
<p>A character vector.</p>
</td></tr>
<tr><td><code id="lex_density_+3A_contents_words">contents_words</code></td>
<td>
<p>A character vector containing values
to be counted as contents words.</p>
</td></tr>
<tr><td><code id="lex_density_+3A_targets">targets</code></td>
<td>
<p>A character vector with which
the denominator of lexical density is filtered before computing values.</p>
</td></tr>
<tr><td><code id="lex_density_+3A_negate">negate</code></td>
<td>
<p>A logical vector of which length is 2.
If passed as <code>TRUE</code>, then respectively negates the predicate functions
for counting contents words or targets.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- tokenize(
  data.frame(
    doc_id = seq_along(ginga[5:8]),
    text = ginga[5:8]
  )
)
df |&gt;
  prettify(col_select = "POS1") |&gt;
  dplyr::group_by(doc_id) |&gt;
  dplyr::summarise(
    noun_ratio = lex_density(POS1,
      "\u540d\u8a5e",
      c("\u52a9\u8a5e", "\u52a9\u52d5\u8a5e"),
      negate = c(FALSE, TRUE)
    ),
    mvr = lex_density(
      POS1,
      c("\u5f62\u5bb9\u8a5e", "\u526f\u8a5e", "\u9023\u4f53\u8a5e"),
      "\u52d5\u8a5e"
    ),
    vnr = lex_density(POS1, "\u52d5\u8a5e", "\u540d\u8a5e")
  )

## End(Not run)
</code></pre>

<hr>
<h2 id='mute_tokens'>Mute tokens by condition</h2><span id='topic+mute_tokens'></span>

<h3>Description</h3>

<p>Replaces tokens in the tidy text dataset with a string scalar
only if they are matched to an expression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mute_tokens(tbl, condition, .as = NA_character_)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mute_tokens_+3A_tbl">tbl</code></td>
<td>
<p>A tidy text dataset.</p>
</td></tr>
<tr><td><code id="mute_tokens_+3A_condition">condition</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
A logical expression.</p>
</td></tr>
<tr><td><code id="mute_tokens_+3A_.as">.as</code></td>
<td>
<p>String with which tokens are replaced
when they are matched to condition.
The default value is <code>NA_character</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- tokenize(
  data.frame(
    doc_id = seq_along(ginga[5:8]),
    text = ginga[5:8]
  )
) |&gt;
  prettify(col_select = "POS1")

head(mute_tokens(df, POS1 %in% c("\u52a9\u8a5e", "\u52a9\u52d5\u8a5e")))

## End(Not run)
</code></pre>

<hr>
<h2 id='ngram_tokenizer'>Ngrams tokenizer</h2><span id='topic+ngram_tokenizer'></span>

<h3>Description</h3>

<p>Makes an ngram tokenizer function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ngram_tokenizer(n = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ngram_tokenizer_+3A_n">n</code></td>
<td>
<p>Integer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ngram tokenizer function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>bigram &lt;- ngram_tokenizer(2)
bigram(letters, sep = "-")
</code></pre>

<hr>
<h2 id='pack'>Pack a data.frame of tokens</h2><span id='topic+pack'></span>

<h3>Description</h3>

<p>Packs a data.frame of tokens into a new data.frame of corpus,
which is compatible with the Text Interchange Formats.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pack(tbl, pull = "token", n = 1L, sep = "-", .collapse = " ")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pack_+3A_tbl">tbl</code></td>
<td>
<p>A data.frame of tokens.</p>
</td></tr>
<tr><td><code id="pack_+3A_pull">pull</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
Column to be packed into text or ngrams body. Default value is <code>token</code>.</p>
</td></tr>
<tr><td><code id="pack_+3A_n">n</code></td>
<td>
<p>Integer internally passed to ngrams tokenizer function
created of <code>gibasa::ngram_tokenizer()</code></p>
</td></tr>
<tr><td><code id="pack_+3A_sep">sep</code></td>
<td>
<p>Character scalar internally used as the concatenator of ngrams.</p>
</td></tr>
<tr><td><code id="pack_+3A_.collapse">.collapse</code></td>
<td>
<p>This argument is passed to <code>stringi::stri_c()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble.
</p>


<h3>Text Interchange Formats (TIF)</h3>

<p>The Text Interchange Formats (TIF) is a set of standards
that allows R text analysis packages to target defined inputs and outputs
for corpora, tokens, and document-term matrices.
</p>


<h3>Valid data.frame of tokens</h3>

<p>The data.frame of tokens here is a data.frame object
compatible with the TIF.
</p>
<p>A TIF valid data.frame of tokens are expected to have one unique key column (named <code>doc_id</code>)
of each text and several feature columns of each tokens.
The feature columns must contain at least <code>token</code> itself.
</p>


<h3>See Also</h3>

<p><a href="https://github.com/ropenscilabs/tif">https://github.com/ropenscilabs/tif</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- tokenize(
  data.frame(
    doc_id = seq_along(ginga[5:8]),
    text = ginga[5:8]
  )
)
pack(df)

## End(Not run)
</code></pre>

<hr>
<h2 id='posDebugRcpp'>Tokenizer for debug use</h2><span id='topic+posDebugRcpp'></span>

<h3>Description</h3>

<p>Tokenizes a character vector
and returns all possible results out of the tokenization process.
The returned data.frame contains additional attributes for debug usage.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="posDebugRcpp_+3A_text">text</code></td>
<td>
<p>String.</p>
</td></tr>
<tr><td><code id="posDebugRcpp_+3A_sys_dic">sys_dic</code></td>
<td>
<p>String.</p>
</td></tr>
<tr><td><code id="posDebugRcpp_+3A_user_dic">user_dic</code></td>
<td>
<p>String.</p>
</td></tr>
<tr><td><code id="posDebugRcpp_+3A_partial">partial</code></td>
<td>
<p>Logical.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame.
</p>

<hr>
<h2 id='posParallelRcpp'>Call tagger inside 'RcppParallel::parallelFor' and return a data.frame.</h2><span id='topic+posParallelRcpp'></span>

<h3>Description</h3>

<p>Call tagger inside 'RcppParallel::parallelFor' and return a data.frame.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="posParallelRcpp_+3A_text">text</code></td>
<td>
<p>Character vector.</p>
</td></tr>
<tr><td><code id="posParallelRcpp_+3A_sys_dic">sys_dic</code></td>
<td>
<p>String scalar.</p>
</td></tr>
<tr><td><code id="posParallelRcpp_+3A_user_dic">user_dic</code></td>
<td>
<p>String scalar.</p>
</td></tr>
<tr><td><code id="posParallelRcpp_+3A_partial">partial</code></td>
<td>
<p>Logical.</p>
</td></tr>
<tr><td><code id="posParallelRcpp_+3A_grain_size">grain_size</code></td>
<td>
<p>Integer (larger than 1).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame.
</p>

<hr>
<h2 id='prettify'>Prettify tokenized output</h2><span id='topic+prettify'></span>

<h3>Description</h3>

<p>Turns a single character column into features
while separating with delimiter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prettify(
  tbl,
  col = "feature",
  into = get_dict_features("ipa"),
  col_select = seq_along(into),
  delim = ","
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prettify_+3A_tbl">tbl</code></td>
<td>
<p>A data.frame that has feature column to be prettified.</p>
</td></tr>
<tr><td><code id="prettify_+3A_col">col</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
Column containing features to be prettified.</p>
</td></tr>
<tr><td><code id="prettify_+3A_into">into</code></td>
<td>
<p>Character vector that is used as column names of
features.</p>
</td></tr>
<tr><td><code id="prettify_+3A_col_select">col_select</code></td>
<td>
<p>Character or integer vector that will be kept
in prettified features.</p>
</td></tr>
<tr><td><code id="prettify_+3A_delim">delim</code></td>
<td>
<p>Character scalar used to separate fields within a feature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prettify(
  data.frame(x = c("x,y", "y,z", "z,x")),
  col = "x",
  into = c("a", "b"),
  col_select = "b"
)

## Not run: 
df &lt;- tokenize(
  data.frame(
    doc_id = seq_along(ginga[5:8]),
    text = ginga[5:8]
  )
)
prettify(df, col_select = 1:3)
prettify(df, col_select = c(1, 3, 6))
prettify(df, col_select = c("POS1", "Original"))

## End(Not run)
</code></pre>

<hr>
<h2 id='tokenize'>Tokenize sentences using 'MeCab'</h2><span id='topic+tokenize'></span>

<h3>Description</h3>

<p>Tokenize sentences using 'MeCab'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize(
  x,
  text_field = "text",
  docid_field = "doc_id",
  sys_dic = "",
  user_dic = "",
  split = FALSE,
  partial = FALSE,
  grain_size = 1L,
  mode = c("parse", "wakati")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_+3A_x">x</code></td>
<td>
<p>A data.frame like object or a character vector to be tokenized.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_text_field">text_field</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
String or symbol; column containing texts to be tokenized.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_docid_field">docid_field</code></td>
<td>
<p>&lt;<code><a href="rlang.html#topic+args_data_masking">data-masked</a></code>&gt;
String or symbol; column containing document IDs.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_sys_dic">sys_dic</code></td>
<td>
<p>Character scalar; path to the system dictionary for 'MeCab'.
Note that the system dictionary is expected to be compiled with UTF-8,
not Shift-JIS or other encodings.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_user_dic">user_dic</code></td>
<td>
<p>Character scalar; path to the user dictionary for 'MeCab'.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_split">split</code></td>
<td>
<p>Logical. When passed as <code>TRUE</code>, the function internally splits the sentences
into sub-sentences using <code>stringi::stri_split_boundaries(type = "sentence")</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_partial">partial</code></td>
<td>
<p>Logical. When passed as <code>TRUE</code>, activates partial parsing mode.
To activate this feature, remember that all spaces at the start and end of
the input chunks are already squashed. In particular, trailing spaces
of chunks sometimes cause fatal errors.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_grain_size">grain_size</code></td>
<td>
<p>Integer value larger than 1.
This argument is internally passed to <code>RcppParallel::parallelFor</code> function.
Setting a larger chunk size could improve the performance in some cases.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_mode">mode</code></td>
<td>
<p>Character scalar to switch output format.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble or a named list of tokens.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- tokenize(
  data.frame(
    doc_id = seq_along(ginga[5:8]),
    text = ginga[5:8]
  )
)
head(df)

## End(Not run)
</code></pre>

<hr>
<h2 id='transition_cost'>Get transition cost between pos attributes</h2><span id='topic+transition_cost'></span>

<h3>Description</h3>

<p>Get transition cost between pos attributes
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="transition_cost_+3A_rcattr">rcAttr</code></td>
<td>
<p>Integer.</p>
</td></tr>
<tr><td><code id="transition_cost_+3A_lcattr">lcAttr</code></td>
<td>
<p>Integer.</p>
</td></tr>
<tr><td><code id="transition_cost_+3A_sys_dic">sys_dic</code></td>
<td>
<p>String.</p>
</td></tr>
<tr><td><code id="transition_cost_+3A_user_dic">user_dic</code></td>
<td>
<p>String.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
