<!DOCTYPE html><html lang="en"><head><title>Help for package rbooster</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rbooster}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#rbooster-package'><p>rbooster: AdaBoost Framework for Any Classifier</p></a></li>
<li><a href='#booster'><p>AdaBoost Framework for Any Classifier</p></a></li>
<li><a href='#classifier_rpart'><p>Functions to be used internally</p></a></li>
<li><a href='#discretize'><p>Discretize</p></a></li>
<li><a href='#plot.booster'><p>plot booster</p></a></li>
<li><a href='#predict.booster'><p>Prediction function for Adaboost framework</p></a></li>
<li><a href='#predict.w_naive_bayes'><p>Predict Discrete Naive Bayes</p></a></li>
<li><a href='#print.booster'><p>Print booster</p></a></li>
<li><a href='#print.w_naive_bayes'><p>Print w_naive_bayes</p></a></li>
<li><a href='#w_naive_bayes'><p>Naive Bayes algorithm with case weights</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>AdaBoost Framework for Any Classifier</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>This is a simple package which provides a function
      that boosts pre-ready or custom-made classifiers. Package
      uses Discrete AdaBoost (&lt;<a href="https://doi.org/10.1006%2Fjcss.1997.1504">doi:10.1006/jcss.1997.1504</a>&gt;) and Real AdaBoost
      (&lt;<a href="https://doi.org/10.1214%2Faos%2F1016218223">doi:10.1214/aos/1016218223</a>&gt;) for two class,
      SAMME (&lt;<a href="https://doi.org/10.4310%2FSII.2009.v2.n3.a8">doi:10.4310/SII.2009.v2.n3.a8</a>&gt;) and
      SAMME.R (&lt;<a href="https://doi.org/10.4310%2FSII.2009.v2.n3.a8">doi:10.4310/SII.2009.v2.n3.a8</a>&gt;)
      for multiclass classification. </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&gt; 4.0.4)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, rpart, earth, Hmisc</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, imbalance, rmarkdown, mlbench</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>false</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-10-27 10:07:15 UTC; Fatih</td>
</tr>
<tr>
<td>Author:</td>
<td>Fatih Saglam <a href="https://orcid.org/0000-0002-2084-2008"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Hasan Bulut [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Fatih Saglam &lt;fatih.saglam@omu.edu.tr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-10-27 12:00:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='rbooster-package'>rbooster: AdaBoost Framework for Any Classifier</h2><span id='topic+rbooster'></span><span id='topic+rbooster-package'></span>

<h3>Description</h3>

<p>This is a simple package which provides a function
that boosts pre-ready or custom-made classifiers. Package
uses Discrete AdaBoost (&lt;doi:10.1006/jcss.1997.1504&gt;) and Real AdaBoost
(&lt;doi:10.1214/aos/1016218223&gt;) for two class,
SAMME (&lt;doi:10.4310/SII.2009.v2.n3.a8&gt;) and
SAMME.R (&lt;doi:10.4310/SII.2009.v2.n3.a8&gt;)
for multiclass classification.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Fatih Saglam <a href="mailto:fatih.saglam@omu.edu.tr">fatih.saglam@omu.edu.tr</a> (<a href="https://orcid.org/0000-0002-2084-2008">ORCID</a>)
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Hasan Bulut <a href="mailto:hasan.bulut@omu.edu.tr">hasan.bulut@omu.edu.tr</a> [contributor]
</p>
</li></ul>


<hr>
<h2 id='booster'>AdaBoost Framework for Any Classifier</h2><span id='topic+booster'></span><span id='topic+discrete_adaboost'></span><span id='topic+real_adaboost'></span>

<h3>Description</h3>

<p>This function allows you to use any classifier to be used in
Discrete or Real AdaBoost framework.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>booster(
  x_train,
  y_train,
  classifier = "rpart",
  predictor = NULL,
  method = "discrete",
  x_test = NULL,
  y_test = NULL,
  weighted_bootstrap = FALSE,
  max_iter = 50,
  lambda = 1,
  print_detail = TRUE,
  print_plot = FALSE,
  bag_frac = 0.5,
  p_weak = NULL,
  ...
)

discrete_adaboost(
  x_train,
  y_train,
  classifier = "rpart",
  predictor = NULL,
  x_test = NULL,
  y_test = NULL,
  weighted_bootstrap = FALSE,
  max_iter = 50,
  lambda = 1,
  print_detail = TRUE,
  print_plot = FALSE,
  bag_frac = 0.5,
  p_weak = NULL,
  ...
)

real_adaboost(
  x_train,
  y_train,
  classifier = "rpart",
  predictor = NULL,
  x_test = NULL,
  y_test = NULL,
  weighted_bootstrap = FALSE,
  max_iter = 50,
  lambda = 1,
  print_detail = TRUE,
  print_plot = FALSE,
  bag_frac = 0.5,
  p_weak = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="booster_+3A_x_train">x_train</code></td>
<td>
<p>feature matrix.</p>
</td></tr>
<tr><td><code id="booster_+3A_y_train">y_train</code></td>
<td>
<p>a factor class variable. Boosting algorithm allows for
k &gt;= 2. However, not all classifiers are capable of multiclass
classification.</p>
</td></tr>
<tr><td><code id="booster_+3A_classifier">classifier</code></td>
<td>
<p>pre-ready or a custom classifier function. Pre-ready
classifiers are &quot;rpart&quot;, &quot;glm&quot;, &quot;gnb&quot;, &quot;dnb&quot;, &quot;earth&quot;.</p>
</td></tr>
<tr><td><code id="booster_+3A_predictor">predictor</code></td>
<td>
<p>prediction function for classifier. It's output must be a
factor variable with the same levels of y_train</p>
</td></tr>
<tr><td><code id="booster_+3A_method">method</code></td>
<td>
<p>&quot;discrete&quot; or &quot;real&quot; for Discrete or Real Adaboost.</p>
</td></tr>
<tr><td><code id="booster_+3A_x_test">x_test</code></td>
<td>
<p>optional test feature matrix. Can be used instead of predict
function. print_detail and print_plot gives information about test.</p>
</td></tr>
<tr><td><code id="booster_+3A_y_test">y_test</code></td>
<td>
<p>optional a factor test class variable with the same levels as
y_train. Can be used instead of predict function. print_detail and print_plot
gives information about test.</p>
</td></tr>
<tr><td><code id="booster_+3A_weighted_bootstrap">weighted_bootstrap</code></td>
<td>
<p>If classifier does not support case weights,
weighted_bootstrap must be TRUE used for weighting. If classifier supports
weights, it must be FALSE. default is FALSE.</p>
</td></tr>
<tr><td><code id="booster_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum number of iterations. Default to 30. Probably should
be higher for classifiers other than decision tree.</p>
</td></tr>
<tr><td><code id="booster_+3A_lambda">lambda</code></td>
<td>
<p>a parameter for model weights. Default to 1. Higher values
leads to unstable weak classifiers, which is good sometimes. Lower values
leads to slower fitting.</p>
</td></tr>
<tr><td><code id="booster_+3A_print_detail">print_detail</code></td>
<td>
<p>a logical for printing errors for each iteration.
Default to TRUE</p>
</td></tr>
<tr><td><code id="booster_+3A_print_plot">print_plot</code></td>
<td>
<p>a logical for plotting errors. Default to FALSE.</p>
</td></tr>
<tr><td><code id="booster_+3A_bag_frac">bag_frac</code></td>
<td>
<p>a value between 0 and 1. It represents the proportion of
cases to be used in each iteration. Smaller datasets may be better to create
weaker classifiers. 1 means all cases. Default to 0.5. Ignored if
<code>weighted_bootstrap == TRUE</code>.</p>
</td></tr>
<tr><td><code id="booster_+3A_p_weak">p_weak</code></td>
<td>
<p>number of variables to use in weak classifiers. It is the
number of columns in <code>x_train</code> by default. Lower values lead to weaker
classifiers.</p>
</td></tr>
<tr><td><code id="booster_+3A_...">...</code></td>
<td>
<p>additional arguments for classifier and predictor functions.
weak classifiers.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>method</code> can be &quot;discrete&quot; and &quot;real&quot; at the moment and indicates Discrete
AdaBoost and Real AdaBoost. For multiclass classification, &quot;discrete&quot; means SAMME,
&quot;real&quot; means SAMME.R algorithm.
</p>
<p>Pre-ready classifiers are &quot;rpart&quot;, &quot;glm&quot;, &quot;dnb&quot;, &quot;gnb&quot;, &quot;earth&quot;, which means
CART, logistic regression, Gaussian naive bayes, discrete naive bayes and MARS
classifier respectively.
</p>
<p><code>predictor</code> is valid only if a custom <code>classifier</code> function is
given. A custom classifier funtion should be as <code>function(x_train, y_train,
weights, ...)</code> and its output is a model object which can be placed in
<code>predictor</code>. <code>predictor</code> function is <code>function(model, x_new, type
...)</code> and its output must be a vector of class predictions. type must be &quot;pred&quot;
or &quot;prob&quot;, which gives a vector of classes or a matrix of probabilities, which
each column represents each class. See <code>vignette("booster", package = "booster")</code>
for examples.
</p>
<p><code>lambda</code> is a multiplier of model weights.
</p>
<p><code>weighted_bootstrap</code> is for bootstrap sampling in each step. If the
classifier accepts case weights then it is better to turn it off. If classifier
does not accept case weights, then weighted bootstrap will make it into
weighted classifier using bootstrap. Learning may be slower this way.
</p>
<p><code>bag_frac</code> helps a classifier to be &quot;weaker&quot; by reducing sample
size. Stronger classifiers may require lower proportions of <code>bag_frac</code>.
<code>p_weak</code> does the same by reducing numbeer of variables.
</p>


<h3>Value</h3>

<p>a booster object with below components.
</p>
<table role = "presentation">
<tr><td><code>n_train</code></td>
<td>
<p>Number of cases in the input dataset.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>Case weights for the final boost.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>Number of features.</p>
</td></tr>
<tr><td><code>weighted_bootstrap</code></td>
<td>
<p>TRUE if weighted bootstrap applied. Otherwise FALSE.</p>
</td></tr>
<tr><td><code>max_iter</code></td>
<td>
<p>Maximum number of boosting steps.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The multiplier of model weights.</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>Function for prediction</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Model weights.</p>
</td></tr>
<tr><td><code>err_train</code></td>
<td>
<p>A vector of train errors in each step of boosting.</p>
</td></tr>
<tr><td><code>err_test</code></td>
<td>
<p>A vector of test errors in each step of boosting. If there are
no test data, it returns NULL</p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>Models obtained in each boosting step</p>
</td></tr>
<tr><td><code>x_classes</code></td>
<td>
<p>A list of datasets, which are <code>x_train</code> separated for
each class.</p>
</td></tr>
<tr><td><code>n_classes</code></td>
<td>
<p>Number of cases for each class in input dataset.</p>
</td></tr>
<tr><td><code>k_classes</code></td>
<td>
<p>Number of classes in class variable.</p>
</td></tr>
<tr><td><code>bag_frac</code></td>
<td>
<p>Proportion of input dataset used in each boosting step.</p>
</td></tr>
<tr><td><code>class_names</code></td>
<td>
<p>Names of classes in class variable.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Fatih Saglam, fatih.saglam@omu.edu.tr
</p>


<h3>References</h3>

<p>Freund, Y., &amp; Schapire, R. E. (1997). A decision-theoretic generalization of
on-line learning and an application to boosting. Journal of computer and
system sciences, 55(1), 119-139.
</p>
<p>Hastie, T., Rosset, S., Zhu, J., &amp; Zou, H. (2009). Multi-class AdaBoost.
Statistics and its Interface, 2(3), 349-360.
</p>


<h3>See Also</h3>

<p><code>predict.booster</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(rbooster)
## n number of cases, p number of variables, k number of classes.
cv_sampler &lt;- function(y, train_proportion) {
 unlist(lapply(unique(y), function(m) sample(which(y==m), round(sum(y==m))*train_proportion)))
}

data_simulation &lt;- function(n, p, k, train_proportion){
 means &lt;- seq(0, k*2.5, length.out = k)
 x &lt;- do.call(rbind, lapply(means,
                            function(m) matrix(data = rnorm(n = round(n/k)*p,
                                                            mean = m,
                                                            sd = 2),
                                               nrow = round(n/k))))
 y &lt;- factor(rep(letters[1:k], each = round(n/k)))
 train_i &lt;- cv_sampler(y, train_proportion)

 data &lt;- data.frame(x, y = y)
 data_train &lt;- data[train_i,]
 data_test &lt;- data[-train_i,]
 return(list(data = data,
             data_train = data_train,
             data_test = data_test))
}
### binary classification
dat &lt;- data_simulation(n = 500, p = 2, k = 2, train_proportion = 0.8)

mm &lt;- booster(x_train = dat$data_train[,1:2],
             y_train = dat$data_train[,3],
             classifier = "rpart",
             method = "discrete",
             x_test = dat$data_test[,1:2],
             y_test = dat$data_test[,3],
             weighted_bootstrap = FALSE,
             max_iter = 100,
             lambda = 1,
             print_detail = TRUE,
             print_plot = TRUE,
             bag_frac = 1,
             p_weak = 2)

## test prediction
mm$test_prediction
## or
pp &lt;- predict(object = mm, newdata = dat$data_test[,1:2], type = "pred")
## test error
tail(mm$err_test, 1)
sum(dat$data_test[,3] != pp)/nrow(dat$data_test)

### multiclass classification
dat &lt;- data_simulation(n = 800, p = 5, k = 3, train_proportion = 0.8)

mm &lt;- booster(x_train = dat$data_train[,1:5],
             y_train = dat$data_train[,6],
             classifier = "rpart",
             method = "real",
             x_test = dat$data_test[,1:5],
             y_test = dat$data_test[,6],
             weighted_bootstrap = FALSE,
             max_iter = 100,
             lambda = 1,
             print_detail = TRUE,
             print_plot = TRUE,
             bag_frac = 1,
             p_weak = 2)

## test prediction
mm$test_prediction
## or
pp &lt;- predict(object = mm, newdata = dat$data_test[,1:5], type = "pred", print_detail = TRUE)
## test error
tail(mm$err_test, 1)
sum(dat$data_test[,6] != pp)/nrow(dat$data_test)

### binary classification, custom classifier
dat &lt;- data_simulation(n = 500, p = 10, k = 2, train_proportion = 0.8)
x &lt;- dat$data[,1:10]
y &lt;- dat$data[,11]

x_train &lt;- dat$data_train[,1:10]
y_train &lt;- dat$data_train[,11]

x_test &lt;- dat$data_test[,1:10]
y_test &lt;- dat$data_test[,11]

## a custom regression classifier function
classifier_lm &lt;- function(x_train, y_train, weights, ...){
 y_train_code &lt;- c(-1,1)
 y_train_coded &lt;- sapply(levels(y_train), function(m) y_train_code[(y_train == m) + 1])
 y_train_coded &lt;- y_train_coded[,1]

 model &lt;- lm.wfit(x = as.matrix(cbind(1,x_train)), y = y_train_coded, w = weights)
 return(list(coefficients = model$coefficients,
             levels = levels(y_train)))
}

## predictor function

predictor_lm &lt;- function(model, x_new, type = "pred", ...) {
 coef &lt;- model$coefficients
 levels &lt;- model$levels

 fit &lt;- as.matrix(cbind(1, x_new))%*%coef
 probs &lt;- 1/(1 + exp(-fit))
 probs &lt;- data.frame(probs, 1 - probs)
 colnames(probs) &lt;- levels

 if (type == "pred") {
   preds &lt;- factor(levels[apply(probs, 1, which.max)], levels = levels, labels = levels)
   return(preds)
 }
 if (type == "prob") {
   return(probs)
 }
}

## real AdaBoost
mm &lt;- booster(x_train = x_train,
             y_train = y_train,
             classifier = classifier_lm,
             predictor = predictor_lm,
             method = "real",
             x_test = x_test,
             y_test = y_test,
             weighted_bootstrap = FALSE,
             max_iter = 50,
             lambda = 1,
             print_detail = TRUE,
             print_plot = TRUE,
             bag_frac = 0.5,
             p_weak = 2)

## test prediction
mm$test_prediction
pp &lt;- predict(object = mm, newdata = x_test, type = "pred", print_detail = TRUE)
## test error
tail(mm$err_test, 1)
sum(y_test != pp)/nrow(x_test)

## discrete AdaBoost
mm &lt;- booster(x_train = x_train,
             y_train = y_train,
             classifier = classifier_lm,
             predictor = predictor_lm,
             method = "discrete",
             x_test = x_test,
             y_test = y_test,
             weighted_bootstrap = FALSE,
             max_iter = 50,
             lambda = 1,
             print_detail = TRUE,
             print_plot = TRUE,
             bag_frac = 0.5,
             p_weak = 2)

## test prediction
mm$test_prediction
pp &lt;- predict(object = mm, newdata = x_test, type = "pred", print_detail = TRUE)
## test error
tail(mm$err_test, 1)
sum(y_test != pp)/nrow(x_test)

# plot function can be used to plot errors
plot(mm)

# more examples are in vignette("booster", package = "rbooster")

</code></pre>

<hr>
<h2 id='classifier_rpart'>Functions to be used internally</h2><span id='topic+classifier_rpart'></span><span id='topic+predictor_rpart'></span><span id='topic+classifier_glm'></span><span id='topic+predictor_glm'></span><span id='topic+classifier_gnb'></span><span id='topic+predictor_gnb'></span><span id='topic+classifier_dnb'></span><span id='topic+predictor_dnb'></span><span id='topic+classifier_earth'></span><span id='topic+predictor_earth'></span>

<h3>Description</h3>

<p>for internal use
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classifier_rpart(x_train, y_train, weights, ...)

predictor_rpart(model, x_new, type = "pred", ...)

classifier_glm(x_train, y_train, weights, ...)

predictor_glm(model, x_new, type = "pred", ...)

classifier_gnb(x_train, y_train, weights, ...)

predictor_gnb(model, x_new, type = "pred", ...)

classifier_dnb(x_train, y_train, weights, ...)

predictor_dnb(model, x_new, type = "pred", ...)

classifier_earth(x_train, y_train, weights, ...)

predictor_earth(model, x_new, type = "pred", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="classifier_rpart_+3A_x_train">x_train</code></td>
<td>
<p>input features.</p>
</td></tr>
<tr><td><code id="classifier_rpart_+3A_y_train">y_train</code></td>
<td>
<p>factor class variable.</p>
</td></tr>
<tr><td><code id="classifier_rpart_+3A_weights">weights</code></td>
<td>
<p>instance weights.</p>
</td></tr>
<tr><td><code id="classifier_rpart_+3A_...">...</code></td>
<td>
<p>other control parameters.</p>
</td></tr>
<tr><td><code id="classifier_rpart_+3A_model">model</code></td>
<td>
<p>model obtained from respective classifier.</p>
</td></tr>
<tr><td><code id="classifier_rpart_+3A_x_new">x_new</code></td>
<td>
<p>new features for prediction.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Classifiers produce an object which is appropriate
for respective predictor. Predictors returns class
predictions for <code>x_new</code>.
</p>

<hr>
<h2 id='discretize'>Discretize</h2><span id='topic+discretize'></span>

<h3>Description</h3>

<p>Discretizes numeric variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discretize(xx, breaks = 3, boundaries = NULL, categories = NULL, w = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="discretize_+3A_xx">xx</code></td>
<td>
<p>matrix or data.frame whose variables needs to be discretized.</p>
</td></tr>
<tr><td><code id="discretize_+3A_breaks">breaks</code></td>
<td>
<p>number of categories for each variable. Ignored if <code>boundaries</code> != <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="discretize_+3A_boundaries">boundaries</code></td>
<td>
<p>user-defined upper and lower limit matrix of discretization
for each variable. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="discretize_+3A_categories">categories</code></td>
<td>
<p>user-defined category names for each variable. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="discretize_+3A_w">w</code></td>
<td>
<p>sample weights for quantile calculation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses quantiles for discretization. However, quantiles may be equal in some cases.
Then equal interval discretization used instead.
</p>


<h3>Value</h3>

<p>a list consists of:
</p>
<table role = "presentation">
<tr><td><code>x_discrete</code></td>
<td>
<p>data.frame of discretized variables. Each variable is a factor.</p>
</td></tr>
<tr><td><code>boundaries</code></td>
<td>
<p>upper and lower limit matrix of discretization
for each variable.</p>
</td></tr>
<tr><td><code>categories</code></td>
<td>
<p>category names for each variable.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Fatih Saglam, fatih.saglam@omu.edu.tr
</p>

<hr>
<h2 id='plot.booster'>plot booster</h2><span id='topic+plot.booster'></span><span id='topic+plot.discrete_adaboost'></span><span id='topic+plot.real_adaboost'></span>

<h3>Description</h3>

<p>plots errors of booster model iterations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'booster'
plot(x, y, ...)

## S3 method for class 'discrete_adaboost'
plot(x, y, ...)

## S3 method for class 'real_adaboost'
plot(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.booster_+3A_x">x</code></td>
<td>
<p>booster object</p>
</td></tr>
<tr><td><code id="plot.booster_+3A_y">y</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="plot.booster_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Summary of &quot;booster&quot; object.
</p>

<hr>
<h2 id='predict.booster'>Prediction function for Adaboost framework</h2><span id='topic+predict.booster'></span><span id='topic+predict.discrete_adaboost'></span><span id='topic+predict.real_adaboost'></span>

<h3>Description</h3>

<p>Makes predictions based on booster function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'booster'
predict(object, newdata, type = "pred", print_detail = FALSE, ...)

## S3 method for class 'discrete_adaboost'
predict(object, newdata, type = "pred", print_detail = FALSE, ...)

## S3 method for class 'real_adaboost'
predict(object, newdata, type = "pred", print_detail = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.booster_+3A_object">object</code></td>
<td>
<p>booster object</p>
</td></tr>
<tr><td><code id="predict.booster_+3A_newdata">newdata</code></td>
<td>
<p>a factor class variable. Boosting algorithm allows for</p>
</td></tr>
<tr><td><code id="predict.booster_+3A_type">type</code></td>
<td>
<p>pre-ready or a custom classifier function.</p>
</td></tr>
<tr><td><code id="predict.booster_+3A_print_detail">print_detail</code></td>
<td>
<p>prints the prediction process. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="predict.booster_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Type &quot;pred&quot; will give class predictions. &quot;prob&quot; will give probabilities for
each class.
</p>


<h3>Value</h3>

<p>A vector of class predictions or a matrix of class probabilities
depending of <code>type</code>
</p>


<h3>See Also</h3>

<p>[predict()]
</p>

<hr>
<h2 id='predict.w_naive_bayes'>Predict Discrete Naive Bayes</h2><span id='topic+predict.w_naive_bayes'></span><span id='topic+predict.w_discrete_naive_bayes'></span><span id='topic+predict.w_gaussian_naive_bayes'></span>

<h3>Description</h3>

<p>Function for Naive Bayes algorithm prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'w_naive_bayes'
predict(object, newdata = NULL, type = "prob", ...)

## S3 method for class 'w_discrete_naive_bayes'
predict(object, newdata, type = "prob", ...)

## S3 method for class 'w_gaussian_naive_bayes'
predict(object, newdata = NULL, type = "prob", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.w_naive_bayes_+3A_object">object</code></td>
<td>
<p>&quot;w_bayes&quot; class object..</p>
</td></tr>
<tr><td><code id="predict.w_naive_bayes_+3A_newdata">newdata</code></td>
<td>
<p>new observations which predictions will be made on.</p>
</td></tr>
<tr><td><code id="predict.w_naive_bayes_+3A_type">type</code></td>
<td>
<p>&quot;pred&quot; or &quot;prob&quot;.</p>
</td></tr>
<tr><td><code id="predict.w_naive_bayes_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calls <code>predict.w_discrete_naive_bayes</code> or <code>predict.w_gaussian_naive_bayes</code>
accordingly
</p>
<p>Type &quot;pred&quot; will give class predictions. &quot;prob&quot; will give probabilities for
each class.
</p>


<h3>Value</h3>

<p>A vector of class predictions or a matrix of class probabilities
depending of <code>type</code>
</p>


<h3>See Also</h3>

<p>[predict()], [rbooster::predict.w_discrete_naive_bayes()], [rbooster::predict.w_gaussian_naive_bayes()]
</p>

<hr>
<h2 id='print.booster'>Print booster</h2><span id='topic+print.booster'></span><span id='topic+print.discrete_adaboost'></span><span id='topic+print.real_adaboost'></span>

<h3>Description</h3>

<p>Prints a summary of booster model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'booster'
print(x, ...)

## S3 method for class 'discrete_adaboost'
print(x, ...)

## S3 method for class 'real_adaboost'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.booster_+3A_x">x</code></td>
<td>
<p>booster object</p>
</td></tr>
<tr><td><code id="print.booster_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Summary of &quot;booster&quot; object.
</p>

<hr>
<h2 id='print.w_naive_bayes'>Print w_naive_bayes</h2><span id='topic+print.w_naive_bayes'></span><span id='topic+print.w_gaussian_naive_bayes'></span><span id='topic+print.w_discrete_naive_bayes'></span>

<h3>Description</h3>

<p>Prints a summary of w_naive_bayes model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'w_naive_bayes'
print(x, ...)

## S3 method for class 'w_gaussian_naive_bayes'
print(x, ...)

## S3 method for class 'w_discrete_naive_bayes'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.w_naive_bayes_+3A_x">x</code></td>
<td>
<p>w_naive_bayes object</p>
</td></tr>
<tr><td><code id="print.w_naive_bayes_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Summary of &quot;w_naive_bayes&quot; object.
</p>

<hr>
<h2 id='w_naive_bayes'>Naive Bayes algorithm with case weights</h2><span id='topic+w_naive_bayes'></span><span id='topic+w_gaussian_naive_bayes'></span><span id='topic+w_discrete_naive_bayes'></span>

<h3>Description</h3>

<p>Function for Naive Bayes algorithm classification with case weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>w_naive_bayes(x_train, y_train, w = NULL, discretize = TRUE, breaks = 3)

w_gaussian_naive_bayes(x_train, y_train, w = NULL)

w_discrete_naive_bayes(x_train, y_train, breaks = 3, w = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="w_naive_bayes_+3A_x_train">x_train</code></td>
<td>
<p>explanatory variables.</p>
</td></tr>
<tr><td><code id="w_naive_bayes_+3A_y_train">y_train</code></td>
<td>
<p>a factor class variable.</p>
</td></tr>
<tr><td><code id="w_naive_bayes_+3A_w">w</code></td>
<td>
<p>a vector of case weights.</p>
</td></tr>
<tr><td><code id="w_naive_bayes_+3A_discretize">discretize</code></td>
<td>
<p>If <code>TRUE</code> numerical variables are discretized and discrete naive bayes is applied,</p>
</td></tr>
<tr><td><code id="w_naive_bayes_+3A_breaks">breaks</code></td>
<td>
<p>number of break points for discretization. Ignored if <code>discretize = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>w_naive_bayes</code> calls <code>w_gaussian_naive_bayes</code> or <code>w_discrete_naive_bayes</code>.
</p>
<p>if <code>discrete = FALSE</code>, <code>w_gaussian_naive_bayes</code> is called. It uses Gaussian densities with case weights and allows
multiclass classification.
</p>
<p>if <code>discrete = TRUE</code>, <code>w_discrete_naive_bayes</code> is called. It uses conditional probabilities for each category with
laplace smoothing and allows multiclass classification.
</p>


<h3>Value</h3>

<p>a <code>w_naive_bayes</code> object with below components.
</p>
<table role = "presentation">
<tr><td><code>n_train</code></td>
<td>
<p>Number of cases in the input dataset.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>Number of explanatory variables.</p>
</td></tr>
<tr><td><code>x_classes</code></td>
<td>
<p>A list of datasets, which are <code>x_train</code> separated
for each class.</p>
</td></tr>
<tr><td><code>n_classes</code></td>
<td>
<p>Number of cases for each class in input dataset.</p>
</td></tr>
<tr><td><code>k_classes</code></td>
<td>
<p>Number of classes in class variable.</p>
</td></tr>
<tr><td><code>priors</code></td>
<td>
<p>Prior probabilities.</p>
</td></tr>
<tr><td><code>class_names</code></td>
<td>
<p>Names of classes in class variable.</p>
</td></tr>
<tr><td><code>means</code></td>
<td>
<p>Weighted mean estimations for each variable.</p>
</td></tr>
<tr><td><code>stds</code></td>
<td>
<p>Weighted standart deviation estimations for each variable.</p>
</td></tr>
<tr><td><code>categories</code></td>
<td>
<p>Labels for discretized variables.</p>
</td></tr>
<tr><td><code>boundaries</code></td>
<td>
<p>Upper and lower boundaries for discretization.</p>
</td></tr>
<tr><td><code>ps</code></td>
<td>
<p>probabilities for each variable categories.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
library(rbooster)
## short functions for cross-validation and data simulation
cv_sampler &lt;- function(y, train_proportion) {
 unlist(lapply(unique(y), function(m) sample(which(y==m), round(sum(y==m))*train_proportion)))
}

data_simulation &lt;- function(n, p, k, train_proportion){
 means &lt;- seq(0, k*1.5, length.out = k)
 x &lt;- do.call(rbind, lapply(means,
                            function(m) matrix(data = rnorm(n = round(n/k)*p,
                                                            mean = m,
                                                            sd = 2),
                                               nrow = round(n/k))))
 y &lt;- factor(rep(letters[1:k], each = round(n/k)))
 train_i &lt;- cv_sampler(y, train_proportion)

 data &lt;- data.frame(x, y = y)
 data_train &lt;- data[train_i,]
 data_test &lt;- data[-train_i,]
 return(list(data = data,
             data_train = data_train,
             data_test = data_test))
}

### binary classification example
n &lt;- 500
p &lt;- 10
k &lt;- 2
dat &lt;- data_simulation(n = n, p = p, k = k, train_proportion = 0.8)
x &lt;- dat$data[,1:p]
y &lt;- dat$data[,p+1]

x_train &lt;- dat$data_train[,1:p]
y_train &lt;- dat$data_train[,p+1]

x_test &lt;- dat$data_test[,1:p]
y_test &lt;- dat$data_test[,p+1]

## discretized Naive Bayes classification
mm1 &lt;- w_naive_bayes(x_train = x_train, y_train = y_train, discretize = TRUE, breaks = 4)
preds1 &lt;- predict(object = mm1, newdata = x_test, type = "pred")
table(y_test, preds1)
# or
mm2 &lt;- w_discrete_naive_bayes(x_train = x_train, y_train = y_train, breaks = 4)
preds2 &lt;- predict(object = mm2, newdata = x_test, type = "pred")
table(y_test, preds2)

## Gaussian Naive Bayes classification
mm3 &lt;- w_naive_bayes(x_train = x_train, y_train = y_train, discretize = FALSE)
preds3 &lt;- predict(object = mm3, newdata = x_test, type = "pred")
table(y_test, preds3)

#or
mm4 &lt;- w_gaussian_naive_bayes(x_train = x_train, y_train = y_train)
preds4 &lt;- predict(object = mm4, newdata = x_test, type = "pred")
table(y_test, preds4)

## multiclass example
n &lt;- 500
p &lt;- 10
k &lt;- 5
dat &lt;- data_simulation(n = n, p = p, k = k, train_proportion = 0.8)
x &lt;- dat$data[,1:p]
y &lt;- dat$data[,p+1]

x_train &lt;- dat$data_train[,1:p]
y_train &lt;- dat$data_train[,p+1]

x_test &lt;- dat$data_test[,1:p]
y_test &lt;- dat$data_test[,p+1]

# discretized
mm5 &lt;- w_discrete_naive_bayes(x_train = x_train, y_train = y_train, breaks = 4)
preds5 &lt;- predict(object = mm5, newdata = x_test, type = "pred")
table(y_test, preds5)

# gaussian
mm6 &lt;- w_gaussian_naive_bayes(x_train = x_train, y_train = y_train)
preds6 &lt;- predict(object = mm6, newdata = x_test, type = "pred")
table(y_test, preds6)

## example for case weights
n &lt;- 500
p &lt;- 10
k &lt;- 5
dat &lt;- data_simulation(n = n, p = p, k = k, train_proportion = 0.8)
x &lt;- dat$data[,1:p]
y &lt;- dat$data[,p+1]

x_train &lt;- dat$data_train[,1:p]
y_train &lt;- dat$data_train[,p+1]

# discretized
weights &lt;- ifelse(y_train == "a" | y_train == "c", 1, 0.01)

mm7 &lt;- w_discrete_naive_bayes(x_train = x_train, y_train = y_train, breaks = 4, w = weights)

preds7 &lt;- predict(object = mm7, newdata = x_test, type = "pred")
table(y_test, preds7)

# gaussian
weights &lt;- ifelse(y_train == "b" | y_train == "d", 1, 0.01)

mm8 &lt;- w_gaussian_naive_bayes(x_train = x_train, y_train = y_train, w = weights)

preds8 &lt;- predict(object = mm8, newdata = x_test, type = "pred")
table(y_test, preds8)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
