<!DOCTYPE html><html><head><title>Help for package tabnet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tabnet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#autoplot.tabnet_explain'><p>Plot tabnet_explain mask importance heatmap</p></a></li>
<li><a href='#autoplot.tabnet_fit'><p>Plot tabnet_fit model loss along epochs</p></a></li>
<li><a href='#check_compliant_node'><p>Check that Node object names are compliant</p></a></li>
<li><a href='#decision_width'><p>Parameters for the tabnet model</p></a></li>
<li><a href='#nn_prune_head.tabnet_fit'><p>Prune top layer(s) of a tabnet network</p></a></li>
<li><a href='#node_to_df'><p>Turn a Node object into predictor and outcome.</p></a></li>
<li><a href='#tabnet'><p>Parsnip compatible tabnet model</p></a></li>
<li><a href='#tabnet_config'><p>Configuration for TabNet models</p></a></li>
<li><a href='#tabnet_explain'><p>Interpretation metrics from a TabNet model</p></a></li>
<li><a href='#tabnet_fit'><p>Tabnet model</p></a></li>
<li><a href='#tabnet_nn'><p>TabNet Model Architecture</p></a></li>
<li><a href='#tabnet_pretrain'><p>Tabnet model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Fit 'TabNet' Models for Classification and Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements the 'TabNet' model by Sercan O. Arik et al. (2019) &lt;<a href="https://doi.org/10.48550/arXiv.1908.07442">doi:10.48550/arXiv.1908.07442</a>&gt; 
    with 'Coherent Hierarchical Multi-label Classification Networks' by Giunchiglia et al.
    &lt;<a href="https://doi.org/10.48550/arXiv.2010.10151">doi:10.48550/arXiv.2010.10151</a>&gt; and provides a consistent interface for fitting and creating 
    predictions. It's also fully compatible with the 'tidymodels' ecosystem.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://mlverse.github.io/tabnet/">https://mlverse.github.io/tabnet/</a>,
<a href="https://github.com/mlverse/tabnet">https://github.com/mlverse/tabnet</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlverse/tabnet/issues">https://github.com/mlverse/tabnet/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>Imports:</td>
<td>torch (&ge; 0.4.0), hardhat (&ge; 1.3.0), magrittr, progress,
rlang, methods, dplyr, tibble, tidyr, coro, vctrs, zeallot</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0), data.tree, Matrix, modeldata, recipes,
rsample, parsnip, dials, withr, knitr, rmarkdown, vip,
tidyverse, ggplot2, purrr, stringr, tune, workflows, yardstick</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>false</td>
</tr>
<tr>
<td>Config/testthat/start-first:</td>
<td>interface, explain, params</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-05 00:32:16 UTC; creg</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel Falbel [aut],
  RStudio [cph],
  Christophe Regouby [cre, ctb],
  Egill Fridgeirsson [ctb],
  Philipp Haarmeyer [ctb],
  Sven Verweij <a href="https://orcid.org/0000-0002-5573-3952"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Christophe Regouby &lt;christophe.regouby@free.fr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-05 10:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Value</h3>

<p>Returns <code>rhs(lhs)</code>.
</p>

<hr>
<h2 id='autoplot.tabnet_explain'>Plot tabnet_explain mask importance heatmap</h2><span id='topic+autoplot.tabnet_explain'></span>

<h3>Description</h3>

<p>Plot tabnet_explain mask importance heatmap
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoplot.tabnet_explain(
  object,
  type = c("mask_agg", "steps"),
  quantile = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoplot.tabnet_explain_+3A_object">object</code></td>
<td>
<p>A <code>tabnet_explain</code> object as a result of <code><a href="#topic+tabnet_explain">tabnet_explain()</a></code>.</p>
</td></tr>
<tr><td><code id="autoplot.tabnet_explain_+3A_type">type</code></td>
<td>
<p>a character value. Either <code>"mask_agg"</code> the default, for a single
heatmap of aggregated mask importance per predictor along the dataset,
or <code>"steps"</code> for one heatmap at each mask step.</p>
</td></tr>
<tr><td><code id="autoplot.tabnet_explain_+3A_quantile">quantile</code></td>
<td>
<p>numerical value between 0 and 1. Provides quantile clipping of the
mask values</p>
</td></tr>
<tr><td><code id="autoplot.tabnet_explain_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plot the tabnet_explain object mask importance per variable along the predicted dataset.
<code>type="mask_agg"</code> output a single heatmap of mask aggregated values,
<code>type="steps"</code> provides a plot faceted along the <code>n_steps</code> mask present in the model.
<code>quantile=.995</code> may be used for strong outlier clipping, in order to better highlight
low values. <code>quantile=1</code>, the default, do not clip any values.
</p>


<h3>Value</h3>

<p>A <code>ggplot</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ggplot2)
data("attrition", package = "modeldata")

## Single-outcome binary classification of `Attrition` in `attrition` dataset
attrition_fit &lt;- tabnet_fit(Attrition ~. , data=attrition, epoch=11)
attrition_explain &lt;- tabnet_explain(attrition_fit, attrition)
# Plot the model aggregated mask interpretation heatmap
autoplot(attrition_explain)

## Multi-outcome regression on `Sale_Price` and `Pool_Area` in `ames` dataset,
data("ames", package = "modeldata")
ids &lt;- sample(nrow(ames), 256)
x &lt;- ames[ids,-which(names(ames) %in% c("Sale_Price", "Pool_Area"))]
y &lt;- ames[ids, c("Sale_Price", "Pool_Area")]
ames_fit &lt;- tabnet_fit(x, y, epochs = 5, verbose=TRUE)
ames_explain &lt;- tabnet_explain(ames_fit, x)
autoplot(ames_explain, quantile = 0.99)

</code></pre>

<hr>
<h2 id='autoplot.tabnet_fit'>Plot tabnet_fit model loss along epochs</h2><span id='topic+autoplot.tabnet_fit'></span><span id='topic+autoplot.tabnet_pretrain'></span>

<h3>Description</h3>

<p>Plot tabnet_fit model loss along epochs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoplot.tabnet_fit(object, ...)

autoplot.tabnet_pretrain(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoplot.tabnet_fit_+3A_object">object</code></td>
<td>
<p>A <code>tabnet_fit</code> or <code>tabnet_pretrain</code> object as a result of
<code><a href="#topic+tabnet_fit">tabnet_fit()</a></code> or <code><a href="#topic+tabnet_pretrain">tabnet_pretrain()</a></code>.</p>
</td></tr>
<tr><td><code id="autoplot.tabnet_fit_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plot the training loss along epochs, and validation loss along epochs if any.
A dot is added on epochs where model snapshot is available, helping
the choice of <code>from_epoch</code> value for later model training resume.
</p>


<h3>Value</h3>

<p>A <code>ggplot</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ggplot2)
data("attrition", package = "modeldata")
attrition_fit &lt;- tabnet_fit(Attrition ~. , data=attrition, valid_split=0.2, epoch=11)

# Plot the model loss over epochs
autoplot(attrition_fit)

</code></pre>

<hr>
<h2 id='check_compliant_node'>Check that Node object names are compliant</h2><span id='topic+check_compliant_node'></span>

<h3>Description</h3>

<p>Check that Node object names are compliant
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_compliant_node(node)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_compliant_node_+3A_node">node</code></td>
<td>
<p>the Node object, or a dataframe ready to be parsed by <code>data.tree::as.Node()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>node if it is compliant, else an Error with the column names to fix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)
library(data.tree)
data(starwars)
starwars_tree &lt;- starwars %&gt;%
  mutate(pathString = paste("tree", species, homeworld, `name`, sep = "/"))

# pre as.Node() check
try(check_compliant_node(starwars_tree))

# post as.Node() check
check_compliant_node(as.Node(starwars_tree))

</code></pre>

<hr>
<h2 id='decision_width'>Parameters for the tabnet model</h2><span id='topic+decision_width'></span><span id='topic+attention_width'></span><span id='topic+num_steps'></span><span id='topic+feature_reusage'></span><span id='topic+num_independent'></span><span id='topic+num_shared'></span><span id='topic+momentum'></span><span id='topic+mask_type'></span>

<h3>Description</h3>

<p>Parameters for the tabnet model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decision_width(range = c(8L, 64L), trans = NULL)

attention_width(range = c(8L, 64L), trans = NULL)

num_steps(range = c(3L, 10L), trans = NULL)

feature_reusage(range = c(1, 2), trans = NULL)

num_independent(range = c(1L, 5L), trans = NULL)

num_shared(range = c(1L, 5L), trans = NULL)

momentum(range = c(0.01, 0.4), trans = NULL)

mask_type(values = c("sparsemax", "entmax"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decision_width_+3A_range">range</code></td>
<td>
<p>the default range for the parameter value</p>
</td></tr>
<tr><td><code id="decision_width_+3A_trans">trans</code></td>
<td>
<p>whether to apply a transformation to the parameter</p>
</td></tr>
<tr><td><code id="decision_width_+3A_values">values</code></td>
<td>
<p>possible values for factor parameters
</p>
<p>These functions are used with <code>tune</code> grid functions to generate
candidates.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>dials</code> parameter to be used when tuning TabNet models.
</p>

<hr>
<h2 id='nn_prune_head.tabnet_fit'>Prune top layer(s) of a tabnet network</h2><span id='topic+nn_prune_head.tabnet_fit'></span><span id='topic+nn_prune_head.tabnet_pretrain'></span>

<h3>Description</h3>

<p>Prune <code>head_size</code> last layers of a tabnet network in order to
use the pruned module as a sequential embedding module.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn_prune_head.tabnet_fit(x, head_size)

nn_prune_head.tabnet_pretrain(x, head_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn_prune_head.tabnet_fit_+3A_x">x</code></td>
<td>
<p>nn_network to prune</p>
</td></tr>
<tr><td><code id="nn_prune_head.tabnet_fit_+3A_head_size">head_size</code></td>
<td>
<p>number of nn_layers to prune, should be less than 2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tabnet network with the top nn_layer removed
</p>

<hr>
<h2 id='node_to_df'>Turn a Node object into predictor and outcome.</h2><span id='topic+node_to_df'></span>

<h3>Description</h3>

<p>Turn a Node object into predictor and outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>node_to_df(x, drop_last_level = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="node_to_df_+3A_x">x</code></td>
<td>
<p>Node object</p>
</td></tr>
<tr><td><code id="node_to_df_+3A_drop_last_level">drop_last_level</code></td>
<td>
<p>TRUE unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list of x and y, being respectively the predictor data-frame and the outcomes data-frame,
as expected inputs for <code>hardhat::mold()</code> function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)
library(data.tree)
data(starwars)
starwars_tree &lt;- starwars %&gt;%
  mutate(pathString = paste("tree", species, homeworld, `name`, sep = "/")) %&gt;%
  as.Node()
node_to_df(starwars_tree)$x %&gt;% head()
node_to_df(starwars_tree)$y %&gt;% head()

</code></pre>

<hr>
<h2 id='tabnet'>Parsnip compatible tabnet model</h2><span id='topic+tabnet'></span>

<h3>Description</h3>

<p>Parsnip compatible tabnet model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tabnet(
  mode = "unknown",
  epochs = NULL,
  penalty = NULL,
  batch_size = NULL,
  learn_rate = NULL,
  decision_width = NULL,
  attention_width = NULL,
  num_steps = NULL,
  feature_reusage = NULL,
  virtual_batch_size = NULL,
  num_independent = NULL,
  num_shared = NULL,
  momentum = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tabnet_+3A_mode">mode</code></td>
<td>
<p>A single character string for the type of model. Possible values
for this model are &quot;unknown&quot;, &quot;regression&quot;, or &quot;classification&quot;.</p>
</td></tr>
<tr><td><code id="tabnet_+3A_epochs">epochs</code></td>
<td>
<p>(int) Number of training epochs.</p>
</td></tr>
<tr><td><code id="tabnet_+3A_penalty">penalty</code></td>
<td>
<p>This is the extra sparsity loss coefficient as proposed
in the original paper. The bigger this coefficient is, the sparser your model
will be in terms of feature selection. Depending on the difficulty of your
problem, reducing this value could help (default 1e-3).</p>
</td></tr>
<tr><td><code id="tabnet_+3A_batch_size">batch_size</code></td>
<td>
<p>(int) Number of examples per batch, large batch sizes are
recommended. (default: 1024^2)</p>
</td></tr>
<tr><td><code id="tabnet_+3A_learn_rate">learn_rate</code></td>
<td>
<p>initial learning rate for the optimizer.</p>
</td></tr>
<tr><td><code id="tabnet_+3A_decision_width">decision_width</code></td>
<td>
<p>(int) Width of the decision prediction layer. Bigger values gives
more capacity to the model with the risk of overfitting. Values typically
range from 8 to 64.</p>
</td></tr>
<tr><td><code id="tabnet_+3A_attention_width">attention_width</code></td>
<td>
<p>(int) Width of the attention embedding for each mask. According to
the paper n_d = n_a is usually a good choice. (default=8)</p>
</td></tr>
<tr><td><code id="tabnet_+3A_num_steps">num_steps</code></td>
<td>
<p>(int) Number of steps in the architecture
(usually between 3 and 10)</p>
</td></tr>
<tr><td><code id="tabnet_+3A_feature_reusage">feature_reusage</code></td>
<td>
<p>(float) This is the coefficient for feature reusage in the masks.
A value close to 1 will make mask selection least correlated between layers.
Values range from 1.0 to 2.0.</p>
</td></tr>
<tr><td><code id="tabnet_+3A_virtual_batch_size">virtual_batch_size</code></td>
<td>
<p>(int) Size of the mini batches used for
&quot;Ghost Batch Normalization&quot; (default=256^2)</p>
</td></tr>
<tr><td><code id="tabnet_+3A_num_independent">num_independent</code></td>
<td>
<p>Number of independent Gated Linear Units layers at each step of the encoder.
Usual values range from 1 to 5.</p>
</td></tr>
<tr><td><code id="tabnet_+3A_num_shared">num_shared</code></td>
<td>
<p>Number of shared Gated Linear Units at each step of the encoder. Usual values
at each step of the decoder. range from 1 to 5</p>
</td></tr>
<tr><td><code id="tabnet_+3A_momentum">momentum</code></td>
<td>
<p>Momentum for batch normalization, typically ranges from 0.01
to 0.4 (default=0.02)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A TabNet <code>parsnip</code> instance. It can be used to fit tabnet models using
<code>parsnip</code> machinery.
</p>


<h3>Threading</h3>

<p>TabNet uses <code>torch</code> as its backend for computation and <code>torch</code> uses all
available threads by default.
</p>
<p>You can control the number of threads used by <code>torch</code> with:
</p>
<div class="sourceCode"><pre>torch::torch_set_num_threads(1)
torch::torch_set_num_interop_threads(1)
</pre></div>


<h3>See Also</h3>

<p>tabnet_fit
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(parsnip)
data("ames", package = "modeldata")
model &lt;- tabnet() %&gt;%
  set_mode("regression") %&gt;%
  set_engine("torch")
model %&gt;%
  fit(Sale_Price ~ ., data = ames)

</code></pre>

<hr>
<h2 id='tabnet_config'>Configuration for TabNet models</h2><span id='topic+tabnet_config'></span>

<h3>Description</h3>

<p>Configuration for TabNet models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tabnet_config(
  batch_size = 1024^2,
  penalty = 0.001,
  clip_value = NULL,
  loss = "auto",
  epochs = 5,
  drop_last = FALSE,
  decision_width = NULL,
  attention_width = NULL,
  num_steps = 3,
  feature_reusage = 1.3,
  mask_type = "sparsemax",
  virtual_batch_size = 256^2,
  valid_split = 0,
  learn_rate = 0.02,
  optimizer = "adam",
  lr_scheduler = NULL,
  lr_decay = 0.1,
  step_size = 30,
  checkpoint_epochs = 10,
  cat_emb_dim = 1,
  num_independent = 2,
  num_shared = 2,
  num_independent_decoder = 1,
  num_shared_decoder = 1,
  momentum = 0.02,
  pretraining_ratio = 0.5,
  verbose = FALSE,
  device = "auto",
  importance_sample_size = NULL,
  early_stopping_monitor = "auto",
  early_stopping_tolerance = 0,
  early_stopping_patience = 0L,
  num_workers = 0L,
  skip_importance = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tabnet_config_+3A_batch_size">batch_size</code></td>
<td>
<p>(int) Number of examples per batch, large batch sizes are
recommended. (default: 1024^2)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_penalty">penalty</code></td>
<td>
<p>This is the extra sparsity loss coefficient as proposed
in the original paper. The bigger this coefficient is, the sparser your model
will be in terms of feature selection. Depending on the difficulty of your
problem, reducing this value could help (default 1e-3).</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_clip_value">clip_value</code></td>
<td>
<p>If a float is given this will clip the gradient at
clip_value. Pass <code>NULL</code> to not clip.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_loss">loss</code></td>
<td>
<p>(character or function) Loss function for training (default to mse
for regression and cross entropy for classification)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_epochs">epochs</code></td>
<td>
<p>(int) Number of training epochs.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_drop_last">drop_last</code></td>
<td>
<p>(logical) Whether to drop last batch if not complete during
training</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_decision_width">decision_width</code></td>
<td>
<p>(int) Width of the decision prediction layer. Bigger values gives
more capacity to the model with the risk of overfitting. Values typically
range from 8 to 64.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_attention_width">attention_width</code></td>
<td>
<p>(int) Width of the attention embedding for each mask. According to
the paper n_d = n_a is usually a good choice. (default=8)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_num_steps">num_steps</code></td>
<td>
<p>(int) Number of steps in the architecture
(usually between 3 and 10)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_feature_reusage">feature_reusage</code></td>
<td>
<p>(float) This is the coefficient for feature reusage in the masks.
A value close to 1 will make mask selection least correlated between layers.
Values range from 1.0 to 2.0.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_mask_type">mask_type</code></td>
<td>
<p>(character) Final layer of feature selector in the attentive_transformer
block, either <code>"sparsemax"</code> or <code>"entmax"</code>.Defaults to <code>"sparsemax"</code>.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_virtual_batch_size">virtual_batch_size</code></td>
<td>
<p>(int) Size of the mini batches used for
&quot;Ghost Batch Normalization&quot; (default=256^2)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_valid_split">valid_split</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;[0, 1)&#8288;</code>) The fraction of the dataset used for validation.
(default = 0 means no split)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_learn_rate">learn_rate</code></td>
<td>
<p>initial learning rate for the optimizer.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_optimizer">optimizer</code></td>
<td>
<p>the optimization method. currently only 'adam' is supported,
you can also pass any torch optimizer function.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_lr_scheduler">lr_scheduler</code></td>
<td>
<p>if <code>NULL</code>, no learning rate decay is used. If &quot;step&quot;
decays the learning rate by <code>lr_decay</code> every <code>step_size</code> epochs. If &quot;reduce_on_plateau&quot;
decays the learning rate by <code>lr_decay</code> when no improvement after <code>step_size</code> epochs.
It can also be a <a href="torch.html#topic+lr_scheduler">torch::lr_scheduler</a> function that only takes the optimizer
as parameter. The <code>step</code> method is called once per epoch.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_lr_decay">lr_decay</code></td>
<td>
<p>multiplies the initial learning rate by <code>lr_decay</code> every
<code>step_size</code> epochs. Unused if <code>lr_scheduler</code> is a <code>torch::lr_scheduler</code>
or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_step_size">step_size</code></td>
<td>
<p>the learning rate scheduler step size. Unused if
<code>lr_scheduler</code> is a <code>torch::lr_scheduler</code> or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_checkpoint_epochs">checkpoint_epochs</code></td>
<td>
<p>checkpoint model weights and architecture every
<code>checkpoint_epochs</code>. (default is 10). This may cause large memory usage.
Use <code>0</code> to disable checkpoints.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_cat_emb_dim">cat_emb_dim</code></td>
<td>
<p>Size of the embedding of categorical features. If int, all categorical
features will have same embedding size, if list of int, every corresponding feature will have
specific embedding size.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_num_independent">num_independent</code></td>
<td>
<p>Number of independent Gated Linear Units layers at each step of the encoder.
Usual values range from 1 to 5.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_num_shared">num_shared</code></td>
<td>
<p>Number of shared Gated Linear Units at each step of the encoder. Usual values
at each step of the decoder. range from 1 to 5</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_num_independent_decoder">num_independent_decoder</code></td>
<td>
<p>For pretraining, number of independent Gated Linear Units layers
Usual values range from 1 to 5.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_num_shared_decoder">num_shared_decoder</code></td>
<td>
<p>For pretraining, number of shared Gated Linear Units at each step of the
decoder. Usual values range from 1 to 5.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_momentum">momentum</code></td>
<td>
<p>Momentum for batch normalization, typically ranges from 0.01
to 0.4 (default=0.02)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_pretraining_ratio">pretraining_ratio</code></td>
<td>
<p>Ratio of features to mask for reconstruction during
pretraining.  Ranges from 0 to 1 (default=0.5)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_verbose">verbose</code></td>
<td>
<p>(logical) Whether to print progress and loss values during
training.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_device">device</code></td>
<td>
<p>the device to use for training. &quot;cpu&quot; or &quot;cuda&quot;. The default (&quot;auto&quot;)
uses  to &quot;cuda&quot; if it's available, otherwise uses &quot;cpu&quot;.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_importance_sample_size">importance_sample_size</code></td>
<td>
<p>sample of the dataset to compute importance metrics.
If the dataset is larger than 1e5 obs we will use a sample of size 1e5 and
display a warning.</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_early_stopping_monitor">early_stopping_monitor</code></td>
<td>
<p>Metric to monitor for early_stopping. One of &quot;valid_loss&quot;, &quot;train_loss&quot; or &quot;auto&quot; (defaults to &quot;auto&quot;).</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_early_stopping_tolerance">early_stopping_tolerance</code></td>
<td>
<p>Minimum relative improvement to reset the patience counter.
0.01 for 1% tolerance (default 0)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_early_stopping_patience">early_stopping_patience</code></td>
<td>
<p>Number of epochs without improving until stopping training. (default=5)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_num_workers">num_workers</code></td>
<td>
<p>(int, optional): how many subprocesses to use for data
loading. 0 means that the data will be loaded in the main process.
(default: <code>0</code>)</p>
</td></tr>
<tr><td><code id="tabnet_config_+3A_skip_importance">skip_importance</code></td>
<td>
<p>if feature importance calculation should be skipped (default: <code>FALSE</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list with all hyperparameters of the TabNet implementation.
</p>

<hr>
<h2 id='tabnet_explain'>Interpretation metrics from a TabNet model</h2><span id='topic+tabnet_explain'></span><span id='topic+tabnet_explain.default'></span><span id='topic+tabnet_explain.tabnet_fit'></span><span id='topic+tabnet_explain.tabnet_pretrain'></span><span id='topic+tabnet_explain.model_fit'></span>

<h3>Description</h3>

<p>Interpretation metrics from a TabNet model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tabnet_explain(object, new_data)

## Default S3 method:
tabnet_explain(object, new_data)

## S3 method for class 'tabnet_fit'
tabnet_explain(object, new_data)

## S3 method for class 'tabnet_pretrain'
tabnet_explain(object, new_data)

## S3 method for class 'model_fit'
tabnet_explain(object, new_data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tabnet_explain_+3A_object">object</code></td>
<td>
<p>a TabNet fit object</p>
</td></tr>
<tr><td><code id="tabnet_explain_+3A_new_data">new_data</code></td>
<td>
<p>a data.frame to obtain interpretation metrics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with
</p>

<ul>
<li> <p><code>M_explain</code>: the aggregated feature importance masks as detailed in
TabNet's paper.
</p>
</li>
<li> <p><code>masks</code> a list containing the masks for each step.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>

set.seed(2021)

n &lt;- 1000
x &lt;- data.frame(
  x = rnorm(n),
  y = rnorm(n),
  z = rnorm(n)
)

y &lt;- x$x

fit &lt;- tabnet_fit(x, y, epochs = 20,
                  num_steps = 1,
                  batch_size = 512,
                  attention_width = 1,
                  num_shared = 1,
                  num_independent = 1)


 ex &lt;- tabnet_explain(fit, x)


</code></pre>

<hr>
<h2 id='tabnet_fit'>Tabnet model</h2><span id='topic+tabnet_fit'></span><span id='topic+tabnet_fit.default'></span><span id='topic+tabnet_fit.data.frame'></span><span id='topic+tabnet_fit.formula'></span><span id='topic+tabnet_fit.recipe'></span><span id='topic+tabnet_fit.Node'></span>

<h3>Description</h3>

<p>Fits the <a href="https://arxiv.org/abs/1908.07442">TabNet: Attentive Interpretable Tabular Learning</a> model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tabnet_fit(x, ...)

## Default S3 method:
tabnet_fit(x, ...)

## S3 method for class 'data.frame'
tabnet_fit(
  x,
  y,
  tabnet_model = NULL,
  config = tabnet_config(),
  ...,
  from_epoch = NULL
)

## S3 method for class 'formula'
tabnet_fit(
  formula,
  data,
  tabnet_model = NULL,
  config = tabnet_config(),
  ...,
  from_epoch = NULL
)

## S3 method for class 'recipe'
tabnet_fit(
  x,
  data,
  tabnet_model = NULL,
  config = tabnet_config(),
  ...,
  from_epoch = NULL
)

## S3 method for class 'Node'
tabnet_fit(
  x,
  tabnet_model = NULL,
  config = tabnet_config(),
  ...,
  from_epoch = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tabnet_fit_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>

<ul>
<li><p> A <strong>data frame</strong> of predictors.
</p>
</li>
<li><p> A <strong>matrix</strong> of predictors.
</p>
</li>
<li><p> A <strong>recipe</strong> specifying a set of preprocessing steps
created from <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>.
</p>
</li></ul>

<p>The predictor data should be standardized (e.g. centered or scaled).
The model treats categorical predictors internally thus, you don't need to
make any treatment.</p>
</td></tr>
<tr><td><code id="tabnet_fit_+3A_...">...</code></td>
<td>
<p>Model hyperparameters.
Any hyperparameters set here will update those set by the config argument.
See <code><a href="#topic+tabnet_config">tabnet_config()</a></code> for a list of all possible hyperparameters.</p>
</td></tr>
<tr><td><code id="tabnet_fit_+3A_y">y</code></td>
<td>
<p>When <code>x</code> is a <strong>data frame</strong> or <strong>matrix</strong>, <code>y</code> is the outcome
specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> with 1 or many numeric column (regression) or 1 or many categorical columns (classification) .
</p>
</li>
<li><p> A <strong>matrix</strong> with 1 column.
</p>
</li>
<li><p> A <strong>vector</strong>, either numeric or categorical.
</p>
</li></ul>
</td></tr>
<tr><td><code id="tabnet_fit_+3A_tabnet_model">tabnet_model</code></td>
<td>
<p>A previously fitted TabNet model object to continue the fitting on.
if <code>NULL</code> (the default) a brand new model is initialized.</p>
</td></tr>
<tr><td><code id="tabnet_fit_+3A_config">config</code></td>
<td>
<p>A set of hyperparameters created using the <code>tabnet_config</code> function.
If no argument is supplied, this will use the default values in <code><a href="#topic+tabnet_config">tabnet_config()</a></code>.</p>
</td></tr>
<tr><td><code id="tabnet_fit_+3A_from_epoch">from_epoch</code></td>
<td>
<p>When a <code>tabnet_model</code> is provided, restore the network weights from a specific epoch.
Default is last available checkpoint for restored model, or last epoch for in-memory model.</p>
</td></tr>
<tr><td><code id="tabnet_fit_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="tabnet_fit_+3A_data">data</code></td>
<td>
<p>When a <strong>recipe</strong> or <strong>formula</strong> is used, <code>data</code> is specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> containing both the predictors and the outcome.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A TabNet model object. It can be used for serialization, predictions, or further fitting.
</p>


<h3>Fitting a pre-trained model</h3>

<p>When providing a parent <code>tabnet_model</code> parameter, the model fitting resumes from that model weights
at the following epoch:
</p>

<ul>
<li><p> last fitted epoch for a model already in torch context
</p>
</li>
<li><p> Last model checkpoint epoch for a model loaded from file
</p>
</li>
<li><p> the epoch related to a checkpoint matching or preceding the <code>from_epoch</code> value if provided
The model fitting metrics append on top of the parent metrics in the returned TabNet model.
</p>
</li></ul>



<h3>Multi-outcome</h3>

<p>TabNet allows multi-outcome prediction, which is usually named <a href="https://en.wikipedia.org/wiki/Multi-label_classification">multi-label classification</a>
or multi-output classification when outcomes are categorical.
Multi-outcome currently expect outcomes to be either all numeric or all categorical.
</p>


<h3>Threading</h3>

<p>TabNet uses <code>torch</code> as its backend for computation and <code>torch</code> uses all
available threads by default.
</p>
<p>You can control the number of threads used by <code>torch</code> with:
</p>
<div class="sourceCode"><pre>torch::torch_set_num_threads(1)
torch::torch_set_num_interop_threads(1)
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>

data("ames", package = "modeldata")
data("attrition", package = "modeldata")
ids &lt;- sample(nrow(attrition), 256)

## Single-outcome regression using formula specification
fit &lt;- tabnet_fit(Sale_Price ~ ., data = ames, epochs = 1)

## Single-outcome classification using data-frame specification
attrition_x &lt;- attrition[,-which(names(attrition) == "Attrition")]
fit &lt;- tabnet_fit(attrition_x, attrition$Attrition, epochs = 1, verbose = TRUE)

## Multi-outcome regression on `Sale_Price` and `Pool_Area` in `ames` dataset using formula,
ames_fit &lt;- tabnet_fit(Sale_Price + Pool_Area ~ ., data = ames[ids,], epochs = 2, valid_split = 0.2)

## Multi-label classification on `Attrition` and `JobSatisfaction` in
## `attrition` dataset using recipe
library(recipes)
rec &lt;- recipe(Attrition + JobSatisfaction ~ ., data = attrition[ids,]) %&gt;%
  step_normalize(all_numeric(), -all_outcomes())

attrition_fit &lt;- tabnet_fit(rec, data = attrition[ids,], epochs = 2, valid_split = 0.2)

## Hierarchical classification on  `acme`
data(acme, package = "data.tree")

acme_fit &lt;- tabnet_fit(acme, epochs = 2, verbose = TRUE)

# Note: Dataset number of rows and model number of epochs should be increased
# for publication-level results.

</code></pre>

<hr>
<h2 id='tabnet_nn'>TabNet Model Architecture</h2><span id='topic+tabnet_nn'></span>

<h3>Description</h3>

<p>This is a <code>nn_module</code> representing the TabNet architecture from
<a href="https://arxiv.org/abs/1908.07442">Attentive Interpretable Tabular Deep Learning</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tabnet_nn(
  input_dim,
  output_dim,
  n_d = 8,
  n_a = 8,
  n_steps = 3,
  gamma = 1.3,
  cat_idxs = c(),
  cat_dims = c(),
  cat_emb_dim = 1,
  n_independent = 2,
  n_shared = 2,
  epsilon = 1e-15,
  virtual_batch_size = 128,
  momentum = 0.02,
  mask_type = "sparsemax"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tabnet_nn_+3A_input_dim">input_dim</code></td>
<td>
<p>Initial number of features.</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_output_dim">output_dim</code></td>
<td>
<p>Dimension of network output examples : one for regression, 2 for
binary classification etc.. Vector of those dimensions in case of multi-output.</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_n_d">n_d</code></td>
<td>
<p>Dimension of the prediction  layer (usually between 4 and 64).</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_n_a">n_a</code></td>
<td>
<p>Dimension of the attention  layer (usually between 4 and 64).</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_n_steps">n_steps</code></td>
<td>
<p>Number of successive steps in the network (usually between 3 and 10).</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_gamma">gamma</code></td>
<td>
<p>Float above 1, scaling factor for attention updates (usually between 1 and 2).</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_cat_idxs">cat_idxs</code></td>
<td>
<p>Index of each categorical column in the dataset.</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_cat_dims">cat_dims</code></td>
<td>
<p>Number of categories in each categorical column.</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_cat_emb_dim">cat_emb_dim</code></td>
<td>
<p>Size of the embedding of categorical features if int, all categorical
features will have same embedding size if list of int, every corresponding feature will have
specific size.</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_n_independent">n_independent</code></td>
<td>
<p>Number of independent GLU layer in each GLU block of the encoder.</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_n_shared">n_shared</code></td>
<td>
<p>Number of independent GLU layer in each GLU block of the encoder.</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_epsilon">epsilon</code></td>
<td>
<p>Avoid log(0), this should be kept very low.</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_virtual_batch_size">virtual_batch_size</code></td>
<td>
<p>Batch size for Ghost Batch Normalization.</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_momentum">momentum</code></td>
<td>
<p>Float value between 0 and 1 which will be used for momentum in all batch norm.</p>
</td></tr>
<tr><td><code id="tabnet_nn_+3A_mask_type">mask_type</code></td>
<td>
<p>Either &quot;sparsemax&quot; or &quot;entmax&quot; : this is the masking function to use.</p>
</td></tr>
</table>

<hr>
<h2 id='tabnet_pretrain'>Tabnet model</h2><span id='topic+tabnet_pretrain'></span><span id='topic+tabnet_pretrain.default'></span><span id='topic+tabnet_pretrain.data.frame'></span><span id='topic+tabnet_pretrain.formula'></span><span id='topic+tabnet_pretrain.recipe'></span><span id='topic+tabnet_pretrain.Node'></span>

<h3>Description</h3>

<p>Pretrain the <a href="https://arxiv.org/abs/1908.07442">TabNet: Attentive Interpretable Tabular Learning</a> model
on the predictor data exclusively (unsupervised training).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tabnet_pretrain(x, ...)

## Default S3 method:
tabnet_pretrain(x, ...)

## S3 method for class 'data.frame'
tabnet_pretrain(
  x,
  y,
  tabnet_model = NULL,
  config = tabnet_config(),
  ...,
  from_epoch = NULL
)

## S3 method for class 'formula'
tabnet_pretrain(
  formula,
  data,
  tabnet_model = NULL,
  config = tabnet_config(),
  ...,
  from_epoch = NULL
)

## S3 method for class 'recipe'
tabnet_pretrain(
  x,
  data,
  tabnet_model = NULL,
  config = tabnet_config(),
  ...,
  from_epoch = NULL
)

## S3 method for class 'Node'
tabnet_pretrain(
  x,
  tabnet_model = NULL,
  config = tabnet_config(),
  ...,
  from_epoch = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tabnet_pretrain_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>

<ul>
<li><p> A <strong>data frame</strong> of predictors.
</p>
</li>
<li><p> A <strong>matrix</strong> of predictors.
</p>
</li>
<li><p> A <strong>recipe</strong> specifying a set of preprocessing steps
created from <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>.
</p>
</li></ul>

<p>The predictor data should be standardized (e.g. centered or scaled).
The model treats categorical predictors internally thus, you don't need to
make any treatment.</p>
</td></tr>
<tr><td><code id="tabnet_pretrain_+3A_...">...</code></td>
<td>
<p>Model hyperparameters.
Any hyperparameters set here will update those set by the config argument.
See <code><a href="#topic+tabnet_config">tabnet_config()</a></code> for a list of all possible hyperparameters.</p>
</td></tr>
<tr><td><code id="tabnet_pretrain_+3A_y">y</code></td>
<td>
<p>(optional) When <code>x</code> is a <strong>data frame</strong> or <strong>matrix</strong>, <code>y</code> is the outcome</p>
</td></tr>
<tr><td><code id="tabnet_pretrain_+3A_tabnet_model">tabnet_model</code></td>
<td>
<p>A pretrained TabNet model object to continue the fitting on.
if <code>NULL</code> (the default) a brand new model is initialized.</p>
</td></tr>
<tr><td><code id="tabnet_pretrain_+3A_config">config</code></td>
<td>
<p>A set of hyperparameters created using the <code>tabnet_config</code> function.
If no argument is supplied, this will use the default values in <code><a href="#topic+tabnet_config">tabnet_config()</a></code>.</p>
</td></tr>
<tr><td><code id="tabnet_pretrain_+3A_from_epoch">from_epoch</code></td>
<td>
<p>When a <code>tabnet_model</code> is provided, restore the network weights from a specific epoch.
Default is last available checkpoint for restored model, or last epoch for in-memory model.</p>
</td></tr>
<tr><td><code id="tabnet_pretrain_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.</p>
</td></tr>
<tr><td><code id="tabnet_pretrain_+3A_data">data</code></td>
<td>
<p>When a <strong>recipe</strong> or <strong>formula</strong> is used, <code>data</code> is specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> containing both the predictors and the outcome.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A TabNet model object. It can be used for serialization, predictions, or further fitting.
</p>


<h3>outcome</h3>

<p>Outcome value are accepted here only for consistent syntax with <code>tabnet_fit</code>, but
by design the outcome, if present, is ignored during pre-training.
</p>


<h3>pre-training from a previous model</h3>

<p>When providing a parent <code>tabnet_model</code> parameter, the model pretraining resumes from that model weights
at the following epoch:
</p>

<ul>
<li><p> last pretrained epoch for a model already in torch context
</p>
</li>
<li><p> Last model checkpoint epoch for a model loaded from file
</p>
</li>
<li><p> the epoch related to a checkpoint matching or preceding the <code>from_epoch</code> value if provided
The model pretraining metrics append on top of the parent metrics in the returned TabNet model.
</p>
</li></ul>



<h3>Threading</h3>

<p>TabNet uses <code>torch</code> as its backend for computation and <code>torch</code> uses all
available threads by default.
</p>
<p>You can control the number of threads used by <code>torch</code> with:
</p>
<div class="sourceCode"><pre>torch::torch_set_num_threads(1)
torch::torch_set_num_interop_threads(1)
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>
data("ames", package = "modeldata")
pretrained &lt;- tabnet_pretrain(Sale_Price ~ ., data = ames, epochs = 1)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
