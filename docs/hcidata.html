<!DOCTYPE html><html lang="en-US"><head><title>Help for package hcidata</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {hcidata}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#hcidata-package'><p>hcidata: HCI Datasets</p></a></li>
<li><a href='#AgencyUX'><p>Sense of Agency and User Experience</p></a></li>
<li><a href='#CasualSteering'><p>Casual Interaction Steering Study</p></a></li>
<li><a href='#HafniaHands'><p>Hafnia Hands study on presence with different hand textures in virtual reality</p></a></li>
<li><a href='#HandSize'><p>Touch Performance by Hand Size</p></a></li>
<li><a href='#OccludedInteraction'><p>Acquisition study for occluded interaction</p></a></li>
<li><a href='#VrPointing'><p>Pointing in Virtual Reality</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>HCI Datasets</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Description:</td>
<td>A collection of datasets of human-computer interaction (HCI) experiments. 
    Each dataset is from an HCI paper, with all fields described and the original publication 
    linked. All paper authors of included data have consented to the inclusion of their data 
    in this package. The datasets include data from a range of HCI studies, such as pointing 
    tasks, user experience ratings, and steering tasks. 
    Dataset sources: 
    Bergström et al. (2022) &lt;<a href="https://doi.org/10.1145%2F3490493">doi:10.1145/3490493</a>&gt;;
    Dalsgaard et al. (2021) &lt;<a href="https://doi.org/10.1145%2F3489849.3489853">doi:10.1145/3489849.3489853</a>&gt;;
    Larsen et al. (2019) &lt;<a href="https://doi.org/10.1145%2F3338286.3340115">doi:10.1145/3338286.3340115</a>&gt;;
    Lilija et al. (2019) &lt;<a href="https://doi.org/10.1145%2F3290605.3300676">doi:10.1145/3290605.3300676</a>&gt;;
    Pohl and Murray-Smith (2013) &lt;<a href="https://doi.org/10.1145%2F2470654.2481307">doi:10.1145/2470654.2481307</a>&gt;;
    Pohl and Mottelson (2022) &lt;<a href="https://doi.org/10.3389%2Ffrvir.2022.719506">doi:10.3389/frvir.2022.719506</a>&gt;.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://creativecommons.org/licenses/by/4.0">CC BY 4.0</a></td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/henningpohl/hcidata">https://github.com/henningpohl/hcidata</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/henningpohl/hcidata/issues">https://github.com/henningpohl/hcidata/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-24 20:55:12 UTC; Henning</td>
</tr>
<tr>
<td>Author:</td>
<td>Henning Pohl <a href="https://orcid.org/0000-0002-1420-4309"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Henning Pohl &lt;HenningPohl@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-12-05 14:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='hcidata-package'>hcidata: HCI Datasets</h2><span id='topic+hcidata-package'></span><span id='topic+_PACKAGE'></span><span id='topic+hcidata'></span>

<h3>Description</h3>

<p>A collection of datasets of human-computer interaction (HCI) experiments. Each dataset is from an HCI paper, with all fields described and the original publication linked. All paper authors of included data have consented to the inclusion of their data in this package. The datasets include data from a range of HCI studies, such as pointing tasks, user experience ratings, and steering tasks. Dataset sources: Bergström et al. (2022) <a href="https://doi.org/10.1145/3490493">doi:10.1145/3490493</a>; Dalsgaard et al. (2021) <a href="https://doi.org/10.1145/3489849.3489853">doi:10.1145/3489849.3489853</a>; Larsen et al. (2019) <a href="https://doi.org/10.1145/3338286.3340115">doi:10.1145/3338286.3340115</a>; Lilija et al. (2019) <a href="https://doi.org/10.1145/3290605.3300676">doi:10.1145/3290605.3300676</a>; Pohl and Murray-Smith (2013) <a href="https://doi.org/10.1145/2470654.2481307">doi:10.1145/2470654.2481307</a>; Pohl and Mottelson (2022) <a href="https://doi.org/10.3389/frvir.2022.719506">doi:10.3389/frvir.2022.719506</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Henning Pohl <a href="mailto:HenningPohl@gmail.com">HenningPohl@gmail.com</a> (<a href="https://orcid.org/0000-0002-1420-4309">ORCID</a>)
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/henningpohl/hcidata">https://github.com/henningpohl/hcidata</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/henningpohl/hcidata/issues">https://github.com/henningpohl/hcidata/issues</a>
</p>
</li></ul>


<hr>
<h2 id='AgencyUX'>Sense of Agency and User Experience</h2><span id='topic+AgencyUX'></span>

<h3>Description</h3>

<p>Aggregated data from an experiment where participants used three
different means of input to control a game. As established in previous work,
the three means of input vary in objective sense of agency. This study collected
subjective measures of agency, as well as subjective measures of user experiences
for comparison.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AgencyUX
</code></pre>


<h3>Format</h3>

<p>A data frame with 126 observations of the following 26 variables:
</p>

<dl>
<dt>ID</dt><dd><p>Participant ID.</p>
</dd>
<dt>Age</dt><dd><p>Participant age.</p>
</dd>
<dt>Gender</dt><dd><p>Participants' self-reported gender.</p>
</dd>
<dt>Condition</dt><dd><p>Which device was used (either touchpad, on-skin tapping, or button).</p>
</dd>
<dt>AttDiff1</dt><dd><p>AttrakDiff (pragmatic) = &quot;I found the device: confusing&ndash;structured&quot; (7 point scale).</p>
</dd>
<dt>AttDiff2</dt><dd><p>AttrakDiff (pragmatic) = &quot;I found the device: impractical&ndash;practical&quot; (7 point scale).</p>
</dd>
<dt>AttDiff3</dt><dd><p>AttrakDiff (pragmatic) = &quot;I found the device: complicated&ndash;simple&quot; (7 point scale).</p>
</dd>
<dt>AttDiff4</dt><dd><p>AttrakDiff (pragmatic) = &quot;I found the device: unpredictable&ndash;predictable&quot; (7 point scale).</p>
</dd>
<dt>AttDiff5</dt><dd><p>AttrakDiff (hedonic) = &quot;I found the device: dull&ndash;captivating&quot; (7 point scale).</p>
</dd>
<dt>AttDiff6</dt><dd><p>AttrakDiff (hedonic) = &quot;I found the device: tacky&ndash;stylish&quot; (7 point scale).</p>
</dd>
<dt>AttDiff7</dt><dd><p>AttrakDiff (hedonic) = &quot;I found the device: cheap&ndash;premium&quot; (7 point scale).</p>
</dd>
<dt>AttDiff8</dt><dd><p>AttrakDiff (hedonic) = &quot;I found the device: unimaginative&ndash;creative&quot; (7 point scale).</p>
</dd>
<dt>Umux1</dt><dd><p>UMUX-LITE 1 = &quot;This system's capabilities meet my requirements: strongly disagree&ndash;strongly agree&quot; (7 point scale).</p>
</dd>
<dt>Umux2</dt><dd><p>UMUX-LITE 2 = &quot;This system is easy to use: strongly disagree&ndash;strongly agree&quot; (7 point scale).</p>
</dd>
<dt>NASA1</dt><dd><p>NASA-TLX (mental demand) = &quot;How mentally demanding was the task? low&ndash;high&quot; (21 point scale).</p>
</dd>
<dt>NASA2</dt><dd><p>NASA-TLX (physical demand) = &quot;How physically demanding was the task? low&ndash;high&quot; (21 point scale).</p>
</dd>
<dt>NASA3</dt><dd><p>NASA-TLX (temporal demand) = &quot;How hurried or rushed was the pace of the task? low&ndash;high&quot; (21 point scale).</p>
</dd>
<dt>NASA4</dt><dd><p>NASA-TLX (performance) = &quot;How successful were you in accomplishing what you were asked to do? low&ndash;high&quot; (21 point scale).</p>
</dd>
<dt>NASA5</dt><dd><p>NASA-TLX (effort) = &quot;How hard did you have to work to accomplish your level of performance? low&ndash;high&quot; (21 point scale).</p>
</dd>
<dt>NASA6</dt><dd><p>NASA-TLX (frustration) = &quot;How insecure, discouraged, irritated, stressed, and annoyed were you? low&ndash;high&quot; (21 point scale).</p>
</dd>
<dt>Ownership</dt><dd><p>Body Ownership = &quot;It felt like the device I was using was part of my body: strongly disagree&ndash;strongly agree&quot; (7 point scale).</p>
</dd>
<dt>Agency1</dt><dd><p>Agency = &quot;It felt like I was in control of the movements during the task: strongly disagree&ndash;strongly agree&quot; (7 point scale).</p>
</dd>
<dt>Agency2</dt><dd><p>Agency = &quot;What is the degree of control you felt? lowest&ndash;highest&quot; (7 point scale).</p>
</dd>
<dt>Agency3</dt><dd><p>Agency = &quot;Indicate how much it felt like pressing/tapping the button/touchpad/arm caused the space craft to shot: not at all&ndash;very much&quot; (7 point scale).</p>
</dd>
<dt>TimePerception</dt><dd><p>Perception of task duration in seconds.</p>
</dd>
<dt>HitRate</dt><dd><p>Hit percentage participants achieved when playing the game.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Bergström J, Knibbe J, Pohl H, Hornbæk K (2022).
&ldquo;Sense of Agency and User Experience: Is There a Link?&rdquo;
<em>ACM Trans. Comput.-Hum. Interact.</em>, <b>29</b>(4).
ISSN 1073-0516, <a href="https://doi.org/10.1145/3490493">doi:10.1145/3490493</a>.
</p>

<hr>
<h2 id='CasualSteering'>Casual Interaction Steering Study</h2><span id='topic+CasualSteering'></span>

<h3>Description</h3>

<p>Data from a study on casual interactions where participants
had to move a ball from one side of a level to the other. They could use
three different kinds of interaction to control the ball: (1) dragging via 
(2) direct touch, rate-controlled movement via hovering, and (3) fling
gestures above the device. Depending on the levels' index of difficulty,
the participants picked different interactions to solve the levels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CasualSteering
</code></pre>


<h3>Format</h3>

<p>A data frame with 84 observations of the following 6 variables:
</p>

<dl>
<dt>PID</dt><dd><p>Participant ID.</p>
</dd>
<dt>level</dt><dd><p>Level ID.</p>
</dd>
<dt>difficulty</dt><dd><p>Index of difficulty of the level.</p>
</dd>
<dt>touch</dt><dd><p>Percentage share of touch interactions.</p>
</dd>
<dt>hover</dt><dd><p>Percentage share of hover interactions.</p>
</dd>
<dt>gestures</dt><dd><p>Number of mid-air gestures performed by the participant.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Pohl H, Murray-Smith R (2013).
&ldquo;Focused and Casual Interactions: Allowing Users to Vary Their Level of Engagement.&rdquo;
In <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>,  CHI '13, 2223–2232.
ISBN 9781450318990, <a href="https://doi.org/10.1145/2470654.2481307">doi:10.1145/2470654.2481307</a>.
</p>


<h3>See Also</h3>

<p>Other mobile interaction: 
<code><a href="#topic+HandSize">HandSize</a></code>
</p>

<hr>
<h2 id='HafniaHands'>Hafnia Hands study on presence with different hand textures in virtual reality</h2><span id='topic+HafniaHands'></span>

<h3>Description</h3>

<p>Data from a remote VR study where participants were tasked to keep their
hand within boxes moving in front of them. They did so with three different textures
for their hands: (1) green alien hands, (2) hands in their own skin tone, and (3) hands
in a mismatched skin tone. After each trial, participants gave ratings on presence and
the look of the hands.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HafniaHands
</code></pre>


<h3>Format</h3>

<p>A list with two entries:
</p>
<p><strong>participants</strong> with 5 fields for 112 study participants:
</p>

<dl>
<dt>pid</dt><dd><p>Participant ID.</p>
</dd>
<dt>Age</dt><dd><p>Participant age.</p>
</dd>
<dt>Sex</dt><dd><p>Participants' self-reported sex.</p>
</dd>
<dt>VR Experience</dt><dd><p>Participants' amount of experience with VR.</p>
</dd>
<dt>Skin Tone</dt><dd><p>Participants' skin tone on the Fitzpatrick scale.</p>
</dd>
</dl>

<p><strong>responses</strong> with 9072 entries in 5 fields:
</p>

<dl>
<dt>pid</dt><dd><p>Participant ID.</p>
</dd>
<dt>Trial</dt><dd><p>Trial number.</p>
</dd>
<dt>Condition</dt><dd><p>Trial condition: Alien Hand, Matched Hand, or Mismatched Hand</p>
</dd>
<dt>Measure</dt><dd><p>Questionnaire item, which is one of: </p>

<dl>
<dt>Agency</dt><dd><p>&quot;I felt that the movements of the virtual hands were caused by my own movements&quot; (Banakou and Slater 2014)</p>
</dd>
<dt>Body Ownership</dt><dd><p>&quot;I felt that the virtual hands I saw were my own hands&quot; (Banakou and Slater 2014)</p>
</dd>
<dt>Resemblance</dt><dd><p>&quot;I felt that my virtual hands resembled my own (real) hands in terms of shape, skin tone, or other visual features&quot; (Banakou and Slater 2014)</p>
</dd>
<dt>HQ0</dt><dd><p>&quot;Please rate the hands based on the opposing adjectives: Inanimate to Living&quot; (Ho and MacDorman 2017)</p>
</dd>
<dt>HQ1</dt><dd><p>&quot;Please rate the hands based on the opposing adjectives: Synthetic to Real&quot; (Ho and MacDorman 2017)</p>
</dd>
<dt>HQ2</dt><dd><p>&quot;Please rate the hands based on the opposing adjectives: Mechanical movement to Biological movement&quot; (Ho and MacDorman 2017)</p>
</dd>
<dt>HQ3</dt><dd><p>&quot;Please rate the hands based on the opposing adjectives: Human-made to Human-like&quot; (Ho and MacDorman 2017)</p>
</dd>
<dt>HQ4</dt><dd><p>&quot;Please rate the hands based on the opposing adjectives: Without definite lifespan to Mortal&quot; (Ho and MacDorman 2017)</p>
</dd>
<dt>Humanness</dt><dd><p>Aggregate of HQ0-HQ4</p>
</dd>
</dl>
</dd>
<dt>Response</dt><dd><p>Participants' response on 7-point (-3 to 3) scale. For Humanness this is the average of HQ0-HQ4.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Pohl H, Mottelson A (2022).
&ldquo;Hafnia Hands: A Multi-Skin Hand Texture Resource for Virtual Reality Research.&rdquo;
<em>Frontiers in Virtual Reality</em>, <b>3</b>.
ISSN 2673-4192, <a href="https://doi.org/10.3389/frvir.2022.719506">doi:10.3389/frvir.2022.719506</a>.
</p>


<h3>See Also</h3>

<p>Other virtual reality: 
<code><a href="#topic+VrPointing">VrPointing</a></code>
</p>

<hr>
<h2 id='HandSize'>Touch Performance by Hand Size</h2><span id='topic+HandSize'></span>

<h3>Description</h3>

<p>Data from a study on the influence of hand size on touch accuracy. 
Contains hand measurements for 27 participants, information on the two phones
used in the study, and 27000 recorded touch samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HandSize
</code></pre>


<h3>Format</h3>

<p>A list with three entries:
</p>
<p><strong>participants</strong> with 16 fields for 27 study participants:
</p>

<dl>
<dt>PID</dt><dd><p>Participant ID.</p>
</dd>
<dt>Age</dt><dd><p>Participant age.</p>
</dd>
<dt>Gender</dt><dd><p>Participants' self-reported gender.</p>
</dd>
<dt>Phone</dt><dd><p>Participants' personal phone.</p>
</dd>
<dt>Handedness</dt><dd><p>Participants' dominant hand.</p>
</dd>
<dt>ThumbLength</dt><dd><p>Length of thumb in cm.</p>
</dd>
<dt>IndexLength</dt><dd><p>Length of index finger in cm.</p>
</dd>
<dt>MiddleLength</dt><dd><p>Length of middle finger in cm.</p>
</dd>
<dt>RingLength</dt><dd><p>Length of ring finger in cm.</p>
</dd>
<dt>PinkyLength</dt><dd><p>Length of pinky finger in cm.</p>
</dd>
<dt>ThumbPadWidth</dt><dd><p>Width of thumb pad in cm.</p>
</dd>
<dt>PalmWidth</dt><dd><p>Width of palm in cm.</p>
</dd>
<dt>PalmLength</dt><dd><p>Length of palm in cm.</p>
</dd>
<dt>IndexThumbLength</dt><dd><p>Distance from index finger tip to base of thumb in cm.</p>
</dd>
<dt>ThumbIndexSpan</dt><dd><p>Distance from thumb tip to index finger tip when hand is spread open in cm.</p>
</dd>
<dt>ThumbPinkySpan</dt><dd><p>Distance from thumb tip to pinky tip when hand is spread open in cm.</p>
</dd>
</dl>

<p><strong>devices</strong> with 5 fields:
</p>

<dl>
<dt>Phone</dt><dd><p>Phone used (Android or iPhone).</p>
</dd>
<dt>ScreenWidth</dt><dd><p>Width of screen in px.</p>
</dd>
<dt>ScreenHeight</dt><dd><p>Height of screen in px.</p>
</dd>
<dt>ScreenDpiX</dt><dd><p>Horizontal screen dpi.</p>
</dd>
<dt>ScreenDpiY</dt><dd><p>Vertical screen dpi.</p>
</dd>
</dl>

<p><strong>data</strong> with 27000 observations in 10 fields:
</p>

<dl>
<dt>PID</dt><dd><p>Participant ID.</p>
</dd>
<dt>Trial</dt><dd><p>Trial number.</p>
</dd>
<dt>Phone</dt><dd><p>Phone used (Android or iPhone).</p>
</dd>
<dt>TouchX</dt><dd><p>Horizontal touch position in pixel.</p>
</dd>
<dt>TouchY</dt><dd><p>Vertical touch position in pixel.</p>
</dd>
<dt>TargetX</dt><dd><p>Horizontal target position in pixel.</p>
</dd>
<dt>TargetY</dt><dd><p>Vertical target position in pixel.</p>
</dd>
<dt>SelectionTimeSeconds</dt><dd><p>Time it took to make the selection in seconds.</p>
</dd>
<dt>ErrorMM</dt><dd><p>Offset from the target position in mm.</p>
</dd>
<dt>Outlier</dt><dd><p>Whether this trial is considered an outlier because selection happened too fast or slow.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Larsen JN, Jacobsen TH, Boring S, Bergström J, Pohl H (2019).
&ldquo;The Influence of Hand Size on Touch Accuracy.&rdquo;
In <em>Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services</em>,  MobileHCI '19.
ISBN 9781450368254, <a href="https://doi.org/10.1145/3338286.3340115">doi:10.1145/3338286.3340115</a>.
</p>


<h3>See Also</h3>

<p>Other mobile interaction: 
<code><a href="#topic+CasualSteering">CasualSteering</a></code>
</p>

<hr>
<h2 id='OccludedInteraction'>Acquisition study for occluded interaction</h2><span id='topic+OccludedInteraction'></span>

<h3>Description</h3>

<p>In this study, participants wore an AR headset and were asked to interact with 
objects occluded by a wall in front of them. They had to reach around to then
manipulate the objects. They were supported in this task by several different
kinds of visualization that showed them the object of interest. After each trial
and at the end of the study, the participants provided ratings of each visualization
as well as a ranking of the different visualizations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>OccludedInteraction
</code></pre>


<h3>Format</h3>

<p>A list with two entries:
</p>
<p><strong>participants</strong> with 6 fields for 24 study participants:
</p>

<dl>
<dt>user</dt><dd><p>Participant ID.</p>
</dd>
<dt>age</dt><dd><p>Participant age.</p>
</dd>
<dt>gender</dt><dd><p>Participants' self-reported gender.</p>
</dd>
<dt>glasses</dt><dd><p>Whether the participant wears glasses.</p>
</dd>
<dt>handedness</dt><dd><p>Participants' dominant hand.</p>
</dd>
<dt>ar_experience</dt><dd><p>Self-reported level of experience with AR on a 5-point scale (&quot;none&quot; to &quot;a lot&quot;).</p>
</dd>
</dl>

<p><strong>ratings</strong> with 6 fields:
</p>

<dl>
<dt>user</dt><dd><p>Participant ID.</p>
</dd>
<dt>block</dt><dd><p>Block ID. 99 is used to denote final questionnaire after the study.</p>
</dd>
<dt>object</dt><dd><p>Occluded object that had to be used. Can be: button, dial, hdmi, hook, or slider.</p>
</dd>
<dt>view</dt><dd><p>Visualization available during the trial. Can be: none, static, dynamic, cloned, or see-though.</p>
</dd>
<dt>question</dt><dd><p>Which question was asked. Can be: 
</p>

<dl>
<dt>liked overall</dt><dd><p>&quot;Overall, I liked using the visualization when interacting with the object.&quot; (&quot;Strongly disagree&quot; to &quot;Strongly agree&quot;)</p>
</dd>
<dt>supported</dt><dd><p>&quot;How well did the visualization support you during the task?&quot; (&quot;Strongly impeded me&quot; to &quot;Strongly supported me&quot;)</p>
</dd>
<dt>manipulate</dt><dd><p>&quot;I could easily manipulate the object.&quot; (&quot;Strongly disagree&quot; to &quot;Strongly agree&quot;)</p>
</dd>
<dt>check state</dt><dd><p>&quot;I could easily check the state of the object.&quot; (&quot;Strongly disagree&quot; to &quot;Strongly agree&quot;)</p>
</dd>
<dt>ranking</dt><dd><p>&quot;How would you rank the five visualization with respect to how easy/hard they made it to interact with the object?&quot;</p>
</dd>
<dt>rating</dt><dd><p>&quot;Please rate each view for how well it overall supported you during the study.&quot; (&quot;Strongly impeded me&quot; to &quot;Strongly supported me&quot;)</p>
</dd>
</dl>

</dd>
<dt>rating</dt><dd><p>Value between 0 and 6 for all ratings and 0 to 4 for the rankings.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Lilija K, Pohl H, Boring S, Hornbæk K (2019).
&ldquo;Augmented Reality Views for Occluded Interaction.&rdquo;
In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>,  CHI '19, 1–12.
ISBN 9781450359702, <a href="https://doi.org/10.1145/3290605.3300676">doi:10.1145/3290605.3300676</a>.
</p>

<hr>
<h2 id='VrPointing'>Pointing in Virtual Reality</h2><span id='topic+VrPointing'></span>

<h3>Description</h3>

<p>Data from a study where participants pointed at one of 27 targets
in the space in front of them. This version contains calibration poses, participant
information, end the final pose for each pointing trial. The full dataset with all
movement within each trial is available at <a href="https://github.com/TorSalve/pointing-data-ATHCC">https://github.com/TorSalve/pointing-data-ATHCC</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VrPointing
</code></pre>


<h3>Format</h3>

<p>A list with three entries:
</p>
<p><strong>participants</strong> with 12 fields for 13 study participants:
</p>

<dl>
<dt>pid</dt><dd><p>Participant ID.</p>
</dd>
<dt>handedness</dt><dd><p>Participants' dominant hand.</p>
</dd>
<dt>gender</dt><dd><p>Participants' self-reported gender.</p>
</dd>
<dt>age</dt><dd><p>Participant age.</p>
</dd>
<dt>forearmLength</dt><dd><p>Length of forearm in meter.</p>
</dd>
<dt>forearmMarkerDist</dt><dd><p>Distance from the forearm marker to the elbow in meter.</p>
</dd>
<dt>indexFingerLength</dt><dd><p>Index finger length in meter.</p>
</dd>
<dt>upperArmLength</dt><dd><p>Length of the upper arm in meter.</p>
</dd>
<dt>upperArmMarkerDist</dt><dd><p>Distance from the upper arm marker to the elbow in meter.</p>
</dd>
<dt>height</dt><dd><p>Participant height in meter.</p>
</dd>
<dt>rightShoulderMarkerDist.X</dt><dd><p>Horizontal distance from the right shoulder marker to the participants' shoulder in meter.</p>
</dd>
<dt>rightShoulderMarkerDist.Y</dt><dd><p>Vertical distance from the right shoulder marker to the participants' shoulder in meter.</p>
</dd>
</dl>

<p><strong>calibration</strong> with 44 fields for 39 observations:
</p>

<dl>
<dt>pid</dt><dd><p>Participant ID.</p>
</dd>
<dt>pose</dt><dd><p>Calibration pose, where 1 = arms pointing down, 2 = arm pointing to the right, and 3 = arm pointing forward.</p>
</dd>
<dt>indexFinger.X, indexFinger.Y, indexFinger.Z</dt><dd><p>Index finger position in meter.</p>
</dd>
<dt>hand.X, hand.Y, hand.Z</dt><dd><p>Hand position in meter.</p>
</dd>
<dt>forearm.X, forearm.Y, forearm.Z</dt><dd><p>Forearm position in meter.</p>
</dd>
<dt>upperArm.X, upperArm.Y, upperArm.Z</dt><dd><p>Upper arm position in meter.</p>
</dd>
<dt>rightShoulder.X, rightShoulder.Y, rightShoulder.Z</dt><dd><p>Right shoulder position in meter.</p>
</dd>
<dt>hmd.X, hmd.Y, hmd.Z</dt><dd><p>Headset position in meter.</p>
</dd>
<dt>leftShoulder.X, leftShoulder.Y, leftShoulder.Z</dt><dd><p>Left Shoulder position in meter.</p>
</dd>
<dt>indexFingerO.X, indexFingerO.Y, indexFingerO.Z</dt><dd><p>Index finger orientation in radians.</p>
</dd>
<dt>handO.X, handO.Y, handO.Z</dt><dd><p>Hand orientation in radians.</p>
</dd>
<dt>forearmO.X, forearmO.Y, forearmO.Z</dt><dd><p>Forearm orientation in radians.</p>
</dd>
<dt>upperArmO.X, upperArmO.Y, upperArmO.Z</dt><dd><p>Upper arm orientation in radians.</p>
</dd>
<dt>rightShoulderO.X, rightShoulderO.Y, rightShoulderO.Z</dt><dd><p>Right shoulder orientation in radians.</p>
</dd>
<dt>hmdO.X, hmdO.Y, hmdO.Z</dt><dd><p>Headset orientation in radians.</p>
</dd>
<dt>leftShoulderO.X, leftShoulderO.Y, leftShoulderO.Z</dt><dd><p>Left shoulder orientation in radians.</p>
</dd>
</dl>

<p><strong>pointing</strong> with 48 fields for 1755 observations:
</p>

<dl>
<dt>pid</dt><dd><p>Participant ID.</p>
</dd>
<dt>trial</dt><dd><p>Trial number.</p>
</dd>
<dt>time</dt><dd><p>Time since beginning of trial in seconds.</p>
</dd>
<dt>indexFinger.X, indexFinger.Y, indexFinger.Z</dt><dd><p>Index finger position in meter.</p>
</dd>
<dt>hand.X, hand.Y, hand.Z</dt><dd><p>Hand position in meter.</p>
</dd>
<dt>forearm.X, forearm.Y, forearm.Z</dt><dd><p>Forearm position in meter.</p>
</dd>
<dt>upperArm.X, upperArm.Y, upperArm.Z</dt><dd><p>Upper arm position in meter.</p>
</dd>
<dt>rightShoulder.X, rightShoulder.Y, rightShoulder.Z</dt><dd><p>Right shoulder position in meter.</p>
</dd>
<dt>hmd.X, hmd.Y, hmd.Z</dt><dd><p>Headset position in meter.</p>
</dd>
<dt>leftShoulder.X, leftShoulder.Y, leftShoulder.Z</dt><dd><p>Left Shoulder position in meter.</p>
</dd>
<dt>indexFingerO.X, indexFingerO.Y, indexFingerO.Z</dt><dd><p>Index finger orientation in radians.</p>
</dd>
<dt>handO.X, handO.Y, handO.Z</dt><dd><p>Hand orientation in radians.</p>
</dd>
<dt>forearmO.X, forearmO.Y, forearmO.Z</dt><dd><p>Forearm orientation in radians.</p>
</dd>
<dt>upperArmO.X, upperArmO.Y, upperArmO.Z</dt><dd><p>Upper arm orientation in radians.</p>
</dd>
<dt>rightShoulderO.X, rightShoulderO.Y, rightShoulderO.Z</dt><dd><p>Right shoulder orientation in radians.</p>
</dd>
<dt>hmdO.X, hmdO.Y, hmdO.Z</dt><dd><p>Headset orientation in radians.</p>
</dd>
<dt>leftShoulderO.X, leftShoulderO.Y, leftShoulderO.Z</dt><dd><p>Left shoulder orientation in radians.</p>
</dd>
<dt>target.X, target.Y, target.Z</dt><dd><p>Target position in meter.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Dalsgaard T, Knibbe J, Bergström J (2021).
&ldquo;Modeling Pointing for 3D Target Selection in VR.&rdquo;
In <em>Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology</em>,  VRST '21.
ISBN 9781450390927, <a href="https://doi.org/10.1145/3489849.3489853">doi:10.1145/3489849.3489853</a>.
</p>


<h3>See Also</h3>

<p>Other virtual reality: 
<code><a href="#topic+HafniaHands">HafniaHands</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
