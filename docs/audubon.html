<!DOCTYPE html><html><head><title>Help for package audubon</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {audubon}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#audubon-package'><p>audubon: Japanese Text Processing Tools</p></a></li>
<li><a href='#bind_lr'><p>Bind importance of bigrams</p></a></li>
<li><a href='#bind_tf_idf2'><p>Bind term frequency and inverse document frequency</p></a></li>
<li><a href='#collapse_tokens'><p>Collapse sequences of tokens by condition</p></a></li>
<li><a href='#get_dict_features'><p>Get dictionary's features</p></a></li>
<li><a href='#hiroba'><p>Whole tokens of 'Porano no Hiroba' written by Miyazawa Kenji</p>
from Aozora Bunko</a></li>
<li><a href='#lex_density'><p>Calculate lexical density</p></a></li>
<li><a href='#mute_tokens'><p>Mute tokens by condition</p></a></li>
<li><a href='#ngram_tokenizer'><p>Ngrams tokenizer</p></a></li>
<li><a href='#pack'><p>Pack a data.frame of tokens</p></a></li>
<li><a href='#polano'><p>Whole text of 'Porano no Hiroba' written by Miyazawa Kenji</p>
from Aozora Bunko</a></li>
<li><a href='#prettify'><p>Prettify tokenized output</p></a></li>
<li><a href='#read_rewrite_def'><p>Read a rewrite.def file</p></a></li>
<li><a href='#strj_fill_iter_mark'><p>Fill Japanese iteration marks</p></a></li>
<li><a href='#strj_hiraganize'><p>Hiraganize Japanese characters</p></a></li>
<li><a href='#strj_katakanize'><p>Katakanize Japanese characters</p></a></li>
<li><a href='#strj_normalize'><p>Convert text following the rules of 'NEologd'</p></a></li>
<li><a href='#strj_rewrite_as_def'><p>Rewrite text using rewrite.def</p></a></li>
<li><a href='#strj_romanize'><p>Romanize Japanese Hiragana and Katakana</p></a></li>
<li><a href='#strj_segment'><p>Segment text into tokens</p></a></li>
<li><a href='#strj_tinyseg'><p>Segment text into phrases</p></a></li>
<li><a href='#strj_tokenize'><p>Split text into tokens</p></a></li>
<li><a href='#strj_transcribe_num'><p>Transcribe Arabic to Kansuji</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Japanese Text Processing Tools</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.1</td>
</tr>
<tr>
<td>Description:</td>
<td>A collection of Japanese text processing tools for filling
    Japanese iteration marks, Japanese character type conversions,
    segmentation by phrase, and text normalization which is based on rules
    for the 'Sudachi' morphological analyzer and the 'NEologd' (Neologism
    dictionary for 'MeCab').  These features are specific to Japanese and
    are not implemented in 'ICU' (International Components for Unicode).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/paithiov909/audubon">https://github.com/paithiov909/audubon</a>,
<a href="https://paithiov909.github.io/audubon/">https://paithiov909.github.io/audubon/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/paithiov909/audubon/issues">https://github.com/paithiov909/audubon/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr (&ge; 1.1.0), magrittr, Matrix, memoise, purrr, readr,
rlang (&ge; 0.4.11), stringi, V8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>roxygen2, spelling, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-02 15:06:27 UTC; paithiov909</td>
</tr>
<tr>
<td>Author:</td>
<td>Akiru Kato [cre, aut],
  Koki Takahashi [cph] (Author of japanese.js),
  Shuhei Iitsuka [cph] (Author of budoux),
  Taku Kudo [cph] (Author of TinySegmenter)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Akiru Kato &lt;paithiov909@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-02 19:00:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='audubon-package'>audubon: Japanese Text Processing Tools</h2><span id='topic+audubon'></span><span id='topic+audubon-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>A collection of Japanese text processing tools for filling Japanese iteration marks, Japanese character type conversions, segmentation by phrase, and text normalization which is based on rules for the 'Sudachi' morphological analyzer and the 'NEologd' (Neologism dictionary for 'MeCab'). These features are specific to Japanese and are not implemented in 'ICU' (International Components for Unicode).
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Akiru Kato <a href="mailto:paithiov909@gmail.com">paithiov909@gmail.com</a>
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Koki Takahashi (Author of japanese.js) [copyright holder]
</p>
</li>
<li><p> Shuhei Iitsuka (Author of budoux) [copyright holder]
</p>
</li>
<li><p> Taku Kudo (Author of TinySegmenter) [copyright holder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/paithiov909/audubon">https://github.com/paithiov909/audubon</a>
</p>
</li>
<li> <p><a href="https://paithiov909.github.io/audubon/">https://paithiov909.github.io/audubon/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/paithiov909/audubon/issues">https://github.com/paithiov909/audubon/issues</a>
</p>
</li></ul>


<hr>
<h2 id='bind_lr'>Bind importance of bigrams</h2><span id='topic+bind_lr'></span>

<h3>Description</h3>

<p>Calculates and binds the importance of bigrams and their synergistic average.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bind_lr(tbl, term = "token", lr_mode = c("n", "dn"), avg_rate = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bind_lr_+3A_tbl">tbl</code></td>
<td>
<p>A tidy text dataset.</p>
</td></tr>
<tr><td><code id="bind_lr_+3A_term">term</code></td>
<td>
<p>Column containing terms as string or symbol.</p>
</td></tr>
<tr><td><code id="bind_lr_+3A_lr_mode">lr_mode</code></td>
<td>
<p>Method for computing 'FL' and 'FR' values.
<code>n</code> is equivalent to 'LN' and 'RN', and <code>dn</code> is equivalent to 'LDN' and 'RDN'.</p>
</td></tr>
<tr><td><code id="bind_lr_+3A_avg_rate">avg_rate</code></td>
<td>
<p>Weight of the 'LR' value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The 'LR' value is the synergistic average of bigram importance
that based on the words and their positions (left or right side).
</p>


<h3>Value</h3>

<p>A data.frame.
</p>


<h3>See Also</h3>

<p><a href="https://doi.org/10.5715/jnlp.10.27">doi:10.5715/jnlp.10.27</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
prettify(hiroba, col_select = "POS1") |&gt;
  mute_tokens(POS1 != "\u540d\u8a5e") |&gt;
  bind_lr()

## End(Not run)
</code></pre>

<hr>
<h2 id='bind_tf_idf2'>Bind term frequency and inverse document frequency</h2><span id='topic+bind_tf_idf2'></span>

<h3>Description</h3>

<p>Calculates and binds the term frequency, inverse document frequency,
and TF-IDF of the dataset.
This function experimentally supports 3 types of term frequencies
and 4 types of inverse document frequencies,
which are implemented in 'RMeCab' package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bind_tf_idf2(
  tbl,
  term = "token",
  document = "doc_id",
  n = "n",
  tf = c("tf", "tf2", "tf3"),
  idf = c("idf", "idf2", "idf3", "idf4"),
  norm = FALSE,
  rmecab_compat = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bind_tf_idf2_+3A_tbl">tbl</code></td>
<td>
<p>A tidy text dataset.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_term">term</code></td>
<td>
<p>Column containing terms as string or symbol.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_document">document</code></td>
<td>
<p>Column containing document IDs as string or symbol.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_n">n</code></td>
<td>
<p>Column containing document-term counts as string or symbol.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_tf">tf</code></td>
<td>
<p>Method for computing term frequency.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_idf">idf</code></td>
<td>
<p>Method for computing inverse document frequency.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_norm">norm</code></td>
<td>
<p>Logical; If passed as <code>TRUE</code>, the raw term counts are normalized
being divided with L2 norms before computing IDF values.</p>
</td></tr>
<tr><td><code id="bind_tf_idf2_+3A_rmecab_compat">rmecab_compat</code></td>
<td>
<p>Logical; If passed as <code>TRUE</code>, computes values while
taking care of compatibility with 'RMeCab'.
Note that 'RMeCab' always computes IDF values using term frequency
rather than raw term counts, and thus TF-IDF values may be
doubly affected by term frequency.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Types of term frequency can be switched with <code>tf</code> argument:
</p>

<ul>
<li> <p><code>tf</code> is term frequency (not raw count of terms).
</p>
</li>
<li> <p><code>tf2</code> is logarithmic term frequency of which base is 10.
</p>
</li>
<li> <p><code>tf3</code> is binary-weighted term frequency.
</p>
</li></ul>

<p>Types of inverse document frequencies can be switched with <code>idf</code> argument:
</p>

<ul>
<li> <p><code>idf</code> is inverse document frequency of which base is 2, with smoothed.
'smoothed' here means just adding 1 to raw counts after logarithmizing.
</p>
</li>
<li> <p><code>idf2</code> is global frequency IDF.
</p>
</li>
<li> <p><code>idf3</code> is probabilistic IDF of which base is 2.
</p>
</li>
<li> <p><code>idf4</code> is global entropy, not IDF in actual.
</p>
</li></ul>



<h3>Value</h3>

<p>A data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- dplyr::add_count(hiroba, doc_id, token)
bind_tf_idf2(df)

## End(Not run)
</code></pre>

<hr>
<h2 id='collapse_tokens'>Collapse sequences of tokens by condition</h2><span id='topic+collapse_tokens'></span>

<h3>Description</h3>

<p>Concatenates sequences of tokens in the tidy text dataset,
while grouping them by an expression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>collapse_tokens(tbl, condition, .collapse = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collapse_tokens_+3A_tbl">tbl</code></td>
<td>
<p>A tidy text dataset.</p>
</td></tr>
<tr><td><code id="collapse_tokens_+3A_condition">condition</code></td>
<td>
<p>A logical expression.</p>
</td></tr>
<tr><td><code id="collapse_tokens_+3A_.collapse">.collapse</code></td>
<td>
<p>String with which tokens are concatenated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that this function drops all columns except but 'token'
and columns for grouping sequences. So, the returned data.frame
has only 'doc_id', 'sentence_id', 'token_id', and 'token' columns.
</p>


<h3>Value</h3>

<p>A data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- prettify(head(hiroba), col_select = "POS1")
collapse_tokens(df, POS1 == "\u540d\u8a5e")

## End(Not run)
</code></pre>

<hr>
<h2 id='get_dict_features'>Get dictionary's features</h2><span id='topic+get_dict_features'></span>

<h3>Description</h3>

<p>Returns dictionary's features.
Currently supports &quot;unidic17&quot; (2.1.2 src schema), &quot;unidic26&quot; (2.1.2 bin schema),
&quot;unidic29&quot; (schema used in 2.2.0, 2.3.0), &quot;cc-cedict&quot;, &quot;ko-dic&quot; (mecab-ko-dic),
&quot;naist11&quot;, &quot;sudachi&quot;, and &quot;ipa&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_dict_features(
  dict = c("ipa", "unidic17", "unidic26", "unidic29", "cc-cedict", "ko-dic", "naist11",
    "sudachi")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_dict_features_+3A_dict">dict</code></td>
<td>
<p>Character scalar; one of &quot;ipa&quot;, &quot;unidic17&quot;, &quot;unidic26&quot;, &quot;unidic29&quot;,
&quot;cc-cedict&quot;, &quot;ko-dic&quot;, &quot;naist11&quot;, or &quot;sudachi&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>See Also</h3>

<p>See also
<a href="https://github.com/ueda-keisuke/CC-CEDICT-MeCab">'CC-CEDICT-MeCab'</a>,
and <a href="https://bitbucket.org/eunjeon/mecab-ko-dic/src/master/">'mecab-ko-dic'</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_dict_features("ipa")
</code></pre>

<hr>
<h2 id='hiroba'>Whole tokens of 'Porano no Hiroba' written by Miyazawa Kenji
from Aozora Bunko</h2><span id='topic+hiroba'></span>

<h3>Description</h3>

<p>A tidy text data of <code>audubon::polano</code> that tokenized with 'MeCab'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hiroba
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 26849 rows and 5 columns.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>head(hiroba)
</code></pre>

<hr>
<h2 id='lex_density'>Calculate lexical density</h2><span id='topic+lex_density'></span>

<h3>Description</h3>

<p>The lexical density is the proportion of content words (lexical items)
in documents. This function is a simple helper for calculating
the lexical density of given datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lex_density(vec, contents_words, targets = NULL, negate = c(FALSE, FALSE))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lex_density_+3A_vec">vec</code></td>
<td>
<p>A character vector.</p>
</td></tr>
<tr><td><code id="lex_density_+3A_contents_words">contents_words</code></td>
<td>
<p>A character vector containing values
to be counted as contents words.</p>
</td></tr>
<tr><td><code id="lex_density_+3A_targets">targets</code></td>
<td>
<p>A character vector with which
the denominator of lexical density is filtered before computing values.</p>
</td></tr>
<tr><td><code id="lex_density_+3A_negate">negate</code></td>
<td>
<p>A logical vector of which length is 2.
If passed as <code>TRUE</code>, then respectively negates the predicate functions
for counting contents words or targets.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>head(hiroba) |&gt;
  prettify(col_select = "POS1") |&gt;
  dplyr::group_by(doc_id) |&gt;
  dplyr::summarise(
    noun_ratio = lex_density(POS1,
      "\u540d\u8a5e",
      c("\u52a9\u8a5e", "\u52a9\u52d5\u8a5e"),
      negate = c(FALSE, TRUE)
    ),
    mvr = lex_density(
      POS1,
      c("\u5f62\u5bb9\u8a5e", "\u526f\u8a5e", "\u9023\u4f53\u8a5e"),
      "\u52d5\u8a5e"
    ),
    vnr = lex_density(POS1, "\u52d5\u8a5e", "\u540d\u8a5e")
  )
</code></pre>

<hr>
<h2 id='mute_tokens'>Mute tokens by condition</h2><span id='topic+mute_tokens'></span>

<h3>Description</h3>

<p>Permutes tokens in the tidy text dataset with a string scalar
only if they are matched to an expression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mute_tokens(tbl, condition, .as = NA_character_)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mute_tokens_+3A_tbl">tbl</code></td>
<td>
<p>A tidy text dataset.</p>
</td></tr>
<tr><td><code id="mute_tokens_+3A_condition">condition</code></td>
<td>
<p>A logical expression.</p>
</td></tr>
<tr><td><code id="mute_tokens_+3A_.as">.as</code></td>
<td>
<p>String with which tokens are replaced
when they are matched to condition.
The default value is <code>NA_character</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df &lt;- prettify(head(hiroba), col_select = "POS1")
mute_tokens(df, POS1 %in% c("\u52a9\u8a5e", "\u52a9\u52d5\u8a5e"))
</code></pre>

<hr>
<h2 id='ngram_tokenizer'>Ngrams tokenizer</h2><span id='topic+ngram_tokenizer'></span>

<h3>Description</h3>

<p>Make an ngram tokenizer function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ngram_tokenizer(n = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ngram_tokenizer_+3A_n">n</code></td>
<td>
<p>Integer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ngram tokenizer function
</p>

<hr>
<h2 id='pack'>Pack a data.frame of tokens</h2><span id='topic+pack'></span>

<h3>Description</h3>

<p>Packs a data.frame of tokens into a new data.frame of corpus,
which is compatible with the Text Interchange Formats.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pack(tbl, pull = "token", n = 1L, sep = "-", .collapse = " ")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pack_+3A_tbl">tbl</code></td>
<td>
<p>A data.frame of tokens.</p>
</td></tr>
<tr><td><code id="pack_+3A_pull">pull</code></td>
<td>
<p>Column to be packed into text or ngrams body. Default value is <code>token</code>.</p>
</td></tr>
<tr><td><code id="pack_+3A_n">n</code></td>
<td>
<p>Integer internally passed to ngrams tokenizer function
created of <code>audubon::ngram_tokenizer()</code></p>
</td></tr>
<tr><td><code id="pack_+3A_sep">sep</code></td>
<td>
<p>Character scalar internally used as the concatenator of ngrams.</p>
</td></tr>
<tr><td><code id="pack_+3A_.collapse">.collapse</code></td>
<td>
<p>This argument is passed to <code>stringi::stri_join()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble.
</p>


<h3>Text Interchange Formats (TIF)</h3>

<p>The Text Interchange Formats (TIF) is a set of standards
that allows R text analysis packages to target defined inputs and outputs
for corpora, tokens, and document-term matrices.
</p>


<h3>Valid data.frame of tokens</h3>

<p>The data.frame of tokens here is a data.frame object
compatible with the TIF.
</p>
<p>A TIF valid data.frame of tokens are expected to have one unique key column (named <code>doc_id</code>)
of each text and several feature columns of each tokens.
The feature columns must contain at least <code>token</code> itself.
</p>


<h3>See Also</h3>

<p><a href="https://github.com/ropenscilabs/tif">https://github.com/ropenscilabs/tif</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pack(strj_tokenize(polano[1:5], format = "data.frame"))
</code></pre>

<hr>
<h2 id='polano'>Whole text of 'Porano no Hiroba' written by Miyazawa Kenji
from Aozora Bunko</h2><span id='topic+polano'></span>

<h3>Description</h3>

<p>Whole text of 'Porano no Hiroba' written by Miyazawa Kenji
from Aozora Bunko
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polano
</code></pre>


<h3>Format</h3>

<p>An object of class <code>character</code> of length 899.
</p>


<h3>Details</h3>

<p>A dataset containing the text of Miyazawa Kenji's novel
&quot;Porano no Hiroba&quot; which was published in 1934, the year after Kenji's death.
Copyright of this work has expired since more than 70 years have passed after the author's death.
</p>
<p>The UTF-8 plain text is sourced from <a href="https://www.aozora.gr.jp/cards/000081/card1935.html">https://www.aozora.gr.jp/cards/000081/card1935.html</a> and is cleaned of meta data.
</p>


<h3>Source</h3>

<p><a href="https://www.aozora.gr.jp/cards/000081/files/1935_ruby_19924.zip">https://www.aozora.gr.jp/cards/000081/files/1935_ruby_19924.zip</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>head(polano)
</code></pre>

<hr>
<h2 id='prettify'>Prettify tokenized output</h2><span id='topic+prettify'></span>

<h3>Description</h3>

<p>Turns a single character column into features
separating with delimiter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prettify(
  tbl,
  col = "feature",
  into = get_dict_features("ipa"),
  col_select = seq_along(into),
  delim = ","
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prettify_+3A_tbl">tbl</code></td>
<td>
<p>A data.frame that has feature column to be prettified.</p>
</td></tr>
<tr><td><code id="prettify_+3A_col">col</code></td>
<td>
<p>Column name where to be prettified.</p>
</td></tr>
<tr><td><code id="prettify_+3A_into">into</code></td>
<td>
<p>Character vector that is used as column names of
features.</p>
</td></tr>
<tr><td><code id="prettify_+3A_col_select">col_select</code></td>
<td>
<p>Character or integer vector that will be kept
in prettified features.</p>
</td></tr>
<tr><td><code id="prettify_+3A_delim">delim</code></td>
<td>
<p>Character scalar used to separate fields within a feature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prettify(
  data.frame(x = c("x,y", "y,z", "z,x")),
  col = "x",
  into = c("a", "b"),
  col_select = "b"
)
</code></pre>

<hr>
<h2 id='read_rewrite_def'>Read a rewrite.def file</h2><span id='topic+read_rewrite_def'></span>

<h3>Description</h3>

<p>Read a rewrite.def file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_rewrite_def(
  def_path = system.file("def/rewrite.def", package = "audubon")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_rewrite_def_+3A_def_path">def_path</code></td>
<td>
<p>Character scalar; path to the rewriting definition file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>str(read_rewrite_def())
</code></pre>

<hr>
<h2 id='strj_fill_iter_mark'>Fill Japanese iteration marks</h2><span id='topic+strj_fill_iter_mark'></span>

<h3>Description</h3>

<p>Fills Japanese iteration marks (Odori-ji) with their previous characters
if the element has more than 5 characters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strj_fill_iter_mark(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strj_fill_iter_mark_+3A_text">text</code></td>
<td>
<p>Character vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strj_fill_iter_mark(c(
  "\u3042\u3044\u3046\u309d\u3003\u304b\u304d",
  "\u91d1\u5b50\u307f\u3059\u309e",
  "\u306e\u305f\u308a\u3033\u3035\u304b\u306a",
  "\u3057\u308d\uff0f\u2033\uff3c\u3068\u3057\u305f"
))
</code></pre>

<hr>
<h2 id='strj_hiraganize'>Hiraganize Japanese characters</h2><span id='topic+strj_hiraganize'></span>

<h3>Description</h3>

<p>Converts Japanese katakana to hiragana.
It is almost similar to <code>stringi::stri_trans_general(text, "kana-hira")</code>,
however, this implementation can also handle some additional symbols
such as Japanese kana ligature (aka. goryaku-gana).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strj_hiraganize(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strj_hiraganize_+3A_text">text</code></td>
<td>
<p>Character vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strj_hiraganize(
  c(
    paste0(
      "\u3042\u306e\u30a4\u30fc\u30cf\u30c8",
      "\u30fc\u30f4\u30a9\u306e\u3059\u304d",
      "\u3068\u304a\u3063\u305f\u98a8"
    ),
    "\u677f\u57a3\u6b7b\u30b9\U0002a708"
  )
)
</code></pre>

<hr>
<h2 id='strj_katakanize'>Katakanize Japanese characters</h2><span id='topic+strj_katakanize'></span>

<h3>Description</h3>

<p>Converts Japanese hiragana to katakana.
It is almost similar to <code>stringi::stri_trans_general(text, "hira-kana")</code>,
however, this implementation can also handle some additional symbols
such as Japanese kana ligature (aka. goryaku-gana).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strj_katakanize(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strj_katakanize_+3A_text">text</code></td>
<td>
<p>Character vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strj_katakanize(
  c(
    paste0(
      "\u3042\u306e\u30a4\u30fc\u30cf\u30c8",
      "\u30fc\u30f4\u30a9\u306e\u3059\u304d",
      "\u3068\u304a\u3063\u305f\u98a8"
    ),
    "\u672c\u65e5\u309f\u304b\u304d\u6c37\u89e3\u7981"
  )
)
</code></pre>

<hr>
<h2 id='strj_normalize'>Convert text following the rules of 'NEologd'</h2><span id='topic+strj_normalize'></span>

<h3>Description</h3>

<p>Converts characters into normalized style
following the rule that is recommended by the Neologism dictionary for 'MeCab'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strj_normalize(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strj_normalize_+3A_text">text</code></td>
<td>
<p>Character vector to be normalized.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>See Also</h3>

<p><a href="https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja">https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strj_normalize(
  paste0(
    "\u2015\u2015\u5357\u30a2\u30eb\u30d7\u30b9",
    "\u306e\u3000\u5929\u7136\u6c34-\u3000\uff33",
    "\uff50\uff41\uff52\uff4b\uff49\uff4e\uff47*",
    "\u3000\uff2c\uff45\uff4d\uff4f\uff4e+",
    "\u3000\u30ec\u30e2\u30f3\u4e00\u7d5e\u308a"
  )
)
</code></pre>

<hr>
<h2 id='strj_rewrite_as_def'>Rewrite text using rewrite.def</h2><span id='topic+strj_rewrite_as_def'></span>

<h3>Description</h3>

<p>Rewrites text using a 'rewrite.def' file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strj_rewrite_as_def(text, as = read_rewrite_def())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strj_rewrite_as_def_+3A_text">text</code></td>
<td>
<p>Character vector to be normalized.</p>
</td></tr>
<tr><td><code id="strj_rewrite_as_def_+3A_as">as</code></td>
<td>
<p>List.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strj_rewrite_as_def(
  paste0(
    "\u2015\u2015\u5357\u30a2\u30eb",
    "\u30d7\u30b9\u306e\u3000\u5929",
    "\u7136\u6c34-\u3000\uff33\uff50",
    "\uff41\uff52\uff4b\uff49\uff4e\uff47*",
    "\u3000\uff2c\uff45\uff4d\uff4f\uff4e+",
    "\u3000\u30ec\u30e2\u30f3\u4e00\u7d5e\u308a"
  )
)
strj_rewrite_as_def(
  "\u60e1\u3068\u5047\u9762\u306e\u30eb\u30fc\u30eb",
  read_rewrite_def(system.file("def/kyuji.def", package = "audubon"))
)
</code></pre>

<hr>
<h2 id='strj_romanize'>Romanize Japanese Hiragana and Katakana</h2><span id='topic+strj_romanize'></span>

<h3>Description</h3>

<p>Romanize Japanese Hiragana and Katakana
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strj_romanize(
  text,
  config = c("wikipedia", "traditional hepburn", "modified hepburn", "kunrei", "nihon")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strj_romanize_+3A_text">text</code></td>
<td>
<p>Character vector.
If elements are composed of except but hiragana and katakana letters,
those letters are dropped from the return value.</p>
</td></tr>
<tr><td><code id="strj_romanize_+3A_config">config</code></td>
<td>
<p>Configuration used to romanize. Default is <code>wikipedia</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are several ways to romanize Japanese.
Using this implementation, you can convert hiragana and katakana as 5 different styles;
the <code>wikipedia</code> style, the <code style="white-space: pre;">&#8288;traditional hepburn&#8288;</code> style, the <code style="white-space: pre;">&#8288;modified hepburn&#8288;</code> style,
the <code>kunrei</code> style, and the <code>nihon</code> style.
</p>
<p>Note that all of these styles return a slightly different form of
<code>stringi::stri_trans_general(text, "Any-latn")</code>.
</p>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>See Also</h3>

<p><a href="https://github.com/hakatashi/japanese.js#japaneseromanizetext-config">https://github.com/hakatashi/japanese.js#japaneseromanizetext-config</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strj_romanize(
  paste0(
    "\u3042\u306e\u30a4\u30fc\u30cf\u30c8",
    "\u30fc\u30f4\u30a9\u306e\u3059\u304d",
    "\u3068\u304a\u3063\u305f\u98a8"
  )
)
</code></pre>

<hr>
<h2 id='strj_segment'>Segment text into tokens</h2><span id='topic+strj_segment'></span>

<h3>Description</h3>

<p>An alias of <code>strj_tokenize(engine = "budoux")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strj_segment(text, format = c("list", "data.frame"), split = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strj_segment_+3A_text">text</code></td>
<td>
<p>Character vector to be tokenized.</p>
</td></tr>
<tr><td><code id="strj_segment_+3A_format">format</code></td>
<td>
<p>Output format. Choose <code>list</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="strj_segment_+3A_split">split</code></td>
<td>
<p>Logical. If passed as, the function splits the vector
into some sentences using <code>stringi::stri_split_boundaries(type = "sentence")</code>
before tokenizing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A List or a data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strj_segment(
  paste0(
    "\u3042\u306e\u30a4\u30fc\u30cf\u30c8",
    "\u30fc\u30f4\u30a9\u306e\u3059\u304d",
    "\u3068\u304a\u3063\u305f\u98a8"
  )
)
strj_segment(
  paste0(
    "\u3042\u306e\u30a4\u30fc\u30cf\u30c8",
    "\u30fc\u30f4\u30a9\u306e\u3059\u304d",
    "\u3068\u304a\u3063\u305f\u98a8"
  ),
  format = "data.frame"
)
</code></pre>

<hr>
<h2 id='strj_tinyseg'>Segment text into phrases</h2><span id='topic+strj_tinyseg'></span>

<h3>Description</h3>

<p>An alias of <code>strj_tokenize(engine = "tinyseg")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strj_tinyseg(text, format = c("list", "data.frame"), split = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strj_tinyseg_+3A_text">text</code></td>
<td>
<p>Character vector to be tokenized.</p>
</td></tr>
<tr><td><code id="strj_tinyseg_+3A_format">format</code></td>
<td>
<p>Output format. Choose <code>list</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="strj_tinyseg_+3A_split">split</code></td>
<td>
<p>Logical. If passed as <code>TRUE</code>, the function splits vectors
into some sentences using <code>stringi::stri_split_boundaries(type = "sentence")</code>
before tokenizing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list or a data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strj_tinyseg(
  paste0(
    "\u3042\u306e\u30a4\u30fc\u30cf\u30c8",
    "\u30fc\u30f4\u30a9\u306e\u3059\u304d",
    "\u3068\u304a\u3063\u305f\u98a8"
  )
)
strj_tinyseg(
  paste0(
    "\u3042\u306e\u30a4\u30fc\u30cf\u30c8",
    "\u30fc\u30f4\u30a9\u306e\u3059\u304d",
    "\u3068\u304a\u3063\u305f\u98a8"
  ),
  format = "data.frame"
)
</code></pre>

<hr>
<h2 id='strj_tokenize'>Split text into tokens</h2><span id='topic+strj_tokenize'></span>

<h3>Description</h3>

<p>Splits text into several tokens using specified tokenizer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strj_tokenize(
  text,
  format = c("list", "data.frame"),
  engine = c("stringi", "budoux", "tinyseg", "mecab", "sudachipy"),
  rcpath = NULL,
  mode = c("C", "B", "A"),
  split = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strj_tokenize_+3A_text">text</code></td>
<td>
<p>Character vector to be tokenized.</p>
</td></tr>
<tr><td><code id="strj_tokenize_+3A_format">format</code></td>
<td>
<p>Output format. Choose <code>list</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="strj_tokenize_+3A_engine">engine</code></td>
<td>
<p>Tokenizer name. Choose one of 'stringi', 'budoux',
'tinyseg', 'mecab', or 'sudachipy'.
Note that the specified tokenizer is installed and available when you use
'mecab' or 'sudachipy'.</p>
</td></tr>
<tr><td><code id="strj_tokenize_+3A_rcpath">rcpath</code></td>
<td>
<p>Path to a setting file for 'MeCab' or 'sudachipy' if any.</p>
</td></tr>
<tr><td><code id="strj_tokenize_+3A_mode">mode</code></td>
<td>
<p>Splitting mode for 'sudachipy'.</p>
</td></tr>
<tr><td><code id="strj_tokenize_+3A_split">split</code></td>
<td>
<p>Logical. If passed as <code>TRUE</code>, the function splits the vector
into some sentences using <code>stringi::stri_split_boundaries(type = "sentence")</code>
before tokenizing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list or a data.frame.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strj_tokenize(
  paste0(
    "\u3042\u306e\u30a4\u30fc\u30cf\u30c8",
    "\u30fc\u30f4\u30a9\u306e\u3059\u304d",
    "\u3068\u304a\u3063\u305f\u98a8"
  )
)
strj_tokenize(
  paste0(
    "\u3042\u306e\u30a4\u30fc\u30cf\u30c8",
    "\u30fc\u30f4\u30a9\u306e\u3059\u304d",
    "\u3068\u304a\u3063\u305f\u98a8"
  ),
  format = "data.frame"
)
</code></pre>

<hr>
<h2 id='strj_transcribe_num'>Transcribe Arabic to Kansuji</h2><span id='topic+strj_transcribe_num'></span>

<h3>Description</h3>

<p>Transcribes Arabic integers to Kansuji with auxiliary numerals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strj_transcribe_num(int)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strj_transcribe_num_+3A_int">int</code></td>
<td>
<p>Integers.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As its implementation is limited, this function can only transcribe
numbers up to trillions.
In case you convert much bigger numbers, try to use the 'arabic2kansuji' package.
</p>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strj_transcribe_num(c(10L, 31415L))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
