<!DOCTYPE html><html><head><title>Help for package maxLik</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {maxLik}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#maxLik-package'>
<p>Maximum Likelihood Estimation</p></a></li>
<li><a href='#activePar'><p>free parameters under maximization</p></a></li>
<li><a href='#AIC.maxLik'><p>Methods for the various standard functions</p></a></li>
<li><a href='#bread.maxLik'><p>Bread for Sandwich Estimator</p></a></li>
<li><a href='#compareDerivatives'><p>function to compare analytic and numeric derivatives</p></a></li>
<li><a href='#condiNumber'><p>Print matrix condition numbers column-by-column</p></a></li>
<li><a href='#confint.maxLik'><p>confint method for maxLik objects</p></a></li>
<li><a href='#fnSubset'>
<p>Call fnFull with variable and fixed parameters</p></a></li>
<li><a href='#gradient'><p>Extract Gradients Evaluated at each Observation</p></a></li>
<li><a href='#hessian'><p>Hessian matrix</p></a></li>
<li><a href='#logLik.maxLik'><p>Return the log likelihood value</p></a></li>
<li><a href='#maxBFGS'><p>BFGS, conjugate gradient, SANN and Nelder-Mead Maximization</p></a></li>
<li><a href='#MaxControl-class'><p>Class <code>"MaxControl"</code></p></a></li>
<li><a href='#maximType'><p>Type of Minimization/Maximization</p></a></li>
<li><a href='#maxLik'><p>Maximum likelihood estimation</p></a></li>
<li><a href='#maxLik-internal'><p> Internal maxLik Functions</p></a></li>
<li><a href='#maxNR'><p>Newton- and Quasi-Newton Maximization</p></a></li>
<li><a href='#maxSGA'><p>Stochastic Gradient Ascent</p></a></li>
<li><a href='#maxValue'><p>Function value at maximum</p></a></li>
<li><a href='#nIter'><p>Return number of iterations for iterative models</p></a></li>
<li><a href='#nObs.maxLik'><p>Number of Observations</p></a></li>
<li><a href='#nParam.maxim'><p>Number of model parameters</p></a></li>
<li><a href='#numericGradient'><p>Functions to Calculate Numeric Derivatives</p></a></li>
<li><a href='#objectiveFn'><p>Optimization Objective Function</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#returnCode'><p>Success or failure of the optimization</p></a></li>
<li><a href='#storedValues'><p>Return the stored values of optimization</p></a></li>
<li><a href='#summary.maxim'><p>Summary method for maximization</p></a></li>
<li><a href='#summary.maxLik'><p>summary the Maximum-Likelihood estimation</p></a></li>
<li><a href='#sumt'>
<p>Equality-constrained optimization</p></a></li>
<li><a href='#tidy.maxLik'><p>tidy and glance methods for maxLik objects</p></a></li>
<li><a href='#vcov.maxLik'><p>Variance Covariance Matrix of maxLik objects</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.5-2.1</td>
</tr>
<tr>
<td>Title:</td>
<td>Maximum Likelihood Estimation and Related Tools</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.4.0), miscTools (&ge; 0.6-8), methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>sandwich, generics</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, clue, dlm, plot3D, tibble, tinytest</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions for Maximum Likelihood (ML) estimation, non-linear
   optimization, and related tools.  It includes a unified way to call
   different optimizers, and classes and methods to handle the results from
   the Maximum Likelihood viewpoint.  It also includes a number of convenience
   tools for testing and developing your own models.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-24 11:40:11 UTC; hornik</td>
</tr>
<tr>
<td>Author:</td>
<td>Ott Toomet [aut, cre],
  Arne Henningsen [aut],
  Spencer Graves [ctb],
  Yves Croissant [ctb],
  David Hugh-Jones [ctb],
  Luca Scrucca [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ott Toomet &lt;otoomet@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-24 14:45:20 UTC</td>
</tr>
</table>
<hr>
<h2 id='maxLik-package'>
Maximum Likelihood Estimation
</h2><span id='topic+maxLik-package'></span>

<h3>Description</h3>

<p>This package contains a set of functions and tools for Maximum Likelihood (ML)
estimation.  The focus of the package is on non-linear
optimization from the ML viewpoint, and it provides several convenience wrappers
and tools, like BHHH algorithm, variance-covariance
matrix and standard errors.
</p>


<h3>Details</h3>

<p><span class="pkg">maxLik</span> package is a set of convenience tools and wrappers
focusing on
Maximum Likelihood (ML) analysis, but it also contains tools for
other optimization tasks.
The package includes a) wrappers for several
existing optimizers (implemented by <code><a href="stats.html#topic+optim">optim</a></code>); b) original
optimizers, including Newton-Raphson and Stochastic Gradient Ascent;
and c) several convenience tools
to use these optimizers from the ML perspective.  Examples are BHHH
optimization (<code><a href="#topic+maxBHHH">maxBHHH</a></code>) and utilities that extract
standard errors from the estimates.  Other highlights include a unified
interface for all included optimizers, tools to test user-provided analytic
derivatives, and constrained optimization.
</p>
<p>A good starting point to learn about the usage of <span class="pkg">maxLik</span> are the
included vignettes &ldquo;Introduction: what is maximum likelihood&rdquo;,
&ldquo;Maximum likelihood estimation with maxLik&rdquo; and
&ldquo;Stochastic Gradient Ascent in maxLik&rdquo;.  Another good
source is
Henningsen &amp; Toomet (2011), an introductory paper to the package.
Use
<code>vignette(package="maxLik")</code> to see the available vignettes, and
<code>vignette("using-maxlik")</code> to read the usage vignette.
</p>
<p>From the user's perspective, the
central function in the package is <code><a href="#topic+maxLik">maxLik</a></code>.  In its
simplest form it takes two arguments: the log-likelihood function, and
a vector of initial parameter values (see the example below).
It returns an object of class
&lsquo;maxLik&rsquo; with convenient methods such as
<code><a href="#topic+summary.maxLik">summary</a></code>,
<code><a href="#topic+coef.maxLik">coef</a></code>, and
<code><a href="#topic+stdEr.maxLik">stdEr</a></code>.  It also supports a plethora
of other arguments, for instance one can supply analytic gradient and
Hessian, select the desired optimizer, and control the optimization in
different ways.
</p>
<p>A useful utility functions in the package is
<code><a href="#topic+compareDerivatives">compareDerivatives</a></code> that
allows one to compare the analytic and numeric derivatives for debugging
purposes.
Another useful function is <code><a href="#topic+condiNumber">condiNumber</a></code> for
analyzing multicollinearity problems in the estimated models.
</p>
<p>In the interest of providing a unified user interface, all the
optimizers are implemented as maximizers in this package.  This includes
the <code><a href="stats.html#topic+optim">optim</a></code>-based methods, such as <code><a href="#topic+maxBFGS">maxBFGS</a></code> and
<code><a href="#topic+maxSGA">maxSGA</a></code>, the maximizer version of popular Stochastic
Gradient Descent.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet &lt;otoomet@gmail.com&gt;,
Arne Henningsen &lt;arne.henningsen@gmail.com&gt;,
with contributions from Spencer Graves, Yves Croissant and David
Hugh-Jones. 
</p>
<p>Maintainer: Ott Toomet &lt;otoomet@gmail.com&gt;
</p>


<h3>References</h3>

<p>Henningsen A, Toomet O (2011). &ldquo;maxLik: A package for maximum
likelihood estimation in R.&rdquo;
Computational Statistics, 26(3), 443-458. doi:
<a href="https://doi.org/10.1007/s00180-010-0217-1">doi:10.1007/s00180-010-0217-1</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### estimate mean and variance of normal random vector

## create random numbers where mu=1, sd=2
set.seed(123)
x &lt;- rnorm(50, 1, 2 )

## log likelihood function.
## Note: 'param' is a 2-vector c(mu, sd)
llf &lt;- function(param) {
   mu &lt;- param[1]
   sd &lt;- param[2]
   llValue &lt;- dnorm(x, mean=mu, sd=sd, log=TRUE)
   sum(llValue)
}

## Estimate it with mu=0, sd=1 as start values
ml &lt;- maxLik(llf, start = c(mu=0, sigma=1) )
print(summary(ml))
## Estimates close to c(1,2) :-)
</code></pre>

<hr>
<h2 id='activePar'>free parameters under maximization</h2><span id='topic+activePar'></span><span id='topic+activePar.default'></span>

<h3>Description</h3>

<p>Return a logical vector, indicating which parameters were free under
maximization, as opposed to the fixed parameters that are treated as
constants.  See argument &ldquo;fixed&rdquo; for <code><a href="#topic+maxNR">maxNR</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>activePar(x, ...)
## Default S3 method:
activePar(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="activePar_+3A_x">x</code></td>
<td>
<p>object, created by a maximization routine, such as
<code><a href="#topic+maxNR">maxNR</a></code> or <code><a href="#topic+maxLik">maxLik</a></code>, or derived from a
maximization object.
</p>
</td></tr>
<tr><td><code id="activePar_+3A_...">...</code></td>
<td>
<p>further arguments for methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Several optimization routines allow the user to fix some parameter
values (or do it automatically in some cases).  For gradient or
Hessian based inference one has to know which parameters carry
optimization-related information.
</p>


<h3>Value</h3>

<p>A logical vector, indicating whether the parameters were free to
change during optimization algorithm.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxNR">maxNR</a></code>, <code><a href="miscTools.html#topic+nObs">nObs</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## a two-dimensional exponential hat
f &lt;- function(a) exp(-a[1]^2 - a[2]^2)

## maximize wrt. both parameters 
free &lt;- maxNR(f, start=1:2) 
summary(free)  # results should be close to (0,0)
activePar(free)

## keep the first parameter constant
cons &lt;- maxNR(f, start=1:2, fixed=c(TRUE,FALSE))
summary(cons) # result should be around (1,0)
activePar(cons)
</code></pre>

<hr>
<h2 id='AIC.maxLik'>Methods for the various standard functions</h2><span id='topic+AIC.maxLik'></span><span id='topic+coef.maxim'></span><span id='topic+coef.maxLik'></span><span id='topic+stdEr.maxLik'></span>

<h3>Description</h3>

<p>These are methods for the maxLik related objects.  See also the
documentation for the
corresponding generic functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxLik'
AIC(object, ..., k=2)
## S3 method for class 'maxim'
coef(object, ...)
## S3 method for class 'maxLik'
coef(object, ...)
## S3 method for class 'maxLik'
stdEr(x, eigentol=1e-12, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AIC.maxLik_+3A_object">object</code></td>
<td>
<p>a &lsquo;maxLik&rsquo; object (<code>coef</code> can also handle
&lsquo;maxim&rsquo; objects)</p>
</td></tr>
<tr><td><code id="AIC.maxLik_+3A_k">k</code></td>
<td>
<p>numeric, the penalty per parameter to be used; the default
&lsquo;k = 2&rsquo; is the classical AIC.</p>
</td></tr>
<tr><td><code id="AIC.maxLik_+3A_x">x</code></td>
<td>
<p>a &lsquo;maxLik&rsquo; object</p>
</td></tr>
<tr><td><code id="AIC.maxLik_+3A_eigentol">eigentol</code></td>
<td>

<p>The standard errors are only calculated if the ratio of the smallest
and largest eigenvalue of the Hessian matrix is less than
&ldquo;eigentol&rdquo;.  Otherwise the Hessian is treated as singular.
</p>
</td></tr>
<tr><td><code id="AIC.maxLik_+3A_...">...</code></td>
<td>
<p>other arguments for methods</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt>AIC</dt><dd><p>calculates Akaike's Information Criterion (and other
information criteria).</p>
</dd>
<dt>coef</dt><dd><p>extracts the estimated parameters (model's
coefficients).</p>
</dd>
<dt>stdEr</dt><dd><p>extracts standard errors (using the Hessian matrix).
</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>## estimate mean and variance of normal random vector
set.seed(123)
x &lt;- rnorm(50, 1, 2)

## log likelihood function.
## Note: 'param' is a vector
llf &lt;- function( param ) {
   mu &lt;- param[ 1 ]
   sigma &lt;- param[ 2 ]
   return(sum(dnorm(x, mean=mu, sd=sigma, log=TRUE)))
}

## Estimate it.  Take standard normal as start values
ml &lt;- maxLik(llf, start = c(mu=0, sigma=1) )

coef(ml)
stdEr(ml)
AIC(ml)
</code></pre>

<hr>
<h2 id='bread.maxLik'>Bread for Sandwich Estimator</h2><span id='topic+bread'></span><span id='topic+bread.maxLik'></span>

<h3>Description</h3>

<p>Extracting an estimator for the &lsquo;bread&rsquo; of the sandwich estimator,
see <code><a href="sandwich.html#topic+bread">bread</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxLik'
bread( x, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bread.maxLik_+3A_x">x</code></td>
<td>
<p>an object of class <code>maxLik</code>.</p>
</td></tr>
<tr><td><code id="bread.maxLik_+3A_...">...</code></td>
<td>
<p>further arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix, the inverse of the expectation of the second derivative
(Hessian matrix) of the log-likelihood function
with respect to the parameters.
In case of the simple Maximum Likelihood, it is
equal to the variance covariance matrix of the parameters,
multiplied by the number of observations.
</p>


<h3>Warnings</h3>

<p>The <span class="pkg">sandwich</span> package is required for this function. 
</p>
<p>This method works only if the observaton-specific gradient information
was available for the estimation.  This is the case if the
observation-specific gradient was supplied (see the <code>grad</code>
argument for <code><a href="#topic+maxLik">maxLik</a></code>), or the log-likelihood function
returns a vector of observation-specific values.
</p>


<h3>Author(s)</h3>

<p>Arne Henningsen
</p>


<h3>See Also</h3>

<p><code><a href="sandwich.html#topic+bread">bread</a></code>, <code><a href="#topic+maxLik">maxLik</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## ML estimation of exponential duration model:
t &lt;- rexp(100, 2)
loglik &lt;- function(theta) log(theta) - theta*t

## Estimate with numeric gradient and hessian
a &lt;- maxLik(loglik, start=1 )

# Extract the "bread"
library( sandwich )
bread( a )

all.equal( bread( a ), vcov( a ) * nObs( a ) )
</code></pre>

<hr>
<h2 id='compareDerivatives'>function to compare analytic and numeric derivatives</h2><span id='topic+compareDerivatives'></span>

<h3>Description</h3>

<p>This function compares analytic and numerical derivative and prints related
diagnostics information.  It is intended for testing and debugging
code for analytic derivatives
for maximization algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compareDerivatives(f, grad, hess=NULL, t0, eps=1e-6,
                   printLevel=1, print=printLevel &gt; 0,
                   max.rows=getOption("max.rows", 20),
                   max.cols=getOption("max.cols", 7),
                   ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compareDerivatives_+3A_f">f</code></td>
<td>

<p>function to be differentiated. The parameter (vector) of interest
must be the first argument. The function may return a vector, in
that case the derivative will be a matrix.
</p>
</td></tr>
<tr><td><code id="compareDerivatives_+3A_grad">grad</code></td>
<td>

<p>analytic gradient.  This may be either a function,
returning the analytic gradient, or a numeric vector, the pre-computed
gradient.  The function must use the same set of
parameters as <code>f</code>.  If <code>f</code> is a vector-valued function,
grad must return/be a matrix where the number of rows equals the number
of components of <code>f</code>, and the number of columns must equal to
the number of components in <code>t0</code>.
</p>
</td></tr>
<tr><td><code id="compareDerivatives_+3A_hess">hess</code></td>
<td>

<p>function returning the analytic hessian.  If present, hessian
matrices are compared too.  Only appropriate for scalar-valued
functions.
</p>
</td></tr>
<tr><td><code id="compareDerivatives_+3A_t0">t0</code></td>
<td>

<p>numeric vector, parameter at which the derivatives are
compared. The derivative is taken with respect to this vector.  both
<code>f</code>m <code>grad</code> (if function) and <code>hess</code> (if present)
must accept this value as the first parameter.
</p>
</td></tr>
<tr><td><code id="compareDerivatives_+3A_eps">eps</code></td>
<td>

<p>numeric. Step size for numeric differentiation. Central derivative
is used.
</p>
</td></tr>
<tr><td><code id="compareDerivatives_+3A_printlevel">printLevel</code></td>
<td>

<p>numeric: a positive number prints summary of the comparison.  0 does
not do any printing, only returns the comparison results (invisibly).
</p>
</td></tr>
<tr><td><code id="compareDerivatives_+3A_print">print</code></td>
<td>

<p>deprecated (for backward compatibility only).
</p>
</td></tr>
<tr><td><code id="compareDerivatives_+3A_max.rows">max.rows</code></td>
<td>
<p>maximum number of matrix rows to be printed.
</p>
</td></tr>
<tr><td><code id="compareDerivatives_+3A_max.cols">max.cols</code></td>
<td>
<p>maximum number of columns to be printed.  
</p>
</td></tr>
<tr><td><code id="compareDerivatives_+3A_...">...</code></td>
<td>

<p>further arguments to <code>f</code>, <code>grad</code> and <code>hess</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Analytic derivatives (and Hessian) substantially improve the
estimation speed and reliability.  However, these are
typically hard to program.  This utility compares the programmed result
and the (internally calculated) numeric derivative.  
For every component of <code>f</code>, it prints the parameter value, analytic and
numeric derivative, and their relative difference
</p>
<p style="text-align: center;"><code class="reqn">\textrm{rel.diff} = \frac{\textrm{analytic} -
      \textrm{numeric}}{\frac{1}{2}(|\textrm{analytic}| + |\textrm{numeric}|)}.</code>
</p>

<p>If <code class="reqn">\textrm{analytic} = 0</code> and
<code class="reqn">\textrm{numeric} = 0</code>, then rel.diff is also set to
0.  If
analytic derivatives are correct and the function is sufficiently
smooth, expect the relative differences to be less than <code class="reqn">10^{-7}</code>.
</p>


<h3>Value</h3>

<p>A list with following components:
</p>
<table>
<tr><td><code>t0</code></td>
<td>
<p>the input argument <code>t0</code></p>
</td></tr>
<tr><td><code>f.t0</code></td>
<td>
<p>f(t0)</p>
</td></tr>
<tr><td><code>compareGrad</code></td>
<td>

<p>a list with components <code>analytic</code> = grad(t0), <code>nmeric</code> =
numericGradient(f, t0), and their <code>rel.diff</code>.
</p>
</td></tr>
<tr><td><code>maxRelDiffGrad</code></td>
<td>
<p>max(abs(rel.diff))</p>
</td></tr>
</table>
<p>If <code>hess</code> is also provided, the following optional components
are also present:
</p>
<table>
<tr><td><code>compareHessian</code></td>
<td>

<p>a list with components <code>analytic</code> = hess(t0), <code>numeric</code>
= numericGradient(grad, t0), and their <code>rel.diff</code>.
</p>
</td></tr>
<tr><td><code>maxRelDiffHess</code></td>
<td>
<p>max(abs(rel.diff)) for the Hessian</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ott Toomet <a href="mailto:otoomet@ut.ee">otoomet@ut.ee</a> and Spencer Graves</p>


<h3>See Also</h3>

<p><code><a href="#topic+numericGradient">numericGradient</a></code>
<code><a href="stats.html#topic+deriv">deriv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple example with sin(x)' = cos(x)
f &lt;- function(x) c(sin=sin(x))
Dsin &lt;- compareDerivatives(f, cos, t0=c(angle=1))
##
## Example of normal log-likelihood.  Two-parameter
## function.
##
x &lt;- rnorm(100, 1, 2) # generate rnorm x
l &lt;- function(b) sum(dnorm(x, mean=b[1], sd=b[2], log=TRUE))
gradl &lt;- function(b) {
    c(mu=sum(x - b[1])/b[2]^2,
    sigma=sum((x - b[1])^2/b[2]^3 - 1/b[2]))
}
gradl. &lt;- compareDerivatives(l, gradl, t0=c(mu=1,sigma=2))

##
## An example with f returning a vector, t0 = a scalar
##
trig &lt;- function(x)c(sin=sin(x), cos=cos(x))
Dtrig &lt;- function(x)c(sin=cos(x), cos=-sin(x))
Dtrig. &lt;- compareDerivatives(trig, Dtrig, t0=1)
</code></pre>

<hr>
<h2 id='condiNumber'>Print matrix condition numbers column-by-column</h2><span id='topic+condiNumber'></span><span id='topic+condiNumber.default'></span><span id='topic+condiNumber.maxLik'></span>

<h3>Description</h3>

<p>This function prints the condition number of a matrix while adding
columns one-by-one.  This is useful for testing multicollinearity and
other numerical problems.  It is a generic function with a default
method, and a method for <code>maxLik</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>condiNumber(x, ...)
## Default S3 method:
condiNumber(x, exact = FALSE, norm = FALSE,
   printLevel=print.level, print.level=1, digits = getOption( "digits" ), ... )
## S3 method for class 'maxLik'
condiNumber(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="condiNumber_+3A_x">x</code></td>
<td>
<p>numeric matrix, condition numbers of which are to be printed</p>
</td></tr>
<tr><td><code id="condiNumber_+3A_exact">exact</code></td>
<td>
<p>logical, should condition numbers be exact or
approximations (see <code><a href="base.html#topic+kappa">kappa</a></code>)</p>
</td></tr>
<tr><td><code id="condiNumber_+3A_norm">norm</code></td>
<td>
<p>logical, whether the columns should be normalised to have
unit norm</p>
</td></tr>
<tr><td><code id="condiNumber_+3A_printlevel">printLevel</code></td>
<td>
<p>numeric, positive value will output the numbers
during the calculations.  Useful for interactive work.</p>
</td></tr>
<tr><td><code id="condiNumber_+3A_print.level">print.level</code></td>
<td>
<p>same as &lsquo;printLevel&rsquo;, for backward
compatibility</p>
</td></tr> 
<tr><td><code id="condiNumber_+3A_digits">digits</code></td>
<td>
<p>minimal number of significant digits to print
(only relevant if argument <code>print.level</code> is larger than zero).</p>
</td></tr>
<tr><td><code id="condiNumber_+3A_...">...</code></td>
<td>
<p>Further arguments to <code>condiNumber.default</code>
are currently ignored;
further arguments to <code>condiNumber.maxLik</code>
are passed to <code>condiNumber.default</code>.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>Statistical model often fail because of a high correlation between
the explanatory variables in the linear index (multicollinearity)
or because the evaluated maximum of a non-linear model is
virtually flat.  In both cases, the (near) singularity of the related
matrices may help to understand the problem.
</p>
<p><code>condiNumber</code> inspects the matrices column-by-column and
indicates which variables lead to a jump in the condition
number (cause singularity).
If the matrix column name does not immediately indicate the
problem, one may run an OLS model by estimating this column
using all the previous columns as explanatory variables.  Those
columns that explain almost all the variation in the current one will
have very high
<code class="reqn">t</code>-values. 
</p>


<h3>Value</h3>

<p>Invisible vector of condition numbers by column.  If the start values
for <code><a href="#topic+maxLik">maxLik</a></code> are named, the condition numbers are named
accordingly. 
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>References</h3>

<p>Greene, W. (2012): <em>Econometrics Analysis</em>, 7th edition,
p. 130.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+kappa">kappa</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>   set.seed(0)
   ## generate a simple nearly multicollinear dataset
   x1 &lt;- runif(100)
   x2 &lt;- runif(100)
   x3 &lt;- x1 + x2 + 0.000001*runif(100) # this is virtually equal to x1 + x2
   x4 &lt;- runif(100)
   y &lt;- x1 + x2 + x3 + x4 + rnorm(100)
   m &lt;- lm(y ~ -1 + x1 + x2 + x3 + x4)
   print(summary(m)) # note the outlandish estimates and standard errors
                     # while R^2 is 0.88. This suggests multicollinearity
   condiNumber(model.matrix(m))   # note the value 'explodes' at x3
   ## we may test the results further:
   print(summary(lm(x3 ~ -1 + x1 + x2)))
   # Note the extremely high t-values and R^2: x3 is (almost) completely
   # explained by x1 and x2
</code></pre>

<hr>
<h2 id='confint.maxLik'>confint method for maxLik objects</h2><span id='topic+confint.maxLik'></span><span id='topic+confint'></span>

<h3>Description</h3>

<p>Wald confidence intervals for Maximum Likelihood Estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxLik'
confint(object, parm, level=0.95,  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint.maxLik_+3A_object">object</code></td>
<td>

<p>object of class &ldquo;maxLik&rdquo; returned by <code><a href="#topic+maxLik">maxLik</a></code> function
</p>
</td></tr>
<tr><td><code id="confint.maxLik_+3A_parm">parm</code></td>
<td>
<p>the name of parameters to compute the confidence
intervals.  If omitted, confidence intervals for all parameters are
computed.</p>
</td></tr>
<tr><td><code id="confint.maxLik_+3A_level">level</code></td>
<td>
<p>the level of confidence interval
</p>
</td></tr>
<tr><td><code id="confint.maxLik_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to the other methods
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of lower and upper confidence interval limits (in the first
and second column respectively).  The matrix rows are labeled by the
parameter names (if any) and columns by the corresponding distribution
quantiles. 
</p>


<h3>Author(s)</h3>

<p>Luca Scrucca</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+confint">confint</a></code> for the generic <code>confint</code>
function, 
<code><a href="#topic+stdEr.maxLik">stdEr</a></code> for computing standard errors and
<code><a href="#topic+summary.maxLik">summary</a></code> for summary output that includes statistical
significance information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## compute MLE parameters of normal random sample
x &lt;- rnorm(100)
loglik &lt;- function(theta) {
   dnorm(x, mean=theta[1], sd=theta[2], log=TRUE)
}
m &lt;- maxLik(loglik, start=c(mu=0, sd=1))
summary(m)
confint(m)
confint(m, "mu", level=0.1)
</code></pre>

<hr>
<h2 id='fnSubset'>
Call fnFull with variable and fixed parameters
</h2><span id='topic+fnSubset'></span>

<h3>Description</h3>

<p>Combine variable parameters with with fixed parameters and pass to
<code>fnFull</code>.  Useful for optimizing over a subset of parameters
without writing a separate function.  Values are combined by name if
available.  Otherwise, <code>xFull</code> is constructed by position (the
default).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fnSubset(x, fnFull, xFixed, xFull=c(x, xFixed), ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fnSubset_+3A_x">x</code></td>
<td>

<p>Variable parameters to be passed to <code>fnFull</code>.  
</p>
</td></tr>
<tr><td><code id="fnSubset_+3A_fnfull">fnFull</code></td>
<td>

<p>Function whose first argument has length = length(xFull).  
</p>
</td></tr>
<tr><td><code id="fnSubset_+3A_xfixed">xFixed</code></td>
<td>

<p>Parameter values to be combined with <code>x</code> to construct the first
argument for a call to <code>fnFull</code>.  
</p>
</td></tr>
<tr><td><code id="fnSubset_+3A_xfull">xFull</code></td>
<td>

<p>Prototype initial argument for <code>fnFull</code>.  
</p>
</td></tr>
<tr><td><code id="fnSubset_+3A_...">...</code></td>
<td>

<p>Optional arguments passed to <code>fnFull</code>.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function first confirms that
<code>length(x) + length(xFixed) == length(xFull)</code>.
Next,
</p>

<ul>
<li><p> If <code>xFull</code> has names, match at least <code>xFixed</code> by
name.
</p>
</li>
<li><p> Else <code>xFull = c(x, xFixes)</code>, the default.  
</p>
</li></ul>

<p>Finally, call <code>fnFull(xFull, ...)</code>.
</p>


<h3>Value</h3>

<p>value returned by <code>fnFull</code> 
</p>


<h3>Author(s)</h3>

<p>Spencer Graves 
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>
<code><a href="dlm.html#topic+dlmMLE">dlmMLE</a></code>
<code><a href="#topic+maxLik">maxLik</a></code>
<code><a href="#topic+maxNR">maxNR</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##
## Example with 'optim'
##
fn &lt;- function(x) (x[2]-2*x[1])^2
# note: true minimum is 0 on line 2*x[1] == x[2]
fullEst &lt;- optim(par=c(1,1), method="BFGS", fn=fn)
fullEst$par
# par = c(0.6, 1.2) at minimum (not convex)

# Fix the last component to 4 
est4 &lt;- optim(par=1, fn=fnSubset, method="BFGS", fnFull=fn, xFixed=4)
est4$par
# now there is a unique minimun x[1] = 2

# Fix the first component
fnSubset(x=1, fnFull=fn, xFixed=c(a=4), xFull=c(a=1, b=2))
# After substitution:  xFull = c(a=4, b=1),
# so fn = (1 - 2*4)^2 = (-7)^2 = 49

est4. &lt;- optim(par=1, fn=fnSubset, method="BFGS",
               fnFull=fn, xFixed=c(a=4), 
               xFull=c(a=1, b=2))
est4.$par
# At optimum: xFull=c(a=4, b=8),
# so fn = (8 - 2*4)^2 = 0

##
## Example with 'maxLik'
##
fn2max &lt;- function(x) -(x[2]-2*x[1])^2
# -&gt; need to have a maximum
max4 &lt;- maxLik(fnSubset, start=1, fnFull=fn2max, xFixed=4)
summary(max4)
# Similar result using fixed parameters in maxNR, called by maxLik 
max4. &lt;- maxLik(fn2max, start=c(1, 4), fixed=2)
summary(max4.)
</code></pre>

<hr>
<h2 id='gradient'>Extract Gradients Evaluated at each Observation</h2><span id='topic+gradient'></span><span id='topic+gradient.maxim'></span><span id='topic+estfun'></span><span id='topic+estfun.maxLik'></span>

<h3>Description</h3>

<p>Extract the gradients of the log-likelihood function evaluated
at each observation (&lsquo;Empirical Estimating Function&rsquo;,
see <code><a href="sandwich.html#topic+estfun">estfun</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxLik'
estfun(x, ...)
## S3 method for class 'maxim'
gradient(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gradient_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>maxim</code> (for <code>gradient</code>)
or <code>maxLik</code>. (for <code>estfun</code>.)</p>
</td></tr>
<tr><td><code id="gradient_+3A_...">...</code></td>
<td>
<p>further arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>gradient</code></td>
<td>
<p>vector, objective function gradient at estimated
maximum (or the last calculated value
if the estimation did not converge.)</p>
</td></tr>
<tr><td><code>estfun</code></td>
<td>

<p>matrix, observation-wise log-likelihood gradients
at the estimated parameter value
evaluated at each observation.  Observations in rows, parameters in columns.</p>
</td></tr>
</table>


<h3>Warnings</h3>

<p>The <span class="pkg">sandwich</span> package must be loaded in order to use <code>estfun</code>.
</p>
<p><code>estfun</code> only works if the observaton-specific gradient information
was available for the estimation.  This is the case of the
observation-specific gradient was supplied (see the <code>grad</code>
argument for <code><a href="#topic+maxLik">maxLik</a></code>), or the log-likelihood function
returns a vector of observation-specific values.
</p>


<h3>Author(s)</h3>

<p>Arne Henningsen, Ott Toomet
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hessian">hessian</a></code>, <code><a href="sandwich.html#topic+estfun">estfun</a></code>, <code><a href="#topic+maxLik">maxLik</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## ML estimation of exponential duration model:
t &lt;- rexp(10, 2)
loglik &lt;- function(theta) log(theta) - theta*t

## Estimate with numeric gradient and hessian
a &lt;- maxLik(loglik, start=1 )

gradient(a)
# Extract the gradients evaluated at each observation
library( sandwich )
estfun( a )

## Estimate with analytic gradient.
## Note: it returns a vector
gradlik &lt;- function(theta) 1/theta - t
b &lt;- maxLik(loglik, gradlik, start=1)
gradient(a)
estfun( b )
</code></pre>

<hr>
<h2 id='hessian'>Hessian matrix</h2><span id='topic+hessian'></span><span id='topic+hessian.default'></span>

<h3>Description</h3>

<p>This function extracts the Hessian of the objective function at optimum.
The Hessian information should be supplied by the underlying
optimization algorithm, possibly by an approximation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hessian(x, ...)
## Default S3 method:
hessian(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hessian_+3A_x">x</code></td>
<td>
<p>an optimization result of class &lsquo;maxim&rsquo; or &lsquo;maxLik&rsquo;</p>
</td></tr>
<tr><td><code id="hessian_+3A_...">...</code></td>
<td>
<p>other arguments for methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix, the Hessian of the model at the estimated parameter
values.  If the maximum is flat, the Hessian
is singular.  In that case you may want to invert only the
non-singular part of the matrix.  You may also want to fix certain
parameters (see <code><a href="#topic+activePar">activePar</a></code>).
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxLik">maxLik</a></code>, <code><a href="#topic+activePar">activePar</a></code>, <code><a href="#topic+condiNumber">condiNumber</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># log-likelihood for normal density
# a[1] - mean
# a[2] - standard deviation
ll &lt;- function(a) sum(-log(a[2]) - (x - a[1])^2/(2*a[2]^2))
x &lt;- rnorm(100) # sample from standard normal
ml &lt;- maxLik(ll, start=c(1,1))
# ignore eventual warnings "NaNs produced in: log(x)"
summary(ml) # result should be close to c(0,1)
hessian(ml) # How the Hessian looks like
sqrt(-solve(hessian(ml))) # Note: standard deviations are on the diagonal
#
# Now run the same example while fixing a[2] = 1
mlf &lt;- maxLik(ll, start=c(1,1), activePar=c(TRUE, FALSE))
summary(mlf) # first parameter close to 0, the second exactly 1.0
hessian(mlf)
# Note that now NA-s are in place of passive
# parameters.
# now invert only the free parameter part of the Hessian
sqrt(-solve(hessian(mlf)[activePar(mlf), activePar(mlf)]))
# gives the standard deviation for the mean
</code></pre>

<hr>
<h2 id='logLik.maxLik'>Return the log likelihood value</h2><span id='topic+logLik.maxLik'></span><span id='topic+logLik.summary.maxLik'></span>

<h3>Description</h3>

<p>Return the log likelihood value of objects of class <code>maxLik</code>
and <code>summary.maxLik</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxLik'
logLik( object, ... )
## S3 method for class 'summary.maxLik'
logLik( object, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logLik.maxLik_+3A_object">object</code></td>
<td>
<p>object of class <code>maxLik</code> or <code>summary.maxLik</code>,
usually a model estimated with Maximum Likelihood</p>
</td></tr>
<tr><td><code id="logLik.maxLik_+3A_...">...</code></td>
<td>
<p>additional arguments to methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar numeric, log likelihood of the estimated model.  It has
attribute &ldquo;df&rdquo;, number of free parameters.
</p>


<h3>Author(s)</h3>

<p>Arne Henningsen,
Ott Toomet
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxLik">maxLik</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## ML estimation of exponential duration model:
t &lt;- rexp(100, 2)
loglik &lt;- function(theta) log(theta) - theta*t
gradlik &lt;- function(theta) 1/theta - t
hesslik &lt;- function(theta) -100/theta^2
## Estimate with analytic gradient and hessian
a &lt;- maxLik(loglik, gradlik, hesslik, start=1)
## print log likelihood value
logLik( a )
## print log likelihood value of summary object
b &lt;- summary( a )
logLik( b )
</code></pre>

<hr>
<h2 id='maxBFGS'>BFGS, conjugate gradient, SANN and Nelder-Mead Maximization</h2><span id='topic+maxBFGS'></span><span id='topic+maxCG'></span><span id='topic+maxSANN'></span><span id='topic+maxNM'></span>

<h3>Description</h3>

<p>These functions are wrappers for <code><a href="stats.html#topic+optim">optim</a></code>, adding
constrained optimization and fixed parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxBFGS(fn, grad=NULL, hess=NULL, start, fixed=NULL,
   control=NULL,
   constraints=NULL,
   finalHessian=TRUE,
   parscale=rep(1, length=length(start)),
   ... )

maxCG(fn, grad=NULL, hess=NULL, start, fixed=NULL,
   control=NULL,
   constraints=NULL,
   finalHessian=TRUE,
   parscale=rep(1, length=length(start)), ...)

maxSANN(fn, grad=NULL, hess=NULL, start, fixed=NULL,
   control=NULL,
   constraints=NULL,
   finalHessian=TRUE,
   parscale=rep(1, length=length(start)),
   ... )

maxNM(fn, grad=NULL, hess=NULL, start, fixed=NULL,
   control=NULL,
   constraints=NULL,
   finalHessian=TRUE,
   parscale=rep(1, length=length(start)),
   ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxBFGS_+3A_fn">fn</code></td>
<td>
<p>function to be maximised.  Must have the parameter vector as
the first argument.  In order to use numeric gradient
and BHHH method, <code>fn</code> must return a vector of
observation-specific likelihood values.  Those are summed internally where
necessary.  If the parameters are out of range, <code>fn</code> should
return <code>NA</code>.  See details for constant parameters.</p>
</td></tr>
<tr><td><code id="maxBFGS_+3A_grad">grad</code></td>
<td>
<p>gradient of <code>fn</code>.  Must have the parameter vector as
the first argument.  If <code>NULL</code>, numeric
gradient is used (<code>maxNM</code> and <code>maxSANN</code> do not use
gradient).  
Gradient may return
a matrix, where columns correspond to the parameters and rows to the
observations (useful for maxBHHH).  The columns are summed internally.</p>
</td></tr>
<tr><td><code id="maxBFGS_+3A_hess">hess</code></td>
<td>
<p>Hessian of <code>fn</code>.  Not used by any of these methods, included for
compatibility with <code><a href="#topic+maxNR">maxNR</a></code>.</p>
</td></tr>
<tr><td><code id="maxBFGS_+3A_start">start</code></td>
<td>
<p>initial values for the parameters.  If start values
are named, those names are also carried over to the results.</p>
</td></tr>
<tr><td><code id="maxBFGS_+3A_fixed">fixed</code></td>
<td>
<p>parameters to be treated as constants at their
<code>start</code> values.  If present, it is treated as an index vector of
<code>start</code> parameters.</p>
</td></tr>
<tr><td><code id="maxBFGS_+3A_control">control</code></td>
<td>
<p>list of control parameters or a &lsquo;MaxControl&rsquo;
object.  If it is a list, the default values are used for the
parameters that are left unspecified by the user.
These functions accept the following parameters:
</p>

<dl>
<dt>reltol</dt><dd><p>sqrt(.Machine$double.eps), stopping
condition.  Relative convergence
tolerance: the algorithm stops if the relative improvement
between iterations is less than &lsquo;reltol&rsquo;.  Note: for
compatibility reason &lsquo;tol&rsquo; is equivalent to
&lsquo;reltol&rsquo; for optim-based optimizers.
</p>
</dd>
<dt>iterlim</dt><dd><p>integer, maximum number of iterations.
Default values are 200 for &lsquo;BFGS&rsquo;, 500
(&lsquo;CG&rsquo; and &lsquo;NM&rsquo;), and 10000 (&lsquo;SANN&rsquo;).
Note that
&lsquo;iteration&rsquo; may mean different things for different
optimizers.
</p>
</dd>
<dt>printLevel</dt><dd><p>integer, larger number prints more working
information.  Default 0, no information.
</p>
</dd>
<dt>nm_alpha</dt><dd><p>1, Nelder-Mead simplex method reflection
coefficient (see Nelder &amp; Mead, 1965)
</p>
</dd>
<dt>nm_beta</dt><dd><p>0.5, Nelder-Mead contraction coefficient</p>
</dd>
<dt>nm_gamma</dt><dd><p>2, Nelder-Mead expansion coefficient</p>
</dd>

<dt>sann_cand</dt><dd><p><code>NULL</code> or a function for <code>"SANN"</code> algorithm
to generate a new candidate point;
if <code>NULL</code>, Gaussian Markov kernel is used
(see argument <code>gr</code> of <code><a href="stats.html#topic+optim">optim</a></code>).</p>
</dd>
<dt>sann_temp</dt><dd><p>10, starting temperature
for the &ldquo;SANN&rdquo; cooling schedule.  See <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</dd>
<dt>sann_tmax</dt><dd><p>10, number of function evaluations at each temperature for
the &ldquo;SANN&rdquo; optimizer.  See <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</dd>
<dt>sann_randomSeed</dt><dd><p>123, integer to seed random numbers to
ensure replicability of &ldquo;SANN&rdquo; optimization and preserve
<code>R</code> random numbers.  Use
options like <code>sann_randomSeed=Sys.time()</code> or
<code>sann_randomSeed=sample(100,1)</code> if you want stochastic
results.
</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="maxBFGS_+3A_constraints">constraints</code></td>
<td>
<p>either <code>NULL</code> for unconstrained optimization
or a list with two components.  The components may be either
<code>eqA</code> and <code>eqB</code> for equality-constrained optimization
<code class="reqn">A \theta + B = 0</code>; or <code>ineqA</code> and
<code>ineqB</code> for inequality constraints <code class="reqn">A \theta + B &gt; 0</code>.  More
than one
row in <code>ineqA</code> and <code>ineqB</code> corresponds to more than
one linear constraint, in that case all these must be zero
(equality) or positive (inequality constraints).
The equality-constrained problem is forwarded
to <code><a href="#topic+sumt">sumt</a></code>, the inequality-constrained case to
<code><a href="#topic+constrOptim2">constrOptim2</a></code>.
</p>
</td></tr>
<tr><td><code id="maxBFGS_+3A_finalhessian">finalHessian</code></td>
<td>
<p>how (and if) to calculate the final Hessian.  Either
<code>FALSE</code> (not calculate), <code>TRUE</code> (use analytic/numeric
Hessian) or <code>"bhhh"</code>/<code>"BHHH"</code> for information equality
approach.  The latter approach is only suitable for maximizing
log-likelihood function.  It requires the gradient/log-likelihood to
be supplied by individual observations, see <code><a href="#topic+maxBHHH">maxBHHH</a></code> for
details. 
</p>
</td></tr>
<tr><td><code id="maxBFGS_+3A_parscale">parscale</code></td>
<td>
<p>A vector of scaling values for the parameters.
Optimization is performed on 'par/parscale' and these should
be comparable in the sense that a unit change in any element
produces about a unit change in the scaled value. (see
<code><a href="stats.html#topic+optim">optim</a></code>)</p>
</td></tr>
<tr><td><code id="maxBFGS_+3A_...">...</code></td>
<td>
<p>further arguments for <code>fn</code> and <code>grad</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In order to provide a consistent interface, all these functions also
accept arguments that other optimizers use.  For instance,
<code>maxNM</code> accepts the &lsquo;grad&rsquo; argument despite being a
gradient-less method.
</p>
<p>The &lsquo;state&rsquo; (or &lsquo;seed&rsquo;) of R's random number generator
is saved at the beginning of the <code>maxSANN</code> function
and restored at the end of this function
so this function does <em>not</em> affect the generation of random numbers
although the random seed is set to argument <code>random.seed</code>
and the &lsquo;SANN&rsquo; algorithm uses random numbers.
</p>


<h3>Value</h3>

<p>object of class &quot;maxim&quot;.  Data can be extracted through the following
functions: 
</p>
<table>
<tr><td><code>maxValue</code></td>
<td>
<p><code>fn</code> value at maximum (the last calculated value
if not converged.)</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>estimated parameter value.</p>
</td></tr>
<tr><td><code>gradient</code></td>
<td>
<p>vector, last calculated gradient value.  Should be
close to 0 in case of normal convergence.</p>
</td></tr>
<tr><td><code>estfun</code></td>
<td>
<p>matrix of gradients at parameter value <code>estimate</code>
evaluated at each observation (only if <code>grad</code> returns a matrix
or <code>grad</code> is not specified and <code>fn</code> returns a vector).</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>Hessian at the maximum (the last calculated value if
not converged).</p>
</td></tr>
<tr><td><code>returnCode</code></td>
<td>
<p>integer. Success code, 0 is success (see
<code><a href="stats.html#topic+optim">optim</a></code>).</p>
</td></tr>
<tr><td><code>returnMessage</code></td>
<td>
<p> a short message, describing the return code.</p>
</td></tr>
<tr><td><code>activePar</code></td>
<td>
<p>logical vector, which parameters are optimized over.
Contains only <code>TRUE</code>-s if no parameters are fixed.</p>
</td></tr>
<tr><td><code>nIter</code></td>
<td>
<p>number of iterations.  Two-element integer vector giving the number of
calls to <code>fn</code> and <code>gr</code>, respectively.
This excludes those calls needed to
compute the Hessian, if requested, and any calls to <code>fn</code> to compute a
finite-difference approximation to the gradient.</p>
</td></tr>
<tr><td><code>maximType</code></td>
<td>
<p>character string, type of maximization.</p>
</td></tr>
<tr><td><code>maxControl</code></td>
<td>
<p>the optimization control parameters in the form of a
<code><a href="#topic+MaxControl-class">MaxControl</a></code> object.</p>
</td></tr>
</table>
<p>The following components can only be extracted directly (with <code>\$</code>):
</p>
<table>
<tr><td><code>constraints</code></td>
<td>
<p>A list, describing the constrained optimization
(<code>NULL</code> if unconstrained).  Includes the following components:
</p>

<dl>
<dt>type</dt><dd><p>type of constrained optimization</p>
</dd>
<dt>outer.iterations</dt><dd><p>number of iterations in the constraints step</p>
</dd>
<dt>barrier.value</dt><dd><p>value of the barrier function</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ott Toomet, Arne Henningsen</p>


<h3>References</h3>

<p>Nelder, J. A. &amp; Mead, R. A, Simplex Method for Function
Minimization, The Computer Journal, 1965, 7, 308-313
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>, <code><a href="stats.html#topic+nlm">nlm</a></code>, <code><a href="#topic+maxNR">maxNR</a></code>,
<code><a href="#topic+maxBHHH">maxBHHH</a></code>, <code><a href="#topic+maxBFGSR">maxBFGSR</a></code> for a
<code><a href="#topic+maxNR">maxNR</a></code>-based BFGS implementation.</p>


<h3>Examples</h3>

<pre><code class='language-R'># Maximum Likelihood estimation of Poissonian distribution
n &lt;- rpois(100, 3)
loglik &lt;- function(l) n*log(l) - l - lfactorial(n)
# we use numeric gradient
summary(maxBFGS(loglik, start=1))
# you would probably prefer mean(n) instead of that ;-)
# Note also that maxLik is better suited for Maximum Likelihood
###
### Now an example of constrained optimization
###
f &lt;- function(theta) {
  x &lt;- theta[1]
  y &lt;- theta[2]
  exp(-(x^2 + y^2))
  ## you may want to use exp(- theta %*% theta) instead
}
## use constraints: x + y &gt;= 1
A &lt;- matrix(c(1, 1), 1, 2)
B &lt;- -1
res &lt;- maxNM(f, start=c(1,1), constraints=list(ineqA=A, ineqB=B),
control=list(printLevel=1))
print(summary(res))
</code></pre>

<hr>
<h2 id='MaxControl-class'>Class <code>"MaxControl"</code></h2><span id='topic+MaxControl-class'></span><span id='topic+maxControl'></span><span id='topic+maxControl+2CMaxControl-method'></span><span id='topic+maxControl+2Cmissing-method'></span><span id='topic+maxControl+2Cmaxim-method'></span><span id='topic+show+2CMaxControl-method'></span>

<h3>Description</h3>

<p>This is the structure that holds the optimization control options.
The corresponding constructors take
the parameters, perform consistency checks, and return the
control structure.  Alternatively, it overwrites the supplied
parameters in an existing <code>MaxControl</code> structure.  There is also
a method to extract the control structure from the estimated
&lsquo;maxim&rsquo;-objects.  
</p>


<h3>Slots</h3>

<p>The default values and definition of the slots:
</p>

<dl>
<dt>tol</dt><dd><p>1e-8, stopping condition 
for <code><a href="#topic+maxNR">maxNR</a></code> and related optimizers.
Stop if the absolute difference
between successive iterations is less than <code>tol</code>, returns
code 2.</p>
</dd>
<dt>reltol</dt><dd><p>sqrt(.Machine$double.eps), relative convergence
tolerance (used by <code><a href="#topic+maxNR">maxNR</a></code> related optimizers, and
<code><a href="stats.html#topic+optim">optim</a></code>-based optimizers.
The algorithm stops if
it iteration increases the value by less than a factor of
<code>reltol*(abs(val) + reltol)</code>.
Returns code 2.</p>
</dd>
<dt>gradtol</dt><dd><p>1e-6, stopping condition
for <code><a href="#topic+maxNR">maxNR</a></code> and related optimizers.
Stops if norm of the gradient is
less than <code>gradtol</code>, returns code 1.</p>
</dd>
<dt>steptol</dt><dd><p>1e-10, stopping/error condition
for <code><a href="#topic+maxNR">maxNR</a></code> and related optimizers.
If <code>qac == "stephalving"</code> and the quadratic
approximation leads to a worse, instead of a better value, or to
<code>NA</code>, the step length
is halved and a new attempt is made.  If necessary, this procedure is repeated
until <code>step &lt; steptol</code>, thereafter code 3 is returned.</p>
</dd>

<dt>lambdatol</dt><dd><p>1e-6, (for <code><a href="#topic+maxNR">maxNR</a></code> related
optimizers)
controls whether Hessian is treated as negative
definite.  If the
largest of the eigenvalues of the Hessian is larger than
<code>-lambdatol</code> (Hessian is not negative definite),
a suitable diagonal matrix is subtracted from the
Hessian (quadratic hill-climbing) in order to enforce negative
definiteness.</p>
</dd>

<dt>qac</dt><dd><p>&quot;stephalving&quot;, character, Qadratic Approximation
Correction for <code><a href="#topic+maxNR">maxNR</a></code> related optimizers.  When the new
guess is worse than the initial one, program attempts to correct it:
<code>"stephalving"</code> decreases the
step but keeps the direction.
<code>"marquardt"</code> uses
<cite>Marquardt (1963)</cite> method by decreasing the step length while also
moving closer to the pure gradient direction.  It may be faster and
more robust choice in areas where quadratic approximation behaves poorly.</p>
</dd>
<dt>qrtol</dt><dd><p>1e-10, QR-decomposition tolerance
for Hessian inversion in <code><a href="#topic+maxNR">maxNR</a></code> related optimizers.
</p>
</dd>
<dt>marquardt_lambda0</dt><dd><p>0.01, a positive numeric, initial correction term
for <cite>Marquardt (1963)</cite> correction in
<code><a href="#topic+maxNR">maxNR</a></code>-related optimizers</p>
</dd>
<dt>marquardt_lambdaStep</dt><dd><p>2, how much the <cite>Marquardt (1963)</cite>
correction is decreased/increased at
successful/unsuccesful step
for <code><a href="#topic+maxNR">maxNR</a></code> related optimizers</p>
</dd>
<dt>marquardt_maxLambda</dt><dd><p>1e12, maximum allowed correction term
for <code><a href="#topic+maxNR">maxNR</a></code> related optimizers.
If exceeded, the
algorithm exits with return code 3.</p>
</dd>

<dt>nm_alpha</dt><dd><p>1, Nelder-Mead simplex method reflection
factor (see Nelder &amp; Mead, 1965)</p>
</dd>
<dt>nm_beta</dt><dd><p>0.5, Nelder-Mead contraction factor</p>
</dd>
<dt>nm_gamma</dt><dd><p>2, Nelder-Mead expansion factor</p>
</dd>

<dt>sann_cand</dt><dd><p><code>NULL</code> or a function for <code>"SANN"</code> algorithm
to generate a new candidate point;
if <code>NULL</code>, Gaussian Markov kernel is used
(see argument <code>gr</code> of <code><a href="stats.html#topic+optim">optim</a></code>).</p>
</dd>
<dt>sann_temp</dt><dd><p>10, starting temperature
for the &ldquo;SANN&rdquo; cooling schedule.  See <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</dd>
<dt>sann_tmax</dt><dd><p>10, number of function evaluations at each temperature for
the &ldquo;SANN&rdquo; optimizer.  See <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</dd>
<dt>sann_randomSeed</dt><dd><p>123, integer to seed random numbers to
ensure replicability of &ldquo;SANN&rdquo; optimization and preserve
<code>R</code> random numbers.  Use
options like <code>SANN_randomSeed=Sys.time()</code> or
<code>SANN_randomeSeed=sample(1000,1)</code> if you want stochastic results.
</p>
</dd>

</dl>
<p>General options for stochastic gradient methods:
</p>
<dl>
<dt>SG_learningRate</dt><dd><p>0.1, learning rate, numeric</p>
</dd>
<dt>SG_batchSize</dt><dd><p><code>NULL</code>, batch size for Stochastic Gradient Ascent.  A
positive integer, or <code>NULL</code> for full-batch gradent ascent.</p>
</dd>
<dt>SG_clip</dt><dd><p><code>NULL</code>, gradient clipping threshold.  This is
the max allowed squared Euclidean norm of the gradient.  If the
actual norm of the gradient exceeds (square root of) this
threshold, the gradient will be scaled back accordingly while
preserving its direction.  <code>NULL</code> means no clipping.
</p>
</dd>
<dt>SG_patience</dt><dd><p><code>NULL</code>, or integer.  Stopping condition: if
the objective function is worse than its largest value so far this
many times, the algorithm stops, and returns not the last
parameter value but the one that
gave the best results so far.  This is mostly useful if gradient
is computed on training data and the
objective function on validation data.
</p>
</dd>
<dt>SG_patienceStep</dt><dd><p>1L, integer.  After how many epochs to check
the patience value.  1 means to check (and hence to compute the
objective function) at each epoch.
</p>
</dd>

</dl>
<p>Options for SGA:
</p>
<dl>
<dt>SGA_momentum</dt><dd><p>0, numeric momentum parameter for SGA.  Must lie
in interval <code class="reqn">[0,1]</code>.
</p>
</dd>

</dl>
<p>Options for Adam:
</p>
<dl>
<dt>Adam_momentum1</dt><dd><p>0.9, numeric in <code class="reqn">[0,1]</code>, the first moment momentum</p>
</dd>
<dt>Adam_momentum2</dt><dd><p>0.999, numeric in <code class="reqn">[0,1]</code>, the second moment momentum</p>
</dd>

</dl>
<p>General options:
</p>
<dl>
<dt>iterlim</dt><dd><p>150, stopping condition (the default differs for
different methods).  Stop if more than <code>iterlim</code>
iterations performed.  Note that &lsquo;iteration&rsquo; may mean
different things for different optimizers.</p>
</dd>
<dt>max.rows</dt><dd><p>20, maximum number of matrix rows to be printed when
requesting verbosity in the optimizers.
</p>
</dd>
<dt>max.cols</dt><dd><p>7, maximum number of columns to be printed.  This
also applies to vectors that are printed horizontally.
</p>
</dd>
<dt>printLevel</dt><dd><p>0, the level of verbosity.  Larger values print
more information.  Result depends on the optimizer.  Form
<code>print.level</code> is also accepted by the methods for
compatibility.</p>
</dd>
<dt>storeParameters</dt><dd><p><code>FALSE</code>, whether to store and return the
parameter
values at each epoch.  If <code>TRUE</code>, the stored values
can be retrieved with <code><a href="#topic+storedParameters">storedParameters</a></code>-method.  The
parameters are stored as a matrix with rows corresponding to the
epochs and columns to the parameter components.
</p>
</dd>
<dt>storeValues</dt><dd><p><code>FALSE</code>, whether to store and return the objective
function values at each epoch.  If <code>TRUE</code>, the stored values
can be retrieved with <code><a href="#topic+storedValues">storedValues</a></code>-method.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>maxControl</dt><dd><p><code>(...)</code> creates a &ldquo;MaxControl&rdquo; object.  The
arguments must be in the form <code>option1 = value1, option2 =
	value2, ...</code>.  The options should be slot names, but the method
also supports selected other parameter forms for compatibility reasons
e.g. &ldquo;print.level&rdquo; instead of &ldquo;printLevel&rdquo;.
In case there are more than one option with
similar name, the last one overwrites the previous values.  This
allows the user to override default parameters in the control
list.  See example in <a href="#topic+maxLik-package">maxLik-package</a>.
</p>
</dd>
<dt>maxControl</dt><dd><p><code>(x = "MaxControl", ...)</code> overwrites parameters
of an existing &ldquo;MaxControl&rdquo; object.  The &lsquo;...&rsquo;
argument must be in the form <code>option1 = value1, option2 =
	value2, ...</code>.  In case there are more than one option with
similar name, only the last one is taken into account.  This
allows the user to override default parameters in the control
list. See example in <a href="#topic+maxLik-package">maxLik-package</a>.
</p>
</dd>
<dt>maxControl</dt><dd><p><code>(x = "maxim")</code> extracts &ldquo;MaxControl&rdquo;
structure from an estimated model</p>
</dd>
<dt>show</dt><dd><p>shows the parameter values</p>
</dd>
</dl>



<h3>Details</h3>

<p>Typically, the control options are supplied in the form of a list, in which
case the corresponding default values are overwritten by the
user-specified ones.  However, one may also create the control
structure by <code>maxControl(opt1=value1, opt2=value2, ...)</code> and
supply such value directly to the optimizer.  In this case the
optimization routine takes all the values from the control object.
</p>


<h3>Note</h3>

<p>Several control parameters can also be supplied directly to the
optimization routines.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet
</p>


<h3>References</h3>


<ul>
<li><p> Nelder, J. A. &amp; Mead, R. A (1965) Simplex Method for Function
Minimization <em>The Computer Journal</em> <b>7</b>, 308&ndash;313
</p>
</li>
<li><p> Marquardt, D. W. (1963) An Algorithm for Least-Squares Estimation of
Nonlinear Parameters <em>Journal of the Society for Industrial and
Applied Mathematics</em> <b>11</b>, 431&ndash;441 
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(maxLik)
## Create a 'maxControl' object:
maxControl(tol=1e-4, sann_tmax=7, printLevel=2)

## Optimize quadratic form t(D) %*% W %*% D with p.d. weight matrix,
## s.t. constraints sum(D) = 1
quadForm &lt;- function(D) {
   return(-t(D) %*% W %*% D)
}
eps &lt;- 0.1
W &lt;- diag(3) + matrix(runif(9), 3, 3)*eps
D &lt;- rep(1/3, 3)
                        # initial values
## create control object and use it for optimization
co &lt;- maxControl(printLevel=2, qac="marquardt", marquardt_lambda0=1)
res &lt;- maxNR(quadForm, start=D, control=co)
print(summary(res))
## Now perform the same with no trace information
co &lt;- maxControl(co, printLevel=0)
res &lt;- maxNR(quadForm, start=D, control=co) # no tracing information
print(summary(res))  # should be the same as above
maxControl(res) # shows the control structure
</code></pre>

<hr>
<h2 id='maximType'>Type of Minimization/Maximization</h2><span id='topic+maximType'></span><span id='topic+maximType.default'></span><span id='topic+maximType.maxim'></span><span id='topic+maximType.MLEstimate'></span>

<h3>Description</h3>

<p>Returns the type of optimization as supplied by the optimisation routine.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  maximType(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maximType_+3A_x">x</code></td>
<td>
<p>object of class 'maxim' or another object which
involves numerical optimisation.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A text message, describing the involved optimisation algorithm
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxNR">maxNR</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## maximize two-dimensional exponential hat.  True maximum c(2,1):
f &lt;- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
m &lt;- maxNR(f, start=c(0,0))
coef(m)
maximType(m)
## Now use BFGS maximisation.
m &lt;- maxBFGS(f, start=c(0,0))
maximType(m)
</code></pre>

<hr>
<h2 id='maxLik'>Maximum likelihood estimation</h2><span id='topic+maxLik'></span><span id='topic+print.maxLik'></span>

<h3>Description</h3>

<p>This is the main interface for the <span class="pkg">maxLik</span> package, and the
function that performs Maximum
Likelihood estimation.  It is a wrapper for different optimizers
returning an object
of class &quot;maxLik&quot;.  Corresponding methods handle the
likelihood-specific properties of the estimates,  including standard
errors. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxLik(logLik, grad = NULL, hess = NULL, start, method,
constraints=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxLik_+3A_loglik">logLik</code></td>
<td>
<p>log-likelihood function.  Must have the parameter vector
as the first argument.  Must return either a single log-likelihood
value, or a numeric vector where each component is log-likelihood
of the corresponding individual observation.</p>
</td></tr>
<tr><td><code id="maxLik_+3A_grad">grad</code></td>
<td>
<p>gradient of log-likelihood.  Must have the parameter
vector as the first argument.  Must return either a single gradient
vector with length equal to the number of parameters, or a matrix
where each row is the gradient vector of the corresponding individual
observation.  If <code>NULL</code>, numeric gradient will be used.</p>
</td></tr>
<tr><td><code id="maxLik_+3A_hess">hess</code></td>
<td>
<p>hessian of log-likelihood.  Must have the parameter
vector as the first argument.  Must return a square matrix.  If
<code>NULL</code>, numeric Hessian will be used.</p>
</td></tr>
<tr><td><code id="maxLik_+3A_start">start</code></td>
<td>
<p>numeric vector, initial value of parameters.  If it has
names, these will also be used for naming the results.</p>
</td></tr>
<tr><td><code id="maxLik_+3A_method">method</code></td>
<td>
<p>maximisation method, currently either 
&quot;NR&quot; (for Newton-Raphson),
&quot;BFGS&quot; (for Broyden-Fletcher-Goldfarb-Shanno), 
&quot;BFGSR&quot; (for the BFGS algorithm implemented in <span class="rlang"><b>R</b></span>),
&quot;BHHH&quot; (for Berndt-Hall-Hall-Hausman), 
&quot;SANN&quot; (for Simulated ANNealing), 
&quot;CG&quot; (for Conjugate Gradients), 
or &quot;NM&quot; (for Nelder-Mead).  
Lower-case letters (such as &quot;nr&quot; for Newton-Raphson) are allowed.
The default method is &quot;NR&quot; for unconstrained problems, and &quot;NM&quot; or
&quot;BFGS&quot; for constrained problems, depending on if the <code>grad</code>
argument was provided.  &quot;BHHH&quot; is a good alternative given the
likelihood is returned observation-wise (see <code><a href="#topic+maxBHHH">maxBHHH</a></code>).
</p>
<p>Note that stochastic gradient ascent (SGA) is currently not supported
as this method seems to be rarely used for maximum likelihood estimation.
</p>
</td></tr>
<tr><td><code id="maxLik_+3A_constraints">constraints</code></td>
<td>
<p>either <code>NULL</code> for unconstrained maximization
or a list, specifying the constraints.  See <code><a href="#topic+maxBFGS">maxBFGS</a></code>.
</p>
</td></tr>
<tr><td><code id="maxLik_+3A_...">...</code></td>
<td>
<p>further arguments, such as <code>control</code>,
<code>iterlim</code>, or <code>tol</code>,
are passed to the selected maximisation routine,
i.e. <code><a href="#topic+maxNR">maxNR</a></code>, <code><a href="#topic+maxBFGS">maxBFGS</a></code>, <code><a href="#topic+maxBFGSR">maxBFGSR</a></code>,
<code><a href="#topic+maxBHHH">maxBHHH</a></code>, <code><a href="#topic+maxSANN">maxSANN</a></code>, <code><a href="#topic+maxCG">maxCG</a></code>,
or <code><a href="#topic+maxNM">maxNM</a></code>
(depending on argument <code>method</code>).  Arguments not used by the
optimizers are forwarded to <code>logLik</code>, <code>grad</code> and
<code>hess</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>maxLik</code> supports constrained optimization in the sense that
constraints are passed further to the underlying optimization
routines, and suitable default method is selected.  However, no
attempt is made to correct the resulting variance-covariance matrix.
Hence the inference may be wrong.  A corresponding warning is issued
by the summary method.
</p>


<h3>Value</h3>

<p>object of class 'maxLik' which inherits from class 'maxim'.
Useful methods
include
</p>

<ul>
<li> <p><code><a href="#topic+AIC.maxLik">AIC</a></code>: estimated parameter value
</p>
</li>
<li> <p><code><a href="#topic+coef.maxLik">coef</a></code>: estimated parameter value
</p>
</li>
<li> <p><code><a href="#topic+logLik.maxLik">logLik</a></code>: log-likelihood value
</p>
</li>
<li> <p><code><a href="#topic+nIter">nIter</a></code>: number of iterations
</p>
</li>
<li> <p><code><a href="#topic+stdEr.maxLik">stdEr</a></code>: standard errors
</p>
</li>
<li> <p><code><a href="#topic+summary.maxLik">summary</a></code>: print summary table
with estimates, standard errors, p, and z-values.
</p>
</li>
<li> <p><code><a href="#topic+vcov.maxLik">vcov</a></code>: variance-covariance matrix
</p>
</li></ul>



<h3>Warning</h3>

<p>The constrained maximum likelihood estimation should
be considered experimental.  In particular, the variance-covariance
matrix is not corrected for constrained parameter space.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet, Arne Henningsen</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxNR">maxNR</a></code>, <code><a href="stats.html#topic+nlm">nlm</a></code> and <code><a href="stats.html#topic+optim">optim</a></code>
for different non-linear optimisation routines, see
<code><a href="#topic+maxBFGS">maxBFGS</a></code> for the constrained maximization examples.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Estimate the parameter of exponential distribution
t &lt;- rexp(100, 2)
loglik &lt;- function(theta) log(theta) - theta*t
gradlik &lt;- function(theta) 1/theta - t
hesslik &lt;- function(theta) -100/theta^2
## Estimate with numeric gradient and hessian
a &lt;- maxLik(loglik, start=1, control=list(printLevel=2))
summary( a )
##
## Estimate with analytic gradient and hessian.
## require much smaller tolerance
## setting 'tol=0' or negative essentially disables this stopping criterion
a &lt;- maxLik(loglik, gradlik, hesslik, start=1,
            control=list(tol=-1, reltol=1e-12, gradtol=1e-12))
summary( a )
##
## Next, we give an example with vector argument:
## fit normal distribution by estimating mean and standard deviation
## by maximum likelihood
##
loglik &lt;- function(param) {
                           # param: vector of 2, c(mean, standard deviation)
   mu &lt;- param[1]
   sigma &lt;- param[2]
   ll &lt;- -0.5*N*log(2*pi) - N*log(sigma) - sum(0.5*(x - mu)^2/sigma^2)
                           # can use dnorm(x, mu, sigma, log=TRUE) instead
   ll
}
x &lt;- rnorm(100, 1, 2) # use mean=1, stdd=2
N &lt;- length(x)
res &lt;- maxLik(loglik, start=c(0,1)) # use 'wrong' start values
summary(res)
##
## Same example, but now with named parameters and a fixed value
##
resFix &lt;- maxLik(loglik, start=c(mu=0, sigma=1), fixed="sigma")
summary(resFix)  # 'sigma' is exactly 1.000 now.
</code></pre>

<hr>
<h2 id='maxLik-internal'> Internal maxLik Functions </h2><span id='topic+checkFuncArgs'></span><span id='topic+constrOptim2'></span><span id='topic+maximMessage'></span><span id='topic+maxNRCompute'></span><span id='topic+observationGradient'></span><span id='topic+print.summary.maxLik'></span><span id='topic+returnCode.maxim'></span>

<h3>Description</h3>

<p>Internal maxLik Functions
</p>


<h3>Details</h3>

<p>These are either various methods, or functions, not intended to be called
directly by the user (or in some cases are just
waiting for proper documentation to be written :).
</p>

<hr>
<h2 id='maxNR'>Newton- and Quasi-Newton Maximization</h2><span id='topic+maxNR'></span><span id='topic+maxBFGSR'></span><span id='topic+maxBHHH'></span>

<h3>Description</h3>

<p>Unconstrained and equality-constrained
maximization based on the quadratic approximation
(Newton) method.
The Newton-Raphson, BFGS (Broyden 1970, Fletcher 1970, Goldfarb 1970,
Shanno 1970), and BHHH (Berndt, Hall, Hall, Hausman 1974) methods
are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxNR(fn, grad = NULL, hess = NULL, start,
      constraints = NULL, finalHessian = TRUE, bhhhHessian=FALSE,
      fixed = NULL, activePar = NULL, control=NULL, ... )
maxBFGSR(fn, grad = NULL, hess = NULL, start,
      constraints = NULL, finalHessian = TRUE,
      fixed = NULL, activePar = NULL, control=NULL, ... )
maxBHHH(fn, grad = NULL, hess = NULL, start, 
      finalHessian = "BHHH", ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxNR_+3A_fn">fn</code></td>
<td>
<p>the function to be maximized.
It must have the parameter vector as the first argument and
it must return either a single number, or a numeric vector (this is
is summed internally).
If the BHHH method is used and argument <code>gradient</code> is not given,
<code>fn</code> must return a numeric vector of observation-specific
log-likelihood values.
If the parameters are out of range, <code>fn</code> should
return <code>NA</code>.  See details for constant parameters.
</p>
<p><code>fn</code> may also return attributes &quot;gradient&quot; and/or &quot;hessian&quot;.
If these attributes are set, the algorithm uses the corresponding
values as
gradient and Hessian.
</p>
</td></tr>
<tr><td><code id="maxNR_+3A_grad">grad</code></td>
<td>
<p>gradient of the objective function.
It must have the parameter vector as the first argument and
it must return either a gradient vector of the objective function,
or a matrix, where <em>columns</em> correspond to individual parameters.
The column sums are treated as gradient components.
If <code>NULL</code>, finite-difference gradients are computed.
If BHHH method is used, <code>grad</code> must return a matrix,
where rows corresponds to the gradient vectors for individual
observations and the columns to the individual parameters.
If <code>fn</code> returns an object with attribute <code>gradient</code>,
this argument is ignored.
</p>
</td></tr>
<tr><td><code id="maxNR_+3A_hess">hess</code></td>
<td>
<p>Hessian matrix of the function.
It must have the parameter vector as the first argument and
it must return the Hessian matrix of the objective function.
If missing, finite-difference Hessian, based on <code>gradient</code>,
is computed.
Hessian is used by the Newton-Raphson method only, and eventually by
the other methods if <code>finalHessian</code> is requested.</p>
</td></tr>
<tr><td><code id="maxNR_+3A_start">start</code></td>
<td>
<p>initial parameter values.  If start values
are named, those names are also carried over to the results.</p>
</td></tr>
<tr><td><code id="maxNR_+3A_constraints">constraints</code></td>
<td>
<p>either <code>NULL</code> for unconstrained optimization
or a list with two components.  The components may be either
<code>eqA</code> and <code>eqB</code> for equality-constrained optimization
<code class="reqn">A \theta + B = 0</code>; or <code>ineqA</code> and
<code>ineqB</code> for inequality constraints <code class="reqn">A \theta + B &gt; 0</code>.  More
than one
row in <code>ineqA</code> and <code>ineqB</code> corresponds to more than
one linear constraint, in that case all these must be zero
(equality) or positive (inequality constraints).
The equality-constrained problem is forwarded
to <code><a href="#topic+sumt">sumt</a></code>, the inequality-constrained case to
<code><a href="#topic+constrOptim2">constrOptim2</a></code>.
</p>
</td></tr>
<tr><td><code id="maxNR_+3A_finalhessian">finalHessian</code></td>
<td>
<p>how (and if) to calculate the final Hessian.  Either
<code>FALSE</code> (do not calculate), <code>TRUE</code> (use analytic/finite-difference
Hessian) or <code>"bhhh"</code>/<code>"BHHH"</code> for the information equality
approach.  The latter approach is only suitable for maximizing
log-likelihood functions.  It requires the gradient/log-likelihood to
be supplied by individual observations.
Note that computing the (actual, not BHHH) final Hessian
does not carry any extra penalty for the NR method,
but does for the other methods.</p>
</td></tr>
<tr><td><code id="maxNR_+3A_bhhhhessian">bhhhHessian</code></td>
<td>
<p>logical. Indicating whether to use the information
equality approximation (Bernd, Hall, Hall, and Hausman, 1974) for
the Hessian.  This effectively transforms <code>maxNR</code> into
<code>maxBHHH</code> and is mainly designed for internal use.</p>
</td></tr>
<tr><td><code id="maxNR_+3A_fixed">fixed</code></td>
<td>
<p>parameters to be treated as constants at their
<code>start</code> values.  If present, it is treated as an index vector of
<code>start</code> parameters.</p>
</td></tr>
<tr><td><code id="maxNR_+3A_activepar">activePar</code></td>
<td>
<p>this argument is retained for backward compatibility only;
please use argument <code>fixed</code> instead.</p>
</td></tr>
<tr><td><code id="maxNR_+3A_control">control</code></td>
<td>
<p>list of control parameters.  The control parameters
used by these optimizers are
</p>

<dl>
<dt>tol</dt><dd><p><code class="reqn">10^{-8}</code>,
stopping condition.  Stop if the absolute difference
between successive iterations is less than <code>tol</code>.  Return
<code>code=2</code>.
</p>
<p>If set to a negative value, the criterion is never fulfilled,
and hence disabled.
</p>
</dd>
<dt>reltol</dt><dd><p>sqrt(.Machine$double.eps), stopping
condition.  Relative convergence
tolerance: the algorithm stops if the relative improvement
between iterations is less than &lsquo;reltol&rsquo;.  Return code
8.  Negative value disables condition.
</p>
</dd>
<dt>gradtol</dt><dd><p>stopping condition.  Stop if norm of the gradient is
less than <code>gradtol</code>.  Return code 1.  Negative value
disables condition.</p>
</dd>
<dt>steptol</dt><dd><p>1e-10, stopping/error condition.
If <code>qac == "stephalving"</code> and the quadratic
approximation leads to a worse, instead of a better value, or to
<code>NA</code>, the step length
is halved and a new attempt is made.  If necessary, this procedure is repeated
until step &lt; <code>steptol</code>, thereafter code 3 is returned.</p>
</dd>
<dt>lambdatol</dt><dd><p><code class="reqn">10^{-6}</code>, 
controls whether Hessian is treated as negative
definite.  If the
largest of the eigenvalues of the Hessian is larger than
<code>-lambdatol</code> (Hessian is not negative definite),
a suitable diagonal matrix is subtracted from the
Hessian (quadratic hill-climbing) in order to enforce negative
definiteness.
</p>
</dd>
<dt>qrtol</dt><dd><p><code class="reqn">10^{-10}</code>,
QR-decomposition tolerance for the Hessian inversion.
</p>
</dd>
<dt>qac</dt><dd><p>&quot;stephalving&quot;, Quadratic Approximation Correction.  When the new
guess is worse than the initial one, the algorithm attemts to correct it:
&quot;stephalving&quot; decreases the
step but keeps the direction,
&quot;marquardt&quot; uses
<cite>Marquardt (1963)</cite> method by decreasing the step length while also
moving closer to the pure gradient direction.  It may be faster and
more robust choice in areas where quadratic approximation
behaves poorly.  <code>maxNR</code> and <code>maxBHHH</code> only.
</p>
</dd>
<dt>marquardt_lambda0</dt><dd><p><code class="reqn">10^{-2}</code>,
positive numeric, initial correction term for <cite>Marquardt (1963)</cite>
correction.
</p>
</dd>
<dt>marquardt_lambdaStep</dt><dd><p>2, how much the <cite>Marquardt
(1963)</cite>
correction term is
decreased/increased at each
successful/unsuccesful step.
<code>maxNR</code> and <code>maxBHHH</code> only.
</p>
</dd>
<dt>marquardt_maxLambda</dt><dd><p><code class="reqn">10^{12}</code>,
maximum allowed <cite>Marquardt (1963)</cite> correction term.  If exceeded, the
algorithm exits with return code 3.
<code>maxNR</code> and <code>maxBHHH</code> only.
</p>
</dd>
<dt>iterlim</dt><dd><p>stopping condition.  Stop if more than <code>iterlim</code>
iterations, return <code>code=4</code>.</p>
</dd>
<dt>printLevel</dt><dd><p>this argument determines the level of
printing which is done during the optimization process. The default
value 0 means that no printing occurs, 1 prints the
initial and final details, 2 prints all the
main tracing information for every iteration.  Higher
values will result in even more output.
</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="maxNR_+3A_...">...</code></td>
<td>
<p>further arguments to <code>fn</code>, <code>grad</code> and
<code>hess</code>.
Further arguments to <code>maxBHHH</code> are also passed to
<code>maxNR</code>.
To maintain compatibility with the earlier versions, ... also passes a
number of control options (<code>tol</code>, <code>reltol</code>,
<code>gradtol</code>, <code>steptol</code>,
<code>lambdatol</code>,  <code>qrtol</code>, <code>iterlim</code>) to the optimizers.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea of the Newton method is to approximate the function at a given
location by a multidimensional quadratic function, and use the estimated
maximum as the start value for the next iteration.  Such an
approximation requires knowledge of both gradient and Hessian, the
latter of which can be quite costly to compute.  Several methods for
approximating Hessian exist, including BFGS and BHHH.
</p>
<p>The BHHH (information equality) approximation is only valid for
log-likelihood functions.
It requires the score (gradient) values by individual observations and hence
those must be returned 
by individual observations by <code>grad</code> or <code>fn</code>.
The Hessian is approximated as the negative of the sum of the outer products
of the gradients of individual observations, or, in the matrix form,
</p>
<p style="text-align: center;"><code class="reqn">
  \mathsf{H}^{BHHH}
  =
  -\frac{1}{N} \sum_{i=1}^N
   \left[
    \frac{\partial \ell(\boldsymbol{\vartheta})}
    {\boldsymbol{\vartheta}}
    \frac{\partial \ell(\boldsymbol{\vartheta})}
    {\boldsymbol{\vartheta}'}
  \right]
  </code>
</p>

<p>The functions <code>maxNR</code>, <code>maxBFGSR</code>, and <code>maxBHHH</code>
can work with constant parameters, useful if a parameter value
converges to the boundary of support, or for testing.  
One way is to put
<code>fixed</code> to non-NULL, specifying which parameters should be treated as
constants.  The
parameters can also be fixed in runtime (only for <code>maxNR</code> and <code>maxBHHH</code>) by
signaling it with the 
<code>fn</code> return value.  See Henningsen &amp; Toomet (2011) for details.
</p>


<h3>Value</h3>

<p>object of class &quot;maxim&quot;.  Data can be extracted through the following
methods: 
</p>
<table>
<tr><td><code>\link{maxValue}</code></td>
<td>
<p><code>fn</code> value at maximum (the last calculated value
if not converged.)</p>
</td></tr>
<tr><td><code>\link[=coef.maxim]{coef}</code></td>
<td>
<p>estimated parameter value.</p>
</td></tr>
<tr><td><code>gradient</code></td>
<td>
<p>vector, last calculated gradient value.  Should be
close to 0 in case of normal convergence.</p>
</td></tr>
<tr><td><code>estfun</code></td>
<td>
<p>matrix of gradients at parameter value <code>estimate</code>
evaluated at each observation (only if <code>grad</code> returns a matrix
or <code>grad</code> is not specified and <code>fn</code> returns a vector).</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>Hessian at the maximum (the last calculated value if
not converged).</p>
</td></tr>
<tr><td><code>returnCode</code></td>
<td>
<p>return code:
</p>

<dl>
<dt>1</dt><dd><p> gradient close to zero (normal convergence).</p>
</dd>
<dt>2</dt><dd><p> successive function values within tolerance limit (normal
convergence).</p>
</dd>
<dt>3</dt><dd><p> last step could not find higher value (probably not
converged).  This is related to line search step getting too
small, usually because hitting the boundary of the parameter
space.  It may also be related to attempts to move to a wrong
direction because of numerical errors.  In some cases it can be
helped by changing <code>steptol</code>.</p>
</dd>
<dt>4</dt><dd><p> iteration limit exceeded.</p>
</dd>
<dt>5</dt><dd><p>infinite value.</p>
</dd>
<dt>6</dt><dd><p>infinite gradient.</p>
</dd>
<dt>7</dt><dd><p>infinite Hessian.</p>
</dd>
<dt>8</dt><dd><p>successive function values within relative tolerance
limit (normal convergence).</p>
</dd>
<dt>9</dt><dd><p>(BFGS) Hessian approximation cannot be improved because of
gradient did not change.  May be related to numerical
approximation problems or wrong analytic gradient.</p>
</dd>
<dt>100</dt><dd><p> Initial value out of range.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>returnMessage</code></td>
<td>
<p> a short message, describing the return code.</p>
</td></tr>
<tr><td><code>activePar</code></td>
<td>
<p>logical vector, which parameters are optimized over.
Contains only <code>TRUE</code>-s if no parameters are fixed.</p>
</td></tr>
<tr><td><code>nIter</code></td>
<td>
<p>number of iterations.</p>
</td></tr>
<tr><td><code>maximType</code></td>
<td>
<p>character string, type of maximization.</p>
</td></tr>
<tr><td><code>maxControl</code></td>
<td>
<p>the optimization control parameters in the form of a
<code><a href="#topic+MaxControl-class">MaxControl</a></code> object.</p>
</td></tr>
</table>
<p>The following components can only be extracted directly (with <code>\$</code>):
</p>
<table>
<tr><td><code>last.step</code></td>
<td>
<p>a list describing the last unsuccessful step if
<code>code=3</code> with following components:
</p>

<dl>
<dt>theta0</dt><dd><p> previous parameter value</p>
</dd>
<dt>f0</dt><dd> <p><code>fn</code> value at <code>theta0</code></p>
</dd>
<dt>climb</dt><dd><p> the movement vector to the maximum of the quadratic approximation</p>
</dd>
</dl>

</td></tr>
<tr><td><code>constraints</code></td>
<td>
<p>A list, describing the constrained optimization
(<code>NULL</code> if unconstrained).  Includes the following components:
</p>

<dl>
<dt>type</dt><dd><p> type of constrained optimization</p>
</dd>
<dt>outer.iterations</dt><dd><p> number of iterations in the constraints step</p>
</dd>
<dt>barrier.value</dt><dd><p> value of the barrier function</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Warning</h3>

<p>No attempt is made to ensure that user-provided analytic
gradient/Hessian is correct.  The users are
encouraged to use <code><a href="#topic+compareDerivatives">compareDerivatives</a></code> function,
designed for this purpose.  If analytic gradient/Hessian are wrong,
the algorithm may not converge, or may converge to a wrong point.
</p>
<p>As the BHHH method
uses the likelihood-specific information equality,
it is only suitable for maximizing log-likelihood functions!
</p>
<p>Quasi-Newton methods, including those mentioned above, do not work
well in non-concave regions.  This is especially the case with the
implementation in <code>maxBFGSR</code>.  The user is advised to
experiment with various tolerance options to achieve convergence.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet, Arne Henningsen,
function <code>maxBFGSR</code> was originally developed by Yves Croissant
(and placed in 'mlogit' package)</p>


<h3>References</h3>

<p>Berndt, E., Hall, B., Hall, R. and Hausman, J. (1974):
Estimation and Inference in Nonlinear Structural Models,
<em>Annals of Social Measurement</em> <b>3</b>, 653&ndash;665.
</p>
<p>Broyden, C.G. (1970):
The Convergence of a Class of Double-rank Minimization Algorithms,
<em>Journal of the Institute of Mathematics and Its Applications</em> <b>6</b>,
76&ndash;90.
</p>
<p>Fletcher, R. (1970):
A New Approach to Variable Metric Algorithms,
<em>Computer Journal</em> <b>13</b>, 317&ndash;322.
</p>
<p>Goldfarb, D. (1970):
A Family of Variable Metric Updates Derived by Variational Means,
<em>Mathematics of Computation</em> <b>24</b>, 23&ndash;26.
</p>
<p>Henningsen, A. and Toomet, O. (2011): maxLik: A package for maximum likelihood
estimation in R <em>Computational Statistics</em> <b>26</b>, 443&ndash;458
</p>
<p>Marquardt, D.W., (1963) An Algorithm for Least-Squares Estimation of
Nonlinear Parameters, <em>Journal of the Society for Industrial &amp;
Applied Mathematics</em> <b>11</b>, 2, 431&ndash;441
</p>
<p>Shanno, D.F. (1970):
Conditioning of Quasi-Newton Methods for Function Minimization,
<em>Mathematics of Computation</em> <b>24</b>, 647&ndash;656.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxLik">maxLik</a></code> for a general framework for maximum likelihood
estimation (MLE);
<code><a href="#topic+maxBHHH">maxBHHH</a></code> for maximizations using the Berndt, Hall, Hall,
Hausman (1974) algorithm (which is a wrapper function to <code>maxNR</code>);
<code><a href="#topic+maxBFGS">maxBFGS</a></code> for maximization using the BFGS, Nelder-Mead (NM),
and Simulated Annealing (SANN) method (based on <code><a href="stats.html#topic+optim">optim</a></code>),
also supporting inequality constraints;
<code><a href="stats.html#topic+nlm">nlm</a></code> for Newton-Raphson optimization; and
<code><a href="stats.html#topic+optim">optim</a></code> for different gradient-based optimization
methods.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Fit exponential distribution by ML
t &lt;- rexp(100, 2)  # create data with parameter 2
loglik &lt;- function(theta) sum(log(theta) - theta*t)
## Note the log-likelihood and gradient are summed over observations
gradlik &lt;- function(theta) sum(1/theta - t)
hesslik &lt;- function(theta) -100/theta^2
## Estimate with finite-difference gradient and Hessian
a &lt;- maxNR(loglik, start=1, control=list(printLevel=2))
summary(a)
## You would probably prefer 1/mean(t) instead ;-)

## The same example with analytic gradient and Hessian
a &lt;- maxNR(loglik, gradlik, hesslik, start=1)
summary(a)

## BFGS estimation with finite-difference gradient
a &lt;- maxBFGSR( loglik, start=1 )
summary(a)

## For the BHHH method we need likelihood values and gradients
## of individual observations, not the sum of those
loglikInd &lt;- function(theta) log(theta) - theta*t
gradlikInd &lt;- function(theta) 1/theta - t
## Estimate with analytic gradient
a &lt;- maxBHHH(loglikInd, gradlikInd, start=1)
summary(a)

## Example with a vector argument:  Estimate the mean and
## variance of a random normal sample by maximum likelihood
## Note: you might want to use maxLik instead
loglik &lt;- function(param) {
                           # param is a 2-vector of c(mean, sd)
  mu &lt;- param[1]
  sigma &lt;- param[2]
  ll &lt;- -0.5*N*log(2*pi) - N*log(sigma) - sum(0.5*(x - mu)^2/sigma^2)
  ll
}
x &lt;- rnorm(100, 1, 2) # use mean=1, sd=2
N &lt;- length(x)
res &lt;- maxNR(loglik, start=c(0,1)) # use 'wrong' start values
summary(res)

## The previous example with named parameters and a fixed value
resFix &lt;- maxNR(loglik, start=c(mu=0, sigma=1), fixed="sigma")
summary(resFix)  # 'sigma' is exactly 1.000 now.

### Constrained optimization
###
## We maximize exp(-x^2 - y^2) where x+y = 1
hatf &lt;- function(theta) {
  x &lt;- theta[1]
  y &lt;- theta[2]
  exp(-(x^2 + y^2))
  ## Note: you may prefer exp(- theta %*% theta) instead
}
## use constraints: x + y = 1
A &lt;- matrix(c(1, 1), 1, 2)
B &lt;- -1
res &lt;- maxNR(hatf, start=c(0,0), constraints=list(eqA=A, eqB=B),
             control=list(printLevel=1))
print(summary(res))
</code></pre>

<hr>
<h2 id='maxSGA'>Stochastic Gradient Ascent</h2><span id='topic+maxSGA'></span><span id='topic+maxAdam'></span>

<h3>Description</h3>

<p>Stochastic Gradient Ascent&ndash;based optimizers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxSGA(fn = NULL, grad = NULL, hess = NULL, start,
       nObs,
       constraints = NULL, finalHessian = FALSE, 
       fixed = NULL, control=NULL, ... )
maxAdam(fn = NULL, grad = NULL, hess = NULL, start,
        nObs,
        constraints = NULL, finalHessian = FALSE, 
        fixed = NULL, control=NULL, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxSGA_+3A_fn">fn</code></td>
<td>
<p>the function to be maximized.  As the objective function
values are not directly used for optimization, this argument is
optional, given <code>grad</code> is provided.
It must have the parameter vector as the first argument, and it must
have an argument <code>index</code> to specify the integer index of the selected
observations.  
It must return either a single number, or a numeric vector (this is
is summed internally).
If the parameters are out of range, <code>fn</code> should
return <code>NA</code>.  See details for constant parameters.
</p>
<p><code>fn</code> may also return attributes &quot;gradient&quot; and/or &quot;hessian&quot;.
If these attributes are set, the algorithm uses the corresponding
values as
gradient and Hessian.
</p>
</td></tr>
<tr><td><code id="maxSGA_+3A_grad">grad</code></td>
<td>
<p>gradient of the objective function.
It must have the parameter vector as the first argument, and it must
have an argument <code>index</code> to specify the integer index of selected
observations.
It must return either a gradient vector of the objective function,
or a matrix, where columns correspond to individual parameters.
The column sums are treated as gradient components.
If <code>NULL</code>, finite-difference gradients are computed.
If <code>fn</code> returns an object with attribute <code>gradient</code>,
this argument is ignored.
</p>
<p>If <code>grad</code> is not supplied, it is computed by finite-difference
method using <code>fn</code>.  However, this is only adviseable for
small-scale tests, not for any production run.  Obviously, <code>fn</code>
must be correctly defined in that case.
</p>
</td></tr>
<tr><td><code id="maxSGA_+3A_hess">hess</code></td>
<td>
<p>Hessian matrix of the function.  Mainly for compatibility
reasons, only used for computing the final Hessian if asked to do
so by setting <code>finalHessian</code> to <code>TRUE</code>. 
It must have the parameter vector as the first argument and
it must return the Hessian matrix of the objective function.
If missing, either finite-difference Hessian, based on
<code>gradient</code> or BHHH approach
is computed if asked to do so.
</p>
</td></tr>
<tr><td><code id="maxSGA_+3A_start">start</code></td>
<td>
<p>initial parameter values.  If these have names, the
names are also used for results.</p>
</td></tr>
<tr><td><code id="maxSGA_+3A_nobs">nObs</code></td>
<td>
<p>number of observations.  This is used to partition the data
into individual batches.  The resulting batch
indices are forwarded to the <code>grad</code> function through the
argument <code>index</code>.</p>
</td></tr>
<tr><td><code id="maxSGA_+3A_constraints">constraints</code></td>
<td>
<p>either <code>NULL</code> for unconstrained optimization
or a list with two components.  The components may be either
<code>eqA</code> and <code>eqB</code> for equality-constrained optimization
<code class="reqn">A \theta + B = 0</code>; or <code>ineqA</code> and
<code>ineqB</code> for inequality constraints <code class="reqn">A \theta + B &gt; 0</code>.  More
than one
row in <code>ineqA</code> and <code>ineqB</code> corresponds to more than
one linear constraint, in that case all these must be zero
(equality) or positive (inequality constraints).
The equality-constrained problem is forwarded
to <code><a href="#topic+sumt">sumt</a></code>, the inequality-constrained case to
<code><a href="#topic+constrOptim2">constrOptim2</a></code>.
</p>
</td></tr>
<tr><td><code id="maxSGA_+3A_finalhessian">finalHessian</code></td>
<td>
<p>how (and if) to calculate the final Hessian.  Either
<code>FALSE</code> (do not calculate), <code>TRUE</code> (use analytic/finite-difference
Hessian) or <code>"bhhh"</code>/<code>"BHHH"</code> for the information equality
approach.  The latter approach is only suitable when working with a
log-likelihood function, and it requires the gradient/log-likelihood to
be supplied by individual observations.
</p>
<p>Hessian matrix is not often used for optimization problems where one
applies SGA, but even if one is not interested in standard errors,
it may provide useful information about the model performance.  If
computed by finite-difference method, the Hessian computation may be
very slow.
</p>
</td></tr>
<tr><td><code id="maxSGA_+3A_fixed">fixed</code></td>
<td>
<p>parameters to be treated as constants at their
<code>start</code> values.  If present, it is treated as an index vector of
<code>start</code> parameters.</p>
</td></tr>
<tr><td><code id="maxSGA_+3A_control">control</code></td>
<td>
<p>list of control parameters.  The ones
used by these optimizers are
</p>

<dl>
<dt>SGA_momentum</dt><dd><p>0, numeric momentum parameter for SGA.  Must lie
in interval <code class="reqn">[0,1]</code>.  See details.
</p>
</dd>
</dl>
<p>Adam-specific parameters
</p>
<dl>
<dt>Adam_momentum1</dt><dd><p>0.9, numeric in interval <code class="reqn">(0,1)</code>, the first moment momentum</p>
</dd>
<dt>Adam_momentum2</dt><dd><p>0.999, numeric in interval <code class="reqn">(0,1)</code>, the second moment momentum</p>
</dd>
</dl>
<p>General stochastic gradient parameters:
</p>
<dl>
<dt>SG_learningRate</dt><dd><p>step size the SGA algorithm takes in the
gradient direction.  If 1, the step equals to the gradient value.  A
good value is often 0.01&ndash;0.3</p>
</dd>
<dt>SG_batchSize</dt><dd><p>SGA batch size, an integer between 1 and
<code>nObs</code>.
If <code>NULL</code> (default), the full batch gradient is computed.
</p>
</dd>
<dt>SG_clip</dt><dd><p><code>NULL</code>, gradient clipping threshold.  The
algorithm ensures that <code class="reqn">||g(\theta)||_2^2 \le
	  \kappa</code> where <code class="reqn">\kappa</code> is
the <code>SG_clip</code> value.  If the
actual norm of the gradient exceeds (square root of)
<code class="reqn">\kappa</code>,
the gradient will be scaled back accordingly while
preserving its direction.  <code>NULL</code> means no clipping.
</p>
</dd>
</dl>
<p>Stopping conditions:
</p>
<dl>
<dt>gradtol</dt><dd><p>stopping condition.  Stop if norm of the gradient is
less than <code>gradtol</code>.  Default 0, i.e. do not use this
condition.  This condition is useful if the
objective is to drive full batch gradient to zero on training data.
It is not a good objective in case of the stochastic
gradient, and if the objective is to optimize the objective on
validation data.
</p>
</dd>
<dt>SG_patience</dt><dd><p><code>NULL</code>, or integer.  Stopping condition:
the algorithm counts how many times
the objective function has been worse than its best value so
far, and if this exceeds <code>SG_patience</code>, the algorithm stops.
</p>
</dd>
<dt>SG_patienceStep</dt><dd><p>1L, integer.  After how many epochs to check
the patience value.  <code>1</code> means to check at each epoch, and hence to compute the
objective function.  This may be undesirable if the objective
function is costly to compute.
</p>
</dd>
<dt>iterlim</dt><dd><p>stopping condition.  Stop if more than <code>iterlim</code>
epochs, return <code>code=4</code>.
Epoch is a set of iterations that cycles through all
observations.  In case of full batch, iterations and epochs are
equivalent.  If <code>iterlim = 0</code>, does not do any learning and
returns the initial values unchanged.
</p>
</dd>
<dt>printLevel</dt><dd><p>this argument determines the level of
printing which is done during the optimization process. The default
value 0 means that no printing occurs, 1 prints the
initial and final details, 2 prints all the
main tracing information for every epoch.  Higher
values will result in even more output.
</p>
</dd>
<dt>storeParameters</dt><dd><p>logical, whether to store and return the
parameter
values at each epoch.  If <code>TRUE</code>, the stored values
can be retrieved with <code><a href="#topic+storedParameters">storedParameters</a></code>-method.  The
parameters are stored as a matrix with rows corresponding to the
epochs and columns to the parameter components.  There are
<code>iterlim</code> + 1 rows, where the first one corresponds to the
initial parameters.
</p>
<p>Default <code>FALSE</code>.
</p>
</dd>
<dt>storeValues</dt><dd><p>logical, whether to store and return the objective
function values at each epoch.  If <code>TRUE</code>, the stored values
can be retrieved with <code><a href="#topic+storedValues">storedValues</a></code>-method.  There are
<code>iterlim</code> + 1 values, where the first one corresponds to
the value at the
initial parameters.
</p>
<p>Default <code>FALSE</code>.
</p>
</dd>
</dl>

<p>See <code><a href="#topic+maxControl">maxControl</a></code> for more information.
</p>
</td></tr>
<tr><td><code id="maxSGA_+3A_...">...</code></td>
<td>
<p>further arguments to <code>fn</code>, <code>grad</code> and
<code>hess</code>.
To maintain compatibility with the earlier versions, ... also
passes certain control options to the optimizers.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gradient Ascent (GA) is a optimization method where the algorithm
repeatedly takes small steps in the gradient's direction, the
parameter vector <code class="reqn">\theta</code> is updated as <code class="reqn">\theta
    \leftarrow theta + \mathrm{learning rate}\cdot \nabla
    f(\theta)</code>.
In case of Stochastic GA (SGA), the gradient is not computed on the
full set of observations but on a small subset, <em>batch</em>,
potentially a single observation only.  In certain circumstances
this converges much faster
than when using all observation (see
<cite>Bottou et al, 2018</cite>).
</p>
<p>If <code>SGA_momentum</code> is positive, the SGA algorithm updates the parameters
<code class="reqn">\theta</code> in two steps.  First, the momentum is used to update
the &ldquo;velocity&rdquo; <code class="reqn">v</code> as
<code class="reqn">v \leftarrow \mathrm{momentum}\cdot v + \mathrm{learning
      rate}\cdot \nabla f(\theta)</code>, and thereafter the parameter
<code class="reqn">\theta</code> is updates as
<code class="reqn">\theta \leftarrow \theta + v</code>.  Initial
velocity is set to 0.
</p>
<p>The Adam algorithm is more complex and uses first and second moments
of stochastic gradients to automatically adjust the learning rate.
See <cite>Goodfellow et al, 2016, page 301</cite>.
</p>
<p>The function <code>fn</code> is not directly used for optimization, only
for printing or as a stopping condition.  In this sense
it is up to the user to decide what the function
returns, if anything.  For instance, it may be useful for <code>fn</code> to compute the
objective function on either full training data, or on validation data,
and just ignore the <code>index</code> argument.  The latter is useful if
using <em>patience</em>-based stopping.
However, one may also
choose to select the observations determined by the index to
compute the objective function on the current data batch.

</p>


<h3>Value</h3>

<p>object of class &quot;maxim&quot;.  Data can be extracted through the following
methods: 
</p>
<table>
<tr><td><code>\link{maxValue}</code></td>
<td>
<p><code>fn</code> value at maximum (the last calculated value
if not converged.)</p>
</td></tr>
<tr><td><code>\link{coef}</code></td>
<td>
<p>estimated parameter value.</p>
</td></tr>
<tr><td><code>\link{gradient}</code></td>
<td>
<p>vector, last calculated gradient value.  Should be
close to 0 in case of normal convergence.</p>
</td></tr>
<tr><td><code>estfun</code></td>
<td>
<p>matrix of gradients at parameter value <code>estimate</code>
evaluated at each observation (only if <code>grad</code> returns a matrix
or <code>grad</code> is not specified and <code>fn</code> returns a vector).</p>
</td></tr>
<tr><td><code>\link{hessian}</code></td>
<td>
<p>Hessian at the maximum (the last calculated value if
not converged).</p>
</td></tr>
<tr><td><code>\link{storedValues}</code></td>
<td>
<p>return values stored at each epoch</p>
</td></tr>
<tr><td><code>\link{storedParameters}</code></td>
<td>
<p>return parameters stored at each epoch</p>
</td></tr>
<tr><td><code>\link{returnCode}</code></td>
<td>

<p>a numeric code that describes the convergence or error.
</p>
</td></tr>
<tr><td><code>\link{returnMessage}</code></td>
<td>
<p>a short message, describing the return code.</p>
</td></tr>
<tr><td><code>\link{activePar}</code></td>
<td>
<p>logical vector, which parameters are optimized over.
Contains only <code>TRUE</code>-s if no parameters are fixed.</p>
</td></tr>
<tr><td><code>\link{nIter}</code></td>
<td>
<p>number of iterations.</p>
</td></tr>
<tr><td><code>\link{maximType}</code></td>
<td>
<p>character string, type of maximization.</p>
</td></tr>
<tr><td><code>\link{maxControl}</code></td>
<td>
<p>the optimization control parameters in the form of a
<code><a href="#topic+MaxControl-class">MaxControl</a></code> object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ott Toomet, Arne Henningsen</p>


<h3>References</h3>

<p>Bottou, L.; Curtis, F. &amp; Nocedal, J.:
Optimization Methods for
Large-Scale Machine Learning <em>SIAM Review</em>, 2018, <b>60</b>,
223&ndash;311.
</p>
<p>Goodfellow, I.; Bengio, Y.; Courville, A. (2016): Deep Learning,
<em>MIT Press</em>
</p>
<p>Henningsen, A. and Toomet, O. (2011): maxLik: A package for maximum likelihood
estimation in R <em>Computational Statistics</em> <b>26</b>, 443&ndash;458
</p>


<h3>See Also</h3>

<p>A good starting point to learn about the usage of stochastic gradient
ascent in <span class="pkg">maxLik</span> package is the vignette &ldquo;Stochastic
Gradient Ascent in maxLik&rdquo;.
</p>
<p>The other related functions are
<code><a href="#topic+maxNR">maxNR</a></code> for Newton-Raphson, a popular Hessian-based maximization;
<code><a href="#topic+maxBFGS">maxBFGS</a></code> for maximization using the BFGS, Nelder-Mead (NM),
and Simulated Annealing (SANN) method (based on <code><a href="stats.html#topic+optim">optim</a></code>),
also supporting inequality constraints;
<code><a href="#topic+maxLik">maxLik</a></code> for a general framework for maximum likelihood
estimation (MLE);
<code><a href="stats.html#topic+optim">optim</a></code> for different gradient-based optimization
methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## estimate the exponential distribution parameter by ML
set.seed(1)
t &lt;- rexp(100, 2)
loglik &lt;- function(theta, index) sum(log(theta) - theta*t[index])
## Note the log-likelihood and gradient are summed over observations
gradlik &lt;- function(theta, index) sum(1/theta - t[index])
## Estimate with full-batch
a &lt;- maxSGA(loglik, gradlik, start=1, control=list(iterlim=1000,
            SG_batchSize=10), nObs=100)
            # note that loglik is not really needed, and is not used
            # here, unless more print verbosity is asked
summary(a)
##
## demonstrate the usage of index, and using
## fn for computing the objective function on validation data.
## Create a linear model where variables are very unequally scaled
##
## OLS loglik function: compute the function value on validation data only
loglik &lt;- function(beta, index) {
   e &lt;- yValid - XValid %*% beta
   -crossprod(e)/length(y)
}
## OLS gradient: compute it on training data only
## Use 'index' to select the subset corresponding to the minibatch
gradlik &lt;- function(beta, index)  {
   e &lt;- yTrain[index] - XTrain[index,,drop=FALSE] %*% beta
   g &lt;- t(-2*t(XTrain[index,,drop=FALSE]) %*% e)
   -g/length(index)
}
N &lt;- 1000
## two random variables: one with scale 1, the other with 100
X &lt;- cbind(rnorm(N), rnorm(N, sd=100))
beta &lt;- c(1, 1)  # true parameter values
y &lt;- X %*% beta + rnorm(N, sd=0.2)
## training-validation split
iTrain &lt;- sample(N, 0.8*N)
XTrain &lt;- X[iTrain,,drop=FALSE]
XValid &lt;- X[-iTrain,,drop=FALSE]
yTrain &lt;- y[iTrain]
yValid &lt;- y[-iTrain]
##
## do this without momentum: learning rate must stay small for the gradient not to explode
cat("  No momentum:\n")
a &lt;- maxSGA(loglik, gradlik, start=c(10,10),
           control=list(printLevel=1, iterlim=50,
                        SG_batchSize=30, SG_learningRate=0.0001, SGA_momentum=0
                        ), nObs=length(yTrain))
print(summary(a))  # the first component is off, the second one is close to the true value
## do with momentum 0.99
cat("  Momentum 0.99:\n")
a &lt;- maxSGA(loglik, gradlik, start=c(10,10),
           control=list(printLevel=1, iterlim=50,
                        SG_batchSize=30, SG_learningRate=0.0001, SGA_momentum=0.99
                        # no momentum
                        ), nObs=length(yTrain))
print(summary(a))  # close to true value
</code></pre>

<hr>
<h2 id='maxValue'>Function value at maximum</h2><span id='topic+maxValue'></span><span id='topic+maxValue.maxim'></span>

<h3>Description</h3>

<p>Returns the function value at (estimated) maximum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxValue(x, ...)
## S3 method for class 'maxim'
maxValue(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxValue_+3A_x">x</code></td>
<td>
<p>a statistical model, or a result of maximisation, created
by <code><a href="#topic+maxLik">maxLik</a></code>, <code><a href="#topic+maxNR">maxNR</a></code> or another optimizer.</p>
</td></tr>
<tr><td><code id="maxValue_+3A_...">...</code></td>
<td>
<p>further arguments for other methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric, the value of the objective function at maximum.  In general,
it is the last calculated value in case the process did not converge.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxLik">maxLik</a></code>, <code><a href="#topic+maxNR">maxNR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Estimate the exponential distribution parameter:
t &lt;- rexp(100, 2)
loglik &lt;- function(theta) sum(log(theta) - theta*t)
## Estimate with numeric gradient and numeric Hessian
a &lt;- maxNR(loglik, start=1)
maxValue(a)
</code></pre>

<hr>
<h2 id='nIter'>Return number of iterations for iterative models</h2><span id='topic+nIter'></span><span id='topic+nIter.default'></span>

<h3>Description</h3>

<p>Returns the number of iterations for iterative models.  The default
method assumes presence of a component <code>iterations</code> in <code>x</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nIter(x, ...)
## Default S3 method:
nIter(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nIter_+3A_x">x</code></td>
<td>
<p>a statistical model, or a result of maximisation, created
by <code><a href="#topic+maxLik">maxLik</a></code>, <code><a href="#topic+maxNR">maxNR</a></code> or another optimizer.</p>
</td></tr>
<tr><td><code id="nIter_+3A_...">...</code></td>
<td>
<p>further arguments for methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function.  The default method returns the component
<code>x$iterations</code>.
</p>


<h3>Value</h3>

<p>numeric, number of iterations.  Note that &lsquo;iteration&rsquo; may mean
different things for different optimizers.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxLik">maxLik</a></code>, <code><a href="#topic+maxNR">maxNR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Estimate the exponential distribution parameter:
t &lt;- rexp(100, 2)
loglik &lt;- function(theta) sum(log(theta) - theta*t)
## Estimate with numeric gradient and numeric Hessian
a &lt;- maxNR(loglik, start=1)
nIter(a)
</code></pre>

<hr>
<h2 id='nObs.maxLik'>Number of Observations</h2><span id='topic+nObs.maxLik'></span>

<h3>Description</h3>

<p>Returns the number of observations for statistical models,
estimated by Maximum Likelihood using <code><a href="#topic+maxLik">maxLik</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxLik'
nObs(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nObs.maxLik_+3A_x">x</code></td>
<td>
<p>a statistical model estimated by Maximum Likelihood
using <code><a href="#topic+maxLik">maxLik</a></code>.</p>
</td></tr>
<tr><td><code id="nObs.maxLik_+3A_...">...</code></td>
<td>
<p>further arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>nObs</code> method for &ldquo;maxLik&rdquo; objects
can return the number of observations only if log-likelihood function
(or the gradient) returns values by individual observation.
</p>


<h3>Value</h3>

<p>numeric, number of observations
</p>


<h3>Author(s)</h3>

<p>Arne Henningsen, Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="miscTools.html#topic+nObs">nObs</a></code>, <code><a href="#topic+maxLik">maxLik</a></code>,
<code><a href="miscTools.html#topic+nParam">nParam</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## fit a normal distribution by ML
# generate a variable from normally distributed random numbers
x &lt;- rnorm( 100, 1, 2 )
# log likelihood function (for individual observations)
llf &lt;- function( param ) {
   return( dnorm( x, mean = param[ 1 ], sd = param[ 2 ], log = TRUE ) )
}
## ML method
ml &lt;- maxLik( llf, start = c( mu = 0, sigma = 1 ) )
# return number of onservations
nObs( ml )
</code></pre>

<hr>
<h2 id='nParam.maxim'>Number of model parameters</h2><span id='topic+nParam.maxim'></span>

<h3>Description</h3>

<p>This function returns the number of model parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxim'
nParam(x, free=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nParam.maxim_+3A_x">x</code></td>
<td>
<p>a model returned by a maximisation method from the <span class="pkg">maxLik</span>
package.</p>
</td></tr>
<tr><td><code id="nParam.maxim_+3A_free">free</code></td>
<td>
<p>logical, whether to report only the free parameters or the
total number of parameters (default)</p>
</td></tr>
<tr><td><code id="nParam.maxim_+3A_...">...</code></td>
<td>
<p>other arguments for methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Free parameters are the parameters with no equality
restrictions.  Some parameters may be jointly restricted (e.g. sum of two
probabilities equals unity).   In this case the
total number of parameters may depend on the
normalization. 
</p>


<h3>Value</h3>

<p>Number of parameters in the model
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="miscTools.html#topic+nObs">nObs</a></code> for number of observations</p>


<h3>Examples</h3>

<pre><code class='language-R'>## fit a normal distribution by ML
# generate a variable from normally distributed random numbers
x &lt;- rnorm( 100, 1, 2 )
# log likelihood function (for individual observations)
llf &lt;- function( param ) {
   return( dnorm( x, mean = param[ 1 ], sd = param[ 2 ], log = TRUE ) )
}
## ML method
ml &lt;- maxLik( llf, start = c( mu = 0, sigma = 1 ) )
# return number of parameters
nParam( ml )
</code></pre>

<hr>
<h2 id='numericGradient'>Functions to Calculate Numeric Derivatives</h2><span id='topic+numericGradient'></span><span id='topic+numericHessian'></span><span id='topic+numericNHessian'></span>

<h3>Description</h3>

<p>Calculate (central) numeric gradient and Hessian, including of
vector-valued functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>numericGradient(f, t0, eps=1e-06, fixed, ...)
numericHessian(f, grad=NULL, t0, eps=1e-06, fixed, ...)
numericNHessian(f, t0, eps=1e-6, fixed, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="numericGradient_+3A_f">f</code></td>
<td>
<p>function to be differentiated.  The first argument must be
the parameter vector with respect to which it is differentiated.
For numeric gradient, <code>f</code> may return a (numeric) vector, for Hessian it
should return a numeric scalar</p>
</td></tr>
<tr><td><code id="numericGradient_+3A_grad">grad</code></td>
<td>
<p>function, gradient of <code>f</code></p>
</td></tr>
<tr><td><code id="numericGradient_+3A_t0">t0</code></td>
<td>
<p>vector, the parameter values</p>
</td></tr>
<tr><td><code id="numericGradient_+3A_eps">eps</code></td>
<td>
<p>numeric, the step for numeric differentiation</p>
</td></tr>
<tr><td><code id="numericGradient_+3A_fixed">fixed</code></td>
<td>
<p>logical index vector, fixed parameters.
Derivative is calculated only with respect to the parameters
for which <code>fixed == FALSE</code>, <code>NA</code> is returned for the fixed
parameters.  If
missing, all parameters are treated as active.</p>
</td></tr>
<tr><td><code id="numericGradient_+3A_...">...</code></td>
<td>
<p>furter arguments for <code>f</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>numericGradient</code> numerically differentiates a (vector valued)
function with respect to it's (vector valued) argument.  If the
functions value is a <code class="reqn">N_{val} \times 1</code>
vector and the argument is
<code class="reqn">N_{par} \times 1</code> vector, the resulting
gradient
is a <code class="reqn">N_{val} \times N_{par}</code>
matrix. 
</p>
<p><code>numericHessian</code> checks whether a gradient function is present.
If yes, it calculates the gradient of the gradient, if not, it
calculates the full
numeric Hessian (<code>numericNHessian</code>).
</p>


<h3>Value</h3>

<p>Matrix.  For <code>numericGradient</code>, the number of rows is equal to the
length of the function value vector, and the number of columns is
equal to the length of the parameter vector.
</p>
<p>For the <code>numericHessian</code>, both numer of rows and columns is
equal to the length of the parameter vector.
</p>


<h3>Warning</h3>

<p>Be careful when using numerical differentiation in optimization
routines.  Although quite precise in simple cases, they may work very
poorly in more complicated conditions.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="#topic+compareDerivatives">compareDerivatives</a></code>, <code><a href="stats.html#topic+deriv">deriv</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># A simple example with Gaussian bell surface
f0 &lt;- function(t0) exp(-t0[1]^2 - t0[2]^2)
numericGradient(f0, c(1,2))
numericHessian(f0, t0=c(1,2))

# An example with the analytic gradient
gradf0 &lt;- function(t0) -2*t0*f0(t0)
numericHessian(f0, gradf0, t0=c(1,2))
# The results should be similar as in the previous case

# The central numeric derivatives are often quite precise
compareDerivatives(f0, gradf0, t0=1:2)
# The difference is around 1e-10
</code></pre>

<hr>
<h2 id='objectiveFn'>Optimization Objective Function</h2><span id='topic+objectiveFn'></span><span id='topic+objectiveFn.maxim'></span>

<h3>Description</h3>

<p>This function returns the optimization objective function from a
&lsquo;maxim&rsquo; object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objectiveFn(x, ...)
## S3 method for class 'maxim'
objectiveFn(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="objectiveFn_+3A_x">x</code></td>
<td>
<p>an optimization result, inheriting from class &lsquo;maxim&rsquo;</p>
</td></tr>
<tr><td><code id="objectiveFn_+3A_...">...</code></td>
<td>
<p>other arguments for methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>function, the function that was optimized.  It can be directly called,
given that all necessary variables are accessible from the current environment.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>Examples</h3>

<pre><code class='language-R'>hatf &lt;- function(theta) exp(- theta %*% theta)
res &lt;- maxNR(hatf, start=c(0,0))
print(summary(res))
print(objectiveFn(res))
print(objectiveFn(res)(2)) # 0.01832
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+tidy'></span><span id='topic+glance'></span>

<h3>Description</h3>

<p>These objects are imported from the &quot;generics&quot; package.
See <code><a href="generics.html#topic+tidy">tidy</a></code> and
<code><a href="generics.html#topic+glance">glance</a></code> for details.
</p>

<hr>
<h2 id='returnCode'>Success or failure of the optimization</h2><span id='topic+returnCode'></span><span id='topic+returnCode.default'></span><span id='topic+returnCode.maxLik'></span><span id='topic+returnMessage'></span><span id='topic+returnMessage.default'></span><span id='topic+returnMessage.maxim'></span><span id='topic+returnMessage.maxLik'></span>

<h3>Description</h3>

<p>These function extract success or failure information from optimization objects.
The <code>returnCode</code> gives a numeric code, and <code>returnMessage</code> a
brief description about the success or
failure of the optimization, and point to the problems occured (see
documentation for the
corresponding functions).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>returnCode(x, ...)
## Default S3 method:
returnCode(x, ...)
## S3 method for class 'maxLik'
returnCode(x, ...)
returnMessage(x, ...)
## S3 method for class 'maxim'
returnMessage(x, ...)
## S3 method for class 'maxLik'
returnMessage(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="returnCode_+3A_x">x</code></td>
<td>
<p>object, usually an optimization result</p>
</td></tr>
<tr><td><code id="returnCode_+3A_...">...</code></td>
<td>
<p>further arguments for other methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>returnMessage</code> and <code>returnCode</code> are a generic functions, with methods
for various optimisation algorithms.
The message should either describe
the convergence (stopping condition),
or the problem.
</p>
<p>The known codes and the related messages are:
</p>

<dl>
<dt>1</dt><dd><p> gradient close to zero (normal convergence).</p>
</dd>
<dt>2</dt><dd><p> successive function values within tolerance limit (normal
convergence).</p>
</dd>
<dt>3</dt><dd><p> last step could not find higher value (probably not
converged).  This is related to line search step getting too
small, usually because hitting the boundary of the parameter
space.  It may also be related to attempts to move to a wrong
direction because of numerical errors.  In some cases it can be
helped by changing <code>steptol</code>.</p>
</dd>
<dt>4</dt><dd><p> iteration limit exceeded.</p>
</dd>
<dt>5</dt><dd><p> Infinite value.</p>
</dd>
<dt>6</dt><dd><p> Infinite gradient.</p>
</dd>
<dt>7</dt><dd><p> Infinite Hessian.</p>
</dd>
<dt>8</dt><dd><p>Successive function values withing relative tolerance
limit (normal convergence).</p>
</dd>
<dt>9</dt><dd><p> (BFGS) Hessian approximation cannot be improved because of
gradient did not change.  May be related to numerical
approximation problems or wrong analytic gradient.
</p>
</dd>
<dt>10</dt><dd>
<p>Lost patience: the optimizer has hit an inferior value too many
times (see <code><a href="#topic+maxSGA">maxSGA</a></code> for more information)
</p>
</dd>
<dt>100</dt><dd><p> Initial value out of range.</p>
</dd>
</dl>



<h3>Value</h3>

<p>Integer for <code>returnCode</code>, character for <code>returnMessage</code>.
Different optimization routines may define it in a different way.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxNR">maxNR</a></code>, <code><a href="#topic+maxBFGS">maxBFGS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## maximise the exponential bell
f1 &lt;- function(x) exp(-x^2)
a &lt;- maxNR(f1, start=2)
returnCode(a) # should be success (1 or 2)
returnMessage(a)
## Now try to maximise log() function
a &lt;- maxNR(log, start=2)
returnCode(a) # should give a failure (4)
returnMessage(a)
</code></pre>

<hr>
<h2 id='storedValues'>Return the stored values of optimization</h2><span id='topic+storedValues'></span><span id='topic+storedValues.maxim'></span><span id='topic+storedParameters'></span><span id='topic+storedParameters.maxim'></span>

<h3>Description</h3>

<p>Retrieve the objective function value for each iteration if stored
during the optimization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>storedValues(x, ...)
## S3 method for class 'maxim'
storedValues(x, ...)
storedParameters(x, ...)
## S3 method for class 'maxim'
storedParameters(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="storedValues_+3A_x">x</code></td>
<td>
<p>a result of maximization, created
by <code><a href="#topic+maxLik">maxLik</a></code>, <code><a href="#topic+maxSGA">maxSGA</a></code> or another optimizer.</p>
</td></tr>
<tr><td><code id="storedValues_+3A_...">...</code></td>
<td>
<p>further arguments for other methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These is a generic method.
If asked by control parameter <code>storeValues=TRUE</code> or <code>storeParameters=TRUE</code>, certain
optimization methods store the objective function value and the
parameter value at each epoch.  These methods
retrieves the stored values.
</p>


<h3>Value</h3>


<ul>
<li> <p><code>storedValues</code>: a numeric vector, one value for each
iteration
</p>
</li>
<li> <p><code>storedParameters</code>: a numeric matrix with rows
corresponding to the iterations and columns to the parameter
components.
</p>
</li></ul>

<p>In both cases, the first value stored corresponds to the initial
parameter. 
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxSGA">maxSGA</a></code>,
<code><a href="#topic+maxControl">maxControl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Estimate the exponential distribution parameter
t &lt;- rexp(100, 2)
loglik &lt;- function(theta, index) sum(log(theta) - theta*t[index])
## Estimate with numeric gradient and numeric Hessian
a &lt;- maxSGA(loglik, start=1,
            control=list(storeValues=TRUE, storeParameters=TRUE, iterlim=10),
            nObs=100)
storedValues(a)
storedParameters(a)
</code></pre>

<hr>
<h2 id='summary.maxim'>Summary method for maximization</h2><span id='topic+summary.maxim'></span><span id='topic+print.summary.maxim'></span>

<h3>Description</h3>

<p>Summarizes the general maximization results in a way that does not
assume the function is log-likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxim'
summary( object, hessian=FALSE, unsucc.step=FALSE, ... )
## S3 method for class 'summary.maxim'
print(x,
                              max.rows=getOption("max.rows", 20),
                              max.cols=getOption("max.cols", 7),
                              ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.maxim_+3A_object">object</code></td>
<td>
<p>optimization result, object of class
<code>maxim</code>.  See <code><a href="#topic+maxNR">maxNR</a></code>.</p>
</td></tr>
<tr><td><code id="summary.maxim_+3A_hessian">hessian</code></td>
<td>
<p>logical, whether to display Hessian matrix.</p>
</td></tr>
<tr><td><code id="summary.maxim_+3A_unsucc.step">unsucc.step</code></td>
<td>
<p>logical, whether to describe last unsuccesful step
if <code>code</code> == 3</p>
</td></tr>
<tr><td><code id="summary.maxim_+3A_x">x</code></td>
<td>
<p>object of class <code>summary.maxim</code>, summary of maximization
result.
</p>
</td></tr>
<tr><td><code id="summary.maxim_+3A_max.rows">max.rows</code></td>
<td>
<p>maximum number of rows to be printed.  This applies to
the resulting coefficients (as those are printed as a matrix where
the other column is the gradient), and to the Hessian if requested.
</p>
</td></tr>
<tr><td><code id="summary.maxim_+3A_max.cols">max.cols</code></td>
<td>
<p>maximum number of columns to be printed.  Only Hessian
output, if requested, uses this argument.
</p>
</td></tr>
<tr><td><code id="summary.maxim_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>summary.maxim</code>, intended to be printed with
corresponding print method.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxNR">maxNR</a></code>, <code><a href="#topic+returnCode">returnCode</a></code>,
<code><a href="#topic+returnMessage">returnMessage</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## minimize a 2D quadratic function:
f &lt;- function(b) {
  x &lt;- b[1]; y &lt;- b[2];
  val &lt;- -(x - 2)^2 - (y - 3)^2  # concave parabola
  attr(val, "gradient") &lt;- c(-2*x + 4, -2*y + 6)
  attr(val, "hessian") &lt;- matrix(c(-2, 0, 0, -2), 2, 2)
  val
}
## Note that NR finds the minimum of a quadratic function with a single
## iteration.  Use c(0,0) as initial value.  
res &lt;- maxNR( f, start = c(0,0) ) 
summary(res)
summary(res, hessian=TRUE)
</code></pre>

<hr>
<h2 id='summary.maxLik'>summary the Maximum-Likelihood estimation</h2><span id='topic+summary.maxLik'></span><span id='topic+coef.summary.maxLik'></span>

<h3>Description</h3>

<p>Summary the Maximum-Likelihood estimation including standard errors
and t-values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxLik'
summary(object, eigentol=1e-12, ... )
## S3 method for class 'summary.maxLik'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.maxLik_+3A_object">object</code></td>
<td>

<p>object of class 'maxLik', or 'summary.maxLik',
usually a result from
Maximum-Likelihood estimation.
</p>
</td></tr>
<tr><td><code id="summary.maxLik_+3A_eigentol">eigentol</code></td>
<td>

<p>The standard errors are only calculated if the ratio of the smallest
and largest eigenvalue of the Hessian matrix is less than
&ldquo;eigentol&rdquo;.  Otherwise the Hessian is treated as singular.
</p>
</td></tr>
<tr><td><code id="summary.maxLik_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class 'summary.maxLik' with following components:
</p>

<dl>
<dt>type</dt><dd><p>type of maximization.</p>
</dd>
<dt>iterations</dt><dd><p>number of iterations.</p>
</dd>
<dt>code</dt><dd><p>code of success.</p>
</dd>
<dt>message</dt><dd><p>a short message describing the code.</p>
</dd>
<dt>loglik</dt><dd><p>the loglik value in the maximum.</p>
</dd>
<dt>estimate</dt><dd><p>numeric matrix, the first column contains the parameter
estimates, the second the standard errors, third t-values and fourth
corresponding probabilities.</p>
</dd>
<dt>fixed</dt><dd><p>logical vector, which parameters are treated as constants.</p>
</dd>
<dt>NActivePar</dt><dd><p>number of free parameters.</p>
</dd>
<dt>constraints</dt><dd><p>information about the constrained optimization.
Passed directly further from <code>maxim</code>-object.  <code>NULL</code> if
unconstrained maximization.
</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Ott Toomet, Arne Henningsen</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxLik">maxLik</a></code> for maximum likelihood estimation,
<code><a href="#topic+confint">confint</a></code> for confidence intervals, and <code><a href="#topic+tidy">tidy</a></code>
and <code><a href="#topic+glance">glance</a></code> for alternative quick summaries of the ML
results. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## ML estimation of exponential distribution:
t &lt;- rexp(100, 2)
loglik &lt;- function(theta) log(theta) - theta*t
gradlik &lt;- function(theta) 1/theta - t
hesslik &lt;- function(theta) -100/theta^2
## Estimate with numeric gradient and hessian
a &lt;- maxLik(loglik, start=1, control=list(printLevel=2))
summary(a)
## Estimate with analytic gradient and hessian
a &lt;- maxLik(loglik, gradlik, hesslik, start=1, control=list(printLevel=2))
summary(a)
</code></pre>

<hr>
<h2 id='sumt'>
Equality-constrained optimization
</h2><span id='topic+sumt'></span>

<h3>Description</h3>

<p>Sequentially Unconstrained Maximization Technique (SUMT) based
optimization for linear equality constraints.
</p>
<p>This implementation is primarily intended to be called from other
maximization routines, such as <code><a href="#topic+maxNR">maxNR</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sumt(fn, grad=NULL, hess=NULL,
start,
maxRoutine, constraints,
SUMTTol = sqrt(.Machine$double.eps),
SUMTPenaltyTol = sqrt(.Machine$double.eps),
SUMTQ = 10,
SUMTRho0 = NULL,
printLevel=print.level, print.level = 0, SUMTMaxIter = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sumt_+3A_fn">fn</code></td>
<td>

<p>function of a (single) vector parameter.  The function may have more
arguments (passed by ...), but those are not treated as the
parameter. 
</p>
</td></tr>
<tr><td><code id="sumt_+3A_grad">grad</code></td>
<td>

<p>gradient function of <code>fn</code>.  NULL if missing
</p>
</td></tr>
<tr><td><code id="sumt_+3A_hess">hess</code></td>
<td>

<p>function, Hessian of the <code>fn</code>.  NULL if missing
</p>
</td></tr>
<tr><td><code id="sumt_+3A_start">start</code></td>
<td>

<p>numeric, initial value of the parameter
</p>
</td></tr>
<tr><td><code id="sumt_+3A_maxroutine">maxRoutine</code></td>
<td>

<p>maximization algorithm, such as <code><a href="#topic+maxNR">maxNR</a></code>
</p>
</td></tr>
<tr><td><code id="sumt_+3A_constraints">constraints</code></td>
<td>
<p>list, information for constrained maximization.
Currently two components are supported: <code>eqA</code> and <code>eqB</code>
for linear equality constraints: <code class="reqn">A \beta + B = 0</code>.  The user must ensure that the matrices <code>A</code> and
<code>B</code> are conformable.</p>
</td></tr>
<tr><td><code id="sumt_+3A_sumttol">SUMTTol</code></td>
<td>

<p>stopping condition.  If the estimates at successive outer
iterations are close enough, i.e. maximum of the absolute value over
the component difference is smaller than SUMTTol, the algorithm
stops.
</p>
<p>Note this does not necessarily mean that the constraints are
satisfied.
If the penalty function is too &ldquo;weak&rdquo;, SUMT may repeatedly find
the same optimum.  In that case a warning is issued.  The user may
set SUMTTol to a lower value, e.g. to zero.
</p>
</td></tr>
<tr><td><code id="sumt_+3A_sumtpenaltytol">SUMTPenaltyTol</code></td>
<td>

<p>stopping condition.  If the barrier value (also called penalty)
<code class="reqn">(A \beta + B)'(A \beta + B)</code> is less than
<code>SUMTTol</code>, the algorithm stops
</p>
</td></tr>
<tr><td><code id="sumt_+3A_sumtq">SUMTQ</code></td>
<td>

<p>a double greater than one, controlling the growth of the <code>rho</code>
as described in Details. Defaults to 10.
</p>
</td></tr>
<tr><td><code id="sumt_+3A_sumtrho0">SUMTRho0</code></td>
<td>

<p>Initial value for <code>rho</code>.  If not specified, a (possibly)
suitable value is
selected.  See Details.
</p>
<p>One should consider supplying <code>SUMTRho0</code> in case where the
unconstrained problem does not have a maximum, or the maximum is too
far from the constrained value.  Otherwise the authomatically selected
value may not lead to convergence.
</p>
</td></tr>
<tr><td><code id="sumt_+3A_printlevel">printLevel</code></td>
<td>

<p>Integer, debugging information.  Larger number prints more details.
</p>
</td></tr>
<tr><td><code id="sumt_+3A_print.level">print.level</code></td>
<td>
<p>same as &lsquo;printLevel&rsquo;, for backward
compatibility</p>
</td></tr> 
<tr><td><code id="sumt_+3A_sumtmaxiter">SUMTMaxIter</code></td>
<td>

<p>Maximum SUMT iterations
</p>
</td></tr>
<tr><td><code id="sumt_+3A_...">...</code></td>
<td>

<p>Other arguments to <code>maxRoutine</code> and <code>fn</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Sequential Unconstrained Minimization Technique is a heuristic
for constrained optimization.  To minimize a function <code class="reqn">f</code>
subject to
constraints, it uses a non-negative penalty function <code class="reqn">P</code>,
such that <code class="reqn">P(x)</code> is zero
iff <code class="reqn">x</code>
satisfies the constraints.  One iteratively minimizes
<code class="reqn">f(x) + \varrho_k P(x)</code>, where the
<code class="reqn">\varrho</code>
values are increased according to the rule
<code class="reqn">\varrho_{k+1} = q \varrho_k</code> for some
constant <code class="reqn">q &gt; 1</code>, until convergence is
achieved in the sense that the barrier value
<code class="reqn">P(x)'P(x)</code> is close to zero.  Note that there is no
guarantee that the global constrained optimum is
found.  Standard practice recommends to use the best solution
found in &ldquo;sufficiently many&rdquo; replications.
</p>
<p>Any of
the maximization algorithms in the <span class="pkg">maxLik</span>, such as
<code><a href="#topic+maxNR">maxNR</a></code>, can be used for the unconstrained step.
</p>
<p>Analytic gradient and hessian
are used if provided.
</p>


<h3>Value</h3>

<p>Object of class 'maxim'.  In addition, a component
</p>
<table>
<tr><td><code>constraints</code></td>
<td>
<p>A list, describing the constrained optimization.
Includes the following components:
</p>

<dl>
<dt>type</dt><dd><p>type of constrained optimization</p>
</dd>
<dt>barrier.value</dt><dd><p>value of the penalty function at maximum</p>
</dd>
<dt>code</dt><dd><p>code for the stopping condition</p>
</dd>
<dt>message</dt><dd><p>a short message, describing the stopping condition</p>
</dd>
<dt>outer.iterations</dt><dd><p>number of iterations in the SUMT step</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Note</h3>

<p>In case of equality constraints, it
may be more efficient to enclose the function
in a wrapper function.  The wrapper calculates full set of parameters
based on a smaller set of parameters, and the
constraints.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet, Arne Henningsen
</p>


<h3>See Also</h3>

<p><code><a href="clue.html#topic+sumt">sumt</a></code> in package <span class="pkg">clue</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## We maximize exp(-x^2 - y^2) where x+y = 1
hatf &lt;- function(theta) {
   x &lt;- theta[1]
   y &lt;- theta[2]
   exp(-(x^2 + y^2))
   ## Note: you may prefer exp(- theta %*% theta) instead
}
## use constraints: x + y = 1
A &lt;- matrix(c(1, 1), 1, 2)
B &lt;- -1
res &lt;- sumt(hatf, start=c(0,0), maxRoutine=maxNR,
            constraints=list(eqA=A, eqB=B))
print(summary(res))
</code></pre>

<hr>
<h2 id='tidy.maxLik'>tidy and glance methods for maxLik objects</h2><span id='topic+tidy.maxLik'></span><span id='topic+glance.maxLik'></span>

<h3>Description</h3>

<p>These methods return summary information about the estimated model.
Both require the <span class="pkg">tibble</span> package to be installed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maxLik'
tidy(x,  ...)
## S3 method for class 'maxLik'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy.maxLik_+3A_x">x</code></td>
<td>

<p>object of class 'maxLik'.
</p>
</td></tr>
<tr><td><code id="tidy.maxLik_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>tidy()</code>, a tibble with columns:
</p>

<dl>
<dt>term</dt><dd><p>The name of the estimated parameter (parameters are
sequentially numbered if names missing).</p>
</dd>
<dt>estimate</dt><dd><p>The estimated parameter.</p>
</dd>
<dt>std.error</dt><dd><p>The standard error of the estimate.</p>
</dd>
<dt>statistic</dt><dd><p>The <code class="reqn">z</code>-statistic of the estimate.</p>
</dd>
<dt>p.value</dt><dd><p>The <code class="reqn">p</code>-value.</p>
</dd>
</dl>

<p>This is essentially the same table as <code>summary</code>-method prints,
just in form of a tibble (data frame).
</p>
<p>For <code>glance()</code>, a one-row tibble with columns:
</p>

<dl>
<dt>df</dt><dd><p>The degrees of freedom of the model.</p>
</dd>
<dt>logLik</dt><dd><p>The log-likelihood of the model.</p>
</dd>
<dt>AIC</dt><dd><p>Akaike's Information Criterion for the model.</p>
</dd>
<dt>nobs</dt><dd><p>The number of observations, if this is available, otherwise <code>NA</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>David Hugh-Jones</p>


<h3>See Also</h3>

<p>The functions <code><a href="generics.html#topic+tidy">tidy</a></code> and
<code><a href="generics.html#topic+glance">glance</a></code> in package <span class="pkg">generics</span>, and
<code><a href="#topic+summary.maxLik">summary</a></code> to display the
&ldquo;standard&rdquo; summary information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example with a single parameter
t &lt;- rexp(100, 2)
loglik &lt;- function(theta) log(theta) - theta*t
a &lt;- maxLik(loglik, start=2)
tidy(a)
glance(a)
## Example with a parameter vector
x &lt;- rnorm(100)
loglik &lt;- function(theta) {
   dnorm(x, mean=theta[1], sd=theta[2], log=TRUE)
}
a &lt;- maxLik(loglik, start=c(mu=0, sd=1))
tidy(a)
glance(a)
</code></pre>

<hr>
<h2 id='vcov.maxLik'>Variance Covariance Matrix of maxLik objects</h2><span id='topic+vcov.maxLik'></span>

<h3>Description</h3>

<p>Extract variance-covariance matrices from <code><a href="#topic+maxLik">maxLik</a></code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   ## S3 method for class 'maxLik'
vcov( object, eigentol=1e-12, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcov.maxLik_+3A_object">object</code></td>
<td>
<p>a &lsquo;maxLik&rsquo; object.</p>
</td></tr>
<tr><td><code id="vcov.maxLik_+3A_eigentol">eigentol</code></td>
<td>

<p>eigenvalue tolerance, controlling when the Hessian matrix is
treated as numerically singular.
</p>
</td></tr>
<tr><td><code id="vcov.maxLik_+3A_...">...</code></td>
<td>
<p>further arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The standard errors are only calculated if the ratio of the smallest
and largest eigenvalue of the Hessian matrix is less than
&ldquo;eigentol&rdquo;.  Otherwise the Hessian is treated as singular.
</p>


<h3>Value</h3>

<p>the estimated variance covariance matrix of the coefficients.  In
case of the estimated Hessian is singular, it's values are
<code>Inf</code>.  The values corresponding to fixed parameters are zero.
</p>


<h3>Author(s)</h3>

<p>Arne Henningsen,
Ott Toomet
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+vcov">vcov</a></code>, <code><a href="#topic+maxLik">maxLik</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## ML estimation of exponential random variables
t &lt;- rexp(100, 2)
loglik &lt;- function(theta) log(theta) - theta*t
gradlik &lt;- function(theta) 1/theta - t
hesslik &lt;- function(theta) -100/theta^2
## Estimate with numeric gradient and hessian
a &lt;- maxLik(loglik, start=1, control=list(printLevel=2))
vcov(a)
## Estimate with analytic gradient and hessian
a &lt;- maxLik(loglik, gradlik, hesslik, start=1)
vcov(a)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
