<!DOCTYPE html><html><head><title>Help for package finetune</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {finetune}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#finetune-package'><p>finetune: Additional Functions for Model Tuning</p></a></li>
<li><a href='#collect_predictions'><p>Obtain and format results produced by racing functions</p></a></li>
<li><a href='#control_race'><p>Control aspects of the grid search racing process</p></a></li>
<li><a href='#control_sim_anneal'><p>Control aspects of the simulated annealing search process</p></a></li>
<li><a href='#plot_race'><p>Plot racing results</p></a></li>
<li><a href='#show_best.tune_race'><p>Investigate best tuning parameters</p></a></li>
<li><a href='#tune_race_anova'><p>Efficient grid search via racing with ANOVA models</p></a></li>
<li><a href='#tune_race_win_loss'><p>Efficient grid search via racing with win/loss statistics</p></a></li>
<li><a href='#tune_sim_anneal'><p>Optimization of model parameters via simulated annealing</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Additional Functions for Model Tuning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>The ability to tune models is important. 'finetune' enhances
    the 'tune' package by providing more specialized methods for finding
    reasonable values of model tuning parameters.  Two racing methods
    described by Kuhn (2014) &lt;<a href="https://doi.org/10.48550/arXiv.1405.6974">doi:10.48550/arXiv.1405.6974</a>&gt; are included. An iterative
    search method using generalized simulated annealing (Bohachevsky,
    Johnson and Stein, 1986) &lt;<a href="https://doi.org/10.1080%2F00401706.1986.10488128">doi:10.1080/00401706.1986.10488128</a>&gt; is also
    included.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/tidymodels/finetune">https://github.com/tidymodels/finetune</a>,
<a href="https://finetune.tidymodels.org">https://finetune.tidymodels.org</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tidymodels/finetune/issues">https://github.com/tidymodels/finetune/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5), tune (&ge; 1.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, dials (&ge; 0.1.0), dplyr (&ge; 1.1.1), ggplot2, parsnip (&ge;
1.1.0), purrr, rlang, tibble, tidyr, tidyselect, utils, vctrs,
workflows (&ge; 0.2.6)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>BradleyTerry2, covr, discrim, kknn, klaR, lme4, modeldata,
ranger, recipes (&ge; 0.2.0), rpart, rsample, spelling, testthat,
yardstick</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>tidyverse/tidytemplate</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-21 01:08:05 UTC; max</td>
</tr>
<tr>
<td>Author:</td>
<td>Max Kuhn <a href="https://orcid.org/0000-0003-2402-136X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Max Kuhn &lt;max@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-21 03:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='finetune-package'>finetune: Additional Functions for Model Tuning</h2><span id='topic+finetune'></span><span id='topic+finetune-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>The ability to tune models is important. 'finetune' enhances the 'tune' package by providing more specialized methods for finding reasonable values of model tuning parameters. Two racing methods described by Kuhn (2014) <a href="https://arxiv.org/abs/1405.6974">arXiv:1405.6974</a> are included. An iterative search method using generalized simulated annealing (Bohachevsky, Johnson and Stein, 1986) <a href="https://doi.org/10.1080/00401706.1986.10488128">doi:10.1080/00401706.1986.10488128</a> is also included.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Max Kuhn <a href="mailto:max@posit.co">max@posit.co</a> (<a href="https://orcid.org/0000-0003-2402-136X">ORCID</a>)
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Posit Software, PBC [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/tidymodels/finetune">https://github.com/tidymodels/finetune</a>
</p>
</li>
<li> <p><a href="https://finetune.tidymodels.org">https://finetune.tidymodels.org</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tidymodels/finetune/issues">https://github.com/tidymodels/finetune/issues</a>
</p>
</li></ul>


<hr>
<h2 id='collect_predictions'>Obtain and format results produced by racing functions</h2><span id='topic+collect_predictions'></span><span id='topic+collect_predictions.tune_race'></span><span id='topic+collect_metrics.tune_race'></span>

<h3>Description</h3>

<p>Obtain and format results produced by racing functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tune_race'
collect_predictions(
  x,
  ...,
  summarize = FALSE,
  parameters = NULL,
  all_configs = FALSE
)

## S3 method for class 'tune_race'
collect_metrics(
  x,
  ...,
  summarize = TRUE,
  type = c("long", "wide"),
  all_configs = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collect_predictions_+3A_x">x</code></td>
<td>
<p>The results of <code><a href="tune.html#topic+tune_grid">tune_grid()</a></code>, <code><a href="tune.html#topic+tune_bayes">tune_bayes()</a></code>, <code><a href="tune.html#topic+fit_resamples">fit_resamples()</a></code>,
or <code><a href="tune.html#topic+last_fit">last_fit()</a></code>. For <code><a href="tune.html#topic+collect_predictions">collect_predictions()</a></code>, the control option <code>save_pred = TRUE</code> should have been used.</p>
</td></tr>
<tr><td><code id="collect_predictions_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="collect_predictions_+3A_summarize">summarize</code></td>
<td>
<p>A logical; should metrics be summarized over resamples
(<code>TRUE</code>) or return the values for each individual resample. Note that, if <code>x</code>
is created by <code><a href="tune.html#topic+last_fit">last_fit()</a></code>, <code>summarize</code> has no effect. For the other object
types, the method of summarizing predictions is detailed below.</p>
</td></tr>
<tr><td><code id="collect_predictions_+3A_parameters">parameters</code></td>
<td>
<p>An optional tibble of tuning parameter values that can be
used to filter the predicted values before processing. This tibble should
only have columns for each tuning parameter identifier (e.g. <code>"my_param"</code>
if <code>tune("my_param")</code> was used).</p>
</td></tr>
<tr><td><code id="collect_predictions_+3A_all_configs">all_configs</code></td>
<td>
<p>A logical: should we return the complete set of model
configurations or just those that made it to the end of the race (the
default).</p>
</td></tr>
<tr><td><code id="collect_predictions_+3A_type">type</code></td>
<td>
<p>One of <code>"long"</code> (the default) or <code>"wide"</code>. When <code>type = "long"</code>,
output has columns <code>.metric</code> and one of <code>.estimate</code> or <code>mean</code>.
<code>.estimate</code>/<code>mean</code> gives the values for the <code>.metric</code>. When <code>type = "wide"</code>,
each metric has its own column and the <code>n</code> and <code>std_err</code> columns are removed,
if they exist.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code><a href="tune.html#topic+collect_metrics">collect_metrics()</a></code> and <code><a href="#topic+collect_predictions">collect_predictions()</a></code>, when unsummarized,
there are columns for each tuning parameter (using the <code>id</code> from <code><a href="tune.html#topic+tune">tune()</a></code>,
if any).
<code><a href="tune.html#topic+collect_metrics">collect_metrics()</a></code> also has columns <code>.metric</code>, and <code>.estimator</code>.  When the
results are summarized, there are columns for <code>mean</code>, <code>n</code>, and <code>std_err</code>.
When not summarized, the additional columns for the resampling identifier(s)
and <code>.estimate</code>.
</p>
<p>For <code><a href="#topic+collect_predictions">collect_predictions()</a></code>, there are additional columns for the resampling
identifier(s), columns for the predicted values (e.g., <code>.pred</code>,
<code>.pred_class</code>, etc.), and a column for the outcome(s) using the original
column name(s) in the data.
</p>
<p><code><a href="#topic+collect_predictions">collect_predictions()</a></code> can summarize the various results over
replicate out-of-sample predictions. For example, when using the bootstrap,
each row in the original training set has multiple holdout predictions
(across assessment sets). To convert these results to a format where every
training set same has a single predicted value, the results are averaged
over replicate predictions.
</p>
<p>For regression cases, the numeric predictions are simply averaged. For
classification models, the problem is more complex. When class probabilities
are used, these are averaged and then re-normalized to make sure that they
add to one. If hard class predictions also exist in the data, then these are
determined from the summarized probability estimates (so that they match).
If only hard class predictions are in the results, then the mode is used to
summarize.
</p>
<p>For racing results, it is best to only
collect model configurations that finished the race (i.e., were completely
resampled). Comparing performance metrics for configurations averaged with
different resamples is likely to lead to inappropriate results.
</p>


<h3>Value</h3>

<p>A tibble. The column names depend on the results and the mode of the
model.
</p>

<hr>
<h2 id='control_race'>Control aspects of the grid search racing process</h2><span id='topic+control_race'></span>

<h3>Description</h3>

<p>Control aspects of the grid search racing process
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_race(
  verbose = FALSE,
  verbose_elim = FALSE,
  allow_par = TRUE,
  extract = NULL,
  save_pred = FALSE,
  burn_in = 3,
  num_ties = 10,
  alpha = 0.05,
  randomize = TRUE,
  pkgs = NULL,
  save_workflow = FALSE,
  event_level = "first",
  parallel_over = "everything",
  backend_options = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_race_+3A_verbose">verbose</code></td>
<td>
<p>A logical for logging results (other than warnings and errors,
which are always shown) as they are generated during training in a single
R process. When using most parallel backends, this argument typically will
not result in any logging. If using a dark IDE theme, some logging messages
might be hard to see; try setting the <code>tidymodels.dark</code> option with
<code>options(tidymodels.dark = TRUE)</code> to print lighter colors.</p>
</td></tr>
<tr><td><code id="control_race_+3A_verbose_elim">verbose_elim</code></td>
<td>
<p>A logical for whether logging of the elimination of
tuning parameter combinations should occur.</p>
</td></tr>
<tr><td><code id="control_race_+3A_allow_par">allow_par</code></td>
<td>
<p>A logical to allow parallel processing (if a parallel
backend is registered).</p>
</td></tr>
<tr><td><code id="control_race_+3A_extract">extract</code></td>
<td>
<p>An optional function with at least one argument (or <code>NULL</code>)
that can be used to retain arbitrary objects from the model fit object,
recipe, or other elements of the workflow.</p>
</td></tr>
<tr><td><code id="control_race_+3A_save_pred">save_pred</code></td>
<td>
<p>A logical for whether the out-of-sample predictions should
be saved for each model <em>evaluated</em>.</p>
</td></tr>
<tr><td><code id="control_race_+3A_burn_in">burn_in</code></td>
<td>
<p>An integer for how many resamples should be completed for all
grid combinations before parameter filtering begins.</p>
</td></tr>
<tr><td><code id="control_race_+3A_num_ties">num_ties</code></td>
<td>
<p>An integer for when tie-breaking should occur. If there are
two final parameter combinations being evaluated, <code>num_ties</code> specified how
many more resampling iterations should be evaluated. After <code>num_ties</code> more
iterations, the parameter combination with the current best results is
retained.</p>
</td></tr>
<tr><td><code id="control_race_+3A_alpha">alpha</code></td>
<td>
<p>The alpha level for a one-sided confidence interval for each
parameter combination.</p>
</td></tr>
<tr><td><code id="control_race_+3A_randomize">randomize</code></td>
<td>
<p>Should the resamples be evaluated in a random order?  By
default, the resamples are evaluated in a random order so the random number
seed should be control prior to calling this method (to be reproducible).
For repeated cross-validation the randomization occurs within each repeat.</p>
</td></tr>
<tr><td><code id="control_race_+3A_pkgs">pkgs</code></td>
<td>
<p>An optional character string of R package names that should be
loaded (by namespace) during parallel processing.</p>
</td></tr>
<tr><td><code id="control_race_+3A_save_workflow">save_workflow</code></td>
<td>
<p>A logical for whether the workflow should be appended
to the output as an attribute.</p>
</td></tr>
<tr><td><code id="control_race_+3A_event_level">event_level</code></td>
<td>
<p>A single string containing either <code>"first"</code> or <code>"second"</code>.
This argument is passed on to yardstick metric functions when any type
of class prediction is made, and specifies which level of the outcome
is considered the &quot;event&quot;.</p>
</td></tr>
<tr><td><code id="control_race_+3A_parallel_over">parallel_over</code></td>
<td>
<p>A single string containing either <code>"resamples"</code> or
<code>"everything"</code> describing how to use parallel processing. Alternatively,
<code>NULL</code> is allowed, which chooses between <code>"resamples"</code> and <code>"everything"</code>
automatically.
</p>
<p>If <code>"resamples"</code>, then tuning will be performed in parallel over resamples
alone. Within each resample, the preprocessor (i.e. recipe or formula) is
processed once, and is then reused across all models that need to be fit.
</p>
<p>If <code>"everything"</code>, then tuning will be performed in parallel at two levels.
An outer parallel loop will iterate over resamples. Additionally, an
inner parallel loop will iterate over all unique combinations of
preprocessor and model tuning parameters for that specific resample. This
will result in the preprocessor being re-processed multiple times, but
can be faster if that processing is extremely fast.
</p>
<p>If <code>NULL</code>, chooses <code>"resamples"</code> if there are more than one resample,
otherwise chooses <code>"everything"</code> to attempt to maximize core utilization.
</p>
<p>Note that switching between <code>parallel_over</code> strategies is not guaranteed
to use the same random number generation schemes. However, re-tuning a
model using the same <code>parallel_over</code> strategy is guaranteed to be
reproducible between runs.</p>
</td></tr>
<tr><td><code id="control_race_+3A_backend_options">backend_options</code></td>
<td>
<p>An object of class <code>"tune_backend_options"</code> as created
by <code>tune::new_backend_options()</code>, used to pass arguments to specific tuning
backend. Defaults to <code>NULL</code> for default backend options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>control_race</code> that echos the argument values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>control_race()
</code></pre>

<hr>
<h2 id='control_sim_anneal'>Control aspects of the simulated annealing search process</h2><span id='topic+control_sim_anneal'></span>

<h3>Description</h3>

<p>Control aspects of the simulated annealing search process
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_sim_anneal(
  verbose = FALSE,
  verbose_iter = TRUE,
  no_improve = Inf,
  restart = 8L,
  radius = c(0.05, 0.15),
  flip = 3/4,
  cooling_coef = 0.02,
  extract = NULL,
  save_pred = FALSE,
  time_limit = NA,
  pkgs = NULL,
  save_workflow = FALSE,
  save_history = FALSE,
  event_level = "first",
  parallel_over = NULL,
  allow_par = TRUE,
  backend_options = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_sim_anneal_+3A_verbose">verbose</code></td>
<td>
<p>A logical for logging results (other than warnings and errors,
which are always shown) as they are generated during training in a single
R process. When using most parallel backends, this argument typically will
not result in any logging. If using a dark IDE theme, some logging messages
might be hard to see; try setting the <code>tidymodels.dark</code> option with
<code>options(tidymodels.dark = TRUE)</code> to print lighter colors.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_verbose_iter">verbose_iter</code></td>
<td>
<p>A logical for logging results of the search
process. Defaults to FALSE. If using a dark IDE theme, some logging
messages might be hard to see; try setting the <code>tidymodels.dark</code> option
with <code>options(tidymodels.dark = TRUE)</code> to print lighter colors.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_no_improve">no_improve</code></td>
<td>
<p>The integer cutoff for the number of iterations without
better results.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_restart">restart</code></td>
<td>
<p>The number of iterations with no improvement before new tuning
parameter candidates are generated from the last, overall best conditions.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_radius">radius</code></td>
<td>
<p>Two real numbers on <code style="white-space: pre;">&#8288;(0, 1)&#8288;</code> describing what a value &quot;in the
neighborhood&quot; of the current result should be. If all numeric parameters were
scaled to be on the <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> scale, these values set the min. and max.
of a radius of a circle used to generate new numeric parameter values.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_flip">flip</code></td>
<td>
<p>A real number between <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> for the probability of changing
any non-numeric parameter values at each iteration.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_cooling_coef">cooling_coef</code></td>
<td>
<p>A real, positive number to influence the cooling
schedule. Larger values decrease the probability of accepting a sub-optimal
parameter setting.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_extract">extract</code></td>
<td>
<p>An optional function with at least one argument (or <code>NULL</code>)
that can be used to retain arbitrary objects from the model fit object,
recipe, or other elements of the workflow.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_save_pred">save_pred</code></td>
<td>
<p>A logical for whether the out-of-sample predictions should
be saved for each model <em>evaluated</em>.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_time_limit">time_limit</code></td>
<td>
<p>A number for the minimum number of <em>minutes</em> (elapsed) that
the function should execute. The elapsed time is evaluated at internal
checkpoints and, if over time, the results at that time are returned (with
a warning). This means that the <code>time_limit</code> is not an exact limit, but a
minimum time limit.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_pkgs">pkgs</code></td>
<td>
<p>An optional character string of R package names that should be
loaded (by namespace) during parallel processing.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_save_workflow">save_workflow</code></td>
<td>
<p>A logical for whether the workflow should be appended
to the output as an attribute.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_save_history">save_history</code></td>
<td>
<p>A logical to save the iteration details of the search.
These are saved to <code>tempdir()</code> named <code>sa_history.RData</code>. These results are
deleted when the R session ends. This option is only useful for teaching
purposes.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_event_level">event_level</code></td>
<td>
<p>A single string containing either <code>"first"</code> or <code>"second"</code>.
This argument is passed on to yardstick metric functions when any type
of class prediction is made, and specifies which level of the outcome
is considered the &quot;event&quot;.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_parallel_over">parallel_over</code></td>
<td>
<p>A single string containing either <code>"resamples"</code> or
<code>"everything"</code> describing how to use parallel processing. Alternatively,
<code>NULL</code> is allowed, which chooses between <code>"resamples"</code> and <code>"everything"</code>
automatically.
</p>
<p>If <code>"resamples"</code>, then tuning will be performed in parallel over resamples
alone. Within each resample, the preprocessor (i.e. recipe or formula) is
processed once, and is then reused across all models that need to be fit.
</p>
<p>If <code>"everything"</code>, then tuning will be performed in parallel at two levels.
An outer parallel loop will iterate over resamples. Additionally, an
inner parallel loop will iterate over all unique combinations of
preprocessor and model tuning parameters for that specific resample. This
will result in the preprocessor being re-processed multiple times, but
can be faster if that processing is extremely fast.
</p>
<p>If <code>NULL</code>, chooses <code>"resamples"</code> if there are more than one resample,
otherwise chooses <code>"everything"</code> to attempt to maximize core utilization.
</p>
<p>Note that switching between <code>parallel_over</code> strategies is not guaranteed
to use the same random number generation schemes. However, re-tuning a
model using the same <code>parallel_over</code> strategy is guaranteed to be
reproducible between runs.</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_allow_par">allow_par</code></td>
<td>
<p>A logical to allow parallel processing (if a parallel
backend is registered).</p>
</td></tr>
<tr><td><code id="control_sim_anneal_+3A_backend_options">backend_options</code></td>
<td>
<p>An object of class <code>"tune_backend_options"</code> as created
by <code>tune::new_backend_options()</code>, used to pass arguments to specific tuning
backend. Defaults to <code>NULL</code> for default backend options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>control_sim_anneal</code> that echos the argument values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>control_sim_anneal()
</code></pre>

<hr>
<h2 id='plot_race'>Plot racing results</h2><span id='topic+plot_race'></span>

<h3>Description</h3>

<p>Plot the model results over stages of the racing results. A line is given
for each submodel that was tested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_race(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_race_+3A_x">x</code></td>
<td>
<p>A object with class <code>tune_results</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>

<hr>
<h2 id='show_best.tune_race'>Investigate best tuning parameters</h2><span id='topic+show_best.tune_race'></span>

<h3>Description</h3>

<p><code><a href="tune.html#topic+show_best">show_best()</a></code> displays the top sub-models and their performance estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tune_race'
show_best(
  x,
  ...,
  metric = NULL,
  eval_time = NULL,
  n = 5,
  call = rlang::current_env()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show_best.tune_race_+3A_x">x</code></td>
<td>
<p>The results of <code><a href="tune.html#topic+tune_grid">tune_grid()</a></code> or <code><a href="tune.html#topic+tune_bayes">tune_bayes()</a></code>.</p>
</td></tr>
<tr><td><code id="show_best.tune_race_+3A_...">...</code></td>
<td>
<p>For <code><a href="tune.html#topic+select_by_one_std_err">select_by_one_std_err()</a></code> and <code><a href="tune.html#topic+select_by_pct_loss">select_by_pct_loss()</a></code>, this
argument is passed directly to <code><a href="dplyr.html#topic+arrange">dplyr::arrange()</a></code> so that the user can sort
the models from <em>most simple to most complex</em>. That is, for a parameter <code>p</code>,
pass the unquoted expression <code>p</code> if smaller values of <code>p</code> indicate a simpler
model, or <code>desc(p)</code> if larger values indicate a simpler model. At
least one term is required for these two functions. See the examples below.</p>
</td></tr>
<tr><td><code id="show_best.tune_race_+3A_metric">metric</code></td>
<td>
<p>A character value for the metric that will be used to sort
the models. (See
<a href="https://yardstick.tidymodels.org/articles/metric-types.html">https://yardstick.tidymodels.org/articles/metric-types.html</a> for
more details). Not required if a single metric exists in <code>x</code>. If there are
multiple metric and none are given, the first in the metric set is used (and
a warning is issued).</p>
</td></tr>
<tr><td><code id="show_best.tune_race_+3A_eval_time">eval_time</code></td>
<td>
<p>A single numeric time point where dynamic event time
metrics should be chosen (e.g., the time-dependent ROC curve, etc). The
values should be consistent with the values used to create <code>x</code>. The <code>NULL</code>
default will automatically use the first evaluation time used by <code>x</code>.</p>
</td></tr>
<tr><td><code id="show_best.tune_race_+3A_n">n</code></td>
<td>
<p>An integer for the maximum number of top results/rows to return.</p>
</td></tr>
<tr><td><code id="show_best.tune_race_+3A_call">call</code></td>
<td>
<p>The call to be shown in errors and warnings.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For racing results (from the <span class="pkg">finetune</span> package), it is best to only
report configurations that finished the race (i.e., were completely
resampled). Comparing performance metrics for configurations averaged with
different resamples is likely to lead to inappropriate results.
</p>

<hr>
<h2 id='tune_race_anova'>Efficient grid search via racing with ANOVA models</h2><span id='topic+tune_race_anova'></span><span id='topic+tune_race_anova.model_spec'></span><span id='topic+tune_race_anova.workflow'></span>

<h3>Description</h3>

<p><code><a href="#topic+tune_race_anova">tune_race_anova()</a></code> computes a set of performance metrics (e.g. accuracy or RMSE)
for a pre-defined set of tuning parameters that correspond to a model or
recipe across one or more resamples of the data. After an initial number of
resamples have been evaluated, the process eliminates tuning parameter
combinations that are unlikely to be the best results using a repeated
measure ANOVA model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_race_anova(object, ...)

## S3 method for class 'model_spec'
tune_race_anova(
  object,
  preprocessor,
  resamples,
  ...,
  param_info = NULL,
  grid = 10,
  metrics = NULL,
  eval_time = NULL,
  control = control_race()
)

## S3 method for class 'workflow'
tune_race_anova(
  object,
  resamples,
  ...,
  param_info = NULL,
  grid = 10,
  metrics = NULL,
  eval_time = NULL,
  control = control_race()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_race_anova_+3A_object">object</code></td>
<td>
<p>A <code>parsnip</code> model specification or a <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code>.</p>
</td></tr>
<tr><td><code id="tune_race_anova_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="tune_race_anova_+3A_preprocessor">preprocessor</code></td>
<td>
<p>A traditional model formula or a recipe created using
<code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>. This is only required when <code>object</code> is not a workflow.</p>
</td></tr>
<tr><td><code id="tune_race_anova_+3A_resamples">resamples</code></td>
<td>
<p>An <code>rset()</code> object that has multiple resamples (i.e., is not
a validation set).</p>
</td></tr>
<tr><td><code id="tune_race_anova_+3A_param_info">param_info</code></td>
<td>
<p>A <code><a href="dials.html#topic+parameters">dials::parameters()</a></code> object or <code>NULL</code>. If none is given,
a parameters set is derived from other arguments. Passing this argument can
be useful when parameter ranges need to be customized.</p>
</td></tr>
<tr><td><code id="tune_race_anova_+3A_grid">grid</code></td>
<td>
<p>A data frame of tuning combinations or a positive integer. The
data frame should have columns for each parameter being tuned and rows for
tuning parameter candidates. An integer denotes the number of candidate
parameter sets to be created automatically.</p>
</td></tr>
<tr><td><code id="tune_race_anova_+3A_metrics">metrics</code></td>
<td>
<p>A <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="tune_race_anova_+3A_eval_time">eval_time</code></td>
<td>
<p>A numeric vector of time points where dynamic event time
metrics should be computed (e.g. the time-dependent ROC curve, etc). The
values must be non-negative and should probably be no greater than the
largest event time in the training set (See Details below).</p>
</td></tr>
<tr><td><code id="tune_race_anova_+3A_control">control</code></td>
<td>
<p>An object used to modify the tuning process. See
<code><a href="#topic+control_race">control_race()</a></code> for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The technical details of this method are described in Kuhn (2014).
</p>
<p>Racing methods are efficient approaches to grid search. Initially, the
function evaluates all tuning parameters on a small initial set of
resamples. The <code>burn_in</code> argument of <code><a href="#topic+control_race">control_race()</a></code> sets the number of
initial resamples.
</p>
<p>The performance statistics from these resamples are analyzed to determine
which tuning parameters are <em>not</em> statistically different from the current
best setting. If a parameter is statistically different, it is excluded from
further resampling.
</p>
<p>The next resample is used with the remaining parameter combinations and the
statistical analysis is updated. More candidate parameters may be excluded
with each new resample that is processed.
</p>
<p>This function determines statistical significance using a repeated measures ANOVA
model where the performance statistic (e.g., RMSE, accuracy, etc.) is the
outcome data and the random effect is due to resamples. The
<code><a href="#topic+control_race">control_race()</a></code> function contains are parameter for the significance cutoff
applied to the ANOVA results as well as other relevant arguments.
</p>
<p>There is benefit to using racing methods in conjunction with parallel
processing. The following section shows a benchmark of results for one
dataset and model.
</p>


<h4>Censored regression models</h4>

<p>With dynamic performance metrics (e.g. Brier or ROC curves), performance is
calculated for every value of <code>eval_time</code> but the <em>first</em> evaluation time
given by the user (e.g., <code>eval_time[1]</code>) is analyzed during racing.
</p>
<p>Also, values of <code>eval_time</code> should be less than the largest observed event
time in the training data. For many non-parametric models, the results beyond
the largest time corresponding to an event are constant (or <code>NA</code>).
</p>



<h4>Benchmarking results</h4>

<p>To demonstrate, we use a SVM model with the <code>kernlab</code> package.
</p>
<div class="sourceCode r"><pre>library(kernlab)
library(tidymodels)
library(finetune)
library(doParallel)

## -----------------------------------------------------------------------------

data(cells, package = "modeldata")
cells &lt;- cells %&gt;% select(-case)

## -----------------------------------------------------------------------------

set.seed(6376)
rs &lt;- bootstraps(cells, times = 25)
</pre></div>
<p>We’ll only tune the model parameters (i.e., not recipe tuning):
</p>
<div class="sourceCode r"><pre>## -----------------------------------------------------------------------------

svm_spec &lt;-
  svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;%
  set_engine("kernlab") %&gt;%
  set_mode("classification")

svm_rec &lt;-
  recipe(class ~ ., data = cells) %&gt;%
  step_YeoJohnson(all_predictors()) %&gt;%
  step_normalize(all_predictors())

svm_wflow &lt;-
  workflow() %&gt;%
  add_model(svm_spec) %&gt;%
  add_recipe(svm_rec)

set.seed(1)
svm_grid &lt;-
  svm_spec %&gt;%
  parameters() %&gt;%
  grid_latin_hypercube(size = 25)
</pre></div>
<p>We’ll get the times for grid search and ANOVA racing with and without
parallel processing:
</p>
<div class="sourceCode r"><pre>## -----------------------------------------------------------------------------
## Regular grid search

system.time({
  set.seed(2)
  svm_wflow %&gt;% tune_grid(resamples = rs, grid = svm_grid)
})
</pre></div>
<div class="sourceCode"><pre>##    user  system elapsed 
## 741.660  19.654 761.357 
</pre></div>
<div class="sourceCode r"><pre>## -----------------------------------------------------------------------------
## With racing

system.time({
  set.seed(2)
  svm_wflow %&gt;% tune_race_anova(resamples = rs, grid = svm_grid)
})
</pre></div>
<div class="sourceCode"><pre>##    user  system elapsed 
## 133.143   3.675 136.822 
</pre></div>
<p>Speed-up of 5.56-fold for racing.
</p>
<div class="sourceCode r"><pre>## -----------------------------------------------------------------------------
## Parallel processing setup

cores &lt;- parallel::detectCores(logical = FALSE)
cores
</pre></div>
<div class="sourceCode"><pre>## [1] 10
</pre></div>
<div class="sourceCode r"><pre>cl &lt;- makePSOCKcluster(cores)
registerDoParallel(cl)
</pre></div>
<div class="sourceCode r"><pre>## -----------------------------------------------------------------------------
## Parallel grid search

system.time({
  set.seed(2)
  svm_wflow %&gt;% tune_grid(resamples = rs, grid = svm_grid)
})
</pre></div>
<div class="sourceCode"><pre>##  user  system elapsed 
## 1.112   0.190 126.650 
</pre></div>
<p>Parallel processing with grid search was 6.01-fold faster than
sequential grid search.
</p>
<div class="sourceCode r"><pre>## -----------------------------------------------------------------------------
## Parallel racing

system.time({
  set.seed(2)
  svm_wflow %&gt;% tune_race_anova(resamples = rs, grid = svm_grid)
})
</pre></div>
<div class="sourceCode"><pre>##  user  system elapsed 
## 1.908   0.261  21.442 
</pre></div>
<p>Parallel processing with racing was 35.51-fold faster than sequential
grid search.
</p>
<p>There is a compounding effect of racing and parallel processing but its
magnitude depends on the type of model, number of resamples, number of
tuning parameters, and so on.
</p>



<h3>Value</h3>

<p>An object with primary class <code>tune_race</code> in the same standard format
as objects produced by <code><a href="tune.html#topic+tune_grid">tune::tune_grid()</a></code>.
</p>


<h3>References</h3>

<p>Kuhn, M 2014. &quot;Futility Analysis in the Cross-Validation of Machine Learning
Models.&quot; <a href="https://arxiv.org/abs/1405.6974">https://arxiv.org/abs/1405.6974</a>.
</p>


<h3>See Also</h3>

<p><code><a href="tune.html#topic+tune_grid">tune::tune_grid()</a></code>, <code><a href="#topic+control_race">control_race()</a></code>, <code><a href="#topic+tune_race_win_loss">tune_race_win_loss()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(parsnip)
library(rsample)
library(dials)

## -----------------------------------------------------------------------------

if (rlang::is_installed(c("discrim", "lme4", "modeldata"))) {
  library(discrim)
  data(two_class_dat, package = "modeldata")

  set.seed(6376)
  rs &lt;- bootstraps(two_class_dat, times = 10)

  ## -----------------------------------------------------------------------------

  # optimize an regularized discriminant analysis model
  rda_spec &lt;-
    discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %&gt;%
    set_engine("klaR")

  ## -----------------------------------------------------------------------------

  ctrl &lt;- control_race(verbose_elim = TRUE)
  set.seed(11)
  grid_anova &lt;-
    rda_spec %&gt;%
    tune_race_anova(Class ~ ., resamples = rs, grid = 10, control = ctrl)

  # Shows only the fully resampled parameters
  show_best(grid_anova, metric = "roc_auc", n = 2)

  plot_race(grid_anova)
}

</code></pre>

<hr>
<h2 id='tune_race_win_loss'>Efficient grid search via racing with win/loss statistics</h2><span id='topic+tune_race_win_loss'></span><span id='topic+tune_race_win_loss.model_spec'></span><span id='topic+tune_race_win_loss.workflow'></span>

<h3>Description</h3>

<p><code><a href="#topic+tune_race_win_loss">tune_race_win_loss()</a></code> computes a set of performance metrics (e.g. accuracy or RMSE)
for a pre-defined set of tuning parameters that correspond to a model or
recipe across one or more resamples of the data. After an initial number of
resamples have been evaluated, the process eliminates tuning parameter
combinations that are unlikely to be the best results using a statistical
model. For each pairwise combinations of tuning parameters, win/loss
statistics are calculated and a logistic regression model is used to measure
how likely each combination is to win overall.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_race_win_loss(object, ...)

## S3 method for class 'model_spec'
tune_race_win_loss(
  object,
  preprocessor,
  resamples,
  ...,
  param_info = NULL,
  grid = 10,
  metrics = NULL,
  eval_time = NULL,
  control = control_race()
)

## S3 method for class 'workflow'
tune_race_win_loss(
  object,
  resamples,
  ...,
  param_info = NULL,
  grid = 10,
  metrics = NULL,
  eval_time = NULL,
  control = control_race()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_race_win_loss_+3A_object">object</code></td>
<td>
<p>A <code>parsnip</code> model specification or a <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code>.</p>
</td></tr>
<tr><td><code id="tune_race_win_loss_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="tune_race_win_loss_+3A_preprocessor">preprocessor</code></td>
<td>
<p>A traditional model formula or a recipe created using
<code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>. This is only required when <code>object</code> is not a workflow.</p>
</td></tr>
<tr><td><code id="tune_race_win_loss_+3A_resamples">resamples</code></td>
<td>
<p>An <code>rset()</code> object that has multiple resamples (i.e., is not
a validation set).</p>
</td></tr>
<tr><td><code id="tune_race_win_loss_+3A_param_info">param_info</code></td>
<td>
<p>A <code><a href="dials.html#topic+parameters">dials::parameters()</a></code> object or <code>NULL</code>. If none is given,
a parameters set is derived from other arguments. Passing this argument can
be useful when parameter ranges need to be customized.</p>
</td></tr>
<tr><td><code id="tune_race_win_loss_+3A_grid">grid</code></td>
<td>
<p>A data frame of tuning combinations or a positive integer. The
data frame should have columns for each parameter being tuned and rows for
tuning parameter candidates. An integer denotes the number of candidate
parameter sets to be created automatically.</p>
</td></tr>
<tr><td><code id="tune_race_win_loss_+3A_metrics">metrics</code></td>
<td>
<p>A <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="tune_race_win_loss_+3A_eval_time">eval_time</code></td>
<td>
<p>A numeric vector of time points where dynamic event time
metrics should be computed (e.g. the time-dependent ROC curve, etc). The
values must be non-negative and should probably be no greater than the
largest event time in the training set (See Details below).</p>
</td></tr>
<tr><td><code id="tune_race_win_loss_+3A_control">control</code></td>
<td>
<p>An object used to modify the tuning process. See
<code><a href="#topic+control_race">control_race()</a></code> for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The technical details of this method are described in Kuhn (2014).
</p>
<p>Racing methods are efficient approaches to grid search. Initially, the
function evaluates all tuning parameters on a small initial set of
resamples. The <code>burn_in</code> argument of <code><a href="#topic+control_race">control_race()</a></code> sets the number of
initial resamples.
</p>
<p>The performance statistics from the current set of resamples are converted
to win/loss/tie results. For example, for two parameters (<code>j</code> and <code>k</code>) in a
classification model that have each been resampled three times:
</p>
<pre>
            |  area under the ROC curve |
            -----------------------------
   resample | parameter j | parameter k | winner
   ---------------------------------------------
       1    |    0.81     |     0.92    |   k
       2    |    0.95     |     0.94    |   j
       3    |    0.79     |     0.81    |   k
   ---------------------------------------------
</pre>
<p>After the third resample, parameter <code>k</code> has a 2:1 win/loss ratio versus <code>j</code>.
Parameters with equal results are treated as a half-win for each setting.
These statistics are determined for all pairwise combinations of the
parameters and a Bradley-Terry model is used to model these win/loss/tie
statistics. This model can compute the ability of a parameter combination to
win overall. A confidence interval for the winning ability is computed and
any settings whose interval includes zero are retained for future resamples
(since it is not statistically different form the best results).
</p>
<p>The next resample is used with the remaining parameter combinations and the
statistical analysis is updated. More candidate parameters may be excluded
with each new resample that is processed.
</p>
<p>The <code><a href="#topic+control_race">control_race()</a></code> function contains are parameter for the significance cutoff
applied to the Bradley-Terry model results as well as other relevant arguments.
</p>


<h4>Censored regression models</h4>

<p>With dynamic performance metrics (e.g. Brier or ROC curves), performance is
calculated for every value of <code>eval_time</code> but the <em>first</em> evaluation time
given by the user (e.g., <code>eval_time[1]</code>) is analyzed during racing.
</p>
<p>Also, values of <code>eval_time</code> should be less than the largest observed event
time in the training data. For many non-parametric models, the results beyond
the largest time corresponding to an event are constant (or <code>NA</code>).
</p>



<h3>Value</h3>

<p>An object with primary class <code>tune_race</code> in the same standard format
as objects produced by <code><a href="tune.html#topic+tune_grid">tune::tune_grid()</a></code>.
</p>


<h3>References</h3>

<p>Kuhn, M 2014. &quot;Futility Analysis in the Cross-Validation of Machine Learning
Models.&quot; <a href="https://arxiv.org/abs/1405.6974">https://arxiv.org/abs/1405.6974</a>.
</p>


<h3>See Also</h3>

<p><code><a href="tune.html#topic+tune_grid">tune::tune_grid()</a></code>, <code><a href="#topic+control_race">control_race()</a></code>, <code><a href="#topic+tune_race_anova">tune_race_anova()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(parsnip)
library(rsample)
library(dials)

## -----------------------------------------------------------------------------

if (rlang::is_installed(c("discrim", "modeldata"))) {
  library(discrim)
  data(two_class_dat, package = "modeldata")

  set.seed(6376)
  rs &lt;- bootstraps(two_class_dat, times = 10)

  ## -----------------------------------------------------------------------------

  # optimize an regularized discriminant analysis model
  rda_spec &lt;-
    discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %&gt;%
    set_engine("klaR")

  ## -----------------------------------------------------------------------------

  ctrl &lt;- control_race(verbose_elim = TRUE)

  set.seed(11)
  grid_wl &lt;-
    rda_spec %&gt;%
    tune_race_win_loss(Class ~ ., resamples = rs, grid = 10, control = ctrl)

  # Shows only the fully resampled parameters
  show_best(grid_wl, metric = "roc_auc")

  plot_race(grid_wl)
}

</code></pre>

<hr>
<h2 id='tune_sim_anneal'>Optimization of model parameters via simulated annealing</h2><span id='topic+tune_sim_anneal'></span><span id='topic+tune_sim_anneal.model_spec'></span><span id='topic+tune_sim_anneal.workflow'></span>

<h3>Description</h3>

<p><code><a href="#topic+tune_sim_anneal">tune_sim_anneal()</a></code> uses an iterative search procedure to generate new
candidate tuning parameter combinations based on previous results. It uses
the generalized simulated annealing method of Bohachevsky, Johnson, and
Stein (1986).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_sim_anneal(object, ...)

## S3 method for class 'model_spec'
tune_sim_anneal(
  object,
  preprocessor,
  resamples,
  ...,
  iter = 10,
  param_info = NULL,
  metrics = NULL,
  eval_time = NULL,
  initial = 1,
  control = control_sim_anneal()
)

## S3 method for class 'workflow'
tune_sim_anneal(
  object,
  resamples,
  ...,
  iter = 10,
  param_info = NULL,
  metrics = NULL,
  eval_time = NULL,
  initial = 1,
  control = control_sim_anneal()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_sim_anneal_+3A_object">object</code></td>
<td>
<p>A <code>parsnip</code> model specification or a <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code>.</p>
</td></tr>
<tr><td><code id="tune_sim_anneal_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="tune_sim_anneal_+3A_preprocessor">preprocessor</code></td>
<td>
<p>A traditional model formula or a recipe created using
<code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>. This is only required when <code>object</code> is not a workflow.</p>
</td></tr>
<tr><td><code id="tune_sim_anneal_+3A_resamples">resamples</code></td>
<td>
<p>An <code>rset()</code> object.</p>
</td></tr>
<tr><td><code id="tune_sim_anneal_+3A_iter">iter</code></td>
<td>
<p>The maximum number of search iterations.</p>
</td></tr>
<tr><td><code id="tune_sim_anneal_+3A_param_info">param_info</code></td>
<td>
<p>A <code><a href="dials.html#topic+parameters">dials::parameters()</a></code> object or <code>NULL</code>. If none is given,
a parameter set is derived from other arguments. Passing this argument can
be useful when parameter ranges need to be customized.</p>
</td></tr>
<tr><td><code id="tune_sim_anneal_+3A_metrics">metrics</code></td>
<td>
<p>A <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> object containing information on how
models will be evaluated for performance. The first metric in <code>metrics</code> is the
one that will be optimized.</p>
</td></tr>
<tr><td><code id="tune_sim_anneal_+3A_eval_time">eval_time</code></td>
<td>
<p>A numeric vector of time points where dynamic event time
metrics should be computed (e.g. the time-dependent ROC curve, etc). The
values must be non-negative and should probably be no greater than the
largest event time in the training set (See Details below).</p>
</td></tr>
<tr><td><code id="tune_sim_anneal_+3A_initial">initial</code></td>
<td>
<p>An initial set of results in a tidy format (as would the result
of <code><a href="tune.html#topic+tune_grid">tune_grid()</a></code>, <code><a href="tune.html#topic+tune_bayes">tune_bayes()</a></code>, <code><a href="#topic+tune_race_win_loss">tune_race_win_loss()</a></code>, or
<code><a href="#topic+tune_race_anova">tune_race_anova()</a></code>) or a positive integer. If the initial object was a
sequential search method, the simulated annealing iterations start after the
last iteration of the initial results.</p>
</td></tr>
<tr><td><code id="tune_sim_anneal_+3A_control">control</code></td>
<td>
<p>The results of <code><a href="#topic+control_sim_anneal">control_sim_anneal()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulated annealing is a global optimization method. For model tuning, it
can be used to iteratively search the parameter space for optimal tuning
parameter combinations. At each iteration, a new parameter combination is
created by perturbing the current parameters in some small way so that they
are within a small neighborhood. This new parameter combination is used to
fit a model and that model's performance is measured using resampling (or a
simple validation set).
</p>
<p>If the new settings have better results than the current settings, they are
accepted and the process continues.
</p>
<p>If the new settings has worse performance, a probability threshold is
computed for accepting these sub-optimal values. The probability is a
function of <em>how</em> sub-optimal the results are as well as how many iterations
have elapsed. This is referred to as the &quot;cooling schedule&quot; for the
algorithm. If the sub-optimal results are accepted, the next iterations
settings are based on these inferior results. Otherwise, new parameter
values are generated from the previous iteration's settings.
</p>
<p>This process continues for a pre-defined number of iterations and the
overall best settings are recommended for use. The <code><a href="#topic+control_sim_anneal">control_sim_anneal()</a></code>
function can specify the number of iterations without improvement for early
stopping. Also, that function can be used to specify a <em>restart</em> threshold;
if no globally best results have not be discovered within a certain number
if iterations, the process can restart using the last known settings that
globally best.
</p>


<h4>Creating new settings</h4>

<p>For each numeric parameter, the range of possible values is known as well
as any transformations. The current values are transformed and scaled to
have values between zero and one (based on the possible range of values). A
candidate set of values that are on a sphere with random radii between
<code>rmin</code> and <code>rmax</code> are generated. Infeasible values are removed and one value
is chosen at random. This value is back transformed to the original units
and scale and are used as the new settings. The argument <code>radius</code> of
<code><a href="#topic+control_sim_anneal">control_sim_anneal()</a></code> controls the range neighborhood sizes.
</p>
<p>For categorical and integer parameters, each is changes with a pre-defined
probability. The <code>flip</code> argument of <code><a href="#topic+control_sim_anneal">control_sim_anneal()</a></code> can be used to
specify this probability. For integer parameters, a nearby integer value is
used.
</p>
<p>Simulated annealing search may not be the preferred method when many of the
parameters are non-numeric or integers with few unique values. In these
cases, it is likely that the same candidate set may be tested more than
once.
</p>



<h4>Cooling schedule</h4>

<p>To determine the probability of accepting a new value, the percent
difference in performance is calculated. If the performance metric is to be
maximized, this would be <code>d = (new-old)/old*100</code>. The probability is
calculated as <code>p = exp(d * coef * iter)</code> were <code>coef</code> is a user-defined
constant that can be used to increase or decrease the probabilities.
</p>
<p>The <code>cooling_coef</code> of <code><a href="#topic+control_sim_anneal">control_sim_anneal()</a></code> can be used for this purpose.
</p>



<h4>Termination criterion</h4>

<p>The restart counter is reset when a new global best results is found.
</p>
<p>The termination counter resets when a new global best is located or when a
suboptimal result is improved.
</p>



<h4>Parallelism</h4>

<p>The <code>tune</code> and <code>finetune</code> packages currently parallelize over resamples.
Specifying a parallel back-end will improve the generation of the initial
set of sub-models (if any). Each iteration of the search are also run in
parallel if a parallel backend is registered.
</p>



<h4>Censored regression models</h4>

<p>With dynamic performance metrics (e.g. Brier or ROC curves), performance is
calculated for every value of <code>eval_time</code> but the <em>first</em> evaluation time
given by the user (e.g., <code>eval_time[1]</code>) is used to guide the optimization.
</p>
<p>Also, values of <code>eval_time</code> should be less than the largest observed event
time in the training data. For many non-parametric models, the results beyond
the largest time corresponding to an event are constant (or <code>NA</code>).
</p>



<h3>Value</h3>

<p>A tibble of results that mirror those generated by <code><a href="tune.html#topic+tune_grid">tune_grid()</a></code>.
However, these results contain an <code>.iter</code> column and replicate the <code>rset</code>
object multiple times over iterations (at limited additional memory costs).
</p>


<h3>References</h3>

<p>Bohachevsky, Johnson, and Stein (1986) &quot;Generalized Simulated Annealing for
Function Optimization&quot;, <em>Technometrics</em>, 28:3, 209-217
</p>


<h3>See Also</h3>

<p><code><a href="tune.html#topic+tune_grid">tune::tune_grid()</a></code>, <code><a href="#topic+control_sim_anneal">control_sim_anneal()</a></code>, <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(finetune)
library(rpart)
library(dplyr)
library(tune)
library(rsample)
library(parsnip)
library(workflows)
library(ggplot2)

## -----------------------------------------------------------------------------
if (rlang::is_installed("modeldata")) {
  data(two_class_dat, package = "modeldata")

  set.seed(5046)
  bt &lt;- bootstraps(two_class_dat, times = 5)

  ## -----------------------------------------------------------------------------

  cart_mod &lt;-
    decision_tree(cost_complexity = tune(), min_n = tune()) %&gt;%
    set_engine("rpart") %&gt;%
    set_mode("classification")

  ## -----------------------------------------------------------------------------

  # For reproducibility, set the seed before running.
  set.seed(10)
  sa_search &lt;-
    cart_mod %&gt;%
    tune_sim_anneal(Class ~ ., resamples = bt, iter = 10)

  autoplot(sa_search, metric = "roc_auc", type = "parameters") +
    theme_bw()

  ## -----------------------------------------------------------------------------
  # More iterations. `initial` can be any other tune_* object or an integer
  # (for new values).

  set.seed(11)
  more_search &lt;-
    cart_mod %&gt;%
    tune_sim_anneal(Class ~ ., resamples = bt, iter = 10, initial = sa_search)

  autoplot(more_search, metric = "roc_auc", type = "performance") +
    theme_bw()
}

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
