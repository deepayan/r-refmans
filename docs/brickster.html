<!DOCTYPE html><html lang="en"><head><title>Help for package brickster</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {brickster}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#access_control_req_group'><p>Access Control Request for Group</p></a></li>
<li><a href='#access_control_req_user'><p>Access Control Request For User</p></a></li>
<li><a href='#access_control_request'><p>Access Control Request</p></a></li>
<li><a href='#add_lib_path'><p>Add Library Path</p></a></li>
<li><a href='#aws_attributes'><p>AWS Attributes</p></a></li>
<li><a href='#azure_attributes'><p>Azure Attributes</p></a></li>
<li><a href='#close_workspace'><p>Close Databricks Workspace Connection</p></a></li>
<li><a href='#cluster_autoscale'><p>Cluster Autoscale</p></a></li>
<li><a href='#cluster_log_conf'><p>Cluster Log Configuration</p></a></li>
<li><a href='#cron_schedule'><p>Cron Schedule</p></a></li>
<li><a href='#DatabricksSqlClient'><p>Databricks SQL Connector</p></a></li>
<li><a href='#db_cluster_action'><p>Cluster Action Helper Function</p></a></li>
<li><a href='#db_cluster_create'><p>Create a Cluster</p></a></li>
<li><a href='#db_cluster_delete'><p>Delete/Terminate a Cluster</p></a></li>
<li><a href='#db_cluster_edit'><p>Edit a Cluster</p></a></li>
<li><a href='#db_cluster_events'><p>List Cluster Activity Events</p></a></li>
<li><a href='#db_cluster_get'><p>Get Details of a Cluster</p></a></li>
<li><a href='#db_cluster_list'><p>List Clusters</p></a></li>
<li><a href='#db_cluster_list_node_types'><p>List Available Cluster Node Types</p></a></li>
<li><a href='#db_cluster_list_zones'><p>List Availability Zones (AWS Only)</p></a></li>
<li><a href='#db_cluster_perm_delete'><p>Permanently Delete a Cluster</p></a></li>
<li><a href='#db_cluster_pin'><p>Pin a Cluster</p></a></li>
<li><a href='#db_cluster_resize'><p>Resize a Cluster</p></a></li>
<li><a href='#db_cluster_restart'><p>Restart a Cluster</p></a></li>
<li><a href='#db_cluster_runtime_versions'><p>List Available Databricks Runtime Versions</p></a></li>
<li><a href='#db_cluster_start'><p>Start a Cluster</p></a></li>
<li><a href='#db_cluster_terminate'><p>Delete/Terminate a Cluster</p></a></li>
<li><a href='#db_cluster_unpin'><p>Unpin a Cluster</p></a></li>
<li><a href='#db_context_command_cancel'><p>Cancel a Command</p></a></li>
<li><a href='#db_context_command_parse'><p>Parse Command Results</p></a></li>
<li><a href='#db_context_command_run'><p>Run a Command</p></a></li>
<li><a href='#db_context_command_run_and_wait'><p>Run a Command and Wait For Results</p></a></li>
<li><a href='#db_context_command_status'><p>Get Information About a Command</p></a></li>
<li><a href='#db_context_create'><p>Create an Execution Context</p></a></li>
<li><a href='#db_context_destroy'><p>Delete an Execution Context</p></a></li>
<li><a href='#db_context_manager'><p>Databricks Execution Context Manager (R6 Class)</p></a></li>
<li><a href='#db_context_status'><p>Get Information About an Execution Context</p></a></li>
<li><a href='#db_current_cloud'><p>Detect Current Workspaces Cloud</p></a></li>
<li><a href='#db_current_user'><p>Get Current User Info</p></a></li>
<li><a href='#db_current_workspace_id'><p>Detect Current Workspace ID</p></a></li>
<li><a href='#db_dbfs_add_block'><p>DBFS Add Block</p></a></li>
<li><a href='#db_dbfs_close'><p>DBFS Close</p></a></li>
<li><a href='#db_dbfs_create'><p>DBFS Create</p></a></li>
<li><a href='#db_dbfs_delete'><p>DBFS Delete</p></a></li>
<li><a href='#db_dbfs_get_status'><p>DBFS Get Status</p></a></li>
<li><a href='#db_dbfs_list'><p>DBFS List</p></a></li>
<li><a href='#db_dbfs_mkdirs'><p>DBFS mkdirs</p></a></li>
<li><a href='#db_dbfs_move'><p>DBFS Move</p></a></li>
<li><a href='#db_dbfs_put'><p>DBFS Put</p></a></li>
<li><a href='#db_dbfs_read'><p>DBFS Read</p></a></li>
<li><a href='#db_host'><p>Generate/Fetch Databricks Host</p></a></li>
<li><a href='#db_jobs_create'><p>Create Job</p></a></li>
<li><a href='#db_jobs_delete'><p>Delete a Job</p></a></li>
<li><a href='#db_jobs_get'><p>Get Job Details</p></a></li>
<li><a href='#db_jobs_list'><p>List Jobs</p></a></li>
<li><a href='#db_jobs_reset'><p>Overwrite All Settings For A Job</p></a></li>
<li><a href='#db_jobs_run_now'><p>Trigger A New Job Run</p></a></li>
<li><a href='#db_jobs_runs_cancel'><p>Cancel Job Run</p></a></li>
<li><a href='#db_jobs_runs_delete'><p>Delete Job Run</p></a></li>
<li><a href='#db_jobs_runs_export'><p>Export Job Run Output</p></a></li>
<li><a href='#db_jobs_runs_get'><p>Get Job Run Details</p></a></li>
<li><a href='#db_jobs_runs_get_output'><p>Get Job Run Output</p></a></li>
<li><a href='#db_jobs_runs_list'><p>List Job Runs</p></a></li>
<li><a href='#db_jobs_runs_submit'><p>Create And Trigger A One-Time Run</p></a></li>
<li><a href='#db_jobs_update'><p>Partially Update A Job</p></a></li>
<li><a href='#db_libs_all_cluster_statuses'><p>Get Status of All Libraries on All Clusters</p></a></li>
<li><a href='#db_libs_cluster_status'><p>Get Status of Libraries on Cluster</p></a></li>
<li><a href='#db_libs_install'><p>Install Library on Cluster</p></a></li>
<li><a href='#db_libs_uninstall'><p>Uninstall Library on Cluster</p></a></li>
<li><a href='#db_mlflow_model_approve_transition_req'><p>Approve Model Version Stage Transition Request</p></a></li>
<li><a href='#db_mlflow_model_delete_transition_req'><p>Delete a Model Version Stage Transition Request</p></a></li>
<li><a href='#db_mlflow_model_open_transition_reqs'><p>Get All Open Stage Transition Requests for the Model Version</p></a></li>
<li><a href='#db_mlflow_model_reject_transition_req'><p>Reject Model Version Stage Transition Request</p></a></li>
<li><a href='#db_mlflow_model_transition_req'><p>Make a Model Version Stage Transition Request</p></a></li>
<li><a href='#db_mlflow_model_transition_stage'><p>Transition a Model Version's Stage</p></a></li>
<li><a href='#db_mlflow_model_version_comment'><p>Make a Comment on a Model Version</p></a></li>
<li><a href='#db_mlflow_model_version_comment_delete'><p>Delete a Comment on a Model Version</p></a></li>
<li><a href='#db_mlflow_model_version_comment_edit'><p>Edit a Comment on a Model Version</p></a></li>
<li><a href='#db_mlflow_registered_model_details'><p>Get Registered Model Details</p></a></li>
<li><a href='#db_oauth_client'><p>Create OAuth 2.0 Client</p></a></li>
<li><a href='#db_perform_request'><p>Perform Databricks API Request</p></a></li>
<li><a href='#db_read_netrc'><p>Read .netrc File</p></a></li>
<li><a href='#db_repl'><p>Remote REPL to Databricks Cluster</p></a></li>
<li><a href='#db_repo_create'><p>Create Repo</p></a></li>
<li><a href='#db_repo_delete'><p>Delete Repo</p></a></li>
<li><a href='#db_repo_get'><p>Get Repo</p></a></li>
<li><a href='#db_repo_get_all'><p>Get All Repos</p></a></li>
<li><a href='#db_repo_update'><p>Update Repo</p></a></li>
<li><a href='#db_req_error_body'><p>Propagate Databricks API Errors</p></a></li>
<li><a href='#db_request'><p>Databricks Request Helper</p></a></li>
<li><a href='#db_request_json'><p>Generate Request JSON</p></a></li>
<li><a href='#db_secrets_delete'><p>Delete Secret in Secret Scope</p></a></li>
<li><a href='#db_secrets_list'><p>List Secrets in Secret Scope</p></a></li>
<li><a href='#db_secrets_put'><p>Put Secret in Secret Scope</p></a></li>
<li><a href='#db_secrets_scope_acl_delete'><p>Delete Secret Scope ACL</p></a></li>
<li><a href='#db_secrets_scope_acl_get'><p>Get Secret Scope ACL</p></a></li>
<li><a href='#db_secrets_scope_acl_list'><p>List Secret Scope ACL's</p></a></li>
<li><a href='#db_secrets_scope_acl_put'><p>Put ACL on Secret Scope</p></a></li>
<li><a href='#db_secrets_scope_create'><p>Create Secret Scope</p></a></li>
<li><a href='#db_secrets_scope_delete'><p>Delete Secret Scope</p></a></li>
<li><a href='#db_secrets_scope_list_all'><p>List Secret Scopes</p></a></li>
<li><a href='#db_sql_client'><p>Create Databricks SQL Connector Client</p></a></li>
<li><a href='#db_sql_exec_cancel'><p>Cancel SQL Query</p></a></li>
<li><a href='#db_sql_exec_query'><p>Execute SQL Query</p></a></li>
<li><a href='#db_sql_exec_result'><p>Get SQL Query Results</p></a></li>
<li><a href='#db_sql_exec_status'><p>Get SQL Query Status</p></a></li>
<li><a href='#db_sql_global_warehouse_get'><p>Get Global Warehouse Config</p></a></li>
<li><a href='#db_sql_query_history'><p>List Warehouse Query History</p></a></li>
<li><a href='#db_sql_warehouse_create'><p>Create Warehouse</p></a></li>
<li><a href='#db_sql_warehouse_delete'><p>Delete Warehouse</p></a></li>
<li><a href='#db_sql_warehouse_edit'><p>Edit Warehouse</p></a></li>
<li><a href='#db_sql_warehouse_get'><p>Get Warehouse</p></a></li>
<li><a href='#db_sql_warehouse_list'><p>List Warehouses</p></a></li>
<li><a href='#db_sql_warehouse_start'><p>Start Warehouse</p></a></li>
<li><a href='#db_sql_warehouse_stop'><p>Stop Warehouse</p></a></li>
<li><a href='#db_token'><p>Fetch Databricks Token</p></a></li>
<li><a href='#db_volume_delete'><p>Volume FileSystem Delete</p></a></li>
<li><a href='#db_volume_dir_create'><p>Volume FileSystem Create Directory</p></a></li>
<li><a href='#db_volume_dir_delete'><p>Volume FileSystem Delete Directory</p></a></li>
<li><a href='#db_volume_dir_exists'><p>Volume FileSystem Check Directory Exists</p></a></li>
<li><a href='#db_volume_file_exists'><p>Volume FileSystem File Status</p></a></li>
<li><a href='#db_volume_list'><p>Volume FileSystem List Directory Contents</p></a></li>
<li><a href='#db_volume_read'><p>Volume FileSystem Read</p></a></li>
<li><a href='#db_volume_write'><p>Volume FileSystem Write</p></a></li>
<li><a href='#db_vs_endpoints_create'><p>Create a Vector Search Endpoint</p></a></li>
<li><a href='#db_vs_endpoints_delete'><p>Delete a Vector Search Endpoint</p></a></li>
<li><a href='#db_vs_endpoints_get'><p>Get a Vector Search Endpoint</p></a></li>
<li><a href='#db_vs_endpoints_list'><p>List Vector Search Endpoints</p></a></li>
<li><a href='#db_vs_indexes_create'><p>Create a Vector Search Index</p></a></li>
<li><a href='#db_vs_indexes_delete'><p>Delete a Vector Search Index</p></a></li>
<li><a href='#db_vs_indexes_delete_data'><p>Delete Data from a Vector Search Index</p></a></li>
<li><a href='#db_vs_indexes_get'><p>Get a Vector Search Index</p></a></li>
<li><a href='#db_vs_indexes_list'><p>List Vector Search Indexes</p></a></li>
<li><a href='#db_vs_indexes_query'><p>Query a Vector Search Index</p></a></li>
<li><a href='#db_vs_indexes_query_next_page'><p>Query Vector Search Next Page</p></a></li>
<li><a href='#db_vs_indexes_scan'><p>Scan a Vector Search Index</p></a></li>
<li><a href='#db_vs_indexes_sync'><p>Synchronize a Vector Search Index</p></a></li>
<li><a href='#db_vs_indexes_upsert_data'><p>Upsert Data into a Vector Search Index</p></a></li>
<li><a href='#db_workspace_delete'><p>Delete Object/Directory (Workspaces)</p></a></li>
<li><a href='#db_workspace_export'><p>Export Notebook or Directory (Workspaces)</p></a></li>
<li><a href='#db_workspace_get_status'><p>Get Object Status (Workspaces)</p></a></li>
<li><a href='#db_workspace_import'><p>Import Notebook/Directory (Workspaces)</p></a></li>
<li><a href='#db_workspace_list'><p>List Directory Contents (Workspaces)</p></a></li>
<li><a href='#db_workspace_mkdirs'><p>Make a Directory (Workspaces)</p></a></li>
<li><a href='#db_wsid'><p>Fetch Databricks Workspace ID</p></a></li>
<li><a href='#dbfs_storage_info'><p>DBFS Storage Information</p></a></li>
<li><a href='#default_config_profile'><p>Returns the default config profile</p></a></li>
<li><a href='#delta_sync_index_spec'><p>Delta Sync Vector Search Index Specification</p></a></li>
<li><a href='#determine_brickster_venv'><p>Determine brickster virtualenv</p></a></li>
<li><a href='#direct_access_index_spec'><p>Delta Sync Vector Search Index Specification</p></a></li>
<li><a href='#docker_image'><p>Docker Image</p></a></li>
<li><a href='#email_notifications'><p>Email Notifications</p></a></li>
<li><a href='#embedding_source_column'><p>Embedding Source Column</p></a></li>
<li><a href='#embedding_vector_column'><p>Embedding Vector Column</p></a></li>
<li><a href='#file_storage_info'><p>File Storage Information</p></a></li>
<li><a href='#gcp_attributes'><p>GCP Attributes</p></a></li>
<li><a href='#get_and_start_cluster'><p>Get and Start Cluster</p></a></li>
<li><a href='#get_and_start_warehouse'><p>Get and Start Warehouse</p></a></li>
<li><a href='#get_latest_dbr'><p>Get Latest Databricks Runtime (DBR)</p></a></li>
<li><a href='#git_source'><p>Git Source for Job Notebook Tasks</p></a></li>
<li><a href='#in_databricks_nb'><p>Detect if running within Databricks Notebook</p></a></li>
<li><a href='#init_script_info'><p>Init Script Info</p></a></li>
<li><a href='#install_db_sql_connector'><p>Install Databricks SQL Connector (Python)</p></a></li>
<li><a href='#is.access_control_req_group'><p>Test if object is of class AccessControlRequestForGroup</p></a></li>
<li><a href='#is.access_control_req_user'><p>Test if object is of class AccessControlRequestForUser</p></a></li>
<li><a href='#is.access_control_request'><p>Test if object is of class AccessControlRequest</p></a></li>
<li><a href='#is.aws_attributes'><p>Test if object is of class AwsAttributes</p></a></li>
<li><a href='#is.azure_attributes'><p>Test if object is of class AzureAttributes</p></a></li>
<li><a href='#is.cluster_autoscale'><p>Test if object is of class AutoScale</p></a></li>
<li><a href='#is.cluster_log_conf'><p>Test if object is of class ClusterLogConf</p></a></li>
<li><a href='#is.cron_schedule'><p>Test if object is of class CronSchedule</p></a></li>
<li><a href='#is.dbfs_storage_info'><p>Test if object is of class DbfsStorageInfo</p></a></li>
<li><a href='#is.delta_sync_index'><p>Test if object is of class DeltaSyncIndex</p></a></li>
<li><a href='#is.direct_access_index'><p>Test if object is of class DirectAccessIndex</p></a></li>
<li><a href='#is.docker_image'><p>Test if object is of class DockerImage</p></a></li>
<li><a href='#is.email_notifications'><p>Test if object is of class JobEmailNotifications</p></a></li>
<li><a href='#is.embedding_source_column'><p>Test if object is of class EmbeddingSourceColumn</p></a></li>
<li><a href='#is.embedding_vector_column'><p>Test if object is of class EmbeddingVectorColumn</p></a></li>
<li><a href='#is.file_storage_info'><p>Test if object is of class FileStorageInfo</p></a></li>
<li><a href='#is.gcp_attributes'><p>Test if object is of class GcpAttributes</p></a></li>
<li><a href='#is.git_source'><p>Test if object is of class GitSource</p></a></li>
<li><a href='#is.init_script_info'><p>Test if object is of class InitScriptInfo</p></a></li>
<li><a href='#is.job_task'><p>Test if object is of class JobTaskSettings</p></a></li>
<li><a href='#is.lib_cran'><p>Test if object is of class CranLibrary</p></a></li>
<li><a href='#is.lib_egg'><p>Test if object is of class EggLibrary</p></a></li>
<li><a href='#is.lib_jar'><p>Test if object is of class JarLibrary</p></a></li>
<li><a href='#is.lib_maven'><p>Test if object is of class MavenLibrary</p></a></li>
<li><a href='#is.lib_pypi'><p>Test if object is of class PyPiLibrary</p></a></li>
<li><a href='#is.lib_whl'><p>Test if object is of class WhlLibrary</p></a></li>
<li><a href='#is.libraries'><p>Test if object is of class Libraries</p></a></li>
<li><a href='#is.library'><p>Test if object is of class Library</p></a></li>
<li><a href='#is.new_cluster'><p>Test if object is of class NewCluster</p></a></li>
<li><a href='#is.notebook_task'><p>Test if object is of class NotebookTask</p></a></li>
<li><a href='#is.pipeline_task'><p>Test if object is of class PipelineTask</p></a></li>
<li><a href='#is.python_wheel_task'><p>Test if object is of class PythonWheelTask</p></a></li>
<li><a href='#is.s3_storage_info'><p>Test if object is of class S3StorageInfo</p></a></li>
<li><a href='#is.spark_jar_task'><p>Test if object is of class SparkJarTask</p></a></li>
<li><a href='#is.spark_python_task'><p>Test if object is of class SparkPythonTask</p></a></li>
<li><a href='#is.spark_submit_task'><p>Test if object is of class SparkSubmitTask</p></a></li>
<li><a href='#is.valid_task_type'><p>Test if object is of class JobTask</p></a></li>
<li><a href='#is.vector_search_index_spec'><p>Test if object is of class VectorSearchIndexSpec</p></a></li>
<li><a href='#job_task'><p>Job Task</p></a></li>
<li><a href='#job_tasks'><p>Job Tasks</p></a></li>
<li><a href='#lib_cran'><p>Cran Library (R)</p></a></li>
<li><a href='#lib_egg'><p>Egg Library (Python)</p></a></li>
<li><a href='#lib_jar'><p>Jar Library (Scala)</p></a></li>
<li><a href='#lib_maven'><p>Maven Library (Scala)</p></a></li>
<li><a href='#lib_pypi'><p>PyPi Library (Python)</p></a></li>
<li><a href='#lib_whl'><p>Wheel Library (Python)</p></a></li>
<li><a href='#libraries'><p>Libraries</p></a></li>
<li><a href='#new_cluster'><p>New Cluster</p></a></li>
<li><a href='#notebook_task'><p>Notebook Task</p></a></li>
<li><a href='#open_workspace'><p>Connect to Databricks Workspace</p></a></li>
<li><a href='#pipeline_task'><p>Pipeline Task</p></a></li>
<li><a href='#py_db_sql_connector'><p>Databricks SQL Connector (Python)</p></a></li>
<li><a href='#python_wheel_task'><p>Python Wheel Task</p></a></li>
<li><a href='#read_databrickscfg'><p>Reads Databricks CLI Config</p></a></li>
<li><a href='#read_env_var'><p>Reads Environment Variables</p></a></li>
<li><a href='#remove_lib_path'><p>Remove Library Path</p></a></li>
<li><a href='#s3_storage_info'><p>S3 Storage Info</p></a></li>
<li><a href='#spark_jar_task'><p>Spark Jar Task</p></a></li>
<li><a href='#spark_python_task'><p>Spark Python Task</p></a></li>
<li><a href='#spark_submit_task'><p>Spark Submit Task</p></a></li>
<li><a href='#use_databricks_cfg'><p>Returns whether or not to use a <code>.databrickscfg</code> file</p></a></li>
<li><a href='#wait_for_lib_installs'><p>Wait for Libraries to Install on Databricks Cluster</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>R Toolkit for 'Databricks'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.6</td>
</tr>
<tr>
<td>Description:</td>
<td>Collection of utilities that improve using 'Databricks' from R. 
  Primarily functions that wrap specific 'Databricks' APIs
  (<a href="https://docs.databricks.com/api">https://docs.databricks.com/api</a>), 'RStudio' connection pane support, quality
  of life functions to make 'Databricks' simpler to use.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>arrow, base64enc, cli, curl, dplyr, glue, httr2, ini,
jsonlite, purrr, reticulate, R6 (&ge; 2.4.0), rlang, tibble,
utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0), huxtable, htmltools, knitr, magick,
rmarkdown, rstudioapi, withr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/databrickslabs/brickster">https://github.com/databrickslabs/brickster</a></td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-21 05:30:21 UTC; zachary.davies</td>
</tr>
<tr>
<td>Author:</td>
<td>Zac Davies [aut, cre],
  Rafi Kurlansik [aut],
  Databricks [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Zac Davies &lt;zac@databricks.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-21 05:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='access_control_req_group'>Access Control Request for Group</h2><span id='topic+access_control_req_group'></span>

<h3>Description</h3>

<p>Access Control Request for Group
</p>


<h3>Usage</h3>

<pre><code class='language-R'>access_control_req_group(
  group,
  permission_level = c("CAN_MANAGE", "CAN_MANAGE_RUN", "CAN_VIEW")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="access_control_req_group_+3A_group">group</code></td>
<td>
<p>Group name. There are two built-in groups: <code>users</code> for all users,
and <code>admins</code> for administrators.</p>
</td></tr>
<tr><td><code id="access_control_req_group_+3A_permission_level">permission_level</code></td>
<td>
<p>Permission level to grant. One of <code>CAN_MANAGE</code>,
<code>CAN_MANAGE_RUN</code>, <code>CAN_VIEW</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+access_control_request">access_control_request()</a></code>
</p>
<p>Other Access Control Request Objects: 
<code><a href="#topic+access_control_req_user">access_control_req_user</a>()</code>
</p>

<hr>
<h2 id='access_control_req_user'>Access Control Request For User</h2><span id='topic+access_control_req_user'></span>

<h3>Description</h3>

<p>Access Control Request For User
</p>


<h3>Usage</h3>

<pre><code class='language-R'>access_control_req_user(
  user_name,
  permission_level = c("CAN_MANAGE", "CAN_MANAGE_RUN", "CAN_VIEW", "IS_OWNER")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="access_control_req_user_+3A_user_name">user_name</code></td>
<td>
<p>Email address for the user.</p>
</td></tr>
<tr><td><code id="access_control_req_user_+3A_permission_level">permission_level</code></td>
<td>
<p>Permission level to grant. One of <code>CAN_MANAGE</code>,
<code>CAN_MANAGE_RUN</code>, <code>CAN_VIEW</code>, <code>IS_OWNER</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+access_control_request">access_control_request()</a></code>
</p>
<p>Other Access Control Request Objects: 
<code><a href="#topic+access_control_req_group">access_control_req_group</a>()</code>
</p>

<hr>
<h2 id='access_control_request'>Access Control Request</h2><span id='topic+access_control_request'></span>

<h3>Description</h3>

<p>Access Control Request
</p>


<h3>Usage</h3>

<pre><code class='language-R'>access_control_request(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="access_control_request_+3A_...">...</code></td>
<td>
<p>Instances of <code><a href="#topic+access_control_req_user">access_control_req_user()</a></code> or
<code><a href="#topic+access_control_req_group">access_control_req_group()</a></code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+db_jobs_create">db_jobs_create()</a></code>, <code><a href="#topic+db_jobs_reset">db_jobs_reset()</a></code>, <code><a href="#topic+db_jobs_update">db_jobs_update()</a></code>
</p>

<hr>
<h2 id='add_lib_path'>Add Library Path</h2><span id='topic+add_lib_path'></span>

<h3>Description</h3>

<p>Add Library Path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_lib_path(path, after, version = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="add_lib_path_+3A_path">path</code></td>
<td>
<p>Directory that will added as location for which packages
are searched. Recursively creates the directory if it doesn't exist. On
Databricks remember to use <code style="white-space: pre;">&#8288;/dbfs/&#8288;</code> or <code style="white-space: pre;">&#8288;/Volumes/...&#8288;</code> as a prefix.</p>
</td></tr>
<tr><td><code id="add_lib_path_+3A_after">after</code></td>
<td>
<p>Location at which to append the <code>path</code> value after.</p>
</td></tr>
<tr><td><code id="add_lib_path_+3A_version">version</code></td>
<td>
<p>If <code>TRUE</code> will add the R version string to the end
of <code>path</code>. This is recommended if using different R versions and sharing a
common <code>path</code> between users.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This functions primary use is when using Databricks notebooks or hosted
RStudio, however, it works anywhere.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+libPaths">base::.libPaths()</a></code>, <code><a href="#topic+remove_lib_path">remove_lib_path()</a></code>
</p>

<hr>
<h2 id='aws_attributes'>AWS Attributes</h2><span id='topic+aws_attributes'></span>

<h3>Description</h3>

<p>AWS Attributes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aws_attributes(
  first_on_demand = 1,
  availability = c("SPOT_WITH_FALLBACK", "SPOT", "ON_DEMAND"),
  zone_id = NULL,
  instance_profile_arn = NULL,
  spot_bid_price_percent = 100,
  ebs_volume_type = c("GENERAL_PURPOSE_SSD", "THROUGHPUT_OPTIMIZED_HDD"),
  ebs_volume_count = 1,
  ebs_volume_size = NULL,
  ebs_volume_iops = NULL,
  ebs_volume_throughput = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aws_attributes_+3A_first_on_demand">first_on_demand</code></td>
<td>
<p>Number of nodes of the cluster that will be placed on
on-demand instances. If this value is greater than 0, the cluster driver node
will be placed on an on-demand instance. If this value is greater than or
equal to the current cluster size, all nodes will be placed on on-demand
instances. If this value is less than the current cluster size,
<code>first_on_demand</code> nodes will be placed on on-demand instances and the
remainder will be placed on availability instances. This value does not
affect cluster size and cannot be mutated over the lifetime of a cluster.</p>
</td></tr>
<tr><td><code id="aws_attributes_+3A_availability">availability</code></td>
<td>
<p>One of <code>SPOT_WITH_FALLBACK</code>, <code>SPOT</code>, <code>ON_DEMAND.</code> Type
used for all subsequent nodes past the <code>first_on_demand</code> ones. If
<code>first_on_demand</code> is zero, this availability type will be used for the entire
cluster.</p>
</td></tr>
<tr><td><code id="aws_attributes_+3A_zone_id">zone_id</code></td>
<td>
<p>Identifier for the availability zone/datacenter in which the
cluster resides. You have three options: availability zone in same region as
the Databricks deployment, <code>auto</code> which selects based on available IPs,
<code>NULL</code> which will use the default availability zone.</p>
</td></tr>
<tr><td><code id="aws_attributes_+3A_instance_profile_arn">instance_profile_arn</code></td>
<td>
<p>Nodes for this cluster will only be placed on AWS
instances with this instance profile. If omitted, nodes will be placed on
instances without an instance profile. The instance profile must have
previously been added to the Databricks environment by an account
administrator. This feature may only be available to certain customer plans.</p>
</td></tr>
<tr><td><code id="aws_attributes_+3A_spot_bid_price_percent">spot_bid_price_percent</code></td>
<td>
<p>The max price for AWS spot instances, as a
percentage of the corresponding instance typeâ€™s on-demand price. For example,
if this field is set to 50, and the cluster needs a new i3.xlarge spot
instance, then the max price is half of the price of on-demand i3.xlarge
instances. Similarly, if this field is set to 200, the max price is twice the
price of on-demand i3.xlarge instances. If not specified, the default value
is 100. When spot instances are requested for this cluster, only spot
instances whose max price percentage matches this field will be considered.
For safety, we enforce this field to be no more than 10000.</p>
</td></tr>
<tr><td><code id="aws_attributes_+3A_ebs_volume_type">ebs_volume_type</code></td>
<td>
<p>Either <code>GENERAL_PURPOSE_SSD</code> or
<code>THROUGHPUT_OPTIMIZED_HDD</code></p>
</td></tr>
<tr><td><code id="aws_attributes_+3A_ebs_volume_count">ebs_volume_count</code></td>
<td>
<p>The number of volumes launched for each instance. You
can choose up to 10 volumes. This feature is only enabled for supported node
types. Legacy node types cannot specify custom EBS volumes. For node types
with no instance store, at least one EBS volume needs to be specified;
otherwise, cluster creation will fail. These EBS volumes will be mounted at
<code style="white-space: pre;">&#8288;/ebs0&#8288;</code>, <code style="white-space: pre;">&#8288;/ebs1&#8288;</code>, and etc. Instance store volumes will be mounted at
<code style="white-space: pre;">&#8288;/local_disk0&#8288;</code>, <code style="white-space: pre;">&#8288;/local_disk1&#8288;</code>, and etc.
</p>
<p>If EBS volumes are attached, Databricks will configure Spark to use only the
EBS volumes for scratch storage because heterogeneously sized scratch devices
can lead to inefficient disk utilization. If no EBS volumes are attached,
Databricks will configure Spark to use instance store volumes.
</p>
<p>If EBS volumes are specified, then the Spark configuration <code>spark.local.dir</code>
will be overridden.</p>
</td></tr>
<tr><td><code id="aws_attributes_+3A_ebs_volume_size">ebs_volume_size</code></td>
<td>
<p>The size of each EBS volume (in <code>GiB</code>) launched for
each instance. For general purpose SSD, this value must be within the
range <code>100 - 4096</code>. For throughput optimized HDD, this value must be
within the range <code>500 - 4096</code>.
</p>
<p>Custom EBS volumes cannot be specified for the legacy node types
(memory-optimized and compute-optimized).</p>
</td></tr>
<tr><td><code id="aws_attributes_+3A_ebs_volume_iops">ebs_volume_iops</code></td>
<td>
<p>The number of IOPS per EBS gp3 volume. This value must
be between 3000 and 16000. The value of IOPS and throughput is calculated
based on AWS documentation to match the maximum performance of a gp2 volume
with the same volume size.</p>
</td></tr>
<tr><td><code id="aws_attributes_+3A_ebs_volume_throughput">ebs_volume_throughput</code></td>
<td>
<p>The throughput per EBS gp3 volume, in <code>MiB</code> per
second. This value must be between 125 and 1000.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>ebs_volume_iops</code>, <code>ebs_volume_throughput</code>, or both are not specified, the
values will be inferred from the throughput and IOPS of a gp2 volume with the
same disk size, by using the following calculation:
</p>

<table>
<tr>
 <td style="text-align: left;">
<strong>Disk size</strong> </td><td style="text-align: center;"> <strong>IOPS</strong> </td><td style="text-align: center;"> <strong>Throughput</strong> </td>
</tr>
<tr>
 <td style="text-align: left;">
Greater than 1000    </td><td style="text-align: center;"> 3 times the disk size up to 16000 </td><td style="text-align: center;"> 250</td>
</tr>
<tr>
 <td style="text-align: left;">
Between 170 and 1000 </td><td style="text-align: center;"> 3000                              </td><td style="text-align: center;"> 250</td>
</tr>
<tr>
 <td style="text-align: left;">
Below 170            </td><td style="text-align: center;"> 3000                              </td><td style="text-align: center;"> 128
</td>
</tr>

</table>



<h3>See Also</h3>

<p><code><a href="#topic+db_cluster_create">db_cluster_create()</a></code>, <code><a href="#topic+db_cluster_edit">db_cluster_edit()</a></code>
</p>
<p>Other Cloud Attributes: 
<code><a href="#topic+azure_attributes">azure_attributes</a>()</code>,
<code><a href="#topic+gcp_attributes">gcp_attributes</a>()</code>
</p>

<hr>
<h2 id='azure_attributes'>Azure Attributes</h2><span id='topic+azure_attributes'></span>

<h3>Description</h3>

<p>Azure Attributes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>azure_attributes(
  first_on_demand = 1,
  availability = c("SPOT_WITH_FALLBACK", "SPOT", "ON_DEMAND"),
  spot_bid_max_price = -1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="azure_attributes_+3A_first_on_demand">first_on_demand</code></td>
<td>
<p>Number of nodes of the cluster that will be placed on
on-demand instances. If this value is greater than 0, the cluster driver node
will be placed on an on-demand instance. If this value is greater than or
equal to the current cluster size, all nodes will be placed on on-demand
instances. If this value is less than the current cluster size,
<code>first_on_demand</code> nodes will be placed on on-demand instances and the
remainder will be placed on availability instances. This value does not
affect cluster size and cannot be mutated over the lifetime of a cluster.</p>
</td></tr>
<tr><td><code id="azure_attributes_+3A_availability">availability</code></td>
<td>
<p>One of <code>SPOT_WITH_FALLBACK</code>, <code>SPOT</code>, <code>ON_DEMAND.</code> Type
used for all subsequent nodes past the <code>first_on_demand</code> ones. If
<code>first_on_demand</code> is zero, this availability type will be used for the entire
cluster.</p>
</td></tr>
<tr><td><code id="azure_attributes_+3A_spot_bid_max_price">spot_bid_max_price</code></td>
<td>
<p>The max bid price used for Azure spot instances.
You can set this to greater than or equal to the current spot price. You can
also set this to -1 (the default), which specifies that the instance cannot
be evicted on the basis of price. The price for the instance will be the
current price for spot instances or the price for a standard instance. You
can view historical pricing and eviction rates in the Azure portal.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+db_cluster_create">db_cluster_create()</a></code>, <code><a href="#topic+db_cluster_edit">db_cluster_edit()</a></code>
</p>
<p>Other Cloud Attributes: 
<code><a href="#topic+aws_attributes">aws_attributes</a>()</code>,
<code><a href="#topic+gcp_attributes">gcp_attributes</a>()</code>
</p>

<hr>
<h2 id='close_workspace'>Close Databricks Workspace Connection</h2><span id='topic+close_workspace'></span>

<h3>Description</h3>

<p>Close Databricks Workspace Connection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>close_workspace(host = db_host())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="close_workspace_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
close_workspace(host = db_host())

## End(Not run)
</code></pre>

<hr>
<h2 id='cluster_autoscale'>Cluster Autoscale</h2><span id='topic+cluster_autoscale'></span>

<h3>Description</h3>

<p>Range defining the min and max number of cluster workers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_autoscale(min_workers, max_workers)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cluster_autoscale_+3A_min_workers">min_workers</code></td>
<td>
<p>The minimum number of workers to which the cluster can
scale down when underutilized. It is also the initial number of workers the
cluster will have after creation.</p>
</td></tr>
<tr><td><code id="cluster_autoscale_+3A_max_workers">max_workers</code></td>
<td>
<p>The maximum number of workers to which the cluster can
scale up when overloaded. <code>max_workers</code> must be strictly greater than
<code>min_workers</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+db_cluster_create">db_cluster_create()</a></code>, <code><a href="#topic+db_cluster_edit">db_cluster_edit()</a></code>
</p>

<hr>
<h2 id='cluster_log_conf'>Cluster Log Configuration</h2><span id='topic+cluster_log_conf'></span>

<h3>Description</h3>

<p>Path to cluster log.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_log_conf(dbfs = NULL, s3 = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cluster_log_conf_+3A_dbfs">dbfs</code></td>
<td>
<p>Instance of <code><a href="#topic+dbfs_storage_info">dbfs_storage_info()</a></code>.</p>
</td></tr>
<tr><td><code id="cluster_log_conf_+3A_s3">s3</code></td>
<td>
<p>Instance of <code><a href="#topic+s3_storage_info">s3_storage_info()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dbfs</code> and <code>s3</code> are mutually exclusive, logs can only be sent to
one destination.
</p>


<h3>See Also</h3>

<p>Other Cluster Log Configuration Objects: 
<code><a href="#topic+dbfs_storage_info">dbfs_storage_info</a>()</code>,
<code><a href="#topic+s3_storage_info">s3_storage_info</a>()</code>
</p>

<hr>
<h2 id='cron_schedule'>Cron Schedule</h2><span id='topic+cron_schedule'></span>

<h3>Description</h3>

<p>Cron Schedule
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cron_schedule(
  quartz_cron_expression,
  timezone_id = "Etc/UTC",
  pause_status = c("UNPAUSED", "PAUSED")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cron_schedule_+3A_quartz_cron_expression">quartz_cron_expression</code></td>
<td>
<p>Cron expression using Quartz syntax that
describes the schedule for a job.
See <a href="https://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html">Cron Trigger</a>
for details.</p>
</td></tr>
<tr><td><code id="cron_schedule_+3A_timezone_id">timezone_id</code></td>
<td>
<p>Java timezone ID. The schedule for a job is resolved with
respect to this timezone.
See <a href="https://docs.oracle.com/javase/7/docs/api/java/util/TimeZone.html">Java TimeZone</a>
for details.</p>
</td></tr>
<tr><td><code id="cron_schedule_+3A_pause_status">pause_status</code></td>
<td>
<p>Indicate whether this schedule is paused or not. Either
<code>UNPAUSED</code> (default) or <code>PAUSED</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+db_jobs_create">db_jobs_create()</a></code>, <code><a href="#topic+db_jobs_reset">db_jobs_reset()</a></code>, <code><a href="#topic+db_jobs_update">db_jobs_update()</a></code>
</p>

<hr>
<h2 id='DatabricksSqlClient'>Databricks SQL Connector</h2><span id='topic+DatabricksSqlClient'></span>

<h3>Description</h3>

<p>Wraps the <a href="https://github.com/databricks/databricks-sql-python"><code>databricks-sql-connector</code></a>
using <a href="https://rstudio.github.io/reticulate/">reticulate</a>.
</p>
<p><a href="https://docs.databricks.com/en/dev-tools/python-sql-connector.html#api-reference">API reference on Databricks docs</a>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-db_sql_client-new"><code>DatabricksSqlClient$new()</code></a>
</p>
</li>
<li> <p><a href="#method-db_sql_client-columns"><code>DatabricksSqlClient$columns()</code></a>
</p>
</li>
<li> <p><a href="#method-db_sql_client-catalogs"><code>DatabricksSqlClient$catalogs()</code></a>
</p>
</li>
<li> <p><a href="#method-db_sql_client-schemas"><code>DatabricksSqlClient$schemas()</code></a>
</p>
</li>
<li> <p><a href="#method-db_sql_client-tables"><code>DatabricksSqlClient$tables()</code></a>
</p>
</li>
<li> <p><a href="#method-db_sql_client-execute"><code>DatabricksSqlClient$execute()</code></a>
</p>
</li>
<li> <p><a href="#method-db_sql_client-execute_many"><code>DatabricksSqlClient$execute_many()</code></a>
</p>
</li>
<li> <p><a href="#method-db_sql_client-clone"><code>DatabricksSqlClient$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-db_sql_client-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>
<p>Note that this object is typically constructed via <code><a href="#topic+db_sql_client">db_sql_client()</a></code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>DatabricksSqlClient$new(
  host,
  token,
  http_path,
  catalog,
  schema,
  use_cloud_fetch,
  session_configuration,
  ...
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>host</code></dt><dd><p>(<code>character(1)</code>)<br />
See <code><a href="#topic+db_sql_client">db_sql_client()</a></code>.</p>
</dd>
<dt><code>token</code></dt><dd><p>(<code>character(1)</code>)<br />
See <code><a href="#topic+db_sql_client">db_sql_client()</a></code>.</p>
</dd>
<dt><code>http_path</code></dt><dd><p>(<code>character(1)</code>)<br />
See <code><a href="#topic+db_sql_client">db_sql_client()</a></code>.</p>
</dd>
<dt><code>catalog</code></dt><dd><p>(<code>character(1)</code>)<br />
See <code><a href="#topic+db_sql_client">db_sql_client()</a></code>.</p>
</dd>
<dt><code>schema</code></dt><dd><p>(<code>character(1)</code>)<br />
See <code><a href="#topic+db_sql_client">db_sql_client()</a></code>.</p>
</dd>
<dt><code>use_cloud_fetch</code></dt><dd><p>(<code>logical(1)</code>)<br />
See <code><a href="#topic+db_sql_client">db_sql_client()</a></code>.</p>
</dd>
<dt><code>session_configuration</code></dt><dd><p>(<code>list(...)</code>)<br />
See <code><a href="#topic+db_sql_client">db_sql_client()</a></code>.</p>
</dd>
<dt><code>...</code></dt><dd><p>Parameters passed to <a href="https://docs.databricks.com/en/dev-tools/python-sql-connector.html#methods">connection method</a></p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="#topic+DatabricksSqlClient">DatabricksSqlClient</a>.
</p>


<hr>
<a id="method-db_sql_client-columns"></a>



<h4>Method <code>columns()</code></h4>

<p>Execute a metadata query about the columns.
</p>


<h5>Usage</h5>

<div class="r"><pre>DatabricksSqlClient$columns(
  catalog_name = NULL,
  schema_name = NULL,
  table_name = NULL,
  column_name = NULL,
  as_tibble = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>catalog_name</code></dt><dd><p>(<code>character(1)</code>)<br />
A catalog name to retrieve information about.
The <code style="white-space: pre;">&#8288;%&#8288;</code> character is interpreted as a wildcard.</p>
</dd>
<dt><code>schema_name</code></dt><dd><p>(<code>character(1)</code>)<br />
A schema name to retrieve information about.
The <code style="white-space: pre;">&#8288;%&#8288;</code> character is interpreted as a wildcard.</p>
</dd>
<dt><code>table_name</code></dt><dd><p>(<code>character(1)</code>)<br />
A table name to retrieve information about.
The <code style="white-space: pre;">&#8288;%&#8288;</code> character is interpreted as a wildcard.</p>
</dd>
<dt><code>column_name</code></dt><dd><p>(<code>character(1)</code>)<br />
A column name to retrieve information about.
The <code style="white-space: pre;">&#8288;%&#8288;</code> character is interpreted as a wildcard.</p>
</dd>
<dt><code>as_tibble</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default) will return <a href="tibble.html#topic+tibble">tibble::tibble</a>, otherwise returns
<a href="arrow.html#topic+Table-class">arrow::Table</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="tibble.html#topic+tibble">tibble::tibble</a> or <a href="arrow.html#topic+Table-class">arrow::Table</a>.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
  client$columns(catalog_name = "defa%")
  client$columns(catalog_name = "default", table_name = "gold_%")
}
</pre>
</div>


<hr>
<a id="method-db_sql_client-catalogs"></a>



<h4>Method <code>catalogs()</code></h4>

<p>Execute a metadata query about the catalogs.
</p>


<h5>Usage</h5>

<div class="r"><pre>DatabricksSqlClient$catalogs(as_tibble = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>as_tibble</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default) will return <a href="tibble.html#topic+tibble">tibble::tibble</a>, otherwise returns
<a href="arrow.html#topic+Table-class">arrow::Table</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="tibble.html#topic+tibble">tibble::tibble</a> or <a href="arrow.html#topic+Table-class">arrow::Table</a>.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
  client$catalogs()
}
</pre>
</div>


<hr>
<a id="method-db_sql_client-schemas"></a>



<h4>Method <code>schemas()</code></h4>

<p>Execute a metadata query about the schemas.
</p>


<h5>Usage</h5>

<div class="r"><pre>DatabricksSqlClient$schemas(
  catalog_name = NULL,
  schema_name = NULL,
  as_tibble = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>catalog_name</code></dt><dd><p>(<code>character(1)</code>)<br />
A catalog name to retrieve information about.
The <code style="white-space: pre;">&#8288;%&#8288;</code> character is interpreted as a wildcard.</p>
</dd>
<dt><code>schema_name</code></dt><dd><p>(<code>character(1)</code>)<br />
A schema name to retrieve information about.
The <code style="white-space: pre;">&#8288;%&#8288;</code> character is interpreted as a wildcard.</p>
</dd>
<dt><code>as_tibble</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default) will return <a href="tibble.html#topic+tibble">tibble::tibble</a>, otherwise returns
<a href="arrow.html#topic+Table-class">arrow::Table</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="tibble.html#topic+tibble">tibble::tibble</a> or <a href="arrow.html#topic+Table-class">arrow::Table</a>.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
  client$schemas(catalog_name = "main")
}
</pre>
</div>


<hr>
<a id="method-db_sql_client-tables"></a>



<h4>Method <code>tables()</code></h4>

<p>Execute a metadata query about tables and views
</p>


<h5>Usage</h5>

<div class="r"><pre>DatabricksSqlClient$tables(
  catalog_name = NULL,
  schema_name = NULL,
  table_name = NULL,
  table_types = NULL,
  as_tibble = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>catalog_name</code></dt><dd><p>(<code>character(1)</code>)<br />
A catalog name to retrieve information about.
The <code style="white-space: pre;">&#8288;%&#8288;</code> character is interpreted as a wildcard.</p>
</dd>
<dt><code>schema_name</code></dt><dd><p>(<code>character(1)</code>)<br />
A schema name to retrieve information about.
The <code style="white-space: pre;">&#8288;%&#8288;</code> character is interpreted as a wildcard.</p>
</dd>
<dt><code>table_name</code></dt><dd><p>(<code>character(1)</code>)<br />
A table name to retrieve information about.
The <code style="white-space: pre;">&#8288;%&#8288;</code> character is interpreted as a wildcard.</p>
</dd>
<dt><code>table_types</code></dt><dd><p>(<code>character()</code>)<br />
A list of table types to match, for example <code>"TABLE"</code> or <code>"VIEW"</code>.</p>
</dd>
<dt><code>as_tibble</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default) will return <a href="tibble.html#topic+tibble">tibble::tibble</a>, otherwise returns
<a href="arrow.html#topic+Table-class">arrow::Table</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="tibble.html#topic+tibble">tibble::tibble</a> or <a href="arrow.html#topic+Table-class">arrow::Table</a>.
</p>


<hr>
<a id="method-db_sql_client-execute"></a>



<h4>Method <code>execute()</code></h4>

<p>Prepares and then runs a database query or command.
</p>


<h5>Usage</h5>

<div class="r"><pre>DatabricksSqlClient$execute(operation, parameters = NULL, as_tibble = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>operation</code></dt><dd><p>(<code>character(1)</code>)<br />
The query or command to prepare and then run.</p>
</dd>
<dt><code>parameters</code></dt><dd><p>(<code>list()</code>)<br />
Optional. A sequence of parameters to use with the operation parameter.</p>
</dd>
<dt><code>as_tibble</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default) will return <a href="tibble.html#topic+tibble">tibble::tibble</a>, otherwise returns
<a href="arrow.html#topic+Table-class">arrow::Table</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="tibble.html#topic+tibble">tibble::tibble</a> or <a href="arrow.html#topic+Table-class">arrow::Table</a>.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
 client$execute("select 1")
 client$execute("select * from x.y.z limit 100")
 client$execute(
   operation = "select * from x.y.z where a &lt; %(threshold)s limit 1000",
   parameters = list(threshold = 100)
 )
}
</pre>
</div>


<hr>
<a id="method-db_sql_client-execute_many"></a>



<h4>Method <code>execute_many()</code></h4>

<p>Prepares and then runs a database query or command using all parameter
sequences in the seq_of_parameters argument. Only the final result set
is retained.
</p>


<h5>Usage</h5>

<div class="r"><pre>DatabricksSqlClient$execute_many(
  operation,
  seq_of_parameters = NULL,
  as_tibble = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>operation</code></dt><dd><p>(<code>character(1)</code>)<br />
The query or command to prepare and then run.</p>
</dd>
<dt><code>seq_of_parameters</code></dt><dd><p>(<code>list(list())</code>)<br />
A sequence of many sets of parameter values to use with the operation
parameter.</p>
</dd>
<dt><code>as_tibble</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default) will return <a href="tibble.html#topic+tibble">tibble::tibble</a>, otherwise returns
<a href="arrow.html#topic+Table-class">arrow::Table</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="tibble.html#topic+tibble">tibble::tibble</a> or <a href="arrow.html#topic+Table-class">arrow::Table</a>.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>\dontrun{
 client$execute_many(
   operation = "select * from x.y.z where a &lt; %(threshold)s limit 1000",
   seq_of_parameters = list(
     list(threshold = 100),
     list(threshold = 200),
     list(threshold = 300)
   )
 )
}
</pre>
</div>


<hr>
<a id="method-db_sql_client-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>DatabricksSqlClient$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## ------------------------------------------------
## Method `DatabricksSqlClient$columns`
## ------------------------------------------------

## Not run: 
  client$columns(catalog_name = "defa%")
  client$columns(catalog_name = "default", table_name = "gold_%")

## End(Not run)

## ------------------------------------------------
## Method `DatabricksSqlClient$catalogs`
## ------------------------------------------------

## Not run: 
  client$catalogs()

## End(Not run)

## ------------------------------------------------
## Method `DatabricksSqlClient$schemas`
## ------------------------------------------------

## Not run: 
  client$schemas(catalog_name = "main")

## End(Not run)

## ------------------------------------------------
## Method `DatabricksSqlClient$execute`
## ------------------------------------------------

## Not run: 
 client$execute("select 1")
 client$execute("select * from x.y.z limit 100")
 client$execute(
   operation = "select * from x.y.z where a &lt; %(threshold)s limit 1000",
   parameters = list(threshold = 100)
 )

## End(Not run)

## ------------------------------------------------
## Method `DatabricksSqlClient$execute_many`
## ------------------------------------------------

## Not run: 
 client$execute_many(
   operation = "select * from x.y.z where a &lt; %(threshold)s limit 1000",
   seq_of_parameters = list(
     list(threshold = 100),
     list(threshold = 200),
     list(threshold = 300)
   )
 )

## End(Not run)
</code></pre>

<hr>
<h2 id='db_cluster_action'>Cluster Action Helper Function</h2><span id='topic+db_cluster_action'></span>

<h3>Description</h3>

<p>Cluster Action Helper Function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_action(
  cluster_id,
  action = c("start", "restart", "delete", "permanent-delete", "pin", "unpin"),
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_action_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_action_+3A_action">action</code></td>
<td>
<p>One of <code>start</code>, <code>restart</code>, <code>delete</code>, <code>permanent-delete</code>, <code>pin</code>,
<code>unpin</code>.</p>
</td></tr>
<tr><td><code id="db_cluster_action_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_action_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_action_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>

<hr>
<h2 id='db_cluster_create'>Create a Cluster</h2><span id='topic+db_cluster_create'></span>

<h3>Description</h3>

<p>Create a Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_create(
  name,
  spark_version,
  node_type_id,
  num_workers = NULL,
  autoscale = NULL,
  spark_conf = list(),
  cloud_attrs = aws_attributes(),
  driver_node_type_id = NULL,
  custom_tags = list(),
  init_scripts = list(),
  spark_env_vars = list(),
  autotermination_minutes = 120,
  log_conf = NULL,
  ssh_public_keys = NULL,
  driver_instance_pool_id = NULL,
  instance_pool_id = NULL,
  idempotency_token = NULL,
  enable_elastic_disk = TRUE,
  apply_policy_default_values = TRUE,
  enable_local_disk_encryption = TRUE,
  docker_image = NULL,
  policy_id = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_create_+3A_name">name</code></td>
<td>
<p>Cluster name requested by the user. This doesnâ€™t have to be
unique. If not specified at creation, the cluster name will be an empty
string.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_spark_version">spark_version</code></td>
<td>
<p>The runtime version of the cluster. You can retrieve a
list of available runtime versions by using <code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_node_type_id">node_type_id</code></td>
<td>
<p>The node type for the worker nodes.
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types()</a></code> can be used to see available node types.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_num_workers">num_workers</code></td>
<td>
<p>Number of worker nodes that this cluster should have. A
cluster has one Spark driver and <code>num_workers</code> executors for a total of
<code>num_workers</code> + 1 Spark nodes.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_autoscale">autoscale</code></td>
<td>
<p>Instance of <code><a href="#topic+cluster_autoscale">cluster_autoscale()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_spark_conf">spark_conf</code></td>
<td>
<p>Named list. An object containing a set of optional,
user-specified Spark configuration key-value pairs. You can also pass in a
string of extra JVM options to the driver and the executors via
<code>spark.driver.extraJavaOptions</code> and <code>spark.executor.extraJavaOptions</code>
respectively. E.g. <code>list("spark.speculation" = true, "spark.streaming.ui.retainedBatches" = 5)</code>.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_cloud_attrs">cloud_attrs</code></td>
<td>
<p>Attributes related to clusters running on specific cloud
provider. Defaults to <code><a href="#topic+aws_attributes">aws_attributes()</a></code>. Must be one of <code><a href="#topic+aws_attributes">aws_attributes()</a></code>,
<code><a href="#topic+azure_attributes">azure_attributes()</a></code>, <code><a href="#topic+gcp_attributes">gcp_attributes()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_driver_node_type_id">driver_node_type_id</code></td>
<td>
<p>The node type of the Spark driver. This field is
optional; if unset, the driver node type will be set as the same value as
<code>node_type_id</code> defined above. <code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types()</a></code> can be used to
see available node types.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_custom_tags">custom_tags</code></td>
<td>
<p>Named list. An object containing a set of tags for cluster
resources. Databricks tags all cluster resources with these tags in addition
to <code>default_tags</code>. Databricks allows at most 45 custom tags.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_init_scripts">init_scripts</code></td>
<td>
<p>Instance of <code><a href="#topic+init_script_info">init_script_info()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_spark_env_vars">spark_env_vars</code></td>
<td>
<p>Named list. User-specified environment variable
key-value pairs. In order to specify an additional set of
<code>SPARK_DAEMON_JAVA_OPTS</code>, we recommend appending them to
<code style="white-space: pre;">&#8288;$SPARK_DAEMON_JAVA_OPTS&#8288;</code> as shown in the following example. This ensures
that all default Databricks managed environmental variables are included as
well. E.g. <code>{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}</code></p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_autotermination_minutes">autotermination_minutes</code></td>
<td>
<p>Automatically terminates the cluster after it
is inactive for this time in minutes. If not set, this cluster will not be
automatically terminated. If specified, the threshold must be between 10 and
10000 minutes. You can also set this value to 0 to explicitly disable
automatic termination. Defaults to 120.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_log_conf">log_conf</code></td>
<td>
<p>Instance of <code><a href="#topic+cluster_log_conf">cluster_log_conf()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_ssh_public_keys">ssh_public_keys</code></td>
<td>
<p>List. SSH public key contents that will be added to each
Spark node in this cluster. The corresponding private keys can be used to
login with the user name ubuntu on port 2200. Up to 10 keys can be specified.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_driver_instance_pool_id">driver_instance_pool_id</code></td>
<td>
<p>ID of the instance pool to use for the
driver node. You must also specify <code>instance_pool_id</code>. Optional.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_instance_pool_id">instance_pool_id</code></td>
<td>
<p>ID of the instance pool to use for cluster nodes. If
<code>driver_instance_pool_id</code> is present, <code>instance_pool_id</code> is used for worker
nodes only. Otherwise, it is used for both the driver and worker nodes.
Optional.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_idempotency_token">idempotency_token</code></td>
<td>
<p>An optional token that can be used to guarantee the
idempotency of cluster creation requests. If an active cluster with the
provided token already exists, the request will not create a new cluster,
but it will return the ID of the existing cluster instead. The existence of a
cluster with the same token is not checked against terminated clusters. If
you specify the idempotency token, upon failure you can retry until the
request succeeds. Databricks guarantees that exactly one cluster will be
launched with that idempotency token. This token should have at most 64
characters.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_enable_elastic_disk">enable_elastic_disk</code></td>
<td>
<p>When enabled, this cluster will dynamically
acquire additional disk space when its Spark workers are running low on
disk space.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_apply_policy_default_values">apply_policy_default_values</code></td>
<td>
<p>Boolean (Default: <code>TRUE</code>), whether to use
policy default values for missing cluster attributes.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_enable_local_disk_encryption">enable_local_disk_encryption</code></td>
<td>
<p>Boolean (Default: <code>TRUE</code>), whether
encryption of disks locally attached to the cluster is enabled.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_docker_image">docker_image</code></td>
<td>
<p>Instance of <code><a href="#topic+docker_image">docker_image()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_policy_id">policy_id</code></td>
<td>
<p>String, ID of a cluster policy.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_create_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a new Apache Spark cluster. This method acquires new instances from
the cloud provider if necessary. This method is asynchronous; the returned
<code>cluster_id</code> can be used to poll the cluster state (<code><a href="#topic+db_cluster_get">db_cluster_get()</a></code>).
When this method returns, the cluster is in a <code>PENDING</code> state. The cluster is
usable once it enters a <code>RUNNING</code> state.
</p>
<p>Databricks may not be able to acquire some of the requested nodes, due to
cloud provider limitations or transient network issues. If Databricks
acquires at least 85% of the requested on-demand nodes, cluster creation will
succeed. Otherwise the cluster will terminate with an informative error
message.
</p>
<p>Cannot specify both <code>autoscale</code> and <code>num_workers</code>, must choose one.
</p>
<p><a href="https://docs.databricks.com/dev-tools/api/latest/clusters.html#create">More Documentation</a>.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_delete'>Delete/Terminate a Cluster</h2><span id='topic+db_cluster_delete'></span>

<h3>Description</h3>

<p>Delete/Terminate a Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_delete(
  cluster_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_delete_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cluster must be in the <code>RUNNING</code> state.
</p>

<hr>
<h2 id='db_cluster_edit'>Edit a Cluster</h2><span id='topic+db_cluster_edit'></span>

<h3>Description</h3>

<p>Edit the configuration of a cluster to match the provided attributes and
size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_edit(
  cluster_id,
  spark_version,
  node_type_id,
  num_workers = NULL,
  autoscale = NULL,
  name = NULL,
  spark_conf = NULL,
  cloud_attrs = NULL,
  driver_node_type_id = NULL,
  custom_tags = NULL,
  init_scripts = NULL,
  spark_env_vars = NULL,
  autotermination_minutes = NULL,
  log_conf = NULL,
  ssh_public_keys = NULL,
  driver_instance_pool_id = NULL,
  instance_pool_id = NULL,
  idempotency_token = NULL,
  enable_elastic_disk = NULL,
  apply_policy_default_values = NULL,
  enable_local_disk_encryption = NULL,
  docker_image = NULL,
  policy_id = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_edit_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_spark_version">spark_version</code></td>
<td>
<p>The runtime version of the cluster. You can retrieve a
list of available runtime versions by using <code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_node_type_id">node_type_id</code></td>
<td>
<p>The node type for the worker nodes.
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types()</a></code> can be used to see available node types.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_num_workers">num_workers</code></td>
<td>
<p>Number of worker nodes that this cluster should have. A
cluster has one Spark driver and <code>num_workers</code> executors for a total of
<code>num_workers</code> + 1 Spark nodes.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_autoscale">autoscale</code></td>
<td>
<p>Instance of <code><a href="#topic+cluster_autoscale">cluster_autoscale()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_name">name</code></td>
<td>
<p>Cluster name requested by the user. This doesnâ€™t have to be
unique. If not specified at creation, the cluster name will be an empty
string.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_spark_conf">spark_conf</code></td>
<td>
<p>Named list. An object containing a set of optional,
user-specified Spark configuration key-value pairs. You can also pass in a
string of extra JVM options to the driver and the executors via
<code>spark.driver.extraJavaOptions</code> and <code>spark.executor.extraJavaOptions</code>
respectively. E.g. <code>list("spark.speculation" = true, "spark.streaming.ui.retainedBatches" = 5)</code>.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_cloud_attrs">cloud_attrs</code></td>
<td>
<p>Attributes related to clusters running on specific cloud
provider. Defaults to <code><a href="#topic+aws_attributes">aws_attributes()</a></code>. Must be one of <code><a href="#topic+aws_attributes">aws_attributes()</a></code>,
<code><a href="#topic+azure_attributes">azure_attributes()</a></code>, <code><a href="#topic+gcp_attributes">gcp_attributes()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_driver_node_type_id">driver_node_type_id</code></td>
<td>
<p>The node type of the Spark driver. This field is
optional; if unset, the driver node type will be set as the same value as
<code>node_type_id</code> defined above. <code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types()</a></code> can be used to
see available node types.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_custom_tags">custom_tags</code></td>
<td>
<p>Named list. An object containing a set of tags for cluster
resources. Databricks tags all cluster resources with these tags in addition
to <code>default_tags</code>. Databricks allows at most 45 custom tags.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_init_scripts">init_scripts</code></td>
<td>
<p>Instance of <code><a href="#topic+init_script_info">init_script_info()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_spark_env_vars">spark_env_vars</code></td>
<td>
<p>Named list. User-specified environment variable
key-value pairs. In order to specify an additional set of
<code>SPARK_DAEMON_JAVA_OPTS</code>, we recommend appending them to
<code style="white-space: pre;">&#8288;$SPARK_DAEMON_JAVA_OPTS&#8288;</code> as shown in the following example. This ensures
that all default Databricks managed environmental variables are included as
well. E.g. <code>{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}</code></p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_autotermination_minutes">autotermination_minutes</code></td>
<td>
<p>Automatically terminates the cluster after it
is inactive for this time in minutes. If not set, this cluster will not be
automatically terminated. If specified, the threshold must be between 10 and
10000 minutes. You can also set this value to 0 to explicitly disable
automatic termination. Defaults to 120.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_log_conf">log_conf</code></td>
<td>
<p>Instance of <code><a href="#topic+cluster_log_conf">cluster_log_conf()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_ssh_public_keys">ssh_public_keys</code></td>
<td>
<p>List. SSH public key contents that will be added to each
Spark node in this cluster. The corresponding private keys can be used to
login with the user name ubuntu on port 2200. Up to 10 keys can be specified.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_driver_instance_pool_id">driver_instance_pool_id</code></td>
<td>
<p>ID of the instance pool to use for the
driver node. You must also specify <code>instance_pool_id</code>. Optional.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_instance_pool_id">instance_pool_id</code></td>
<td>
<p>ID of the instance pool to use for cluster nodes. If
<code>driver_instance_pool_id</code> is present, <code>instance_pool_id</code> is used for worker
nodes only. Otherwise, it is used for both the driver and worker nodes.
Optional.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_idempotency_token">idempotency_token</code></td>
<td>
<p>An optional token that can be used to guarantee the
idempotency of cluster creation requests. If an active cluster with the
provided token already exists, the request will not create a new cluster,
but it will return the ID of the existing cluster instead. The existence of a
cluster with the same token is not checked against terminated clusters. If
you specify the idempotency token, upon failure you can retry until the
request succeeds. Databricks guarantees that exactly one cluster will be
launched with that idempotency token. This token should have at most 64
characters.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_enable_elastic_disk">enable_elastic_disk</code></td>
<td>
<p>When enabled, this cluster will dynamically
acquire additional disk space when its Spark workers are running low on
disk space.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_apply_policy_default_values">apply_policy_default_values</code></td>
<td>
<p>Boolean (Default: <code>TRUE</code>), whether to use
policy default values for missing cluster attributes.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_enable_local_disk_encryption">enable_local_disk_encryption</code></td>
<td>
<p>Boolean (Default: <code>TRUE</code>), whether
encryption of disks locally attached to the cluster is enabled.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_docker_image">docker_image</code></td>
<td>
<p>Instance of <code><a href="#topic+docker_image">docker_image()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_policy_id">policy_id</code></td>
<td>
<p>String, ID of a cluster policy.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_edit_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can edit a cluster if it is in a <code>RUNNING</code> or <code>TERMINATED</code> state. If you
edit a cluster while it is in a <code>RUNNING</code> state, it will be restarted so that
the new attributes can take effect. If you edit a cluster while it is in a
<code>TERMINATED</code> state, it will remain <code>TERMINATED.</code> The next time it is started
using the clusters/start API, the new attributes will take effect. An attempt
to edit a cluster in any other state will be rejected with an <code>INVALID_STATE</code>
error code.
</p>
<p>Clusters created by the Databricks Jobs service cannot be edited.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_events'>List Cluster Activity Events</h2><span id='topic+db_cluster_events'></span>

<h3>Description</h3>

<p>List Cluster Activity Events
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_events(
  cluster_id,
  start_time = NULL,
  end_time = NULL,
  event_types = NULL,
  order = c("DESC", "ASC"),
  offset = 0,
  limit = 50,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_events_+3A_cluster_id">cluster_id</code></td>
<td>
<p>The ID of the cluster to retrieve events about.</p>
</td></tr>
<tr><td><code id="db_cluster_events_+3A_start_time">start_time</code></td>
<td>
<p>The start time in epoch milliseconds. If empty, returns
events starting from the beginning of time.</p>
</td></tr>
<tr><td><code id="db_cluster_events_+3A_end_time">end_time</code></td>
<td>
<p>The end time in epoch milliseconds. If empty, returns events
up to the current time.</p>
</td></tr>
<tr><td><code id="db_cluster_events_+3A_event_types">event_types</code></td>
<td>
<p>List. Optional set of event types to filter by. Default
is to return all events. <a href="https://docs.databricks.com/dev-tools/api/latest/clusters.html#clustereventtype">Event Types</a>.</p>
</td></tr>
<tr><td><code id="db_cluster_events_+3A_order">order</code></td>
<td>
<p>Either <code>DESC</code> (default) or <code>ASC</code>.</p>
</td></tr>
<tr><td><code id="db_cluster_events_+3A_offset">offset</code></td>
<td>
<p>The offset in the result set. Defaults to 0 (no offset). When
an offset is specified and the results are requested in descending order, the
end_time field is required.</p>
</td></tr>
<tr><td><code id="db_cluster_events_+3A_limit">limit</code></td>
<td>
<p>Maximum number of events to include in a page of events.
Defaults to 50, and maximum allowed value is 500.</p>
</td></tr>
<tr><td><code id="db_cluster_events_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_events_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_events_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Retrieve a list of events about the activity of a cluster. You can retrieve
events from active clusters (running, pending, or reconfiguring) and
terminated clusters within 30 days of their last termination. This API is
paginated. If there are more events to read, the response includes all the
parameters necessary to request the next page of events.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_get'>Get Details of a Cluster</h2><span id='topic+db_cluster_get'></span>

<h3>Description</h3>

<p>Get Details of a Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_get(
  cluster_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_get_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_get_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_get_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_get_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Retrieve the information for a cluster given its identifier. Clusters can be
described while they are running or up to 30 days after they are terminated.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_list'>List Clusters</h2><span id='topic+db_cluster_list'></span>

<h3>Description</h3>

<p>List Clusters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_list(host = db_host(), token = db_token(), perform_request = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Return information about all pinned clusters, active clusters, up to 150 of
the most recently terminated all-purpose clusters in the past 30 days, and up
to 30 of the most recently terminated job clusters in the past 30 days.
</p>
<p>For example, if there is 1 pinned cluster, 4 active clusters, 45 terminated
all-purpose clusters in the past 30 days, and 50 terminated job clusters in
the past 30 days, then this API returns:
</p>

<ul>
<li><p> the 1 pinned cluster
</p>
</li>
<li><p> 4 active clusters
</p>
</li>
<li><p> All 45 terminated all-purpose clusters
</p>
</li>
<li><p> The 30 most recently terminated job clusters
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_list_node_types'>List Available Cluster Node Types</h2><span id='topic+db_cluster_list_node_types'></span>

<h3>Description</h3>

<p>List Available Cluster Node Types
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_list_node_types(
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_list_node_types_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_list_node_types_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_list_node_types_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Return a list of supported Spark node types. These node types can be used to
launch a cluster.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_list_zones'>List Availability Zones (AWS Only)</h2><span id='topic+db_cluster_list_zones'></span>

<h3>Description</h3>

<p>List Availability Zones (AWS Only)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_list_zones(
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_list_zones_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_list_zones_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_list_zones_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Amazon Web Services (AWS) ONLY!</strong>
Return a list of availability zones where clusters can be created in
(ex: us-west-2a). These zones can be used to launch a cluster.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_perm_delete'>Permanently Delete a Cluster</h2><span id='topic+db_cluster_perm_delete'></span>

<h3>Description</h3>

<p>Permanently Delete a Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_perm_delete(
  cluster_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_perm_delete_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_perm_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_perm_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_perm_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the cluster is running, it is terminated and its resources are
asynchronously removed. If the cluster is terminated, then it is immediately
removed.
</p>
<p>You cannot perform *any action, including retrieve the clusterâ€™s permissions,
on a permanently deleted cluster. A permanently deleted cluster is also no
longer returned in the cluster list.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_pin'>Pin a Cluster</h2><span id='topic+db_cluster_pin'></span>

<h3>Description</h3>

<p>Pin a Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_pin(
  cluster_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_pin_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_pin_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_pin_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_pin_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ensure that an all-purpose cluster configuration is retained even after a
cluster has been terminated for more than 30 days. Pinning ensures that the
cluster is always returned by <code><a href="#topic+db_cluster_list">db_cluster_list()</a></code>. Pinning a cluster that is
already pinned has no effect.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_resize'>Resize a Cluster</h2><span id='topic+db_cluster_resize'></span>

<h3>Description</h3>

<p>Resize a Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_resize(
  cluster_id,
  num_workers = NULL,
  autoscale = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_resize_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_resize_+3A_num_workers">num_workers</code></td>
<td>
<p>Number of worker nodes that this cluster should have. A
cluster has one Spark driver and <code>num_workers</code> executors for a total of
<code>num_workers</code> + 1 Spark nodes.</p>
</td></tr>
<tr><td><code id="db_cluster_resize_+3A_autoscale">autoscale</code></td>
<td>
<p>Instance of <code><a href="#topic+cluster_autoscale">cluster_autoscale()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_resize_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_resize_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_resize_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cluster must be in the <code>RUNNING</code> state.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_restart'>Restart a Cluster</h2><span id='topic+db_cluster_restart'></span>

<h3>Description</h3>

<p>Restart a Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_restart(
  cluster_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_restart_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_restart_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_restart_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_restart_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cluster must be in the <code>RUNNING</code> state.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_runtime_versions'>List Available Databricks Runtime Versions</h2><span id='topic+db_cluster_runtime_versions'></span>

<h3>Description</h3>

<p>List Available Databricks Runtime Versions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_runtime_versions(
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_runtime_versions_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_runtime_versions_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_runtime_versions_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Return the list of available runtime versions. These versions can be used to
launch a cluster.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_start'>Start a Cluster</h2><span id='topic+db_cluster_start'></span>

<h3>Description</h3>

<p>Start a Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_start(
  cluster_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_start_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_start_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_start_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_start_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Start a terminated cluster given its ID.
</p>
<p>This is similar to <code><a href="#topic+db_cluster_create">db_cluster_create()</a></code>, except:
</p>

<ul>
<li><p> The terminated cluster ID and attributes are preserved.
</p>
</li>
<li><p> The cluster starts with the last specified cluster size. If the terminated
cluster is an autoscaling cluster, the cluster starts with the minimum number
of nodes.
</p>
</li>
<li><p> If the cluster is in the <code>RESTARTING</code> state, a <code>400</code> error is returned.
</p>
</li>
<li><p> You cannot start a cluster launched to run a job.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_terminate'>Delete/Terminate a Cluster</h2><span id='topic+db_cluster_terminate'></span>

<h3>Description</h3>

<p>Delete/Terminate a Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_terminate(
  cluster_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_terminate_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_terminate_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_terminate_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_terminate_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cluster is removed asynchronously. Once the termination has completed,
the cluster will be in the <code>TERMINATED</code> state. If the cluster is already in a
<code>TERMINATING</code> or <code>TERMINATED</code> state, nothing will happen.
</p>
<p>Unless a cluster is pinned, 30 days after the cluster is terminated, it is
permanently deleted.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_cluster_unpin'>Unpin a Cluster</h2><span id='topic+db_cluster_unpin'></span>

<h3>Description</h3>

<p>Unpin a Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_cluster_unpin(
  cluster_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_cluster_unpin_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="db_cluster_unpin_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_unpin_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_cluster_unpin_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Allows the cluster to eventually be removed from the list returned by
<code><a href="#topic+db_cluster_list">db_cluster_list()</a></code>. Unpinning a cluster that is not pinned has no effect.
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='db_context_command_cancel'>Cancel a Command</h2><span id='topic+db_context_command_cancel'></span>

<h3>Description</h3>

<p>Cancel a Command
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_context_command_cancel(
  cluster_id,
  context_id,
  command_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_context_command_cancel_+3A_cluster_id">cluster_id</code></td>
<td>
<p>The ID of the cluster to create the context for.</p>
</td></tr>
<tr><td><code id="db_context_command_cancel_+3A_context_id">context_id</code></td>
<td>
<p>The ID of the execution context.</p>
</td></tr>
<tr><td><code id="db_context_command_cancel_+3A_command_id">command_id</code></td>
<td>
<p>The ID of the command to get information about.</p>
</td></tr>
<tr><td><code id="db_context_command_cancel_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_command_cancel_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_command_cancel_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Execution Context API: 
<code><a href="#topic+db_context_command_parse">db_context_command_parse</a>()</code>,
<code><a href="#topic+db_context_command_run">db_context_command_run</a>()</code>,
<code><a href="#topic+db_context_command_run_and_wait">db_context_command_run_and_wait</a>()</code>,
<code><a href="#topic+db_context_command_status">db_context_command_status</a>()</code>,
<code><a href="#topic+db_context_create">db_context_create</a>()</code>,
<code><a href="#topic+db_context_destroy">db_context_destroy</a>()</code>,
<code><a href="#topic+db_context_status">db_context_status</a>()</code>
</p>

<hr>
<h2 id='db_context_command_parse'>Parse Command Results</h2><span id='topic+db_context_command_parse'></span>

<h3>Description</h3>

<p>Parse Command Results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_context_command_parse(x, language = c("r", "py", "scala", "sql"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_context_command_parse_+3A_x">x</code></td>
<td>
<p>command output from <code>db_context_command_status</code> or
<code>db_context_manager</code>'s <code>cmd_run</code></p>
</td></tr>
<tr><td><code id="db_context_command_parse_+3A_language">language</code></td>
<td>
</td></tr>
</table>


<h3>Value</h3>

<p>command results
</p>


<h3>See Also</h3>

<p>Other Execution Context API: 
<code><a href="#topic+db_context_command_cancel">db_context_command_cancel</a>()</code>,
<code><a href="#topic+db_context_command_run">db_context_command_run</a>()</code>,
<code><a href="#topic+db_context_command_run_and_wait">db_context_command_run_and_wait</a>()</code>,
<code><a href="#topic+db_context_command_status">db_context_command_status</a>()</code>,
<code><a href="#topic+db_context_create">db_context_create</a>()</code>,
<code><a href="#topic+db_context_destroy">db_context_destroy</a>()</code>,
<code><a href="#topic+db_context_status">db_context_status</a>()</code>
</p>

<hr>
<h2 id='db_context_command_run'>Run a Command</h2><span id='topic+db_context_command_run'></span>

<h3>Description</h3>

<p>Run a Command
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_context_command_run(
  cluster_id,
  context_id,
  language = c("python", "sql", "scala", "r"),
  command = NULL,
  command_file = NULL,
  options = list(),
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_context_command_run_+3A_cluster_id">cluster_id</code></td>
<td>
<p>The ID of the cluster to create the context for.</p>
</td></tr>
<tr><td><code id="db_context_command_run_+3A_context_id">context_id</code></td>
<td>
<p>The ID of the execution context.</p>
</td></tr>
<tr><td><code id="db_context_command_run_+3A_language">language</code></td>
<td>
<p>The language for the context. One of <code>python</code>, <code>sql</code>, <code>scala</code>,
<code>r</code>.</p>
</td></tr>
<tr><td><code id="db_context_command_run_+3A_command">command</code></td>
<td>
<p>The command string to run.</p>
</td></tr>
<tr><td><code id="db_context_command_run_+3A_command_file">command_file</code></td>
<td>
<p>The path to a file containing the command to run.</p>
</td></tr>
<tr><td><code id="db_context_command_run_+3A_options">options</code></td>
<td>
<p>Named list of values used downstream. For example, a
'displayRowLimit' override (used in testing).</p>
</td></tr>
<tr><td><code id="db_context_command_run_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_command_run_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_command_run_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Execution Context API: 
<code><a href="#topic+db_context_command_cancel">db_context_command_cancel</a>()</code>,
<code><a href="#topic+db_context_command_parse">db_context_command_parse</a>()</code>,
<code><a href="#topic+db_context_command_run_and_wait">db_context_command_run_and_wait</a>()</code>,
<code><a href="#topic+db_context_command_status">db_context_command_status</a>()</code>,
<code><a href="#topic+db_context_create">db_context_create</a>()</code>,
<code><a href="#topic+db_context_destroy">db_context_destroy</a>()</code>,
<code><a href="#topic+db_context_status">db_context_status</a>()</code>
</p>

<hr>
<h2 id='db_context_command_run_and_wait'>Run a Command and Wait For Results</h2><span id='topic+db_context_command_run_and_wait'></span>

<h3>Description</h3>

<p>Run a Command and Wait For Results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_context_command_run_and_wait(
  cluster_id,
  context_id,
  language = c("python", "sql", "scala", "r"),
  command = NULL,
  command_file = NULL,
  options = list(),
  parse_result = TRUE,
  host = db_host(),
  token = db_token()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_context_command_run_and_wait_+3A_cluster_id">cluster_id</code></td>
<td>
<p>The ID of the cluster to create the context for.</p>
</td></tr>
<tr><td><code id="db_context_command_run_and_wait_+3A_context_id">context_id</code></td>
<td>
<p>The ID of the execution context.</p>
</td></tr>
<tr><td><code id="db_context_command_run_and_wait_+3A_language">language</code></td>
<td>
<p>The language for the context. One of <code>python</code>, <code>sql</code>, <code>scala</code>,
<code>r</code>.</p>
</td></tr>
<tr><td><code id="db_context_command_run_and_wait_+3A_command">command</code></td>
<td>
<p>The command string to run.</p>
</td></tr>
<tr><td><code id="db_context_command_run_and_wait_+3A_command_file">command_file</code></td>
<td>
<p>The path to a file containing the command to run.</p>
</td></tr>
<tr><td><code id="db_context_command_run_and_wait_+3A_options">options</code></td>
<td>
<p>Named list of values used downstream. For example, a
'displayRowLimit' override (used in testing).</p>
</td></tr>
<tr><td><code id="db_context_command_run_and_wait_+3A_parse_result">parse_result</code></td>
<td>
<p>Boolean, determines if results are parsed automatically.</p>
</td></tr>
<tr><td><code id="db_context_command_run_and_wait_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_command_run_and_wait_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Execution Context API: 
<code><a href="#topic+db_context_command_cancel">db_context_command_cancel</a>()</code>,
<code><a href="#topic+db_context_command_parse">db_context_command_parse</a>()</code>,
<code><a href="#topic+db_context_command_run">db_context_command_run</a>()</code>,
<code><a href="#topic+db_context_command_status">db_context_command_status</a>()</code>,
<code><a href="#topic+db_context_create">db_context_create</a>()</code>,
<code><a href="#topic+db_context_destroy">db_context_destroy</a>()</code>,
<code><a href="#topic+db_context_status">db_context_status</a>()</code>
</p>

<hr>
<h2 id='db_context_command_status'>Get Information About a Command</h2><span id='topic+db_context_command_status'></span>

<h3>Description</h3>

<p>Get Information About a Command
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_context_command_status(
  cluster_id,
  context_id,
  command_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_context_command_status_+3A_cluster_id">cluster_id</code></td>
<td>
<p>The ID of the cluster to create the context for.</p>
</td></tr>
<tr><td><code id="db_context_command_status_+3A_context_id">context_id</code></td>
<td>
<p>The ID of the execution context.</p>
</td></tr>
<tr><td><code id="db_context_command_status_+3A_command_id">command_id</code></td>
<td>
<p>The ID of the command to get information about.</p>
</td></tr>
<tr><td><code id="db_context_command_status_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_command_status_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_command_status_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Execution Context API: 
<code><a href="#topic+db_context_command_cancel">db_context_command_cancel</a>()</code>,
<code><a href="#topic+db_context_command_parse">db_context_command_parse</a>()</code>,
<code><a href="#topic+db_context_command_run">db_context_command_run</a>()</code>,
<code><a href="#topic+db_context_command_run_and_wait">db_context_command_run_and_wait</a>()</code>,
<code><a href="#topic+db_context_create">db_context_create</a>()</code>,
<code><a href="#topic+db_context_destroy">db_context_destroy</a>()</code>,
<code><a href="#topic+db_context_status">db_context_status</a>()</code>
</p>

<hr>
<h2 id='db_context_create'>Create an Execution Context</h2><span id='topic+db_context_create'></span>

<h3>Description</h3>

<p>Create an Execution Context
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_context_create(
  cluster_id,
  language = c("python", "sql", "scala", "r"),
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_context_create_+3A_cluster_id">cluster_id</code></td>
<td>
<p>The ID of the cluster to create the context for.</p>
</td></tr>
<tr><td><code id="db_context_create_+3A_language">language</code></td>
<td>
<p>The language for the context. One of <code>python</code>, <code>sql</code>, <code>scala</code>,
<code>r</code>.</p>
</td></tr>
<tr><td><code id="db_context_create_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_create_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_create_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Execution Context API: 
<code><a href="#topic+db_context_command_cancel">db_context_command_cancel</a>()</code>,
<code><a href="#topic+db_context_command_parse">db_context_command_parse</a>()</code>,
<code><a href="#topic+db_context_command_run">db_context_command_run</a>()</code>,
<code><a href="#topic+db_context_command_run_and_wait">db_context_command_run_and_wait</a>()</code>,
<code><a href="#topic+db_context_command_status">db_context_command_status</a>()</code>,
<code><a href="#topic+db_context_destroy">db_context_destroy</a>()</code>,
<code><a href="#topic+db_context_status">db_context_status</a>()</code>
</p>

<hr>
<h2 id='db_context_destroy'>Delete an Execution Context</h2><span id='topic+db_context_destroy'></span>

<h3>Description</h3>

<p>Delete an Execution Context
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_context_destroy(
  cluster_id,
  context_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_context_destroy_+3A_cluster_id">cluster_id</code></td>
<td>
<p>The ID of the cluster to create the context for.</p>
</td></tr>
<tr><td><code id="db_context_destroy_+3A_context_id">context_id</code></td>
<td>
<p>The ID of the execution context.</p>
</td></tr>
<tr><td><code id="db_context_destroy_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_destroy_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_destroy_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Execution Context API: 
<code><a href="#topic+db_context_command_cancel">db_context_command_cancel</a>()</code>,
<code><a href="#topic+db_context_command_parse">db_context_command_parse</a>()</code>,
<code><a href="#topic+db_context_command_run">db_context_command_run</a>()</code>,
<code><a href="#topic+db_context_command_run_and_wait">db_context_command_run_and_wait</a>()</code>,
<code><a href="#topic+db_context_command_status">db_context_command_status</a>()</code>,
<code><a href="#topic+db_context_create">db_context_create</a>()</code>,
<code><a href="#topic+db_context_status">db_context_status</a>()</code>
</p>

<hr>
<h2 id='db_context_manager'>Databricks Execution Context Manager (R6 Class)</h2><span id='topic+db_context_manager'></span>

<h3>Description</h3>

<p>Databricks Execution Context Manager (R6 Class)
</p>
<p>Databricks Execution Context Manager (R6 Class)
</p>


<h3>Details</h3>

<p><code>db_context_manager()</code> provides a simple interface to send commands to
Databricks cluster and return the results.
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-databricks_context_manager-new"><code>db_context_manager$new()</code></a>
</p>
</li>
<li> <p><a href="#method-databricks_context_manager-close"><code>db_context_manager$close()</code></a>
</p>
</li>
<li> <p><a href="#method-databricks_context_manager-cmd_run"><code>db_context_manager$cmd_run()</code></a>
</p>
</li>
<li> <p><a href="#method-databricks_context_manager-clone"><code>db_context_manager$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-databricks_context_manager-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new context manager object.
</p>


<h5>Usage</h5>

<div class="r"><pre>db_context_manager$new(
  cluster_id,
  language = c("r", "py", "scala", "sql", "sh"),
  host = db_host(),
  token = db_token()
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>cluster_id</code></dt><dd><p>The ID of the cluster to execute command on.</p>
</dd>
<dt><code>language</code></dt><dd><p>One of <code>r</code>, <code>py</code>, <code>scala</code>, <code>sql</code>, or <code>sh</code>.</p>
</dd>
<dt><code>host</code></dt><dd><p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</dd>
<dt><code>token</code></dt><dd><p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A new <code>databricks_context_manager</code> object.
</p>


<hr>
<a id="method-databricks_context_manager-close"></a>



<h4>Method <code>close()</code></h4>

<p>Destroy the execution context
</p>


<h5>Usage</h5>

<div class="r"><pre>db_context_manager$close()</pre></div>


<hr>
<a id="method-databricks_context_manager-cmd_run"></a>



<h4>Method <code>cmd_run()</code></h4>

<p>Execute a command against a Databricks cluster
</p>


<h5>Usage</h5>

<div class="r"><pre>db_context_manager$cmd_run(cmd, language = c("r", "py", "scala", "sql", "sh"))</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>cmd</code></dt><dd><p>code to execute against Databricks cluster</p>
</dd>
<dt><code>language</code></dt><dd><p>One of <code>r</code>, <code>py</code>, <code>scala</code>, <code>sql</code>, or <code>sh</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Command results
</p>


<hr>
<a id="method-databricks_context_manager-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>db_context_manager$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='db_context_status'>Get Information About an Execution Context</h2><span id='topic+db_context_status'></span>

<h3>Description</h3>

<p>Get Information About an Execution Context
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_context_status(
  cluster_id,
  context_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_context_status_+3A_cluster_id">cluster_id</code></td>
<td>
<p>The ID of the cluster to create the context for.</p>
</td></tr>
<tr><td><code id="db_context_status_+3A_context_id">context_id</code></td>
<td>
<p>The ID of the execution context.</p>
</td></tr>
<tr><td><code id="db_context_status_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_status_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_context_status_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Execution Context API: 
<code><a href="#topic+db_context_command_cancel">db_context_command_cancel</a>()</code>,
<code><a href="#topic+db_context_command_parse">db_context_command_parse</a>()</code>,
<code><a href="#topic+db_context_command_run">db_context_command_run</a>()</code>,
<code><a href="#topic+db_context_command_run_and_wait">db_context_command_run_and_wait</a>()</code>,
<code><a href="#topic+db_context_command_status">db_context_command_status</a>()</code>,
<code><a href="#topic+db_context_create">db_context_create</a>()</code>,
<code><a href="#topic+db_context_destroy">db_context_destroy</a>()</code>
</p>

<hr>
<h2 id='db_current_cloud'>Detect Current Workspaces Cloud</h2><span id='topic+db_current_cloud'></span>

<h3>Description</h3>

<p>Detect Current Workspaces Cloud
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_current_cloud(host = db_host(), token = db_token(), perform_request = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_current_cloud_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_current_cloud_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_current_cloud_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>String
</p>

<hr>
<h2 id='db_current_user'>Get Current User Info</h2><span id='topic+db_current_user'></span>

<h3>Description</h3>

<p>Get Current User Info
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_current_user(host = db_host(), token = db_token(), perform_request = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_current_user_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_current_user_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_current_user_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of user metadata
</p>

<hr>
<h2 id='db_current_workspace_id'>Detect Current Workspace ID</h2><span id='topic+db_current_workspace_id'></span>

<h3>Description</h3>

<p>Detect Current Workspace ID
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_current_workspace_id(
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_current_workspace_id_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_current_workspace_id_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_current_workspace_id_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>String
</p>

<hr>
<h2 id='db_dbfs_add_block'>DBFS Add Block</h2><span id='topic+db_dbfs_add_block'></span>

<h3>Description</h3>

<p>Append a block of data to the stream specified by the input handle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_dbfs_add_block(
  handle,
  data,
  convert_to_raw = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_dbfs_add_block_+3A_handle">handle</code></td>
<td>
<p>Handle on an open stream.</p>
</td></tr>
<tr><td><code id="db_dbfs_add_block_+3A_data">data</code></td>
<td>
<p>Either a path for file on local system or a character/raw
vector that will be base64-encoded. This has a limit of 1 MB.</p>
</td></tr>
<tr><td><code id="db_dbfs_add_block_+3A_convert_to_raw">convert_to_raw</code></td>
<td>
<p>Boolean (Default: <code>FALSE</code>), if <code>TRUE</code> will convert
character vector to raw via <code><a href="base.html#topic+raw">base::as.raw()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_add_block_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_add_block_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_add_block_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> If the handle does not exist, this call will throw an exception with
<code>RESOURCE_DOES_NOT_EXIST.</code>
</p>
</li>
<li><p> If the block of data exceeds 1 MB, this call will throw an exception with
<code>MAX_BLOCK_SIZE_EXCEEDED.</code>
</p>
</li></ul>



<h3>Typical File Upload Flow</h3>


<ul>
<li><p> Call create and get a handle via <code><a href="#topic+db_dbfs_create">db_dbfs_create()</a></code>
</p>
</li>
<li><p> Make one or more <code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block()</a></code> calls with the handle you have
</p>
</li>
<li><p> Call <code><a href="#topic+db_dbfs_close">db_dbfs_close()</a></code> with the handle you have
</p>
</li></ul>



<h3>See Also</h3>

<p>Other DBFS API: 
<code><a href="#topic+db_dbfs_close">db_dbfs_close</a>()</code>,
<code><a href="#topic+db_dbfs_create">db_dbfs_create</a>()</code>,
<code><a href="#topic+db_dbfs_delete">db_dbfs_delete</a>()</code>,
<code><a href="#topic+db_dbfs_get_status">db_dbfs_get_status</a>()</code>,
<code><a href="#topic+db_dbfs_list">db_dbfs_list</a>()</code>,
<code><a href="#topic+db_dbfs_mkdirs">db_dbfs_mkdirs</a>()</code>,
<code><a href="#topic+db_dbfs_move">db_dbfs_move</a>()</code>,
<code><a href="#topic+db_dbfs_put">db_dbfs_put</a>()</code>,
<code><a href="#topic+db_dbfs_read">db_dbfs_read</a>()</code>
</p>

<hr>
<h2 id='db_dbfs_close'>DBFS Close</h2><span id='topic+db_dbfs_close'></span>

<h3>Description</h3>

<p>Close the stream specified by the input handle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_dbfs_close(
  handle,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_dbfs_close_+3A_handle">handle</code></td>
<td>
<p>The handle on an open stream. This field is required.</p>
</td></tr>
<tr><td><code id="db_dbfs_close_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_close_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_close_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the handle does not exist, this call throws an exception with
<code>RESOURCE_DOES_NOT_EXIST.</code>
</p>


<h3>Value</h3>

<p>HTTP Response
</p>


<h3>Typical File Upload Flow</h3>


<ul>
<li><p> Call create and get a handle via <code><a href="#topic+db_dbfs_create">db_dbfs_create()</a></code>
</p>
</li>
<li><p> Make one or more <code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block()</a></code> calls with the handle you have
</p>
</li>
<li><p> Call <code><a href="#topic+db_dbfs_close">db_dbfs_close()</a></code> with the handle you have
</p>
</li></ul>



<h3>See Also</h3>

<p>Other DBFS API: 
<code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block</a>()</code>,
<code><a href="#topic+db_dbfs_create">db_dbfs_create</a>()</code>,
<code><a href="#topic+db_dbfs_delete">db_dbfs_delete</a>()</code>,
<code><a href="#topic+db_dbfs_get_status">db_dbfs_get_status</a>()</code>,
<code><a href="#topic+db_dbfs_list">db_dbfs_list</a>()</code>,
<code><a href="#topic+db_dbfs_mkdirs">db_dbfs_mkdirs</a>()</code>,
<code><a href="#topic+db_dbfs_move">db_dbfs_move</a>()</code>,
<code><a href="#topic+db_dbfs_put">db_dbfs_put</a>()</code>,
<code><a href="#topic+db_dbfs_read">db_dbfs_read</a>()</code>
</p>

<hr>
<h2 id='db_dbfs_create'>DBFS Create</h2><span id='topic+db_dbfs_create'></span>

<h3>Description</h3>

<p>Open a stream to write to a file and returns a handle to this stream.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_dbfs_create(
  path,
  overwrite = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_dbfs_create_+3A_path">path</code></td>
<td>
<p>The path of the new file. The path should be the absolute DBFS
path (for example <code style="white-space: pre;">&#8288;/mnt/my-file.txt&#8288;</code>).</p>
</td></tr>
<tr><td><code id="db_dbfs_create_+3A_overwrite">overwrite</code></td>
<td>
<p>Boolean, specifies whether to overwrite existing file or
files.</p>
</td></tr>
<tr><td><code id="db_dbfs_create_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_create_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_create_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is a 10 minute idle timeout on this handle. If a file or directory
already exists on the given path and overwrite is set to <code>FALSE</code>, this call
throws an exception with <code>RESOURCE_ALREADY_EXISTS.</code>
</p>


<h3>Value</h3>

<p>Handle which should subsequently be passed into <code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block()</a></code>
and <code><a href="#topic+db_dbfs_close">db_dbfs_close()</a></code> when writing to a file through a stream.
</p>


<h3>Typical File Upload Flow</h3>


<ul>
<li><p> Call create and get a handle via <code><a href="#topic+db_dbfs_create">db_dbfs_create()</a></code>
</p>
</li>
<li><p> Make one or more <code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block()</a></code> calls with the handle you have
</p>
</li>
<li><p> Call <code><a href="#topic+db_dbfs_close">db_dbfs_close()</a></code> with the handle you have
</p>
</li></ul>



<h3>See Also</h3>

<p>Other DBFS API: 
<code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block</a>()</code>,
<code><a href="#topic+db_dbfs_close">db_dbfs_close</a>()</code>,
<code><a href="#topic+db_dbfs_delete">db_dbfs_delete</a>()</code>,
<code><a href="#topic+db_dbfs_get_status">db_dbfs_get_status</a>()</code>,
<code><a href="#topic+db_dbfs_list">db_dbfs_list</a>()</code>,
<code><a href="#topic+db_dbfs_mkdirs">db_dbfs_mkdirs</a>()</code>,
<code><a href="#topic+db_dbfs_move">db_dbfs_move</a>()</code>,
<code><a href="#topic+db_dbfs_put">db_dbfs_put</a>()</code>,
<code><a href="#topic+db_dbfs_read">db_dbfs_read</a>()</code>
</p>

<hr>
<h2 id='db_dbfs_delete'>DBFS Delete</h2><span id='topic+db_dbfs_delete'></span>

<h3>Description</h3>

<p>DBFS Delete
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_dbfs_delete(
  path,
  recursive = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_dbfs_delete_+3A_path">path</code></td>
<td>
<p>The path of the new file. The path should be the absolute DBFS
path (for example <code style="white-space: pre;">&#8288;/mnt/my-file.txt&#8288;</code>).</p>
</td></tr>
<tr><td><code id="db_dbfs_delete_+3A_recursive">recursive</code></td>
<td>
<p>Whether or not to recursively delete the directoryâ€™s
contents. Deleting empty directories can be done without providing the recursive flag.</p>
</td></tr>
<tr><td><code id="db_dbfs_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other DBFS API: 
<code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block</a>()</code>,
<code><a href="#topic+db_dbfs_close">db_dbfs_close</a>()</code>,
<code><a href="#topic+db_dbfs_create">db_dbfs_create</a>()</code>,
<code><a href="#topic+db_dbfs_get_status">db_dbfs_get_status</a>()</code>,
<code><a href="#topic+db_dbfs_list">db_dbfs_list</a>()</code>,
<code><a href="#topic+db_dbfs_mkdirs">db_dbfs_mkdirs</a>()</code>,
<code><a href="#topic+db_dbfs_move">db_dbfs_move</a>()</code>,
<code><a href="#topic+db_dbfs_put">db_dbfs_put</a>()</code>,
<code><a href="#topic+db_dbfs_read">db_dbfs_read</a>()</code>
</p>

<hr>
<h2 id='db_dbfs_get_status'>DBFS Get Status</h2><span id='topic+db_dbfs_get_status'></span>

<h3>Description</h3>

<p>Get the file information of a file or directory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_dbfs_get_status(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_dbfs_get_status_+3A_path">path</code></td>
<td>
<p>The path of the new file. The path should be the absolute DBFS
path (for example <code style="white-space: pre;">&#8288;/mnt/my-file.txt&#8288;</code>).</p>
</td></tr>
<tr><td><code id="db_dbfs_get_status_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_get_status_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_get_status_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> If the file or directory does not exist, this call throws an exception with
<code>RESOURCE_DOES_NOT_EXIST.</code>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other DBFS API: 
<code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block</a>()</code>,
<code><a href="#topic+db_dbfs_close">db_dbfs_close</a>()</code>,
<code><a href="#topic+db_dbfs_create">db_dbfs_create</a>()</code>,
<code><a href="#topic+db_dbfs_delete">db_dbfs_delete</a>()</code>,
<code><a href="#topic+db_dbfs_list">db_dbfs_list</a>()</code>,
<code><a href="#topic+db_dbfs_mkdirs">db_dbfs_mkdirs</a>()</code>,
<code><a href="#topic+db_dbfs_move">db_dbfs_move</a>()</code>,
<code><a href="#topic+db_dbfs_put">db_dbfs_put</a>()</code>,
<code><a href="#topic+db_dbfs_read">db_dbfs_read</a>()</code>
</p>

<hr>
<h2 id='db_dbfs_list'>DBFS List</h2><span id='topic+db_dbfs_list'></span>

<h3>Description</h3>

<p>List the contents of a directory, or details of the file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_dbfs_list(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_dbfs_list_+3A_path">path</code></td>
<td>
<p>The path of the new file. The path should be the absolute DBFS
path (for example <code style="white-space: pre;">&#8288;/mnt/my-file.txt&#8288;</code>).</p>
</td></tr>
<tr><td><code id="db_dbfs_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When calling list on a large directory, the list operation will time out
after approximately 60 seconds.
</p>
<p>We <strong>strongly</strong> recommend using list only on
directories containing less than 10K files and discourage using the DBFS REST
API for operations that list more than 10K files. Instead, we recommend that
you perform such operations in the context of a cluster, using the File
system utility (<code>dbutils.fs</code>), which provides the same functionality without
timing out.
</p>

<ul>
<li><p> If the file or directory does not exist, this call throws an exception with
<code>RESOURCE_DOES_NOT_EXIST.</code>
</p>
</li></ul>



<h3>Value</h3>

<p>data.frame
</p>


<h3>See Also</h3>

<p>Other DBFS API: 
<code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block</a>()</code>,
<code><a href="#topic+db_dbfs_close">db_dbfs_close</a>()</code>,
<code><a href="#topic+db_dbfs_create">db_dbfs_create</a>()</code>,
<code><a href="#topic+db_dbfs_delete">db_dbfs_delete</a>()</code>,
<code><a href="#topic+db_dbfs_get_status">db_dbfs_get_status</a>()</code>,
<code><a href="#topic+db_dbfs_mkdirs">db_dbfs_mkdirs</a>()</code>,
<code><a href="#topic+db_dbfs_move">db_dbfs_move</a>()</code>,
<code><a href="#topic+db_dbfs_put">db_dbfs_put</a>()</code>,
<code><a href="#topic+db_dbfs_read">db_dbfs_read</a>()</code>
</p>

<hr>
<h2 id='db_dbfs_mkdirs'>DBFS mkdirs</h2><span id='topic+db_dbfs_mkdirs'></span>

<h3>Description</h3>

<p>Create the given directory and necessary parent directories if they do not
exist.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_dbfs_mkdirs(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_dbfs_mkdirs_+3A_path">path</code></td>
<td>
<p>The path of the new file. The path should be the absolute DBFS
path (for example <code style="white-space: pre;">&#8288;/mnt/my-file.txt&#8288;</code>).</p>
</td></tr>
<tr><td><code id="db_dbfs_mkdirs_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_mkdirs_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_mkdirs_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> If there exists a file (not a directory) at any prefix of the input path,
this call throws an exception with <code>RESOURCE_ALREADY_EXISTS.</code>
</p>
</li>
<li><p> If this operation fails it may have succeeded in creating some of the
necessary parent directories.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other DBFS API: 
<code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block</a>()</code>,
<code><a href="#topic+db_dbfs_close">db_dbfs_close</a>()</code>,
<code><a href="#topic+db_dbfs_create">db_dbfs_create</a>()</code>,
<code><a href="#topic+db_dbfs_delete">db_dbfs_delete</a>()</code>,
<code><a href="#topic+db_dbfs_get_status">db_dbfs_get_status</a>()</code>,
<code><a href="#topic+db_dbfs_list">db_dbfs_list</a>()</code>,
<code><a href="#topic+db_dbfs_move">db_dbfs_move</a>()</code>,
<code><a href="#topic+db_dbfs_put">db_dbfs_put</a>()</code>,
<code><a href="#topic+db_dbfs_read">db_dbfs_read</a>()</code>
</p>

<hr>
<h2 id='db_dbfs_move'>DBFS Move</h2><span id='topic+db_dbfs_move'></span>

<h3>Description</h3>

<p>Move a file from one location to another location within DBFS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_dbfs_move(
  source_path,
  destination_path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_dbfs_move_+3A_source_path">source_path</code></td>
<td>
<p>The source path of the file or directory. The path
should be the absolute DBFS path (for example, <code style="white-space: pre;">&#8288;/mnt/my-source-folder/&#8288;</code>).</p>
</td></tr>
<tr><td><code id="db_dbfs_move_+3A_destination_path">destination_path</code></td>
<td>
<p>The destination path of the file or directory. The
path should be the absolute DBFS path (for example,
<code style="white-space: pre;">&#8288;/mnt/my-destination-folder/&#8288;</code>).</p>
</td></tr>
<tr><td><code id="db_dbfs_move_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_move_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_move_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the given source path is a directory, this call always recursively moves
all files.
</p>
<p>When moving a large number of files, the API call will time out after
approximately 60 seconds, potentially resulting in partially moved data.
Therefore, for operations that move more than 10K files, we <strong>strongly</strong>
discourage using the DBFS REST API. Instead, we recommend that you perform
such operations in the context of a cluster, using the File system utility
(<code>dbutils.fs</code>) from a notebook, which provides the same functionality without
timing out.
</p>

<ul>
<li><p> If the source file does not exist, this call throws an exception with
<code>RESOURCE_DOES_NOT_EXIST.</code>
</p>
</li>
<li><p> If there already exists a file in the destination path, this call throws an
exception with <code>RESOURCE_ALREADY_EXISTS.</code>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other DBFS API: 
<code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block</a>()</code>,
<code><a href="#topic+db_dbfs_close">db_dbfs_close</a>()</code>,
<code><a href="#topic+db_dbfs_create">db_dbfs_create</a>()</code>,
<code><a href="#topic+db_dbfs_delete">db_dbfs_delete</a>()</code>,
<code><a href="#topic+db_dbfs_get_status">db_dbfs_get_status</a>()</code>,
<code><a href="#topic+db_dbfs_list">db_dbfs_list</a>()</code>,
<code><a href="#topic+db_dbfs_mkdirs">db_dbfs_mkdirs</a>()</code>,
<code><a href="#topic+db_dbfs_put">db_dbfs_put</a>()</code>,
<code><a href="#topic+db_dbfs_read">db_dbfs_read</a>()</code>
</p>

<hr>
<h2 id='db_dbfs_put'>DBFS Put</h2><span id='topic+db_dbfs_put'></span>

<h3>Description</h3>

<p>Upload a file through the use of multipart form post.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_dbfs_put(
  path,
  file = NULL,
  contents = NULL,
  overwrite = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_dbfs_put_+3A_path">path</code></td>
<td>
<p>The path of the new file. The path should be the absolute DBFS
path (for example <code style="white-space: pre;">&#8288;/mnt/my-file.txt&#8288;</code>).</p>
</td></tr>
<tr><td><code id="db_dbfs_put_+3A_file">file</code></td>
<td>
<p>Path to a file on local system, takes precedent over <code>path</code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_put_+3A_contents">contents</code></td>
<td>
<p>String that is base64 encoded.</p>
</td></tr>
<tr><td><code id="db_dbfs_put_+3A_overwrite">overwrite</code></td>
<td>
<p>Flag (Default: <code>FALSE</code>) that specifies whether to overwrite
existing files.</p>
</td></tr>
<tr><td><code id="db_dbfs_put_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_put_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_put_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Either <code>contents</code> or <code>file</code> must be specified. <code>file</code> takes precedent over
<code>contents</code> if both are specified.
</p>
<p>Mainly used for streaming uploads, but can also be used as a convenient
single call for data upload.
</p>
<p>The amount of data that can be passed using the contents parameter is limited
to 1 MB if specified as a string (<code>MAX_BLOCK_SIZE_EXCEEDED</code> is thrown if
exceeded) and 2 GB as a file.
</p>


<h3>See Also</h3>

<p>Other DBFS API: 
<code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block</a>()</code>,
<code><a href="#topic+db_dbfs_close">db_dbfs_close</a>()</code>,
<code><a href="#topic+db_dbfs_create">db_dbfs_create</a>()</code>,
<code><a href="#topic+db_dbfs_delete">db_dbfs_delete</a>()</code>,
<code><a href="#topic+db_dbfs_get_status">db_dbfs_get_status</a>()</code>,
<code><a href="#topic+db_dbfs_list">db_dbfs_list</a>()</code>,
<code><a href="#topic+db_dbfs_mkdirs">db_dbfs_mkdirs</a>()</code>,
<code><a href="#topic+db_dbfs_move">db_dbfs_move</a>()</code>,
<code><a href="#topic+db_dbfs_read">db_dbfs_read</a>()</code>
</p>

<hr>
<h2 id='db_dbfs_read'>DBFS Read</h2><span id='topic+db_dbfs_read'></span>

<h3>Description</h3>

<p>Return the contents of a file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_dbfs_read(
  path,
  offset = 0,
  length = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_dbfs_read_+3A_path">path</code></td>
<td>
<p>The path of the new file. The path should be the absolute DBFS
path (for example <code style="white-space: pre;">&#8288;/mnt/my-file.txt&#8288;</code>).</p>
</td></tr>
<tr><td><code id="db_dbfs_read_+3A_offset">offset</code></td>
<td>
<p>Offset to read from in bytes.</p>
</td></tr>
<tr><td><code id="db_dbfs_read_+3A_length">length</code></td>
<td>
<p>Number of bytes to read starting from the offset. This has a
limit of 1 MB, and a default value of 0.5 MB.</p>
</td></tr>
<tr><td><code id="db_dbfs_read_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_read_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_dbfs_read_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If offset + length exceeds the number of bytes in a file, reads contents
until the end of file.
</p>

<ul>
<li><p> If the file does not exist, this call throws an exception with
<code>RESOURCE_DOES_NOT_EXIST.</code>
</p>
</li>
<li><p> If the path is a directory, the read length is negative, or if the offset
is negative, this call throws an exception with <code>INVALID_PARAMETER_VALUE.</code>
</p>
</li>
<li><p> If the read length exceeds 1 MB, this call throws an exception with
<code>MAX_READ_SIZE_EXCEEDED.</code>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other DBFS API: 
<code><a href="#topic+db_dbfs_add_block">db_dbfs_add_block</a>()</code>,
<code><a href="#topic+db_dbfs_close">db_dbfs_close</a>()</code>,
<code><a href="#topic+db_dbfs_create">db_dbfs_create</a>()</code>,
<code><a href="#topic+db_dbfs_delete">db_dbfs_delete</a>()</code>,
<code><a href="#topic+db_dbfs_get_status">db_dbfs_get_status</a>()</code>,
<code><a href="#topic+db_dbfs_list">db_dbfs_list</a>()</code>,
<code><a href="#topic+db_dbfs_mkdirs">db_dbfs_mkdirs</a>()</code>,
<code><a href="#topic+db_dbfs_move">db_dbfs_move</a>()</code>,
<code><a href="#topic+db_dbfs_put">db_dbfs_put</a>()</code>
</p>

<hr>
<h2 id='db_host'>Generate/Fetch Databricks Host</h2><span id='topic+db_host'></span>

<h3>Description</h3>

<p>If both <code>id</code> and <code>prefix</code> are <code>NULL</code> then the function will check for
the <code>DATABRICKS_HOST</code> environment variable.
<code>.databrickscfg</code> will be searched if <code>db_profile</code> and <code>use_databrickscfg</code> are set or if
Posit Workbench managed OAuth credentials are detected.
</p>
<p>When defining <code>id</code> and <code>prefix</code> you do not need to specify the whole URL.
E.g. <code style="white-space: pre;">&#8288;https://&lt;prefix&gt;.&lt;id&gt;.cloud.databricks.com/&#8288;</code> is the form to follow.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_host(id = NULL, prefix = NULL, profile = default_config_profile())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_host_+3A_id">id</code></td>
<td>
<p>The workspace string</p>
</td></tr>
<tr><td><code id="db_host_+3A_prefix">prefix</code></td>
<td>
<p>Workspace prefix</p>
</td></tr>
<tr><td><code id="db_host_+3A_profile">profile</code></td>
<td>
<p>Profile to use when fetching from environment variable
(e.g. <code>.Renviron</code>) or <code>.databricksfg</code> file</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The behaviour is subject to change depending if <code>db_profile</code> and
<code>use_databrickscfg</code> options are set.
</p>

<ul>
<li> <p><code>use_databrickscfg</code>: Boolean (default: <code>FALSE</code>), determines if credentials
are fetched from profile of <code>.databrickscfg</code> or <code>.Renviron</code>
</p>
</li>
<li> <p><code>db_profile</code>: String (default: <code>NULL</code>), determines profile used.
<code>.databrickscfg</code> will automatically be used when Posit Workbench managed OAuth credentials are detected.
</p>
</li></ul>

<p>See vignette on authentication for more details.
</p>


<h3>Value</h3>

<p>workspace URL
</p>


<h3>See Also</h3>

<p>Other Databricks Authentication Helpers: 
<code><a href="#topic+db_read_netrc">db_read_netrc</a>()</code>,
<code><a href="#topic+db_token">db_token</a>()</code>,
<code><a href="#topic+db_wsid">db_wsid</a>()</code>
</p>

<hr>
<h2 id='db_jobs_create'>Create Job</h2><span id='topic+db_jobs_create'></span>

<h3>Description</h3>

<p>Create Job
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_create(
  name,
  tasks,
  schedule = NULL,
  job_clusters = NULL,
  email_notifications = NULL,
  timeout_seconds = NULL,
  max_concurrent_runs = 1,
  access_control_list = NULL,
  git_source = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_create_+3A_name">name</code></td>
<td>
<p>Name for the job.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_tasks">tasks</code></td>
<td>
<p>Task specifications to be executed by this job. Use
<code><a href="#topic+job_tasks">job_tasks()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_schedule">schedule</code></td>
<td>
<p>Instance of <code><a href="#topic+cron_schedule">cron_schedule()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_job_clusters">job_clusters</code></td>
<td>
<p>Named list of job cluster specifications (using
<code><a href="#topic+new_cluster">new_cluster()</a></code>) that can be shared and reused by tasks of this job.
Libraries cannot be declared in a shared job cluster. You must declare
dependent libraries in task settings.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_email_notifications">email_notifications</code></td>
<td>
<p>Instance of <code><a href="#topic+email_notifications">email_notifications()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_timeout_seconds">timeout_seconds</code></td>
<td>
<p>An optional timeout applied to each run of this job.
The default behavior is to have no timeout.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_max_concurrent_runs">max_concurrent_runs</code></td>
<td>
<p>Maximum allowed number of concurrent runs of the
job. Set this value if you want to be able to execute multiple runs of the
same job concurrently. This setting affects only new runs. This value cannot
exceed 1000. Setting this value to 0 causes all new runs to be skipped.
The default behavior is to allow only 1 concurrent run.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_access_control_list">access_control_list</code></td>
<td>
<p>Instance of <code><a href="#topic+access_control_request">access_control_request()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_git_source">git_source</code></td>
<td>
<p>Optional specification for a remote repository containing
the notebooks used by this job's notebook tasks. Instance of <code><a href="#topic+git_source">git_source()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_create_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><a href="https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsCreate">Full Documentation</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+job_tasks">job_tasks()</a></code>, <code><a href="#topic+job_task">job_task()</a></code>, <code><a href="#topic+email_notifications">email_notifications()</a></code>,
<code><a href="#topic+cron_schedule">cron_schedule()</a></code>, <code><a href="#topic+access_control_request">access_control_request()</a></code>, <code><a href="#topic+access_control_req_user">access_control_req_user()</a></code>,
<code><a href="#topic+access_control_req_group">access_control_req_group()</a></code>, <code><a href="#topic+git_source">git_source()</a></code>
</p>
<p>Other Jobs API: 
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_delete'>Delete a Job</h2><span id='topic+db_jobs_delete'></span>

<h3>Description</h3>

<p>Delete a Job
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_delete(
  job_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_delete_+3A_job_id">job_id</code></td>
<td>
<p>The canonical identifier of the job.</p>
</td></tr>
<tr><td><code id="db_jobs_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_get'>Get Job Details</h2><span id='topic+db_jobs_get'></span>

<h3>Description</h3>

<p>Get Job Details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_get(
  job_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_get_+3A_job_id">job_id</code></td>
<td>
<p>The canonical identifier of the job.</p>
</td></tr>
<tr><td><code id="db_jobs_get_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_get_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_get_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_list'>List Jobs</h2><span id='topic+db_jobs_list'></span>

<h3>Description</h3>

<p>List Jobs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_list(
  limit = 25,
  offset = 0,
  expand_tasks = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_list_+3A_limit">limit</code></td>
<td>
<p>Number of jobs to return. This value must be greater than 0 and
less or equal to 25. The default value is 25. If a request specifies a limit
of 0, the service instead uses the maximum limit.</p>
</td></tr>
<tr><td><code id="db_jobs_list_+3A_offset">offset</code></td>
<td>
<p>The offset of the first job to return, relative to the most
recently created job.</p>
</td></tr>
<tr><td><code id="db_jobs_list_+3A_expand_tasks">expand_tasks</code></td>
<td>
<p>Whether to include task and cluster details in the
response.</p>
</td></tr>
<tr><td><code id="db_jobs_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_reset'>Overwrite All Settings For A Job</h2><span id='topic+db_jobs_reset'></span>

<h3>Description</h3>

<p>Overwrite All Settings For A Job
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_reset(
  job_id,
  name,
  schedule,
  tasks,
  job_clusters = NULL,
  email_notifications = NULL,
  timeout_seconds = NULL,
  max_concurrent_runs = 1,
  access_control_list = NULL,
  git_source = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_reset_+3A_job_id">job_id</code></td>
<td>
<p>The canonical identifier of the job.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_name">name</code></td>
<td>
<p>Name for the job.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_schedule">schedule</code></td>
<td>
<p>Instance of <code><a href="#topic+cron_schedule">cron_schedule()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_tasks">tasks</code></td>
<td>
<p>Task specifications to be executed by this job. Use
<code><a href="#topic+job_tasks">job_tasks()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_job_clusters">job_clusters</code></td>
<td>
<p>Named list of job cluster specifications (using
<code><a href="#topic+new_cluster">new_cluster()</a></code>) that can be shared and reused by tasks of this job.
Libraries cannot be declared in a shared job cluster. You must declare
dependent libraries in task settings.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_email_notifications">email_notifications</code></td>
<td>
<p>Instance of <code><a href="#topic+email_notifications">email_notifications()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_timeout_seconds">timeout_seconds</code></td>
<td>
<p>An optional timeout applied to each run of this job.
The default behavior is to have no timeout.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_max_concurrent_runs">max_concurrent_runs</code></td>
<td>
<p>Maximum allowed number of concurrent runs of the
job. Set this value if you want to be able to execute multiple runs of the
same job concurrently. This setting affects only new runs. This value cannot
exceed 1000. Setting this value to 0 causes all new runs to be skipped.
The default behavior is to allow only 1 concurrent run.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_access_control_list">access_control_list</code></td>
<td>
<p>Instance of <code><a href="#topic+access_control_request">access_control_request()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_git_source">git_source</code></td>
<td>
<p>Optional specification for a remote repository containing
the notebooks used by this job's notebook tasks. Instance of <code><a href="#topic+git_source">git_source()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_reset_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_run_now'>Trigger A New Job Run</h2><span id='topic+db_jobs_run_now'></span>

<h3>Description</h3>

<p>Trigger A New Job Run
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_run_now(
  job_id,
  jar_params = list(),
  notebook_params = list(),
  python_params = list(),
  spark_submit_params = list(),
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_run_now_+3A_job_id">job_id</code></td>
<td>
<p>The canonical identifier of the job.</p>
</td></tr>
<tr><td><code id="db_jobs_run_now_+3A_jar_params">jar_params</code></td>
<td>
<p>Named list. Parameters are used to invoke the main
function of the main class specified in the Spark JAR task. If not specified
upon run-now, it defaults to an empty list. <code>jar_params</code> cannot be specified
in conjunction with <code>notebook_params</code>.</p>
</td></tr>
<tr><td><code id="db_jobs_run_now_+3A_notebook_params">notebook_params</code></td>
<td>
<p>Named list. Parameters is passed to the notebook
and is accessible through the <code>dbutils.widgets.get</code> function. If not specified
upon run-now, the triggered run uses the jobâ€™s base parameters.</p>
</td></tr>
<tr><td><code id="db_jobs_run_now_+3A_python_params">python_params</code></td>
<td>
<p>Named list. Parameters are passed to Python file as
command-line parameters. If specified upon run-now, it would overwrite the
parameters specified in job setting.</p>
</td></tr>
<tr><td><code id="db_jobs_run_now_+3A_spark_submit_params">spark_submit_params</code></td>
<td>
<p>Named list. Parameters are passed to spark-submit
script as command-line parameters. If specified upon run-now, it would
overwrite the parameters specified in job setting.</p>
</td></tr>
<tr><td><code id="db_jobs_run_now_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_run_now_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_run_now_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;*_params&#8288;</code> parameters cannot exceed 10,000 bytes when serialized to JSON.
</p>
</li>
<li> <p><code>jar_params</code> and <code>notebook_params</code> are mutually exclusive.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_runs_cancel'>Cancel Job Run</h2><span id='topic+db_jobs_runs_cancel'></span>

<h3>Description</h3>

<p>Cancels a run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_runs_cancel(
  run_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_runs_cancel_+3A_run_id">run_id</code></td>
<td>
<p>The canonical identifier of the run.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_cancel_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_cancel_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_cancel_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The run is canceled asynchronously, so when this request completes, the run
may still be running. The run are terminated shortly. If the run is already
in a terminal <code>life_cycle_state</code>, this method is a no-op.
</p>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_runs_delete'>Delete Job Run</h2><span id='topic+db_jobs_runs_delete'></span>

<h3>Description</h3>

<p>Delete Job Run
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_runs_delete(
  run_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_runs_delete_+3A_run_id">run_id</code></td>
<td>
<p>The canonical identifier of the run.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_runs_export'>Export Job Run Output</h2><span id='topic+db_jobs_runs_export'></span>

<h3>Description</h3>

<p>Export and retrieve the job run task.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_runs_export(
  run_id,
  views_to_export = c("CODE", "DASHBOARDS", "ALL"),
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_runs_export_+3A_run_id">run_id</code></td>
<td>
<p>The canonical identifier of the run.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_export_+3A_views_to_export">views_to_export</code></td>
<td>
<p>Which views to export. One of <code>CODE</code>, <code>DASHBOARDS</code>,
<code>ALL</code>. Defaults to <code>CODE</code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_export_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_export_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_export_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_runs_get'>Get Job Run Details</h2><span id='topic+db_jobs_runs_get'></span>

<h3>Description</h3>

<p>Retrieve the metadata of a run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_runs_get(
  run_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_runs_get_+3A_run_id">run_id</code></td>
<td>
<p>The canonical identifier of the run.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_get_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_get_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_get_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_runs_get_output'>Get Job Run Output</h2><span id='topic+db_jobs_runs_get_output'></span>

<h3>Description</h3>

<p>Get Job Run Output
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_runs_get_output(
  run_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_runs_get_output_+3A_run_id">run_id</code></td>
<td>
<p>The canonical identifier of the run.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_get_output_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_get_output_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_get_output_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_runs_list'>List Job Runs</h2><span id='topic+db_jobs_runs_list'></span>

<h3>Description</h3>

<p>List runs in descending order by start time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_runs_list(
  job_id,
  active_only = FALSE,
  completed_only = FALSE,
  offset = 0,
  limit = 25,
  run_type = c("JOB_RUN", "WORKFLOW_RUN", "SUBMIT_RUN"),
  expand_tasks = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_runs_list_+3A_job_id">job_id</code></td>
<td>
<p>The canonical identifier of the job.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_list_+3A_active_only">active_only</code></td>
<td>
<p>Boolean (Default: <code>FALSE</code>). If <code>TRUE</code> only active runs are
included in the results; otherwise, lists both active and completed runs.
An active run is a run in the <code>PENDING</code>, <code>RUNNING</code>, or <code>TERMINATING</code>. This
field cannot be true when <code>completed_only</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_list_+3A_completed_only">completed_only</code></td>
<td>
<p>Boolean (Default: <code>FALSE</code>). If <code>TRUE</code>, only completed
runs are included in the results; otherwise, lists both active and completed
runs. This field cannot be true when <code>active_only</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_list_+3A_offset">offset</code></td>
<td>
<p>The offset of the first job to return, relative to the most
recently created job.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_list_+3A_limit">limit</code></td>
<td>
<p>Number of jobs to return. This value must be greater than 0 and
less or equal to 25. The default value is 25. If a request specifies a limit
of 0, the service instead uses the maximum limit.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_list_+3A_run_type">run_type</code></td>
<td>
<p>The type of runs to return. One of <code>JOB_RUN</code>, <code>WORKFLOW_RUN</code>,
<code>SUBMIT_RUN</code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_list_+3A_expand_tasks">expand_tasks</code></td>
<td>
<p>Whether to include task and cluster details in the
response.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_runs_submit'>Create And Trigger A One-Time Run</h2><span id='topic+db_jobs_runs_submit'></span>

<h3>Description</h3>

<p>Create And Trigger A One-Time Run
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_runs_submit(
  tasks,
  run_name,
  timeout_seconds = NULL,
  idempotency_token = NULL,
  access_control_list = NULL,
  git_source = NULL,
  job_clusters = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_runs_submit_+3A_tasks">tasks</code></td>
<td>
<p>Task specifications to be executed by this job. Use
<code><a href="#topic+job_tasks">job_tasks()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_submit_+3A_run_name">run_name</code></td>
<td>
<p>Name for the run.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_submit_+3A_timeout_seconds">timeout_seconds</code></td>
<td>
<p>An optional timeout applied to each run of this job.
The default behavior is to have no timeout.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_submit_+3A_idempotency_token">idempotency_token</code></td>
<td>
<p>An optional token that can be used to guarantee the
idempotency of job run requests. If an active run with the provided token
already exists, the request does not create a new run, but returns the ID of
the existing run instead. If you specify the idempotency token, upon failure
you can retry until the request succeeds. Databricks guarantees that exactly
one run is launched with that idempotency token. This token must have at most
64 characters.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_submit_+3A_access_control_list">access_control_list</code></td>
<td>
<p>Instance of <code><a href="#topic+access_control_request">access_control_request()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_submit_+3A_git_source">git_source</code></td>
<td>
<p>Optional specification for a remote repository containing
the notebooks used by this job's notebook tasks. Instance of <code><a href="#topic+git_source">git_source()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_submit_+3A_job_clusters">job_clusters</code></td>
<td>
<p>Named list of job cluster specifications (using
<code><a href="#topic+new_cluster">new_cluster()</a></code>) that can be shared and reused by tasks of this job.
Libraries cannot be declared in a shared job cluster. You must declare
dependent libraries in task settings.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_submit_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_submit_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_runs_submit_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_update">db_jobs_update</a>()</code>
</p>

<hr>
<h2 id='db_jobs_update'>Partially Update A Job</h2><span id='topic+db_jobs_update'></span>

<h3>Description</h3>

<p>Partially Update A Job
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_jobs_update(
  job_id,
  fields_to_remove = list(),
  name = NULL,
  schedule = NULL,
  tasks = NULL,
  job_clusters = NULL,
  email_notifications = NULL,
  timeout_seconds = NULL,
  max_concurrent_runs = NULL,
  access_control_list = NULL,
  git_source = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_jobs_update_+3A_job_id">job_id</code></td>
<td>
<p>The canonical identifier of the job.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_fields_to_remove">fields_to_remove</code></td>
<td>
<p>Remove top-level fields in the job settings. Removing
nested fields is not supported. This field is optional. Must be a <code>list()</code>.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_name">name</code></td>
<td>
<p>Name for the job.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_schedule">schedule</code></td>
<td>
<p>Instance of <code><a href="#topic+cron_schedule">cron_schedule()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_tasks">tasks</code></td>
<td>
<p>Task specifications to be executed by this job. Use
<code><a href="#topic+job_tasks">job_tasks()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_job_clusters">job_clusters</code></td>
<td>
<p>Named list of job cluster specifications (using
<code><a href="#topic+new_cluster">new_cluster()</a></code>) that can be shared and reused by tasks of this job.
Libraries cannot be declared in a shared job cluster. You must declare
dependent libraries in task settings.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_email_notifications">email_notifications</code></td>
<td>
<p>Instance of <code><a href="#topic+email_notifications">email_notifications()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_timeout_seconds">timeout_seconds</code></td>
<td>
<p>An optional timeout applied to each run of this job.
The default behavior is to have no timeout.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_max_concurrent_runs">max_concurrent_runs</code></td>
<td>
<p>Maximum allowed number of concurrent runs of the
job. Set this value if you want to be able to execute multiple runs of the
same job concurrently. This setting affects only new runs. This value cannot
exceed 1000. Setting this value to 0 causes all new runs to be skipped.
The default behavior is to allow only 1 concurrent run.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_access_control_list">access_control_list</code></td>
<td>
<p>Instance of <code><a href="#topic+access_control_request">access_control_request()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_git_source">git_source</code></td>
<td>
<p>Optional specification for a remote repository containing
the notebooks used by this job's notebook tasks. Instance of <code><a href="#topic+git_source">git_source()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_jobs_update_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parameters which are shared with <code><a href="#topic+db_jobs_create">db_jobs_create()</a></code> are optional, only
specify those that are changing.
</p>


<h3>See Also</h3>

<p>Other Jobs API: 
<code><a href="#topic+db_jobs_create">db_jobs_create</a>()</code>,
<code><a href="#topic+db_jobs_delete">db_jobs_delete</a>()</code>,
<code><a href="#topic+db_jobs_get">db_jobs_get</a>()</code>,
<code><a href="#topic+db_jobs_list">db_jobs_list</a>()</code>,
<code><a href="#topic+db_jobs_reset">db_jobs_reset</a>()</code>,
<code><a href="#topic+db_jobs_run_now">db_jobs_run_now</a>()</code>,
<code><a href="#topic+db_jobs_runs_cancel">db_jobs_runs_cancel</a>()</code>,
<code><a href="#topic+db_jobs_runs_delete">db_jobs_runs_delete</a>()</code>,
<code><a href="#topic+db_jobs_runs_export">db_jobs_runs_export</a>()</code>,
<code><a href="#topic+db_jobs_runs_get">db_jobs_runs_get</a>()</code>,
<code><a href="#topic+db_jobs_runs_get_output">db_jobs_runs_get_output</a>()</code>,
<code><a href="#topic+db_jobs_runs_list">db_jobs_runs_list</a>()</code>,
<code><a href="#topic+db_jobs_runs_submit">db_jobs_runs_submit</a>()</code>
</p>

<hr>
<h2 id='db_libs_all_cluster_statuses'>Get Status of All Libraries on All Clusters</h2><span id='topic+db_libs_all_cluster_statuses'></span>

<h3>Description</h3>

<p>Get Status of All Libraries on All Clusters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_libs_all_cluster_statuses(
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_libs_all_cluster_statuses_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_libs_all_cluster_statuses_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_libs_all_cluster_statuses_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A status will be available for all libraries installed on clusters via the
API or the libraries UI as well as libraries set to be installed on all
clusters via the libraries UI.
</p>
<p>If a library has been set to be installed on all clusters,
<code>is_library_for_all_clusters</code> will be true, even if the library was
also installed on this specific cluster.
</p>


<h3>See Also</h3>

<p>Other Libraries API: 
<code><a href="#topic+db_libs_cluster_status">db_libs_cluster_status</a>()</code>,
<code><a href="#topic+db_libs_install">db_libs_install</a>()</code>,
<code><a href="#topic+db_libs_uninstall">db_libs_uninstall</a>()</code>
</p>

<hr>
<h2 id='db_libs_cluster_status'>Get Status of Libraries on Cluster</h2><span id='topic+db_libs_cluster_status'></span>

<h3>Description</h3>

<p>Get Status of Libraries on Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_libs_cluster_status(
  cluster_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_libs_cluster_status_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Unique identifier of a Databricks cluster.</p>
</td></tr>
<tr><td><code id="db_libs_cluster_status_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_libs_cluster_status_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_libs_cluster_status_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+wait_for_lib_installs">wait_for_lib_installs()</a></code>
</p>
<p>Other Libraries API: 
<code><a href="#topic+db_libs_all_cluster_statuses">db_libs_all_cluster_statuses</a>()</code>,
<code><a href="#topic+db_libs_install">db_libs_install</a>()</code>,
<code><a href="#topic+db_libs_uninstall">db_libs_uninstall</a>()</code>
</p>

<hr>
<h2 id='db_libs_install'>Install Library on Cluster</h2><span id='topic+db_libs_install'></span>

<h3>Description</h3>

<p>Install Library on Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_libs_install(
  cluster_id,
  libraries,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_libs_install_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Unique identifier of a Databricks cluster.</p>
</td></tr>
<tr><td><code id="db_libs_install_+3A_libraries">libraries</code></td>
<td>
<p>An object created by <code><a href="#topic+libraries">libraries()</a></code> and the appropriate
<code style="white-space: pre;">&#8288;lib_*()&#8288;</code> functions.</p>
</td></tr>
<tr><td><code id="db_libs_install_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_libs_install_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_libs_install_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Installation is asynchronous - it completes in the background after the request.
</p>
<p>This call will fail if the cluster is terminated. Installing a wheel library
on a cluster is like running the pip command against the wheel file directly
on driver and executors.
</p>
<p>Installing a wheel library on a cluster is like running the pip command
against the wheel file directly on driver and executors. All the
dependencies specified in the library setup.py file are installed and this
requires the library name to satisfy the wheel file name convention.
</p>
<p>The installation on the executors happens only when a new task is launched.
With Databricks Runtime 7.1 and below, the installation order of libraries
is nondeterministic. For wheel libraries, you can ensure a deterministic
installation order by creating a zip file with suffix .wheelhouse.zip that
includes all the wheel files.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lib_egg">lib_egg()</a></code>, <code><a href="#topic+lib_cran">lib_cran()</a></code>, <code><a href="#topic+lib_jar">lib_jar()</a></code>, <code><a href="#topic+lib_maven">lib_maven()</a></code>, <code><a href="#topic+lib_pypi">lib_pypi()</a></code>,
<code><a href="#topic+lib_whl">lib_whl()</a></code>
</p>
<p>Other Libraries API: 
<code><a href="#topic+db_libs_all_cluster_statuses">db_libs_all_cluster_statuses</a>()</code>,
<code><a href="#topic+db_libs_cluster_status">db_libs_cluster_status</a>()</code>,
<code><a href="#topic+db_libs_uninstall">db_libs_uninstall</a>()</code>
</p>

<hr>
<h2 id='db_libs_uninstall'>Uninstall Library on Cluster</h2><span id='topic+db_libs_uninstall'></span>

<h3>Description</h3>

<p>Uninstall Library on Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_libs_uninstall(
  cluster_id,
  libraries,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_libs_uninstall_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Unique identifier of a Databricks cluster.</p>
</td></tr>
<tr><td><code id="db_libs_uninstall_+3A_libraries">libraries</code></td>
<td>
<p>An object created by <code><a href="#topic+libraries">libraries()</a></code> and the appropriate
<code style="white-space: pre;">&#8288;lib_*()&#8288;</code> functions.</p>
</td></tr>
<tr><td><code id="db_libs_uninstall_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_libs_uninstall_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_libs_uninstall_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The libraries arenâ€™t uninstalled until the cluster is restarted.
</p>
<p>Uninstalling libraries that are not installed on the cluster has no impact
but is not an error.
</p>


<h3>See Also</h3>

<p>Other Libraries API: 
<code><a href="#topic+db_libs_all_cluster_statuses">db_libs_all_cluster_statuses</a>()</code>,
<code><a href="#topic+db_libs_cluster_status">db_libs_cluster_status</a>()</code>,
<code><a href="#topic+db_libs_install">db_libs_install</a>()</code>
</p>

<hr>
<h2 id='db_mlflow_model_approve_transition_req'>Approve Model Version Stage Transition Request</h2><span id='topic+db_mlflow_model_approve_transition_req'></span>

<h3>Description</h3>

<p>Approve Model Version Stage Transition Request
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_mlflow_model_approve_transition_req(
  name,
  version,
  stage = c("None", "Staging", "Production", "Archived"),
  archive_existing_versions = TRUE,
  comment = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_mlflow_model_approve_transition_req_+3A_name">name</code></td>
<td>
<p>Name of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_approve_transition_req_+3A_version">version</code></td>
<td>
<p>Version of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_approve_transition_req_+3A_stage">stage</code></td>
<td>
<p>Target stage of the transition. Valid values are: <code>None</code>,
<code>Staging</code>, <code>Production</code>, <code>Archived</code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_approve_transition_req_+3A_archive_existing_versions">archive_existing_versions</code></td>
<td>
<p>Boolean (Default: <code>TRUE</code>). Specifies whether
to archive all current model versions in the target stage.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_approve_transition_req_+3A_comment">comment</code></td>
<td>
<p>User-provided comment on the action.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_approve_transition_req_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_approve_transition_req_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_approve_transition_req_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Model Registry API: 
<code><a href="#topic+db_mlflow_model_delete_transition_req">db_mlflow_model_delete_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_open_transition_reqs">db_mlflow_model_open_transition_reqs</a>()</code>,
<code><a href="#topic+db_mlflow_model_reject_transition_req">db_mlflow_model_reject_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_req">db_mlflow_model_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_stage">db_mlflow_model_transition_stage</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment">db_mlflow_model_version_comment</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_delete">db_mlflow_model_version_comment_delete</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_edit">db_mlflow_model_version_comment_edit</a>()</code>,
<code><a href="#topic+db_mlflow_registered_model_details">db_mlflow_registered_model_details</a>()</code>
</p>

<hr>
<h2 id='db_mlflow_model_delete_transition_req'>Delete a Model Version Stage Transition Request</h2><span id='topic+db_mlflow_model_delete_transition_req'></span>

<h3>Description</h3>

<p>Delete a Model Version Stage Transition Request
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_mlflow_model_delete_transition_req(
  name,
  version,
  stage = c("None", "Staging", "Production", "Archived"),
  creator,
  comment = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_mlflow_model_delete_transition_req_+3A_name">name</code></td>
<td>
<p>Name of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_delete_transition_req_+3A_version">version</code></td>
<td>
<p>Version of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_delete_transition_req_+3A_stage">stage</code></td>
<td>
<p>Target stage of the transition. Valid values are: <code>None</code>,
<code>Staging</code>, <code>Production</code>, <code>Archived</code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_delete_transition_req_+3A_creator">creator</code></td>
<td>
<p>Username of the user who created this request. Of the
transition requests matching the specified details, only the one transition
created by this user will be deleted.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_delete_transition_req_+3A_comment">comment</code></td>
<td>
<p>User-provided comment on the action.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_delete_transition_req_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_delete_transition_req_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_delete_transition_req_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Model Registry API: 
<code><a href="#topic+db_mlflow_model_approve_transition_req">db_mlflow_model_approve_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_open_transition_reqs">db_mlflow_model_open_transition_reqs</a>()</code>,
<code><a href="#topic+db_mlflow_model_reject_transition_req">db_mlflow_model_reject_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_req">db_mlflow_model_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_stage">db_mlflow_model_transition_stage</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment">db_mlflow_model_version_comment</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_delete">db_mlflow_model_version_comment_delete</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_edit">db_mlflow_model_version_comment_edit</a>()</code>,
<code><a href="#topic+db_mlflow_registered_model_details">db_mlflow_registered_model_details</a>()</code>
</p>

<hr>
<h2 id='db_mlflow_model_open_transition_reqs'>Get All Open Stage Transition Requests for the Model Version</h2><span id='topic+db_mlflow_model_open_transition_reqs'></span>

<h3>Description</h3>

<p>Get All Open Stage Transition Requests for the Model Version
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_mlflow_model_open_transition_reqs(
  name,
  version,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_mlflow_model_open_transition_reqs_+3A_name">name</code></td>
<td>
<p>Name of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_open_transition_reqs_+3A_version">version</code></td>
<td>
<p>Version of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_open_transition_reqs_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_open_transition_reqs_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_open_transition_reqs_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Model Registry API: 
<code><a href="#topic+db_mlflow_model_approve_transition_req">db_mlflow_model_approve_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_delete_transition_req">db_mlflow_model_delete_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_reject_transition_req">db_mlflow_model_reject_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_req">db_mlflow_model_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_stage">db_mlflow_model_transition_stage</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment">db_mlflow_model_version_comment</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_delete">db_mlflow_model_version_comment_delete</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_edit">db_mlflow_model_version_comment_edit</a>()</code>,
<code><a href="#topic+db_mlflow_registered_model_details">db_mlflow_registered_model_details</a>()</code>
</p>

<hr>
<h2 id='db_mlflow_model_reject_transition_req'>Reject Model Version Stage Transition Request</h2><span id='topic+db_mlflow_model_reject_transition_req'></span>

<h3>Description</h3>

<p>Reject Model Version Stage Transition Request
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_mlflow_model_reject_transition_req(
  name,
  version,
  stage = c("None", "Staging", "Production", "Archived"),
  comment = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_mlflow_model_reject_transition_req_+3A_name">name</code></td>
<td>
<p>Name of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_reject_transition_req_+3A_version">version</code></td>
<td>
<p>Version of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_reject_transition_req_+3A_stage">stage</code></td>
<td>
<p>Target stage of the transition. Valid values are: <code>None</code>,
<code>Staging</code>, <code>Production</code>, <code>Archived</code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_reject_transition_req_+3A_comment">comment</code></td>
<td>
<p>User-provided comment on the action.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_reject_transition_req_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_reject_transition_req_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_reject_transition_req_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Model Registry API: 
<code><a href="#topic+db_mlflow_model_approve_transition_req">db_mlflow_model_approve_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_delete_transition_req">db_mlflow_model_delete_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_open_transition_reqs">db_mlflow_model_open_transition_reqs</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_req">db_mlflow_model_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_stage">db_mlflow_model_transition_stage</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment">db_mlflow_model_version_comment</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_delete">db_mlflow_model_version_comment_delete</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_edit">db_mlflow_model_version_comment_edit</a>()</code>,
<code><a href="#topic+db_mlflow_registered_model_details">db_mlflow_registered_model_details</a>()</code>
</p>

<hr>
<h2 id='db_mlflow_model_transition_req'>Make a Model Version Stage Transition Request</h2><span id='topic+db_mlflow_model_transition_req'></span>

<h3>Description</h3>

<p>Make a Model Version Stage Transition Request
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_mlflow_model_transition_req(
  name,
  version,
  stage = c("None", "Staging", "Production", "Archived"),
  comment = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_mlflow_model_transition_req_+3A_name">name</code></td>
<td>
<p>Name of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_req_+3A_version">version</code></td>
<td>
<p>Version of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_req_+3A_stage">stage</code></td>
<td>
<p>Target stage of the transition. Valid values are: <code>None</code>,
<code>Staging</code>, <code>Production</code>, <code>Archived</code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_req_+3A_comment">comment</code></td>
<td>
<p>User-provided comment on the action.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_req_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_req_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_req_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Model Registry API: 
<code><a href="#topic+db_mlflow_model_approve_transition_req">db_mlflow_model_approve_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_delete_transition_req">db_mlflow_model_delete_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_open_transition_reqs">db_mlflow_model_open_transition_reqs</a>()</code>,
<code><a href="#topic+db_mlflow_model_reject_transition_req">db_mlflow_model_reject_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_stage">db_mlflow_model_transition_stage</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment">db_mlflow_model_version_comment</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_delete">db_mlflow_model_version_comment_delete</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_edit">db_mlflow_model_version_comment_edit</a>()</code>,
<code><a href="#topic+db_mlflow_registered_model_details">db_mlflow_registered_model_details</a>()</code>
</p>

<hr>
<h2 id='db_mlflow_model_transition_stage'>Transition a Model Version's Stage</h2><span id='topic+db_mlflow_model_transition_stage'></span>

<h3>Description</h3>

<p>Transition a Model Version's Stage
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_mlflow_model_transition_stage(
  name,
  version,
  stage = c("None", "Staging", "Production", "Archived"),
  archive_existing_versions = TRUE,
  comment = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_mlflow_model_transition_stage_+3A_name">name</code></td>
<td>
<p>Name of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_stage_+3A_version">version</code></td>
<td>
<p>Version of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_stage_+3A_stage">stage</code></td>
<td>
<p>Target stage of the transition. Valid values are: <code>None</code>,
<code>Staging</code>, <code>Production</code>, <code>Archived</code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_stage_+3A_archive_existing_versions">archive_existing_versions</code></td>
<td>
<p>Boolean (Default: <code>TRUE</code>). Specifies whether
to archive all current model versions in the target stage.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_stage_+3A_comment">comment</code></td>
<td>
<p>User-provided comment on the action.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_stage_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_stage_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_transition_stage_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a Databricks version of the MLflow endpoint that also accepts a
comment associated with the transition to be recorded.
</p>


<h3>See Also</h3>

<p>Other Model Registry API: 
<code><a href="#topic+db_mlflow_model_approve_transition_req">db_mlflow_model_approve_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_delete_transition_req">db_mlflow_model_delete_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_open_transition_reqs">db_mlflow_model_open_transition_reqs</a>()</code>,
<code><a href="#topic+db_mlflow_model_reject_transition_req">db_mlflow_model_reject_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_req">db_mlflow_model_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment">db_mlflow_model_version_comment</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_delete">db_mlflow_model_version_comment_delete</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_edit">db_mlflow_model_version_comment_edit</a>()</code>,
<code><a href="#topic+db_mlflow_registered_model_details">db_mlflow_registered_model_details</a>()</code>
</p>

<hr>
<h2 id='db_mlflow_model_version_comment'>Make a Comment on a Model Version</h2><span id='topic+db_mlflow_model_version_comment'></span>

<h3>Description</h3>

<p>Make a Comment on a Model Version
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_mlflow_model_version_comment(
  name,
  version,
  comment,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_mlflow_model_version_comment_+3A_name">name</code></td>
<td>
<p>Name of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_+3A_version">version</code></td>
<td>
<p>Version of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_+3A_comment">comment</code></td>
<td>
<p>User-provided comment on the action.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Model Registry API: 
<code><a href="#topic+db_mlflow_model_approve_transition_req">db_mlflow_model_approve_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_delete_transition_req">db_mlflow_model_delete_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_open_transition_reqs">db_mlflow_model_open_transition_reqs</a>()</code>,
<code><a href="#topic+db_mlflow_model_reject_transition_req">db_mlflow_model_reject_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_req">db_mlflow_model_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_stage">db_mlflow_model_transition_stage</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_delete">db_mlflow_model_version_comment_delete</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_edit">db_mlflow_model_version_comment_edit</a>()</code>,
<code><a href="#topic+db_mlflow_registered_model_details">db_mlflow_registered_model_details</a>()</code>
</p>

<hr>
<h2 id='db_mlflow_model_version_comment_delete'>Delete a Comment on a Model Version</h2><span id='topic+db_mlflow_model_version_comment_delete'></span>

<h3>Description</h3>

<p>Delete a Comment on a Model Version
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_mlflow_model_version_comment_delete(
  id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_mlflow_model_version_comment_delete_+3A_id">id</code></td>
<td>
<p>Unique identifier of an activity.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Model Registry API: 
<code><a href="#topic+db_mlflow_model_approve_transition_req">db_mlflow_model_approve_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_delete_transition_req">db_mlflow_model_delete_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_open_transition_reqs">db_mlflow_model_open_transition_reqs</a>()</code>,
<code><a href="#topic+db_mlflow_model_reject_transition_req">db_mlflow_model_reject_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_req">db_mlflow_model_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_stage">db_mlflow_model_transition_stage</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment">db_mlflow_model_version_comment</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_edit">db_mlflow_model_version_comment_edit</a>()</code>,
<code><a href="#topic+db_mlflow_registered_model_details">db_mlflow_registered_model_details</a>()</code>
</p>

<hr>
<h2 id='db_mlflow_model_version_comment_edit'>Edit a Comment on a Model Version</h2><span id='topic+db_mlflow_model_version_comment_edit'></span>

<h3>Description</h3>

<p>Edit a Comment on a Model Version
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_mlflow_model_version_comment_edit(
  id,
  comment,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_mlflow_model_version_comment_edit_+3A_id">id</code></td>
<td>
<p>Unique identifier of an activity.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_edit_+3A_comment">comment</code></td>
<td>
<p>User-provided comment on the action.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_edit_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_edit_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_model_version_comment_edit_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Model Registry API: 
<code><a href="#topic+db_mlflow_model_approve_transition_req">db_mlflow_model_approve_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_delete_transition_req">db_mlflow_model_delete_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_open_transition_reqs">db_mlflow_model_open_transition_reqs</a>()</code>,
<code><a href="#topic+db_mlflow_model_reject_transition_req">db_mlflow_model_reject_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_req">db_mlflow_model_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_stage">db_mlflow_model_transition_stage</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment">db_mlflow_model_version_comment</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_delete">db_mlflow_model_version_comment_delete</a>()</code>,
<code><a href="#topic+db_mlflow_registered_model_details">db_mlflow_registered_model_details</a>()</code>
</p>

<hr>
<h2 id='db_mlflow_registered_model_details'>Get Registered Model Details</h2><span id='topic+db_mlflow_registered_model_details'></span>

<h3>Description</h3>

<p>Get Registered Model Details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_mlflow_registered_model_details(
  name,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_mlflow_registered_model_details_+3A_name">name</code></td>
<td>
<p>Name of the model.</p>
</td></tr>
<tr><td><code id="db_mlflow_registered_model_details_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_registered_model_details_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_mlflow_registered_model_details_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Model Registry API: 
<code><a href="#topic+db_mlflow_model_approve_transition_req">db_mlflow_model_approve_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_delete_transition_req">db_mlflow_model_delete_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_open_transition_reqs">db_mlflow_model_open_transition_reqs</a>()</code>,
<code><a href="#topic+db_mlflow_model_reject_transition_req">db_mlflow_model_reject_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_req">db_mlflow_model_transition_req</a>()</code>,
<code><a href="#topic+db_mlflow_model_transition_stage">db_mlflow_model_transition_stage</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment">db_mlflow_model_version_comment</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_delete">db_mlflow_model_version_comment_delete</a>()</code>,
<code><a href="#topic+db_mlflow_model_version_comment_edit">db_mlflow_model_version_comment_edit</a>()</code>
</p>

<hr>
<h2 id='db_oauth_client'>Create OAuth 2.0 Client</h2><span id='topic+db_oauth_client'></span>

<h3>Description</h3>

<p>Create OAuth 2.0 Client
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_oauth_client(host = db_host())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_oauth_client_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Creates an OAuth 2.0 Client, support for U2M flows only.
May later be extended for account U2M and all M2M flows.
</p>


<h3>Value</h3>

<p>List that contains httr2_oauth_client and relevant auth url
</p>

<hr>
<h2 id='db_perform_request'>Perform Databricks API Request</h2><span id='topic+db_perform_request'></span>

<h3>Description</h3>

<p>Perform Databricks API Request
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_perform_request(req, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_perform_request_+3A_req">req</code></td>
<td>
<p><code>{httr2}</code> request.</p>
</td></tr>
<tr><td><code id="db_perform_request_+3A_...">...</code></td>
<td>
<p>Parameters passed to <code><a href="httr2.html#topic+resp_body_raw">httr2::resp_body_json()</a></code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Request Helpers: 
<code><a href="#topic+db_req_error_body">db_req_error_body</a>()</code>,
<code><a href="#topic+db_request">db_request</a>()</code>,
<code><a href="#topic+db_request_json">db_request_json</a>()</code>
</p>

<hr>
<h2 id='db_read_netrc'>Read .netrc File</h2><span id='topic+db_read_netrc'></span>

<h3>Description</h3>

<p>Read .netrc File
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_read_netrc(path = "~/.netrc")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_read_netrc_+3A_path">path</code></td>
<td>
<p>path of <code>.netrc</code> file, default is <code style="white-space: pre;">&#8288;~/.netrc&#8288;</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>named list of <code>.netrc</code> entries
</p>


<h3>See Also</h3>

<p>Other Databricks Authentication Helpers: 
<code><a href="#topic+db_host">db_host</a>()</code>,
<code><a href="#topic+db_token">db_token</a>()</code>,
<code><a href="#topic+db_wsid">db_wsid</a>()</code>
</p>

<hr>
<h2 id='db_repl'>Remote REPL to Databricks Cluster</h2><span id='topic+db_repl'></span>

<h3>Description</h3>

<p>Remote REPL to Databricks Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_repl(
  cluster_id,
  language = c("r", "py", "scala", "sql", "sh"),
  host = db_host(),
  token = db_token()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_repl_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Cluster Id to create REPL context against.</p>
</td></tr>
<tr><td><code id="db_repl_+3A_language">language</code></td>
<td>
<p>for REPL ('r', 'py', 'scala', 'sql', 'sh') are
supported.</p>
</td></tr>
<tr><td><code id="db_repl_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repl_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>db_repl()</code> will take over the existing console and allow execution of
commands against a Databricks cluster. For RStudio users there are Addins
which can be bound to keyboard shortcuts to improve usability.
</p>

<hr>
<h2 id='db_repo_create'>Create Repo</h2><span id='topic+db_repo_create'></span>

<h3>Description</h3>

<p>Creates a repo in the workspace and links it to the remote Git repo specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_repo_create(
  url,
  provider,
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_repo_create_+3A_url">url</code></td>
<td>
<p>URL of the Git repository to be linked.</p>
</td></tr>
<tr><td><code id="db_repo_create_+3A_provider">provider</code></td>
<td>
<p>Git provider. This field is case-insensitive. The available
Git providers are <code>gitHub</code>, <code>bitbucketCloud</code>, <code>gitLab</code>, <code>azureDevOpsServices</code>,
<code>gitHubEnterprise</code>, <code>bitbucketServer</code> and <code>gitLabEnterpriseEdition.</code></p>
</td></tr>
<tr><td><code id="db_repo_create_+3A_path">path</code></td>
<td>
<p>Desired path for the repo in the workspace. Must be in the format
<code style="white-space: pre;">&#8288;/Repos/{folder}/{repo-name}&#8288;</code>.</p>
</td></tr>
<tr><td><code id="db_repo_create_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repo_create_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repo_create_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Repos API: 
<code><a href="#topic+db_repo_delete">db_repo_delete</a>()</code>,
<code><a href="#topic+db_repo_get">db_repo_get</a>()</code>,
<code><a href="#topic+db_repo_get_all">db_repo_get_all</a>()</code>,
<code><a href="#topic+db_repo_update">db_repo_update</a>()</code>
</p>

<hr>
<h2 id='db_repo_delete'>Delete Repo</h2><span id='topic+db_repo_delete'></span>

<h3>Description</h3>

<p>Deletes the specified repo
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_repo_delete(
  repo_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_repo_delete_+3A_repo_id">repo_id</code></td>
<td>
<p>The ID for the corresponding repo to access.</p>
</td></tr>
<tr><td><code id="db_repo_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repo_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repo_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Repos API: 
<code><a href="#topic+db_repo_create">db_repo_create</a>()</code>,
<code><a href="#topic+db_repo_get">db_repo_get</a>()</code>,
<code><a href="#topic+db_repo_get_all">db_repo_get_all</a>()</code>,
<code><a href="#topic+db_repo_update">db_repo_update</a>()</code>
</p>

<hr>
<h2 id='db_repo_get'>Get Repo</h2><span id='topic+db_repo_get'></span>

<h3>Description</h3>

<p>Returns the repo with the given repo ID.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_repo_get(
  repo_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_repo_get_+3A_repo_id">repo_id</code></td>
<td>
<p>The ID for the corresponding repo to access.</p>
</td></tr>
<tr><td><code id="db_repo_get_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repo_get_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repo_get_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Repos API: 
<code><a href="#topic+db_repo_create">db_repo_create</a>()</code>,
<code><a href="#topic+db_repo_delete">db_repo_delete</a>()</code>,
<code><a href="#topic+db_repo_get_all">db_repo_get_all</a>()</code>,
<code><a href="#topic+db_repo_update">db_repo_update</a>()</code>
</p>

<hr>
<h2 id='db_repo_get_all'>Get All Repos</h2><span id='topic+db_repo_get_all'></span>

<h3>Description</h3>

<p>Get All Repos
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_repo_get_all(
  path_prefix,
  next_page_token = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_repo_get_all_+3A_path_prefix">path_prefix</code></td>
<td>
<p>Filters repos that have paths starting with the given path
prefix.</p>
</td></tr>
<tr><td><code id="db_repo_get_all_+3A_next_page_token">next_page_token</code></td>
<td>
<p>Token used to get the next page of results. If not
specified, returns the first page of results as well as a next page token if
there are more results.</p>
</td></tr>
<tr><td><code id="db_repo_get_all_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repo_get_all_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repo_get_all_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns repos that the calling user has Manage permissions on.
Results are paginated with each page containing twenty repos.
</p>


<h3>See Also</h3>

<p>Other Repos API: 
<code><a href="#topic+db_repo_create">db_repo_create</a>()</code>,
<code><a href="#topic+db_repo_delete">db_repo_delete</a>()</code>,
<code><a href="#topic+db_repo_get">db_repo_get</a>()</code>,
<code><a href="#topic+db_repo_update">db_repo_update</a>()</code>
</p>

<hr>
<h2 id='db_repo_update'>Update Repo</h2><span id='topic+db_repo_update'></span>

<h3>Description</h3>

<p>Updates the repo to the given branch or tag.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_repo_update(
  repo_id,
  branch = NULL,
  tag = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_repo_update_+3A_repo_id">repo_id</code></td>
<td>
<p>The ID for the corresponding repo to access.</p>
</td></tr>
<tr><td><code id="db_repo_update_+3A_branch">branch</code></td>
<td>
<p>Branch that the local version of the repo is checked out to.</p>
</td></tr>
<tr><td><code id="db_repo_update_+3A_tag">tag</code></td>
<td>
<p>Tag that the local version of the repo is checked out to.</p>
</td></tr>
<tr><td><code id="db_repo_update_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repo_update_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_repo_update_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Specify either <code>branch</code> or <code>tag</code>, not both.
</p>
<p>Updating the repo to a tag puts the repo in a detached HEAD state.
Before committing new changes, you must update the repo to a branch instead
of the detached HEAD.
</p>


<h3>See Also</h3>

<p>Other Repos API: 
<code><a href="#topic+db_repo_create">db_repo_create</a>()</code>,
<code><a href="#topic+db_repo_delete">db_repo_delete</a>()</code>,
<code><a href="#topic+db_repo_get">db_repo_get</a>()</code>,
<code><a href="#topic+db_repo_get_all">db_repo_get_all</a>()</code>
</p>

<hr>
<h2 id='db_req_error_body'>Propagate Databricks API Errors</h2><span id='topic+db_req_error_body'></span>

<h3>Description</h3>

<p>Propagate Databricks API Errors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_req_error_body(resp)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_req_error_body_+3A_resp">resp</code></td>
<td>
<p>Object with class <code>httr2_response</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Request Helpers: 
<code><a href="#topic+db_perform_request">db_perform_request</a>()</code>,
<code><a href="#topic+db_request">db_request</a>()</code>,
<code><a href="#topic+db_request_json">db_request_json</a>()</code>
</p>

<hr>
<h2 id='db_request'>Databricks Request Helper</h2><span id='topic+db_request'></span>

<h3>Description</h3>

<p>Databricks Request Helper
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_request(endpoint, method, version = NULL, body = NULL, host, token, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_request_+3A_endpoint">endpoint</code></td>
<td>
<p>Databricks REST API Endpoint</p>
</td></tr>
<tr><td><code id="db_request_+3A_method">method</code></td>
<td>
<p>Passed to <code><a href="httr2.html#topic+req_method">httr2::req_method()</a></code></p>
</td></tr>
<tr><td><code id="db_request_+3A_version">version</code></td>
<td>
<p>String, API version of endpoint. E.g. <code>2.0</code>.</p>
</td></tr>
<tr><td><code id="db_request_+3A_body">body</code></td>
<td>
<p>Named list, passed to <code><a href="httr2.html#topic+req_body">httr2::req_body_json()</a></code>.</p>
</td></tr>
<tr><td><code id="db_request_+3A_host">host</code></td>
<td>
<p>Databricks host, defaults to <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_request_+3A_token">token</code></td>
<td>
<p>Databricks token, defaults to <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_request_+3A_...">...</code></td>
<td>
<p>Parameters passed on to <code><a href="httr2.html#topic+req_body">httr2::req_body_json()</a></code> when <code>body</code> is not <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>request
</p>


<h3>See Also</h3>

<p>Other Request Helpers: 
<code><a href="#topic+db_perform_request">db_perform_request</a>()</code>,
<code><a href="#topic+db_req_error_body">db_req_error_body</a>()</code>,
<code><a href="#topic+db_request_json">db_request_json</a>()</code>
</p>

<hr>
<h2 id='db_request_json'>Generate Request JSON</h2><span id='topic+db_request_json'></span>

<h3>Description</h3>

<p>Generate Request JSON
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_request_json(req)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_request_json_+3A_req">req</code></td>
<td>
<p>a httr2 request, ideally from <code><a href="#topic+db_request">db_request()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>JSON string
</p>


<h3>See Also</h3>

<p>Other Request Helpers: 
<code><a href="#topic+db_perform_request">db_perform_request</a>()</code>,
<code><a href="#topic+db_req_error_body">db_req_error_body</a>()</code>,
<code><a href="#topic+db_request">db_request</a>()</code>
</p>

<hr>
<h2 id='db_secrets_delete'>Delete Secret in Secret Scope</h2><span id='topic+db_secrets_delete'></span>

<h3>Description</h3>

<p>Delete Secret in Secret Scope
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_secrets_delete(
  scope,
  key,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_secrets_delete_+3A_scope">scope</code></td>
<td>
<p>Name of the scope that contains the secret to delete.</p>
</td></tr>
<tr><td><code id="db_secrets_delete_+3A_key">key</code></td>
<td>
<p>Name of the secret to delete.</p>
</td></tr>
<tr><td><code id="db_secrets_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You must have <code>WRITE</code> or <code>MANAGE</code> permission on the secret scope.
</p>

<ul>
<li><p> Throws <code>RESOURCE_DOES_NOT_EXIST</code> if no such secret scope or secret exists.
</p>
</li>
<li><p> Throws <code>PERMISSION_DENIED</code> if you do not have permission to make this API
call.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Secrets API: 
<code><a href="#topic+db_secrets_list">db_secrets_list</a>()</code>,
<code><a href="#topic+db_secrets_put">db_secrets_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_delete">db_secrets_scope_acl_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_get">db_secrets_scope_acl_get</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_list">db_secrets_scope_acl_list</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_put">db_secrets_scope_acl_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_create">db_secrets_scope_create</a>()</code>,
<code><a href="#topic+db_secrets_scope_delete">db_secrets_scope_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_list_all">db_secrets_scope_list_all</a>()</code>
</p>

<hr>
<h2 id='db_secrets_list'>List Secrets in Secret Scope</h2><span id='topic+db_secrets_list'></span>

<h3>Description</h3>

<p>List Secrets in Secret Scope
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_secrets_list(
  scope,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_secrets_list_+3A_scope">scope</code></td>
<td>
<p>Name of the scope whose secrets you want to list</p>
</td></tr>
<tr><td><code id="db_secrets_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a metadata-only operation; you cannot retrieve secret data using this
API. You must have <code>READ</code> permission to make this call.
</p>
<p>The <code>last_updated_timestamp</code> returned is in milliseconds since epoch.
</p>

<ul>
<li><p> Throws <code>RESOURCE_DOES_NOT_EXIST</code> if no such secret scope exists.
</p>
</li>
<li><p> Throws <code>PERMISSION_DENIED</code> if you do not have permission to make this API
call.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Secrets API: 
<code><a href="#topic+db_secrets_delete">db_secrets_delete</a>()</code>,
<code><a href="#topic+db_secrets_put">db_secrets_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_delete">db_secrets_scope_acl_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_get">db_secrets_scope_acl_get</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_list">db_secrets_scope_acl_list</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_put">db_secrets_scope_acl_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_create">db_secrets_scope_create</a>()</code>,
<code><a href="#topic+db_secrets_scope_delete">db_secrets_scope_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_list_all">db_secrets_scope_list_all</a>()</code>
</p>

<hr>
<h2 id='db_secrets_put'>Put Secret in Secret Scope</h2><span id='topic+db_secrets_put'></span>

<h3>Description</h3>

<p>Insert a secret under the provided scope with the given name.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_secrets_put(
  scope,
  key,
  value,
  as_bytes = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_secrets_put_+3A_scope">scope</code></td>
<td>
<p>Name of the scope to which the secret will be associated with</p>
</td></tr>
<tr><td><code id="db_secrets_put_+3A_key">key</code></td>
<td>
<p>Unique name to identify the secret.</p>
</td></tr>
<tr><td><code id="db_secrets_put_+3A_value">value</code></td>
<td>
<p>Contents of the secret to store, must be a string.</p>
</td></tr>
<tr><td><code id="db_secrets_put_+3A_as_bytes">as_bytes</code></td>
<td>
<p>Boolean (default: <code>FALSE</code>). Determines if <code>value</code> is stored
as bytes.</p>
</td></tr>
<tr><td><code id="db_secrets_put_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_put_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_put_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a secret already exists with the same name, this command overwrites the
existing secretâ€™s value.
</p>
<p>The server encrypts the secret using the secret scopeâ€™s encryption settings
before storing it. You must have <code>WRITE</code> or <code>MANAGE</code> permission on the secret
scope.
</p>
<p>The secret key must consist of alphanumeric characters, dashes, underscores,
and periods, and cannot exceed 128 characters. The maximum allowed secret
value size is 128 KB. The maximum number of secrets in a given scope is 1000.
</p>
<p>You can read a secret value only from within a command on a cluster
(for example, through a notebook); there is no API to read a secret value
outside of a cluster. The permission applied is based on who is invoking the
command and you must have at least <code>READ</code> permission.
</p>
<p>The input fields <code>string_value</code> or <code>bytes_value</code> specify the type of the
secret, which will determine the value returned when the secret value is
requested. Exactly one must be specified, this function interfaces these
parameters via <code>as_bytes</code> which defaults to <code>FALSE</code>.
</p>

<ul>
<li><p> Throws <code>RESOURCE_DOES_NOT_EXIST</code> if no such secret scope exists.
</p>
</li>
<li><p> Throws <code>RESOURCE_LIMIT_EXCEEDED</code> if maximum number of secrets in scope is
exceeded.
</p>
</li>
<li><p> Throws <code>INVALID_PARAMETER_VALUE</code> if the key name or value length is
invalid.
</p>
</li>
<li><p> Throws <code>PERMISSION_DENIED</code> if the user does not have permission to make
this API call.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Secrets API: 
<code><a href="#topic+db_secrets_delete">db_secrets_delete</a>()</code>,
<code><a href="#topic+db_secrets_list">db_secrets_list</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_delete">db_secrets_scope_acl_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_get">db_secrets_scope_acl_get</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_list">db_secrets_scope_acl_list</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_put">db_secrets_scope_acl_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_create">db_secrets_scope_create</a>()</code>,
<code><a href="#topic+db_secrets_scope_delete">db_secrets_scope_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_list_all">db_secrets_scope_list_all</a>()</code>
</p>

<hr>
<h2 id='db_secrets_scope_acl_delete'>Delete Secret Scope ACL</h2><span id='topic+db_secrets_scope_acl_delete'></span>

<h3>Description</h3>

<p>Delete the given ACL on the given scope.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_secrets_scope_acl_delete(
  scope,
  principal,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_secrets_scope_acl_delete_+3A_scope">scope</code></td>
<td>
<p>Name of the scope to remove permissions.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_delete_+3A_principal">principal</code></td>
<td>
<p>Principal to remove an existing ACL.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You must have the <code>MANAGE</code> permission to invoke this API.
</p>

<ul>
<li><p> Throws <code>RESOURCE_DOES_NOT_EXIST</code> if no such secret scope, principal, or
ACL exists.
</p>
</li>
<li><p> Throws <code>PERMISSION_DENIED</code> if you do not have permission to make this API
call.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Secrets API: 
<code><a href="#topic+db_secrets_delete">db_secrets_delete</a>()</code>,
<code><a href="#topic+db_secrets_list">db_secrets_list</a>()</code>,
<code><a href="#topic+db_secrets_put">db_secrets_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_get">db_secrets_scope_acl_get</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_list">db_secrets_scope_acl_list</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_put">db_secrets_scope_acl_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_create">db_secrets_scope_create</a>()</code>,
<code><a href="#topic+db_secrets_scope_delete">db_secrets_scope_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_list_all">db_secrets_scope_list_all</a>()</code>
</p>

<hr>
<h2 id='db_secrets_scope_acl_get'>Get Secret Scope ACL</h2><span id='topic+db_secrets_scope_acl_get'></span>

<h3>Description</h3>

<p>Get Secret Scope ACL
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_secrets_scope_acl_get(
  scope,
  principal,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_secrets_scope_acl_get_+3A_scope">scope</code></td>
<td>
<p>Name of the scope to fetch ACL information from.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_get_+3A_principal">principal</code></td>
<td>
<p>Principal to fetch ACL information from.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_get_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_get_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_get_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You must have the <code>MANAGE</code> permission to invoke this
</p>

<ul>
<li><p> Throws <code>RESOURCE_DOES_NOT_EXIST</code> if no such secret scope exists.
</p>
</li>
<li><p> Throws <code>PERMISSION_DENIED</code> if you do not have permission to make this API
call.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Secrets API: 
<code><a href="#topic+db_secrets_delete">db_secrets_delete</a>()</code>,
<code><a href="#topic+db_secrets_list">db_secrets_list</a>()</code>,
<code><a href="#topic+db_secrets_put">db_secrets_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_delete">db_secrets_scope_acl_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_list">db_secrets_scope_acl_list</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_put">db_secrets_scope_acl_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_create">db_secrets_scope_create</a>()</code>,
<code><a href="#topic+db_secrets_scope_delete">db_secrets_scope_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_list_all">db_secrets_scope_list_all</a>()</code>
</p>

<hr>
<h2 id='db_secrets_scope_acl_list'>List Secret Scope ACL's</h2><span id='topic+db_secrets_scope_acl_list'></span>

<h3>Description</h3>

<p>List Secret Scope ACL's
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_secrets_scope_acl_list(
  scope,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_secrets_scope_acl_list_+3A_scope">scope</code></td>
<td>
<p>Name of the scope to fetch ACL information from.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You must have the <code>MANAGE</code> permission to invoke this API.
</p>

<ul>
<li><p> Throws <code>RESOURCE_DOES_NOT_EXIST</code> if no such secret scope exists.
</p>
</li>
<li><p> Throws <code>PERMISSION_DENIED</code> if you do not have permission to make this API
call.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Secrets API: 
<code><a href="#topic+db_secrets_delete">db_secrets_delete</a>()</code>,
<code><a href="#topic+db_secrets_list">db_secrets_list</a>()</code>,
<code><a href="#topic+db_secrets_put">db_secrets_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_delete">db_secrets_scope_acl_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_get">db_secrets_scope_acl_get</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_put">db_secrets_scope_acl_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_create">db_secrets_scope_create</a>()</code>,
<code><a href="#topic+db_secrets_scope_delete">db_secrets_scope_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_list_all">db_secrets_scope_list_all</a>()</code>
</p>

<hr>
<h2 id='db_secrets_scope_acl_put'>Put ACL on Secret Scope</h2><span id='topic+db_secrets_scope_acl_put'></span>

<h3>Description</h3>

<p>Put ACL on Secret Scope
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_secrets_scope_acl_put(
  scope,
  principal,
  permission = c("READ", "WRITE", "MANAGE"),
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_secrets_scope_acl_put_+3A_scope">scope</code></td>
<td>
<p>Name of the scope to apply permissions.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_put_+3A_principal">principal</code></td>
<td>
<p>Principal to which the permission is applied</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_put_+3A_permission">permission</code></td>
<td>
<p>Permission level applied to the principal. One of <code>READ</code>,
<code>WRITE</code>, <code>MANAGE</code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_put_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_put_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_acl_put_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create or overwrite the ACL associated with the given principal (user or
group) on the specified scope point. In general, a user or group will use
the most powerful permission available to them, and permissions are ordered
as follows:
</p>

<ul>
<li> <p><code>MANAGE</code> - Allowed to change ACLs, and read and write to this secret scope.
</p>
</li>
<li> <p><code>WRITE</code> - Allowed to read and write to this secret scope.
</p>
</li>
<li> <p><code>READ</code> - Allowed to read this secret scope and list what secrets are
available.
</p>
</li></ul>

<p>You must have the <code>MANAGE</code> permission to invoke this API.
</p>
<p>The principal is a user or group name corresponding to an existing Databricks
principal to be granted or revoked access.
</p>

<ul>
<li><p> Throws <code>RESOURCE_DOES_NOT_EXIST</code> if no such secret scope exists.
</p>
</li>
<li><p> Throws <code>RESOURCE_ALREADY_EXISTS</code> if a permission for the principal already
exists.
</p>
</li>
<li><p> Throws <code>INVALID_PARAMETER_VALUE</code> if the permission is invalid.
</p>
</li>
<li><p> Throws <code>PERMISSION_DENIED</code> if you do not have permission to make this API
call.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Secrets API: 
<code><a href="#topic+db_secrets_delete">db_secrets_delete</a>()</code>,
<code><a href="#topic+db_secrets_list">db_secrets_list</a>()</code>,
<code><a href="#topic+db_secrets_put">db_secrets_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_delete">db_secrets_scope_acl_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_get">db_secrets_scope_acl_get</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_list">db_secrets_scope_acl_list</a>()</code>,
<code><a href="#topic+db_secrets_scope_create">db_secrets_scope_create</a>()</code>,
<code><a href="#topic+db_secrets_scope_delete">db_secrets_scope_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_list_all">db_secrets_scope_list_all</a>()</code>
</p>

<hr>
<h2 id='db_secrets_scope_create'>Create Secret Scope</h2><span id='topic+db_secrets_scope_create'></span>

<h3>Description</h3>

<p>Create Secret Scope
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_secrets_scope_create(
  scope,
  initial_manage_principal = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_secrets_scope_create_+3A_scope">scope</code></td>
<td>
<p>Scope name requested by the user. Scope names are unique.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_create_+3A_initial_manage_principal">initial_manage_principal</code></td>
<td>
<p>The principal that is initially granted
<code>MANAGE</code> permission to the created scope.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_create_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_create_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_create_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a Databricks-backed secret scope in which secrets are stored in
Databricks-managed storage and encrypted with a cloud-based specific
encryption key.
</p>
<p>The scope name:
</p>

<ul>
<li><p> Must be unique within a workspace.
</p>
</li>
<li><p> Must consist of alphanumeric characters, dashes, underscores, and periods,
and may not exceed 128 characters.
</p>
</li></ul>

<p>The names are considered non-sensitive and are readable by all users in the
workspace. A workspace is limited to a maximum of 100 secret scopes.
</p>
<p>If <code>initial_manage_principal</code> is specified, the initial ACL applied to the
scope is applied to the supplied principal (user or group) with <code>MANAGE</code>
permissions. The only supported principal for this option is the group users,
which contains all users in the workspace. If <code>initial_manage_principal</code> is
not specified, the initial ACL with <code>MANAGE</code> permission applied to the scope
is assigned to the API request issuerâ€™s user identity.
</p>

<ul>
<li><p> Throws <code>RESOURCE_ALREADY_EXISTS</code> if a scope with the given name already
exists.
</p>
</li>
<li><p> Throws <code>RESOURCE_LIMIT_EXCEEDED</code> if maximum number of scopes in the
workspace is exceeded.
</p>
</li>
<li><p> Throws <code>INVALID_PARAMETER_VALUE</code> if the scope name is invalid.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Secrets API: 
<code><a href="#topic+db_secrets_delete">db_secrets_delete</a>()</code>,
<code><a href="#topic+db_secrets_list">db_secrets_list</a>()</code>,
<code><a href="#topic+db_secrets_put">db_secrets_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_delete">db_secrets_scope_acl_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_get">db_secrets_scope_acl_get</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_list">db_secrets_scope_acl_list</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_put">db_secrets_scope_acl_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_delete">db_secrets_scope_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_list_all">db_secrets_scope_list_all</a>()</code>
</p>

<hr>
<h2 id='db_secrets_scope_delete'>Delete Secret Scope</h2><span id='topic+db_secrets_scope_delete'></span>

<h3>Description</h3>

<p>Delete Secret Scope
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_secrets_scope_delete(
  scope,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_secrets_scope_delete_+3A_scope">scope</code></td>
<td>
<p>Name of the scope to delete.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Throws <code>RESOURCE_DOES_NOT_EXIST</code> if the scope does not exist.
</p>
</li>
<li><p> Throws <code>PERMISSION_DENIED</code> if the user does not have permission to make
this API call.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Secrets API: 
<code><a href="#topic+db_secrets_delete">db_secrets_delete</a>()</code>,
<code><a href="#topic+db_secrets_list">db_secrets_list</a>()</code>,
<code><a href="#topic+db_secrets_put">db_secrets_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_delete">db_secrets_scope_acl_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_get">db_secrets_scope_acl_get</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_list">db_secrets_scope_acl_list</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_put">db_secrets_scope_acl_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_create">db_secrets_scope_create</a>()</code>,
<code><a href="#topic+db_secrets_scope_list_all">db_secrets_scope_list_all</a>()</code>
</p>

<hr>
<h2 id='db_secrets_scope_list_all'>List Secret Scopes</h2><span id='topic+db_secrets_scope_list_all'></span>

<h3>Description</h3>

<p>List Secret Scopes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_secrets_scope_list_all(
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_secrets_scope_list_all_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_list_all_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_secrets_scope_list_all_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> Throws <code>PERMISSION_DENIED</code> if you do not have permission to make this API
call.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Secrets API: 
<code><a href="#topic+db_secrets_delete">db_secrets_delete</a>()</code>,
<code><a href="#topic+db_secrets_list">db_secrets_list</a>()</code>,
<code><a href="#topic+db_secrets_put">db_secrets_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_delete">db_secrets_scope_acl_delete</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_get">db_secrets_scope_acl_get</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_list">db_secrets_scope_acl_list</a>()</code>,
<code><a href="#topic+db_secrets_scope_acl_put">db_secrets_scope_acl_put</a>()</code>,
<code><a href="#topic+db_secrets_scope_create">db_secrets_scope_create</a>()</code>,
<code><a href="#topic+db_secrets_scope_delete">db_secrets_scope_delete</a>()</code>
</p>

<hr>
<h2 id='db_sql_client'>Create Databricks SQL Connector Client</h2><span id='topic+db_sql_client'></span>

<h3>Description</h3>

<p>Create Databricks SQL Connector Client
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_client(
  id,
  catalog = NULL,
  schema = NULL,
  compute_type = c("warehouse", "cluster"),
  use_cloud_fetch = FALSE,
  session_configuration = list(),
  host = db_host(),
  token = db_token(),
  workspace_id = db_current_workspace_id(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_client_+3A_id">id</code></td>
<td>
<p>String, ID of either the SQL warehouse or all purpose cluster.
Important to set <code>compute_type</code> to the associated type of <code>id</code>.</p>
</td></tr>
<tr><td><code id="db_sql_client_+3A_catalog">catalog</code></td>
<td>
<p>Initial catalog to use for the connection. Defaults to <code>NULL</code>
in which case the default catalog will be used.</p>
</td></tr>
<tr><td><code id="db_sql_client_+3A_schema">schema</code></td>
<td>
<p>Initial schema to use for the connection. Defaults to <code>NULL</code>
in which case the default catalog will be used.</p>
</td></tr>
<tr><td><code id="db_sql_client_+3A_compute_type">compute_type</code></td>
<td>
<p>One of <code>"warehouse"</code> (default) or <code>"cluster"</code>, corresponding to
associated compute type of the resource specified in <code>id</code>.</p>
</td></tr>
<tr><td><code id="db_sql_client_+3A_use_cloud_fetch">use_cloud_fetch</code></td>
<td>
<p>Boolean (default is <code>FALSE</code>). <code>TRUE</code> to send fetch
requests directly to the cloud object store to download chunks of data.
<code>FALSE</code> to send fetch requests directly to Databricks.
</p>
<p>If <code>use_cloud_fetch</code> is set to <code>TRUE</code> but network access is blocked, then
the fetch requests will fail.</p>
</td></tr>
<tr><td><code id="db_sql_client_+3A_session_configuration">session_configuration</code></td>
<td>
<p>A optional named list of Spark session
configuration parameters. Setting a configuration is equivalent to using the
<code style="white-space: pre;">&#8288;SET key=val&#8288;</code> SQL command.
Run the SQL command <code>SET -v</code> to get a full list of available configurations.</p>
</td></tr>
<tr><td><code id="db_sql_client_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_client_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_client_+3A_workspace_id">workspace_id</code></td>
<td>
<p>String, workspace Id used to build the http path for the
connection. This defaults to using <code><a href="#topic+db_wsid">db_wsid()</a></code> to get <code>DATABRICKS_WSID</code>
environment variable. Not required if <code>compute_type</code> is <code>"cluster"</code>.</p>
</td></tr>
<tr><td><code id="db_sql_client_+3A_...">...</code></td>
<td>
<p>passed onto <code><a href="#topic+DatabricksSqlClient">DatabricksSqlClient()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create client using Databricks SQL Connector.
</p>


<h3>Value</h3>

<p><code><a href="#topic+DatabricksSqlClient">DatabricksSqlClient()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  client &lt;- db_sql_client(id = "&lt;warehouse_id&gt;", use_cloud_fetch = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='db_sql_exec_cancel'>Cancel SQL Query</h2><span id='topic+db_sql_exec_cancel'></span>

<h3>Description</h3>

<p>Cancel SQL Query
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_exec_cancel(
  statement_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_exec_cancel_+3A_statement_id">statement_id</code></td>
<td>
<p>String, query execution <code>statement_id</code></p>
</td></tr>
<tr><td><code id="db_sql_exec_cancel_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_exec_cancel_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_exec_cancel_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Requests that an executing statement be canceled. Callers must poll for
status to see the terminal state.
</p>
<p><a href="https://docs.databricks.com/api/workspace/statementexecution/cancelexecution">Read more on Databricks API docs</a>
</p>


<h3>See Also</h3>

<p>Other SQL Execution APIs: 
<code><a href="#topic+db_sql_exec_query">db_sql_exec_query</a>()</code>,
<code><a href="#topic+db_sql_exec_result">db_sql_exec_result</a>()</code>,
<code><a href="#topic+db_sql_exec_status">db_sql_exec_status</a>()</code>
</p>

<hr>
<h2 id='db_sql_exec_query'>Execute SQL Query</h2><span id='topic+db_sql_exec_query'></span>

<h3>Description</h3>

<p>Execute SQL Query
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_exec_query(
  statement,
  warehouse_id,
  catalog = NULL,
  schema = NULL,
  parameters = NULL,
  row_limit = NULL,
  byte_limit = NULL,
  disposition = c("INLINE", "EXTERNAL_LINKS"),
  format = c("JSON_ARRAY", "ARROW_STREAM", "CSV"),
  wait_timeout = "10s",
  on_wait_timeout = c("CONTINUE", "CANCEL"),
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_exec_query_+3A_statement">statement</code></td>
<td>
<p>String, the SQL statement to execute. The statement can
optionally be parameterized, see <code>parameters</code>.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_warehouse_id">warehouse_id</code></td>
<td>
<p>String, ID of warehouse upon which to execute a statement.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_catalog">catalog</code></td>
<td>
<p>String, sets default catalog for statement execution, similar
to <code style="white-space: pre;">&#8288;USE CATALOG&#8288;</code> in SQL.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_schema">schema</code></td>
<td>
<p>String, sets default schema for statement execution, similar
to <code style="white-space: pre;">&#8288;USE SCHEMA&#8288;</code> in SQL.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_parameters">parameters</code></td>
<td>
<p>List of Named Lists, parameters to pass into a SQL
statement containing parameter markers.
</p>
<p>A parameter consists of a name, a value, and <em>optionally</em> a type.
To represent a <code>NULL</code> value, the value field may be omitted or set to <code>NULL</code>
explicitly.
</p>
<p>See <a href="https://docs.databricks.com/api/workspace/statementexecution/executestatement">docs</a>
for more details.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_row_limit">row_limit</code></td>
<td>
<p>Integer, applies the given row limit to the statement's
result set, but unlike the <code>LIMIT</code> clause in SQL, it also sets the
<code>truncated</code> field in the response to indicate whether the result was trimmed
due to the limit or not.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_byte_limit">byte_limit</code></td>
<td>
<p>Integer, applies the given byte limit to the statement's
result size. Byte counts are based on internal data representations and
might not match the final size in the requested format. If the result was
truncated due to the byte limit, then <code>truncated</code> in the response is set to
true. When using <code>EXTERNAL_LINKS</code> disposition, a default byte_limit of
100 GiB is applied if <code>byte_limit</code> is not explicitly set.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_disposition">disposition</code></td>
<td>
<p>One of <code>"INLINE"</code> (default) or <code>"EXTERNAL_LINKS"</code>. See
<a href="https://docs.databricks.com/api/workspace/statementexecution/executestatement">docs</a>
for details.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_format">format</code></td>
<td>
<p>One of <code>"JSON_ARRAY"</code> (default), <code>"ARROW_STREAM"</code>, or <code>"CSV"</code>.
See <a href="https://docs.databricks.com/api/workspace/statementexecution/executestatement">docs</a>
for details.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_wait_timeout">wait_timeout</code></td>
<td>
<p>String, default is <code>"10s"</code>. The time in seconds the call
will wait for the statement's result set as <code>Ns</code>, where <code>N</code> can be set to
<code>0</code> or to a value between <code>5</code> and <code>50</code>.
When set to <code style="white-space: pre;">&#8288;0s&#8288;</code>, the statement will execute in asynchronous mode and the
call will not wait for the execution to finish. In this case, the call
returns directly with <code>PENDING</code> state and a statement ID which can be used
for polling with <code><a href="#topic+db_sql_exec_status">db_sql_exec_status()</a></code>.
</p>
<p>When set between <code>5</code> and <code>50</code> seconds, the call will behave synchronously up
to this timeout and wait for the statement execution to finish. If the
execution finishes within this time, the call returns immediately with a
manifest and result data (or a <code>FAILED</code> state in case of an execution error).
</p>
<p>If the statement takes longer to execute, <code>on_wait_timeout</code> determines what
should happen after the timeout is reached.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_on_wait_timeout">on_wait_timeout</code></td>
<td>
<p>One of <code>"CONTINUE"</code> (default) or <code>"CANCEL"</code>.
When <code>wait_timeout</code> &gt; <code style="white-space: pre;">&#8288;0s&#8288;</code>, the call will block up to the specified time.
If the statement execution doesn't finish within this time,
<code>on_wait_timeout</code> determines whether the execution should continue or be
canceled.
</p>
<p>When set to <code>CONTINUE</code>, the statement execution continues asynchronously and
the call returns a statement ID which can be used for polling with
<code><a href="#topic+db_sql_exec_status">db_sql_exec_status()</a></code>.
</p>
<p>When set to <code>CANCEL</code>, the statement execution is canceled and the call
returns with a <code>CANCELED</code> state.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_exec_query_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Refer to the
<a href="https://docs.databricks.com/api/workspace/statementexecution/executestatement">web documentation</a>
for detailed material on interaction of the various parameters and general recommendations
</p>


<h3>See Also</h3>

<p>Other SQL Execution APIs: 
<code><a href="#topic+db_sql_exec_cancel">db_sql_exec_cancel</a>()</code>,
<code><a href="#topic+db_sql_exec_result">db_sql_exec_result</a>()</code>,
<code><a href="#topic+db_sql_exec_status">db_sql_exec_status</a>()</code>
</p>

<hr>
<h2 id='db_sql_exec_result'>Get SQL Query Results</h2><span id='topic+db_sql_exec_result'></span>

<h3>Description</h3>

<p>Get SQL Query Results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_exec_result(
  statement_id,
  chunk_index,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_exec_result_+3A_statement_id">statement_id</code></td>
<td>
<p>String, query execution <code>statement_id</code></p>
</td></tr>
<tr><td><code id="db_sql_exec_result_+3A_chunk_index">chunk_index</code></td>
<td>
<p>Integer, chunk index to fetch result. Starts from <code>0</code>.</p>
</td></tr>
<tr><td><code id="db_sql_exec_result_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_exec_result_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_exec_result_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After the statement execution has <code>SUCCEEDED</code>, this request can be used to
fetch any chunk by index.
</p>
<p>Whereas the first chunk with chunk_index = <code>0</code> is typically fetched with
<code><a href="#topic+db_sql_exec_result">db_sql_exec_result()</a></code> or <code><a href="#topic+db_sql_exec_status">db_sql_exec_status()</a></code>, this request can be used
to fetch subsequent chunks
</p>
<p>The response structure is identical to the nested result element described
in the <code><a href="#topic+db_sql_exec_result">db_sql_exec_result()</a></code> request, and similarly includes the
<code>next_chunk_index</code> and <code>next_chunk_internal_link</code> fields for simple
iteration through the result set.
</p>
<p><a href="https://docs.databricks.com/api/workspace/statementexecution/getstatementresultchunkn">Read more on Databricks API docs</a>
</p>


<h3>See Also</h3>

<p>Other SQL Execution APIs: 
<code><a href="#topic+db_sql_exec_cancel">db_sql_exec_cancel</a>()</code>,
<code><a href="#topic+db_sql_exec_query">db_sql_exec_query</a>()</code>,
<code><a href="#topic+db_sql_exec_status">db_sql_exec_status</a>()</code>
</p>

<hr>
<h2 id='db_sql_exec_status'>Get SQL Query Status</h2><span id='topic+db_sql_exec_status'></span>

<h3>Description</h3>

<p>Get SQL Query Status
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_exec_status(
  statement_id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_exec_status_+3A_statement_id">statement_id</code></td>
<td>
<p>String, query execution <code>statement_id</code></p>
</td></tr>
<tr><td><code id="db_sql_exec_status_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_exec_status_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_exec_status_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This request can be used to poll for the statement's status.
When the <code>status.state</code> field is <code>SUCCEEDED</code> it will also return the result
manifest and the first chunk of the result data.
</p>
<p>When the statement is in the terminal states <code>CANCELED</code>, <code>CLOSED</code> or
<code>FAILED</code>, it returns HTTP <code>200</code> with the state set.
</p>
<p>After at least 12 hours in terminal state, the statement is removed from the
warehouse and further calls will receive an HTTP <code>404</code> response.
</p>
<p><a href="https://docs.databricks.com/api/workspace/statementexecution/getstatement">Read more on Databricks API docs</a>
</p>


<h3>See Also</h3>

<p>Other SQL Execution APIs: 
<code><a href="#topic+db_sql_exec_cancel">db_sql_exec_cancel</a>()</code>,
<code><a href="#topic+db_sql_exec_query">db_sql_exec_query</a>()</code>,
<code><a href="#topic+db_sql_exec_result">db_sql_exec_result</a>()</code>
</p>

<hr>
<h2 id='db_sql_global_warehouse_get'>Get Global Warehouse Config</h2><span id='topic+db_sql_global_warehouse_get'></span>

<h3>Description</h3>

<p>Get Global Warehouse Config
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_global_warehouse_get(
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_global_warehouse_get_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_global_warehouse_get_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_global_warehouse_get_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Warehouse API: 
<code><a href="#topic+db_sql_warehouse_create">db_sql_warehouse_create</a>()</code>,
<code><a href="#topic+db_sql_warehouse_delete">db_sql_warehouse_delete</a>()</code>,
<code><a href="#topic+db_sql_warehouse_edit">db_sql_warehouse_edit</a>()</code>,
<code><a href="#topic+db_sql_warehouse_get">db_sql_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_list">db_sql_warehouse_list</a>()</code>,
<code><a href="#topic+db_sql_warehouse_start">db_sql_warehouse_start</a>()</code>,
<code><a href="#topic+db_sql_warehouse_stop">db_sql_warehouse_stop</a>()</code>,
<code><a href="#topic+get_and_start_warehouse">get_and_start_warehouse</a>()</code>
</p>

<hr>
<h2 id='db_sql_query_history'>List Warehouse Query History</h2><span id='topic+db_sql_query_history'></span>

<h3>Description</h3>

<p>For more details refer to the <a href="https://docs.databricks.com/sql/api/query-history.html#list">query history documentation</a>.
This function elevates the sub-components of <code>filter_by</code> parameter to the R
function directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_query_history(
  statuses = NULL,
  user_ids = NULL,
  endpoint_ids = NULL,
  start_time_ms = NULL,
  end_time_ms = NULL,
  max_results = 100,
  page_token = NULL,
  include_metrics = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_query_history_+3A_statuses">statuses</code></td>
<td>
<p>Allows filtering by query status. Possible values are:
<code>QUEUED</code>, <code>RUNNING</code>, <code>CANCELED</code>, <code>FAILED</code>, <code>FINISHED</code>. Multiple permitted.</p>
</td></tr>
<tr><td><code id="db_sql_query_history_+3A_user_ids">user_ids</code></td>
<td>
<p>Allows filtering by user ID's. Multiple permitted.</p>
</td></tr>
<tr><td><code id="db_sql_query_history_+3A_endpoint_ids">endpoint_ids</code></td>
<td>
<p>Allows filtering by endpoint ID's. Multiple permitted.</p>
</td></tr>
<tr><td><code id="db_sql_query_history_+3A_start_time_ms">start_time_ms</code></td>
<td>
<p>Integer, limit results to queries that started after this time.</p>
</td></tr>
<tr><td><code id="db_sql_query_history_+3A_end_time_ms">end_time_ms</code></td>
<td>
<p>Integer, limit results to queries that started before this time.</p>
</td></tr>
<tr><td><code id="db_sql_query_history_+3A_max_results">max_results</code></td>
<td>
<p>Limit the number of results returned in one page. Default is 100.</p>
</td></tr>
<tr><td><code id="db_sql_query_history_+3A_page_token">page_token</code></td>
<td>
<p>Opaque token used to get the next page of results. Optional.</p>
</td></tr>
<tr><td><code id="db_sql_query_history_+3A_include_metrics">include_metrics</code></td>
<td>
<p>Whether to include metrics about query execution.</p>
</td></tr>
<tr><td><code id="db_sql_query_history_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_query_history_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_query_history_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default the filter parameters <code>statuses</code>, <code>user_ids</code>, and <code>endpoints_ids</code>
are <code>NULL</code>.
</p>

<hr>
<h2 id='db_sql_warehouse_create'>Create Warehouse</h2><span id='topic+db_sql_warehouse_create'></span>

<h3>Description</h3>

<p>Create Warehouse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_warehouse_create(
  name,
  cluster_size,
  min_num_clusters = 1,
  max_num_clusters = 1,
  auto_stop_mins = 30,
  tags = list(),
  spot_instance_policy = c("COST_OPTIMIZED", "RELIABILITY_OPTIMIZED"),
  enable_photon = TRUE,
  warehouse_type = c("CLASSIC", "PRO"),
  enable_serverless_compute = NULL,
  disable_uc = FALSE,
  channel = c("CHANNEL_NAME_CURRENT", "CHANNEL_NAME_PREVIEW"),
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_warehouse_create_+3A_name">name</code></td>
<td>
<p>Name of the SQL warehouse. Must be unique.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_cluster_size">cluster_size</code></td>
<td>
<p>Size of the clusters allocated to the warehouse. One of
<code style="white-space: pre;">&#8288;2X-Small&#8288;</code>, <code>X-Small</code>, <code>Small</code>, <code>Medium</code>, <code>Large</code>, <code>X-Large</code>, <code style="white-space: pre;">&#8288;2X-Large&#8288;</code>,
<code style="white-space: pre;">&#8288;3X-Large&#8288;</code>, <code style="white-space: pre;">&#8288;4X-Large&#8288;</code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_min_num_clusters">min_num_clusters</code></td>
<td>
<p>Minimum number of clusters available when a SQL
warehouse is running. The default is 1.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_max_num_clusters">max_num_clusters</code></td>
<td>
<p>Maximum number of clusters available when a SQL
warehouse is running. If multi-cluster load balancing is not enabled,
this is limited to 1.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_auto_stop_mins">auto_stop_mins</code></td>
<td>
<p>Time in minutes until an idle SQL warehouse terminates
all clusters and stops. Defaults to 30. For Serverless SQL warehouses
(<code>enable_serverless_compute</code> = <code>TRUE</code>), set this to 10.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_tags">tags</code></td>
<td>
<p>Named list that describes the warehouse. Databricks tags all
warehouse resources with these tags.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_spot_instance_policy">spot_instance_policy</code></td>
<td>
<p>The spot policy to use for allocating instances
to clusters. This field is not used if the SQL warehouse is a Serverless SQL
warehouse.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_enable_photon">enable_photon</code></td>
<td>
<p>Whether queries are executed on a native vectorized
engine that speeds up query execution. The default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_warehouse_type">warehouse_type</code></td>
<td>
<p>Either &quot;CLASSIC&quot; (default), or &quot;PRO&quot;</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_enable_serverless_compute">enable_serverless_compute</code></td>
<td>
<p>Whether this SQL warehouse is a Serverless
warehouse. To use a Serverless SQL warehouse, you must enable Serverless SQL
warehouses for the workspace. If Serverless SQL warehouses are disabled for the
workspace, the default is <code>FALSE</code> If Serverless SQL warehouses are enabled for
the workspace, the default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_disable_uc">disable_uc</code></td>
<td>
<p>If <code>TRUE</code> will use Hive Metastore (HMS). If <code>FALSE</code>
(default), then it will be enabled for Unity Catalog (UC).</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_channel">channel</code></td>
<td>
<p>Whether to use the current SQL warehouse compute version or the
preview version. Databricks does not recommend using preview versions for
production workloads. The default is <code>CHANNEL_NAME_CURRENT.</code></p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_create_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Warehouse API: 
<code><a href="#topic+db_sql_global_warehouse_get">db_sql_global_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_delete">db_sql_warehouse_delete</a>()</code>,
<code><a href="#topic+db_sql_warehouse_edit">db_sql_warehouse_edit</a>()</code>,
<code><a href="#topic+db_sql_warehouse_get">db_sql_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_list">db_sql_warehouse_list</a>()</code>,
<code><a href="#topic+db_sql_warehouse_start">db_sql_warehouse_start</a>()</code>,
<code><a href="#topic+db_sql_warehouse_stop">db_sql_warehouse_stop</a>()</code>,
<code><a href="#topic+get_and_start_warehouse">get_and_start_warehouse</a>()</code>
</p>

<hr>
<h2 id='db_sql_warehouse_delete'>Delete Warehouse</h2><span id='topic+db_sql_warehouse_delete'></span>

<h3>Description</h3>

<p>Delete Warehouse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_warehouse_delete(
  id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_warehouse_delete_+3A_id">id</code></td>
<td>
<p>ID of the SQL warehouse.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Warehouse API: 
<code><a href="#topic+db_sql_global_warehouse_get">db_sql_global_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_create">db_sql_warehouse_create</a>()</code>,
<code><a href="#topic+db_sql_warehouse_edit">db_sql_warehouse_edit</a>()</code>,
<code><a href="#topic+db_sql_warehouse_get">db_sql_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_list">db_sql_warehouse_list</a>()</code>,
<code><a href="#topic+db_sql_warehouse_start">db_sql_warehouse_start</a>()</code>,
<code><a href="#topic+db_sql_warehouse_stop">db_sql_warehouse_stop</a>()</code>,
<code><a href="#topic+get_and_start_warehouse">get_and_start_warehouse</a>()</code>
</p>

<hr>
<h2 id='db_sql_warehouse_edit'>Edit Warehouse</h2><span id='topic+db_sql_warehouse_edit'></span>

<h3>Description</h3>

<p>Edit Warehouse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_warehouse_edit(
  id,
  name = NULL,
  cluster_size = NULL,
  min_num_clusters = NULL,
  max_num_clusters = NULL,
  auto_stop_mins = NULL,
  tags = NULL,
  spot_instance_policy = NULL,
  enable_photon = NULL,
  warehouse_type = NULL,
  enable_serverless_compute = NULL,
  channel = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_warehouse_edit_+3A_id">id</code></td>
<td>
<p>ID of the SQL warehouse.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_name">name</code></td>
<td>
<p>Name of the SQL warehouse. Must be unique.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_cluster_size">cluster_size</code></td>
<td>
<p>Size of the clusters allocated to the warehouse. One of
<code style="white-space: pre;">&#8288;2X-Small&#8288;</code>, <code>X-Small</code>, <code>Small</code>, <code>Medium</code>, <code>Large</code>, <code>X-Large</code>, <code style="white-space: pre;">&#8288;2X-Large&#8288;</code>,
<code style="white-space: pre;">&#8288;3X-Large&#8288;</code>, <code style="white-space: pre;">&#8288;4X-Large&#8288;</code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_min_num_clusters">min_num_clusters</code></td>
<td>
<p>Minimum number of clusters available when a SQL
warehouse is running. The default is 1.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_max_num_clusters">max_num_clusters</code></td>
<td>
<p>Maximum number of clusters available when a SQL
warehouse is running. If multi-cluster load balancing is not enabled,
this is limited to 1.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_auto_stop_mins">auto_stop_mins</code></td>
<td>
<p>Time in minutes until an idle SQL warehouse terminates
all clusters and stops. Defaults to 30. For Serverless SQL warehouses
(<code>enable_serverless_compute</code> = <code>TRUE</code>), set this to 10.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_tags">tags</code></td>
<td>
<p>Named list that describes the warehouse. Databricks tags all
warehouse resources with these tags.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_spot_instance_policy">spot_instance_policy</code></td>
<td>
<p>The spot policy to use for allocating instances
to clusters. This field is not used if the SQL warehouse is a Serverless SQL
warehouse.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_enable_photon">enable_photon</code></td>
<td>
<p>Whether queries are executed on a native vectorized
engine that speeds up query execution. The default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_warehouse_type">warehouse_type</code></td>
<td>
<p>Either &quot;CLASSIC&quot; (default), or &quot;PRO&quot;</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_enable_serverless_compute">enable_serverless_compute</code></td>
<td>
<p>Whether this SQL warehouse is a Serverless
warehouse. To use a Serverless SQL warehouse, you must enable Serverless SQL
warehouses for the workspace. If Serverless SQL warehouses are disabled for the
workspace, the default is <code>FALSE</code> If Serverless SQL warehouses are enabled for
the workspace, the default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_channel">channel</code></td>
<td>
<p>Whether to use the current SQL warehouse compute version or the
preview version. Databricks does not recommend using preview versions for
production workloads. The default is <code>CHANNEL_NAME_CURRENT.</code></p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_edit_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Modify a SQL warehouse. All fields are optional. Missing fields
default to the current values.
</p>


<h3>See Also</h3>

<p>Other Warehouse API: 
<code><a href="#topic+db_sql_global_warehouse_get">db_sql_global_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_create">db_sql_warehouse_create</a>()</code>,
<code><a href="#topic+db_sql_warehouse_delete">db_sql_warehouse_delete</a>()</code>,
<code><a href="#topic+db_sql_warehouse_get">db_sql_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_list">db_sql_warehouse_list</a>()</code>,
<code><a href="#topic+db_sql_warehouse_start">db_sql_warehouse_start</a>()</code>,
<code><a href="#topic+db_sql_warehouse_stop">db_sql_warehouse_stop</a>()</code>,
<code><a href="#topic+get_and_start_warehouse">get_and_start_warehouse</a>()</code>
</p>

<hr>
<h2 id='db_sql_warehouse_get'>Get Warehouse</h2><span id='topic+db_sql_warehouse_get'></span>

<h3>Description</h3>

<p>Get Warehouse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_warehouse_get(
  id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_warehouse_get_+3A_id">id</code></td>
<td>
<p>ID of the SQL warehouse.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_get_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_get_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_get_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Warehouse API: 
<code><a href="#topic+db_sql_global_warehouse_get">db_sql_global_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_create">db_sql_warehouse_create</a>()</code>,
<code><a href="#topic+db_sql_warehouse_delete">db_sql_warehouse_delete</a>()</code>,
<code><a href="#topic+db_sql_warehouse_edit">db_sql_warehouse_edit</a>()</code>,
<code><a href="#topic+db_sql_warehouse_list">db_sql_warehouse_list</a>()</code>,
<code><a href="#topic+db_sql_warehouse_start">db_sql_warehouse_start</a>()</code>,
<code><a href="#topic+db_sql_warehouse_stop">db_sql_warehouse_stop</a>()</code>,
<code><a href="#topic+get_and_start_warehouse">get_and_start_warehouse</a>()</code>
</p>

<hr>
<h2 id='db_sql_warehouse_list'>List Warehouses</h2><span id='topic+db_sql_warehouse_list'></span>

<h3>Description</h3>

<p>List Warehouses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_warehouse_list(
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_warehouse_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Warehouse API: 
<code><a href="#topic+db_sql_global_warehouse_get">db_sql_global_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_create">db_sql_warehouse_create</a>()</code>,
<code><a href="#topic+db_sql_warehouse_delete">db_sql_warehouse_delete</a>()</code>,
<code><a href="#topic+db_sql_warehouse_edit">db_sql_warehouse_edit</a>()</code>,
<code><a href="#topic+db_sql_warehouse_get">db_sql_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_start">db_sql_warehouse_start</a>()</code>,
<code><a href="#topic+db_sql_warehouse_stop">db_sql_warehouse_stop</a>()</code>,
<code><a href="#topic+get_and_start_warehouse">get_and_start_warehouse</a>()</code>
</p>

<hr>
<h2 id='db_sql_warehouse_start'>Start Warehouse</h2><span id='topic+db_sql_warehouse_start'></span>

<h3>Description</h3>

<p>Start Warehouse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_warehouse_start(
  id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_warehouse_start_+3A_id">id</code></td>
<td>
<p>ID of the SQL warehouse.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_start_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_start_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_start_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Warehouse API: 
<code><a href="#topic+db_sql_global_warehouse_get">db_sql_global_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_create">db_sql_warehouse_create</a>()</code>,
<code><a href="#topic+db_sql_warehouse_delete">db_sql_warehouse_delete</a>()</code>,
<code><a href="#topic+db_sql_warehouse_edit">db_sql_warehouse_edit</a>()</code>,
<code><a href="#topic+db_sql_warehouse_get">db_sql_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_list">db_sql_warehouse_list</a>()</code>,
<code><a href="#topic+db_sql_warehouse_stop">db_sql_warehouse_stop</a>()</code>,
<code><a href="#topic+get_and_start_warehouse">get_and_start_warehouse</a>()</code>
</p>

<hr>
<h2 id='db_sql_warehouse_stop'>Stop Warehouse</h2><span id='topic+db_sql_warehouse_stop'></span>

<h3>Description</h3>

<p>Stop Warehouse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_sql_warehouse_stop(
  id,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_sql_warehouse_stop_+3A_id">id</code></td>
<td>
<p>ID of the SQL warehouse.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_stop_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_stop_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_sql_warehouse_stop_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Warehouse API: 
<code><a href="#topic+db_sql_global_warehouse_get">db_sql_global_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_create">db_sql_warehouse_create</a>()</code>,
<code><a href="#topic+db_sql_warehouse_delete">db_sql_warehouse_delete</a>()</code>,
<code><a href="#topic+db_sql_warehouse_edit">db_sql_warehouse_edit</a>()</code>,
<code><a href="#topic+db_sql_warehouse_get">db_sql_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_list">db_sql_warehouse_list</a>()</code>,
<code><a href="#topic+db_sql_warehouse_start">db_sql_warehouse_start</a>()</code>,
<code><a href="#topic+get_and_start_warehouse">get_and_start_warehouse</a>()</code>
</p>

<hr>
<h2 id='db_token'>Fetch Databricks Token</h2><span id='topic+db_token'></span>

<h3>Description</h3>

<p>The function will check for a token in the <code>DATABRICKS_HOST</code> environment variable.
<code>.databrickscfg</code> will be searched if <code>db_profile</code> and <code>use_databrickscfg</code> are set or
if Posit Workbench managed OAuth credentials are detected.
If none of the above are found then will default to using OAuth U2M flow.
</p>
<p>Refer to <a href="https://docs.databricks.com/dev-tools/api/latest/authentication.html">api authentication docs</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_token(profile = default_config_profile())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_token_+3A_profile">profile</code></td>
<td>
<p>Profile to use when fetching from environment variable
(e.g. <code>.Renviron</code>) or <code>.databricksfg</code> file</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The behaviour is subject to change depending if <code>db_profile</code> and
<code>use_databrickscfg</code> options are set.
</p>

<ul>
<li> <p><code>use_databrickscfg</code>: Boolean (default: <code>FALSE</code>), determines if credentials
are fetched from profile of <code>.databrickscfg</code> or <code>.Renviron</code>
</p>
</li>
<li> <p><code>db_profile</code>: String (default: <code>NULL</code>), determines profile used.
<code>.databrickscfg</code> will automatically be used when Posit Workbench managed OAuth credentials are detected.
</p>
</li></ul>

<p>See vignette on authentication for more details.
</p>


<h3>Value</h3>

<p>databricks token
</p>


<h3>See Also</h3>

<p>Other Databricks Authentication Helpers: 
<code><a href="#topic+db_host">db_host</a>()</code>,
<code><a href="#topic+db_read_netrc">db_read_netrc</a>()</code>,
<code><a href="#topic+db_wsid">db_wsid</a>()</code>
</p>

<hr>
<h2 id='db_volume_delete'>Volume FileSystem Delete</h2><span id='topic+db_volume_delete'></span>

<h3>Description</h3>

<p>Volume FileSystem Delete
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_volume_delete(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_volume_delete_+3A_path">path</code></td>
<td>
<p>Absolute path of the file in the Files API, omitting the initial
slash.</p>
</td></tr>
<tr><td><code id="db_volume_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Volumes FileSystem API: 
<code><a href="#topic+db_volume_dir_create">db_volume_dir_create</a>()</code>,
<code><a href="#topic+db_volume_dir_delete">db_volume_dir_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_exists">db_volume_dir_exists</a>()</code>,
<code><a href="#topic+db_volume_file_exists">db_volume_file_exists</a>()</code>,
<code><a href="#topic+db_volume_list">db_volume_list</a>()</code>,
<code><a href="#topic+db_volume_read">db_volume_read</a>()</code>,
<code><a href="#topic+db_volume_write">db_volume_write</a>()</code>
</p>

<hr>
<h2 id='db_volume_dir_create'>Volume FileSystem Create Directory</h2><span id='topic+db_volume_dir_create'></span>

<h3>Description</h3>

<p>Volume FileSystem Create Directory
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_volume_dir_create(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_volume_dir_create_+3A_path">path</code></td>
<td>
<p>Absolute path of the file in the Files API, omitting the initial
slash.</p>
</td></tr>
<tr><td><code id="db_volume_dir_create_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_dir_create_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_dir_create_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Volumes FileSystem API: 
<code><a href="#topic+db_volume_delete">db_volume_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_delete">db_volume_dir_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_exists">db_volume_dir_exists</a>()</code>,
<code><a href="#topic+db_volume_file_exists">db_volume_file_exists</a>()</code>,
<code><a href="#topic+db_volume_list">db_volume_list</a>()</code>,
<code><a href="#topic+db_volume_read">db_volume_read</a>()</code>,
<code><a href="#topic+db_volume_write">db_volume_write</a>()</code>
</p>

<hr>
<h2 id='db_volume_dir_delete'>Volume FileSystem Delete Directory</h2><span id='topic+db_volume_dir_delete'></span>

<h3>Description</h3>

<p>Volume FileSystem Delete Directory
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_volume_dir_delete(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_volume_dir_delete_+3A_path">path</code></td>
<td>
<p>Absolute path of the file in the Files API, omitting the initial
slash.</p>
</td></tr>
<tr><td><code id="db_volume_dir_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_dir_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_dir_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Volumes FileSystem API: 
<code><a href="#topic+db_volume_delete">db_volume_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_create">db_volume_dir_create</a>()</code>,
<code><a href="#topic+db_volume_dir_exists">db_volume_dir_exists</a>()</code>,
<code><a href="#topic+db_volume_file_exists">db_volume_file_exists</a>()</code>,
<code><a href="#topic+db_volume_list">db_volume_list</a>()</code>,
<code><a href="#topic+db_volume_read">db_volume_read</a>()</code>,
<code><a href="#topic+db_volume_write">db_volume_write</a>()</code>
</p>

<hr>
<h2 id='db_volume_dir_exists'>Volume FileSystem Check Directory Exists</h2><span id='topic+db_volume_dir_exists'></span>

<h3>Description</h3>

<p>Volume FileSystem Check Directory Exists
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_volume_dir_exists(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_volume_dir_exists_+3A_path">path</code></td>
<td>
<p>Absolute path of the file in the Files API, omitting the initial
slash.</p>
</td></tr>
<tr><td><code id="db_volume_dir_exists_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_dir_exists_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_dir_exists_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Volumes FileSystem API: 
<code><a href="#topic+db_volume_delete">db_volume_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_create">db_volume_dir_create</a>()</code>,
<code><a href="#topic+db_volume_dir_delete">db_volume_dir_delete</a>()</code>,
<code><a href="#topic+db_volume_file_exists">db_volume_file_exists</a>()</code>,
<code><a href="#topic+db_volume_list">db_volume_list</a>()</code>,
<code><a href="#topic+db_volume_read">db_volume_read</a>()</code>,
<code><a href="#topic+db_volume_write">db_volume_write</a>()</code>
</p>

<hr>
<h2 id='db_volume_file_exists'>Volume FileSystem File Status</h2><span id='topic+db_volume_file_exists'></span>

<h3>Description</h3>

<p>Volume FileSystem File Status
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_volume_file_exists(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_volume_file_exists_+3A_path">path</code></td>
<td>
<p>Absolute path of the file in the Files API, omitting the initial
slash.</p>
</td></tr>
<tr><td><code id="db_volume_file_exists_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_file_exists_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_file_exists_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Volumes FileSystem API: 
<code><a href="#topic+db_volume_delete">db_volume_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_create">db_volume_dir_create</a>()</code>,
<code><a href="#topic+db_volume_dir_delete">db_volume_dir_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_exists">db_volume_dir_exists</a>()</code>,
<code><a href="#topic+db_volume_list">db_volume_list</a>()</code>,
<code><a href="#topic+db_volume_read">db_volume_read</a>()</code>,
<code><a href="#topic+db_volume_write">db_volume_write</a>()</code>
</p>

<hr>
<h2 id='db_volume_list'>Volume FileSystem List Directory Contents</h2><span id='topic+db_volume_list'></span>

<h3>Description</h3>

<p>Volume FileSystem List Directory Contents
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_volume_list(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_volume_list_+3A_path">path</code></td>
<td>
<p>Absolute path of the file in the Files API, omitting the initial
slash.</p>
</td></tr>
<tr><td><code id="db_volume_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Volumes FileSystem API: 
<code><a href="#topic+db_volume_delete">db_volume_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_create">db_volume_dir_create</a>()</code>,
<code><a href="#topic+db_volume_dir_delete">db_volume_dir_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_exists">db_volume_dir_exists</a>()</code>,
<code><a href="#topic+db_volume_file_exists">db_volume_file_exists</a>()</code>,
<code><a href="#topic+db_volume_read">db_volume_read</a>()</code>,
<code><a href="#topic+db_volume_write">db_volume_write</a>()</code>
</p>

<hr>
<h2 id='db_volume_read'>Volume FileSystem Read</h2><span id='topic+db_volume_read'></span>

<h3>Description</h3>

<p>Return the contents of a file within a volume (up to 2GiB).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_volume_read(
  path,
  destination,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_volume_read_+3A_path">path</code></td>
<td>
<p>Absolute path of the file in the Files API, omitting the initial
slash.</p>
</td></tr>
<tr><td><code id="db_volume_read_+3A_destination">destination</code></td>
<td>
<p>Path to write downloaded file to.</p>
</td></tr>
<tr><td><code id="db_volume_read_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_read_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_read_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Volumes FileSystem API: 
<code><a href="#topic+db_volume_delete">db_volume_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_create">db_volume_dir_create</a>()</code>,
<code><a href="#topic+db_volume_dir_delete">db_volume_dir_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_exists">db_volume_dir_exists</a>()</code>,
<code><a href="#topic+db_volume_file_exists">db_volume_file_exists</a>()</code>,
<code><a href="#topic+db_volume_list">db_volume_list</a>()</code>,
<code><a href="#topic+db_volume_write">db_volume_write</a>()</code>
</p>

<hr>
<h2 id='db_volume_write'>Volume FileSystem Write</h2><span id='topic+db_volume_write'></span>

<h3>Description</h3>

<p>Upload a file to volume filesystem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_volume_write(
  path,
  file = NULL,
  overwrite = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_volume_write_+3A_path">path</code></td>
<td>
<p>Absolute path of the file in the Files API, omitting the initial
slash.</p>
</td></tr>
<tr><td><code id="db_volume_write_+3A_file">file</code></td>
<td>
<p>Path to a file on local system, takes precedent over <code>path</code>.</p>
</td></tr>
<tr><td><code id="db_volume_write_+3A_overwrite">overwrite</code></td>
<td>
<p>Flag (Default: <code>FALSE</code>) that specifies whether to overwrite
existing files.</p>
</td></tr>
<tr><td><code id="db_volume_write_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_write_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_volume_write_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uploads a file of up to 5 GiB.
</p>


<h3>See Also</h3>

<p>Other Volumes FileSystem API: 
<code><a href="#topic+db_volume_delete">db_volume_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_create">db_volume_dir_create</a>()</code>,
<code><a href="#topic+db_volume_dir_delete">db_volume_dir_delete</a>()</code>,
<code><a href="#topic+db_volume_dir_exists">db_volume_dir_exists</a>()</code>,
<code><a href="#topic+db_volume_file_exists">db_volume_file_exists</a>()</code>,
<code><a href="#topic+db_volume_list">db_volume_list</a>()</code>,
<code><a href="#topic+db_volume_read">db_volume_read</a>()</code>
</p>

<hr>
<h2 id='db_vs_endpoints_create'>Create a Vector Search Endpoint</h2><span id='topic+db_vs_endpoints_create'></span>

<h3>Description</h3>

<p>Create a Vector Search Endpoint
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_endpoints_create(
  name,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_endpoints_create_+3A_name">name</code></td>
<td>
<p>Name of vector search endpoint</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_create_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_create_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_create_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can take a few moments to run.
</p>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_endpoints_delete'>Delete a Vector Search Endpoint</h2><span id='topic+db_vs_endpoints_delete'></span>

<h3>Description</h3>

<p>Delete a Vector Search Endpoint
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_endpoints_delete(
  endpoint,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_endpoints_delete_+3A_endpoint">endpoint</code></td>
<td>
<p>Name of vector search endpoint</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_endpoints_get'>Get a Vector Search Endpoint</h2><span id='topic+db_vs_endpoints_get'></span>

<h3>Description</h3>

<p>Get a Vector Search Endpoint
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_endpoints_get(
  endpoint,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_endpoints_get_+3A_endpoint">endpoint</code></td>
<td>
<p>Name of vector search endpoint</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_get_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_get_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_get_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_endpoints_list'>List Vector Search Endpoints</h2><span id='topic+db_vs_endpoints_list'></span>

<h3>Description</h3>

<p>List Vector Search Endpoints
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_endpoints_list(
  page_token = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_endpoints_list_+3A_page_token">page_token</code></td>
<td>
<p>Token for pagination</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_endpoints_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_indexes_create'>Create a Vector Search Index</h2><span id='topic+db_vs_indexes_create'></span>

<h3>Description</h3>

<p>Create a Vector Search Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_indexes_create(
  name,
  endpoint,
  primary_key,
  spec,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_indexes_create_+3A_name">name</code></td>
<td>
<p>Name of vector search index</p>
</td></tr>
<tr><td><code id="db_vs_indexes_create_+3A_endpoint">endpoint</code></td>
<td>
<p>Name of vector search endpoint</p>
</td></tr>
<tr><td><code id="db_vs_indexes_create_+3A_primary_key">primary_key</code></td>
<td>
<p>Vector search primary key column name</p>
</td></tr>
<tr><td><code id="db_vs_indexes_create_+3A_spec">spec</code></td>
<td>
<p>Either <code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec()</a></code> or <code><a href="#topic+direct_access_index_spec">direct_access_index_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_create_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_create_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_create_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_indexes_delete'>Delete a Vector Search Index</h2><span id='topic+db_vs_indexes_delete'></span>

<h3>Description</h3>

<p>Delete a Vector Search Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_indexes_delete(
  index,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_indexes_delete_+3A_index">index</code></td>
<td>
<p>Name of vector search index</p>
</td></tr>
<tr><td><code id="db_vs_indexes_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_indexes_delete_data'>Delete Data from a Vector Search Index</h2><span id='topic+db_vs_indexes_delete_data'></span>

<h3>Description</h3>

<p>Delete Data from a Vector Search Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_indexes_delete_data(
  index,
  primary_keys,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_indexes_delete_data_+3A_index">index</code></td>
<td>
<p>Name of vector search index</p>
</td></tr>
<tr><td><code id="db_vs_indexes_delete_data_+3A_primary_keys">primary_keys</code></td>
<td>
<p>primary keys to be deleted from index</p>
</td></tr>
<tr><td><code id="db_vs_indexes_delete_data_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_delete_data_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_delete_data_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_indexes_get'>Get a Vector Search Index</h2><span id='topic+db_vs_indexes_get'></span>

<h3>Description</h3>

<p>Get a Vector Search Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_indexes_get(
  index,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_indexes_get_+3A_index">index</code></td>
<td>
<p>Name of vector search index</p>
</td></tr>
<tr><td><code id="db_vs_indexes_get_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_get_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_get_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_indexes_list'>List Vector Search Indexes</h2><span id='topic+db_vs_indexes_list'></span>

<h3>Description</h3>

<p>List Vector Search Indexes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_indexes_list(
  endpoint,
  page_token = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_indexes_list_+3A_endpoint">endpoint</code></td>
<td>
<p>Name of vector search endpoint</p>
</td></tr>
<tr><td><code id="db_vs_indexes_list_+3A_page_token">page_token</code></td>
<td>
<p><code>page_token</code> returned from prior query</p>
</td></tr>
<tr><td><code id="db_vs_indexes_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_indexes_query'>Query a Vector Search Index</h2><span id='topic+db_vs_indexes_query'></span>

<h3>Description</h3>

<p>Query a Vector Search Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_indexes_query(
  index,
  columns,
  filters_json,
  query_vector = NULL,
  query_text = NULL,
  score_threshold = 0,
  query_type = c("ANN", "HYBRID"),
  num_results = 10,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_indexes_query_+3A_index">index</code></td>
<td>
<p>Name of vector search index</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_+3A_columns">columns</code></td>
<td>
<p>Column names to include in response</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_+3A_filters_json">filters_json</code></td>
<td>
<p>JSON string representing query filters, see details.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_+3A_query_vector">query_vector</code></td>
<td>
<p>Numeric vector. Required for direct vector access index
and delta sync index using self managed vectors.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_+3A_query_text">query_text</code></td>
<td>
<p>Required for delta sync index using model endpoint.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_+3A_score_threshold">score_threshold</code></td>
<td>
<p>Numeric score threshold for the approximate nearest
neighbour (ANN) search. Defaults to 0.0.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_+3A_query_type">query_type</code></td>
<td>
<p>One of <code>ANN</code> (default) or <code>HYBRID</code></p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_+3A_num_results">num_results</code></td>
<td>
<p>Number of returns to return (default: 10).</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You cannot specify both <code>query_vector</code> and <code>query_text</code> at the same time.
</p>
<p><code>filter_jsons</code> examples:
</p>

<ul>
<li> <p><code>'{"id &lt;": 5}'</code>: Filter for id less than 5
</p>
</li>
<li> <p><code>'{"id &gt;": 5}'</code>: Filter for id greater than 5
</p>
</li>
<li> <p><code>'{"id &lt;=": 5}'</code>: Filter for id less than equal to 5
</p>
</li>
<li> <p><code>'{"id &gt;=": 5}'</code>: Filter for id greater than equal to 5
</p>
</li>
<li> <p><code>'{"id": 5}'</code>: Filter for id equal to 5
</p>
</li>
<li> <p><code>'{"id": 5, "age &gt;=": 18}'</code>: Filter for id equal to 5 and age greater than
equal to 18
</p>
</li></ul>

<p><code>filter_jsons</code> will convert attempt to use <code>jsonlite::toJSON</code> on any
non character vectors.
</p>
<p>Refer to docs for <a href="https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#use-filters-on-queries">Vector Search</a>.
</p>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
db_vs_indexes_sync(
  index = "myindex",
  columns = c("id", "text"),
  query_vector = c(1, 2, 3)
)

## End(Not run)

</code></pre>

<hr>
<h2 id='db_vs_indexes_query_next_page'>Query Vector Search Next Page</h2><span id='topic+db_vs_indexes_query_next_page'></span>

<h3>Description</h3>

<p>Query Vector Search Next Page
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_indexes_query_next_page(
  index,
  endpoint,
  page_token = NULL,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_indexes_query_next_page_+3A_index">index</code></td>
<td>
<p>Name of vector search index</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_next_page_+3A_endpoint">endpoint</code></td>
<td>
<p>Name of vector search endpoint</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_next_page_+3A_page_token">page_token</code></td>
<td>
<p><code>page_token</code> returned from prior query</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_next_page_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_next_page_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_query_next_page_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_indexes_scan'>Scan a Vector Search Index</h2><span id='topic+db_vs_indexes_scan'></span>

<h3>Description</h3>

<p>Scan a Vector Search Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_indexes_scan(
  endpoint,
  index,
  last_primary_key,
  num_results = 10,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_indexes_scan_+3A_endpoint">endpoint</code></td>
<td>
<p>Name of vector search endpoint to scan</p>
</td></tr>
<tr><td><code id="db_vs_indexes_scan_+3A_index">index</code></td>
<td>
<p>Name of vector search index to scan</p>
</td></tr>
<tr><td><code id="db_vs_indexes_scan_+3A_last_primary_key">last_primary_key</code></td>
<td>
<p>Primary key of the last entry returned in previous
scan</p>
</td></tr>
<tr><td><code id="db_vs_indexes_scan_+3A_num_results">num_results</code></td>
<td>
<p>Number of returns to return (default: 10)</p>
</td></tr>
<tr><td><code id="db_vs_indexes_scan_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_scan_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_scan_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Scan the specified vector index and return the first <code>num_results</code> entries
after the exclusive <code>primary_key</code>.
</p>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_indexes_sync'>Synchronize a Vector Search Index</h2><span id='topic+db_vs_indexes_sync'></span>

<h3>Description</h3>

<p>Synchronize a Vector Search Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_indexes_sync(
  index,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_indexes_sync_+3A_index">index</code></td>
<td>
<p>Name of vector search index</p>
</td></tr>
<tr><td><code id="db_vs_indexes_sync_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_sync_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_sync_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Triggers a synchronization process for a specified vector index. The index
must be a 'Delta Sync' index.
</p>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_vs_indexes_upsert_data'>Upsert Data into a Vector Search Index</h2><span id='topic+db_vs_indexes_upsert_data'></span>

<h3>Description</h3>

<p>Upsert Data into a Vector Search Index
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_vs_indexes_upsert_data(
  index,
  df,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_vs_indexes_upsert_data_+3A_index">index</code></td>
<td>
<p>Name of vector search index</p>
</td></tr>
<tr><td><code id="db_vs_indexes_upsert_data_+3A_df">df</code></td>
<td>
<p>data.frame containing data to upsert</p>
</td></tr>
<tr><td><code id="db_vs_indexes_upsert_data_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_upsert_data_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_vs_indexes_upsert_data_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='db_workspace_delete'>Delete Object/Directory (Workspaces)</h2><span id='topic+db_workspace_delete'></span>

<h3>Description</h3>

<p>Delete Object/Directory (Workspaces)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_workspace_delete(
  path,
  recursive = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_workspace_delete_+3A_path">path</code></td>
<td>
<p>Absolute path of the notebook or directory.</p>
</td></tr>
<tr><td><code id="db_workspace_delete_+3A_recursive">recursive</code></td>
<td>
<p>Flag that specifies whether to delete the object
recursively. <code>False</code> by default.</p>
</td></tr>
<tr><td><code id="db_workspace_delete_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_delete_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_delete_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Delete an object or a directory (and optionally recursively deletes all
objects in the directory). If path does not exist, this call returns an error
<code>RESOURCE_DOES_NOT_EXIST</code>. If path is a non-empty directory and recursive is
set to false, this call returns an error <code>DIRECTORY_NOT_EMPTY.</code>
</p>
<p>Object deletion cannot be undone and deleting a directory recursively is not
atomic.
</p>


<h3>See Also</h3>

<p>Other Workspace API: 
<code><a href="#topic+db_workspace_export">db_workspace_export</a>()</code>,
<code><a href="#topic+db_workspace_get_status">db_workspace_get_status</a>()</code>,
<code><a href="#topic+db_workspace_import">db_workspace_import</a>()</code>,
<code><a href="#topic+db_workspace_list">db_workspace_list</a>()</code>,
<code><a href="#topic+db_workspace_mkdirs">db_workspace_mkdirs</a>()</code>
</p>

<hr>
<h2 id='db_workspace_export'>Export Notebook or Directory (Workspaces)</h2><span id='topic+db_workspace_export'></span>

<h3>Description</h3>

<p>Export Notebook or Directory (Workspaces)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_workspace_export(
  path,
  format = c("AUTO", "SOURCE", "HTML", "JUPYTER", "DBC", "R_MARKDOWN"),
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_workspace_export_+3A_path">path</code></td>
<td>
<p>Absolute path of the notebook or directory.</p>
</td></tr>
<tr><td><code id="db_workspace_export_+3A_format">format</code></td>
<td>
<p>One of <code>AUTO</code>, <code>SOURCE</code>, <code>HTML</code>, <code>JUPYTER</code>, <code>DBC</code>, <code>R_MARKDOWN</code>.
Default is <code>SOURCE</code>.</p>
</td></tr>
<tr><td><code id="db_workspace_export_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_export_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_export_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Export a notebook or contents of an entire directory. If path does not exist,
this call returns an error <code>RESOURCE_DOES_NOT_EXIST.</code>
</p>
<p>You can export a directory only in <code>DBC</code> format. If the exported data exceeds
the size limit, this call returns an error <code>MAX_NOTEBOOK_SIZE_EXCEEDED.</code> This
API does not support exporting a library.
</p>
<p>At this time we do not support the <code>direct_download</code> parameter and returns a
base64 encoded string.
</p>
<p><a href="https://docs.databricks.com/dev-tools/api/latest/workspace.html#export">See More</a>.
</p>


<h3>Value</h3>

<p>base64 encoded string
</p>


<h3>See Also</h3>

<p>Other Workspace API: 
<code><a href="#topic+db_workspace_delete">db_workspace_delete</a>()</code>,
<code><a href="#topic+db_workspace_get_status">db_workspace_get_status</a>()</code>,
<code><a href="#topic+db_workspace_import">db_workspace_import</a>()</code>,
<code><a href="#topic+db_workspace_list">db_workspace_list</a>()</code>,
<code><a href="#topic+db_workspace_mkdirs">db_workspace_mkdirs</a>()</code>
</p>

<hr>
<h2 id='db_workspace_get_status'>Get Object Status (Workspaces)</h2><span id='topic+db_workspace_get_status'></span>

<h3>Description</h3>

<p>Gets the status of an object or a directory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_workspace_get_status(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_workspace_get_status_+3A_path">path</code></td>
<td>
<p>Absolute path of the notebook or directory.</p>
</td></tr>
<tr><td><code id="db_workspace_get_status_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_get_status_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_get_status_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If path does not exist, this call returns an error <code>RESOURCE_DOES_NOT_EXIST.</code>
</p>


<h3>See Also</h3>

<p>Other Workspace API: 
<code><a href="#topic+db_workspace_delete">db_workspace_delete</a>()</code>,
<code><a href="#topic+db_workspace_export">db_workspace_export</a>()</code>,
<code><a href="#topic+db_workspace_import">db_workspace_import</a>()</code>,
<code><a href="#topic+db_workspace_list">db_workspace_list</a>()</code>,
<code><a href="#topic+db_workspace_mkdirs">db_workspace_mkdirs</a>()</code>
</p>

<hr>
<h2 id='db_workspace_import'>Import Notebook/Directory (Workspaces)</h2><span id='topic+db_workspace_import'></span>

<h3>Description</h3>

<p>Import a notebook or the contents of an entire directory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_workspace_import(
  path,
  file = NULL,
  content = NULL,
  format = c("AUTO", "SOURCE", "HTML", "JUPYTER", "DBC", "R_MARKDOWN"),
  language = NULL,
  overwrite = FALSE,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_workspace_import_+3A_path">path</code></td>
<td>
<p>Absolute path of the notebook or directory.</p>
</td></tr>
<tr><td><code id="db_workspace_import_+3A_file">file</code></td>
<td>
<p>Path of local file to upload. See <code>formats</code> parameter.</p>
</td></tr>
<tr><td><code id="db_workspace_import_+3A_content">content</code></td>
<td>
<p>Content to upload, this will be base64-encoded and has a limit
of 10MB.</p>
</td></tr>
<tr><td><code id="db_workspace_import_+3A_format">format</code></td>
<td>
<p>One of <code>AUTO</code>, <code>SOURCE</code>, <code>HTML</code>, <code>JUPYTER</code>, <code>DBC</code>, <code>R_MARKDOWN</code>.
Default is <code>SOURCE</code>.</p>
</td></tr>
<tr><td><code id="db_workspace_import_+3A_language">language</code></td>
<td>
<p>One of <code>R</code>, <code>PYTHON</code>, <code>SCALA</code>, <code>SQL</code>. Required when <code>format</code>
is <code>SOURCE</code> otherwise ignored.</p>
</td></tr>
<tr><td><code id="db_workspace_import_+3A_overwrite">overwrite</code></td>
<td>
<p>Flag that specifies whether to overwrite existing object.
<code>FALSE</code> by default. For <code>DBC</code> overwrite is not supported since it may contain
a directory.</p>
</td></tr>
<tr><td><code id="db_workspace_import_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_import_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_import_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>file</code> and <code>content</code> are mutually exclusive. If both are specified <code>content</code>
will be ignored.
</p>
<p>If path already exists and <code>overwrite</code> is set to <code>FALSE</code>, this call returns
an error <code>RESOURCE_ALREADY_EXISTS.</code> You can use only <code>DBC</code> format to import
a directory.
</p>


<h3>See Also</h3>

<p>Other Workspace API: 
<code><a href="#topic+db_workspace_delete">db_workspace_delete</a>()</code>,
<code><a href="#topic+db_workspace_export">db_workspace_export</a>()</code>,
<code><a href="#topic+db_workspace_get_status">db_workspace_get_status</a>()</code>,
<code><a href="#topic+db_workspace_list">db_workspace_list</a>()</code>,
<code><a href="#topic+db_workspace_mkdirs">db_workspace_mkdirs</a>()</code>
</p>

<hr>
<h2 id='db_workspace_list'>List Directory Contents (Workspaces)</h2><span id='topic+db_workspace_list'></span>

<h3>Description</h3>

<p>List Directory Contents (Workspaces)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_workspace_list(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_workspace_list_+3A_path">path</code></td>
<td>
<p>Absolute path of the notebook or directory.</p>
</td></tr>
<tr><td><code id="db_workspace_list_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_list_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_list_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>List the contents of a directory, or the object if it is not a directory.
If the input path does not exist, this call returns an error
<code>RESOURCE_DOES_NOT_EXIST.</code>
</p>


<h3>See Also</h3>

<p>Other Workspace API: 
<code><a href="#topic+db_workspace_delete">db_workspace_delete</a>()</code>,
<code><a href="#topic+db_workspace_export">db_workspace_export</a>()</code>,
<code><a href="#topic+db_workspace_get_status">db_workspace_get_status</a>()</code>,
<code><a href="#topic+db_workspace_import">db_workspace_import</a>()</code>,
<code><a href="#topic+db_workspace_mkdirs">db_workspace_mkdirs</a>()</code>
</p>

<hr>
<h2 id='db_workspace_mkdirs'>Make a Directory (Workspaces)</h2><span id='topic+db_workspace_mkdirs'></span>

<h3>Description</h3>

<p>Make a Directory (Workspaces)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_workspace_mkdirs(
  path,
  host = db_host(),
  token = db_token(),
  perform_request = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_workspace_mkdirs_+3A_path">path</code></td>
<td>
<p>Absolute path of the directory.</p>
</td></tr>
<tr><td><code id="db_workspace_mkdirs_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_mkdirs_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="db_workspace_mkdirs_+3A_perform_request">perform_request</code></td>
<td>
<p>If <code>TRUE</code> (default) the request is performed, if
<code>FALSE</code> the httr2 request is returned <em>without</em> being performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create the given directory and necessary parent directories if they do not
exists. If there exists an object (not a directory) at any prefix of the
input path, this call returns an error <code>RESOURCE_ALREADY_EXISTS.</code> If this
operation fails it may have succeeded in creating some of the necessary
parent directories.
</p>


<h3>See Also</h3>

<p>Other Workspace API: 
<code><a href="#topic+db_workspace_delete">db_workspace_delete</a>()</code>,
<code><a href="#topic+db_workspace_export">db_workspace_export</a>()</code>,
<code><a href="#topic+db_workspace_get_status">db_workspace_get_status</a>()</code>,
<code><a href="#topic+db_workspace_import">db_workspace_import</a>()</code>,
<code><a href="#topic+db_workspace_list">db_workspace_list</a>()</code>
</p>

<hr>
<h2 id='db_wsid'>Fetch Databricks Workspace ID</h2><span id='topic+db_wsid'></span>

<h3>Description</h3>

<p>Workspace ID, optionally specified to make connections pane more powerful.
Specified as an environment variable <code>DATABRICKS_WSID</code>.
<code>.databrickscfg</code> will be searched if <code>db_profile</code> and <code>use_databrickscfg</code> are set or
if Posit Workbench managed OAuth credentials are detected.
</p>
<p>Refer to <a href="https://docs.databricks.com/dev-tools/api/latest/authentication.html">api authentication docs</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db_wsid(profile = default_config_profile())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="db_wsid_+3A_profile">profile</code></td>
<td>
<p>Profile to use when fetching from environment variable
(e.g. <code>.Renviron</code>) or <code>.databricksfg</code> file</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The behaviour is subject to change depending if <code>db_profile</code> and
<code>use_databrickscfg</code> options are set.
</p>

<ul>
<li> <p><code>use_databrickscfg</code>: Boolean (default: <code>FALSE</code>), determines if credentials
are fetched from profile of <code>.databrickscfg</code> or <code>.Renviron</code>
</p>
</li>
<li> <p><code>db_profile</code>: String (default: <code>NULL</code>), determines profile used.
<code>.databrickscfg</code> will automatically be used when Posit Workbench managed OAuth credentials are detected.
</p>
</li></ul>

<p>See vignette on authentication for more details.
</p>


<h3>Value</h3>

<p>databricks workspace ID
</p>


<h3>See Also</h3>

<p>Other Databricks Authentication Helpers: 
<code><a href="#topic+db_host">db_host</a>()</code>,
<code><a href="#topic+db_read_netrc">db_read_netrc</a>()</code>,
<code><a href="#topic+db_token">db_token</a>()</code>
</p>

<hr>
<h2 id='dbfs_storage_info'>DBFS Storage Information</h2><span id='topic+dbfs_storage_info'></span>

<h3>Description</h3>

<p>DBFS Storage Information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dbfs_storage_info(destination)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dbfs_storage_info_+3A_destination">destination</code></td>
<td>
<p>DBFS destination. Example: <code style="white-space: pre;">&#8288;dbfs:/my/path&#8288;</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+cluster_log_conf">cluster_log_conf()</a></code>, <code><a href="#topic+init_script_info">init_script_info()</a></code>
</p>
<p>Other Cluster Log Configuration Objects: 
<code><a href="#topic+cluster_log_conf">cluster_log_conf</a>()</code>,
<code><a href="#topic+s3_storage_info">s3_storage_info</a>()</code>
</p>
<p>Other Init Script Info Objects: 
<code><a href="#topic+file_storage_info">file_storage_info</a>()</code>,
<code><a href="#topic+s3_storage_info">s3_storage_info</a>()</code>
</p>

<hr>
<h2 id='default_config_profile'>Returns the default config profile</h2><span id='topic+default_config_profile'></span>

<h3>Description</h3>

<p>Returns the default config profile
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_config_profile()
</code></pre>


<h3>Details</h3>

<p>Returns the config profile first looking at <code>DATABRICKS_CONFIG_PROFILE</code>
and then the <code>db_profile</code> option.
</p>


<h3>Value</h3>

<p>profile name
</p>

<hr>
<h2 id='delta_sync_index_spec'>Delta Sync Vector Search Index Specification</h2><span id='topic+delta_sync_index_spec'></span>

<h3>Description</h3>

<p>Delta Sync Vector Search Index Specification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delta_sync_index_spec(
  source_table,
  embedding_writeback_table = NULL,
  embedding_source_columns = NULL,
  embedding_vector_columns = NULL,
  pipeline_type = c("TRIGGERED", "CONTINUOUS")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delta_sync_index_spec_+3A_source_table">source_table</code></td>
<td>
<p>The name of the source table.</p>
</td></tr>
<tr><td><code id="delta_sync_index_spec_+3A_embedding_writeback_table">embedding_writeback_table</code></td>
<td>
<p>Name of table to sync index contents and
computed embeddings back to delta table, see details.</p>
</td></tr>
<tr><td><code id="delta_sync_index_spec_+3A_embedding_source_columns">embedding_source_columns</code></td>
<td>
<p>The columns that contain the embedding
source, must be one or list of <code><a href="#topic+embedding_source_column">embedding_source_column()</a></code></p>
</td></tr>
<tr><td><code id="delta_sync_index_spec_+3A_embedding_vector_columns">embedding_vector_columns</code></td>
<td>
<p>The columns that contain the embedding, must
be one or list of <code><a href="#topic+embedding_vector_column">embedding_vector_column()</a></code></p>
</td></tr>
<tr><td><code id="delta_sync_index_spec_+3A_pipeline_type">pipeline_type</code></td>
<td>
<p>Pipeline execution mode, see details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>pipeline_type</code> is either:
</p>

<ul>
<li> <p><code>"TRIGGERED"</code>:  If the pipeline uses the triggered execution mode, the
system stops processing after successfully refreshing the source table in
the pipeline once, ensuring the table is updated based on the data available
when the update started.
</p>
</li>
<li> <p><code>"CONTINUOUS"</code> If the pipeline uses continuous execution, the pipeline
processes new data as it arrives in the source table to keep vector index
fresh.
</p>
</li></ul>

<p>The only supported naming convention for <code>embedding_writeback_table</code> is
<code>"&lt;index_name&gt;_writeback_table"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create()</a></code>
</p>
<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='determine_brickster_venv'>Determine brickster virtualenv</h2><span id='topic+determine_brickster_venv'></span>

<h3>Description</h3>

<p>Determine brickster virtualenv
</p>


<h3>Usage</h3>

<pre><code class='language-R'>determine_brickster_venv()
</code></pre>


<h3>Details</h3>

<p>Returns <code>NULL</code> when running within Databricks,
otherwise <code>"r-brickster"</code>
</p>

<hr>
<h2 id='direct_access_index_spec'>Delta Sync Vector Search Index Specification</h2><span id='topic+direct_access_index_spec'></span>

<h3>Description</h3>

<p>Delta Sync Vector Search Index Specification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>direct_access_index_spec(
  embedding_source_columns = NULL,
  embedding_vector_columns = NULL,
  schema
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="direct_access_index_spec_+3A_embedding_source_columns">embedding_source_columns</code></td>
<td>
<p>The columns that contain the embedding
source, must be one or list of <code><a href="#topic+embedding_source_column">embedding_source_column()</a></code></p>
</td></tr>
<tr><td><code id="direct_access_index_spec_+3A_embedding_vector_columns">embedding_vector_columns</code></td>
<td>
<p>The columns that contain the embedding, must
be one or list of <code><a href="#topic+embedding_vector_column">embedding_vector_column()</a></code>
vectors.</p>
</td></tr>
<tr><td><code id="direct_access_index_spec_+3A_schema">schema</code></td>
<td>
<p>Named list, names are column names, values are types. See
details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The supported types are:
</p>

<ul>
<li> <p><code>"integer"</code>
</p>
</li>
<li> <p><code>"long"</code>
</p>
</li>
<li> <p><code>"float"</code>
</p>
</li>
<li> <p><code>"double"</code>
</p>
</li>
<li> <p><code>"boolean"</code>
</p>
</li>
<li> <p><code>"string"</code>
</p>
</li>
<li> <p><code>"date"</code>
</p>
</li>
<li> <p><code>"timestamp"</code>
</p>
</li>
<li> <p><code>"array&lt;float&gt;"</code>: supported for vector columns
</p>
</li>
<li> <p><code>"array&lt;double&gt;"</code>: supported for vector columns
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create()</a></code>
</p>
<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='docker_image'>Docker Image</h2><span id='topic+docker_image'></span>

<h3>Description</h3>

<p>Docker image connection information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>docker_image(url, username, password)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="docker_image_+3A_url">url</code></td>
<td>
<p>URL for the Docker image.</p>
</td></tr>
<tr><td><code id="docker_image_+3A_username">username</code></td>
<td>
<p>User name for the Docker repository.</p>
</td></tr>
<tr><td><code id="docker_image_+3A_password">password</code></td>
<td>
<p>Password for the Docker repository.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses basic authentication, <strong>strongly</strong> recommended that credentials are not
stored in any scripts and environment variables should be used.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+db_cluster_create">db_cluster_create()</a></code>, <code><a href="#topic+db_cluster_edit">db_cluster_edit()</a></code>
</p>

<hr>
<h2 id='email_notifications'>Email Notifications</h2><span id='topic+email_notifications'></span>

<h3>Description</h3>

<p>Email Notifications
</p>


<h3>Usage</h3>

<pre><code class='language-R'>email_notifications(
  on_start = NULL,
  on_success = NULL,
  on_failure = NULL,
  no_alert_for_skipped_runs = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="email_notifications_+3A_on_start">on_start</code></td>
<td>
<p>List of email addresses to be notified when a run begins.
If not specified on job creation, reset, or update, the list is empty, and
notifications are not sent.</p>
</td></tr>
<tr><td><code id="email_notifications_+3A_on_success">on_success</code></td>
<td>
<p>List of email addresses to be notified when a run
successfully completes. A run is considered to have completed successfully if
it ends with a <code>TERMINATED</code> <code>life_cycle_state</code> and a <code>SUCCESSFUL</code>
<code>result_state.</code> If not specified on job creation, reset, or update, the list
is empty, and notifications are not sent.</p>
</td></tr>
<tr><td><code id="email_notifications_+3A_on_failure">on_failure</code></td>
<td>
<p>List of email addresses to be notified when a run
unsuccessfully completes. A run is considered to have completed
unsuccessfully if it ends with an <code>INTERNAL_ERROR</code> <code>life_cycle_state</code> or a
<code>SKIPPED</code>, <code>FAILED</code>, or <code>TIMED_OUT</code> <code>result_state.</code> If this is not specified
on job creation, reset, or update the list is empty, and notifications are
not sent.</p>
</td></tr>
<tr><td><code id="email_notifications_+3A_no_alert_for_skipped_runs">no_alert_for_skipped_runs</code></td>
<td>
<p>If <code>TRUE</code> (default), do not send email to
recipients specified in <code>on_failure</code> if the run is skipped.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+job_task">job_task()</a></code>
</p>
<p>Other Task Objects: 
<code><a href="#topic+libraries">libraries</a>()</code>,
<code><a href="#topic+new_cluster">new_cluster</a>()</code>,
<code><a href="#topic+notebook_task">notebook_task</a>()</code>,
<code><a href="#topic+pipeline_task">pipeline_task</a>()</code>,
<code><a href="#topic+python_wheel_task">python_wheel_task</a>()</code>,
<code><a href="#topic+spark_jar_task">spark_jar_task</a>()</code>,
<code><a href="#topic+spark_python_task">spark_python_task</a>()</code>,
<code><a href="#topic+spark_submit_task">spark_submit_task</a>()</code>
</p>

<hr>
<h2 id='embedding_source_column'>Embedding Source Column</h2><span id='topic+embedding_source_column'></span>

<h3>Description</h3>

<p>Embedding Source Column
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embedding_source_column(name, model_endpoint_name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="embedding_source_column_+3A_name">name</code></td>
<td>
<p>Name of the column</p>
</td></tr>
<tr><td><code id="embedding_source_column_+3A_model_endpoint_name">model_endpoint_name</code></td>
<td>
<p>Name of the embedding model endpoint</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_vector_column">embedding_vector_column</a>()</code>
</p>

<hr>
<h2 id='embedding_vector_column'>Embedding Vector Column</h2><span id='topic+embedding_vector_column'></span>

<h3>Description</h3>

<p>Embedding Vector Column
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embedding_vector_column(name, dimension)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="embedding_vector_column_+3A_name">name</code></td>
<td>
<p>Name of the column</p>
</td></tr>
<tr><td><code id="embedding_vector_column_+3A_dimension">dimension</code></td>
<td>
<p>dimension of the embedding vector</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Vector Search API: 
<code><a href="#topic+db_vs_endpoints_create">db_vs_endpoints_create</a>()</code>,
<code><a href="#topic+db_vs_endpoints_delete">db_vs_endpoints_delete</a>()</code>,
<code><a href="#topic+db_vs_endpoints_get">db_vs_endpoints_get</a>()</code>,
<code><a href="#topic+db_vs_endpoints_list">db_vs_endpoints_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_create">db_vs_indexes_create</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete">db_vs_indexes_delete</a>()</code>,
<code><a href="#topic+db_vs_indexes_delete_data">db_vs_indexes_delete_data</a>()</code>,
<code><a href="#topic+db_vs_indexes_get">db_vs_indexes_get</a>()</code>,
<code><a href="#topic+db_vs_indexes_list">db_vs_indexes_list</a>()</code>,
<code><a href="#topic+db_vs_indexes_query">db_vs_indexes_query</a>()</code>,
<code><a href="#topic+db_vs_indexes_query_next_page">db_vs_indexes_query_next_page</a>()</code>,
<code><a href="#topic+db_vs_indexes_scan">db_vs_indexes_scan</a>()</code>,
<code><a href="#topic+db_vs_indexes_sync">db_vs_indexes_sync</a>()</code>,
<code><a href="#topic+db_vs_indexes_upsert_data">db_vs_indexes_upsert_data</a>()</code>,
<code><a href="#topic+delta_sync_index_spec">delta_sync_index_spec</a>()</code>,
<code><a href="#topic+direct_access_index_spec">direct_access_index_spec</a>()</code>,
<code><a href="#topic+embedding_source_column">embedding_source_column</a>()</code>
</p>

<hr>
<h2 id='file_storage_info'>File Storage Information</h2><span id='topic+file_storage_info'></span>

<h3>Description</h3>

<p>File Storage Information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>file_storage_info(destination)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="file_storage_info_+3A_destination">destination</code></td>
<td>
<p>File destination. Example: <code style="white-space: pre;">&#8288;file:/my/file.sh&#8288;</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The file storage type is only available for clusters set up using Databricks
Container Services.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+init_script_info">init_script_info()</a></code>
</p>
<p>Other Init Script Info Objects: 
<code><a href="#topic+dbfs_storage_info">dbfs_storage_info</a>()</code>,
<code><a href="#topic+s3_storage_info">s3_storage_info</a>()</code>
</p>

<hr>
<h2 id='gcp_attributes'>GCP Attributes</h2><span id='topic+gcp_attributes'></span>

<h3>Description</h3>

<p>GCP Attributes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gcp_attributes(use_preemptible_executors = TRUE, google_service_account = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gcp_attributes_+3A_use_preemptible_executors">use_preemptible_executors</code></td>
<td>
<p>Boolean (Default: <code>TRUE</code>). If <code>TRUE</code> Uses
preemptible executors</p>
</td></tr>
<tr><td><code id="gcp_attributes_+3A_google_service_account">google_service_account</code></td>
<td>
<p>Google service account email address that the
cluster uses to authenticate with Google Identity. This field is used for
authentication with the GCS and BigQuery data sources.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For use with GCS and BigQuery, your Google service account that you use to
access the data source must be in the same project as the SA that you
specified when setting up your Databricks account.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+db_cluster_create">db_cluster_create()</a></code>, <code><a href="#topic+db_cluster_edit">db_cluster_edit()</a></code>
</p>
<p>Other Cloud Attributes: 
<code><a href="#topic+aws_attributes">aws_attributes</a>()</code>,
<code><a href="#topic+azure_attributes">azure_attributes</a>()</code>
</p>

<hr>
<h2 id='get_and_start_cluster'>Get and Start Cluster</h2><span id='topic+get_and_start_cluster'></span>

<h3>Description</h3>

<p>Get and Start Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_and_start_cluster(
  cluster_id,
  polling_interval = 5,
  host = db_host(),
  token = db_token(),
  silent = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_and_start_cluster_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Canonical identifier for the cluster.</p>
</td></tr>
<tr><td><code id="get_and_start_cluster_+3A_polling_interval">polling_interval</code></td>
<td>
<p>Number of seconds to wait between status checks</p>
</td></tr>
<tr><td><code id="get_and_start_cluster_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="get_and_start_cluster_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="get_and_start_cluster_+3A_silent">silent</code></td>
<td>
<p>Boolean (default: <code>FALSE</code>), will emit cluster state progress
if <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Get information regarding a Databricks cluster. If the cluster is
inactive it will be started and wait until the cluster is active.
</p>


<h3>Value</h3>

<p><code>db_cluster_get()</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+db_cluster_get">db_cluster_get()</a></code> and <code><a href="#topic+db_cluster_start">db_cluster_start()</a></code>.
</p>
<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>
<p>Other Cluster Helpers: 
<code><a href="#topic+get_latest_dbr">get_latest_dbr</a>()</code>
</p>

<hr>
<h2 id='get_and_start_warehouse'>Get and Start Warehouse</h2><span id='topic+get_and_start_warehouse'></span>

<h3>Description</h3>

<p>Get and Start Warehouse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_and_start_warehouse(
  id,
  polling_interval = 5,
  host = db_host(),
  token = db_token()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_and_start_warehouse_+3A_id">id</code></td>
<td>
<p>ID of the SQL warehouse.</p>
</td></tr>
<tr><td><code id="get_and_start_warehouse_+3A_polling_interval">polling_interval</code></td>
<td>
<p>Number of seconds to wait between status checks</p>
</td></tr>
<tr><td><code id="get_and_start_warehouse_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="get_and_start_warehouse_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Get information regarding a Databricks cluster. If the cluster is
inactive it will be started and wait until the cluster is active.
</p>


<h3>Value</h3>

<p><code>db_sql_warehouse_get()</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+db_sql_warehouse_get">db_sql_warehouse_get()</a></code> and <code><a href="#topic+db_sql_warehouse_start">db_sql_warehouse_start()</a></code>.
</p>
<p>Other Warehouse API: 
<code><a href="#topic+db_sql_global_warehouse_get">db_sql_global_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_create">db_sql_warehouse_create</a>()</code>,
<code><a href="#topic+db_sql_warehouse_delete">db_sql_warehouse_delete</a>()</code>,
<code><a href="#topic+db_sql_warehouse_edit">db_sql_warehouse_edit</a>()</code>,
<code><a href="#topic+db_sql_warehouse_get">db_sql_warehouse_get</a>()</code>,
<code><a href="#topic+db_sql_warehouse_list">db_sql_warehouse_list</a>()</code>,
<code><a href="#topic+db_sql_warehouse_start">db_sql_warehouse_start</a>()</code>,
<code><a href="#topic+db_sql_warehouse_stop">db_sql_warehouse_stop</a>()</code>
</p>

<hr>
<h2 id='get_latest_dbr'>Get Latest Databricks Runtime (DBR)</h2><span id='topic+get_latest_dbr'></span>

<h3>Description</h3>

<p>Get Latest Databricks Runtime (DBR)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_latest_dbr(lts, ml, gpu, photon, host = db_host(), token = db_token())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_latest_dbr_+3A_lts">lts</code></td>
<td>
<p>Boolean, if <code>TRUE</code> returns only LTS runtimes</p>
</td></tr>
<tr><td><code id="get_latest_dbr_+3A_ml">ml</code></td>
<td>
<p>Boolean, if <code>TRUE</code> returns only ML runtimes</p>
</td></tr>
<tr><td><code id="get_latest_dbr_+3A_gpu">gpu</code></td>
<td>
<p>Boolean, if <code>TRUE</code> returns only ML GPU runtimes</p>
</td></tr>
<tr><td><code id="get_latest_dbr_+3A_photon">photon</code></td>
<td>
<p>Boolean, if <code>TRUE</code> returns only photon runtimes</p>
</td></tr>
<tr><td><code id="get_latest_dbr_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="get_latest_dbr_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are runtime combinations that are not possible, such as GPU/ML and
photon. This function does not permit invalid combinations.
</p>


<h3>Value</h3>

<p>Named list
</p>


<h3>See Also</h3>

<p>Other Clusters API: 
<code><a href="#topic+db_cluster_create">db_cluster_create</a>()</code>,
<code><a href="#topic+db_cluster_edit">db_cluster_edit</a>()</code>,
<code><a href="#topic+db_cluster_events">db_cluster_events</a>()</code>,
<code><a href="#topic+db_cluster_get">db_cluster_get</a>()</code>,
<code><a href="#topic+db_cluster_list">db_cluster_list</a>()</code>,
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types</a>()</code>,
<code><a href="#topic+db_cluster_list_zones">db_cluster_list_zones</a>()</code>,
<code><a href="#topic+db_cluster_perm_delete">db_cluster_perm_delete</a>()</code>,
<code><a href="#topic+db_cluster_pin">db_cluster_pin</a>()</code>,
<code><a href="#topic+db_cluster_resize">db_cluster_resize</a>()</code>,
<code><a href="#topic+db_cluster_restart">db_cluster_restart</a>()</code>,
<code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions</a>()</code>,
<code><a href="#topic+db_cluster_start">db_cluster_start</a>()</code>,
<code><a href="#topic+db_cluster_terminate">db_cluster_terminate</a>()</code>,
<code><a href="#topic+db_cluster_unpin">db_cluster_unpin</a>()</code>,
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>
</p>
<p>Other Cluster Helpers: 
<code><a href="#topic+get_and_start_cluster">get_and_start_cluster</a>()</code>
</p>

<hr>
<h2 id='git_source'>Git Source for Job Notebook Tasks</h2><span id='topic+git_source'></span>

<h3>Description</h3>

<p>Git Source for Job Notebook Tasks
</p>


<h3>Usage</h3>

<pre><code class='language-R'>git_source(
  git_url,
  git_provider,
  reference,
  type = c("branch", "tag", "commit")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="git_source_+3A_git_url">git_url</code></td>
<td>
<p>URL of the repository to be cloned by this job. The maximum
length is 300 characters.</p>
</td></tr>
<tr><td><code id="git_source_+3A_git_provider">git_provider</code></td>
<td>
<p>Unique identifier of the service used to host the Git
repository. Must be one of: <code>github</code>, <code>bitbucketcloud</code>, <code>azuredevopsservices</code>,
<code>githubenterprise</code>, <code>bitbucketserver</code>, <code>gitlab</code>, <code>gitlabenterpriseedition</code>,
<code>awscodecommit</code>.</p>
</td></tr>
<tr><td><code id="git_source_+3A_reference">reference</code></td>
<td>
<p>Branch, tag, or commit to be checked out and used by this job.</p>
</td></tr>
<tr><td><code id="git_source_+3A_type">type</code></td>
<td>
<p>Type of reference being used, one of: <code>branch</code>, <code>tag</code>, <code>commit</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='in_databricks_nb'>Detect if running within Databricks Notebook</h2><span id='topic+in_databricks_nb'></span>

<h3>Description</h3>

<p>Detect if running within Databricks Notebook
</p>


<h3>Usage</h3>

<pre><code class='language-R'>in_databricks_nb()
</code></pre>


<h3>Details</h3>

<p>R sessions on Databricks can be detected via various environment variables
and directories.
</p>


<h3>Value</h3>

<p>Boolean
</p>

<hr>
<h2 id='init_script_info'>Init Script Info</h2><span id='topic+init_script_info'></span>

<h3>Description</h3>

<p>Init Script Info
</p>


<h3>Usage</h3>

<pre><code class='language-R'>init_script_info(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="init_script_info_+3A_...">...</code></td>
<td>
<p>Accepts multiple instances <code><a href="#topic+s3_storage_info">s3_storage_info()</a></code>,
<code><a href="#topic+file_storage_info">file_storage_info()</a></code>, or <code><a href="#topic+dbfs_storage_info">dbfs_storage_info()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+file_storage_info">file_storage_info()</a></code> is only available for clusters set up using Databricks
Container Services.
</p>
<p>For instructions on using init scripts with Databricks Container Services,
see <a href="https://docs.databricks.com/clusters/custom-containers.html#containers-init-script">Use an init script</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+db_cluster_create">db_cluster_create()</a></code>, <code><a href="#topic+db_cluster_edit">db_cluster_edit()</a></code>
</p>

<hr>
<h2 id='install_db_sql_connector'>Install Databricks SQL Connector (Python)</h2><span id='topic+install_db_sql_connector'></span>

<h3>Description</h3>

<p>Install Databricks SQL Connector (Python)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_db_sql_connector(
  envname = determine_brickster_venv(),
  method = "auto",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="install_db_sql_connector_+3A_envname">envname</code></td>
<td>
<p>The name, or full path, of the environment in which Python
packages are to be installed. When <code>NULL</code> (the default), the active
environment as set by the <code>RETICULATE_PYTHON_ENV</code> variable will be used;
if that is unset, then the <code>r-reticulate</code> environment will be used.</p>
</td></tr>
<tr><td><code id="install_db_sql_connector_+3A_method">method</code></td>
<td>
<p>Installation method. By default, &quot;auto&quot; automatically finds a
method that will work in the local environment. Change the default to force
a specific installation method. Note that the &quot;virtualenv&quot; method is not
available on Windows.</p>
</td></tr>
<tr><td><code id="install_db_sql_connector_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="reticulate.html#topic+conda_install">conda_install()</a></code>
or <code><a href="reticulate.html#topic+virtualenv_install">virtualenv_install()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Installs <a href="https://github.com/databricks/databricks-sql-python"><code>databricks-sql-connector</code></a>.
Environemnt is resolved by <code><a href="#topic+determine_brickster_venv">determine_brickster_venv()</a></code> which defaults to
<code>r-brickster</code> virtualenv.
</p>
<p>When running within Databricks it will use the existing python environment.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: install_db_sql_connector()
</code></pre>

<hr>
<h2 id='is.access_control_req_group'>Test if object is of class AccessControlRequestForGroup</h2><span id='topic+is.access_control_req_group'></span>

<h3>Description</h3>

<p>Test if object is of class AccessControlRequestForGroup
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.access_control_req_group(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.access_control_req_group_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>AccessControlRequestForGroup</code>
class.
</p>

<hr>
<h2 id='is.access_control_req_user'>Test if object is of class AccessControlRequestForUser</h2><span id='topic+is.access_control_req_user'></span>

<h3>Description</h3>

<p>Test if object is of class AccessControlRequestForUser
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.access_control_req_user(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.access_control_req_user_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>AccessControlRequestForUser</code>
class.
</p>

<hr>
<h2 id='is.access_control_request'>Test if object is of class AccessControlRequest</h2><span id='topic+is.access_control_request'></span>

<h3>Description</h3>

<p>Test if object is of class AccessControlRequest
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.access_control_request(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.access_control_request_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>AccessControlRequest</code> class.
</p>

<hr>
<h2 id='is.aws_attributes'>Test if object is of class AwsAttributes</h2><span id='topic+is.aws_attributes'></span>

<h3>Description</h3>

<p>Test if object is of class AwsAttributes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.aws_attributes(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.aws_attributes_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>AwsAttributes</code> class.
</p>

<hr>
<h2 id='is.azure_attributes'>Test if object is of class AzureAttributes</h2><span id='topic+is.azure_attributes'></span>

<h3>Description</h3>

<p>Test if object is of class AzureAttributes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.azure_attributes(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.azure_attributes_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>AzureAttributes</code> class.
</p>

<hr>
<h2 id='is.cluster_autoscale'>Test if object is of class AutoScale</h2><span id='topic+is.cluster_autoscale'></span>

<h3>Description</h3>

<p>Test if object is of class AutoScale
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.cluster_autoscale(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.cluster_autoscale_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>AutoScale</code> class.
</p>

<hr>
<h2 id='is.cluster_log_conf'>Test if object is of class ClusterLogConf</h2><span id='topic+is.cluster_log_conf'></span>

<h3>Description</h3>

<p>Test if object is of class ClusterLogConf
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.cluster_log_conf(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.cluster_log_conf_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>ClusterLogConf</code> class.
</p>

<hr>
<h2 id='is.cron_schedule'>Test if object is of class CronSchedule</h2><span id='topic+is.cron_schedule'></span>

<h3>Description</h3>

<p>Test if object is of class CronSchedule
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.cron_schedule(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.cron_schedule_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>CronSchedule</code> class.
</p>

<hr>
<h2 id='is.dbfs_storage_info'>Test if object is of class DbfsStorageInfo</h2><span id='topic+is.dbfs_storage_info'></span>

<h3>Description</h3>

<p>Test if object is of class DbfsStorageInfo
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.dbfs_storage_info(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.dbfs_storage_info_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>DbfsStorageInfo</code> class.
</p>

<hr>
<h2 id='is.delta_sync_index'>Test if object is of class DeltaSyncIndex</h2><span id='topic+is.delta_sync_index'></span>

<h3>Description</h3>

<p>Test if object is of class DeltaSyncIndex
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.delta_sync_index(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.delta_sync_index_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>DeltaSyncIndex</code> class.
</p>

<hr>
<h2 id='is.direct_access_index'>Test if object is of class DirectAccessIndex</h2><span id='topic+is.direct_access_index'></span>

<h3>Description</h3>

<p>Test if object is of class DirectAccessIndex
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.direct_access_index(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.direct_access_index_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>DirectAccessIndex</code> class.
</p>

<hr>
<h2 id='is.docker_image'>Test if object is of class DockerImage</h2><span id='topic+is.docker_image'></span>

<h3>Description</h3>

<p>Test if object is of class DockerImage
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.docker_image(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.docker_image_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>DockerImage</code> class.
</p>

<hr>
<h2 id='is.email_notifications'>Test if object is of class JobEmailNotifications</h2><span id='topic+is.email_notifications'></span>

<h3>Description</h3>

<p>Test if object is of class JobEmailNotifications
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.email_notifications(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.email_notifications_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>JobEmailNotifications</code> class.
</p>

<hr>
<h2 id='is.embedding_source_column'>Test if object is of class EmbeddingSourceColumn</h2><span id='topic+is.embedding_source_column'></span>

<h3>Description</h3>

<p>Test if object is of class EmbeddingSourceColumn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.embedding_source_column(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.embedding_source_column_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>EmbeddingSourceColumn</code> class.
</p>

<hr>
<h2 id='is.embedding_vector_column'>Test if object is of class EmbeddingVectorColumn</h2><span id='topic+is.embedding_vector_column'></span>

<h3>Description</h3>

<p>Test if object is of class EmbeddingVectorColumn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.embedding_vector_column(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.embedding_vector_column_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>EmbeddingVectorColumn</code> class.
</p>

<hr>
<h2 id='is.file_storage_info'>Test if object is of class FileStorageInfo</h2><span id='topic+is.file_storage_info'></span>

<h3>Description</h3>

<p>Test if object is of class FileStorageInfo
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.file_storage_info(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.file_storage_info_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>FileStorageInfo</code> class.
</p>

<hr>
<h2 id='is.gcp_attributes'>Test if object is of class GcpAttributes</h2><span id='topic+is.gcp_attributes'></span>

<h3>Description</h3>

<p>Test if object is of class GcpAttributes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.gcp_attributes(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.gcp_attributes_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>GcpAttributes</code> class.
</p>

<hr>
<h2 id='is.git_source'>Test if object is of class GitSource</h2><span id='topic+is.git_source'></span>

<h3>Description</h3>

<p>Test if object is of class GitSource
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.git_source(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.git_source_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>GitSource</code>
class.
</p>

<hr>
<h2 id='is.init_script_info'>Test if object is of class InitScriptInfo</h2><span id='topic+is.init_script_info'></span>

<h3>Description</h3>

<p>Test if object is of class InitScriptInfo
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.init_script_info(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.init_script_info_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>InitScriptInfo</code> class.
</p>

<hr>
<h2 id='is.job_task'>Test if object is of class JobTaskSettings</h2><span id='topic+is.job_task'></span>

<h3>Description</h3>

<p>Test if object is of class JobTaskSettings
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.job_task(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.job_task_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>JobTaskSettings</code> class.
</p>

<hr>
<h2 id='is.lib_cran'>Test if object is of class CranLibrary</h2><span id='topic+is.lib_cran'></span>

<h3>Description</h3>

<p>Test if object is of class CranLibrary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.lib_cran(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.lib_cran_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>CranLibrary</code> class.
</p>

<hr>
<h2 id='is.lib_egg'>Test if object is of class EggLibrary</h2><span id='topic+is.lib_egg'></span>

<h3>Description</h3>

<p>Test if object is of class EggLibrary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.lib_egg(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.lib_egg_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>EggLibrary</code> class.
</p>

<hr>
<h2 id='is.lib_jar'>Test if object is of class JarLibrary</h2><span id='topic+is.lib_jar'></span>

<h3>Description</h3>

<p>Test if object is of class JarLibrary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.lib_jar(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.lib_jar_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>JarLibrary</code> class.
</p>

<hr>
<h2 id='is.lib_maven'>Test if object is of class MavenLibrary</h2><span id='topic+is.lib_maven'></span>

<h3>Description</h3>

<p>Test if object is of class MavenLibrary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.lib_maven(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.lib_maven_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>MavenLibrary</code> class.
</p>

<hr>
<h2 id='is.lib_pypi'>Test if object is of class PyPiLibrary</h2><span id='topic+is.lib_pypi'></span>

<h3>Description</h3>

<p>Test if object is of class PyPiLibrary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.lib_pypi(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.lib_pypi_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>PyPiLibrary</code> class.
</p>

<hr>
<h2 id='is.lib_whl'>Test if object is of class WhlLibrary</h2><span id='topic+is.lib_whl'></span>

<h3>Description</h3>

<p>Test if object is of class WhlLibrary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.lib_whl(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.lib_whl_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>WhlLibrary</code> class.
</p>

<hr>
<h2 id='is.libraries'>Test if object is of class Libraries</h2><span id='topic+is.libraries'></span>

<h3>Description</h3>

<p>Test if object is of class Libraries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.libraries(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.libraries_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>Libraries</code> class.
</p>

<hr>
<h2 id='is.library'>Test if object is of class Library</h2><span id='topic+is.library'></span>

<h3>Description</h3>

<p>Test if object is of class Library
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.library(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.library_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>Library</code> class.
</p>

<hr>
<h2 id='is.new_cluster'>Test if object is of class NewCluster</h2><span id='topic+is.new_cluster'></span>

<h3>Description</h3>

<p>Test if object is of class NewCluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.new_cluster(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.new_cluster_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>NewCluster</code> class.
</p>

<hr>
<h2 id='is.notebook_task'>Test if object is of class NotebookTask</h2><span id='topic+is.notebook_task'></span>

<h3>Description</h3>

<p>Test if object is of class NotebookTask
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.notebook_task(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.notebook_task_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>NotebookTask</code> class.
</p>

<hr>
<h2 id='is.pipeline_task'>Test if object is of class PipelineTask</h2><span id='topic+is.pipeline_task'></span>

<h3>Description</h3>

<p>Test if object is of class PipelineTask
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.pipeline_task(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.pipeline_task_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>PipelineTask</code> class.
</p>

<hr>
<h2 id='is.python_wheel_task'>Test if object is of class PythonWheelTask</h2><span id='topic+is.python_wheel_task'></span>

<h3>Description</h3>

<p>Test if object is of class PythonWheelTask
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.python_wheel_task(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.python_wheel_task_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>PythonWheelTask</code> class.
</p>

<hr>
<h2 id='is.s3_storage_info'>Test if object is of class S3StorageInfo</h2><span id='topic+is.s3_storage_info'></span>

<h3>Description</h3>

<p>Test if object is of class S3StorageInfo
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.s3_storage_info(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.s3_storage_info_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>S3StorageInfo</code> class.
</p>

<hr>
<h2 id='is.spark_jar_task'>Test if object is of class SparkJarTask</h2><span id='topic+is.spark_jar_task'></span>

<h3>Description</h3>

<p>Test if object is of class SparkJarTask
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.spark_jar_task(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.spark_jar_task_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>SparkJarTask</code> class.
</p>

<hr>
<h2 id='is.spark_python_task'>Test if object is of class SparkPythonTask</h2><span id='topic+is.spark_python_task'></span>

<h3>Description</h3>

<p>Test if object is of class SparkPythonTask
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.spark_python_task(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.spark_python_task_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>SparkPythonTask</code> class.
</p>

<hr>
<h2 id='is.spark_submit_task'>Test if object is of class SparkSubmitTask</h2><span id='topic+is.spark_submit_task'></span>

<h3>Description</h3>

<p>Test if object is of class SparkSubmitTask
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.spark_submit_task(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.spark_submit_task_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>SparkSubmitTask</code> class.
</p>

<hr>
<h2 id='is.valid_task_type'>Test if object is of class JobTask</h2><span id='topic+is.valid_task_type'></span>

<h3>Description</h3>

<p>Test if object is of class JobTask
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.valid_task_type(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.valid_task_type_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>JobTask</code> class.
</p>

<hr>
<h2 id='is.vector_search_index_spec'>Test if object is of class VectorSearchIndexSpec</h2><span id='topic+is.vector_search_index_spec'></span>

<h3>Description</h3>

<p>Test if object is of class VectorSearchIndexSpec
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.vector_search_index_spec(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.vector_search_index_spec_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>VectorSearchIndexSpec</code> class.
</p>

<hr>
<h2 id='job_task'>Job Task</h2><span id='topic+job_task'></span>

<h3>Description</h3>

<p>Job Task
</p>


<h3>Usage</h3>

<pre><code class='language-R'>job_task(
  task_key,
  description = NULL,
  depends_on = c(),
  existing_cluster_id = NULL,
  new_cluster = NULL,
  job_cluster_key = NULL,
  task,
  libraries = NULL,
  email_notifications = NULL,
  timeout_seconds = NULL,
  max_retries = 0,
  min_retry_interval_millis = 0,
  retry_on_timeout = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="job_task_+3A_task_key">task_key</code></td>
<td>
<p>A unique name for the task. This field is used to refer to
this task from other tasks. This field is required and must be unique within
its parent job. On <code><a href="#topic+db_jobs_update">db_jobs_update()</a></code> or <code><a href="#topic+db_jobs_reset">db_jobs_reset()</a></code>, this field is
used to reference the tasks to be updated or reset. The maximum length is
100 characters.</p>
</td></tr>
<tr><td><code id="job_task_+3A_description">description</code></td>
<td>
<p>An optional description for this task. The maximum length
is 4096 bytes.</p>
</td></tr>
<tr><td><code id="job_task_+3A_depends_on">depends_on</code></td>
<td>
<p>Vector of <code>task_key</code>'s specifying the dependency graph of
the task. All <code>task_key</code>'s specified in this field must complete successfully
before executing this task. This field is required when a job consists of
more than one task.</p>
</td></tr>
<tr><td><code id="job_task_+3A_existing_cluster_id">existing_cluster_id</code></td>
<td>
<p>ID of an existing cluster that is used for all
runs of this task.</p>
</td></tr>
<tr><td><code id="job_task_+3A_new_cluster">new_cluster</code></td>
<td>
<p>Instance of <code><a href="#topic+new_cluster">new_cluster()</a></code>.</p>
</td></tr>
<tr><td><code id="job_task_+3A_job_cluster_key">job_cluster_key</code></td>
<td>
<p>Task is executed reusing the cluster specified in
<code><a href="#topic+db_jobs_create">db_jobs_create()</a></code> with <code>job_clusters</code> parameter.</p>
</td></tr>
<tr><td><code id="job_task_+3A_task">task</code></td>
<td>
<p>One of <code><a href="#topic+notebook_task">notebook_task()</a></code>, <code><a href="#topic+spark_jar_task">spark_jar_task()</a></code>,
<code><a href="#topic+spark_python_task">spark_python_task()</a></code>, <code><a href="#topic+spark_submit_task">spark_submit_task()</a></code>, <code><a href="#topic+pipeline_task">pipeline_task()</a></code>,
<code><a href="#topic+python_wheel_task">python_wheel_task()</a></code>.</p>
</td></tr>
<tr><td><code id="job_task_+3A_libraries">libraries</code></td>
<td>
<p>Instance of <code><a href="#topic+libraries">libraries()</a></code>.</p>
</td></tr>
<tr><td><code id="job_task_+3A_email_notifications">email_notifications</code></td>
<td>
<p>Instance of <a href="#topic+email_notifications">email_notifications</a>.</p>
</td></tr>
<tr><td><code id="job_task_+3A_timeout_seconds">timeout_seconds</code></td>
<td>
<p>An optional timeout applied to each run of this job
task. The default behavior is to have no timeout.</p>
</td></tr>
<tr><td><code id="job_task_+3A_max_retries">max_retries</code></td>
<td>
<p>An optional maximum number of times to retry an
unsuccessful run. A run is considered to be unsuccessful if it completes with
the <code>FAILED</code> <code>result_state</code> or <code>INTERNAL_ERROR</code> <code>life_cycle_state.</code> The value
-1 means to retry indefinitely and the value 0 means to never retry. The
default behavior is to never retry.</p>
</td></tr>
<tr><td><code id="job_task_+3A_min_retry_interval_millis">min_retry_interval_millis</code></td>
<td>
<p>Optional minimal interval in milliseconds
between the start of the failed run and the subsequent retry run. The default
behavior is that unsuccessful runs are immediately retried.</p>
</td></tr>
<tr><td><code id="job_task_+3A_retry_on_timeout">retry_on_timeout</code></td>
<td>
<p>Optional policy to specify whether to retry a task
when it times out. The default behavior is to not retry on timeout.</p>
</td></tr>
</table>

<hr>
<h2 id='job_tasks'>Job Tasks</h2><span id='topic+job_tasks'></span>

<h3>Description</h3>

<p>Job Tasks
</p>


<h3>Usage</h3>

<pre><code class='language-R'>job_tasks(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="job_tasks_+3A_...">...</code></td>
<td>
<p>Multiple Instance of tasks <code><a href="#topic+job_task">job_task()</a></code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+db_jobs_create">db_jobs_create()</a></code>, <code><a href="#topic+db_jobs_reset">db_jobs_reset()</a></code>, <code><a href="#topic+db_jobs_update">db_jobs_update()</a></code>
</p>

<hr>
<h2 id='lib_cran'>Cran Library (R)</h2><span id='topic+lib_cran'></span>

<h3>Description</h3>

<p>Cran Library (R)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lib_cran(package, repo = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lib_cran_+3A_package">package</code></td>
<td>
<p>The name of the CRAN package to install.</p>
</td></tr>
<tr><td><code id="lib_cran_+3A_repo">repo</code></td>
<td>
<p>The repository where the package can be found. If not specified,
the default CRAN repo is used.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+libraries">libraries()</a></code>
</p>
<p>Other Library Objects: 
<code><a href="#topic+lib_egg">lib_egg</a>()</code>,
<code><a href="#topic+lib_jar">lib_jar</a>()</code>,
<code><a href="#topic+lib_maven">lib_maven</a>()</code>,
<code><a href="#topic+lib_pypi">lib_pypi</a>()</code>,
<code><a href="#topic+lib_whl">lib_whl</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>
</p>

<hr>
<h2 id='lib_egg'>Egg Library (Python)</h2><span id='topic+lib_egg'></span>

<h3>Description</h3>

<p>Egg Library (Python)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lib_egg(egg)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lib_egg_+3A_egg">egg</code></td>
<td>
<p>URI of the egg to be installed. DBFS and S3 URIs are supported.
For example: <code style="white-space: pre;">&#8288;dbfs:/my/egg&#8288;</code> or <code style="white-space: pre;">&#8288;s3://my-bucket/egg&#8288;</code>. If S3 is used, make sure
the cluster has read access on the library. You may need to launch the
cluster with an instance profile to access the S3 URI.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+libraries">libraries()</a></code>
</p>
<p>Other Library Objects: 
<code><a href="#topic+lib_cran">lib_cran</a>()</code>,
<code><a href="#topic+lib_jar">lib_jar</a>()</code>,
<code><a href="#topic+lib_maven">lib_maven</a>()</code>,
<code><a href="#topic+lib_pypi">lib_pypi</a>()</code>,
<code><a href="#topic+lib_whl">lib_whl</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>
</p>

<hr>
<h2 id='lib_jar'>Jar Library (Scala)</h2><span id='topic+lib_jar'></span>

<h3>Description</h3>

<p>Jar Library (Scala)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lib_jar(jar)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lib_jar_+3A_jar">jar</code></td>
<td>
<p>URI of the JAR to be installed. DBFS and S3 URIs are supported.
For example: <code style="white-space: pre;">&#8288;dbfs:/mnt/databricks/library.jar&#8288;</code> or
<code style="white-space: pre;">&#8288;s3://my-bucket/library.jar&#8288;</code>. If S3 is used, make sure the cluster has read
access on the library. You may need to launch the cluster with an instance
profile to access the S3 URI.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+libraries">libraries()</a></code>
</p>
<p>Other Library Objects: 
<code><a href="#topic+lib_cran">lib_cran</a>()</code>,
<code><a href="#topic+lib_egg">lib_egg</a>()</code>,
<code><a href="#topic+lib_maven">lib_maven</a>()</code>,
<code><a href="#topic+lib_pypi">lib_pypi</a>()</code>,
<code><a href="#topic+lib_whl">lib_whl</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>
</p>

<hr>
<h2 id='lib_maven'>Maven Library (Scala)</h2><span id='topic+lib_maven'></span>

<h3>Description</h3>

<p>Maven Library (Scala)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lib_maven(coordinates, repo = NULL, exclusions = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lib_maven_+3A_coordinates">coordinates</code></td>
<td>
<p>Gradle-style Maven coordinates. For example:
<code style="white-space: pre;">&#8288;org.jsoup:jsoup:1.7.2&#8288;</code>.</p>
</td></tr>
<tr><td><code id="lib_maven_+3A_repo">repo</code></td>
<td>
<p>Maven repo to install the Maven package from. If omitted, both
Maven Central Repository and Spark Packages are searched.</p>
</td></tr>
<tr><td><code id="lib_maven_+3A_exclusions">exclusions</code></td>
<td>
<p>List of dependencies to exclude. For example:
<code>list("slf4j:slf4j", "*:hadoop-client")</code>.
<a href="https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html">Maven dependency exclusions</a>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+libraries">libraries()</a></code>
</p>
<p>Other Library Objects: 
<code><a href="#topic+lib_cran">lib_cran</a>()</code>,
<code><a href="#topic+lib_egg">lib_egg</a>()</code>,
<code><a href="#topic+lib_jar">lib_jar</a>()</code>,
<code><a href="#topic+lib_pypi">lib_pypi</a>()</code>,
<code><a href="#topic+lib_whl">lib_whl</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>
</p>

<hr>
<h2 id='lib_pypi'>PyPi Library (Python)</h2><span id='topic+lib_pypi'></span>

<h3>Description</h3>

<p>PyPi Library (Python)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lib_pypi(package, repo = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lib_pypi_+3A_package">package</code></td>
<td>
<p>The name of the PyPI package to install. An optional exact
version specification is also supported. Examples: <code>simplejson</code> and
<code style="white-space: pre;">&#8288;simplejson==3.8.0&#8288;</code>.</p>
</td></tr>
<tr><td><code id="lib_pypi_+3A_repo">repo</code></td>
<td>
<p>The repository where the package can be found. If not specified,
the default pip index is used.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+libraries">libraries()</a></code>
</p>
<p>Other Library Objects: 
<code><a href="#topic+lib_cran">lib_cran</a>()</code>,
<code><a href="#topic+lib_egg">lib_egg</a>()</code>,
<code><a href="#topic+lib_jar">lib_jar</a>()</code>,
<code><a href="#topic+lib_maven">lib_maven</a>()</code>,
<code><a href="#topic+lib_whl">lib_whl</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>
</p>

<hr>
<h2 id='lib_whl'>Wheel Library (Python)</h2><span id='topic+lib_whl'></span>

<h3>Description</h3>

<p>Wheel Library (Python)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lib_whl(whl)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lib_whl_+3A_whl">whl</code></td>
<td>
<p>URI of the wheel or zipped wheels to be installed.
DBFS and S3 URIs are supported. For example: <code style="white-space: pre;">&#8288;dbfs:/my/whl&#8288;</code> or
<code style="white-space: pre;">&#8288;s3://my-bucket/whl&#8288;</code>. If S3 is used, make sure the cluster has read access on
the library. You may need to launch the cluster with an instance profile to
access the S3 URI. Also the wheel file name needs to use the correct
convention. If zipped wheels are to be installed, the file name suffix should
be <code>.wheelhouse.zip</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+libraries">libraries()</a></code>
</p>
<p>Other Library Objects: 
<code><a href="#topic+lib_cran">lib_cran</a>()</code>,
<code><a href="#topic+lib_egg">lib_egg</a>()</code>,
<code><a href="#topic+lib_jar">lib_jar</a>()</code>,
<code><a href="#topic+lib_maven">lib_maven</a>()</code>,
<code><a href="#topic+lib_pypi">lib_pypi</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>
</p>

<hr>
<h2 id='libraries'>Libraries</h2><span id='topic+libraries'></span>

<h3>Description</h3>

<p>Libraries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>libraries(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="libraries_+3A_...">...</code></td>
<td>
<p>Accepts multiple instances of <code><a href="#topic+lib_jar">lib_jar()</a></code>, <code><a href="#topic+lib_cran">lib_cran()</a></code>,
<code><a href="#topic+lib_maven">lib_maven()</a></code>, <code><a href="#topic+lib_pypi">lib_pypi()</a></code>, <code><a href="#topic+lib_whl">lib_whl()</a></code>, <code><a href="#topic+lib_egg">lib_egg()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Optional list of libraries to be installed on the cluster that executes the
task.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+job_task">job_task()</a></code>, <code><a href="#topic+lib_jar">lib_jar()</a></code>, <code><a href="#topic+lib_cran">lib_cran()</a></code>, <code><a href="#topic+lib_maven">lib_maven()</a></code>,
<code><a href="#topic+lib_pypi">lib_pypi()</a></code>, <code><a href="#topic+lib_whl">lib_whl()</a></code>, <code><a href="#topic+lib_egg">lib_egg()</a></code>
</p>
<p>Other Task Objects: 
<code><a href="#topic+email_notifications">email_notifications</a>()</code>,
<code><a href="#topic+new_cluster">new_cluster</a>()</code>,
<code><a href="#topic+notebook_task">notebook_task</a>()</code>,
<code><a href="#topic+pipeline_task">pipeline_task</a>()</code>,
<code><a href="#topic+python_wheel_task">python_wheel_task</a>()</code>,
<code><a href="#topic+spark_jar_task">spark_jar_task</a>()</code>,
<code><a href="#topic+spark_python_task">spark_python_task</a>()</code>,
<code><a href="#topic+spark_submit_task">spark_submit_task</a>()</code>
</p>
<p>Other Library Objects: 
<code><a href="#topic+lib_cran">lib_cran</a>()</code>,
<code><a href="#topic+lib_egg">lib_egg</a>()</code>,
<code><a href="#topic+lib_jar">lib_jar</a>()</code>,
<code><a href="#topic+lib_maven">lib_maven</a>()</code>,
<code><a href="#topic+lib_pypi">lib_pypi</a>()</code>,
<code><a href="#topic+lib_whl">lib_whl</a>()</code>
</p>

<hr>
<h2 id='new_cluster'>New Cluster</h2><span id='topic+new_cluster'></span>

<h3>Description</h3>

<p>New Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_cluster(
  num_workers,
  spark_version,
  node_type_id,
  driver_node_type_id = NULL,
  autoscale = NULL,
  cloud_attrs = NULL,
  spark_conf = NULL,
  spark_env_vars = NULL,
  custom_tags = NULL,
  ssh_public_keys = NULL,
  log_conf = NULL,
  init_scripts = NULL,
  enable_elastic_disk = TRUE,
  driver_instance_pool_id = NULL,
  instance_pool_id = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="new_cluster_+3A_num_workers">num_workers</code></td>
<td>
<p>Number of worker nodes that this cluster should have. A
cluster has one Spark driver and <code>num_workers</code> executors for a total of
<code>num_workers</code> + 1 Spark nodes.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_spark_version">spark_version</code></td>
<td>
<p>The runtime version of the cluster. You can retrieve a
list of available runtime versions by using <code><a href="#topic+db_cluster_runtime_versions">db_cluster_runtime_versions()</a></code>.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_node_type_id">node_type_id</code></td>
<td>
<p>The node type for the worker nodes.
<code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types()</a></code> can be used to see available node types.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_driver_node_type_id">driver_node_type_id</code></td>
<td>
<p>The node type of the Spark driver. This field is
optional; if unset, the driver node type will be set as the same value as
<code>node_type_id</code> defined above. <code><a href="#topic+db_cluster_list_node_types">db_cluster_list_node_types()</a></code> can be used to
see available node types.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_autoscale">autoscale</code></td>
<td>
<p>Instance of <code><a href="#topic+cluster_autoscale">cluster_autoscale()</a></code>.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_cloud_attrs">cloud_attrs</code></td>
<td>
<p>Attributes related to clusters running on specific cloud
provider. Defaults to <code><a href="#topic+aws_attributes">aws_attributes()</a></code>. Must be one of <code><a href="#topic+aws_attributes">aws_attributes()</a></code>,
<code><a href="#topic+azure_attributes">azure_attributes()</a></code>, <code><a href="#topic+gcp_attributes">gcp_attributes()</a></code>.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_spark_conf">spark_conf</code></td>
<td>
<p>Named list. An object containing a set of optional,
user-specified Spark configuration key-value pairs. You can also pass in a
string of extra JVM options to the driver and the executors via
<code>spark.driver.extraJavaOptions</code> and <code>spark.executor.extraJavaOptions</code>
respectively. E.g. <code>list("spark.speculation" = true, "spark.streaming.ui.retainedBatches" = 5)</code>.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_spark_env_vars">spark_env_vars</code></td>
<td>
<p>Named list. User-specified environment variable
key-value pairs. In order to specify an additional set of
<code>SPARK_DAEMON_JAVA_OPTS</code>, we recommend appending them to
<code style="white-space: pre;">&#8288;$SPARK_DAEMON_JAVA_OPTS&#8288;</code> as shown in the following example. This ensures
that all default Databricks managed environmental variables are included as
well. E.g. <code>{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}</code></p>
</td></tr>
<tr><td><code id="new_cluster_+3A_custom_tags">custom_tags</code></td>
<td>
<p>Named list. An object containing a set of tags for cluster
resources. Databricks tags all cluster resources with these tags in addition
to <code>default_tags</code>. Databricks allows at most 45 custom tags.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_ssh_public_keys">ssh_public_keys</code></td>
<td>
<p>List. SSH public key contents that will be added to each
Spark node in this cluster. The corresponding private keys can be used to
login with the user name ubuntu on port 2200. Up to 10 keys can be specified.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_log_conf">log_conf</code></td>
<td>
<p>Instance of <code><a href="#topic+cluster_log_conf">cluster_log_conf()</a></code>.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_init_scripts">init_scripts</code></td>
<td>
<p>Instance of <code><a href="#topic+init_script_info">init_script_info()</a></code>.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_enable_elastic_disk">enable_elastic_disk</code></td>
<td>
<p>When enabled, this cluster will dynamically
acquire additional disk space when its Spark workers are running low on
disk space.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_driver_instance_pool_id">driver_instance_pool_id</code></td>
<td>
<p>ID of the instance pool to use for the
driver node. You must also specify <code>instance_pool_id</code>. Optional.</p>
</td></tr>
<tr><td><code id="new_cluster_+3A_instance_pool_id">instance_pool_id</code></td>
<td>
<p>ID of the instance pool to use for cluster nodes. If
<code>driver_instance_pool_id</code> is present, <code>instance_pool_id</code> is used for worker
nodes only. Otherwise, it is used for both the driver and worker nodes.
Optional.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+job_task">job_task()</a></code>
</p>
<p>Other Task Objects: 
<code><a href="#topic+email_notifications">email_notifications</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>,
<code><a href="#topic+notebook_task">notebook_task</a>()</code>,
<code><a href="#topic+pipeline_task">pipeline_task</a>()</code>,
<code><a href="#topic+python_wheel_task">python_wheel_task</a>()</code>,
<code><a href="#topic+spark_jar_task">spark_jar_task</a>()</code>,
<code><a href="#topic+spark_python_task">spark_python_task</a>()</code>,
<code><a href="#topic+spark_submit_task">spark_submit_task</a>()</code>
</p>

<hr>
<h2 id='notebook_task'>Notebook Task</h2><span id='topic+notebook_task'></span>

<h3>Description</h3>

<p>Notebook Task
</p>


<h3>Usage</h3>

<pre><code class='language-R'>notebook_task(notebook_path, base_parameters = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="notebook_task_+3A_notebook_path">notebook_path</code></td>
<td>
<p>The absolute path of the notebook to be run in the
Databricks workspace. This path must begin with a slash.</p>
</td></tr>
<tr><td><code id="notebook_task_+3A_base_parameters">base_parameters</code></td>
<td>
<p>Named list of base parameters to be used for each run
of this job.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the run is initiated by a call to <code><a href="#topic+db_jobs_run_now">db_jobs_run_now()</a></code> with parameters
specified, the two parameters maps are merged. If the same key is specified
in base_parameters and in run-now, the value from run-now is used.
</p>
<p>Use Task parameter variables to set parameters containing information about
job runs.
</p>
<p>If the notebook takes a parameter that is not specified in the jobâ€™s
<code>base_parameters</code> or the run-now override parameters, the default value from
the notebook is used.
</p>
<p>Retrieve these parameters in a notebook using <code>dbutils.widgets.get</code>.
</p>


<h3>See Also</h3>

<p>Other Task Objects: 
<code><a href="#topic+email_notifications">email_notifications</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>,
<code><a href="#topic+new_cluster">new_cluster</a>()</code>,
<code><a href="#topic+pipeline_task">pipeline_task</a>()</code>,
<code><a href="#topic+python_wheel_task">python_wheel_task</a>()</code>,
<code><a href="#topic+spark_jar_task">spark_jar_task</a>()</code>,
<code><a href="#topic+spark_python_task">spark_python_task</a>()</code>,
<code><a href="#topic+spark_submit_task">spark_submit_task</a>()</code>
</p>

<hr>
<h2 id='open_workspace'>Connect to Databricks Workspace</h2><span id='topic+open_workspace'></span>

<h3>Description</h3>

<p>Connect to Databricks Workspace
</p>


<h3>Usage</h3>

<pre><code class='language-R'>open_workspace(host = db_host(), token = db_token(), name = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="open_workspace_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="open_workspace_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
<tr><td><code id="open_workspace_+3A_name">name</code></td>
<td>
<p>Desired name to assign the connection</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
open_workspace(host = db_host(), token = db_token, name = "MyWorkspace")

## End(Not run)
</code></pre>

<hr>
<h2 id='pipeline_task'>Pipeline Task</h2><span id='topic+pipeline_task'></span>

<h3>Description</h3>

<p>Pipeline Task
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pipeline_task(pipeline_id)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pipeline_task_+3A_pipeline_id">pipeline_id</code></td>
<td>
<p>The full name of the pipeline task to execute.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Task Objects: 
<code><a href="#topic+email_notifications">email_notifications</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>,
<code><a href="#topic+new_cluster">new_cluster</a>()</code>,
<code><a href="#topic+notebook_task">notebook_task</a>()</code>,
<code><a href="#topic+python_wheel_task">python_wheel_task</a>()</code>,
<code><a href="#topic+spark_jar_task">spark_jar_task</a>()</code>,
<code><a href="#topic+spark_python_task">spark_python_task</a>()</code>,
<code><a href="#topic+spark_submit_task">spark_submit_task</a>()</code>
</p>

<hr>
<h2 id='py_db_sql_connector'>Databricks SQL Connector (Python)</h2><span id='topic+py_db_sql_connector'></span>

<h3>Description</h3>

<p>Access the Databricks SQL connector from Python via
<code>{reticulate}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>py_db_sql_connector
</code></pre>


<h3>Format</h3>

<p>An object of class <code>python.builtin.module</code> (inherits from <code>python.builtin.object</code>) of length 0.
</p>


<h3>Details</h3>

<p>This requires that the connector has been installed via
<code><a href="#topic+install_db_sql_connector">install_db_sql_connector()</a></code>.
</p>
<p>For more documentation of the methods, refer to the
<a href="https://github.com/databricks/databricks-sql-python">python documentation</a>.
</p>

<hr>
<h2 id='python_wheel_task'>Python Wheel Task</h2><span id='topic+python_wheel_task'></span>

<h3>Description</h3>

<p>Python Wheel Task
</p>


<h3>Usage</h3>

<pre><code class='language-R'>python_wheel_task(package_name, entry_point = NULL, parameters = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="python_wheel_task_+3A_package_name">package_name</code></td>
<td>
<p>Name of the package to execute.</p>
</td></tr>
<tr><td><code id="python_wheel_task_+3A_entry_point">entry_point</code></td>
<td>
<p>Named entry point to use, if it does not exist in the
metadata of the package it executes the function from the package directly
using <code style="white-space: pre;">&#8288;$packageName.$entryPoint()&#8288;</code>.</p>
</td></tr>
<tr><td><code id="python_wheel_task_+3A_parameters">parameters</code></td>
<td>
<p>Command-line parameters passed to python wheel task.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Task Objects: 
<code><a href="#topic+email_notifications">email_notifications</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>,
<code><a href="#topic+new_cluster">new_cluster</a>()</code>,
<code><a href="#topic+notebook_task">notebook_task</a>()</code>,
<code><a href="#topic+pipeline_task">pipeline_task</a>()</code>,
<code><a href="#topic+spark_jar_task">spark_jar_task</a>()</code>,
<code><a href="#topic+spark_python_task">spark_python_task</a>()</code>,
<code><a href="#topic+spark_submit_task">spark_submit_task</a>()</code>
</p>

<hr>
<h2 id='read_databrickscfg'>Reads Databricks CLI Config</h2><span id='topic+read_databrickscfg'></span>

<h3>Description</h3>

<p>Reads Databricks CLI Config
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_databrickscfg(key = c("token", "host", "wsid"), profile = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_databrickscfg_+3A_key">key</code></td>
<td>
<p>The value to fetch from profile. One of <code>token</code>, <code>host</code>, or <code>wsid</code></p>
</td></tr>
<tr><td><code id="read_databrickscfg_+3A_profile">profile</code></td>
<td>
<p>Character, the name of the profile to retrieve values</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Reads <code>.databrickscfg</code> file and retrieves the values associated to
a given profile. Brickster searches for the config file in the user's home directory by default.
To see where this is you can run Sys.getenv(&quot;HOME&quot;) on unix-like operating systems,
or, Sys.getenv(&quot;USERPROFILE&quot;) on windows.
An alternate location will be used if the environment variable <code>DATABRICKS_CONFIG_FILE</code> is set.
</p>


<h3>Value</h3>

<p>named list of values associated with profile
</p>

<hr>
<h2 id='read_env_var'>Reads Environment Variables</h2><span id='topic+read_env_var'></span>

<h3>Description</h3>

<p>Reads Environment Variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_env_var(key = c("token", "host", "wsid"), profile = NULL, error = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_env_var_+3A_key">key</code></td>
<td>
<p>The value to fetch from profile. One of <code>token</code>, <code>host</code>, or <code>wsid</code></p>
</td></tr>
<tr><td><code id="read_env_var_+3A_profile">profile</code></td>
<td>
<p>Character, the name of the profile to retrieve values</p>
</td></tr>
<tr><td><code id="read_env_var_+3A_error">error</code></td>
<td>
<p>Boolean, when key isn't found should error be raised</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fetches relevant environment variables based on profile
</p>


<h3>Value</h3>

<p>named list of values associated with profile
</p>

<hr>
<h2 id='remove_lib_path'>Remove Library Path</h2><span id='topic+remove_lib_path'></span>

<h3>Description</h3>

<p>Remove Library Path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_lib_path(path, version = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="remove_lib_path_+3A_path">path</code></td>
<td>
<p>Directory to remove from <code><a href="base.html#topic+.libPaths">.libPaths()</a></code>.</p>
</td></tr>
<tr><td><code id="remove_lib_path_+3A_version">version</code></td>
<td>
<p>If <code>TRUE</code> will add the R version string to the end
of <code>path</code> before removal.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="base.html#topic+libPaths">base::.libPaths()</a></code>, <code><a href="#topic+remove_lib_path">remove_lib_path()</a></code>
</p>

<hr>
<h2 id='s3_storage_info'>S3 Storage Info</h2><span id='topic+s3_storage_info'></span>

<h3>Description</h3>

<p>S3 Storage Info
</p>


<h3>Usage</h3>

<pre><code class='language-R'>s3_storage_info(
  destination,
  region = NULL,
  endpoint = NULL,
  enable_encryption = FALSE,
  encryption_type = c("sse-s3", "sse-kms"),
  kms_key = NULL,
  canned_acl = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="s3_storage_info_+3A_destination">destination</code></td>
<td>
<p>S3 destination. For example: <code style="white-space: pre;">&#8288;s3://my-bucket/some-prefix&#8288;</code>.
You must configure the cluster with an instance profile and the instance
profile must have write access to the destination. <strong>You cannot use AWS
keys</strong>.</p>
</td></tr>
<tr><td><code id="s3_storage_info_+3A_region">region</code></td>
<td>
<p>S3 region. For example: <code>us-west-2</code>. Either region or endpoint
must be set. If both are set, endpoint is used.</p>
</td></tr>
<tr><td><code id="s3_storage_info_+3A_endpoint">endpoint</code></td>
<td>
<p>S3 endpoint. For example:
<code style="white-space: pre;">&#8288;https://s3-us-west-2.amazonaws.com&#8288;</code>. Either region or endpoint must be set.
If both are set, endpoint is used.</p>
</td></tr>
<tr><td><code id="s3_storage_info_+3A_enable_encryption">enable_encryption</code></td>
<td>
<p>Boolean (Default: <code>FALSE</code>). If <code>TRUE</code> Enable server
side encryption.</p>
</td></tr>
<tr><td><code id="s3_storage_info_+3A_encryption_type">encryption_type</code></td>
<td>
<p>Encryption type, it could be <code>sse-s3</code> or <code>sse-kms</code>. It
is used only when encryption is enabled and the default type is <code>sse-s3</code>.</p>
</td></tr>
<tr><td><code id="s3_storage_info_+3A_kms_key">kms_key</code></td>
<td>
<p>KMS key used if encryption is enabled and encryption type is
set to <code>sse-kms</code>.</p>
</td></tr>
<tr><td><code id="s3_storage_info_+3A_canned_acl">canned_acl</code></td>
<td>
<p>Set canned access control list. For example:
<code>bucket-owner-full-control</code>. If <code>canned_acl</code> is set, the cluster instance
profile must have <code>s3:PutObjectAcl</code> permission on the destination bucket and
prefix. The full list of possible canned ACLs can be found in
<a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#canned-acl">docs</a>.
By default only the object owner gets full control. If you are using cross
account role for writing data, you may want to set
<code>bucket-owner-full-control</code> to make bucket owner able to read the logs.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+cluster_log_conf">cluster_log_conf()</a></code>, <code><a href="#topic+init_script_info">init_script_info()</a></code>
</p>
<p>Other Cluster Log Configuration Objects: 
<code><a href="#topic+cluster_log_conf">cluster_log_conf</a>()</code>,
<code><a href="#topic+dbfs_storage_info">dbfs_storage_info</a>()</code>
</p>
<p>Other Init Script Info Objects: 
<code><a href="#topic+dbfs_storage_info">dbfs_storage_info</a>()</code>,
<code><a href="#topic+file_storage_info">file_storage_info</a>()</code>
</p>

<hr>
<h2 id='spark_jar_task'>Spark Jar Task</h2><span id='topic+spark_jar_task'></span>

<h3>Description</h3>

<p>Spark Jar Task
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_jar_task(main_class_name, parameters = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_jar_task_+3A_main_class_name">main_class_name</code></td>
<td>
<p>The full name of the class containing the main method
to be executed. This class must be contained in a JAR provided as a library.
The code must use <code>SparkContext.getOrCreate</code> to obtain a Spark context;
otherwise, runs of the job fail.</p>
</td></tr>
<tr><td><code id="spark_jar_task_+3A_parameters">parameters</code></td>
<td>
<p>Named list. Parameters passed to the main method. Use Task
parameter variables to set parameters containing information about job runs.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Task Objects: 
<code><a href="#topic+email_notifications">email_notifications</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>,
<code><a href="#topic+new_cluster">new_cluster</a>()</code>,
<code><a href="#topic+notebook_task">notebook_task</a>()</code>,
<code><a href="#topic+pipeline_task">pipeline_task</a>()</code>,
<code><a href="#topic+python_wheel_task">python_wheel_task</a>()</code>,
<code><a href="#topic+spark_python_task">spark_python_task</a>()</code>,
<code><a href="#topic+spark_submit_task">spark_submit_task</a>()</code>
</p>

<hr>
<h2 id='spark_python_task'>Spark Python Task</h2><span id='topic+spark_python_task'></span>

<h3>Description</h3>

<p>Spark Python Task
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_python_task(python_file, parameters = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_python_task_+3A_python_file">python_file</code></td>
<td>
<p>The URI of the Python file to be executed. DBFS and S3
paths are supported.</p>
</td></tr>
<tr><td><code id="spark_python_task_+3A_parameters">parameters</code></td>
<td>
<p>List. Command line parameters passed to the Python file.
Use Task parameter variables to set parameters containing information about
job runs.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Task Objects: 
<code><a href="#topic+email_notifications">email_notifications</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>,
<code><a href="#topic+new_cluster">new_cluster</a>()</code>,
<code><a href="#topic+notebook_task">notebook_task</a>()</code>,
<code><a href="#topic+pipeline_task">pipeline_task</a>()</code>,
<code><a href="#topic+python_wheel_task">python_wheel_task</a>()</code>,
<code><a href="#topic+spark_jar_task">spark_jar_task</a>()</code>,
<code><a href="#topic+spark_submit_task">spark_submit_task</a>()</code>
</p>

<hr>
<h2 id='spark_submit_task'>Spark Submit Task</h2><span id='topic+spark_submit_task'></span>

<h3>Description</h3>

<p>Spark Submit Task
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spark_submit_task(parameters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spark_submit_task_+3A_parameters">parameters</code></td>
<td>
<p>List. Command-line parameters passed to spark submit. Use
Task parameter variables to set parameters containing information about job runs.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Task Objects: 
<code><a href="#topic+email_notifications">email_notifications</a>()</code>,
<code><a href="#topic+libraries">libraries</a>()</code>,
<code><a href="#topic+new_cluster">new_cluster</a>()</code>,
<code><a href="#topic+notebook_task">notebook_task</a>()</code>,
<code><a href="#topic+pipeline_task">pipeline_task</a>()</code>,
<code><a href="#topic+python_wheel_task">python_wheel_task</a>()</code>,
<code><a href="#topic+spark_jar_task">spark_jar_task</a>()</code>,
<code><a href="#topic+spark_python_task">spark_python_task</a>()</code>
</p>

<hr>
<h2 id='use_databricks_cfg'>Returns whether or not to use a <code>.databrickscfg</code> file</h2><span id='topic+use_databricks_cfg'></span>

<h3>Description</h3>

<p>Returns whether or not to use a <code>.databrickscfg</code> file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>use_databricks_cfg()
</code></pre>


<h3>Details</h3>

<p>Indicates <code>.databrickscfg</code> should be used instead of environment variables when
either the <code>use_databrickscfg</code> option is set or Posit Workbench managed OAuth credentials are detected.
</p>


<h3>Value</h3>

<p>boolean
</p>

<hr>
<h2 id='wait_for_lib_installs'>Wait for Libraries to Install on Databricks Cluster</h2><span id='topic+wait_for_lib_installs'></span>

<h3>Description</h3>

<p>Wait for Libraries to Install on Databricks Cluster
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wait_for_lib_installs(
  cluster_id,
  polling_interval = 5,
  allow_failures = FALSE,
  host = db_host(),
  token = db_token()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wait_for_lib_installs_+3A_cluster_id">cluster_id</code></td>
<td>
<p>Unique identifier of a Databricks cluster.</p>
</td></tr>
<tr><td><code id="wait_for_lib_installs_+3A_polling_interval">polling_interval</code></td>
<td>
<p>Number of seconds to wait between status checks</p>
</td></tr>
<tr><td><code id="wait_for_lib_installs_+3A_allow_failures">allow_failures</code></td>
<td>
<p>If <code>FALSE</code> (default) will error if any libraries status
is <code>FAILED</code>. When <code>TRUE</code> any <code>FAILED</code> installs will be presented as a
warning.</p>
</td></tr>
<tr><td><code id="wait_for_lib_installs_+3A_host">host</code></td>
<td>
<p>Databricks workspace URL, defaults to calling <code><a href="#topic+db_host">db_host()</a></code>.</p>
</td></tr>
<tr><td><code id="wait_for_lib_installs_+3A_token">token</code></td>
<td>
<p>Databricks workspace token, defaults to calling <code><a href="#topic+db_token">db_token()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Library installs on Databricks clusters are asynchronous, this function
allows you to repeatedly check installation status of each library.
</p>
<p>Can be used to block any scripts until required dependencies are installed.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+db_libs_cluster_status">db_libs_cluster_status()</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
