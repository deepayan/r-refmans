<!DOCTYPE html><html><head><title>Help for package scrapeR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {scrapeR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#scrapeR'><p>Web Page Content Scraper</p></a></li>
<li><a href='#scrapeR_in_batches'>
<p>Batch Web Page Content Scraper</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>These Functions Fetch and Extract Text Content from Specified
Web Pages</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.8</td>
</tr>
<tr>
<td>Description:</td>
<td>The 'scrapeR' package utilizes functions that fetch and extract text content from specified web pages. It handles HTTP errors and parses HTML efficiently. The package can handle hundreds of websites at a time using the scrapeR_in_batches() command.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>httr, rvest, utils, magrittr, stringr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-21 16:39:06 UTC; mathieu.dubeau</td>
</tr>
<tr>
<td>Author:</td>
<td>Mathieu Dubeau [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mathieu Dubeau &lt;mdubeau20@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-22 00:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='scrapeR'>Web Page Content Scraper</h2><span id='topic+scrapeR'></span>

<h3>Description</h3>

<p>The <code>scrapeR</code> function fetches and extracts text content from the specified web page.
It handles HTTP errors and parses HTML efficiently.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scrapeR(url)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scrapeR_+3A_url">url</code></td>
<td>
<p>A character string specifying the URL of the web page to be scraped.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses <code>tryCatch</code> to handle potential web scraping errors. It fetches
the webpage content, checks for HTTP errors, and then parses the HTML content to extract
text. The text from different HTML nodes like headings and paragraphs is combined into a
single string.
</p>


<h3>Value</h3>

<p>A character string containing the combined text from the specified HTML nodes of the web
page. Returns <code>NA</code> if an error occurs or if the page content is not accessible.
</p>


<h3>Note</h3>

<p>This function requires the <span class="pkg">httr</span> and <span class="pkg">rvest</span> packages. Ensure that these dependencies
are installed and loaded in your R environment.
</p>


<h3>Author(s)</h3>

<p>Mathieu Dubeau, Ph.D.</p>


<h3>References</h3>

<p>Refer to the <a href="https://www.rdocumentation.org/packages/rvest">rvest package documentation</a>
for underlying HTML parsing and extraction methods.
</p>


<h3>See Also</h3>

<p><code><a href="httr.html#topic+GET">GET</a></code>, <code><a href="rvest.html#topic+read_html">read_html</a></code>, <code><a href="rvest.html#topic+html_nodes">html_nodes</a></code>,
<code><a href="rvest.html#topic+html_text">html_text</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 url &lt;- "http://www.example.com"
scraped_text &lt;- scrapeR(url)

</code></pre>

<hr>
<h2 id='scrapeR_in_batches'>
Batch Web Page Content Scraper
</h2><span id='topic+scrapeR_in_batches'></span>

<h3>Description</h3>

<p>The <code>scrapeR_in_batches</code> function processes a dataframe in batches, scraping web content from URLs in a specified column and writing the scraped content to a column in df.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scrapeR_in_batches(df, url_column, extract_contacts)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scrapeR_in_batches_+3A_df">df</code></td>
<td>

<p>A dataframe containing the URLs to be scraped.
</p>
</td></tr>
<tr><td><code id="scrapeR_in_batches_+3A_url_column">url_column</code></td>
<td>

<p>The name of the column in <code>df</code> that contains the URLs.
</p>
</td></tr>
<tr><td><code id="scrapeR_in_batches_+3A_extract_contacts">extract_contacts</code></td>
<td>

<p>A function that searches scraped content for emails and phone numbers, defaults to FALSE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function divides the input dataframe into batches of a fixed size (default: 100). For each batch, it extracts the combined text content from the web pages of the URLs in the specified column. The results are appended to the df. The function also includes a throttling mechanism to pause between batch processing, reducing the load on the server being scraped.
</p>


<h3>Value</h3>

<p>The values are returned to content column and optionally to an email and phone_number column if extract_contacts is TRUE.
</p>


<h3>Note</h3>

<p>Ensure that the <span class="pkg">httr</span>, <span class="pkg">rvest</span>, and <span class="pkg">stringr</span> packages are installed and loaded. Also, handle large datasets and output files with care to avoid memory issues.
</p>


<h3>Author(s)</h3>

<p>Mathieu Dubeau Ph.D
</p>


<h3>References</h3>

<p>Refer to <a href="https://www.rdocumentation.org/packages/rvest">rvest package documentation</a> and <a href="https://www.rdocumentation.org/packages/httr">httr package documentation</a> for underlying web scraping methods.
</p>


<h3>See Also</h3>

<p><code><a href="httr.html#topic+GET">GET</a></code>, <code><a href="rvest.html#topic+read_html">read_html</a></code>, <code><a href="rvest.html#topic+html_nodes">html_nodes</a></code>, <code><a href="rvest.html#topic+html_text">html_text</a></code>, <code><a href="utils.html#topic+write.table">write.table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  mock_scrapeR &lt;- function(url) {
    return(paste("Scraped content from", url))
  }

  df &lt;- data.frame(url = c("http://site1.com", "http://site2.com"), stringsAsFactors = FALSE)

  ## Not run: 
    scrapeR_in_batches(df, url_column = "url", extract_contacts = FALSE)
  
## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
