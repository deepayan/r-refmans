<!DOCTYPE html><html><head><title>Help for package word2vec</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {word2vec}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as.matrix.word2vec'><p>Get the word vectors of a word2vec model</p></a></li>
<li><a href='#doc2vec'><p>Get document vectors based on a word2vec model</p></a></li>
<li><a href='#predict.word2vec'><p>Predict functionalities for a word2vec model</p></a></li>
<li><a href='#read.word2vec'><p>Read a binary word2vec model from disk</p></a></li>
<li><a href='#read.wordvectors'><p>Read word vectors from a word2vec model from disk</p></a></li>
<li><a href='#txt_clean_word2vec'><p>Text cleaning specific for input to word2vec</p></a></li>
<li><a href='#word2vec'><p>Train a word2vec model on text</p></a></li>
<li><a href='#word2vec_similarity'><p>Similarity between word vectors as used in word2vec</p></a></li>
<li><a href='#word2vec.character'><p>Train a word2vec model on text</p></a></li>
<li><a href='#word2vec.list'><p>Train a word2vec model on text</p></a></li>
<li><a href='#write.word2vec'><p>Save a word2vec model to disk</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Distributed Representations of Words</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jan Wijffels &lt;jwijffels@bnosac.be&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Learn vector representations of words by continuous bag of words and skip-gram implementations of the 'word2vec' algorithm. 
    The techniques are detailed in the paper "Distributed Representations of Words and Phrases and their Compositionality" by Mikolov et al. (2013), available at &lt;<a href="https://doi.org/10.48550/arXiv.1310.4546">doi:10.48550/arXiv.1310.4546</a>&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bnosac/word2vec">https://github.com/bnosac/word2vec</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2.0)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.5), stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppProgress</td>
</tr>
<tr>
<td>Suggests:</td>
<td>udpipe</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-07 20:27:05 UTC; jwijf</td>
</tr>
<tr>
<td>Author:</td>
<td>Jan Wijffels [aut, cre, cph] (R wrapper),
  Kohei Watanabe <a href="https://orcid.org/0000-0001-6519-5265"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  BNOSAC [cph] (R wrapper),
  Max Fomichev [ctb, cph] (Code in src/word2vec)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-07 21:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as.matrix.word2vec'>Get the word vectors of a word2vec model</h2><span id='topic+as.matrix.word2vec'></span>

<h3>Description</h3>

<p>Get the word vectors of a word2vec model as a dense matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'word2vec'
as.matrix(x, encoding = "UTF-8", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.matrix.word2vec_+3A_x">x</code></td>
<td>
<p>a word2vec model as returned by <code><a href="#topic+word2vec">word2vec</a></code> or <code><a href="#topic+read.word2vec">read.word2vec</a></code></p>
</td></tr>
<tr><td><code id="as.matrix.word2vec_+3A_encoding">encoding</code></td>
<td>
<p>set the encoding of the row names to the specified encoding. Defaults to 'UTF-8'.</p>
</td></tr>
<tr><td><code id="as.matrix.word2vec_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with the word vectors where the rownames are the words from the model vocabulary
</p>


<h3>See Also</h3>

<p><code><a href="#topic+word2vec">word2vec</a></code>, <code><a href="#topic+read.word2vec">read.word2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>path  &lt;- system.file(package = "word2vec", "models", "example.bin")
model &lt;- read.word2vec(path)

embedding &lt;- as.matrix(model)
</code></pre>

<hr>
<h2 id='doc2vec'>Get document vectors based on a word2vec model</h2><span id='topic+doc2vec'></span>

<h3>Description</h3>

<p>Document vectors are the sum of the vectors of the words which are part of the document standardised by the scale of the vector space.
This scale is the sqrt of the average inner product of the vector elements.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>doc2vec(object, newdata, split = " ", encoding = "UTF-8", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="doc2vec_+3A_object">object</code></td>
<td>
<p>a word2vec model as returned by <code><a href="#topic+word2vec">word2vec</a></code> or <code><a href="#topic+read.word2vec">read.word2vec</a></code></p>
</td></tr>
<tr><td><code id="doc2vec_+3A_newdata">newdata</code></td>
<td>
<p>either a  list of tokens where each list element is a character vector of tokens which form the document and the list name is considered the document identifier; 
or a data.frame with columns doc_id and text; or a character vector with texts where the character vector names will be considered the document identifier</p>
</td></tr>
<tr><td><code id="doc2vec_+3A_split">split</code></td>
<td>
<p>in case <code>newdata</code> is not a list of tokens, text will be splitted into tokens by splitting based on function <code><a href="base.html#topic+strsplit">strsplit</a></code> with the provided <code>split</code> argument</p>
</td></tr>
<tr><td><code id="doc2vec_+3A_encoding">encoding</code></td>
<td>
<p>set the encoding of the text elements to the specified encoding. Defaults to 'UTF-8'.</p>
</td></tr>
<tr><td><code id="doc2vec_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with 1 row per document containing the text document vectors, the rownames of this matrix are the document identifiers
</p>


<h3>See Also</h3>

<p><code><a href="#topic+word2vec">word2vec</a></code>, <code><a href="#topic+predict.word2vec">predict.word2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>path  &lt;- system.file(package = "word2vec", "models", "example.bin")
model &lt;- read.word2vec(path)
x &lt;- data.frame(doc_id = c("doc1", "doc2", "testmissingdata"), 
                text = c("there is no toilet. on the bus", "no tokens from dictionary", NA),
                stringsAsFactors = FALSE)
emb &lt;- doc2vec(model, x, type = "embedding")
emb

newdoc &lt;- doc2vec(model, "i like busses with a toilet")
word2vec_similarity(emb, newdoc)

## similar way of extracting embeddings
x &lt;- setNames(object = c("there is no toilet. on the bus", "no tokens from dictionary", NA), 
              nm = c("a", "b", "c"))
emb &lt;- doc2vec(model, x, type = "embedding")
emb

## similar way of extracting embeddings
x &lt;- setNames(object = c("there is no toilet. on the bus", "no tokens from dictionary", NA), 
              nm = c("a", "b", "c"))
x &lt;- strsplit(x, "[ .]")
emb &lt;- doc2vec(model, x, type = "embedding")
emb

## show behaviour in case of NA or character data of no length
x &lt;- list(a = character(), b = c("bus", "toilet"), c = NA)
emb &lt;- doc2vec(model, x, type = "embedding")
emb
</code></pre>

<hr>
<h2 id='predict.word2vec'>Predict functionalities for a word2vec model</h2><span id='topic+predict.word2vec'></span>

<h3>Description</h3>

<p>Get either 
</p>

<ul>
<li><p>the embedding of words
</p>
</li>
<li><p>the nearest words which are similar to either a word or a word vector
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'word2vec'
predict(
  object,
  newdata,
  type = c("nearest", "embedding"),
  top_n = 10L,
  encoding = "UTF-8",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.word2vec_+3A_object">object</code></td>
<td>
<p>a word2vec model as returned by <code><a href="#topic+word2vec">word2vec</a></code> or <code><a href="#topic+read.word2vec">read.word2vec</a></code></p>
</td></tr>
<tr><td><code id="predict.word2vec_+3A_newdata">newdata</code></td>
<td>
<p>for type 'embedding', <code>newdata</code> should be a character vector of words<br />
for type 'nearest', <code>newdata</code> should be a character vector of words or a matrix in the embedding space</p>
</td></tr>
<tr><td><code id="predict.word2vec_+3A_type">type</code></td>
<td>
<p>either 'embedding' or 'nearest'. Defaults to 'nearest'.</p>
</td></tr>
<tr><td><code id="predict.word2vec_+3A_top_n">top_n</code></td>
<td>
<p>show only the top n nearest neighbours. Defaults to 10.</p>
</td></tr>
<tr><td><code id="predict.word2vec_+3A_encoding">encoding</code></td>
<td>
<p>set the encoding of the text elements to the specified encoding. Defaults to 'UTF-8'.</p>
</td></tr>
<tr><td><code id="predict.word2vec_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>depending on the type, you get a different result back:
</p>

<ul>
<li><p>for type nearest: a list of data.frames with columns term, similarity and rank indicating with words which are closest to the provided <code>newdata</code> words or word vectors. If <code>newdata</code> is just one vector instead of a matrix, it returns a data.frame
</p>
</li>
<li><p>for type embedding: a matrix of word vectors of the words provided in <code>newdata</code>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+word2vec">word2vec</a></code>, <code><a href="#topic+read.word2vec">read.word2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>path  &lt;- system.file(package = "word2vec", "models", "example.bin")
model &lt;- read.word2vec(path)
emb &lt;- predict(model, c("bus", "toilet", "unknownword"), type = "embedding")
emb
nn  &lt;- predict(model, c("bus", "toilet"), type = "nearest", top_n = 5)
nn

# Do some calculations with the vectors and find similar terms to these
emb &lt;- as.matrix(model)
vector &lt;- emb["buurt", ] - emb["rustige", ] + emb["restaurants", ]
predict(model, vector, type = "nearest", top_n = 10)

vector &lt;- emb["gastvrouw", ] - emb["gastvrij", ]
predict(model, vector, type = "nearest", top_n = 5)

vectors &lt;- emb[c("gastheer", "gastvrouw"), ]
vectors &lt;- rbind(vectors, avg = colMeans(vectors))
predict(model, vectors, type = "nearest", top_n = 10)
</code></pre>

<hr>
<h2 id='read.word2vec'>Read a binary word2vec model from disk</h2><span id='topic+read.word2vec'></span>

<h3>Description</h3>

<p>Read a binary word2vec model from disk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.word2vec(file, normalize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.word2vec_+3A_file">file</code></td>
<td>
<p>the path to the model file</p>
</td></tr>
<tr><td><code id="read.word2vec_+3A_normalize">normalize</code></td>
<td>
<p>logical indicating to normalize the embeddings by dividing by the factor (sqrt(sum(x . x) / length(x))). Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class w2v which is a list with elements
</p>

<ul>
<li><p>model: a Rcpp pointer to the model
</p>
</li>
<li><p>model_path: the path to the model on disk
</p>
</li>
<li><p>dim: the dimension of the embedding matrix
</p>
</li>
<li><p>n: the number of words in the vocabulary
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>path  &lt;- system.file(package = "word2vec", "models", "example.bin")
model &lt;- read.word2vec(path)
vocab &lt;- summary(model, type = "vocabulary")
emb &lt;- predict(model, c("bus", "naar", "unknownword"), type = "embedding")
emb
nn  &lt;- predict(model, c("bus", "toilet"), type = "nearest")
nn

# Do some calculations with the vectors and find similar terms to these
emb &lt;- as.matrix(model)
vector &lt;- emb["gastvrouw", ] - emb["gastvrij", ]
predict(model, vector, type = "nearest", top_n = 5)
vectors &lt;- emb[c("gastheer", "gastvrouw"), ]
vectors &lt;- rbind(vectors, avg = colMeans(vectors))
predict(model, vectors, type = "nearest", top_n = 10)
</code></pre>

<hr>
<h2 id='read.wordvectors'>Read word vectors from a word2vec model from disk</h2><span id='topic+read.wordvectors'></span>

<h3>Description</h3>

<p>Read word vectors from a word2vec model from disk into a dense matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.wordvectors(
  file,
  type = c("bin", "txt"),
  n = .Machine$integer.max,
  normalize = FALSE,
  encoding = "UTF-8"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.wordvectors_+3A_file">file</code></td>
<td>
<p>the path to the model file</p>
</td></tr>
<tr><td><code id="read.wordvectors_+3A_type">type</code></td>
<td>
<p>either 'bin' or 'txt' indicating the <code>file</code> is a binary file or a text file</p>
</td></tr>
<tr><td><code id="read.wordvectors_+3A_n">n</code></td>
<td>
<p>integer, indicating to limit the number of words to read in. Defaults to reading all words.</p>
</td></tr>
<tr><td><code id="read.wordvectors_+3A_normalize">normalize</code></td>
<td>
<p>logical indicating to normalize the embeddings by dividing by the factor (sqrt(sum(x . x) / length(x))). Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="read.wordvectors_+3A_encoding">encoding</code></td>
<td>
<p>encoding to be assumed for the words. Defaults to 'UTF-8'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with the embeddings of the words. The rownames of the matrix are the words which are by default set to UTF-8 encoding.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>path  &lt;- system.file(package = "word2vec", "models", "example.bin")
embed &lt;- read.wordvectors(path, type = "bin", n = 10)
embed &lt;- read.wordvectors(path, type = "bin", n = 10, normalize = TRUE)
embed &lt;- read.wordvectors(path, type = "bin")

path  &lt;- system.file(package = "word2vec", "models", "example.txt")
embed &lt;- read.wordvectors(path, type = "txt", n = 10)
embed &lt;- read.wordvectors(path, type = "txt", n = 10, normalize = TRUE)
embed &lt;- read.wordvectors(path, type = "txt")
</code></pre>

<hr>
<h2 id='txt_clean_word2vec'>Text cleaning specific for input to word2vec</h2><span id='topic+txt_clean_word2vec'></span>

<h3>Description</h3>

<p>Standardise text by
</p>

<ul>
<li><p>Conversion of text from UTF-8 to ASCII
</p>
</li>
<li><p>Keeping only alphanumeric characters: letters and numbers
</p>
</li>
<li><p>Removing multiple spaces
</p>
</li>
<li><p>Removing leading/trailing spaces
</p>
</li>
<li><p>Performing lowercasing
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>txt_clean_word2vec(x, ascii = TRUE, alpha = TRUE, tolower = TRUE, trim = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="txt_clean_word2vec_+3A_x">x</code></td>
<td>
<p>a character vector in UTF-8 encoding</p>
</td></tr>
<tr><td><code id="txt_clean_word2vec_+3A_ascii">ascii</code></td>
<td>
<p>logical indicating to use <code>iconv</code> to convert the input from UTF-8 to ASCII. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="txt_clean_word2vec_+3A_alpha">alpha</code></td>
<td>
<p>logical indicating to keep only alphanumeric characters. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="txt_clean_word2vec_+3A_tolower">tolower</code></td>
<td>
<p>logical indicating to lowercase <code>x</code>. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="txt_clean_word2vec_+3A_trim">trim</code></td>
<td>
<p>logical indicating to trim leading/trailing white space. Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of the same length as <code>x</code> 
which is standardised by converting the encoding to ascii, lowercasing and 
keeping only alphanumeric elements
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("  Just some.texts,  ok?", "123.456 and\tsome MORE!  ")
txt_clean_word2vec(x)
</code></pre>

<hr>
<h2 id='word2vec'>Train a word2vec model on text</h2><span id='topic+word2vec'></span>

<h3>Description</h3>

<p>Construct a word2vec model on text. The algorithm is explained at <a href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>word2vec(
  x,
  type = c("cbow", "skip-gram"),
  dim = 50,
  window = ifelse(type == "cbow", 5L, 10L),
  iter = 5L,
  lr = 0.05,
  hs = FALSE,
  negative = 5L,
  sample = 0.001,
  min_count = 5L,
  stopwords = character(),
  threads = 1L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="word2vec_+3A_x">x</code></td>
<td>
<p>a character vector with text or the path to the file on disk containing training data or a list of tokens. See the examples.</p>
</td></tr>
<tr><td><code id="word2vec_+3A_type">type</code></td>
<td>
<p>the type of algorithm to use, either 'cbow' or 'skip-gram'. Defaults to 'cbow'</p>
</td></tr>
<tr><td><code id="word2vec_+3A_dim">dim</code></td>
<td>
<p>dimension of the word vectors. Defaults to 50.</p>
</td></tr>
<tr><td><code id="word2vec_+3A_window">window</code></td>
<td>
<p>skip length between words. Defaults to 5.</p>
</td></tr>
<tr><td><code id="word2vec_+3A_iter">iter</code></td>
<td>
<p>number of training iterations. Defaults to 5.</p>
</td></tr>
<tr><td><code id="word2vec_+3A_lr">lr</code></td>
<td>
<p>initial learning rate also known as alpha. Defaults to 0.05</p>
</td></tr>
<tr><td><code id="word2vec_+3A_hs">hs</code></td>
<td>
<p>logical indicating to use hierarchical softmax instead of negative sampling. Defaults to FALSE indicating to do negative sampling.</p>
</td></tr>
<tr><td><code id="word2vec_+3A_negative">negative</code></td>
<td>
<p>integer with the number of negative samples. Only used in case hs is set to FALSE</p>
</td></tr>
<tr><td><code id="word2vec_+3A_sample">sample</code></td>
<td>
<p>threshold for occurrence of words. Defaults to 0.001</p>
</td></tr>
<tr><td><code id="word2vec_+3A_min_count">min_count</code></td>
<td>
<p>integer indicating the number of time a word should occur to be considered as part of the training vocabulary. Defaults to 5.</p>
</td></tr>
<tr><td><code id="word2vec_+3A_stopwords">stopwords</code></td>
<td>
<p>a character vector of stopwords to exclude from training</p>
</td></tr>
<tr><td><code id="word2vec_+3A_threads">threads</code></td>
<td>
<p>number of CPU threads to use. Defaults to 1.</p>
</td></tr>
<tr><td><code id="word2vec_+3A_...">...</code></td>
<td>
<p>further arguments passed on to the methods <code><a href="#topic+word2vec.character">word2vec.character</a></code>, <code><a href="#topic+word2vec.list">word2vec.list</a></code> as well as the C++ function <code>w2v_train</code> - for expert use only</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some advice on the optimal set of parameters to use for training as defined by Mikolov et al.
</p>

<ul>
<li><p>argument type: skip-gram (slower, better for infrequent words) vs cbow (fast)
</p>
</li>
<li><p>argument hs: the training algorithm: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors)
</p>
</li>
<li><p>argument dim: dimensionality of the word vectors: usually more is better, but not always
</p>
</li>
<li><p>argument window: for skip-gram usually around 10, for cbow around 5
</p>
</li>
<li><p>argument sample: sub-sampling of frequent words: can improve both accuracy and speed for large data sets (useful values are in range 0.001 to 0.00001)
</p>
</li></ul>



<h3>Value</h3>

<p>an object of class <code>w2v_trained</code> which is a list with elements 
</p>

<ul>
<li><p>model: a Rcpp pointer to the model
</p>
</li>
<li><p>data: a list with elements file: the training data used, stopwords: the character vector of stopwords, n
</p>
</li>
<li><p>vocabulary: the number of words in the vocabulary
</p>
</li>
<li><p>success: logical indicating if training succeeded
</p>
</li>
<li><p>error_log: the error log in case training failed
</p>
</li>
<li><p>control: as list of the training arguments used, namely min_count, dim, window, iter, lr, skipgram, hs, negative, sample, split_words, split_sents, expTableSize and expValueMax
</p>
</li></ul>



<h3>Note</h3>

<p>Some notes on the tokenisation
</p>

<ul>
<li><p>If you provide to <code>x</code> a list, each list element should correspond to a sentence (or what you consider as a sentence) and should contain a character vector of tokens. The word2vec model is then executed using <code><a href="#topic+word2vec.list">word2vec.list</a></code>
</p>
</li>
<li><p>If you provide to <code>x</code> a character vector or the path to the file on disk, the tokenisation into words depends on the first element provided in <code>split</code> and the tokenisation into sentences depends on the second element provided in <code>split</code> when passed on to <code><a href="#topic+word2vec.character">word2vec.character</a></code>
</p>
</li></ul>



<h3>References</h3>

<p><a href="https://github.com/maxoodf/word2vec">https://github.com/maxoodf/word2vec</a>, <a href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.word2vec">predict.word2vec</a></code>, <code><a href="#topic+as.matrix.word2vec">as.matrix.word2vec</a></code>, <code><a href="#topic+word2vec">word2vec</a></code>, <code><a href="#topic+word2vec.character">word2vec.character</a></code>, <code><a href="#topic+word2vec.list">word2vec.list</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(udpipe)
## Take data and standardise it a bit
data(brussels_reviews, package = "udpipe")
x &lt;- subset(brussels_reviews, language == "nl")
x &lt;- tolower(x$feedback)

## Build the model get word embeddings and nearest neighbours
model &lt;- word2vec(x = x, dim = 15, iter = 20)
emb   &lt;- as.matrix(model)
head(emb)
emb   &lt;- predict(model, c("bus", "toilet", "unknownword"), type = "embedding")
emb
nn    &lt;- predict(model, c("bus", "toilet"), type = "nearest", top_n = 5)
nn

## Get vocabulary
vocab   &lt;- summary(model, type = "vocabulary")

# Do some calculations with the vectors and find similar terms to these
emb     &lt;- as.matrix(model)
vector  &lt;- emb["buurt", ] - emb["rustige", ] + emb["restaurants", ]
predict(model, vector, type = "nearest", top_n = 10)

vector  &lt;- emb["gastvrouw", ] - emb["gastvrij", ]
predict(model, vector, type = "nearest", top_n = 5)

vectors &lt;- emb[c("gastheer", "gastvrouw"), ]
vectors &lt;- rbind(vectors, avg = colMeans(vectors))
predict(model, vectors, type = "nearest", top_n = 10)

## Save the model to hard disk
path &lt;- "mymodel.bin"

write.word2vec(model, file = path)
model &lt;- read.word2vec(path)


## 
## Example of word2vec with a list of tokens 
## 
toks  &lt;- strsplit(x, split = "[[:space:][:punct:]]+")
model &lt;- word2vec(x = toks, dim = 15, iter = 20)
emb   &lt;- as.matrix(model)
emb   &lt;- predict(model, c("bus", "toilet", "unknownword"), type = "embedding")
emb
nn    &lt;- predict(model, c("bus", "toilet"), type = "nearest", top_n = 5)
nn

## 
## Example getting word embeddings 
##   which are different depending on the parts of speech tag
## Look to the help of the udpipe R package 
##   to get parts of speech tags on text
## 
library(udpipe)
data(brussels_reviews_anno, package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language == "fr")
x &lt;- subset(x, grepl(xpos, pattern = paste(LETTERS, collapse = "|")))
x$text &lt;- sprintf("%s/%s", x$lemma, x$xpos)
x &lt;- subset(x, !is.na(lemma))
x &lt;- split(x$text, list(x$doc_id, x$sentence_id))

model &lt;- word2vec(x = x, dim = 15, iter = 20)
emb   &lt;- as.matrix(model)
nn    &lt;- predict(model, c("cuisine/NN", "rencontrer/VB"), type = "nearest")
nn
nn    &lt;- predict(model, c("accueillir/VBN", "accueillir/VBG"), type = "nearest")
nn


</code></pre>

<hr>
<h2 id='word2vec_similarity'>Similarity between word vectors as used in word2vec</h2><span id='topic+word2vec_similarity'></span>

<h3>Description</h3>

<p>The similarity between word vectors is defined 
</p>

<ul>
<li><p>for type 'dot': as the square root of the average inner product of the vector elements (sqrt(sum(x . y) / ncol(x))) capped to zero
</p>
</li>
<li><p>for type 'cosine': as the the cosine similarity, namely sum(x . y) / (sum(x^2)*sum(y^2)) 
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>word2vec_similarity(x, y, top_n = +Inf, type = c("dot", "cosine"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="word2vec_similarity_+3A_x">x</code></td>
<td>
<p>a matrix with embeddings where the rownames of the matrix provide the label of the term</p>
</td></tr>
<tr><td><code id="word2vec_similarity_+3A_y">y</code></td>
<td>
<p>a matrix with embeddings where the rownames of the matrix provide the label of the term</p>
</td></tr>
<tr><td><code id="word2vec_similarity_+3A_top_n">top_n</code></td>
<td>
<p>integer indicating to return only the top n most similar terms from y for each row of x. 
If <code>top_n</code> is supplied, a data.frame will be returned with only the highest similarities between x and y 
instead of all pairwise similarities</p>
</td></tr>
<tr><td><code id="word2vec_similarity_+3A_type">type</code></td>
<td>
<p>character string with the type of similarity. Either 'dot' or 'cosine'. Defaults to 'dot'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>By default, the function returns a similarity matrix between the rows of <code>x</code> and the rows of <code>y</code>. 
The similarity between row i of <code>x</code> and row j of <code>y</code> is found in cell <code>[i, j]</code> of the returned similarity matrix.<br />
If <code>top_n</code> is provided, the return value is a data.frame with columns term1, term2, similarity and rank 
indicating the similarity between the provided terms in <code>x</code> and <code>y</code> 
ordered from high to low similarity and keeping only the top_n most similar records.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+word2vec">word2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(6), nrow = 2, ncol = 3)
rownames(x) &lt;- c("word1", "word2")
y &lt;- matrix(rnorm(15), nrow = 5, ncol = 3)
rownames(y) &lt;- c("term1", "term2", "term3", "term4", "term5")

word2vec_similarity(x, y)
word2vec_similarity(x, y, top_n = 1)
word2vec_similarity(x, y, top_n = 2)
word2vec_similarity(x, y, top_n = +Inf)
word2vec_similarity(x, y, type = "cosine")
word2vec_similarity(x, y, top_n = 1, type = "cosine")
word2vec_similarity(x, y, top_n = 2, type = "cosine")
word2vec_similarity(x, y, top_n = +Inf, type = "cosine")

## Example with a word2vec model
path  &lt;- system.file(package = "word2vec", "models", "example.bin")
model &lt;- read.word2vec(path)
emb   &lt;- as.matrix(model)

x &lt;- emb[c("gastheer", "gastvrouw", "kamer"), ]
y &lt;- emb
word2vec_similarity(x, x)
word2vec_similarity(x, y, top_n = 3)
predict(model, x, type = "nearest", top_n = 3)
</code></pre>

<hr>
<h2 id='word2vec.character'>Train a word2vec model on text</h2><span id='topic+word2vec.character'></span>

<h3>Description</h3>

<p>Construct a word2vec model on text. The algorithm is explained at <a href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'character'
word2vec(
  x,
  type = c("cbow", "skip-gram"),
  dim = 50,
  window = ifelse(type == "cbow", 5L, 10L),
  iter = 5L,
  lr = 0.05,
  hs = FALSE,
  negative = 5L,
  sample = 0.001,
  min_count = 5L,
  stopwords = character(),
  threads = 1L,
  split = c(" \n,.-!?:;/\"#$%&amp;'()*+&lt;=&gt;@[]\\^_`{|}~\t\v\f\r", ".\n?!"),
  encoding = "UTF-8",
  useBytes = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="word2vec.character_+3A_x">x</code></td>
<td>
<p>a character vector with text or the path to the file on disk containing training data or a list of tokens. See the examples.</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_type">type</code></td>
<td>
<p>the type of algorithm to use, either 'cbow' or 'skip-gram'. Defaults to 'cbow'</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_dim">dim</code></td>
<td>
<p>dimension of the word vectors. Defaults to 50.</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_window">window</code></td>
<td>
<p>skip length between words. Defaults to 5.</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_iter">iter</code></td>
<td>
<p>number of training iterations. Defaults to 5.</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_lr">lr</code></td>
<td>
<p>initial learning rate also known as alpha. Defaults to 0.05</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_hs">hs</code></td>
<td>
<p>logical indicating to use hierarchical softmax instead of negative sampling. Defaults to FALSE indicating to do negative sampling.</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_negative">negative</code></td>
<td>
<p>integer with the number of negative samples. Only used in case hs is set to FALSE</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_sample">sample</code></td>
<td>
<p>threshold for occurrence of words. Defaults to 0.001</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_min_count">min_count</code></td>
<td>
<p>integer indicating the number of time a word should occur to be considered as part of the training vocabulary. Defaults to 5.</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_stopwords">stopwords</code></td>
<td>
<p>a character vector of stopwords to exclude from training</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_threads">threads</code></td>
<td>
<p>number of CPU threads to use. Defaults to 1.</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_split">split</code></td>
<td>
<p>a character vector of length 2 where the first element indicates how to split words and the second element indicates how to split sentences in <code>x</code></p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_encoding">encoding</code></td>
<td>
<p>the encoding of <code>x</code> and <code>stopwords</code>. Defaults to 'UTF-8'. 
Calculating the model always starts from files allowing to build a model on large corpora. The encoding argument 
is passed on to <code>file</code> when writing <code>x</code> to hard disk in case you provided it as a character vector.</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_usebytes">useBytes</code></td>
<td>
<p>logical passed on to <code><a href="base.html#topic+writeLines">writeLines</a></code> when writing the text and stopwords on disk before building the model. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="word2vec.character_+3A_...">...</code></td>
<td>
<p>further arguments passed on to the methods <code><a href="#topic+word2vec.character">word2vec.character</a></code>, <code><a href="#topic+word2vec.list">word2vec.list</a></code> as well as the C++ function <code>w2v_train</code> - for expert use only</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some advice on the optimal set of parameters to use for training as defined by Mikolov et al.
</p>

<ul>
<li><p>argument type: skip-gram (slower, better for infrequent words) vs cbow (fast)
</p>
</li>
<li><p>argument hs: the training algorithm: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors)
</p>
</li>
<li><p>argument dim: dimensionality of the word vectors: usually more is better, but not always
</p>
</li>
<li><p>argument window: for skip-gram usually around 10, for cbow around 5
</p>
</li>
<li><p>argument sample: sub-sampling of frequent words: can improve both accuracy and speed for large data sets (useful values are in range 0.001 to 0.00001)
</p>
</li></ul>



<h3>Value</h3>

<p>an object of class <code>w2v_trained</code> which is a list with elements 
</p>

<ul>
<li><p>model: a Rcpp pointer to the model
</p>
</li>
<li><p>data: a list with elements file: the training data used, stopwords: the character vector of stopwords, n
</p>
</li>
<li><p>vocabulary: the number of words in the vocabulary
</p>
</li>
<li><p>success: logical indicating if training succeeded
</p>
</li>
<li><p>error_log: the error log in case training failed
</p>
</li>
<li><p>control: as list of the training arguments used, namely min_count, dim, window, iter, lr, skipgram, hs, negative, sample, split_words, split_sents, expTableSize and expValueMax
</p>
</li></ul>



<h3>References</h3>

<p><a href="https://github.com/maxoodf/word2vec">https://github.com/maxoodf/word2vec</a>, <a href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.word2vec">predict.word2vec</a></code>, <code><a href="#topic+as.matrix.word2vec">as.matrix.word2vec</a></code>, <code><a href="#topic+word2vec">word2vec</a></code>, <code><a href="#topic+word2vec.character">word2vec.character</a></code>, <code><a href="#topic+word2vec.list">word2vec.list</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(udpipe)
## Take data and standardise it a bit
data(brussels_reviews, package = "udpipe")
x &lt;- subset(brussels_reviews, language == "nl")
x &lt;- tolower(x$feedback)

## Build the model get word embeddings and nearest neighbours
model &lt;- word2vec(x = x, dim = 15, iter = 20)
emb   &lt;- as.matrix(model)
head(emb)
emb   &lt;- predict(model, c("bus", "toilet", "unknownword"), type = "embedding")
emb
nn    &lt;- predict(model, c("bus", "toilet"), type = "nearest", top_n = 5)
nn

## Get vocabulary
vocab   &lt;- summary(model, type = "vocabulary")

# Do some calculations with the vectors and find similar terms to these
emb     &lt;- as.matrix(model)
vector  &lt;- emb["buurt", ] - emb["rustige", ] + emb["restaurants", ]
predict(model, vector, type = "nearest", top_n = 10)

vector  &lt;- emb["gastvrouw", ] - emb["gastvrij", ]
predict(model, vector, type = "nearest", top_n = 5)

vectors &lt;- emb[c("gastheer", "gastvrouw"), ]
vectors &lt;- rbind(vectors, avg = colMeans(vectors))
predict(model, vectors, type = "nearest", top_n = 10)

## Save the model to hard disk
path &lt;- "mymodel.bin"

write.word2vec(model, file = path)
model &lt;- read.word2vec(path)


## 
## Example of word2vec with a list of tokens 
## 
toks  &lt;- strsplit(x, split = "[[:space:][:punct:]]+")
model &lt;- word2vec(x = toks, dim = 15, iter = 20)
emb   &lt;- as.matrix(model)
emb   &lt;- predict(model, c("bus", "toilet", "unknownword"), type = "embedding")
emb
nn    &lt;- predict(model, c("bus", "toilet"), type = "nearest", top_n = 5)
nn

## 
## Example getting word embeddings 
##   which are different depending on the parts of speech tag
## Look to the help of the udpipe R package 
##   to get parts of speech tags on text
## 
library(udpipe)
data(brussels_reviews_anno, package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language == "fr")
x &lt;- subset(x, grepl(xpos, pattern = paste(LETTERS, collapse = "|")))
x$text &lt;- sprintf("%s/%s", x$lemma, x$xpos)
x &lt;- subset(x, !is.na(lemma))
x &lt;- split(x$text, list(x$doc_id, x$sentence_id))

model &lt;- word2vec(x = x, dim = 15, iter = 20)
emb   &lt;- as.matrix(model)
nn    &lt;- predict(model, c("cuisine/NN", "rencontrer/VB"), type = "nearest")
nn
nn    &lt;- predict(model, c("accueillir/VBN", "accueillir/VBG"), type = "nearest")
nn


</code></pre>

<hr>
<h2 id='word2vec.list'>Train a word2vec model on text</h2><span id='topic+word2vec.list'></span>

<h3>Description</h3>

<p>Construct a word2vec model on text. The algorithm is explained at <a href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'list'
word2vec(
  x,
  type = c("cbow", "skip-gram"),
  dim = 50,
  window = ifelse(type == "cbow", 5L, 10L),
  iter = 5L,
  lr = 0.05,
  hs = FALSE,
  negative = 5L,
  sample = 0.001,
  min_count = 5L,
  stopwords = character(),
  threads = 1L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="word2vec.list_+3A_x">x</code></td>
<td>
<p>a character vector with text or the path to the file on disk containing training data or a list of tokens. See the examples.</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_type">type</code></td>
<td>
<p>the type of algorithm to use, either 'cbow' or 'skip-gram'. Defaults to 'cbow'</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_dim">dim</code></td>
<td>
<p>dimension of the word vectors. Defaults to 50.</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_window">window</code></td>
<td>
<p>skip length between words. Defaults to 5.</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_iter">iter</code></td>
<td>
<p>number of training iterations. Defaults to 5.</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_lr">lr</code></td>
<td>
<p>initial learning rate also known as alpha. Defaults to 0.05</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_hs">hs</code></td>
<td>
<p>logical indicating to use hierarchical softmax instead of negative sampling. Defaults to FALSE indicating to do negative sampling.</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_negative">negative</code></td>
<td>
<p>integer with the number of negative samples. Only used in case hs is set to FALSE</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_sample">sample</code></td>
<td>
<p>threshold for occurrence of words. Defaults to 0.001</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_min_count">min_count</code></td>
<td>
<p>integer indicating the number of time a word should occur to be considered as part of the training vocabulary. Defaults to 5.</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_stopwords">stopwords</code></td>
<td>
<p>a character vector of stopwords to exclude from training</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_threads">threads</code></td>
<td>
<p>number of CPU threads to use. Defaults to 1.</p>
</td></tr>
<tr><td><code id="word2vec.list_+3A_...">...</code></td>
<td>
<p>further arguments passed on to the methods <code><a href="#topic+word2vec.character">word2vec.character</a></code>, <code><a href="#topic+word2vec.list">word2vec.list</a></code> as well as the C++ function <code>w2v_train</code> - for expert use only</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some advice on the optimal set of parameters to use for training as defined by Mikolov et al.
</p>

<ul>
<li><p>argument type: skip-gram (slower, better for infrequent words) vs cbow (fast)
</p>
</li>
<li><p>argument hs: the training algorithm: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors)
</p>
</li>
<li><p>argument dim: dimensionality of the word vectors: usually more is better, but not always
</p>
</li>
<li><p>argument window: for skip-gram usually around 10, for cbow around 5
</p>
</li>
<li><p>argument sample: sub-sampling of frequent words: can improve both accuracy and speed for large data sets (useful values are in range 0.001 to 0.00001)
</p>
</li></ul>



<h3>Value</h3>

<p>an object of class <code>w2v_trained</code> which is a list with elements 
</p>

<ul>
<li><p>model: a Rcpp pointer to the model
</p>
</li>
<li><p>data: a list with elements file: the training data used, stopwords: the character vector of stopwords, n
</p>
</li>
<li><p>vocabulary: the number of words in the vocabulary
</p>
</li>
<li><p>success: logical indicating if training succeeded
</p>
</li>
<li><p>error_log: the error log in case training failed
</p>
</li>
<li><p>control: as list of the training arguments used, namely min_count, dim, window, iter, lr, skipgram, hs, negative, sample, split_words, split_sents, expTableSize and expValueMax
</p>
</li></ul>



<h3>References</h3>

<p><a href="https://github.com/maxoodf/word2vec">https://github.com/maxoodf/word2vec</a>, <a href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.word2vec">predict.word2vec</a></code>, <code><a href="#topic+as.matrix.word2vec">as.matrix.word2vec</a></code>, <code><a href="#topic+word2vec">word2vec</a></code>, <code><a href="#topic+word2vec.character">word2vec.character</a></code>, <code><a href="#topic+word2vec.list">word2vec.list</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(udpipe)
data(brussels_reviews, package = "udpipe")
x     &lt;- subset(brussels_reviews, language == "nl")
x     &lt;- tolower(x$feedback)
toks  &lt;- strsplit(x, split = "[[:space:][:punct:]]+")
model &lt;- word2vec(x = toks, dim = 15, iter = 20)
emb   &lt;- as.matrix(model)
head(emb)
emb   &lt;- predict(model, c("bus", "toilet", "unknownword"), type = "embedding")
emb
nn    &lt;- predict(model, c("bus", "toilet"), type = "nearest", top_n = 5)
nn

## 
## Example of word2vec with a list of tokens
## which gives the same embeddings as with a similarly tokenised character vector of texts 
## 
txt   &lt;- txt_clean_word2vec(x, ascii = TRUE, alpha = TRUE, tolower = TRUE, trim = TRUE)
table(unlist(strsplit(txt, "")))
toks  &lt;- strsplit(txt, split = " ")
set.seed(1234)
modela &lt;- word2vec(x = toks, dim = 15, iter = 20)
set.seed(1234)
modelb &lt;- word2vec(x = txt, dim = 15, iter = 20, split = c(" \n\r", "\n\r"))
all.equal(as.matrix(modela), as.matrix(modelb))

</code></pre>

<hr>
<h2 id='write.word2vec'>Save a word2vec model to disk</h2><span id='topic+write.word2vec'></span>

<h3>Description</h3>

<p>Save a word2vec model as a binary file to disk or as a text file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write.word2vec(x, file, type = c("bin", "txt"), encoding = "UTF-8")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write.word2vec_+3A_x">x</code></td>
<td>
<p>an object of class <code>w2v</code> or <code>w2v_trained</code> as returned by <code><a href="#topic+word2vec">word2vec</a></code></p>
</td></tr>
<tr><td><code id="write.word2vec_+3A_file">file</code></td>
<td>
<p>the path to the file where to store the model</p>
</td></tr>
<tr><td><code id="write.word2vec_+3A_type">type</code></td>
<td>
<p>either 'bin' or 'txt' to write respectively the file as binary or as a text file. Defaults to 'bin'.</p>
</td></tr>
<tr><td><code id="write.word2vec_+3A_encoding">encoding</code></td>
<td>
<p>encoding to use when writing a file with type 'txt' to disk. Defaults to 'UTF-8'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a logical indicating if the save process succeeded
</p>


<h3>See Also</h3>

<p><code><a href="#topic+word2vec">word2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>path  &lt;- system.file(package = "word2vec", "models", "example.bin")
model &lt;- read.word2vec(path)


## Save the model to hard disk as a binary file
path &lt;- "mymodel.bin"

write.word2vec(model, file = path)



## Save the model to hard disk as a text file (uses package udpipe)
library(udpipe)
path &lt;- "mymodel.txt"

write.word2vec(model, file = path, type = "txt")


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
