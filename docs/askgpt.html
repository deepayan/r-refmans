<!DOCTYPE html><html lang="en-GB"><head><title>Help for package askgpt</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {askgpt}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#annotate_code'><p>Annotate R code with inline comments</p></a></li>
<li><a href='#askgpt'><p>Ask openai's GPT models a question</p></a></li>
<li><a href='#chat_api'><p>Request answer from openai's chat API</p></a></li>
<li><a href='#completions_api'><p>Request answer from openai's completions API</p></a></li>
<li><a href='#document_code'><p>Document R Code</p></a></li>
<li><a href='#estimate_token'><p>Estimate token count</p></a></li>
<li><a href='#explain_code'><p>Explain R code</p></a></li>
<li><a href='#improve_addin'><p>Improve code/documentation/writing using a prompt</p></a></li>
<li><a href='#list_models'><p>List Models</p></a></li>
<li><a href='#log_init'><p>Initiate error logging</p></a></li>
<li><a href='#login'><p>Log in to OpenAI</p></a></li>
<li><a href='#new_conversation'><p>Start a new conversation</p></a></li>
<li><a href='#parse_response'><p>Parse response from API functions</p></a></li>
<li><a href='#prompt_history'><p>Return the prompt/response history</p></a></li>
<li><a href='#response_history'><p>Return the prompt/response history</p></a></li>
<li><a href='#test_function'><p>Test R code</p></a></li>
<li><a href='#token_limits'><p>Max tokens limits of the different models</p></a></li>
<li><a href='#tutorialise_addin'><p>Turn R code into a tutorial</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Asking GPT About R Stuff</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.3</td>
</tr>
<tr>
<td>Description:</td>
<td>A chat package connecting to API endpoints by 'OpenAI' 
  (<a href="https://platform.openai.com/">https://platform.openai.com/</a>) to answer questions (about R).</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, callr, dplyr, glue, methods, rlang, httr2, rappdirs,
jsonlite</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, knitr, miniUI, rmarkdown, rstudioapi, shiny,
shinycssloaders, spelling, testthat (&ge; 3.0.0), withr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/JBGruber/askgpt">https://github.com/JBGruber/askgpt</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/JBGruber/askgpt/issues">https://github.com/JBGruber/askgpt/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Language:</td>
<td>en-GB</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-08 08:14:55 UTC; johannes</td>
</tr>
<tr>
<td>Author:</td>
<td>Johannes Gruber [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Johannes Gruber &lt;johannesb.gruber@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-08 08:30:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='annotate_code'>Annotate R code with inline comments</h2><span id='topic+annotate_code'></span>

<h3>Description</h3>

<p>Annotate R code with inline comments
</p>


<h3>Usage</h3>

<pre><code class='language-R'>annotate_code(code, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="annotate_code_+3A_code">code</code></td>
<td>
<p>A character vector of R code. If missing the code currently
selected in RStudio is documented (If RStudio is used).</p>
</td></tr>
<tr><td><code id="annotate_code_+3A_...">...</code></td>
<td>
<p>passed on to <code><a href="#topic+askgpt">askgpt</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>

<hr>
<h2 id='askgpt'>Ask openai's GPT models a question</h2><span id='topic+askgpt'></span>

<h3>Description</h3>

<p>Ask openai's GPT models a question
</p>


<h3>Usage</h3>

<pre><code class='language-R'>askgpt(prompt, chat = TRUE, progress = TRUE, return_answer = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="askgpt_+3A_prompt">prompt</code></td>
<td>
<p>What you want to ask</p>
</td></tr>
<tr><td><code id="askgpt_+3A_chat">chat</code></td>
<td>
<p>whether to use the chat API (i.e., the same model as ChatGPT) or
the completions API.</p>
</td></tr>
<tr><td><code id="askgpt_+3A_progress">progress</code></td>
<td>
<p>Show a progress spinner while the request to the API has not
been fulfilled.</p>
</td></tr>
<tr><td><code id="askgpt_+3A_return_answer">return_answer</code></td>
<td>
<p>Should the answer be returned as an object instead of
printing it to the screen?</p>
</td></tr>
<tr><td><code id="askgpt_+3A_...">...</code></td>
<td>
<p>additional options forwarded to <code><a href="#topic+chat_api">chat_api</a></code> or
<code><a href="#topic+completions_api">completions_api</a></code> respectively.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>either an httr2 response from one of the APIs or a character vector
(if return_answer).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
askgpt("What is an R function?")
askgpt("What is wrong with my last command?")
askgpt("Can you help me with the function aes() from ggplot2?")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_api'>Request answer from openai's chat API</h2><span id='topic+chat_api'></span>

<h3>Description</h3>

<p>Request answer from openai's chat API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_api(
  prompt,
  model = NULL,
  config = NULL,
  max_tokens = NULL,
  api_key = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_api_+3A_prompt">prompt</code></td>
<td>
<p>character string of the prompt to be completed.</p>
</td></tr>
<tr><td><code id="chat_api_+3A_model">model</code></td>
<td>
<p>character string of the model to be used (defaults to
&quot;text-davinci-003&quot;).</p>
</td></tr>
<tr><td><code id="chat_api_+3A_config">config</code></td>
<td>
<p>a configuration prompt to tell the model how it should behave.</p>
</td></tr>
<tr><td><code id="chat_api_+3A_max_tokens">max_tokens</code></td>
<td>
<p>The maximum number of tokens to generate in the completion.
2048L is the maximum the models accept.</p>
</td></tr>
<tr><td><code id="chat_api_+3A_api_key">api_key</code></td>
<td>
<p>set the API key. If NULL, looks for the env OPENAI_API_KEY.</p>
</td></tr>
<tr><td><code id="chat_api_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed to the API (see [the API
documentation](https://platform.openai.com/docs/api-reference/completions)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with available models
</p>
<p>a httr2 response object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chat_api("Hi, how are you?", config = "answer as a friendly chat bot")

## End(Not run)
</code></pre>

<hr>
<h2 id='completions_api'>Request answer from openai's completions API</h2><span id='topic+completions_api'></span>

<h3>Description</h3>

<p>Mostly used under the hood for <code><a href="#topic+askgpt">askgpt</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>completions_api(
  prompt,
  model = NULL,
  temperature = NULL,
  max_tokens = NULL,
  api_key = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="completions_api_+3A_prompt">prompt</code></td>
<td>
<p>character string of the prompt to be completed.</p>
</td></tr>
<tr><td><code id="completions_api_+3A_model">model</code></td>
<td>
<p>character string of the model to be used (defaults to
&quot;text-davinci-003&quot;).</p>
</td></tr>
<tr><td><code id="completions_api_+3A_temperature">temperature</code></td>
<td>
<p>numeric value between 0 and 1 to control the randomness of
the output (defaults to 0.2; lower values like 0.2 will make answers more
focused and deterministic).</p>
</td></tr>
<tr><td><code id="completions_api_+3A_max_tokens">max_tokens</code></td>
<td>
<p>The maximum number of tokens to generate in the completion.
2048L is the maximum the models accept.</p>
</td></tr>
<tr><td><code id="completions_api_+3A_api_key">api_key</code></td>
<td>
<p>set the API key. If NULL, looks for the env OPENAI_API_KEY.</p>
</td></tr>
<tr><td><code id="completions_api_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed to the API (see [the API
documentation](https://platform.openai.com/docs/api-reference/completions)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only a few parameters are implemented by name. Most can be sent
through the <code>...</code>. For example, you could use the <code>n</code> parameter
just like this <code>completions_api("The quick brown fox", n = 2)</code>.
</p>
<p>A couple of defaults are used by the package:
</p>

<ul>
<li><p>the model used by default is &quot;text-davinci-003&quot;
</p>
</li>
<li><p>the default temperature is 0.2
</p>
</li>
<li><p>the default for max_tokens is 2048L
</p>
</li></ul>

<p>You can configure how <code><a href="#topic+askgpt">askgpt</a></code> makes requests by setting
options that start with <code>askgpt_*</code>. For example, to use a different
model use <code>options(askgpt_model = "text-curie-001")</code>. It does not
matter if the API parameter ist listed in the function or not. All are
used.
</p>


<h3>Value</h3>

<p>a httr2 response object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
completions_api("The quick brown fox")

## End(Not run)
</code></pre>

<hr>
<h2 id='document_code'>Document R Code</h2><span id='topic+document_code'></span>

<h3>Description</h3>

<p>Document R Code
</p>


<h3>Usage</h3>

<pre><code class='language-R'>document_code(code, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="document_code_+3A_code">code</code></td>
<td>
<p>A character vector of R code. If missing the code currently
selected in RStudio is documented (If RStudio is used).</p>
</td></tr>
<tr><td><code id="document_code_+3A_...">...</code></td>
<td>
<p>passed on to <code><a href="#topic+askgpt">askgpt</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
document_code()

## End(Not run)
</code></pre>

<hr>
<h2 id='estimate_token'>Estimate token count</h2><span id='topic+estimate_token'></span>

<h3>Description</h3>

<p>Estimate token count
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_token(x, mult = 1.6)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estimate_token_+3A_x">x</code></td>
<td>
<p>character vector</p>
</td></tr>
<tr><td><code id="estimate_token_+3A_mult">mult</code></td>
<td>
<p>the multiplier used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function estimates how many tokens the API will make of the
input words. For the models 1 word is more than one token. The default
multiplier value resulted from testing the API. See
&lt;https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them&gt;
for more information.
</p>


<h3>Value</h3>

<p>a integer vector of token counts
</p>


<h3>Examples</h3>

<pre><code class='language-R'>estimate_token("this is a test")
</code></pre>

<hr>
<h2 id='explain_code'>Explain R code</h2><span id='topic+explain_code'></span>

<h3>Description</h3>

<p>Explain R code
</p>


<h3>Usage</h3>

<pre><code class='language-R'>explain_code(code, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="explain_code_+3A_code">code</code></td>
<td>
<p>A character vector of R code. If missing the code currently
selected in RStudio is documented (If RStudio is used).</p>
</td></tr>
<tr><td><code id="explain_code_+3A_...">...</code></td>
<td>
<p>passed on to <code><a href="#topic+askgpt">askgpt</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>

<hr>
<h2 id='improve_addin'>Improve code/documentation/writing using a prompt</h2><span id='topic+improve_addin'></span>

<h3>Description</h3>

<p>'tutorialise_addin()' opens an [RStudio
gadget](https://shiny.rstudio.com/articles/gadgets.html) and
[addin](http://rstudio.github.io/rstudioaddins/) that can be used to
improve existing code, documentation, or writing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>improve_addin()
</code></pre>


<h3>Value</h3>

<p>No return value, opens a new file in RStudio
</p>

<hr>
<h2 id='list_models'>List Models</h2><span id='topic+list_models'></span>

<h3>Description</h3>

<p>List the models available in the API. You can refer to the [Models
documentation](https://platform.openai.com/docs/models) to understand what
models are available and the differences between them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_models(api_key = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_models_+3A_api_key">api_key</code></td>
<td>
<p>set the API key. If NULL, looks for the env OPENAI_API_KEY.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with available models
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
completions_api("The quick brown fox")

## End(Not run)
</code></pre>

<hr>
<h2 id='log_init'>Initiate error logging</h2><span id='topic+log_init'></span>

<h3>Description</h3>

<p>Initiate error logging
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_init(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_init_+3A_...">...</code></td>
<td>
<p>forwarded to <code><a href="rlang.html#topic+global_entrace">global_entrace</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Just an alias for rlang::global_entrace() with a more fitting name (for the
purpose here).
</p>


<h3>Value</h3>

<p>No return value, called to enable rlang error logging
</p>

<hr>
<h2 id='login'>Log in to OpenAI</h2><span id='topic+login'></span>

<h3>Description</h3>

<p>Log in to OpenAI
</p>


<h3>Usage</h3>

<pre><code class='language-R'>login(api_key, force_refresh = FALSE, cache_dir = NULL, no_cache = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="login_+3A_api_key">api_key</code></td>
<td>
<p>API key to use for authentication. If not provided, the
function look for a cached key or guide the user to obtain one.</p>
</td></tr>
<tr><td><code id="login_+3A_force_refresh">force_refresh</code></td>
<td>
<p>Log in again even if an API key is already cached.</p>
</td></tr>
<tr><td><code id="login_+3A_cache_dir">cache_dir</code></td>
<td>
<p>dir location to save keys on disk. The default is to use
<code>rappdirs::user_cache_dir("askgpt")</code>.</p>
</td></tr>
<tr><td><code id="login_+3A_no_cache">no_cache</code></td>
<td>
<p>Don't cache the API key, only load it into the environment.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector with an API key
</p>

<hr>
<h2 id='new_conversation'>Start a new conversation</h2><span id='topic+new_conversation'></span>

<h3>Description</h3>

<p>Deletes the local prompt and response history to start a new conversation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_conversation()
</code></pre>


<h3>Value</h3>

<p>Does not return a value
</p>

<hr>
<h2 id='parse_response'>Parse response from API functions</h2><span id='topic+parse_response'></span>

<h3>Description</h3>

<p>Parse response from API functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_response(response)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="parse_response_+3A_response">response</code></td>
<td>
<p>a response object from <code><a href="#topic+chat_api">chat_api</a></code> or
<code><a href="#topic+completions_api">completions_api</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector
</p>

<hr>
<h2 id='prompt_history'>Return the prompt/response history</h2><span id='topic+prompt_history'></span>

<h3>Description</h3>

<p>Return the prompt/response history
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prompt_history(n = Inf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prompt_history_+3A_n">n</code></td>
<td>
<p>number of prompts/responses to return.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector
</p>

<hr>
<h2 id='response_history'>Return the prompt/response history</h2><span id='topic+response_history'></span>

<h3>Description</h3>

<p>Return the prompt/response history
</p>


<h3>Usage</h3>

<pre><code class='language-R'>response_history(n = Inf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="response_history_+3A_n">n</code></td>
<td>
<p>number of prompts/responses to return.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector
</p>

<hr>
<h2 id='test_function'>Test R code</h2><span id='topic+test_function'></span>

<h3>Description</h3>

<p>Test R code
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_function(code, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="test_function_+3A_code">code</code></td>
<td>
<p>A character vector of R code. If missing the code currently
selected in RStudio is documented (If RStudio is used).</p>
</td></tr>
<tr><td><code id="test_function_+3A_...">...</code></td>
<td>
<p>passed on to <code><a href="#topic+askgpt">askgpt</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>

<hr>
<h2 id='token_limits'>Max tokens limits of the different models</h2><span id='topic+token_limits'></span>

<h3>Description</h3>

<p>OpenAI's token limits for different models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>token_limits
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 6 rows and 2 columns.
</p>


<h3>Source</h3>

<p>&lt;https://platform.openai.com/docs/models/overview&gt;
</p>

<hr>
<h2 id='tutorialise_addin'>Turn R code into a tutorial</h2><span id='topic+tutorialise_addin'></span>

<h3>Description</h3>

<p>'tutorialise_addin()' opens an [RStudio
gadget](https://shiny.rstudio.com/articles/gadgets.html) and
[addin](http://rstudio.github.io/rstudioaddins/) that turns selected code
into an R Markdown/Quarto Tutorial.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tutorialise_addin()
</code></pre>


<h3>Value</h3>

<p>No return value, opens a new file in RStudio
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
