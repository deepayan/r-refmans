<!DOCTYPE html><html><head><title>Help for package gglasso</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gglasso}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bardet'><p>Simplified gene expression data from Scheetz et al. (2006)</p></a></li>
<li><a href='#coef.cv.gglasso'><p>get coefficients or make coefficient predictions from a &quot;cv.gglasso&quot; object.</p></a></li>
<li><a href='#coef.gglasso'><p>get coefficients or make coefficient predictions from an &quot;gglasso&quot; object.</p></a></li>
<li><a href='#colon'><p>Simplified gene expression data from Alon et al. (1999)</p></a></li>
<li><a href='#cv.gglasso'><p>Cross-validation for gglasso</p></a></li>
<li><a href='#gglasso'><p>Fits the regularization paths for group-lasso penalized learning problems</p></a></li>
<li><a href='#plot.cv.gglasso'><p>plot the cross-validation curve produced by cv.gglasso</p></a></li>
<li><a href='#plot.gglasso'><p>Plot solution paths from a &quot;gglasso&quot; object</p></a></li>
<li><a href='#predict.cv.gglasso'><p>make predictions from a &quot;cv.gglasso&quot; object.</p></a></li>
<li><a href='#predict.gglasso'><p>make predictions from a &quot;gglasso&quot; object.</p></a></li>
<li><a href='#print.gglasso'><p>print a gglasso object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Group Lasso Penalized Learning Using a Unified BMD Algorithm</td>
</tr>
<tr>
<td>Version:</td>
<td>1.5.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yi Yang &lt;yi.yang6@mcgill.ca&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A unified algorithm, blockwise-majorization-descent (BMD), for efficiently computing the solution paths of the group-lasso penalized least squares, logistic regression, Huberized SVM and squared SVM. The package is an implementation of Yang, Y. and Zou, H. (2015) DOI: &lt;<a href="https://doi.org/10.1007%2Fs11222-014-9498-5">doi:10.1007/s11222-014-9498-5</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>methods</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/emeryyi/gglasso/issues">https://github.com/emeryyi/gglasso/issues</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-24 10:55:14 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-24 14:45:12 UTC</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Author:</td>
<td>Yi Yang [aut, cre] (http://www.math.mcgill.ca/yyang/),
  Hui Zou [aut] (http://users.stat.umn.edu/~zouxx019/),
  Sahir Bhatnagar [aut] (http://sahirbhatnagar.com/)</td>
</tr>
</table>
<hr>
<h2 id='bardet'>Simplified gene expression data from Scheetz et al. (2006)</h2><span id='topic+bardet'></span>

<h3>Description</h3>

<p>Gene expression data (20 genes for 120 samples) from the microarray
experiments of mammalian eye tissue samples of Scheetz et al. (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bardet
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 2.</p>


<h3>Details</h3>

<p>This data set contains 120 samples with 100 predictors (expanded from 20
genes using 5 basis B-splines, as described in Yang, Y. and Zou, H. (2015)).
</p>


<h3>Value</h3>

<p>A list with the following elements: </p>
<table>
<tr><td><code>x</code></td>
<td>
<p>a [120 x 100] matrix
(expanded from a [120 x 20] matrix) giving the expression levels of 20
filtered genes for the 120 samples. Each row corresponds to a subject, each
5 consecutive columns to a grouped gene.</p>
</td></tr> <tr><td><code>y</code></td>
<td>
<p>a numeric vector of
length 120 giving expression level of gene TRIM32, which causes Bardet-Biedl
syndrome.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Scheetz, T., Kim, K., Swiderski, R., Philp, A., Braun, T.,
Knudtson, K., Dorrance, A., DiBona, G., Huang, J., Casavant, T. et al.
(2006), &ldquo;Regulation of gene expression in the mammalian eye and its
relevance to eye disease&rdquo;, <em>Proceedings of the National Academy of
Sciences</em> <b>103</b>(39), 14429-14434. <br />
</p>
<p>Huang, J., S. Ma, and C.-H. Zhang (2008). &ldquo;Adaptive Lasso for sparse
high-dimensional regression models&rdquo;. <em>Statistica Sinica</em> 18,
1603-1618.<br />
</p>
<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for Computing
Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and Computing</em>.
25(6), 1129-1141.<br /> BugReport: <a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load data set
data(bardet)

# how many samples and how many predictors ?
dim(bardet$x)

# repsonse y
bardet$y

</code></pre>

<hr>
<h2 id='coef.cv.gglasso'>get coefficients or make coefficient predictions from a &quot;cv.gglasso&quot; object.</h2><span id='topic+coef.cv.gglasso'></span>

<h3>Description</h3>

<p>This function gets coefficients or makes coefficient predictions from a
cross-validated <code>gglasso</code> model, using the stored <code>"gglasso.fit"</code>
object, and the optimal value chosen for <code>lambda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.gglasso'
coef(object, s = c("lambda.1se", "lambda.min"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.cv.gglasso_+3A_object">object</code></td>
<td>
<p>fitted <code><a href="#topic+cv.gglasso">cv.gglasso</a></code> object.</p>
</td></tr>
<tr><td><code id="coef.cv.gglasso_+3A_s">s</code></td>
<td>
<p>value(s) of the penalty parameter <code>lambda</code> at which
predictions are required. Default is the value <code>s="lambda.1se"</code> stored
on the CV <code>object</code>, it is the largest value of <code>lambda</code> such that
error is within 1 standard error of the minimum. Alternatively
<code>s="lambda.min"</code> can be used, it is the optimal value of <code>lambda</code>
that gives minimum cross validation error <code>cvm</code>. If <code>s</code> is
numeric, it is taken as the value(s) of <code>lambda</code> to be used.</p>
</td></tr>
<tr><td><code id="coef.cv.gglasso_+3A_...">...</code></td>
<td>
<p>not used. Other arguments to predict.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function makes it easier to use the results of cross-validation to get
coefficients or make coefficient predictions.
</p>


<h3>Value</h3>

<p>The coefficients at the requested values for <code>lambda</code>.
</p>


<h3>Author(s)</h3>

<p>Yi Yang and Hui Zou<br /> Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for
Computing Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and
Computing</em>. 25(6), 1129-1141.<br /> BugReport:
<a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>
<p>Friedman, J., Hastie, T., and Tibshirani, R. (2010), &quot;Regularization paths
for generalized linear models via coordinate descent,&quot; <em>Journal of
Statistical Software, 33, 1.</em><br /> <a href="http://www.jstatsoft.org/v33/i01/">http://www.jstatsoft.org/v33/i01/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.gglasso">cv.gglasso</a></code>, and <code><a href="#topic+predict.cv.gglasso">predict.cv.gglasso</a></code>
methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load data set
data(colon)

# define group index
group &lt;- rep(1:20,each=5)

# 5-fold cross validation using group lasso 
# penalized logisitic regression
cv &lt;- cv.gglasso(x=colon$x, y=colon$y, group=group, loss="logit",
pred.loss="misclass", lambda.factor=0.05, nfolds=5)

# the coefficients at lambda = lambda.1se
pre = coef(cv$gglasso.fit, s = cv$lambda.1se)
</code></pre>

<hr>
<h2 id='coef.gglasso'>get coefficients or make coefficient predictions from an &quot;gglasso&quot; object.</h2><span id='topic+coef.gglasso'></span>

<h3>Description</h3>

<p>Computes the coefficients at the requested values for <code>lambda</code> from a
fitted <code><a href="#topic+gglasso">gglasso</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gglasso'
coef(object, s = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.gglasso_+3A_object">object</code></td>
<td>
<p>fitted <code><a href="#topic+gglasso">gglasso</a></code> model object.</p>
</td></tr>
<tr><td><code id="coef.gglasso_+3A_s">s</code></td>
<td>
<p>value(s) of the penalty parameter <code>lambda</code> at which
predictions are required. Default is the entire sequence used to create the
model.</p>
</td></tr>
<tr><td><code id="coef.gglasso_+3A_...">...</code></td>
<td>
<p>not used. Other arguments to predict.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>s</code> is the new vector at which predictions are requested. If <code>s</code>
is not in the lambda sequence used for fitting the model, the <code>coef</code>
function will use linear interpolation to make predictions. The new values
are interpolated using a fraction of coefficients from both left and right
<code>lambda</code> indices.
</p>


<h3>Value</h3>

<p>The coefficients at the requested values for <code>lambda</code>.
</p>


<h3>Author(s)</h3>

<p>Yi Yang and Hui Zou<br /> Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for
Computing Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and
Computing</em>. 25(6), 1129-1141.<br /> BugReport:
<a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.gglasso">predict.gglasso</a></code> method
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load data set
data(colon)

# define group index
group &lt;- rep(1:20,each=5)

# fit group lasso
m1 &lt;- gglasso(x=colon$x,y=colon$y,group=group,loss="logit")

# the coefficients at lambda = 0.01 and 0.02
coef(m1,s=c(0.01,0.02))

</code></pre>

<hr>
<h2 id='colon'>Simplified gene expression data from Alon et al. (1999)</h2><span id='topic+colon'></span>

<h3>Description</h3>

<p>Gene expression data (20 genes for 62 samples) from the microarray
experiments of colon tissue samples of Alon et al. (1999).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colon
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 2.</p>


<h3>Details</h3>

<p>This data set contains 62 samples with 100 predictors (expanded from 20
genes using 5 basis B-splines, as described in Yang, Y. and Zou, H. (2015)):
40 tumor tissues, coded 1 and 22 normal tissues, coded -1.
</p>


<h3>Value</h3>

<p>A list with the following elements: </p>
<table>
<tr><td><code>x</code></td>
<td>
<p>a [62 x 100] matrix
(expanded from a [62 x 20] matrix) giving the expression levels of 20 genes
for the 62 colon tissue samples. Each row corresponds to a patient, each 5
consecutive columns to a grouped gene.</p>
</td></tr> <tr><td><code>y</code></td>
<td>
<p>a numeric vector of length
62 giving the type of tissue sample (tumor or normal).</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The data are described in Alon et al. (1999) and can be freely
downloaded from
<a href="http://microarray.princeton.edu/oncology/affydata/index.html">http://microarray.princeton.edu/oncology/affydata/index.html</a>.
</p>


<h3>References</h3>

<p>Alon, U. and Barkai, N. and Notterman, D.A. and Gish, K. and
Ybarra, S. and Mack, D. and Levine, A.J. (1999). &ldquo;Broad patterns of gene
expression revealed by clustering analysis of tumor and normal colon tissues
probed by oligonucleotide arrays&rdquo;, <em>Proc. Natl. Acad. Sci. USA</em>,
<b>96</b>(12), 6745&ndash;6750.<br />
</p>
<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for Computing
Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and Computing</em>.
25(6), 1129-1141.<br /> BugReport: <a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load data set
data(colon)

# how many samples and how many predictors ?
dim(colon$x)

# how many samples of class -1 and 1 respectively ?
sum(colon$y==-1)
sum(colon$y==1)

</code></pre>

<hr>
<h2 id='cv.gglasso'>Cross-validation for gglasso</h2><span id='topic+cv.gglasso'></span><span id='topic+cv.ls'></span><span id='topic+cv.logit'></span><span id='topic+cv.hsvm'></span><span id='topic+cv.sqsvm'></span>

<h3>Description</h3>

<p>Does k-fold cross-validation for gglasso, produces a plot, and returns a
value for <code>lambda</code>. This function is modified based on the <code>cv</code>
function from the <code>glmnet</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.gglasso(
  x,
  y,
  group,
  lambda = NULL,
  pred.loss = c("misclass", "loss", "L1", "L2"),
  nfolds = 5,
  foldid,
  delta,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.gglasso_+3A_x">x</code></td>
<td>
<p>matrix of predictors, of dimension <code class="reqn">n \times p</code>; each row
is an observation vector.</p>
</td></tr>
<tr><td><code id="cv.gglasso_+3A_y">y</code></td>
<td>
<p>response variable. This argument should be quantitative for
regression (least squares), and a two-level factor for classification
(logistic model, huberized SVM, squared SVM).</p>
</td></tr>
<tr><td><code id="cv.gglasso_+3A_group">group</code></td>
<td>
<p>a vector of consecutive integers describing the grouping of the
coefficients (see example below).</p>
</td></tr>
<tr><td><code id="cv.gglasso_+3A_lambda">lambda</code></td>
<td>
<p>optional user-supplied lambda sequence; default is
<code>NULL</code>, and <code><a href="#topic+gglasso">gglasso</a></code> chooses its own sequence.</p>
</td></tr>
<tr><td><code id="cv.gglasso_+3A_pred.loss">pred.loss</code></td>
<td>
<p>loss to use for cross-validation error. Valid options are:
</p>
 <ul>
<li> <p><code>"loss"</code> for classification, margin based loss
function.  </p>
</li>
<li> <p><code>"misclass"</code> for classification, it gives
misclassification error.  </p>
</li>
<li> <p><code>"L1"</code> for regression, mean square
error used by least squares regression <code>loss="ls"</code>, it measure the
deviation from the fitted mean to the response.  </p>
</li>
<li> <p><code>"L2"</code> for
regression, mean absolute error used by least squares regression
<code>loss="ls"</code>, it measure the deviation from the fitted mean to the
response.  </p>
</li></ul>
<p> Default is <code>"loss"</code>.</p>
</td></tr>
<tr><td><code id="cv.gglasso_+3A_nfolds">nfolds</code></td>
<td>
<p>number of folds - default is 5. Although <code>nfolds</code> can be
as large as the sample size (leave-one-out CV), it is not recommended for
large datasets. Smallest value allowable is <code>nfolds=3</code>.</p>
</td></tr>
<tr><td><code id="cv.gglasso_+3A_foldid">foldid</code></td>
<td>
<p>an optional vector of values between 1 and <code>nfold</code>
identifying what fold each observation is in. If supplied, <code>nfold</code> can
be missing.</p>
</td></tr>
<tr><td><code id="cv.gglasso_+3A_delta">delta</code></td>
<td>
<p>parameter <code class="reqn">\delta</code> only used in huberized SVM for
computing log-likelihood on validation set, only available with
<code>pred.loss = "loss"</code>, <code>loss = "hsvm"</code>.</p>
</td></tr>
<tr><td><code id="cv.gglasso_+3A_...">...</code></td>
<td>
<p>other arguments that can be passed to gglasso.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function runs <code><a href="#topic+gglasso">gglasso</a></code> <code>nfolds</code>+1 times; the first to
get the <code>lambda</code> sequence, and then the remainder to compute the fit
with each of the folds omitted. The average error and standard deviation
over the folds are computed.
</p>


<h3>Value</h3>

<p>an object of class <code><a href="#topic+cv.gglasso">cv.gglasso</a></code> is returned, which is a
list with the ingredients of the cross-validation fit.  </p>
<table>
<tr><td><code>lambda</code></td>
<td>
<p>the
values of <code>lambda</code> used in the fits.</p>
</td></tr> <tr><td><code>cvm</code></td>
<td>
<p>the mean
cross-validated error - a vector of length <code>length(lambda)</code>.</p>
</td></tr>
<tr><td><code>cvsd</code></td>
<td>
<p>estimate of standard error of <code>cvm</code>.</p>
</td></tr> <tr><td><code>cvupper</code></td>
<td>
<p>upper
curve = <code>cvm+cvsd</code>.</p>
</td></tr> <tr><td><code>cvlower</code></td>
<td>
<p>lower curve = <code>cvm-cvsd</code>.</p>
</td></tr>
<tr><td><code>name</code></td>
<td>
<p>a text string indicating type of measure (for plotting
purposes).</p>
</td></tr> <tr><td><code>gglasso.fit</code></td>
<td>
<p>a fitted <code><a href="#topic+gglasso">gglasso</a></code> object for the
full data.</p>
</td></tr> <tr><td><code>lambda.min</code></td>
<td>
<p>The optimal value of <code>lambda</code> that gives
minimum cross validation error <code>cvm</code>.</p>
</td></tr> <tr><td><code>lambda.1se</code></td>
<td>
<p>The largest
value of <code>lambda</code> such that error is within 1 standard error of the
minimum.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yi Yang and Hui Zou<br /> Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for
Computing Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and
Computing</em>. 25(6), 1129-1141.<br /> BugReport:
<a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gglasso">gglasso</a></code>, <code><a href="#topic+plot.cv.gglasso">plot.cv.gglasso</a></code>,
<code><a href="#topic+predict.cv.gglasso">predict.cv.gglasso</a></code>, and <code><a href="#topic+coef.cv.gglasso">coef.cv.gglasso</a></code> methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load data set
data(bardet)

# define group index
group &lt;- rep(1:20,each=5)

# 5-fold cross validation using group lasso 
# penalized logisitic regression
cv &lt;- cv.gglasso(x=bardet$x, y=bardet$y, group=group, loss="ls",
pred.loss="L2", lambda.factor=0.05, nfolds=5)

</code></pre>

<hr>
<h2 id='gglasso'>Fits the regularization paths for group-lasso penalized learning problems</h2><span id='topic+gglasso'></span>

<h3>Description</h3>

<p>Fits regularization paths for group-lasso penalized learning problems at a
sequence of regularization parameters lambda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gglasso(
  x,
  y,
  group = NULL,
  loss = c("ls", "logit", "sqsvm", "hsvm", "wls"),
  nlambda = 100,
  lambda.factor = ifelse(nobs &lt; nvars, 0.05, 0.001),
  lambda = NULL,
  pf = sqrt(bs),
  weight = NULL,
  dfmax = as.integer(max(group)) + 1,
  pmax = min(dfmax * 1.2, as.integer(max(group))),
  eps = 1e-08,
  maxit = 3e+08,
  delta,
  intercept = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gglasso_+3A_x">x</code></td>
<td>
<p>matrix of predictors, of dimension <code class="reqn">n \times p</code>; each row
is an observation vector.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_y">y</code></td>
<td>
<p>response variable. This argument should be quantitative for
regression (least squares), and a two-level factor for classification
(logistic model, huberized SVM, squared SVM).</p>
</td></tr>
<tr><td><code id="gglasso_+3A_group">group</code></td>
<td>
<p>a vector of consecutive integers describing the grouping of the
coefficients (see example below).</p>
</td></tr>
<tr><td><code id="gglasso_+3A_loss">loss</code></td>
<td>
<p>a character string specifying the loss function to use, valid
options are: </p>
 <ul>
<li> <p><code>"ls"</code> least squares loss (regression),
</p>
</li>
<li> <p><code>"logit"</code> logistic loss (classification).  </p>
</li>
<li> <p><code>"hsvm"</code>
Huberized squared hinge loss (classification), </p>
</li>
<li> <p><code>"sqsvm"</code> Squared
hinge loss (classification), </p>
</li></ul>
<p>Default is <code>"ls"</code>.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_nlambda">nlambda</code></td>
<td>
<p>the number of <code>lambda</code> values - default is 100.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_lambda.factor">lambda.factor</code></td>
<td>
<p>the factor for getting the minimal lambda in
<code>lambda</code> sequence, where <code>min(lambda)</code> = <code>lambda.factor</code> *
<code>max(lambda)</code>.  <code>max(lambda)</code> is the smallest value of
<code>lambda</code> for which all coefficients are zero. The default depends on
the relationship between <code class="reqn">n</code> (the number of rows in the matrix of
predictors) and <code class="reqn">p</code> (the number of predictors). If <code class="reqn">n &gt;= p</code>, the
default is <code>0.001</code>, close to zero.  If <code class="reqn">n&lt;p</code>, the default is
<code>0.05</code>.  A very small value of <code>lambda.factor</code> will lead to a
saturated fit. It takes no effect if there is user-defined <code>lambda</code>
sequence.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_lambda">lambda</code></td>
<td>
<p>a user supplied <code>lambda</code> sequence. Typically, by leaving
this option unspecified users can have the program compute its own
<code>lambda</code> sequence based on <code>nlambda</code> and <code>lambda.factor</code>.
Supplying a value of <code>lambda</code> overrides this. It is better to supply a
decreasing sequence of <code>lambda</code> values than a single (small) value, if
not, the program will sort user-defined <code>lambda</code> sequence in
decreasing order automatically.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_pf">pf</code></td>
<td>
<p>penalty factor, a vector in length of bn (bn is the total number of
groups). Separate penalty weights can be applied to each group of
<code class="reqn">\beta</code>s to allow differential shrinkage. Can be 0 for some
groups, which implies no shrinkage, and results in that group always being
included in the model. Default value for each entry is the square-root of
the corresponding size of each group.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_weight">weight</code></td>
<td>
<p>a <code class="reqn">nxn</code> observation weight matrix in the where <code class="reqn">n</code> is
the number of observations. Only used if <code>loss='wls'</code> is specified.
Note that cross-validation is NOT IMPLEMENTED for <code>loss='wls'</code>.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_dfmax">dfmax</code></td>
<td>
<p>limit the maximum number of groups in the model. Useful for very
large <code>bs</code> (group size), if a partial path is desired. Default is
<code>bs+1</code>.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_pmax">pmax</code></td>
<td>
<p>limit the maximum number of groups ever to be nonzero. For
example once a group enters the model, no matter how many times it exits or
re-enters model through the path, it will be counted only once. Default is
<code>min(dfmax*1.2,bs)</code>.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_eps">eps</code></td>
<td>
<p>convergence termination tolerance. Defaults value is <code>1e-8</code>.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of outer-loop iterations allowed at fixed lambda
value. Default is 3e8. If models do not converge, consider increasing
<code>maxit</code>.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_delta">delta</code></td>
<td>
<p>the parameter <code class="reqn">\delta</code> in <code>"hsvm"</code> (Huberized
squared hinge loss). Default is 1.</p>
</td></tr>
<tr><td><code id="gglasso_+3A_intercept">intercept</code></td>
<td>
<p>Whether to include intercept in the model. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the objective function for <code>"ls"</code> least squares is
</p>
<p style="text-align: center;"><code class="reqn">RSS/(2*n) + lambda * penalty;</code>
</p>
<p> for <code>"hsvm"</code> Huberized squared
hinge loss, <code>"sqsvm"</code> Squared hinge loss and <code>"logit"</code> logistic
regression, the objective function is </p>
<p style="text-align: center;"><code class="reqn">-loglik/n + lambda * penalty.</code>
</p>

<p>Users can also tweak the penalty by choosing different penalty factor.
</p>
<p>For computing speed reason, if models are not converging or running slow,
consider increasing <code>eps</code>, decreasing <code>nlambda</code>, or increasing
<code>lambda.factor</code> before increasing <code>maxit</code>.
</p>


<h3>Value</h3>

<p>An object with S3 class <code><a href="#topic+gglasso">gglasso</a></code>.  </p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the call
that produced this object</p>
</td></tr> <tr><td><code>b0</code></td>
<td>
<p>intercept sequence of length
<code>length(lambda)</code></p>
</td></tr> <tr><td><code>beta</code></td>
<td>
<p>a <code>p*length(lambda)</code> matrix of
coefficients.</p>
</td></tr> <tr><td><code>df</code></td>
<td>
<p>the number of nonzero groups for each value of
<code>lambda</code>.</p>
</td></tr> <tr><td><code>dim</code></td>
<td>
<p>dimension of coefficient matrix (ices)</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the actual sequence of <code>lambda</code> values used</p>
</td></tr>
<tr><td><code>npasses</code></td>
<td>
<p>total number of iterations (the most inner loop) summed over
all lambda values</p>
</td></tr> <tr><td><code>jerr</code></td>
<td>
<p>error flag, for warnings and errors, 0 if no
error.</p>
</td></tr> <tr><td><code>group</code></td>
<td>
<p>a vector of consecutive integers describing the
grouping of the coefficients.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yi Yang and Hui Zou<br /> Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for
Computing Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and
Computing</em>. 25(6), 1129-1141.<br /> BugReport:
<a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>


<h3>See Also</h3>

<p><code>plot.gglasso</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load bardet data set
data(bardet)

# define group index
group1 &lt;- rep(1:20,each=5)

# fit group lasso penalized least squares
m1 &lt;- gglasso(x=bardet$x,y=bardet$y,group=group1,loss="ls")

# load colon data set
data(colon)

# define group index
group2 &lt;- rep(1:20,each=5)

# fit group lasso penalized logistic regression
m2 &lt;- gglasso(x=colon$x,y=colon$y,group=group2,loss="logit")

</code></pre>

<hr>
<h2 id='plot.cv.gglasso'>plot the cross-validation curve produced by cv.gglasso</h2><span id='topic+plot.cv.gglasso'></span>

<h3>Description</h3>

<p>Plots the cross-validation curve, and upper and lower standard deviation
curves, as a function of the <code>lambda</code> values used. This function is
modified based on the <code>plot.cv</code> function from the <code>glmnet</code>
package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.gglasso'
plot(x, sign.lambda = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cv.gglasso_+3A_x">x</code></td>
<td>
<p>fitted <code><a href="#topic+cv.gglasso">cv.gglasso</a></code> object</p>
</td></tr>
<tr><td><code id="plot.cv.gglasso_+3A_sign.lambda">sign.lambda</code></td>
<td>
<p>either plot against <code>log(lambda)</code> (default) or its
negative if <code>sign.lambda=-1</code>.</p>
</td></tr>
<tr><td><code id="plot.cv.gglasso_+3A_...">...</code></td>
<td>
<p>other graphical parameters to plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A plot is produced.
</p>


<h3>Author(s)</h3>

<p>Yi Yang and Hui Zou<br /> Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for
Computing Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and
Computing</em>. 25(6), 1129-1141.<br /> BugReport:
<a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>
<p>Friedman, J., Hastie, T., and Tibshirani, R. (2010), &ldquo;Regularization paths
for generalized linear models via coordinate descent,&rdquo; <em>Journal of
Statistical Software</em>, 33, 1.<br /> <a href="http://www.jstatsoft.org/v33/i01/">http://www.jstatsoft.org/v33/i01/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.gglasso">cv.gglasso</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load data set
data(colon)

# define group index
group &lt;- rep(1:20,each=5)

# 5-fold cross validation using group lasso 
# penalized logisitic regression
cv &lt;- cv.gglasso(x=colon$x, y=colon$y, group=group, loss="logit",
pred.loss="misclass", lambda.factor=0.05, nfolds=5)

# make a CV plot
plot(cv)
</code></pre>

<hr>
<h2 id='plot.gglasso'>Plot solution paths from a &quot;gglasso&quot; object</h2><span id='topic+plot.gglasso'></span>

<h3>Description</h3>

<p>Produces a coefficient profile plot of the coefficient paths for a fitted
<code><a href="#topic+gglasso">gglasso</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gglasso'
plot(x, group = FALSE, log.l = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.gglasso_+3A_x">x</code></td>
<td>
<p>fitted <code><a href="#topic+gglasso">gglasso</a></code> model</p>
</td></tr>
<tr><td><code id="plot.gglasso_+3A_group">group</code></td>
<td>
<p>what is on the Y-axis. Plot the norm of each group if
<code>TRUE</code>. Plot each coefficient if <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="plot.gglasso_+3A_log.l">log.l</code></td>
<td>
<p>what is on the X-axis. Plot against the log-lambda sequence if
<code>TRUE</code>. Plot against the lambda sequence if <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="plot.gglasso_+3A_...">...</code></td>
<td>
<p>other graphical parameters to plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A coefficient profile plot is produced.
</p>


<h3>Author(s)</h3>

<p>Yi Yang and Hui Zou<br /> Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for
Computing Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and
Computing</em>. 25(6), 1129-1141.<br /> BugReport:
<a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load data set
data(bardet)

# define group index
group &lt;- rep(1:20,each=5)

# fit group lasso
m1 &lt;- gglasso(x=bardet$x,y=bardet$y,group=group,loss="ls")

# make plots
par(mfrow=c(1,3))
plot(m1) # plots the coefficients against the log-lambda sequence 
plot(m1,group=TRUE) # plots group norm against the log-lambda sequence 
plot(m1,log.l=FALSE) # plots against the lambda sequence
</code></pre>

<hr>
<h2 id='predict.cv.gglasso'>make predictions from a &quot;cv.gglasso&quot; object.</h2><span id='topic+predict.cv.gglasso'></span>

<h3>Description</h3>

<p>This function makes predictions from a cross-validated <code>gglasso</code> model,
using the stored <code>"gglasso.fit"</code> object, and the optimal value chosen
for <code>lambda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.gglasso'
predict(object, newx, s = c("lambda.1se", "lambda.min"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cv.gglasso_+3A_object">object</code></td>
<td>
<p>fitted <code><a href="#topic+cv.gglasso">cv.gglasso</a></code> object.</p>
</td></tr>
<tr><td><code id="predict.cv.gglasso_+3A_newx">newx</code></td>
<td>
<p>matrix of new values for <code>x</code> at which predictions are to be
made. Must be a matrix. See documentation for <code>predict.gglasso</code>.</p>
</td></tr>
<tr><td><code id="predict.cv.gglasso_+3A_s">s</code></td>
<td>
<p>value(s) of the penalty parameter <code>lambda</code> at which
predictions are required. Default is the value <code>s="lambda.1se"</code> stored
on the CV object. Alternatively <code>s="lambda.min"</code> can be used. If
<code>s</code> is numeric, it is taken as the value(s) of <code>lambda</code> to be
used.</p>
</td></tr>
<tr><td><code id="predict.cv.gglasso_+3A_...">...</code></td>
<td>
<p>not used. Other arguments to predict.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function makes it easier to use the results of cross-validation to make
a prediction.
</p>


<h3>Value</h3>

<p>The returned object depends on the ... argument which is passed
on to the <code><a href="stats.html#topic+predict">predict</a></code> method for <code><a href="#topic+gglasso">gglasso</a></code> objects.
</p>


<h3>Author(s)</h3>

<p>Yi Yang and Hui Zou<br /> Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for
Computing Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and
Computing</em>. 25(6), 1129-1141.<br /> BugReport:
<a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.gglasso">cv.gglasso</a></code>, and <code><a href="#topic+coef.cv.gglasso">coef.cv.gglasso</a></code>
methods.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load data set
data(colon)

# define group index
group &lt;- rep(1:20,each=5)

# 5-fold cross validation using group lasso 
# penalized logisitic regression
cv &lt;- cv.gglasso(x=colon$x, y=colon$y, group=group, loss="logit",
pred.loss="misclass", lambda.factor=0.05, nfolds=5)

# the coefficients at lambda = lambda.min, newx = x[1,]
pre = predict(cv$gglasso.fit, newx = colon$x[1:10,], 
s = cv$lambda.min, type = "class")
</code></pre>

<hr>
<h2 id='predict.gglasso'>make predictions from a &quot;gglasso&quot; object.</h2><span id='topic+predict.gglasso'></span>

<h3>Description</h3>

<p>Similar to other predict methods, this functions predicts fitted values and
class labels from a fitted <code><a href="#topic+gglasso">gglasso</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gglasso'
predict(object, newx, s = NULL, type = c("class", "link"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.gglasso_+3A_object">object</code></td>
<td>
<p>fitted <code><a href="#topic+gglasso">gglasso</a></code> model object.</p>
</td></tr>
<tr><td><code id="predict.gglasso_+3A_newx">newx</code></td>
<td>
<p>matrix of new values for <code>x</code> at which predictions are to be
made. Must be a matrix.</p>
</td></tr>
<tr><td><code id="predict.gglasso_+3A_s">s</code></td>
<td>
<p>value(s) of the penalty parameter <code>lambda</code> at which
predictions are required. Default is the entire sequence used to create the
model.</p>
</td></tr>
<tr><td><code id="predict.gglasso_+3A_type">type</code></td>
<td>
<p>type of prediction required: </p>
 <ul>
<li><p> Type <code>"link"</code>,
for regression it returns the fitted response; for classification it gives
the linear predictors.  </p>
</li>
<li><p> Type <code>"class"</code>, only valid for
classification, it produces the predicted class label corresponding to the
maximum probability.</p>
</li></ul>
</td></tr>
<tr><td><code id="predict.gglasso_+3A_...">...</code></td>
<td>
<p>Not used. Other arguments to predict.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>s</code> is the new vector at which predictions are requested. If <code>s</code>
is not in the lambda sequence used for fitting the model, the <code>predict</code>
function will use linear interpolation to make predictions. The new values
are interpolated using a fraction of predicted values from both left and
right <code>lambda</code> indices.
</p>


<h3>Value</h3>

<p>The object returned depends on type.
</p>


<h3>Author(s)</h3>

<p>Yi Yang and Hui Zou<br /> Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for
Computing Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and
Computing</em>. 25(6), 1129-1141.<br /> BugReport:
<a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+coef">coef</a></code> method
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load data set
data(colon)

# define group index
group &lt;- rep(1:20,each=5)

# fit group lasso
m1 &lt;- gglasso(x=colon$x,y=colon$y,group=group,loss="logit")

# predicted class label at x[10,]
print(predict(m1,type="class",newx=colon$x[10,]))

# predicted linear predictors at x[1:5,]
print(predict(m1,type="link",newx=colon$x[1:5,]))

</code></pre>

<hr>
<h2 id='print.gglasso'>print a gglasso object</h2><span id='topic+print.gglasso'></span>

<h3>Description</h3>

<p>Print the nonzero group counts at each lambda along the gglasso path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gglasso'
print(x, digits = max(3, getOption("digits") - 3), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.gglasso_+3A_x">x</code></td>
<td>
<p>fitted <code><a href="#topic+gglasso">gglasso</a></code> object</p>
</td></tr>
<tr><td><code id="print.gglasso_+3A_digits">digits</code></td>
<td>
<p>significant digits in printout</p>
</td></tr>
<tr><td><code id="print.gglasso_+3A_...">...</code></td>
<td>
<p>additional print arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Print the information about the nonzero group counts at each lambda step in
the <code><a href="#topic+gglasso">gglasso</a></code> object. The result is a two-column matrix with
columns <code>Df</code> and <code>Lambda</code>. The <code>Df</code> column is the number of
the groups that have nonzero within-group coefficients, the <code>Lambda</code>
column is the the corresponding lambda.
</p>


<h3>Value</h3>

<p>a two-column matrix, the first columns is the number of nonzero
group counts and the second column is <code>Lambda</code>.
</p>


<h3>Author(s)</h3>

<p>Yi Yang and Hui Zou<br /> Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;A Fast Unified Algorithm for
Computing Group-Lasso Penalized Learning Problems,&rdquo; <em>Statistics and
Computing</em>. 25(6), 1129-1141.<br /> BugReport:
<a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load gglasso library
library(gglasso)

# load data set
data(colon)

# define group index
group &lt;- rep(1:20,each=5)

# fit group lasso
m1 &lt;- gglasso(x=colon$x,y=colon$y,group=group,loss="logit")

# print out results
print(m1)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
