<!DOCTYPE html><html lang="en"><head><title>Help for package Ultimixt</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Ultimixt}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Ultimixt-package'>
<p>set of R functions for estimating the parameters of mixture distribution with a Bayesian non-informative</p>
prior</a></li>
<li><a href='#K.MixPois'>
<p>Sample from a Poisson mixture posterior associated with a noninformative prior and obtained by Metropolis-within-Gibbs sampling</p></a></li>
<li><a href='#K.MixReparametrized'>
<p>Sample from a Gaussian mixture posterior associated with a noninformative prior and obtained by Metropolis-within-Gibbs sampling</p></a></li>
<li><a href='#Plot.MixReparametrized'>
<p>plot of the MCMC output produced by K.MixReparametrized</p></a></li>
<li><a href='#SM.MAP.MixReparametrized'>
<p>summary of the output produced by K.MixReparametrized</p></a></li>
<li><a href='#SM.MixPois'>
<p>summary of the output produced by K.MixPois</p></a></li>
<li><a href='#SM.MixReparametrized'>
<p>summary of the output produced by K.MixReparametrized</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Analysis of Location-Scale Mixture Models using a
Weakly Informative Prior</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2017-03-07</td>
</tr>
<tr>
<td>Author:</td>
<td>Kaniav Kamary, Kate Lee</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kaniav Kamary &lt;kamary@ceremade.dauphine.fr&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>coda, gtools, graphics, grDevices, stats</td>
</tr>
<tr>
<td>Description:</td>
<td>A generic reference Bayesian analysis of unidimensional mixture distributions obtained by a location-scale parameterisation of the model is implemented. The including functions simulate and summarize posterior samples for location-scale mixture models using a weakly informative prior. There is no need to define priors for scale-location parameters except two hyperparameters in which are associated with a Dirichlet prior for weights and a simplex.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2.0)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-03-08 20:42:00 UTC; kkamary</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-03-09 00:33:27</td>
</tr>
</table>
<hr>
<h2 id='Ultimixt-package'>
set of R functions for estimating the parameters of mixture distribution with a Bayesian non-informative
prior
</h2><span id='topic+Ultimixt-package'></span><span id='topic+Ultimixt'></span>

<h3>Description</h3>

<p>Despite a comprehensive literature on estimating mixtures of Gaussian distributions, there does not exist a
well-accepted reference Bayesian approach to such models. One reason for the difficulty is the general prohibition
against using improper priors (Fruhwirth-Schnatter, 2006) due to the ill-posed nature of such statistical objects.
Kamary, Lee and Robert (2017) took advantage of a mean-variance reparametrisation of a Gaussian mixture model to propose
improper but valid reference priors in this setting. This R package implements the proposal and computes posterior
estimates of the parameters of a Gaussian mixture distribution. The approach applies with an arbitrary number of
components. The Ultimixt R package contains an MCMC algorithm function and further functions for
summarizing and plotting posterior estimates of the model parameters for any number of components. 
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> Ultimixt</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 2.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2017-03-07</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;=2.0)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Beyond simulating MCMC samples from the posterior distribution of the Gaussian mixture model, this package also produces
summaries of the  MCMC outputs through numerical and graphical methods. 
</p>
<p>Note:  The proposed parameterisation of the Gaussian mixture distribution is given by 
</p>
<p style="text-align: center;"><code class="reqn">
f(x| \mu, \sigma , {\bf p}, \varphi, {\bf \varpi, \xi})=\sum_{i=1}^k p_i f\left(x| \mu + \sigma \gamma_i/\sqrt{p_i}, \sigma \eta_i/\sqrt{p_i}\right)
</code>
</p>

<p>under the non-informative prior <code class="reqn">\pi(\mu, \sigma)=1/\sigma</code>. Here, the vector of the <code class="reqn">\gamma_i=\varphi
\Psi_i\Big({\bf \varpi}, {\bf p}\Big)_i</code>'s belongs to an hypersphere of radius <code class="reqn">\varphi</code> intersecting with an
hyperplane.  It is thus expressed in terms of spherical coordinates within that hyperplane that depend on <code class="reqn">k-2</code>
angular coordinates <code class="reqn">\varpi_i</code>. Similarly, the vector of <code class="reqn">\eta_i=\sqrt{1-\varphi^2}\Psi_i\Big({\bf
\xi}\Big)_i</code>'s can be turned
into a spherical coordinate in a k-dimensional Euclidean space, involving a radial coordinate
<code class="reqn">\sqrt{1-\varphi^2}</code> and <code class="reqn">k-1</code> angular coordinates <code class="reqn">\xi_i</code>. A natural prior for <code class="reqn">\varpi</code> is made of uniforms, <code class="reqn">\varpi_1, \ldots, \varpi_{k-3}\sim U[0, \pi]</code> and <code class="reqn">\varpi_{k-2} \sim U[0, 2\pi]</code>, and for <code class="reqn">\varphi</code>, we consider a beta prior <code class="reqn">Beta(\alpha, \alpha)</code>. A reference prior on the angles <code class="reqn">\xi</code> is <code class="reqn">(\xi_1, \ldots, \xi_{k-1})\sim U[0, \pi/2]^{k-1}</code> and a Dirichlet prior <code class="reqn">Dir(\alpha_0, \ldots, \alpha_0)</code> is assigned to the weights <code class="reqn">p_1, \ldots, p_k</code>. 
</p>
<p>For a Poisson mixture, we consider 
</p>
<p style="text-align: center;"><code class="reqn">
f(x|\lambda_1, \ldots, \lambda_k)=\frac{1}{x!}\sum_{i=1}^k p_i \lambda_i^x e^{-\lambda_i}
</code>
</p>

<p>with a reparameterisation as <code class="reqn">\lambda=\bf{E}[X]</code> and <code class="reqn">\lambda_i=\lambda
\gamma_i/p_i</code>. In this case, we can use the equivalent to the Jeffreys prior for the Poisson
distribution, namely, <code class="reqn">\pi(\lambda)=1/\lambda</code>, since it leads to a
well-defined posterior with a single positive observation. 
</p>


<h3>Author(s)</h3>

<p>Kaniav Kamary
</p>
<p>Maintainer: <a href="mailto:kamary@ceremade.dauphine.fr">kamary@ceremade.dauphine.fr</a>
</p>


<h3>References</h3>

<p>Fruhwirth-Schnatter, S. (2006). Finite Mixture and Markov Switching Models. Springer-Verlag, New York, New York. 
</p>
<p>Kamary, K., Lee, J.Y., and Robert, C.P. (2017) Weakly informative reparameterisation for location-scale mixtures. arXiv.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Ultimixt">Ultimixt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	#K.MixReparametrized(faithful[,2], k=2, alpha0=.5, alpha=.5, Nsim=10000)
</code></pre>

<hr>
<h2 id='K.MixPois'>
Sample from a Poisson mixture posterior associated with a noninformative prior and obtained by Metropolis-within-Gibbs sampling
</h2><span id='topic+K.MixPois'></span>

<h3>Description</h3>

<p>After having reparameterized the Poisson mixture based on the global mean of the mixture distribution (Kamary et al. (2017)), a Jeffreys prior can be used since it leads a well-defined posterior with a single positive observation. This function returns a sample from the posterior distribution of the parameters of the Poisson mixture. To do so, a Metropolis-within-Gibbs algorithm is applied with an adaptive calibration of the proposal distribution scales. Adaptation is driven by the formally optimal acceptance rates of <code class="reqn">0.44</code> and <code class="reqn">0.234</code> in one and larger dimensions, respectively (Roberts et al.,1997). This algorithm monitors the convergence of the MCMC sequences via Gelman's and Rubin's (1992) criterion.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>K.MixPois(xobs, k, alpha0, alpha, Nsim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="K.MixPois_+3A_xobs">xobs</code></td>
<td>

<p>vector of the observations or dataset
</p>
</td></tr>
<tr><td><code id="K.MixPois_+3A_k">k</code></td>
<td>

<p>number of components in the mixture model
</p>
</td></tr>
<tr><td><code id="K.MixPois_+3A_alpha0">alpha0</code></td>
<td>

<p>hyperparameter of Dirichlet prior distribution of the mixture model weights which is .5 by default
</p>
</td></tr>
<tr><td><code id="K.MixPois_+3A_alpha">alpha</code></td>
<td>

<p>hyperparameter of beta prior distribution of the component mean hyperparameter (noted by <code class="reqn">\gamma_i</code>. See Kamary et al. (2017)) which is .5 by default
</p>
</td></tr>
<tr><td><code id="K.MixPois_+3A_nsim">Nsim</code></td>
<td>

<p>number of MCMC iterations after calibration step of proposal scales
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output of this function contains a simulated sample for each parameter of the mixture distribution, the evolution of
the proposal scales and acceptance rates over the number of iterations during the calibration stage, and their final
values after calibration.
</p>


<h3>Value</h3>

<p>The output of this function contains a list of the following variables, where the dimension of the vectors is the number of simulations:
</p>
<table role = "presentation">
<tr><td><code>mean global</code></td>
<td>
<p>vector of simulated draws from the conditional posterior of the mixture model mean</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>matrix of simulated draws from the conditional posterior of the mixture model weights with a number of columns equal to the number of components <code class="reqn">k</code></p>
</td></tr>
<tr><td><code>gammas</code></td>
<td>
<p>matrix of simulated draws from the conditional posterior of the component mean hyperparameters</p>
</td></tr>
<tr><td><code>accept rat</code></td>
<td>
<p>vector of resulting acceptance rates of the proposal distributions without calibration step of the proposal scales</p>
</td></tr>
<tr><td><code>optimal para</code></td>
<td>
<p>vector of resulting proposal scales after optimisation obtained by adaptive MCMC</p>
</td></tr>
<tr><td><code>adapt rat</code></td>
<td>
<p> list of acceptance rates of batch of 50 iterations obtained when calibrating the proposal scales by adaptive MCMC. The number of columns depends on the number of proposal distributions.</p>
</td></tr>
<tr><td><code>adapt scale</code></td>
<td>
<p>list of proposal scales calibrated by adaptive MCMC for each batch of 50 iterations with respect to the optimal acceptance rate. The number of columns depends on the number of proposal distribution scales.</p>
</td></tr>
<tr><td><code>component means</code></td>
<td>
<p>matrix of MCMC samples of the component means of the mixture model with a number of columns equal to <code class="reqn">k</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p>If the number of MCMC iterations specified in the input of this function exceeds 15,000, after each 1000 supplementry iterations the convergence of simulated chains is checked using the convergence monitoring technique by Gelman and Rubin (1992).
</p>


<h3>Author(s)</h3>

<p>Kaniav Kamary
</p>


<h3>References</h3>

<p>Kamary, K., Lee, J.Y., and Robert, C.P. (2017) Weakly informative reparameterisation of location-scale mixtures. arXiv.
</p>
<p>Robert, C. and Casella, G. (2009). Introducing Monte Carlo Methods with R. Springer-Verlag.
</p>
<p>Roberts, G. O., Gelman, A. and Gilks, W. R. (1997). Weak convergence and optimal scaling of random walk Metropolis algorithms. Ann. Applied Probability, 7, 110&ndash;120.
</p>
<p>Gelman, A. and Rubin, D. (1992). Inference from iterative simulation using multiple sequences (with discussion). Statistical Science, 457&ndash;472.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Ultimixt">Ultimixt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#N=500
#U =runif(N)                                            
#xobs = rep(NA,N)
#for(i in 1:N){
#    if(U[i]&lt;.6){
#        xobs[i] = rpois(1,lambda=1)
#    }else{
#        xobs[i] = rpois(1,lambda=5)
#    }
#}
#estimate=K.MixPois(xobs, k=2, alpha0=.5, alpha=.5, Nsim=10000)
</code></pre>

<hr>
<h2 id='K.MixReparametrized'>
Sample from a Gaussian mixture posterior associated with a noninformative prior and obtained by Metropolis-within-Gibbs sampling
</h2><span id='topic+K.MixReparametrized'></span>

<h3>Description</h3>

<p>This function returns a sample simulated from the posterior distribution of the parameters of a Gaussian mixture under a non-informative prior. This prior is derived from a mean-variance reparameterisation of the mixture distribution, as proposed by Kamary et al. (2017). The algorithm is a Metropolis-within-Gibbs scheme with an adaptive calibration of the proposal distribution scales. Adaptation is driven by the formally optimal acceptance rates of <code class="reqn">0.44</code> and <code class="reqn">0.234</code> in one and larger dimensions, respectively (Roberts et al.,1997). This algorithm monitors the convergence of the MCMC sequences via Gelman's and Rubin's (1992) criterion.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>K.MixReparametrized(xobs, k, alpha0, alpha, Nsim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="K.MixReparametrized_+3A_xobs">xobs</code></td>
<td>

<p>vector of the observations or dataset
</p>
</td></tr>
<tr><td><code id="K.MixReparametrized_+3A_k">k</code></td>
<td>

<p>number of components in the mixture model
</p>
</td></tr>
<tr><td><code id="K.MixReparametrized_+3A_alpha0">alpha0</code></td>
<td>

<p>hyperparameter of Dirichlet prior distribution of the mixture model weights which is .5 by default 
</p>
</td></tr>
<tr><td><code id="K.MixReparametrized_+3A_alpha">alpha</code></td>
<td>

<p>hyperparameter of beta prior distribution of the radial coordinate which is .5 by default
</p>
</td></tr>
<tr><td><code id="K.MixReparametrized_+3A_nsim">Nsim</code></td>
<td>

<p>number of MCMC iterations after calibration step of proposal scales
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output of this function contains a simulated sample for each parameter of the mixture distribution, the evolution of
the proposal scales and acceptance rates over the number of iterations during the calibration stage, and their final
values after calibration.
</p>


<h3>Value</h3>

<p>The output of this function is a list of the following variables, where the dimension of the vectors is the number of simulations:
</p>
<table role = "presentation">
<tr><td><code>mean global</code></td>
<td>
<p>vector of simulated draws from the conditional posterior of the mixture model mean</p>
</td></tr>
<tr><td><code>sigma global</code></td>
<td>
<p>vector of simulated draws from the conditional posterior of the mixture model standard deviation</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>matrix of simulated draws from the conditional posterior of the mixture model weights with a number of columns equal to the number of components <code class="reqn">k</code></p>
</td></tr>
<tr><td><code>angles xi</code></td>
<td>
<p>matrix of simulated draws from the conditional posterior of the angular coordinates of the component standard deviations with a number of columns equal to <code class="reqn">k-1</code></p>
</td></tr>
<tr><td><code>phi</code></td>
<td>
<p>vector of simulated draws from the conditional posterior of the radian coordinate</p>
</td></tr>
<tr><td><code>angles varpi</code></td>
<td>
<p>matrix of simulated draws from the conditional posterior of the angular coordinates of the component means with a number of columns equal to <code class="reqn">k-2</code></p>
</td></tr>
<tr><td><code>accept rat</code></td>
<td>
<p>vector of resulting acceptance rates of the proposal distributions without calibration step of the proposal scales</p>
</td></tr>
<tr><td><code>optimal para</code></td>
<td>
<p>vector of resulting proposal scales after optimisation obtained by adaptive MCMC</p>
</td></tr>
<tr><td><code>adapt rat</code></td>
<td>
<p> list of acceptance rates of batch of 50 iterations obtained when calibrating the proposal scales by adaptive MCMC. The number of columns depends on the number of proposal distributions.</p>
</td></tr>
<tr><td><code>adapt scale</code></td>
<td>
<p>list of proposal scales calibrated by adaptive MCMC for each batch of 50 iterations with respect to the optimal acceptance rate. The number of columns depends on the number of proposal distribution scales.</p>
</td></tr>
<tr><td><code>component means</code></td>
<td>
<p>matrix of MCMC samples of the component means of the mixture model with a number of columns equal to <code class="reqn">k</code></p>
</td></tr>
<tr><td><code>component sigmas</code></td>
<td>
<p>matrix of MCMC samples of the component standard deviations of the mixture model with a number of columns equal to <code class="reqn">k</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p>If the number of MCMC iterations specified in the input of this function exceeds 15,000, after each 1000 supplementry iterations the convergence of simulated chains is checked using the convergence monitoring technique by Gelman and Rubin (1992).
</p>


<h3>Author(s)</h3>

<p>Kaniav Kamary
</p>


<h3>References</h3>

<p>Kamary, K., Lee, J.Y., and Robert, C.P. (2017) Weakly informative reparameterisation of location-scale mixtures. arXiv.
</p>
<p>Robert, C. and Casella, G. (2009). Introducing Monte Carlo Methods with R. Springer-Verlag.
</p>
<p>Roberts, G. O., Gelman, A. and Gilks, W. R. (1997). Weak convergence and optimal scaling of random walk Metropolis algorithms. Ann. Applied Probability, 7, 110&ndash;120.
</p>
<p>Gelman, A. and Rubin, D. (1992). Inference from iterative simulation using multiple sequences (with discussion). Statistical Science, 457&ndash;472.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Ultimixt">Ultimixt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#data(faithful)
#xobs=faithful[,1]
#estimate=K.MixReparametrized(xobs, k=2, alpha0=.5, alpha=.5, Nsim=10000)
</code></pre>

<hr>
<h2 id='Plot.MixReparametrized'>
plot of the MCMC output produced by K.MixReparametrized 
</h2><span id='topic+Plot.MixReparametrized'></span>

<h3>Description</h3>

<p>This is a generic function for a graphical rendering of the MCMC samples produced by K.MixReparametrized function. The
function draws boxplots for unimodal variables and for multimodal arguments after clustering them by applying a k-means
algorithm. It also plots line charts for other variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Plot.MixReparametrized(xobs, estimate)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Plot.MixReparametrized_+3A_xobs">xobs</code></td>
<td>

<p>vector of the observations
</p>
</td></tr>
<tr><td><code id="Plot.MixReparametrized_+3A_estimate">estimate</code></td>
<td>

<p>output of the K. MixReparametrized function
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Boxplots are produced using the boxplot.default method.
</p>


<h3>Value</h3>

<p>The output of this function consists of 
</p>
<table role = "presentation">
<tr><td><code>boxplot</code></td>
<td>
<p> three boxplots for the radial coordinates, the mean and the standard deviation of the mixture distribution, <code class="reqn">k</code> boxplots for each of the mixture model weights,  component means and component standard deviations.</p>
</td></tr> 
<tr><td><code>histogram</code></td>
<td>
<p> an histogram of the observations against an overlaid curve of the density estimate, obtained by averaging over all mixtures corresponding to the MCMC draws,</p>
</td></tr>
<tr><td><code>line chart</code></td>
<td>
<p> line charts that report the evolution of the proposal scales and of the acceptance rates over the number of batch of 50 iterations.</p>
</td></tr>  
</table>


<h3>Note</h3>

<p>The mixture density estimate is based on the draws simulated of the parameters obtained by K.MixReparametrized function.
</p>


<h3>Author(s)</h3>

<p>Kaniav Kamary
</p>


<h3>References</h3>

<p>Kamary, K., Lee, J.Y., and Robert, C.P. (2017) Weakly informative reparameterisation of location-scale mixtures. arXiv.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+K.MixReparametrized">K.MixReparametrized</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#data(faithful)
#xobs=faithful[,1]
#estimate=K.MixReparametrized(xobs, k=2, alpha0=.5, alpha=.5, Nsim=20000)
#plo=Plot.MixReparametrized(xobs, estimate)
</code></pre>

<hr>
<h2 id='SM.MAP.MixReparametrized'>
summary of the output produced by K.MixReparametrized
</h2><span id='topic+SM.MAP.MixReparametrized'></span>

<h3>Description</h3>

<p>Label switching in a simulated Markov chain produced by K.MixReparametrized is removed by the technique of Marin et al.
(2004). Namely, component labels are reorded by the shortest Euclidian distance between a posterior sample and the maximum a
posteriori (MAP) estimate. Let <code class="reqn">\theta_i</code> be the <code class="reqn">i</code>-th vector of computed component means, standard deviations
and weights. The MAP estimate is derived from the MCMC sequence and denoted by <code class="reqn">\theta_{MAP}</code>. For a permutation
<code class="reqn">\tau \in \Im_k</code> the labelling of <code class="reqn">\theta_i</code> is reordered by
</p>
<p style="text-align: center;"><code class="reqn">
	\tilde{\theta}_i=\tau_i(\theta_i)
	</code>
</p>

<p>where <code class="reqn">\tau_i=\arg \min_{\tau \in \Im_k} \mid \mid \tau(\theta_i)-\theta_{MAP}\mid \mid</code>.
</p>
<p>Angular parameters <code class="reqn">\xi_1^{(i)}, \ldots, \xi_{k-1}^{(i)}</code> and <code class="reqn">\varpi_1^{(i)}, \ldots, \varpi_{k-2}^{(i)}</code>s are
derived from <code class="reqn">\tilde{\theta}_i</code>. There exists an unique solution in <code class="reqn">\varpi_1^{(i)}, \ldots, \varpi_{k-2}^{(i)}</code>
while there are multiple solutions in <code class="reqn">\xi^{(i)}</code> due to the symmetry of <code class="reqn">\mid\cos(\xi) \mid</code> and
<code class="reqn">\mid\sin(\xi) \mid</code>. The output of <code class="reqn">\xi_1^{(i)}, \ldots, \xi_{k-1}^{(i)}</code> only includes angles on <code class="reqn">[-\pi, \pi]</code>.
</p>
<p>The label of components of <code class="reqn">\theta_i</code> (before the above transform) is defined by
</p>
<p style="text-align: center;"><code class="reqn">
	\tau_i^*=\arg \min_{\tau \in \Im_k}\mid \mid \theta_i-\tau(\theta_{MAP}) \mid \mid.
</code>
</p>

<p>The number of label switching occurrences is defined by the number of changes in <code class="reqn">\tau^*</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SM.MAP.MixReparametrized(estimate, xobs, alpha0, alpha)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SM.MAP.MixReparametrized_+3A_estimate">estimate</code></td>
<td>

<p>Output of K.MixReparametrized
</p>
</td></tr>
<tr><td><code id="SM.MAP.MixReparametrized_+3A_xobs">xobs</code></td>
<td>

<p>Data set
</p>
</td></tr>
<tr><td><code id="SM.MAP.MixReparametrized_+3A_alpha0">alpha0</code></td>
<td>

<p>Hyperparameter of Dirichlet prior distribution of the mixture model weights
</p>
</td></tr>
<tr><td><code id="SM.MAP.MixReparametrized_+3A_alpha">alpha</code></td>
<td>

<p>Hyperparameter of beta prior distribution of the radial coordinate
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Details.</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>MU</code></td>
<td>
<p>Matrix of MCMC samples of the component means of the mixture model</p>
</td></tr>
<tr><td><code>SIGMA</code></td>
<td>
<p>Matrix of MCMC samples of the component standard deviations of the
mixture model</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>Matrix of MCMC samples of the component weights of the mixture model</p>
</td></tr>
<tr><td><code>Ang_SIGMA</code></td>
<td>
<p>Matrix of computed <code class="reqn">\xi</code>'s corresponding to SIGMA</p>
</td></tr>
<tr><td><code>Ang_MU</code></td>
<td>
<p>Matrix of computed <code class="reqn">\varpi</code>'s corresponding to MU. This output only appears when <code class="reqn">k &gt; 2</code>.</p>
</td></tr>
<tr><td><code>Global_Mean</code></td>
<td>
<p>Mean, median and <code class="reqn">95\%</code> credible interval for the global mean parameter</p>
</td></tr>
<tr><td><code>Global_Std</code></td>
<td>
<p>Mean, median and <code class="reqn">95\%</code> credible interval for the global standard deviation parameter</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>
<p>Mean, median and <code class="reqn">95\%</code> credible interval for the radius parameter</p>
</td></tr>
<tr><td><code>component_mu</code></td>
<td>
<p>Mean, median and <code class="reqn">95\%</code> credible interval of MU</p>
</td></tr>
<tr><td><code>component_sigma</code></td>
<td>
<p>Mean, median and <code class="reqn">95\%</code> credible interval of SIGMA</p>
</td></tr>
<tr><td><code>component_p</code></td>
<td>
<p>Mean, median and <code class="reqn">95\%</code> credible interval of P</p>
</td></tr>
<tr><td><code>l_stay</code></td>
<td>
<p>Number of MCMC iterations between changes in labelling</p>
</td></tr>
<tr><td><code>n_switch</code></td>
<td>
<p>Number of label switching occurrences</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Note.</p>


<h3>Author(s)</h3>

<p>Kate Lee
</p>


<h3>References</h3>

<p>Marin, J.-M., Mengersen, K. and Robert, C. P. (2004) Bayesian Modelling
and Inference on Mixtures of Distributions, Handbook of Statistics, Elsevier,
Volume 25, Pages 459&ndash;507.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+K.MixReparametrized">K.MixReparametrized</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#data(faithful)
#xobs=faithful[,1]
#estimate=K.MixReparametrized(xobs,k=2,alpha0=0.5,alpha=0.5,Nsim=1e4)
#result=SM.MAP.MixReparametrized(estimate,xobs,alpha0=0.5,alpha=0.5)
</code></pre>

<hr>
<h2 id='SM.MixPois'>
summary of the output produced by K.MixPois
</h2><span id='topic+SM.MixPois'></span>

<h3>Description</h3>

<p>This generic function summarizes the MCMC samples produced by K.MixPois when several estimation methods have been invoked depending on the unimodality or multimodality of the argument.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SM.MixPois(estimate, xobs)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SM.MixPois_+3A_estimate">estimate</code></td>
<td>

<p>output of K.MixPois
</p>
</td></tr>
<tr><td><code id="SM.MixPois_+3A_xobs">xobs</code></td>
<td>

<p>vector of observations
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output of this function contains posterior point estimates for all parameters of the reparameterized Poisson mixture model.
It summarizes unimodal MCMC samples by computing
measures of centrality, including mean and median, while multimodal outputs require a preprocessing, due to the
label switching phenomenon (Jasra et al., 2005). The summary measures are then computed after performing a multi-dimensional k-means clustering (Hartigan and Wong, 1979) following the suggestion of Fruhwirth-Schnatter (2006).
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>lambda</code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the mixture model mean</p>
</td></tr>
<tr><td><code>gamma.i</code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the component mean hyperparameters; <code class="reqn">i=1, \ldots, k</code></p>
</td></tr>
<tr><td><code>weight.i</code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the component weights of the mixture distribution; <code class="reqn">i=1, \ldots, k</code></p>
</td></tr>
<tr><td><code>lambda.i</code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the component means of the mixture distribution; <code class="reqn">i=1, \ldots, k</code></p>
</td></tr>
<tr><td><code>Acc rat</code></td>
<td>
<p>vector of final acceptance rate of the proposal distributions of the algorithm with no calibration
stage for the proposal scales</p>
</td></tr>
<tr><td><code>Opt scale</code></td>
<td>
<p>vector of optimal proposal scales obtained the by calibration stage</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For multimodal outputs such as the mixture model weights, component means, and component mean hyperparameters, for each MCMC draw,
first the labels of the weights <code class="reqn">p_i, i=1, \ldots, k</code> and corresponding component means are permuted in
such a way that <code class="reqn">p_1\le \ldots \le p_k</code>. Then the posterior component means are partitioned into <code class="reqn">k</code>
clusters by applying a standard k-means algorithm with <code class="reqn">k</code> clusters, following Fruhwirth-Schnatter (2006) method. The obtained classification sequence was then used to reorder and identify the other component-specific parameters, namely component mean hyperparameters and weights.
For each group,  cluster centers are considered as parameter
estimates. 
</p>


<h3>Author(s)</h3>

<p>Kaniav Kamary
</p>


<h3>References</h3>

<p>Jasra, A., Holmes, C. and Stephens, D. (2005). Markov Chain Monte Carlo methods and the label switching problem in
Bayesian mixture modeling. Statistical Science, 20, 50&ndash;67.
</p>
<p>Hartigan, J. A. and Wong, M. A. (1979). A K-means clustering algorithm. Applied Statistics 28, 100&ndash;108.
</p>
<p>Fruhwirth-Schnatter, S. (2006). Finite mixture and Markov switching models. Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+K.MixPois">K.MixPois</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N=500
U =runif(N)                                            
xobs = rep(NA,N)
for(i in 1:N){
    if(U[i]&lt;.6){
        xobs[i] = rpois(1,lambda=1)
    }else{
        xobs[i] = rpois(1,lambda=5)
    }
}
#estimate=K.MixPois(xobs, k=2, alpha0=.5, alpha=.5, Nsim=10000)
#SM.MixPois(estimate, xobs)
#plot(estimate[[8]][,1],estimate[[2]][,1],pch=19,col="skyblue",cex=0.5,xlab="lambda",ylab="p")
#points(estimate[[8]][,2], estimate[[2]][,2], pch=19, col="gold", cex=0.5)
#points(c(1,5), c(0.6,0.4), pch=19, cex=1)
</code></pre>

<hr>
<h2 id='SM.MixReparametrized'>
summary of the output produced by K.MixReparametrized 
</h2><span id='topic+SM.MixReparametrized'></span>

<h3>Description</h3>

<p>This is a generic function that summarizes the MCMC samples produced by K.MixReparametrized. The function invokes
several estimation methods which choice depends on the unimodality or multimodality of the argument. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SM.MixReparametrized(xobs, estimate)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SM.MixReparametrized_+3A_xobs">xobs</code></td>
<td>

<p>vector of observations 
</p>
</td></tr>
<tr><td><code id="SM.MixReparametrized_+3A_estimate">estimate</code></td>
<td>

<p>output of K.MixReparametrized 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function outputs posterior  point estimates for all parameters of the mixture model. They mostly
differ from the generaly useless posterior means. The output summarizes unimodal MCMC samples by computing
measures of centrality, including mean and median, while multimodal outputs require a pre-processing, due to the
label switching phenomenon (Jasra et al., 2005). The summary measures are then computed after performing a multi-dimensional k-means clustering (Hartigan and Wong, 1979) following the suggestion of
Fruhwirth-Schnatter (2006). 
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Mean</code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the mixture model mean</p>
</td></tr>
<tr><td><code>Sd</code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the mixture model standard deviation</p>
</td></tr>
<tr><td><code>Phi</code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the radial coordinate</p>
</td></tr>
<tr><td><code>Angles. 1.</code></td>
<td>
<p>vector of means of the angular coordinates used for the component means in the mixture distribution</p>
</td></tr>
<tr><td><code>Angles. 2.</code></td>
<td>
<p>vector of means of the angular coordinates used for the component standard deviations in the mixture distribution</p>
</td></tr>
<tr><td><code>weight.i</code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the component weights of the mixture distribution; <code class="reqn">i=1, \ldots, k</code></p>
</td></tr>
<tr><td><code>mean.i</code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the component means of the mixture distribution; <code class="reqn">i=1, \ldots, k</code></p>
</td></tr>
<tr><td><code>sd.i</code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the component standard deviations of the mixture distribution; <code class="reqn">i=1, \ldots, k</code></p>
</td></tr>
<tr><td><code>Acc rat</code></td>
<td>
<p>vector of final acceptance rate of the proposal distributions of the algorithm with no calibration
stage for the proposal scales</p>
</td></tr>
<tr><td><code>Opt scale</code></td>
<td>
<p>vector of optimal proposal scales obtained the by calibration stage</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For multimodal outputs such as the mixture model weights, component means, and component variances, for each MCMC draw,
first the labels of the weights <code class="reqn">p_i, i=1, \ldots, k</code> and corresponding component means and standard deviations are permuted in
such a way that <code class="reqn">p_1\le \ldots \le p_k</code>. Then the component means and standard deviations are jointly partitioned into <code class="reqn">k</code>
clusters by applying a standard k-means algorithm with <code class="reqn">k</code> clusters, following Fruhwirth-Schnatter (2006) method. The obtained classification sequence was then used to reorder and identify the other component-specific parameters, namely component mean hyperparameters and weights. For each group,  cluster centers are considered as parameter
estimates. 
</p>


<h3>Author(s)</h3>

<p>Kaniav Kamary
</p>


<h3>References</h3>

<p>Jasra, A., Holmes, C. and Stephens, D. (2005). Markov Chain Monte Carlo methods and the label switching problem in
Bayesian mixture modeling. Statistical Science, 20, 50&ndash;67.
</p>
<p>Hartigan, J. A. and Wong, M. A. (1979). A K-means clustering algorithm. Applied Statistics 28, 100&ndash;108.
</p>
<p>Fruhwirth-Schnatter, S. (2006). Finite mixture and Markov switching models. Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+K.MixReparametrized">K.MixReparametrized</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#data(faithful)
#xobs=faithful[,1]
#estimate=K.MixReparametrized(xobs, k=2, alpha0=.5, alpha=.5, Nsim=20000)
#summari=SM.MixReparametrized(xobs,estimate)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
