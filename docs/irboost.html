<!DOCTYPE html><html><head><title>Help for package irboost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {irboost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dataLS'><p>generate random data for classification as in Long and Servedio (2010)</p></a></li>
<li><a href='#irb.train'><p>fit a robust predictive model with iteratively reweighted boosting algorithm</p></a></li>
<li><a href='#irb.train_aft'><p>fit a robust accelerated failure time model with iteratively reweighted boosting algorithm</p></a></li>
<li><a href='#irboost'><p>fit a robust predictive model with iteratively reweighted boosting algorithm</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Iteratively Reweighted Boosting for Robust Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1-1.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-04-18</td>
</tr>
<tr>
<td>Author:</td>
<td>Zhu Wang <a href="https://orcid.org/0000-0002-0773-0052"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Zhu Wang &lt;zhuwang@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Fit a predictive model using iteratively reweighted boosting (IRBoost) to minimize robust loss functions within the CC-family (concave-convex). This constitutes an application of iteratively reweighted convex optimization (IRCO), where convex optimization is performed using the functional descent boosting algorithm. IRBoost assigns weights to facilitate outlier identification. Applications include robust generalized linear models and robust accelerated failure time models. Wang (2021) &lt;<a href="https://doi.org/10.48550%2FarXiv.2101.07718">doi:10.48550/arXiv.2101.07718</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>mpath (&ge; 0.4-2.21), xgboost</td>
</tr>
<tr>
<td>Suggests:</td>
<td>R.rsp, DiagrammeR, survival, Hmisc</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-18 14:39:27 UTC; zhu</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-18 17:52:58 UTC</td>
</tr>
</table>
<hr>
<h2 id='dataLS'>generate random data for classification as in Long and Servedio (2010)</h2><span id='topic+dataLS'></span>

<h3>Description</h3>

<p>generate random data for classification as in Long and Servedio (2010)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataLS(ntr, ntu = ntr, nte, percon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataLS_+3A_ntr">ntr</code></td>
<td>
<p>number of training data</p>
</td></tr>
<tr><td><code id="dataLS_+3A_ntu">ntu</code></td>
<td>
<p>number of tuning data, default is the same as <code>ntr</code></p>
</td></tr>
<tr><td><code id="dataLS_+3A_nte">nte</code></td>
<td>
<p>number of test data</p>
</td></tr>
<tr><td><code id="dataLS_+3A_percon">percon</code></td>
<td>
<p>proportion of contamination, must between 0 and 1. If <code>percon &gt; 0</code>, the labels of the corresponding percenrage of response variable in the training and tuning data are flipped.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with elements xtr, xtu, xte, ytr, ytu, yte for predictors of disjoint training, tuning and test data, and response variable -1/1 of training, tuning and test data.
</p>


<h3>Author(s)</h3>

<p>Zhu Wang<br /> Maintainer: Zhu Wang <a href="mailto:zhuwang@gmail.com">zhuwang@gmail.com</a>
</p>


<h3>References</h3>

<p>P. Long and R. Servedio (2010), <em>Random classification noise defeats all convex potential boosters</em>, <em>Machine Learning Journal</em>, 78(3), 287&ndash;304.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- dataLS(ntr=100, nte=100, percon=0)
</code></pre>

<hr>
<h2 id='irb.train'>fit a robust predictive model with iteratively reweighted boosting algorithm</h2><span id='topic+irb.train'></span>

<h3>Description</h3>

<p>Fit a predictive model with the iteratively reweighted convex optimization (IRCO) that minimizes the robust loss functions in the CC-family (concave-convex). The convex optimization is conducted by functional descent boosting algorithm in the R package <span class="pkg">xgboost</span>. The iteratively reweighted boosting (IRBoost) algorithm reduces the weight of the observation that leads to a large loss; it also provides weights to help identify outliers. Applications include the robust generalized
linear models and extensions, where the mean is related to the predictors by boosting, and robust accelerated failure time models. <code>irb.train</code> is an advanced interface for training an irboost model. The <code>irboost</code> function is a simpler wrapper for <code>irb.train</code>. See <code>xgboost::xgb.train</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>irb.train(
  params = list(),
  data,
  z_init = NULL,
  cfun = "ccave",
  s = 1,
  delta = 0.1,
  iter = 10,
  nrounds = 100,
  del = 1e-10,
  trace = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="irb.train_+3A_params">params</code></td>
<td>
<p>the list of parameters, <code>params</code> is passed to function xgboost::xgb.train which requires the same argument. The list must include <code>objective</code>, a convex component in the CC-family, the second C, or convex down. It is the same as <code>objective</code> in the <code>xgboost::xgb.train</code>. The following objective functions are currently implemented: 
</p>

<ul>
<li> <p><code>reg:squarederror</code> Regression with squared loss.
</p>
</li>
<li> <p><code>binary:logitraw</code> logistic regression for binary classification, predict linear predictor, not probabilies.
</p>
</li>
<li> <p><code>binary:hinge</code> hinge loss for binary classification. This makes predictions of -1 or 1, rather than   producing probabilities.
</p>
</li>
<li> <p><code>multi:softprob</code> softmax loss function for multiclass problems. The result contains predicted probabilities of each data point in each class, say p_k, k=0, ..., nclass-1. Note, <code>label</code> is coded as in [0, ..., nclass-1]. The loss function cross-entropy for the i-th observation is computed as -log(p_k) with k=lable_i, i=1, ..., n.
</p>
</li>
<li> <p><code>count:poisson</code>: Poisson regression for count data, predict mean of poisson distribution.
</p>
</li>
<li> <p><code>reg:gamma</code>: gamma regression with log-link, predict mean of gamma distribution. The implementation in <code>xgboost::xgb.train</code> takes a parameterization in the exponential family:<br />
xgboost/src/src/metric/elementwise_metric.cu.<br />
In particularly, there is only one parameter psi and set to 1. The implementation of the IRCO algorithm follows this parameterization. See Table 2.1, McCullagh and Nelder, Generalized linear models, Chapman &amp; Hall, 1989, second edition.
</p>
</li>
<li> <p><code>reg:tweedie</code>: Tweedie regression with log-link. See also <br /> 
<code>tweedie_variance_power</code> in range: (1,2). A value close to 2 is like a gamma distribution. A value close to 1 is like a Poisson distribution.
</p>
</li>
<li> <p><code>survival:aft</code>: Accelerated failure time model for censored survival time data. <code>irb.train</code> invokes <code>irb.train_aft</code>. 
</p>
</li></ul>
</td></tr>
<tr><td><code id="irb.train_+3A_data">data</code></td>
<td>
<p>training dataset. <code>irb.train</code> accepts only an <code>xgboost::xgb.DMatrix</code> as the input. <code>irboost</code>, in addition, also accepts <code>matrix</code>, <code>dgCMatrix</code>, or name of a local data file. See <code>xgboost::xgb.train</code>.</p>
</td></tr>
<tr><td><code id="irb.train_+3A_z_init">z_init</code></td>
<td>
<p>vector of nobs with initial convex component values, must be non-negative with default values = weights if data has provided, otherwise z_init = vector of 1s</p>
</td></tr>
<tr><td><code id="irb.train_+3A_cfun">cfun</code></td>
<td>
<p>concave component of CC-family, can be <code>"hacve", "acave", "bcave", "ccave"</code>, 
<code>"dcave", "ecave", "gcave", "hcave"</code>.<br /> 
See Table 2 https://arxiv.org/pdf/2010.02848.pdf</p>
</td></tr>
<tr><td><code id="irb.train_+3A_s">s</code></td>
<td>
<p>tuning parameter of <code>cfun</code>. <code>s &gt; 0</code> and can be equal to 0 for <code>cfun="tcave"</code>. If <code>s</code> is too close to 0 for    <code>cfun="acave", "bcave", "ccave"</code>, the calculated weights can become 0 for all observations, thus crash the program</p>
</td></tr>
<tr><td><code id="irb.train_+3A_delta">delta</code></td>
<td>
<p>a small positive number provided by user only if <code>cfun="gcave"</code> and <code>0 &lt; s &lt;1</code></p>
</td></tr>
<tr><td><code id="irb.train_+3A_iter">iter</code></td>
<td>
<p>number of iteration in the IRCO algorithm</p>
</td></tr>
<tr><td><code id="irb.train_+3A_nrounds">nrounds</code></td>
<td>
<p>boosting iterations within each IRCO iteration</p>
</td></tr>
<tr><td><code id="irb.train_+3A_del">del</code></td>
<td>
<p>convergency criteria in the IRCO algorithm, no relation to <code>delta</code></p>
</td></tr>
<tr><td><code id="irb.train_+3A_trace">trace</code></td>
<td>
<p>if <code>TRUE</code>, fitting progress is reported</p>
</td></tr>
<tr><td><code id="irb.train_+3A_...">...</code></td>
<td>
<p>other arguments passing to <code>xgb.train</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with S3 class <code>xgb.train</code> with the additional elments:
</p>

<ul>
<li> <p><code>weight_update_log</code> a matrix of <code>nobs</code> row by <code>iter</code>      column of observation weights in each iteration of the IRCO algorithm
</p>
</li>
<li> <p><code>weight_update</code> a vector of observation weights in the last IRCO iteration that produces the final model fit
</p>
</li>
<li><p><code>loss_log</code> sum of loss value of the composite function in each IRCO iteration. Note, <code>cfun</code> requires <code>objective</code> non-negative in some cases. Thus care must be taken. For instance, with <code>objective="reg:gamma"</code>, the loss value is defined by gamma-nloglik - (1+log(min(y))), where y=label. The second term is introduced such that the loss value is non-negative. In fact, gamma-nloglik=y/ypre + log(ypre) in the <code>xgboost::xgb.train</code>, where ypre is the mean prediction value, can
be negative. It can be derived that for fixed <code>y</code>, the minimum value of gamma-nloglik is achived at ypre=y, or 1+log(y). Thus, among all <code>label</code> values, the minimum of gamma-nloglik is 1+log(min(y)).
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zhu Wang<br /> Maintainer: Zhu Wang <a href="mailto:zhuwang@gmail.com">zhuwang@gmail.com</a>
</p>


<h3>References</h3>

<p>Wang, Zhu (2021), <em>Unified Robust Boosting</em>, arXiv eprint, <a href="https://arxiv.org/abs/2101.07718">https://arxiv.org/abs/2101.07718</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# logistic boosting
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')

dtrain &lt;- with(agaricus.train, xgboost::xgb.DMatrix(data, label = label))
dtest &lt;- with(agaricus.test, xgboost::xgb.DMatrix(data, label = label))
watchlist &lt;- list(train = dtrain, eval = dtest)

# A simple irb.train example:
param &lt;- list(max_depth = 2, eta = 1, nthread = 2, 
objective = "binary:logitraw", eval_metric = "auc")
bst &lt;- xgboost::xgb.train(params=param, data=dtrain, nrounds = 2, 
                          watchlist=watchlist, verbose=2)
bst &lt;- irb.train(params=param, data=dtrain, nrounds = 2)
summary(bst$weight_update)
# a bug in xgboost::xgb.train
#bst &lt;- irb.train(params=param, data=dtrain, nrounds = 2, 
#                 watchlist=watchlist, trace=TRUE, verbose=2) 

# time-to-event analysis
X &lt;- matrix(1:5, ncol=1)
# Associate ranged labels with the data matrix.
# This example shows each kind of censored labels.
# uncensored  right  left  interval
y_lower = c(10,  15, -Inf, 30, 100)
y_upper = c(Inf, Inf,   20, 50, Inf)
dtrain &lt;- xgboost::xgb.DMatrix(data=X, label_lower_bound=y_lower, 
                               label_upper_bound=y_upper)
param &lt;- list(objective="survival:aft", aft_loss_distribution="normal", 
              aft_loss_distribution_scale=1, max_depth=3, min_child_weight=0)
watchlist &lt;- list(train = dtrain)
bst &lt;- xgboost::xgb.train(params=param, data=dtrain, nrounds=15, 
                          watchlist=watchlist)
predict(bst, dtrain)
bst_cc &lt;- irb.train(params=param, data=dtrain, nrounds=15, cfun="hcave",
                    s=1.5, trace=TRUE, verbose=0)
bst_cc$weight_update

</code></pre>

<hr>
<h2 id='irb.train_aft'>fit a robust accelerated failure time model with iteratively reweighted boosting algorithm</h2><span id='topic+irb.train_aft'></span>

<h3>Description</h3>

<p>Fit an accelerated failure time model with the iteratively reweighted convex optimization   (IRCO) that minimizes the robust loss functions in the CC-family (concave-convex).     The convex optimization is conducted by functional descent boosting algorithm   in the R package <span class="pkg">xgboost</span>. The iteratively reweighted boosting (IRBoost) algorithm reduces the weight of the        observation that leads to a large loss; it also provides weights to help        identify outliers. For time-to-event data, an accelerated failure time model (AFT
model) provides an alternative to the commonly used proportional hazards models. Note, function <code>irboost_aft</code> was developed to facilitate a data input format used with function <code>xgb.train</code> for <code>objective=survival:aft</code> in package <code>xgboost</code>. In other ojective functions, the input format is different with function <code>xgboost</code> at the time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>irb.train_aft(
  params = list(),
  data,
  z_init = NULL,
  cfun = "ccave",
  s = 1,
  delta = 0.1,
  iter = 10,
  nrounds = 100,
  del = 1e-10,
  trace = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="irb.train_aft_+3A_params">params</code></td>
<td>
<p>the list of parameters used in <code>xgb.train</code> of <span class="pkg">xgboost</span>.<br /> 
Must include <code>aft_loss_distribution</code>, <code>aft_loss_distribution_scale</code>, but there is no need to include <code>objective</code>. The complete list of parameters is
available in the <a href="http://xgboost.readthedocs.io/en/latest/parameter.html">online documentation</a>.</p>
</td></tr>
<tr><td><code id="irb.train_aft_+3A_data">data</code></td>
<td>
<p>training dataset. <code>irboost_aft</code> accepts only an <code>xgb.DMatrix</code> as the input.</p>
</td></tr>
<tr><td><code id="irb.train_aft_+3A_z_init">z_init</code></td>
<td>
<p>vector of nobs with initial convex component values, must be   non-negative with default values = weights if provided, otherwise z_init =      vector of 1s</p>
</td></tr>
<tr><td><code id="irb.train_aft_+3A_cfun">cfun</code></td>
<td>
<p>concave component of CC-family, can be <code>"hacve", "acave", "bcave", "ccave"</code>, 
<code>"dcave", "ecave", "gcave", "hcave"</code>.<br /> 
See Table 2 at https://arxiv.org/pdf/2010.02848.pdf</p>
</td></tr>
<tr><td><code id="irb.train_aft_+3A_s">s</code></td>
<td>
<p>tuning parameter of <code>cfun</code>. <code>s &gt; 0</code> and can be equal to 0 for <code>cfun="tcave"</code>. If <code>s</code> is too close to 0 for                     <code>cfun="acave", "bcave", "ccave"</code>, the calculated weights can become 0 for all observations, thus crash the program</p>
</td></tr>
<tr><td><code id="irb.train_aft_+3A_delta">delta</code></td>
<td>
<p>a small positive number provided by user only if <code>cfun="gcave"</code> and <code>0 &lt; s &lt;1</code></p>
</td></tr>
<tr><td><code id="irb.train_aft_+3A_iter">iter</code></td>
<td>
<p>number of iteration in the IRCO algorithm</p>
</td></tr>
<tr><td><code id="irb.train_aft_+3A_nrounds">nrounds</code></td>
<td>
<p>boosting iterations in <code>xgb.train</code> within each IRCO iteration</p>
</td></tr>
<tr><td><code id="irb.train_aft_+3A_del">del</code></td>
<td>
<p>convergency criteria in the IRCO algorithm, no relation to <code>delta</code></p>
</td></tr>
<tr><td><code id="irb.train_aft_+3A_trace">trace</code></td>
<td>
<p>if <code>TRUE</code>, fitting progress is reported</p>
</td></tr>
<tr><td><code id="irb.train_aft_+3A_...">...</code></td>
<td>
<p>other arguments passing to <code>xgb.train</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>xgb.Booster</code> with additional elements:
</p>

<ul>
<li> <p><code>weight_update_log</code> a matrix of <code>nobs</code> row by <code>iter</code> column of observation weights in each iteration of the IRCO algorithm
</p>
</li>
<li> <p><code>weight_update</code> a vector of observation weights in the last IRCO iteration that produces the final model fit
</p>
</li>
<li> <p><code>loss_log</code> sum of loss value of the composite function <code>cfun(survival_aft_distribution)</code> in each IRCO iteration
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zhu Wang<br /> Maintainer: Zhu Wang <a href="mailto:zhuwang@gmail.com">zhuwang@gmail.com</a>
</p>


<h3>References</h3>

<p>Wang, Zhu (2021), <em>Unified Robust Boosting</em>, arXiv eprint, <a href="https://arxiv.org/abs/2101.07718">https://arxiv.org/abs/2101.07718</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+irboost">irboost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("xgboost")
X &lt;- matrix(1:5, ncol=1)

# Associate ranged labels with the data matrix.
# This example shows each kind of censored labels.
#          uncensored  right  left  interval
y_lower = c(10,  15, -Inf, 30, 100)
y_upper = c(Inf, Inf,   20, 50, Inf)
dtrain &lt;- xgb.DMatrix(data=X, label_lower_bound=y_lower, label_upper_bound=y_upper)
                      params = list(objective="survival:aft", aft_loss_distribution="normal",
                      aft_loss_distribution_scale=1, max_depth=3, min_child_weight= 0)
watchlist &lt;- list(train = dtrain)
bst &lt;- xgb.train(params, data=dtrain, nrounds=15, watchlist=watchlist)
predict(bst, dtrain)
bst_cc &lt;- irb.train_aft(params, data=dtrain, nrounds=15, watchlist=watchlist, cfun="hcave", 
                       s=1.5, trace=TRUE, verbose=0)
bst_cc$weight_update
predict(bst_cc, dtrain)


</code></pre>

<hr>
<h2 id='irboost'>fit a robust predictive model with iteratively reweighted boosting algorithm</h2><span id='topic+irboost'></span>

<h3>Description</h3>

<p>Fit a predictive model with the iteratively reweighted convex optimization (IRCO) that minimizes the robust loss functions in the CC-family (concave-convex). The convex optimization is conducted by functional descent boosting algorithm in the R package <span class="pkg">xgboost</span>. The iteratively reweighted boosting (IRBoost) algorithm reduces the weight of the observation that leads to a large loss; it also provides weights to help identify outliers. Applications include the robust generalized
linear models and extensions, where the mean is related to the predictors by boosting, and robust accelerated failure time models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>irboost(
  data,
  label,
  weights,
  params = list(),
  z_init = NULL,
  cfun = "ccave",
  s = 1,
  delta = 0.1,
  iter = 10,
  nrounds = 100,
  del = 1e-10,
  trace = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="irboost_+3A_data">data</code></td>
<td>
<p>input data, if <code>objective="survival:aft"</code>, it must be an <code>xgb.DMatrix</code>; otherwise, it can be a matrix of dimension nobs x nvars; each row is an observation vector. Can accept <code>dgCMatrix</code></p>
</td></tr>
<tr><td><code id="irboost_+3A_label">label</code></td>
<td>
<p>response variable. Quantitative for <code>objective="reg:squarederror"</code>,<br /> 
<code>objective="count:poisson"</code> (non-negative counts) or <code>objective="reg:gamma"</code> (positive). 
For <code>objective="binary:logitraw" or "binary:hinge"</code>, <code>label</code> should be a factor with two levels</p>
</td></tr>
<tr><td><code id="irboost_+3A_weights">weights</code></td>
<td>
<p>vector of nobs with non-negative weights</p>
</td></tr>
<tr><td><code id="irboost_+3A_params">params</code></td>
<td>
<p>the list of parameters, <code>params</code> is passed to function xgboost::xgboost which requires the same argument. The list must include <code>objective</code>, a convex component in the CC-family, the second C, or convex down. It is the same as <code>objective</code> in the <code>xgboost::xgboost</code>. The following objective functions are currently implemented: 
</p>

<ul>
<li> <p><code>reg:squarederror</code> Regression with squared loss.
</p>
</li>
<li> <p><code>binary:logitraw</code> logistic regression for binary classification, predict linear predictor, not probabilies.
</p>
</li>
<li> <p><code>binary:hinge</code> hinge loss for binary classification. This makes predictions of -1 or 1, rather than   producing probabilities.
</p>
</li>
<li> <p><code>multi:softprob</code> softmax loss function for multiclass problems. The result contains predicted probabilities of each data point in each class, say p_k, k=0, ..., nclass-1. Note, <code>label</code> is coded as in [0, ..., nclass-1]. The loss function cross-entropy for the i-th observation is computed as -log(p_k) with k=lable_i, i=1, ..., n.
</p>
</li>
<li> <p><code>count:poisson</code>: Poisson regression for count data, predict mean of poisson distribution.
</p>
</li>
<li> <p><code>reg:gamma</code>: gamma regression with log-link, predict mean of gamma distribution. The implementation in <code>xgboost</code> takes a parameterization in the exponential family:<br />
xgboost/src/src/metric/elementwise_metric.cu.<br />
In particularly, there is only one parameter psi and set to 1. The implementation of the IRCO algorithm follows this parameterization. See Table 2.1, McCullagh and Nelder, Generalized linear models, Chapman &amp; Hall, 1989, second edition.
</p>
</li>
<li> <p><code>reg:tweedie</code>: Tweedie regression with log-link. See also<br /> <code>tweedie_variance_power</code> in range: (1,2). A value close to 2 is like a gamma distribution. A value close to 1 is like a Poisson distribution.
</p>
</li>
<li> <p><code>survival:aft</code>: Accelerated failure time model for censored survival time data. <code>irboost</code> invokes <code>irb.train_aft</code>. 
</p>
</li></ul>
</td></tr>
<tr><td><code id="irboost_+3A_z_init">z_init</code></td>
<td>
<p>vector of nobs with initial convex component values, must be non-negative with default values = weights if provided, otherwise z_init = vector of 1s</p>
</td></tr>
<tr><td><code id="irboost_+3A_cfun">cfun</code></td>
<td>
<p>concave component of CC-family, can be <code>"hacve", "acave", "bcave", "ccave"</code>, 
<code>"dcave", "ecave", "gcave", "hcave"</code>.<br />
See Table 2 at https://arxiv.org/pdf/2010.02848.pdf</p>
</td></tr>
<tr><td><code id="irboost_+3A_s">s</code></td>
<td>
<p>tuning parameter of <code>cfun</code>. <code>s &gt; 0</code> and can be equal to 0 for <code>cfun="tcave"</code>. If <code>s</code> is too close to 0 for    <code>cfun="acave", "bcave", "ccave"</code>, the calculated weights can become 0 for all observations, thus crash the program</p>
</td></tr>
<tr><td><code id="irboost_+3A_delta">delta</code></td>
<td>
<p>a small positive number provided by user only if <code>cfun="gcave"</code> and <code>0 &lt; s &lt;1</code></p>
</td></tr>
<tr><td><code id="irboost_+3A_iter">iter</code></td>
<td>
<p>number of iteration in the IRCO algorithm</p>
</td></tr>
<tr><td><code id="irboost_+3A_nrounds">nrounds</code></td>
<td>
<p>boosting iterations within each IRCO iteration</p>
</td></tr>
<tr><td><code id="irboost_+3A_del">del</code></td>
<td>
<p>convergency criteria in the IRCO algorithm, no relation to <code>delta</code></p>
</td></tr>
<tr><td><code id="irboost_+3A_trace">trace</code></td>
<td>
<p>if <code>TRUE</code>, fitting progress is reported</p>
</td></tr>
<tr><td><code id="irboost_+3A_...">...</code></td>
<td>
<p>other arguments passing to <code>xgboost</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with S3 class <code>xgboost</code> with the additional elments:
</p>

<ul>
<li> <p><code>weight_update_log</code> a matrix of <code>nobs</code> row by <code>iter</code>      column of observation weights in each iteration of the IRCO algorithm
</p>
</li>
<li> <p><code>weight_update</code> a vector of observation weights in the last IRCO iteration that produces the final model fit
</p>
</li>
<li><p><code>loss_log</code> sum of loss value of the composite function in each IRCO iteration. Note, <code>cfun</code> requires <code>objective</code> non-negative in some cases. Thus care must be taken. For instance, with <code>objective="reg:gamma"</code>, the loss value is defined by gamma-nloglik - (1+log(min(y))), where y=label. The second term is introduced such that the loss value is non-negative. In fact, gamma-nloglik=y/ypre + log(ypre) in the <code>xgboost</code>, where ypre is the mean prediction value, can
be negative. It can be derived that for fixed <code>y</code>, the minimum value of gamma-nloglik is achived at ypre=y, or 1+log(y). Thus, among all <code>label</code> values, the minimum of gamma-nloglik is 1+log(min(y)).
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zhu Wang<br /> Maintainer: Zhu Wang <a href="mailto:zhuwang@gmail.com">zhuwang@gmail.com</a>
</p>


<h3>References</h3>

<p>Wang, Zhu (2021), <em>Unified Robust Boosting</em>, arXiv eprint, <a href="https://arxiv.org/abs/2101.07718">https://arxiv.org/abs/2101.07718</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# regression, logistic regression, Poisson regression
x &lt;- matrix(rnorm(100*2),100,2)
g2 &lt;- sample(c(0,1),100,replace=TRUE)
fit1 &lt;- irboost(data=x, label=g2, cfun="acave",s=0.5, 
                params=list(objective="reg:squarederror", max_depth=1), trace=TRUE, 
                verbose=0, nrounds=50)
fit2 &lt;- irboost(data=x, label=g2, cfun="acave",s=0.5, 
                params=list(objective="binary:logitraw", max_depth=1), trace=TRUE,  
                verbose=0, nrounds=50)
fit3 &lt;- irboost(data=x, label=g2, cfun="acave",s=0.5, 
                params=list(objective="binary:hinge", max_depth=1), trace=TRUE,  
                verbose=0, nrounds=50)
fit4 &lt;- irboost(data=x, label=g2, cfun="acave",s=0.5, 
                params=list(objective="count:poisson", max_depth=1), trace=TRUE,      
                verbose=0, nrounds=50)

# Gamma regression
x &lt;- matrix(rnorm(100*2),100,2)
g2 &lt;- sample(rgamma(100, 1))
library("xgboost")
param &lt;- list(objective="reg:gamma", max_depth=1)
fit5 &lt;- xgboost(data=x, label=g2, params=param, nrounds=50)
fit6 &lt;- irboost(data=x, label=g2, cfun="acave",s=5, params=param, trace=TRUE, 
                verbose=0, nrounds=50)
plot(predict(fit5, newdata=x), predict(fit6, newdata=x))
hist(fit6$weight_update)
plot(fit6$loss_log)
summary(fit6$weight_update)

# Tweedie regression 
param &lt;- list(objective="reg:tweedie", max_depth=1)
fit6t &lt;- irboost(data=x, label=g2, cfun="acave",s=5, params=param, 
                 trace=TRUE, verbose=0, nrounds=50)
# Gamma vs Tweedie regression
hist(fit6$weight_update)
hist(fit6t$weight_update)
plot(predict(fit6, newdata=x), predict(fit6t, newdata=x))

# multiclass classification in iris dataset:
lb &lt;- as.numeric(iris$Species)-1
num_class &lt;- 3
set.seed(11)

param &lt;- list(objective="multi:softprob", max_depth=4, eta=0.5, nthread=2, 
subsample=0.5, num_class=num_class)
fit7 &lt;- irboost(data=as.matrix(iris[, -5]), label=lb, cfun="acave", s=50,
                params=param, trace=TRUE, verbose=0, nrounds=10)
# predict for softmax returns num_class probability numbers per case:
pred7 &lt;- predict(fit7, newdata=as.matrix(iris[, -5]))
# reshape it to a num_class-columns matrix
pred7 &lt;- matrix(pred7, ncol=num_class, byrow=TRUE)
# convert the probabilities to softmax labels
pred7_labels &lt;- max.col(pred7) - 1
# classification error: 0!
sum(pred7_labels != lb)/length(lb)
table(lb, pred7_labels)
hist(fit7$weight_update)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
