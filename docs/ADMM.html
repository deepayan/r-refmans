<!DOCTYPE html><html><head><title>Help for package ADMM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ADMM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ADMM'><p>ADMM : Algorithms using Alternating Direction Method of Multipliers</p></a></li>
<li><a href='#admm.bp'><p>Basis Pursuit</p></a></li>
<li><a href='#admm.enet'><p>Elastic Net Regularization</p></a></li>
<li><a href='#admm.genlasso'><p>Generalized LASSO</p></a></li>
<li><a href='#admm.lad'><p>Least Absolute Deviations</p></a></li>
<li><a href='#admm.lasso'><p>Least Absolute Shrinkage and Selection Operator</p></a></li>
<li><a href='#admm.rpca'><p>Robust Principal Component Analysis</p></a></li>
<li><a href='#admm.sdp'><p>Semidefinite Programming</p></a></li>
<li><a href='#admm.spca'><p>Sparse PCA</p></a></li>
<li><a href='#admm.tv'><p>Total Variation Minimization</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Algorithms using Alternating Direction Method of Multipliers</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides algorithms to solve popular optimization problems in statistics such as regression or denoising based on Alternating Direction Method of Multipliers (ADMM).
    See Boyd et al (2010) &lt;<a href="https://doi.org/10.1561%2F2200000016">doi:10.1561/2200000016</a>&gt; for complete introduction to the method.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, Matrix, Rdpack, stats, doParallel, foreach, parallel,
utils</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-08-07 04:02:53 UTC; kisung</td>
</tr>
<tr>
<td>Author:</td>
<td>Kisung You <a href="https://orcid.org/0000-0002-8584-459X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Xiaozhi Zhu [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kisung You &lt;kisungyou@outlook.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-08-08 04:20:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='ADMM'>ADMM : Algorithms using Alternating Direction Method of Multipliers</h2><span id='topic+ADMM'></span><span id='topic+ADMM-package'></span>

<h3>Description</h3>

<p>An introduction of Alternating Direction Method of Multipliers (ADMM) method has been a breakthrough in
solving complex and non-convex optimization problems in a reasonably stable as well as scalable fashion.
Our package aims at providing handy tools for fast computation on well-known problems using the method.
For interested users/readers, please visit Prof. Stephen Boyd's <a href="https://stanford.edu/~boyd/papers/admm_distr_stats.html">website</a>
entirely devoted to the topic.
</p>

<hr>
<h2 id='admm.bp'>Basis Pursuit</h2><span id='topic+admm.bp'></span>

<h3>Description</h3>

<p>For an underdetermined system, Basis Pursuit
aims to find a sparse solution that solves
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_x ~  \|x\|_1 \quad \textrm{s.t} \quad Ax=b</code>
</p>

<p>which is a relaxed version of strict non-zero support finding problem.
The implementation is borrowed from Stephen Boyd's
<a href="https://web.stanford.edu/~boyd/papers/admm/basis_pursuit/basis_pursuit.html">MATLAB code</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>admm.bp(
  A,
  b,
  xinit = NA,
  rho = 1,
  alpha = 1,
  abstol = 1e-04,
  reltol = 0.01,
  maxiter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="admm.bp_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(m \times n)</code> regressor matrix</p>
</td></tr>
<tr><td><code id="admm.bp_+3A_b">b</code></td>
<td>
<p>a length-<code class="reqn">m</code> response vector</p>
</td></tr>
<tr><td><code id="admm.bp_+3A_xinit">xinit</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector for initial value</p>
</td></tr>
<tr><td><code id="admm.bp_+3A_rho">rho</code></td>
<td>
<p>an augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="admm.bp_+3A_alpha">alpha</code></td>
<td>
<p>an overrelaxation parameter in [1,2]</p>
</td></tr>
<tr><td><code id="admm.bp_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.bp_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.bp_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>x</dt><dd><p>a length-<code class="reqn">n</code> solution vector</p>
</dd>
<dt>history</dt><dd><p>dataframe recording iteration numerics. See the section for more details.</p>
</dd>
</dl>



<h3>Iteration History</h3>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates,
</p>

<dl>
<dt>objval</dt><dd><p>object (cost) function value</p>
</dd>
<dt>r_norm</dt><dd><p>norm of primal residual</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition</p>
</dd>
</dl>

<p>In accordance with the paper, iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate sample data
n = 30
m = 10

A = matrix(rnorm(n*m), nrow=m)    # design matrix
x = c(stats::rnorm(3),rep(0,n-3)) # coefficient
x = base::sample(x)
b = as.vector(A%*%x)              # response

## run example
output  = admm.bp(A, b)
niter   = length(output$history$s_norm)
history = output$history

## report convergence plot
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(1:niter, history$objval, "b", main="cost function")
plot(1:niter, history$r_norm, "b", main="primal residual")
plot(1:niter, history$s_norm, "b", main="dual residual")
par(opar)

</code></pre>

<hr>
<h2 id='admm.enet'>Elastic Net Regularization</h2><span id='topic+admm.enet'></span>

<h3>Description</h3>

<p>Elastic Net regularization is a combination of <code class="reqn">\ell_2</code> stability and
<code class="reqn">\ell_1</code> sparsity constraint simulatenously solving the following,
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_x ~ \frac{1}{2}\|Ax-b\|_2^2 + \lambda_1 \|x\|_1 + \lambda_2 \|x\|_2^2</code>
</p>

<p>with nonnegative constraints <code class="reqn">\lambda_1</code> and <code class="reqn">\lambda_2</code>. Note that if both lambda values are 0,
it reduces to least-squares solution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>admm.enet(
  A,
  b,
  lambda1 = 1,
  lambda2 = 1,
  rho = 1,
  abstol = 1e-04,
  reltol = 0.01,
  maxiter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="admm.enet_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(m\times n)</code> regressor matrix</p>
</td></tr>
<tr><td><code id="admm.enet_+3A_b">b</code></td>
<td>
<p>a length-<code class="reqn">m</code> response vector</p>
</td></tr>
<tr><td><code id="admm.enet_+3A_lambda1">lambda1</code></td>
<td>
<p>a regularization parameter for <code class="reqn">\ell_1</code> term</p>
</td></tr>
<tr><td><code id="admm.enet_+3A_lambda2">lambda2</code></td>
<td>
<p>a regularization parameter for <code class="reqn">\ell_2</code> term</p>
</td></tr>
<tr><td><code id="admm.enet_+3A_rho">rho</code></td>
<td>
<p>an augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="admm.enet_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.enet_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.enet_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>x</dt><dd><p>a length-<code class="reqn">n</code> solution vector</p>
</dd>
<dt>history</dt><dd><p>dataframe recording iteration numerics. See the section for more details.</p>
</dd>
</dl>



<h3>Iteration History</h3>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates,
</p>

<dl>
<dt>objval</dt><dd><p>object (cost) function value</p>
</dd>
<dt>r_norm</dt><dd><p>norm of primal residual</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition</p>
</dd>
</dl>

<p>In accordance with the paper, iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Author(s)</h3>

<p>Xiaozhi Zhu
</p>


<h3>References</h3>

<p>Zou H, Hastie T (2005).
&ldquo;Regularization and variable selection via the elastic net.&rdquo;
<em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <b>67</b>(2), 301&ndash;320.
ISSN 1369-7412, 1467-9868, doi: <a href="https://doi.org/10.1111/j.1467-9868.2005.00503.x">10.1111/j.1467-9868.2005.00503.x</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+admm.lasso">admm.lasso</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate underdetermined design matrix
m = 50
n = 100
p = 0.1   # percentange of non-zero elements

x0 = matrix(Matrix::rsparsematrix(n,1,p))
A  = matrix(rnorm(m*n),nrow=m)
for (i in 1:ncol(A)){
  A[,i] = A[,i]/sqrt(sum(A[,i]*A[,i]))
}
b = A%*%x0 + sqrt(0.001)*matrix(rnorm(m))

## run example with both regularization values = 1
output = admm.enet(A, b, lambda1=1, lambda2=1)
niter  = length(output$history$s_norm)
history = output$history

## report convergence plot
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(1:niter, history$objval, "b", main="cost function")
plot(1:niter, history$r_norm, "b", main="primal residual")
plot(1:niter, history$s_norm, "b", main="dual residual")
par(opar)

</code></pre>

<hr>
<h2 id='admm.genlasso'>Generalized LASSO</h2><span id='topic+admm.genlasso'></span>

<h3>Description</h3>

<p>Generalized LASSO is solving the following equation,
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_x ~ \frac{1}{2}\|Ax-b\|_2^2 + \lambda \|Dx\|_1</code>
</p>

<p>where the choice of regularization matrix <code class="reqn">D</code> leads to different problem formulations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>admm.genlasso(
  A,
  b,
  D = diag(length(b)),
  lambda = 1,
  rho = 1,
  alpha = 1,
  abstol = 1e-04,
  reltol = 0.01,
  maxiter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="admm.genlasso_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(m\times n)</code> regressor matrix</p>
</td></tr>
<tr><td><code id="admm.genlasso_+3A_b">b</code></td>
<td>
<p>a length-<code class="reqn">m</code> response vector</p>
</td></tr>
<tr><td><code id="admm.genlasso_+3A_d">D</code></td>
<td>
<p>a regularization matrix of <code class="reqn">n</code> columns</p>
</td></tr>
<tr><td><code id="admm.genlasso_+3A_lambda">lambda</code></td>
<td>
<p>a regularization parameter</p>
</td></tr>
<tr><td><code id="admm.genlasso_+3A_rho">rho</code></td>
<td>
<p>an augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="admm.genlasso_+3A_alpha">alpha</code></td>
<td>
<p>an overrelaxation parameter in [1,2]</p>
</td></tr>
<tr><td><code id="admm.genlasso_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.genlasso_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.genlasso_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>x</dt><dd><p>a length-<code class="reqn">n</code> solution vector</p>
</dd>
<dt>history</dt><dd><p>dataframe recording iteration numerics. See the section for more details.</p>
</dd>
</dl>



<h3>Iteration History</h3>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates,
</p>

<dl>
<dt>objval</dt><dd><p>object (cost) function value</p>
</dd>
<dt>r_norm</dt><dd><p>norm of primal residual</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition</p>
</dd>
</dl>

<p>In accordance with the paper, iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Author(s)</h3>

<p>Xiaozhi Zhu
</p>


<h3>References</h3>

<p>Tibshirani RJ, Taylor J (2011).
&ldquo;The solution path of the generalized lasso.&rdquo;
<em>The Annals of Statistics</em>, <b>39</b>(3), 1335&ndash;1371.
ISSN 0090-5364, doi: <a href="https://doi.org/10.1214/11-AOS878">10.1214/11-AOS878</a>.
</p>
<p>Zhu Y (2017).
&ldquo;An Augmented ADMM Algorithm With Application to the Generalized Lasso Problem.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>26</b>(1), 195&ndash;204.
ISSN 1061-8600, 1537-2715, doi: <a href="https://doi.org/10.1080/10618600.2015.1114491">10.1080/10618600.2015.1114491</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate sample data
m = 100
n = 200
p = 0.1   # percentange of non-zero elements

x0 = matrix(Matrix::rsparsematrix(n,1,p))
A  = matrix(rnorm(m*n),nrow=m)
for (i in 1:ncol(A)){
  A[,i] = A[,i]/sqrt(sum(A[,i]*A[,i]))
}
b = A%*%x0 + sqrt(0.001)*matrix(rnorm(m))
D = diag(n);

## set regularization lambda value
regval = 0.1*Matrix::norm(t(A)%*%b, 'I')

## solve LASSO via reducing from Generalized LASSO
output  = admm.genlasso(A,b,D,lambda=regval) # set D as identity matrix
niter   = length(output$history$s_norm)
history = output$history

## report convergence plot
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(1:niter, history$objval, "b", main="cost function")
plot(1:niter, history$r_norm, "b", main="primal residual")
plot(1:niter, history$s_norm, "b", main="dual residual")
par(opar)

</code></pre>

<hr>
<h2 id='admm.lad'>Least Absolute Deviations</h2><span id='topic+admm.lad'></span>

<h3>Description</h3>

<p>Least Absolute Deviations (LAD) is an alternative to traditional Least Sqaures by using cost function
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_x ~ \|Ax-b\|_1</code>
</p>

<p>to use <code class="reqn">\ell_1</code> norm instead of square loss for robust estimation of coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>admm.lad(
  A,
  b,
  xinit = NA,
  rho = 1,
  alpha = 1,
  abstol = 1e-04,
  reltol = 0.01,
  maxiter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="admm.lad_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(m\times n)</code> regressor matrix</p>
</td></tr>
<tr><td><code id="admm.lad_+3A_b">b</code></td>
<td>
<p>a length-<code class="reqn">m</code> response vector</p>
</td></tr>
<tr><td><code id="admm.lad_+3A_xinit">xinit</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector for initial value</p>
</td></tr>
<tr><td><code id="admm.lad_+3A_rho">rho</code></td>
<td>
<p>an augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="admm.lad_+3A_alpha">alpha</code></td>
<td>
<p>an overrelaxation parameter in [1,2]</p>
</td></tr>
<tr><td><code id="admm.lad_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.lad_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.lad_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>x</dt><dd><p>a length-<code class="reqn">n</code> solution vector</p>
</dd>
<dt>history</dt><dd><p>dataframe recording iteration numerics. See the section for more details.</p>
</dd>
</dl>



<h3>Iteration History</h3>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates,
</p>

<dl>
<dt>objval</dt><dd><p>object (cost) function value</p>
</dd>
<dt>r_norm</dt><dd><p>norm of primal residual</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition</p>
</dd>
</dl>

<p>In accordance with the paper, iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate data
m = 1000
n = 100
A = matrix(rnorm(m*n),nrow=m)
x = 10*matrix(rnorm(n))
b = A%*%x

## add impulsive noise to 10% of positions
idx = sample(1:m, round(m/10))
b[idx] = b[idx] + 100*rnorm(length(idx))

## run the code
output  = admm.lad(A,b)
niter   = length(output$history$s_norm)
history = output$history

## report convergence plot
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(1:niter, history$objval, "b", main="cost function")
plot(1:niter, history$r_norm, "b", main="primal residual")
plot(1:niter, history$s_norm, "b", main="dual residual")
par(opar)


</code></pre>

<hr>
<h2 id='admm.lasso'>Least Absolute Shrinkage and Selection Operator</h2><span id='topic+admm.lasso'></span>

<h3>Description</h3>

<p>LASSO, or L1-regularized regression, is an optimization problem to solve
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_x ~ \frac{1}{2}\|Ax-b\|_2^2 + \lambda \|x\|_1</code>
</p>

<p>for sparsifying the coefficient vector <code class="reqn">x</code>.
The implementation is borrowed from Stephen Boyd's
<a href="https://stanford.edu/~boyd/papers/admm/lasso/lasso.html">MATLAB code</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>admm.lasso(
  A,
  b,
  lambda = 1,
  rho = 1,
  alpha = 1,
  abstol = 1e-04,
  reltol = 0.01,
  maxiter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="admm.lasso_+3A_a">A</code></td>
<td>
<p>an <code class="reqn">(m\times n)</code> regressor matrix</p>
</td></tr>
<tr><td><code id="admm.lasso_+3A_b">b</code></td>
<td>
<p>a length-<code class="reqn">m</code> response vector</p>
</td></tr>
<tr><td><code id="admm.lasso_+3A_lambda">lambda</code></td>
<td>
<p>a regularization parameter</p>
</td></tr>
<tr><td><code id="admm.lasso_+3A_rho">rho</code></td>
<td>
<p>an augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="admm.lasso_+3A_alpha">alpha</code></td>
<td>
<p>an overrelaxation parameter in [1,2]</p>
</td></tr>
<tr><td><code id="admm.lasso_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.lasso_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.lasso_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>x</dt><dd><p>a length-<code class="reqn">n</code> solution vector</p>
</dd>
<dt>history</dt><dd><p>dataframe recording iteration numerics. See the section for more details.</p>
</dd>
</dl>



<h3>Iteration History</h3>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates,
</p>

<dl>
<dt>objval</dt><dd><p>object (cost) function value</p>
</dd>
<dt>r_norm</dt><dd><p>norm of primal residual</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition</p>
</dd>
</dl>

<p>In accordance with the paper, iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>References</h3>

<p>Tibshirani R (1996).
&ldquo;Regression Shrinkage and Selection via the Lasso.&rdquo;
<em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <b>58</b>(1), 267&ndash;288.
ISSN 00359246.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate sample data
m = 50
n = 100
p = 0.1   # percentange of non-zero elements

x0 = matrix(Matrix::rsparsematrix(n,1,p))
A  = matrix(rnorm(m*n),nrow=m)
for (i in 1:ncol(A)){
  A[,i] = A[,i]/sqrt(sum(A[,i]*A[,i]))
}
b = A%*%x0 + sqrt(0.001)*matrix(rnorm(m))

## set regularization lambda value
lambda = 0.1*base::norm(t(A)%*%b, "F")

## run example
output  = admm.lasso(A, b, lambda)
niter   = length(output$history$s_norm)
history = output$history

## report convergence plot
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(1:niter, history$objval, "b", main="cost function")
plot(1:niter, history$r_norm, "b", main="primal residual")
plot(1:niter, history$s_norm, "b", main="dual residual")
par(opar)


</code></pre>

<hr>
<h2 id='admm.rpca'>Robust Principal Component Analysis</h2><span id='topic+admm.rpca'></span>

<h3>Description</h3>

<p>Given a data matrix <code class="reqn">M</code>, it finds a decomposition
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}~\|L\|_*+\lambda \|S\|_1\quad \textrm{s.t.}\quad L+S=M</code>
</p>

<p>where <code class="reqn">\|L\|_*</code> represents a nuclear norm for a matrix <code class="reqn">L</code> and
<code class="reqn">\|S\|_1 = \sum |S_{i,j}|</code>, and <code class="reqn">\lambda</code> a balancing/regularization
parameter. The choice of such norms leads to impose <em>low-rank</em> property for <code class="reqn">L</code> and
<em>sparsity</em> on <code class="reqn">S</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>admm.rpca(
  M,
  lambda = 1/sqrt(max(nrow(M), ncol(M))),
  mu = 1,
  tol = 1e-07,
  maxiter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="admm.rpca_+3A_m">M</code></td>
<td>
<p>an <code class="reqn">(m\times n)</code> data matrix</p>
</td></tr>
<tr><td><code id="admm.rpca_+3A_lambda">lambda</code></td>
<td>
<p>a regularization parameter</p>
</td></tr>
<tr><td><code id="admm.rpca_+3A_mu">mu</code></td>
<td>
<p>an augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="admm.rpca_+3A_tol">tol</code></td>
<td>
<p>relative tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.rpca_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>L</dt><dd><p>an <code class="reqn">(m\times n)</code> low-rank matrix</p>
</dd>
<dt>S</dt><dd><p>an <code class="reqn">(m\times n)</code> sparse matrix</p>
</dd>
<dt>history</dt><dd><p>dataframe recording iteration numerics. See the section for more details.</p>
</dd>
</dl>



<h3>Iteration History</h3>

<p>For RPCA implementation, we chose a very simple stopping criterion
</p>
<p style="text-align: center;"><code class="reqn">\|M-(L_k+S_k)\|_F \le tol*\|M\|_F</code>
</p>

<p>for each iteration step <code class="reqn">k</code>. So for this method, we provide a vector of only relative errors,
</p>

<dl>
<dt>error</dt><dd><p>relative error computed</p>
</dd>
</dl>



<h3>References</h3>

<p>Cand√®s EJ, Li X, Ma Y, Wright J (2011).
&ldquo;Robust principal component analysis?&rdquo;
<em>Journal of the ACM</em>, <b>58</b>(3), 1&ndash;37.
ISSN 00045411, doi: <a href="https://doi.org/10.1145/1970392.1970395">10.1145/1970392.1970395</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data matrix from standard normal
X = matrix(rnorm(20*5),nrow=5)

## try different regularization values
out1 = admm.rpca(X, lambda=0.01)
out2 = admm.rpca(X, lambda=0.1)
out3 = admm.rpca(X, lambda=1)

## visualize sparsity
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
image(out1$S, main="lambda=0.01")
image(out2$S, main="lambda=0.1")
image(out3$S, main="lambda=1")
par(opar)

</code></pre>

<hr>
<h2 id='admm.sdp'>Semidefinite Programming</h2><span id='topic+admm.sdp'></span>

<h3>Description</h3>

<p>We solve the following standard semidefinite programming (SDP) problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_X ~ \textrm{tr}(CX)</code>
</p>

<p style="text-align: center;"><code class="reqn">\textrm{s.t.} A(X)=b, ~ X \geq 0 </code>
</p>

<p>with <code class="reqn">A(X)_i = \textrm{tr}(A_i^\top X) = b_i</code> for <code class="reqn">i=1,\ldots,m</code> and <code class="reqn">X \geq 0</code> stands for positive-definiteness of the matrix <code class="reqn">X</code>. In the standard form,
matrices <code class="reqn">C, A_1,A_2,\ldots,A_m</code> are symmetric and solution <code class="reqn">X</code> would be symmetric and positive semidefinite. This function implements alternating direction augmented Lagrangian methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>admm.sdp(
  C,
  A,
  b,
  mu = 1,
  rho = 1,
  abstol = 1e-10,
  maxiter = 496,
  print.progress = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="admm.sdp_+3A_c">C</code></td>
<td>
<p>an <code class="reqn">(n\times n)</code> symmetric matrix for cost.</p>
</td></tr>
<tr><td><code id="admm.sdp_+3A_a">A</code></td>
<td>
<p>a length-<code class="reqn">m</code> list of <code class="reqn">(n\times n)</code> symmetric matrices for constraint.</p>
</td></tr>
<tr><td><code id="admm.sdp_+3A_b">b</code></td>
<td>
<p>a length-<code class="reqn">m</code> vector for equality condition.</p>
</td></tr>
<tr><td><code id="admm.sdp_+3A_mu">mu</code></td>
<td>
<p>penalty parameter; positive real number.</p>
</td></tr>
<tr><td><code id="admm.sdp_+3A_rho">rho</code></td>
<td>
<p>step size for updating in <code class="reqn">(0, \frac{1+\sqrt{5}}{2})</code>.</p>
</td></tr>
<tr><td><code id="admm.sdp_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance stopping criterion.</p>
</td></tr>
<tr><td><code id="admm.sdp_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="admm.sdp_+3A_print.progress">print.progress</code></td>
<td>
<p>a logical; <code>TRUE</code> to show the progress, <code>FALSE</code> to go silent.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>x</dt><dd><p>a length-<code class="reqn">n</code> solution vector</p>
</dd>
<dt>history</dt><dd><p>dataframe recording iteration numerics. See the section for more details.</p>
</dd>
</dl>



<h3>Iteration History</h3>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates,
</p>

<dl>
<dt>objval</dt><dd><p>object (cost) function value</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition</p>
</dd>
<dt>gap</dt><dd><p>gap between primal and dual cost function.</p>
</dd>
</dl>

<p>We use the stopping criterion which breaks the iteration when all <code>eps_pri</code>,<code>eps_dual</code>, and <code>gap</code>
become smaller than <code>abstol</code>.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Wen Z, Goldfarb D, Yin W (2010).
&ldquo;Alternating direction augmented Lagrangian methods for semidefinite programming.&rdquo;
<em>Mathematical Programming Computation</em>, <b>2</b>(3-4), 203&ndash;230.
ISSN 1867-2949, 1867-2957, doi: <a href="https://doi.org/10.1007/s12532-010-0017-1">10.1007/s12532-010-0017-1</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## a toy example
#  generate parameters
C  = matrix(c(1,2,3,2,9,0,3,0,7),nrow=3,byrow=TRUE)
A1 = matrix(c(1,0,1,0,3,7,1,7,5),nrow=3,byrow=TRUE)
A2 = matrix(c(0,2,8,2,6,0,8,0,4),nrow=3,byrow=TRUE)

A  = list(A1, A2)
b  = c(11, 19)

# run the algorithm
run = admm.sdp(C,A,b)
hst = run$history

# visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2))
plot(hst$objval,   type="b", cex=0.25, main="objective value")
plot(hst$eps_pri,  type="b", cex=0.25, main="primal feasibility")
plot(hst$eps_dual, type="b", cex=0.25, main="dual feasibility")
plot(hst$gap,      type="b", cex=0.25, main="primal-dual gap")
par(opar)

## Not run: 
## comparison with CVXR's result
require(CVXR)

#  problems definition
X = Variable(3,3,PSD=TRUE)
myobj = Minimize(sum_entries(C*X)) # objective
mycon = list(                      # constraint
  sum_entries(A[[1]]*X) == b[1],
  sum_entries(A[[2]]*X) == b[2]
)
myp = Problem(myobj, mycon)        # problem

# run and visualize
res  = solve(myp)
Xsol = res$getValue(X)

opar = par(no.readonly=TRUE)
par(mfrow=c(1,2), pty="s")
image(run$X, axes=FALSE, main="ADMM result")
image(Xsol,  axes=FALSE, main="CVXR result")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='admm.spca'>Sparse PCA</h2><span id='topic+admm.spca'></span>

<h3>Description</h3>

<p>Sparse Principal Component Analysis aims at finding a sparse vector by solving
</p>
<p style="text-align: center;"><code class="reqn">\textrm{max}_x~x^T\Sigma x \quad \textrm{s.t.} \quad \|x\|_2\le 1,~\|x\|_0\le K</code>
</p>

<p>where <code class="reqn">\|x\|_0</code> is the number of non-zero elements in a vector <code class="reqn">x</code>. A convex relaxation
of this problem was proposed to solve the following problem,
</p>
<p style="text-align: center;"><code class="reqn">\textrm{max}_X~&lt;\Sigma,X&gt; ~\textrm{s.t.} \quad Tr(X)=1,~\|X\|_0 \le K^2, ~X\ge 0,~\textrm{rank}(X)=1</code>
</p>

<p>where <code class="reqn">X=xx^T</code> is a <code class="reqn">(p\times p)</code> matrix that is outer product of a vector <code class="reqn">x</code> by itself,
and <code class="reqn">X\ge 0</code> means the matrix <code class="reqn">X</code> is positive semidefinite.
With the rank condition dropped, it can be restated as
</p>
<p style="text-align: center;"><code class="reqn">\textrm{max}_X~ &lt;\Sigma,X&gt;-\rho\|X\|_1 \quad \textrm{s.t.}\quad Tr(X)=1,X\ge 0.</code>
</p>

<p>After acquiring each principal component vector, an iterative step based on Schur complement deflation method
is applied to regress out the impact of previously-computed projection vectors. It should be noted that
those sparse basis may <em>not be orthonormal</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>admm.spca(
  Sigma,
  numpc,
  mu = 1,
  rho = 1,
  abstol = 1e-04,
  reltol = 0.01,
  maxiter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="admm.spca_+3A_sigma">Sigma</code></td>
<td>
<p>a <code class="reqn">(p\times p)</code> (sample) covariance matrix.</p>
</td></tr>
<tr><td><code id="admm.spca_+3A_numpc">numpc</code></td>
<td>
<p>number of principal components to be extracted.</p>
</td></tr>
<tr><td><code id="admm.spca_+3A_mu">mu</code></td>
<td>
<p>an augmented Lagrangian parameter.</p>
</td></tr>
<tr><td><code id="admm.spca_+3A_rho">rho</code></td>
<td>
<p>a regularization parameter for sparsity.</p>
</td></tr>
<tr><td><code id="admm.spca_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance stopping criterion.</p>
</td></tr>
<tr><td><code id="admm.spca_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance stopping criterion.</p>
</td></tr>
<tr><td><code id="admm.spca_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>basis</dt><dd><p>a <code class="reqn">(p\times numpc)</code> matrix whose columns are sparse principal components.</p>
</dd>
<dt>history</dt><dd><p>a length-<code>numpc</code> list of dataframes recording iteration numerics. See the section for more details.</p>
</dd>
</dl>



<h3>Iteration History</h3>

<p>For SPCA implementation, main computation is sequentially performed for each projection vector. The <code>history</code>
field is a list of length <code>numpc</code>, where each element is a data frame containing iteration history recording
following fields over iterates,
</p>

<dl>
<dt>r_norm</dt><dd><p>norm of primal residual</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition</p>
</dd>
</dl>

<p>In accordance with the paper, iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>References</h3>

<p>Ma S (2013).
&ldquo;Alternating Direction Method of Multipliers for Sparse Principal Component Analysis.&rdquo;
<em>Journal of the Operations Research Society of China</em>, <b>1</b>(2), 253&ndash;274.
ISSN 2194-668X, 2194-6698, doi: <a href="https://doi.org/10.1007/s40305-013-0016-9">10.1007/s40305-013-0016-9</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate a random matrix and compute its sample covariance
X    = matrix(rnorm(1000*5),nrow=1000)
covX = stats::cov(X)

## compute 3 sparse basis
output = admm.spca(covX, 3)

</code></pre>

<hr>
<h2 id='admm.tv'>Total Variation Minimization</h2><span id='topic+admm.tv'></span>

<h3>Description</h3>

<p>1-dimensional total variation minimization - also known as
signal denoising - is to solve the following
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_x ~ \frac{1}{2}\|x-b\|_2^2 + \lambda \sum_i |x_{i+1}-x_i|</code>
</p>

<p>for a given signal <code class="reqn">b</code>.
The implementation is borrowed from Stephen Boyd's
<a href="https://stanford.edu/~boyd/papers/admm/total_variation/total_variation.html">MATLAB code</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>admm.tv(
  b,
  lambda = 1,
  xinit = NA,
  rho = 1,
  alpha = 1,
  abstol = 1e-04,
  reltol = 0.01,
  maxiter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="admm.tv_+3A_b">b</code></td>
<td>
<p>a length-<code class="reqn">m</code> response vector</p>
</td></tr>
<tr><td><code id="admm.tv_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter</p>
</td></tr>
<tr><td><code id="admm.tv_+3A_xinit">xinit</code></td>
<td>
<p>a length-<code class="reqn">m</code> vector for initial value</p>
</td></tr>
<tr><td><code id="admm.tv_+3A_rho">rho</code></td>
<td>
<p>an augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="admm.tv_+3A_alpha">alpha</code></td>
<td>
<p>an overrelaxation parameter in <code class="reqn">[1,2]</code></p>
</td></tr>
<tr><td><code id="admm.tv_+3A_abstol">abstol</code></td>
<td>
<p>absolute tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.tv_+3A_reltol">reltol</code></td>
<td>
<p>relative tolerance stopping criterion</p>
</td></tr>
<tr><td><code id="admm.tv_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>x</dt><dd><p>a length-<code class="reqn">m</code> solution vector</p>
</dd>
<dt>history</dt><dd><p>dataframe recording iteration numerics. See the section for more details.</p>
</dd>
</dl>



<h3>Iteration History</h3>

<p>When you run the algorithm, output returns not only the solution, but also the iteration history recording
following fields over iterates,
</p>

<dl>
<dt>objval</dt><dd><p>object (cost) function value</p>
</dd>
<dt>r_norm</dt><dd><p>norm of primal residual</p>
</dd>
<dt>s_norm</dt><dd><p>norm of dual residual</p>
</dd>
<dt>eps_pri</dt><dd><p>feasibility tolerance for primal feasibility condition</p>
</dd>
<dt>eps_dual</dt><dd><p>feasibility tolerance for dual feasibility condition</p>
</dd>
</dl>

<p>In accordance with the paper, iteration stops when both <code>r_norm</code> and <code>s_norm</code> values
become smaller than <code>eps_pri</code> and <code>eps_dual</code>, respectively.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate sample data
x1 = as.vector(sin(1:100)+0.1*rnorm(100))
x2 = as.vector(cos(1:100)+0.1*rnorm(100)+5)
x3 = as.vector(sin(1:100)+0.1*rnorm(100)+2.5)
xsignal = c(x1,x2,x3)

## run example
output  = admm.tv(xsignal)

## visualize
opar &lt;- par(no.readonly=TRUE)
plot(1:300, xsignal, type="l", main="TV Regularization")
lines(1:300, output$x, col="red", lwd=2)
par(opar)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
