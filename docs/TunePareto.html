<!DOCTYPE html><html><head><title>Help for package TunePareto</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {TunePareto}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#TunePareto-package'>
<p>Multi-objective parameter tuning for classifiers</p></a></li>
<li><a href='#allCombinations'>
<p>Build a list of all possible combinations of parameter values</p></a></li>
<li><a href='#as.interval'>
<p>Specify a continous interval</p></a></li>
<li><a href='#createObjective'>
<p>Create a new objective function</p></a></li>
<li><a href='#generateCVRuns'>
<p>Generate cross-validation partitions</p></a></li>
<li><a href='#mergeTuneParetoResults'>
<p>Calculate optimal solutions from several calls of tunePareto</p></a></li>
<li><a href='#plotDominationGraph'>
<p>Visualize the Pareto fronts of parameter configuration scores</p></a></li>
<li><a href='#plotObjectivePairs'>
<p>Plot a matrix of Pareto front panels</p></a></li>
<li><a href='#plotParetoFronts2D'>
<p>A classical 2-dimensional plot of Pareto fronts</p></a></li>
<li><a href='#precalculation'>
<p>Predefined precalculation functions for objectives</p></a></li>
<li><a href='#predefinedClassifiers'>
<p>TunePareto wrappers for certain classifiers</p></a></li>
<li><a href='#predefinedObjectiveFunctions'>
<p>Predefined objective functions for parameter tuning</p></a></li>
<li><a href='#predict.TuneParetoModel'>
<p>Prediction method for TuneParetoClassifier objects</p></a></li>
<li><a href='#print.TuneParetoResult'>
<p>Print method for objects used in TunePareto</p></a></li>
<li><a href='#rankByDesirability'>
<p>Rank results according to their desirabilities</p></a></li>
<li><a href='#recalculateParetoSet'>
<p>Recalculate Pareto-optimal solutions</p></a></li>
<li><a href='#trainTuneParetoClassifier'>
<p>Train a TunePareto classifier</p></a></li>
<li><a href='#tunePareto'>
<p>Generic function for multi-objective parameter tuning of classifiers</p></a></li>
<li><a href='#tuneParetoClassifier'>
<p>Create a classifier object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Multi-Objective Parameter Tuning for Classifiers</td>
</tr>
<tr>
<td>Version:</td>
<td>2.5.3</td>
</tr>
<tr>
<td>Author:</td>
<td>Christoph Müssel, Ludwig Lausser, Hans Kestler</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hans Kestler &lt;hans.kestler@uni-ulm.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Generic methods for parameter tuning of classification algorithms using multiple scoring functions (Muessel et al. (2012), &lt;<a href="https://doi.org/10.18637%2Fjss.v046.i05">doi:10.18637/jss.v046.i05</a>&gt;).</td>
</tr>
<tr>
<td>Suggests:</td>
<td>snowfall, igraph, gsl, class, tree, e1071, randomForest, klaR</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-02 12:47:07 UTC; julian_schwab</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-02 14:40:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='TunePareto-package'>
Multi-objective parameter tuning for classifiers
</h2><span id='topic+TunePareto-package'></span><span id='topic+TunePareto'></span>

<h3>Description</h3>

<p>Generic methods for parameter tuning of classification algorithms using multiple scoring functions
</p>


<h3>Details</h3>

<p>The methods of this package allow to assess the performance of classifiers with respect to certain parameter values and multiple scoring functions, such as the cross-validation error or the sensitivity. It provides the <code><a href="#topic+tunePareto">tunePareto</a></code> function which can be configured to run most common classification methods implemented in R. Several sampling strategies for parameters are supplied, including Latin Hypercube sampling, quasi-random sequences, and evolutionary algorithms.
</p>
<p>Classifiers are wrapped in generic <code>TuneParetoClassifier</code> objects which can be created using <code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>. For state-of-the-art classifiers, the package includes the corresponding wrapper objects (see <code><a href="#topic+tunePareto.knn">tunePareto.knn</a></code>, <code><a href="#topic+tunePareto.tree">tunePareto.tree</a></code>, <code><a href="#topic+tunePareto.randomForest">tunePareto.randomForest</a></code>, <code><a href="#topic+tunePareto.svm">tunePareto.svm</a></code>, <code><a href="#topic+tunePareto.NaiveBayes">tunePareto.NaiveBayes</a></code>). 
</p>
<p>The method tests combinations of the supplied classifier parameters according to the supplied scoring functions and calculates the Pareto front of optimal parameter configurations. The Pareto fronts can be visualized using <code><a href="#topic+plotDominationGraph">plotDominationGraph</a></code>, <code><a href="#topic+plotParetoFronts2D">plotParetoFronts2D</a></code> and <code><a href="#topic+plotObjectivePairs">plotObjectivePairs</a></code>.
</p>
<p>A number of predefined scoring functions are provided (see <code><a href="#topic+predefinedObjectiveFunctions">predefinedObjectiveFunctions</a></code>), but the user is free to implement own scores (see <code><a href="#topic+createObjective">createObjective</a></code>).
</p>


<h3>Author(s)</h3>

<p>Christoph Müssel, Ludwig Lausser, Hans Kestler
</p>
<p>Maintainer: Hans Kestler &lt;hans.kestler@uni-ulm.de&gt;
</p>


<h3>References</h3>

<p>Christoph Müssel, Ludwig Lausser, Markus Maucher, Hans A. Kestler (2012). Multi-Objective Parameter Selection for Classifiers. Journal of Statistical Software, 46(5), 1-27. DOI https://doi.org/10.18637/jss.v046.i05.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# optimize the 'cost' and 'kernel' parameters of an SVM according
# to CV error and CV Specificity on the 'iris' data set
# using several predefined values for the cost
r &lt;- tunePareto(data = iris[, -ncol(iris)], 
                labels = iris[, ncol(iris)],
                classifier=tunePareto.svm(),
                cost=c(0.001,0.01,0.1,1,10), 
                kernel=c("linear", "polynomial", 
                         "radial", "sigmoid"),
                objectiveFunctions=list(cvError(10, 10), 
                                        cvSpecificity(10, 10, caseClass="setosa")))

# print Pareto-optimal solutions
print(r)

# use a continuous interval for the 'cost' parameter 
# and optimize it using evolutionary algorithms and
# parallel execution with snowfall
library(snowfall)
sfInit(parallel=TRUE, cpus=2, type="SOCK")
r &lt;- tunePareto(data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 classifier = tunePareto.svm(), 
                 cost = as.interval(0.001,10), 
                 kernel = c("linear", "polynomial",
                            "radial", "sigmoid"),
                 sampleType="evolution",
                 numCombinations=20,
                 numIterations=20,                      
                 objectiveFunctions = list(cvError(10, 10),
                                           cvSensitivity(10, 10, caseClass="setosa"),
                                           cvSpecificity(10, 10, caseClass="setosa")),
                useSnowfall=TRUE)
sfStop()

# print Pareto-optimal solutions
print(r)

# plot the Pareto fronts
plotDominationGraph(r, legend.x="topright")
</code></pre>

<hr>
<h2 id='allCombinations'>
Build a list of all possible combinations of parameter values
</h2><span id='topic+allCombinations'></span>

<h3>Description</h3>

<p>Builds a list of all possible combinations of parameter values from supplied ranges of parameter values. That is, each of the specified values is combined with all specified values for other parameters. The resulting lists can be used in the <code>classifierParameterCombinations</code> and <code>predictorParameterCombinations</code> parameters of <code><a href="#topic+tunePareto">tunePareto</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>allCombinations(parameterRanges)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="allCombinations_+3A_parameterranges">parameterRanges</code></td>
<td>

<p>A list of lists of parameter ranges. That is, each element of the list specifies the values of a single parameter to be tested and is named according to this parameter. It is also possible to set parameters to fixed values by specifying only one value. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list of lists, where each of the inner lists represents one parameter combination and consists of named elements for the parameters.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tunePareto">tunePareto</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(class)
# Combine only valid combinations of 'k' and 'l'
# for the k-NN classifier:
comb &lt;- c(allCombinations(list(k=1,l=0)),
          allCombinations(list(k=3,l=0:2)),
          allCombinations(list(k=5,l=0:4)),
          allCombinations(list(k=7,l=0:6)))
print(comb)

print(tunePareto(data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 classifier = tunePareto.knn(),
                 parameterCombinations = comb,
                 objectiveFunctions = list(cvError(10, 10),
                                           reclassError())))
</code></pre>

<hr>
<h2 id='as.interval'>
Specify a continous interval
</h2><span id='topic+as.interval'></span>

<h3>Description</h3>

<p>Specifies a continous interval by supplying a lower and upper bound. Such intervals can be supplied as parameter value ranges in <code><a href="#topic+tunePareto">tunePareto</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.interval(lower, upper)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.interval_+3A_lower">lower</code></td>
<td>

<p>The lower bound of the interval
</p>
</td></tr>
<tr><td><code id="as.interval_+3A_upper">upper</code></td>
<td>

<p>The upper bound of the interval
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class <code>Interval</code> specifying the lower and upper bound.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tunePareto">tunePareto</a></code>
</p>

<hr>
<h2 id='createObjective'>
Create a new objective function
</h2><span id='topic+createObjective'></span>

<h3>Description</h3>

<p>Creates a new <code>TuneParetoObjective</code> object. An objective consists of two parts: The precalculation function, which applies the classifier to the data, and the objective itself, which is calculated from the predicted class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createObjective(precalculationFunction, 
                precalculationParams = NULL, 
                objectiveFunction, 
                objectiveFunctionParams = NULL,
                direction = c("minimize", "maximize"), 
                name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createObjective_+3A_precalculationfunction">precalculationFunction</code></td>
<td>

<p>The name of the precalculation function that applies the classifiers to the data. Two predefined precalculation functions are <code>reclassification</code> and <code>crossValidation</code>.
</p>
</td></tr>
<tr><td><code id="createObjective_+3A_precalculationparams">precalculationParams</code></td>
<td>

<p>A named list of parameters for the precalculation function. 
</p>
</td></tr>
<tr><td><code id="createObjective_+3A_objectivefunction">objectiveFunction</code></td>
<td>

<p>The name of the objective function that calculates the objective from the precalculated class labels.
</p>
</td></tr>
<tr><td><code id="createObjective_+3A_objectivefunctionparams">objectiveFunctionParams</code></td>
<td>

<p>A named list of further parameters for the objective function.
</p>
</td></tr>
<tr><td><code id="createObjective_+3A_direction">direction</code></td>
<td>

<p>Specifies whether the objective is minimized or maximized.
</p>
</td></tr>
<tr><td><code id="createObjective_+3A_name">name</code></td>
<td>

<p>A readable name of the objective.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The objective calculation is divided into a precalculation step and the objective calculation itself. The main reason for this is the possibility to aggregate precalculation across objectives. For example, if both the specificity and the sensitivity of a cross-validation (with the same parameters) are required, the cross-validation is run only once to save computational time. Afterwards, the results are passed to both objective functions.
</p>
<p>A precalculation function has the following parameters:
</p>

<dl>
<dt>data</dt><dd>
<p>The data set to be used for the precalculation. This is usually a matrix or data frame with the samples in the rows and the features in the columns.
</p>
</dd>
<dt>labels</dt><dd>
<p>A vector of class labels for the samples in <code>data</code>.
</p>
</dd>
<dt>classifier</dt><dd>
<p>A <code>TuneParetoClassifier</code> wrapper object containing the classifier to tune. A number of state-of-the-art classifiers are included in <span class="pkg">TunePareto</span> (see <code><a href="#topic+predefinedClassifiers">predefinedClassifiers</a></code>). Custom classifiers can be employed using <code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>.
</p>
</dd>
<dt>classifierParams</dt><dd>
<p>A named list of parameter assignments for the classifier.
</p>
</dd>
<dt>predictorParams</dt><dd>
<p>If the classifier has separate training and prediction functions, a named list of parameter assignments for the predictor.
</p>
</dd>
</dl>

<p>Additionally, the function can have further parameters which are supplied in <code>precalculationParams</code>. To train a classifier and obtain predictions, the precalculation function can call the generic <code><a href="#topic+trainTuneParetoClassifier">trainTuneParetoClassifier</a></code> and <code><a href="#topic+predict.TuneParetoModel">predict.TuneParetoModel</a></code> functions. 
</p>
<p>The precalculation function usually returns the predicted labels, the true labels and the model, but the only requirement of the return value is that it can be processed by the corresponding objective function. Predefined precalculation functions are <code><a href="#topic+reclassification">reclassification</a></code> and <code><a href="#topic+crossValidation">crossValidation</a></code>.
</p>
<p>The objective function has a single obligatory parameter named <code>result</code> which supplies the result of the precalculation. Furthermore, optional parameters can be specified. Their values are taken from <code>objectiveFunctionParams</code>. The function either returns a single number specifying the objective value, or a list with a <code>score</code> component containing the objective value and a <code>additionalData</code> component that contains additional information to be stored in the <code>additionalData</code> component of the <code>TuneParetoResult</code> object (see <code><a href="#topic+tunePareto">tunePareto</a></code>).
</p>


<h3>Value</h3>

<p>Retuns an object of class <code>TuneParetoObjective</code> with the following components:
</p>
<table>
<tr><td><code>precalculationFunction</code></td>
<td>
<p>The supplied precalculation function</p>
</td></tr>
<tr><td><code>precalculationParams</code></td>
<td>
<p>The additional parameters to be passed to the precalculation function</p>
</td></tr>
<tr><td><code>objectiveFunction</code></td>
<td>
<p>The objective function</p>
</td></tr>
<tr><td><code>minimize</code></td>
<td>
<p><code>TRUE</code> if the objective is minimized, <code>FALSE</code> if it is maximized.</p>
</td></tr>
<tr><td><code>name</code></td>
<td>
<p>The readable name of the objective.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+predefinedObjectiveFunctions">predefinedObjectiveFunctions</a></code>, <code><a href="#topic+trainTuneParetoClassifier">trainTuneParetoClassifier</a></code>, <code><a href="#topic+predict.TuneParetoModel">predict.TuneParetoModel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create new objective minimizing the number of support vectors
# for a support vector machine

reclassSupportVectors &lt;- function (saveModel = FALSE) 
{
    createObjective(precalculationFunction = reclassification, 
        precalculationParams = NULL, objectiveFunction = 
        function(result, saveModel) 
        {
	        if(result$model$classifier$name != "svm")
		        stop("This objective function can only be applied 
		              to classifiers of type tunePareto.svm()")

      		res &lt;- result$model$model$tot.nSV

		      if (saveModel) 
		      # return a list containing the objective value as well as the model
		      {
		         return(list(additionalData = result$model, fitness = res))
		      }
		      else 
		      # only return the objective value
		        return(res)
        }, 
        objectiveFunctionParams = list(saveModel = saveModel), 
        direction = "minimize", 
        name = "Reclass.SupportVectors")
}

# tune error vs. number of support vectors on the 'iris' data set
r &lt;- tunePareto(data = iris[, -ncol(iris)], 
                labels = iris[, ncol(iris)],
                classifier = tunePareto.svm(),
                cost=c(0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50),
                objectiveFunctions=list(reclassError(), reclassSupportVectors()))

print(r)
</code></pre>

<hr>
<h2 id='generateCVRuns'>
Generate cross-validation partitions
</h2><span id='topic+generateCVRuns'></span>

<h3>Description</h3>

<p>This function generates a set of partitions for a cross-validation. It can be employed if the same cross-validation settings should be used in the objective functions of several experiments. The resulting fold list can be passed to the cross-validation objective functions (see <code><a href="#topic+predefinedObjectiveFunctions">predefinedObjectiveFunctions</a></code>) and the internal cross-validation precalculation function <code><a href="#topic+crossValidation">crossValidation</a></code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generateCVRuns(labels, 
               ntimes = 10, 
               nfold = 10, 
               leaveOneOut = FALSE, 
               stratified = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generateCVRuns_+3A_labels">labels</code></td>
<td>

<p>A vector of class labels of the data set to be used for the cross-validation.
</p>
</td></tr>
<tr><td><code id="generateCVRuns_+3A_nfold">nfold</code></td>
<td>

<p>The number of groups of the cross-validation. Ignored if <code>leaveOneOut=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="generateCVRuns_+3A_ntimes">ntimes</code></td>
<td>

<p>The number of repeated runs of the cross-validation. Ignored if <code>leaveOneOut=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="generateCVRuns_+3A_leaveoneout">leaveOneOut</code></td>
<td>

<p>If this is true, a leave-one-out cross-validation is performed, i.e. each sample is left out once in the training phase and used as a test sample
</p>
</td></tr>
<tr><td><code id="generateCVRuns_+3A_stratified">stratified</code></td>
<td>

<p>If set to true, a stratified cross-validation is carried out. That is, the percentage of samples from different classes in the cross-validation folds corresponds to the class sizes in the complete data set. If set to false, the folds may be unbalanced.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with <code>ntimes</code> elements, each representing a cross-validation run. Each of the runs is a list of <code>nfold</code> vectors specifying the indices of the samples to be left out in the folds.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predefinedObjectiveFunctions">predefinedObjectiveFunctions</a></code>, <code><a href="#topic+crossValidation">crossValidation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# precalculate the cross-validation partitions
foldList &lt;- generateCVRuns(labels = iris[, ncol(iris)],
                           ntimes = 10,
                           nfold = 10,
                           stratified=TRUE)

 # build a list of objective functions
objectiveFunctions &lt;- list(cvError(foldList=foldList),
                           cvSensitivity(foldList=foldList,caseClass="setosa"))

# pass them to tunePareto
print(tunePareto(data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 classifier = tunePareto.knn(),
                 k = c(3,5,7,9),
                 objectiveFunctions = objectiveFunctions))
</code></pre>

<hr>
<h2 id='mergeTuneParetoResults'>
Calculate optimal solutions from several calls of tunePareto
</h2><span id='topic+mergeTuneParetoResults'></span>

<h3>Description</h3>

<p>Merges the results of multiple <code>TuneParetoResult</code> objects as returned by <code><a href="#topic+tunePareto">tunePareto</a></code>, and recalculates the optimal solutions for the merged solution set. All supplied <code>TuneParetoResult</code> objects must use the same objective functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mergeTuneParetoResults(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mergeTuneParetoResults_+3A_...">...</code></td>
<td>

<p>A set of <code>TuneParetoResult</code> objects to be merged.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>TuneParetoResult</code> object containing the parameter configurations of all objects in the <code>...</code> argument and selecting the Pareto-optimal solutions among all these configurations. For more details on the object structure, refer to <code><a href="#topic+tunePareto">tunePareto</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tunePareto">tunePareto</a></code>, <code><a href="#topic+recalculateParetoSet">recalculateParetoSet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# optimize an SVM with small costs on 
# the 'iris' data set
r1 &lt;- tunePareto(classifier = tunePareto.svm(),
                 data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 cost=seq(0.01,0.1,0.01),
                 objectiveFunctions=list(cvWeightedError(10, 10),
                                         cvSensitivity(10, 10, caseClass="setosa")))
print(r1)
                                         
# another call to tunePareto with higher costs
r2 &lt;- tunePareto(classifier = tunePareto.svm(),
                 data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 cost=seq(0.5,10,0.5),
                 objectiveFunctions=list(cvWeightedError(10, 10),
                                         cvSensitivity(10, 10, caseClass="setosa")))
print(r2)

# merge the results
print(mergeTuneParetoResults(r1,r2))

</code></pre>

<hr>
<h2 id='plotDominationGraph'>
Visualize the Pareto fronts of parameter configuration scores
</h2><span id='topic+plotDominationGraph'></span>

<h3>Description</h3>

<p>Draws the Pareto fronts and domination relations of tested parameter configurations in a graph. Here, the leftmost column of nodes represents the non-dominated configurations (i.e. the first Pareto front). The second column contains the second Pareto front, i.e. the configurations that are only dominated by the first Pareto front, and so on. An edge between two configurations indicate that the first configuration is dominated by the second.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotDominationGraph(tuneParetoResult, 
                    transitiveReduction = TRUE, 
                    drawDominatedObjectives = TRUE, 
                    drawLabels = TRUE, 
                    drawLegend = TRUE,
                    x.legend = "topleft",
                    cex.legend = 0.7,
                    col.indicator, 
                    pch.indicator = 15, 
                    cex.indicator = 0.8,
                    ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotDominationGraph_+3A_tuneparetoresult">tuneParetoResult</code></td>
<td>

<p>An object of class <code>TuneParetoResult</code> as returned by <code><a href="#topic+tunePareto">tunePareto</a></code>.
</p>
</td></tr>
<tr><td><code id="plotDominationGraph_+3A_transitivereduction">transitiveReduction</code></td>
<td>

<p>If this is true, transitive edges in the graph are removed to enhance readability. That is, if configuration <code>c1</code> dominates configuration <code>c2</code> and <code>c2</code> dominates <code>c3</code>, no edge from <code>c3</code> to <code>c1</code> is drawn.
</p>
</td></tr>
<tr><td><code id="plotDominationGraph_+3A_drawdominatedobjectives">drawDominatedObjectives</code></td>
<td>

<p>If set to true, color indicators are drawn next to the nodes. Here, each color corresponds to one objective. The color is drawn next to a node if this node has the best score in this objectives among all solutions of the same Pareto front (i.e., column of the graph).
</p>
</td></tr>
<tr><td><code id="plotDominationGraph_+3A_drawlabels">drawLabels</code></td>
<td>

<p>Specifies whether the parameter configurations should be printed next to the corresponding edges.
</p>
</td></tr>
<tr><td><code id="plotDominationGraph_+3A_drawlegend">drawLegend</code></td>
<td>

<p>If <code>drawDominatedObjectives=TRUE</code>, this specifies whether a legend with the objective colors should be drawn.
</p>
</td></tr>
<tr><td><code id="plotDominationGraph_+3A_x.legend">x.legend</code></td>
<td>

<p>The position of the legend. For details, refer to the <code>x</code> parameter of <code><a href="graphics.html#topic+legend">legend</a></code>.
</p>
</td></tr>
<tr><td><code id="plotDominationGraph_+3A_cex.legend">cex.legend</code></td>
<td>

<p>Specifies the size of the text in the legend if <code>drawLegend</code> is true.  
</p>
</td></tr>
<tr><td><code id="plotDominationGraph_+3A_col.indicator">col.indicator</code></td>
<td>

<p>Specifies an optional list of colors, one for each objective function. These colors will be used for the indicators if <code>drawDominatedObjectives</code> is true. By default, a predefined set of colors is used. 
</p>
</td></tr>
<tr><td><code id="plotDominationGraph_+3A_pch.indicator">pch.indicator</code></td>
<td>

<p>Specifies a single plotting character or a list of plotting characters for the objective functions in the indicators which is used for the indicators if <code>drawDominatedObjectives</code> is true.  
</p>
</td></tr>
<tr><td><code id="plotDominationGraph_+3A_cex.indicator">cex.indicator</code></td>
<td>

<p>Specifies the size of the symbols in the indicators which is be used for the indicators if <code>drawDominatedObjectives</code> is true. This can also be a vector of sizes for the symbols of the objectives. 
</p>
</td></tr>
<tr><td><code id="plotDominationGraph_+3A_...">...</code></td>
<td>

<p>Further graphical parameters for <code><a href="igraph.html#topic+plot.graph">plot.igraph</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns the <code>igraph</code> object representing the graph.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tunePareto">tunePareto</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># call tunePareto using a k-NN classifier 
# with different 'k' and 'l' on the 'iris' data set
x &lt;- tunePareto(data = iris[, -ncol(iris)], 
                labels = iris[, ncol(iris)],
                classifier = tunePareto.knn(),
                k = c(5,7,9),
                l = c(1,2,3),
                objectiveFunctions=list(cvError(10, 10),
                                        cvSpecificity(10, 10, caseClass="setosa")))

# plot the graph                 
plotDominationGraph(x)
</code></pre>

<hr>
<h2 id='plotObjectivePairs'>
Plot a matrix of Pareto front panels
</h2><span id='topic+plotObjectivePairs'></span>

<h3>Description</h3>

<p>Plots a matrix of Pareto front panels for each pair of objectives. The plot for <code>n</code> objectives consists of <code>n x n</code> panels, where the panel in row <code>i</code> and column <code>j</code> depicts the Pareto fronts of the <code>i</code>-th and the <code>j</code>-th objective. Each of the panels is drawn in the same way as <code><a href="#topic+plotParetoFronts2D">plotParetoFronts2D</a></code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotObjectivePairs(tuneParetoResult, 
                   drawLabels = TRUE, 
                   drawBoundaries = TRUE, 
                   labelPos = 4,
                   fitLabels=TRUE, 
                   cex.conf=0.5, 
                   lty.fronts=1, 
                   pch.fronts=8, 
                   col.fronts,
                   ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotObjectivePairs_+3A_tuneparetoresult">tuneParetoResult</code></td>
<td>

<p>An object of class <code>TuneParetoResult</code> as returned by <code><a href="#topic+tunePareto">tunePareto</a></code>.
</p>
</td></tr>
<tr><td><code id="plotObjectivePairs_+3A_drawlabels">drawLabels</code></td>
<td>

<p>If set to true, the descriptions of the configurations are printed next to the points in the plot.
</p>
</td></tr>
<tr><td><code id="plotObjectivePairs_+3A_drawboundaries">drawBoundaries</code></td>
<td>

<p>If set to true, the upper or lower objective limits supplied in the <code>objectiveBoundaries</code> parameter of <code><a href="#topic+tunePareto">tunePareto</a></code> are drawn as horizontal and vertical lines.
</p>
</td></tr>
<tr><td><code id="plotObjectivePairs_+3A_labelpos">labelPos</code></td>
<td>

<p>The position of the configuration labels in the plot (if <code>drawLabels</code> is true). Values of 1, 2, 3 and 4 denote positions below, to the left of, above and to the right of the points on the Pareto fronts.
</p>
</td></tr>
<tr><td><code id="plotObjectivePairs_+3A_fitlabels">fitLabels</code></td>
<td>

<p>If this parameter is true (and <code>drawLabels</code> is true), overlapping or partially hidden configuration labels are removed from the plot to improve the readability of the remaining labels.
</p>
</td></tr>
<tr><td><code id="plotObjectivePairs_+3A_cex.conf">cex.conf</code></td>
<td>

<p>The size of the configuration labels in the plots (if <code>drawLabels</code> is true).  
</p>
</td></tr>
<tr><td><code id="plotObjectivePairs_+3A_lty.fronts">lty.fronts</code></td>
<td>

<p>A vector of line types to use for the Pareto fronts. By default, straight lines are drawn for all fronts.  
</p>
</td></tr>
<tr><td><code id="plotObjectivePairs_+3A_pch.fronts">pch.fronts</code></td>
<td>

<p>A vector of symbols to use for points on the Pareto fronts. All points on the same front will have the same symbol. By default, an asterisk is used.
</p>
</td></tr>
<tr><td><code id="plotObjectivePairs_+3A_col.fronts">col.fronts</code></td>
<td>

<p>A vector of colors to use for the Pareto fronts. By default, a predefined set of colors is used.  
</p>
</td></tr>
<tr><td><code id="plotObjectivePairs_+3A_...">...</code></td>
<td>

<p>Further graphical parameters to be passed to the <code><a href="graphics.html#topic+plot">plot</a></code> function.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function does not have a return value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tunePareto">tunePareto</a></code>, <code><a href="#topic+plotParetoFronts2D">plotParetoFronts2D</a></code>, <code><a href="#topic+plotDominationGraph">plotDominationGraph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# optimize the 'cost' parameter of an SVM according
# to CV error, CV error variance, and CV Specificity 
# on two classes of the 'iris' data set
r &lt;- tunePareto(data = iris[, -ncol(iris)], 
                labels = iris[, ncol(iris)],
                classifier = tunePareto.svm(),
                cost=c(0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50), 
                objectiveFunctions=list(cvError(10, 10),
                                        cvErrorVariance(10, 10),
                                        cvSpecificity(10, 10, caseClass="virginica")))

# plot the matrix of Pareto fronts
plotObjectivePairs(r)

</code></pre>

<hr>
<h2 id='plotParetoFronts2D'>
A classical 2-dimensional plot of Pareto fronts
</h2><span id='topic+plotParetoFronts2D'></span>

<h3>Description</h3>

<p>Draws a classical Pareto front plot of 2 objectives of a parameter tuning. The first objective is on the x axis of the plot, and the second objective is on the y axis. Points on a Pareto front are connected by lines. Each Pareto front is drawn in a different color.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotParetoFronts2D(tuneParetoResult, 
                   objectives, 
                   drawLabels = TRUE, 
                   drawBoundaries = TRUE, 
                   labelPos = 4,
                   fitLabels=TRUE,
                   cex.conf=0.5, 
                   lty.fronts=1, 
                   pch.fronts=8, 
                   col.fronts,
                   ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotParetoFronts2D_+3A_tuneparetoresult">tuneParetoResult</code></td>
<td>

<p>An object of class <code>TuneParetoResult</code> as returned by <code><a href="#topic+tunePareto">tunePareto</a></code>.
</p>
</td></tr>
<tr><td><code id="plotParetoFronts2D_+3A_objectives">objectives</code></td>
<td>

<p>The names or indices of the two objectives to plot. Pareto-optimality is determined only on the basis of these two objectives. Optional if the parameter tuning has exactly two objectives.
</p>
</td></tr>
<tr><td><code id="plotParetoFronts2D_+3A_drawlabels">drawLabels</code></td>
<td>

<p>If set to true, the descriptions of the configurations are printed next to the points in the plot.
</p>
</td></tr>
<tr><td><code id="plotParetoFronts2D_+3A_drawboundaries">drawBoundaries</code></td>
<td>

<p>If set to true, the upper or lower objective limits supplied in the <code>objectiveBoundaries</code> parameter of <code><a href="#topic+tunePareto">tunePareto</a></code> are drawn as horizontal and vertical lines.
</p>
</td></tr>
<tr><td><code id="plotParetoFronts2D_+3A_labelpos">labelPos</code></td>
<td>

<p>The position of the configuration labels in the plot (if <code>drawLabels</code> is true). Values of 1, 2, 3 and 4 denote positions below, to the left of, above and to the right of the points on the Pareto fronts.
</p>
</td></tr>
<tr><td><code id="plotParetoFronts2D_+3A_fitlabels">fitLabels</code></td>
<td>

<p>If this parameter is true (and <code>drawLabels</code> is true), overlapping or partially hidden configuration labels are removed from the plot to improve the readability of the remaining labels.
</p>
</td></tr>
<tr><td><code id="plotParetoFronts2D_+3A_cex.conf">cex.conf</code></td>
<td>

<p>The size of the configuration labels in the plots (if <code>drawLabels</code> is true).  
</p>
</td></tr>
<tr><td><code id="plotParetoFronts2D_+3A_lty.fronts">lty.fronts</code></td>
<td>

<p>A vector of line types to use for the Pareto fronts. By default, straight lines are drawn for all fronts.  
</p>
</td></tr>
<tr><td><code id="plotParetoFronts2D_+3A_pch.fronts">pch.fronts</code></td>
<td>

<p>A vector of symbols to use for points on the Pareto fronts. All points on the same front will have the same symbol. By default, an asterisk is used.
</p>
</td></tr>
<tr><td><code id="plotParetoFronts2D_+3A_col.fronts">col.fronts</code></td>
<td>

<p>A vector of colors to use for the Pareto fronts. By default, a predefined set of colors is used.  
</p>
</td></tr>
<tr><td><code id="plotParetoFronts2D_+3A_...">...</code></td>
<td>

<p>Further graphical parameters to be passed to the <code><a href="graphics.html#topic+plot">plot</a></code> function.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function does not have a return value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tunePareto">tunePareto</a></code>, <code><a href="#topic+plotDominationGraph">plotDominationGraph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# optimize the 'cost' parameter according
# to CV error and CV Specificity on the 'iris' data set
r &lt;- tunePareto(data = iris[, -ncol(iris)], 
                labels = iris[, ncol(iris)],
                classifier = tunePareto.svm(),
                cost=c(0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50), 
                objectiveFunctions=list(cvError(10, 10),
                                        cvSpecificity(10, 10, caseClass="setosa")))
                   
# plot the Pareto graph
plotParetoFronts2D(r)

</code></pre>

<hr>
<h2 id='precalculation'>
Predefined precalculation functions for objectives
</h2><span id='topic+precalculation'></span><span id='topic+reclassification'></span><span id='topic+crossValidation'></span>

<h3>Description</h3>

<p>These predefined precalculation functions can be employed to create own objectives using <code><a href="#topic+createObjective">createObjective</a></code>. They perform a reclassification or a cross-validation and return the true labels and the predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reclassification(data, labels, 
                 classifier, classifierParams, predictorParams)

crossValidation(data, labels, 
                classifier, classifierParams, predictorParams, 
                ntimes = 10, nfold = 10, 
                leaveOneOut = FALSE, stratified = FALSE,
                foldList = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="precalculation_+3A_data">data</code></td>
<td>

<p>The data set to be used for the precalculation. This is usually a matrix or data frame with the samples in the rows and the features in the columns.
</p>
</td></tr>
<tr><td><code id="precalculation_+3A_labels">labels</code></td>
<td>

<p>A vector of class labels for the samples in <code>data</code>.
</p>
</td></tr>
<tr><td><code id="precalculation_+3A_classifier">classifier</code></td>
<td>

<p>A <code>TuneParetoClassifier</code> wrapper object containing the classifier to tune. A number of state-of-the-art classifiers are included in <span class="pkg">TunePareto</span>  (see <code><a href="#topic+predefinedClassifiers">predefinedClassifiers</a></code>). Custom classifiers can be employed using <code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>.</p>
</td></tr>
<tr><td><code id="precalculation_+3A_classifierparams">classifierParams</code></td>
<td>

<p>A named list of parameter assignments for the training routine of the classifier.
</p>
</td></tr>
<tr><td><code id="precalculation_+3A_predictorparams">predictorParams</code></td>
<td>

<p>If the classifier consists of separate training and prediction functions, a named list of parameter assignments for the predictor function.
</p>
</td></tr>
<tr><td><code id="precalculation_+3A_nfold">nfold</code></td>
<td>

<p>The number of groups of the cross-validation. Ignored if <code>leaveOneOut=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="precalculation_+3A_ntimes">ntimes</code></td>
<td>

<p>The number of repeated runs of the cross-validation. Ignored if <code>leaveOneOut=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="precalculation_+3A_leaveoneout">leaveOneOut</code></td>
<td>

<p>If this is true, a leave-one-out cross-validation is performed, i.e. each sample is left out once in the training phase and used as a test sample
</p>
</td></tr>
<tr><td><code id="precalculation_+3A_stratified">stratified</code></td>
<td>

<p>If set to true, a stratified cross-validation is carried out. That is, the percentage of samples from different classes in the cross-validation folds corresponds to the class sizes in the complete data set. If set to false, the folds may be unbalanced.
</p>
</td></tr>
<tr><td><code id="precalculation_+3A_foldlist">foldList</code></td>
<td>

<p>If this parameter is set, the other cross-validation parameters (<code>ntimes</code>, <code>nfold</code>, <code>leaveOneOut</code>, <code>stratified</code>) are ignored. Instead, the precalculated cross-validation partition supplied in <code>foldList</code> is used. This allows for using the same cross-validation experiment in multiple <code><a href="#topic+tunePareto">tunePareto</a></code> calls. Partitions can be generated using <code><a href="#topic+generateCVRuns">generateCVRuns</a></code>. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>reclassification</code> trains the classifier with the full data set. Afterwards, the classifier is applied to the same data set. 
</p>
<p><code>crossValidate</code> partitions the samples in the data set into a number of groups (depending on <code>nfold</code> and <code>leaveOneOut</code>). Each of these groups is left out once in the training phase and used for prediction. The whole procedure is repeated several times (as specified in <code>ntimes</code>). 
</p>


<h3>Value</h3>

<p><code>reclassification</code> returns a list with the following components:
</p>

<dl>
<dt>trueLabels</dt><dd><p>The original labels of the dataset as supplied in <code>labels</code></p>
</dd>
<dt>predictedLabels</dt><dd><p>A vector of predicted labels of the data set</p>
</dd>
<dt>model</dt><dd><p>The <code>TuneParetoModel</code> object resulting from the classifier training</p>
</dd>
</dl>

<p><code>crossValidation</code> returns a nested list structure. At the top level, there is one list element for each run of the cross-validation. Each of these elements consists of a list of sub-structures for each fold. The sub-structures have the following components:
</p>

<dl>
<dt>trueLabels</dt><dd><p>The original labels of the test samples in the fold</p>
</dd>
<dt>predictedLabels</dt><dd><p>A vector of predicted labels of the test samples in the fold</p>
</dd>
<dt>model</dt><dd><p>The <code>TuneParetoModel</code> object resulting from the classifier training in the fold</p>
</dd>
</dl>

<p>That is, for a cross-validation with <code>n</code> runs and <code>m</code> folds, there are <code>n</code> top-level lists, each having <code>m</code> sub-lists comprising the true labels and the predicted labels.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+createObjective">createObjective</a></code>, <code><a href="#topic+generateCVRuns">generateCVRuns</a></code>. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create new objective minimizing the
# false positives of a reclassification

cvFalsePositives &lt;- function(nfold=10, ntimes=10, leaveOneOut=FALSE, foldList=NULL, caseClass)
{
  return(createObjective(
            precalculationFunction = "crossValidation",
            precalculationParams = list(nfold=nfold, 
                                        ntimes=ntimes, 
                                        leaveOneOut=leaveOneOut,
                                        foldList=foldList),
            objectiveFunction = 
            function(result, caseClass)
            {
             
              # take mean value over the cv runs
              return(mean(sapply(result,
                    function(run)
                    # iterate over runs of cross-validation
                    {
                      # extract all predicted labels in the folds
                      predictedLabels &lt;- 
                            unlist(lapply(run,
                                         function(fold)fold$predictedLabels))
    
                      # extract all true labels in the folds
                      trueLabels &lt;- 
                            unlist(lapply(run,
                                          function(fold)fold$trueLabels))
                      
                      # calculate number of false positives in the run
                      return(sum(predictedLabels == caseClass &amp; 
                                 trueLabels != caseClass))
                    })))
            },
            objectiveFunctionParams = list(caseClass=caseClass),
            direction = "minimize",        
            name = "CV.FalsePositives"))                  
}

# use the objective in an SVM cost parameter tuning on the 'iris' data set
r &lt;- tunePareto(data = iris[, -ncol(iris)], 
                labels = iris[, ncol(iris)],
                classifier = tunePareto.svm(),
                cost = c(0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50),
                objectiveFunctions=list(cvFalsePositives(10, 10, caseClass="setosa")))
print(r)

</code></pre>

<hr>
<h2 id='predefinedClassifiers'>
TunePareto wrappers for certain classifiers
</h2><span id='topic+predefinedClassifiers'></span><span id='topic+tunePareto.knn'></span><span id='topic+tunePareto.svm'></span><span id='topic+tunePareto.tree'></span><span id='topic+tunePareto.randomForest'></span><span id='topic+tunePareto.NaiveBayes'></span>

<h3>Description</h3>

<p>Creates TunePareto classifier objects for the k-Nearest Neighbour classifier, support vector machines, and trees.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tunePareto.knn()
               
tunePareto.svm()
               
tunePareto.tree()
                
tunePareto.randomForest()

tunePareto.NaiveBayes()
</code></pre>


<h3>Details</h3>

<p><code>tunePareto.knn</code> encapsulates a k-Nearest Neighbour classifier as defined in <code>link[class]{knn}</code> in package <span class="pkg">class</span>. The classifier allows for supplying and tuning the following parameters of <code>link[class]{knn}</code>: 
</p>
<p><code>k, l, use.all</code>
</p>
<p><code>tunePareto.svm</code> encapsulates the support vector machine <code><a href="e1071.html#topic+svm">svm</a></code> classifier in package <span class="pkg">e1071</span>. The classifier allows for supplying and tuning the following parameters: 
</p>
<p><code>kernel, degree, gamma, 
      coef0, cost, nu, 
      class.weights, cachesize, 
      tolerance, epsilon, 
      scale, shrinking, fitted,
      subset, na.action</code>
</p>
<p><code>tunePareto.tree</code> encapsulates the CART classifier <code><a href="tree.html#topic+tree">tree</a></code> in package <span class="pkg">tree</span>. The classifier allows for supplying and tuning the following parameters: 
</p>
<p><code>weights, subset, 
      na.action, method,
      split, mincut, minsize, mindev</code>
</p>
<p>as well as the <code>type</code> parameter of <code><a href="tree.html#topic+predict.tree">predict.tree</a></code>.
</p>
<p><code>tunePareto.randomForest</code> encapsulates the <code><a href="randomForest.html#topic+randomForest">randomForest</a></code> classifier in package <span class="pkg">randomForest</span>. The classifier allows for supplying and tuning the following parameters:
</p>
<p><code>subset, na.action,
      ntree,  mtry,
      replace, classwt, 
      cutoff, strata,
      sampsize, nodesize,
      maxnodes</code>
</p>
<p><code>tunePareto.NaiveBayes</code> encapsulates the <code><a href="klaR.html#topic+NaiveBayes">NaiveBayes</a></code> classifier in package <span class="pkg">klaR</span>. The classifier allows for supplying and tuning the following parameters:
</p>
<p><code>prior, usekernel, fL, subset,
      na.action, bw, adjust, kernel, weights,
      window, width, give.Rkern, n,
      from, to, cut, na.rm</code>
</p>


<h3>Value</h3>

<p>Returns objects of class <code>TuneParetoClassifier</code> as described in <code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>. These can be passed to functions like <code><a href="#topic+tunePareto">tunePareto</a></code> or <code><a href="#topic+trainTuneParetoClassifier">trainTuneParetoClassifier</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>, <code><a href="#topic+tunePareto">tunePareto</a></code>, <code><a href="#topic+trainTuneParetoClassifier">trainTuneParetoClassifier</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# tune a k-NN classifier with different 'k' and 'l' 
# on the 'iris' data set
print(tunePareto(classifier = tunePareto.knn(),
                 data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 k = c(5,7,9),
                 l = c(1,2,3),
                 objectiveFunctions=list(cvError(10, 10),
                                         cvSpecificity(10, 10, caseClass="setosa"))))
                 
# tune an SVM with different costs on 
# the 'iris' data set
# using Halton sequences for sampling
print(tunePareto(classifier = tunePareto.svm(),
                 data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 cost = as.interval(0.001,10),
                 sampleType = "halton",
                 numCombinations=20,                 
                 objectiveFunctions=list(cvWeightedError(10, 10),
                                         cvSensitivity(10, 10, caseClass="setosa"))))

# tune a CART classifier with different 
# splitting criteria on the 'iris' data set
print(tunePareto(classifier = tunePareto.tree(),
                 data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 split = c("deviance","gini"),
                 objectiveFunctions=list(cvError(10, 10),
                                         cvErrorVariance(10, 10))))

# tune a Random Forest with different numbers of trees 
# on the 'iris' data set
print(tunePareto(classifier = tunePareto.randomForest(),
                 data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 ntree = seq(50,300,50),
                 objectiveFunctions=list(cvError(10, 10),
                                         cvSpecificity(10, 10, caseClass="setosa"))))

# tune a Naive Bayes classifier with different kernels
# on the 'iris' data set
print(tunePareto(classifier = tunePareto.NaiveBayes(),
                 data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 kernel = c("gaussian", "epanechnikov", "rectangular",
                            "triangular", "biweight",
                            "cosine", "optcosine"),
                 objectiveFunctions=list(cvError(10, 10),
                                         cvSpecificity(10, 10, caseClass="setosa"))))
                             

</code></pre>

<hr>
<h2 id='predefinedObjectiveFunctions'>
Predefined objective functions for parameter tuning
</h2><span id='topic+predefinedObjectiveFunctions'></span><span id='topic+reclassAccuracy'></span><span id='topic+reclassError'></span><span id='topic+reclassWeightedError'></span><span id='topic+reclassSensitivity'></span><span id='topic+reclassRecall'></span><span id='topic+reclassTruePositive'></span><span id='topic+reclassSpecificity'></span><span id='topic+reclassTrueNegative'></span><span id='topic+reclassFallout'></span><span id='topic+reclassFalsePositive'></span><span id='topic+reclassMiss'></span><span id='topic+reclassFalseNegative'></span><span id='topic+reclassPrecision'></span><span id='topic+reclassPPV'></span><span id='topic+reclassNPV'></span><span id='topic+reclassConfusion'></span><span id='topic+cvAccuracy'></span><span id='topic+cvError'></span><span id='topic+cvErrorVariance'></span><span id='topic+cvWeightedError'></span><span id='topic+cvSensitivity'></span><span id='topic+cvRecall'></span><span id='topic+cvTruePositive'></span><span id='topic+cvSpecificity'></span><span id='topic+cvTrueNegative'></span><span id='topic+cvFallout'></span><span id='topic+cvFalsePositive'></span><span id='topic+cvMiss'></span><span id='topic+cvFalseNegative'></span><span id='topic+cvPrecision'></span><span id='topic+cvPPV'></span><span id='topic+cvNPV'></span><span id='topic+cvConfusion'></span>

<h3>Description</h3>

<p>Predefined objective functions that calculate several performance criteria of reclassification or cross-validation experiments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reclassAccuracy(saveModel = FALSE)
reclassError(saveModel = FALSE)
reclassWeightedError(saveModel = FALSE)
reclassSensitivity(caseClass, saveModel = FALSE)
reclassRecall(caseClass, saveModel = FALSE)
reclassTruePositive(caseClass, saveModel = FALSE)
reclassSpecificity(caseClass, saveModel = FALSE)
reclassTrueNegative(caseClass, saveModel = FALSE)
reclassFallout(caseClass, saveModel = FALSE)
reclassFalsePositive(caseClass, saveModel = FALSE)
reclassMiss(caseClass, saveModel = FALSE)
reclassFalseNegative(caseClass, saveModel = FALSE)
reclassPrecision(caseClass, saveModel = FALSE)
reclassPPV(caseClass, saveModel = FALSE)
reclassNPV(caseClass, saveModel = FALSE)
reclassConfusion(trueClass, predictedClass, saveModel = FALSE)

cvAccuracy(nfold = 10, ntimes = 10,
           leaveOneOut = FALSE, stratified = FALSE,
           foldList = NULL,
           saveModel = FALSE) 
cvError(nfold = 10, ntimes = 10, 
        leaveOneOut = FALSE, stratified=FALSE, 
        foldList=NULL,
        saveModel = FALSE)
cvErrorVariance(nfold = 10, ntimes = 10, 
                leaveOneOut = FALSE, stratified=FALSE, 
                foldList=NULL,
                saveModel = FALSE)
cvWeightedError(nfold = 10, ntimes = 10, 
                leaveOneOut = FALSE, stratified=FALSE, 
                foldList=NULL,
                saveModel = FALSE)
cvSensitivity(nfold = 10, ntimes = 10, 
              leaveOneOut = FALSE, stratified=FALSE, 
              foldList=NULL, caseClass,
              saveModel = FALSE)
cvRecall(nfold = 10, ntimes = 10,
         leaveOneOut = FALSE, stratified=FALSE,
         foldList=NULL, caseClass,
         saveModel = FALSE)
cvTruePositive(nfold = 10, ntimes = 10, 
               leaveOneOut = FALSE, stratified=FALSE, 
               foldList=NULL, caseClass,
               saveModel = FALSE)
cvSpecificity(nfold = 10, ntimes = 10, 
              leaveOneOut = FALSE, stratified=FALSE, 
              foldList=NULL, caseClass,
              saveModel = FALSE)
cvTrueNegative(nfold = 10, ntimes = 10, 
               leaveOneOut = FALSE, stratified=FALSE, 
               foldList=NULL, caseClass,
               saveModel = FALSE)
cvFallout(nfold = 10, ntimes = 10, 
          leaveOneOut = FALSE, stratified=FALSE, 
          foldList=NULL, caseClass,
          saveModel = FALSE)
cvFalsePositive(nfold = 10, ntimes = 10, 
                leaveOneOut = FALSE, stratified=FALSE, 
                foldList=NULL, caseClass,
                saveModel = FALSE)          
cvMiss(nfold = 10, ntimes = 10, 
       leaveOneOut = FALSE, stratified=FALSE, 
       foldList=NULL, caseClass,
       saveModel = FALSE)
cvFalseNegative(nfold = 10, ntimes = 10,
                leaveOneOut = FALSE, stratified=FALSE,
                foldList=NULL, caseClass,
                saveModel = FALSE)
cvPrecision(nfold = 10, ntimes = 10,
            leaveOneOut = FALSE, stratified=FALSE,
            foldList=NULL, caseClass,
            saveModel = FALSE)
cvPPV(nfold = 10, ntimes = 10,
      leaveOneOut = FALSE, stratified=FALSE,
      foldList=NULL, caseClass,
      saveModel = FALSE)
cvNPV(nfold = 10, ntimes = 10,
      leaveOneOut = FALSE, stratified=FALSE,
      foldList=NULL, caseClass,
      saveModel = FALSE)
cvConfusion(nfold = 10, ntimes = 10, 
            leaveOneOut = FALSE, stratified=FALSE,
            foldList=NULL, trueClass, predictedClass,
            saveModel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predefinedObjectiveFunctions_+3A_nfold">nfold</code></td>
<td>

<p>The number of groups of the cross-validation. Ignored if <code>leaveOneOut=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="predefinedObjectiveFunctions_+3A_ntimes">ntimes</code></td>
<td>

<p>The number of repeated runs of the cross-validation. Ignored if <code>leaveOneOut=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="predefinedObjectiveFunctions_+3A_leaveoneout">leaveOneOut</code></td>
<td>

<p>If this is true, a leave-one-out cross-validation is performed, i.e. each sample is left out once in the training phase and used as a test sample
</p>
</td></tr>
<tr><td><code id="predefinedObjectiveFunctions_+3A_stratified">stratified</code></td>
<td>

<p>If set to true, a stratified cross-validation is carried out. That is, the percentage of samples from different classes in the cross-validation folds corresponds to the class sizes in the complete data set. If set to false, the folds may be unbalanced.
</p>
</td></tr>
<tr><td><code id="predefinedObjectiveFunctions_+3A_foldlist">foldList</code></td>
<td>

<p>If this parameter is set, the other cross-validation parameters (<code>ntimes</code>, <code>nfold</code>, <code>leaveOneOut</code>, <code>stratified</code>) are ignored. Instead, the precalculated cross-validation partition supplied in <code>foldList</code> is used. This allows for using the same cross-validation experiment in multiple <code><a href="#topic+tunePareto">tunePareto</a></code> calls. Partitions can be generated using <code><a href="#topic+generateCVRuns">generateCVRuns</a></code>.
</p>
</td></tr>
<tr><td><code id="predefinedObjectiveFunctions_+3A_caseclass">caseClass</code></td>
<td>
<p>The class containing the positive samples for the calculation of specificity and sensitivity. All samples with different class labels are regarded as controls (negative samples).</p>
</td></tr>
<tr><td><code id="predefinedObjectiveFunctions_+3A_trueclass">trueClass</code></td>
<td>

<p>When calculating the confusion of two classes, the class to which a sample truly belongs.
</p>
</td></tr>
<tr><td><code id="predefinedObjectiveFunctions_+3A_predictedclass">predictedClass</code></td>
<td>

<p>When calculating the confusion of two classes, the class to which a sample is erreneously assigned.
</p>
</td></tr>
<tr><td><code id="predefinedObjectiveFunctions_+3A_savemodel">saveModel</code></td>
<td>

<p>If set to true, the trained model(s) are stored to the <code>additionalData</code> component of the resulting <code>TuneParetoResult</code> objects (see <code><a href="#topic+tunePareto">tunePareto</a></code> for details). In case of a reclassification, a single model is stored. In case of a cross-validation, a list of length <code>nruns</code>, each containing a sub-list of <code>nfold</code> models, is stored. If the size of a model is large, setting <code>saveModel = TRUE</code> can result in a high memory consumption. As the model information is the same for all reclassification objectives or for cross-validation objectives with the same parameters, it is usually sufficient to set <code>saveModel=TRUE</code> for only one of the objective functions.
</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>The functions do not calculate the objectives directly, but return a structure of class <code>TuneParetoObjectives</code> that provides all information on the objective function for later use in <code><a href="#topic+tunePareto">tunePareto</a></code>.
</p>
<p>The behaviour of the functions in <code><a href="#topic+tunePareto">tunePareto</a></code> is as follows:
</p>
<p>The reclassification functions train the classifiers with the full data set. Afterwards, the classifiers are applied to the same data set. <code>reclassAccuracy</code> measures the fraction of correctly classified samples, while <code>reclassError</code> calculates the fraction of misclassified samples. <code>reclassWeightedError</code> calculates the sum of fractions of misclassified samples in each class weighted by the class size. <code>reclassSensitivity</code> measures the sensitivity, and <code>reclassSpecificity</code> measures the specificity of the reclassification experiment. <code>reclassTruePositive</code> and <code>reclassRecall</code> are aliases for <code>reclassSensitivity</code>, and <code>reclassTrueNegative</code> is an alias for <code>reclassSpecificity</code>. <code>reclassFallout</code> and its equivalent alias <code>reclassFalsePositive</code> give the ratio of false positives to all negative samples, and <code>reclassMiss</code> and its alias <code>reclassFalseNegative</code> measure the ratio of false negatives to all positive samples. <code>reclassPrecision</code> calculates the precision of the reclassification experiment, i.e. the ratio of true positives to all samples classified as positive. This is equivalent to the positive predictive value (<code>reclassPPV</code>). <code>reclassNPV</code> measures the negative predictive value, i.e. the ratio of true negatives to all samples classified as negative. <code>reclassConfusion</code> calculates the fraction of samples in <code>trueClass</code> that have been confused with <code>predictedClass</code>.
</p>
<p><code>reclassError</code>, <code>reclassWeightedError</code>, <code>reclassFallout</code>, <code>reclassFalsePositive</code>, <code>reclassMiss</code>, <code>reclassFalsePositive</code>  and <code>reclassConfusion</code> are minimization objectives, whereas <code>reclassAccuracy</code>, <code>reclassSensitivity</code>, <code>reclassTruePositive</code>, <code>reclassRecall</code>, <code>reclassSpecificity</code>, <code>reclassTrueNegative</code> <code>reclassPrecision</code>, <code>reclassPPV</code> and <code>reclassNPV</code> are maximization objectives.
</p>
<p>The cross-validation functions partition the samples in the data set into a number of groups (depending on <code>nfold</code> and <code>leaveOneOut</code>). Each of these groups is left out once in the training phase and used for prediction. The whole procedure is usually repeated several times (as specified in <code>ntimes</code>), and the results are averaged. Similar to the reclassification functions, <code>cvAccuracy</code> calculates the fraction of correctly classified samples over the runs, <code>cvError</code> calculates the average fraction of misclassified samples over the runs, and <code>cvWeightedError</code> calculates the mean sum of fractions of misclassified samples in each class weighted by the class size. <code>cvErrorVariance</code> calculates the variance of the cross-validation error. <code>cvSensitivity</code>, <code>cvRecall</code> and <code>cvTruePositive</code> calculate the average sensitivity, and <code>cvSpecificity</code> and <code>cvTrueNegative</code> calculate the average specificity. <code>cvFallout</code> and <code>cvFalsePositive</code> calculate the average false positive rate over the runs. <code>cvMiss</code> and <code>cvFalseNegative</code> calculate the average false negative rate over the runs. <code>cvPrecision</code> and <code>cvPPV</code> calculate the average precision/positive predictive rate. <code>cvNPV</code> gives the average negative predictive rate over all runs. <code>cvConfusion</code> calculates the average fraction of samples in <code>trueClass</code> that have been confused with <code>predictedClass</code>.
</p>
<p><code>cvError</code>, <code>cvWeightedError</code>, <code>cvErrorVariance</code>, <code>cvFallout</code>, <code>cvFalsePositive</code>, <code>cvMiss</code>, <code>cvFalseNegative</code> and <code>cvConfusion</code> are minimization objectives, and <code>cvAccuracy</code>, <code>cvSensitivity</code>, <code>cvRecall</code>, <code>cvTruePositive</code>, <code>cvSpecificity</code>, <code>cvTrueNegative</code>, <code>cvPrecision</code>, <code>cvPPV</code> and <code>cvNPV</code> are maximization objectives.
</p>


<h3>Value</h3>

<p>An object of class <code>TuneParetoObjective</code> representing the objective function. For more details, see <code><a href="#topic+createObjective">createObjective</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+createObjective">createObjective</a></code>, <code><a href="#topic+tunePareto">tunePareto</a></code>, <code><a href="#topic+generateCVRuns">generateCVRuns</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# build a list of objective functions
objectiveFunctions &lt;- list(cvError(10, 10),
                           reclassSpecificity(caseClass="setosa"), 
                           reclassSensitivity(caseClass="setosa"))

# pass them to tunePareto
print(tunePareto(data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 classifier = tunePareto.knn(),
                 k = c(3,5,7,9),
                 objectiveFunctions = objectiveFunctions))
</code></pre>

<hr>
<h2 id='predict.TuneParetoModel'>
Prediction method for TuneParetoClassifier objects
</h2><span id='topic+predict.TuneParetoModel'></span>

<h3>Description</h3>

<p>S3 method that predicts the labels of unknown samples using a trained <code>TunePareteModel</code> model of a <code>TuneParetoClassifier</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'TuneParetoModel'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.TuneParetoModel_+3A_object">object</code></td>
<td>

<p>A <code>TuneParetoTraining</code> object as returned by <code><a href="#topic+trainTuneParetoClassifier">trainTuneParetoClassifier</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.TuneParetoModel_+3A_newdata">newdata</code></td>
<td>

<p>The samples whose class labels are predicted. This is usually a matrix or data frame with the samples in the rows and the features in the columns.
</p>
</td></tr>
<tr><td><code id="predict.TuneParetoModel_+3A_...">...</code></td>
<td>

<p>Further parameters for the predictor. These must be defined in the <code>predictorParamNames</code> argument of <code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a vector of class labels for the samples in <code>newdata</code>-
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>, <code><a href="#topic+predefinedClassifiers">predefinedClassifiers</a></code>, <code><a href="#topic+trainTuneParetoClassifier">trainTuneParetoClassifier</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># train an SVM classifier
cl &lt;- tunePareto.svm()
tr &lt;- trainTuneParetoClassifier(cl, 
                                iris[,-ncol(iris)], 
                                iris[,ncol(iris)], 
                                cost=0.001)

# re-apply the classifier to predict the training data
print(iris[,ncol(iris)])
print(predict(tr, iris[,-ncol(iris)]))
</code></pre>

<hr>
<h2 id='print.TuneParetoResult'>
Print method for objects used in TunePareto
</h2><span id='topic+print.TuneParetoResult'></span><span id='topic+print.TuneParetoClassifier'></span><span id='topic+print.TuneParetoModel'></span>

<h3>Description</h3>

<p>Customized printing methods for several objects used in TunePareto: For <code>TuneParetoResult</code> objects, the Pareto-optimal parameter configurations are printed. For <code>TuneParetoClassifier</code> and <code>TuneParetoModel</code> objects, information on the classifier and its parameters is printed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'TuneParetoResult'
print(x, ...)
## S3 method for class 'TuneParetoClassifier'
print(x, ...)
## S3 method for class 'TuneParetoModel'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.TuneParetoResult_+3A_x">x</code></td>
<td>
<p>An object of class <code>TuneParetoResult</code>, <code>TuneParetoClassifier</code> or <code>TuneParetoModel</code> to be printed.
</p>
</td></tr>
<tr><td><code id="print.TuneParetoResult_+3A_...">...</code></td>
<td>
<p>Further parameters (currently unused).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns the printed object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tunePareto">tunePareto</a></code>, <code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>, <code><a href="#topic+trainTuneParetoClassifier">trainTuneParetoClassifier</a></code>
</p>

<hr>
<h2 id='rankByDesirability'>
Rank results according to their desirabilities
</h2><span id='topic+rankByDesirability'></span>

<h3>Description</h3>

<p>Calculates the desirability index for each Pareto-optimal combination (or for all combinations), and ranks the combinations according to this value. The desirability index was introduced by Harrington in 1965 for multicriteria optimization. Desirability functions specify the desired values of each objective and are aggregated in a single desirability index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rankByDesirability(tuneParetoResult, 
                   desirabilityIndex, 
                   optimalOnly = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rankByDesirability_+3A_tuneparetoresult">tuneParetoResult</code></td>
<td>

<p>A <code>TuneParetoResult</code> object containing the parameter configurations to be examined
</p>
</td></tr>
<tr><td><code id="rankByDesirability_+3A_desirabilityindex">desirabilityIndex</code></td>
<td>

<p>A function accepting a vector of objective values and returning a desirability index in [0,1].
</p>
</td></tr>
<tr><td><code id="rankByDesirability_+3A_optimalonly">optimalOnly</code></td>
<td>

<p>If set to true, only the Pareto-optimal solutions are ranked. Otherwise, all tested solutions are ranked. Defaults to TRUE.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of objective values with an additional column for the desirability index. The rows of the matrix are sorted according to the index.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


# optimize the 'cost' parameter of an SVM on
# the 'iris' data set
res &lt;- tunePareto(classifier = tunePareto.svm(),
                  data = iris[, -ncol(iris)], 
                  labels = iris[, ncol(iris)],
                  cost=c(0.01,0.05,0.1,0.5,1,5,10,50,100),
                  objectiveFunctions=list(cvWeightedError(10, 10),
                                          cvSensitivity(10, 10, caseClass="setosa"),
                                          cvSpecificity(10, 10, caseClass="setosa")))
             
# create desirability functions 
# aggregate functions in desirability index (e.g. harrington desirability function)
# here, for the sake of simplicity a random number generator
di &lt;- function(x) {runif(1)}

# rank all tuning results according to their desirabilities
print(rankByDesirability(res,di,optimalOnly=FALSE))

</code></pre>

<hr>
<h2 id='recalculateParetoSet'>
Recalculate Pareto-optimal solutions
</h2><span id='topic+recalculateParetoSet'></span>

<h3>Description</h3>

<p>Recalculates the Pareto-optimal solutions in a <code>TuneParetoResult</code> according to the specified objectives only, and returns another <code>TuneParetoResult</code> object reduced to these objectives. This avoids time-consuming recalculations of objective values if only a subset of objectives should be considered for a previously evaluated set of parameter combinations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recalculateParetoSet(tuneParetoResult, 
                     objectives)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recalculateParetoSet_+3A_tuneparetoresult">tuneParetoResult</code></td>
<td>

<p>The <code>TuneParetoResult</code> object containing the parameter configurations to be examined
</p>
</td></tr>
<tr><td><code id="recalculateParetoSet_+3A_objectives">objectives</code></td>
<td>

<p>A vector of objective function indices. The Pareto set is recalculated according to these objectives, i.e. omitting other objectives. If this argument is not supplied, all objectives are used, which usually returns a copy of the input.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a reduced <code>TuneParetoResult</code> object. For more details on the object structure, refer to <code><a href="#topic+tunePareto">tunePareto</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tunePareto">tunePareto</a></code>, <code><a href="#topic+mergeTuneParetoResults">mergeTuneParetoResults</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# optimize the 'cost' parameter of an SVM on
# the 'iris' data set
res &lt;- tunePareto(classifier = tunePareto.svm(),
                  data = iris[, -ncol(iris)], 
                  labels = iris[, ncol(iris)],
                  cost=seq(0.01,0.1,0.01),
                  objectiveFunctions=list(cvWeightedError(10, 10),
                                          cvSensitivity(10, 10, caseClass="setosa"),
                                          cvSpecificity(10, 10, caseClass="setosa")))
print(res)

# select only specificity and sensitivity
print(recalculateParetoSet(res, 2:3))

</code></pre>

<hr>
<h2 id='trainTuneParetoClassifier'>
Train a TunePareto classifier
</h2><span id='topic+trainTuneParetoClassifier'></span>

<h3>Description</h3>

<p>Trains a classifier wrapped in a <code>TuneParetoClassifier</code> object. The trained classifier model can then be passed to <code><a href="#topic+predict.TuneParetoModel">predict.TuneParetoModel</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainTuneParetoClassifier(classifier, trainData, trainLabels, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trainTuneParetoClassifier_+3A_classifier">classifier</code></td>
<td>

<p>A <code>TuneParetoClassifier</code> object as returned by <code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code> or one of the predefined classification functions (see <code><a href="#topic+predefinedClassifiers">predefinedClassifiers</a></code>).
</p>
</td></tr>
<tr><td><code id="trainTuneParetoClassifier_+3A_traindata">trainData</code></td>
<td>

<p>The data set to be used for the classifier training. This is usually a matrix or data frame with the samples in the rows and the features in the columns.
</p>
</td></tr>
<tr><td><code id="trainTuneParetoClassifier_+3A_trainlabels">trainLabels</code></td>
<td>

<p>A vector of class labels for the samples in <code>trainData</code>.
</p>
</td></tr>
<tr><td><code id="trainTuneParetoClassifier_+3A_...">...</code></td>
<td>

<p>Further parameters to be passed to the classifier. These must be parameters specified in the <code>classifierParameterNames</code> parameter of <code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code> and usually correspond to the tuned parameters.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>TuneParetoModel</code> with the following entries
</p>
<table>
<tr><td><code>classifier</code></td>
<td>
<p>The classifier object supplied in the <code>classifier</code> parameter</p>
</td></tr>
<tr><td><code>classifierParams</code></td>
<td>
<p>The additional parameters supplied to the classifier in the <code>...</code> parameter</p>
</td></tr>
<tr><td><code>trainData</code></td>
<td>
<p>If <code>classifier</code> is an all-in-one classifier without a separate prediction method, this stores the input training data.</p>
</td></tr>
<tr><td><code>trainLabels</code></td>
<td>
<p>If <code>classifier</code> is an all-in-one classifier without a separate prediction method, this stores the input training labels.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>If <code>classifier</code> consists of separate training and prediction methods, this contains the trained classifier model.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>, <code><a href="#topic+predefinedClassifiers">predefinedClassifiers</a></code>, <code><a href="#topic+predict.TuneParetoModel">predict.TuneParetoModel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# train an SVM classifier
cl &lt;- tunePareto.svm()
tr &lt;- trainTuneParetoClassifier(cl, 
                                iris[,-ncol(iris)], 
                                iris[,ncol(iris)], 
                                cost=0.001)

# re-apply the classifier to predict the training data
print(iris[,ncol(iris)])
print(predict(tr, iris[,-ncol(iris)]))
</code></pre>

<hr>
<h2 id='tunePareto'>
Generic function for multi-objective parameter tuning of classifiers
</h2><span id='topic+tunePareto'></span>

<h3>Description</h3>

<p>This generic function tunes parameters of arbitrary classifiers in a multi-objective setting and returns the Pareto-optimal parameter combinations. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tunePareto(..., data, labels, 
           classifier, parameterCombinations,
           sampleType = c("full","uniform",
                          "latin","halton",
                          "niederreiter","sobol",
                          "evolution"), 
           numCombinations, 
           mu=10, lambda=20, numIterations=100,
           objectiveFunctions, objectiveBoundaries,
           keepSeed = TRUE, useSnowfall = FALSE, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tunePareto_+3A_data">data</code></td>
<td>

<p>The data set to be used for the parameter tuning. This is usually a matrix or data frame with the samples in the rows and the features in the columns.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_labels">labels</code></td>
<td>

<p>A vector of class labels for the samples in <code>data</code>.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_classifier">classifier</code></td>
<td>

<p>A <code>TuneParetoClassifier</code> wrapper object containing the classifier to tune. A number of state-of-the-art classifiers are included in <span class="pkg">TunePareto</span> (see <code><a href="#topic+predefinedClassifiers">predefinedClassifiers</a></code>). Custom classifiers can be employed using <code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>.</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_parametercombinations">parameterCombinations</code></td>
<td>
<p>If not all combinations of parameter ranges for the classifier are meaningful, you can set this parameter instead of specifying parameter values in the ... argument. It holds an explicit list of possible combinations, where each element of the list is a named sublist with one entry for each parameter.</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_sampletype">sampleType</code></td>
<td>

<p>Determines the way parameter configurations are sampled. 
</p>
<p>If <code>type="full"</code>, all possible combinations are tried. This is only possible if all supplied parameter ranges are discrete or if the combinations are supplied explicitly in <code>parameterCombinations</code>. 
</p>
<p>If <code>type="uniform"</code>, <code>numCombinations</code> combinations are drawn uniformly at random.
</p>
<p>If <code>type="latin"</code>, Latin Hypercube sampling is applied. This is particularly encouraged when tuning using continuous parameters.
</p>
<p>If <code>type="halton","niederreiter","sobol"</code>, <code>numCombinations</code> parameter combinations are drawn on the basis of the corresponding quasi-random sequences (initialized at a random step to ensure that different values are drawn in repeated runs). This is particularly encouraged when tuning using continuous parameters. <code>type="niederreiter"</code> and <code>type="sobol"</code> require the <span class="pkg">gsl</span> package to be installed.
</p>
<p>If <code>type="evolution"</code>, an evolutionary algorithm is applied. In details, this employs <code>mu+lambda</code> Evolution Strategies with uncorrelated mutations and non-dominated sorting for survivor selection. This is encouraged when the space of possible parameter configurations is very large. For smaller parameter spaces, the above sampling methods may be faster.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_numcombinations">numCombinations</code></td>
<td>

<p>If this parameter is set, at most <code>numCombinations</code> randomly chosen parameter configurations are tested. Otherwise, all possible combinations of the supplied parameter ranges are tested.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_mu">mu</code></td>
<td>

<p>The number of individuals used in the Evolution Strategies if <code>type="evolution"</code>.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_lambda">lambda</code></td>
<td>

<p>The number of offspring per generation in the Evolution Strategies if <code>type="evolution"</code>.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_numiterations">numIterations</code></td>
<td>

<p>The number of iterations/generations the evolutionary algorithm is run if <code>type="evolution"</code>.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_objectivefunctions">objectiveFunctions</code></td>
<td>

<p>A list of objective functions used to tune the parameters. There are a number of predefined objective functions (see <code><a href="#topic+predefinedObjectiveFunctions">predefinedObjectiveFunctions</a></code>). Custom objective functions can be created using <code><a href="#topic+createObjective">createObjective</a></code>.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_objectiveboundaries">objectiveBoundaries</code></td>
<td>

<p>If this parameter is set, it specifies boundaries of the objective functions for valid solutions. That is, each element of the supplied vector specifies the upper or lower limit of an objective (depending on whether the objective is maximized or minimized). Parameter combinations that do not meet all these restrictions are not included in the result set, even if they are Pareto-optimal. If only some of the objectives should have bounds, supply <code>NA</code> for the remaining objectives.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_keepseed">keepSeed</code></td>
<td>

<p>If this is true, the random seed is reset to the same value for each of the tested parameter configurations. This is an easy way to guarantee comparability in randomized objective functions. E.g., cross-validation runs of the classifiers will all start with the same seed, which results in the same partitions. 
</p>
<p><b>Attention: </b> If you set this parameter to <code>FALSE</code>, you must ensure that all configuration are treated equally in the objective functions: There may be randomness in processes such as classifier training, but there should be no random difference in the rating itself. In particular, the choice of subsets for subsampling experiments should always be the same for all configurations. For example, you can provide precalculated fold lists to the cross-validation objectives in the <code>foldList</code> parameter. If parameter configurations are rated under varying conditions, this may yield over-optimistic or over-pessimistic ratings for some configurations due to outliers.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_usesnowfall">useSnowfall</code></td>
<td>

<p>If this parameter is true, the routine loads the <span class="pkg">snowfall</span> package and processes the parameter configurations in parallel. Please note that the <span class="pkg">snowfall</span> cluster has to be initialized properly before running the tuning function and stopped after the run.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_verbose">verbose</code></td>
<td>

<p>If this parameter is true, status messages are printed. In particular, the algorithm prints the currently tested combination.
</p>
</td></tr>
<tr><td><code id="tunePareto_+3A_...">...</code></td>
<td>

<p>The parameters of the classifier and predictor functions that should be tuned. The names of the parameters must correspond to the parameters specified in <code>classifierParameterNames</code> and <code>predictorParameterNames</code> of <code><a href="#topic+tuneParetoClassifier">tuneParetoClassifier</a></code>.
Each supplied argument describes the possible values of a single parameter. These can be specified in two ways: Discrete parameter ranges are specified as lists of possible values. Continous parameter ranges are specified as intervals using <code><a href="#topic+as.interval">as.interval</a></code>. The algorithm then generates combinations of possible parameter values. Alternatively, the combinations can be defined explicitly using the <code>parameterCombinations</code> parameter.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function that allows for parameter tuning of a wide variety of classifiers. You can either specify the values or intervals of tuned parameters in the <code>...</code> argument, or supply selected combinations of parameter values using <code>parameterCombinations</code>. In the first case, combinations of parameter values specified in the <code>...</code> argument are generated. If <code>sampleType="uniform"</code>, <code>sampleType="latin"</code>, <code>sampleType="halton"</code>, <code>sampleType="niederreiter"</code> or <code>sampleType="sobol"</code>, a random subset of the possible combinations is drawn. If <code>sampleType="evolution"</code>, random parameter combinations are generated and optimized using Evolution Strategies.
</p>
<p>In the latter case, only the parameter combinations specified explicitly in <code>parameterCombinations</code> are tested. This is useful if certain parameter combinations are invalid. You can create parameter combinations by concatenating results of calls to <code><a href="#topic+allCombinations">allCombinations</a></code>. Only <code>sampleType="full"</code> is allowed in this mode.
</p>
<p>For each of the combinations, the specified objective functions are calculated. This usually involves training and testing a classifier. From the resulting objective values, the non-dominated parameter configurations are calculated and returned.
</p>
<p>The <code>...</code> argument is the first argument of <code>tunePareto</code> for technical reasons (to prevent partial matching of the supplied parameters with argument names of <code>tunePareto</code>. This requires all arguments to be named.
</p>


<h3>Value</h3>

<p>Returns a list of class <code>TuneParetoResult</code> with the following components:
</p>
<table>
<tr><td><code>bestCombinations</code></td>
<td>
<p>A list of Pareto-optimal parameter configurations. Each element of the list consists of a sub-list with named elements corresponding to the parameter values.</p>
</td></tr>
<tr><td><code>bestObjectiveValues</code></td>
<td>
<p>A matrix containing the objective function values of the Pareto-optimal configurations in <code>bestCombinations</code>. Each row corresponds to a parameter configuration, and each column corresponds to an objective function.</p>
</td></tr>
<tr><td><code>testedCombinations</code></td>
<td>
<p>A list of all tested parameter configurations with the same structure as <code>bestCombinations</code>.</p>
</td></tr>
<tr><td><code>testedObjectiveValues</code></td>
<td>
<p>A matrix containing the objective function values of all tested configurations with the same structure as <code>bestObjectiveValues</code>.</p>
</td></tr>
<tr><td><code>dominationMatrix</code></td>
<td>
<p>A Boolean matrix specifying which parameter configurations dominate each other. If a configuration <code>i</code> dominates a configuration <code>j</code>, the entry in the <code>i</code>th row and the <code>j</code>th column is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code>minimizeObjectives</code></td>
<td>
<p>A Boolean vector specifying which of the objectives are minimization objectives. This is derived from the objective functions supplied to <code>tunePareto</code>.</p>
</td></tr>
<tr><td><code>additionalData</code></td>
<td>
<p>A list containing additional data that may have been returned by the objective functions. The list has one element for each tested parameter configuration, each comprising one sub-element for each objective function that returned additional data. The structure of these sub-elements depends on the corresponding objective function. For example, the predefined objective functions (see <code><a href="#topic+predefinedObjectiveFunctions">predefinedObjectiveFunctions</a></code>) save the trained models here if <code>saveModel</code> is true.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+predefinedClassifiers">predefinedClassifiers</a></code>, <code><a href="#topic+predefinedObjectiveFunctions">predefinedObjectiveFunctions</a></code>, <code><a href="#topic+createObjective">createObjective</a></code>, <code><a href="#topic+allCombinations">allCombinations</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# tune 'k' of a k-NN classifier 
# on two classes of the 'iris' data set --
# see ?knn
print(tunePareto(data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 classifier = tunePareto.knn(), 
                 k = c(1,3,5,7,9),
                 objectiveFunctions = list(cvError(10, 10),
                                           reclassError())))
                 
# example using predefined parameter configurations,
# as certain combinations of k and l are invalid:
comb &lt;- c(allCombinations(list(k=1,l=0)),
          allCombinations(list(k=3,l=0:2)),
          allCombinations(list(k=5,l=0:4)),
          allCombinations(list(k=7,l=0:6)))

print(tunePareto(data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 classifier = tunePareto.knn(), 
                 parameterCombinations = comb,
                 objectiveFunctions = list(cvError(10, 10),
                                           reclassError())))
                                           

# tune 'cost' and 'kernel' of an SVM on
# the 'iris' data set using Latin Hypercube sampling --
# see ?svm and ?predict.svm
print(tunePareto(data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 classifier = tunePareto.svm(), 
                 cost = as.interval(0.001,10), 
                 kernel = c("linear", "polynomial",
                          "radial", "sigmoid"),
                 sampleType="latin",
                 numCombinations=20,                          
                 objectiveFunctions = list(cvError(10, 10),
                                           cvSensitivity(10, 10, caseClass="setosa"))))

# tune the same parameters using Evolution Strategies
print(tunePareto(data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 classifier = tunePareto.svm(), 
                 cost = as.interval(0.001,10), 
                 kernel = c("linear", "polynomial",
                          "radial", "sigmoid"),
                 sampleType="evolution",
                 numCombinations=20,
                 numIterations=20,                      
                 objectiveFunctions = list(cvError(10, 10),
                                           cvSensitivity(10, 10, caseClass="setosa"),
                                           cvSpecificity(10, 10, caseClass="setosa"))))

</code></pre>

<hr>
<h2 id='tuneParetoClassifier'>
Create a classifier object
</h2><span id='topic+tuneParetoClassifier'></span>

<h3>Description</h3>

<p>Creates a wrapper object mapping all information necessary to call a classifier which can be passed to <code><a href="#topic+tunePareto">tunePareto</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneParetoClassifier(name,
                     classifier, 
                     classifierParamNames = NULL, 
                     predefinedClassifierParams = NULL, 
                     predictor = NULL, 
                     predictorParamNames = NULL, 
                     predefinedPredictorParams = NULL, 
                     useFormula = FALSE, 
                     formulaName = "formula", 
                     trainDataName = "x", 
                     trainLabelName = "y", 
                     testDataName = "newdata", 
                     modelName = "object", 
                     requiredPackages = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuneParetoClassifier_+3A_name">name</code></td>
<td>

<p>A human-readable name of the classifier
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_classifier">classifier</code></td>
<td>

<p>The classification function to use. If <code>predictor</code> is <code>NULL</code>, this function is an all-in-one classification method that receives both training data and test data and returns the predicted labels for the test data. If <code>predictor</code> is not <code>NULL</code>, this is the training function of the classifier that builds a model from the training data. This model is then passed to <code>predictor</code> along with the test data to obtain the predicted labels for the test data.
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_classifierparamnames">classifierParamNames</code></td>
<td>

<p>A vector of names of possible arguments for <code>classifier</code>.
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_predefinedclassifierparams">predefinedClassifierParams</code></td>
<td>

<p>A named list of default values for the classifier parameters.
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_predictor">predictor</code></td>
<td>

<p>If the classification method consists of separate training and prediction functions, this points to the prediction function that receives a model and the test data as inputs and returns the predicted class labels.
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_predictorparamnames">predictorParamNames</code></td>
<td>

<p>If <code>predictor != NULL</code>, a vector of names of possible arguments for <code>predictor</code>.
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_predefinedpredictorparams">predefinedPredictorParams</code></td>
<td>

<p>If <code>predictor != NULL</code>, a named list of default values for the parameters of <code>predictor</code>.
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_useformula">useFormula</code></td>
<td>

<p>Set this to true if the classifier expects a formula to describe the relation between features and class labels. The formula itself is built automatically.
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_formulaname">formulaName</code></td>
<td>

<p>If <code>useFormula</code> is true, this is the name of the parameter of the classifier's training function that holds the formula.
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_traindataname">trainDataName</code></td>
<td>

<p>The name of the paramater of the classifier's training function that holds the training data. 
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_trainlabelname">trainLabelName</code></td>
<td>

<p>If <code>useFormula=FALSE</code>, the name of the paramater of the classifier's training function that holds the training labels. Otherwise, the training labels are added to the training data and supplied in parameter <code>trainDataName</code>.
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_testdataname">testDataName</code></td>
<td>

<p>If <code>predictor=NULL</code>, this is the name of the parameter of <code>classifier</code> that receives the test data. Otherwise, it is the parameter of <code>predictor</code> that holds the test data.
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_modelname">modelName</code></td>
<td>

<p>If <code>predictor</code> is not NULL, this is the name of the parameter of <code>predictor</code> that receives the training model (i.e., the return value of <code>classifier</code>).
</p>
</td></tr>
<tr><td><code id="tuneParetoClassifier_+3A_requiredpackages">requiredPackages</code></td>
<td>

<p>A vector containing the names of packages that are required to run the classifier. These packages are loaded automatically when running the classifier using <code><a href="#topic+tunePareto">tunePareto</a></code>. They are also loaded in the <span class="pkg">snowfall</span> cluster if necessary.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>TunePareto classifier objects are wrappers containing all information necessary to run the classifier, including the training and prediction function, the required packages, and the names of certain arguments. <span class="pkg">TunePareto</span> provides a set of predefined objects for state-of-the-art classifiers (see <code><a href="#topic+predefinedClassifiers">predefinedClassifiers</a></code>).
</p>
<p>The main <code><a href="#topic+tunePareto">tunePareto</a></code> routine evaluates <code>TuneParetoClassifier</code> objects to call the training and prediction methods. Furthermore, direct calls to the classifiers are possible using  <code><a href="#topic+trainTuneParetoClassifier">trainTuneParetoClassifier</a></code> and <code><a href="#topic+predict.TuneParetoModel">predict.TuneParetoModel</a></code>.
</p>


<h3>Value</h3>

<p>An object of class <code>TuneParetoClassifier</code> with components corresponding to the above parameters.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trainTuneParetoClassifier">trainTuneParetoClassifier</a></code>, <code><a href="#topic+predict.TuneParetoModel">predict.TuneParetoModel</a></code>, <code><a href="#topic+tunePareto">tunePareto</a></code>, <code><a href="#topic+predefinedClassifiers">predefinedClassifiers</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # equivalent to tunePareto.svm()
  cl &lt;- tuneParetoClassifier(name = "svm",
                             classifier = svm, 
                             predictor = predict, 
                             classifierParamNames = c("kernel", "degree", "gamma",
                                                      "coef0", "cost", "nu",
                                                      "class.weights", "cachesize", 
                                                      "tolerance", "epsilon",
                                                      "subset", "na.action"),
                              useFormula = FALSE,
                              trainDataName = "x",
                              trainLabelName = "y",
                              testDataName = "newdata",
                              modelName = "object",
                              requiredPackages="e1071")
  
  # call TunePareto with the classifier
  print(tunePareto(classifier = cl,
                   data = iris[, -ncol(iris)], 
                   labels = iris[, ncol(iris)],
                   cost = c(0.001,0.01,0.1,1,10), 
                   objectiveFunctions=
                     list(cvError(10, 10),
                          cvSpecificity(10, 10, 
                            caseClass="setosa"))))                           
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
