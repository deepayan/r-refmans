<!DOCTYPE html><html lang="en"><head><title>Help for package validateIt</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {validateIt}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#allR4WSItasktest'><p>Example R4WSI Tasks with Regular and Gold-Standard Tasks</p></a></li>
<li><a href='#checkAgree'><p>Check Agreement Rate between Identical Trails</p></a></li>
<li><a href='#combMass'><p>Combine the mass of words with the same root</p></a></li>
<li><a href='#evalResults'><p>Evaluate results</p></a></li>
<li><a href='#getResults'><p>Get results from Mturk</p></a></li>
<li><a href='#goldR4WSItest'><p>Example Gold-Standard R4WSI0 Tasks</p></a></li>
<li><a href='#heldouttest'><p>An Example Heldout Test Set</p></a></li>
<li><a href='#keypostedtest'><p>Example Answer Keys</p></a></li>
<li><a href='#masstest'><p>An Example of the Combined Mass for Words with the Same Roots</p></a></li>
<li><a href='#mixGold'><p>Mix the gold-standard tasks with the tasks need to be validated</p></a></li>
<li><a href='#modtest'><p>An Example Topic Model</p></a></li>
<li><a href='#pickLabel'><p>Pick the optimal label from candidate labels</p></a></li>
<li><a href='#plotResults'><p>Plot results</p></a></li>
<li><a href='#R4WSItasktest'><p>Example R4WSI0 Tasks</p></a></li>
<li><a href='#record'><p>Reform tasks to facilitate sending to Mturk</p></a></li>
<li><a href='#recordtest'><p>Example Local Record of the R4WSI Tasks</p></a></li>
<li><a href='#resultstest'><p>Example Results Retrieved from Mturk</p></a></li>
<li><a href='#sendTasks'><p>Send prepared task to Mturk and record the API-returned HIT ids.</p></a></li>
<li><a href='#stmPreptest'><p>An Example Object of Prepared Documents</p></a></li>
<li><a href='#tidyeval'><p>Tidy eval helpers</p></a></li>
<li><a href='#Topic_Model_Validation_Overview'><p>Topic_Model_Validation Repository Overview</p></a></li>
<li><a href='#validateLabel'><p>Create validation tasks for labels assigned to the topics in the topic model of choice.</p></a></li>
<li><a href='#validateTopic'><p>Create validation tasks for topic model selection</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Validating Topic Coherence and Topic Labels</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Description:</td>
<td>By creating crowd-sourcing tasks that can be easily posted and results retrieved using Amazon's Mechanical Turk (MTurk) API, researchers can use this solution to validate the quality of topics obtained from unsupervised or semi-supervised learning methods, and the relevance of topic labels assigned. This helps ensure that the topic modeling results are accurate and useful for research purposes. See Ying and others (2022) &lt;<a href="https://doi.org/10.1101%2F2023.05.02.538599">doi:10.1101/2023.05.02.538599</a>&gt;. For more information, please visit <a href="https://github.com/Triads-Developer/Topic_Model_Validation">https://github.com/Triads-Developer/Topic_Model_Validation</a>.  </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>pyMTurkR, rlang (&ge; 0.4.11), tm (&ge; 0.7-11), here, SnowballC</td>
</tr>
<tr>
<td>Suggests:</td>
<td>roxygen2, testthat</td>
</tr>
<tr>
<td>Author:</td>
<td>Luwei Ying <a href="https://orcid.org/0000-0001-7307-4834"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Jacob Montgomery [aut],
  Brandon Stewart [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Luwei Ying &lt;triads.developers@wustl.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-15 12:34:05 UTC; jessiewalker</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-16 08:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='allR4WSItasktest'>Example R4WSI Tasks with Regular and Gold-Standard Tasks</h2><span id='topic+allR4WSItasktest'></span>

<h3>Description</h3>

<p>Data frame of 20 example R4WSI0 Tasks, with 5 of them being gold-standard and 15 of them not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(allR4WSItasktest)
</code></pre>


<h3>Format</h3>

<p>A data frame of 20 rows and 6 columns.
</p>

<dl>
<dt><code>topic</code></dt><dd><p>Index of topics</p>
</dd>
<dt><code>id</code></dt><dd><p>Index of topics</p>
</dd>
<dt><code>doc</code></dt><dd><p>Example documents associated with each topic</p>
</dd>
<dt><code>opt1</code></dt><dd><p>Words set option 1</p>
</dd>
<dt><code>opt2</code></dt><dd><p>Words set option 2</p>
</dd>
<dt><code>opt3</code></dt><dd><p>Words set option 3</p>
</dd>
<dt><code>optcrt</code></dt><dd><p>Words set option 4, also the correct choice</p>
</dd>
</dl>


<hr>
<h2 id='checkAgree'>Check Agreement Rate between Identical Trails</h2><span id='topic+checkAgree'></span>

<h3>Description</h3>

<p>Check Agreement Rate between Identical Trails
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkAgree(results1, results2, key, type = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="checkAgree_+3A_results1">results1</code></td>
<td>
<p>first batch of results; outputs from getResults()</p>
</td></tr>
<tr><td><code id="checkAgree_+3A_results2">results2</code></td>
<td>
<p>first batch of results; outputs from getResults()</p>
</td></tr>
<tr><td><code id="checkAgree_+3A_key">key</code></td>
<td>
<p>the local task record; outputs from recordTasks()</p>
</td></tr>
<tr><td><code id="checkAgree_+3A_type">type</code></td>
<td>
<p>Task structures to be specified. Must be one of &quot;WI&quot; (word intrusion),
&quot;T8WSI&quot; (top 8 word set intrusion), &quot;R4WSI&quot; (random 4 word set intrusion),
&quot;LI&quot; (Label Intrusion), and &quot;OL&quot; (Optimal Label)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Evaluate workers' performance by agreement rate between identical trails
(Notice that this means the two input, results1 and results2, must be identical.);
Return 1) the exact agreement rate when both workers agree on the exact same choice, and
2) the binary agreement rate when both workers get the task either right or wrong simultaneously
</p>


<h3>Value</h3>

<p>A numeric value to be returned with output.
</p>

<hr>
<h2 id='combMass'>Combine the mass of words with the same root</h2><span id='topic+combMass'></span>

<h3>Description</h3>

<p>Combine the mass of words with the same root
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combMass(mod = NULL, vocab = NULL, beta = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="combMass_+3A_mod">mod</code></td>
<td>
<p>Fitted structural topic models.</p>
</td></tr>
<tr><td><code id="combMass_+3A_vocab">vocab</code></td>
<td>
<p>A character vector specifying the words in the corpus. Usually, it
can be found in topic model output.</p>
</td></tr>
<tr><td><code id="combMass_+3A_beta">beta</code></td>
<td>
<p>A matrix of word probabilities for each topic. Each row represents a
topic and each column represents a word. Note this should not be in the logged form.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use as a preparing step for validating unstemmed topic models.
</p>


<h3>Value</h3>

<p>A list with two elements: 
</p>
<table role = "presentation">
<tr><td><code>newvocab</code></td>
<td>
<p>A matrix of new vocabulary. Each row represents a topic and each column represents a unique stemmed word.</p>
</td></tr>
<tr><td><code>newbeta</code></td>
<td>
<p>A matrix of new beta. Each row represents a topic and each column represents the sum of the probabilities of the words with the same root.</p>
</td></tr>
</table>

<hr>
<h2 id='evalResults'>Evaluate results</h2><span id='topic+evalResults'></span>

<h3>Description</h3>

<p>Evaluate results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalResults(results, key, type = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evalResults_+3A_results">results</code></td>
<td>
<p>results of human choice; outputs from getResults()</p>
</td></tr>
<tr><td><code id="evalResults_+3A_key">key</code></td>
<td>
<p>the local task record; outputs form recordTasks()</p>
</td></tr>
<tr><td><code id="evalResults_+3A_type">type</code></td>
<td>
<p>Task structures to be specified. Must be one of &quot;WI&quot; (word intrusion),
&quot;T8WSI&quot; (top 8 word set intrusion), &quot;R4WSI&quot; (random 4 word set intrusion),
&quot;LI&quot; (Label Intrusion), and &quot;OL&quot; (Optimal Label)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Evaluate worker performance by gold-standard HITs;
Return the accuracy rate (proportion correct) for a specified batch
</p>


<h3>Value</h3>

<p>A list containing the gold-standard HIT correct rate, gold-standard HIT correct rate by workers, and non-gold-standard HIT correct rate
</p>

<hr>
<h2 id='getResults'>Get results from Mturk</h2><span id='topic+getResults'></span>

<h3>Description</h3>

<p>Get results from Mturk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getResults(
  batch_id = "unspecified",
  hit_ids,
  retry = TRUE,
  retry_in_seconds = 60,
  AWS_id = Sys.getenv("AWS_ACCESS_KEY_ID"),
  AWS_secret = Sys.getenv("AWS_SECRET_ACCESS_KEY"),
  sandbox = getOption("pyMTurkR.sandbox", TRUE)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getResults_+3A_batch_id">batch_id</code></td>
<td>
<p>any number or string to annotate the batch</p>
</td></tr>
<tr><td><code id="getResults_+3A_hit_ids">hit_ids</code></td>
<td>
<p>hit ids returned from the MTurk API, i.e., output of sendTasks()</p>
</td></tr>
<tr><td><code id="getResults_+3A_retry">retry</code></td>
<td>
<p>if TRUE, retry retriving results from Mturk API five times; default to TRUE</p>
</td></tr>
<tr><td><code id="getResults_+3A_retry_in_seconds">retry_in_seconds</code></td>
<td>
<p>default to 60 seconds</p>
</td></tr>
<tr><td><code id="getResults_+3A_aws_id">AWS_id</code></td>
<td>
<p>AWS_ACCESS_KEY_ID</p>
</td></tr>
<tr><td><code id="getResults_+3A_aws_secret">AWS_secret</code></td>
<td>
<p>AWS_SECRET_ACCESS_KEY</p>
</td></tr>
<tr><td><code id="getResults_+3A_sandbox">sandbox</code></td>
<td>
<p>sanbox setting</p>
</td></tr>
</table>


<h3>Details</h3>

<p>this function works for complete or incomplete batches
</p>


<h3>Value</h3>

<p>a data frame with columns:
</p>
<table role = "presentation">
<tr><td><code>batch_id</code></td>
<td>
<p>an annotation for the batch</p>
</td></tr>
<tr><td><code>local_task_id</code></td>
<td>
<p>an identifier for the task in the batch</p>
</td></tr>
<tr><td><code>mturk_hit_id</code></td>
<td>
<p>the ID of the HIT in MTurk</p>
</td></tr>
<tr><td><code>assignment_id</code></td>
<td>
<p>the ID of the assignment in MTurk</p>
</td></tr>
<tr><td><code>worker_id</code></td>
<td>
<p>the ID of the worker who completed the assignment</p>
</td></tr>
<tr><td><code>result</code></td>
<td>
<p>the worker's response to the task</p>
</td></tr>
<tr><td><code>completed_at</code></td>
<td>
<p>the time when the worker submitted the assignment</p>
</td></tr>
</table>

<hr>
<h2 id='goldR4WSItest'>Example Gold-Standard R4WSI0 Tasks</h2><span id='topic+goldR4WSItest'></span>

<h3>Description</h3>

<p>Data frame of 5 example gold-standard R4WSI0 Tasks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(goldR4WSItest)
</code></pre>


<h3>Format</h3>

<p>A data frame of 5 rows and 6 columns.
</p>

<dl>
<dt><code>topic</code></dt><dd><p>Index of topics</p>
</dd>
<dt><code>doc</code></dt><dd><p>Example documents associated with each topic</p>
</dd>
<dt><code>opt1</code></dt><dd><p>Words set option 1</p>
</dd>
<dt><code>opt2</code></dt><dd><p>Words set option 2</p>
</dd>
<dt><code>opt3</code></dt><dd><p>Words set option 3</p>
</dd>
<dt><code>optcrt</code></dt><dd><p>Words set option 4, also the correct choice</p>
</dd>
</dl>


<hr>
<h2 id='heldouttest'>An Example Heldout Test Set</h2><span id='topic+heldouttest'></span>

<h3>Description</h3>

<p>An output from the <code>make.heldout</code> function of the <code>stm</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(heldouttest)
</code></pre>


<h3>Format</h3>

<p>A list of the heldout documents, vocab, and missing.
</p>


<h3>Source</h3>

<p>See <a href="https://CRAN.R-project.org/package=stm">https://CRAN.R-project.org/package=stm</a> for more details.
</p>


<h3>References</h3>

<p>Roberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. &quot;Stm: An R package for structural
topic models.&quot; Journal of Statistical Software 91 (2019): 1-40.
</p>

<hr>
<h2 id='keypostedtest'>Example Answer Keys</h2><span id='topic+keypostedtest'></span>

<h3>Description</h3>

<p>Example Answer Keys
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(keypostedtest)
</code></pre>


<h3>Format</h3>

<p>A list of two data frames. Similar to <code>recordtest</code>.
</p>

<dl>
<dt><code>data.frame1</code></dt><dd><p>A data frame of tasks with the <code>optcrt</code> indicating the 
machine predicted choice.</p>
</dd>
<dt><code>data.frame2</code></dt><dd><p>A data frame of tasks with randomized choices. 
Exactly the same with what would be sent online.</p>
</dd>
</dl>


<hr>
<h2 id='masstest'>An Example of the Combined Mass for Words with the Same Roots</h2><span id='topic+masstest'></span>

<h3>Description</h3>

<p>A list of two with the words (the most frequent form in each topic) and the corresponding word 
probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(masstest)
</code></pre>


<h3>Format</h3>

<p>A list of two.
</p>


<h3>Details</h3>


<dl>
<dt><code>vocab</code></dt><dd><p>A matrix of words for each topic. Each row represents a
topic and each column represents the words. Words with the same roots are
only represented by the most common form in that topic.</p>
</dd>
<dt><code>beta</code></dt><dd><p>A matrix of combined word probabilities for each topic. 
Each row represents a topic and each column represents a combined word.</p>
</dd>
</dl>


<hr>
<h2 id='mixGold'>Mix the gold-standard tasks with the tasks need to be validated</h2><span id='topic+mixGold'></span>

<h3>Description</h3>

<p>Mix the gold-standard tasks with the tasks need to be validated
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixGold(tasks, golds)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mixGold_+3A_tasks">tasks</code></td>
<td>
<p>All tasks need to be validated</p>
</td></tr>
<tr><td><code id="mixGold_+3A_golds">golds</code></td>
<td>
<p>Gold standard tasks with the same structure</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with the same structure as the input, where gold-standard tasks are randomly inserted
</p>

<hr>
<h2 id='modtest'>An Example Topic Model</h2><span id='topic+modtest'></span>

<h3>Description</h3>

<p>A structural topic model (STM) object generated from the <code>stm</code> package using a random 
sample of US senators' Facebook posts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(modtest)
</code></pre>


<h3>Format</h3>

<p>A STM object.
</p>


<h3>Source</h3>

<p>See <a href="https://CRAN.R-project.org/package=stm">https://CRAN.R-project.org/package=stm</a> for more details.
</p>


<h3>References</h3>

<p>Roberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. &quot;Stm: An R package for structural
topic models.&quot; Journal of Statistical Software 91 (2019): 1-40.
</p>

<hr>
<h2 id='pickLabel'>Pick the optimal label from candidate labels</h2><span id='topic+pickLabel'></span>

<h3>Description</h3>

<p>Pick the optimal label from candidate labels
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pickLabel(
  n,
  text.predict = NULL,
  text.name = "text",
  top1.name = "top1",
  labels.index = NULL,
  candidate.labels = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pickLabel_+3A_n">n</code></td>
<td>
<p>The number of desired tasks</p>
</td></tr>
<tr><td><code id="pickLabel_+3A_text.predict">text.predict</code></td>
<td>
<p>A data frame or matrix containing both the text and the indicator(s)
of the model predicted topic(s).</p>
</td></tr>
<tr><td><code id="pickLabel_+3A_text.name">text.name</code></td>
<td>
<p>variable name in 'text.predict' that indicates the text</p>
</td></tr>
<tr><td><code id="pickLabel_+3A_top1.name">top1.name</code></td>
<td>
<p>variable name in 'text.predict' that indicates the top1 model predicted topic</p>
</td></tr>
<tr><td><code id="pickLabel_+3A_labels.index">labels.index</code></td>
<td>
<p>The topic index in correspondence with the labels, e.g., c(10, 12, 15).</p>
</td></tr>
<tr><td><code id="pickLabel_+3A_candidate.labels">candidate.labels</code></td>
<td>
<p>A list of vectors containing the user-defined labels assigned to the topics,
Must be in the same length and order with 'labels.index'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users need to specify four plausible labels for each topic
</p>


<h3>Value</h3>

<p>A matrix with n rows and 6 columns (topic, doc, opt1, opt2, opt3, optcrt) where optcrt is the correct label that was picked.
</p>

<hr>
<h2 id='plotResults'>Plot results</h2><span id='topic+plotResults'></span>

<h3>Description</h3>

<p>Plot results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotResults(path, x, n, taskname, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotResults_+3A_path">path</code></td>
<td>
<p>path to store the plot</p>
</td></tr>
<tr><td><code id="plotResults_+3A_x">x</code></td>
<td>
<p>a vector of counts of successes; could be obtained from getResults()</p>
</td></tr>
<tr><td><code id="plotResults_+3A_n">n</code></td>
<td>
<p>a vector of counts of trials</p>
</td></tr>
<tr><td><code id="plotResults_+3A_taskname">taskname</code></td>
<td>
<p>the name of the task for labeling, e.g., Word Intrusion, Optimal Label.</p>
</td></tr>
<tr><td><code id="plotResults_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to plot function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Visualize the accuracy rate (proportion correct) for a specified batch
</p>


<h3>Value</h3>

<p>Nothing is returned; a plot is created and saved as a pdf file.
</p>

<hr>
<h2 id='R4WSItasktest'>Example R4WSI0 Tasks</h2><span id='topic+R4WSItasktest'></span>

<h3>Description</h3>

<p>Data of 15 example R4WSI0 Tasks structured as a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(R4WSItasktest)
</code></pre>


<h3>Format</h3>

<p>A matrix with 15 rows and 6 columns.
</p>

<dl>
<dt><code>topic</code></dt><dd><p>Index of topics</p>
</dd>
<dt><code>doc</code></dt><dd><p>Example documents associated with each topic</p>
</dd>
<dt><code>opt1</code></dt><dd><p>Words set option 1</p>
</dd>
<dt><code>opt2</code></dt><dd><p>Words set option 2</p>
</dd>
<dt><code>opt3</code></dt><dd><p>Words set option 3</p>
</dd>
<dt><code>optcrt</code></dt><dd><p>Words set option 4, also the correct choice</p>
</dd>
</dl>



<h3>Details</h3>

<p>Please note that the difference between the R4WSI0 examples used here and the R4WSI tasks
is that the R4WSI tasks do not present any documents.
</p>

<hr>
<h2 id='record'>Reform tasks to facilitate sending to Mturk</h2><span id='topic+record'></span>

<h3>Description</h3>

<p>Reform tasks to facilitate sending to Mturk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>record(type, tasks, path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="record_+3A_type">type</code></td>
<td>
<p>(character) one of WI, T8WSI, R4WSI</p>
</td></tr>
<tr><td><code id="record_+3A_tasks">tasks</code></td>
<td>
<p>(data.frame) outputs from validateTopic(), validateLabel(), or mixGold() if users mix in gold-standard HITs</p>
</td></tr>
<tr><td><code id="record_+3A_path">path</code></td>
<td>
<p>(character) path to record the tasks (with meta-information)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Randomize the order of options and record the tasks in a specified local directory
</p>


<h3>Value</h3>

<p>A list of two data frames, containing the original tasks and the randomized options respectively.
</p>

<hr>
<h2 id='recordtest'>Example Local Record of the R4WSI Tasks</h2><span id='topic+recordtest'></span>

<h3>Description</h3>

<p>Local record generated by the <code>recordTasks</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(recordtest)
</code></pre>


<h3>Format</h3>

<p>A list of two data frames.
</p>

<dl>
<dt><code>data.frame1</code></dt><dd><p>A data frame of tasks with the <code>optcrt</code> indicating the 
machine preficted choice.</p>
</dd>
<dt><code>data.frame2</code></dt><dd><p>A data frame of tasks with randomized choices. 
Exactly the same with what would be sent online.</p>
</dd>
</dl>



<h3>Details</h3>

<p>To be compared with the answers from the online workers to evaluate the topic model performance.
</p>

<hr>
<h2 id='resultstest'>Example Results Retrieved from Mturk</h2><span id='topic+resultstest'></span>

<h3>Description</h3>

<p>Example Results Retrieved from Mturk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(resultstest)
</code></pre>


<h3>Format</h3>

<p>A data frame of ten example tasks retrieved from the Mturk with or without 
online workers' answers.
</p>

<dl>
<dt><code>assignment_id</code></dt><dd><p>Assignment id. Mturk assigned. If 0, then the task hasn't been completed.</p>
</dd>
<dt><code>batch_id</code></dt><dd><p>User specified batch id.</p>
</dd>
<dt><code>completed_at</code></dt><dd><p>Timestamp when the task was completed. If 0, then the task hasn't been completed.</p>
</dd>
<dt><code>local_task_id</code></dt><dd><p>Local task id.</p>
</dd>
<dt><code>mturk_hit_id</code></dt><dd><p>Mturk HIT id. Mturk assigned.</p>
</dd>
<dt><code>result</code></dt><dd><p>Choice made by the worker. 1-4. If 0, then the task hasn't been completed.</p>
</dd>
<dt><code>worker_id</code></dt><dd><p>Mturk worker id. If 0, then the task hasn't been completed.</p>
</dd>
</dl>


<hr>
<h2 id='sendTasks'>Send prepared task to Mturk and record the API-returned HIT ids.</h2><span id='topic+sendTasks'></span>

<h3>Description</h3>

<p>Send prepared task to Mturk and record the API-returned HIT ids.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sendTasks(
  hit_type = NULL,
  hit_layout = NULL,
  type = NULL,
  tasksrecord = NULL,
  tasksids = NULL,
  HITidspath = NULL,
  n_assignments = "1",
  expire_in_seconds = as.character(60 * 60 * 8),
  batch_annotation = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sendTasks_+3A_hit_type">hit_type</code></td>
<td>
<p>find from the Mturk requester's dashboard</p>
</td></tr>
<tr><td><code id="sendTasks_+3A_hit_layout">hit_layout</code></td>
<td>
<p>find from the Mturk requester's dashboard</p>
</td></tr>
<tr><td><code id="sendTasks_+3A_type">type</code></td>
<td>
<p>one of WI, T8WSI, R4WSI</p>
</td></tr>
<tr><td><code id="sendTasks_+3A_tasksrecord">tasksrecord</code></td>
<td>
<p>output of recordTasks()</p>
</td></tr>
<tr><td><code id="sendTasks_+3A_tasksids">tasksids</code></td>
<td>
<p>ids of tasks to send in numeric form. If left unspecified, the whole batch will be posted</p>
</td></tr>
<tr><td><code id="sendTasks_+3A_hitidspath">HITidspath</code></td>
<td>
<p>path to record the returned HITids</p>
</td></tr>
<tr><td><code id="sendTasks_+3A_n_assignments">n_assignments</code></td>
<td>
<p>number of of assignments per task. For the validation tasks, people almost always want 1</p>
</td></tr>
<tr><td><code id="sendTasks_+3A_expire_in_seconds">expire_in_seconds</code></td>
<td>
<p>default 8 hours</p>
</td></tr>
<tr><td><code id="sendTasks_+3A_batch_annotation">batch_annotation</code></td>
<td>
<p>add if needed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Pairs the local ids with Mturk ids and save them to specified paths
</p>


<h3>Value</h3>

<p>A list containing two elements:
</p>

<ul>
<li><p>current_HIT_ids:A vector of the HIT IDs returned by the API.</p>
</li></ul>


<ul>
<li><p>map_ids:A data frame that maps the tasksids to their corresponding HIT ids.
</p>
</li></ul>


<hr>
<h2 id='stmPreptest'>An Example Object of Prepared Documents</h2><span id='topic+stmPreptest'></span>

<h3>Description</h3>

<p>An output from the <code>prepDocuments</code> function of the <code>stm</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(stmPreptest)
</code></pre>


<h3>Format</h3>

<p>A list containing a documents and vocab object.
</p>


<h3>Source</h3>

<p>See <a href="https://CRAN.R-project.org/package=stm">https://CRAN.R-project.org/package=stm</a> for more details.
</p>


<h3>References</h3>

<p>Roberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. &quot;Stm: An R package for structural
topic models.&quot; Journal of Statistical Software 91 (2019): 1-40.
</p>

<hr>
<h2 id='tidyeval'>Tidy eval helpers</h2><span id='topic+tidyeval'></span><span id='topic+enquo'></span><span id='topic+enquos'></span><span id='topic+.data'></span><span id='topic++3A+3D'></span><span id='topic+as_name'></span><span id='topic+as_label'></span>

<h3>Description</h3>

<p>This page lists the tidy eval tools reexported in this package from
rlang. To learn about using tidy eval in scripts and packages at a
high level, see the <a href="https://dplyr.tidyverse.org/articles/programming.html">dplyr programming vignette</a>
and the <a href="https://ggplot2.tidyverse.org/articles/ggplot2-in-packages.html">ggplot2 in packages vignette</a>.
The <a href="https://adv-r.hadley.nz/metaprogramming.html">Metaprogramming section</a> of <a href="https://adv-r.hadley.nz">Advanced R</a> may also be useful for a deeper dive.
</p>

<ul>
<li><p> The tidy eval operators <code style="white-space: pre;">&#8288;{{&#8288;</code>, <code style="white-space: pre;">&#8288;!!&#8288;</code>, and <code style="white-space: pre;">&#8288;!!!&#8288;</code> are syntactic
constructs which are specially interpreted by tidy eval functions.
You will mostly need <code style="white-space: pre;">&#8288;{{&#8288;</code>, as <code style="white-space: pre;">&#8288;!!&#8288;</code> and <code style="white-space: pre;">&#8288;!!!&#8288;</code> are more advanced
operators which you should not have to use in simple cases.
</p>
<p>The curly-curly operator <code style="white-space: pre;">&#8288;{{&#8288;</code> allows you to tunnel data-variables
passed from function arguments inside other tidy eval functions.
<code style="white-space: pre;">&#8288;{{&#8288;</code> is designed for individual arguments. To pass multiple
arguments contained in dots, use <code>...</code> in the normal way.
</p>
<div class="sourceCode"><pre>my_function &lt;- function(data, var, ...) {
  data %&gt;%
    group_by(...) %&gt;%
    summarise(mean = mean({{ var }}))
}
</pre></div>
</li>
<li> <p><code><a href="#topic+enquo">enquo()</a></code> and <code><a href="#topic+enquos">enquos()</a></code> delay the execution of one or several
function arguments. The former returns a single expression, the
latter returns a list of expressions. Once defused, expressions
will no longer evaluate on their own. They must be injected back
into an evaluation context with <code style="white-space: pre;">&#8288;!!&#8288;</code> (for a single expression) and
<code style="white-space: pre;">&#8288;!!!&#8288;</code> (for a list of expressions).
</p>
<div class="sourceCode"><pre>my_function &lt;- function(data, var, ...) {
  # Defuse
  var &lt;- enquo(var)
  dots &lt;- enquos(...)

  # Inject
  data %&gt;%
    group_by(!!!dots) %&gt;%
    summarise(mean = mean(!!var))
}
</pre></div>
<p>In this simple case, the code is equivalent to the usage of <code style="white-space: pre;">&#8288;{{&#8288;</code>
and <code>...</code> above. Defusing with <code>enquo()</code> or <code>enquos()</code> is only
needed in more complex cases, for instance if you need to inspect
or modify the expressions in some way.
</p>
</li>
<li><p> The <code>.data</code> pronoun is an object that represents the current
slice of data. If you have a variable name in a string, use the
<code>.data</code> pronoun to subset that variable with <code>[[</code>.
</p>
<div class="sourceCode"><pre>my_var &lt;- "disp"
mtcars %&gt;% summarise(mean = mean(.data[[my_var]]))
</pre></div>
</li>
<li><p> Another tidy eval operator is <code style="white-space: pre;">&#8288;:=&#8288;</code>. It makes it possible to use
glue and curly-curly syntax on the LHS of <code>=</code>. For technical
reasons, the R language doesn't support complex expressions on
the left of <code>=</code>, so we use <code style="white-space: pre;">&#8288;:=&#8288;</code> as a workaround.
</p>
<div class="sourceCode"><pre>my_function &lt;- function(data, var, suffix = "foo") {
  # Use `{{` to tunnel function arguments and the usual glue
  # operator `{` to interpolate plain strings.
  data %&gt;%
    summarise("{{ var }}_mean_{suffix}" := mean({{ var }}))
}
</pre></div>
</li>
<li><p> Many tidy eval functions like <code>dplyr::mutate()</code> or
<code>dplyr::summarise()</code> give an automatic name to unnamed inputs. If
you need to create the same sort of automatic names by yourself,
use <code>as_label()</code>. For instance, the glue-tunnelling syntax above
can be reproduced manually with:
</p>
<div class="sourceCode"><pre>my_function &lt;- function(data, var, suffix = "foo") {
  var &lt;- enquo(var)
  prefix &lt;- as_label(var)
  data %&gt;%
    summarise("{prefix}_mean_{suffix}" := mean(!!var))
}
</pre></div>
<p>Expressions defused with <code>enquo()</code> (or tunnelled with <code style="white-space: pre;">&#8288;{{&#8288;</code>) need
not be simple column names, they can be arbitrarily complex.
<code>as_label()</code> handles those cases gracefully. If your code assumes
a simple column name, use <code>as_name()</code> instead. This is safer
because it throws an error if the input is not a name as expected.
</p>
</li></ul>



<h3>Value</h3>

<p>This function does not return any value (NULL). It only serves to document
the tidy eval tools reexported in this package from rlang.
</p>

<hr>
<h2 id='Topic_Model_Validation_Overview'>Topic_Model_Validation Repository Overview</h2><span id='topic+Topic_Model_Validation_Overview'></span>

<h3>Description</h3>

<p>The 'Topic_Model_Validation' repository is a collection of scripts and functions for performing topic modeling and evaluating topic models. This document provides an overview of the different scripts and functions in the repository and their purpose.
</p>


<h3>Details</h3>

<p>## Python Scripts
### evaluate.py
The 'evaluate.py' script provides functions for evaluating the performance of topic models on different datasets and tasks. The functions within this script include:
- <code>R4WSItasktest()</code>: Evaluates the performance of a topic model on the R4WSI task, which involves predicting the top k words for a given topic.
- <code>allR4WSItasktest()</code>: Evaluates the performance of a topic model on multiple versions of the R4WSI task.
- <code>goldR4WSItest()</code>: Evaluates the performance of a topic model on a gold-standard R4WSI dataset.
- <code>heldouttest()</code>: Evaluates the performance of a topic model on held-out data.
- <code>keypostedtest()</code>: Evaluates the performance of a topic model on a key-posted dataset.
- <code>masstest()</code>: Evaluates the performance of a topic model on a massive dataset.
- <code>modtest()</code>: Evaluates the performance of a topic model on a given dataset.
- <code>resultstest()</code>: Evaluates the performance of a topic model on a given dataset and stores the results.
### record.py
The 'record.py' script provides a function for storing the results of topic model evaluations. The function within this script is:
- <code>record()</code>: Stores the results of topic model evaluations.
## R Scripts
### lda.R
The 'lda.R' script provides functions for performing Latent Dirichlet Allocation (LDA) topic modeling on text data. The functions within this script include:
- <code>lda_model()</code>: Fits an LDA model to text data.
### lsa.R
The 'lsa.R' script provides functions for performing Latent Semantic Analysis (LSA) topic modeling on text data. The functions within this script include:
- <code>lsa_model()</code>: Fits an LSA model to text data.
### evaluate.R
The 'evaluate.R' script provides functions for evaluating the performance of topic models using various metrics, such as perplexity and coherence. The functions within this script include:
- <code>evaluate_model()</code>: Evaluates the performance of a topic model using various metrics.
### helpers.R
The 'helpers.R' script provides various helper functions that are used by the other scripts in the repository. The functions within this script include:
- <code>clean_text()</code>: Cleans and preprocesses text data for use in topic modeling.
- <code>read_data()</code>: Reads in text data from a file.
- <code>write_data()</code>: Writes text data to a file.
</p>

<hr>
<h2 id='validateLabel'>Create validation tasks for labels assigned to the topics in the topic model of choice.</h2><span id='topic+validateLabel'></span>

<h3>Description</h3>

<p>Create validation tasks for labels assigned to the topics in the topic model of choice.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validateLabel(
  type,
  n,
  text.predict = NULL,
  text.name = "text",
  top1.name = "top1",
  top2.name = "top2",
  top3.name = "top3",
  labels = NULL,
  labels.index = NULL,
  labels.add = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validateLabel_+3A_type">type</code></td>
<td>
<p>Task structures to be specified. Must be one of &quot;LI&quot; (Label Intrusion) 
and &quot;OL&quot; (Optimal Label).</p>
</td></tr>
<tr><td><code id="validateLabel_+3A_n">n</code></td>
<td>
<p>The number of desired tasks</p>
</td></tr>
<tr><td><code id="validateLabel_+3A_text.predict">text.predict</code></td>
<td>
<p>A data frame or matrix containing both the text and the indicator(s)
of the model predicted topic(s).</p>
</td></tr>
<tr><td><code id="validateLabel_+3A_text.name">text.name</code></td>
<td>
<p>variable name in 'text.predict' that indicates the text</p>
</td></tr>
<tr><td><code id="validateLabel_+3A_top1.name">top1.name</code></td>
<td>
<p>variable name in 'text.predict' that indicates the top1 model predicted topic</p>
</td></tr>
<tr><td><code id="validateLabel_+3A_top2.name">top2.name</code></td>
<td>
<p>variable name in 'text.predict' that indicates the top2 model predicted topic</p>
</td></tr>
<tr><td><code id="validateLabel_+3A_top3.name">top3.name</code></td>
<td>
<p>variable name in 'text.predict' that indicates the top3 model predicted topic</p>
</td></tr>
<tr><td><code id="validateLabel_+3A_labels">labels</code></td>
<td>
<p>The user-defined labels assigned to the topics</p>
</td></tr>
<tr><td><code id="validateLabel_+3A_labels.index">labels.index</code></td>
<td>
<p>The topic index in correspondence with the labels, e.g., c(10, 12, 15).
Must be in the same length and order with 'label'.</p>
</td></tr>
<tr><td><code id="validateLabel_+3A_labels.add">labels.add</code></td>
<td>
<p>Labels from other broad catagories. Default to NULL. Users could 
specify them to evaluate how well different broad categories are distinguished from
one another.
</p>
<p>#' value A matrix containing the validation tasks as described in the return section.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users need to pick a topic model that they deem to be good and label the topics 
they later would like to use as measures.
</p>


<h3>Value</h3>

<p>A matrix containing the validation tasks. The matrix has six value columns: 
</p>

<dl>
<dt>topic</dt><dd><p>The topic index associated with the document.</p>
</dd>
<dt>doc</dt><dd><p>The text of the document.</p>
</dd>
<dt>opt1</dt><dd><p>The first option label presented to the user.</p>
</dd>
<dt>opt2</dt><dd><p>The second option label presented to the user.</p>
</dd>
<dt>opt3</dt><dd><p>The third option label presented to the user.</p>
</dd>
<dt>optcrt</dt><dd><p>The correct label for the document.</p>
</dd>
</dl>


<hr>
<h2 id='validateTopic'>Create validation tasks for topic model selection</h2><span id='topic+validateTopic'></span>

<h3>Description</h3>

<p>Create validation tasks for topic model selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validateTopic(type, n, text = NULL, vocab, beta, theta = NULL, thres = 20)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validateTopic_+3A_type">type</code></td>
<td>
<p>Task structures to be specified. Must be one of &quot;WI&quot; (word intrusion),
&quot;T8WSI&quot; (top 8 word set intrusion), and &quot;R4WSI&quot; (random 4 word set intrusion).</p>
</td></tr>
<tr><td><code id="validateTopic_+3A_n">n</code></td>
<td>
<p>The number of desired tasks</p>
</td></tr>
<tr><td><code id="validateTopic_+3A_text">text</code></td>
<td>
<p>The pool of documents to be shown to the Mturk workers</p>
</td></tr>
<tr><td><code id="validateTopic_+3A_vocab">vocab</code></td>
<td>
<p>A character vector specifying the words in the corpus. Usually, it
can be found in topic model output.</p>
</td></tr>
<tr><td><code id="validateTopic_+3A_beta">beta</code></td>
<td>
<p>A matrix of word probabilities for each topic. Each row represents a
topic and each column represents a word. Note this should not be in the logged form.</p>
</td></tr>
<tr><td><code id="validateTopic_+3A_theta">theta</code></td>
<td>
<p>A matrix of topic proportions. Each row represents a document and each
clums represents a topic. Must be specified if task = &quot;T8WSI&quot; or &quot;R4WSI&quot;.</p>
</td></tr>
<tr><td><code id="validateTopic_+3A_thres">thres</code></td>
<td>
<p>the threshold to draw words from, default to top 50 words.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users need to fit their own topic models.
</p>


<h3>Value</h3>

<p>A matrix of validation tasks. Each row represents a task and each column
represents an aspect of a task, including the topic label, the document text (for
&quot;T8WSI&quot; and &quot;R4WSI&quot;), and five words, including four non-intrusive words and one
intrusive word.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
