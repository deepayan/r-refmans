<!DOCTYPE html><html lang="en"><head><title>Help for package bestglm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bestglm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bestglm-package'><p> bestglm: Best Subset GLM</p></a></li>
<li><a href='#AirQuality'>
<p>Daily ozone pollution  with meteorlogical and date inputs</p></a></li>
<li><a href='#asbinary'><p>Binary representation of non-negative integer</p></a></li>
<li><a href='#bestglm'><p>Best Subset GLM using Information Criterion or Cross-Validation</p></a></li>
<li><a href='#CVd'><p> Cross-validation using delete-d method.</p></a></li>
<li><a href='#CVDH'><p> Adjusted K-fold Cross-Validation</p></a></li>
<li><a href='#CVHTF'><p> K-fold Cross-Validation</p></a></li>
<li><a href='#Detroit'><p>Detroit homicide data for 1961-73 used in the book Subset Regression by</p>
A.J. Miller</a></li>
<li><a href='#dgrid'>
<p>Scaled Variables Dependency Plots: Output vs Inputs</p></a></li>
<li><a href='#Fires'><p>Forest fires in Montesinho natural park. Standardized inputs.</p></a></li>
<li><a href='#fitted.pcreg'>
<p>Fitted values in PCR and PLS.</p></a></li>
<li><a href='#glmnetGridTable'>
<p>Multipanel Display and Table Glmnet CV Output.</p></a></li>
<li><a href='#glmnetPredict'><p>Glmnet Prediction Using CVAV.</p></a></li>
<li><a href='#grpregPredict'>
<p>Predictions on Test Data with Grpreg</p></a></li>
<li><a href='#hivif'>
<p>Simulated Linear Regression (Train) with Nine Highly Correlated Inputs</p></a></li>
<li><a href='#Iowa'>
<p>Iowa School Test</p></a></li>
<li><a href='#LOOCV'><p> Leave-one-out cross-validation</p></a></li>
<li><a href='#manpower'><p>Hospital manpower data</p></a></li>
<li><a href='#mcdonald'>
<p>Pollution dataset from McDonald and Schwing (1973)</p></a></li>
<li><a href='#MontesinhoFires'><p>Forest fires in Montesinho natural park</p></a></li>
<li><a href='#NNPredict'>
<p>Nearest Neighbour Regression Prediction</p></a></li>
<li><a href='#oneSDRule'><p> Utility function. Implements the 1-sd rule.</p></a></li>
<li><a href='#pcreg'>
<p>Principal Component and Partial Least Squares Regression</p></a></li>
<li><a href='#plot.pcreg'>
<p>Diagnostic plots for PCR and PLS</p></a></li>
<li><a href='#plot1SDRule'>
<p>Plot Regularization Path and One Standard Deviation Rule</p></a></li>
<li><a href='#predict.pcreg'>
<p>Predict Method for Pcreg.</p></a></li>
<li><a href='#print.bestglm'><p>Print method for 'bestglm' object</p></a></li>
<li><a href='#print.pcreg'><p>Print method for 'pcreg' object</p></a></li>
<li><a href='#residuals.pcreg'>
<p>Residuals Fitted PCR or PLS</p></a></li>
<li><a href='#rubber'>
<p>Abrasion loss for various hardness and tensile strength</p></a></li>
<li><a href='#SAheart'><p> South African Hearth Disease Data</p></a></li>
<li><a href='#Shao'><p> Simulated Regression Data</p></a></li>
<li><a href='#sphereX'>
<p>Sphere Data Matrix</p></a></li>
<li><a href='#summary.bestglm'><p> summary of  'bestglm' object</p></a></li>
<li><a href='#summary.pcreg'>
<p>Summary Method for Pcreg.</p></a></li>
<li><a href='#trainTestPartition'>
<p>Partition Dataframe into Train/Test Samples</p></a></li>
<li><a href='#vifx'>
<p>Variance Inflation Factor for a Design Matrix</p></a></li>
<li><a href='#znuclear'><p> Nuclear plant data. Quantitative inputs logged and standardized.</p></a></li>
<li><a href='#zprostate'><p> Prostate cancer data. Standardized.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Best Subset GLM and Regression Utilities</td>
</tr>
<tr>
<td>Version:</td>
<td>0.37.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-03-13</td>
</tr>
<tr>
<td>Author:</td>
<td>A.I. McLeod, Changjiang Xu and Yuanhao Lai</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yuanhao Lai &lt;ylai72@uwo.ca&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0), leaps</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS</td>
</tr>
<tr>
<td>Imports:</td>
<td>lattice, glmnet, grpreg, pls</td>
</tr>
<tr>
<td>Enhances:</td>
<td>caret</td>
</tr>
<tr>
<td>Description:</td>
<td>Best subset glm using information criteria or cross-validation,
        carried by using 'leaps' algorithm (Furnival and Wilson, 1974) &lt;<a href="https://doi.org/10.2307%2F1267601">doi:10.2307/1267601</a>&gt;
        or complete enumeration (Morgan and Tatar, 1972) &lt;<a href="https://doi.org/10.1080%2F00401706.1972.10488918">doi:10.1080/00401706.1972.10488918</a>&gt;.
        Implements PCR and PLS using AIC/BIC.
        Implements one-standard deviation rule for use with the 'caret' package.</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>Classification/ACM:</td>
<td>G.3, G.4, I.5.1</td>
</tr>
<tr>
<td>Classification/MSC:</td>
<td>62M10, 91B84</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-03-13 01:11:06 UTC; Hao</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-03-13 10:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bestglm-package'> bestglm: Best Subset GLM</h2><span id='topic+bestglm-package'></span>

<h3>Description</h3>

<p>Provides new information criterion BICq as well as AIC, BIC and EBIC for selecting the best model. 
Additionally, various CV algorithms are also provided.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> bestglm</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.33</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2011-11-03</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GLP 2.0 or greater</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyData: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>bestglm is the main function. All other functions are utility functions and are not normally invoked.
</p>
<p>Many examples are provided in the vignettes accompanying this package.
The vignettes are produced using the R package <code>Sweave</code> and so R scripts
can easily be extracted.
</p>
<p>The R package <code>xtable</code> is needed for the vignette in <code>SimExperimentBICq.Rnw</code>.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod and Changjiang Xu</p>


<h3>References</h3>

<p>Xu, C. and McLeod, A.I. (2009). 
Bayesian Information Criterion with Bernouilli Prior.
</p>


<h3>See Also</h3>

<p><code><a href="leaps.html#topic+leaps">leaps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(zprostate)
train&lt;-(zprostate[zprostate[,10],])[,-10]
#Best subset using AIC
bestglm(train, IC="AIC")
#Best subset using BIC
bestglm(train, IC="BIC")
#Best subset using EBIC
bestglm(train, IC="BICg")
#Best subset using BICg with g=0.5 (tuning parameter)
bestglm(train, IC="BICg", t=0.5)
#Best subset using BICq. Note BICq with q=0.25 is default.
bestglm(train, IC="BICq")
#Best subset using BICq with q=0.5 (equivalent to BIC)
bestglm(train, IC="BICq", t=0.5)
#Remark: set seed since CV depends on it
set.seed(123321123)
bestglm(train, IC="CV", t=10)
#using HTF method
bestglm(train, IC="CV", CVArgs=list(Method="HTF", K=10, REP=1))
#Best subset, logistic regression
data(SAheart)
bestglm(SAheart, IC="BIC", family=binomial)
#Best subset, factor variables with more than 2 levels
data(AirQuality)
#subset
bestglm(AirQuality, IC="BICq")

## End(Not run)
</code></pre>

<hr>
<h2 id='AirQuality'>
Daily ozone pollution  with meteorlogical and date inputs
</h2><span id='topic+AirQuality'></span>

<h3>Description</h3>

<p>This dataset was derived from the R built-in dataset 'airquality' by adding
date information and deleting all missing values.
This dataset is referred to as 'environmental' in Cleveland (1993).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(AirQuality)</code></pre>


<h3>Format</h3>

<p>A data frame with 111 observations on the following 6 variables.
</p>

<dl>
<dt><code>Solar.R</code></dt><dd><p>input, a numeric vector</p>
</dd>
<dt><code>Wind</code></dt><dd><p>input, a numeric vector</p>
</dd>
<dt><code>Temp</code></dt><dd><p>input, a numeric vector</p>
</dd>
<dt><code>month</code></dt><dd><p>input, a factor with levels <code>May</code> <code>Jun</code> 
<code>Jul</code> <code>Aug</code> <code>Sep</code> <code>Oct</code> <code>Nov</code> <code>Dec</code> <code>Jan</code> 
<code>Feb</code> <code>Mar</code> <code>Apr</code></p>
</dd>
<dt><code>weekday</code></dt><dd><p>input, a factor with levels <code>Sunday</code> 
<code>Monday</code> <code>Tuesday</code> <code>Wednesday</code> <code>Thursday</code> <code>Friday</code> 
<code>Saturday</code></p>
</dd> <dt><code>Ozone</code></dt><dd><p>output, a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p>Cleveland (1993, Chapter 5) presents an insightful analysis using co-plots and 
the scatterplot matrix.
Several interesting interactions are noted.
For a fixed 'Wind&lsquo;, the effect of &rsquo;Solar.R' changes as 'Temp' increases.
And for a fixed 'Temp', as 'Wind' decreases, the effect of 'Solar.R' is less.
</p>


<h3>Source</h3>

<p><code><a href="datasets.html#topic+airquality">airquality</a></code>
</p>


<h3>References</h3>

<p>Cleveland, W.S. (1993). Visualizing Data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(AirQuality)
#Example 1. Find best model
bestglm(AirQuality, IC="BIC")

</code></pre>

<hr>
<h2 id='asbinary'>Binary representation of non-negative integer</h2><span id='topic+to.binary'></span>

<h3>Description</h3>

<p>A non-negative integer is represented as a binary number.
The digits, 0 or 1, of this number are returned in a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to.binary(n, k = ceiling(logb(n+1,base=2)))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="asbinary_+3A_n">n</code></td>
<td>

<p>a non-negative integers
</p>
</td></tr>
<tr><td><code id="asbinary_+3A_k">k</code></td>
<td>

<p>number of digits to be returned. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of length k. The first element is the least significant digit.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod
</p>


<h3>Examples</h3>

<pre><code class='language-R'>to.binary(63)
to.binary(64)
#sometimes we want to pad result with 'leading' 0's
to.binary(63, k=20)
to.binary(64, k=20)
</code></pre>

<hr>
<h2 id='bestglm'>Best Subset GLM using Information Criterion or Cross-Validation </h2><span id='topic+bestglm'></span>

<h3>Description</h3>

<p>Best subset selection using 'leaps' algorithm  (Furnival and Wilson, 1974)
or complete enumeration (Morgan and Tatar, 1972).
Complete enumeration is used for the non-Gaussian and for the
case where the input matrix contains factor variables with more than 2 levels.
The best fit may be found using
the information criterion IC: AIC, BIC, EBIC, or BICq.
Alternatively, with IC=&lsquo;CV&rsquo; various types of cross-validation may be used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bestglm(Xy, family = gaussian, IC = "BIC", t = "default", 
 CVArgs = "default", qLevel = 0.99, TopModels = 5, 
 method = "exhaustive", intercept = TRUE, weights = NULL, 
 nvmax = "default", RequireFullEnumerationQ = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bestglm_+3A_xy">Xy</code></td>
<td>
<p> Dataframe containing the design matrix X and the output variable y. All columns must be named.</p>
</td></tr>
<tr><td><code id="bestglm_+3A_family">family</code></td>
<td>
<p>One of the glm distribution functions. The glm function is not used
in the Gaussian case. Instead for efficiency either 'leaps' is used or when factor
variables are present with more than 2 levels, 'lm' may be used. </p>
</td></tr>
<tr><td><code id="bestglm_+3A_ic">IC</code></td>
<td>
<p> Information criteria to use: &quot;AIC&quot;, &quot;BIC&quot;, &quot;BICg&quot;, &quot;BICq&quot;, &quot;LOOCV&quot;, &quot;CV&quot;.</p>
</td></tr>
<tr><td><code id="bestglm_+3A_t">t</code></td>
<td>
<p> adjustable parameter for BICg, BICq or CV. For BICg, default is g=t=1. 
For BICq, default is q=t=0.25.  For CV, default the delete-d method with d=ceil(n(1-1/(log n - 1)))
and REP=t=1000. The default value of the parameter may be changed by changing t.</p>
</td></tr>
<tr><td><code id="bestglm_+3A_cvargs">CVArgs</code></td>
<td>
<p> Used when IC is set to 'CV'.  The default is use the delete-d algorithm
with d=ceil(n(1-1/(log n - 1))) and t=100 repetitions. Note that the number of repetitions
can be changed using t. 
More generally, CVArgs is a list with
3 named components: Method, K, REP, where Method is one of \&quot;HTF\&quot;, \&quot;DH\&quot;, \&quot;d\&quot; 
corresponding to using the functions CVHTM (Hastie et al., 2009, K-fold CV), CVDH (adjusted K-fold
CV, Davison and Hartigan, 1997) and CVd (delete-d CV with random subsamples, Shao, 1997).
</p>
</td></tr>
<tr><td><code id="bestglm_+3A_qlevel">qLevel</code></td>
<td>
<p> the alpha level for determining interval for best q. Larger alpha's result in larger intervals. </p>
</td></tr>
<tr><td><code id="bestglm_+3A_topmodels">TopModels</code></td>
<td>
<p>Finds the best <code>TopModels</code> models.</p>
</td></tr>
<tr><td><code id="bestglm_+3A_method">method</code></td>
<td>
<p> Method used in leaps algorithm for searching for the best subset.</p>
</td></tr>
<tr><td><code id="bestglm_+3A_intercept">intercept</code></td>
<td>
<p>Default TRUE means the intercept term is always included. 
If set to FALSE, no intercept term is included.
If you want only include the intercept term when it is signficant then set IncludeInterceptQ=FALSE and include
a column of 1's in the design matrix.</p>
</td></tr>
<tr><td><code id="bestglm_+3A_weights">weights</code></td>
<td>
<p>weights </p>
</td></tr>
<tr><td><code id="bestglm_+3A_nvmax">nvmax</code></td>
<td>
<p> maximum number of independent variables allowed. By default, all variables</p>
</td></tr>
<tr><td><code id="bestglm_+3A_requirefullenumerationq">RequireFullEnumerationQ</code></td>
<td>
<p>Use exhaustive search algorithm instead of 'leaps'</p>
</td></tr>
<tr><td><code id="bestglm_+3A_...">...</code></td>
<td>
<p>Optional arguments which are passed to <code>lm</code> or <code>glm</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the Gaussian case, 
the loglikelihood may be written <code class="reqn">logL = -(n/2) log (RSS/n)</code>,
where RSS is the residual sum-of-squares and n is the number of observations.
When the function 'glm' is used, the log-likelihood, logL, is obtained using 'logLik'.
The penalty for EBIC and BICq depends on the tuning parameter argument, <code>t</code>.
The argument <code>t</code> also controls the number of replications used when
the delete-d CV is used as default. In this case, the parameter d is chosen
using the formula recommended by Shao (1997).
See   <code><a href="#topic+CVd">CVd</a></code> for more details.
</p>
<p>In the binomial GLM, nonlogistic, case the last two columns of Xy
are the counts of 'success' and 'failures'.
</p>
<p>Cross-validation may also be used to select the best subset.
When cross-validation is used, the best models of size k according to the
log-likelihood	are compared for k=0,1,...,p, where p is the number of inputs.
Cross-validation is not available when there are categorical variables since
in this case it is likely that the training sample may not contain all levels
and in this case we can't predict the response in the validation sample.
In the case of GLM, the \&quot;DH\&quot; method for CV is not available. 
</p>
<p>Usually it is a good idea to keep the intercept term even if it is not
significant. See discussion in vignette.
</p>
<p>Cross-validation is not available for models with no intercept term or
when <code>force.in</code> is non-null or when <code>nvmax</code> is set
to less than the full number of independent variables.
</p>
<p>Please see the package vignette for more details and examples.
</p>


<h3>Value</h3>

<p>A list with class attribute 'bestglm' and named components:
</p>
<table role = "presentation">
<tr><td><code>BestModel</code></td>
<td>
<p>An lm-object representing the best fitted regression.</p>
</td></tr>
<tr><td><code>Title</code></td>
<td>
<p>A brief title describing the algorithm used: CV(K=K), CVadj(K=K), CVd(d=K).
The range of q for an equivalent BICq model is given.</p>
</td></tr>
<tr><td><code>Subsets</code></td>
<td>
<p>The best subsets of size, k=0,1,...,p are indicated as well the value of the
log-likelihood and information criterion for each best subset. 
In the case of categorical variables with more than 2 levels, the degrees of freedom
are also shown.
</p>
</td></tr>
<tr><td><code>qTable</code></td>
<td>
<p>Table showing range of q for choosing each possible subset size. Assuming
intercept=TRUE, k=1 corresponds to model with only an intercept term and k=p+1,
where p is the number of input variables, corresponds to including all variables.</p>
</td></tr>
<tr><td><code>Bestq</code></td>
<td>
<p>Optimal q</p>
</td></tr>
<tr><td><code>ModelReport</code></td>
<td>
<p>A list with components: NullModel, LEAPSQ, glmQ, gaussianQ, NumDF, CategoricalQ, Bestk. </p>
</td></tr>
<tr><td><code>BestModels</code></td>
<td>
<p>Variables in the <code>TopModels</code> best list</p>
</td></tr>
</table>
<p>Methods function 'print.bestglm' and 'summary.bestglm' are provided.
</p>


<h3>Author(s)</h3>

<p> C. Xu and A.I. McLeod</p>


<h3>References</h3>

 
<p>Xu, C. and McLeod, A.I. (2009).
Bayesian Information Criterion with Bernouilli Prior.
</p>
<p>Chen, J. and Chen, Z. (2008). Extended Bayesian 
Information Criteria for Model Selection with Large Model Space. Biometrika 2008 95: 759-771.
</p>
<p>Furnival, G.M. and Wilson, R. W. (1974).  
Regressions by Leaps and Bounds.  Technometrics, 16, 499&ndash;511.
</p>
<p>Morgan, J. A.  and  Tatar, J. F. (1972). 
Calculation of the Residual Sum of Squares for All Possible Regressions. 
Technometrics 14, 317-325.
</p>
<p>Miller, A. J. (2002), 
Subset Selection in Regression, 2nd Ed. London, Chapman and Hall.
</p>
<p>Shao, Jun (1997). An Asymptotic Theory for Linear Model Selection.  Statistica Sinica 7, 221-264.
</p>


<h3>See Also</h3>

 
<p><code><a href="stats.html#topic+glm">glm</a></code>,
<code><a href="stats.html#topic+lm">lm</a></code>,
<code><a href="leaps.html#topic+leaps">leaps</a></code>
<code><a href="#topic+CVHTF">CVHTF</a></code>,
<code><a href="#topic+CVDH">CVDH</a></code>,
<code><a href="#topic+CVd">CVd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example 1. 
#White noise test.
set.seed(123321123)
p&lt;-25   #number of inputs
n&lt;-100  #number of observations
X&lt;-matrix(rnorm(n*p), ncol=p)
y&lt;-rnorm(n)
Xy&lt;-as.data.frame(cbind(X,y))
names(Xy)&lt;-c(paste("X",1:p,sep=""),"y")
bestAIC &lt;- bestglm(Xy, IC="AIC")
bestBIC &lt;- bestglm(Xy, IC="BIC")
bestEBIC &lt;- bestglm(Xy, IC="BICg")
bestBICq &lt;- bestglm(Xy, IC="BICq")
NAIC &lt;- length(coef(bestAIC$BestModel))-1
NBIC &lt;- length(coef(bestBIC$BestModel))-1
NEBIC &lt;- length(coef(bestEBIC$BestModel))-1
NBICq &lt;- length(coef(bestBICq$BestModel))-1
ans&lt;-c(NAIC, NBIC, NEBIC, NBICq)
names(ans)&lt;-c("AIC", "BIC", "BICg", "BICq")
ans
# AIC  BIC EBIC BICq 
#   3    1    0    0 

#Example 2. bestglm with BICq
#Find best model. Default is BICq with q=0.25
data(znuclear) #standardized data. 
#Rest of examples assume this dataset is loaded.
out&lt;-bestglm(znuclear, IC="BICq")
out
#The optimal range for q
out$Bestq
#The possible models that can be chosen
out$qTable
#The best models for each subset size
out$Subsets
#The overall best models
out$BestModels
#
#Example 3. Normal probability plot, residuals, best model
ans&lt;-bestglm(znuclear, IC="BICq")
e&lt;-resid(ans$BestModel)
qqnorm(e, ylab="residuals, best model")
#
#To save time, none of the remaining examples are run
## Not run: 
#Example 4. bestglm, using EBIC, g=1
bestglm(znuclear, IC="BICg")
#EBIC with g=0.5
bestglm(znuclear, IC="BICg", t=0.5)
#
#Example 5. bestglm, CV
data(zprostate)
train&lt;-(zprostate[zprostate[,10],])[,-10]
#the default CV method takes too long, set t=10 to do only
# 10 replications instead of the recommended 1000
bestglm(train, IC="CV", t=10)
bestglm(train, IC="CV", CVArgs=list(Method="HTF", K=10, REP=1))
#Compare with DH Algorithm. Normally set REP=100 is recommended.
bestglm(train, IC="CV", CVArgs=list(Method="DH", K=10, REP=1))
#Compare LOOCV
bestglm(train, IC="LOOCV")
#
#Example 6. Optimal q for manpower dataset
data(manpower)
out&lt;-bestglm(manpower)
out$Bestq
#
#Example 7. Factors with more than 2 levels
data(AirQuality)
bestglm(AirQuality)
#
#Example 8. Logistic regression
data(SAheart)
bestglm(SAheart, IC="BIC", family=binomial)
#BIC agrees with backward stepwise approach
out&lt;-glm(chd~., data=SAheart, family=binomial)
step(out, k=log(nrow(SAheart)))
#but BICq with q=0.25
bestglm(SAheart, IC="BICq", t=0.25, family=binomial)
#
#Cross-validation with glm
#make reproducible results
set.seed(33997711)
#takes about 15 seconds and selects 5 variables
bestglm(SAheart, IC="CV", family=binomial)
#about 6 seconds and selects 2 variables
bestglm(SAheart, IC="CV", CVArgs=list(Method="HTF", K=10, REP=1), family=binomial)
#Will produce an error -- NA
\dontrun{bestglm(SAheart, IC="CV", CVArgs=list(Method="DH", K=10, REP=1), family=binomial)}
\dontrun{bestglm(SAheart, IC="LOOCV", family=binomial)}
#
#Example 9. Model with no intercept term
X&lt;-matrix(rnorm(200*3), ncol=3)
b&lt;-c(0, 1.5, 0)
y&lt;-X%*%b + rnorm(40)
Xy&lt;-data.frame(as.matrix.data.frame(X), y=y)
bestglm(Xy, intercept=FALSE) 

## End(Not run)

</code></pre>

<hr>
<h2 id='CVd'> Cross-validation using delete-d method.</h2><span id='topic+CVd'></span>

<h3>Description</h3>

<p>The delete-d method for cross-validation uses a random sample of d observations 
as the validation sample.  This is repeated many times.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CVd(X, y, d = ceiling(n * (1 - 1/(log(n) - 1))), REP = 100, family = gaussian, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CVd_+3A_x">X</code></td>
<td>
<p> training inputs </p>
</td></tr>
<tr><td><code id="CVd_+3A_y">y</code></td>
<td>
<p> training output </p>
</td></tr>
<tr><td><code id="CVd_+3A_d">d</code></td>
<td>
<p> size of validation sample </p>
</td></tr>
<tr><td><code id="CVd_+3A_rep">REP</code></td>
<td>
<p> number of replications </p>
</td></tr>
<tr><td><code id="CVd_+3A_family">family</code></td>
<td>
<p>glm family</p>
</td></tr>
<tr><td><code id="CVd_+3A_...">...</code></td>
<td>
<p> optional arguments passed to <code>glm</code> or <code>lm</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Shao (1993, 1997) suggested the delete-d algorithm implemented in this function. 
In this algorithm, a random sample of d observations are taken as the validation
sample.
This random sampling is repeated <code>REP</code> times.
Shao (1997, p.234, eqn. 4.5 and p.236) suggests <code class="reqn">d= n(1-1/(log n - 1))</code>,
This is obtained by taking <code class="reqn">\lambda_n = log n</code> on page 236 (Shao, 1997).
As shown in the table Shao's recommended choice of the d parameter corresponds
to validation samples that are typically much larger that used in 10-fold or 
5-fold
cross-validation.  LOOCV corresponds to d=1 only!
</p>

<table>
<tr>
 <td style="text-align: left;">
   n  </td><td style="text-align: left;">  d   </td><td style="text-align: left;">  K=10   </td><td style="text-align: left;">  K=5   	</td>
</tr>
<tr>
 <td style="text-align: left;">
   50 </td><td style="text-align: left;">  33  </td><td style="text-align: left;">  5      </td><td style="text-align: left;">  10    	</td>
</tr>
<tr>
 <td style="text-align: left;">
  100 </td><td style="text-align: left;">  73  </td><td style="text-align: left;">  10     </td><td style="text-align: left;">  20   	</td>
</tr>
<tr>
 <td style="text-align: left;">
  200 </td><td style="text-align: left;"> 154  </td><td style="text-align: left;">  20     </td><td style="text-align: left;">  40   	</td>
</tr>
<tr>
 <td style="text-align: left;">
  500 </td><td style="text-align: left;"> 405  </td><td style="text-align: left;">  50     </td><td style="text-align: left;">  100   	</td>
</tr>
<tr>
 <td style="text-align: left;">
 1000 </td><td style="text-align: left;"> 831  </td><td style="text-align: left;">  100    </td><td style="text-align: left;">  200  	</td>
</tr>
<tr>
 <td style="text-align: left;">
       </td>
</tr>

</table>



<h3>Value</h3>

<p>Vector of two components comprising the cross-validation MSE and its sd based 
on the MSE in each validation sample.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod and C. Xu</p>


<h3>References</h3>

<p>Shao, Jun (1993). Linear Model Selection by Cross-Validation. 
Journal of the American Statistical Assocation 88, 486-494.
</p>
<p>Shao, Jun (1997). An Asymptotic Theory for Linear Model Selection.  
Statistica Sinica 7, 221-264.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+bestglm">bestglm</a></code>, 
<code><a href="#topic+CVHTF">CVHTF</a></code>, 
<code><a href="#topic+CVDH">CVDH</a></code>, 
<code><a href="#topic+LOOCV">LOOCV</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example 1. delete-d method
#For the training set, n=67. So 10-fold CV is like using delete-d
#with d=7, approximately.
data(zprostate)
train&lt;-(zprostate[zprostate[,10],])[,-10]
X&lt;-train[,1:2]
y&lt;-train[,9]
set.seed(123321123)
CVd(X, y, d=7, REP=10)
#should set to 1000. Used 10 to save time in example.
</code></pre>

<hr>
<h2 id='CVDH'> Adjusted K-fold Cross-Validation </h2><span id='topic+CVDH'></span>

<h3>Description</h3>

<p>An adjustment to K-fold cross-validation is made to reduce bias.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CVDH(X, y, K = 10, REP = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CVDH_+3A_x">X</code></td>
<td>
<p> training inputs </p>
</td></tr>
<tr><td><code id="CVDH_+3A_y">y</code></td>
<td>
<p> training output </p>
</td></tr>
<tr><td><code id="CVDH_+3A_k">K</code></td>
<td>
<p> size of validation sample </p>
</td></tr>
<tr><td><code id="CVDH_+3A_rep">REP</code></td>
<td>
<p> number of replications </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Algorithm 6.5 (Davison and Hinkley, p.295) is implemented.
</p>


<h3>Value</h3>

<p>Vector of two components comprising the cross-validation MSE and its sd based 
on the MSE in each validation sample.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod and C. Xu</p>


<h3>References</h3>

<p>Davison, A.C. and Hinkley, D.V. (1997). 
Bootstrap Methods and their Application. Cambridge University Press.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+bestglm">bestglm</a></code>, 
<code><a href="#topic+CVHTF">CVHTF</a></code>, 
<code><a href="#topic+CVd">CVd</a></code>, 
<code><a href="#topic+LOOCV">LOOCV</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example 1. Variability in 10-fold CV with Davison-Hartigan Algorithm.
#Plot the CVs obtained by using 10-fold CV on the best subset
#model of size 2 for the prostate data. We assume the best model is
#the model with the first two inputs and then we compute the CV's
#using 10-fold CV, 100 times. The result is summarized by a boxplot as well 
#as the sd.
NUMSIM&lt;-10
data(zprostate)
train&lt;-(zprostate[zprostate[,10],])[,-10]
X&lt;-train[,1:2]
y&lt;-train[,9]
cvs&lt;-numeric(NUMSIM)
set.seed(123321123)
for (isim in 1:NUMSIM)
    cvs[isim]&lt;-CVDH(X,y,K=10,REP=1)[1]
summary(cvs)
</code></pre>

<hr>
<h2 id='CVHTF'> K-fold Cross-Validation </h2><span id='topic+CVHTF'></span>

<h3>Description</h3>

<p>K-fold cross-validation. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CVHTF(X, y, K = 10, REP = 1, family = gaussian, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CVHTF_+3A_x">X</code></td>
<td>
<p> training inputs </p>
</td></tr>
<tr><td><code id="CVHTF_+3A_y">y</code></td>
<td>
<p> training output </p>
</td></tr>
<tr><td><code id="CVHTF_+3A_k">K</code></td>
<td>
<p> size of validation sample </p>
</td></tr>
<tr><td><code id="CVHTF_+3A_rep">REP</code></td>
<td>
<p> number of replications </p>
</td></tr>
<tr><td><code id="CVHTF_+3A_family">family</code></td>
<td>
<p>glm family</p>
</td></tr>
<tr><td><code id="CVHTF_+3A_...">...</code></td>
<td>
<p> optional arguments passed to <code>glm</code> or <code>lm</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>HTF (2009) describe K-fold cross-validation. 
The observations are partitioned into K non-overlapping subsets of approximately
equal size. Each subset is used as the validation sample while the remaining
K-1 subsets are used as training data. When <code class="reqn">K=n</code>, 
where n is the number of observations
the algorithm is equivalent to leave-one-out CV.
Normally <code class="reqn">K=10</code> or <code class="reqn">K=5</code> are used.
When <code class="reqn">K&lt;n-1</code>, their are may be many possible partitions and so the results 
of K-fold CV may vary somewhat depending on the partitions used.
In our implementation, random partitions are used and we allow for many
replications. Note that in the Shao's delete-d method, random samples are
used to select the valiation data whereas in this method the whole partition
is selected as random. This is acomplished using,
<code>fold &lt;- sample(rep(1:K,length=n))</code>. 
Then <code>fold</code> indicates each validation sample in the partition.
</p>


<h3>Value</h3>

<p>Vector of two components comprising the cross-validation MSE and its sd based 
on the MSE in each validation sample.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod and C. Xu</p>


<h3>References</h3>

<p>Hastie, T., Tibshirani, R. and Friedman, J. (2009). 
The Elements of Statistical Learning. 2nd Ed. Springer-Verlag.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+bestglm">bestglm</a></code>, 
<code><a href="#topic+CVd">CVd</a></code>, 
<code><a href="#topic+CVDH">CVDH</a></code>, 
<code><a href="#topic+LOOCV">LOOCV</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example 1. 10-fold CV
data(zprostate)
train&lt;-(zprostate[zprostate[,10],])[,-10]
X&lt;-train[,1:2]
y&lt;-train[,9]
CVHTF(X,y,K=10,REP=1)[1]
</code></pre>

<hr>
<h2 id='Detroit'>Detroit homicide data for 1961-73 used in the book Subset Regression by
A.J. Miller</h2><span id='topic+Detroit'></span>

<h3>Description</h3>

<p>For convenience we have labelled the input variables 1 through 11 to be 
consistent with the notation used in Miller (2002).
Only the first 11 variables were used in Miller's analyses.
The best fitting subset regression with these 11 variables, uses only 3 inputs 
and has a residual sum of squares of
6.77 while using forward selection produces a best fit with 3 inputs with 
residual sum of squares 21.19.  
Backward selection and stagewise methods produce similar results.
It is remarkable that there is such a big difference.
Note that the usual forward and backward selection algorithms may fail since
the linear regression using 11 variables gives essentially a perfect fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Detroit)</code></pre>


<h3>Format</h3>

<p>A data frame with 13 observations on the following 14 variables.
</p>

<dl>
<dt><code>FTP.1</code></dt><dd><p>Full-time police per 100,000 population</p>
</dd>
<dt><code>UEMP.2</code></dt><dd><p>Percent unemployed in the population</p>
</dd>
<dt><code>MAN.3</code></dt><dd><p>Number of manufacturing workers in thousands</p>
</dd>
<dt><code>LIC.4</code></dt><dd><p>Number of handgun licences per 100,000 population</p>
</dd>
<dt><code>GR.5</code></dt><dd><p>Number of handgun registrations per 100,000 population</p>
</dd>
<dt><code>CLEAR.6</code></dt><dd><p>Percent homicides cleared by arrests</p>
</dd>
<dt><code>WM.7</code></dt><dd><p>Number of white males in the population</p>
</dd>
<dt><code>NMAN.8</code></dt><dd><p>Number of non-manufacturing workers in thousands</p>
</dd>
<dt><code>GOV.9</code></dt><dd><p>Number of government workers in thousands</p>
</dd>
<dt><code>HE.10</code></dt><dd><p>Average hourly earnings</p>
</dd>
<dt><code>WE.11</code></dt><dd><p>Average weekly earnings</p>
</dd>
<dt><code>ACC</code></dt><dd><p>Death rate in accidents per 100,000 population</p>
</dd>
<dt><code>ASR</code></dt><dd><p>Number of assaults per 100,000 population</p>
</dd>
<dt><code>HOM</code></dt><dd><p>Number of homicides per 100,000 of population</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data were orginally collected and discussed by Fisher (1976) but
the complete dataset first appeared in Gunst and Mason (1980, Appendix A).
Miller (2002) discusses this dataset throughout his book.
The data were obtained from StatLib.
</p>


<h3>Source</h3>

<p><a href="http://lib.stat.cmu.edu/datasets/detroit">http://lib.stat.cmu.edu/datasets/detroit</a>
</p>


<h3>References</h3>

<p>Fisher, J.C. (1976).  Homicide in Detroit: The Role of Firearms. Criminology, 
vol.14, 387-400.
</p>
<p>Gunst, R.F. and Mason, R.L. (1980). 
Regression analysis and its application: A data-oriented approach. 
Marcel Dekker.
</p>
<p>Miller, A. J. (2002). Subset Selection in Regression. 2nd Ed. 
Chapman &amp; Hall/CRC. Boca Raton.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Detroit data example
data(Detroit)
#As in Miller (2002) columns 1-11 are used as inputs
p&lt;-11
#For possible comparison with other algorithms such as LARS
#  it is preferable to work with the scaled inputs.
#From Miller (2002, Table 3.14), we see that the
#best six inputs are: 1, 2, 4, 6, 7, 11
X&lt;-as.data.frame(scale(Detroit[,c(1,2,4,6,7,11)]))
y&lt;-Detroit[,ncol(Detroit)]
Xy&lt;-cbind(X,HOM=y)
#Use backward stepwise regression with BIC selects full model
out &lt;- lm(HOM~., data=Xy)
step(out, k=log(nrow(Xy)))
#
#Same story with exhaustive search algorithm
out&lt;-bestglm(Xy, IC="BIC")
out
#But many coefficients have p-values that are quite large considering
#  the selection bias. Note: 1, 6 and 7 are all about 5% only.
#We can use BICq to reduce the number of variables.
#The qTable let's choose q for other possible models,
out$qTable
#This suggest we try q=0.05 or q=0.0005 
bestglm(Xy,IC="BICq", t=0.05)
bestglm(Xy,IC="BICq", t=0.00005)
#It is interesting that the subset model of size 2 is not a subset
# itself of the size 3 model. These results agree with 
#Miller (2002, Table 3.14).
#
#Using delete-d CV with d=4 suggests variables 2,4,6,11
set.seed(1233211)
bestglm(Xy, IC="CV", CVArgs=list(Method="d", K=4, REP=50))
</code></pre>

<hr>
<h2 id='dgrid'>
Scaled Variables Dependency Plots: Output vs Inputs
</h2><span id='topic+dgrid'></span>

<h3>Description</h3>

<p>A lattice grid plot is produced for the output vs. each input.
The variables are scaled to have mean zero and variance one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dgrid(XyDF, span=0.8)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dgrid_+3A_xydf">XyDF</code></td>
<td>

<p>Must be a dataframe with the last column corresponding to the output
</p>
</td></tr>
<tr><td><code id="dgrid_+3A_span">span</code></td>
<td>

<p>smoothing parameter for loess
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a lattice plot
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+pairs">pairs</a></code>,
<code><a href="lattice.html#topic+splom">splom</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mcdonald)
dgrid(mcdonald)
</code></pre>

<hr>
<h2 id='Fires'>Forest fires in Montesinho natural park. Standardized inputs.</h2><span id='topic+Fires'></span>

<h3>Description</h3>

<p>The forest fire data were collected during January 2000 to December 2003 for 
fires in the 
Montesinho natural park located in the northeast region of Portugal.  
The response variable of interest was area burned in ha.  
When the area burned as less than one-tenth of a hectare, the response variable 
as set to zero.  
In all there were 517 fires and 247 of them recorded as zero.  
The region was divided into a 10-by-10 grid with coordinates X and Y running 
from 1 to 9.  
The categorical variable xyarea indicates the region in this grid for the fire.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Fires)</code></pre>


<h3>Format</h3>

<p>A data frame with 517 observations on the following 12 variables.
All quantitative variables have been standardized.
</p>

<dl>
<dt><code>xyarea</code></dt><dd><p>a factor with 36 levels </p>
</dd>
<dt><code>month</code></dt><dd><p>an ordered factor with 12 levels</p>
</dd>
<dt><code>day</code></dt><dd><p>an ordered factor with 7 levels </p>
</dd>
<dt><code>FFMC</code></dt><dd><p>fine fuel moisture code</p>
</dd>
<dt><code>DMC</code></dt><dd><p>Duff moisture code</p>
</dd>
<dt><code>DC</code></dt><dd><p>drought code</p>
</dd>
<dt><code>ISI</code></dt><dd><p>initial spread index</p>
</dd>
<dt><code>temp</code></dt><dd><p>average ambient temperature</p>
</dd>
<dt><code>RH</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>wind</code></dt><dd><p>wind speed</p>
</dd>
<dt><code>rain</code></dt><dd><p>rainfall</p>
</dd>
<dt><code>lburned</code></dt><dd><p>log(x+1), x is burned area with x=0 for small fires</p>
</dd>
</dl>



<h3>Details</h3>

<p>The original data may be found at the website below as well
as an analysis.
The  quantitative variables in this dataset have been standardized.
For convenience, the original data is provided in  
<code><a href="#topic+MontesinhoFires">MontesinhoFires</a></code>.
</p>


<h3>Source</h3>

<p><a href="http://archive.ics.uci.edu/ml/datasets/Forest+Fires">http://archive.ics.uci.edu/ml/datasets/Forest+Fires</a>
</p>


<h3>References</h3>

<p>P. Cortez and A. Morais, 2007. 
A Data Mining Approach to Predict Forest Fires using Meteorological Data. 
In J. Neves, M. F. Santos and J. Machado Eds., 
New Trends in Artificial Intelligence, 
Proceedings of the 13th EPIA 2007 - Portuguese Conference on 
Artificial Intelligence, December, Guimaraes, Portugal, pp. 512-523, 2007. 
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+MontesinhoFires">MontesinhoFires</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Fires)
names(Fires)
#ANOVA for xyarea is significant at 1.1%.
summary(aov(lburned~xyarea, data=Fires))
</code></pre>

<hr>
<h2 id='fitted.pcreg'>
Fitted values in PCR and PLS.
</h2><span id='topic+fitted.pcreg'></span>

<h3>Description</h3>

<p>The fitted values are returned given the output from <code>pcreg</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pcreg'
fitted(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fitted.pcreg_+3A_object">object</code></td>
<td>

<p><code>object</code> output
</p>
</td></tr>
<tr><td><code id="fitted.pcreg_+3A_...">...</code></td>
<td>

<p>additional parameters
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method function for pcreg.
</p>


<h3>Value</h3>

<p>residuals</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcreg">pcreg</a></code>,
<code><a href="#topic+residuals.pcreg">residuals.pcreg</a></code>,
<code><a href="#topic+plot.pcreg">plot.pcreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fitted(pcreg(mcdonald, scale=TRUE))
</code></pre>

<hr>
<h2 id='glmnetGridTable'>
Multipanel Display and Table Glmnet CV Output.
</h2><span id='topic+glmnetGridTable'></span>

<h3>Description</h3>

<p>Four panels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glmnetGridTable(XyList, alpha = 0, nfolds=10, family = "gaussian")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="glmnetGridTable_+3A_xylist">XyList</code></td>
<td>

<p>input
</p>
</td></tr>
<tr><td><code id="glmnetGridTable_+3A_alpha">alpha</code></td>
<td>

<p>elastic net parameter
</p>
</td></tr>
<tr><td><code id="glmnetGridTable_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of folds, K, in regularized K-fold CV, must be &gt;3 and &lt;=10.
</p>
</td></tr>
<tr><td><code id="glmnetGridTable_+3A_family">family</code></td>
<td>

<p>distribution
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>tba
</p>


<h3>Value</h3>

<p>plot produced by side-effect.
Table.
</p>


<h3>Note</h3>

<p>Set random seed beforehand if you want reproducibility.
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trainTestPartition">trainTestPartition</a></code>,
<code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code>,
<code><a href="glmnet.html#topic+glmnet">glmnet</a></code>,
<code><a href="glmnet.html#topic+predict.glmnet">predict.glmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(7733551)
out &lt;- trainTestPartition(mcdonald)
round(glmnetGridTable(out),4)
</code></pre>

<hr>
<h2 id='glmnetPredict'>Glmnet Prediction Using CVAV.</h2><span id='topic+glmnetPredict'></span>

<h3>Description</h3>

<p>Predict by averaging the predictions from cv.glmnet().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glmnetPredict(XyList, NREP = 15, alpha = 0, nfolds=10, 
 family = c("gaussian", "binomial", "poisson", "multinomial"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="glmnetPredict_+3A_xylist">XyList</code></td>
<td>

<p>list with components XyTr, XTr, yTr, XTe.
</p>
</td></tr>
<tr><td><code id="glmnetPredict_+3A_nrep">NREP</code></td>
<td>

<p>number of replications to use in average
</p>
</td></tr>
<tr><td><code id="glmnetPredict_+3A_alpha">alpha</code></td>
<td>

<p>elastic net parameter
</p>
</td></tr>
<tr><td><code id="glmnetPredict_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of folds, K, in regularized K-fold CV, must be &gt;3 and &lt;=10.
</p>
</td></tr>
<tr><td><code id="glmnetPredict_+3A_family">family</code></td>
<td>

<p>model
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with predictions
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trainTestPartition">trainTestPartition</a></code>,
<code><a href="#topic+glmnetGridTable">glmnetGridTable</a></code>,
<code><a href="glmnet.html#topic+glmnet">glmnet</a></code>,
<code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code>,
<code><a href="glmnet.html#topic+predict.glmnet">predict.glmnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(7733551)
out &lt;- trainTestPartition(mcdonald)
round(glmnetGridTable(out),4)
yh &lt;- glmnetPredict(out, NREP=5)
sqrt(mean((out$yTe - yh)^2))
</code></pre>

<hr>
<h2 id='grpregPredict'>
Predictions on Test Data with Grpreg
</h2><span id='topic+grpregPredict'></span>

<h3>Description</h3>

<p>A dataframe is partitioned randomly into training and test samples.
The function grpreg::grpreg() is used to fit the training data
using Lasso, SCAD and MCP penalty functions. The BIC criterion is used
to selecting the penalty parameter lambda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grpregPredict(Xy, trainFrac = 2/3, XyList=NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="grpregPredict_+3A_xy">Xy</code></td>
<td>

<p>a dataframe that may contain factor variables
</p>
</td></tr>
<tr><td><code id="grpregPredict_+3A_trainfrac">trainFrac</code></td>
<td>

<p>the fraction of data to be used for training
</p>
</td></tr>
<tr><td><code id="grpregPredict_+3A_xylist">XyList</code></td>
<td>

<p>instead of supplying Xy you can provide XyList.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of RMSEs
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glmnetPredict">glmnetPredict</a></code>,
<code><a href="#topic+glmnetGridTable">glmnetGridTable</a></code>,
<code><a href="#topic+trainTestPartition">trainTestPartition</a></code>,
<code><a href="grpreg.html#topic+grpreg">grpreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>grpregPredict(mcdonald)
</code></pre>

<hr>
<h2 id='hivif'>
Simulated Linear Regression (Train) with Nine Highly Correlated Inputs
</h2><span id='topic+hivif'></span>

<h3>Description</h3>

<p>The script that generated this data is given below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("hivif")</code></pre>


<h3>Format</h3>

<p>A data frame with 1000 observations on the following 10 variables.
</p>

<dl>
<dt><code>x1</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x2</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x3</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x4</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x5</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x6</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x7</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x8</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x9</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>y</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>#Simple example
data(hivif)
lm(y ~ ., data=hivif)
#
#This example shows how the original data was simulated and
#how additional test data may be simulated.
## Not run: 
 set.seed(778851) #needed for original training data
 n &lt;- 100
 p &lt;- 9 #9 covariates plus intercept
 sig &lt;- toeplitz(0.9^(0:(p-1)))
 X &lt;- MASS::mvrnorm(n=n, rep(0, p), Sigma=sig)
 colnames(X) &lt;- paste0("x", 1:p)
 b &lt;- c(0,-0.3,0,0,-0.3,0,0,0.3,0.3) #
 names(b) &lt;- paste0("x", 1:p)
 y &lt;- 1 +  X
 Xy &lt;- cbind(as.data.frame.matrix(X), y=y) #=hivif
#Test data
 nTe &lt;- 10^3
 XTe &lt;- MASS::mvrnorm(n=nTe, rep(0, p), Sigma=sig)
 colnames(XTe) &lt;- paste0("x", 1:p)
  yTe &lt;- 1 +  XTe
 XyTe &lt;- cbind(as.data.frame.matrix(XTe), y=yTe) #test data
 ans &lt;- lm(y ~ ., data=Xy) #fit training data
 mean((XyTe$y - predict(ans, newdata=XyTe))^2) #MSE on test data
 
## End(Not run)
</code></pre>

<hr>
<h2 id='Iowa'>
Iowa School Test
</h2><span id='topic+Iowa'></span>

<h3>Description</h3>

<p>Dataset on poverty and academic performance. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Iowa")</code></pre>


<h3>Format</h3>

<p>A data frame with 133 observations on the following 3 variables.
</p>

<dl>
<dt><code>City</code></dt><dd><p>a factor with 6 levels <code>Cedar Rapids</code> <code>Davenport</code>
<code>Des Moines</code> <code>Iowa City</code> <code>Sioux City</code> <code>Waterloo</code></p>
</dd>
<dt><code>Poverty</code></dt><dd><p>percentage subsidized</p>
</dd>
<dt><code>Test</code></dt><dd><p>achievement test score</p>
</dd>
</dl>



<h3>Details</h3>

<p>There are n=133 average test scores for schools in the K=6 largest cities. 
The test score offers a standardized measure of academic achievement. 
The purpose of the study is to investigate if there is a relationship between 
academic achievement, as measured by the test, and poverty. 
It is expected that students from economically disadvantaged backgrounds 
will do less well. Data on the average income in the school district was not 
available so a proxy variable for poverty was used. 
The percentage of students who received subsidized meals was available so 
this was used as the &quot;Poverty&quot; variable.
</p>


<h3>Source</h3>

<p>Abraham and Ledholter, Introduction to Regression, Wiley.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Iowa)
table(Iowa$City)
</code></pre>

<hr>
<h2 id='LOOCV'> Leave-one-out cross-validation </h2><span id='topic+LOOCV'></span>

<h3>Description</h3>

<p>An observation is removed and the model is fit the the remaining data and this 
fit used to predict the value of the deleted observation.
This is repeated, n times, for each of the n observations and the mean square 
error is computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LOOCV(X, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="LOOCV_+3A_x">X</code></td>
<td>
<p> training inputs </p>
</td></tr>
<tr><td><code id="LOOCV_+3A_y">y</code></td>
<td>
<p> training output </p>
</td></tr>
</table>


<h3>Details</h3>

<p>LOOCV for linear regression is exactly equivalent to the PRESS method
suggested by Allen (1971) who also provided an efficient algorithm.
</p>


<h3>Value</h3>

<p>Vector of two components comprising the cross-validation MSE and its sd based 
on the MSE in each validation sample.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod and C. Xu</p>


<h3>References</h3>

<p>Hastie, T., Tibshirani, R. and Friedman, J. (2009). 
The Elements of Statistical Learning. 2nd Ed.
</p>
<p>Allen, D.M. (1971). Mean Square Error of Prediction as a Criterion 
for Selecting Variables. Technometrics, 13, 469 -475. 
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+bestglm">bestglm</a></code>, 
<code><a href="#topic+CVd">CVd</a></code>, 
<code><a href="#topic+CVDH">CVDH</a></code>, 
<code><a href="#topic+CVHTF">CVHTF</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example. Compare LOO CV with K-fold CV.
#Find CV MSE's for LOOCV and compare with K=5, 10, 20, 40, 50, 60
#Takes about 30 sec
## Not run: 
 data(zprostate)
 train&lt;-(zprostate[zprostate[,10],])[,-10]
 X&lt;-train[,1:2]
 y&lt;-train[,9]
 CVLOO&lt;-LOOCV(X,y)
 KS&lt;-c(5,10,20,40,50,60)
 nKS&lt;-length(KS)
 cvs&lt;-numeric(nKS)
 set.seed(1233211231)
 for (iK in 1:nKS)
    cvs[iK]&lt;-CVDH(X,y,K=KS[iK],REP=10)[1]
 boxplot(cvs)
 abline(h=CVLOO, lwd=3, col="red")
 title(sub="Boxplot of CV's with K=5,10,20,40,50,60 and LOO CV in red")
 
## End(Not run)
</code></pre>

<hr>
<h2 id='manpower'>Hospital manpower data</h2><span id='topic+manpower'></span>

<h3>Description</h3>

<p>The goal of this study is to predict the manpower requirement as given in 
the output 
variable Hours given the five other input variables.
Data is from Table 3.8 of Myers (1990). See also Examples 3.8, 4.5, 8.8.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(manpower)</code></pre>


<h3>Format</h3>

<p>A data frame with 17 observations. The output variable is Hours and the
inputs are Load, Xray, BedDays, AreaPop and Stay. The site 1 through 17 
is indicated by the row name.
</p>

<dl>
<dt><code>Load</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>Xray</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>BedDays</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>AreaPop</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>Stay</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>Hours</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p>This data illustrates the multicollinearity problem and the use of VIF to 
identify it. It provides an illustrative example for ridge regression and 
more modern methods such as lasso and lars.
</p>


<h3>Source</h3>

<p>Myers (1990) indicates the source was 
&quot;Procedures and Analysis for Staffing Standards Development: 
Data/Regression Analysis Handbook&quot;, 
Navy Manpower and Material Analysis
Center, San Diego, 1979. 
</p>


<h3>References</h3>

<p>Myers, R. (1990). Classical and Modern Regression with Applications. 
The Duxbury Advanced Series in Statistics and Decision Sciences. 
Boston: PWS-KENT Publishing Company.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(manpower)
</code></pre>

<hr>
<h2 id='mcdonald'>
Pollution dataset from McDonald and Schwing (1973)
</h2><span id='topic+mcdonald'></span>

<h3>Description</h3>

<p>Regression data used to illustrate ridge regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("mcdonald")</code></pre>


<h3>Format</h3>

<p>A data frame with 60 observations on the following 16 variables.
</p>

<dl>
<dt><code>PREC</code></dt><dd><p>Average annual precipitation in inches</p>
</dd>	
<dt><code>JANT</code></dt><dd><p>Average January temperature in degrees F</p>
</dd>	
<dt><code>JULT</code></dt><dd><p>Same for July</p>
</dd>	
<dt><code>OVR65</code></dt><dd><p>Percent of 1960 SMSA population aged 65 or older</p>
</dd>	
<dt><code>POPN</code></dt><dd><p>Average household size</p>
</dd> 	
<dt><code>EDUC</code></dt><dd><p>Median school years completed by those over 22</p>
</dd>	
<dt><code>HOUS</code></dt><dd><p>Percent of housing units which are sound &amp; with all facilities</p>
</dd>	
<dt><code>DENS</code></dt><dd><p>Population per sq. mile in urbanized areas, 1960</p>
</dd>	
<dt><code>NONW</code></dt><dd><p>Percent non-white population in urbanized areas, 1960</p>
</dd>	
<dt><code>WWDRK</code></dt><dd><p>Percent employed in white collar occupations</p>
</dd>	
<dt><code>POOR</code></dt><dd><p>Percent of families with income &lt; $3000</p>
</dd>	
<dt><code>HC</code></dt><dd><p>Relative hydrocarbon pollution potential</p>
</dd>	
<dt><code>NOX</code></dt><dd><p>Same for nitric oxides</p>
</dd>	
<dt><code>SOx</code></dt><dd><p>Same for sulphur dioxide</p>
</dd>
<dt><code>HUMID</code></dt><dd><p>Annual average percent relative humidity at 1pm</p>
</dd>	
<dt><code>MORT</code></dt><dd><p>Total age-adjusted mortality rate per 100,000</p>
</dd>	  
</dl>



<h3>Details</h3>

<p>Ridge regression example
</p>


<h3>Source</h3>

<p>Gary C. McDonald and Richard C. Schwing (1973),
Instabilities of Regression Estimates Relating Air Pollution to Mortality,
Technometrics 15/3, 463-481.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mcdonald)
vifx(mcdonald[, -ncol(mcdonald)])
</code></pre>

<hr>
<h2 id='MontesinhoFires'>Forest fires in Montesinho natural park</h2><span id='topic+MontesinhoFires'></span>

<h3>Description</h3>

<p>The forest fire data were collected during January 2000 to December 2003 
for fires in the 
Montesinho natural park located in the northeast region of Portugal.  
The response variable of interest was area burned in ha.  
When the area burned as less than one-tenth of a hectare, the response variable 
as set to zero.  
In all there were 517 fires and 247 of them recorded as zero.  
The region was divided into a 10-by-10 grid with coordinates X and Y 
running from 1 to 9.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(MontesinhoFires)</code></pre>


<h3>Format</h3>

<p>A data frame with 517 observations on the following 13 variables.
</p>

<dl>
<dt><code>X</code></dt><dd><p>X coordinate for region, 0-10</p>
</dd>
<dt><code>Y</code></dt><dd><p>X coordinate for region, 0-10</p>
</dd>
<dt><code>month</code></dt><dd><p>an ordered factor with 12 levels</p>
</dd>
<dt><code>day</code></dt><dd><p>an ordered factor with 7 levels </p>
</dd>
<dt><code>FFMC</code></dt><dd><p>fine fuel moisture code</p>
</dd>
<dt><code>DMC</code></dt><dd><p>Duff moisture code</p>
</dd>
<dt><code>DC</code></dt><dd><p>drought code</p>
</dd>
<dt><code>ISI</code></dt><dd><p>initial spread index</p>
</dd>
<dt><code>temp</code></dt><dd><p>average ambient temperature</p>
</dd>
<dt><code>RH</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>wind</code></dt><dd><p>wind speed</p>
</dd>
<dt><code>rain</code></dt><dd><p>rainfall</p>
</dd>
<dt><code>burned</code></dt><dd><p>area burned in hectares</p>
</dd>
</dl>



<h3>Details</h3>

<p>This is the original data taken from the website below.
</p>


<h3>Source</h3>

<p><a href="http://archive.ics.uci.edu/ml/datasets/Forest+Fires">http://archive.ics.uci.edu/ml/datasets/Forest+Fires</a>
</p>


<h3>References</h3>

<p>P. Cortez and A. Morais, 2007. 
A Data Mining Approach to Predict Forest Fires using Meteorological Data. 
In J. Neves, M. F. Santos and J. Machado Eds., 
New Trends in Artificial Intelligence, 
Proceedings of the 13th EPIA 2007 - 
Portuguese Conference on Artificial Intelligence, 
December, Guimaraes, Portugal, pp. 512-523, 2007. 
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+Fires">Fires</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MontesinhoFires)
names(MontesinhoFires)
data(Fires)
names(Fires)
#Anova for month
summary(aov(burned~month, data=MontesinhoFires))
</code></pre>

<hr>
<h2 id='NNPredict'>
Nearest Neighbour Regression Prediction
</h2><span id='topic+NNPredict'></span>

<h3>Description</h3>

<p>Given training/test data in the predictions on the test data computed.
L1, L2 and correlation distances may be used.
The data is sphered prior to making the NN predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NNPredict(XyList, dist = c("L2", "COR", "L1"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NNPredict_+3A_xylist">XyList</code></td>
<td>

<p>list with six elements
</p>
</td></tr>
<tr><td><code id="NNPredict_+3A_dist">dist</code></td>
<td>

<p>distance used
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of predictions
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sphereX">sphereX</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>AQ &lt;- airquality[complete.cases(airquality),c(2,3,4,1)]
XyList &lt;- trainTestPartition(AQ)
NNPredict(XyList) 
</code></pre>

<hr>
<h2 id='oneSDRule'> Utility function. Implements the 1-sd rule. </h2><span id='topic+oneSDRule'></span>

<h3>Description</h3>

<p>The CV and its standard devation are provided for a range of models
ordered by the number of parameters estimated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oneSDRule(CVout)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="oneSDRule_+3A_cvout">CVout</code></td>
<td>
<p> A matrix with two columns. First column is the CV and second, 
its sd. Row ordering is from fewest parameter to most. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>The row corresponding to the best model.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod and C. Xu</p>


<h3>References</h3>

 
<p>Hastie, T., Tibshirani, R. and Friedman, J. (2009). 
The Elements of Statistical Learning. 2nd Ed. Springer-Verlag.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>CV&lt;-c(1.4637799,0.7036285,0.6242480,0.6069406,0.6006877,0.6005472,0.5707958,
      0.5907897,0.5895489)
CVsd&lt;-c(0.24878992,0.14160499,0.08714908,0.11376041,0.08522291,
 0.11897327,0.07960879,0.09235052,0.12860983)
CVout &lt;- matrix(c(CV,CVsd), ncol=2)
oneSDRule(CVout)
</code></pre>

<hr>
<h2 id='pcreg'>
Principal Component and Partial Least Squares Regression
</h2><span id='topic+pcreg'></span>

<h3>Description</h3>

<p>Regression using the principal components or latent variables as inputs.
The best model is selected using
components 1, 2, ..., r, where r, the number of components to use
is determined by the AIC or BIC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcreg(Xy, scale = TRUE, method = c("PC", "LV"), ic = c("BIC", "AIC"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pcreg_+3A_xy">Xy</code></td>
<td>

<p>dataframe with variable names in columns
</p>
</td></tr>
<tr><td><code id="pcreg_+3A_scale">scale</code></td>
<td>

<p>Whether or not to scale. Default is TRUE.
</p>
</td></tr>
<tr><td><code id="pcreg_+3A_method">method</code></td>
<td>

<p>either principal components, &quot;PC&quot;, or partial least squares latent variables,
&quot;LV&quot;
</p>
</td></tr>
<tr><td><code id="pcreg_+3A_ic">ic</code></td>
<td>

<p>&quot;BIC&quot; or &quot;AIC&quot;
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An S3 class list &quot;pcreg&quot; with components
</p>
<table role = "presentation">
<tr><td><code>lmfit</code></td>
<td>
<p>lm model</p>
</td></tr>
<tr><td><code>PLSFit</code></td>
<td>
<p>column sd</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>matrix of principal components or latent vector</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>'pcr' or 'pls'</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.pcreg">predict.pcreg</a></code>,
<code><a href="#topic+summary.pcreg">summary.pcreg</a></code>,
<code><a href="#topic+plot.pcreg">plot.pcreg</a></code>,
<code><a href="#topic+fitted.pcreg">fitted.pcreg</a></code>,
<code><a href="#topic+residuals.pcreg">residuals.pcreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pcreg(mcdonald, scale=TRUE, method="PC")
pcreg(mcdonald, scale=TRUE, method="LV")
</code></pre>

<hr>
<h2 id='plot.pcreg'>
Diagnostic plots for PCR and PLS
</h2><span id='topic+plot.pcreg'></span>

<h3>Description</h3>

<p>Diagnostic plots available with lm-objects are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pcreg'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.pcreg_+3A_x">x</code></td>
<td>

<p><code>x</code> output from pcreg(). It has S3 class 'pcreg'.
</p>
</td></tr>
<tr><td><code id="plot.pcreg_+3A_...">...</code></td>
<td>

<p>additional parameters
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See plot method for S3 class 'lm'.
</p>


<h3>Value</h3>

<p>Nothing. The plot is produced.
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcreg">pcreg</a></code>,
<code><a href="#topic+fitted.pcreg">fitted.pcreg</a></code>,
<code><a href="#topic+residuals.pcreg">residuals.pcreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ans &lt;- pcreg(mcdonald, scale=TRUE)
plot(ans)
</code></pre>

<hr>
<h2 id='plot1SDRule'>
Plot Regularization Path and One Standard Deviation Rule
</h2><span id='topic+plot1SDRule'></span>

<h3>Description</h3>

<p>Takes input either matrix with 2 columns or output from caret::train()
and produces a plot showing the best model selected using the 1 SD rule.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot1SDRule(ans, main = "", sub = "", xlab = "df", ylab = "EPE")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot1SDRule_+3A_ans">ans</code></td>
<td>

<p>matrix or output from train
</p>
</td></tr>
<tr><td><code id="plot1SDRule_+3A_main">main</code></td>
<td>

<p>optional plot title
</p>
</td></tr>
<tr><td><code id="plot1SDRule_+3A_sub">sub</code></td>
<td>

<p>optional plot subtitle
</p>
</td></tr>
<tr><td><code id="plot1SDRule_+3A_xlab">xlab</code></td>
<td>

<p>optional x-axis label
</p>
</td></tr>
<tr><td><code id="plot1SDRule_+3A_ylab">ylab</code></td>
<td>

<p>optional y-axis label
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>tuning parameter value for best model
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>References</h3>

<p>Hastie, Tibsharani and Friedman, &quot;Elements of Statistical Learning&quot;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+oneSDRule">oneSDRule</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>CV&lt;-c(1.4637799,0.7036285,0.6242480,0.6069406,0.6006877,0.6005472,0.5707958,
      0.5907897,0.5895489)
CVsd&lt;-c(0.24878992,0.14160499,0.08714908,0.11376041,0.08522291,
 0.11897327,0.07960879,0.09235052,0.12860983)
CVout &lt;- matrix(c(CV,CVsd), ncol=2)
oneSDRule(CVout)
</code></pre>

<hr>
<h2 id='predict.pcreg'>
Predict Method for Pcreg.
</h2><span id='topic+predict.pcreg'></span>

<h3>Description</h3>

<p>Prediction for models fit using <code>pcreg()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pcreg'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.pcreg_+3A_object">object</code></td>
<td>

<p>the S3 class object produced as output from the function pcreg()
</p>
</td></tr>
<tr><td><code id="predict.pcreg_+3A_newdata">newdata</code></td>
<td>

<p>dataframe with new data and with same column names as used in the original 
argument to pcreg.
</p>
</td></tr>
<tr><td><code id="predict.pcreg_+3A_...">...</code></td>
<td>

<p>additional arguments
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prediction method, <code>predict.mvr()</code>, which is available in the pls 
package is used.
We take advantage of this since it avoids fussing with scaling issues 
since it is automatically handled
for us by <code>predict.mvr()</code>
</p>


<h3>Value</h3>

<p>the predicted values
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.pcreg">predict.pcreg</a></code>,
<code><a href="#topic+summary.pcreg">summary.pcreg</a></code>,
<code><a href="#topic+plot.pcreg">plot.pcreg</a></code>,
<code><a href="#topic+fitted.pcreg">fitted.pcreg</a></code>,
<code><a href="#topic+residuals.pcreg">residuals.pcreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>XyList &lt;- trainTestPartition(mcdonald)
XyTr &lt;- XyList$XyTr
XyTe &lt;- XyList$XyTe
ans &lt;- pcreg(XyTr, scale=TRUE)
predict(ans, newdata=XyTe)
</code></pre>

<hr>
<h2 id='print.bestglm'>Print method for 'bestglm' object </h2><span id='topic+print.bestglm'></span>

<h3>Description</h3>

<p>A brief description of the best fit is given.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bestglm'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.bestglm_+3A_x">x</code></td>
<td>
<p> Output from the bestglm function </p>
</td></tr>
<tr><td><code id="print.bestglm_+3A_...">...</code></td>
<td>
<p> optional arguments </p>
</td></tr>
</table>


<h3>Value</h3>

<p>No value. Output to terminal only.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod and C. Xu</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+bestglm">bestglm</a></code>,
<code><a href="#topic+summary.bestglm">summary.bestglm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(znuclear)
bestglm(znuclear)
</code></pre>

<hr>
<h2 id='print.pcreg'>Print method for 'pcreg' object </h2><span id='topic+print.pcreg'></span>

<h3>Description</h3>

<p>A brief description of the best fit is given.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pcreg'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.pcreg_+3A_x">x</code></td>
<td>
<p> Output from the pcreg function </p>
</td></tr>
<tr><td><code id="print.pcreg_+3A_...">...</code></td>
<td>
<p> optional arguments </p>
</td></tr>
</table>


<h3>Value</h3>

<p>No value. Output to terminal only.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod and C. Xu</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+pcreg">pcreg</a></code>,
<code><a href="#topic+summary.pcreg">summary.pcreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pcreg(znuclear, scale=TRUE)
</code></pre>

<hr>
<h2 id='residuals.pcreg'>
Residuals Fitted PCR or PLS  
</h2><span id='topic+residuals.pcreg'></span><span id='topic+resid.pcreg'></span>

<h3>Description</h3>

<p>The residuals from a model fitted using pcreg are returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pcreg'
residuals(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="residuals.pcreg_+3A_object">object</code></td>
<td>

<p><code>object</code> output
</p>
</td></tr>
<tr><td><code id="residuals.pcreg_+3A_...">...</code></td>
<td>

<p>additional parameters
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method function for pcreg.
</p>


<h3>Value</h3>

<p>residuals</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcreg">pcreg</a></code>,
<code><a href="stats.html#topic+fitted">fitted</a></code>,
<code><a href="base.html#topic+plot">plot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>resid(pcreg(mcdonald, scale=TRUE))
</code></pre>

<hr>
<h2 id='rubber'>
Abrasion loss for various hardness and tensile strength
</h2><span id='topic+rubber'></span>

<h3>Description</h3>

<p>The data come from an experiment to investigate how the resistance of rubber 
to abrasion is affected by the hardness of the rubber and its tensile strength.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(rubber)</code></pre>


<h3>Format</h3>

<p>A data frame with 30 observations on the following 3 variables.
</p>

<dl>
<dt><code>hardness</code></dt><dd><p>hardness in degree Shore</p>
</dd>
<dt><code>tensile.strength</code></dt><dd><p>tensile strength in kg per square meter</p>
</dd>
<dt><code>abrasion.loss</code></dt><dd><p>abrasion loss in gram per hour</p>
</dd>
<dt><code>ts.low</code></dt><dd><p>tensile strength minus the breakpoint 180 km/m^2</p>
</dd>
<dt><code>ts.high</code></dt><dd><p>tensile strength minus the breakpoint 180 km/m^2</p>
</dd>
</dl>



<h3>Source</h3>

<p>Hand, D.J., Daly, F., Lunn, A.D., McConway, K.J. and Ostrowski, E. (1993). 
A Handbook of Small Datasets. Chapman and Hall.	
</p>


<h3>References</h3>

<p>Cleveland, W. S. (1993). Visualizing data. Hobart Press, Summit: New Jersey. 
</p>
<p>Davies, O.L. and Goldsmith, P.L.(1972) 
Statistical methods in Research and Production.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(rubber)
ans &lt;- lm(abrasion.loss~hardness+tensile.strength, data=rubber)
</code></pre>

<hr>
<h2 id='SAheart'> South African Hearth Disease Data   </h2><span id='topic+SAheart'></span>

<h3>Description</h3>

<p>A retrospective sample of males in a heart-disease high-risk region
of the Western Cape, South Africa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(SAheart)</code></pre>


<h3>Format</h3>

<p>A data frame with 462 observations on the following 10 variables.
</p>

<dl>
<dt>sbp</dt><dd><p>systolic blood pressure</p>
</dd>
<dt>tobacco</dt><dd><p>cumulative tobacco (kg)</p>
</dd>
<dt>ldl</dt><dd><p>low density lipoprotein cholesterol</p>
</dd>
<dt>adiposity</dt><dd><p>a numeric vector</p>
</dd>
<dt>famhist</dt><dd><p>family history of heart disease, a factor with levels 
<code>Absent</code> <code>Present</code></p>
</dd>
<dt>typea</dt><dd><p>type-A behavior</p>
</dd>
<dt>obesity</dt><dd><p>a numeric vector</p>
</dd>
<dt>alcohol</dt><dd><p>current alcohol consumption</p>
</dd>
<dt>age</dt><dd><p>age at onset</p>
</dd>
<dt>chd</dt><dd><p>response, coronary heart disease</p>
</dd>
</dl>



<h3>Details</h3>

<p>A retrospective sample of males in a heart-disease high-risk region
of the Western Cape, South Africa. There are roughly two controls per
case of CHD. Many of the CHD positive men have undergone blood
pressure reduction treatment and other programs to reduce their risk
factors after their CHD event. In some cases the measurements were
made after these treatments. These data are taken from a larger
dataset, described in  Rousseauw et al, 1983, South African Medical
Journal. 
</p>


<h3>Source</h3>

<p> Rousseauw, J., du Plessis, J., Benade, A., Jordaan, P., Kotze,
J. and Ferreira, J. (1983). Coronary risk factor screening in three
rural communities, South African Medical Journal 64: 430&ndash;436.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(SAheart)
str(SAheart) 
summary(SAheart)
</code></pre>

<hr>
<h2 id='Shao'> Simulated Regression Data</h2><span id='topic+Shao'></span>

<h3>Description</h3>

<p>Data a simulation study reported by Shao (1993, Table 1).
The linear regression model 
Shao (1993, Table 2) reported 4 simulation experiments using
4 different values for the regression coefficients:
</p>
<p style="text-align: center;"><code class="reqn">y = 2 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_5 + e,</code>
</p>

<p>where <code class="reqn">e</code> is an independent normal error with unit variance.
</p>
<p>The four regression coefficients for the four experiments
are shown in the table below,
</p>

<table>
<tr>
 <td style="text-align: center;">
Experiment 	</td><td style="text-align: right;"> <code class="reqn">\beta_2</code> 
		</td><td style="text-align: right;"> <code class="reqn">\beta_3</code>
		</td><td style="text-align: right;"> <code class="reqn">\beta_4</code>
		</td><td style="text-align: right;"> <code class="reqn">\beta_5</code></td>
</tr>
<tr>
 <td style="text-align: center;"> 
1</td><td style="text-align: right;">	0</td><td style="text-align: right;"> 0</td><td style="text-align: right;"> 4</td><td style="text-align: right;"> 0</td>
</tr>
<tr>
 <td style="text-align: center;"> 
2</td><td style="text-align: right;">	0</td><td style="text-align: right;"> 0</td><td style="text-align: right;"> 4</td><td style="text-align: right;"> 8</td>
</tr>
<tr>
 <td style="text-align: center;"> 
3</td><td style="text-align: right;">	9</td><td style="text-align: right;"> 0</td><td style="text-align: right;"> 4</td><td style="text-align: right;"> 8</td>
</tr>
<tr>
 <td style="text-align: center;"> 
4</td><td style="text-align: right;">	9</td><td style="text-align: right;"> 6</td><td style="text-align: right;"> 4</td><td style="text-align: right;"> 8</td>
</tr>

</table>

<p>The table below summarizes the probability of correct model selection
in the experiment reported by Shao (1993, Table 2).
Three model selection methods are compared: LOOCV (leave-one-out CV),
CV(d=25) or the delete-d method with d=25 and APCV which is
a very efficient computation CV method but specialized to the
case of linear regression.
</p>

<table>
<tr>
 <td style="text-align: right;">                           
Experiment </td><td style="text-align: left;"> LOOCV </td><td style="text-align: left;"> CV(d=25) </td><td style="text-align: left;"> APCV</td>
</tr>
<tr>
 <td style="text-align: right;">  
1	</td><td style="text-align: left;"> 0.484 </td><td style="text-align: left;"> 0.934  </td><td style="text-align: left;"> 0.501</td>
</tr>
<tr>
 <td style="text-align: right;"> 
2	</td><td style="text-align: left;"> 0.641 </td><td style="text-align: left;"> 0.947  </td><td style="text-align: left;"> 0.651</td>
</tr>
<tr>
 <td style="text-align: right;"> 
3	</td><td style="text-align: left;"> 0.801 </td><td style="text-align: left;"> 0.965  </td><td style="text-align: left;"> 0.818</td>
</tr>
<tr>
 <td style="text-align: right;"> 
4	</td><td style="text-align: left;"> 0.985 </td><td style="text-align: left;"> 0.948  </td><td style="text-align: left;"> 0.999        
</td>
</tr>

</table>

<p>The CV(d=25) outperforms LOOCV in all cases and it also outforms APCV
by a large margin in Experiments 1, 2 and 3 but in case 4 APCV
is slightly better. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Shao)</code></pre>


<h3>Format</h3>

<p>A data frame with 40 observations on the following 4 inputs.
</p>

<dl>
<dt><code>x2</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x3</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x4</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>x5</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Source</h3>

<p>Shao, Jun (1993). Linear Model Selection by Cross-Validation. 
Journal of the American Statistical Assocation 88, 486-494.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#In this example BICq(q=0.25) selects the correct model but BIC does not
data(Shao)
X&lt;-as.matrix.data.frame(Shao)
b&lt;-c(0,0,4,0)
set.seed(123321123)
#Note: matrix multiplication must be escaped in Rd file
y&lt;-X%*%b+rnorm(40)
Xy&lt;-data.frame(Shao, y=y)
bestglm(Xy)
bestglm(Xy, IC="BICq")

</code></pre>

<hr>
<h2 id='sphereX'>
Sphere Data Matrix
</h2><span id='topic+sphereX'></span>

<h3>Description</h3>

<p>The data matrix is scaled and sphered so it is orthonormal.
The Cholesky decomposition is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sphereX(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sphereX_+3A_x">X</code></td>
<td>

<p><code>X</code> rectangular data matrix
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>sphered matrix
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+scale">scale</a></code>, 
<code><a href="#topic+NNPredict">NNPredict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(longley)
longley.x &lt;- data.matrix(longley[, 1:6])
sphereX(longley.x)
</code></pre>

<hr>
<h2 id='summary.bestglm'> summary of  'bestglm' object </h2><span id='topic+summary.bestglm'></span>

<h3>Description</h3>

<p>An analysis of deviance and a likelihood-ratio test
with p-value. 
The p-value is greatly exagerated due to selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bestglm'
summary(object, SubsetsQ=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.bestglm_+3A_object">object</code></td>
<td>
<p> Output from the bestglm function </p>
</td></tr>
<tr><td><code id="summary.bestglm_+3A_subsetsq">SubsetsQ</code></td>
<td>
<p> List best subsets of each size</p>
</td></tr>
<tr><td><code id="summary.bestglm_+3A_...">...</code></td>
<td>
<p> optional arguments </p>
</td></tr>
</table>


<h3>Value</h3>

<p>No value. Output to terminal only.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod and C. Xu</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+bestglm">bestglm</a></code>,
<code><a href="#topic+print.bestglm">print.bestglm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(znuclear)
summary(bestglm(znuclear))
#
#find statistical signficance of overall regression
data(Fires)
summary(bestglm(Fires, IC="BICq", t=1))
</code></pre>

<hr>
<h2 id='summary.pcreg'>
Summary Method for Pcreg.
</h2><span id='topic+summary.pcreg'></span>

<h3>Description</h3>

<p>The summary is based on the summary method for S3 class 'lm'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pcreg'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.pcreg_+3A_object">object</code></td>
<td>

<p><code>object</code> output
</p>
</td></tr>
<tr><td><code id="summary.pcreg_+3A_...">...</code></td>
<td>

<p>additional parameters
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method function for pcreg.
</p>


<h3>Value</h3>

<p>residuals</p>


<h3>Note</h3>

<p>The standard errors and p-values are wrong due to selection bias.
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcreg">pcreg</a></code>,
<code><a href="stats.html#topic+fitted">fitted</a></code>,
<code><a href="base.html#topic+plot">plot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>resid(pcreg(mcdonald, scale=TRUE))
</code></pre>

<hr>
<h2 id='trainTestPartition'>
Partition Dataframe into Train/Test Samples
</h2><span id='topic+trainTestPartition'></span>

<h3>Description</h3>

<p>Dataframe used to create training and test datasets using
specified fraction for the training sample.
The data matrix must be comprised of continuous variables
only (no factors).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainTestPartition(Xy, trainFrac = 2/3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainTestPartition_+3A_xy">Xy</code></td>
<td>

<p>Dataframe with column names, last column is the response
variable and others are the regression input variables.
The data matrix must be comprised of continuous variables
only (no factors).
</p>
</td></tr>
<tr><td><code id="trainTestPartition_+3A_trainfrac">trainFrac</code></td>
<td>

<p>Fraction to be used for the training sample.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table role = "presentation">
<tr><td><code>XyTr</code></td>
<td>
<p>Training dataframe.</p>
</td></tr>
<tr><td><code>XTr</code></td>
<td>
<p>Matrix, input training variables.</p>
</td></tr>
<tr><td><code>yTr</code></td>
<td>
<p>Vector, output training variable.</p>
</td></tr>
<tr><td><code>XyTe</code></td>
<td>
<p>Training dataframe.</p>
</td></tr>
<tr><td><code>XTe</code></td>
<td>
<p>Matrix, input test variables.</p>
</td></tr>
<tr><td><code>yTe</code></td>
<td>
<p>Vector, output test variable.</p>
</td></tr>
<tr><td><code>XyTr</code></td>
<td>
<p>Training dataframe.</p>
</td></tr>
<tr><td><code>XyTr</code></td>
<td>
<p>Training dataframe.</p>
</td></tr>
<tr><td><code>XyTr</code></td>
<td>
<p>Training dataframe.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(7733551)
out &lt;- trainTestPartition(mcdonald)
round(glmnetGridTable(out),4)
</code></pre>

<hr>
<h2 id='vifx'>
Variance Inflation Factor for a Design Matrix
</h2><span id='topic+vifx'></span>

<h3>Description</h3>

<p>Barplot of the VIF is produced
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vifx(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vifx_+3A_x">X</code></td>
<td>

<p>A design matrix
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The VIF are the diagonal elements in the inverse 
<code class="reqn">t(X*) X*</code>, where X* is the rescaled design matrix.
</p>


<h3>Value</h3>

<p>vector with VIF's
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>References</h3>

<p>Marquardt, D. W. (1970). 
Generalized Inverses, Ridge Regression, Biased Linear Estimation, and Nonlinear 
Estimation. Technometrics 12(3), 591-612. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mcdonald)
vifx(mcdonald[, -ncol(mcdonald)])
</code></pre>

<hr>
<h2 id='znuclear'> Nuclear plant data. Quantitative inputs logged and standardized.</h2><span id='topic+znuclear'></span>

<h3>Description</h3>

<p>Data on 32 nuclear power plants.
The response variable is cost and there are ten covariates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(znuclear)</code></pre>


<h3>Format</h3>

<p>A data frame with 32 observations on the following 12 variables. 
All quantitative variables, except date, have been logged and standardized 
to have mean 0 and variance 1.
</p>

<dl>
<dt><code>date</code></dt><dd><p>Quantitative covariate. The date on which the 
construction permit was issued. The data are measured in years since 
January 1 1990 to the nearest month.</p>
</dd>
<dt><code>T1</code></dt><dd><p>Quantitative covariate. The time between application for 
and issue of the construction permit.</p>
</dd>
<dt><code>T2</code></dt><dd><p>Quantitative covariate. The time between issue of 
operating license and construction permit.</p>
</dd>
<dt><code>capacity</code></dt><dd><p>Quantitative covariate. The net capacity of the 
power plant (MWe).</p>
</dd>
<dt><code>PR</code></dt><dd><p>Binary covariate. Value 1, indicates the prior existence 
of a LWR plant at the same site.</p>
</dd>
<dt><code>NE</code></dt><dd><p>Binary covariate, located in North-East USA</p>
</dd>
<dt><code>CT</code></dt><dd><p>Binary covariate, presence of cooling tower</p>
</dd>
<dt><code>BW</code></dt><dd><p>Binary covariate, where 1 indicates that the nuclear 
steam supply system was manufactured by Babcock-Wilcox.</p>
</dd>
<dt><code>N</code></dt><dd><p>Quantitative covariate. The cumulative number of power 
plants constructed by each architect-engineer.</p>
</dd>
<dt><code>PT</code></dt><dd><p>Binary covariate, partial turnkey guarantee.</p>
</dd>
<dt><code>cost</code></dt><dd><p>Outcome. The capital cost of construction in millions 
of dollars adjusted to 1976 base. 
</p>
</dd>
</dl>



<h3>Details</h3>

<p>Davison (2003) explores fitting models to this data using forward
and backward stepwise regression. In this modelling logs of quantiative 
variablesare used. We have also standardized this data to facilitate comparison 
with other techniques such as LARS and principal component regression.
</p>
<p>Davison and Hinkley (1997, Example 6.8, 6.10, 6.12) use this data in
a series of examples.
Example 6.8: estimation of prediction error.
Example 6.10: prediction error using cross-validation and bootstrapping.
Example 6.12: subset model selection using cross-validation.
</p>


<h3>Source</h3>

<p>Obtained from the CRAN package boot.
</p>


<h3>References</h3>

<p>Davison, A. C. (2003). Statistical Models. Cambridge: Cambridge University Press. 
</p>
<p>Davison, A.C. and Hinkley, D.V. (1997). Bootstrap Methods and their Application.
Cambridge University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(znuclear)
bestglm(znuclear, IC="BICq")
</code></pre>

<hr>
<h2 id='zprostate'> Prostate cancer data. Standardized.</h2><span id='topic+zprostate'></span>

<h3>Description</h3>

<p>Data with 8 inputs and one output used to illustrate the prediction
problem and regression in the textbook of 
Hastie, Tibshirani and Freedman (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(zprostate)</code></pre>


<h3>Format</h3>

<p>A data frame with 97 observations, 9 inputs and 1 output.
All input variables have been standardized.
</p>

<dl>
<dt><code>lcavol</code></dt><dd><p>log-cancer volume</p>
</dd>
<dt><code>lweight</code></dt><dd><p>log prostate weight</p>
</dd>
<dt><code>age</code></dt><dd><p>age in years</p>
</dd>
<dt><code>lbph</code></dt><dd><p>log benign prostatic hyperplasia</p>
</dd>
<dt><code>svi</code></dt><dd><p>seminal vesicle invasion</p>
</dd>
<dt><code>lcp</code></dt><dd><p>log of capsular penetration</p>
</dd>
<dt><code>gleason</code></dt><dd><p>Gleason score</p>
</dd>
<dt><code>pgg45</code></dt><dd><p>percent of Gleascores 4/5</p>
</dd>
<dt><code>lpsa</code></dt><dd><p>Outcome. Log of PSA</p>
</dd>
<dt><code>train</code></dt><dd><p>TRUE or FALSE</p>
</dd>
</dl>



<h3>Details</h3>

<p>A study of 97 men with prostate cancer examined the correlation 
between PSA (prostate specific antigen) and a number of clinical measurements:
lcavol, lweight, lbph, svi, lcp, gleason, pgg45
</p>


<h3>References</h3>

<p>Hastie, Tibshirani &amp; Friedman. (2009).  
The Elements of Statistical Learning: Data Mining, Inference, and Prediction.  
2nd Ed. Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Prostate data. Table 3.3 HTF.
data(zprostate)
#full dataset
trainQ&lt;-zprostate[,10]
train &lt;-zprostate[trainQ,-10]
test &lt;-zprostate[!trainQ,-10]
ans&lt;-lm(lpsa~., data=train)
sig&lt;-summary(ans)$sigma
yHat&lt;-predict(ans, newdata=test)
yTest&lt;-zprostate$lpsa[!trainQ]
TE&lt;-mean((yTest-yHat)^2)
#subset
ansSub&lt;-bestglm(train, IC="BICq")$BestModel
sigSub&lt;-summary(ansSub)$sigma
yHatSub&lt;-predict(ansSub, newdata=test)
TESub&lt;-mean((yTest-yHatSub)^2)
m&lt;-matrix(c(TE,sig,TESub,sigSub), ncol=2)
dimnames(m)&lt;-list(c("TestErr","Sd"),c("LS","Best"))
m

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
